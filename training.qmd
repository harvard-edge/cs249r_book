# AI Training

::: {.callout-tip}
## Learning Objectives

* Understand the mathematics behind deep learning (i.e: backpropagation, gradient descent)
* Identify the key operations and system bottlenecks in AI training
* Learn how CPUs and GPUs accelerate AI inference and training by speeding up key operations

:::

## Introduction

Deep learning has revolutionized the fields of machine learning and artificial intelligence, enabling computers to learn complex patterns and make intelligent decisions. At the heart of the deep learning revolution is the neural network, which, as discussed in section 3 "Deep Learning Primer", is a cornerstone in some of these advancements. In this section, we will explore the mathematics behind neural networks, specifically how neural networks work and how to train them, then dive into some of the key system challenges that underpin these models, and finally explore how to leverage CPUs and GPUs to accelerate them. 

## Mathematics behind Neural Networks and Deep Learning

At a high level, neural networks are mathematical models that consist of alternating linear and nonlinear operations, parameterized by a set of "weights" that are trained to minimize some loss function. This loss function is a measure of how good our model is with respect to fitting our training data, and it produces a numerical value (i.e: the "loss") when evaluated on our model against the training data. Fundamentally, training neural networks involve evaluating the loss function to get a measure of how good our model is, then continuously tweaking the weights of our model so that the loss decreases, which ultimately optimizes the model to fit our training data.

### Neural Network Notation

Diving into the details, the core of a neural network as introduced in section 3 can be viewed as a sequence of alternating linear and nonlinear operations:
$$
L_i = F_{i}(W_i \times L_{i-1})
$$

:::{.callout-note}
Convolutions are also linear operators, and can be cast as a matrix multiplication.
:::

where $L_{0}$ is a vector input to the neural network (i.e: an image that we want the neural network to classify, or some other data that the neural network operates on), $L_{n}$ (where $n$ is the number of layers of the network) is the vector output of the neural network (i.e: a vector of size 10 in the case of classifying pictures of handwritten digits), $W_i$s are the weights of the neural network that are tweaked at training time to fit our data, and $F_{i}$ is that layer's nonlinear activation function (i.e: ReLU, softmax, etc). As defined, the intermediate output of the neural network is a vector of real-valued numbers with dimensions:

$$
L_i \in \mathbb{R}^{d_{i}}
$$

where $d_{i}$ is the number of neurons at layer $i$; in the case of the first layer $i=0$, $d_{i}$ is the dimension of the input data, and in the last layer $i=n$, $d_{n}$ is the dimension of the output label, and anything in between can be set arbitrarily and may be viewed as the "architecture" of the neural network (i.e: dimensionality of the intermediate layers). The weights, which determine how each layer of the neural network interacts with each other, therefore are matrices of real numbers with shape

$$
W_i \in \mathbb{R}^{d_{i} \times d_{i-1}}
$$

Our neural network, as defined, performs a sequence of linear and nonlinear operations on the input data ($L_{0}$), to optain predictions ($L_{n}$) which hopefully is a good answer to what we want the neural network to do on the input (i.e: classify if the input image is a cat or not). Our neural network may then be represented succinctly as a function $N$ which takes in an input $x \in \mathbb{R}^{d_0}$ parameterized by $W_1, ...,  W_n$: 

$$
\begin{align*}
N(x; W_1, ... W_n) &= \text{Let } L_0 = x, \text{ then output } L_n
\end{align*}
$$

Next we will see how to evaluate this neural network against training data by introducing a loss function.

### Loss Function as a Measure of Goodness of Fit against Training Data

After defining our neural network, we are given some training data, which is a set of points ${(x_j, y_j)}$ for $j=1..M$, and we want to evaluate how good our neural network is on fitting this data. To do this, we introduce a "loss function", which is a function that takes the output of the neural network on a particular datapoint ($N(x_j; W_1, ..., W_n)$), and compares it against the "label" of that particular datapoint (the corresponding $y_j$), and outputs a single numerical scalar (i.e: one real number) that represents how "good" the neural network fit that particular data point; the final measure of how good the neural network is on the entire dataset is therefore just the average of the losses across all datapoints.

There are many different types of loss functions, for example, in the case of image classification, we might use the cross-entropy loss function, which tells us how good two vectors that represent classification predictions compare (i.e: if our prediction predicts that an image is more likely a dog, but the label says it is a cat, it will return a high "loss" indicating a bad fit).

Mathematically, this loss function is a function which takes in two real-valued vectors of the shape of the label, and outputs a single numerical scalar
$$
L: \mathbb{R}^{d_{n}} \times \mathbb{R}^{d_{n}} \longrightarrow \mathbb{R}
$$

and the loss across the entire dataset can be written as the average loss across all datapoints in the training data

> Loss Function for Optimizing Neural Network Model on a Dataset 
$$
L_{full} = \frac{1}{M} \sum_{j=1}^{M} L(N(x_j; W_1,...W_n), y_j)
$$

### Training Neural Networks with Gradient Descent

Now that we have a measure of how good our network fits the training data, we can optimize the weights of the neural network to minimize this loss. At a high level, we tweak the parameters of the real-valued matrices $W_i$s so that the loss function $L_{full}$ is minimized. Overall, our mathematical objective is

> Neural Network Training Objective
$$
min_{W_1, ..., W_n} L_{full}
$$
$$
= min_{W_1, ..., W_n} \frac{1}{M} \sum_{j=1}^{M} L(N(x_j; W_1,...W_n), y_j)
$$

So how do we optimize this objective? Recall from calculus that minimizing a function can be done by taking the derivative of the function with respect to the input parameters and tweaking the parameters in the direction of the gradient. This technique is called "gradient descent" and concretely involves calculating the derivative of the loss function $L_{full}$ with respect to $W_1, ..., W_n$ to obtain a gradient for these parameters to take a step in, then updating these parameters in the direction of the gradient. Thus, we can train our neural network using gradient descent which repeatedly applies the update rule

> Gradient Descent Update Rule
$$
W_i := W_i - \lambda \frac{\partial L_{full}}{\partial W_i} \mbox{ for } i=1..n
$$

:::{.callout-note}
In practice, the gradient is computed over a minibatch of datapoints, to improve computational efficiency. This is called stochastic gradient descent or batch gradient descent. 
:::

where $\lambda$ is the stepsize or learning rate of our tweaks. In training our neural network, we repeatedly perform the step above until convergence, or when the loss no longer decreases. This prior approach is known as "full gradient descent" since we are computing the derivative with respect to the entire training data, and only then taking a single gradient step; a more efficient approach is to calculate the gradient with respect to just a random "batch" of datapoints and then taking a step, a process known as "batch gradient descent" or "stochastic gradient descent", which is more efficient since now we are taking many more steps per pass of the entire training data. Next we will cover the mathematics behind computing the gradient of the loss function with respect to the $W_i$s, a process known as "backpropagation".

### Backpropagation

Training neural networks involve repeated applications of the gradient descent algorithm, which involves computing the derivative of the loss function with respect to the $W_i$s. How do we compute the derivative of the loss with respect to the $W_i$s given that the $W_i$s are nested functions of each other in a deep neural network? The trick is to leverage the "chain rule": we can compute the derivative of the loss with respect to the $W_i$s by repeatedly applying the chain rule, in a complete process known as "backpropagation". Specifically, we can calculate the gradients by computing the derivative of the loss with respect to the outputs of the last layer, then progressively use this to compute the derivative of the loss with respect to each prior layer, all the way to the input layer. This process starts from the end of the network (the layer closest to the output) and progresses backwards, and hence gets its name "backpropagation".

Let's break this down. We can compute the derivative of the loss with respect to the _the outputs of each layer of the neural network_ by using repeated applications of the chain rule

$$
\frac{\partial L_{full}}{L_{n}} = \frac{\partial \frac{1}{M} \sum_{j=1}^{M} L(N(x_j; W_1, ..., W_n), y_j)}{\partial L_{n}}
$$

$$
\frac{\partial L_{full}}{\partial L_{n-1}} = \frac{\partial L_{full}}{\partial L_{n}} \frac{\partial L_{n}}{\partial L_{n-1}}
$$

or more generally

$$
\frac{\partial L_{full}}{L_{i}} = \frac{\partial L_{full}}{\partial L_{n}} \frac{\partial L_{n}}{\partial L_{n-1}} \frac{\partial L_{n-1}}{\partial L_{n-2}} ...  \frac{\partial L_{i+1}}{\partial L_{i}}
$$

After computing the derivative of the loss with respect to the _output of each layer_, we can easily obtain the derivative of the loss with respect to the _parameters_, again using the chain rule:

$$
\frac{\partial L_{full}}{W_{i}} = \frac{\partial L_{full}}{L_{i}} \frac{L_{i}}{W_{i}}
$$

And this is ultimately how the derivatives of the layers' weights are computed using backpropagation! What does this concretely look like in a specific example? Below we walk through a specific example on a simple 2 layer neural network, on a regression task using a MSE loss function, with 100-dimensional inputs and a 30-dimensional hidden layer:

> Example of Backpropagation\
Suppose we have a two-layer neural network
$$
L_1 = ReLU(W_1 \times L_{0})
$$
$$
L_2 = ReLU(W_2 \times L_{1})
$$
$$
NN(x) = \mbox{Let } L_{0} = x \mbox{ then output } L_2
$$
where $W_1 \in \mathbb{R}^{30 \times 100}$ and $W_2 \in \mathbb{R}^{1 \times 30}$. Furthermore suppose we use the MSE loss function:
$$
L(x, y) = (x-y)^2
$$
We wish to compute
$$
\frac{\partial L(NN(x), y)}{\partial W_i} \mbox{ for } i=1,2
$$
We start by computing the gradient with respect to the final output:
$$
\frac{\partial L(NN(x), y)}{\partial L_2} = \frac{\partial (L_2 - y)^2}{\partial L_2} = 2(L_2 - y)
$$
With respect to the output $L_1$
$$
\frac{\partial L(NN(x), y)}{\partial L_1} = \frac{\partial L(NN(x), y)}{\partial L_2} \frac{\partial L_2}{\partial L_1} 
$$
$$
= [2(L_2 - y)] \times \frac{\partial ReLU(W_2 \times L_1)}{\partial L_1} = [2(L_2 - y)] \times W_2^T ReLU'(W_2 \times L_1) 
$$
where
$$
ReLU'(x) = \begin{cases} 
      0 & x\leq 0 \\
      1 & x > 0
   \end{cases}
$$
Then we can compute the gradients with respect to the weights
$$
\frac{\partial L(NN(x), y)}{\partial W_2} = \frac{\partial L(NN(x), y)}{\partial L_2} \frac{\partial L_2}{\partial W_2} = [2(L_2 - y)] \times ReLU'(W_2 \times L_1) L_1^T
$$
$$
\frac{\partial L(NN(x), y)}{\partial W_1} = \frac{\partial L(NN(x), y)}{\partial L_1} \frac{\partial L_1}{\partial W_1} =  [(2(L_2 - y)) \times W_2^T ReLU'(W_2 \times L_1)] \times ReLU'(W_1 \times L_0) 
$$
IMPORTANT: QUICKLY WRITTEN SO THIS IS NOT REALLY CORRECT THERE IS ACTUALLY A BUG HERE, REDO!

## Optimization Algorithms

Stochastic Gradient Descent involves updating the model's parameters by considering the gradient of the loss function with respect to the parameters for each training example. While the basic concept of SGD is straightforward, finding the optimal set of parameters that minimizes the overall loss across the entire dataset may be difficult as the loss landscape is nonconvex.

To address the complexities of training neural networks, various optimization algorithms have been developed. These optimizers are designed to enhance the efficiency and convergence speed of the training process. They achieve this by adjusting the learning rates, incorporating momentum, and implementing adaptive strategies, among other techniques.

Some optimizers include:

* ADAM
* AdaGrad
* RMSProp
* Momentum SGD

Generally, from our experience Adam is the most popular optimizer and usually outperforms SGD in terms of training neural networks.

## Hyperparameter Tuning

The performance of the neural network model depends on a set of
crucial configurations known as hyperparameters.  These
hyperparameters, which are external to the model and cannot be learned
during training, play a pivotal role in determining the model's
effectiveness, generalization capabilities, and overall performance on
diverse datasets.  These hyperparameters include learning rate, batch
size, regularization strengths, and network architectures.

Several techniques are employed to conduct hyperparameter search,
ranging from manual tuning by domain experts to automated methods such
as grid search, random search, and more sophisticated optimization
algorithms like Bayesian optimization.

## Regularization

Neural networks are optimized to fit the training data, however, we
would like the network to generalize and perform well to unseen data.

Regularization stands as a key technique in achieving this. It serves
as a tool in preventing overfitting, a common pitfall where a model
becomes overly complex and tailored to the training data, losing its
ability to generalize.

Common regularization techniques include

* L1 and L2 Regularization

    These methods add a penalty term based on the magnitudes of the model's parameters to the loss function. L1 regularization encourages sparsity by introducing a penalty proportional to the absolute values of the parameters, while L2 regularization penalizes the square of the parameter values, promoting smaller weights.
    
* Dropout

    A technique commonly applied to neural networks, dropout involves randomly "dropping out" a fraction of neurons during each training iteration. This helps prevent co-adaptation of neurons, promoting more robust and generalized learning.

* Early Stopping

    By monitoring the model's performance on a validation set during training, early stopping interrupts the training process when the model's performance ceases to improve. This prevents the model from becoming overly specialized to the training data.

## Weight Initialization

Neural Networks are trained starting with weights initialized randomly, but how the weights are initialized may have a considerable impact on training convergence and feasibility.
Proper weight initialization helps in overcoming issues like vanishing or exploding gradients, which can hinder the learning process. Here are some commonly used neural network weight initialization techniques:


 as orthogonal matrices. This helps in preserving the gradients during backpropagation and can be particularly useful in recurrent neural networks (RNNs).
 Uniform and Normal Initialization:

Weights are initialized with random values drawn from a uniform or normal distribution. The choice between uniform and normal depends on the specific requirements of the model and the activation functions used.
Choosing the right initialization method depends on the architecture of the neural network, the activation functions, and the specific problem being solved. Experimentation with different techniques is often necessary to find the most suitable initialization for a given scenario.

* Xavier/Glorot Initialization

    Proposed by Xavier Glorot and Yoshua Bengio, this initialization is designed to work well with activation functions like tanh or logistic sigmoid. It initializes weights with random values drawn from a distribution with mean 0 and variance 2 / (number of input units + number of output units).

* He Initialization

    Proposed by Kaiming He et al., this initialization is tailored for ReLU (Rectified Linear Unit) activation functions. It initializes weights with random values drawn from a distribution with mean 0 and variance 2 / number of input units. This helps to mitigate the vanishing gradient problem often associated with deep networks.

## Activation Functions

Activation functions play a critical role in neural networks by introducing non-linearities into the model. These non-linearities enable neural networks to learn complex relationships and patterns in data, making them capable of solving a wide range of problems. Here are some commonly used activation functions:

* Rectified Linear Unit (ReLU):

    ReLU is a popular activation function that returns zero for
    negative input values and passes positive input values
    unchanged. It is computationally efficient and has been widely
    used in deep learning models.

* Sigmoid Function

    The sigmoid function, also known as the logistic function,
    squashes input values between 0 and 1. It is often used in the
    output layer of binary classification models, where the goal is to
    produce probabilities.

* Hyperbolic Tangent Function (tanh):

    The hyperbolic tangent function is similar to the sigmoid but
    squashes input values between -1 and 1. It is often used in hidden
    layers of neural networks, especially when zero-centered outputs
    are desired.

## Key System Bottlenecks in AI inference & training

As introduced, neural networks consist of alternating linear and nonlinear operations. The main performance bottleneck is the linear layer, a matrix multiplication that maps the previous activations inputs to the next layer's activation function.

### Runtime Complexity of Matrix Multiplication in Neural Networks

Matrix multiplication is the main performance bottleneck in both
neural network inference and training since its runtime complexity is
the product of the dimensions of the input and output layers of the
neural network, as well as the batch size.  Generally, using a batch
size of $B$ (i.e: training on batches of $B$ datapoints at a time),
with an input layer dimension of $M$ nodes, and an output layer
dimensions of $N$ nodes, the linear layer is a matrix-matrix
multiplication size $N \times M$ by a $M \times B$, leading to a
complexity of $O(NMB)$. This far dominates the computational
complexity of the nonlinear activation functions which only need to be
applied element-wise to the output vectors.

### Compute vs Memory Bottleneck in Neural Network Training and Inference

As previously outlined, matrix multiplication is the key operational
bottleneck in both neural network training and inference. However,
this operation may pose a bottleneck to either the memory or
computational capability of the underlying hardware system.
Concretely, batched matrix multiplication (i.e: matrix-multiplicaton
with a high batch-size $B$) exhibits a much higher computation to
memory ratio, and hence the underlying computational hardware must
have more arithmetic computing capabilities than memory-transfer
abilities to maximize performance in these scenarios (this is often
the case in both CPUs and GPUs). This is because matrix multiplication
performs $O(NMB)$ arithmetic operations, but only $O(NM + MB)$ memory
operations; hence when $B$ is large, neural network inference/training
requires more arithmetic operations, and when $B$ is small it requires
relatively more memory operations. This detail is important since most
of today's hardware is better at performing computation rather than
memory transfer, and hence from a computational perspective it is
better to maximize batch size $B$ at training time to fully utilize
the hardware. However, using a larger batch size $B$ at training time
means doing fewer updates per pass of the dataset, which might
decrease convergence time. Hence, selecting batch size $B$ has a
considerable impact on both the convergence, computational efficiency,
and runtime of neural networks, and must be tuned to attain high
performance. Generally, a good rule of thumb for batch size is between
8-128.

### Optimizing Matrix Multiplication

Matrix multiplication is essential to the performance of neural
network inference and training, and hence efficiently evaluating
matrix multiplication is crucial to performance.  Broadly, various
methods to accelerate matrix-multiplication include carefully writing
the matrix-multiply routine to leverage the caches of the CPU,
leveraging hardware-level parallel operations (i.e: SIMD), using
hardware capabilities that perform multiply-adds simultaneously (i.e:
fused add-multiply, and others. Even better at performing matrix
multiplication than the CPU are GPUs, which have hardware that is
suitable for performing a mass amount of parallel operations, a
capability that is perfect for executing matrix-multiplication whose
individual steps are highly parallelizable. Using the GPU, matrix
multiplication can be accelerated by a significant factor over the GPU
since the matrix-multiplication can be efficiently parallelized on a
GPU, using standard techniques like blocking and tiling to maximize
resource utilization. Other alternative ways to accelerate
matrix-multiplication include using FPGAs, or dedicated hardware like
TPUs (i.e: systolic arrays).

## Parallelizing AI training

### Data Parallel

### Model Parallel


## Efficient and Distributed Training

## Debugging and Profiling