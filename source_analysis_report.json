{
  "timestamp": "2025-07-23T21:20:22.381281",
  "summary": {
    "academic_citations": 45,
    "company_sources": 47,
    "link_sources": 57,
    "problematic_asterisk": 1,
    "missing_periods": 0,
    "lowercase_sources": 0,
    "double_periods": 7,
    "malformed_citations": 0,
    "total_files": 61,
    "files_with_sources": 16
  },
  "problems": {
    "asterisk_sources": [
      {
        "file": "contents/core/robust_ai/robust_ai.qmd",
        "line": 875,
        "text": "*Source: [Google](HTTPS://www.Google.com/url?sa=i&url=HTTP%3A%2F%2fresearch.Google%2fblog%2funsupervised-and-semi-supervised-\n:::\n\n###### Consistency checks and data validation {#sec-robust-ai-consistency-checks-data-validation-63f2}\n\nConsistency checks and data validation techniques ensure data integrity and correctness at different processing stages in an ML system [@lindholm2019data]. These checks help detect data corruption, inconsistencies, or errors that may propagate and affect the system's behavior. Example: In a distributed ML system where multiple nodes collaborate to train a model, consistency checks can be implemented to validate the integrity of the shared model parameters. Each node can compute a checksum or hash of the model parameters before and after the training iteration, as shown in @fig-ad. Any inconsistencies or data corruption can be detected by comparing the checksums across nodes. Additionally, range checks can be applied to the input data and model outputs to ensure they fall within expected bounds. For instance, if an autonomous vehicle's perception system detects an object with unrealistic dimensions or velocities, it can indicate a fault in the sensor data or the perception algorithms [@wan2023vpp].\n\n###### Heartbeat and timeout mechanisms {#sec-robust-ai-heartbeat-timeout-mechanisms-c3a4}\n\nHeartbeat mechanisms and timeouts are commonly used to detect faults in distributed systems and ensure the liveness and responsiveness of components [@kawazoe1997heartbeat]. These are quite similar to the watchdog timers found in hardware. For example, in a distributed ML system, where multiple nodes collaborate to perform tasks such as data preprocessing, model training, or inference, heartbeat mechanisms can be implemented to monitor the health and availability of each node. Each node periodically sends a heartbeat message to a central coordinator or its peer nodes, indicating its status and availability. Suppose a node fails to send a heartbeat within a specified timeout period, as shown in @fig-heartbeat. In that case, it is considered faulty, and appropriate actions can be taken, such as redistributing the workload or initiating a failover mechanism. Timeouts can also be used to detect and handle hanging or unresponsive components. For example, if a data loading process exceeds a predefined timeout threshold, it may indicate a fault in the data pipeline, and the system can take corrective measures.\n\n::: {#fig-heartbeat fig-env=\"figure\" fig-pos=\"htb\"}\n```{.tikz}\n\\scalebox{0.8}{%\n\\begin{tikzpicture}[line join=round,font=\\usefont{T1}{phv}{m}{n}\\footnotesize]\n\\definecolor{Green}{RGB}{5,130,88}\n\\tikzset{%\nhelvetica/.style={align=flush center,font=\\small\\usefont{T1}{phv}{m}{n}},\nLine/.style={line width=1.0pt,black!50,text=black},\nDLine/.style={draw=RedLine!30, line width=1mm, -{Triangle[length=3mm, bend]},\nshorten >=1.1mm, shorten <=1.15mm},\nBox/.style={circle,\n    inner xsep=2pt,\n    node distance=3.0,\n    draw=GreenLine,\n    line width=0.75pt,\n    font=\\usefont{T1}{phv}{m}{n}\\small,\n    align=flush center,\n    fill=GreenL,\n    minimum size=19mm\n  },\n}\n\\node[Box](B1){Node 1};\n\\node[Box,right= of B1](B2){Node 2};\n\\node[Box, right=of B2](B3){Node 3};\n\\draw[Line,-latex](B1.15)--node[fill=BlueL,sloped,pos=0.3]{Ack}(B2.165);\n\\draw[Line,-latex](B2.195)--node[fill=OliveL,sloped,pos=0.3]{Ack}(B1.345);\n\\draw[Line,-latex](B3)--node[fill=VioletL,sloped,pos=0.5]{Ack}(B2);\n%\n\\draw[DLine,distance=35](B2.120)to[out=120,in=60]\n             node[fill=BlueL,sloped,pos=0.5]{Heartbeat}(B1.60);\n\\draw[DLine,distance=35](B1.300)to[out=300,in=240]\n             node[fill=OliveL,sloped,pos=0.5]{Heartbeat}(B2.240);\n\\draw[DLine,distance=35](B2.60)to[out=60,in=120]\n             node[fill=VioletL,sloped,pos=0.5]{Heartbeat}(B3.120);\n%\n\\coordinate(L)at($(B1.west)+(0,-2.2)$);\n\\path[red](L)-|coordinate(D)(B3.east);\n\\draw[Green,line width=1pt](L)--node[pos=0.9](SR){}(D);\n\\node[below=2pt of $(L)!0.2!(D)$]{What are Heartbeat Messages?};\n%GeeksforGeeks logo\n\\begin{scope}[scale=0.4, every node/.append style={transform shape},\nlocal bounding box=D1,shift={($(SR)+(0,0.4)$)}]\n\\fill[white] (-2.2,-1.1) rectangle (0.4,0.3);\n\\draw[Green,line width=2pt] (0,0) arc[start angle=50, end angle=350, radius=0.5]--++(180:1.03);\n\\begin{scope}[xscale=-1]\n\\draw[Green,line width=2pt] (1.8,0) arc[start angle=50, end angle=350, radius=0.5]--++(180:1.03);\n\\end{scope}\n\\end{scope}\n\\end{tikzpicture}}\n```\n*"
      }
    ],
    "missing_periods": [],
    "lowercase_sources": [],
    "double_periods": [
      {
        "file": "contents/core/privacy_security/privacy_security.qmd",
        "line": 403,
        "text": ": **Model Stealing Costs**: Attackers can extract model weights with a relatively low query cost using publicly available apis; the table quantifies this threat for OpenAI’s ada and babbage models, showing that extracting weights achieves low root mean squared error (RMSE) with fewer than \\(4 \\cdot 10^6\\) queries. Estimated costs for weight extraction range from $1 to $12, demonstrating the economic feasibility of model stealing attacks despite API rate limits and associated expenses. Source: carlini et al., 2024.. {#tbl-openai-theft .striped .hover}"
      },
      {
        "file": "contents/core/data_engineering/data_engineering.qmd",
        "line": 827,
        "text": "**Synthetic Data Augmentation**: Combining algorithmically generated data with historical datasets expands training set size and diversity, mitigating limitations caused by scarce or biased real-world data and improving model generalization. This approach enables robust machine learning system development when acquiring sufficient real-world data is impractical or unethical. Source: [anylogic](HTTPS://www.anylogic.com/features/artificial-intelligence/synthetic-data/).."
      },
      {
        "file": "contents/core/dnn_architectures/dnn_architectures.qmd",
        "line": 1229,
        "text": "**Query-Key-Value Interaction**: Transformer attention mechanisms dynamically weigh input sequence elements by computing relationships between queries, keys, and values, enabling the model to focus on relevant information. These projections facilitate the creation of an attention matrix that determines the contribution of each value vector to the final output, effectively capturing contextual dependencies within the sequence. Source: [transformer explainer](HTTPS://poloclub.GitHub.io/transformer-explainer/).."
      },
      {
        "file": "contents/core/dnn_architectures/dnn_architectures.qmd",
        "line": 1387,
        "text": "**Dynamic Attention Weights**: Transformer models calculate attention weights dynamically based on the relationships between query, key, and value vectors, allowing the model to focus on relevant parts of the input sequence for each processing step. This contrasts with fixed-weight architectures and enables adaptive pattern processing crucial for handling variable-length inputs and complex dependencies. Source: [transformer explainer](HTTPS://poloclub.GitHub.io/transformer-explainer/).."
      },
      {
        "file": "contents/core/optimizations/optimizations.qmd",
        "line": 5565,
        "text": "![**Sparsity Distribution**: Pruned neural networks exhibit varying degrees of weight removal across layers; darker shades indicate higher sparsity, revealing which parts of the model were most affected by the pruning process. Analyzing this distribution helps practitioners understand and refine sparsity-aware optimization strategies for model compression and efficiency. Source: [numenta](HTTPS://www.numenta.com/blog/)..](https://www.numenta.com/wp-content/uploads/2020/10/Picture1.png){#fig-sprase-heat-map}"
      },
      {
        "file": "contents/core/robust_ai/robust_ai.qmd",
        "line": 179,
        "text": "**Silent Data Corruption**: Unexpected faults can return incorrect file sizes, leading to data loss during decompression and propagating errors through distributed querying systems despite apparent operational success. This example from Facebook emphasizes the challenge of undetected errors—silent data corruption—and the importance of robust error detection mechanisms in large-scale data processing pipelines. Source: [Facebook](HTTPS://arxiv.org/PDF/2102.11245).."
      },
      {
        "file": "contents/core/robust_ai/robust_ai.qmd",
        "line": 1104,
        "text": "![**Adversarial Perturbations**: Subtle, intentionally crafted noise added to an image can cause a trained deep neural network (googlenet) to misclassify it, even though the perturbed image remains visually indistinguishable to humans. This vulnerability underscores the lack of robustness in many machine learning models and motivates research into adversarial training and defense mechanisms. Source: goodfellow et al., 2014..](./images/png/adversarial_googlenet.png){#fig-adversarial-googlenet}"
      }
    ],
    "malformed_citations": [],
    "extra_brackets": []
  },
  "files_analyzed": 61,
  "recommendations": [
    "Run cleanup to fix asterisk-wrapped sources",
    "Remove double periods from source citations"
  ]
}