{
  "metadata": {
    "type": "master_glossary",
    "version": "3.0.0",
    "generated": "2025-09-15T16:26:34.715924",
    "total_terms": 617,
    "standardized": true,
    "cleaned": true,
    "description": "Cleaned and standardized master glossary with merged duplicates and consistent formatting"
  },
  "terms": [
    {
      "term": "3dmark",
      "definition": "Graphics performance benchmark suite that evaluates real-time 3D rendering capabilities, measuring triangle throughput, texture fill rates, and modern features like ray tracing and DLSS performance.",
      "chapter_source": "benchmarking",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "a/b testing",
      "definition": "A controlled experimental method for comparing two versions of a system or model by randomly dividing users into groups and measuring performance differences between the variants.",
      "chapter_source": "workflow",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "accountability",
      "definition": "The mechanisms by which individuals or organizations are held responsible for the outcomes of AI systems, involving traceability, documentation, auditing, and the ability to remedy harms.",
      "chapter_source": "responsible_ai",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "activation checkpointing",
      "definition": "A memory optimization technique that reduces memory usage during backpropagation by selectively discarding and recomputing activations instead of storing all intermediate results.",
      "chapter_source": "training",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "activation function",
      "definition": "A mathematical function applied to the output of a neural network layer to introduce non-linearity, enabling the network to learn complex patterns. Common activation functions include ReLU, sigmoid, tanh, and softmax. Alternative definition: A mathematical function applied to the weighted sum of inputs in a neural network neuron to introduce nonlinearity, enabling the network to learn complex patterns beyond simple linear combinations. Alternative definition: A mathematical function applied to the output of neural network layers to introduce non-linearity, enabling the network to learn complex patterns beyond linear relationships. Alternative definition: A mathematical function applied element-wise to neural network outputs to introduce non-linearity, enabling networks to learn complex patterns and preventing them from collapsing into linear models.",
      "chapter_source": "dl_primer",
      "aliases": [],
      "see_also": [],
      "appears_in": [
        "dl_primer",
        "dnn_architectures",
        "frameworks",
        "training"
      ]
    },
    {
      "term": "activation-based pruning",
      "definition": "A pruning method that evaluates the average activation values of neurons or filters over a dataset to identify and remove neurons that consistently produce low activations and contribute little information to the network's decision process.",
      "chapter_source": "optimizations",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "active learning",
      "definition": "Iteratively selecting the most informative samples for labeling to maximize learning efficiency, achieving target performance with 50-90% less labeled data compared to random sampling strategies.",
      "chapter_source": "data_engineering",
      "aliases": [],
      "see_also": [],
      "appears_in": [
        "data_engineering",
        "efficient_ai"
      ]
    },
    {
      "term": "adam optimization",
      "definition": "An adaptive learning rate optimization algorithm that combines momentum and RMSprop by maintaining exponentially decaying averages of both gradients and squared gradients for each parameter.",
      "chapter_source": "training",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "adapter modules",
      "definition": "Small trainable neural network components inserted between frozen layers of a pretrained model to enable lightweight adaptation without modifying the base architecture.",
      "chapter_source": "ondevice_learning",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "adaptive resource pattern",
      "definition": "A design pattern that enables systems to dynamically adjust their operations in response to varying resource availability, ensuring efficiency and resilience by scaling up or down based on computational load, network bandwidth, and storage capacity.",
      "chapter_source": "ai_for_good",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "adversarial attack",
      "definition": "A type of attack where carefully crafted inputs are designed to cause machine learning models to make incorrect predictions while remaining nearly indistinguishable from legitimate data to humans. Alternative definition: A deliberate attempt to deceive machine learning models by crafting carefully designed inputs that cause incorrect predictions or behaviors while appearing benign to humans.",
      "chapter_source": "privacy_security",
      "aliases": [],
      "see_also": [],
      "appears_in": [
        "privacy_security",
        "robust_ai"
      ]
    },
    {
      "term": "adversarial example",
      "definition": "A maliciously modified input that is designed to fool a machine learning model into making an incorrect prediction, often created by adding small, imperceptible perturbations to legitimate data.",
      "chapter_source": "privacy_security",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "adversarial examples",
      "definition": "Maliciously crafted inputs designed to fool machine learning models into making incorrect predictions, often by adding imperceptible perturbations to legitimate data. Alternative definition: Carefully crafted inputs that are nearly identical to legitimate inputs but cause machine learning models to make incorrect predictions with high confidence.",
      "chapter_source": "responsible_ai",
      "aliases": [],
      "see_also": [],
      "appears_in": [
        "responsible_ai",
        "robust_ai"
      ]
    },
    {
      "term": "adversarial training",
      "definition": "A training technique where two neural networks compete against each other, with one generating data and another trying to distinguish real from generated content.",
      "chapter_source": "generative_ai",
      "aliases": [],
      "see_also": [],
      "appears_in": [
        "generative_ai",
        "privacy_security",
        "responsible_ai",
        "robust_ai"
      ]
    },
    {
      "term": "ai for good",
      "definition": "The design, development, and deployment of machine learning systems aimed at addressing important societal and environmental challenges to enhance human welfare, promote sustainability, and contribute to global development goals.",
      "chapter_source": "ai_for_good",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "alerting",
      "definition": "Automated notification systems that inform teams when metrics exceed predefined thresholds or anomalies are detected in production ML systems.",
      "chapter_source": "ops",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "alexnet",
      "definition": "A breakthrough deep neural network from 2012 that won the ImageNet competition by a large margin and helped spark the deep learning revolution, proving that neural networks could outperform traditional computer vision methods when given enough data and computing power. Alternative definition: Pioneering 8-layer convolutional neural network developed in 2012 that revolutionized computer vision by introducing key innovations like ReLU activations, dropout regularization, and data augmentation techniques. Alternative definition: A groundbreaking convolutional neural network architecture that won the 2012 ImageNet challenge, reducing error rates from 26% to 16% and sparking the deep learning renaissance.",
      "chapter_source": "benchmarking",
      "aliases": [],
      "see_also": [],
      "appears_in": [
        "benchmarking",
        "dnn_architectures",
        "introduction"
      ]
    },
    {
      "term": "algorithmic fairness",
      "definition": "The principle that automated systems should not disproportionately disadvantage individuals or groups based on protected attributes such as race, gender, or age.",
      "chapter_source": "responsible_ai",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "algorithmic_efficiency",
      "definition": "The design and optimization of algorithms to maximize performance within given resource constraints, focusing on techniques like model compression, architectural optimization, and algorithmic refinement.",
      "chapter_source": "efficient_ai",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "all-reduce",
      "definition": "A collective communication operation in distributed computing where each process contributes data and all processes receive the combined result, commonly used for gradient aggregation in distributed training.",
      "chapter_source": "training",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "alphafold",
      "definition": "A landmark AI system developed by DeepMind that predicts the three-dimensional structure of proteins from their amino acid sequences, solving the decades-old \"protein folding problem\" and demonstrating how large-scale ML systems can accelerate scientific discovery.",
      "chapter_source": "introduction",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "anomaly detection",
      "definition": "The identification of patterns in data that do not conform to expected behavior, often used to detect outliers, faults, or malicious activities in systems.",
      "chapter_source": "robust_ai",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "anonymization",
      "definition": "The process of removing or modifying personally identifiable information from datasets to protect individual privacy, though often insufficient against sophisticated re-identification attacks.",
      "chapter_source": "privacy_security",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "apache kafka",
      "definition": "A distributed streaming platform that handles real-time data feeds using a publish-subscribe messaging system, commonly used for building ML data pipelines with high throughput and fault tolerance.",
      "chapter_source": "data_engineering",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "apache spark",
      "definition": "An open-source distributed computing framework that enables large-scale data processing across clusters of computers, revolutionizing ETL operations with in-memory computing capabilities.",
      "chapter_source": "data_engineering",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "application-specific integrated circuit",
      "definition": "A specialized chip designed for specific tasks that offers maximum efficiency by abandoning general-purpose flexibility, exemplified by Cerebras Wafer-Scale Engine for machine learning training.",
      "chapter_source": "training",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "application-specific integrated circuit (asic)",
      "definition": "Custom chips designed for specific computational tasks that offer superior performance and energy efficiency compared to general-purpose processors, exemplified by Google's TPUs and Bitcoin mining ASICs. Alternative definition: A custom-designed microchip optimized for a specific computational task, offering maximum performance and energy efficiency by implementing only the necessary functionality in silicon.",
      "chapter_source": "benchmarking",
      "aliases": [],
      "see_also": [],
      "appears_in": [
        "benchmarking",
        "hw_acceleration"
      ]
    },
    {
      "term": "architectural efficiency",
      "definition": "The dimension of model optimization that focuses on how computations are performed efficiently during training and inference by exploiting sparsity, factorizing large components, and dynamically adjusting computation based on input complexity.",
      "chapter_source": "optimizations",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "artificial general intelligence",
      "definition": "A hypothetical form of AI that matches or exceeds human cognitive abilities across all domains, representing the ultimate goal of AI research beyond current narrow AI systems.",
      "chapter_source": "frontiers",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "artificial intelligence",
      "definition": "The systematic pursuit of understanding and replicating intelligent behavior through the development of computer systems that can perform tasks typically requiring human intelligence, such as learning, problem-solving, perception, and decision-making. Alternative definition: Computer systems designed to perform tasks that typically require human intelligence, including learning, reasoning, perception, and decision-making. Alternative definition: Computer systems able to perform tasks that typically require human intelligence, including learning, reasoning, perception, and decision-making. Alternative definition: The simulation of human intelligence processes by machines, particularly computer systems, encompassing learning, reasoning, and self-correction capabilities. Alternative definition: A broad field of computer science focused on creating systems that can perform tasks typically requiring human intelligence, including learning, reasoning, and decision-making. Alternative definition: The field of computer science focused on creating systems that can perform tasks typically requiring human intelligence, such as perception, reasoning, learning, and decision-making. Alternative definition: A field of computer science focused on creating systems capable of performing tasks that typically require human intelligence, such as perception, reasoning, learning, and decision-making.",
      "chapter_source": "benchmarking",
      "aliases": [],
      "see_also": [],
      "appears_in": [
        "benchmarking",
        "conclusion",
        "dl_primer",
        "introduction",
        "ml_systems",
        "responsible_ai",
        "sustainable_ai"
      ]
    },
    {
      "term": "artificial neural network",
      "definition": "A computational model inspired by biological neural networks, consisting of interconnected nodes (neurons) organized in layers that can learn patterns from data through adjustable weights and biases.",
      "chapter_source": "dl_primer",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "artificial neurons",
      "definition": "Basic computational units in neural networks that mimic biological neurons, taking multiple inputs, applying weights and biases, and producing an output signal through an activation function.",
      "chapter_source": "introduction",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "attention mechanism",
      "definition": "A neural network component that computes weighted connections between elements based on their content, allowing dynamic focus on relevant parts of the input rather than fixed architectural connections.",
      "chapter_source": "dnn_architectures",
      "aliases": [],
      "see_also": [],
      "appears_in": [
        "dnn_architectures",
        "generative_ai"
      ]
    },
    {
      "term": "autoencoder",
      "definition": "A neural network architecture that learns to compress data into a lower-dimensional representation and then reconstruct the original data, often used as a building block for generative models. Alternative definition: A neural network architecture that learns compressed data representations by minimizing reconstruction error, commonly used for anomaly detection and dimensionality reduction.",
      "chapter_source": "generative_ai",
      "aliases": [],
      "see_also": [],
      "appears_in": [
        "generative_ai",
        "robust_ai"
      ]
    },
    {
      "term": "automatic differentiation",
      "definition": "A computational technique that automatically calculates exact derivatives of functions implemented as computer programs by systematically applying the chain rule at the elementary operation level, essential for training neural networks through gradient-based optimization.",
      "chapter_source": "frameworks",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "automatic mixed precision",
      "definition": "A training technique that automatically manages the use of different numerical precisions (FP16, FP32) to optimize memory usage and computational speed while maintaining model accuracy.",
      "chapter_source": "training",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "automation bias",
      "definition": "The tendency for humans to over-rely on automated system outputs even when clear errors are present, potentially compromising human oversight.",
      "chapter_source": "responsible_ai",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "automl",
      "definition": "Automated Machine Learning that uses machine learning itself to automate model design decisions, including architecture search, hyperparameter optimization, and feature selection to create efficient models without manual intervention. Alternative definition: Automated machine learning that enables exploration of different model architectures, hyperparameter configurations, and feature engineering techniques to identify optimal performance-efficiency balances. Alternative definition: Automated machine learning that uses algorithms to automate the process of applying machine learning to real-world problems, including feature engineering, model selection, and hyperparameter tuning.",
      "chapter_source": "efficient_ai",
      "aliases": [],
      "see_also": [],
      "appears_in": [
        "efficient_ai",
        "frontiers",
        "optimizations"
      ]
    },
    {
      "term": "autoregressive",
      "definition": "Models that generate sequences by predicting the next element based on previous elements, such as GPT models that generate text one token at a time.",
      "chapter_source": "efficient_ai",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "autoregressive_model",
      "definition": "A generative model that produces output sequences one element at a time, where each new element depends on all previously generated elements.",
      "chapter_source": "generative_ai",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "autoscaling",
      "definition": "Dynamic adjustment of compute resources based on workload demand, automatically scaling up during peak usage and scaling down during low usage to optimize costs and performance.",
      "chapter_source": "ops",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "availability attack",
      "definition": "A type of data poisoning attack that aims to degrade the overall performance of a machine learning model by introducing noise or corrupting training data across multiple classes.",
      "chapter_source": "privacy_security",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "backdoor attack",
      "definition": "A type of data poisoning where hidden triggers are embedded in training data, causing models to behave maliciously when specific patterns are encountered during inference. Alternative definition: A type of attack where hidden triggers are embedded in a model during training, causing malicious behavior only when specific trigger patterns are present in the input.",
      "chapter_source": "privacy_security",
      "aliases": [],
      "see_also": [],
      "appears_in": [
        "privacy_security",
        "robust_ai"
      ]
    },
    {
      "term": "backpropagation",
      "definition": "The algorithm used to train neural networks by computing gradients of the loss function with respect to network parameters through reverse-mode automatic differentiation, enabling efficient parameter updates during training. Alternative definition: An algorithm that computes gradients of the loss function with respect to network weights by propagating error signals backward through the network layers, enabling systematic weight updates during training. Alternative definition: The algorithm used to train neural networks by computing gradients of the loss function with respect to network parameters and propagating error signals backward through the network layers. Alternative definition: The algorithm for computing gradients in neural networks by propagating error signals backward through layers, essential for training but computationally expensive on resource-constrained devices. Alternative definition: The algorithm used to train neural networks by calculating gradients and propagating error signals backward through network layers to update weights. Alternative definition: An algorithm that computes gradients by systematically applying the chain rule backward through a neural network's computational graph, enabling parameter updates during training.",
      "chapter_source": "dl_primer",
      "aliases": [],
      "see_also": [],
      "appears_in": [
        "dl_primer",
        "dnn_architectures",
        "frameworks",
        "ondevice_learning",
        "sustainable_ai",
        "training"
      ]
    },
    {
      "term": "bandwidth",
      "definition": "The maximum rate of data transfer across a communication channel or memory interface, typically measured in bytes per second and critical for optimizing data movement in AI accelerators.",
      "chapter_source": "hw_acceleration",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "batch inference",
      "definition": "Processing large volumes of data predictions asynchronously in scheduled batches rather than real-time, suitable for non-interactive applications like reporting or analytics.",
      "chapter_source": "ops",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "batch ingestion",
      "definition": "A data processing pattern that collects and processes data in groups or batches at scheduled intervals, suitable for scenarios where real-time processing is not critical.",
      "chapter_source": "data_engineering",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "batch normalization",
      "definition": "A technique that normalizes inputs to each layer to have zero mean and unit variance, which stabilizes training and often allows for higher learning rates and faster convergence. Alternative definition: A technique that normalizes the inputs to each layer in a neural network to have zero mean and unit variance, which helps stabilize training and allows higher learning rates. Alternative definition: A technique that normalizes the inputs of each layer to have zero mean and unit variance, stabilizing training and allowing higher learning rates in deep networks.",
      "chapter_source": "dnn_architectures",
      "aliases": [],
      "see_also": [],
      "appears_in": [
        "dnn_architectures",
        "frameworks",
        "training"
      ]
    },
    {
      "term": "batch processing",
      "definition": "The technique of processing multiple data samples simultaneously to amortize computation and memory access costs, improving overall throughput in neural network training and inference. Alternative definition: Method of processing multiple inputs simultaneously to improve computational efficiency by leveraging hardware optimizations and amortizing fixed costs across multiple samples. Alternative definition: A computational approach that processes large volumes of data in discrete chunks during scheduled time windows, enabling efficient handling of massive datasets for ML training.",
      "chapter_source": "benchmarking",
      "aliases": [],
      "see_also": [],
      "appears_in": [
        "benchmarking",
        "data_engineering",
        "hw_acceleration"
      ]
    },
    {
      "term": "batch size",
      "definition": "The number of training examples processed simultaneously during one iteration of neural network training, affecting both computational efficiency and gradient estimation quality.",
      "chapter_source": "dl_primer",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "batched operations",
      "definition": "Matrix computations that process multiple inputs simultaneously, converting matrix-vector operations into more efficient matrix-matrix operations to improve hardware utilization.",
      "chapter_source": "training",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "bayesian neural networks",
      "definition": "Neural networks that incorporate probability distributions over their weights, enabling uncertainty quantification in predictions and more robust decision making.",
      "chapter_source": "robust_ai",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "benchmark harness",
      "definition": "Systematic infrastructure component that controls test execution, manages input delivery, and collects performance measurements under controlled conditions to ensure reproducible evaluations.",
      "chapter_source": "benchmarking",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "benchmarking",
      "definition": "Systematic evaluation methodology that measures and compares system performance using standardized tests, datasets, and metrics to enable objective assessment and improvement.",
      "chapter_source": "benchmarking",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "bert",
      "definition": "Bidirectional Encoder Representations from Transformers, a transformer-based language model introduced by Google in 2018 that revolutionized natural language processing through masked language modeling pre-training.",
      "chapter_source": "benchmarking",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "bfloat16",
      "definition": "A 16-bit floating-point format developed by Google Brain that maintains the same dynamic range as FP32 but with reduced precision, making it particularly suitable for deep learning training.",
      "chapter_source": "training",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "bias",
      "definition": "A learnable parameter added to the weighted sum in each neuron that allows the activation function to shift, providing additional flexibility for the network to fit complex patterns.",
      "chapter_source": "dl_primer",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "bias-only adaptation",
      "definition": "A lightweight training strategy that freezes all model weights and updates only scalar bias terms, drastically reducing memory requirements and computational overhead for on-device learning.",
      "chapter_source": "ondevice_learning",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "binarization",
      "definition": "An extreme quantization technique that reduces neural network weights and activations to binary values (typically -1 and +1), achieving maximum compression but often requiring specialized training procedures and hardware support.",
      "chapter_source": "optimizations",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "biodiversity monitoring",
      "definition": "The systematic observation and measurement of biological diversity using technology such as camera traps and sensor networks to track species populations, habitat changes, and conservation effectiveness.",
      "chapter_source": "ai_for_good",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "biological neuron",
      "definition": "A cell in the nervous system that receives, processes, and transmits information through electrical and chemical signals, serving as inspiration for artificial neural networks.",
      "chapter_source": "dl_primer",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "bit flip",
      "definition": "A hardware fault where a single bit in memory or a register unexpectedly changes its value from 0 to 1 or vice versa, potentially corrupting data or computations.",
      "chapter_source": "robust_ai",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "black box",
      "definition": "A system where you can observe the inputs and outputs but cannot see or understand the internal workings, particularly problematic in AI when systems make important decisions affecting people's lives without providing explanations for their reasoning.",
      "chapter_source": "introduction",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "black-box attack",
      "definition": "An adversarial attack where the attacker has no knowledge of the model's internal architecture, parameters, or training data, and must rely solely on querying the model and observing outputs.",
      "chapter_source": "privacy_security",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "blas",
      "definition": "Basic Linear Algebra Subprograms, a specification for low-level routines that perform common linear algebra operations such as vector addition, scalar multiplication, dot products, and matrix operations, forming the computational foundation of modern ML frameworks.",
      "chapter_source": "frameworks",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "bounding box",
      "definition": "A rectangular annotation that identifies object locations in images by drawing a box around each object of interest, commonly used in computer vision training datasets.",
      "chapter_source": "data_engineering",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "brain-computer interface",
      "definition": "A direct communication pathway between the brain and an external device, enabling control of computers or prosthetics through neural signals and representing a convergence of ML with neurotechnology.",
      "chapter_source": "frontiers",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "brittleness",
      "definition": "The tendency of rule-based AI systems to fail completely when encountering inputs that fall outside their programmed scenarios, no matter how similar those inputs might be to what they were designed to handle.",
      "chapter_source": "introduction",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "built-in self-test (bist)",
      "definition": "Hardware testing mechanisms that allow components to test themselves for faults using dedicated circuitry and predefined test patterns.",
      "chapter_source": "robust_ai",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "cache timing attack",
      "definition": "A type of side-channel attack that exploits variations in memory cache access patterns to infer sensitive information about program execution or data.",
      "chapter_source": "privacy_security",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "caching",
      "definition": "A technique for storing frequently accessed data in high-speed storage systems to reduce retrieval latency and improve system performance in ML pipelines.",
      "chapter_source": "data_engineering",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "calibration",
      "definition": "The process in post-training quantization of analyzing a representative dataset to determine optimal quantization parameters, including scale factors and zero points, that minimize accuracy loss when converting from high to low precision.",
      "chapter_source": "optimizations",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "canary deployment",
      "definition": "Gradual rollout strategy where a new model version serves a small percentage of traffic while monitoring performance before full deployment, allowing safe validation in production.",
      "chapter_source": "ops",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "carbon footprint",
      "definition": "The total amount of greenhouse gas emissions produced directly and indirectly by an individual, organization, event, or product, typically measured in CO2 equivalent.",
      "chapter_source": "sustainable_ai",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "carbon-aware scheduling",
      "definition": "A computational approach that schedules AI workloads based on the carbon intensity of the electricity grid, prioritizing execution when renewable energy sources are most available.",
      "chapter_source": "sustainable_ai",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "catastrophic forgetting",
      "definition": "The phenomenon where neural networks lose previously learned knowledge when adapting to new tasks, a critical challenge in continual on-device learning scenarios.",
      "chapter_source": "ondevice_learning",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "cerebras wafer-scale engine",
      "definition": "A revolutionary single-wafer processor containing 2.6 trillion transistors and 850,000 cores, designed to eliminate inter-device communication bottlenecks in large-scale machine learning training.",
      "chapter_source": "training",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "channelwise quantization",
      "definition": "A quantization granularity approach where each channel in a layer uses its own set of quantization parameters, providing more precise representation than layerwise quantization while maintaining hardware efficiency.",
      "chapter_source": "optimizations",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "checkpoint and restart mechanisms",
      "definition": "Techniques that periodically save a program's state so it can resume from the last saved state after a failure, improving system resilience.",
      "chapter_source": "robust_ai",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "ci/cd pipelines",
      "definition": "Continuous Integration and Continuous Delivery automated workflows that streamline model development by integrating testing, validation, and deployment processes.",
      "chapter_source": "ops",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "cifar10",
      "definition": "Canadian Institute for Advanced Research dataset with 60,000 32Ã—32 color images across 10 classes, serving as a standard benchmark in computer vision despite its small image size by modern standards.",
      "chapter_source": "efficient_ai",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "classification labels",
      "definition": "Simple categorical annotations that assign specific tags or categories to data examples, representing the most basic form of supervised learning annotation.",
      "chapter_source": "data_engineering",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "client scheduling",
      "definition": "The process of selecting which devices participate in federated learning rounds based on availability, data quality, and resource constraints to ensure representative model updates.",
      "chapter_source": "ondevice_learning",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "cloud machine learning",
      "definition": "The deployment of machine learning models on centralized computing infrastructures such as data centers, offering scalability and computational capacity for large-scale datasets and complex models.",
      "chapter_source": "ml_systems",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "cloud ml",
      "definition": "Machine learning systems that leverage cloud computing infrastructure to provide scalable computational resources for training and inference, typically offering high-bandwidth connectivity and substantial processing power.",
      "chapter_source": "ai_for_good",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "cloudsuite",
      "definition": "Benchmark suite developed at EPFL that addresses modern datacenter workloads including web search, data analytics, and media streaming, measuring end-to-end performance across network, storage, and compute dimensions.",
      "chapter_source": "benchmarking",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "co_design",
      "definition": "Holistic approach where model architectures, hardware platforms, and data pipelines are designed in tandem to work seamlessly together, mitigating trade-offs through end-to-end optimization.",
      "chapter_source": "efficient_ai",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "cold-start performance",
      "definition": "Time required for a system to transition from idle state to active execution, particularly important in serverless environments where models are loaded on demand.",
      "chapter_source": "benchmarking",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "combinational logic",
      "definition": "Digital logic circuits where the output depends only on the current input states, not any past states or memory elements.",
      "chapter_source": "robust_ai",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "computational graph",
      "definition": "A directed acyclic graph representation of mathematical operations where nodes represent operations or variables and edges represent data flow, enabling automatic differentiation and optimization of neural network computations.",
      "chapter_source": "frameworks",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "compute_efficiency",
      "definition": "The optimization of computational resources including hardware and energy utilization to maximize processing speed while minimizing resource consumption during training and deployment.",
      "chapter_source": "efficient_ai",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "computer engineering",
      "definition": "An engineering discipline that emerged in the late 1960s to address the growing complexity of integrating hardware and software systems, combining expertise from electrical engineering and computer science to design and build complex computing systems.",
      "chapter_source": "introduction",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "concept bottleneck models",
      "definition": "Neural network architectures that first predict interpretable intermediate concepts before making final predictions, combining deep learning power with transparency.",
      "chapter_source": "responsible_ai",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "concept drift",
      "definition": "Performance degradation that occurs when the underlying relationship between input features and target outcomes changes over time, requiring model retraining. Alternative definition: A change in the relationship between input features and target outputs over time, requiring model adaptation to maintain performance.",
      "chapter_source": "ops",
      "aliases": [],
      "see_also": [],
      "appears_in": [
        "ops",
        "robust_ai"
      ]
    },
    {
      "term": "conditional computation",
      "definition": "A dynamic optimization technique where different parts of a neural network are selectively activated based on input characteristics, reducing computational load by skipping unnecessary computations for specific inputs.",
      "chapter_source": "optimizations",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "conditional_generation",
      "definition": "The process of generating new data samples based on specific input conditions or constraints, such as generating images from text descriptions.",
      "chapter_source": "generative_ai",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "consensus labeling",
      "definition": "A quality control approach that collects multiple annotations for the same data point to identify controversial cases and improve label reliability through inter-annotator agreement.",
      "chapter_source": "data_engineering",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "conservation technology",
      "definition": "Technological solutions designed to protect and monitor wildlife and ecosystems, including camera traps, sensor networks, and satellite monitoring systems for tracking animal behavior and detecting threats.",
      "chapter_source": "ai_for_good",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "containerization",
      "definition": "Packaging applications and their dependencies into portable, isolated containers using tools like Docker to ensure consistent execution across different environments.",
      "chapter_source": "ops",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "continual learning",
      "definition": "The ability of machine learning systems to learn continuously from a stream of data while retaining previously acquired knowledge, addressing the challenge of catastrophic forgetting in neural networks. Alternative definition: The ability of machine learning models to learn new tasks sequentially without forgetting previously acquired knowledge, essential for adaptive on-device systems. Alternative definition: The ability of machine learning models to learn continuously from new data distributions while retaining knowledge from previous distributions without catastrophic forgetting.",
      "chapter_source": "frontiers",
      "aliases": [],
      "see_also": [],
      "appears_in": [
        "frontiers",
        "ondevice_learning",
        "robust_ai"
      ]
    },
    {
      "term": "continuous integration",
      "definition": "A software development practice where code changes are automatically integrated, tested, and validated multiple times per day to detect issues early in the development cycle.",
      "chapter_source": "workflow",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "convolution",
      "definition": "A mathematical operation fundamental to convolutional neural networks that applies filters (kernels) to input data to extract features such as edges, textures, or patterns, particularly effective for processing images and spatial data.",
      "chapter_source": "frameworks",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "convolution operation",
      "definition": "A mathematical operation that slides a filter (kernel) across input data to detect local features, forming the foundation of convolutional neural networks for spatial pattern recognition.",
      "chapter_source": "dnn_architectures",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "convolutional neural network",
      "definition": "A type of neural network architecture designed for processing spatial data like images, using convolutional layers with local receptive fields and weight sharing to detect spatial patterns. Alternative definition: A specialized neural network architecture designed for processing grid-like data such as images, using convolutional layers that apply filters to detect local features. Alternative definition: Deep learning architecture specifically designed for processing grid-like data such as images, using convolutional layers to detect local features and patterns.",
      "chapter_source": "benchmarking",
      "aliases": [],
      "see_also": [],
      "appears_in": [
        "benchmarking",
        "dl_primer",
        "dnn_architectures"
      ]
    },
    {
      "term": "cooling effectiveness",
      "definition": "The efficiency with which a data center cooling system removes heat from computing equipment, typically measured as the ratio of heat removed to energy consumed for cooling.",
      "chapter_source": "sustainable_ai",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "counterfactual explanations",
      "definition": "Explanations that describe how a model's output would change if specific input features were modified, particularly useful for understanding decision boundaries.",
      "chapter_source": "responsible_ai",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "covariate shift",
      "definition": "A type of distribution shift where the input distribution changes while the conditional relationship between inputs and outputs remains stable.",
      "chapter_source": "robust_ai",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "cp decomposition",
      "definition": "CANDECOMP/PARAFAC decomposition that expresses a tensor as a sum of rank-one components, used to compress neural network layers by reducing the number of parameters while preserving computational functionality.",
      "chapter_source": "optimizations",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "crisp-dm",
      "definition": "Cross-Industry Standard Process for Data Mining, a structured methodology developed in 1996 that defines six phases for data projects: business understanding, data understanding, data preparation, modeling, evaluation, and deployment.",
      "chapter_source": "workflow",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "cross-entropy loss",
      "definition": "A loss function commonly used in classification tasks that measures the difference between predicted probability distributions and true class labels, providing strong gradients for effective learning.",
      "chapter_source": "dl_primer",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "crowdsourcing",
      "definition": "A collaborative data collection approach that leverages distributed individuals via the internet to perform annotation tasks, enabling scalable dataset creation through platforms like Amazon Mechanical Turk.",
      "chapter_source": "data_engineering",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "cublas",
      "definition": "NVIDIA's CUDA Basic Linear Algebra Subprograms library that provides GPU-accelerated implementations of standard linear algebra operations, enabling high-performance matrix computations on NVIDIA graphics processing units.",
      "chapter_source": "frameworks",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "cuda",
      "definition": "NVIDIA's parallel computing platform and programming model that enables general-purpose computing on graphics processing units (GPUs), allowing machine learning frameworks to leverage massive parallelism for accelerated tensor operations.",
      "chapter_source": "frameworks",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "cuda (compute unified device architecture)",
      "definition": "NVIDIA's parallel computing platform and programming model that enables developers to use GPUs for general-purpose computing beyond graphics rendering.",
      "chapter_source": "hw_acceleration",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "curriculum_learning",
      "definition": "Training strategy where models learn from easy examples before progressing to harder ones, mimicking human education and improving convergence speed by 25-50%.",
      "chapter_source": "efficient_ai",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "dartmouth conference",
      "definition": "The legendary 8-week workshop at Dartmouth College in 1956 where AI was officially born, organized by John McCarthy, Marvin Minsky, Nathaniel Rochester, and Claude Shannon, where the term \"artificial intelligence\" was first coined.",
      "chapter_source": "introduction",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "data augmentation",
      "definition": "Artificially expanding datasets through transformations like rotations, crops, or noise to improve model performance by 5-15% and reduce overfitting when labeled data is scarce.",
      "chapter_source": "data_engineering",
      "aliases": [],
      "see_also": [],
      "appears_in": [
        "data_engineering",
        "efficient_ai"
      ]
    },
    {
      "term": "data cascades",
      "definition": "Systemic failures where data quality issues compound over time, creating downstream negative consequences such as model failures, costly rebuilding, or project termination.",
      "chapter_source": "data_engineering",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "data center",
      "definition": "A facility housing computer systems and associated components such as telecommunications and storage systems, typically including redundant power supplies, cooling systems, and network connections. Alternative definition: A facility that houses computer systems and associated components such as telecommunications and storage systems, typically containing thousands of servers for cloud computing operations.",
      "chapter_source": "ml_systems",
      "aliases": [],
      "see_also": [],
      "appears_in": [
        "ml_systems",
        "sustainable_ai"
      ]
    },
    {
      "term": "data compression",
      "definition": "Techniques for reducing the size and complexity of training data through encoding, quantization, or feature extraction to enable efficient storage and processing on memory-constrained devices.",
      "chapter_source": "ondevice_learning",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "data curation",
      "definition": "The process of selecting, organizing, and maintaining high-quality datasets by removing irrelevant information, correcting errors, and ensuring data meets specific standards for machine learning applications.",
      "chapter_source": "workflow",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "data drift",
      "definition": "The phenomenon where the statistical properties of input data change over time, causing machine learning model performance to degrade even when the underlying code remains unchanged. Alternative definition: Changes in the statistical distribution of input data over time that can degrade model performance, requiring monitoring and potential model updates.",
      "chapter_source": "ops",
      "aliases": [],
      "see_also": [],
      "appears_in": [
        "ops",
        "workflow"
      ]
    },
    {
      "term": "data governance",
      "definition": "The framework of policies, procedures, and technologies that ensure data security, privacy, compliance, and ethical use throughout the machine learning pipeline.",
      "chapter_source": "data_engineering",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "data ingestion",
      "definition": "The process of collecting and importing raw data from various sources into a system where it can be stored, processed, and prepared for machine learning applications. Alternative definition: The process of collecting and importing data from various sources into a storage system for further processing, involving both batch and stream processing patterns.",
      "chapter_source": "data_engineering",
      "aliases": [],
      "see_also": [],
      "appears_in": [
        "data_engineering",
        "workflow"
      ]
    },
    {
      "term": "data lake",
      "definition": "A storage repository that holds structured, semi-structured, and unstructured data in its native format, using schema-on-read approaches for flexible data analysis.",
      "chapter_source": "data_engineering",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "data lineage",
      "definition": "The documentation and tracking of data flow through various transformations and processes, providing visibility into data origins and modifications for compliance and debugging. Alternative definition: Complete traceable record of data flow from source to consumption, including transformations and dependencies, essential for reproducibility and debugging.",
      "chapter_source": "data_engineering",
      "aliases": [],
      "see_also": [],
      "appears_in": [
        "data_engineering",
        "ops"
      ]
    },
    {
      "term": "data parallelism",
      "definition": "A distributed training strategy that splits the dataset across multiple devices while each device maintains a complete copy of the model, enabling parallel computation of gradients.",
      "chapter_source": "benchmarking",
      "aliases": [],
      "see_also": [],
      "appears_in": [
        "benchmarking",
        "efficient_ai",
        "training"
      ]
    },
    {
      "term": "data pipeline",
      "definition": "The infrastructure and workflows that automate the movement and transformation of data from sources through processing stages to final storage or consumption.",
      "chapter_source": "data_engineering",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "data poisoning",
      "definition": "An attack method where adversaries inject carefully crafted malicious data points into the training dataset to manipulate model behavior in targeted or systematic ways. Alternative definition: An attack where training data is deliberately manipulated or corrupted to compromise model performance, behavior, or security during deployment.",
      "chapter_source": "privacy_security",
      "aliases": [],
      "see_also": [],
      "appears_in": [
        "privacy_security",
        "robust_ai"
      ]
    },
    {
      "term": "data quality",
      "definition": "The degree to which data meets requirements for accuracy, completeness, consistency, and timeliness, directly impacting machine learning model performance.",
      "chapter_source": "data_engineering",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "data sanitization",
      "definition": "The process of deliberately and permanently removing or destroying data stored on memory devices to make it unrecoverable, ensuring data security.",
      "chapter_source": "robust_ai",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "data validation",
      "definition": "The systematic verification that collected data meets quality standards, is properly formatted, and contains accurate information suitable for machine learning model training and evaluation. Alternative definition: The process of ensuring incoming data meets quality standards and conforms to expected schemas, preventing downstream issues in ML pipelines.",
      "chapter_source": "data_engineering",
      "aliases": [],
      "see_also": [],
      "appears_in": [
        "data_engineering",
        "workflow"
      ]
    },
    {
      "term": "data versioning",
      "definition": "The practice of tracking and managing different versions of datasets over time, similar to code versioning, to ensure reproducibility and enable rollback to previous data states when needed. Alternative definition: Systematic tracking and management of dataset versions over time, enabling reproducibility and rollback capabilities for machine learning experiments.",
      "chapter_source": "ops",
      "aliases": [],
      "see_also": [],
      "appears_in": [
        "ops",
        "workflow"
      ]
    },
    {
      "term": "data warehouse",
      "definition": "A centralized repository optimized for analytical queries (OLAP) that stores integrated, structured data from multiple sources in a standardized schema.",
      "chapter_source": "data_engineering",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "data-centric approach",
      "definition": "A machine learning paradigm that prioritizes improving data quality, diversity, and curation rather than solely focusing on model architecture improvements to achieve better performance.",
      "chapter_source": "conclusion",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "data_centric_ai",
      "definition": "Paradigm shift from model-centric to data-centric development that focuses on systematically improving data quality rather than just model architecture, often yielding greater performance gains.",
      "chapter_source": "efficient_ai",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "data_efficiency",
      "definition": "Optimizing the amount and quality of data required to train machine learning models effectively, focusing on maximizing information gained while minimizing required data volume.",
      "chapter_source": "efficient_ai",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "dead letter queue",
      "definition": "A separate storage mechanism for data that fails processing, allowing for later analysis and potential reprocessing of problematic data without blocking the main pipeline.",
      "chapter_source": "data_engineering",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "deep learning",
      "definition": "A subfield of machine learning that uses artificial neural networks with multiple layers to automatically learn hierarchical representations from data without explicit feature engineering. Alternative definition: A subset of machine learning using neural networks with multiple hidden layers to automatically learn hierarchical representations from data. Alternative definition: Subset of machine learning using artificial neural networks with multiple layers to learn hierarchical representations of data for complex pattern recognition tasks.",
      "chapter_source": "benchmarking",
      "aliases": [],
      "see_also": [],
      "appears_in": [
        "benchmarking",
        "dl_primer",
        "dnn_architectures",
        "sustainable_ai"
      ]
    },
    {
      "term": "defensive distillation",
      "definition": "A technique that trains a student model to mimic a teacher model's behavior using soft labels, reducing sensitivity to adversarial perturbations.",
      "chapter_source": "robust_ai",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "demographic parity",
      "definition": "A fairness criterion requiring that the probability of receiving a positive prediction is independent of group membership across protected attributes.",
      "chapter_source": "responsible_ai",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "dennard scaling",
      "definition": "The historical observation that as transistors become smaller, their power density remains approximately constant, allowing for more transistors without proportional increases in power consumption.",
      "chapter_source": "sustainable_ai",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "denoising_diffusion",
      "definition": "A generative modeling approach that learns to reverse a gradual noise-adding process, starting from pure noise and iteratively removing it to create realistic data.",
      "chapter_source": "generative_ai",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "dense layer",
      "definition": "A fully-connected neural network layer where each neuron receives input from all neurons in the previous layer, enabling comprehensive information integration across features. Alternative definition: A fully connected neural network layer where every neuron connects to every neuron in the adjacent layers, enabling unrestricted feature interactions across all inputs.",
      "chapter_source": "dl_primer",
      "aliases": [],
      "see_also": [],
      "appears_in": [
        "dl_primer",
        "dnn_architectures"
      ]
    },
    {
      "term": "dense matrix-matrix multiplication",
      "definition": "The fundamental computational operation in neural networks that dominates training time, accounting for 60-90% of computation in typical models.",
      "chapter_source": "training",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "deployment constraints",
      "definition": "Operational limitations such as hardware resources, network connectivity, regulatory requirements, and integration requirements that influence how machine learning models are implemented in production environments.",
      "chapter_source": "workflow",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "depthwise separable convolutions",
      "definition": "A computational technique that decomposes standard convolutions into depthwise and pointwise operations, reducing parameters and computation by 8-9x for mobile-optimized architectures.",
      "chapter_source": "ondevice_learning",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "devops",
      "definition": "Software development practice that combines development and operations teams to shorten development cycles and deliver high-quality software through automation and collaboration.",
      "chapter_source": "ops",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "dhrystone",
      "definition": "Integer-based benchmark introduced in 1984 that measures integer and string operations in DMIPS (Dhrystone MIPS), designed to complement floating-point benchmarks with typical programming constructs.",
      "chapter_source": "benchmarking",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "diabetic retinopathy",
      "definition": "A diabetes complication that damages blood vessels in the retina, serving as a leading cause of preventable blindness and a key application area for medical AI screening systems.",
      "chapter_source": "workflow",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "differential privacy",
      "definition": "A mathematical framework that provides formal privacy guarantees by adding calibrated noise to computations, ensuring that the inclusion or exclusion of any individual's data has a provably limited effect on the output. Alternative definition: A mathematical framework for quantifying and limiting privacy leakage by adding calibrated noise to data or model updates, commonly used in federated learning. Alternative definition: A mathematical framework for quantifying and limiting the privacy loss when releasing statistical information about datasets, ensuring individual privacy while enabling useful data analysis. Alternative definition: A mathematical framework that provides formal guarantees about individual privacy by adding calibrated noise to computations or outputs. Alternative definition: A mathematical framework that adds calibrated noise to datasets or query results to protect individual privacy while preserving overall data utility for analysis.",
      "chapter_source": "conclusion",
      "aliases": [],
      "see_also": [],
      "appears_in": [
        "conclusion",
        "data_engineering",
        "ondevice_learning",
        "privacy_security",
        "responsible_ai"
      ]
    },
    {
      "term": "diffusion_model",
      "definition": "A class of generative models that create data by learning to reverse a diffusion process that gradually adds noise to training data.",
      "chapter_source": "generative_ai",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "digital divide",
      "definition": "The gap between those who have access to modern information and communication technology and those who do not, particularly affecting underserved communities' ability to benefit from digital solutions.",
      "chapter_source": "ai_for_good",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "digital twin",
      "definition": "A virtual representation of a physical system that uses real-time data and machine learning to mirror, predict, and optimize the behavior of its physical counterpart.",
      "chapter_source": "frontiers",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "disaster response systems",
      "definition": "Automated systems that use machine learning to detect, predict, and respond to natural disasters through satellite imagery analysis, sensor networks, and resource allocation optimization.",
      "chapter_source": "ai_for_good",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "discriminator",
      "definition": "The component in a generative adversarial network that learns to distinguish between real data and generated samples, providing feedback to improve the generator.",
      "chapter_source": "generative_ai",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "distributed computing",
      "definition": "An approach that processes data across multiple machines or processors simultaneously, enabling scalable handling of large datasets through frameworks like Apache Spark.",
      "chapter_source": "data_engineering",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "distributed intelligence",
      "definition": "The placement of computational capabilities across multiple devices and locations rather than relying on a single centralized system, enabling local processing and decision-making.",
      "chapter_source": "ml_systems",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "distributed knowledge pattern",
      "definition": "A design pattern that addresses collective learning and inference across decentralized nodes, emphasizing peer-to-peer knowledge sharing and collaborative model improvement while maintaining operational independence.",
      "chapter_source": "ai_for_good",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "distributed training",
      "definition": "A method of training machine learning models across multiple machines or devices to handle larger datasets and models that exceed single-device computational or memory capacity. Alternative definition: A machine learning technique that splits the training process across multiple computing devices or nodes to handle large datasets and complex models more efficiently. Alternative definition: The practice of training machine learning models across multiple computing nodes or devices to reduce training time and enable larger model architectures. Alternative definition: Training approach that splits machine learning workloads across multiple compute nodes to handle large models and datasets that exceed single-node capabilities.",
      "chapter_source": "benchmarking",
      "aliases": [],
      "see_also": [],
      "appears_in": [
        "benchmarking",
        "conclusion",
        "sustainable_ai",
        "training"
      ]
    },
    {
      "term": "distribution shift",
      "definition": "The phenomenon where data encountered during model deployment differs from the training distribution, potentially degrading model performance. Alternative definition: The mismatch between training data and real-world deployment conditions that can degrade model performance and reliability over time.",
      "chapter_source": "responsible_ai",
      "aliases": [],
      "see_also": [],
      "appears_in": [
        "responsible_ai",
        "robust_ai"
      ]
    },
    {
      "term": "domain-specific architecture",
      "definition": "Hardware designs tailored to optimize specific computational workloads, trading flexibility for improved performance and energy efficiency compared to general-purpose processors.",
      "chapter_source": "hw_acceleration",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "double modular redundancy (dmr)",
      "definition": "A fault-tolerance technique where computations are duplicated across two independent systems to identify and correct errors through comparison.",
      "chapter_source": "robust_ai",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "dropout",
      "definition": "A regularization technique that randomly sets a fraction of input units to zero during training to prevent overfitting and improve generalization.",
      "chapter_source": "dnn_architectures",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "dual-use dilemma",
      "definition": "The challenge of mitigating misuse of technology that has both positive and negative potential applications, particularly relevant in AI security.",
      "chapter_source": "robust_ai",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "dying relu problem",
      "definition": "A phenomenon where ReLU neurons become permanently inactive and output zero for all inputs, preventing them from contributing to learning when weighted inputs consistently produce negative values.",
      "chapter_source": "training",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "dynamic graph",
      "definition": "A computational graph that is built and modified during program execution, allowing for flexible model architectures and easier debugging but potentially limiting optimization opportunities compared to static graphs.",
      "chapter_source": "frameworks",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "dynamic pruning",
      "definition": "An adaptive pruning approach that adjusts pruning decisions in real-time based on input data or training dynamics, allowing models to temporarily deactivate parameters without permanently removing them.",
      "chapter_source": "optimizations",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "dynamic quantization",
      "definition": "A quantization approach where quantization parameters are computed at runtime based on the actual range of values during inference, providing flexibility but requiring additional computational overhead.",
      "chapter_source": "optimizations",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "dynamic random access memory (dram)",
      "definition": "A type of volatile memory that stores data in capacitors and requires periodic refresh cycles, commonly used as main memory in computer systems.",
      "chapter_source": "hw_acceleration",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "dynamic voltage and frequency scaling (dvfs)",
      "definition": "Power management technique that adjusts processor voltage and clock frequency based on workload demands to optimize energy consumption while maintaining performance.",
      "chapter_source": "benchmarking",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "eager execution",
      "definition": "An execution mode where operations are evaluated immediately as they are called in the code, providing intuitive debugging and development experience but potentially sacrificing some optimization opportunities available in graph-based execution.",
      "chapter_source": "frameworks",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "early exit architectures",
      "definition": "Neural network designs that include multiple prediction heads at different depths, allowing samples to exit early when confident predictions can be made, reducing average computational cost per inference.",
      "chapter_source": "optimizations",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "edge ai",
      "definition": "The deployment of artificial intelligence algorithms directly on edge devices like smartphones, IoT sensors, and embedded systems, enabling real-time processing without cloud connectivity.",
      "chapter_source": "frontiers",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "edge computing",
      "definition": "A distributed computing paradigm that brings computation and data storage closer to data sources, enabling real-time processing with reduced latency and lower bandwidth requirements. Alternative definition: A distributed computing paradigm that brings computation and data storage closer to the sources of data, reducing latency and bandwidth usage. Alternative definition: The deployment of computation and data storage closer to the sources of data, enabling real-time processing with low latency and reduced bandwidth requirements. Alternative definition: A distributed computing paradigm that brings computation and data storage closer to data sources and end users, reducing latency and bandwidth usage. Alternative definition: A distributed computing paradigm that brings computation and data storage closer to the location where it is needed, reducing latency and bandwidth usage in ML applications.",
      "chapter_source": "ai_for_good",
      "aliases": [],
      "see_also": [],
      "appears_in": [
        "ai_for_good",
        "conclusion",
        "hw_acceleration",
        "ml_systems",
        "ondevice_learning",
        "sustainable_ai"
      ]
    },
    {
      "term": "edge deployment",
      "definition": "A deployment strategy where machine learning models run locally on devices at the network edge rather than in centralized cloud servers, reducing latency and enabling operation without constant internet connectivity.",
      "chapter_source": "workflow",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "edge machine learning",
      "definition": "The deployment of machine learning models at or near the edge of the network on localized hardware optimized for real-time processing with reduced latency.",
      "chapter_source": "ml_systems",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "edge ml",
      "definition": "Machine learning systems that perform inference and sometimes training at the edge of networks, typically on resource-constrained devices like smartphones or embedded systems with limited computational power.",
      "chapter_source": "ai_for_good",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "efficientnet",
      "definition": "A family of neural network architectures discovered through Neural Architecture Search that achieves better accuracy-efficiency trade-offs by using compound scaling to balance network depth, width, and input resolution. Alternative definition: Neural network architecture achieving state-of-the-art accuracy with 10Ã— fewer parameters than previous models, demonstrating that clever design can dramatically improve efficiency.",
      "chapter_source": "efficient_ai",
      "aliases": [],
      "see_also": [],
      "appears_in": [
        "efficient_ai",
        "optimizations"
      ]
    },
    {
      "term": "electromigration",
      "definition": "The movement of metal atoms in a conductor under the influence of an electric field, potentially causing permanent hardware faults over time.",
      "chapter_source": "robust_ai",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "eliza",
      "definition": "One of the first chatbots created by MIT's Joseph Weizenbaum in 1966 that could simulate human conversation through pattern matching and substitution, notable because people began forming emotional attachments to this simple program.",
      "chapter_source": "introduction",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "elt (extract, load, transform)",
      "definition": "A data processing paradigm that first loads raw data into the target system before applying transformations, providing flexibility for evolving analytical needs.",
      "chapter_source": "data_engineering",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "embedded systems",
      "definition": "Computer systems with dedicated functions within larger mechanical or electrical systems, typically designed for specific tasks with real-time computing constraints.",
      "chapter_source": "ml_systems",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "embodied carbon",
      "definition": "The total greenhouse gas emissions generated during the manufacturing, transportation, and installation of a product before it begins operation.",
      "chapter_source": "sustainable_ai",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "emergent behaviors",
      "definition": "Unexpected system-wide patterns or characteristics that arise from the interaction of individual components, often becoming apparent only when systems operate at scale or in real-world conditions.",
      "chapter_source": "workflow",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "encoder-decoder",
      "definition": "An architectural pattern where an encoder processes input into a compressed representation and a decoder generates output from this representation, commonly used in sequence-to-sequence tasks.",
      "chapter_source": "dnn_architectures",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "end-to-end benchmarks",
      "definition": "Comprehensive evaluation methodology that assesses entire AI system pipelines including data processing, model execution, post-processing, and infrastructure components.",
      "chapter_source": "benchmarking",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "energy efficiency",
      "definition": "The measure of computational work performed per unit of energy consumed, typically expressed as operations per joule and crucial for battery-powered and data center deployments. Alternative definition: The ratio of useful output to energy input, measuring how effectively a system converts energy into desired computational work.",
      "chapter_source": "benchmarking",
      "aliases": [],
      "see_also": [],
      "appears_in": [
        "benchmarking",
        "hw_acceleration",
        "sustainable_ai"
      ]
    },
    {
      "term": "energy star",
      "definition": "EPA certification program that establishes energy efficiency standards for computing equipment, requiring systems to meet strict efficiency requirements during operation and sleep modes.",
      "chapter_source": "benchmarking",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "ensemble methods",
      "definition": "Techniques combining multiple models to improve performance, like Random Forest and Gradient Boosting, which dominated machine learning competitions before deep learning.",
      "chapter_source": "efficient_ai",
      "aliases": [],
      "see_also": [],
      "appears_in": [
        "efficient_ai",
        "robust_ai"
      ]
    },
    {
      "term": "environmental monitoring",
      "definition": "The systematic collection and analysis of environmental data using sensor networks and machine learning to track ecosystem health, pollution levels, and climate change impacts.",
      "chapter_source": "ai_for_good",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "epoch",
      "definition": "One complete pass through the entire training dataset during neural network training, consisting of multiple batch iterations depending on dataset size and batch size. Alternative definition: One complete pass through the entire training dataset during the neural network training process, after which the model has seen every training example once.",
      "chapter_source": "dl_primer",
      "aliases": [],
      "see_also": [],
      "appears_in": [
        "dl_primer",
        "frameworks"
      ]
    },
    {
      "term": "equality of opportunity",
      "definition": "A fairness criterion focused on ensuring equal true positive rates across groups, guaranteeing that qualified individuals are treated equally regardless of group membership.",
      "chapter_source": "responsible_ai",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "equalized odds",
      "definition": "A fairness definition requiring that true positive and false positive rates are equal across different demographic groups.",
      "chapter_source": "responsible_ai",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "error-correcting codes",
      "definition": "Methods used in data storage and transmission to detect and correct errors, improving system reliability and data integrity.",
      "chapter_source": "robust_ai",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "esp32",
      "definition": "A low-cost microcontroller unit widely used in IoT applications, featuring a 240 MHz processor and 520 KB of RAM, commonly deployed in resource-constrained social impact applications.",
      "chapter_source": "ai_for_good",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "etl (extract, transform, load)",
      "definition": "A traditional data processing paradigm that transforms data before loading it into a data warehouse, resulting in ready-to-query formatted data.",
      "chapter_source": "data_engineering",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "exact model theft",
      "definition": "An attack that aims to extract the precise internal structure, parameters, and architecture of a machine learning model, allowing complete reproduction of the original model.",
      "chapter_source": "privacy_security",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "experience replay",
      "definition": "A memory-based technique that stores past training examples in a buffer to prevent catastrophic forgetting and stabilize learning in streaming or continual adaptation scenarios.",
      "chapter_source": "ondevice_learning",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "experiment tracking",
      "definition": "The systematic recording and management of machine learning experiments, including hyperparameters, model versions, training data, and performance metrics, to enable comparison and reproducibility. Alternative definition: Systematic logging and management of machine learning experiments, including hyperparameters, metrics, and artifacts, to enable reproducibility and comparison.",
      "chapter_source": "ops",
      "aliases": [],
      "see_also": [],
      "appears_in": [
        "ops",
        "workflow"
      ]
    },
    {
      "term": "expert systems",
      "definition": "AI systems from the mid-1970s that captured human expert knowledge in specific domains, exemplified by MYCIN for diagnosing blood infections, representing a shift from general AI to domain-specific applications.",
      "chapter_source": "introduction",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "explainability",
      "definition": "The ability of stakeholders to understand how a machine learning model produces its outputs through post-hoc explanation techniques.",
      "chapter_source": "responsible_ai",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "explainable ai",
      "definition": "AI systems designed to provide clear, interpretable explanations for their decisions and predictions, addressing the \"black box\" problem of complex machine learning models.",
      "chapter_source": "frontiers",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "f1 score",
      "definition": "A measure of model accuracy that combines precision and recall into a single metric, calculated as their harmonic mean.",
      "chapter_source": "robust_ai",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "fairness constraints",
      "definition": "Technical and policy restrictions designed to ensure equitable treatment across demographic groups in machine learning systems.",
      "chapter_source": "responsible_ai",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "farmbeats",
      "definition": "A Microsoft Research project that applies machine learning and IoT technologies to agriculture, using edge computing to collect real-time data on soil conditions and crop health while demonstrating distributed AI systems in challenging real-world environments.",
      "chapter_source": "introduction",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "fast gradient sign method (fgsm)",
      "definition": "A gradient-based adversarial attack that generates adversarial examples by adding small perturbations in the direction of the gradient.",
      "chapter_source": "robust_ai",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "fault injection attack",
      "definition": "A physical attack that deliberately disrupts hardware operations through techniques like voltage manipulation or electromagnetic interference to induce computational errors and compromise system integrity.",
      "chapter_source": "privacy_security",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "fault tolerance",
      "definition": "The ability of a system to continue operating correctly even when some of its components fail or encounter errors.",
      "chapter_source": "robust_ai",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "feature engineering",
      "definition": "The process of manually designing and extracting relevant features from raw data to improve machine learning model performance, largely automated in deep learning systems. Alternative definition: The process of using domain knowledge to create new features from existing data that make machine learning algorithms work more effectively.",
      "chapter_source": "data_engineering",
      "aliases": [],
      "see_also": [],
      "appears_in": [
        "data_engineering",
        "dl_primer"
      ]
    },
    {
      "term": "feature map",
      "definition": "The output of a convolutional layer representing the response of learned filters to different spatial locations in the input, capturing detected features at various positions.",
      "chapter_source": "dnn_architectures",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "feature store",
      "definition": "A specialized data storage system that provides standardized, reusable features for machine learning, enabling feature sharing across multiple models and teams. Alternative definition: Centralized repository for storing and serving engineered features consistently across training and inference workflows, preventing training-serving skew.",
      "chapter_source": "data_engineering",
      "aliases": [],
      "see_also": [],
      "appears_in": [
        "data_engineering",
        "ops"
      ]
    },
    {
      "term": "federated averaging",
      "definition": "The standard algorithm for federated learning where client model updates are aggregated using weighted averaging based on local dataset sizes to produce a global model.",
      "chapter_source": "ondevice_learning",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "federated learning",
      "definition": "A machine learning approach that enables training models across decentralized data sources without centralizing the data, allowing multiple parties to collaboratively train a model while preserving data privacy. Alternative definition: A machine learning approach that trains algorithms across decentralized data sources without requiring data to be centralized, improving privacy and reducing data transmission energy costs. Alternative definition: A machine learning approach where models are trained across multiple decentralized devices or servers without centralizing the raw training data, preserving privacy while enabling collaborative learning. Alternative definition: A distributed machine learning approach where models are trained across decentralized data sources without centralizing raw data. Alternative definition: A machine learning approach that enables model training across decentralized devices while keeping data localized, allowing collaborative learning without sharing raw data. Alternative definition: A distributed machine learning approach where models are trained across multiple devices using local data, with only model updates shared rather than raw data to preserve privacy. Alternative definition: A distributed machine learning approach where models are trained across multiple devices or organizations without centralizing the raw data, helping preserve privacy by keeping data local. Alternative definition: A machine learning approach that trains algorithms across decentralized edge devices or servers holding local data samples, without exchanging the raw data. Alternative definition: A machine learning approach that trains algorithms across decentralized devices while keeping training data localized, enabling collaborative model improvement without sharing raw data. Alternative definition: A distributed machine learning approach where models are trained across multiple decentralized devices or servers holding local data, without exchanging the raw data itself.",
      "chapter_source": "ai_for_good",
      "aliases": [],
      "see_also": [],
      "appears_in": [
        "ai_for_good",
        "conclusion",
        "frameworks",
        "frontiers",
        "ml_systems",
        "ondevice_learning",
        "privacy_security",
        "responsible_ai",
        "sustainable_ai",
        "workflow"
      ]
    },
    {
      "term": "feedback loops",
      "definition": "Cyclical processes where outputs from later stages of the machine learning lifecycle inform and influence decisions in earlier stages, enabling continuous system improvement and adaptation.",
      "chapter_source": "workflow",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "feedforward network",
      "definition": "A neural network architecture where information flows in one direction from input to output layers without cycles, forming the foundation for many deep learning models. Alternative definition: A neural network where information flows in one direction from input to output without cycles, contrasting with recurrent networks that have feedback connections.",
      "chapter_source": "dl_primer",
      "aliases": [],
      "see_also": [],
      "appears_in": [
        "dl_primer",
        "dnn_architectures"
      ]
    },
    {
      "term": "few-shot learning",
      "definition": "A machine learning paradigm that enables models to adapt to new tasks using only a small number of labeled examples, critical for data-sparse on-device scenarios.",
      "chapter_source": "ondevice_learning",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "field-programmable gate array",
      "definition": "Reconfigurable hardware that can be programmed for specific tasks, offering flexibility between general-purpose processors and application-specific integrated circuits, useful for custom ML accelerations.",
      "chapter_source": "training",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "field-programmable gate array (fpga)",
      "definition": "A reconfigurable integrated circuit that can be programmed after manufacturing to implement custom digital circuits and specialized computations.",
      "chapter_source": "hw_acceleration",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "fine_tuning",
      "definition": "The process of adapting a pre-trained generative model to a specific task or domain by training on a smaller, task-specific dataset.",
      "chapter_source": "generative_ai",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "floating-point unit (fpu)",
      "definition": "A specialized processor component designed to perform arithmetic operations on floating-point numbers with high precision and efficiency.",
      "chapter_source": "hw_acceleration",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "flops",
      "definition": "Floating Point Operations Per Second, a measure of computational complexity that counts the number of arithmetic operations required to execute a model, commonly used to compare model efficiency. Alternative definition: Floating Point Operations Per Second, a measure of computational throughput that quantifies the number of mathematical operations involving decimal numbers a system can perform. Alternative definition: Floating-Point Operations Per Second, a measure of computational performance that quantifies how many floating-point arithmetic operations a system can execute in one second. Alternative definition: Floating-Point Operations Per Second, the standard measure of computational throughput used to quantify training and inference computational requirements. Alternative definition: Floating-point operations per second, a measure of computer performance indicating how many mathematical calculations involving decimal numbers a system can perform per second.",
      "chapter_source": "benchmarking",
      "aliases": [],
      "see_also": [],
      "appears_in": [
        "benchmarking",
        "dl_primer",
        "efficient_ai",
        "optimizations",
        "sustainable_ai"
      ]
    },
    {
      "term": "forward pass",
      "definition": "The computation phase where input data flows through a neural network's layers to produce outputs, involving matrix multiplications and activation function applications.",
      "chapter_source": "training",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "forward propagation",
      "definition": "The process of computing neural network predictions by passing input data through successive layers, applying weights, biases, and activation functions at each stage.",
      "chapter_source": "dl_primer",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "foundation models",
      "definition": "Large-scale, general-purpose AI models trained on broad data that can be adapted for many tasks, including models like GPT-3, BERT, and DALL-E with billions of parameters.",
      "chapter_source": "efficient_ai",
      "aliases": [],
      "see_also": [],
      "appears_in": [
        "efficient_ai",
        "frontiers",
        "ml_systems"
      ]
    },
    {
      "term": "foundation_model",
      "definition": "A large-scale machine learning model trained on broad data that can be adapted for various downstream generative tasks through fine-tuning or prompting.",
      "chapter_source": "generative_ai",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "fp16",
      "definition": "16-bit floating-point numerical representation that reduces memory usage and accelerates computation while maintaining acceptable precision for many machine learning applications.",
      "chapter_source": "benchmarking",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "fp16 computation",
      "definition": "The use of 16-bit floating-point arithmetic for neural network operations to reduce memory usage and increase computational speed on modern hardware accelerators.",
      "chapter_source": "training",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "fp32",
      "definition": "32-bit floating-point numerical representation that provides standard precision for mathematical computations but requires more memory and computational resources than lower-precision formats.",
      "chapter_source": "benchmarking",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "gan",
      "definition": "Generative Adversarial Network, an architecture consisting of two competing neural networks that learn to generate realistic data through adversarial training.",
      "chapter_source": "generative_ai",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "gdpr",
      "definition": "The General Data Protection Regulation, a European Union law that imposes strict requirements on personal data processing and significantly influences privacy-preserving machine learning design. Alternative definition: The General Data Protection Regulation, European Union legislation that requires meaningful information about automated decision-making logic and grants individuals rights over their data.",
      "chapter_source": "ondevice_learning",
      "aliases": [],
      "see_also": [],
      "appears_in": [
        "ondevice_learning",
        "responsible_ai"
      ]
    },
    {
      "term": "gemm",
      "definition": "General Matrix Multiply operations that follow the pattern C = Î±AB + Î²C, representing the fundamental computational kernel underlying most neural network operations including fully connected layers and convolutional layers.",
      "chapter_source": "frameworks",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "gemv",
      "definition": "General Matrix-Vector multiplication operations that compute the product of a matrix and a vector, commonly used in neural network computations and requiring careful optimization for memory access patterns.",
      "chapter_source": "frameworks",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "generalization",
      "definition": "The ability of a machine learning model to perform well on unseen data that differs from the training set, often improved through diverse and high-quality training data.",
      "chapter_source": "data_engineering",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "generative adversarial networks",
      "definition": "A class of machine learning systems where two neural networks compete against each other, with one generating fake data and the other trying to detect it, leading to highly realistic synthetic data generation.",
      "chapter_source": "frontiers",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "generative ai",
      "definition": "A category of artificial intelligence systems capable of creating new content such as text, images, audio, or video based on learned patterns from training data.",
      "chapter_source": "conclusion",
      "aliases": [],
      "see_also": [],
      "appears_in": [
        "conclusion",
        "generative_ai"
      ]
    },
    {
      "term": "generative_model",
      "definition": "A machine learning model that learns the underlying distribution of training data to generate new, similar samples.",
      "chapter_source": "generative_ai",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "generator",
      "definition": "The component in a generative adversarial network responsible for creating new data samples that attempt to fool the discriminator.",
      "chapter_source": "generative_ai",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "glitches",
      "definition": "Momentary deviations in voltage, current, or signal that can cause incorrect operation in digital systems and circuits.",
      "chapter_source": "robust_ai",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "gpt3",
      "definition": "OpenAI's 175-billion parameter language model released in 2020, costing an estimated $4.6 million to train and consuming approximately 1,287 MWh of electricity.",
      "chapter_source": "efficient_ai",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "gpt4",
      "definition": "OpenAI's most advanced language model as of 2023, reportedly using a mixture-of-experts architecture with approximately 1.8 trillion parameters and training costs exceeding $100 million.",
      "chapter_source": "efficient_ai",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "gpu",
      "definition": "Graphics Processing Unit, a specialized electronic circuit designed to rapidly manipulate and alter memory to accelerate the creation of images and parallel processing tasks like AI training. Alternative definition: Graphics Processing Unit, specialized processor originally designed for graphics rendering but now widely used for parallel computation in machine learning due to its many-core architecture.",
      "chapter_source": "benchmarking",
      "aliases": [],
      "see_also": [],
      "appears_in": [
        "benchmarking",
        "ml_systems",
        "sustainable_ai"
      ]
    },
    {
      "term": "graceful degradation",
      "definition": "A system design principle where services continue functioning with reduced capabilities when faced with partial failures or data unavailability.",
      "chapter_source": "data_engineering",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "gradient accumulation",
      "definition": "A technique that simulates larger batch sizes by accumulating gradients from multiple smaller batches before updating model parameters, enabling training with limited memory.",
      "chapter_source": "training",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "gradient clipping",
      "definition": "A technique used during neural network training to prevent exploding gradients by limiting the magnitude of gradients to a specified threshold, helping maintain training stability in deep networks. Alternative definition: A regularization technique that prevents gradient explosion by limiting the magnitude of gradients during backpropagation, typically by scaling gradients when their norm exceeds a threshold.",
      "chapter_source": "frameworks",
      "aliases": [],
      "see_also": [],
      "appears_in": [
        "frameworks",
        "training"
      ]
    },
    {
      "term": "gradient compression",
      "definition": "A technique used in distributed training to reduce the communication overhead by compressing gradient information exchanged between computing nodes.",
      "chapter_source": "conclusion",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "gradient descent",
      "definition": "An optimization algorithm that iteratively adjusts model parameters in the direction of the negative gradient of the loss function to minimize prediction errors and improve model performance. Alternative definition: An optimization algorithm that iteratively adjusts neural network parameters in the direction that minimizes the loss function by following the negative gradient. Alternative definition: An optimization algorithm that iteratively adjusts neural network parameters in the direction that minimizes the loss function, using gradients to determine update directions and magnitudes. Alternative definition: An optimization algorithm that iteratively updates model parameters by moving in the direction opposite to the gradient of the loss function, fundamental to neural network training.",
      "chapter_source": "dl_primer",
      "aliases": [],
      "see_also": [],
      "appears_in": [
        "dl_primer",
        "dnn_architectures",
        "frameworks",
        "ondevice_learning",
        "training"
      ]
    },
    {
      "term": "gradient synchronization",
      "definition": "The process in distributed training where locally computed gradients are aggregated across devices to ensure all devices update their parameters consistently.",
      "chapter_source": "training",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "gradient-based pruning",
      "definition": "A pruning method that uses gradient information during training to identify neurons or filters with smaller gradient magnitudes, which contribute less to reducing the loss function and can be safely removed.",
      "chapter_source": "optimizations",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "graphics processing unit",
      "definition": "Specialized parallel processors originally designed for graphics rendering but widely adopted for machine learning due to their ability to efficiently execute matrix operations and handle thousands of parallel computations. Alternative definition: A specialized processor originally designed for rendering graphics that provides parallel processing capabilities essential for efficient neural network computation and training.",
      "chapter_source": "dl_primer",
      "aliases": [],
      "see_also": [],
      "appears_in": [
        "dl_primer",
        "training"
      ]
    },
    {
      "term": "graphics processing unit (gpu)",
      "definition": "A specialized processor originally designed for graphics rendering that provides massive parallel computing capabilities well-suited for neural network computations.",
      "chapter_source": "hw_acceleration",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "green computing",
      "definition": "The practice of designing, manufacturing, using, and disposing of computers and computer systems in an environmentally responsible manner.",
      "chapter_source": "sustainable_ai",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "green500",
      "definition": "Ranking system that evaluates the world's most powerful supercomputers based on energy efficiency measured in FLOPS per watt rather than raw computational performance.",
      "chapter_source": "benchmarking",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "grey-box attack",
      "definition": "An adversarial attack where the attacker has partial knowledge about the model, such as knowing the architecture but not the specific parameters or training data.",
      "chapter_source": "privacy_security",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "groupwise quantization",
      "definition": "A quantization approach where parameters are divided into groups, with each group sharing quantization parameters, offering a balance between compression and accuracy by providing more granular control than layerwise methods.",
      "chapter_source": "optimizations",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "gru",
      "definition": "Gated Recurrent Unit, a simplified variant of LSTM that uses fewer gates while maintaining the ability to capture long-term dependencies in sequential data.",
      "chapter_source": "dnn_architectures",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "hallucination",
      "definition": "The phenomenon where generative models produce outputs that appear plausible but are factually incorrect or inconsistent with reality.",
      "chapter_source": "generative_ai",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "hardware abstraction",
      "definition": "The layer in ML frameworks that provides a unified interface to diverse computing hardware (CPUs, GPUs, TPUs, accelerators) while handling device-specific optimizations and memory management behind the scenes.",
      "chapter_source": "frameworks",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "hardware acceleration",
      "definition": "The use of specialized computing hardware to perform certain operations faster and more efficiently than software running on general-purpose processors.",
      "chapter_source": "hw_acceleration",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "hardware accelerator",
      "definition": "Specialized computing hardware designed to efficiently execute specific types of computations, such as GPUs for parallel processing or TPUs for machine learning workloads.",
      "chapter_source": "benchmarking",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "hardware redundancy",
      "definition": "The duplication of critical hardware components to provide backup functionality and improve system reliability through voting mechanisms.",
      "chapter_source": "robust_ai",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "hardware trojan",
      "definition": "A malicious modification embedded in hardware components during manufacturing that can remain dormant under normal conditions but trigger harmful behavior when specific conditions are met.",
      "chapter_source": "privacy_security",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "hardware-aware design",
      "definition": "The practice of designing neural network architectures specifically optimized for target hardware platforms, considering factors like memory hierarchy, compute units, and data movement patterns to maximize efficiency.",
      "chapter_source": "optimizations",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "hdfs (hadoop distributed file system)",
      "definition": "A distributed file system designed to store large datasets across clusters of commodity hardware, providing scalability and fault tolerance for big data applications.",
      "chapter_source": "data_engineering",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "heartbeat mechanisms",
      "definition": "Periodic signals sent between system components to monitor health and detect failures, enabling timely fault detection and recovery.",
      "chapter_source": "robust_ai",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "hidden layer",
      "definition": "An intermediate layer in a neural network between input and output layers that learns abstract representations by transforming data through learned weights and activation functions.",
      "chapter_source": "dl_primer",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "hidden state",
      "definition": "The internal memory of recurrent neural networks that carries information from previous time steps, enabling the network to maintain context across sequential inputs.",
      "chapter_source": "dnn_architectures",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "hierarchical processing",
      "definition": "A multi-tier system architecture where data and intelligence flow between different levels of the computing stack, from sensors to edge devices to cloud systems.",
      "chapter_source": "ml_systems",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "hierarchical processing pattern",
      "definition": "A design pattern that organizes systems into tiers (edge, regional, cloud) that share responsibilities based on available resources and capabilities, optimizing resource usage across the computing spectrum.",
      "chapter_source": "ai_for_good",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "high bandwidth memory (hbm)",
      "definition": "An advanced memory technology that provides much higher bandwidth than traditional DRAM by using 3D stacking and wide interfaces, critical for data-intensive AI workloads.",
      "chapter_source": "hw_acceleration",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "homomorphic encryption",
      "definition": "A cryptographic technique that allows computations to be performed directly on encrypted data without decrypting it first, enabling privacy-preserving machine learning inference.",
      "chapter_source": "privacy_security",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "horizontal scaling",
      "definition": "Increasing system capacity by adding more machines or instances rather than upgrading existing hardware, providing better fault tolerance and load distribution.",
      "chapter_source": "ops",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "hot spares",
      "definition": "Backup components kept ready to instantaneously replace failing components without disrupting system operation, providing redundancy.",
      "chapter_source": "robust_ai",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "huber loss",
      "definition": "A robust loss function used in regression that is less sensitive to outliers compared to squared error loss, improving training stability.",
      "chapter_source": "robust_ai",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "human oversight",
      "definition": "The principle that human judgment should supervise, correct, or halt automated decisions, maintaining meaningful human control over AI systems.",
      "chapter_source": "responsible_ai",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "human-ai collaboration",
      "definition": "The synergistic partnership between humans and AI systems where each contributes their unique strengths to solve complex problems more effectively than either could alone.",
      "chapter_source": "frontiers",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "hybrid machine learning",
      "definition": "The integration of multiple ML paradigms such as cloud, edge, mobile, and tiny ML to form unified distributed systems that leverage complementary strengths.",
      "chapter_source": "ml_systems",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "hybrid parallelism",
      "definition": "A distributed training approach that combines data parallelism and model parallelism to leverage the benefits of both strategies for training very large models.",
      "chapter_source": "training",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "hyperparameter",
      "definition": "A configuration setting for machine learning algorithms that is set before training begins and controls aspects of the learning process, such as learning rate, batch size, or network architecture, distinct from model parameters learned during training. Alternative definition: A configuration setting that controls the learning process but is not learned from data, such as learning rate, batch size, or network architecture choices. Alternative definition: Configuration setting that governs the machine learning training process independently of the training data, including learning rates, batch sizes, and network architectures.",
      "chapter_source": "benchmarking",
      "aliases": [],
      "see_also": [],
      "appears_in": [
        "benchmarking",
        "dl_primer",
        "frameworks"
      ]
    },
    {
      "term": "hyperparameter optimization",
      "definition": "The process of finding the optimal configuration of hyperparameters (learning rate, batch size, network architecture parameters) that control the machine learning training process.",
      "chapter_source": "sustainable_ai",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "hyperparameters",
      "definition": "Configuration settings that control the learning process of machine learning algorithms but are not learned from data, such as learning rate, batch size, and network architecture parameters.",
      "chapter_source": "workflow",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "hyperscale data center",
      "definition": "Large-scale data center facilities containing thousands of servers and covering extensive floor space, designed to efficiently support massive computing workloads.",
      "chapter_source": "ml_systems",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "imagenet",
      "definition": "A massive visual database containing over 14 million labeled images across 20,000+ categories, created by Stanford's Fei-Fei Li starting in 2009, whose annual challenge became instrumental in driving breakthrough advances in computer vision. Alternative definition: Large-scale image dataset containing 14 million images across 20,000 categories, widely used as a benchmark for computer vision algorithms and credited with sparking the deep learning revolution. Alternative definition: Large-scale visual recognition dataset with 14+ million images across 20,000+ categories that drove computer vision breakthroughs through annual competitions from 2010-2017.",
      "chapter_source": "benchmarking",
      "aliases": [],
      "see_also": [],
      "appears_in": [
        "benchmarking",
        "efficient_ai",
        "introduction"
      ]
    },
    {
      "term": "imperative programming",
      "definition": "A programming paradigm where operations are executed immediately as they are encountered in the code, allowing for natural control flow and easier debugging but potentially limiting optimization opportunities.",
      "chapter_source": "frameworks",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "in_context_learning",
      "definition": "The ability of large language models to perform new tasks by providing examples or instructions within the input prompt, without additional training.",
      "chapter_source": "generative_ai",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "inference",
      "definition": "The phase of machine learning where a trained model makes predictions on new input data, typically requiring lower precision and computational resources than training. Alternative definition: The process of using a trained machine learning model to make predictions or decisions on new, previously unseen data. Alternative definition: Process of using a trained machine learning model to make predictions on new, unseen data in production environments. Alternative definition: The phase in machine learning where a trained model is used to make predictions or generate outputs on new, previously unseen data.",
      "chapter_source": "benchmarking",
      "aliases": [],
      "see_also": [],
      "appears_in": [
        "benchmarking",
        "hw_acceleration",
        "ml_systems",
        "sustainable_ai"
      ]
    },
    {
      "term": "infrastructure as code",
      "definition": "Practice of managing and provisioning computing infrastructure through machine-readable configuration files rather than manual processes, enabling version control and automation.",
      "chapter_source": "ops",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "instruction set architecture (isa)",
      "definition": "The interface between software and hardware that defines the set of instructions a processor can execute, including data types and addressing modes.",
      "chapter_source": "hw_acceleration",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "int8",
      "definition": "8-bit integer numerical representation used in quantized neural networks to reduce memory usage and accelerate inference while attempting to maintain model accuracy.",
      "chapter_source": "benchmarking",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "int8 quantization",
      "definition": "A numerical precision reduction technique that represents model weights and activations using 8-bit integers instead of 32-bit floating point numbers, reducing memory usage and enabling faster inference on specialized hardware. Alternative definition: A numerical precision reduction technique that represents neural network weights and activations using 8-bit integers instead of 32-bit floating-point numbers, improving speed and reducing memory usage.",
      "chapter_source": "hw_acceleration",
      "aliases": [],
      "see_also": [],
      "appears_in": [
        "hw_acceleration",
        "optimizations"
      ]
    },
    {
      "term": "intermittent faults",
      "definition": "Hardware faults that occur sporadically and unpredictably, appearing and disappearing without consistent patterns, making diagnosis challenging.",
      "chapter_source": "robust_ai",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "internet of things",
      "definition": "A network of physical objects embedded with sensors, software, and other technologies that connect and exchange data with other devices and systems over the internet.",
      "chapter_source": "ml_systems",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "interpretability",
      "definition": "The degree to which humans can understand the reasoning behind a machine learning model's predictions, often referring to inherently transparent models.",
      "chapter_source": "responsible_ai",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "iot sensors",
      "definition": "Internet of Things devices that collect and transmit environmental or behavioral data, often operating on limited power budgets and using low-bandwidth communication protocols.",
      "chapter_source": "ai_for_good",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "iterative pruning",
      "definition": "A gradual pruning strategy that removes parameters in multiple stages with fine-tuning between each stage, allowing the model to adapt to reduced capacity and typically achieving better accuracy than one-shot pruning.",
      "chapter_source": "optimizations",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "jax",
      "definition": "A numerical computing library developed by Google Research that combines NumPy's API with functional programming transformations including automatic differentiation, just-in-time compilation, and automatic vectorization for high-performance machine learning research.",
      "chapter_source": "frameworks",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "jit compilation",
      "definition": "Just-In-Time compilation that analyzes and optimizes code at runtime, enabling frameworks to balance the flexibility of eager execution with the performance benefits of graph optimization by compiling frequently used functions.",
      "chapter_source": "frameworks",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "k-anonymity",
      "definition": "A privacy technique that ensures each record in a dataset is indistinguishable from at least k-1 other records by generalizing quasi-identifiers.",
      "chapter_source": "data_engineering",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "kernel",
      "definition": "A small matrix of learnable weights used in convolutional layers to detect specific features through the convolution operation, also called a filter.",
      "chapter_source": "dnn_architectures",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "kernel fusion",
      "definition": "An optimization technique that combines multiple computational operations into a single kernel to reduce memory transfers and improve performance on parallel processors.",
      "chapter_source": "hw_acceleration",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "key performance indicators",
      "definition": "Specific, measurable metrics used to evaluate the success and effectiveness of machine learning systems, such as accuracy, precision, recall, latency, and throughput.",
      "chapter_source": "workflow",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "keyword spotting (kws)",
      "definition": "A technology that detects specific wake words or phrases in audio streams, typically used in voice-activated devices with constraints on power consumption and latency.",
      "chapter_source": "data_engineering",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "knowledge distillation",
      "definition": "A model compression technique where a smaller \"student\" network learns to mimic the behavior of a larger \"teacher\" network by training on the teacher's soft output probabilities rather than just hard labels.",
      "chapter_source": "efficient_ai",
      "aliases": [],
      "see_also": [],
      "appears_in": [
        "efficient_ai",
        "ondevice_learning",
        "optimizations",
        "sustainable_ai"
      ]
    },
    {
      "term": "label shift",
      "definition": "A type of distribution shift where the distribution of target labels changes while the conditional relationship between features and labels remains constant.",
      "chapter_source": "robust_ai",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "lapack",
      "definition": "Linear Algebra Package that extends BLAS with higher-level linear algebra operations including matrix decompositions, eigenvalue problems, and linear system solutions, providing essential mathematical foundations for machine learning computations.",
      "chapter_source": "frameworks",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "large language models",
      "definition": "Neural networks with billions or trillions of parameters trained on vast text corpora, capable of understanding and generating human-like text across diverse domains and tasks.",
      "chapter_source": "frontiers",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "large_language_model",
      "definition": "A neural network with billions of parameters trained on vast amounts of text data, capable of generating human-like text and performing various language tasks.",
      "chapter_source": "generative_ai",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "latency",
      "definition": "Time delay between receiving an input and producing an output, a critical performance metric for real-time applications that require immediate responses. Alternative definition: The time delay between initiating a request and receiving the response, critical for real-time AI applications and user experience. Alternative definition: The time delay between a request for data and the delivery of that data, critical in real-time applications where immediate responses are required.",
      "chapter_source": "benchmarking",
      "aliases": [],
      "see_also": [],
      "appears_in": [
        "benchmarking",
        "hw_acceleration",
        "ml_systems"
      ]
    },
    {
      "term": "latent_space",
      "definition": "A lower-dimensional representation space where generative models encode the essential features of data, enabling manipulation and generation of new samples.",
      "chapter_source": "generative_ai",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "layer normalization",
      "definition": "A normalization technique that normalizes inputs across the features dimension for each sample, commonly used in transformer architectures to stabilize training.",
      "chapter_source": "dnn_architectures",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "layerwise quantization",
      "definition": "A quantization granularity where all parameters within a single layer share the same quantization parameters, providing computational efficiency but potentially limiting representational precision compared to finer-grained approaches.",
      "chapter_source": "optimizations",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "learning rate",
      "definition": "A hyperparameter that determines the step size at which a model's parameters are updated during training, controlling how quickly or slowly the model learns from the training data. Alternative definition: A hyperparameter that determines the step size for weight updates during gradient descent optimization, critically affecting training stability and convergence speed.",
      "chapter_source": "dl_primer",
      "aliases": [],
      "see_also": [],
      "appears_in": [
        "dl_primer",
        "frameworks"
      ]
    },
    {
      "term": "lifecycle assessment",
      "definition": "A systematic approach to evaluating the environmental impacts of a product or system throughout its entire life cycle, from raw material extraction to disposal.",
      "chapter_source": "sustainable_ai",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "linpack",
      "definition": "Benchmark developed at Argonne National Laboratory that measures system performance by solving dense systems of linear equations, famous for its use in Top500 supercomputer rankings.",
      "chapter_source": "benchmarking",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "load balancing",
      "definition": "Distribution of incoming requests across multiple server instances to prevent bottlenecks, improve response times, and ensure high availability.",
      "chapter_source": "ops",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "lookup table",
      "definition": "A data structure that replaces runtime computation with simpler array indexing operations, commonly used for performance optimization.",
      "chapter_source": "robust_ai",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "lora technology",
      "definition": "Long Range wireless communication protocol that enables IoT devices to communicate over 15+ kilometers with minimal power consumption, ideal for agricultural and environmental monitoring applications.",
      "chapter_source": "ai_for_good",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "loss function",
      "definition": "A mathematical function that quantifies the difference between neural network predictions and true labels, providing the optimization objective for training algorithms.",
      "chapter_source": "dl_primer",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "loss scaling",
      "definition": "A technique used in mixed-precision training that multiplies the loss by a large factor before backpropagation to prevent gradient underflow in reduced precision formats.",
      "chapter_source": "training",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "lottery ticket hypothesis",
      "definition": "The theory that large neural networks contain sparse subnetworks that, when trained in isolation from proper initialization, can achieve comparable accuracy to the full network while being significantly smaller.",
      "chapter_source": "optimizations",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "low-rank adaptation",
      "definition": "A parameter-efficient fine-tuning method that approximates weight updates using low-rank matrices, reducing trainable parameters while maintaining adaptation capability.",
      "chapter_source": "ondevice_learning",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "low-rank factorization",
      "definition": "A matrix decomposition technique that approximates large weight matrices as products of smaller matrices, reducing the number of parameters and computational operations required for neural network layers.",
      "chapter_source": "optimizations",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "lstm",
      "definition": "Long Short-Term Memory, a type of recurrent neural network architecture designed to handle long-term dependencies through gating mechanisms that control information flow.",
      "chapter_source": "dnn_architectures",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "machine consciousness",
      "definition": "The hypothetical emergence of conscious awareness in artificial systems, representing a frontier research area exploring whether machines can develop subjective experiences.",
      "chapter_source": "frontiers",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "machine learning",
      "definition": "The methodological approach to implementing intelligent systems through computational techniques that automatically discover patterns in data, rather than through predetermined rules or explicit programming. Alternative definition: A subset of artificial intelligence that enables systems to automatically learn and improve from experience without being explicitly programmed for specific tasks. Alternative definition: A subset of artificial intelligence that enables computers to learn and make decisions from data without being explicitly programmed for each specific task. Alternative definition: A subset of artificial intelligence that enables systems to automatically improve performance on tasks through experience and data rather than explicit programming. Alternative definition: Field of artificial intelligence that enables computer systems to automatically learn and improve performance on specific tasks through experience without being explicitly programmed.",
      "chapter_source": "benchmarking",
      "aliases": [],
      "see_also": [],
      "appears_in": [
        "benchmarking",
        "dl_primer",
        "introduction",
        "ml_systems",
        "sustainable_ai"
      ]
    },
    {
      "term": "machine learning accelerator (ml accelerator)",
      "definition": "Specialized computing hardware designed to efficiently execute machine learning workloads through optimized matrix operations, memory hierarchies, and parallel processing units.",
      "chapter_source": "hw_acceleration",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "machine learning framework",
      "definition": "A software platform that provides tools and abstractions for designing, training, and deploying machine learning models, bridging user applications with infrastructure through computational graphs, hardware optimization, and workflow orchestration.",
      "chapter_source": "frameworks",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "machine learning frameworks",
      "definition": "Software libraries and platforms that provide tools, APIs, and abstractions for developing, training, and deploying machine learning models, such as TensorFlow and PyTorch.",
      "chapter_source": "conclusion",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "machine learning lifecycle",
      "definition": "A structured, iterative process that encompasses all stages involved in developing, deploying, and maintaining machine learning systems, from problem definition through ongoing monitoring and improvement. Alternative definition: The complete process of developing, deploying, and maintaining ML systems, from data collection through model retirement.",
      "chapter_source": "responsible_ai",
      "aliases": [],
      "see_also": [],
      "appears_in": [
        "responsible_ai",
        "workflow"
      ]
    },
    {
      "term": "machine learning operations",
      "definition": "The practice and set of tools focused on operationalizing machine learning models through automation, monitoring, and management of the entire ML pipeline from development to production.",
      "chapter_source": "workflow",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "machine learning operations (mlops)",
      "definition": "The practice of deploying and maintaining machine learning models in production reliably and efficiently through automated pipelines.",
      "chapter_source": "robust_ai",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "machine learning security",
      "definition": "The protection of data, models, and infrastructure from unauthorized access, manipulation, or disruption throughout the entire machine learning lifecycle.",
      "chapter_source": "privacy_security",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "machine learning systems engineering",
      "definition": "The engineering discipline focused on building reliable, efficient, and scalable AI systems across computational platforms, spanning the entire AI lifecycle from data acquisition through deployment and operations with emphasis on resource-awareness and system-level optimization.",
      "chapter_source": "introduction",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "machine unlearning",
      "definition": "Techniques for removing the influence of specific data points from trained models without complete retraining, supporting data deletion rights.",
      "chapter_source": "responsible_ai",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "macro benchmarks",
      "definition": "Evaluation methodology that assesses complete machine learning models to understand how architectural choices and component interactions affect overall system behavior and performance.",
      "chapter_source": "benchmarking",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "magnitude-based pruning",
      "definition": "The most common pruning method that removes parameters with the smallest absolute values, based on the assumption that weights with smaller magnitudes contribute less to the model's output.",
      "chapter_source": "optimizations",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "masking",
      "definition": "An anonymization technique that alters or obfuscates sensitive values so they cannot be directly traced back to the original data subject.",
      "chapter_source": "data_engineering",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "megawatt-hour",
      "definition": "A unit of energy equal to one megawatt of power used for one hour, commonly used to measure electricity consumption in large facilities like data centers.",
      "chapter_source": "sustainable_ai",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "membership inference attack",
      "definition": "An attack that attempts to determine whether a specific data point was included in a model's training dataset by analyzing the model's behavior and outputs.",
      "chapter_source": "privacy_security",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "membership inference attacks",
      "definition": "Privacy attacks that attempt to determine whether a specific data point was included in a model's training set by analyzing model behavior.",
      "chapter_source": "responsible_ai",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "memory bandwidth",
      "definition": "Rate at which data can be read from or written to memory, measured in bytes per second, which often becomes a bottleneck in memory-intensive machine learning workloads.",
      "chapter_source": "benchmarking",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "memory hierarchy",
      "definition": "The organization of memory systems with different access speeds and capacities, from fast on-chip caches to slower off-chip main memory.",
      "chapter_source": "hw_acceleration",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "meta-learning",
      "definition": "The process of learning how to learn, where models are trained to quickly adapt to new tasks with minimal data, particularly useful for personalization in on-device systems. Alternative definition: Learning algorithms that can learn how to learn, adapting quickly to new tasks with minimal training data by leveraging prior experience across related tasks.",
      "chapter_source": "frontiers",
      "aliases": [],
      "see_also": [],
      "appears_in": [
        "frontiers",
        "ondevice_learning"
      ]
    },
    {
      "term": "metadata",
      "definition": "Descriptive information about datasets that includes details about data collection, quality metrics, validation status, and other contextual information essential for data management.",
      "chapter_source": "data_engineering",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "micro benchmarks",
      "definition": "Specialized evaluation tools that assess individual components or specific operations within machine learning systems, such as tensor operations or neural network layers.",
      "chapter_source": "benchmarking",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "microcontroller",
      "definition": "A compact integrated circuit designed to govern specific operations in embedded systems, typically featuring limited processing power and memory but optimized for low power consumption and real-time applications. Alternative definition: A small computer on a single integrated circuit containing a processor core, memory, and programmable input/output peripherals, commonly used in embedded systems.",
      "chapter_source": "ai_for_good",
      "aliases": [],
      "see_also": [],
      "appears_in": [
        "ai_for_good",
        "ml_systems"
      ]
    },
    {
      "term": "mini-batch gradient descent",
      "definition": "A training approach that computes gradients and updates weights using a small subset of training examples simultaneously, balancing computational efficiency with gradient estimation quality.",
      "chapter_source": "dl_primer",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "mini-batch processing",
      "definition": "An optimization approach that computes gradients over small batches of examples, balancing the computational efficiency of batch processing with the memory constraints of stochastic methods.",
      "chapter_source": "training",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "minimax",
      "definition": "A decision-making strategy used in game theory that attempts to minimize the maximum possible loss in adversarial scenarios.",
      "chapter_source": "robust_ai",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "mixed precision training",
      "definition": "A technique that uses different numerical precisions for different parts of neural network training, typically combining 16-bit and 32-bit floating-point arithmetic to reduce memory usage and increase training speed.",
      "chapter_source": "sustainable_ai",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "mixed-precision computing",
      "definition": "A technique that uses different numerical precisions at various stages of computation, such as FP16 for matrix multiplications and FP32 for accumulations.",
      "chapter_source": "hw_acceleration",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "mixed-precision training",
      "definition": "A training technique that uses different numerical precisions for different operations, typically combining FP16 for forward and backward passes with FP32 for gradient updates to balance efficiency and numerical stability. Alternative definition: A training methodology that combines different numerical precisions (typically FP16 and FP32) to optimize memory usage and computational speed while maintaining training stability. Alternative definition: Training technique that uses both 16-bit and 32-bit floating-point representations to accelerate training while maintaining model accuracy, enabling larger batch sizes and faster convergence. Alternative definition: A training technique that uses both 16-bit and 32-bit floating-point representations to accelerate training while maintaining model accuracy and reducing memory usage.",
      "chapter_source": "benchmarking",
      "aliases": [],
      "see_also": [],
      "appears_in": [
        "benchmarking",
        "conclusion",
        "optimizations",
        "training"
      ]
    },
    {
      "term": "ml benchmarking",
      "definition": "Systematic evaluation of compute performance, algorithmic effectiveness, and data quality in machine learning systems to optimize performance across diverse workloads and ensure reproducibility.",
      "chapter_source": "benchmarking",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "ml lifecycle",
      "definition": "The iterative process that guides the development, evaluation, and continual improvement of machine learning systems, involving stages from data collection through model monitoring with feedback loops for continuous adaptation.",
      "chapter_source": "introduction",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "ml systems",
      "definition": "Integrated computing systems comprising three core components: data that guides algorithmic behavior, learning algorithms that extract patterns from data, and computing infrastructure that enables both training and inference processes.",
      "chapter_source": "introduction",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "mlcommons",
      "definition": "Organization that develops and maintains industry-standard benchmarks for machine learning systems, including the MLPerf suite for training and inference evaluation.",
      "chapter_source": "benchmarking",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "mlops",
      "definition": "Engineering discipline that manages the end-to-end lifecycle of machine learning systems, combining ML development with operational practices for reliable production deployment. Alternative definition: The practice of applying DevOps principles to machine learning systems, encompassing the entire ML lifecycle from development and deployment to monitoring and maintenance.",
      "chapter_source": "conclusion",
      "aliases": [],
      "see_also": [],
      "appears_in": [
        "conclusion",
        "ops"
      ]
    },
    {
      "term": "mlperf",
      "definition": "Industry-standard benchmark suite that provides standardized tests for training and inference across various deep learning workloads, enabling fair comparisons of machine learning systems.",
      "chapter_source": "benchmarking",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "mlperf inference",
      "definition": "Benchmark framework that evaluates machine learning inference performance across different deployment environments, from cloud data centers to mobile devices and embedded systems.",
      "chapter_source": "benchmarking",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "mlperf mobile",
      "definition": "Specialized benchmark that extends MLPerf evaluation to smartphones and mobile devices, measuring latency and responsiveness under strict power and memory constraints.",
      "chapter_source": "benchmarking",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "mlperf tiny",
      "definition": "Benchmark designed for embedded and ultra-low-power AI systems such as IoT devices, wearables, and microcontrollers operating with minimal processing capabilities.",
      "chapter_source": "benchmarking",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "mlperf training",
      "definition": "Standardized benchmark that evaluates machine learning training performance by measuring time-to-accuracy, throughput, and resource utilization across different hardware platforms.",
      "chapter_source": "benchmarking",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "mnist",
      "definition": "Modified National Institute of Standards and Technology database of handwritten digits containing 70,000 28Ã—28 pixel images, serving as the \"Hello World\" of computer vision.",
      "chapter_source": "efficient_ai",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "mobile machine learning",
      "definition": "The execution of machine learning models directly on portable, battery-powered devices like smartphones and tablets, enabling personalized and responsive applications.",
      "chapter_source": "ml_systems",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "mobile ml",
      "definition": "Machine learning systems optimized for mobile devices like smartphones and tablets, balancing computational efficiency with inference accuracy for on-device processing.",
      "chapter_source": "ai_for_good",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "mobilenet",
      "definition": "Efficient neural network architecture using depthwise separable convolutions, achieving approximately 50Ã— fewer parameters than traditional models while enabling smartphone deployment. Alternative definition: A family of efficient neural network architectures designed for mobile devices using depthwise separable convolutions to achieve significant reductions in model size and computation.",
      "chapter_source": "efficient_ai",
      "aliases": [],
      "see_also": [],
      "appears_in": [
        "efficient_ai",
        "ondevice_learning"
      ]
    },
    {
      "term": "mode_collapse",
      "definition": "A failure mode in generative models where the model generates only a limited variety of samples, failing to capture the full diversity of the training data.",
      "chapter_source": "generative_ai",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "model cards",
      "definition": "Documentation framework that provides structured information about machine learning models, including intended use, performance characteristics, and limitations.",
      "chapter_source": "responsible_ai",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "model compression",
      "definition": "Techniques to reduce the size and computational requirements of machine learning models through methods like quantization, pruning, and knowledge distillation to enable deployment on resource-constrained devices. Alternative definition: Techniques used to reduce the size and computational requirements of machine learning models while preserving their performance, including pruning, quantization, and knowledge distillation. Alternative definition: A set of techniques designed to reduce the size and computational requirements of machine learning models while preserving their accuracy and functionality. Alternative definition: Techniques used to reduce the size and computational requirements of machine learning models while preserving accuracy, enabling deployment on resource-constrained devices. Alternative definition: Techniques such as quantization, pruning, and distillation used to reduce model size and computational requirements while maintaining acceptable performance. Alternative definition: The broad category of techniques aimed at reducing model size and computational requirements while maintaining accuracy, including pruning, quantization, knowledge distillation, and architectural modifications.",
      "chapter_source": "ai_for_good",
      "aliases": [],
      "see_also": [],
      "appears_in": [
        "ai_for_good",
        "conclusion",
        "ml_systems",
        "ondevice_learning",
        "ops",
        "optimizations",
        "sustainable_ai"
      ]
    },
    {
      "term": "model deployment",
      "definition": "The process of integrating trained machine learning models into production systems where they can make predictions on new data and provide value to end users. Alternative definition: Process of integrating trained machine learning models into production environments where they can serve predictions to end users or applications.",
      "chapter_source": "ops",
      "aliases": [],
      "see_also": [],
      "appears_in": [
        "ops",
        "workflow"
      ]
    },
    {
      "term": "model drift",
      "definition": "The degradation of machine learning model performance over time due to changes in data patterns, user behavior, or environmental conditions that differ from the original training conditions. Alternative definition: Gradual decline in model performance over time due to changes in data patterns, requiring continuous monitoring and periodic retraining.",
      "chapter_source": "ops",
      "aliases": [],
      "see_also": [],
      "appears_in": [
        "ops",
        "workflow"
      ]
    },
    {
      "term": "model evaluation",
      "definition": "The systematic assessment of machine learning model performance using various metrics and validation techniques to determine whether the model meets requirements and is ready for deployment.",
      "chapter_source": "workflow",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "model extraction",
      "definition": "The process of stealing or recreating a machine learning model by observing its input-output behavior, often through systematic querying of model APIs.",
      "chapter_source": "privacy_security",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "model inversion attack",
      "definition": "An attack that attempts to reconstruct training data or infer sensitive information about the dataset by analyzing a model's outputs and confidence scores.",
      "chapter_source": "privacy_security",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "model optimization",
      "definition": "The systematic refinement of machine learning models to enhance their efficiency while maintaining effectiveness, balancing trade-offs between accuracy, computational cost, memory usage, latency, and energy efficiency. Alternative definition: The process of improving machine learning models for better performance, reduced resource consumption, or compatibility with deployment constraints while maintaining acceptable accuracy levels.",
      "chapter_source": "optimizations",
      "aliases": [],
      "see_also": [],
      "appears_in": [
        "optimizations",
        "workflow"
      ]
    },
    {
      "term": "model parallelism",
      "definition": "A distributed computing strategy that partitions a neural network model across multiple devices, enabling training of large models that exceed the memory capacity of individual devices by distributing layers or components across hardware.",
      "chapter_source": "benchmarking",
      "aliases": [],
      "see_also": [],
      "appears_in": [
        "benchmarking",
        "conclusion",
        "efficient_ai",
        "frameworks",
        "training"
      ]
    },
    {
      "term": "model pruning",
      "definition": "The process of removing unnecessary weights, neurons, or connections from a trained neural network to reduce its size and computational requirements.",
      "chapter_source": "sustainable_ai",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "model quantization",
      "definition": "A technique that reduces the precision of model parameters (typically from 32-bit to 8-bit or lower) to decrease model size and computational requirements while maintaining acceptable accuracy. Alternative definition: The process of reducing the precision of numerical representations in machine learning models, typically from 32-bit to 8-bit integers, to decrease model size and increase inference speed.",
      "chapter_source": "ai_for_good",
      "aliases": [],
      "see_also": [],
      "appears_in": [
        "ai_for_good",
        "ml_systems"
      ]
    },
    {
      "term": "model registry",
      "definition": "Centralized repository for storing, versioning, and managing trained machine learning models with associated metadata, facilitating model governance and deployment.",
      "chapter_source": "ops",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "model serving",
      "definition": "Infrastructure and systems that expose deployed machine learning models through APIs to handle prediction requests at scale with appropriate latency and throughput.",
      "chapter_source": "ops",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "model theft",
      "definition": "The unauthorized acquisition of proprietary machine learning models, either through extracting exact parameters and architecture or by replicating the model's behavior.",
      "chapter_source": "privacy_security",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "model training",
      "definition": "The process of using machine learning algorithms to learn patterns from training data, adjusting model parameters to minimize prediction errors and create a functional predictive system.",
      "chapter_source": "workflow",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "model uncertainty",
      "definition": "The inadequacy of a machine learning model to capture the full complexity of the underlying data-generating process, leading to prediction uncertainty.",
      "chapter_source": "robust_ai",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "model validation",
      "definition": "The process of testing machine learning models on independent datasets to assess their generalization ability and ensure they perform reliably on unseen data. Alternative definition: Systematic evaluation process to ensure models meet performance, robustness, and reliability criteria before production deployment.",
      "chapter_source": "ops",
      "aliases": [],
      "see_also": [],
      "appears_in": [
        "ops",
        "workflow"
      ]
    },
    {
      "term": "model versioning",
      "definition": "The systematic tracking and management of different versions of machine learning models, including their parameters, training data, and performance metrics, to enable comparison and rollback capabilities. Alternative definition: Practice of systematically tracking different iterations of machine learning models with associated metadata to enable rollbacks and comparisons.",
      "chapter_source": "ops",
      "aliases": [],
      "see_also": [],
      "appears_in": [
        "ops",
        "workflow"
      ]
    },
    {
      "term": "model watermarking",
      "definition": "A technique for embedding verifiable ownership signatures into machine learning models that can be used to detect unauthorized use or prove intellectual property theft.",
      "chapter_source": "privacy_security",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "momentum",
      "definition": "An optimization technique that accumulates a velocity vector across iterations to help gradient descent navigate through local minima and accelerate convergence in consistent gradient directions.",
      "chapter_source": "training",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "monitoring",
      "definition": "The continuous observation and measurement of machine learning system performance, data quality, and operational metrics in production to detect issues and trigger maintenance actions.",
      "chapter_source": "workflow",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "monte carlo dropout",
      "definition": "A technique that uses multiple forward passes with different dropout masks at inference time to estimate prediction uncertainty.",
      "chapter_source": "robust_ai",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "moore's law",
      "definition": "The observation that the number of transistors on a microchip doubles approximately every two years while the cost of computers is halved.",
      "chapter_source": "sustainable_ai",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "moores_law",
      "definition": "Intel co-founder Gordon Moore's 1965 observation that transistor density doubles every 2 years, with hardware improvements following this trend while AI algorithmic efficiency improved 44Ã— in 7 years.",
      "chapter_source": "efficient_ai",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "multi-head attention",
      "definition": "An attention mechanism that uses multiple parallel attention heads, each focusing on different aspects of the input to capture diverse types of relationships simultaneously.",
      "chapter_source": "dnn_architectures",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "multi-layer perceptron",
      "definition": "A feedforward neural network with one or more hidden layers between input and output, capable of learning non-linear mappings through dense connections and activation functions.",
      "chapter_source": "dnn_architectures",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "multicalibration",
      "definition": "A fairness technique ensuring that model predictions remain calibrated across intersecting subgroups, addressing complex demographic interactions.",
      "chapter_source": "responsible_ai",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "multilayer perceptron",
      "definition": "A feedforward neural network with one or more hidden layers between input and output layers, capable of learning nonlinear relationships in data.",
      "chapter_source": "dl_primer",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "multimodal ai",
      "definition": "AI systems that can process and understand multiple types of data simultaneously, such as text, images, audio, and video, enabling more comprehensive understanding and interaction.",
      "chapter_source": "frontiers",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "multimodal_generation",
      "definition": "The ability to generate content across different data types, such as creating images from text descriptions or generating captions for images.",
      "chapter_source": "generative_ai",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "mycin",
      "definition": "One of the first large-scale expert systems developed at Stanford in 1976 to diagnose blood infections, representing the shift toward capturing human expert knowledge in specific domains rather than pursuing general artificial intelligence.",
      "chapter_source": "introduction",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "neural architecture search",
      "definition": "An automated approach that uses machine learning algorithms to discover optimal neural network architectures by searching through possible combinations of layers, connections, and hyperparameters for specific constraints.",
      "chapter_source": "efficient_ai",
      "aliases": [],
      "see_also": [],
      "appears_in": [
        "efficient_ai",
        "frontiers",
        "optimizations",
        "sustainable_ai"
      ]
    },
    {
      "term": "neural engine",
      "definition": "Specialized hardware accelerators designed for machine learning inference and training, such as Apple's Neural Engine or Google's Edge TPU, optimized for on-device AI workloads.",
      "chapter_source": "ondevice_learning",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "neural network",
      "definition": "A computational model inspired by biological neural networks, consisting of interconnected nodes (neurons) that process information through weighted connections and activation functions. Alternative definition: A computational model inspired by biological neural networks, consisting of interconnected nodes (neurons) organized in layers that can learn complex patterns from data. Alternative definition: A computational model consisting of interconnected nodes organized in layers that can learn to map inputs to outputs through adjustable connection weights.",
      "chapter_source": "benchmarking",
      "aliases": [],
      "see_also": [],
      "appears_in": [
        "benchmarking",
        "dl_primer",
        "dnn_architectures",
        "sustainable_ai"
      ]
    },
    {
      "term": "neural processing unit",
      "definition": "Specialized processors designed specifically for accelerating neural network operations and machine learning computations, optimized for parallel processing of AI workloads.",
      "chapter_source": "ml_systems",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "neural processing unit (npu)",
      "definition": "Specialized processor designed specifically for artificial neural network computations, optimized for the mathematical operations common in machine learning workloads. Alternative definition: A specialized processor designed specifically for neural network computations, optimizing common operations like matrix multiplication and activation functions.",
      "chapter_source": "benchmarking",
      "aliases": [],
      "see_also": [],
      "appears_in": [
        "benchmarking",
        "hw_acceleration"
      ]
    },
    {
      "term": "neural_language_model",
      "definition": "A neural network trained to predict the probability of word sequences, forming the foundation for text generation and understanding tasks.",
      "chapter_source": "generative_ai",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "neuromorphic computing",
      "definition": "Computing architectures inspired by the structure and function of biological neural networks, designed to process information more efficiently than traditional digital computers. Alternative definition: A computing approach that mimics the structure and function of biological neural networks, potentially offering more energy-efficient processing for AI applications.",
      "chapter_source": "conclusion",
      "aliases": [],
      "see_also": [],
      "appears_in": [
        "conclusion",
        "frontiers"
      ]
    },
    {
      "term": "non-iid data",
      "definition": "Non-independent and identically distributed data where samples are not uniformly distributed across devices or time, creating challenges for federated learning convergence and generalization.",
      "chapter_source": "ondevice_learning",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "nosql",
      "definition": "A category of database systems designed to handle large volumes of unstructured or semi-structured data with flexible schemas, often used in big data applications.",
      "chapter_source": "data_engineering",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "numerical precision optimization",
      "definition": "The dimension of model optimization that addresses how numerical values are represented and processed, including quantization techniques that map high-precision values to lower-bit representations.",
      "chapter_source": "optimizations",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "observability",
      "definition": "Comprehensive monitoring approach that provides insight into system behavior through metrics, logs, and traces, enabling understanding of internal states from external outputs.",
      "chapter_source": "ops",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "olap (online analytical processing)",
      "definition": "A database approach optimized for complex analytical queries across large datasets, typically used in data warehouses for business intelligence.",
      "chapter_source": "data_engineering",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "oltp (online transaction processing)",
      "definition": "A database approach optimized for frequent, short transactions and real-time processing, commonly used in operational applications.",
      "chapter_source": "data_engineering",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "on-chip memory",
      "definition": "Fast memory integrated directly onto the processor chip, including caches and scratchpad memory, providing high bandwidth and low latency data access.",
      "chapter_source": "hw_acceleration",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "on-device learning",
      "definition": "The local adaptation or training of machine learning models directly on deployed hardware devices without reliance on continuous connectivity to centralized servers. Alternative definition: The capability for machine learning models to adapt and learn directly on edge devices without requiring data transmission to external servers.",
      "chapter_source": "conclusion",
      "aliases": [],
      "see_also": [],
      "appears_in": [
        "conclusion",
        "ondevice_learning"
      ]
    },
    {
      "term": "one-shot pruning",
      "definition": "A pruning strategy where a large fraction of parameters is removed in a single step, typically followed by fine-tuning to recover accuracy, offering simplicity but potentially requiring more aggressive fine-tuning.",
      "chapter_source": "optimizations",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "online inference",
      "definition": "Real-time prediction serving that processes individual requests with low latency, suitable for interactive applications requiring immediate responses.",
      "chapter_source": "ops",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "onnx",
      "definition": "Open Neural Network Exchange, a standardized format for representing machine learning models that enables interoperability between different frameworks, allowing models trained in one framework to be deployed using another.",
      "chapter_source": "frameworks",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "onnx runtime",
      "definition": "Cross-platform inference engine that optimizes machine learning models through techniques like operator fusion and kernel tuning to improve inference speed and reduce computational overhead.",
      "chapter_source": "benchmarking",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "optimizer",
      "definition": "An algorithm that adjusts model parameters during training to minimize the loss function, with common examples including SGD (Stochastic Gradient Descent), Adam, and RMSprop, each with different strategies for parameter updates.",
      "chapter_source": "frameworks",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "orchestration",
      "definition": "Coordination and management of complex workflows and distributed computing tasks, often using platforms like Kubernetes for container management.",
      "chapter_source": "ops",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "outlier detection",
      "definition": "The process of identifying data points that significantly deviate from normal patterns, which may represent errors, anomalies, or valuable rare events.",
      "chapter_source": "data_engineering",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "overfitting",
      "definition": "A phenomenon where a model learns specific details of training data so well that it fails to generalize to new, unseen examples, typically indicated by high training accuracy but poor validation performance.",
      "chapter_source": "dl_primer",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "oxide breakdown",
      "definition": "The failure of an oxide layer in transistors due to excessive electric field stress, causing permanent hardware faults.",
      "chapter_source": "robust_ai",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "padding",
      "definition": "A technique in convolutional networks that adds zeros or other values around the input borders to control the spatial dimensions of the output feature maps.",
      "chapter_source": "dnn_architectures",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "paradigm shift",
      "definition": "A fundamental change in scientific approach, like the shift from symbolic reasoning to statistical learning in AI during the 1990s, and from shallow to deep learning in the 2010s, requiring researchers to abandon established methods for radically different approaches.",
      "chapter_source": "introduction",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "parallelism",
      "definition": "The simultaneous execution of multiple computational tasks or operations, fundamental to achieving high performance in neural network processing.",
      "chapter_source": "hw_acceleration",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "parameter",
      "definition": "A learnable variable in a machine learning model, such as weights and biases in neural networks, that are adjusted during training to optimize performance. Alternative definition: A learnable component of a neural network, including weights and biases, that gets adjusted during training to minimize the loss function.",
      "chapter_source": "dl_primer",
      "aliases": [],
      "see_also": [],
      "appears_in": [
        "dl_primer",
        "sustainable_ai"
      ]
    },
    {
      "term": "parameter_efficient_finetuning",
      "definition": "Methods like LoRA and Adapters that update less than 1% of model parameters while achieving full fine-tuning performance, reducing memory requirements from gigabytes to megabytes.",
      "chapter_source": "efficient_ai",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "partitioning",
      "definition": "A database technique that divides large datasets into smaller, manageable segments based on specific criteria to improve query performance and system scalability.",
      "chapter_source": "data_engineering",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "perceptron",
      "definition": "One of the first computational learning algorithms developed by Frank Rosenblatt in 1957, a system that could learn to classify patterns by making yes/no decisions based on inputs, paving the way for modern neural networks. Alternative definition: The fundamental building block of neural networks, consisting of weighted inputs, a bias term, and an activation function that produces a single output.",
      "chapter_source": "dl_primer",
      "aliases": [],
      "see_also": [],
      "appears_in": [
        "dl_primer",
        "introduction"
      ]
    },
    {
      "term": "performance insights",
      "definition": "Analytical observations derived from monitoring production machine learning systems that reveal opportunities for improvement in model accuracy, system efficiency, or user experience.",
      "chapter_source": "workflow",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "permanent faults",
      "definition": "Hardware defects that persist irreversibly until repair or component replacement, consistently affecting system behavior.",
      "chapter_source": "robust_ai",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "perplexity",
      "definition": "Measurement of how well a language model predicts text, calculated as 2^(cross-entropy loss), with lower values indicating better prediction capability.",
      "chapter_source": "efficient_ai",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "personalization layers",
      "definition": "Model components, typically the final classification layers, that are adapted locally to user-specific data while keeping shared backbone layers frozen.",
      "chapter_source": "ondevice_learning",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "physical attack",
      "definition": "Direct manipulation or tampering with computing hardware to compromise the security and integrity of machine learning systems, bypassing traditional software defenses.",
      "chapter_source": "privacy_security",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "pipeline parallelism",
      "definition": "A model parallelism technique that distributes consecutive layers of a neural network across different devices, enabling parallel processing where each device works on different stages of the computation pipeline. Alternative definition: A form of model parallelism where different layers of a model are placed on different devices and data flows through them in a pipeline fashion, allowing multiple batches to be processed simultaneously.",
      "chapter_source": "frameworks",
      "aliases": [],
      "see_also": [],
      "appears_in": [
        "frameworks",
        "training"
      ]
    },
    {
      "term": "pooling",
      "definition": "A downsampling operation in convolutional networks that reduces spatial dimensions while retaining important features, commonly using max or average operations over local regions.",
      "chapter_source": "dnn_architectures",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "positional encoding",
      "definition": "A method used in transformer architectures to inject information about the position of tokens in a sequence, since transformers lack inherent sequential processing.",
      "chapter_source": "dnn_architectures",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "post-hoc explanations",
      "definition": "Explanation methods applied after model training that treat the model as a black box and infer reasoning patterns from input-output behavior.",
      "chapter_source": "responsible_ai",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "post-training quantization",
      "definition": "A quantization approach applied to already-trained models without modifying the training process, typically involving calibration on representative data to determine optimal quantization parameters.",
      "chapter_source": "optimizations",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "power usage effectiveness",
      "definition": "A metric used to determine the energy efficiency of a data center, calculated as the ratio of total facility energy consumption to IT equipment energy consumption.",
      "chapter_source": "sustainable_ai",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "power usage effectiveness (pue)",
      "definition": "Metric used in data centers to measure energy efficiency, calculated as the ratio of total facility power consumption to IT equipment power consumption.",
      "chapter_source": "benchmarking",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "precision",
      "definition": "In numerical computing, the number of bits used to represent numbers, affecting both computational accuracy and resource requirements in machine learning systems.",
      "chapter_source": "benchmarking",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "precision agriculture",
      "definition": "The use of technology including GPS, sensors, and machine learning to optimize farming practices by precisely monitoring and managing crop inputs like water, fertilizer, and pesticides.",
      "chapter_source": "ai_for_good",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "prefetching",
      "definition": "A system optimization technique that loads data into memory before it is needed, overlapping data loading with computation to reduce idle time and improve training throughput.",
      "chapter_source": "training",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "principal_component_analysis",
      "definition": "Dimensionality reduction technique that identifies the most important directions of variation in data, reducing computational complexity while preserving 90%+ of data variance.",
      "chapter_source": "efficient_ai",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "principle of least privilege",
      "definition": "A security concept where users are given the minimum access levels necessary to complete their job functions, reducing security risks.",
      "chapter_source": "robust_ai",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "privacy budget",
      "definition": "A concept in differential privacy that represents the total amount of privacy loss allowed across all queries or computations, with each operation consuming part of this finite budget.",
      "chapter_source": "privacy_security",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "privacy-preserving machine learning",
      "definition": "Techniques and approaches that enable machine learning while protecting the privacy of individuals whose data is used for training or inference.",
      "chapter_source": "privacy_security",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "privacy-preserving techniques",
      "definition": "Methods designed to protect individual privacy in machine learning, including differential privacy, federated learning, and local processing.",
      "chapter_source": "responsible_ai",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "problem definition",
      "definition": "The initial stage of machine learning development that involves clearly specifying objectives, constraints, success metrics, and operational requirements to guide all subsequent development decisions.",
      "chapter_source": "workflow",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "progressive enhancement pattern",
      "definition": "A design pattern that establishes baseline functionality under minimal resource conditions and incrementally incorporates advanced features as additional resources become available.",
      "chapter_source": "ai_for_good",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "prompt engineering",
      "definition": "The practice of designing and optimizing text prompts to effectively communicate with large language models and achieve desired outputs from AI systems.",
      "chapter_source": "frontiers",
      "aliases": [],
      "see_also": [],
      "appears_in": [
        "frontiers",
        "generative_ai"
      ]
    },
    {
      "term": "protein folding problem",
      "definition": "The scientific challenge of predicting the three-dimensional structure of proteins from their amino acid sequences, a problem that puzzled scientists for decades until systems like AlphaFold achieved breakthrough accuracy using deep learning approaches.",
      "chapter_source": "introduction",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "pruning",
      "definition": "A model optimization technique that removes unnecessary parameters from neural networks while maintaining predictive performance, reducing model size and computational cost by eliminating redundant weights, neurons, or layers. Alternative definition: A model compression technique that removes unnecessary connections or neurons from neural networks to reduce model size and computational requirements without significantly impacting performance. Alternative definition: Removing unimportant neural network connections to reduce model size, with techniques achieving 9Ã— compression on models like AlexNet without accuracy loss.",
      "chapter_source": "efficient_ai",
      "aliases": [],
      "see_also": [],
      "appears_in": [
        "efficient_ai",
        "ondevice_learning",
        "optimizations"
      ]
    },
    {
      "term": "pseudonymization",
      "definition": "A privacy technique that replaces direct identifiers with artificial identifiers while maintaining the ability to trace records for analysis purposes.",
      "chapter_source": "data_engineering",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "pytorch",
      "definition": "A deep learning framework developed by Facebook's AI Research lab that emphasizes dynamic computational graphs, eager execution, and intuitive Python integration, particularly popular for research and experimentation.",
      "chapter_source": "frameworks",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "quantization",
      "definition": "A model compression technique that reduces the precision of model parameters and activations from higher precision formats (like 32-bit floats) to lower precision (like 8-bit integers), significantly reducing memory usage and computational requirements. Alternative definition: The process of reducing the precision of model weights and activations from floating-point to lower-bit representations to decrease memory usage and accelerate computation on edge devices. Alternative definition: Reducing numerical precision from 32-bit floats to 8-bit integers or lower, achieving 4Ã— memory reduction and speed improvements while maintaining approximately 99% of original accuracy. Alternative definition: The process of reducing numerical precision in neural networks by mapping high-precision weights and activations to lower-bit representations, significantly reducing memory usage and computational requirements. Alternative definition: A model optimization technique that reduces the precision of numerical representations in neural networks, typically from 32-bit to 8-bit or lower, to decrease model size and inference time. Alternative definition: A technique that reduces the precision of numerical representations in neural networks, typically converting from 32-bit floating-point to lower-precision formats like 8-bit integers. Alternative definition: Model optimization technique that reduces the numerical precision of model parameters and computations to decrease memory usage and accelerate inference while attempting to preserve accuracy. Alternative definition: The process of reducing the numerical precision of neural network parameters and activations to decrease memory usage and computational requirements while maintaining acceptable accuracy.",
      "chapter_source": "benchmarking",
      "aliases": [],
      "see_also": [],
      "appears_in": [
        "benchmarking",
        "conclusion",
        "efficient_ai",
        "frameworks",
        "hw_acceleration",
        "ondevice_learning",
        "optimizations",
        "sustainable_ai"
      ]
    },
    {
      "term": "quantization-aware training",
      "definition": "A training approach where quantization effects are simulated during the training process, allowing the model to adapt to reduced precision and typically achieving better accuracy than post-training quantization.",
      "chapter_source": "optimizations",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "quantum machine learning",
      "definition": "The intersection of quantum computing and machine learning, exploring how quantum algorithms and quantum computers can enhance or transform machine learning tasks.",
      "chapter_source": "frontiers",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "queries per second (qps)",
      "definition": "Performance metric that measures how many inference requests a system can process in one second, commonly used to evaluate throughput in production deployments.",
      "chapter_source": "benchmarking",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "query key value",
      "definition": "The three components of attention mechanisms where queries determine what to look for, keys represent what is available, and values contain the actual information to be weighted and combined.",
      "chapter_source": "dnn_architectures",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "real-time processing",
      "definition": "The processing of data as it becomes available, with guaranteed response times that meet strict timing constraints for immediate decision-making.",
      "chapter_source": "ml_systems",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "receptive field",
      "definition": "The region of the input that influences a particular neuron's output, determining the spatial extent of patterns that can be detected by that neuron.",
      "chapter_source": "dnn_architectures",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "rectified linear unit",
      "definition": "An activation function that outputs the input if positive and zero otherwise, widely used in modern neural networks for its computational simplicity and ability to avoid vanishing gradients.",
      "chapter_source": "training",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "recurrent neural network",
      "definition": "A type of neural network designed for sequential data processing, featuring connections that create loops allowing information to persist across time steps.",
      "chapter_source": "dnn_architectures",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "regularization",
      "definition": "Techniques used to prevent overfitting in neural networks by adding constraints or penalties, including methods like dropout, weight decay, and data augmentation. Alternative definition: Methods used in machine learning to prevent overfitting by adding penalty terms to the loss function, constraining model complexity.",
      "chapter_source": "dnn_architectures",
      "aliases": [],
      "see_also": [],
      "appears_in": [
        "dnn_architectures",
        "robust_ai"
      ]
    },
    {
      "term": "relu",
      "definition": "Rectified Linear Unit activation function defined as f(x) = max(0,x) that introduces nonlinearity while maintaining computational efficiency and avoiding vanishing gradient problems.",
      "chapter_source": "dl_primer",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "renewable energy",
      "definition": "Energy collected from renewable resources that are naturally replenished, including solar, wind, hydroelectric, geothermal, and biomass sources.",
      "chapter_source": "sustainable_ai",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "residual connection",
      "definition": "A skip connection that adds the input of a layer to its output, enabling the training of very deep networks by mitigating the vanishing gradient problem.",
      "chapter_source": "dnn_architectures",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "resnet",
      "definition": "Residual Network, a deep convolutional architecture that introduced skip connections, enabling the training of networks with hundreds of layers and achieving breakthrough performance. Alternative definition: Residual Network architecture enabling training of very deep networks through skip connections, achieving the first superhuman performance on ImageNet with 3.6% error rate. Alternative definition: Residual Networks architecture introduced by Microsoft that solved the vanishing gradient problem using skip connections, enabling very deep neural networks with 152+ layers.",
      "chapter_source": "benchmarking",
      "aliases": [],
      "see_also": [],
      "appears_in": [
        "benchmarking",
        "dnn_architectures",
        "efficient_ai"
      ]
    },
    {
      "term": "resource paradox",
      "definition": "The challenge in social impact applications where areas with the greatest needs often lack the basic infrastructure required for traditional technology deployments, requiring innovative engineering solutions.",
      "chapter_source": "ai_for_good",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "resource-constrained environments",
      "definition": "Deployment contexts with limited computational power, network bandwidth, or power availability, typically requiring specialized system design and optimization techniques.",
      "chapter_source": "ai_for_good",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "responsible ai",
      "definition": "The practice of developing and deploying AI systems in ways that are ethical, fair, transparent, and beneficial to society while minimizing potential harms and biases. Alternative definition: The development and deployment of machine learning systems that explicitly uphold ethical principles, minimize harm, and promote socially beneficial outcomes.",
      "chapter_source": "frontiers",
      "aliases": [],
      "see_also": [],
      "appears_in": [
        "frontiers",
        "responsible_ai"
      ]
    },
    {
      "term": "retinal fundus photographs",
      "definition": "Medical images of the interior surface of the eye, including the retina, optic disc, and blood vessels, commonly used for diagnosing eye diseases and training medical AI systems.",
      "chapter_source": "workflow",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "retrieval_augmented_generation",
      "definition": "A technique that combines generative models with information retrieval systems to ground generated content in factual knowledge from external sources.",
      "chapter_source": "generative_ai",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "reverse-mode differentiation",
      "definition": "An automatic differentiation technique that computes gradients by traversing the computational graph in reverse order, highly efficient for functions with many inputs and few outputs, making it ideal for neural network training.",
      "chapter_source": "frameworks",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "reward hacking",
      "definition": "The phenomenon where AI systems exploit unintended aspects of reward functions to maximize scores while violating the intended objectives.",
      "chapter_source": "responsible_ai",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "rmsprop",
      "definition": "An adaptive learning rate optimization algorithm that maintains a moving average of squared gradients to automatically adjust learning rates for each parameter during training.",
      "chapter_source": "training",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "robust ai",
      "definition": "The ability of artificial intelligence systems to maintain performance and reliability despite internal errors, external perturbations, and environmental changes.",
      "chapter_source": "robust_ai",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "robustness",
      "definition": "A model's ability to maintain stable and consistent performance under input variations, environmental changes, or adversarial conditions.",
      "chapter_source": "responsible_ai",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "rollback",
      "definition": "Process of reverting to a previous stable version of a model or system when issues are detected in production, ensuring service continuity.",
      "chapter_source": "ops",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "sampling_strategy",
      "definition": "Methods for selecting outputs from generative models, such as temperature scaling, top-k sampling, or nucleus sampling, affecting the diversity and quality of generated content.",
      "chapter_source": "generative_ai",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "scalability",
      "definition": "The ability of machine learning systems to handle increasing amounts of data, users, or computational demands without significant degradation in performance or user experience. Alternative definition: System's ability to handle increased workloads by adding computational resources, measuring how performance improves when additional hardware is deployed.",
      "chapter_source": "benchmarking",
      "aliases": [],
      "see_also": [],
      "appears_in": [
        "benchmarking",
        "workflow"
      ]
    },
    {
      "term": "scaling_laws",
      "definition": "Empirical relationships that quantify the correlation between model performance and training resources, following predictable power-law relationships with model size, dataset size, and compute budget.",
      "chapter_source": "efficient_ai",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "scan chains",
      "definition": "Dedicated test paths in processors that provide access to internal registers and logic for comprehensive hardware testing and fault detection.",
      "chapter_source": "robust_ai",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "schema",
      "definition": "The structure and format definition of data that specifies data types, field names, and relationships, essential for data validation and processing consistency.",
      "chapter_source": "data_engineering",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "schema-on-read",
      "definition": "An approach used in data lakes where data structure is defined and enforced at the time of reading rather than when storing, providing flexibility for diverse data types.",
      "chapter_source": "data_engineering",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "scope 1 emissions",
      "definition": "Direct greenhouse gas emissions from sources owned or controlled by an organization, such as on-site fuel combustion and company vehicles.",
      "chapter_source": "sustainable_ai",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "scope 2 emissions",
      "definition": "Indirect greenhouse gas emissions from the generation of purchased electricity, steam, heating, or cooling consumed by an organization.",
      "chapter_source": "sustainable_ai",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "scope 3 emissions",
      "definition": "All other indirect greenhouse gas emissions that occur in an organization's value chain, including manufacturing, transportation, and end-of-life disposal.",
      "chapter_source": "sustainable_ai",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "secure aggregation",
      "definition": "A cryptographic protocol that enables federated learning servers to compute aggregate model updates without accessing individual client contributions, enhancing privacy protection. Alternative definition: A cryptographic protocol used in federated learning to combine model updates from multiple parties without revealing individual contributions to the central server.",
      "chapter_source": "ondevice_learning",
      "aliases": [],
      "see_also": [],
      "appears_in": [
        "ondevice_learning",
        "privacy_security"
      ]
    },
    {
      "term": "secure multi-party computation",
      "definition": "A cryptographic method that allows multiple parties to jointly compute a function over their private inputs without revealing those inputs to each other.",
      "chapter_source": "privacy_security",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "segmentation maps",
      "definition": "Detailed annotations that classify objects at the pixel level, providing the most granular labeling information but requiring significantly more storage and processing resources.",
      "chapter_source": "data_engineering",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "self-attention",
      "definition": "An attention mechanism where queries, keys, and values all come from the same sequence, allowing each position to attend to all positions including itself.",
      "chapter_source": "dnn_architectures",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "self-supervised learning",
      "definition": "A machine learning paradigm where models learn representations from unlabeled data by predicting parts of the input from other parts, reducing dependence on manually labeled datasets.",
      "chapter_source": "frontiers",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "self_attention",
      "definition": "A mechanism allowing each position in a sequence to attend to all positions in the same sequence, enabling models to capture long-range dependencies.",
      "chapter_source": "generative_ai",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "self_supervised_learning",
      "definition": "Training method where models create their own labels from input data structure, enabling learning from billions of unlabeled examples and revolutionizing NLP and computer vision.",
      "chapter_source": "efficient_ai",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "semi-supervised learning",
      "definition": "A machine learning approach that uses both labeled and unlabeled data for training, leveraging structural assumptions to improve model performance with limited labels.",
      "chapter_source": "data_engineering",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "sequence_to_sequence",
      "definition": "A neural network architecture that transforms input sequences into output sequences, commonly used for tasks like machine translation and text summarization.",
      "chapter_source": "generative_ai",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "sequential neural networks",
      "definition": "Neural network architectures designed to process data that occurs in sequences over time, maintaining a form of memory of previous inputs to inform current decisions, essential for tasks like predicting pedestrian movement patterns.",
      "chapter_source": "introduction",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "serverless",
      "definition": "Cloud computing model where infrastructure is automatically managed by the provider, allowing code execution without server management concerns.",
      "chapter_source": "ops",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "service level agreement (sla)",
      "definition": "Formal contract specifying minimum performance standards and uptime guarantees for production services, with penalties for non-compliance.",
      "chapter_source": "ops",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "service level objective (slo)",
      "definition": "Internal targets for service reliability and performance metrics such as latency, error rates, and availability that guide operational decisions.",
      "chapter_source": "ops",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "shadow deployment",
      "definition": "Testing strategy where new model versions process live traffic in parallel with production models without affecting user-facing results, enabling safe validation.",
      "chapter_source": "ops",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "shallow learning",
      "definition": "Machine learning approaches that use algorithms with limited complexity, such as support vector machines and decision trees, which require carefully engineered features but cannot automatically discover hierarchical representations like deep learning methods.",
      "chapter_source": "introduction",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "side-channel attack",
      "definition": "An attack that exploits information leaked through the physical implementation of computing systems, such as power consumption, electromagnetic emissions, or timing variations.",
      "chapter_source": "privacy_security",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "sigmoid",
      "definition": "An activation function that maps inputs to the range (0,1) through an S-shaped curve, commonly used in output layers for binary classification but prone to vanishing gradient problems. Alternative definition: An activation function that maps input values to a range between 0 and 1, historically popular but prone to vanishing gradient problems in deep networks.",
      "chapter_source": "dl_primer",
      "aliases": [],
      "see_also": [],
      "appears_in": [
        "dl_primer",
        "training"
      ]
    },
    {
      "term": "silent data corruption (sdc)",
      "definition": "Undetected errors during computation or data transfer that propagate through system layers without triggering alerts, potentially compromising results.",
      "chapter_source": "robust_ai",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "simd (single instruction, multiple data)",
      "definition": "A parallel computing architecture that applies the same operation to multiple data elements simultaneously, effective for regular data-parallel computations.",
      "chapter_source": "hw_acceleration",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "simt (single instruction, multiple thread)",
      "definition": "An extension of SIMD that enables parallel execution across multiple independent threads, each maintaining its own state and program counter.",
      "chapter_source": "hw_acceleration",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "singular value decomposition",
      "definition": "A matrix factorization technique that decomposes a matrix into the product of three matrices, commonly used in low-rank approximations to compress neural network layers by retaining only the most significant singular values.",
      "chapter_source": "optimizations",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "skip connection",
      "definition": "A direct connection that bypasses one or more layers, allowing gradients to flow more easily through deep networks and enabling better training of very deep architectures.",
      "chapter_source": "dnn_architectures",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "smallholder farmers",
      "definition": "Farmers operating on plots smaller than 2 hectares who produce a significant portion of global food supply but often lack access to modern agricultural technology and credit.",
      "chapter_source": "ai_for_good",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "softmax",
      "definition": "An activation function that converts a vector of real numbers into a probability distribution, commonly used in the output layer for multi-class classification tasks. Alternative definition: An activation function that converts raw scores into a probability distribution where outputs sum to 1, essential for multi-class classification tasks.",
      "chapter_source": "dnn_architectures",
      "aliases": [],
      "see_also": [],
      "appears_in": [
        "dnn_architectures",
        "training"
      ]
    },
    {
      "term": "software fault",
      "definition": "Unintended behavior in software systems resulting from defects, bugs, or design oversights that can impair performance or compromise security.",
      "chapter_source": "robust_ai",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "sparse training",
      "definition": "A training approach that maintains sparsity in neural network weights throughout the training process, reducing computational requirements and memory usage.",
      "chapter_source": "sustainable_ai",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "sparse updates",
      "definition": "A training strategy that selectively updates only a subset of model parameters based on their importance or contribution to performance, reducing computational and memory overhead.",
      "chapter_source": "ondevice_learning",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "sparsity",
      "definition": "The property of neural networks where many weights are zero or near-zero, which can be exploited for computational efficiency through specialized hardware support and algorithms designed for sparse operations.",
      "chapter_source": "optimizations",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "spec cpu",
      "definition": "Standardized benchmark suite developed by the System Performance Evaluation Cooperative that measures processor performance using real-world applications rather than synthetic tests.",
      "chapter_source": "benchmarking",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "spec power",
      "definition": "Benchmark methodology that measures server energy efficiency across varying workload levels, enabling direct comparisons of power-performance trade-offs in computing systems.",
      "chapter_source": "benchmarking",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "specification gaming",
      "definition": "When AI systems find unexpected ways to achieve high rewards that technically satisfy the objective function but violate the intended purpose.",
      "chapter_source": "responsible_ai",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "speculative execution",
      "definition": "A performance optimization in processors that executes instructions before confirming they are needed, which can inadvertently expose sensitive data through microarchitectural side channels.",
      "chapter_source": "privacy_security",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "squeezenet",
      "definition": "Compact CNN architecture achieving AlexNet-level accuracy with 50Ã— fewer parameters, demonstrating that clever architecture design can dramatically reduce model size without sacrificing performance.",
      "chapter_source": "efficient_ai",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "static graph",
      "definition": "A computational graph that is defined completely before execution begins, enabling comprehensive optimization and efficient deployment but requiring all operations to be specified upfront, limiting runtime flexibility.",
      "chapter_source": "frameworks",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "static quantization",
      "definition": "A quantization approach where quantization parameters are determined once during calibration and remain fixed during inference, providing computational efficiency but less adaptability than dynamic approaches.",
      "chapter_source": "optimizations",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "statistical learning",
      "definition": "The era of machine learning that emerged in the 1990s, shifting focus from rule-based symbolic AI to algorithms that could learn patterns from data, laying the groundwork for modern data-driven approaches to artificial intelligence.",
      "chapter_source": "introduction",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "stochastic computing",
      "definition": "Computing techniques that use random bits and probabilistic operations to perform arithmetic, potentially offering better fault tolerance than traditional methods.",
      "chapter_source": "robust_ai",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "stochastic gradient descent",
      "definition": "A variant of gradient descent that estimates gradients using individual training examples or small batches rather than the entire dataset, reducing memory requirements and enabling online learning.",
      "chapter_source": "efficient_ai",
      "aliases": [],
      "see_also": [],
      "appears_in": [
        "efficient_ai",
        "training"
      ]
    },
    {
      "term": "stream ingestion",
      "definition": "A data processing pattern that handles data in real-time as it arrives, essential for applications requiring immediate processing and low-latency responses.",
      "chapter_source": "data_engineering",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "stride",
      "definition": "The step size by which a convolutional filter moves across the input, controlling the spatial dimensions of the output and the degree of overlap between filter applications.",
      "chapter_source": "dnn_architectures",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "structured pruning",
      "definition": "A pruning approach that removes entire computational units such as neurons, channels, or layers, producing smaller dense models that are more hardware-friendly than the sparse matrices created by unstructured pruning.",
      "chapter_source": "optimizations",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "stuck-at fault",
      "definition": "A permanent hardware fault where a signal line becomes fixed at a logical 0 or 1 regardless of input, causing incorrect computations.",
      "chapter_source": "robust_ai",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "student system",
      "definition": "One of the first AI programs from 1964 by Daniel Bobrow that demonstrated natural language understanding by converting English algebra word problems into mathematical equations, marking an important milestone in symbolic AI.",
      "chapter_source": "introduction",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "student-teacher learning",
      "definition": "The core mechanism of knowledge distillation where a smaller student network learns from a larger teacher network, typically using soft targets that provide more information than hard classification labels.",
      "chapter_source": "optimizations",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "supervised learning",
      "definition": "A machine learning approach where models learn from labeled training examples to make predictions on new, unlabeled data.",
      "chapter_source": "dl_primer",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "supply chain attack",
      "definition": "An attack that compromises hardware or software components during the manufacturing, distribution, or integration process, potentially affecting multiple downstream systems.",
      "chapter_source": "privacy_security",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "support_vector_machines",
      "definition": "Machine learning algorithm using the \"kernel trick\" to find optimal decision boundaries, dominating competitions before deep learning until neural networks gained prominence around 2010.",
      "chapter_source": "efficient_ai",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "sustainable ai",
      "definition": "The practice of developing and deploying artificial intelligence systems that minimize environmental impact while maintaining effectiveness and accessibility.",
      "chapter_source": "sustainable_ai",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "sustainable development goals",
      "definition": "A collection of 17 global goals adopted by the United Nations to address pressing social, economic, and environmental challenges by 2030, providing a framework for AI applications in social good.",
      "chapter_source": "ai_for_good",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "swarm intelligence",
      "definition": "Collective intelligence emerging from decentralized, self-organized systems, often inspired by biological swarms and applied to distributed ML systems and robotics.",
      "chapter_source": "frontiers",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "symbolic ai",
      "definition": "The early approach to artificial intelligence that attempted to implement intelligence through symbol manipulation and rule-based systems, exemplified by programs like STUDENT that could only handle inputs matching their pre-programmed patterns.",
      "chapter_source": "introduction",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "symbolic programming",
      "definition": "A programming paradigm where computations are represented as abstract symbols and expressions that are constructed first and executed later, allowing for comprehensive optimization but requiring explicit execution phases.",
      "chapter_source": "frameworks",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "synthetic benchmark",
      "definition": "Artificial test program designed to measure specific aspects of system performance, as opposed to benchmarks based on real-world applications and workloads.",
      "chapter_source": "benchmarking",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "synthetic data",
      "definition": "Artificially generated data created using algorithms, simulations, or generative models to supplement real-world datasets, addressing limitations in data availability or privacy concerns.",
      "chapter_source": "data_engineering",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "synthetic data generation",
      "definition": "The creation of artificial datasets that approximate the statistical properties of real data while reducing privacy risks and avoiding direct exposure of sensitive information.",
      "chapter_source": "privacy_security",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "system on chip",
      "definition": "An integrated circuit that incorporates most or all components of a computer or electronic system, including CPU, GPU, memory, and specialized processors on a single chip.",
      "chapter_source": "ml_systems",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "system-on-chip (soc)",
      "definition": "Integrated circuit that contains most or all components of a computer system, commonly used in mobile devices and embedded systems for space and power efficiency.",
      "chapter_source": "benchmarking",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "system_efficiency",
      "definition": "Optimization of machine learning systems across algorithmic, compute, and data efficiency dimensions to minimize computational, memory, and energy demands while maintaining performance.",
      "chapter_source": "efficient_ai",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "systems integration",
      "definition": "The process of combining various components and subsystems into a unified, functional system that operates efficiently and reliably as a whole.",
      "chapter_source": "conclusion",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "systems thinking",
      "definition": "A holistic approach to analysis that views machine learning systems as interconnected wholes rather than individual components, emphasizing relationships and dependencies between different lifecycle stages.",
      "chapter_source": "workflow",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "systolic array",
      "definition": "A specialized hardware architecture that efficiently performs matrix operations by streaming data through a grid of processing elements, minimized data movement and energy consumption. Alternative definition: A specialized computing architecture where data flows rhythmically through a grid of processing elements, optimized for matrix operations in neural networks.",
      "chapter_source": "hw_acceleration",
      "aliases": [],
      "see_also": [],
      "appears_in": [
        "hw_acceleration",
        "training"
      ]
    },
    {
      "term": "tail latency",
      "definition": "Worst-case response times in a system, typically measured as 95th or 99th percentile latency, important for understanding system reliability under peak load conditions.",
      "chapter_source": "benchmarking",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "tanh",
      "definition": "An activation function that maps inputs to the range (-1,1) with zero-centered output, helping to stabilize gradient-based optimization compared to sigmoid functions.",
      "chapter_source": "training",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "targeted attack",
      "definition": "A type of data poisoning attack that aims to cause misclassification of specific inputs or classes while leaving the model's general performance largely intact.",
      "chapter_source": "privacy_security",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "technical debt",
      "definition": "Long-term maintenance cost accumulated from expedient design decisions during development, particularly problematic in ML systems due to data dependencies and model complexity.",
      "chapter_source": "ops",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "telemetry",
      "definition": "Automated collection and transmission of performance data and metrics from distributed systems, enabling remote monitoring and analysis.",
      "chapter_source": "ops",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "tensor",
      "definition": "A multidimensional array that generalizes scalars, vectors, and matrices to higher dimensions, serving as the fundamental data structure in machine learning frameworks for representing and manipulating numerical data. Alternative definition: A multi-dimensional array used to represent data and parameters in neural networks, generalizing scalars, vectors, and matrices to higher dimensions. Alternative definition: A multi-dimensional array used to represent data in neural networks, generalizing scalars (0D), vectors (1D), and matrices (2D) to higher dimensions. Alternative definition: Multi-dimensional array that serves as the fundamental data structure in machine learning, representing scalars, vectors, matrices, and higher-dimensional data.",
      "chapter_source": "benchmarking",
      "aliases": [],
      "see_also": [],
      "appears_in": [
        "benchmarking",
        "dl_primer",
        "frameworks",
        "hw_acceleration"
      ]
    },
    {
      "term": "tensor decomposition",
      "definition": "The extension of matrix factorization to higher-order tensors, used to compress neural network layers by representing weight tensors as combinations of smaller tensors with fewer parameters.",
      "chapter_source": "optimizations",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "tensor parallelism",
      "definition": "A distributed computing technique that partitions individual tensors and operations across multiple devices, reducing per-device memory requirements while maintaining computational efficiency through coordinated parallel execution.",
      "chapter_source": "frameworks",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "tensor processing unit",
      "definition": "Google's custom application-specific integrated circuit designed specifically for machine learning workloads, optimized for matrix operations and featuring systolic array architecture. Alternative definition: Google's custom application-specific integrated circuit designed specifically for neural network machine learning, optimized for TensorFlow operations.",
      "chapter_source": "ml_systems",
      "aliases": [],
      "see_also": [],
      "appears_in": [
        "ml_systems",
        "training"
      ]
    },
    {
      "term": "tensor processing unit (tpu)",
      "definition": "Google's custom ASIC designed specifically for neural network workloads, achieving significant performance and energy efficiency improvements over general-purpose processors. Alternative definition: Google's custom-designed accelerator optimized specifically for tensor operations and neural network workloads, featuring systolic array architecture.",
      "chapter_source": "benchmarking",
      "aliases": [],
      "see_also": [],
      "appears_in": [
        "benchmarking",
        "hw_acceleration"
      ]
    },
    {
      "term": "tensorflow",
      "definition": "A comprehensive machine learning framework developed by Google that provides tools for the entire ML pipeline from research to production, featuring both eager execution and graph-based computation with extensive ecosystem support.",
      "chapter_source": "frameworks",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "tensorrt",
      "definition": "NVIDIA's inference optimization library that applies techniques like operator fusion and precision reduction to accelerate deep learning inference on GPU hardware.",
      "chapter_source": "benchmarking",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "ternarization",
      "definition": "An extreme quantization technique that constrains weights to three values (typically -1, 0, +1), providing significant compression while maintaining more representational capacity than binary quantization.",
      "chapter_source": "optimizations",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "test_time_compute",
      "definition": "Dynamic resource allocation during inference that adjusts computational effort based on task complexity or importance, enabling flexible performance-accuracy trade-offs.",
      "chapter_source": "efficient_ai",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "text_to_image",
      "definition": "Generative models capable of creating images based on natural language descriptions, combining computer vision and natural language processing.",
      "chapter_source": "generative_ai",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "thermal stress",
      "definition": "Hardware degradation caused by repeated cycling through high and low temperatures, leading to material fatigue and potential failures.",
      "chapter_source": "robust_ai",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "throughput",
      "definition": "The rate at which a system can process data or complete operations, typically measured in operations per second and crucial for training large models. Alternative definition: Number of operations, tasks, or data items processed per unit time, measuring the productive capacity of a system under sustained load.",
      "chapter_source": "benchmarking",
      "aliases": [],
      "see_also": [],
      "appears_in": [
        "benchmarking",
        "hw_acceleration"
      ]
    },
    {
      "term": "time-to-accuracy",
      "definition": "Duration required for a machine learning model to reach a predefined accuracy threshold during training, serving as a key metric for training efficiency evaluation.",
      "chapter_source": "benchmarking",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "tiny machine learning",
      "definition": "The execution of machine learning models on ultra-constrained devices such as microcontrollers and sensors, operating in the milliwatt to sub-watt power range.",
      "chapter_source": "ml_systems",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "tiny ml",
      "definition": "Machine learning systems designed to run on extremely resource-constrained devices like microcontrollers, typically with models under 1 MB and power consumption under 150 mW.",
      "chapter_source": "ai_for_good",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "tinyml",
      "definition": "Machine learning on microcontrollers and edge devices with less than 1KB-1MB memory and less than 1mW power consumption, enabling AI in IoT devices where traditional deployment is impossible. Alternative definition: A field focused on deploying machine learning models on resource-constrained embedded devices and microcontrollers with severe limitations on memory, power, and computational capacity. Alternative definition: A field focused on deploying machine learning models on microcontrollers and extremely resource-constrained devices with kilobytes of memory and milliwatts of power consumption.",
      "chapter_source": "conclusion",
      "aliases": [],
      "see_also": [],
      "appears_in": [
        "conclusion",
        "efficient_ai",
        "ondevice_learning"
      ]
    },
    {
      "term": "tokenization",
      "definition": "The process of converting text into discrete units (tokens) that can be processed by machine learning models, fundamental for text generation systems.",
      "chapter_source": "generative_ai",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "tokens",
      "definition": "Individual units of text that language models process, typically words or subword pieces, with modern models like GPT-3 trained on hundreds of billions of tokens.",
      "chapter_source": "efficient_ai",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "tops",
      "definition": "Tera Operations Per Second, a measure of computational performance indicating how many trillion operations a system can execute in one second.",
      "chapter_source": "benchmarking",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "tpu",
      "definition": "Tensor Processing Unit, Google's custom Application-Specific Integrated Circuits (ASICs) designed specifically for accelerating tensor operations in machine learning workloads, offering significant performance and energy efficiency improvements over general-purpose processors. Alternative definition: Tensor Processing Unit, a specialized AI accelerator chip developed by Google specifically designed for machine learning workloads, particularly neural network computations. Alternative definition: Tensor Processing Unit, Google's custom silicon designed specifically for machine learning workloads, delivering 275 teraFLOPs with 90% lower energy per operation compared to general-purpose processors.",
      "chapter_source": "efficient_ai",
      "aliases": [],
      "see_also": [],
      "appears_in": [
        "efficient_ai",
        "frameworks",
        "sustainable_ai"
      ]
    },
    {
      "term": "training",
      "definition": "The phase of machine learning where model parameters are learned from data through iterative optimization, typically requiring high numerical precision and computational resources. Alternative definition: The process of adjusting neural network parameters using labeled data and optimization algorithms to minimize prediction errors and improve performance. Alternative definition: The process of teaching a machine learning algorithm to make accurate predictions by feeding it large amounts of labeled data and allowing it to learn patterns. Alternative definition: Process of optimizing machine learning model parameters using training data to minimize prediction errors and improve performance on the target task. Alternative definition: The process of teaching a machine learning model to make predictions by showing it examples and adjusting its parameters based on performance feedback.",
      "chapter_source": "benchmarking",
      "aliases": [],
      "see_also": [],
      "appears_in": [
        "benchmarking",
        "dl_primer",
        "hw_acceleration",
        "ml_systems",
        "sustainable_ai"
      ]
    },
    {
      "term": "training-serving skew",
      "definition": "Inconsistency between feature preprocessing logic used during model training versus serving, leading to degraded production performance.",
      "chapter_source": "ops",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "transfer learning",
      "definition": "A machine learning technique that leverages knowledge gained from pre-trained models on related tasks, allowing faster training and better performance on new tasks with limited data by reusing learned features and representations.",
      "chapter_source": "conclusion",
      "aliases": [],
      "see_also": [],
      "appears_in": [
        "conclusion",
        "data_engineering",
        "efficient_ai",
        "frameworks",
        "frontiers",
        "ondevice_learning",
        "robust_ai",
        "sustainable_ai",
        "workflow"
      ]
    },
    {
      "term": "transformer",
      "definition": "Neural network architecture introduced by Vaswani et al. in 2017 that revolutionized natural language processing through attention mechanisms, forming the foundation of models like GPT, BERT, and T5. Alternative definition: Neural network architecture that uses self-attention mechanisms to process sequential data, forming the foundation for modern language models like BERT and GPT. Alternative definition: A neural network architecture that uses self-attention mechanisms to process sequential data, forming the foundation for many modern language models. Alternative definition: A neural network architecture based entirely on attention mechanisms, serving as the foundation for most modern large language models and many generative AI systems. Alternative definition: A neural network architecture based entirely on attention mechanisms, eliminating recurrence and convolution while achieving state-of-the-art performance across many domains.",
      "chapter_source": "benchmarking",
      "aliases": [],
      "see_also": [],
      "appears_in": [
        "benchmarking",
        "dnn_architectures",
        "efficient_ai",
        "generative_ai",
        "sustainable_ai"
      ]
    },
    {
      "term": "transformer architecture",
      "definition": "A neural network architecture based on attention mechanisms that has revolutionized natural language processing and is increasingly applied to other domains like computer vision.",
      "chapter_source": "frontiers",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "transient faults",
      "definition": "Temporary hardware faults that do not persist or cause permanent damage but can lead to incorrect computations if not handled properly.",
      "chapter_source": "robust_ai",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "translation invariance",
      "definition": "The property of convolutional networks to recognize patterns regardless of their position in the input, achieved through weight sharing and pooling operations.",
      "chapter_source": "dnn_architectures",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "transparency",
      "definition": "Openness about how AI systems are built, trained, validated, and deployed, including disclosure of data sources, design assumptions, and limitations.",
      "chapter_source": "responsible_ai",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "triple modular redundancy (tmr)",
      "definition": "A fault-tolerance technique where three instances of a computation are performed, with majority voting determining the correct result.",
      "chapter_source": "robust_ai",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "trusted execution environment",
      "definition": "A secure area within a processor that provides hardware-based protection for code and data, ensuring confidentiality and integrity even from privileged system software.",
      "chapter_source": "privacy_security",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "tucker decomposition",
      "definition": "A tensor decomposition method that generalizes singular value decomposition to higher-order tensors using a core tensor and factor matrices, commonly used for compressing convolutional neural network layers.",
      "chapter_source": "optimizations",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "tv white spaces",
      "definition": "Unused broadcasting frequencies that can be repurposed for internet connectivity, as employed by systems like FarmBeats to extend network access to remote agricultural sensors and IoT devices.",
      "chapter_source": "introduction",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "uci_machine_learning_repository",
      "definition": "Established in 1987 by the University of California Irvine, one of the most widely-used resources for machine learning datasets containing over 600 datasets cited in thousands of research papers.",
      "chapter_source": "efficient_ai",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "universal approximation theorem",
      "definition": "A theoretical result proving that neural networks with sufficient width and non-linear activation functions can approximate any continuous function on a compact domain.",
      "chapter_source": "dnn_architectures",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "unstructured pruning",
      "definition": "A pruning approach that removes individual weights while preserving the overall network architecture, creating sparse weight matrices that require specialized hardware support to realize computational benefits.",
      "chapter_source": "optimizations",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "validation issues",
      "definition": "Problems identified during model testing that indicate poor performance, overfitting, data quality problems, or other issues that must be resolved before deployment.",
      "chapter_source": "workflow",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "value alignment",
      "definition": "The principle that AI systems should pursue goals consistent with human intent and ethical norms, addressing the challenge of encoding human values in machine objectives.",
      "chapter_source": "responsible_ai",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "value-sensitive design",
      "definition": "A methodology for incorporating human values into technology design through systematic stakeholder engagement and ethical consideration of system impacts.",
      "chapter_source": "responsible_ai",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "vanishing gradient",
      "definition": "A problem in deep neural networks where gradients become exponentially smaller as they propagate backward through layers, making it difficult for early layers to learn effectively.",
      "chapter_source": "dl_primer",
      "aliases": [],
      "see_also": [],
      "appears_in": [
        "dl_primer",
        "dnn_architectures"
      ]
    },
    {
      "term": "vanishing gradient problem",
      "definition": "A challenge in training deep neural networks where gradients become exponentially smaller as they propagate backward through layers, making it difficult to train early layers effectively.",
      "chapter_source": "training",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "variational_autoencoder",
      "definition": "A type of autoencoder that learns a probabilistic latent representation, enabling both data compression and generation of new samples from the learned distribution.",
      "chapter_source": "generative_ai",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "vector operations",
      "definition": "Computational operations that process multiple data elements simultaneously, enabling efficient parallel execution of element-wise transformations in neural networks.",
      "chapter_source": "hw_acceleration",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "vector-borne diseases",
      "definition": "Diseases transmitted by insects or other vectors, such as malaria carried by mosquitoes, which can be monitored and controlled using machine learning-powered detection systems.",
      "chapter_source": "ai_for_good",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "versioning",
      "definition": "The practice of tracking changes to datasets, models, and pipelines over time, enabling reproducibility, rollback capabilities, and audit trails in ML systems.",
      "chapter_source": "data_engineering",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "vision-language models",
      "definition": "AI systems that can understand and reason about both visual and textual information simultaneously, enabling tasks like image captioning, visual question answering, and multimodal understanding.",
      "chapter_source": "frontiers",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "von neumann bottleneck",
      "definition": "The performance limitation caused by the shared bus between processor and memory in traditional computer architectures, where data movement becomes more expensive than computation.",
      "chapter_source": "hw_acceleration",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "watchdog timer",
      "definition": "A hardware component that monitors system execution and triggers recovery actions if the system becomes unresponsive or stuck.",
      "chapter_source": "robust_ai",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "water usage effectiveness",
      "definition": "A metric that measures the efficiency of water use in data centers, calculated as the ratio of total water consumed to IT equipment energy consumption.",
      "chapter_source": "sustainable_ai",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "waymo",
      "definition": "A subsidiary of Alphabet Inc. that represents one of the most ambitious applications of machine learning systems in autonomous vehicle technology, demonstrating how ML systems can span from embedded systems to cloud infrastructure in safety-critical environments.",
      "chapter_source": "introduction",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "weak supervision",
      "definition": "An approach that uses lower-quality labels obtained more efficiently through heuristics, distant supervision, or programmatic methods rather than manual expert annotation.",
      "chapter_source": "data_engineering",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "web scraping",
      "definition": "An automated technique for extracting data from websites to build custom datasets, requiring careful consideration of legal, ethical, and technical constraints.",
      "chapter_source": "data_engineering",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "weight",
      "definition": "A learnable parameter that determines the strength of connection between neurons in different layers, adjusted during training to minimize the loss function.",
      "chapter_source": "dl_primer",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "weight freezing",
      "definition": "A technique that fixes most model parameters during training while allowing only specific layers or components to be updated, reducing computational requirements for on-device adaptation.",
      "chapter_source": "ondevice_learning",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "weight matrix",
      "definition": "An organized collection of weights connecting one layer to another in a neural network, enabling efficient computation through matrix operations.",
      "chapter_source": "dl_primer",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "weight sharing",
      "definition": "The practice of using the same parameters across different spatial locations, as in convolutional networks, reducing the number of parameters while maintaining pattern detection capabilities.",
      "chapter_source": "dnn_architectures",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "whetstone",
      "definition": "Early benchmark introduced in 1964 that measured floating-point arithmetic performance in KIPS (thousands of instructions per second), becoming the first widely-adopted standardized performance test.",
      "chapter_source": "benchmarking",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "white-box attack",
      "definition": "An adversarial attack where the attacker has complete knowledge of the model's architecture, parameters, training data, and internal workings, enabling highly effective attack strategies.",
      "chapter_source": "privacy_security",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "xla",
      "definition": "Accelerated Linear Algebra, a domain-specific compiler for linear algebra operations that optimizes TensorFlow and JAX computations by generating efficient code for various hardware platforms including CPUs, GPUs, and TPUs.",
      "chapter_source": "frameworks",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "zero-day vulnerability",
      "definition": "A previously unknown security flaw in software or hardware that can be exploited by attackers before developers have had a chance to create and distribute a patch.",
      "chapter_source": "privacy_security",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "zero-shot learning",
      "definition": "The ability of machine learning models to perform tasks or classify objects they have never seen during training, often achieved through sophisticated representation learning or large-scale pre-training.",
      "chapter_source": "frontiers",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "zero_shot_learning",
      "definition": "The ability of generative models to perform tasks they were not explicitly trained for, using only natural language instructions or examples.",
      "chapter_source": "generative_ai",
      "aliases": [],
      "see_also": []
    }
  ]
}