---
bibliography: introduction.bib
quiz: introduction_quizzes.json
concepts: introduction_concepts.yml
glossary: introduction_glossary.json
crossrefs: introduction_xrefs.json
---

# Introduction {#sec-introduction}
::: {layout-narrow}
::: {.column-margin}
_DALL·E 3 Prompt: A detailed, rectangular, flat 2D illustration depicting a roadmap of a book's chapters on machine learning systems, set on a crisp, clean white background. The image features a winding road traveling through various symbolic landmarks. Each landmark represents a chapter topic: Introduction, ML Systems, Deep Learning, AI Workflow, Data Engineering, AI Frameworks, AI Training, Efficient AI, Model Optimizations, AI Acceleration, Benchmarking AI, On-Device Learning, Embedded AIOps, Security & Privacy, Responsible AI, Sustainable AI, AI for Good, Robust AI, Generative AI. The style is clean, modern, and flat, suitable for a technical book, with each landmark clearly labeled with its chapter title._
:::

\noindent
![](images/png/cover_introduction.png)

:::

## Purpose {.unnumbered}

_What does it mean to engineer machine learning systems, not just design models?_

The transformation from research prototype to production system defines a critical engineering discipline. Machine Learning Systems Engineering bridges the gap between experimental models that work in controlled conditions and reliable systems that serve millions of users. This discipline encompasses the complete development lifecycle: ensuring data quality, managing system versions, building experimentation frameworks, monitoring performance, and creating resilient architectures. Real-world deployment brings distinct challenges: tracking data source reliability, maintaining privacy compliance, optimizing performance under varying conditions, scaling traffic loads, recovering from failures, and adapting to evolving requirements. These engineering principles become essential as machine learning transitions from laboratory experiments to the backbone of modern technological infrastructure.

::: {.callout-tip title="Learning Objectives"}

- Define machine learning systems engineering and distinguish it from traditional model development

- Analyze the pervasive role of AI across different sectors and scales of human activity

- Explain the historical evolution of AI from symbolic systems to modern deep learning approaches

- Compare fundamental differences between AI research and ML systems implementation practices

- Identify key challenges in transitioning from research prototypes to production ML systems

- Evaluate real-world ML applications through data, algorithmic, and infrastructure considerations

- Apply the five-pillar framework to categorize ML system components and their interdependencies

:::

## AI Pervasiveness {#sec-introduction-ai-pervasiveness-8891}

Artificial Intelligence (AI) has emerged as one of the most transformative forces in human history. From the moment we wake up to when we go to sleep, AI systems invisibly shape our world. They manage traffic flows in our cities, optimize power distribution across electrical grids, and enable billions of wireless devices to communicate seamlessly through IoT networks. In hospitals, AI analyzes medical images and helps doctors diagnose diseases. In research laboratories, it accelerates scientific discovery by simulating molecular interactions and processing vast datasets from particle accelerators. In space exploration, it helps rovers traverse distant planets and telescopes detect new celestial phenomena.

Throughout history, certain technologies have fundamentally transformed human civilization, defining their eras. The 18th and 19th centuries were shaped by the Industrial Revolution, where steam power and mechanization transformed how humans could use physical energy. The 20th century was defined by the Digital Revolution, where the computer and internet transformed how we process and share information. Now, the 21st century appears to be the era of Artificial Intelligence, a shift noted by leading thinkers in technological evolution [@brynjolfsson2014second; @domingos2015master].

The vision driving AI development encompasses broader goals than current practical applications. The goal is creating systems that work alongside humanity, enhancing problem-solving capabilities and accelerating scientific progress. AI systems may help understand consciousness, decode biological system complexities, or address global challenges like climate change, disease, and sustainable energy production. This is not just about automation or efficiency, it's about expanding the boundaries of human knowledge and capability.

The impact of this revolution operates at multiple scales, each with profound implications. At the individual level, AI personalizes our experiences and augments our daily decision-making capabilities. At the organizational level, it transforms how businesses operate and how research institutions make discoveries. At the societal level, it reshapes everything from transportation systems to healthcare delivery. At the global level, it offers new approaches to addressing humanity's greatest challenges, from climate change to drug discovery.

This transformation proceeds rapidly. While the Industrial Revolution unfolded over centuries and the Digital Revolution over decades, AI capabilities advance at an accelerated rate. Technologies that seemed impossible years ago, systems understanding human speech, generating content, or making complex decisions, are now commonplace. This acceleration indicates we are beginning to understand AI's profound impact on society. We stand at a historic inflection point. The Industrial Revolution required mastering mechanical engineering to control steam and machinery. The Digital Revolution demanded electrical and computer engineering expertise to build the internet age. The AI Revolution presents a new engineering challenge. Building systems that learn, reason, and potentially achieve superhuman capabilities[^fn-superhuman-capabilities] in specific domains requires new expertise.

[^fn-superhuman-capabilities]: **Superhuman AI Capabilities**: AI systems already exceed human performance in specific domains. AlphaGo defeated the world champion in Go (a game with more possible positions than atoms in the observable universe), protein folding prediction models like AlphaFold achieved accuracy that would take human scientists decades to match, and modern language models can process and synthesize information from millions of documents in seconds.

## AI and ML Basics {#sec-introduction-ai-ml-basics-fa82}

Artificial intelligence's transformative impact across society raises a fundamental question: How can we create these intelligent capabilities? The relationship between AI and ML provides the theoretical and practical framework to address this question.

Artificial Intelligence represents the systematic pursuit of understanding and replicating intelligent behavior, specifically the capacity to learn, reason, and adapt to new situations. As the theoretical framework, AI encompasses fundamental questions about the nature of intelligence itself: How do we recognize patterns? How do we learn from experience? How do we adapt our behavior based on new information? AI explores these questions by drawing insights from cognitive science, psychology, neuroscience, and computer science, establishing the conceptual foundations for what it means to be intelligent.

**Machine Learning**, in contrast, constitutes the methodological approach and practical discipline for creating systems that demonstrate intelligent behavior. Rather than implementing intelligence through predetermined rules, machine learning provides the computational techniques to automatically discover patterns in data through mathematical processes. This methodology transforms AI's theoretical insights into functioning systems. Object recognition in machine learning systems parallels human visual learning, requiring exposure to numerous examples to develop robust recognition capabilities. Similarly, natural language processing systems acquire linguistic capabilities through extensive analysis of textual data, demonstrating how ML operationalizes AI's understanding of intelligence. The architectural foundations of these modern deep learning systems are detailed in @sec-dl-primer and @sec-dnn-architectures.

The relationship between AI and ML exemplifies connections between theoretical understanding and practical engineering implementation in scientific fields. Physics provides theoretical foundations for mechanical engineering applications in structural design and machinery, while AI's theoretical frameworks inform machine learning's practical development of intelligent systems. Electrical engineering's transformation of electromagnetic theory into functional power systems parallels machine learning's implementation of intelligence theories into operational systems.

::: {.callout-definition title="AI and ML"}
- **Artificial Intelligence (AI)**: The systematic pursuit of understanding and replicating intelligent behavior. This provides the theoretical framework for comprehending how systems can learn, reason, and adapt to new situations.

- **Machine Learning (ML)**: The methodological approach to implementing intelligent systems through computational techniques that automatically discover patterns in data, rather than through predetermined rules.

:::

Machine learning emerged as a viable scientific discipline through extensive research and fundamental paradigm shifts[^fn-paradigm-shift] in artificial intelligence. The progression of artificial intelligence encompasses both theoretical advances in understanding intelligence and practical developments in implementation methodologies. The modern deep learning approaches that emerged from this evolution are comprehensively covered in @sec-dl-primer.

[^fn-paradigm-shift]: **Paradigm Shift**: A term coined by philosopher Thomas Kuhn in 1962 to describe fundamental changes in scientific approach, such as the shift from Newtonian to Einstein's physics. In AI, key paradigm shifts include moving from symbolic reasoning to statistical learning (1990s), and from shallow to deep learning (2010s). Each shift required researchers to abandon established methods and embrace radically different approaches to understanding intelligence.

This development mirrors other engineering fields. Mechanical engineering advanced from basic force principles to modern robotics. Electrical engineering progressed from electromagnetic theory to power and communication systems. Similarly, AI has evolved from theoretical foundations to practical ML systems. Analysis of this historical trajectory reveals both the technological innovations leading to current machine learning approaches and the emergence of advanced learning approaches that inform contemporary AI system development.

## AI Evolution {#sec-introduction-ai-evolution-8ff4}

The evolution of AI, depicted in the timeline shown in @fig-ai-timeline, highlights key milestones such as the development of the perceptron[^fn-early] in 1957 by Frank Rosenblatt [@rosenblatt1957perceptron], an early computational learning algorithm. Computer labs in 1965 contained room-sized mainframes[^fn-mainframes] running programs that could prove basic mathematical theorems or play simple games like tic-tac-toe. These early artificial intelligence systems, though groundbreaking for their time, differed substantially from today's machine learning systems that detect cancer in medical images or understand human speech. The timeline shows the progression from early innovations like the ELIZA[^fn-eliza] chatbot in 1966, to significant breakthroughs such as IBM's Deep Blue defeating chess champion Garry Kasparov in 1997. More recent advancements include the introduction of OpenAI's GPT-3 in 2020 and GPT-4 in 2023, demonstrating the dramatic evolution and increasing complexity of AI systems over the decades.

[^fn-early]: **Perceptron**: One of the first computational learning algorithms. This system could learn to classify patterns by making yes/no decisions based on inputs.

[^fn-mainframes]: **Mainframes**: Room-sized computers that dominated the 1960s-70s, typically costing millions of dollars and requiring dedicated cooling systems. IBM's System/360 mainframe from 1964 weighed 20,000 pounds and had just 64KB of memory, about 1/millionth the memory of a modern smartphone, yet represented the cutting edge of computing power that enabled early AI research.

[^fn-eliza]: **ELIZA**: Created by MIT's Joseph Weizenbaum in 1966, ELIZA was one of the first chatbots that could simulate human conversation by pattern matching and substitution. Ironically, Weizenbaum was horrified when people began forming emotional attachments to his simple program, leading him to become a critic of AI.

::: {#fig-ai-timeline fig-env="figure" fig-pos="t!"}
```{.tikz}
\begin{tikzpicture}[line join=round,font=\usefont{T1}{phv}{m}{n}\small]
\definecolor{bluegraph}{RGB}{0,102,204}
    \pgfmathsetlengthmacro\MajorTickLength{
      \pgfkeysvalueof{/pgfplots/major tick length} * 1.5
    }
\tikzset{%
   textt/.style={line width=0.5pt,draw=bluegraph,text width=26mm,align=flush center,
                        font=\usefont{T1}{phv}{m}{n}\footnotesize,fill=cyan!7},
   Line/.style={line width=0.85pt,draw=bluegraph,dash pattern=on 3pt off 2pt,
   {Circle[bluegraph,length=4.5pt]}-   }
}

\begin{axis}[clip=false,
  axis line style={thick},
  axis lines*=left,
  axis on top,
  width=18cm,
  height=20cm,
  xmin=1950,
  xmax=2023,
  ymin=0.000000,
  ymax=0.00033,
  xtick={1950,1960,1970,1980,1990,2000,2010,2020},
  extra x ticks={1955,1965,1975,1985,1995,2005,2015},
  extra x tick labels={},
  xticklabels={1950,1960,1970,1980,1990,2000,2010,2020},
  ytick={0.0000,0.00005, 0.00010, 0.00015, 0.00020, 0.00025, 0.00030},
  yticklabels={0.0000,0.00005, 0.00010, 0.00015, 0.00020, 0.00025, 0.00030},
  grid=none,
    tick label style={/pgf/number format/assume math mode=true},
    xticklabel style={font=\footnotesize\usefont{T1}{phv}{m}{n},
},
   yticklabel style={
  font=\footnotesize\usefont{T1}{phv}{m}{n},
  /pgf/number format/fixed,
  /pgf/number format/fixed zerofill,
  /pgf/number format/precision=5
},
scaled y ticks=false,
tick style = {line width=1.0pt},
tick align = outside,
major tick length=\MajorTickLength,
]
\fill[fill=BrownL!70](axis cs:1974,0)rectangle(axis cs:1980,0.00031)
        node[above,align=center,xshift=-7mm]{1st AI \\ Winter};
\fill[fill=BrownL!70](axis cs:1987,0)rectangle(axis cs:1993,0.00031)
        node[above,align=center,xshift=-7mm]{2nd AI \\ Winter};
\addplot[line width=2pt,color=RedLine,smooth,samples=100] coordinates {
(1950,0.0000006281)
(1951,0.0000000683)
(1952,0.0000003056)
(1953,0.0000002927)
(1954,0.0000004296)
(1955,0.0000004593)
(1956,0.0000016705)
(1957,0.0000006570)
(1958,0.0000021902)
(1959,0.0000032832)
(1960,0.0000126863)
(1961,0.0000063721)
(1962,0.0000240680)
(1963,0.0000141502)
(1964,0.0000111442)
(1965,0.0000143832)
(1966,0.0000147726)
(1967,0.0000169539)
(1968,0.0000167880)
(1969,0.0000175559)
(1970,0.0000155680)
(1971,0.0000206809)
(1972,0.0000223804)
(1973,0.0000218203)
(1974,0.0000256138)
(1975,0.0000282924)
(1976,0.0000247784)
(1977,0.0000404966)
(1978,0.0000358032)
(1979,0.0000436903)
(1980,0.0000472788)
(1981,0.0000561471)
(1982,0.0000767864)
(1983,0.0001064465)
(1984,0.0001592212)
(1985,0.0002133700)
(1986,0.0002559067)
(1987,0.0002608470)
(1988,0.0002623321)
(1989,0.0002358150)
(1990,0.0002301105)
(1991,0.0002051343)
(1992,0.0001789229)
(1993,0.0001560935)
(1994,0.0001508219)
(1995,0.0001401406)
(1996,0.0001169577)
(1997,0.0001150365)
(1998,0.0001051385)
(1999,0.0000981740)
(2000,0.0001010236)
(2001,0.0000976966)
(2002,0.0001038084)
(2003,0.0000980004)
(2004,0.0000989412)
(2005,0.0000977251)
(2006,0.0000899964)
(2007,0.0000864005)
(2008,0.0000911872)
(2009,0.0000852932)
(2010,0.0000822649)
(2011,0.0000913442)
(2012,0.0001104912)
(2013,0.0001023061)
(2014,0.0001022477)
(2015,0.0000919719)
(2016,0.0001134797)
(2017,0.0001384348)
(2018,0.0002057324)
(2019,0.0002328642)
}
node[left,pos=1,align=center,black]{Last year of\\ date: 2019};

\node[textt,text width=20mm](1950)at(axis cs:1957,0.00014){\textcolor{red}{1950}\\
Alan Turing publishes \textbf{``Computing Machinery and Intelligence''} in the journal \textit{Mind}.};
\node[red,align=center,above=2mm of 1950]{Milestones\\ in AI};
\draw[Line] (axis cs:1950,0) -- (1950.235);
%
\node[textt,text width=19mm](1956)at(axis cs:1958,0.00007){\textcolor{red}{Summer 1956}\\
\textbf{Dartmouth Workshop} A formative conference organized by AI pioneer John McCarthy.};
\draw[Line] (axis cs:1956,0) -- (1956.255);
%
\node[textt](1957)at(axis cs:1969,0.00022){\textcolor{red}{1957}\\
\textbf{Cornell psychologist Frank Rosenblatt invents the perceptron}, a system that paves the way for
modern neural networks
(see "The Turbulent Past and Uncertain Future of Artificial Intelligence," p. 26).};
\draw[Line] (axis cs:1957,0) -- ++(0mm,17mm)-|(1957.248);
%
\node[textt,text width=21mm](1966)at(axis cs:1972,0.00012){\textcolor{red}{1966}\\
\textbf{ELIZA chatbot} An early example of natural-language programming created by
MIT professor Joseph Weizenbaum.};
\draw[Line] (axis cs:1966,0) -- ++(0mm,17mm)-|(1966);
%
\node[textt,text width=20mm](1979)at(axis cs:1985,0.00012){\textcolor{red}{1979}\\
Hans Moravec builds the \textbf{Stanford Cart}, one of the first autonomous vehicles.};
\draw[Line] (axis cs:1979,0) -- ++(0mm,17mm)-|(1979.245);
%
\node[textt,text width=21mm](1981)at(axis cs:1990,0.00006){\textcolor{red}{1981}\\
Japanese \textbf{Fifth-Generation Computer Systems} project begins. The infusion of
research funding helps end first "AI winter."};
\draw[Line] (axis cs:1981,0) -- ++(0mm,10mm)-|(1981);
%
\node[textt,text width=15mm](1997)at(axis cs:2001,0.00007){\textcolor{red}{1997}\\
\textbf{IBM's Deep Blue} beats world chess champion Garry Kasparov};
\draw[Line] (axis cs:1997,0) -- ++(0mm,10mm)-|(1997);
%
\node[textt,text width=15mm](2011)at(axis cs:2014,0.00003){\textcolor{red}{2011}\\
\textbf{IBM's Watson} wins at Jeopardy!};
\draw[Line] (axis cs:2011,0) -- (2011);
%
\node[textt,text width=19mm](2005)at(axis cs:2012,0.00009){\textcolor{red}{2005}\\
\textbf{DARPA Grand Challenge} Stanford wins the agency's second driverless-car
competition by driving 211 kilometers on an unhearsed trail};
\draw[Line] (axis cs:2005,0) -- (2005);
%
\node[textt,text width=30mm](2020)at(axis cs:2010,0.00017){\textcolor{red}{2020}\\
\textbf{OpenAI introduces GPT-3}. The enormously powerful natural-language model
later causes an outcry when it begins spouting bigoted remarks};
\draw[Line] (axis cs:2020,0) |- (2020);
%
\draw[Line,solid,-] (axis cs:1991,0.0002) --++(50:35mm)
node[bluegraph,above,align=center,text width=30mm]{Percent of U.S.-published books
in Google's database that mention artificial intelligence};
\end{axis}
\end{tikzpicture}
```
**AI Development Timeline**: Early AI research focused on symbolic reasoning and rule-based systems, while modern AI leverages data-driven approaches like neural networks to achieve increasingly complex tasks. This progression exposes a shift from hand-coded intelligence to learned intelligence, marked by milestones such as the perceptron, deep blue, and large language models like GPT-3.
:::

This historical progression reveals several distinct eras of development.

### Symbolic AI Era {#sec-introduction-symbolic-ai-era-0e23}

The story of machine learning begins at the historic Dartmouth Conference[^fn-dartmouth-conference] in 1956, where pioneers like John McCarthy, Marvin Minsky, and Claude Shannon first coined the term "artificial intelligence." Their approach embodied a compelling premise: intelligence could be reduced to symbol manipulation. Daniel Bobrow's STUDENT system from 1964 [@bobrow1964student] exemplifies this era as one of the first AI programs solving algebra word problems. It was one of the first AI programs to demonstrate natural language understanding by converting English text into algebraic equations, marking an important milestone in symbolic AI.

[^fn-dartmouth-conference]: **Dartmouth Conference (1956)**: The legendary 8-week workshop at Dartmouth College where AI was officially born. Organized by John McCarthy, Marvin Minsky, Nathaniel Rochester, and Claude Shannon, it was the first time researchers gathered specifically to discuss "artificial intelligence," a term McCarthy coined for the proposal. The ambitious goal was to make machines "simulate every aspect of learning or any other feature of intelligence." Though overly optimistic, this gathering launched AI as a formal research field.

::: {.callout-example title="STUDENT (1964)"}
```
Problem: "If the number of customers Tom gets is twice the
square of 20% of the number of advertisements he runs, and
the number of advertisements is 45, what is the number of
customers Tom gets?"

STUDENT would:

1. Parse the English text
2. Convert it to algebraic equations
3. Solve the equation: n = 2(0.2 × 45)²
4. Provide the answer: 162 customers
```
:::

Early AI like STUDENT suffered from a fundamental limitation: they could only handle inputs that exactly matched their pre-programmed patterns and rules. A language translator that only works with perfect grammatical structure demonstrates this limitation. Even slight variations like changed word order, synonyms, or natural speech patterns would cause the system to fail. This "brittleness"[^fn-brittleness] meant that while these solutions could appear intelligent when handling very specific cases they were designed for, they would break down completely when faced with even minor variations or real-world complexity. This limitation revealed a deeper problem with rule-based AI approaches: they couldn't genuinely understand or generalize from programming, only match and manipulate text patterns exactly as specified.

[^fn-brittleness]: **Brittleness in AI Systems**: The tendency of rule-based systems to fail completely when encountering inputs that fall outside their programmed scenarios, no matter how similar those inputs might be to what they were designed to handle. This contrasts with human intelligence, which can adapt and make reasonable guesses even in unfamiliar situations. The brittleness problem drove researchers toward machine learning approaches that could generalize from examples rather than relying on exhaustive rule sets.

### Expert Systems Era {#sec-introduction-expert-systems-era-8ba7}

By the mid-1970s, researchers recognized general AI as overly ambitious and focused on capturing human expert knowledge in specific domains. MYCIN [@shortliffe1976mycin], developed at Stanford, was one of the first large-scale expert systems designed to diagnose blood infections.

::: {.content-visible when-format="html"}
::: {.callout-example title="MYCIN (1976)"}
```
Rule Example from MYCIN:
IF
  The infection is primary-bacteremia
  The site of the culture is one of the sterile sites
  The suspected portal of entry is the gastrointestinal tract
THEN
  Found suggestive evidence (0.7) that infection is bacteroid
```
:::
:::

::: {.content-visible when-format="pdf"}
::: {.callout-example title="MYCIN (1976)"}
```
Rule Example from MYCIN:
IF
  The infection is primary-bacteremia
  The site of the culture is one of the sterile sites
  The suspected portal of entry is the gastrointestinal tract
THEN
  Found suggestive evidence (0.7) that infection is bacteroid
```
:::
:::

MYCIN represented a major advance in medical AI with 600 expert rules for diagnosing blood infections, yet it revealed key challenges persisting in contemporary ML. Getting domain knowledge from human experts and converting it into precise rules proved incredibly time-consuming and difficult, as doctors often couldn't explain exactly how they made decisions. MYCIN struggled with uncertain or incomplete information, unlike human doctors who could make educated guesses. Perhaps most importantly, maintaining and updating the rule base became exponentially more complex as MYCIN grew, as adding new rules frequently conflicted with existing ones, while medical knowledge itself continued to evolve. Knowledge capture, uncertainty handling, and maintenance remain central concerns in modern machine learning, addressed through different technical approaches.

### Statistical Learning Era {#sec-introduction-statistical-learning-era-c064}

The 1990s marked a radical transformation in artificial intelligence as the field shifted from hand-coded rules toward statistical learning approaches. Three converging factors made statistical methods both possible and powerful. The digital revolution meant massive amounts of data were suddenly available to train the algorithms. Moore's Law[^fn-mooreslaw] delivered the computational power needed to process this data effectively. And researchers developed new algorithms like Support Vector Machines and improved neural networks that could actually learn patterns from this data rather than following pre-programmed rules. This combination fundamentally changed AI development: rather than encoding human knowledge directly, machines could discover patterns automatically from examples, creating more robust and adaptable systems.

[^fn-mooreslaw]: **Moore's Law**: The observation made by Intel co-founder Gordon Moore in 1965 that the number of transistors on a microchip doubles approximately every two years, while the cost halves. This exponential growth in computing power has been a key driver of advances in machine learning, though the pace has begun to slow in recent years.

Email spam filtering evolution illustrates this transformation. Early rule-based systems used explicit patterns but proved brittle and easily circumvented. Statistical systems took a different approach: if the word 'viagra' appears in 90% of spam emails but only 1% of normal emails, we can use this pattern to identify spam. Rather than writing explicit rules, statistical systems learn these patterns automatically from thousands of example emails, making them adaptable to new spam techniques. The mathematical foundation relies on Bayes' theorem to calculate the probability that an email is spam given specific words: $P(\text{spam}|\text{word}) = P(\text{word}|\text{spam}) \times P(\text{spam}) / P(\text{word})$. For emails with multiple words, we combine these probabilities across the entire message.

::: {.callout-example title="Early Spam Detection Systems"}
```
Rule-based (1980s):
IF contains("viagra") OR contains("winner") THEN spam

Statistical (1990s):
P(spam|word) = (frequency in spam emails) / (total frequency)

Combined using Naive Bayes:
P(spam|email) ∝ P(spam) × ∏ P(word|spam)
```
:::

Statistical approaches introduced three core concepts that remain fundamental to AI development. First, the quality and quantity of training data became as important as the algorithms themselves. AI could only learn patterns that were present in its training examples. Second, we needed rigorous ways to evaluate how well AI actually performed, leading to metrics that could measure success and compare different approaches. Third, we discovered an inherent tension between precision (being right when we make a prediction) and recall (catching all the cases we should find), forcing designers to make explicit trade-offs based on their application's needs. The engineering practices for managing these data quality challenges and performance evaluation metrics are covered in @sec-data-engineering and @sec-benchmarking-ai. Spam filters might tolerate some spam to avoid blocking important emails, while medical diagnosis systems prioritize catching every potential case despite increased false alarms.

@tbl-ai-evolution-strengths summarizes the evolutionary journey of AI approaches, highlighting key strengths and capabilities emerging with each paradigm. Moving from left to right reveals important trends. Before examining shallow and deep learning, understanding trade-offs between existing approaches provides important context.

+---------------------+--------------------------+--------------------------+--------------------------+-------------------------------+
| Aspect              | Symbolic AI              | Expert Systems           | Statistical Learning     | Shallow / Deep Learning       |
+:====================+:=========================+:=========================+:=========================+:==============================+
| Key Strength        | Logical reasoning        | Domain expertise         | Versatility              | Pattern recognition           |
+---------------------+--------------------------+--------------------------+--------------------------+-------------------------------+
| Best Use Case       | Well-defined, rule-based | Specific domain problems | Various structured data  | Complex, unstructured data    |
|                     | problems                 |                          | problems                 | problems                      |
+---------------------+--------------------------+--------------------------+--------------------------+-------------------------------+
| Data Handling       | Minimal data needed      | Domain knowledge-based   | Moderate data required   | Large-scale data processing   |
+---------------------+--------------------------+--------------------------+--------------------------+-------------------------------+
| Adaptability        | Fixed rules              | Domain-specific          | Adaptable to various     | Highly adaptable to diverse   |
|                     |                          | adaptability             | domains                  | tasks                         |
+---------------------+--------------------------+--------------------------+--------------------------+-------------------------------+
| Problem Complexity  | Simple, logic-based      | Complicated, domain-     | Complex, structured      | Highly complex, unstructured  |
|                     |                          | specific                 |                          |                               |
+---------------------+--------------------------+--------------------------+--------------------------+-------------------------------+

: **AI Paradigm Evolution**: Shifting from symbolic AI to statistical approaches fundamentally changed machine learning by prioritizing data quantity and quality, enabling rigorous performance evaluation, and necessitating explicit trade-offs between precision and recall to optimize system behavior for specific applications. The table outlines how each paradigm addressed these challenges, revealing a progression towards data-driven systems capable of handling complex, real-world problems. {#tbl-ai-evolution-strengths}

This analysis bridges early approaches with recent developments in shallow and deep learning. It explains why certain approaches gained prominence in different eras and how each paradigm built upon predecessors while addressing their limitations. Earlier approaches continue to influence and enhance modern AI techniques, particularly in foundation model development.

### Shallow Learning Era {#sec-introduction-shallow-learning-era-3448}

The 2000s marked a significant period in machine learning history known as the "shallow learning" era. The term "shallow" refers to architectural depth: shallow learning typically employed one or two processing levels, contrasting with deep learning's multiple hierarchical layers that emerged later.

During this time, several powerful algorithms dominated the machine learning landscape. Each brought unique strengths to different problems: Decision trees provided interpretable results by making choices much like a flowchart. K-nearest neighbors made predictions by finding similar examples in past data, like asking your most experienced neighbors for advice. Linear and logistic regression offered straightforward, interpretable models that worked well for many real-world problems. Support Vector Machines (SVMs) excelled at finding complex boundaries between categories using the "kernel trick." This technique transforms complex patterns by projecting data into higher dimensions where linear separation becomes possible. These algorithms formed the foundation of practical machine learning.

A typical computer vision solution from 2005 exemplifies this approach:

::: {.callout-example title="Traditional Computer Vision Pipeline"}
```
1. Manual Feature Extraction
  - SIFT (Scale-Invariant Feature Transform)
  - HOG (Histogram of Oriented Gradients)
  - Gabor filters
2. Feature Selection/Engineering
3. "Shallow" Learning Model (e.g., SVM)
4. Post-processing
```
:::

This era's hybrid approach combined human-engineered features with statistical learning. They had strong mathematical foundations (researchers could prove why they worked). They performed well even with limited data. They were computationally efficient. They produced reliable, reproducible results.

The Viola-Jones algorithm [@viola2001rapidobject][^fn-viola-jones] (2001) exemplifies this era, achieving real-time face detection using simple rectangular features and cascaded classifiers[^fn-cascade]. This algorithm powered digital camera face detection for nearly a decade.

[^fn-viola-jones]: **Viola-Jones Algorithm**: A groundbreaking computer vision algorithm that could detect faces in real-time by using simple rectangular patterns (like comparing the brightness of eye regions versus cheek regions) and making decisions in stages, filtering out non-faces quickly and spending more computation only on promising candidates.

[^fn-cascade]: **Cascade of Classifiers**: A multi-stage decision system where each stage acts as a filter, quickly rejecting obvious non-matches and passing promising candidates to the next, more sophisticated stage. This approach is similar to how security screening works at airports with multiple checkpoints of increasing thoroughness.

### Deep Learning Era {#sec-introduction-deep-learning-era-155a}

While Support Vector Machines excelled at finding complex category boundaries through mathematical transformations, deep learning adopted a radically different approach inspired by brain architecture. Deep learning employs layers of simple computational units inspired by brain neurons[^fn-neurons], with each layer transforming input data into increasingly abstract representations. The detailed architecture and functioning of these neural networks are explored in @sec-dl-primer and @sec-dnn-architectures.

[^fn-neurons]: **Artificial Neurons**: Basic computational units in neural networks that mimic biological neurons, taking multiple inputs, applying weights and biases, and producing an output signal through an activation function.

In image processing, this layered approach works systematically. The first layer detects simple edges and contrasts, subsequent layers combine these into basic shapes and textures, higher layers recognize specific features like whiskers and ears, and final layers assemble these into concepts like "cat."

Unlike shallow learning methods requiring carefully engineered features, deep learning networks automatically discover useful features from raw data. This layered approach to learning—building from simple patterns to complex concepts—defines "deep" learning and proves remarkably effective for complex, real-world data like images, speech, and text.

AlexNet, shown in @fig-alexnet, achieved a breakthrough in the 2012 ImageNet[^fn-intro-imagenet] competition that transformed machine learning. The challenge required correctly classifying 1.2 million high-resolution images into 1,000 categories. While previous approaches struggled with error rates above 25%, AlexNet[^fn-intro-alexnet] achieved a 15.3% top-5 error rate [@krizhevsky2012imagenet], dramatically outperforming all existing methods.

[^fn-intro-imagenet]: **ImageNet**: A massive visual database containing over 14 million labeled images across 20,000+ categories, created by Stanford's Fei-Fei Li starting in 2009 [@deng2009imagenet]. The annual ImageNet challenge became the Olympics of computer vision, driving breakthrough after breakthrough in image recognition until neural networks became so good they essentially solved the competition.

[^fn-intro-alexnet]: **AlexNet**: A breakthrough deep neural network from 2012 that won the ImageNet competition by a large margin and helped spark the deep learning revolution. Named after Alex Krizhevsky, it proved that neural networks could outperform traditional computer vision methods when given enough data and computing power.

The success of AlexNet wasn't just a technical achievement; it was a watershed moment that demonstrated the practical viability of deep learning. It showed that with sufficient data, computational power, and architectural innovations, neural networks could outperform hand-engineered features and shallow learning methods that had dominated the field for decades. This single result triggered an explosion of research and applications in deep learning that continues to this day. The specialized hardware acceleration that enabled this breakthrough is examined in @sec-ai-acceleration, while the optimization techniques that make such models practical are covered in @sec-model-optimizations.

::: {#fig-alexnet fig-env="figure" fig-pos="htb"}
```{.tikz}
\begin{tikzpicture}[line join=round,font=\usefont{T1}{phv}{m}{n}\small]
\clip (-11.2,-2) rectangle (15.5,5.45);
%\draw[red](-11.2,-1.7) rectangle (15.5,5.45);
\tikzset{%
 LineD/.style={line width=0.7pt,black!50,dashed,dash pattern=on 3pt off 2pt},
  LineG/.style={line width=0.75pt,GreenLine},
  LineR/.style={line width=0.75pt,RedLine},
  LineA/.style={line width=0.75pt,BrownLine,-latex,text=black}
}
\newcommand\FillCube[4]{
\def\depth{#2}
\def\width{#3}
\def\height{#4}
\def\nc{#1}
% Lower front left corner
\coordinate (A\nc) at (0, 0);
% Donji prednji desni
\coordinate (B\nc) at (\width, 0);
% Upper front right
\coordinate (C\nc) at (\width, \height);
% Upper front left
\coordinate (D\nc) at (0, \height);
% Pomak u "dubinu"
\coordinate (shift) at (-0.7*\depth, \depth);
% Last points (moved)
\coordinate (E\nc) at ($(A\nc) + (shift)$);
\coordinate (F\nc) at ($(B\nc) + (shift)$);
\coordinate (G\nc) at ($(C\nc) + (shift)$);
\coordinate (H\nc) at ($(D\nc) + (shift)$);
% Front side
\draw[GreenLine,fill=green!08,line width=0.5pt] (A\nc) -- (B\nc) -- (C\nc) --(D\nc) -- cycle;
% Top side
\draw[GreenLine,fill=green!20,line width=0.5pt] (D\nc) -- (H\nc) -- (G\nc) -- (C\nc);
% Left
\draw[GreenLine,fill=green!15] (A\nc) -- (E\nc) -- (H\nc)--(D\nc)--cycle;
\draw[] (E\nc) -- (H\nc);
\draw[GreenLine,line width=0.75pt](A\nc)--(B\nc)--(C\nc)--(D\nc)--(A\nc)
(A\nc)--(E\nc)--(H\nc)--(G\nc)--(C\nc)
(D\nc)--(H\nc);
}
%%%
\newcommand\SmallCube[4]{
\def\nc{#1}
\def\depth{#2}
\def\width{#3}
\def\height{#4}
\coordinate (A\nc) at (0, 0);
\coordinate (B\nc) at (\width, 0);
\coordinate (C\nc) at (\width, \height);
\coordinate (D\nc) at (0, \height);
\coordinate (shift) at (-0.7*\depth, \depth);
\coordinate (E\nc) at ($(A\nc) + (shift)$);
\coordinate (F\nc) at ($(B\nc) + (shift)$);
\coordinate (G\nc) at ($(C\nc) + (shift)$);
\coordinate (H\nc) at ($(D\nc) + (shift)$);
\draw[RedLine,fill=red!08,line width=0.5pt,fill opacity=0.7] (A\nc) -- (B\nc) -- (C\nc) -- (D\nc) -- cycle;
\draw[RedLine,fill=red!20,line width=0.5pt,fill opacity=0.7] (D\nc) -- (H\nc) -- (G\nc) -- (C\nc);
\draw[RedLine,fill=red!15,fill opacity=0.7] (A\nc) -- (E\nc) -- (H\nc)--(D\nc)--cycle;
\draw[] (E\nc) -- (H\nc);
}
%%%%%%%%%%%%%%%%%%%%%
%%4 column
%%%%%%%%%%%%%%%%%%%%
\begin{scope}
%big cube
\begin{scope}
\FillCube{4VD}{0.8}{3}{2}
\end{scope}
%%small cube
\begin{scope}[shift={(-0.10,0.4)},line width=0.5pt]
\SmallCube{4MD}{0.4}{3}{0.6}
%%
\draw[LineR](A\nc)-- (B\nc)--node[left,text=black]{3}
(C\nc)--(D\nc)-- (A\nc)
(A\nc)--(E\nc)--(H\nc)--(G\nc)--node[left,text=black]{3}(C\nc)
(D\nc)-- (H\nc);
%
\def\nc{4VD}
\draw[LineG](A\nc)--node[below,text=black]{192} (B\nc)--
(C\nc)--(D\nc)--node[right,text=black,text opacity=1]{13} (A\nc)
(A\nc)--(E\nc)--(H\nc)--(G\nc)--(C\nc)
(D\nc)--node[right,text=black,text opacity=1]{13} (H\nc);
\end{scope}
\end{scope}
%%Above
\begin{scope}[shift={(0,3.5)}]
%big cube
\begin{scope}
\FillCube{4VG}{0.8}{3}{2}
\end{scope}
%%small cube
\begin{scope}[shift={(-0.18,0.55)}]
\SmallCube{4MG}{0.4}{3}{0.6}
%%
\draw[LineR](A\nc)-- (B\nc)--node[left,text=black]{3}
(C\nc)--(D\nc)-- (A\nc)
(A\nc)--(E\nc)--(H\nc)--(G\nc)--node[left,text=black]{3}(C\nc)
(D\nc)-- (H\nc);
\def\nc{4VG}
\draw[LineG](A\nc)--node[below,text=black]{192} (B\nc)--
(C\nc)--(D\nc)--node[right,text=black,text opacity=1]{} (A\nc)
(A\nc)--(E\nc)--(H\nc)--(G\nc)--(C\nc)
(D\nc)--node[right,text=black,text opacity=1]{} (H\nc);
\end{scope}
\end{scope}
%%%%%
%%5 column
%%%%
%%small cube
\begin{scope}[shift={(4.15,0)}]
%big cube
\begin{scope}
\FillCube{5VD}{0.8}{3}{2}
\end{scope}
%%small cube
\begin{scope}[shift={(-0.10,1.25)}]
\SmallCube{5MD}{0.4}{3}{0.6}
%%
\draw[LineR](A\nc)-- (B\nc)--node[left,text=black]{3}
(C\nc)--(D\nc)-- (A\nc)
(A\nc)--(E\nc)--(H\nc)--(G\nc)--node[left,text=black]{3}(C\nc)
(D\nc)-- (H\nc);
%
\def\nc{5VD}
\draw[LineG](A\nc)--node[below,text=black]{192} (B\nc)--
(C\nc)--(D\nc)--node[right,text=black,text opacity=1]{13} (A\nc)
(A\nc)--(E\nc)--(H\nc)--(G\nc)--(C\nc)
(D\nc)--node[right,text=black,text opacity=1]{13} (H\nc);
\end{scope}
\end{scope}
%%Above
\begin{scope}[shift={(4.15,3.5)}]
%big cube
\begin{scope}
\FillCube{5VG}{0.8}{3}{2}
\end{scope}
%%small cube
\begin{scope}[shift={(-0.08,0.28)}]
\SmallCube{5MG}{0.4}{3}{0.6}
%%
\draw[LineR](A\nc)-- (B\nc)--node[left,text=black]{3}
(C\nc)--(D\nc)-- (A\nc)
(A\nc)--(E\nc)--(H\nc)--(G\nc)--node[left,text=black]{3}(C\nc)
(D\nc)-- (H\nc);
%
\def\nc{5VG}
\draw[LineG](A\nc)--node[below,text=black]{192} (B\nc)--
(C\nc)--(D\nc)--node[right,text=black,text opacity=1]{} (A\nc)
(A\nc)--(E\nc)--(H\nc)--(G\nc)--(C\nc)
(D\nc)--node[right,text=black,text opacity=1]{} (H\nc);
\end{scope}
\end{scope}
%%%%%%%%%%%%%%%%%%%%%%%
%%3 column
%%%%%%%%%%%%%%%%%%%%%%%
\begin{scope}[shift={(-3.75,-0.5)}]
%big cube
\begin{scope}
\FillCube{3VD}{1.5}{2.33}{3}
\end{scope}
%%small cube-down
\begin{scope}[shift={(-0.10,0.45)}]
\SmallCube{3MDI}{0.4}{2.33}{0.6}
%%
\draw[LineR](A\nc)-- (B\nc)--node[left,text=black]{3}
(C\nc)--(D\nc)-- (A\nc)
(A\nc)--(E\nc)--(H\nc)--(G\nc)--node[left,text=black]{3}(C\nc)
(D\nc)-- (H\nc);
%
\end{scope}
%%small cube - up
\begin{scope}[shift={(-0.12,2.23)}]
\SmallCube{3MDII}{0.4}{2.33}{0.6}
%%
\draw[LineR](A\nc)-- (B\nc)--node[left,text=black]{3}
(C\nc)--(D\nc)-- (A\nc)
(A\nc)--(E\nc)--(H\nc)--(G\nc)--node[left,text=black]{3}(C\nc)
(D\nc)-- (H\nc);
%
\def\nc{3VD}
\draw[LineG](A\nc)--node[below,text=black]{128} (B\nc)--
(C\nc)--(D\nc)--node[right,text=black,text opacity=1,pos=0.4]{27} (A\nc)
(A\nc)--(E\nc)--(H\nc)--(G\nc)--(C\nc)
(D\nc)--node[right,text=black,text opacity=1]{27} (H\nc);
\end{scope}
\end{scope}
%%Above
\begin{scope}[shift={(-3.75,3.5)}]
%big cube
\begin{scope}
\FillCube{3VG}{1.5}{2.33}{3}
\end{scope}
%%small cube-down
\begin{scope}[shift={(-0.42,0.75)}]
\SmallCube{3MGI}{0.4}{2.33}{0.6}
%%
\draw[LineR](A\nc)-- (B\nc)--node[left,text=black]{}
(C\nc)--(D\nc)-- (A\nc)
(A\nc)--(E\nc)--(H\nc)--(G\nc)--node[left,text=black]{3}(C\nc)
(D\nc)-- (H\nc);
%
\def\nc{3VG}
\draw[GreenLine,line width=0.75pt](A\nc)--node[below,text=black]{128} (B\nc)--
(C\nc)--(D\nc)--node[right,text=black,text opacity=1]{} (A\nc)
(A\nc)--(E\nc)--(H\nc)--(G\nc)--(C\nc)
(D\nc)--node[right,text=black,text opacity=1]{} (H\nc);
\end{scope}
%%small cube-up
\begin{scope}[shift={(-0.06,0.18)}]
\SmallCube{3MGII}{0.4}{2.33}{0.6}
%%
\draw[LineR](A\nc)-- (B\nc)--node[left,text=black]{3}
(C\nc)--(D\nc)-- (A\nc)
(A\nc)--(E\nc)--(H\nc)--(G\nc)--node[left,text=black]{3}(C\nc)
(D\nc)-- (H\nc);
%
\def\nc{3VG}
\draw[LineG](A\nc)--node[below,text=black]{128} (B\nc)--
(C\nc)--(D\nc)--node[right,text=black,text opacity=1]{} (A\nc)
(A\nc)--(E\nc)--(H\nc)--(G\nc)--(C\nc)
(D\nc)--node[right,text=black,text opacity=1]{} (H\nc);
\end{scope}
\end{scope}
%%%%%%%%%%%%%%%%%%%%%%%
%%2 column
%%%%%%%%%%%%%%%%%%%%%%%
\begin{scope}[shift={(-6.8,-1)}]
%big cube
\begin{scope}
\FillCube{2VD}{2}{1.3}{3.8}
\end{scope}
%%small cube
\begin{scope}[shift={(-0.2,2.5)}]
\SmallCube{2MD}{0.4}{1.3}{1}
%%
\draw[LineR](A\nc)-- (B\nc)--node[left,text=black]{5}
(C\nc)--(D\nc)-- (A\nc)
(A\nc)--(E\nc)--(H\nc)--(G\nc)--node[left,text=black]{5}(C\nc)
(D\nc)-- (H\nc);
%
\def\nc{2VD}
\draw[LineG](A\nc)--node[below,text=black]{48} (B\nc)--
(C\nc)--(D\nc)--node[pos=0.6,right,text=black,text opacity=1]{55} (A\nc)
(A\nc)--(E\nc)--(H\nc)--(G\nc)--(C\nc)
(D\nc)--node[pos=0.26,right,text=black,text opacity=1]{55} (H\nc);
\end{scope}
\end{scope}
%%Above
\begin{scope}[shift={(-6.8,3.5)}]
%big cube
\begin{scope}
\FillCube{2VG}{2}{1.3}{3.8}
\end{scope}
%%small cube
\begin{scope}[shift={(-0.1,0.5)}]
\SmallCube{2MG}{0.4}{1.3}{1}
%%
\draw[LineR](A\nc)-- (B\nc)--node[left,text=black]{5}
(C\nc)--(D\nc)-- (A\nc)
(A\nc)--(E\nc)--(H\nc)--(G\nc)--node[left,text=black]{5}(C\nc)
(D\nc)-- (H\nc);
%
\def\nc{2VG}
\draw[LineG](A\nc)--node[above,text=black]{48} (B\nc)--
(C\nc)--(D\nc)--node[right,text=black,text opacity=1]{} (A\nc)
(A\nc)--(E\nc)--(H\nc)--(G\nc)--(C\nc)
(D\nc)--node[right,text=black,text opacity=1]{} (H\nc);
\end{scope}
\end{scope}
%%%%%%%%%%%%%%%%%%%%%%%
%%1 column
%%%%%%%%%%%%%%%%%%%%%%%
\begin{scope}[shift={(-9.0,-1.2)}]
%big cube
\begin{scope}
\FillCube{1VD}{2}{0.2}{4.55}
\end{scope}
%%small cube=down
\begin{scope}[shift={(-0.25,0.5)}]
\SmallCube{1MDI}{0.8}{0.15}{1.7}
%%
\draw[LineR](A\nc)-- (B\nc)--
(C\nc)--(D\nc)-- node[left=-2pt,text=black,pos=0.4]{11}(A\nc)
(A\nc)--(E\nc)--(H\nc)--(G\nc)--node[left=3pt,text=black,pos=0.9]{11}(C\nc)
(D\nc)-- (H\nc);
%
\def\nc{1VD}
\draw[LineG](A\nc)--node[below,text=black]{3} (B\nc)--
(C\nc)--(D\nc)--node[right,text=black,text opacity=1]{} (A\nc)
(A\nc)--node[below left,text=black]{224}(E\nc)--
node[left,text=black,text opacity=1]{224}(H\nc)--(G\nc)--(C\nc)
(D\nc)-- (H\nc);
\end{scope}
%%small cube=up
\begin{scope}[shift={(-0.75,3.4)}]
\SmallCube{1MDII}{0.8}{0.15}{1.7}
%%
\draw[LineR](A\nc)-- (B\nc)--
(C\nc)--(D\nc)-- node[left=-2pt,text=black,pos=0.4]{11}(A\nc)
(A\nc)--(E\nc)--(H\nc)--(G\nc)--node[left=3pt,text=black,pos=0.9]{11}(C\nc)
(D\nc)-- (H\nc);
%
\def\nc{1VD}
\draw[LineG](A\nc)--node[below,text=black]{3} (B\nc)--
(C\nc)--(D\nc)--node[right,text=black,text opacity=1]{} (A\nc)
(A\nc)--node[below left,text=black]{224}(E\nc)--
node[left,text=black,text opacity=1]{224}(H\nc)--(G\nc)--(C\nc)
(D\nc)-- (H\nc);
\end{scope}
\end{scope}
%%%%
\begin{scope}[shift={(8.15,0)}]
\begin{scope}
\FillCube{6VD}{0.8}{2.0}{2}
\path(A6VD)--node[below]{128}(B6VD);
\path(A6VD)--node[right]{13}(D6VD);
\path(D6VD)--node[right]{13}(H6VD);
\end{scope}
%up
\begin{scope}[shift={(0,3.5)}]
\FillCube{6VG}{0.8}{2.0}{2}
\path(A6VG)--node[below]{128}(B6VG);
\end{scope}
\end{scope}

\newcommand\Boxx[3]{
\node[draw,LineG,fill=green!10,rectangle,minimum width=7mm,minimum height=#2](#1){};
\node[below=2pt of #1]{#3};
}
\begin{scope}[shift={(11.7,1.0)}]
 \Boxx{B1D}{35mm}{2048}
\end{scope}
\begin{scope}[shift={(11.7,5.25)}]
 \Boxx{B1G}{35mm}{2048}
\end{scope}
\begin{scope}[shift={(13.5,1.0)}]
 \Boxx{B2D}{35mm}{2048}
\end{scope}
\begin{scope}[shift={(13.5,5.25)}]
 \Boxx{B2G}{35mm}{2048}
\end{scope}
\begin{scope}[shift={(15.0,1.0)}]
 \Boxx{B3}{19mm}{1000}
\end{scope}
%%%
\node[right=3pt of B1VD,align=center]{Stride\\ of 4};
\node[right=3pt of B2VD,align=center]{Max\\ pooling};
\node[right=3pt of B3VD,align=center]{Max\\ pooling};
\node[below=3pt of B6VD,align=center]{Max\\ pooling};
%
\coordinate(1C2)at($(A2VD)!0.4!(D2VD)$);
\foreach\i in{B,C,G}{
\draw[LineD](\i 1MDI)--(1C2);
}
\coordinate(2C2)at($(E2VG)!0.2!(H2VG)$);
\foreach\i in{B,C,G}{
\draw[LineD](\i 1MDII)--(2C2);
}
%3
\coordinate(1C3)at($(A3VD)!0.55!(H3VD)$);
\foreach\i in{B,C,G}{
\draw[LineD](\i 2MD)--(1C3);
}
\coordinate(2C3)at($(A3MGI)!0.35!(D3MGI)$);
\foreach\i in{B,C,G}{
\draw[LineD](\i 2MG)--(2C3);
}
%4
\coordinate(1C4)at($(A4VG)!0.15!(D4VG)$);
\foreach\i in{B,C,G}{
\draw[LineD](\i 3MGI)--(1C4);
}
\coordinate(2C4)at($(G4MD)!0.15!(H4MD)$);
\foreach\i in{B,C,G}{
\draw[LineD](\i 3MGII)--(2C4);
}
\coordinate(3C4)at($(A4MG)!0.5!(C4MG)$);
\foreach\i in{B,C,G}{
\draw[LineD](\i 3MDII)--(3C4);
}
\coordinate(3C4)at($(A4VD)!0.12!(D4VD)$);
\foreach\i in{B,C,G}{
\draw[LineD](\i 3MDI)--(3C4);
}
%5
\coordinate(1C5)at($(A5MG)!0.82!(H5MG)$);
\foreach\i in{B,C,G}{
\draw[LineD](\i 4MG)--(1C5);
}
\coordinate(2C5)at($(A5VD)!0.52!(C5VD)$);
\foreach\i in{B,C,G}{
\draw[LineD](\i 4MD)--(2C5);
}
%6
\coordinate(1C6)at($(A6VG)!0.52!(C6VG)$);
\foreach\i in{B,C,G}{
\draw[LineD](\i 5MG)--(1C6);
}
\coordinate(1C6)at($(D6VD)!0.3!(B6VD)$);
\foreach\i in{B,C,G}{
\draw[LineD](\i 5MD)--(1C6);
}
%
\draw[LineA]($(B6VD)!0.52!(C6VD)$)coordinate(X1)--
node[below]{dense}(X1-|B1D.north west);
\draw[LineA](B1D)--node[below]{dense}(B2D);
\draw[LineA](B2D)--(B3);
%
\draw[LineA]($(B6VG)!0.52!(C6VG)$)coordinate(X1)--(X1-|B1G.north west);
\draw[LineA]($(B6VG)!0.52!(C6VG)$)--(B1D);
\draw[LineA]($(B6VD)!0.52!(C6VD)$)--(B1G);
\draw[LineA](B1D)--(B2G);
\draw[LineA](B1G)--(B2D);
\draw[LineA](B2G)--node[right]{dense}(B3);
\draw[LineA]($(B1G.north east)!0.7!(B1G.south east)$)--($(B2G.north west)!0.7!(B2G.south west)$);
\end{tikzpicture}
```
**Convolutional Neural Network Architecture**: AlexNet demonstrated that deep neural networks could automatically learn effective features from images, dramatically outperforming traditional computer vision methods. This breakthrough showed that with sufficient data and computing power, neural networks could achieve remarkable accuracy in image recognition tasks.
:::

Deep learning subsequently entered an era of extraordinary scale. By the late 2010s, companies like Google, Facebook, and OpenAI trained neural networks thousands of times larger than AlexNet. These massive models, often called "foundation models"[^fn-intro-foundation-models], took deep learning to new heights.

[^fn-intro-foundation-models]: **Foundation Models**: Large-scale AI models trained on broad datasets that serve as the "foundation" for many different applications through fine-tuning—like GPT for language tasks or CLIP for vision tasks. The term was coined by Stanford's AI researchers in 2021 to capture how these models became the basis for building more specific AI systems.

GPT-3, released in 2020 [@brown2020language], contained 175 billion parameters (the adjustable components that determine the model's behavior)—orders of magnitude larger than earlier neural networks like AlexNet[^fn-parameters]—with training data encompassing vast text corpora enabling comprehensive pattern learning. These models showed remarkable abilities: writing human-like text, engaging in conversation, generating images from descriptions, and even writing computer code.

[^fn-parameters]: **Parameters**: The adjustable values within a neural network that are modified during training, similar to how the brain's neural connections grow stronger as you learn a new skill. Having more parameters generally means that the model can learn more complex patterns.

A key insight emerged: larger neural networks trained on more data became capable of solving increasingly complex tasks. This scale introduced significant systems challenges[^fn-training-challenges]. Efficiently training large models requires thousands of parallel GPUs, storing and serving models hundreds of gigabytes in size, and handling massive training datasets.

[^fn-training-challenges]: **Large-Scale Training Challenges**: Training GPT-3 required 3,640 petaflop-days of compute (equivalent to running 1,000 GPUs continuously for a year) and cost an estimated $4.6 million [@li2020estimating]. Modern foundation models can consume 100+ terabytes of training data and require specialized distributed training techniques to coordinate thousands of accelerators across multiple data centers.

The 2012 deep learning revolution built upon neural network research dating to the 1950s. The story begins with Frank Rosenblatt's Perceptron in 1957, which captured the imagination of researchers by showing how a simple artificial neuron could learn to classify patterns. Though limited to linearly separable problems—as Minsky and Papert's 1969 book "Perceptrons" [@minsky1969perceptrons] demonstrated—it introduced the fundamental concept of trainable neural networks. The 1980s brought more important breakthroughs: Rumelhart, Hinton, and Williams introduced backpropagation [@rumelhart1986learning][^fn-backprop-history] in 1986, providing a systematic way to train multi-layer networks, while Yann LeCun demonstrated its practical application in recognizing handwritten digits using specialized neural networks designed for image processing [@lecun1989backpropagation][^fn-cnn].

[^fn-backprop-history]: **Backpropagation (Historical Context)**: A mathematical technique that allows neural networks to learn by calculating how much each component contributed to errors and adjusting accordingly—like a coach analyzing a team's mistakes and giving each player specific feedback to improve their performance.

[^fn-cnn]: **Convolutional Neural Network (CNN)**: A type of neural network specially designed for processing images, inspired by how the human visual system works. The "convolutional" part refers to how it scans images in small chunks, similar to how our eyes focus on different parts of a scene.

These networks largely stagnated through the 1990s and 2000s not because the ideas were incorrect, but because they preceded necessary technological developments. The field lacked three important ingredients: sufficient data to train complex networks, enough computational power to process this data, and the technical innovations needed to train very deep networks effectively.

Deep learning's potential required the convergence of big data, advanced computing hardware, and algorithmic breakthroughs. This extended development period explains why the 2012 ImageNet breakthrough represented accumulated research culminating rather than sudden revolution. This evolution produced two significant developments. First, it established machine learning systems engineering as a discipline bridging theoretical advancements with practical implementation. Second, it necessitated comprehensive machine learning system definitions encompassing algorithms, data, and computing infrastructure. Today's challenges of scale echo many of the same fundamental questions about computation, data, and learning methods that researchers have grappled with since the field's inception, but now within a more complex and interconnected framework.

{{< margin-video "https://www.youtube.com/watch?v=FwFduRA_L6Q&ab_channel=YannLeCun" "Convolutional Network Demo from 1989" "Yann LeCun" >}}

As AI progressed from symbolic reasoning to statistical learning and deep learning, applications became increasingly ambitious and complex. This growth introduced challenges extending beyond algorithms, necessitating engineering entire systems capable of deploying and sustaining AI at scale—giving rise to Machine Learning Systems Engineering.

## The Bitter Lesson: Why Systems Matter {#sec-introduction-bitter-lesson-systems-matter-a8f2}

The evolution we've traced reveals a profound pattern that reinforcement learning pioneer Richard Sutton articulated in his influential 2019 essay "The Bitter Lesson" [@sutton2019bitter]. Sutton[^fn-sutton-turing] observed that the greatest breakthroughs in AI have consistently come not from incorporating human knowledge and expertise, but from scaling general-purpose methods that leverage massive computational resources.

[^fn-sutton-turing]: **Richard Sutton**: A foundational figure in reinforcement learning and AI, Sutton co-authored the seminal textbook "Reinforcement Learning: An Introduction" and developed key algorithms like temporal difference learning. He received the 2024 Turing Award—computing's highest honor—for his pioneering contributions to reinforcement learning that have fundamentally shaped how AI systems learn and adapt. His perspective on AI's development carries particular weight given his decades of contributions to both the theoretical foundations and practical advancement of machine learning.

This pattern appears repeatedly throughout AI history. In chess, IBM's Deep Blue defeated world champion Garry Kasparov in 1997 not by encoding sophisticated chess strategies, but through brute-force search evaluating millions of positions per second. In Go, DeepMind's AlphaGo [@silver2016mastering] achieved superhuman performance by learning from self-play rather than studying centuries of human Go wisdom. In computer vision, convolutional neural networks that learn features directly from data have surpassed decades of carefully hand-crafted feature engineering. In speech recognition, end-to-end deep learning systems have outperformed approaches built on detailed models of human phonetics and linguistics.

The "bitter" aspect of this lesson is that human expertise and domain knowledge, while providing short-term improvements, ultimately become obstacles to long-term progress. Sutton writes: "The biggest lesson that can be read from 70 years of AI research is that general methods that leverage computation are ultimately the most effective, and by a large margin."

This insight fundamentally explains why ML systems have become so critical. If the path to AI progress lies in scaling computation rather than encoding human knowledge, then our success depends entirely on our ability to build systems that can effectively harness computational resources. The algorithms matter, but the systems that can train models on massive datasets, deploy them at global scale, and continuously improve them through feedback loops matter even more.

Consider modern language models like GPT-4 or image generation systems like DALL-E. Their capabilities emerge not from sophisticated linguistic or artistic theories encoded by humans, but from training general-purpose neural networks on vast amounts of data using enormous computational resources. The engineering challenge is building systems that can manage this scale: collecting and processing petabytes[^fn-petabytes] of training data, coordinating training across thousands of GPUs, serving models to millions of users with millisecond latency, and continuously updating systems based on real-world performance. These large-scale training challenges are addressed in @sec-ai-training, while the infrastructure considerations for serving models at scale are covered in @sec-ai-acceleration and @sec-ai-frameworks.

[^fn-petabytes]: **Petabytes**: One million gigabytes, or enough storage for 500 billion pages of text. To put this in perspective, the entire printed collection of the U.S. Library of Congress is about 20 terabytes—a petabyte could store 50 copies. Modern AI training datasets like Common Crawl contain multiple petabytes of web text, representing essentially the entire public internet's text content.

Sutton's bitter lesson provides the fundamental motivation for this book. If AI progress depends on our ability to scale computation effectively, then understanding how to build, deploy, and maintain these computational systems becomes the most important skill for AI practitioners. The theoretical algorithms are important, but the systems that bring them to life at scale determine what's actually possible in practice.

## ML Systems Engineering {#sec-introduction-ml-systems-engineering-e9d8}

Sutton's bitter lesson reveals why a new engineering discipline has emerged. If AI progress depends on scaling computation rather than encoding human expertise, then building systems that can effectively harness computational resources becomes the critical bottleneck. This realization has fundamentally shifted how we approach AI development—from focusing primarily on algorithmic innovations to recognizing that the engineering of scalable systems often determines what's actually achievable in practice.

The emergence of this systems-focused approach mirrors a similar transition in computer science and engineering evolution in the late 1960s and early 1970s. As computing systems grew more complex, Computer Engineering[^fn-computer-engineering] emerged as a new discipline to address the growing complexity of integrating hardware and software systems. This field bridged the gap between Electrical Engineering's hardware expertise and Computer Science's focus on algorithms and software. Computer Engineering arose because the challenges of designing and building complex computing systems required an integrated approach that neither discipline could fully address on its own.

[^fn-computer-engineering]: **Computer Engineering**: This discipline emerged in the late 1960s when IBM System/360 and other complex computing systems required expertise that spanned both hardware and software. Before Computer Engineering, electrical engineers focused on circuits while computer scientists worked on algorithms, but no one specialized in the integration challenges. Today's Computer Engineering programs, established at schools like Case Western Reserve and Stanford in the 1970s, combine hardware design, software systems, and computer architecture—laying the groundwork for what ML Systems Engineering is becoming today.

A similar transition is occurring in AI. While Computer Science advances ML algorithms and Electrical Engineering develops specialized AI hardware, neither discipline fully addresses engineering principles needed to deploy, optimize, and sustain ML systems at scale. This gap necessitates a new discipline: Machine Learning Systems Engineering.

This field lacks universal definition but can be broadly characterized as:

::: {.callout-definition title="Machine Learning Systems Engineering"}
**Machine Learning Systems Engineering (MLSysEng)** is the engineering discipline focused on building *reliable*, *efficient*, and *scalable* AI systems across computational platforms, ranging from *embedded devices* to *data centers*. It spans the entire AI lifecycle, including *data acquisition*, *model development*, *system integration*, *deployment*, and *operations*, with an emphasis on *resource-awareness* and *system-level optimization*.
:::

Space exploration provides an apt analogy. Astronauts venture into new frontiers, but their discoveries depend on complex engineering systems: rockets providing lift, life support systems sustaining them, and communication networks maintaining Earth connectivity. Similarly, AI researchers advance learning algorithms, but breakthroughs become practical reality through careful systems engineering. Modern AI systems require robust infrastructure for data collection and management, powerful computing systems for model training, and reliable deployment platforms serving millions of users.

Machine learning systems engineering's emergence as an important discipline reflects a broader reality: converting AI algorithms into real-world systems requires bridging theoretical possibilities with practical implementation. A brilliant algorithm requires efficient data collection and processing, distributed computation across hundreds of machines, reliable service to millions of users, and production performance monitoring.

Understanding the interplay between algorithms and engineering is fundamental for modern AI practitioners. Researchers advance algorithmic possibilities while engineers address the complex challenge of reliable, efficient real-world implementation. This raises a fundamental question: what constitutes a machine learning system, and how does it differ from traditional software systems?

## Defining ML Systems {#sec-introduction-defining-ml-systems-bf7d}

No universally accepted definition of machine learning systems exists. This ambiguity stems from practitioners, researchers, and industries referring to machine learning systems in varying contexts with different scopes. Some focus solely on algorithmic aspects while others include the entire pipeline from data collection to model deployment. This loose usage reflects the field's rapidly evolving and multidisciplinary nature.

Given this diversity of perspectives, establishing a clear and comprehensive definition encompassing all aspects is important. This textbook adopts a holistic approach to machine learning systems, considering algorithms and the entire ecosystem in which they operate. We define a machine learning system as:

:::{.callout-definition title="Machine Learning System"}
A machine learning system is an integrated computing system comprising three core components: (1) data that guides algorithmic behavior, (2) learning algorithms that extract patterns from this data, and (3) computing infrastructure that enables both the learning process (i.e., training) and the application of learned knowledge (i.e., inference/serving). Together, these components create a computing system capable of making predictions, generating content, or taking actions based on learned patterns.
:::

Any machine learning system's core consists of three interrelated components illustrated in @fig-ai-triangle: Models/Algorithms, Data, and Computing Infrastructure. These components form a triangular dependency where each element fundamentally shapes the possibilities of the others. The model architecture dictates both the computational demands for training and inference, as well as the volume and structure of data required for effective learning. The data's scale and complexity influence what infrastructure is needed for storage and processing, while simultaneously determining which model architectures are feasible. The infrastructure capabilities establish practical limits on both model scale and data processing capacity, creating a framework within which the other components must operate.

::: {#fig-ai-triangle fig-env="figure" fig-pos="htb"}
```{.tikz}
\scalebox{0.8}{%
\begin{tikzpicture}[line join=round,font=\usefont{T1}{phv}{m}{n}\small]
\tikzset{
 Line/.style={line width=0.35pt,black!50,text=black},
 ALineA/.style={violet!80!black!50,line width=3pt,shorten <=2pt,shorten >=2pt,
  {Triangle[width=1.1*6pt,length=0.8*6pt]}-{Triangle[width=1.1*6pt,length=0.8*6pt]}},
LineD/.style={line width=0.75pt,black!50,text=black,dashed,dash pattern=on 5pt off 3pt},
Circle/.style={inner xsep=2pt,
  circle,
    draw=BrownLine,
    line width=0.75pt,
    fill=BrownL!40,
    minimum size=16mm
  },
 circles/.pic={
\pgfkeys{/channel/.cd, #1}
\node[circle,draw=\channelcolor,line width=\Linewidth,fill=\channelcolor!10,
minimum size=2.5mm](\picname){};
        }
}
\tikzset {
pics/cloud/.style = {
        code = {
\colorlet{red}{RedLine}
\begin{scope}[local bounding box=CLO,scale=0.5, every node/.append style={transform shape}]
\draw[red,fill=white,line width=0.9pt](0.67,1.21)to[out=55,in=90,distance=13](1.5,0.96)
to[out=360,in=30,distance=9](1.68,0.42);
\draw[red,fill=white,line width=0.9pt](0,0)to[out=170,in=180,distance=11](0.1,0.61)
to[out=90,in=105,distance=17](1.07,0.71)
to[out=20,in=75,distance=7](1.48,0.36)
to[out=350,in=0,distance=7](1.48,0)--(0,0);
\draw[red,fill=white,line width=0.9pt](0.27,0.71)to[bend left=25](0.49,0.96);

\end{scope}
    }
  }
}
%streaming
\tikzset{%
 LineST/.style={-{Circle[\channelcolor,fill=RedLine,length=4pt]},draw=\channelcolor,line width=\Linewidth,rounded corners},
 ellipseST/.style={fill=\channelcolor,ellipse,minimum width = 2.5mm, inner sep=2pt, minimum height =1.5mm},
 BoxST/.style={line width=\Linewidth,fill=white,draw=\channelcolor,rectangle,minimum width=56,
 minimum height=16,rounded corners=1.2pt},
 pics/streaming/.style = {
        code = {
        \pgfkeys{/channel/.cd, #1}
\begin{scope}[local bounding box=STREAMING,scale=\scalefac, every node/.append style={transform shape}]
\node[BoxST,minimum width=44,minimum height=48](\picname-RE1){};
\foreach \i/\j in{1/north,2/center,3/south}{
\node[BoxST](\picname-GR\i)at(\picname-RE1.\j){};
\node[ellipseST]at($(\picname-GR\i.west)!0.2!(\picname-GR\i.east)$){};
\node[ellipseST]at($(\picname-GR\i.west)!0.4!(\picname-GR\i.east)$){};
}
\draw[LineST](\picname-GR3)--++(2,0)coordinate(\picname-C4);
\draw[LineST](\picname-GR3.320)--++(0,-0.7)--++(0.8,0)coordinate(\picname-C5);
\draw[LineST](\picname-GR3.220)--++(0,-0.7)--++(-0.8,0)coordinate(\picname-C6);
\draw[LineST](\picname-GR3)--++(-2,0)coordinate(\picname-C7);
 \end{scope}
     }
  }
}
%data
\tikzset{mycylinder/.style={cylinder, shape border rotate=90, aspect=1.3, draw, fill=white,
minimum width=25mm,minimum height=11mm,line width=\Linewidth,node distance=-0.15},
pics/data/.style = {
        code = {
        \pgfkeys{/channel/.cd, #1}
\begin{scope}[local bounding box=STREAMING,scale=\scalefac, every node/.append style={transform shape}]
\node[mycylinder,fill=\channelcolor!50] (A) {};
\node[mycylinder, above=of A,fill=\channelcolor!30] (B) {};
\node[mycylinder, above=of B,fill=\channelcolor!10] (C) {};
 \end{scope}
     }
  }
}
\pgfkeys{
  /channel/.cd,
  channelcolor/.store in=\channelcolor,
  drawchannelcolor/.store in=\drawchannelcolor,
  scalefac/.store in=\scalefac,
  Linewidth/.store in=\Linewidth,
  picname/.store in=\picname,
  channelcolor=BrownLine,
  drawchannelcolor=BrownLine,
  scalefac=1,
  Linewidth=0.5pt,
  picname=C
}
\node[Circle](MO){};
\node[Circle,below left=1 and 2.5 of MO,draw=GreenLine,fill=GreenL!40,](IN){};
\node[Circle,below right=1 and 2.5 of MO,draw=OrangeLine,fill=OrangeL!40,](DA){};
\draw[ALineA](MO)--(IN);
\draw[ALineA](MO)--(DA);
\draw[ALineA](DA)--(IN);
\node[below=2pt of MO]{Model};
\node[below=2pt of IN]{Infra};
\node[below=2pt of DA]{Data};
%%
\begin{scope}[local bounding box=CIRCLE1,shift={($(MO)+(0.04,-0.24)$)},
scale=0.55, every node/.append style={transform shape}]
%1 column
\foreach \j in {1,2,3} {
  \pgfmathsetmacro{\y}{(1.5-\j)*0.43 + 0.7}
  \pic at (-0.8,\y) {circles={channelcolor=RedLine,picname=1CD\j}};
}
%2 column
\foreach \i in {1,...,4} {
  \pgfmathsetmacro{\y}{(2-\i)*0.43+0.7}
  \pic at (0,\y) {circles={channelcolor=RedLine, picname=2CD\i}};
}
%3 column
\foreach \j in {1,2} {
  \pgfmathsetmacro{\y}{(1-\j)*0.43 + 0.7}
  \pic at (0.8,\y) {circles={channelcolor=RedLine,picname=3CD\j}};
}
\foreach \i in {1,2,3}{
  \foreach \j in {1,2,3,4}{
\draw[Line](1CD\i)--(2CD\j);
}}
\foreach \i in {1,2,3,4}{
  \foreach \j in {1,2}{
\draw[Line](2CD\i)--(3CD\j);
}}
\end{scope}
%
\pic[shift={(-0.4,-0.08)}] at (IN) {cloud};
%
\pic[shift={(-0.05,-0.13)}] at  (IN){streaming={scalefac=0.25,picname=2,channelcolor=RedLine, Linewidth=0.65pt}};
%
\pic[shift={(0,-0.3)}] at  (DA){data={scalefac=0.3,picname=1,channelcolor=green!70!black, Linewidth=0.4pt}};
\end{tikzpicture}}
```
**Component Interdependencies**: Machine learning system performance relies on the coordinated interaction of models, data, and computing infrastructure; limitations in any one component constrain the capabilities of the others. Effective system design requires balancing these interdependencies to optimize overall performance and feasibility.
:::

Each of these components serves a distinct but interconnected purpose:

- **Algorithms**: Mathematical models and methods that learn patterns from data to make predictions or decisions

- **Data**: Processes and infrastructure for collecting, storing, processing, managing, and serving data for both training and inference.

- **Computing**: Hardware and software infrastructure that enables efficient training, serving, and operation of models at scale.

The interdependency of these components means no single element can function in isolation. The most sophisticated algorithm cannot learn without data or computing resources to run on. The largest datasets are useless without algorithms to extract patterns or infrastructure to process them. And the most powerful computing infrastructure serves no purpose without algorithms to execute or data to process.

Space exploration provides an apt analogy for these relationships. Algorithm developers resemble astronauts exploring new frontiers and making discoveries. Data science teams function like mission control specialists ensuring constant flow of critical information and resources for mission operations. Computing infrastructure engineers resemble rocket engineers designing and building systems that enable missions. Just as space missions require seamless integration of astronauts, mission control, and rocket systems, machine learning systems demand careful orchestration of algorithms, data, and computing infrastructure.

The 2012 AlexNet breakthrough illustrates how hardware and software evolve together in ML systems. This deep learning revolution required not just algorithmic innovation but also GPU architectures capable of executing massive parallel matrix operations efficiently. Without NVIDIA's CUDA-enabled GPUs[^fn-cuda], the computational demands would have remained prohibitively expensive, potentially delaying the deep learning era by years. This interdependence between algorithms and hardware infrastructure continues to shape ML system development, from cloud deployments leveraging specialized accelerators to edge devices requiring model compression to fit memory constraints.

[^fn-cuda]: **CUDA (Compute Unified Device Architecture)**: NVIDIA's parallel computing platform introduced in 2007 that transformed gaming graphics cards into general-purpose computing powerhouses. CUDA enabled developers to harness GPU's thousands of cores for AI computations, providing 10-100x speedups over traditional CPUs for machine learning tasks. This breakthrough made deep learning practically feasible, as training a neural network that would take months on a CPU could be completed in days on CUDA-enabled hardware.

## Lifecycle of ML Systems {#sec-introduction-lifecycle-ml-systems-6194}

Traditional software systems follow predictable lifecycles where developers write explicit computer instructions. These systems build on decades of established software engineering practices. Version control systems maintain precise histories of code changes. Continuous integration and deployment pipelines automate testing and release processes. Static analysis tools measure code quality and identify potential issues. This infrastructure enables reliable software system development, testing, and deployment following well-defined software engineering principles.

Machine learning systems fundamentally depart from this traditional paradigm. Traditional systems execute explicit programming logic while machine learning systems derive behavior from data patterns. This shift from code to data as the primary behavior driver introduces new complexities. The specialized development workflows needed to manage these data-driven systems are explored in @sec-ai-workflow.

@fig-ml_lifecycle_overview illustrates the ML lifecycle's interconnected stages from data collection through model monitoring, with feedback loops for continuous improvement when performance degrades or models require enhancement.

::: {#fig-ml_lifecycle_overview fig-env="figure" fig-pos="htb"}
```{.tikz}
\begin{tikzpicture}[font=\small\usefont{T1}{phv}{m}{n}]
\tikzset{
  Box/.style={inner xsep=2pt,
  draw=GreenLine,
    line width=0.75pt,
    fill=GreenL,
    anchor=west,
    text width=20mm,align=flush center,
    minimum width=20mm, minimum height=8mm
  },
 Line/.style={line width=1.0pt,black!50,text=black,-{Triangle[width=0.8*6pt,length=0.98*6pt]}},
  Text/.style={inner sep=4pt,
    draw=none, line width=0.75pt,
    fill=TextColor!70,
    font=\fontsize{8pt}{9}\selectfont\usefont{T1}{phv}{m}{n},
    align=flush center,
    minimum width=7mm, minimum height=5mm
  },
}

\node[Box](B1){ Data\\ Preparation};
\node[Box,node distance=15mm,right=of B1,fill=RedL,draw=RedLine](B2){Model\\ Evaluation};
\node[Box,node distance=32mm,right=of B2,fill=VioletL,draw=VioletLine](B3){Model \\ Deployment};
\node[Box,node distance=9mm,above=of $(B1)!0.5!(B2)$,
fill=BackColor!60!yellow!90,draw=BackLine](GB){Model\\ Training};
\node[Box,node distance=9mm,below left=1.1 and 0 of B1.south west,
fill=BlueL,draw=BlueLine](DB1){Data\\ Collection};
\node[Box,node distance=9mm,below right=1.1 and 0 of B3.south east,
fill=OrangeL,draw=OrangeLine](DB2){Model \\Monitoring};
\draw[Line](B2)--node[Text,pos=0.5]{Meets\\ Requirements}(B3);
\draw[Line](B2)--++(270:1.2)-|node[Text,pos=0.25]{Needs\\ Improvement}(B1);
\draw[Line](DB2)--node[Text,pos=0.25]{Performance\\Degrades}(DB1);
\draw[Line](DB1)|-(B1);
\draw[Line](B1)|-(GB);
\draw[Line](GB)-|(B2);
\draw[Line](B3)-|(DB2);
\end{tikzpicture}
```
**ML System Lifecycle**: Continuous iteration defines successful machine learning systems, requiring feedback loops to refine models and address performance degradation across data collection, model training, evaluation, and deployment. This cyclical process contrasts with traditional software development and emphasizes the importance of ongoing monitoring and adaptation to maintain system reliability and accuracy in dynamic environments.
:::

Unlike source code changing only through developer modifications, data reflects real-world dynamics. Data distribution changes can silently alter system behavior. Traditional software engineering tools designed for deterministic code-based systems prove insufficient for managing data-dependent systems. Version control systems excelling at tracking discrete code changes struggle with large, evolving datasets. Testing frameworks designed for deterministic outputs require adaptation for probabilistic predictions. The specialized data engineering practices needed to address these challenges are covered in @sec-data-engineering, while operational approaches for monitoring data-dependent systems are explored in @sec-ml-operations. This data-dependent nature creates dynamic lifecycles requiring continuous monitoring and adaptation to maintain system relevance as real-world data patterns evolve.

Understanding the machine learning system lifecycle requires examining distinct stages. Each stage presents unique requirements from learning and infrastructure perspectives. This dual consideration of learning needs and systems support is critical for building effective machine learning systems.

ML lifecycle stages in production are deeply interconnected rather than isolated. This interconnectedness creates virtuous or vicious cycles. In virtuous cycles, high-quality data enables effective learning, robust infrastructure supports efficient processing, and well-engineered systems facilitate better data collection. In vicious cycles, poor data quality undermines learning, inadequate infrastructure hampers processing, and system limitations prevent data collection improvements—each problem compounds others.

## ML Systems in the Wild {#sec-introduction-ml-systems-wild-8f2f}

Managing machine learning systems' complexity becomes apparent when considering the broad deployment spectrum. ML systems exist at vastly different scales and in diverse environments, each presenting unique challenges and constraints.

At one spectrum end, cloud-based ML systems run in massive data centers[^fn-data-centers]. These systems, including large language models and recommendation engines, process petabytes of data while serving millions of users simultaneously. They leverage virtually unlimited computing resources but manage enormous operational complexity and costs. The architectural approaches for building such large-scale systems are covered in @sec-ml-systems and @sec-ai-acceleration.

[^fn-data-centers]: **Data Centers**: Massive facilities housing thousands of servers, often consuming 100-300 megawatts of power—equivalent to a small city. Google operates over 20 data centers globally, each one costing $1-2 billion to build. These facilities maintain temperatures of exactly 80°F (27°C) with backup power systems that can run for days, enabling the reliable operation of AI services used by billions of people worldwide.

At the other end, TinyML systems run on microcontrollers[^fn-microcontrollers] and embedded devices, performing ML tasks with severe memory, computing power, and energy consumption constraints. Smart home devices like Alexa or Google Assistant must recognize voice commands using less power than LED bulbs, while sensors must detect anomalies on battery power for months or years. The specialized techniques for deploying ML on such constrained devices are explored in @sec-efficient-ai and @sec-model-optimizations, while the unique challenges of embedded ML systems are covered in @sec-ondevice-learning.

[^fn-microcontrollers]: **Microcontrollers**: Tiny computers-on-a-chip costing under $1 each, with just kilobytes of memory—about 1/millionth the memory of a smartphone. Popular chips like the Arduino Uno have only 32KB of storage and 2KB of RAM, yet can run simple AI models that classify sensor data, recognize voice commands, or detect movement patterns while consuming less power than a digital watch.

Between these extremes lies a rich variety of ML systems adapted for different contexts. Edge ML systems bring computation closer to data sources, reducing latency[^fn-latency] and bandwidth requirements while managing local computing resources. Mobile ML systems must balance sophisticated capabilities with battery life and processor limitations on smartphones and tablets. Enterprise ML systems often operate within specific business constraints, focusing on particular tasks while integrating with existing infrastructure. Some organizations employ hybrid approaches, distributing ML capabilities across multiple tiers to balance various requirements.

[^fn-latency]: **Latency**: The time delay between when a request is made and when a response is received. In ML systems, this is critical—autonomous vehicles need <10ms latency for safety decisions, while voice assistants target <100ms for natural conversation. For comparison, sending data to a distant cloud server typically adds 50-100ms, which is why edge computing became essential for real-time AI applications.

## ML Systems Impact on Lifecycle {#sec-introduction-ml-systems-impact-lifecycle-fb60}

The diversity of ML systems across the spectrum represents a complex interplay of requirements, constraints, and trade-offs. These decisions fundamentally impact every stage of the ML lifecycle we discussed earlier, from data collection to continuous operation.

Performance requirements often drive initial architectural decisions. Latency-sensitive applications, like autonomous vehicles or real-time fraud detection, might require edge or embedded architectures despite their resource constraints. Conversely, applications requiring massive computational power for training, such as large language models, naturally gravitate toward centralized cloud architectures. However, raw performance is just one consideration in a complex decision space.

Resource management varies dramatically across architectures. Cloud systems must optimize for cost efficiency at scale—balancing expensive GPU clusters, storage systems, and network bandwidth. Edge systems face fixed resource limits and must carefully manage local compute and storage. Mobile and embedded systems operate under the strictest constraints, where every byte of memory and milliwatt of power matters. These resource considerations directly influence both model design and system architecture.

Operational complexity increases with system distribution. While centralized cloud architectures benefit from mature deployment tools and managed services, edge and hybrid systems must handle the complexity of distributed system management[^fn-distributed-systems]. This complexity manifests throughout the ML lifecycle—from data collection and version control to model deployment and monitoring. This operational complexity can compound over time if not carefully managed.

[^fn-distributed-systems]: **Distributed Systems**: Computing systems where components located on networked computers communicate and coordinate their actions. In ML, this might mean training a single model across 1,000+ GPUs in different server racks, or deploying models across thousands of edge devices globally. The complexity comes from handling network failures, coordinating updates, ensuring data consistency, and managing the "Two Generals' Problem"—confirming that all parts of the system agree on the current state.

Data considerations often introduce competing pressures. Privacy requirements or data sovereignty regulations might push toward edge or embedded architectures, while the need for large-scale training data might favor cloud approaches. The velocity and volume of data also influence architectural choices—real-time sensor data might require edge processing to manage bandwidth, while batch analytics might be better suited to cloud processing.

Evolution and maintenance requirements must be considered from the start. Cloud architectures offer flexibility for system evolution but can incur significant ongoing costs. Edge and embedded systems might be harder to update but could offer lower operational overhead. The continuous cycle of ML systems we discussed earlier becomes particularly challenging in distributed architectures, where updating models and maintaining system health requires careful orchestration across multiple tiers.

These trade-offs are rarely simple binary choices. Modern ML systems often adopt hybrid approaches, carefully balancing these considerations based on specific use cases and constraints. The key is understanding how these decisions will impact the system throughout its lifecycle, from initial development through continuous operation and evolution.

### Emerging Trends {#sec-introduction-emerging-trends-e527}

The landscape of machine learning systems is evolving rapidly, with innovations happening from user-facing applications down to core infrastructure. These changes are reshaping how we design and deploy ML systems.

#### Application-Level Innovation {#sec-introduction-applicationlevel-innovation-e97b}

The rise of agentic systems marks a profound shift from traditional reactive ML systems that simply made predictions based on input data. Modern applications can now take actions, learn from outcomes, and adapt their behavior accordingly through multi-agent systems[^fn-mas] and advanced planning algorithms. These autonomous agents can plan, reason, and execute complex tasks, introducing new requirements for decision-making frameworks and safety constraints.

[^fn-mas]: **Multi-Agent System**: A computational system where multiple intelligent agents interact within an environment, each pursuing their own objectives while potentially cooperating or competing with other agents.

This increased sophistication extends to operational intelligence. Applications will likely incorporate sophisticated self-monitoring, automated resource management, and adaptive deployment strategies. They can automatically handle data distribution shifts, model updates, and system optimization, marking a significant advance in autonomous operation.

#### System Architecture Evolution {#sec-introduction-system-architecture-evolution-c8cc}

Supporting these advanced applications requires fundamental changes in the underlying system architecture. Integration frameworks are evolving to handle increasingly complex interactions between ML systems and broader technology ecosystems. Modern ML systems must seamlessly connect with existing software, process diverse data sources, and operate across organizational boundaries, driving new approaches to system design.

Resource efficiency has become a central architectural concern as ML systems scale. Innovation in model compression and efficient training techniques is being driven by both environmental and economic factors. Future architectures must carefully balance the pursuit of more powerful models against growing sustainability concerns.

At the infrastructure level, new hardware is reshaping deployment possibilities. Specialized AI accelerators are emerging across the spectrum—from powerful data center chips to efficient edge processors[^fn-edge] to tiny neural processing units in mobile devices. This heterogeneous computing landscape enables dynamic model distribution across tiers based on computing capabilities and conditions, blurring traditional boundaries between cloud, edge, and embedded systems.

[^fn-edge]: **Edge Processor**: A specialized computing device designed to perform AI computations close to where data is generated, optimized for low latency and energy efficiency rather than raw computing power.

These trends are creating ML systems that are more capable and efficient while managing increasing complexity. Success in this evolving landscape requires understanding how application requirements flow down to infrastructure decisions, ensuring systems can grow sustainably while delivering increasingly sophisticated capabilities.

## Practical Applications {#sec-introduction-practical-applications-0728}

The diverse architectures and scales of ML systems demonstrate their potential to revolutionize industries. By examining real-world applications, we can see how these systems address practical challenges and drive innovation. Their ability to operate effectively across varying scales and environments has already led to significant changes in numerous sectors. This section highlights examples where theoretical concepts and practical considerations converge to produce tangible, impactful results.

### FarmBeats: ML in Agriculture {#sec-introduction-farmbeats-ml-agriculture-1ec9}

[FarmBeats](https://www.microsoft.com/en-us/research/project/farmbeats-iot-agriculture/), a project developed by Microsoft Research, shown in @fig-farmbeats-overview is a significant advancement in the application of machine learning to agriculture. This system aims to increase farm productivity and reduce costs by leveraging AI and IoT technologies. FarmBeats exemplifies how edge and embedded ML systems can be deployed in challenging, real-world environments to solve practical problems. By bringing ML capabilities directly to the farm, FarmBeats demonstrates the potential of distributed AI systems in transforming traditional industries.

![**Edge-Based Agricultural System**: FarmBeats leverages IoT devices and edge computing to collect and process real-time data on soil conditions, microclimate, and plant health, enabling data-driven decision-making for optimized resource allocation and increased crop yields. This distributed architecture minimizes reliance on cloud connectivity, reducing latency and improving responsiveness in remote or bandwidth-constrained agricultural environments.](./images/png/farmbeats.png){#fig-farmbeats-overview width=95%}

#### Data Considerations {#sec-introduction-data-considerations-0dfa}

The data ecosystem in FarmBeats is diverse and distributed. Sensors deployed across fields collect real-time data on soil moisture, temperature, and nutrient levels. Drones equipped with multispectral cameras capture high-resolution imagery of crops, providing insights into plant health and growth patterns. Weather stations contribute local climate data, while historical farming records offer context for long-term trends. The challenge lies not just in collecting this heterogeneous data, but in managing its flow from dispersed, often remote locations with limited connectivity. FarmBeats employs innovative data transmission techniques, such as using TV white spaces (unused broadcasting frequencies) to extend internet connectivity to far-flung sensors. This approach to data collection and transmission embodies the principles of edge computing we discussed earlier, where data processing begins at the source to reduce bandwidth requirements and enable real-time decision making. The systematic approaches to managing such distributed data pipelines are covered in @sec-data-engineering.

#### Algorithmic Considerations {#sec-introduction-algorithmic-considerations-d292}

FarmBeats uses a variety of ML algorithms tailored to agricultural applications. For soil moisture prediction, it uses temporal neural networks that can capture the complex dynamics of water movement in soil. Image analysis algorithms process drone imagery to detect crop stress, pest infestations, and yield estimates. These models must be robust to noisy data and capable of operating with limited computational resources. Machine learning methods such as transfer learning[^fn-intro-transfer-learning] allow models to learn on data-rich farms to be adapted for use in areas with limited historical data. The training techniques that enable such adaptation, including transfer learning and domain adaptation, are explored in @sec-ai-training.

[^fn-intro-transfer-learning]: **Transfer Learning**: A machine learning technique where a model developed for one task is reused as the starting point for a model on a related task, significantly reducing the amount of training data and computation required—particularly valuable in domains like agriculture where labeled data may be scarce.

#### Infrastructure Considerations {#sec-introduction-infrastructure-considerations-346b}

FarmBeats exemplifies the edge computing paradigm we explored in our discussion of the ML system spectrum. At the lowest level, embedded ML models run directly on IoT devices and sensors, performing basic data filtering and anomaly detection. Edge devices, such as ruggedized field gateways, aggregate data from multiple sensors and run more complex models for local decision-making. These edge devices operate in challenging conditions, requiring robust hardware designs and efficient power management to function reliably in remote agricultural settings. The system employs a hierarchical architecture, with more computationally intensive tasks offloaded to on-premises servers or the cloud. This tiered approach allows FarmBeats to balance the need for real-time processing with the benefits of centralized data analysis and model training. The infrastructure also includes mechanisms for over-the-air model updates, ensuring that edge devices can receive improved models as more data becomes available and algorithms are refined. The operational practices for managing such distributed model updates are covered in @sec-ml-operations and @sec-ondevice-learning.

#### Future Implications {#sec-introduction-future-implications-8871}

FarmBeats shows how ML systems can be deployed in resource-constrained, real-world environments to drive significant improvements in traditional industries. By providing farmers with AI-driven insights, the system has shown potential to increase crop yields, reduce water usage, and optimize resource allocation. Looking forward, the FarmBeats approach could be extended to address global challenges in food security and sustainable agriculture. The success of this system also highlights the growing importance of edge and embedded ML in IoT applications, where bringing intelligence closer to the data source can lead to more responsive, efficient, and scalable solutions. As edge computing capabilities continue to advance, we can expect to see similar distributed ML architectures applied to other domains, from smart cities to environmental monitoring.

However, realizing this vision requires navigating significant deployment challenges that reveal how the three components of ML systems—data, algorithms, and infrastructure—must be carefully balanced. Connectivity failures in rural areas force the system to make algorithmic trade-offs, buffering data locally and running simpler models when cloud resources are unavailable. Harsh environmental conditions that damage sensors directly impact data quality, requiring robust data validation algorithms to filter unreliable measurements. The infrastructure costs and maintenance overhead illustrate the hidden complexity of deploying ML systems beyond controlled laboratory environments.

### AlphaFold: Scientific ML {#sec-introduction-alphafold-scientific-ml-d3fd}

[AlphaFold](https://deepmind.google/technologies/alphafold/) [@jumper2021highly], developed by DeepMind, is a landmark achievement in the application of machine learning to complex scientific problems. This AI system is designed to predict the three-dimensional structure of proteins, as shown in @fig-alphafold-overview, from their amino acid sequences, a challenge known as the "protein folding problem" that has puzzled scientists for decades. AlphaFold's success demonstrates how large-scale ML systems can accelerate scientific discovery and potentially revolutionize fields like structural biology and drug design. This case study exemplifies the use of advanced ML techniques and massive computational resources to tackle problems at the frontiers of science.

::: {.content-visible when-format="html"}
![**AlphaFold**: Protein targets that AlphaFold can predict solely from amino acid sequences, showcasing its prowess in tackling the protein folding problem.](images/gif/alphafold.gif){#fig-alphafold-overview}
:::

::: {.content-visible when-format="pdf"}
![Examples of protein targets within the free modeling category. Source: Google DeepMind.](images/png/alphafold.png){#fig-alphafold-overview}
:::

#### Data Considerations {#sec-introduction-data-considerations-e519}

The data underpinning AlphaFold's success is vast and multifaceted. The primary dataset is the Protein Data Bank (PDB), which contains the experimentally determined structures of over 180,000 proteins. This is complemented by databases of protein sequences, which number in the hundreds of millions. AlphaFold also utilizes evolutionary data in the form of multiple sequence alignments (MSAs), which provide insights into the conservation patterns of amino acids across related proteins. The challenge lies not just in the volume of data, but in its quality and representation. Experimental protein structures can contain errors or be incomplete, requiring sophisticated data cleaning and validation processes. The representation of protein structures and sequences in a form amenable to machine learning is a significant challenge in itself. AlphaFold's data pipeline involves complex preprocessing steps to convert raw sequence and structural data into meaningful features that capture the physical and chemical properties relevant to protein folding.

#### Algorithmic Considerations {#sec-introduction-algorithmic-considerations-c152}

AlphaFold's algorithmic approach represents a tour de force in the application of deep learning to scientific problems. At its core, AlphaFold uses a novel neural network architecture that combines with techniques from computational biology. The model learns to predict how protein components relate to each other in 3D space, using advanced neural network techniques specifically designed for scientific data. The learning process involves multiple stages, including initial "pretraining" on a large corpus of protein sequences, followed by fine-tuning on known structures. AlphaFold also incorporates domain knowledge in the form of physics-based constraints and scoring functions, creating a hybrid system that leverages both data-driven learning and scientific prior knowledge. The model's ability to generate accurate confidence estimates for its predictions is crucial, allowing researchers to assess the reliability of the predicted structures.

#### Infrastructure Considerations {#sec-introduction-infrastructure-considerations-fcd1}

The computational demands of AlphaFold epitomize the challenges of large-scale scientific ML systems. Training the model requires massive parallel computing resources, leveraging clusters of GPUs or specialized AI chips (TPUs)[^fn-intro-tpu] in a distributed computing environment. DeepMind utilized Google's cloud infrastructure, with the final version of AlphaFold trained on 128 TPUv3 cores for several weeks. The distributed training techniques and hardware acceleration strategies that enable such large-scale training are covered in @sec-ai-training and @sec-ai-acceleration.

[^fn-intro-tpu]: **Tensor Processing Unit (TPU)**: A specialized AI accelerator chip designed by Google specifically for neural network machine learning, particularly efficient at matrix operations common in deep learning workloads.

#### Future Implications {#sec-introduction-future-implications-b549}

AlphaFold's impact on structural biology has been profound, with the potential to accelerate research in areas ranging from fundamental biology to drug discovery. By providing accurate structural predictions for proteins that have resisted experimental methods, AlphaFold opens new avenues for understanding disease mechanisms and designing targeted therapies. The success of AlphaFold also serves as a powerful demonstration of how ML can be applied to other complex scientific problems, potentially leading to breakthroughs in fields like materials science or climate modeling. However, it also raises important questions about the role of AI in scientific discovery and the changing nature of scientific inquiry in the age of large-scale ML systems. As we look to the future, the AlphaFold approach suggests a new paradigm for scientific ML, where massive computational resources are combined with domain-specific knowledge to push the boundaries of human understanding.

Yet this paradigm shift also illustrates the infrastructure-algorithm trade-offs fundamental to ML systems engineering. The massive computational requirements—over $100,000 in training costs—demonstrate how algorithmic sophistication demands proportional infrastructure investment. The system's data dependencies reveal another core ML systems principle: model performance degrades when deployment data differs from training data, as seen with understudied protein families. This exemplifies why successful ML systems require coordinated optimization across all three components, not just algorithmic advancement.

### Autonomous Vehicles {#sec-introduction-autonomous-vehicles-2910}

[Waymo](https://waymo.com/), a subsidiary of Alphabet Inc., stands at the forefront of autonomous vehicle technology, representing one of the most ambitious applications of machine learning systems to date. Evolving from the Google Self-Driving Car Project initiated in 2009, Waymo's approach to autonomous driving exemplifies how ML systems can span the entire spectrum from embedded systems to cloud infrastructure. This case study demonstrates the practical implementation of complex ML systems in a safety-critical, real-world environment, integrating real-time decision-making with long-term learning and adaptation.

#### Data Considerations {#sec-introduction-data-considerations-8b4d}

The data ecosystem underpinning Waymo's technology is vast and dynamic. Each vehicle serves as a roving data center, its sensor suite, which comprises LiDAR, radar, and high-resolution cameras, generating approximately one terabyte of data per hour of driving. This real-world data is complemented by an even more extensive simulated dataset, with Waymo's vehicles having traversed over 20 billion miles in simulation and more than 20 million miles on public roads. The challenge lies not just in the volume of data, but in its heterogeneity and the need for real-time processing. Waymo must handle both structured (e.g., GPS coordinates) and unstructured data (e.g., camera images) simultaneously. The data pipeline spans from edge processing on the vehicle itself to massive cloud-based storage and processing systems. Sophisticated data cleaning and validation processes are necessary, given the safety-critical nature of the application. The representation of the vehicle's environment in a form amenable to machine learning presents significant challenges, requiring complex preprocessing to convert raw sensor data into meaningful features that capture the dynamics of traffic scenarios.

#### Algorithmic Considerations {#sec-introduction-algorithmic-considerations-a0ae}

Waymo's ML stack represents a sophisticated ensemble of algorithms tailored to the multifaceted challenge of autonomous driving. The perception system employs specialized neural networks to process visual data for object detection and tracking. Prediction models, needed for anticipating the behavior of other road users, use neural networks that can understand patterns over time[^fn-rnn] in road user behavior. The architectural patterns for building such complex multi-model systems are explored in @sec-dnn-architectures and @sec-ai-frameworks. Waymo has developed custom ML models like VectorNet for predicting vehicle trajectories. The planning and decision-making systems may incorporate learning-from-experience techniques to handle complex traffic scenarios.

[^fn-rnn]: **Sequential Neural Networks**: Neural network architectures designed to process data that occurs in sequences over time, such as predicting where a pedestrian will move next based on their previous movements. These networks maintain a form of "memory" of previous inputs to inform current decisions.

#### Infrastructure Considerations {#sec-introduction-infrastructure-considerations-3779}

The computing infrastructure supporting Waymo's autonomous vehicles epitomizes the challenges of deploying ML systems across the full spectrum from edge to cloud. Each vehicle is equipped with a custom-designed compute platform capable of processing sensor data and making decisions in real-time, often leveraging specialized hardware like GPUs or tensor processing units (TPUs). This edge computing is complemented by extensive use of cloud infrastructure, leveraging the power of Google's data centers for training models, running large-scale simulations, and performing fleet-wide learning. The specialized hardware architectures and edge-cloud coordination strategies that enable such systems are covered in @sec-ai-acceleration and @sec-ml-systems. The connectivity between these tiers is critical, with vehicles requiring reliable, high-bandwidth communication for real-time updates and data uploading. Waymo's infrastructure must be designed for robustness and fault tolerance, ensuring safe operation even in the face of hardware failures or network disruptions. The scale of Waymo's operation presents significant challenges in data management, model deployment, and system monitoring across a geographically distributed fleet of vehicles.

#### Future Implications {#sec-introduction-future-implications-2934}

Waymo's impact extends beyond technological advancement, potentially revolutionizing transportation, urban planning, and numerous aspects of daily life. The launch of Waymo One, a commercial ride-hailing service using autonomous vehicles in Phoenix, Arizona, represents a significant milestone in the practical deployment of AI systems in safety-critical applications. Waymo's progress has broader implications for the development of robust, real-world AI systems, driving innovations in sensor technology, edge computing, and AI safety that have applications far beyond the automotive industry. However, it also raises important questions about liability, ethics, and the interaction between AI systems and human society. As Waymo continues to expand its operations and explore applications in trucking and last-mile delivery, it serves as an important test bed for advanced ML systems, driving progress in areas such as continual learning, robust perception, and human-AI interaction. The Waymo case study underscores both the tremendous potential of ML systems to transform industries and the complex challenges involved in deploying AI in the real world.

\medskip
## Challenges in ML Systems {#sec-introduction-challenges-ml-systems-7167}

The pattern evident across our case studies—from FarmBeats' rural connectivity issues to AlphaFold's computational requirements to Waymo's safety validation complexity—reflects deeper challenges inherent to building and deploying machine learning systems. These challenges distinguish ML engineering from traditional software development and help explain why creating effective ML systems requires understanding data, algorithms, and infrastructure simultaneously.

### Data-Related Challenges {#sec-introduction-datarelated-challenges-9e99}

The foundation of any ML system is its data, and managing this data introduces several fundamental challenges. First, there's the basic question of data quality, as real-world data is often messy and inconsistent. Imagine a healthcare application that needs to process patient records from different hospitals. Each hospital might record information differently, use different units of measurement, or have different standards for what data to collect. Some records might have missing information, while others might contain errors or inconsistencies that need to be cleaned up before the data can be useful.

As ML systems grow, they often need to handle increasingly large amounts of data. A video streaming service like Netflix, for example, needs to process billions of viewer interactions to power its recommendation system. This scale introduces new challenges in how to store, process, and manage such large datasets efficiently.

Another critical challenge is how data changes over time. This phenomenon, known as "data drift"[^fn-drift], occurs when the patterns in new data begin to differ from the patterns the system originally learned from. For example, many predictive models struggled during the COVID-19 pandemic because consumer behavior changed so dramatically that historical patterns became less relevant. ML systems need ways to detect when this happens and adapt accordingly.

[^fn-drift]: **Data Drift**: The gradual change in the statistical properties of the target variable (what the model is trying to predict) over time, which can degrade model performance if not properly monitored and addressed.

### Model-Related Challenges {#sec-introduction-modelrelated-challenges-6c1a}

Creating and maintaining the ML models themselves presents another set of challenges. Modern ML models, particularly in deep learning, can be extremely complex. Consider a language model like GPT-3, which has hundreds of billions of parameters that need to be optimized through optimization processes[^fn-backprop]. This complexity creates practical challenges: these models require enormous computing power to train and run, making it difficult to deploy them in situations with limited resources, like on mobile phones or IoT devices.

[^fn-backprop]: **Backpropagation**: The primary algorithm used to train neural networks, which calculates how each parameter in the network should be adjusted to minimize prediction errors by propagating error gradients backward through the network layers.

Training these models effectively is itself a significant challenge. Unlike traditional programming where we write explicit instructions, ML models learn from examples[^fn-transfer]. This learning process involves many choices: How should we structure the model? How long should we train it? How can we tell if it's learning the right things? Making these decisions often requires both technical expertise and considerable trial and error. The systematic approaches to these training challenges are explored in detail in @sec-ai-training, while the frameworks that support efficient model development are covered in @sec-ai-frameworks.

[^fn-transfer]: **Transfer Learning**: A machine learning method where a model developed for one task is reused as the starting point for a model on a second task, significantly reducing the amount of training data and computation required.

A particularly important challenge is ensuring that models work well in real-world conditions. A model might perform excellently on its training data but fail when faced with slightly different situations in the real world. This gap between training performance and real-world performance is a central challenge in machine learning, especially for critical applications like autonomous vehicles or medical diagnosis systems.

### System-Related Challenges {#sec-introduction-systemrelated-challenges-fb4f}

Getting ML systems to work reliably in the real world introduces its own set of challenges. Unlike traditional software that follows fixed rules, ML systems need to handle uncertainty and variability in their inputs and outputs. They also typically need both training systems (for learning from data) and serving systems (for making predictions), each with different requirements and constraints.

Consider a company building a speech recognition system. They need infrastructure to collect and store audio data, systems to train models on this data, and then separate systems to actually process users' speech in real-time. Each part of this pipeline needs to work reliably and efficiently, and all the parts need to work together seamlessly. The engineering principles for building such robust data pipelines are covered comprehensively in @sec-data-engineering, while the operational practices for maintaining these systems in production are explored in @sec-ml-operations.

These systems also need constant monitoring and updating. How do we know if the system is working correctly? How do we update models without interrupting service? How do we handle errors or unexpected inputs? These operational challenges become particularly complex when ML systems are serving millions of users.

### Ethical Considerations {#sec-introduction-ethical-considerations-c579}

As ML systems become more prevalent in our daily lives, their broader impacts on society become increasingly important to consider. One major concern is fairness, as ML systems can sometimes learn to make decisions that discriminate against certain groups of people. This often happens unintentionally, as the systems pick up biases present in their training data. For example, a job application screening system might inadvertently learn to favor certain demographics if those groups were historically more likely to be hired.

Another important consideration is transparency. Many modern ML models, particularly deep learning models, work as "black boxes"[^fn-black-box]---while they can make predictions, it's often difficult to understand how they arrived at their decisions.

[^fn-black-box]: **Black Box**: A system where you can observe the inputs and outputs but cannot see or understand the internal workings—like how a radio receives signals and produces sound without most users understanding the electronics inside. In AI, this opacity becomes problematic when the system makes important decisions affecting people's lives.

This becomes particularly problematic when ML systems are making important decisions about people's lives, such as in healthcare or financial services.

Privacy is also a major concern. ML systems often need large amounts of data to work effectively, but this data might contain sensitive personal information. How do we balance the need for data with the need to protect individual privacy? How do we ensure that models don't inadvertently memorize and reveal private information through inference attacks[^fn-inference]? These challenges aren't merely technical problems to be solved, but ongoing considerations that shape how we approach ML system design and deployment. The approaches for addressing these critical concerns are covered in @sec-responsible-ai, @sec-security-privacy, and @sec-robust-ai.

[^fn-inference]: **Inference Attack**: A technique where an adversary attempts to extract sensitive information about the training data by making careful queries to a trained model, exploiting patterns the model may have inadvertently memorized during training.

These challenges aren't merely technical problems to be solved, but ongoing considerations that shape how we approach ML system design and deployment. Throughout this book, we'll explore these challenges in detail and examine strategies for addressing them effectively.

## Looking Ahead {#sec-introduction-looking-ahead-34a3}

As we look to the future of machine learning systems, several exciting trends are shaping the field. These developments promise to both solve existing challenges and open new possibilities for what ML systems can achieve.

One of the most significant trends is the democratization of AI technology. Just as personal computers transformed computing from specialized mainframes to everyday tools, ML systems are becoming more accessible to developers and organizations of all sizes. Cloud providers now offer pre-trained models and automated ML platforms that reduce the expertise needed to deploy AI solutions. This democratization is enabling new applications across industries, from small businesses using AI for customer service to researchers applying ML to previously intractable problems.

As concerns about computational costs and environmental impact grow, there's an increasing focus on making ML systems more efficient. Researchers are developing new techniques for training models with less data and computing power. Innovation in specialized hardware, from improved GPUs to custom AI chips, is making ML systems faster and more energy-efficient. These efficiency considerations are explored in @sec-efficient-ai and @sec-sustainable-ai, while the hardware innovations enabling more efficient AI are covered in @sec-ai-acceleration.

Perhaps the most transformative trend is the development of more autonomous ML systems that can adapt and improve themselves. These systems are beginning to handle their own maintenance tasks, such as detecting when they need retraining, automatically finding and correcting errors, and optimizing their own performance. This automation could dramatically reduce the operational overhead of running ML systems while improving their reliability. The operational frameworks that enable such self-managing systems are explored in @sec-ml-operations, while the emerging techniques for autonomous adaptation are discussed in @sec-agi-systems.

While these trends are promising, it's important to recognize the field's limitations. Creating truly artificial general intelligence remains a distant goal. Current ML systems excel at specific tasks but lack the flexibility and understanding that humans take for granted. Challenges around bias, transparency, and privacy continue to require careful consideration. As ML systems become more prevalent, addressing these limitations while leveraging new capabilities will be crucial. These fundamental challenges and emerging solutions are explored in @sec-responsible-ai, @sec-robust-ai, and @sec-agi-systems.

## Book Structure and Learning Path {#sec-introduction-book-structure-learning-path-f3ea}

This book addresses the engineering challenges we've explored through a systematic framework organized around five fundamental disciplines that encompass the complete ML systems lifecycle. These disciplines reflect the core engineering capabilities required to bridge the gap between research prototypes and production systems capable of operating at scale.

As illustrated in @fig-pillars, the five pillars central to the framework are:

![**ML System Lifecycle**: Machine learning systems engineering encompasses five interconnected disciplines that address the real-world challenges of building, deploying, and maintaining AI systems at scale. Each pillar represents critical engineering capabilities needed to bridge the gap between research prototypes and production systems.](images/png/book_pillars.png){#fig-pillars}

The five engineering disciplines illustrated in @fig-pillars address the systematic challenges of ML systems development:

- **Data** (@sec-data-engineering): Engineering robust data pipelines that ensure quality, handle scale, and maintain privacy while providing the foundational infrastructure upon which all ML systems depend.

- **Training** (@sec-ai-training): Developing efficient training systems that can manage large datasets and complex models while optimizing computational resource utilization across distributed environments.

- **Deployment** (@sec-ml-operations, @sec-ondevice-learning): Building reliable deployment infrastructure that can serve models at scale, handle failures gracefully, and adapt to evolving requirements in production environments.

- **Operations** (@sec-ml-operations, @sec-benchmarking-ai): Creating monitoring and maintenance systems that ensure continued performance, enable early issue detection, and support safe system updates in production.

- **Ethics & Governance** (@sec-responsible-ai, @sec-security-privacy, @sec-sustainable-ai): Implementing responsible AI practices that address bias, ensure transparency, and protect user privacy throughout the system lifecycle.

Each pillar represents a critical phase in the lifecycle of ML systems and is composed of foundational elements that build upon each other. This structure ensures a comprehensive understanding of MLSE, from basic principles to advanced applications and ethical considerations.

For more detailed information about the book's overview, contents, learning outcomes, target audience, prerequisites, and navigation guide—including how to use the cross-reference system that connects topics throughout the book—please refer to the [About the Book](../../frontmatter/about/about.qmd) section. There, you'll also find valuable details about our learning community and how to maximize your experience with this resource.

The chapters that follow build systematically on these foundations: @sec-ml-systems explores the architectural principles for different ML deployment scenarios, @sec-dl-primer and @sec-dnn-architectures provide the algorithmic foundations, while the subsequent sections cover each pillar of the ML systems engineering discipline in detail.
