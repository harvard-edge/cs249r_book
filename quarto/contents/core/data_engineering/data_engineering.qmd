---
bibliography: data_engineering.bib
quiz: data_engineering_quizzes.json
concepts: data_engineering_concepts.yml
glossary: data_engineering_glossary.json
crossrefs: data_engineering_xrefs.json
---

# Data Engineering {#sec-data-engineering}

::: {layout-narrow}
::: {.column-margin}
_DALLÂ·E 3 Prompt: Create a rectangular illustration visualizing the concept of data engineering. Include elements such as raw data sources, data processing pipelines, storage systems, and refined datasets. Show how raw data is transformed through cleaning, processing, and storage to become valuable information that can be analyzed and used for decision-making._
:::

\noindent
![](images/png/cover_data_engineering.png)

:::

## Purpose {.unnumbered}

_Why do successful machine learning systems depend more on data engineering excellence than algorithmic sophistication?_

In machine learning systems, data quality determines success more than algorithmic sophistication. Poor data leads to unreliable models regardless of architectural complexity, while well-engineered data pipelines enable simpler models to achieve superior results. Data engineering encompasses the complete lifecycle: sourcing reliable datasets, ensuring label accuracy, managing storage formats, implementing quality validation, and maintaining governance standards. These engineering decisions directly impact model accuracy, system reliability, computational requirements, and regulatory compliance. Data engineering principles enable the construction of ML systems that perform consistently in production environments.

::: {.callout-tip title="Learning Objectives"}

* Analyze different data sourcing methods (datasets, web scraping, crowdsourcing, synthetic data).

* Explain the importance of data labeling and ensure label quality.

* Evaluate data storage systems for ML workloads (databases, data warehouses, data lakes).

* Describe the role of data pipelines in ML systems.

* Explain the importance of data governance in ML (security, privacy, ethics).

* Identify key challenges in data engineering for ML.

:::

## Data Engineering Systems Framework {#sec-data-engineering-systems-framework}

Data engineering provides a systematic approach to building reliable, scalable, and maintainable data infrastructure for machine learning systems. This chapter presents data engineering as a coherent engineering discipline organized around four foundational pillars: **Quality**, **Reliability**, **Scalability**, and **Governance**. These pillars work together to ensure that data systems can support the full lifecycle of ML applications, from initial problem definition through production deployment.

::: {.callout-definition title="Definition of Data Engineering"}

**Data Engineering** is the _systematic discipline_ of designing, building, and maintaining data infrastructure that transforms raw information into reliable, accessible, and ML-ready datasets. This discipline encompasses _data acquisition, processing, storage, and governance_, ensuring systems are _scalable, secure, and aligned_ with both technical requirements and business objectives. Data engineering focuses on _creating stable foundations_ that enable successful machine learning systems through principled data management.

:::

### The Four Pillars of Data Engineering Systems

Every data engineering decision, from choosing storage formats to designing ingestion pipelines, should be evaluated against these four foundational principles. Each pillar contributes to system success through systematic decision-making:

**Quality Foundation**: Data quality determines system success more than algorithmic sophistication. The concept of "Data Cascades," introduced by @sambasivan2021everyone, demonstrates how quality issues compound throughout the ML lifecycle. When IBM Watson Health's oncology recommendations came under scrutiny in 2019 for producing potentially unsafe cancer treatment recommendations [@strickland2019ibm], investigations revealed systematic data quality failures as root causes that cascaded through the entire system[^fn-watson-health]. Quality encompasses accuracy, completeness, consistency, and fitness for the intended ML task. High-quality data is essential for model success, with the mathematical foundations of this relationship explored in @sec-dl-primer and @sec-dnn-architectures.

[^fn-watson-health]: **IBM Watson Health**: IBM's AI health initiative, which accumulated approximately $4 billion in investment over its lifetime, was sold to Francisco Partners in 2022 after failing to deliver promised breakthroughs due to core data quality issues and over-hyped capabilities.

**Reliability Infrastructure**: ML systems require consistent, predictable data processing that handles failures gracefully. Reliability means building systems that continue operating despite component failures, data anomalies, or unexpected load patterns. This includes implementing proper error handling, monitoring, and recovery mechanisms throughout the data pipeline.

**Scalability Architecture**: While reliability ensures consistent operation, scalability addresses the challenge of growth. As ML systems grow from prototypes to production services, data volumes and processing requirements increase dramatically. Scalability involves designing systems that can handle growing data volumes, user bases, and computational demands without requiring complete system redesigns.

**Governance Principles**: Finally, governance provides the framework within which quality, reliability, and scalability operate. Data governance ensures systems operate within legal, ethical, and business constraints while maintaining transparency and accountability. This includes privacy protection, bias mitigation, regulatory compliance, and establishing clear data ownership and access controls.

### Integrating the Pillars Through Systems Thinking

While understanding each pillar individually provides important insights, recognizing their individual importance is only the first step toward effective data engineering. These four pillars are not independent components but interconnected aspects of a unified system where decisions in one area cascade through all others. Quality improvements must consider scalability constraints, reliability requirements affect governance implementations, and governance policies influence quality metrics. This systems perspective guides the organization of this chapter, where each technical topic is examined through the lens of how it supports and balances these foundational principles while managing their inherent tensions.

As @fig-ds-time illustrates, data scientists spend up to 60% of their time on data preparation tasks [@kaggle2021state][^fn-data-quality-stats]. This statistic reflects the current state where data engineering practices are often ad-hoc rather than systematic. By applying the four-pillar framework consistently, teams can reduce this overhead while building more reliable and maintainable systems.

[^fn-data-quality-stats]: **Data Quality Reality**: The famous "garbage in, garbage out" principle was first coined by IBM computer programmer George Fuechsel in the 1960s, describing how flawed input data produces nonsense output. This principle remains critically relevant in modern ML systems.

::: {#fig-ds-time fig-env="figure" fig-pos="htb"}
```{.tikz}
\scalebox{0.8}{%
\begin{tikzpicture}[line join=round,font=\small\usefont{T1}{phv}{m}{n}]
\makeatletter
\def\pgfpie@legend#1{%
  \coordinate[xshift=15mm,
  yshift={(\the\pgfpie@sliceLength*0.5+1)*0.5cm}] (pgfpie@legendpos) at
  (current bounding box.east);

\scope[node distance=2.25mm]
    \foreach \pgfpie@p/\pgfpie@t [count=\pgfpie@i from 0] in {#1}
    {
      \pgfpie@findColor{\pgfpie@i}
      \node[circle,draw, fill={\pgfpie@thecolor}, draw=none,inner sep=5 pt,below =1.6mm of {pgfpie@legendpos},
      label={[font=\footnotesize\usefont{T1}{phv}{m}{n}]0:{\pgfpie@t}}] (pgfpie@legendpos) {};
    }
  \endscope
}
\makeatother
\definecolor{Greenn}{RGB}{84,180,53}
\definecolor{Redd}{RGB}{249,56,39}
\definecolor{Orangee}{RGB}{255,157,35}
\definecolor{Brownn}{RGB}{214,128,96}
\definecolor{Bluee}{RGB}{0,97,168}
\definecolor{Violett}{RGB}{178,108,186}
\definecolor{Yelloww}{RGB}{255,210,76}
\tikzset{lines/.style={
  draw=none,
  line width=0.75pt
}}

\pie[text=legend,radius=2.65,
     style={lines},
     color={Greenn!60, Redd!90, Orangee, Bluee!80, Yelloww, Violett},
     every slice/.style={draw=blue}
     ]
{60/Cleaning and organizing data,
19/Collecting data sets,
9/Mining data for patterns,
5/Building training sets,
4/Refining algorithms,
3/Other}
\end{tikzpicture}}
```
**Data Scientist Time Allocation**: Data preparation consumes a majority of data science effort, up to 60%, underscoring the need for systematic data engineering practices to prevent downstream model failures and ensure project success. Prioritizing data quality and pipeline development yields greater returns than solely focusing on advanced algorithms. Source: Various industry reports.
:::

### Framework Application Throughout the Data Lifecycle

This four-pillar framework guides our exploration of data engineering systems from problem definition through production operations. We begin by establishing clear problem definitions and governance principles that shape all subsequent technical decisions. The framework then guides us through data acquisition strategies, where quality and reliability requirements determine how we source and validate data. Processing and storage decisions follow naturally from scalability and governance constraints, while operational practices ensure all four pillars are maintained throughout the system lifecycle.

Building on this foundation, each major section of this chapter demonstrates how the four pillars work together in practice. Data sourcing techniques must balance quality requirements with scalability constraints. Processing pipelines must ensure reliability while meeting governance standards. Storage architectures must support both performance needs and privacy requirements. This integrated approach transforms data engineering from a collection of ad-hoc techniques into a systematic engineering discipline.

To make these abstract principles concrete, we will illustrate them through a detailed case study: building a Keyword Spotting (KWS) system for voice-activated devices. This example demonstrates how the four-pillar framework guides real-world engineering decisions, from initial data collection through production deployment, showing how quality, reliability, scalability, and governance requirements shape every aspect of system design.

## Problem Definition and Governance Foundations {#sec-data-engineering-problem-definition-f820}

Building on the four-pillar framework established above, clear problem definitions and governance principles must guide all subsequent engineering decisions. These foundational choices determine system architecture and operational characteristics throughout the ML lifecycle.

The importance of this foundational work becomes clear when examining "Data Cascades," the phenomenon identified by @sambasivan2021everyone where quality issues compound throughout ML systems, leading to model failures, project termination, and potential harm to users. These cascades occur when teams skip establishing clear quality criteria, reliability requirements, and governance principles before beginning data collection and processing work.

@fig-cascades illustrates these potential data pitfalls at every stage and how they influence the entire process down the line. The influence of data collection errors is especially pronounced. As illustrated in the figure, any lapses in this initial stage will become apparent at later stages (in model evaluation and deployment) and might lead to costly consequences, such as abandoning the entire model and restarting anew. Therefore, investing in data engineering techniques from the onset will help us detect errors early, mitigating these cascading effects.

::: {#fig-cascades fig-env="figure" fig-pos="htb"}
```{.tikz}
\begin{tikzpicture}[line join=round,font=\small\usefont{T1}{phv}{m}{n}]
\definecolor{Green}{RGB}{84,180,53}
\definecolor{Red}{RGB}{249,56,39}
\definecolor{Orange}{RGB}{255,157,35}
\definecolor{Blue}{RGB}{0,97,168}
\definecolor{Violet}{RGB}{178,108,186}

\tikzset{%
Line/.style={line width=1.0pt,black!50,shorten <=6pt,shorten >=8pt},
LineD/.style={line width=2.0pt,black!50,shorten <=6pt,shorten >=8pt},
Text/.style={rotate=60,align=right,anchor=north east,font=\footnotesize\usefont{T1}{phv}{m}{n}},
Text2/.style={align=left,anchor=north west,font=\footnotesize\usefont{T1}{phv}{m}{n},text depth=0.7}
}

\draw[line width=1.5pt,black!30](0,0)coordinate(P)--(10,0)coordinate(K);

\foreach \i in {0,...,6} {
\path let \n1 = {(\i/6)*10} in coordinate (P\i) at (\n1,0);
\fill[black] (P\i) circle (2pt);
  }

\draw[LineD,Red](P0)to[out=60,in=120](P6);
\draw[LineD,Red](P0)to[out=60,in=125](P5);
\draw[LineD,Blue](P1)to[out=60,in=120](P6);
\draw[LineD,Red](P1)to[out=50,in=125](P6);
\draw[LineD,Blue](P4)to[out=60,in=125](P6);
\draw[LineD,Blue](P3)to[out=60,in=120](P6);
%
\draw[Line,Orange](P1)to[out=44,in=132](P6);
\draw[Line,Green](P1)to[out=38,in=135](P6);
\draw[Line,Orange](P1)to[out=30,in=135](P5);
\draw[Line,Green](P1)to[out=36,in=130](P5);
%
\draw[Line,Orange](P2)to[out=40,in=135](P6);
\draw[Line,Orange](P2)to[out=40,in=135](P5);
%
\draw[draw=none,fill=VioletLine!50]($(P5)+(-0.1,0.15)$)to[bend left=10]($(P5)+(-0.1,0.61)$)--
                ($(P5)+(-0.25,0.50)$)--($(P5)+(-0.85,1.20)$)to[bend left=20]($(P5)+(-1.38,0.76)$)--
                ($(P5)+(-0.51,0.33)$)to[bend left=10]($(P5)+(-0.64,0.22)$)to[bend left=10]cycle;
\draw[draw=none,fill=VioletLine!50]($(P6)+(-0.1,0.15)$)to[bend left=10]($(P6)+(-0.1,0.61)$)--
                ($(P6)+(-0.25,0.50)$)--($(P6)+(-0.7,1.30)$)to[bend left=20]($(P6)+(-1.38,0.70)$)--
                ($(P6)+(-0.51,0.33)$)to[bend left=10]($(P6)+(-0.64,0.22)$)to[bend left=10]cycle;
%
\draw[dashed,red,thick,-latex](P1)--++(90:2)to[out=90,in=0](0.8,2.7);
\draw[dashed,red,thick,-latex](P6)--++(90:2)to[out=90,in=0](9.1,2.7);
\node[below=0.1of P0,Text]{Problem\\ Statement};
\node[below=0.1of P1,Text]{Data collection \\and labeling};
\node[below=0.1of P2,Text]{Data analysis\\ and cleaning};
\node[below=0.1of P3,Text]{Model \\selection};
\node[below=0.1of P4,Text]{Model\\ training};
\node[below=0.1of P5,Text]{Model\\ evaluation};
\node[below=0.1of P6,Text]{Model\\ deployment};
%Legend
\node[circle,minimum size=4pt,fill=Blue](L1)at(11.5,2.6){};
\node[above right=0.1 and 0.1of L1,Text2]{Interacting with physical\\  world brittleness};

\node[circle,minimum size=4pt,fill=Red,below =0.5 of L1](L2){};
\node[above right=0.1 and 0.1of L2,Text2]{Inadequate \\application-domain expertise};

\node[circle,minimum size=4pt,fill=Green,below =0.5 of L2](L3){};
\node[above right=0.1 and 0.1of L3,Text2]{Conflicting reward\\ systems};

\node[circle,minimum size=4pt,fill=Orange,below =0.5 of L3](L4){};
\node[above right=0.1 and 0.1of L4,Text2]{Poor cross-organizational\\ documentation};

\draw[-{Triangle[width=8pt,length=8pt]}, line width=3pt,Violet](11.4,-0.85)--++(0:0.8)coordinate(L5);
\node[above right=0.23 and 0of L5,Text2]{Impacts of cascades};

\draw[-{Triangle[width=4pt,length=8pt]}, line width=2pt,Red,dashed](11.4,-1.35)--++(0:0.8)coordinate(L6);
\node[above right=0.23 and 0of L6,Text2]{Abandon / re-start process};
\end{tikzpicture}
```
**Data Quality Cascades**: Errors introduced early in the machine learning workflow amplify across subsequent stages, increasing costs and potentially leading to flawed predictions or harmful outcomes. Recognizing these cascades motivates proactive investment in data engineering and quality control to mitigate risks and ensure reliable system performance. Source: [@sambasivan2021everyone].
:::

### Governance Principles for Data Engineering

With this understanding of how quality issues cascade through ML systems, we must establish governance principles that ensure our data engineering systems operate within ethical, legal, and business constraints. These principles are not afterthoughts to be applied later but foundational requirements that shape every technical decision from the outset.

Central to these governance principles, data systems must protect user privacy and maintain security throughout their lifecycle. This means implementing access controls, encryption, and data minimization practices from the initial system design, not adding them as later enhancements. Privacy requirements directly influence data collection methods, storage architectures, and processing approaches.

Beyond privacy protection, data engineering systems must actively work to identify and mitigate bias in data collection, labeling, and processing. This requires diverse data collection strategies, representative sampling approaches, and systematic bias detection throughout the pipeline. Technical choices about data sources, labeling methodologies, and quality metrics all impact system fairness.

Complementing these fairness efforts, systems must maintain clear documentation about data sources, processing decisions, and quality criteria. This includes implementing data lineage tracking, maintaining processing logs, and establishing clear ownership and responsibility for data quality decisions.

Finally, data systems must comply with relevant regulations such as GDPR, CCPA, and domain-specific requirements. Compliance requirements influence data retention policies, user consent mechanisms, and cross-border data transfer protocols.

These governance principles work hand-in-hand with our technical pillars of quality, reliability, and scalability. A system cannot be truly reliable if it violates user privacy, and quality metrics are meaningless if they perpetuate unfair outcomes.

### Systematic Problem Definition Process

Building on these governance foundations, we need a systematic approach to problem definition. As @sculley2015hidden emphasize, ML systems require problem framing that goes beyond traditional software development approaches. Whether developing recommendation engines processing millions of user interactions, computer vision systems analyzing medical images, or natural language models handling diverse text data, each system brings unique challenges that must be carefully considered within our governance and technical framework.

Within this context, establishing clear objectives provides unified direction that guides the entire project, from data collection strategies through deployment operations. These objectives must balance technical performance with governance requirements, creating measurable outcomes that include both accuracy metrics and fairness criteria.

This systematic approach to problem definition ensures that governance principles and technical requirements are integrated from the start rather than retrofitted later. To achieve this integration, we identify the key steps that must precede any data collection effort:

1. Identify and clearly state the problem definition
2. Set clear objectives to meet
3. Establish success benchmarks
4. Understand end-user engagement/use
5. Understand the constraints and limitations of deployment
6. Perform data collection.
7. Iterate and refine.

### Applying the Framework: Keyword Spotting Case Study

To demonstrate how these systematic principles work in practice, Keyword Spotting (KWS) systems provide an ideal case study for applying our four-pillar framework to real-world data engineering challenges. These systems, which power voice-activated devices like smartphones and smart speakers, must detect specific wake words (such as "OK, Google" or "Alexa") within continuous audio streams while operating under strict resource constraints.

As shown in @fig-keywords, KWS systems operate as lightweight, always-on front-ends that trigger more complex voice processing systems. This deployment context creates specific requirements for each of our four pillars:

![**Keyword Spotting System**: A typical deployment of keyword spotting (KWS) technology in a voice-activated device, where a constantly-listening system detects a wake word to initiate further processing. this example demonstrates how KWS serves as a lightweight, always-on front-end for more complex voice interfaces.](images/png/data_engineering_kws.png){#fig-keywords width=55%}

**Quality Challenges**: KWS systems must achieve high accuracy across diverse acoustic environments, speaker accents, and background noise conditions. Quality requirements include distinguishing target keywords from similar-sounding words, handling whispered commands and shouted instructions with equal precision, and maintaining performance across demographic and linguistic diversity.

**Reliability Requirements**: Building on these quality demands, these systems operate continuously on battery-powered devices, requiring consistent performance over extended periods. Reliability encompasses graceful handling of audio quality variations, consistent behavior across device wake/sleep cycles, and robust operation despite limited computational resources.

**Scalability Constraints**: Beyond reliability considerations, KWS models must operate on low-power microcontrollers with severe memory and computational limitations, while potentially serving millions of devices worldwide. Scalability challenges include model size optimization, efficient inference algorithms, and distributed deployment across diverse hardware platforms.

**Governance Complexities**: Overlaying these technical constraints, always-listening systems raise significant privacy concerns, requiring data minimization, local processing, and transparent user controls. Governance requirements include protecting recorded voice data, ensuring fair performance across demographic groups, and complying with privacy regulations across global markets.

These interconnected challenges become evident in the current landscape: many KWS systems support only a limited number of languages, primarily due to the governance and scalability challenges of collecting high-quality, representative voice data for smaller linguistic populations. This limitation demonstrates how all four pillars must work together to achieve successful deployment.

With this framework understanding established, we can apply the systematic problem definition process to our KWS example, demonstrating how the four pillars guide practical engineering decisions:

1. **Identifying the Problem**: KWS detects specific keywords amidst ambient sounds and other spoken words. The primary problem is to design a system that can recognize these keywords with high accuracy, low latency, and minimal false positives or negatives, especially when deployed on devices with limited computational resources. A well-specified problem definition for developing a new KWS model should identify the desired keywords along with the envisioned application and deployment scenario.

2. **Setting Clear Objectives**: The objectives for a KWS system might include:
   * Achieving a specific accuracy rate (e.g., 98% accuracy in keyword detection).
   * Ensuring low latency (e.g., keyword detection and response within 200 milliseconds).
   * Minimizing power consumption to extend battery life on embedded devices.
   * Ensuring the model's size is optimized for the available memory on the device.

3. **Benchmarks for Success**: Establish clear metrics to measure the success of the KWS system. This could include:
   * *True Positive Rate:* The percentage of correctly identified keywords relative to all spoken keywords.
   * *False Positive Rate:* The percentage of non-keywords (including silence, background noise, and out-of-vocabulary words) incorrectly identified as keywords.
   * *Detection/Error Tradeoff* These curves evaluate KWS on streaming audio representative of a real-world deployment scenario, by comparing the number of false accepts per hour (the number of false positives over the total duration of the evaluation audio) against the false rejection rate (the number of missed keywords relative to the number of spoken keywords in the evaluation audio). @nayak2022improving provides one example of this.
   * *Response Time:* The time taken from keyword utterance to system response.
   * *Power Consumption:* Average power used during keyword detection.

4. **Stakeholder Engagement and Understanding**: Engage with stakeholders, which include device manufacturers, hardware and software developers, and end-users. Understand their needs, capabilities, and constraints. For instance:
   * Device manufacturers might prioritize low power consumption.
   * Software developers might emphasize ease of integration.
   * End-users would prioritize accuracy and responsiveness.

5. **Understanding the Constraints and Limitations of Embedded Systems**: Embedded devices come with their own set of challenges:
   * *Memory Limitations:* KWS models must be lightweight to fit within the memory constraints of embedded devices. Typically, KWS models need to be as small as 16 KB to fit in the always-on island of the SoC. This represents only the model size. Application code for preprocessing must also fit within the memory constraints.
   * Processing Power: The computational capabilities of embedded devices are limited (a few hundred MHz of clock speed), so the KWS model must be optimized for efficiency.
   * *Power Consumption:* Since many embedded devices are battery-powered, the KWS system must be power-efficient.
   * *Environmental Challenges:* Devices might be deployed in various environments, from quiet bedrooms to noisy industrial settings. The KWS system must function effectively across these scenarios.

6. **Data Collection and Analysis**: For a KWS system, data quality and diversity determine success. Considerations include:
   * *Demographics:* Collect data from speakers with various accents across age and gender to ensure wide-ranging recognition support.
   * *Keyword Variations:* People might pronounce keywords differently or express slight variations in the wake word itself. Ensure the dataset captures these nuances.
   * *Background Noises:* Include or augment data samples with different ambient noises to train the model for real-world scenarios.

7. **Iterative Feedback and Refinement**: Finally, once a prototype KWS system is developed, teams must ensure the system remains aligned with the defined problem and objectives as deployment scenarios change over time and use-cases evolve.

   * Test it in real-world scenarios
   * Gather feedback - are some users or deployment scenarios encountering underperformance relative to others?
   * Iteratively refine the dataset and model

This comprehensive KWS example illustrates the broader principles of problem definition, showing how initial decisions about data requirements ripple throughout a project's lifecycle. By carefully considering each aspect, from core problem identification, through performance benchmarks, to deployment constraints, teams can build a strong foundation for their ML systems. The methodical problem definition process provides a framework applicable across the ML spectrum. Whether developing computer vision systems for medical diagnostics, recommendation engines processing millions of user interactions, or natural language models analyzing diverse text corpora, this structured approach helps teams anticipate and plan for their data needs.

Having established our problem definition and governance principles, we now turn to the systematic implementation of these requirements through data pipeline architecture. Pipelines translate our four-pillar framework into operational reality, ensuring that quality, reliability, scalability, and governance principles are maintained throughout the data transformation process.

## Data Pipeline Architecture {#sec-data-engineering-pipeline-basics-31ba}

Data pipelines serve as the systematic implementation of our four-pillar framework, transforming raw data into ML-ready formats while maintaining quality, reliability, scalability, and governance standards. Rather than simple linear data flows, these are complex systems that must orchestrate multiple data sources, transformation processes, and storage systems while ensuring consistent performance under varying load conditions.

To illustrate these concepts, our KWS system pipeline architecture must handle continuous audio streams, maintain low-latency processing for real-time keyword detection, and ensure privacy-preserving data handling. The pipeline must scale from development environments processing sample audio files to production deployments handling millions of concurrent audio streams while maintaining strict quality and governance standards.

::: {#fig-pipeline-flow fig-env="figure" fig-pos="htb"}
```{.tikz}
\resizebox{.7\textwidth}{!}{%
\begin{tikzpicture}[font=\small\usefont{T1}{phv}{m}{n}]
%
\tikzset{%
 Line/.style={line width=1.0pt,black!50,text=black},
 Box/.style={align=flush center,
    inner xsep=2pt,
    node distance=0.8,
    draw=GreenLine,
    line width=0.75pt,
    fill=GreenL,
    text width=27mm,
    minimum width=26mm, minimum height=9mm
  },
}
%
\begin{scope}[local bounding box = scope1]
\node[Box](B1){Raw Data Sources};
\node[Box,right=of B1](B2){External APIs};
\node[Box,right=of B2](B3){Streaming Sources};
\end{scope}
%
\begin{scope}[shift={($(scope1.south)+(-2.84,-2.2)$)},anchor=center]
\node[Box, fill=BlueL,draw=BlueLine](2B1){Batch Ingestion};
\node[Box, fill=BlueL,draw=BlueLine,  node distance=2.8,right=of 2B1](2B2){Stream Processing};
\end{scope}
%
\node[Box,  node distance=1.2,below=of $(2B1)!0.5!(2B2)$](3B1){Storage Layer};
%
\node[Box, fill=OrangeL,draw=OrangeLine,below left=1 and 0.2 of 3B1](4B1){Training Data};
\node[Box, fill=RedL,draw=RedLine,node distance=1.3,below right=1 and 0.2of 3B1](4B2){Data Validation \& Quality Checks};
\node[Box, fill=OrangeL,draw=OrangeLine, node distance=0.6,below =of 4B1](5B1){Model Training};
\node[Box,fill=RedL,draw=RedLine,node distance=0.6,below =of 4B2](5B2){Transformation};
\node[Box, fill=RedL,draw=RedLine, node distance=0.6,below =of 5B2](6B1){Feature Creation / Engineering};
\node[Box, fill=RedL,draw=RedLine, node distance=0.6,below =of 6B1](7B1){Data Labeling};
%
\scoped[on background layer]
\node[draw=BackLine,inner xsep=5mm,inner ysep=5mm,yshift=1mm,
           fill=BackColor,minimum width=113mm,fit=(B1)(B2)(B3),line width=0.75pt](BB1){};
\node[below=8pt of  BB1.north east,anchor=east]{Sources};
\scoped[on background layer]
\node[draw=BackLine,inner xsep=5mm,inner ysep=5mm,yshift=1mm,
           fill=BackColor,minimum width=113mm,fit=(2B1)(2B2),line width=0.75pt](BB2){};
\node[below=8pt of  BB2.north east,anchor=east]{Data Ingestion};
\scoped[on background layer]
\node[draw=BackLine,inner xsep=9mm,inner ysep=5mm,yshift=-2mm,
           fill=BackColor,fit=(4B1)(5B1),line width=0.75pt](BB3){};
\node[above=7pt of  BB3.south east,anchor=east]{ML Training};
%
\scoped[on background layer]
\node[draw=BackLine,inner xsep=9mm,inner ysep=5mm,yshift=-2mm,
           fill=BackColor,fit=(4B2)(7B1),line width=0.75pt](BB4){};
\node[above=7pt of  BB4.south east,anchor=east]{Processing Layer};
%
\scoped[on background layer]
\node[draw=OrangeLine,inner xsep=3mm,inner ysep=6mm,yshift=3mm,
           fill=none,fit=(BB1)(BB4),line width=0.75pt](BB4){};
\node[below=4pt of  BB4.north,anchor=north]{Data Governance};
%
\draw[Line,-latex](B1)--++(270:1.2)-|(2B1);
\draw[Line,-latex](B2)--++(270:1.2)-|(2B1);
\draw[Line,-latex](B3)--++(270:1.2)-|(2B2);
%
\draw[Line,-latex](2B1)|-(3B1);
\draw[Line,-latex](2B2)|-(3B1);
%
\draw[Line,-latex](3B1)--++(270:0.9)-|(4B1);
\draw[Line,-latex](3B1)--++(270:0.9)-|(4B2);
%
\draw[Line,-latex](4B1)--(5B1);
\draw[Line,-latex](4B2)--(5B2);
\draw[Line,-latex](5B2)--(6B1);
\draw[Line,-latex](6B1)--(7B1);
\draw[Line,-latex](7B1.east)--++(0:0.6)|-(3B1);
\end{tikzpicture}}
```
**Data Pipeline Architecture**: Modular pipelines ingest, process, and deliver data for machine learning tasks, enabling independent scaling of components and improved data quality control. Distinct stages (ingestion, storage, and preparation) transform raw data into a format suitable for model training and validation, forming the foundation of reliable ML systems.
:::

As shown in the architecture diagram, ML data pipelines consist of several distinct layers: data sources, ingestion, processing, labeling, storage, and ML training (@fig-pipeline-flow). Each layer plays a specific role in the data preparation workflow, and selecting appropriate technologies for each layer requires understanding core trade-offs.

Central to these design decisions, data pipeline design is constrained by storage hierarchies and I/O bandwidth limitations rather than CPU capacity. Understanding these constraints enables building efficient systems that can handle modern ML workloads. The storage hierarchy creates distinct tiers with different performance characteristics: object storage provides high latency but low cost with virtually unlimited scale, making it ideal for archival and batch processing. Distributed filesystems offer medium latency at moderate cost with petabyte scale, optimized for analytics workloads. In-memory stores deliver low latency but at high cost with capacity limited by RAM, essential for real-time serving.

Building on these storage considerations, the bandwidth hierarchy further shapes pipeline performance, with concrete performance tiers that directly influence architectural decisions. Traditional spinning disks deliver 100-200 MB/s sequential throughput, sufficient for batch ETL processing but inadequate for real-time model serving. SSD storage achieves 1-7 GB/s sequential access, enabling faster data loading for training pipelines but still creating bottlenecks for large model training. Network storage systems typically provide 1-10 GB/s depending on infrastructure configuration, with high-end configurations approaching the performance of local SSDs. RAM access delivers 50-200 GB/s typical bandwidth, explaining why in-memory data stores dramatically outperform disk-based systems for latency-sensitive applications. These bandwidth constraints explain why ETL operations frequently achieve low CPU utilization, with processors waiting on I/O operations. Training large models requires sustained read bandwidth that often exceeds storage capabilities, demonstrating why storage architecture directly impacts training feasibility.

Given these performance constraints, design decisions must align with specific requirements. For streaming data, consider whether you need message durability (ability to replay failed processing), ordering guarantees (maintaining event sequence), or geographic distribution. For batch processing, the key decision factors include data volume relative to memory, processing complexity, and whether computation must be distributed. Single-machine tools suffice for gigabyte-scale data, but terabyte-scale processing requires distributed frameworks that partition work across clusters. Storage decisions depend on access patterns: structured query workloads benefit from columnar storage and indexing, while unstructured data requires flexible schemas and horizontal scaling. The interactions between these layers determine the system's overall effectiveness, with each component's performance characteristics directly influencing the overall pipeline design and ML system capabilities.

### Pipeline Monitoring and Debugging {#sec-data-engineering-pipeline-monitoring-debugging}

Once pipelines are operational, production data pipelines require comprehensive monitoring and debugging capabilities to maintain reliability and detect issues before they cascade into model failures. Experience shows that 70% of production ML failures stem from data pipeline issues that could have been caught with proper monitoring. The four-pillar framework guides monitoring strategy: Quality metrics track data accuracy and completeness, Reliability monitoring ensures consistent pipeline operation, Scalability metrics prevent resource exhaustion, and Governance monitoring maintains compliance and auditability.

To achieve this level of monitoring, **Pipeline Observability Implementation** encompasses three critical dimensions: metrics, logging, and tracing. Key metrics include data freshness (time from source update to availability), throughput rates (records processed per unit time), error rates (percentage of failed processing attempts), and data quality scores (automated assessment of accuracy and completeness). Logging captures detailed processing events for debugging, while distributed tracing tracks data lineage across pipeline stages to enable root cause analysis when issues occur.

Beyond establishing observability infrastructure, **Common Failure Patterns and Detection** in production pipelines follow predictable categories that require specific monitoring approaches. Silent failures represent the most dangerous pattern where processing continues but produces incorrect results, such as when a data transformation subtly corrupts values without throwing errors. Schema evolution failures occur when upstream data sources change field names, add new columns, or modify data types without coordination. Resource exhaustion manifests as gradually increasing processing times, memory consumption, or storage usage that eventually overwhelms system capacity. External dependency failures include database timeouts, API rate limiting, or network partitions that disrupt data ingestion.

Complementing failure detection capabilities, **Automated Recovery and Alerting Strategies** enable systems to handle failures gracefully through intelligent retry logic, circuit breakers, and escalation procedures. Exponential backoff prevents overwhelming recovering services, while dead letter queues capture failed messages for later analysis and reprocessing. Alert design must balance sensitivity with actionability: alerts should fire early enough to prevent cascading failures but include sufficient context for rapid diagnosis. Effective production monitoring implements tiered alerting where critical issues trigger immediate response while warnings accumulate for batch review.

These concepts become concrete when we consider a recommendation system pipeline that processes user interaction events. Monitoring would track event ingestion rates, feature computation latency, and model serving freshness. When weekend traffic patterns cause 3x normal load, monitoring detects increased processing delays and automatically scales compute resources while alerting the operations team. This proactive approach prevents user-visible degradation and maintains system reliability during demand spikes.

With our pipeline architecture and monitoring capabilities established, we now examine how to source data that meets our four-pillar framework requirements. Data acquisition strategy directly determines the foundational quality of our entire system.

## Strategic Data Acquisition {#sec-data-engineering-data-sources-c8d9}

The approaches we choose for sourcing training data directly determine our system's quality foundation, reliability characteristics, scalability potential, and governance compliance. Rather than treating data sources as independent options, we examine them as strategic choices that must align with our established framework requirements.

For our KWS system, data source decisions have profound implications: existing datasets may provide quick prototyping but lack the linguistic diversity needed for global deployment; web scraping can increase coverage but raises privacy and quality concerns; crowdsourcing enables targeted data collection but requires careful bias management; and synthetic data offers controlled generation but may not capture real-world complexity. Each approach presents distinct trade-offs across our four pillars.

The strategic framework for data acquisition evaluates each source against specific criteria:

### Existing Datasets {#sec-data-engineering-existing-datasets-4f21}

Within this strategic framework, platforms like [Kaggle](https://www.kaggle.com/)[^fn-kaggle] and [UCI Machine Learning Repository](https://archive.ics.uci.edu/)[^fn-uci-repository] provide ML practitioners with ready-to-use datasets that can jumpstart system development. These pre-existing datasets are particularly valuable when building ML systems as they offer immediate access to cleaned, formatted data with established benchmarks. One of their primary advantages is cost efficiency, as creating datasets from scratch requires significant time and resources, especially when building production ML systems that need large amounts of high-quality training data.

[^fn-kaggle]: **Kaggle**: Founded in 2010 and acquired by Google in 2017, Kaggle hosts over 80,000 public datasets and serves 15+ million registered users globally. Its competition platform has driven major ML breakthroughs, including the $1 million Netflix Prize that advanced collaborative filtering algorithms.

[^fn-uci-repository]: **UCI ML Repository**: Established in 1987 by UC Irvine's Machine Learning Group, it's one of the oldest and most cited ML dataset repositories. Contains 600+ datasets including classics like Iris (1936) and Wine (1991) that shaped decades of ML research and education.

Building on this cost efficiency, many of these datasets, such as [ImageNet](https://www.image-net.org/), have become standard benchmarks in the machine learning community, enabling consistent performance comparisons across different models and architectures. For ML system developers, this standardization provides clear metrics for evaluating model improvements and system performance. The immediate availability of these datasets allows teams to begin experimentation and prototyping without delays in data collection and preprocessing.

Despite these advantages, ML practitioners must carefully consider the quality assurance aspects of pre-existing datasets. For instance, the ImageNet dataset was found to have label errors on 6.4% of the validation set [@northcutt2021pervasive]. While popular datasets benefit from community scrutiny that helps identify and correct errors and biases, most datasets remain "untended gardens" where quality issues can significantly impact downstream system performance if not properly addressed. As [@gebru2018datasheets] highlighted in her paper, simply providing a dataset without documentation can lead to misuse and misinterpretation, potentially amplifying biases present in the data.

Beyond quality concerns, supporting documentation accompanying existing datasets is invaluable, yet is often only present in widely-used datasets. Good documentation provides insights into the data collection process and variable definitions and sometimes even offers baseline model performances. This information not only aids understanding but also promotes reproducibility in research, a cornerstone of scientific integrity; currently, there is a crisis around improving reproducibility in machine learning systems [@pineau2021improving][^fn-reproducibility-crisis]. When other researchers have access to the same data, they can validate findings, test new hypotheses, or apply different methodologies, thus allowing us to build on each other's work more rapidly.

[^fn-reproducibility-crisis]: **ML Reproducibility Crisis**: Only 15% of ML papers include code, and fewer than 6% provide complete reproducible implementations. NeurIPS 2019 introduced mandatory reproducibility checklists, while venues like MLSys require artifact evaluation to address this systemic problem.

Even with proper documentation, understanding the context in which the data was collected becomes necessary. Researchers must avoid potential overfitting when using popular datasets such as ImageNet [@beyer2020we], which can lead to inflated performance metrics. Sometimes, these [datasets do not reflect the real-world data](https://venturebeat.com/uncategorized/3-big-problems-with-datasets-in-ai-and-machine-learning/).

Central to these contextual concerns, a key consideration for ML systems is how well pre-existing datasets reflect real-world deployment conditions. Relying on standard datasets can create a concerning disconnect between training and production environments. This misalignment becomes particularly problematic when multiple ML systems are trained on the same datasets (@fig-misalignment), potentially propagating biases and limitations throughout an entire ecosystem of deployed models.

::: {#fig-misalignment fig-env="figure" fig-pos="htb"}
```{.tikz}
\begin{tikzpicture}[font=\small\usefont{T1}{phv}{m}{n}]
%
\tikzset{%
  Line/.style={line width=1.0pt,black!50,text=black},
  Box/.style={align=flush center,
    inner xsep=2pt,
    node distance=1.4,
    draw=GreenLine,
    line width=0.75pt,
    fill=GreenL,
    text width=17mm,
    minimum width=17mm, minimum height=9mm
  },
 Text/.style={%
    inner sep=2pt,
    draw=none,
    line width=0.75pt,
    fill=TextColor!80,
    text=black,
    font=\footnotesize\usefont{T1}{phv}{m}{n},
    align=flush center,
    minimum width=7mm, minimum height=5mm
  },
}
%
\node[Box](B1){Model A};
\node[Box,right=of B1](B2){Model B};
\node[Box,right=of B2](B3){Model C};
\node[Box,right=of B3](B4){Model D};
\node[Box,right=of B4](B5){Model E};
\node[Box, fill=OrangeL,draw=OrangeLine,above=1.5 of B3,text width=53mm](G){Central Training Dataset Repository};
\node[Box, fill=RedL,draw=RedLine,below=1.3 of B3,text width=53mm](D){Limited Real-World Alignment};
%
\scoped[on background layer]
\node[draw=BackLine,inner xsep=6mm,inner ysep=3mm,yshift=0mm,
           fill=BackColor,minimum width=113mm,fit=(B1)(B5)(D),line width=0.75pt](BB1){};
\node[above=11pt of  BB1.south east,anchor=east]{Potential Issues};
\draw[latex-,Line](B2)--node[Text,pos=0.9]{Same Data}++(90:1.5)--(G);
\draw[latex-,Line](B3)--node[Text,pos=0.9]{Same Data}++(90:1.5)--(G);
\draw[latex-,Line](B4)--node[Text,pos=0.9]{Same Data}++(90:1.5)--(G);
\draw[latex-,Line](B1)|-node[Text,pos=0.22]{Training Data}(G);
\draw[latex-,Line](B5)|-node[Text,pos=0.22]{Same Data}(G);
%
\draw[-latex,Line](B2)--node[Text,pos=0.6]{Shared Limitations}++(270:1.5)--(D);
\draw[-latex,Line](B3)--node[Text,pos=0.6]{Dataset Blind Spots}++(270:1.5)--(D);
\draw[-latex,Line](B4)--node[Text,pos=0.6]{Common Weaknesses}++(270:1.5)--(D);
\draw[-latex,Line](B1)|-node[Text,pos=0.17]{Propagated Biases}(D);
\draw[-latex,Line](B5)|-node[Text,pos=0.17]{Systemic Issues}(D);
\end{tikzpicture}
```
**Dataset Convergence**: Shared datasets can mask limitations and propagate biases across multiple machine learning systems, potentially leading to overoptimistic performance evaluations and reduced generalization to unseen data. Reliance on common datasets creates a false sense of progress within an ecosystem of models, hindering the development of robust and reliable AI applications.
:::

### Web Scraping {#sec-data-engineering-web-scraping-fa9f}

When these limitations of existing datasets become problematic, particularly in domains where pre-existing datasets are insufficient, web scraping offers a powerful approach to gathering training data at scale. This automated technique for extracting data from websites has become a powerful tool in modern ML system development. It enables teams to build custom datasets tailored to their specific needs.

This automated technique demonstrates its value when human-labeled data is scarce. Consider computer vision systems: major datasets like [ImageNet](https://www.image-net.org/) and [OpenImages](https://storage.googleapis.com/openimages/web/index.html)[^fn-open-images] were built through systematic web scraping, significantly advancing the field of computer vision.

[^fn-open-images]: **Open Images Dataset**: Google's Open Images V7 contains 9 million images with 16 million bounding boxes across 600 object classes. Released in 2016 and continuously updated, it became the largest publicly available dataset for object detection, enabling breakthrough research in computer vision. In production environments, companies regularly scrape e-commerce sites to gather product images for recognition systems or social media platforms for computer vision applications. Stanford's LabelMe project demonstrated this approach's potential early on, scraping Flickr to create a diverse dataset of over 63,000 annotated images.

Expanding beyond these computer vision applications, the impact of web scraping extends well beyond computer vision systems. In natural language processing, web-scraped data has enabled the development of increasingly sophisticated ML systems. Large language models, such as ChatGPT and Claude, rely on vast amounts of text scraped from the public internet and media to learn language patterns and generate responses [@groeneveld2024olmo]. Similarly, specialized ML systems like GitHub's Copilot demonstrate how targeted web scraping, in this case of code repositories, can create powerful domain-specific assistants [@chen2021evaluating].

Building on these foundational developments, production ML systems often require continuous data collection to maintain relevance and performance. Web scraping facilitates this by gathering structured data like stock prices, weather patterns, or product information for analytical applications. This continuous collection introduces unique challenges for ML systems. Data consistency becomes crucial, as variations in website structure or content formatting can disrupt the data pipeline and affect model performance. Proper data management through databases or warehouses becomes essential not just for storage, but for maintaining data quality and enabling model updates.

However, alongside these powerful capabilities, web scraping presents several challenges that ML system developers must carefully consider. Legal and ethical constraints can limit data collection, as not all websites permit scraping, and violating these restrictions can have [serious consequences](https://hls.harvard.edu/today/does-chatgpt-violate-new-york-times-copyrights/). When building ML systems with scraped data, teams must carefully document data sources and ensure compliance with terms of service and copyright laws. Privacy considerations become important when dealing with user-generated content, often requiring systematic anonymization procedures.

Complementing these legal and ethical constraints, technical limitations also affect the reliability of web-scraped training data. Rate limiting by websites can slow data collection, while the dynamic nature of web content can introduce inconsistencies that impact model training. As shown in @fig-traffic-light, web scraping can yield unexpected or irrelevant data, for example, historical images appearing in contemporary image searches, that can pollute training datasets and degrade model performance. These issues highlight the importance of thorough data validation and cleaning processes in ML pipelines built on web-scraped data.

![**Data Source Noise**: Web scraping introduces irrelevant or outdated data into training sets, requiring systematic data validation and cleaning to maintain model performance and prevent spurious correlations. Historical images appearing in contemporary searches exemplify this noise, underscoring the need for careful filtering and quality control in web-sourced datasets. Source: Vox.](images/jpg/1914_traffic.jpeg){#fig-traffic-light}

### Crowdsourcing {#sec-data-engineering-crowdsourcing-e093}

When web scraping cannot provide the human judgment needed for complex labeling tasks, crowdsourcing offers a collaborative approach to data collection, leveraging the collective efforts of distributed individuals via the internet to tackle tasks requiring human judgment. By engaging a global pool of contributors, this method accelerates the creation of high-quality, labeled datasets for machine learning systems, especially in scenarios where pre-existing data is scarce or domain-specific. Platforms like [Amazon Mechanical Turk](https://www.mturk.com/) exemplify how crowdsourcing facilitates this process by distributing annotation tasks to a global workforce. This enables the rapid collection of labels for complex tasks such as sentiment analysis, image recognition, and speech transcription, significantly expediting the data preparation phase.

To illustrate the power of this approach, one of the most impactful examples of crowdsourcing in machine learning is the creation of the [ImageNet dataset](https://image-net.org/). ImageNet, which revolutionized computer vision, was built by distributing image labeling tasks to contributors via Amazon Mechanical Turk. The contributors categorized millions of images into thousands of classes, enabling researchers to train and benchmark models for a wide variety of visual recognition tasks.

Building on this massive labeling effort, the dataset's availability spurred advancements in deep learning, including the breakthrough AlexNet model in 2012 that demonstrated the power of large-scale neural networks and showed how large-scale, crowdsourced datasets could drive innovation. ImageNet's success highlights how leveraging a diverse group of contributors for annotation can enable machine learning systems to achieve unprecedented performance.

Extending beyond academic research, another example of crowdsourcing's potential is Google's [Crowdsource](https://crowdsource.google.com/)[^fn-google-crowdsource], a platform where volunteers contribute labeled data to improve AI systems in applications like language translation, handwriting recognition, and image understanding.

[^fn-google-crowdsource]: **Google Crowdsource**: Launched in 2016, this gamified platform has collected over 4 billion contributions from volunteers in 120+ languages. It powers improvements to Google Translate, Maps, and other AI services while demonstrating sustainable crowdsourcing through community engagement rather than monetary incentives. By gamifying the process and engaging global participants, Google leverages diverse datasets, particularly for underrepresented languages. This approach not only enhances the quality of AI systems but also empowers communities by enabling their contributions to influence technological development.

Beyond these static dataset creation efforts, crowdsourcing has also been instrumental in applications beyond traditional dataset annotation. For instance, the navigation app [Waze](https://www.waze.com/)[^fn-waze-crowdsourcing] uses crowdsourced data from its users to provide real-time traffic updates, route suggestions, and incident reporting.

[^fn-waze-crowdsourcing]: **Waze Crowdsourcing Model**: Founded in Israel (2006), Waze processes 25 billion miles of driving data monthly from 140+ million users. Its real-time crowdsourced traffic model influenced Google Maps after Google's $1.3 billion acquisition (2013), demonstrating how user-generated data creates network effects. While this involves dynamic data collection rather than static dataset labeling, it demonstrates how crowdsourcing can generate continuously updated datasets essential for applications like mobile or edge ML systems. These systems often require real-time input to maintain relevance and accuracy in changing environments.

These diverse applications highlight one of the primary advantages of crowdsourcing: its scalability. By distributing microtasks to a large audience, projects can process enormous volumes of data quickly and cost-effectively. This scalability is particularly beneficial for machine learning systems that require extensive datasets to achieve high performance. The diversity of contributors introduces a wide range of perspectives, cultural insights, and linguistic variations, enriching datasets and improving models' ability to generalize across populations.

Complementing this scalability advantage, flexibility is a key benefit of crowdsourcing. Tasks can be adjusted dynamically based on initial results, allowing for iterative improvements in data collection. For example, Google's [reCAPTCHA](https://www.google.com/recaptcha/about/)[^fn-recaptcha-evolution] system uses crowdsourcing to verify human users while simultaneously labeling datasets for training machine learning models.

[^fn-recaptcha-evolution]: **reCAPTCHA Evolution**: Originally created at Carnegie Mellon in 2007 to digitize books, reCAPTCHA v1 helped transcribe 13 million books. After Google's 2009 acquisition, v2 (2014) shifted to image recognition for Street View, while v3 (2018) uses behavioral analysis, processing 1+ billion CAPTCHAs weekly. Users identify objects in images, including street signs and cars, contributing to the training of autonomous systems. This clever integration demonstrates how crowdsourcing can scale effectively when embedded into everyday workflows.

However, alongside these compelling advantages, crowdsourcing presents challenges that require careful management. Quality control is a major concern, as the variability in contributors' expertise and attention can lead to inconsistent or inaccurate annotations. Providing clear instructions and training materials helps ensure participants understand the task requirements. Techniques such as embedding known test cases, leveraging consensus algorithms, or using redundant annotations can mitigate quality issues and align the process with the problem definition discussed earlier.

Beyond quality control issues, ethical considerations are important in crowdsourcing, especially when datasets are built at scale using global contributors. Teams must ensure that participants are fairly compensated for their work and that they are informed about how their contributions will be used. Privacy concerns must be addressed, particularly when dealing with sensitive or personal information. Transparent sourcing practices, clear communication with contributors, and systematic auditing mechanisms are necessary for building trust and maintaining ethical standards.

These ethical concerns became particularly visible when the issue of fair compensation and ethical data sourcing was brought into sharp focus during the development of large-scale AI systems like OpenAI's ChatGPT. Reports revealed that [OpenAI outsourced data annotation tasks to workers in Kenya](https://time.com/6247678/openai-chatgpt-kenya-workers/), employing them to moderate content and identify harmful or inappropriate material that the model might generate. This involved reviewing and labeling distressing content, such as graphic violence and explicit material, to train the AI in recognizing and avoiding such outputs. While this approach enabled OpenAI to improve the safety and utility of ChatGPT, significant ethical concerns arose around the working conditions, the nature of the tasks, and the compensation provided to Kenyan workers.

Many of the contributors were reportedly paid as little as $1.32 per hour for reviewing and labeling highly traumatic material. The emotional toll of such work, coupled with low wages, raised serious questions about the fairness and transparency of the crowdsourcing process. This controversy highlights a critical gap in ethical crowdsourcing practices. The workers, often from economically disadvantaged regions, were not adequately supported to cope with the psychological impact of their tasks. The lack of mental health resources and insufficient compensation underscored the power imbalances that can emerge when outsourcing data annotation tasks to lower-income regions.

Unfortunately, the challenges highlighted by the ChatGPT Kenya controversy are not unique to OpenAI. Many organizations that rely on crowdsourcing for data annotation face similar issues. As machine learning systems grow more complex and require larger datasets, the demand for annotated data will continue to increase. This shows the need for industry-wide standards and best practices to ensure ethical data sourcing. This case emphasizes the importance of considering the human labor behind AI systems. While crowdsourcing offers scalability and diversity, it also brings ethical responsibilities that cannot be overlooked. Organizations must prioritize the well-being and fair treatment of contributors as they build the datasets that drive AI innovation.

Beyond these ethical considerations, when dealing with specialized applications like mobile ML, edge ML, or cloud ML, additional challenges may arise. These applications often require data collected from specific environments or devices, which can be difficult to gather through general crowdsourcing platforms. For example, data for mobile applications utilizing smartphone sensors may necessitate participants with specific hardware features or software versions. Similarly, edge ML systems deployed in industrial settings may require data involving proprietary processes or secure environments, introducing privacy and accessibility challenges.

To address these diverse challenges, hybrid approaches that combine crowdsourcing with other data collection methods can provide solutions. Organizations may engage specialized communities, partner with relevant stakeholders, or create targeted initiatives to collect domain-specific data. Synthetic data generation, as discussed in the next section, can augment real-world data when crowdsourcing falls short.

### Anonymization Techniques {#sec-data-engineering-anonymization-techniques-b90b}

As privacy concerns become increasingly important in crowdsourced and web-scraped data collection, anonymization emerges as a critical capability. From a systems engineering perspective, anonymization represents more than regulatory compliance; it constitutes a core design constraint that affects data pipeline architecture, storage strategies, and processing efficiency. ML systems must handle sensitive data throughout their lifecycle: during collection, storage, transformation, model training, and even in error logs and debugging outputs. A single privacy breach can compromise not just individual records but entire datasets, making the system unusable for future development. This systemic risk means privacy protection must be built into the data engineering pipeline from the ground up, not retrofitted later.

These systemic risks demand careful consideration of the systems implications: anonymized data often requires different storage schemas, impacts join operations across tables, affects caching strategies, and changes how errors can be logged and debugged. Feature engineering becomes more complex when direct identifiers cannot be used. Model serving must handle anonymized inputs consistently with training data. These technical constraints fundamentally shape how ML systems are architected and operated.

Given these complex technical constraints, practitioners have developed a range of anonymization techniques to mitigate these risks. These methods transform datasets such that individual identities and sensitive attributes become difficult or nearly impossible to re-identify, while preserving, to varying extents, the overall utility of the data for analysis and the operational requirements of the ML system.

The most straightforward approach, masking, involves altering or obfuscating sensitive values so that they cannot be directly traced back to the original data subject. For instance, digits in financial account numbers or credit card numbers can be replaced with asterisks, a fixed set of dummy characters, or hashed values[^fn-hashing-security] to protect sensitive information during display or logging.

[^fn-hashing-security]: **Cryptographic Hashing**: One-way mathematical functions like SHA-256 that transform input data into fixed-length strings. Critical for data anonymization because they're computationally infeasible to reverse: even small input changes produce dramatically different outputs, ensuring original data cannot be recovered from the hash. This anonymization technique is straightforward to implement and understand while clearly protecting identifiable values from being viewed, but may struggle with protecting broader context (e.g. relationships between data points).

Building on this direct protection approach, generalization reduces the precision or granularity of data to decrease the likelihood of re-identification. Instead of revealing an exact date of birth or address, the data is aggregated into broader categories (e.g., age ranges, zip code prefixes). For example, a user's exact age of 37 might be generalized to an age range of 30-39, while their exact address might be bucketed into a city level granularity. This technique clearly reduces the risk of identifying an individual by sharing data in aggregated form; however, we might consequently lose analytical prediction. If granularity is not chosen correctly, individuals may still be able to be identified under certain conditions.

While generalization reduces data precision, pseudonymization[^fn-pseudonymization-gdpr] takes a different approach, replacing direct identifiers (like names, Social Security numbers, or email addresses) with artificial identifiers, or "pseudonyms." These pseudonyms must not reveal, or be easily traceable to, the original data subject.

[^fn-pseudonymization-gdpr]: **Pseudonymization under GDPR**: GDPR Article 4(5) formally defines pseudonymization as a privacy-enhancing technique. Unlike anonymization, pseudonymized data remains personal data under EU law, requiring continued protection but enabling reduced regulatory restrictions for research and analytics purposes. This is commonly used in health records or in any situation where datasets need personal identities removed, but maintain unique entries. This approach allow maintaining individual-level data for analysis (since records can be traced through pseudonyms), while reducing the risk of direct identification. However, if the "key" linking the pseudonym to the real identifier is compromised, re-identification becomes possible.

Moving beyond simple identifier replacement, $k$-anonymity[^fn-k-anonymity] provides a more formal approach, ensuring that each record in a dataset is indistinguishable from at least $ðâ1$ other records. This is achieved by suppressing or generalizing quasi-identifiers, or attributes that, in combination, could be used to re-identify an individual (e.g., zip code, age, gender). For example, if  $k=5$, every record in the dataset must share the same combination of quasi-identifiers with at least four other records. Thus, an attacker cannot pinpoint a single individual simply by looking at these attributes. This approach provides a formal privacy guarantee that helps reduce chances of individual re-identification. However, it is extremely high touch and may require a significant level of data distortion and does not protect against things like [homogeneity or background knowledge attacks](https://en.wikipedia.org/wiki/K-anonymity#Attacks).

[^fn-k-anonymity]: **k-Anonymity**: Introduced by Latanya Sweeney in 2002, k-anonymity ensures each record is identical to at least k-1 others on quasi-identifiers. Despite being groundbreaking for privacy research, it has critical limitations: l-diversity (2007) and t-closeness (2007) were developed to address homogeneity and background knowledge attacks that k-anonymity cannot prevent.

At the most sophisticated end of this spectrum, differential privacy (DP) adds carefully [calibrated "noise" or randomized data perturbations](https://digitalprivacy.ieee.org/publications/topics/what-is-differential-privacy#:~:text=At%20its%20roots%2C%20differential%20privacy,a%20result%20of%20providing%20data.) to query results or datasets. The goal is to ensure that the inclusion or exclusion of any single individual's data does not significantly affect the output, thereby concealing their presence. Introduced noise is controlled by the $\epsilon$ parameter in $\epsilon$-Differential Privacy, balancing data utility and privacy guarantees. The clear advantages this approach provides are strong mathematical guarantees of privacy, and DP is widely used in academic and industrial settings (e.g., large-scale data analysis). However, the added noise can affect data accuracy and subsequent model performance; proper parameter tuning is crucial to ensure both privacy and usefulness.

@tbl-anonymization-comparison summarizes the key characteristics of each anonymization approach to help practitioners select appropriate techniques based on their specific privacy requirements and data utility needs.

+---------------------+--------------+---------------+----------------+-------------------------------------+
| Technique           | Data Utility | Privacy Level | Implementation | Best Use Case                       |
+=====================+==============+===============+================+=====================================+
| Masking             | High         | Low-Medium    | Simple         | Displaying sensitive data           |
+---------------------+--------------+---------------+----------------+-------------------------------------+
| Generalization      | Medium       | Medium        | Moderate       | Age ranges, location bucketing      |
+---------------------+--------------+---------------+----------------+-------------------------------------+
| Pseudonymization    | High         | Medium        | Moderate       | Individual tracking needed          |
+---------------------+--------------+---------------+----------------+-------------------------------------+
| K-anonymity         | Low-Medium   | High          | Complex        | Formal privacy guarantees           |
+---------------------+--------------+---------------+----------------+-------------------------------------+
| Differential Privacy| Medium       | Very High     | Complex        | Statistical guarantees              |
+---------------------+--------------+---------------+----------------+-------------------------------------+

: Anonymization Techniques Comparison {#tbl-anonymization-comparison}

As the comparison table illustrates, effective data anonymization is a balancing act between privacy and utility. Techniques such as masking, generalization, pseudonymization, k-anonymity, and differential privacy each target different aspects of re-identification risk. By carefully selecting and combining these methods, organizations can responsibly derive value from sensitive datasets while respecting the privacy rights and expectations of the individuals represented within them.

### Synthetic Data Creation {#sec-data-engineering-synthetic-data-creation-f53d}

When privacy concerns make real data collection challenging, synthetic data generation has emerged as a powerful tool for addressing limitations in data collection, particularly in machine learning applications where real-world data is scarce, expensive, or ethically challenging to obtain. This approach involves creating artificial data using algorithms, simulations, or generative models to mimic real-world datasets. The generated data can be used to supplement or replace real-world data, expanding the possibilities for training robust and accurate machine learning systems. This democratization of data access becomes particularly valuable for the social impact applications discussed in @sec-ai-good, where synthetic data can enable ML development for underserved populations and sensitive domains without compromising privacy. The theoretical foundations for how diverse synthetic data improves model performance are explored in @sec-agi-systems. @fig-synthetic-data illustrates the process of combining synthetic data with historical datasets to create larger, more diverse training sets.

::: {#fig-synthetic-data fig-env="figure" fig-pos="htb"}
```{.tikz}
\begin{tikzpicture}[line join=round,font=\usefont{T1}{phv}{m}{n}\small]
\tikzset{
 Line/.style={line width=0.35pt,black!50,text=black},
 LineDO/.style={single arrow, draw=VioletLine, fill=VioletLine!50,
      minimum width = 10pt, single arrow head extend=3pt,
      minimum height=10mm},
 ALineA/.style={violet!80!black!50,line width=3pt,shorten <=2pt,shorten >=2pt,
{Triangle[width=1.1*6pt,length=0.8*6pt]}-{Triangle[width=1.1*6pt,length=0.8*6pt]}},
LineD/.style={line width=0.75pt,black!50,text=black,dashed,dash pattern=on 5pt off 3pt},
Circle/.style={inner xsep=2pt,
  % node distance=1.15,
  circle,
    draw=BrownLine,
    line width=0.75pt,
    fill=BrownL!40,
    minimum size=18mm
  },
 circles/.pic={
\pgfkeys{/channel/.cd, #1}
\node[circle,draw=\channelcolor,line width=\Linewidth,fill=\channelcolor!10,
minimum size=2.5mm](\picname){};
        }
}
\tikzset {
pics/cloud/.style = {
        code = {
\colorlet{red}{RedLine}
\begin{scope}[local bounding box=CLO,scale=0.5, every node/.append style={transform shape},,shift={($(SIM)+(0,0)$)},]
\draw[red,fill=white,line width=0.9pt](0.67,1.21)to[out=55,in=90,distance=13](1.5,0.96)
to[out=360,in=30,distance=9](1.68,0.42);
\draw[red,fill=white,line width=0.9pt](0,0)to[out=170,in=180,distance=11](0.1,0.61)
to[out=90,in=105,distance=17](1.07,0.71)
to[out=20,in=75,distance=7](1.48,0.36)
to[out=350,in=0,distance=7](1.48,0)--(0,0);
\draw[red,fill=white,line width=0.9pt](0.27,0.71)to[bend left=25](0.49,0.96);

\end{scope}
     }
  }
}
%streaming
\tikzset{%
 LineST/.style={-{Circle[\channelcolor,fill=RedLine,length=4pt]},draw=\channelcolor,line width=\Linewidth,rounded corners},
 ellipseST/.style={fill=\channelcolor,ellipse,minimum width = 2.5mm, inner sep=2pt, minimum height =1.5mm},
 BoxST/.style={line width=\Linewidth,fill=white,draw=\channelcolor,rectangle,minimum width=56,
 minimum height=16,rounded corners=1.2pt},
 pics/streaming/.style = {
        code = {
        \pgfkeys{/channel/.cd, #1}
\begin{scope}[local bounding box=STREAMING,scale=\scalefac, every node/.append style={transform shape}]
\node[BoxST,minimum width=44,minimum height=48](\picname-RE1){};
\foreach \i/\j in{1/north,2/center,3/south}{
\node[BoxST](\picname-GR\i)at(\picname-RE1.\j){};
\node[ellipseST]at($(\picname-GR\i.west)!0.2!(\picname-GR\i.east)$){};
\node[ellipseST]at($(\picname-GR\i.west)!0.4!(\picname-GR\i.east)$){};
}
\draw[LineST](\picname-GR3)--++(2,0)coordinate(\picname-C4);
\draw[LineST](\picname-GR3.320)--++(0,-0.7)--++(0.8,0)coordinate(\picname-C5);
\draw[LineST](\picname-GR3.220)--++(0,-0.7)--++(-0.8,0)coordinate(\picname-C6);
\draw[LineST](\picname-GR3)--++(-2,0)coordinate(\picname-C7);
 \end{scope}
     }
  }
}
%data
\tikzset{mycylinder/.style={cylinder, shape border rotate=90, aspect=1.3, draw, fill=white,
minimum width=25mm,minimum height=11mm,line width=\Linewidth,node distance=-0.15},
pics/data/.style = {
        code = {
        \pgfkeys{/channel/.cd, #1}
\begin{scope}[local bounding box=STREAMING,scale=\scalefac, every node/.append style={transform shape}]
\node[mycylinder,fill=\channelcolor!50] (A) {};
\node[mycylinder, above=of A,fill=\channelcolor!30] (B) {};
\node[mycylinder, above=of B,fill=\channelcolor!10] (C) {};
 \end{scope}
     }
  }
}

\tikzset{pics/brain/.style = {
        code = {
        \pgfkeys{/channel/.cd, #1}
\begin{scope}[local bounding box=BRAIN,scale=\scalefac, every node/.append style={transform shape}]
\fill[fill=\filllcolor!50](0.1,-0.5)to[out=0,in=180](0.33,-0.5)
to[out=0,in=270](0.45,-0.38)to(0.45,-0.18)
to[out=40,in=240](0.57,-0.13)to[out=110,in=310](0.52,-0.05)
to[out=130,in=290](0.44,0.15)to[out=90,in=340,distance=8](0.08,0.69)
to[out=160,in=80](-0.42,-0.15)to (-0.48,-0.7)to(0.07,-0.7)to(0.1,-0.5)
(-0.10,-0.42)to[out=310,in=180](0.1,-0.5);
\draw[draw=\drawchannelcolor,line width=\Linewidth](0.1,-0.5)to[out=0,in=180](0.33,-0.5)
to[out=0,in=270](0.45,-0.38)to(0.45,-0.18)
to[out=40,in=240](0.57,-0.13)to[out=110,in=310](0.52,-0.05)
to[out=130,in=290](0.44,0.15)to[out=90,in=340,distance=8](0.08,0.69)
(-0.42,-0.15)to (-0.48,-0.7)
(0.07,-0.7)to(0.1,-0.5)
(-0.10,-0.42)to[out=310,in=180](0.1,-0.5);
%brain
\draw[fill=\filllcolor,line width=\Linewidth](-0.3,-0.10)to(0.08,0.60)
to[out=60,in=50,distance=3](-0.1,0.69)to[out=160,in=80](-0.26,0.59)to[out=170,in=90](-0.46,0.42)
to[out=170,in=110](-0.54,0.25)to[out=210,in=150](-0.54,0.04)
to[out=240,in=130](-0.52,-0.1)to[out=300,in=240](-0.3,-0.10);
\draw[fill=\filllcolor,line width=\Linewidth]
(-0.04,0.64)to[out=120,in=0](-0.1,0.69)(-0.19,0.52)to[out=120,in=330](-0.26,0.59)
(-0.4,0.33)to[out=150,in=280](-0.46,0.42)
%
(-0.44,-0.03)to[bend left=30](-0.34,-0.04)
(-0.33,0.08)to[bend left=40](-0.37,0.2) (-0.37,0.12)to[bend left=40](-0.45,0.14)
(-0.26,0.2)to[bend left=30](-0.24,0.13)
(-0.16,0.32)to[bend right=30](-0.27,0.3)to[bend right=30](-0.29,0.38)
(-0.13,0.49)to[bend left=30](-0.04,0.51);

\draw[rounded corners=0.8pt,\drawcircle,-{Circle[fill=\filllcirclecolor,length=2.5pt]}](-0.23,0.03)--(-0.15,-0.03)--(-0.19,-0.18)--(-0.04,-0.28);
\draw[rounded corners=0.8pt,\drawcircle,-{Circle[fill=\filllcirclecolor,length=2.5pt]}](-0.17,0.13)--(-0.04,0.05)--(-0.06,-0.06)--(0.14,-0.11);
\draw[rounded corners=0.8pt,\drawcircle,-{Circle[fill=\filllcirclecolor,length=2.5pt]}](-0.12,0.23)--(0.31,0.0);
\draw[rounded corners=0.8pt,\drawcircle,-{Circle[fill=\filllcirclecolor,length=2.5pt]}](-0.07,0.32)--(0.06,0.26)--(0.16,0.33)--(0.34,0.2);
\draw[rounded corners=0.8pt,\drawcircle,-{Circle[fill=\filllcirclecolor,length=2.5pt]}](-0.01,0.43)--(0.06,0.39)--(0.18,0.51)--(0.31,0.4);
\end{scope}
     }
  }
}

\tikzset{pics/tube/.style = {
        code = {
        \pgfkeys{/channel/.cd, #1}
\begin{scope}[local bounding box=BRAIN,scale=\scalefac, every node/.append style={transform shape}]
\draw[draw=\drawchannelcolor,line width=\Linewidth,fill=white](-0.1,0.26)to(-0.1,0.1)to[out=240,in=60](-0.23,-0.14)
to[out=240,in=180,distance=3](-0.13,-0.27)to(0.09,-0.27)
to[out=0,in=300,distance=3](0.19,-0.14)
to[out=120,in=290]((0.06,0.1)to(0.06,0.26)
to cycle;
\fill[fill=\filllcolor!50](-0.23,-0.14)
to[out=240,in=180,distance=3](-0.13,-0.27)to(0.09,-0.27)
to[out=0,in=300,distance=3](0.19,-0.14)to[out=200,in=20]cycle;
\draw[draw=\drawchannelcolor,line width=\Linewidth,fill=none](-0.1,0.26)to(-0.1,0.1)to[out=240,in=60](-0.23,-0.14)
to[out=240,in=180,distance=3](-0.13,-0.27)to(0.09,-0.27)
to[out=0,in=300,distance=3](0.19,-0.14)
to[out=120,in=290]((0.06,0.1)to(0.06,0.26)
to cycle;
\end{scope}
     }
  }
}

\tikzset{pics/factory/.style = {
        code = {
        \pgfkeys{/channel/.cd, #1}
\begin{scope}[local bounding box=FACTORY,scale=\scalefac, every node/.append style={transform shape}]
\node[rectangle,draw=\drawchannelcolor,fill=\filllcolor!50,minimum height=15,minimum width=23,,line width=\Linewidth](R1){};
\draw[fill=\filllcolor!50,line width=1.0pt]($(R1.40)+(0,-0.01)$)--++(110:0.2)--++(180:0.12)|-($(R1.40)+(0,-0.01)$);
\draw[line width=\Linewidth,fill=green](-0.68,-0.27)--++(88:0.85)--++(0:0.15)--(-0.48,-0.27)--cycle;
\draw[line width=2.5pt](-0.8,-0.27)--(0.55,-0.27);

\foreach \x in{0.25,0.45,0.65}{
\node[rectangle,fill=black,minimum height=2,minimum width=5,thick,inner sep=0pt]
at ($(R1.north)!\x!(R1.south)$){};
}
\foreach \x in{0.25,0.45,0.65}{
\node[rectangle,fill=black,minimum height=2,minimum width=5,thick,inner sep=0pt]
at ($(R1.130)!\x!(R1.230)$){};
}
\foreach \x in{0.25,0.45,0.65}{
\node[rectangle,fill=black,minimum height=2,minimum width=5,thick,inner sep=0pt]
at ($(R1.50)!\x!(R1.310)$){};
}
\end{scope}
     }
  }
}

\tikzset {
pics/cloud/.style = {
        code = {
\colorlet{red}{BrownLine}
\pgfkeys{/channel/.cd, #1}
\begin{scope}[local bounding box=CLO,scale=\scalefac, every node/.append style={transform shape}]
\draw[red,line width=\Linewidth,fill=red!10](0,0)to[out=170,in=180,distance=11](0.1,0.61)
to[out=90,in=105,distance=17](1.07,0.71)
to[out=20,in=75,distance=7](1.48,0.36)
to[out=350,in=0,distance=7](1.48,0)--(0,0);
\draw[red,line width=\Linewidth](0.27,0.71)to[bend left=25](0.49,0.96);
%\draw[red,line width=\Linewidth](0.67,1.21)to[out=55,in=90,distance=13](1.5,0.96)
%to[out=360,in=30,distance=9](1.68,0.42);
\end{scope}
     }
  }
}
\tikzset{
pics/square/.style = {
        code = {
        \pgfkeys{/channel/.cd, #1}
\begin{scope}[local bounding box=SQUARE,scale=\scalefac,every node/.append style={transform shape}]
% Right Face
\draw[fill=\channelcolor!70,line width=\Linewidth]
(\Depth,0,0)coordinate(\picname-ZDD)--(\Depth,\Width,0)--(\Depth,\Width,\Height)--(\Depth,0,\Height)--cycle;
% Front Face
\draw[fill=\channelcolor!40,line width=\Linewidth]
(0,0,\Height)coordinate(\picname-DL)--(0,\Width,\Height)coordinate(\picname-GL)--
(\Depth,\Width,\Height)coordinate(\picname-GD)--(\Depth,0,\Height)coordinate(\picname-DD)--(0,0,\Height);
% Top Face
\draw[fill=\channelcolor!20,line width=\Linewidth]
(0,\Width,0)coordinate(\picname-ZGL)--(0,\Width,\Height)coordinate(\picname-ZGL)--
(\Depth,\Width,\Height)--(\Depth,\Width,0)coordinate(\picname-ZGD)--cycle;
\end{scope}
    }
  }
}

\tikzset{
pics/plus/.style = {
        code = {
        \pgfkeys{/channel/.cd, #1}
\begin{scope}[local bounding box=PLUS,scale=\scalefac,every node/.append style={transform shape}]
% Right Face
\fill[fill=\channelcolor!70] (-0.7,-0.15)rectangle(0.7,0.15);
\fill[fill=\channelcolor!70] (-0.15,-0.7)rectangle(0.15,0.7);
\end{scope}
    }
  }
}

\pgfkeys{
  /channel/.cd,
    Depth/.store in=\Depth,
  Height/.store in=\Height,
  Width/.store in=\Width,
   channelcolor/.store in=\channelcolor,
  filllcirclecolor/.store in=\filllcirclecolor,
  filllcolor/.store in=\filllcolor,
  drawchannelcolor/.store in=\drawchannelcolor,
  drawcircle/.store in=\drawcircle,
  scalefac/.store in=\scalefac,
  Linewidth/.store in=\Linewidth,
  picname/.store in=\picname,
  filllcolor=BrownLine,
  filllcirclecolor=violet!20,
  drawchannelcolor=black,
  drawcircle=violet,
  channelcolor=BrownLine,
  scalefac=1,
  Linewidth=1.6pt,
    Depth=1.3,
  Height=0.8,
  Width=1.1,
  picname=C
}

\node[Circle](SIM){};
\node[Circle,right=2 of SIM,draw=GreenLine,fill=GreenL!40,](SYN){};
\node[Circle,below=1.75 of SIM,draw=OrangeLine,fill=OrangeL!40,](REA){};
\node[Circle,right=2 of REA,draw=RedLine,fill=RedL!40,](HIS){};
%
\node[Circle, right=3.5 of $(SYN)!0.5!(HIS)$,draw=BlueLine,fill=BlueL!40,](MLA){};
\node[Circle,right=2 of MLA,draw=VioletLine,fill=VioletL2!40,](TRA){};
\node[LineDO]at($(SIM)!0.5!(SYN)$){};
\node[LineDO]at($(REA)!0.5!(HIS)$){};
\node[LineDO]at($(MLA)!0.5!(TRA)$){};
\coordinate(LG)at($(SYN.east)+(6mm,0)$);
\coordinate(LD)at($(HIS.east)+(6mm,0)$);
\draw[line width=4pt,violet!40](LG)--++(5mm,0)|-coordinate[pos=0.25](S)(LD);
\node[LineDO]at($(S)!0.1!(MLA)$){};
%%
\begin{scope}[local bounding box=CIRCLE1,shift={($(TRA)+(0.04,-0.24)$)},
scale=0.6, every node/.append style={transform shape}]
%1 column
\foreach \j in {1,2,3} {
  \pgfmathsetmacro{\y}{(1.5-\j)*0.53 + 0.7}
  \pic at (-0.8,\y) {circles={channelcolor=green!70!black,picname=1CD\j}};
}
%2 column
\foreach \i in {1,...,4} {
  \pgfmathsetmacro{\y}{(2-\i)*0.53+0.7}
  \pic at (0,\y) {circles={channelcolor=green!70!black, picname=2CD\i}};
}
%3 column
\foreach \j in {1,2} {
  \pgfmathsetmacro{\y}{(1-\j)*0.53 + 0.7}
  \pic at (0.8,\y) {circles={channelcolor=green!70!black,picname=3CD\j}};
}
\foreach \i in {1,2,3}{
  \foreach \j in {1,2,3,4}{
\draw[Line](1CD\i)--(2CD\j);
}}
\foreach \i in {1,2,3,4}{
  \foreach \j in {1,2}{
\draw[Line](2CD\i)--(3CD\j);
}}
\end{scope}

 \tikzset{
    comp/.style = {draw,
        minimum width =18mm,
        minimum height = 15mm,
        inner sep= 0pt,
        rounded corners=1pt,
       draw = BlueLine,
       fill=cyan!10,
       line width=1.2pt
    }
}
\begin{scope}[local bounding box=COMPUTER,scale=0.6, every node/.append style={transform shape}]
 \node[comp](COM){};
 \draw[draw = BlueLine,line width=1.0pt]
 ($(COM.north west)!0.85!(COM.south west)$)-- ($(COM.north east)!0.85!(COM.south east)$);
\draw[draw = BlueLine,line width=1.0pt]($(COM.south west)!0.4!(COM.south east)$)--++(270:0.2)coordinate(DL);
\draw[draw = BlueLine,line width=1.0pt]($(COM.south west)!0.6!(COM.south east)$)--++(270:0.2)coordinate(DD);
\draw[draw = BlueLine,line width=3.0pt,shorten <=-3mm,shorten >=-3mm](DL)--(DD);
\end{scope}
%
\pic[shift={(0,-0.4)}] at  (HIS){data={scalefac=0.35,picname=1,channelcolor=green!70!black, Linewidth=0.4pt}};
\pic[shift={(0,0)}] at  (MLA){brain={scalefac=0.9,picname=1,filllcolor=orange!30!, Linewidth=0.7pt}};
\pic[shift={(0,-0.4)}] at  (SYN){data={scalefac=0.35,picname=1,channelcolor=cyan!70!black, Linewidth=0.4pt}};
\pic[shift={(0.25,-0.35)}] at  (SYN){tube={scalefac=1.2,picname=1,filllcolor=blue!90!, Linewidth=0.5pt}};
\pic[shift={(0.13,-0.00)}] at  (REA){factory={scalefac=0.9,picname=1,filllcolor=brown!, Linewidth=0.5pt}};
\pic[shift={(-0.32,-0.65)}] at (REA) {cloud={scalefac=0.5, Linewidth=1.0pt}};
\pic[shift={(-0.16,-0.1)}] at  (SIM){square={scalefac=0.35,picname=1,channelcolor=red, Linewidth=0.5pt}};
%
\pic[shift={(0,0)}] at  ($(SYN)!0.55!(HIS)$){plus={scalefac=0.4,channelcolor=violet}};
%
\node[below=1mm of SIM]{Simulation model};
\node[below=1mm of SYN]{Synthetic data};
\node[below=1mm of REA]{Real system};;
\node[below=1mm of HIS]{Historical  data};
\node[below=1mm of MLA]{ML algorithm};
\node[below=1mm of TRA]{Trained ML model};
\end{tikzpicture}

```
**Synthetic Data Augmentation**: Combining algorithmically generated data with historical datasets expands training set size and diversity, mitigating limitations caused by scarce or biased real-world data and improving model generalization. This approach enables robust machine learning system development when acquiring sufficient real-world data is impractical or unethical. Source: [anylogic](HTTPS://www.anylogic.com/features/artificial-intelligence/synthetic-data/).
:::

Building on this foundation, advancements in generative modeling techniques have greatly enhanced the quality of synthetic data. Modern AI systems can produce data that closely resembles real-world distributions, making it suitable for applications ranging from computer vision to natural language processing. For example, generative models have been used to create synthetic images for object recognition tasks, producing diverse datasets that closely match real-world images. Similarly, synthetic data has been leveraged to simulate speech patterns, enhancing the robustness of voice recognition systems.

Beyond these quality improvements, synthetic data has become particularly valuable in domains where obtaining real-world data is either impractical or costly.
The automotive industry has embraced synthetic data to train autonomous vehicle systems; there are only so many cars you can physically crash to get crash-test data that might help an ML system know how to avoid crashes in the first place. Capturing real-world scenarios, especially rare edge cases such as near-accidents or unusual road conditions, is inherently difficult. Synthetic data allows researchers to [simulate these scenarios in a controlled virtual environment](https://www.nvidia.com/en-us/use-cases/autonomous-vehicle-simulation/), ensuring that models are trained to handle a wide range of conditions. This approach has proven invaluable for advancing the capabilities of self-driving cars.

Complementing these safety-critical applications, another important application of synthetic data lies in augmenting existing datasets. Introducing variations into datasets enhances model robustness by exposing the model to diverse conditions. For instance, in speech recognition, data augmentation techniques like SpecAugment [@park2019specaugment] introduce noise, shifts, or pitch variations, enabling models to generalize better across different environments and speaker styles. This principle extends to other domains as well, where synthetic data can fill gaps in underrepresented scenarios or edge cases.

Beyond dataset augmentation capabilities, synthetic data addresses critical ethical and privacy concerns. Unlike real-world data, synthetic data attempts to not tie back to specific individuals or entities. This makes it especially useful in sensitive domains such as finance, healthcare, or human resources, where data confidentiality is paramount. The ability to preserve statistical properties while removing identifying information allows researchers to maintain high ethical standards without compromising the quality of their models. In healthcare, privacy regulations such as [GDPR](https://gdpr.eu/) and [HIPAA](https://www.hhs.gov/hipaa/for-professionals/privacy/laws-regulations/index.html)[^fn-privacy-regulations] limit the sharing of sensitive patient information. Synthetic data generation enables the creation of realistic yet anonymized datasets that can be used for training diagnostic models without compromising patient privacy.

[^fn-privacy-regulations]: **Privacy Regulation Timeline**: GDPR (2016, effective 2018) imposed maximum fines of â¬20 million or 4% of annual global turnover (whichever is higher) for violations, followed by California's CCPA (2018), and now dozens of similar laws globally. These regulations significantly transformed how ML systems handle personal data, making privacy-by-design essential for any AI system.

Despite these compelling advantages, synthetic data generation faces important limitations. Poorly generated data can misrepresent underlying real-world distributions, introducing biases or inaccuracies that degrade model performance. Validating synthetic data against real-world benchmarks is essential to ensure its reliability. Models trained primarily on synthetic data must be rigorously tested in real-world scenarios to confirm their ability to generalize effectively. Another challenge is the potential amplification of biases present in the original datasets used to inform synthetic data generation. If these biases are not carefully addressed, they may be inadvertently reinforced in the resulting models. A critical consideration is maintaining proper balance between synthetic and real-world data during training - if models are overly trained on synthetic data, their outputs may become nonsensical and model performance may collapse.

Despite these challenges, synthetic data has revolutionized the way machine learning systems are trained, providing flexibility, diversity, and scalability in data preparation. However, as its adoption grows, practitioners must remain vigilant about its limitations and ethical implications. By combining synthetic data with rigorous validation and thoughtful application, machine learning researchers and engineers can unlock its full potential while ensuring reliability and fairness in their systems.

### Continuing the KWS Example {#sec-data-engineering-continuing-kws-example-1222}

Bringing together all these data acquisition strategies, our KWS case study demonstrates how different data collection approaches combine effectively, each with distinct trade-offs:

To establish the foundational data layer, pre-existing datasets like Google's Speech Commands [@warden2018speech] provide a foundation for initial development, offering carefully curated voice samples for common wake words. However, these datasets often lack diversity in accents, environments, and languages, necessitating additional data collection strategies.

To address these coverage gaps, web scraping can supplement these baseline datasets by gathering diverse voice samples from video platforms, podcast repositories, and speech databases. This helps capture natural speech patterns and wake word variations, though careful attention must be paid to audio quality and privacy considerations when scraping voice data.

For targeted data collection needs, crowdsourcing becomes valuable for collecting specific wake word samples across different demographics and environments. Platforms like Amazon Mechanical Turk[^fn-mechanical-turk] can engage contributors to record wake words in various accents, speaking styles, and background conditions. This approach is particularly useful for gathering data for underrepresented languages or specific acoustic environments.

[^fn-mechanical-turk]: **Mechanical Turk Origins**: Named after the 18th-century chess-playing "automaton" (actually a human chess master hidden inside), Amazon's MTurk (2005) pioneered human-in-the-loop AI by enabling distributed human computation at scale, ironically reversing the original Turk's deception of AI capabilities.

Finally, synthetic data generation helps fill remaining gaps by creating unlimited variations of wake word utterances. Using speech synthesis [@werchniak2021exploring] and audio augmentation techniques, developers can generate training data that captures different acoustic environments (busy streets, quiet rooms, moving vehicles), speaker characteristics (age, accent, gender), and background noise conditions. This synthetic data approach becomes particularly valuable when developing efficient models discussed in @sec-efficient-ai, where diverse training data can enable better model performance.

This comprehensive, multi-faceted approach to data collection enables the development of KWS systems that perform robustly across diverse real-world conditions. The combination of methods helps address the unique challenges of wake word detection, from handling various accents and background noise to maintaining consistent performance across different devices and environments. The theoretical foundations for how diverse, high-quality training data improves model performance are detailed in @sec-dl-primer and @sec-agi-systems.

## Data Ingestion {#sec-data-engineering-data-ingestion-5dfc}

Once our data acquisition strategy is established, the collected data must be reliably and efficiently ingested into our ML systems through well-designed data pipelines. This transformation presents several challenges that ML engineers must address.

### Ingestion Patterns {#sec-data-engineering-ingestion-patterns-b977}

To address these ingestion challenges systematically, ML systems typically follow two primary patterns: batch ingestion and stream ingestion. Each pattern has distinct characteristics and use cases that students should understand to design effective ML systems.

Batch ingestion[^fn-batch-processing] involves collecting data in groups or batches over a specified period before processing. This method is appropriate when real-time data processing is not critical and data can be processed at scheduled intervals. It's also useful for loading large volumes of historical data. For example, a retail company might use batch ingestion to process daily sales data overnight, updating their ML models for inventory prediction each morning [@akidau2015dataflow].

[^fn-batch-processing]: **Batch Processing Evolution**: Batch processing dates back to IBM mainframes in the 1950s but was revolutionized by Google's MapReduce (2004), which enabled distributed batch processing across thousands of machines. This paradigm shift made "big data" analytics economically feasible for the first time.

In contrast to this scheduled approach, stream ingestion processes data in real-time as it arrives. This pattern is crucial for applications requiring immediate data processing, scenarios where data loses value quickly, and systems that need to respond to events as they occur. A financial institution, for instance, might use stream ingestion for real-time fraud detection, processing each transaction as it occurs to flag suspicious activity immediately [@kleppmann2017designing]. However, stream processing must handle backpressure when downstream systems cannot keep pace by implementing buffer limits, sampling strategies, or graceful degradation to prevent system overload. Data freshness Service Level Agreements (SLAs) formalize these requirements, specifying maximum acceptable delays between data generation and availability for processing.

Recognizing the limitations of either approach alone, many modern ML systems employ a hybrid approach, combining both batch and stream ingestion to handle different data velocities and use cases. This flexibility allows systems to process both historical data in batches and real-time data streams, providing a comprehensive view of the data landscape. Production systems must balance cost versus latency trade-offs: real-time processing can cost 10-100x more than batch processing. This cost differential arises from several factors: streaming systems require always-on infrastructure rather than schedulable resources, maintain redundant processing for fault tolerance, need low-latency networking and storage, and cannot benefit from the economies of scale that batch processing achieves by amortizing startup costs across large data volumes. A batch job processing 1TB might use 100 machines for 10 minutes, while a streaming system processing the same data over 24 hours needs dedicated resources continuously available. Techniques for managing streaming systems at scale, including backpressure handling and cost optimization, are detailed in @sec-ml-operations.

### ETL and ELT Comparison {#sec-data-engineering-etl-elt-comparison-bbb7}

Beyond choosing ingestion patterns, designing effective data ingestion pipelines requires understanding the differences between Extract, Transform, Load (ETL) and Extract, Load, Transform (ELT) approaches, as illustrated in @fig-etl-vs-elt. These paradigms determine when data transformations occur relative to the loading phase, significantly impacting the flexibility and efficiency of your ML pipeline.

::: {#fig-etl-vs-elt fig-env="figure" fig-pos="htb"}
```{.tikz}
\begin{tikzpicture}[line join=round,font=\small\usefont{T1}{phv}{m}{n}]
\tikzset{
 Line/.style={line width=0.75pt,black!50,text=black},
 LineD/.style={line width=0.5pt,black!50,text=black,dashed},
}

\tikzset{
channel/.pic={
\pgfkeys{/channel/.cd, #1}
\begin{scope}[yscale=\scalefac,xscale=\scalefac,every node/.append style={scale=\scalefac}]
\node[rectangle,draw=\drawchannelcolor,line width=0.5pt,fill=\channelcolor!50,
minimum width=50,minimum height=28.5](\picname){};
\end{scope}
        },
cyl/.pic={
\pgfkeys{/channel/.cd, #1}
\begin{scope}[yscale=\scalefac,xscale=\scalefac,every node/.append style={scale=\scalefac}]
\node[cylinder, draw=\drawchannelcolor,shape border rotate=90, aspect=1.99,inner ysep=0pt,
    minimum height=20mm,minimum width=21mm, cylinder uses custom fill,
 cylinder body fill=\channelcolor!10,cylinder end fill=\channelcolor!35](\picname){};
\end{scope}
        },
tableicon/.pic={
\pgfkeys{/channel/.cd, #1}
    \begin{scope}[yscale=\scalefac,xscale=\scalefac,every node/.append style={scale=\scalefac}]
      \draw[line width=0.5pt,fill=\channelcolor!20] (0,0)coordinate(DO\picname)
      rectangle (2,1.5)coordinate(GO\picname);
% Horizontal line
      \foreach \y in {0.5,1} {
        \draw (0,\y) -- (2,\y);
      }
      % Vertical line
      \foreach \x in {0.5,1,1.5} {
        \draw (\x,0) -- (\x,1.5);
      }
    \end{scope}
  }
}

\pgfkeys{
  /channel/.cd,
  channelcolor/.store in=\channelcolor,
  drawchannelcolor/.store in=\drawchannelcolor,
  scalefac/.store in=\scalefac,
  picname/.store in=\picname,
  channelcolor=BrownLine,
  drawchannelcolor=BrownLine,
  scalefac=1,
  picname=C
}
% #1 number of teeths
% #2 radius intern
% #3 radius extern
% #4 angle from start to end of the first arc
% #5 angle to decale the second arc from the first
% #6 inner radius to cut off
\newcommand{\gear}[6]{%
  (0:#2)
  \foreach \i [evaluate=\i as \n using {\i-1)*360/#1}] in {1,...,#1}{%
    arc (\n:\n+#4:#2) {[rounded corners=1.5pt] -- (\n+#4+#5:#3)
    arc (\n+#4+#5:\n+360/#1-#5:#3)} --  (\n+360/#1:#2)
  }%
  (0,0) circle[radius=#6];
}
\begin{scope}[local bounding box=RIGHT,shift={(0,0)},
scale=1,every node/.append style={scale=1}]
\begin{scope}[local bounding box=TARGET,shift={(0,0)},
scale=1,every node/.append style={scale=1}]
\scoped[on background layer]
\pic at(0,0) {cyl={scalefac=1.95,picname=1-CYL}};
\node[align=center]at($(1-CYL.before top)!0.5!(1-CYL.after top)$){Target\\ (MPP database)};
%%
\begin{scope}[local bounding box=GEAR,shift={(-0.80,0.3)},
scale=1,every node/.append style={scale=1}]
\colorlet{black}{brown!70!black}
\fill[draw=none,fill=black,even odd rule,xshift=-2mm]coordinate(GE1)\gear{10}{0.23}{0.28}{10}{2}{0.1};
\fill[draw=none,fill=black,even odd rule,xshift=2.9mm,yshift=-0.6mm]coordinate(GE2)\gear{10}{0.18}{0.22}{10}{2}{0.08};
\fill[draw=none,fill=black,even odd rule,xshift=-5.7mm,yshift=-2.8mm]coordinate(GE3)\gear{10}{0.15}{0.19}{10}{2}{0.08};
\node[draw=none,inner xsep=8,inner ysep=8,yshift=0mm,
           fill=none,fit=(GE1)(GE2)(GE3),line width=1.0pt](BB1){};
\node[below=-2pt of BB1,align=center]{Staging\\ tables};
\end{scope}

\begin{scope}[local bounding box=TAB,shift={(0.1,-0.25)},
scale=1,every node/.append style={scale=1}]
  \pic at (0,0) {tableicon={scalefac=0.35,channelcolor=red,picname=T1}}coordinate(GE1);
  \pic at (0.85,0){tableicon={scalefac=0.254,channelcolor=green,picname=T2}}coordinate(GE2);
  \pic at (0.85,0.5){tableicon={scalefac=0.23,channelcolor=cyan,picname=T3}}coordinate(GE3);
    \pic at (0.15,0.65){tableicon={scalefac=0.18,channelcolor=orange,picname=T4}}coordinate(GE4);
\scoped[on background layer]
\node[draw=black!60,inner xsep=4,inner ysep=5,yshift=0.5mm,
           fill=yellow!20,fit=(DOT1)(GOT2)(GOT3),line width=1.0pt](BB2){};
\node[below=1pt of BB2,align=center]{Final\\ tables};
\end{scope}
\end{scope}

\begin{scope}[local bounding box=SOURCE,shift={(-4.5,0)},
scale=1,every node/.append style={scale=1}]
\begin{scope}[local bounding box=SOURCE1,shift={(0,1.8)},
scale=1,every node/.append style={scale=1}]
\pic at(0,0) {cyl={scalefac=0.75,channelcolor=violet!80!,picname=2-CYL}};
\node at(2-CYL){Source 1};
\end{scope}

\begin{scope}[local bounding box=SOURCE2,shift={(0,0.1)},
scale=1,every node/.append style={scale=1}]
\scoped[on background layer]
\pic at(0,0) {cyl={scalefac=0.75,channelcolor=orange!80!,picname=3-CYL}};
\node at(3-CYL){Source 2};
\end{scope}

\begin{scope}[local bounding box=SOURCE3,shift={(-0.15,-1.2)},
scale=1,every node/.append style={scale=1}]
\foreach \j in {1,2,3} {
\pic at ({\j*0.15}, {-0.15*\j}) {channel={scalefac=1.15,channelcolor=green!40!,picname=\j-CH1}};
}
\node at(3-CH1){Source 3};
\end{scope}
\foreach \j in {1,2,3} {
\draw[Line,-latex,shorten >=10pt,shorten <=10pt](SOURCE\j.east)--(TARGET.west);
}
\end{scope}
\path[red](3-CH1.south)--++(0,-0.6)-|coordinate[pos=0.35](SR1)(1-CYL.south);
\node[single arrow, draw=black,thick, fill=VioletL,
      minimum width = 17pt, single arrow head extend=3pt,
      minimum height=9mm](AR1)at(SR1) {};
\node[left=6pt of AR1,anchor=east]{Extract \& Load};
\node[right=6pt of AR1,anchor=west]{Transform};
\node[below=8pt of AR1]{\normalsize E \textcolor{red}{$\to$ L $\to$ T}};
\end{scope}
%%%%%%%%%%%%%
%LEFT
\begin{scope}[local bounding box=LEFT,shift={(-8,0)},
scale=1,every node/.append style={scale=1}]
\begin{scope}[local bounding box=TARGET,shift={(0,0)},
scale=1,every node/.append style={scale=1}]
\pic at(0,0) {cyl={scalefac=1.25,picname=1-CYL}};
\node at(1-CYL){Target};
\end{scope}
%%
\begin{scope}[local bounding box=GEAR,shift={(-3.2,0.3)},
scale=1.5,every node/.append style={scale=1}]
\colorlet{black}{brown!70!black}
\fill[draw=none,fill=black,even odd rule,xshift=-2mm]coordinate(GE1)\gear{10}{0.23}{0.28}{10}{2}{0.1};
\fill[draw=none,fill=black,even odd rule,xshift=2.9mm,yshift=-0.6mm]coordinate(GE2)\gear{10}{0.18}{0.22}{10}{2}{0.08};
\fill[draw=none,fill=black,even odd rule,xshift=-5.7mm,yshift=-2.8mm]coordinate(GE3)\gear{10}{0.15}{0.19}{10}{2}{0.08};
\node[draw=none,inner xsep=8,inner ysep=8,yshift=0mm,
           fill=none,fit=(GE1)(GE2)(GE3),line width=1.0pt](BB1){};
\end{scope}

\begin{scope}[local bounding box=SOURCE,shift={(-6.9,-0.14)},
scale=1,every node/.append style={scale=1}]
\begin{scope}[local bounding box=SOURCE1,shift={(0,1.8)},
scale=1,every node/.append style={scale=1}]
\pic at(0,0) {cyl={scalefac=0.75,channelcolor=violet!80!,picname=2-CYL}};
\node at(2-CYL){Source 1};
\end{scope}

\begin{scope}[local bounding box=SOURCE2,shift={(0,0.1)},
scale=1,every node/.append style={scale=1}]
\scoped[on background layer]
\pic at(0,0) {cyl={scalefac=0.75,channelcolor=orange!80!,picname=3-CYL}};
\node at(3-CYL){Source 2};
\end{scope}

\begin{scope}[local bounding box=SOURCE3,shift={(-0.15,-1.2)},
scale=1,every node/.append style={scale=1}]
\foreach \j in {1,2,3} {
\pic at ({\j*0.15}, {-0.15*\j}) {channel={scalefac=1.15,channelcolor=green!40!,picname=\j-CH1}};
}
\node at(3-CH1){Source 3};
\end{scope}
\foreach \j in {1,2,3} {
\draw[Line,-latex,shorten >=10pt,shorten <=10pt](SOURCE\j.east)--(BB1.west);
}\draw[Line,-latex,shorten >=5pt,shorten <=5pt](BB1.08)--(TARGET.west);
\end{scope}
\path[red](3-CH1.south)--++(0,-0.5)-|coordinate[pos=0.35](SR1)(1-CYL.south);
\node[single arrow, draw=black,thick, fill=VioletL,
      minimum width = 17pt, single arrow head extend=3pt,
      minimum height=9mm](AR1)at(SR1) {};
\node[left=6pt of AR1,anchor=east](TRA){Transform};
\node[right=6pt of AR1,anchor=west]{Load};
\node[left=4pt of TRA,single arrow, draw=black,thick, fill=VioletL,
      minimum width = 17pt, single arrow head extend=3pt,
      minimum height=9mm](AR2) {};
\node[left=6pt of AR2,anchor=east]{Extract};
\node[below=8pt of TRA]{\normalsize E \textcolor{red}{$\to$ T $\to$ L}};
\end{scope}
\draw[line width=2pt,red!40]($(LEFT.north east)!0.5!(RIGHT.north west)$)--
($(LEFT.south east)!0.5!(RIGHT.south west)$);
\end{tikzpicture}
```
**Data Pipeline Architectures**: ETL pipelines transform data *before* loading it into a data warehouse, while ELT pipelines load raw data first and transform it within the warehouse, impacting system flexibility and resource allocation for machine learning workflows. Choosing between ETL and ELT depends on data volume, transformation complexity, and the capabilities of the target data storage system.
:::

ETL[^fn-etl-history] is a well-established paradigm in which data is first gathered from a source, then transformed to match the target schema or model, and finally loaded into a data warehouse or other repository. This approach typically results in data being stored in a ready-to-query format, which can be advantageous for ML systems that require consistent, pre-processed data. For instance, an ML system predicting customer churn might use ETL to standardize and aggregate customer interaction data from multiple sources before loading it into a format suitable for model training [@inmon2005building].

[^fn-etl-history]: **ETL Evolution**: ETL emerged in the 1970s with early data warehouses but was revolutionized by Apache Spark in 2009, which enabled distributed data processing at unprecedented scale. Spark's in-memory computing made ETL pipelines 100x faster than traditional MapReduce approaches.

However, ETL can be less flexible when schemas or requirements change frequently, a common occurrence in evolving ML projects. This is where the ELT approach comes into play. ELT reverses the order by first loading raw data and then applying transformations as needed. This method is often seen in modern data lake or schema-on-read environments, allowing for a more agile approach when addressing evolving analytical needs in ML systems.

By deferring transformations, ELT can accommodate varying uses of the same dataset, which is particularly useful in exploratory data analysis phases of ML projects or when multiple models with different data requirements are being developed simultaneously. However, ELT places greater demands on storage systems and query engines, which must handle large amounts of unprocessed information.

In practice, many ML systems employ a hybrid approach, selecting ETL or ELT on a case-by-case basis depending on the specific requirements of each data source or ML model. For example, a system might use ETL for structured data from relational databases where schemas are well-defined and stable, while employing ELT for unstructured data like text or images where transformation requirements may evolve as the ML models are refined.

When implementing streaming components within ETL/ELT architectures, distributed systems principles become critical. The CAP theorem[^fn-cap-theorem] fundamentally constrains streaming system design choices. Apache Kafka prioritizes consistency and partition tolerance, making it ideal for reliable event ordering but potentially experiencing availability issues during network partitions. Apache Pulsar emphasizes availability and partition tolerance, providing better fault tolerance but with relaxed consistency guarantees. Amazon Kinesis balances all three properties through careful configuration but requires understanding these trade-offs for proper deployment. These choices directly impact ML pipeline reliability: a fraud detection system might choose Kafka's strong consistency to ensure transaction ordering, while a recommendation system might prefer Pulsar's availability to maintain service during infrastructure failures.

[^fn-cap-theorem]: **CAP Theorem**: Formulated by Eric Brewer in 2000, states that distributed systems can guarantee at most two of three properties: Consistency (all nodes see the same data simultaneously), Availability (system remains operational), and Partition tolerance (system continues despite network failures). This fundamental constraint shapes all distributed ML system design decisions, from database choices to streaming architectures.

### Data Source Integration {#sec-data-engineering-data-source-integration-53f9}

Regardless of whether ETL or ELT approaches are used, integrating diverse data sources is a key challenge in data ingestion for ML systems. Data may come from various origins, including databases, APIs, file systems, and IoT devices. Each source may have its own data format, access protocol, and update frequency.

Given this source diversity, ML engineers must develop robust connectors or adapters for each data source to effectively integrate these sources. These connectors handle the specifics of data extraction, including authentication, rate limiting, and error handling. For example, when integrating with a REST API, the connector would manage API keys, respect rate limits, and handle HTTP status codes appropriately.

Beyond basic connectivity, source integration often involves data transformation at the ingestion point. This might include parsing JSON or XML responses, converting timestamps to a standard format, or performing basic data cleaning operations. The goal is to standardize the data format as it enters the ML pipeline, simplifying downstream processing.

In addition to data format standardization, it's essential to consider the reliability and availability of data sources. Some sources may experience downtime or have inconsistent data quality. Implementing retry mechanisms, data quality checks, and fallback procedures can help ensure a steady flow of reliable data into the ML system.

### Validation Techniques {#sec-data-engineering-validation-techniques-5b83}

Once data sources are integrated, validation becomes an important step in the ingestion process, ensuring that incoming data meets quality standards and conforms to expected schemas. This step helps prevent downstream issues in ML pipelines caused by data anomalies or inconsistencies.

To achieve these quality objectives, validation at the ingestion stage typically encompasses several key aspects. Schema conformity checks ensure that incoming data adheres to the expected structure, including data types and field names. However, production systems face the reality of schema evolution[^fn-schema-evolution]: fields get renamed (e.g., 'user_id' becoming 'userId'), types change (integers becoming floats), and new fields appear while old ones deprecate. Managing these changes requires versioning strategies where schemas coexist during transition periods, with validation logic that can handle both old and new formats gracefully. Backward compatibility must be maintained to process historical data, while forward compatibility prepares for upcoming changes.

[^fn-schema-evolution]: **Schema Evolution**: The process of adapting data schemas over time as business requirements change. Critical in ML systems because breaking changes can cause pipeline failures. Modern solutions include schema registries (Confluent Schema Registry), backward/forward compatibility rules, and versioning strategies. Production incidents increase 27% for each percentage point increase in schema drift.

Building on these fundamental checks, **Statistical Data Validation** goes beyond basic schema checks to detect subtle data quality degradation. Distribution comparison validates that incoming data maintains statistical properties consistent with training data, such as detecting when user age distributions shift unexpectedly or when categorical feature frequencies change dramatically. Outlier detection using statistical methods like z-score analysis or isolation forests can identify anomalous values that might indicate upstream processing errors or data corruption. For high-volume streams, statistical validation often relies on sampling strategies to balance validation thoroughness with system performance, typically validating 1-10% of records while maintaining detection sensitivity.

Extending these statistical approaches, **Cross-validation between Training and Serving Data** represents a critical production concern where feature distributions in real-time serving data gradually diverge from training distributions. This drift detection requires continuous monitoring of feature statistics, automated alerts when distributions exceed defined thresholds, and decision frameworks for when models require retraining. Modern validation systems implement automated schema evolution detection that flags when new fields appear, existing fields change types, or required fields become optional, enabling graceful handling of upstream changes without pipeline failures.

While these validation approaches provide comprehensive coverage, **Performance Implications of Validation at Scale** require careful trade-off analysis between validation thoroughness and system throughput. Full validation of every record may introduce unacceptable latency for real-time systems, while sampling-based validation reduces overhead but may miss edge cases. Production systems often implement tiered validation where critical fields receive full validation while optional features use statistical sampling. Asynchronous validation patterns enable immediate data processing while background validation detects issues for future remediation.

For example, in a healthcare ML system ingesting patient data, validation might include checking that age values are positive integers, diagnosis codes are from a predefined set, and admission dates are not in the future. Statistical validation would monitor whether the distribution of patient ages remains consistent with training data and alert if emergency department visits spike unexpectedly. This comprehensive approach enables early detection of data quality issues before they impact model accuracy. Comprehensive schema evolution and production version management are covered in @sec-ml-operations and @sec-robust-ai.

### Error Management {#sec-data-engineering-error-management-e2e9}

Complementing validation techniques, effective error handling in data ingestion enables building resilient ML systems. Errors can occur at various points in the ingestion process, from source connection issues to data validation failures. Effective error handling strategies ensure that the ML pipeline can continue to operate even when faced with data ingestion challenges.

Central to resilient system design, a key concept in error handling is graceful degradation. This involves designing systems to continue functioning, possibly with reduced capabilities, when faced with partial data loss or temporary source unavailability. Production systems must handle partial failures where some data sources succeed while others fail, such as when real-time streams continue but batch updates lag, or when certain geographic regions experience outages while others operate normally. Late-arriving data presents another common challenge, requiring systems to decide whether to wait for completeness or proceed with available data based on freshness requirements.

To handle these various failure scenarios, implementing intelligent retry logic for transient errors, such as network interruptions or temporary service outages, requires exponential backoff[^fn-exponential-backoff] strategies to avoid overwhelming recovering services. Many ML systems employ the concept of dead letter queues[^fn-dead-letter-queue], using separate storage for data that fails processing. This allows for later analysis and potential reprocessing of problematic data [@kleppmann2017designing].

[^fn-exponential-backoff]: **Exponential Backoff**: Reattempt strategy where wait time doubles after each failure (1s, 2s, 4s, 8s, etc.) plus random jitter to prevent thundering herd problems. Originally developed for Ethernet collision detection (1980s), now essential for ML systems where thousands of workers might simultaneously reattempt failed operations, potentially overwhelming recovering services.

[^fn-dead-letter-queue]: **Dead Letter Queue**: Storage mechanism for messages that cannot be processed successfully after multiple attempts. Coined in postal services for undeliverable mail, the pattern became critical in distributed ML systems for preserving failed training examples or corrupted feature computations for later analysis, enabling debugging without data loss.

Moving beyond ad-hoc error handling, systematic failure mode analysis for ML data pipelines reveals predictable patterns that require specific engineering countermeasures. Data corruption failures occur when upstream systems introduce subtle format changes, encoding issues, or field value modifications that pass basic validation but corrupt model inputs. Schema evolution failures happen when source systems add fields, rename columns, or change data types without coordination, breaking downstream processing assumptions. Resource exhaustion manifests as gradually degrading performance when data volume growth outpaces capacity planning, eventually causing pipeline failures during peak load periods.

Building on this failure analysis, cascade failure prevention requires circuit breaker[^fn-circuit-breaker] patterns and bulkhead isolation to prevent single component failures from propagating throughout the system. When a feature computation service fails, the system should isolate the failure to prevent it from affecting other feature computations or downstream model serving. Checkpoint-restart mechanisms enable recovery from the last successful processing state rather than full pipeline recomputation, critical for long-running data processing jobs operating on terabyte-scale datasets.

[^fn-circuit-breaker]: **Circuit Breaker Pattern**: Named after electrical circuit breakers, this pattern automatically stops calling a failing service to prevent cascade failures. Popularized by Martin Fowler (2007) and Netflix's Hystrix library, it's critical in ML systems where feature computation failures could bring down entire recommendation pipelines serving millions of users. Has three states: Closed (normal operation), Open (calls blocked), Half-Open (testing recovery).

To complete the resilience picture, automated recovery engineering implements sophisticated strategies beyond simple retry logic. Progressive timeout increases prevent overwhelming struggling services while maintaining rapid recovery for transient issues. Multi-tier fallback systems provide degraded service when primary data sources failâsuch as serving slightly stale cached features when real-time computation fails, or using approximate features when exact computation times out. Comprehensive alerting and escalation procedures ensure human intervention occurs when automated recovery fails, with sufficient diagnostic information to enable rapid debugging. Production patterns for handling distributed failures, circuit breakers, and comprehensive recovery strategies are detailed in @sec-robust-ai.

These concepts become concrete when we consider, for instance, a financial ML system ingesting market data, where error handling might involve falling back to slightly delayed data sources if real-time feeds fail, while simultaneously alerting the operations team to the issue. This approach ensures that the system continues to function and that responsible parties are aware of and can address the problem.

This comprehensive approach to error management ensures that downstream processes have access to reliable, high-quality data for training and inference tasks, even in the face of ingestion challenges. Understanding these concepts of data validation and error handling is essential for students and practitioners aiming to build robust, production-ready ML systems.

With robust error handling in place, once ingestion is complete and data is validated, it is typically loaded into a storage environment suited to the organization's analytical or machine learning needs. Some datasets flow into data warehouses for structured queries, whereas others are retained in data lakes for exploratory or large-scale analyses. Advanced systems may also employ feature stores to provide standardized features for machine learning.

### Continuing the KWS Example {#sec-data-engineering-continuing-kws-example-698c}

Bringing these ingestion concepts together, our KWS example demonstrates how production systems employ both streaming and batch ingestion patterns. The streaming pattern handles real-time audio data from active devices, where wake words must be detected with minimal latency. This requires careful implementation of pub/sub mechanismsâsuch as using Apache Kafka-like streams to buffer incoming audio data and enable parallel processing across multiple inference servers.

Parallel to this real-time processing, the system processes batch data for model training and updates. This includes ingesting new wake word recordings from crowdsourcing efforts, synthetic data from voice generation systems, and validated user interactions. The batch processing typically follows an ETL pattern, where audio data is preprocessed (normalized, filtered, segmented) before being stored in a format optimized for model training.

Beyond basic ingestion patterns, KWS systems must integrate data from diverse sources, such as real-time audio streams from deployed devices, crowdsourced recordings from data collection platforms etc. Each source presents unique challenges. Real-time audio streams require rate limiting to prevent system overload during usage spikes. Crowdsourced data needs systematic validation to ensure recording quality and correct labeling. Synthetic data must be verified for realistic representation of wake word variations.

Given these diverse data requirements, KWS systems employ sophisticated error handling mechanisms due to the nature of voice interaction. When processing real-time audio, dead letter queues store failed recognition attempts for analysis, helping identify patterns in false negatives or system failures. Data validation becomes important for maintaining wake word detection accuracyâincoming audio must be checked for quality issues such as clipping, noise levels, and appropriate sampling rates.

To illustrate these validation requirements, consider a smart home device processing the wake word "Alexa." The ingestion pipeline must validate:

* Audio quality metrics (signal-to-noise ratio, sample rate, bit depth)
* Recording duration (typically 1-2 seconds for wake words)
* Background noise levels
* Speaker proximity indicators

Invalid samples are routed to dead letter queues for analysis, while valid samples are processed in real-time for wake word detection.

This comprehensive KWS case study illustrates how real-world ML systems must carefully balance different ingestion patterns, handle multiple data sources, and maintain robust error handlingâwhile meeting strict latency and reliability requirements. The lessons from KWS systems apply broadly to other ML applications requiring real-time processing capabilities alongside continuous model improvement.

With reliable data ingestion established, we now focus on systematic data transformation that implements our four-pillar framework at the processing level. Data processing decisions must maintain quality standards while ensuring reliability, supporting scalability requirements, and adhering to governance principles throughout the transformation pipeline.

## Systematic Data Processing {#sec-data-engineering-data-processing-c336}

With reliable data ingestion established, data processing represents the systematic application of our quality pillar, transforming raw data into ML-ready formats while maintaining reliability and scalability standards. This stage implements the data quality requirements defined in our problem definition phase, ensuring that every transformation preserves data integrity while improving model readiness.

For our KWS system, processing decisions directly impact all four pillars: quality transformations must preserve acoustic characteristics essential for wake word detection; reliability requires consistent processing despite varying audio formats; scalability demands efficient algorithms that handle millions of audio streams; and governance ensures privacy-preserving transformations that protect user voice data.

Data processing implementation follows the architectural patterns (ETL or ELT) established in @sec-data-engineering-pipeline-basics-31ba during pipeline design[^fn-etl-vs-elt]. As detailed in the pipeline architecture section, the choice between ETL and ELT directly impacts when and where transformations occur, influencing resource allocation, system flexibility, and operational complexity.

[^fn-etl-vs-elt]: **ETL vs ELT Performance**: ETL processes 1-10GB/hour on traditional systems but scales poorly; ELT leverages cloud warehouses like Snowflake (100GB/hour+) and BigQuery (1TB/hour+) by utilizing distributed compute for transformations. This 10-100x performance difference drives modern data architecture decisions.

Regardless of architectural approach, the following data processing steps ensure that data becomes clean, relevant, and optimally formatted for machine learning algorithms. These transformations implement the quality requirements defined during problem definition while maintaining reliability, scalability, and governance standards across the processing pipeline.

### Cleaning Techniques {#sec-data-engineering-cleaning-techniques-e81b}

At the foundation of data processing, data cleaning involves identifying and correcting errors, inconsistencies, and inaccuracies in datasets. Raw data frequently contains issues such as missing values, duplicates, or outliers that can significantly impact model performance if left unaddressed.

To address these data quality issues, data cleaning might involve removing duplicate records, handling missing values through imputation or deletion, and correcting formatting inconsistencies. For instance, in a customer database, names might be inconsistently capitalized or formatted. A data cleaning process would standardize these entries, ensuring that "John Doe," "john doe," and "DOE, John" are all treated as the same entity.

Beyond these basic cleaning operations, ensuring consistency between training and serving pipelines represents the most critical challenge in data cleaning. Studies show that training-serving skew causes approximately 70% of production ML failures when cleaning logic differs between environments [@sculley2015hidden]. Consider a simple example: normalizing transaction amounts during training by removing currency symbols and converting to floats, but forgetting to apply identical preprocessing during serving. This seemingly minor inconsistency can degrade model accuracy by 20-40%, as the model receives differently formatted inputs than it was trained on.

Complementing consistency management, outlier detection and treatment is another important aspect of data cleaning. Outliers can sometimes represent valuable information about rare events, but they can also be the result of measurement errors or data corruption. ML practitioners must carefully consider the nature of their data and the requirements of their models when deciding how to handle outliers.

### Data Quality Assessment {#sec-data-engineering-data-quality-assessment-b8b7}

Building on these cleaning techniques, quality assessment goes hand in hand with data cleaning, providing a systematic approach to evaluating the reliability and usefulness of data. This process involves examining various aspects of data quality, including accuracy, completeness, consistency, and timeliness. In production systems, data quality degrades in subtle ways that basic metrics miss: fields that never contain nulls suddenly show sparse patterns, numeric distributions drift from their training ranges, or categorical values appear that weren't present during model development.

To address these subtle degradation patterns, production quality monitoring requires specific metrics beyond simple missing value counts. Critical indicators include null value patterns by feature (sudden increases suggest upstream failures), count anomalies (10x increases often indicate data duplication or pipeline errors), value range violations (prices becoming negative, ages exceeding realistic bounds), and join failure rates between data sources. Statistical drift detection becomes essential by monitoring means, variances, and quantiles of features over time to catch gradual degradation before it impacts model performance. For example, in an e-commerce recommendation system, the average user session length might gradually increase from 8 minutes to 12 minutes over six months due to improved site design, but a sudden drop to 3 minutes suggests a data collection bug. Similarly, if product price features trained on a range of $10-$500 suddenly show values of $50,000, this indicates either new luxury items (requiring model retraining) or data corruption (requiring investigation). Comprehensive production monitoring strategies and drift detection techniques are covered in @sec-ml-operations and @sec-benchmarking-ai.

Supporting these monitoring requirements, tools and techniques for quality assessment range from simple statistical measures to more complex machine learning-based approaches. Data profiling tools provide summary statistics and visualizations that help identify potential quality issues, while more advanced techniques employ unsupervised learning algorithms to detect anomalies or inconsistencies in large datasets. Establishing clear quality metrics and thresholds is essential for maintaining data quality over time, with regular assessments ensuring that data entering the ML pipeline meets the necessary standards for reliable model training and inference.

### Transformation Techniques {#sec-data-engineering-transformation-techniques-2d54}

Once data quality is assured through cleaning and assessment, transformation converts the data from its raw form into a format more suitable for analysis and modeling. This process can include a wide range of operations, from simple conversions to complex mathematical transformations.

Central to effective transformation, common transformation tasks include normalization and standardization[^fn-normalization-techniques], which scale numerical features to a common range or distribution. For example, in a housing price prediction model, features like square footage and number of rooms might be on vastly different scales. Normalizing these features ensures that they contribute more equally to the model's predictions [@bishop2006pattern]. However, transformation consistency between training and serving becomes critical here: if training data is normalized using the global dataset mean and standard deviation, the serving pipeline must store and apply these exact same statistics. Computing normalization parameters on serving data batches leads to distribution shift and degraded model performance.

[^fn-normalization-techniques]: **Normalization in ML**: Min-max scaling (range 0-1) vs z-score standardization (mean=0, std=1) can dramatically affect model performance. Gradient descent converges 10-100x faster on normalized features [@goodfellow2016deep], while tree-based models (Random Forest, XGBoost) are largely unaffected by scaling differences.

Beyond numerical scaling, other transformations might involve encoding categorical variables, handling date and time data, or creating derived features. For instance, one-hot encoding[^fn-categorical-encoding] is often used to convert categorical variables into a format that can be readily understood by many machine learning algorithms. Training-serving consistency requires careful handling of categorical encodingsâif the training dataset encounters categories A, B, and C, the serving pipeline must handle the same set, including unknown categories that weren't present during training.

[^fn-categorical-encoding]: **Categorical Encoding Impact**: One-hot encoding can explode feature dimensionsâa single categorical variable with 1000 categories creates 1000 binary features. High-cardinality encoding techniques like target encoding or embeddings (used in deep learning) can reduce dimensions by 10-100x while preserving predictive power.

### Feature Engineering {#sec-data-engineering-feature-engineering-ea20}

Building on these foundational transformations, feature engineering is the process of using domain knowledge to create new features that make machine learning algorithms work more effectively. This step is often considered more of an art than a science, requiring creativity and deep understanding of both the data and the problem at hand.

To leverage domain expertise effectively, feature engineering might involve combining existing features, extracting information from complex data types, or creating entirely new features based on domain insights. For example, in a retail recommendation system, engineers might create features that capture the recency, frequency, and monetary value of customer purchases, known as RFM analysis[^fn-rfm-analysis] [@kuhn2013applied].

[^fn-rfm-analysis]: **RFM Analysis Origins**: Developed by direct marketers in the 1960s, RFM (Recency, Frequency, Monetary) analysis segments customers by purchase behavior. Modern ML systems extend this to 100+ behavioral features, but the core RFM triplet remains among the most predictive features for customer lifetime value models.

Given these creative possibilities, the importance of feature engineering cannot be overstated. Well-engineered features can often lead to significant improvements in model performance, sometimes outweighing the impact of algorithm selection or hyperparameter tuning.

### Processing Pipeline Design {#sec-data-engineering-processing-pipeline-design-48d4}

Integrating these cleaning, assessment, transformation, and feature engineering steps, processing pipelines bring together the various data processing steps into a coherent, reproducible workflow. These pipelines ensure that data is consistently prepared across training and inference stages, reducing the risk of data leakage and improving the reliability of ML systems.

To support these workflow requirements, modern ML frameworks and tools often provide capabilities for building and managing data processing pipelines. For instance, Apache Beam[^fn-apache-beam] and TensorFlow Transform allow developers to define data processing steps that can be applied consistently during both model training and serving. The choice of data processing framework must align with the broader ML framework ecosystem discussed in @sec-ai-frameworks, where framework-specific data loaders and preprocessing utilities can significantly impact development velocity and system performance.

[^fn-apache-beam]: **Apache Beam Architecture**: Google's unified programming model (2016) abstracts batch and streaming data processing across multiple execution engines (Dataflow, Spark, Flink). Its key innovation: write once, run anywhere with automatic scaling from single machines to thousands of workers processing petabyte-scale data.

Beyond tool selection, effective pipeline design involves considerations such as modularity, scalability, and version control. Modular pipelines allow for easy updates and maintenance of individual processing steps. Version control for pipelines is crucial, ensuring that changes in data processing can be tracked and correlated with changes in model performance. This modular breakdown of pipeline components is well illustrated by TensorFlow Extended in @fig-tfx-pipeline-example, which shows the complete flow from initial data ingestion through to final model deployment.

::: {#fig-tfx-pipeline-example fig-env="figure" fig-pos="htb"}
```{.tikz}
\begin{tikzpicture}[line join=round,font=\usefont{T1}{phv}{m}{n}\small]
\tikzset{%
    Line/.style={line width=0.75pt,black!50,text=black
},
  Box/.style={align=flush center,
    inner xsep=2pt,
    node distance=0.45,
    draw=BlueLine,
    line width=0.75pt,
     top color=white, bottom color=BlueL,
    text width=27mm,
    minimum width=27mm, minimum height=8mm
  },
  Box2/.style={Box,draw=GreenLine,
    top color=white, bottom color=GreenL
  },
}

\tikzset{
channel/.pic={
\pgfkeys{/channel/.cd, #1}
\begin{scope}[yscale=\scalefac,xscale=\scalefac,every node/.append style={scale=\scalefac}]
\draw[draw=BrownLine,fill=BrownLine!10](0,0.20)coordinate(W1)--
(0.75,-0.20)coordinate(W2)coordinate(\picname-W2)--(1.75,0.4)coordinate(W3)--
(1.0,0.8)coordinate(W4)coordinate(\picname-W4)--cycle;
\draw[BrownLine,shorten <=4pt,shorten >=5pt]($(W4)!0.3!(W1)$)--($(W3)!0.3!(W2)$);
\draw[BrownLine,shorten <=4pt,shorten >=7pt]($(W4)!0.5!(W1)$)--($(W3)!0.5!(W2)$);
\draw[BrownLine,shorten <=4pt,shorten >=9pt]($(W4)!0.7!(W1)$)--($(W3)!0.7!(W2)$);
\end{scope}
        },
}
\pgfkeys{
  /channel/.cd,
  channelcolor/.store in=\channelcolor,
  drawchannelcolor/.store in=\drawchannelcolor,
  scalefac/.store in=\scalefac,
  picname/.store in=\picname,
  channelcolor=BrownLine,
  drawchannelcolor=BrownLine,
  scalefac=1,
  picname=C
}

\node[Box](B1){ExampleGen};
\node[Box,below=of B1](B2){StatisticsGen};
\node[Box,below=of B2](B3){SchemaGen};
\node[Box,left=of B3](B4){Example Validator};
\node[Box,right=of B3](B5){Transform};
\node[Box2,below left=0.7 and -0.7 of B5](B6){Tuner};
\node[Box2,below right=0.7 and -0.7 of B5](B7){Trainer};
\node[Box2,below left=0.7 and -0.7 of B7](B8){Evaluator};
\node[Box2,below right=0.7 and -0.7 of B7](B9){Infra Validator};
\node[Box2,below=of B9](B10){Pusher};
%
\draw[Line,-latex](B1)edge (B2) (B2)edge (B3)
(B2) -|(B4);
\draw[Line,-latex](B1)-|(B5);
\draw[Line,-latex](B5)--++(0,-0.75)-|(B6);
\draw[Line,-latex](B5)--++(0,-0.75)-|(B7);
\draw[Line,-latex](B7)--++(0,-0.75)-|(B8);
\draw[Line,-latex](B7)--++(0,-0.75)-|(B9);
\draw[Line](B5)--(B3);
\draw[Line,-latex](B9)--(B10);
%
\begin{scope}[local bounding box=F1,shift={($(B10)+(-4.6,-2.25)$)},
scale=1,every node/.append style={scale=1}]
\foreach \j in {1,2,3} {
\pic at ({\j*0.02}, {0.16*\j}) {channel={scalefac=0.8,picname=1\j}};
}
\node[below=3pt of 11-W2,align=center]{Tensorflow\\ serving};
\end{scope}
\begin{scope}[local bounding box=F2,shift={($(B10)+(-2.6,-2.25)$)},
scale=1,every node/.append style={scale=1}]
\foreach \j in {1,2,3} {
\pic at ({\j*0.02}, {0.16*\j}) {channel={scalefac=0.8,picname=2\j}};
}
\node[below=3pt of 21-W2,align=center]{Tensorflow\\ JS};
\end{scope}
\begin{scope}[local bounding box=F3,shift={($(B10)+(-0.6,-2.25)$)},
scale=1,every node/.append style={scale=1}]
\foreach \j in {1,2,3} {
\pic at ({\j*0.02}, {0.16*\j}) {channel={scalefac=0.8,picname=3\j}};
}
\node[below=3pt of 31-W2,align=center]{Tensorflow\\ Lite};
\end{scope}
\foreach \j in {1,2,3} {
\draw[Line,-latex](B10)--++(0,-0.7)-|(\j3-W4);
}
\end{tikzpicture}
```
**Data Processing Pipeline**: A modular end-to-end ML pipeline, as implemented in TensorFlow extended, highlighting key stages from raw data ingestion to trained model deployment and serving. this decomposition enables independent development, versioning, and scaling of each component, improving maintainability and reproducibility of ML systems.
:::

### Scalability Considerations {#sec-data-engineering-scalability-considerations-1083}

While modular pipeline design enables maintainable systems, as datasets grow larger and ML systems become more complex, the scalability of data processing becomes increasingly important. Consider the data processing stages we've discussedâcleaning, quality assessment, transformation, and feature engineering. When these operations must handle terabytes of data, a single machine becomes insufficient. The cleaning techniques that work on gigabytes of data in memory must be redesigned to work across distributed systems.

These challenges become concrete in several ways. Quality assessment that scans for null patterns or value range violations must process data faster than it arrives. Feature engineering operations like normalization require computing statistics across the entire dataset before transforming individual records. Transformation pipelines that work sequentially on small datasets create bottlenecks when processing massive volumes.

To address these scaling bottlenecks, the solution lies in partitioning data across multiple computing resources, but introduces coordination challenges rooted in physical constraints. Distributed coordination is fundamentally limited by the fact that achieving consistency requires synchronization, which is bounded by network round-trip times. When local operations complete in microseconds but network coordination requires milliseconds, the 1000x latency difference creates unavoidable bottlenecks. This constraint explains why some operations are inherently harder to parallelize than others.

These physical constraints become evident when we consider computing global statistics for normalization when data is split across 100 machines. Each partition can compute local statistics quickly, but combining these into global statistics requires information from all partitions. This coordination becomes exponentially more complex as partition count increases and geographic distribution expands. Ensuring consistent quality checks when different machines see different data subsets requires sophisticated synchronization protocols that balance consistency guarantees with system performance.

Given these coordination complexities, data locality becomes critical at this scale. Moving 1TB of training data across the network takes 100+ seconds at 10GB/s, while local SSD access requires only 200 seconds at 5GB/s. This bandwidth hierarchy drives ML system design toward compute-follows-data architectures rather than data-follows-compute patterns. Geographic distribution amplifies these challenges significantly. Cross-datacenter coordination must handle network latency (often 50-200ms between regions), partial failures where some datacenters become unreachable, and regulatory constraints that prevent data from crossing national boundaries.

To address these bandwidth hierarchies, distributed frameworks like Apache Spark implement map-reduce style partitioning, but the fundamental challenge remains: operations requiring global coordination (like computing dataset statistics for normalization) create bottlenecks because they require information from all partitions. This is why techniques like approximate algorithms and hierarchical aggregation become essential at scale. Understanding which operations can be parallelized easily versus those that require expensive coordination determines system architecture and performance characteristics. When processing nodes can access local data at SSD speeds (1-7 GB/s) or RAM speeds (50-200 GB/s) but must coordinate over networks limited to 1-10 GB/s, the bandwidth mismatch creates fundamental coordination bottlenecks. Geographic distribution amplifies these challenges significantly. Cross-datacenter coordination must handle network latency (often 50-200ms between regions), partial failures where some datacenters become unreachable, and regulatory constraints that prevent data from crossing national boundaries. Consider a global ML system processing user interactions: European data must remain in EU datacenters under GDPR, while Chinese data requires local processing. These constraints force complex partitioning strategies and sophisticated coordination protocols to maintain data consistency while respecting sovereignty requirements. Understanding these trade-offs becomes essential for designing efficient data pipelines, particularly when implementing the comprehensive distributed architectures covered in @sec-ai-workflow and the advanced partitioning strategies detailed in @sec-ml-operations.

Beyond coordination challenges, another important consideration is the balance between preprocessing and on-the-fly computation. While extensive preprocessing can speed up model training and inference, it can also lead to increased storage requirements and potential data staleness. Production systems often implement hybrid approaches, preprocessing computationally expensive features while computing rapidly changing features on-the-fly. This balance depends on storage costs, computation resources, and freshness requirements specific to each use case.

Effective data processing forms the cornerstone of successful ML systems. By carefully cleaning, transforming, and engineering data, practitioners can significantly improve the performance and reliability of their models. As the field of machine learning continues to evolve, so too do the techniques and tools for data processing, making this an exciting and dynamic area of study and practice.

### Continuing the KWS Example {#sec-data-engineering-continuing-kws-example-8ed1}

Applying these data processing concepts to our running example, the KWS system requires careful cleaning of audio recordings to ensure reliable wake word detection. Raw audio data often contains various imperfectionsâbackground noise, clipped signals, varying volumes, and inconsistent sampling rates. For example, when processing the wake word "Alexa," the system must clean recordings to standardize volume levels, remove ambient noise, and ensure consistent audio quality across different recording environments, all while preserving the essential characteristics that make the wake word recognizable.

Once cleaning is established, quality assessment becomes important for KWS systems. Quality metrics for KWS data are uniquely focused on audio characteristics, including background noise levels, audio clarity scores, and speaking rate consistency. For instance, a KWS quality assessment pipeline might automatically flag recordings where background noise exceeds acceptable thresholds or where the wake word is spoken too quickly or unclearly, ensuring only high-quality samples are used for model development.

To ensure practical relevance, these quality metrics must be carefully calibrated to reflect real-world operating conditions. A robust training dataset incorporates both pristine recordings and samples containing controlled levels of environmental variations. For instance, while recordings with signal-masking interference are excluded, the dataset should include samples with measured background acoustics, variable speaker distances, and concurrent speech or other forms of audio signals. This approach to data diversity ensures the system maintains wake word detection reliability across the full spectrum of deployment environments and acoustic conditions.

With quality controls in place, transforming audio data for KWS involves converting raw waveforms into formats suitable for ML models. The typical transformation pipeline converts audio signals into standardized feature representations that emphasize speech-relevant characteristics while reducing noise and variability across different recording conditions. This transformation must be consistently applied across both training and inference, often with additional considerations for real-time processing on edge devices.

@fig-spectrogram-example illustrates this transformation process. The top panel shows a raw waveform of a simulated audio signal, which consists of a sine wave mixed with noise. This time-domain representation highlights the challenges posed by real-world recordings, where noise and variability must be addressed. The middle panel shows a frequency-based representation of the signal, which maps its frequency content over time, providing a detailed view of how energy is distributed across frequencies. The bottom panel shows compressed feature coefficients that emphasize speech-related characteristics, making them well-suited for KWS tasks.

![**Audio Feature Transformation**: Advanced audio features compress raw audio waveforms into representations that emphasize perceptually relevant characteristics for machine learning tasks. This transformation reduces noise and data dimensionality while preserving essential speech information, improving model performance in applications like keyword spotting.](images/png/kws_spectrogram.png){#fig-spectrogram-example fig-pos="t!"}

Building on these transformed representations, feature engineering for KWS focuses on extracting characteristics that help distinguish wake words from background speech. Engineers might create features capturing tonal variations, speech energy patterns, or temporal characteristics. For the wake word "Alexa," features might include energy distribution across frequency bands, pitch contours, and duration patterns that characterize typical pronunciations. While hand-engineered speech features have seen much success, learned features [@zeghidour2021leaf] are increasingly common.

Integrating all these processing components, KWS processing pipelines must handle both batch processing for training and real-time processing for inference. The pipeline typically includes stages for audio preprocessing, feature extraction, and quality filtering. These pipelines must be designed to operate efficiently on edge devices while maintaining consistent processing steps between training and deployment.

## Data Labeling {#sec-data-engineering-data-labeling-95e7}

With systematic data processing established, data labeling emerges as a particularly complex systems challenge within the broader data engineering landscape. As training datasets grow to millions or billions of examples, the infrastructure supporting labeling operations becomes increasingly critical to system performance.

Building on our processing foundations, modern machine learning systems must efficiently handle the creation, storage, and management of labels across their data pipeline. The systems architecture must support various labeling workflows while maintaining data consistency, ensuring quality, and managing computational resources effectively. These requirements compound when dealing with large-scale datasets or real-time labeling needs.

These storage and management requirements represent only part of the challenge. The systematic challenges extend beyond just storing and managing labels. Production ML systems need robust pipelines that integrate labeling workflows with data ingestion, preprocessing, and training components. These pipelines must maintain high throughput while ensuring label quality and adapting to changing requirements. For instance, a speech recognition system might need to continuously update its training data with new audio samples and corresponding transcription labels, requiring careful coordination between data collection, labeling, and training subsystems.

Given these diverse workflow requirements, infrastructure requirements vary significantly based on labeling approach and scale. Manual expert labeling may require specialized interfaces and security controls, while automated labeling systems need substantial compute resources for inference. Organizations must carefully balance these requirements against performance needs and resource constraints.

Understanding these infrastructure trade-offs, we explore how data labeling significantly shapes machine learning system design. From storage architectures to quality control pipelines, each aspect of the labeling process introduces unique technical challenges that ripple throughout the ML infrastructure. Understanding these systems-level implications enables building reliable, scalable labeling solutions which are an integral part of data engineering.

### Types of Labels {#sec-data-engineering-types-labels-76ae}

To build effective labeling systems, we must first understand how different types of labels affect our system architecture and resource requirements. Consider a practical example: building a smart city system that needs to detect and track various objects like vehicles, pedestrians, and traffic signs from video feeds. Labels capture information about key tasks or concepts.

* **Classification labels** represent the simplest form, categorizing images with a specific tag or (in multi-label classification) tags (e.g., labeling an image as "car" or "pedestrian"). While conceptually straightforward, a production system processing millions of video frames must efficiently store and retrieve these labels.

* **Bounding boxes** extend beyond simple classification by identifying object locations, drawing a box around each object of interest. Our system now needs to track not just what objects exist, but where they are in each frame. This spatial information introduces new storage and processing challenges, especially when tracking moving objects across video frames.

* **Segmentation maps** provide the most comprehensive information by classifying objects at the pixel level, highlighting each object in a distinct color. For our traffic monitoring system, this might mean precisely outlining each vehicle, pedestrian, and road sign. These detailed annotations significantly increase our storage and processing requirements.

@fig-labels illustrates the common label types:

![**Data Annotation Granularity**: Increasing levels of detail in data labelingâfrom bounding boxes to pixel-level segmentationâimpact both annotation cost and potential model accuracy. Fine-grained segmentation provides richer information for training but demands significantly more labeling effort and storage capacity than coarser annotations.](images/png/cs249r_labels_new.png){#fig-labels width=90%}

Given these increasing complexity levels, the choice of label format depends heavily on our system requirements and resource constraints [@10.1109/ICRA.2017.7989092]. While classification labels might suffice for simple traffic counting, autonomous vehicles need detailed segmentation maps to make precise navigation decisions. Leading autonomous vehicle companies often maintain hybrid systems that store multiple label types for the same data, allowing flexible use across different applications.

Extending beyond these basic label types, production systems must also handle rich metadata. The Common Voice dataset [@ardila2020common], for instance, exemplifies this in its management of audio data for speech recognition. The system tracks speaker demographics for model fairness, recording quality metrics for data filtering, validation status for label reliability, and language information for multilingual support.

To address these metadata challenges, modern labeling platforms have built sophisticated metadata management systems to handle these complex relationships. This metadata becomes important for maintaining and managing data quality and debugging model behavior. If our traffic monitoring system performs poorly in rainy conditions, having metadata about weather conditions during data collection helps identify and address the issue. The infrastructure must efficiently index and query this metadata alongside the primary labels.

These metadata requirements demonstrate how the choice of label type cascades through our entire system design. A system built for simple classification labels would need significant modifications to handle segmentation maps efficiently. The infrastructure must optimize storage systems for the chosen label format, implement efficient data retrieval patterns for training, maintain quality control pipelines for validation, and manage version control for label updates. Resource allocation becomes particularly critical as data volume grows, requiring careful capacity planning across storage, compute, and networking components.

### Annotation Techniques {#sec-data-engineering-annotation-techniques-6ebe}

With label types and their system implications established, we examine how different annotation techniques address these requirements. Manual labeling by experts is the primary approach in many annotation pipelines. This method produces high-quality results but also raises considerable system design challenges. For instance, in medical imaging systems[^fn-medical-imaging], experienced radiologists offer essential annotations. Such systems necessitate specialized interfaces for accurate labeling, secure data access controls to protect patient privacy, and reliable version control mechanisms to monitor annotation revisions.

[^fn-medical-imaging]: **Medical Imaging AI Revolution**: The 2012 AlexNet breakthrough began with ImageNet's 14 million labeled images, but medical AI required specialized datasets. The NIH Clinical Center released 112,120 chest X-ray images in 2017, becoming one of the largest public medical imaging datasets for ML research. Despite the dependable outcomes of expert labeling, the scarcity and high expenses of specialists render it challenging to implement on a large scale for extensive datasets.

To address these scalability limitations, crowdsourcing offers a path to greater scalability by distributing annotation tasks across many annotators [@victor2019machine]. Crowdsourcing enables non-experts to distribute annotation tasks, often through dedicated platforms [@victor2019machine]. Several companies have emerged as leaders in this space, building sophisticated platforms for large-scale annotation. For instance, companies such as [Scale AI](https://scale.com/) specialize in managing thousands of concurrent annotators through their platform. [Appen](https://www.appen.com/) focuses on linguistic annotation and text data, while [Labelbox](https://labelbox.com/) has developed specialized tools for computer vision tasks. These platforms allow dataset creators to access a large pool of annotators, making it possible to label vast amounts of data relatively quickly.

Beyond human-powered approaches, weakly supervised and programmatic methods represent a third approach, using automation to reduce manual effort [@ratner2018snorkel]. These systems leverage existing knowledge bases and heuristics to automatically generate labels. For example, distant supervision techniques might use a knowledge base to label mentions of companies in text data. While these methods can rapidly label large datasets, they require substantial compute resources for inference, sophisticated caching systems to avoid redundant computation, and careful monitoring to manage potential noise and bias.

Recognizing the limitations of any single approach, most production systems combine multiple annotation approaches to balance speed, cost, and quality. A common pattern employs programmatic labeling for initial coverage, followed by crowdsourced verification and expert review of uncertain cases. This hybrid approach requires careful system design to manage the flow of data between different annotation stages. The infrastructure must track label provenance, manage quality control at each stage, and ensure consistent data access patterns across different annotator types.

These hybrid workflows demonstrate how the choice of annotation method significantly impacts system architecture. Expert-only systems might employ centralized architectures with high-speed access to a single data store. Crowdsourcing demands distributed architectures to handle concurrent annotators. Automated systems need substantial compute resources and caching infrastructure. Many organizations implement tiered architectures where different annotation methods operate on different subsets of data based on complexity and criticality.

Across all these architectural variations, clear guidelines and thorough training remain essential regardless of the chosen architecture. The system must provide consistent interfaces, documentation, and quality metrics across all annotation methods. This becomes particularly challenging when managing diverse annotator pools with varying levels of expertise. Some platforms address this by offering access to specialized annotators. For instance, providing medical professionals for healthcare datasets or domain experts for technical content.

### Label Quality Assessment {#sec-data-engineering-label-quality-assessment-9c43}

Regardless of the annotation technique chosen, label quality determines machine learning system performance. A model can only be as good as its training data. However, ensuring quality at scale presents significant systems challenges. The primary challenge stems from label uncertainty.

@fig-hard-labels illustrates common failure modes in labeling systems: some errors arise from data quality issues (like the blurred frog image), while others require deep domain expertise (as with the black stork identification). Even with clear instructions and careful system design, some fraction of labels will inevitably be incorrect [@northcutt2021pervasive, @thyagarajan2023multilabel].

![**Labeling Ambiguity**: How subjective or difficult examples, such as blurry images or rare species, can introduce errors during data labeling, highlighting the need for careful quality control and potentially expert annotation. Source: [@northcutt2021pervasive].](images/png/label-errors-examples_new.png){#fig-hard-labels}

Given these fundamental quality challenges, production ML systems implement multiple layers of quality control to address these challenges. Typically, systematic quality checks continuously monitor the labeling pipeline. These systems randomly sample labeled data for expert review and employ statistical methods to flag potential errors. The infrastructure must efficiently process these checks across millions of examples without creating bottlenecks in the labeling pipeline.

Beyond random sampling approaches, collecting multiple labels per data point, often referred to as "consensus labeling," can help identify controversial or ambiguous cases. Professional labeling companies have developed sophisticated infrastructure for this process. For example, [Labelbox](https://labelbox.com/) has consensus tools that track inter-annotator agreement rates and automatically route controversial cases for expert review. [Scale AI](https://scale.com) implements tiered quality control, where experienced annotators verify the work of newer team members.

While technical infrastructure provides the foundation, successful labeling systems must also consider human factors. When working with annotators, organizations need robust systems for training and guidance. This includes good documentation, clear examples of correct labeling, and regular feedback mechanisms. For complex or domain-specific tasks, the system might implement tiered access levels, routing challenging cases to annotators with appropriate expertise.

Extending beyond training and guidance, ethical considerations also significantly impact system design. For datasets containing potentially disturbing content, systems should implement protective features like grayscale viewing options [@googleinformation]. This requires additional image processing pipelines and careful interface design. We need to develop workload management systems that track annotator exposure to sensitive content and enforce appropriate limits.

These protective measures demonstrate how the quality control system itself generates substantial data that must be efficiently processed and monitored. Organizations typically track inter-annotator agreement rates, label confidence scores, time spent per annotation, error patterns and types, annotator performance metrics, and bias indicators. These metrics must be computed and updated efficiently across millions of examples, often requiring dedicated analytics pipelines.

Complementing these performance metrics, regular bias audits are another critical component of quality control. Systems must monitor for cultural, personal, or professional biases that could skew the labeled dataset. This requires infrastructure for collecting and analyzing demographic information, measuring label distributions across different annotator groups, identifying systematic biases in the labeling process, and implementing corrective measures when biases are detected.

Underlying all these quality control mechanisms, perhaps the most important aspect is that the process must remain iterative. As new challenges emerge, quality control systems must adapt and evolve. Through careful system design and implementation of these quality control mechanisms, organizations can maintain high label quality even at a massive scale.

### AI in Annotation {#sec-data-engineering-ai-annotation-41b4}

Building on these quality control foundations, organizations increasingly leverage AI to accelerate and enhance their labeling pipelines as machine learning systems grow in scale and complexity. This approach introduces new system design considerations around model deployment, resource management, and human-AI collaboration. At the core of these scaling challenges lies data volume. Manual annotation alone cannot keep pace with modern ML systems' data needs. As illustrated in @fig-weak-supervision, AI assistance offers several paths to scale labeling operations, each requiring careful system design to balance speed, quality, and resource usage.

::: {#fig-weak-supervision fig-env="figure" fig-pos="htb"}
```{.tikz}
\begin{tikzpicture}[font=\small\usefont{T1}{phv}{m}{n}]
%
\tikzset{%
  Line/.style={line width=1.0pt,black!50,text=black},
  Box/.style={align=flush center,
   inner sep=5pt,
    node distance=0.75,
    draw=GreenLine,
    line width=0.75pt,
    fill=GreenL,
    text width=55mm,
    minimum width=53mm, minimum height=9mm
  },
   Box1/.style={Box,
    node distance=0.35,
    draw=OrangeLine,
    line width=0.75pt,
    fill=OrangeL,
    text width=31mm,
    minimum width=31mm, minimum height=8.2mm
  },
  Box2/.style={Box,
    node distance=0.5,
    draw=BlueLine,
    line width=0.75pt,
    fill=BlueL,
    text width=42mm,
    minimum width=42mm, minimum height=9mm
  },
 Text/.style={%
    inner sep=2pt,
    draw=none,
    line width=0.75pt,
    fill=TextColor,
    text=black,
    font=\footnotesize\usefont{T1}{phv}{m}{n},
    align=flush center,
    minimum width=7mm, minimum height=5mm
  },
}
%
\node[Box,text width=59mm,minimum width=59mm, minimum height=10mm,
           fill=RedL,draw=RedLine](B1){\textbf{How to get more labeled training data?}};
\node[Box, node distance=1.05,below=of B1,xshift=9mm](B2){\textbf{Traditional Supervision:} Have subject
              matter experts (SMEs) hand-label more training data};
\node[Box,below=of B2](B3){\textbf{Semi-supervised Learning:} Use structural
            assumptions to automatically leverage unlabeled data};
\node[Box,below=of B3](B4){\textbf{Weak Supervision:}\\ Get lower-quality
            labels more efficiently and/or at a higher abstraction level};
\node[Box,below=of B4](B5){\textbf{Transfer Learning:}\\ Use models
            already trained on a different task};
%
\node[Box2,above right=0.7 and 1.75 of B2,fill=BrownL,draw=BrownLine](2B1){Too expensive!};
\node[Box2,below = of 2B1,fill=BrownL,draw=BrownLine](2B2){\textbf{Active Learning:} Estimate
            which points are most valuable to solicit labels for};
%
\node[Box2,above right=1.1 and 1.75 of B4](2B3){Get cheaper, lower-quality labels from non-experts};
\node[Box2,below =of 2B3](2B4){Get higher-level supervision
            over unlabeled data from SMEs};
\node[Box2,below =of 2B4](2B5){Use one or more
              (noisy/biased) pre-trained models to provide supervision};
%
\node[Box1,above right=0.25 and 1.45 of 2B3](3B1){Heuristics};
\node[Box1,below =of 3B1](3B2){Distant Supervision};
\node[Box1,below =of 3B2](3B3){Constraints};
\node[Box1,below =of 3B3](3B4){Expected distributions};
\node[Box1,below =of 3B4](3B5){Invariances};
%%
\foreach \x in{2,3,4,5}{
\draw[-latex,Line](B1.191)|-(B\x);
}

\foreach \x in{1,2}{
\draw[-latex,Line](B2.east)--++(0:1.1)|-(2B\x);
}

\foreach \x in{3,4,5}{
\draw[-latex,Line](B4.355)--++(0:1.1)|-(2B\x);
}

\foreach \x in{1,2,3,4,5}{
\draw[-latex,Line](2B4.east)--++(0:0.8)|-(3B\x);
}
\draw[-latex,Line](2B2)--++(270:1.35)--++(180:3.5)|-(B4.05);
\draw[-latex,Line](B5.355)-|(2B5);
\end{tikzpicture}
```
**AI-Augmented Labeling**: Programmatic labeling, distant supervision, and active learning scale data annotation by trading potential labeling errors for increased throughput, necessitating careful system design to balance labeling speed, cost, and model quality. These strategies enable machine learning systems to overcome limitations imposed by manual annotation alone, facilitating deployment in data-scarce environments. Source: Stanford AI Lab.
:::

Modern AI-assisted labeling typically employs a combination of approaches. Pre-annotation involves using AI models to generate preliminary labels for a dataset, which humans can then review and correct. Major labeling platforms have made significant investments in this technology. [Snorkel AI](https://snorkel.ai/) uses programmatic labeling to automatically generate initial labels at scale. Scale AI deploys pre-trained models to accelerate annotation in specific domains like autonomous driving, while manycompanies like [SuperAnnotate](https://www.superannotate.com/) provide automated pre-labeling tools that can reduce manual effort drastically. This method, which often employs semi-supervised learning techniques [@chapelle2009semisupervised], can save a significant amount of time, especially for extremely large datasets.

The emergence of Large Language Models (LLMs) like ChatGPT has further transformed labeling pipelines. Beyond simple classification, LLMs can generate rich text descriptions, create labeling guidelines, and even explain their reasoning. For instance, content moderation systems use LLMs to perform initial content classification and generate explanations for policy violations. However, integrating LLMs introduces new system challenges around inference costs, rate limiting, and output validation. Many organizations adopt a tiered approach, using smaller specialized models for routine cases while reserving larger LLMs for complex scenarios.

Methods such as active learning complement these approaches by intelligently prioritizing which examples need human attention [@coleman2022similarity]. These systems continuously analyze model uncertainty to identify valuable labeling candidates for humans to label. The infrastructure must efficiently compute uncertainty metrics, maintain task queues, and adapt prioritization strategies based on incoming labels. Consider a medical imaging system: active learning might identify unusual pathologies for expert review while handling routine cases automatically.

Quality control becomes increasingly crucial as these AI components interact. The system must monitor both AI and human performance, detect potential errors, and maintain clear label provenance. This requires dedicated infrastructure tracking metrics like model confidence and human-AI agreement rates. In safety-critical domains like self-driving cars, these systems must maintain particularly rigorous standards while processing massive streams of sensor data.

Real-world deployments demonstrate these principles at scale. Medical imaging systems [@krishnan2022selfsupervised] combine pre-annotation for common conditions with active learning for unusual cases, all while maintaining strict patient privacy.

Self-driving vehicle systems coordinate multiple AI models to label diverse sensor data in real-time. Social media platforms process millions of items hourly, using tiered approaches where simpler models handle clear cases while complex content routes to more sophisticated models or human reviewers.

While AI assistance offers clear benefits, it also introduces new failure modes. Systems must guard against bias amplification, where AI models trained on biased data perpetuate those biases in new labels. The infrastructure needs robust monitoring to detect such issues and mechanisms to break problematic feedback loops. These concerns align closely with the responsible AI principles in @sec-responsible-ai, where we examine how data quality decisions impact algorithmic fairness and accountability. Human oversight remains essential, requiring careful interface design to help annotators effectively supervise and correct AI output.

### Labeling Challenges {#sec-data-engineering-labeling-challenges-d658}

Despite the sophistication of AI-assisted annotation systems, data labeling comes with its own set of challenges and limitations that practitioners must be aware of and address. One of the primary challenges in data labeling is the inherent subjectivity in many labeling tasks. Even with clear guidelines, human annotators may interpret data differently, leading to inconsistencies in labeling. This is particularly evident in tasks involving sentiment analysis, image classification of ambiguous objects, or labeling of complex medical conditions. For instance, in a study of medical image annotation, @oakden2020hidden found significant variability in labels assigned by different radiologists, highlighting the challenge of obtaining "ground truth" in inherently subjective tasks.

Beyond these subjective interpretation issues, scalability presents another significant challenge, especially as datasets grow larger and more complex. Manual labeling is time-consuming and expensive, often becoming a bottleneck in the machine learning pipeline. While crowdsourcing and AI-assisted methods can help address this issue to some extent, they introduce their own complications in terms of quality control and potential biases.

Compounding these scalability and quality challenges, the issue of bias in data labeling is particularly concerning. Annotators bring their own cultural, personal, and professional biases to the labeling process, which can be reflected in the resulting dataset. For example, @wang2019balanced found that image datasets labeled predominantly by annotators from one geographic region showed biases in object recognition tasks, performing poorly on images from other regions. This highlights the need for diverse annotator pools and careful consideration of potential biases in the labeling process.

Adding another layer of complexity, data privacy and ethical considerations also pose challenges in data labeling. Leading data labeling companies have developed specialized solutions for these challenges. Scale AI, for instance, maintains dedicated teams and secure infrastructure for handling sensitive data in healthcare and finance. Appen implements strict data access controls and anonymization protocols, while Labelbox offers private cloud deployments for organizations with strict security requirements. When dealing with sensitive data, such as medical records or personal communications, ensuring annotator access while maintaining data privacy can be complex. These privacy-preserving techniques connect directly to the security considerations in @sec-security-privacy, where we explore comprehensive approaches to protecting sensitive data throughout the ML lifecycle.

Beyond privacy and ethical concerns, the dynamic nature of real-world data presents another limitation. Labels that are accurate at the time of annotation may become outdated or irrelevant as the underlying distribution of data changes over time. This concept, known as concept drift[^fn-concept-drift], necessitates ongoing labeling efforts and periodic re-evaluation of existing labels.

[^fn-concept-drift]: **Concept Drift Challenge**: First formalized by Schlimmer & Granger (1986), concept drift became critical in ML systems deployment. Amazon's recommendation algorithms must continuously adapt as user preferences shift, while spam detection systems face adversarial concept drift as spammers evolve their tactics.

Finally, the limitations of current labeling approaches become apparent when dealing with edge cases or rare events. In many real-world applications, it's the unusual or rare instances that are often most critical (e.g., rare diseases in medical diagnosis, or unusual road conditions in autonomous driving). However, these cases are, by definition, underrepresented in most datasets and may be overlooked or mislabeled in large-scale annotation efforts.

### Continuing the KWS Example {#sec-data-engineering-continuing-kws-example-52dc}

Given these labeling challenges, the complex requirements of KWS reveal the role of automated data labeling in modern machine learning. The Multilingual Spoken Words Corpus (MSWC) [@mazumder2021multilingual] illustrates this through its innovative approach to generating labeled wake word data at scale. MSWC is large, containing over 23.4 million one-second spoken examples across 340,000 keywords in 50 different languages.

To understand this automated approach, the core of this system, as illustrated in @fig-mswc, begins with paired sentence audio recordings and corresponding transcriptions, which can be sourced from projects like [Common Voice](https://commonvoice.mozilla.org/en) or multilingual captioned content platforms such as YouTube.  The system processes paired audio-text inputs through forced alignment to identify word boundaries, extracts individual keywords as one-second segments, and generates a large-scale multilingual dataset suitable for training keyword spotting models. For example, when a speaker says, "He gazed up the steep bank," their voice generates a complex acoustic signal that conveys more than just the words themselves. This signal encapsulates subtle transitions between words, variations in pronunciation, and the natural rhythm of speech. The primary challenge lies in accurately pinpointing the exact location of each word within this continuous audio stream.

![**Multilingual Data Preparation**: Forced alignment and segmentation transform paired audio-text data into labeled one-second segments, creating a large-scale corpus for training keyword spotting models across 50+ languages. This automated process enables scalable development of KWS systems by efficiently generating training examples from readily available speech resources like common voice and multilingual captioned content.](images/png/data_engineering_kws2.png){#fig-mswc}

To address this fundamental challenge, automated forced alignment proves useful. Tools such as the Montreal Forced Aligner [@mcauliffe17_interspeech] analyze both the audio and its transcription, mapping the timing relationship between written words and spoken sounds, and attempts to mark the boundaries of when each word begins and ends in a speech recording at millisecond-level precision. For high-resource languages such as English, high-quality automated alignments are available "out-of-box" while alignments for low-resource languages must be bootstrapped on the speech data and transcriptions themselves, which can negatively impact timing quality.

Building on these precise timing markers, the extraction system can generate clean, one-second samples of individual keywords. This process requires careful engineering decisions. Background noise might interfere with detecting word boundaries. Speakers may stretch, compress, or mispronounce words in unexpected ways. Longer words may not fit within the default 1-second boundary. To aid ML practitioners in filtering out lower-quality samples automatically, MSWC provides an automated quality assessment system that analyzes audio characteristics to identify potential issues with recording quality, speech clarity, or background noise. This automated validation becomes particularly crucial given the scale of the dataset, which includes over 23 million samples across more than 340,000 words in 50+ languages. Traditional manual review could not maintain consistent standards across such volume without significant expense.

Leveraging this automated infrastructure, modern voice assistant developers often build upon this type of labeling foundation. An automated corpus like MSWC may not contain the specific keywords an application developer wishes to use for their envisioned KWS system, but the corpus can provide a starting point for KWS prototyping in many underserved languages spoken around the world. While MSWC provides automated labeling at scale, production systems may add targeted human recording and verification for challenging cases, rare words, or difficult acoustic environments. The infrastructure must gracefully coordinate between automated processing and human expertise.

Beyond immediate practical applications, the impact of this careful engineering extends far beyond the dataset itself. Automated labeling pipelines open new avenues to how we approach wake word detection and other ML tasks across languages or other demographic boundaries. Where manual collection and annotation might yield thousands of examples, automated dataset generation can yield millions while maintaining consistent quality. This enables voice interfaces to understand an ever-expanding vocabulary across the world's languages.

Through this comprehensive approach to data labeling, MSWC demonstrates how thoughtful data engineering directly impacts production machine learning systems. The careful orchestration of forced alignment, extraction, and quality control creates a foundation for reliable voice interaction across languages. When a voice assistant responds to its wake word, it draws upon this sophisticated labeling infrastructure, which is a testament to the power of automated data processing in modern machine learning systems.

After establishing systematic processing pipelines that transform raw data into ML-ready formats, we must design storage architectures that support the entire ML lifecycle while maintaining our four-pillar framework. Storage decisions determine how effectively we can maintain data quality over time, ensure reliable access under varying loads, scale to handle growing data volumes, and implement governance controls.

## Strategic Storage Architecture {#sec-data-engineering-data-storage-6296}

Storage architecture represents the intersection of all four pillars in our framework: quality requirements determine how data is organized and accessed; reliability needs shape redundancy and backup strategies; scalability demands influence storage system selection; and governance requirements drive access controls and retention policies.

For our KWS system, storage decisions have profound implications across the ML lifecycle. Training data storage must support high-throughput reads for model development while maintaining quality through versioning and lineage tracking. Feature storage must enable low-latency access for real-time inference while ensuring privacy through access controls. Model artifacts and metadata require reliable storage with comprehensive governance for production deployment.

Machine learning workloads create distinctive storage requirements that differ markedly from traditional applications. While transactional systems optimize for frequent writes and row-level updates, ML pipelines require high-throughput reads, large-scale data scans, and evolving schemas that support iterative model development and feature engineering.

ML pipelines must accommodate real-world considerations such as evolving business requirements, new data sources, and changes in data availability. These realities push storage solutions to be both scalable and flexible, ensuring that organizations can manage data collected from diverse channels, ranging from sensor feeds to social media text, without constantly retooling the entire infrastructure. In this section, we will compare the practical use of databases, data warehouses, and data lakes for ML projects, then examine how specialized services, metadata, and governance practices unify these varied systems into a coherent strategy.

### Storage System Types {#sec-data-engineering-storage-system-types-f802}

The following sections provide a systematic comparison of storage architectures for ML systems, evaluated through our four-pillar framework. We begin with high-level architectural trade-offs before examining specific implementation considerations.

Storage system selection represents a critical architectural decision that affects all aspects of the ML lifecycle. Databases, data warehouses, and data lakes each provide distinct advantages for different aspects of ML workflows, and modern systems often employ multiple storage types in coordinated architectures.

@tbl-storage provides a framework-oriented comparison of these storage systems: Databases usually support operational and transactional purposes. They work well for smaller, well-structured datasets, but can become cumbersome and expensive when applied to large-scale ML contexts involving unstructured data (such as images, audio, or free-form text).

+--------------------------+-------------------------------+-------------------------------+-----------------------------------+
| Attribute                | Conventional Database         | Data Warehouse                | Data Lake                         |
+:=========================+:==============================+:==============================+:==================================+
| Purpose                  | Operational and transactional | Analytical and reporting      | Storage for raw and diverse       |
|                          |                               |                               | data for future processing        |
+--------------------------+-------------------------------+-------------------------------+-----------------------------------+
| Data type                | Structured                    | Structured                    | Structured, semi-structured,      |
|                          |                               |                               | and unstructured                  |
+--------------------------+-------------------------------+-------------------------------+-----------------------------------+
| Scale                    | Small to medium volumes       | Medium to large volumes       | Large volumes of diverse          |
|                          |                               |                               | data                              |
+--------------------------+-------------------------------+-------------------------------+-----------------------------------+
| Performance Optimization | Optimized for transactional   | Optimized for analytical      | Optimized for scalable            |
|                          | queries (OLTP)                | queries (OLAP)                | storage and retrieval             |
+--------------------------+-------------------------------+-------------------------------+-----------------------------------+
| Examples                 | MySQL, PostgreSQL, Oracle DB  | Google BigQuery, Amazon       | Google Cloud Storage, AWS         |
|                          |                               | Redshift, Microsoft Azure     | S3, Azure Data Lake Storage       |
|                          |                               | Synapse                       |                                   |
+--------------------------+-------------------------------+-------------------------------+-----------------------------------+

: **Storage System Characteristics**: Different storage systems suit distinct stages of machine learning workflows based on data structure and purpose; databases manage transactional data, data warehouses support analytical reporting, and data lakes accommodate diverse, raw data for future processing. Understanding these characteristics enables efficient data management and supports the scalability of machine learning applications. {#tbl-storage}

Data warehouses, by contrast, are optimized for analytical queries across integrated datasets that have been transformed into a standardized schema. As indicated in the table, they handle large volumes of integrated data. Many ML systems successfully draw on data warehouses to power model training because the structured environment simplifies data exploration and feature engineering. Yet one limitation remains: a data warehouse may not accommodate truly unstructured data or rapidly changing data formats, particularly if the data originates from web scraping or Internet of Things (IoT)[^fn-iot-data-volume] sensors.

[^fn-iot-data-volume]: **IoT Data Explosion**: IDC predicts 41.6 billion IoT devices will generate 79.4 zettabytes of data by 2025âthat's 79.4 trillion gigabytes. A single autonomous vehicle generates 4TB/day, while smart city sensors produce 2.5 quintillion bytes daily. Traditional data warehouses struggle with this velocity and variety.

Data lakes[^fn-data-lake-origins] address this gap by storing structured, semi-structured, and unstructured data in its native format, deferring schema definitions until the point of reading or analysis (sometimes called _schema-on-read_). As @tbl-storage shows, data lakes can handle large volumes of diverse data types. This approach grants data scientists tremendous latitude when dealing with experimental use cases or novel data types. However, data lakes also demand careful cataloging and metadata management. Without sufficient governance, these expansive repositories risk devolving into unsearchable, disorganized silos.

[^fn-data-lake-origins]: **Data Lake Origins**: The term "data lake" was coined by Pentaho CTO James Dixon in 2010 to contrast with data warehouses, comparing them to "a data swamp" if poorly managed. The concept emerged from Hadoop's ability to store vast amounts of unstructured data cheaply.

The examples provided in @tbl-storage illustrate the range of technologies available for each storage system type. For instance, MySQL[^fn-mysql-scale] represents a traditional database system, while solutions like Google BigQuery[^fn-bigquery-performance] and Amazon Redshift are examples of modern, cloud-based data warehouses.

[^fn-mysql-scale]: **MySQL at Scale**: Originally developed by MySQL AB in 1995, MySQL powers 39% of all websites including Facebook, Twitter, and YouTube. However, single MySQL instances typically max out at 10-50TB before requiring complex sharding strategies that make ML feature extraction significantly more complex.

[^fn-bigquery-performance]: **BigQuery Serverless Power**: Google BigQuery can scan petabytes in seconds using thousands of parallel workers. Its columnar storage and automatic query optimization enables ML feature extraction from trillion-row tables in minutesâperformance impossible with traditional databases at any scale. For data lakes, cloud storage solutions such as Google Cloud Storage, AWS S3, and Azure Data Lake Storage are commonly used due to their scalability and flexibility.

#### Storage Architecture Decision Matrix

Choosing appropriate storage architecture requires systematic evaluation of requirements against system characteristics. The following decision matrix guides storage selection based on concrete ML workload patterns and constraints:

**Database Systems** are optimal when data volume is under 1TB, query patterns involve frequent updates and complex joins, latency requirements demand sub-second response times, and strong consistency is mandatory. Common patterns include user profile management, real-time recommendation serving, and fraud detection feature stores. Avoid databases when analytical queries span large datasets, schema evolution is frequent, or storage costs exceed $500/TB/month.

**Data Warehouses** excel for data volumes from 1TB to 100TB, when analytical query patterns dominate over transactional operations, when batch processing latency (minutes to hours) is acceptable, and when structured data with stable schemas represents the primary workload. Typical use cases include model training data preparation, batch feature engineering, and historical analysis. Migration from databases becomes necessary when query complexity increases or join operations span multiple gigabytes. Warehouses become inadequate when real-time streaming ingestion is required or when unstructured data comprises more than 20% of the workload.

**Data Lakes** are essential for data volumes exceeding 100TB, when schema flexibility is critical for evolving data sources, when cost optimization is paramount (often 10x cheaper than warehouses), and when diverse data types (logs, images, audio, text) must coexist. Data lakes suit exploratory machine learning, large-scale model training, and multi-modal AI systems. However, they require sophisticated catalog management and metadata governance to prevent degradation into unusable "data swamps."

**Migration Paths and Anti-Patterns** follow predictable trajectories as systems scale. Database-to-warehouse migration typically occurs when analytical query performance degrades or storage costs become prohibitive. Warehouse-to-lake migration happens when schema rigidity constrains data ingestion or when cost optimization becomes critical. Common anti-patterns include using databases for analytical workloads exceeding 100GB, implementing data lakes without proper governance frameworks, or choosing warehouses for highly variable or unstructured data patterns.

### Storage Considerations {#sec-data-engineering-storage-considerations-5f3e}

While traditional storage systems provide a foundation for ML workflows, the unique characteristics of machine learning workloads necessitate additional considerations. These ML-specific storage needs stem from the nature of ML development, training, and deployment processes, and addressing them is necessary for building efficient and scalable ML systems.

One of the primary challenges in ML storage is handling large model weights. Modern ML models can have millions or billions of numerical values that need to be stored. For instance, GPT-3[^fn-model-scaling], a large language model, requires approximately 350 GB of storage just for the model weights [@brown2020language]. Storage systems need to be capable of handling these large, often dense, numerical arrays efficiently, both in terms of storage capacity and access speed.

[^fn-model-scaling]: **Model Scaling Explosion**: From AlexNet's 60 million parameters [@krizhevsky2012imagenet] (2012) to GPT-3's 175 billion [@brown2020language] (2020), model size grew 3,000x in 8 years. GPT-4's rumored 1.7 trillion parameters would require 3.5 TB of storageâequivalent to 1,000 DVDs worth of model weights alone. This requirement exceeds traditional data storage and requires high-performance computing storage solutions.

The iterative nature of ML development introduces another critical storage consideration: versioning for both datasets and models. Unlike traditional software version control, ML versioning needs to track large binary files efficiently. As data scientists experiment with different model architectures and hyperparameters, they generate numerous versions of models and datasets. Effective storage systems for ML must provide mechanisms to track these changes, revert to previous versions, and maintain reproducibility throughout the ML lifecycle. This capability is essential not only for development efficiency but also for regulatory compliance and model auditing in production environments.

Distributed training, often necessary for large models or datasets, generates substantial intermediate data, including partial model updates, gradients, and checkpoints. Storage systems for ML need to handle frequent, possibly concurrent, read and write operations of these intermediate results. They should also provide low-latency access to support efficient synchronization between distributed workers. This requirement pushes storage systems to balance between high throughput for large data transfers and low latency for quick synchronization operations.

The diversity of data types in ML workflows presents another unique challenge. ML systems often work with a wide variety of data, ranging from structured tabular data to unstructured images, audio, and text. Storage systems need to efficiently handle this diversity, often requiring a combination of different storage technologies optimized for specific data types. For instance, a single ML project might need to store and process tabular data in a columnar format for efficient feature extraction, while also managing large volumes of image data for computer vision tasks.

As organizations collect more data and create more sophisticated models, storage systems need to scale effectively. This scalability should support not just growing data volumes, but also increasing concurrent access from multiple data scientists and ML models. Cloud-based object storage systems have emerged as a popular solution due to their virtually unlimited scalability, but they introduce their own challenges in terms of data access latency and cost management.

The tension between sequential read performance for training and random access for inference is another key consideration. While training on large datasets benefits from high-throughput sequential reads, many ML serving scenarios require fast random access to individual data points or features. Storage systems for ML need to balance these potentially conflicting requirements, often leading to tiered storage architectures where frequently accessed data is kept in high-performance storage while less frequently used data is moved to cheaper, higher-latency storage.

The choice and configuration of storage systems can significantly impact the performance, cost-effectiveness, and overall success of ML initiatives. As the field of machine learning continues to evolve, storage solutions will need to adapt to meet the changing demands of increasingly sophisticated ML workflows.

### Performance Factors {#sec-data-engineering-performance-factors-56f2}

The performance of storage systems is critical in ML workflows, directly impacting the efficiency of model training, the responsiveness of inference, and the overall productivity of data science teams. Understanding and optimizing storage performance requires a focus on several key metrics and strategies tailored to ML workloads.

One of the primary performance metrics for ML storage is throughput, particularly for large-scale data processing and model training. High throughput is essential when ingesting and preprocessing vast datasets or when reading large batches of data during model training. For instance, large-scale model training on large datasets may require sustained read throughput of several gigabytes per second to keep GPU accelerators fully utilized. This requirement explains why traditional spinning disks (100-200 MB/s) are inadequate for modern ML training, while SSD storage (1-7 GB/s) provides sufficient bandwidth for single-node training but may still bottleneck distributed training scenarios that require coordinating data across network storage systems (1-10 GB/s) or accessing large datasets that exceed local storage capacity.

Latency is another metric, especially for online inference and interactive data exploration. Low latency access to individual data points or small batches of data is vital for maintaining responsive ML services. In recommendation systems or real-time fraud detection, for example, storage systems must be able to retrieve relevant features or model parameters within milliseconds to meet strict service level agreements (SLAs).

**Storage Bottleneck Analysis**: Understanding the quantitative relationship between storage performance and training throughput requires systems thinking about bandwidth hierarchies. Modern ML training throughput is fundamentally determined by the equation:

```
Training Throughput = min(Compute Capacity, Data Supply Rate)
Data Supply Rate = Storage Bandwidth Ã (1 - Overhead)
```

Consider a high-end GPU with 312 TFLOPS computational capacity processing ResNet-50 training. If the model requires 25 million parameters (100MB weights) plus 32 images Ã 150KB (5MB input data) per batch, the system moves 105MB per 4 billion operationsâa ratio of 26 bytes per operation. When storage can only deliver 1 GB/s but the GPU could theoretically process 10 GB/s worth of data, the 10x bandwidth mismatch creates an immediate bottleneck. This analysis explains why traditional spinning disks (100-200 MB/s) fail completely for modern ML training, while even high-end SSDs (1-7 GB/s) may constrain distributed training scenarios where multiple accelerators compete for storage bandwidth.

This storage-compute mismatch becomes critical as models scale. Large language model training may require processing hundreds of gigabytes of text per hour, while computer vision models processing high-resolution imagery can demand sustained data rates exceeding 50 GB/s across distributed clusters. Understanding these quantitative relationships enables engineers to make informed architectural decisions about storage hierarchies, caching strategies, and data placement that directly impact training efficiency and cost.

The choice of file format can significantly impact both throughput and latency. Columnar storage formats[^fn-columnar-formats] such as Parquet or ORC are particularly well-suited for ML workloads, delivering 5-10x I/O reduction compared to row-based formats like CSV or JSON. These formats allow for efficient retrieval of specific features without reading entire records, substantially reducing I/O operations and speeding up data loading for model training and inference. Consider a fraud detection dataset with 100 columns where models typically use only 20 featuresâcolumnar formats read only the needed columns, achieving 80% I/O reduction. Combined with column-level compression (often 20-50x for categorical features), columnar formats can achieve total I/O reduction of 20-100x compared to uncompressed row formats. This dramatic improvement directly translates to faster training iterations and reduced infrastructure costs, important for large-scale ML systems processing terabytes of data daily.

[^fn-columnar-formats]: **Columnar Format Revolution**: Columnar storage was pioneered by C-Store [@stonebraker2005cstore] in 2005, leading to Parquet (developed at Twitter in 2013) and ORC (optimized row columnar, created at Hortonworks). These formats revolutionized analytics by enabling 10-100x faster queries for ML feature extraction.

Compression is another key factor in storage performance optimization. While compression reduces storage costs and can improve read performance by reducing the amount of data transferred from disk, it also introduces computational overhead for decompression. The choice of compression algorithm often involves a trade-off between compression ratio and decompression speed. For ML workloads, fast decompression is usually prioritized over maximum compression, with algorithms like Snappy[^fn-snappy-compression] or LZ4 being popular choices.

[^fn-snappy-compression]: **Snappy Compression Trade-offs**: Developed by Google (2011), Snappy achieves 250MB/s compression and 500MB/s decompression speedsâroughly 3-4x faster than gzip. While compression ratios are lower (2-3x vs gzip's 6-8x), the speed advantage makes it ideal for ML pipelines where training throughput matters more than storage costs.

Data partitioning strategies play a role in optimizing query performance for ML workloads. By intelligently partitioning data based on frequently used query parameters (such as date ranges or categorical variables), systems can dramatically improve the efficiency of data retrieval operations. For instance, in a recommendation system processing user interactions, partitioning data by user demographic attributes and time periods can significantly speed up the retrieval of relevant training data for personalized models.

To handle the scale of data in modern ML systems, distributed storage architectures are often employed. These systems, such as [HDFS (Hadoop Distributed File System)](https://hadoop.apache.org/docs/r1.2.1/hdfs_design.html)[^fn-hdfs-origins] or cloud-based object stores like [Amazon S3](https://aws.amazon.com/s3/), distribute data across multiple machines or data centers. This approach not only provides scalability but also enables parallel data access, which can substantially improve read performance for large-scale data processing tasks common in ML workflows.

[^fn-hdfs-origins]: **HDFS Origins**: HDFS was inspired by Google's MapReduce paper (2004) and created at Yahoo! in 2006 to handle web-scale data. It enabled the "big data" revolution by making petabyte-scale storage affordable using commodity hardware instead of expensive specialized systems.

Caching strategies are also vital for optimizing storage performance in ML systems. In-memory caching of frequently accessed data or computed features can significantly reduce latency and computational overhead. Distributed caching systems like Redis[^fn-redis-performance] or Memcached are often used to scale caching capabilities across clusters of machines, providing low-latency access to hot data for distributed training or serving systems.

[^fn-redis-performance]: **Redis Performance**: Redis achieves sub-millisecond latency with 1M+ operations/second on modest hardware. Its in-memory architecture makes it ideal for ML feature serving, with companies like Twitter using Redis clusters to serve 400,000+ timeline requests per second for real-time recommendation systems.

As ML workflows increasingly span from cloud to edge devices, storage performance considerations must extend to these distributed environments. Edge caching and intelligent data synchronization strategies become needed for maintaining performance in scenarios where network connectivity may be limited or unreliable. In the end, the goal is to create a storage infrastructure that can handle the volume and velocity of data in ML workflows while providing the low-latency access needed for responsive model training and inference.

### Storage in ML Lifecycle {#sec-data-engineering-storage-ml-lifecycle-a3a7}

The storage needs of machine learning systems evolve significantly across different phases of the ML lifecycle. Understanding these changing requirements is important for designing effective and efficient ML data infrastructures.

#### Development Phase {#sec-data-engineering-development-phase-ebf9}

In the development phase, storage systems play a critical role in supporting exploratory data analysis and iterative model development. This stage demands flexibility and collaboration, as data scientists often work with various datasets, experiment with feature engineering techniques, and rapidly iterate on model designs to refine their approaches.

One of the key challenges at this stage is managing the versions of datasets used in experiments. While traditional version control systems like Git excel at tracking code changes, they fall short when dealing with large datasets[^fn-dataeng-data-versioning]. This gap has led to the emergence of specialized tools like [DVC (Data Version Control)](https://dvc.org/), which enable data scientists to efficiently track dataset changes, revert to previous versions, and share large files without duplication. These tools ensure that teams can maintain reproducibility and transparency throughout the iterative development process.

[^fn-dataeng-data-versioning]: **Data Versioning Challenges**: Git's inability to handle large binary files efficiently led to the "GitHub is not a CDN" problem. DVC, created in 2017, solved this by treating data like code using content-addressable storage, enabling Git-like workflows for terabyte-scale datasets.

Balancing data accessibility and security further complicates the storage requirements in this phase. Data scientists require efficient access to datasets for experimentation, but organizations must simultaneously safeguard sensitive data. This tension often results in the implementation of sophisticated access control mechanisms, ensuring that datasets remain both accessible and protected. Secure data sharing systems enhance collaboration while adhering to strict organizational and regulatory requirements, enabling teams to work productively without compromising data integrity.

#### Training Phase {#sec-data-engineering-training-phase-b13d}

The training phase presents unique storage challenges due to the sheer volume of data processed and the computational intensity of model training. At this stage, the interplay between storage performance and computational efficiency becomes critical, as modern ML algorithms demand efficient integration between data access and processing.

To meet these demands, high-performance storage systems must provide the throughput required to feed data to multiple GPU or TPU accelerators simultaneously. Distributed training scenarios amplify this need, often requiring data transfer rates in the gigabytes per second range to ensure that accelerators remain fully utilized. Modern GPUs can process data faster than traditional storage can supply it: while RAM can deliver 50-200 GB/s bandwidth, network storage systems typically provide only 1-10 GB/s, and even high-end SSDs max out at 1-7 GB/s sequential throughput. This bandwidth hierarchy explains why distributed training often implements sophisticated data loading strategies, including data prefetching, parallel I/O, and strategic caching in RAM to bridge the performance gap between compute and storage capabilities.

**Data Movement Economics Analysis** reveals why modern ML systems have shifted from compute-bound to data movement-bound workloads. Consider ResNet-50 training as a concrete example: the model contains 25 million parameters requiring 100MB of weights, while a typical batch of 32 images consumes 5MB of input data. During the forward pass, approximately 4 billion mathematical operations are performed, but the system must move 105MB of data (weights plus inputs). This yields a bytes-per-operation ratio of 26 bytes per operationâextraordinarily high compared to traditional computing workloads that typically operate with ratios below 1 byte per operation.

This shift has profound implications for system design. When a GPU can theoretically process 10 GB/s worth of computation but storage can only supply 1 GB/s of data, the 10x bandwidth mismatch creates an immediate bottleneck that no amount of computational optimization can resolve. The situation becomes more extreme with large language models, where parameter counts exceed billions and training batches may require gigabytes of data movement per iteration. Understanding these data movement economics enables engineers to make informed architectural decisions about data placement, storage hierarchies, and caching strategies that directly impact training efficiency and cost.

Beyond data ingestion, managing intermediate results and checkpoints is another critical challenge in the training phase. Long-running training jobs frequently save intermediate model states to allow for resumption in case of interruptions. These checkpoints can grow significantly in size, especially for large-scale models, necessitating storage solutions that enable efficient saving and retrieval without impacting overall performance.

Complementing these systems is the concept of burst buffers[^fn-burst-buffers], borrowed from high-performance computing. These high-speed, temporary storage layers are particularly valuable during training, as they can absorb large, bursty I/O operations.

[^fn-burst-buffers]: **Burst Buffers**: High-speed SSD-based storage layers that buffer data between slower traditional storage and fast compute. Originally developed for supercomputers, they're now critical for ML training where GPUs can demand 100GB/s+ data ratesâfar exceeding traditional storage capabilities. By buffering these spikes in demand, burst buffers help smooth out performance fluctuations and reduce the load on primary storage systems, ensuring that training pipelines remain efficient and reliable.

#### Deployment Phase {#sec-data-engineering-deployment-phase-a4c1}

In the deployment and serving phase, the focus shifts from high-throughput batch operations during training to low-latency, often real-time, data access. This transition highlights the need to balance conflicting requirements, where storage systems must simultaneously support responsive model serving and enable continued learning in dynamic environments.

Real-time inference demands storage solutions capable of extremely fast access to model parameters and relevant features. To achieve this, systems often rely on in-memory databases or sophisticated caching strategies, ensuring that predictions can be made within milliseconds. These requirements become even more challenging in edge deployment scenarios, where devices operate with limited storage resources and intermittent connectivity to central data stores.

Adding to this complexity is the need to manage model updates in production environments. Storage systems must facilitate smooth transitions between model versions, ensuring minimal disruption to ongoing services. Techniques like shadow deployment, where new models run alongside existing ones for validation, allow organizations to iteratively roll out updates while monitoring their performance in real-world conditions.

#### Maintenance Phase {#sec-data-engineering-maintenance-phase-86d3}

The monitoring and maintenance phase brings its own set of storage challenges, centered on ensuring the long-term reliability and performance of ML systems. At this stage, the focus shifts to capturing and analyzing data to monitor model behavior, detect issues, and maintain compliance with regulatory requirements.

A critical aspect of this phase is managing data drift, where the characteristics of incoming data change over time. Storage systems must efficiently capture and store incoming data along with prediction results, enabling ongoing analysis to detect and address shifts in data distributions. This ensures that models remain accurate and aligned with their intended use cases. For edge and mobile deployments discussed in @sec-ondevice-learning, data drift detection becomes particularly challenging due to storage constraints and intermittent connectivity, requiring specialized approaches for local adaptation and selective data transmission.

The sheer volume of logging and monitoring data generated by high-traffic ML services introduces questions of data retention and accessibility. Organizations must balance the need to retain historical data for analysis against the cost and complexity of storing it. Strategies such as tiered storage and compression can help manage costs while ensuring that critical data remains accessible when needed.

Regulated industries often require immutable storage to support auditing and compliance efforts. Storage systems designed for this purpose guarantee data integrity and non-repudiability, ensuring that stored data cannot be altered or deleted. Blockchain-inspired solutions and write-once-read-many (WORM) technologies are commonly employed to meet these stringent requirements.

### Feature Storage {#sec-data-engineering-feature-storage-3423}

Feature stores[^fn-feature-stores] are a centralized repository that stores and serves pre-computed features for machine learning models, ensuring consistency between training and inference workflows. They have emerged as a critical component in the ML infrastructure stack, addressing the unique challenges of managing and serving features for machine learning models. They act as a central repository for storing, managing, and serving machine learning features, bridging the gap between data engineering and machine learning operations.

[^fn-feature-stores]: **Feature Store Evolution**: Feature stores were pioneered by Uber's Michelangelo platform in 2017 to solve feature consistency issues at scale. The concept gained widespread adoption after Airbnb open-sourced their Zipline feature store, leading to modern solutions like Feast and Tecton.

What makes feature stores particularly interesting is their role in solving several key challenges in ML pipelines. First, they address the problem of feature consistency between training and serving environments. In traditional ML workflows, features are often computed differently in offline (training) and online (serving) environments, leading to discrepancies that can degrade model performance. Feature stores provide a single source of truth for feature definitions, ensuring consistency across all stages of the ML lifecycle.

Another fascinating aspect of feature stores is their ability to promote feature reuse across different models and teams within an organization. By centralizing feature computation and storage, feature stores can significantly reduce redundant work. For instance, if multiple teams are working on different models that require similar features (e.g., customer lifetime value in a retail context), these features can be computed once and reused across projects, improving efficiency and consistency.

Feature stores also play a role in managing the temporal aspects of features. Many ML use cases require correct point-in-time feature values, especially in scenarios involving time-series data or where historical context is important. Feature stores typically offer time-travel capabilities, allowing data scientists to retrieve feature values as they were at any point in the past. This is crucial for training models on historical data and for ensuring consistency between training and serving environments, as illustrated in @fig-feature-store-overview which shows how data flows through these systems to eventually yield a model.

::: {#fig-feature-store-overview fig-env="figure" fig-pos="htb"}
```{.tikz}
\begin{tikzpicture}[line cap=round,line join=round,font=\usefont{T1}{phv}{m}{n}\small]
\tikzset{%
LineA/.style={line width=4pt,violet!40,text=black,-{Triangle[width=1.8*6pt,length=2.2*6pt]}},
Line/.style={line width=1.5pt,violet!40,text=black},
Box/.style={align=flush center,
    inner xsep=4pt,
    node distance=0.5,
    line width=0.75pt,
    fill= BrownL,draw= BrownLine,
    minimum height=10mm
  }
}
\tikzset{%
 LineST/.style={-{Circle[\channelcolor,fill=white,length=13pt]},draw=\channelcolor,line width=\Linewidth,rounded corners},
 ellipseST/.style={fill=\channelcolor,ellipse,minimum width = 2.5mm, inner sep=2pt, minimum height =1.5mm},
 BoxST/.style={line width=\Linewidth,fill=white,draw=\channelcolor,rectangle,minimum width=56,minimum height=16,rounded corners},
 pics/streaming/.style = {
        code = {
        \pgfkeys{/channel/.cd, #1}
\begin{scope}[local bounding box=STREAMING,scale=\scalefac, every node/.append style={transform shape}]
\node[BoxST,minimum width=44,minimum height=48](\picname-RE1){};
\foreach \i/\j in{1/north,2/center,3/south}{
\node[BoxST](\picname-GR\i)at(\picname-RE1.\j){};
\node[ellipseST]at($(\picname-GR\i.west)!0.2!(\picname-GR\i.east)$){};
\node[ellipseST]at($(\picname-GR\i.west)!0.4!(\picname-GR\i.east)$){};
}
\draw[LineST](\picname-GR1.40)--++(0,0.5)--++(1,0)coordinate(\picname-C1);
\draw[LineST](\picname-GR1)--++(2,0)coordinate(\picname-C2);
\draw[LineST](\picname-GR2)--++(1.8,0)coordinate(\picname-C3);
\draw[LineST](\picname-GR3)--++(2,0)coordinate(\picname-C4);
\draw[LineST](\picname-GR3.320)--++(0,-0.5)--++(1,0)coordinate(\picname-C5);
\draw[LineST](\picname-GR3.220)--++(0,-0.5)--++(-1,0)coordinate(\picname-C6);
\draw[LineST](\picname-GR3)--++(-2,0)coordinate(\picname-C7);
\draw[LineST](\picname-GR2)--++(-1.8,0)coordinate(\picname-C8);
\draw[LineST](\picname-GR1)--++(-2,0)coordinate(\picname-C9);
\draw[LineST](\picname-GR1.140)--++(0,0.5)--++(-1,0)coordinate(\picname-C10);
 \end{scope}
     }
  }
}
\tikzset{mycylinder/.style={cylinder, shape border rotate=90, aspect=1.3, draw, fill=white,
minimum width=25mm,minimum height=11mm,line width=\Linewidth,node distance=-0.15},
pics/data/.style = {
        code = {
        \pgfkeys{/channel/.cd, #1}
\begin{scope}[local bounding box=STREAMING,scale=\scalefac, every node/.append style={transform shape}]
\node[mycylinder,fill=\channelcolor!50] (A) {};
\node[mycylinder, above=of A,fill=\channelcolor!30] (B) {};
\node[mycylinder, above=of B,fill=\channelcolor!10] (C) {};
 \end{scope}
     }
  }
}
\tikzset{%
 LinePE/.style={line width=\Linewidth,draw=\drawchannelcolor},
 ellipsePE/.style={line width=\Linewidth,draw=\drawchannelcolor,ellipse,minimum width = 2.5mm, inner sep=2pt,minimum width=29,
 minimum height=40},
 pics/person/.style = {
        code = {
        \pgfkeys{/channel/.cd, #1}
\begin{scope}[local bounding box=PERSON,scale=\scalefac, every node/.append style={transform shape}]
\node[ellipsePE, fill=\channelcolor!99](\picname-EL1)at(0,0.44){};
\draw[LinePE, fill=\channelcolor!40](-0.6,0)to[out=210,in=85](-1.1,-1)
to[out=270,in=180](-0.9,-1.2)to(0.9,-1.2)to[out=0,in=270](1.1,-1)
to[out=85,in=325](0.6,0)to[out=250,in=290,distance=17](-0.6,0);
 \end{scope}
     }
  }
}
\pgfkeys{
  /channel/.cd,
  channelcolor/.store in=\channelcolor,
  drawchannelcolor/.store in=\drawchannelcolor,
  scalefac/.store in=\scalefac,
  Linewidth/.store in=\Linewidth,
  picname/.store in=\picname,
  channelcolor=BrownLine,
  drawchannelcolor=BrownLine,
  scalefac=1,
  Linewidth=1.6pt,
  picname=C
}

\begin{scope}[local bounding box=STREAMING1,shift={($(0,0)+(0,0)$)},scale=1, every node/.append style={transform shape}]
\pic[shift={(0,0)}] at  (0,0){streaming={scalefac=0.7,picname=1,channelcolor=BlueLine, Linewidth=2.5pt}};
 \end{scope}

 \begin{scope}[local bounding box=DATA1,shift={($(STREAMING1)+(0,-6.0)$)},
scale=0.9, every node/.append style={transform shape}]
\pic[shift={(0,0)}] at  (0,0){data={scalefac=1,picname=1,channelcolor=red, Linewidth=1.0pt}};
 \end{scope}

\begin{scope}[local bounding box=FEATURE,shift={($(DATA1)+(9.8,4.7)$)},
scale=1, every node/.append style={transform shape}]
\node[Box,text width=76mm](B1){Monitoring};
\node[Box,below=of B1.south west,minimum width=26mm,anchor=north west](B2){Transformations};
\node[Box,below=of B1.south east,minimum width=20mm,anchor=north east](B3){Serving};
\node[Box, minimum width=20mm](B4)at($(B2.east)!0.5!(B3.west)$){Storage};
%
\node[Box,text width=76mm,fill= BrownL,below=2 of B1, draw= BrownLine](B5){Registry};
\scoped[on background layer]
\node[draw=BackLine,inner xsep=13,line width=0.75pt,inner ysep=28,
fill=BackColor!30,yshift=6,fit=(B1)(B5)](BB1){};
\node[below=1pt of BB1.north,anchor=north]{\textbf{Feature Store}};
\end{scope}

 \begin{scope}[local bounding box=PERSON1,shift={($(FEATURE)+(0,-5.0)$)},scale=1, every node/.append style={transform shape}]
\pic[shift={(0,0)}] at  (0,0){person={scalefac=0.8,picname=1,drawchannelcolor=OliveLine,
channelcolor=OliveLine!50!, Linewidth=1.5pt}};
 \end{scope}

 \node[Box,right=20mm of PERSON.east,minimum width=26mm,
       fill=GreenL,draw=GreenLine](MT){Model training};
 \node[Box,above right=10mm and 30mm of FEATURE.east,minimum width=26mm,
       fill=OrangeL,draw=OrangeLine](MS){Model serving};
%
\draw[LineA,shorten <=5pt](PERSON1)--node[right,align=center,pos=0.31]{Define feature}(B5);
\draw[LineA](PERSON1)--(MT);
\draw[LineA,shorten <=4pt](B5.190)|-node[left,align=center,pos=0.31]{Search and\\discover features}(PERSON1);
\draw[Line,shorten <=4pt,rounded corners](STREAMING1.east)--++(2.5,0)|-
node[above left=14pt and 7pt,pos=0.2]{Ingest data}(BB1.175);
\draw[Line,shorten <=4pt,rounded corners](DATA1.east)--++(2.8,0)|-(BB1.185);
\draw[Line,rounded corners](MS.west)--++(-2.0,0)|-
node[above right=14pt and -3pt,pos=0.2,align=center]{Fetch feature\\ vectors}(BB1.5);
\draw[Line,rounded corners](MT.east)--++(1.0,0)|-
node[right=6pt,pos=0.25,align=center]{Fetch training\\ dataset}(BB1.355);
%
\node[below =4pt of PERSON1]{Data Scientist};
\node[below =4pt of DATA1]{Batch data};
\node[below =14pt of STREAMING1]{Streaming data};
\end{tikzpicture}
```
**Feature Store Architecture**: Centralizing feature engineering and storage enables consistent feature access across model training and serving, resolving data inconsistencies and reducing redundant computation. Time-travel capabilities ensure point-in-time correctness, critical for historical analysis and maintaining model performance in dynamic environments.
:::

The performance characteristics of feature stores are particularly intriguing from a storage perspective. They need to support both high-throughput batch retrieval for model training and low-latency lookups for online inference. This often leads to hybrid architectures where feature stores maintain both an offline store (optimized for batch operations) and an online store (optimized for real-time serving). Synchronization between these stores becomes a critical consideration.

Feature stores also introduce interesting challenges in terms of data freshness and update strategies. Some features may need to be updated in real-time (e.g., current user session information), while others might be updated on a daily or weekly basis (e.g., aggregated customer behavior metrics). Managing these different update frequencies and ensuring that the most up-to-date features are always available for inference can be complex.

From a storage perspective, feature stores often leverage a combination of different storage technologies to meet their diverse requirements. This might include columnar storage formats like Parquet for the offline store, in-memory databases or key-value stores for the online store, and streaming platforms like Apache Kafka for real-time feature updates.

**Production Operational Complexity** emerges as feature stores scale to handle enterprise workloads. Feature freshness guarantees become critical when serving real-time modelsâlate features can cause prediction accuracy to degrade, but implementing strict SLAs requires sophisticated monitoring and fallback strategies. When feature computation fails or produces partial data, systems must decide whether to serve stale features, computed defaults, or fail requests entirely. Each choice affects model performance differently depending on feature importance and business requirements.

**Cost Management for Real-Time Serving** represents a significant operational challenge. In-memory feature stores can consume hundreds of gigabytes of RAM for large feature sets, making cost optimization critical. Intelligent caching strategies must balance feature access patterns against memory costsâfrequently accessed features justify RAM costs while rarely used features can tolerate higher latency storage. Real-time feature computation for streaming data often requires dedicated compute resources that must be sized for peak load but operate efficiently during normal periods.

**Feature Store Migration and Vendor Strategy** requires careful planning as organizations mature their ML infrastructure. Migrating from homegrown feature systems to commercial solutions involves complex data migration, schema evolution, and application integration challenges. Vendor changes become particularly complex when feature definitions are tightly coupled to specific feature store APIs or when large volumes of historical features must be migrated without disrupting production services. Successful feature store implementations maintain abstraction layers that enable vendor-agnostic feature access patterns.

### Caching Techniques {#sec-data-engineering-caching-techniques-ac59}

Caching plays a role in optimizing the performance of ML systems, particularly in scenarios involving frequent data access or computation-intensive operations. In the context of machine learning, caching strategies extend beyond traditional web or database caching, addressing unique challenges posed by ML workflows.

One of the primary applications of caching in ML systems is in feature computation and serving. Many features used in ML models are computationally expensive to calculate, especially those involving complex aggregations or time-window operations. By caching these computed features, systems can significantly reduce latency in both training and inference scenarios. For instance, in a recommendation system, caching user embedding vectors can dramatically speed up the generation of personalized recommendations.

Caching strategies in ML systems often need to balance between memory usage and computation time. This trade-off is particularly evident in large-scale distributed training scenarios. Caching frequently accessed data shards or mini-batches in memory can significantly reduce I/O overhead, but it requires careful memory management to avoid out-of-memory errors, especially when working with large datasets or models. These memory optimization decisions become increasingly important when considering the environmental impact discussed in @sec-sustainable-ai, where energy-efficient data access patterns can significantly reduce the carbon footprint of large-scale ML training.

Another interesting application of caching in ML systems is model caching. In scenarios where multiple versions of a model are deployed (e.g., for A/B testing or gradual rollout), caching the most frequently used model versions in memory can significantly reduce inference latency. This becomes especially important in edge computing scenarios, where storage and computation resources are limited.

Caching also plays a vital role in managing intermediate results in ML pipelines. For instance, in feature engineering pipelines that involve multiple transformation steps, caching intermediate results can prevent redundant computations when rerunning pipelines with minor changes. This is particularly useful during the iterative process of model development and experimentation.

One of the challenges in implementing effective caching strategies for ML is managing cache invalidation and updates. ML systems often deal with dynamic data where feature values or model parameters may change over time. Implementing efficient cache update mechanisms that balance between data freshness and system performance is an ongoing area of research and development.

Distributed caching becomes particularly important in large-scale ML systems. Technologies like Redis or Memcached are often employed to create distributed caching layers that can serve multiple training or inference nodes. These distributed caches need to handle challenges like maintaining consistency across nodes and managing failover scenarios.

Edge caching is another fascinating area in ML systems, especially with the growing trend of edge AI. In these scenarios, caching strategies need to account for limited storage and computational resources on edge devices, as well as potentially intermittent network connectivity. Intelligent caching strategies that prioritize the most relevant data or model components for each edge device can significantly improve the performance and reliability of edge ML systems.

Lastly, the concept of semantic caching is gaining traction in ML systems. Unlike traditional caching that operates on exact matches, semantic caching attempts to reuse cached results for semantically similar queries. This can be particularly useful in ML systems where slight variations in input may not significantly change the output, potentially leading to substantial performance improvements.

### Data Access Patterns {#sec-data-engineering-data-access-patterns-f6e7}

Understanding the access patterns in ML systems is useful for designing efficient storage solutions and optimizing the overall system performance. ML workloads exhibit distinct data access patterns that often differ significantly from traditional database or analytics workloads.

One of the most prominent access patterns in ML systems is sequential reading of large datasets during model training. Unlike transactional systems that typically access small amounts of data randomly, ML training often involves reading entire datasets multiple times (epochs) in a sequential manner. This pattern is particularly evident in ML training tasks, where large volumes of data are processed repeatedly to improve model performance. Storage systems optimized for high-throughput sequential reads, such as distributed file systems or object stores, are well-suited for this access pattern.

However, the sequential read pattern is often combined with random shuffling between epochs to prevent overfitting and improve model generalization. This introduces an interesting challenge for storage systems, as they need to efficiently support both sequential and random access patterns, often within the same training job.

In contrast to the bulk sequential reads common in training, inference workloads often require fast random access to specific data points or features. For example, a recommendation system might need to quickly retrieve user and item features for real-time personalization. This necessitates storage solutions with low-latency random read capabilities, often leading to the use of in-memory databases or caching layers.

Feature stores, which we discussed earlier, introduce their own unique access patterns. They typically need to support both high-throughput batch reads for offline training and low-latency point lookups for online inference. This dual-nature access pattern often leads to the implementation of separate offline and online storage layers, each optimized for its specific access pattern.

Time-series data, common in many ML applications such as financial forecasting or IoT analytics, presents another interesting access pattern. These workloads often involve reading contiguous blocks of time-ordered data, but may also require efficient retrieval of specific time ranges or periodic patterns. Specialized time-series databases or carefully designed partitioning schemes in general-purpose databases are often employed to optimize these access patterns.

Another important consideration is the write access pattern in ML systems. While training workloads are often read-heavy, there are scenarios that involve significant write operations. For instance, continual learning systems may frequently update model parameters, and online learning systems may need to efficiently append new training examples to existing datasets.

Understanding these diverse access patterns is helpful in designing and optimizing storage systems for ML workloads. It often leads to hybrid storage architectures that combine different technologies to address various access patterns efficiently. For example, a system might use object storage for large-scale sequential reads during training, in-memory databases for low-latency random access during inference, and specialized time-series storage for temporal data analysis.

As ML systems continue to evolve, new access patterns are likely to emerge, driving further innovation in storage technologies and architectures. The challenge lies in creating flexible, scalable storage solutions that can efficiently support the diverse and often unpredictable access patterns of modern ML workloads.

### Continuing the KWS Example {#sec-data-engineering-continuing-kws-example-11a6}

During development and training, KWS systems must efficiently store and manage large collections of audio data. This includes raw audio recordings from various sources (crowd-sourced, synthetic, and real-world captures), processed features (like spectrograms or MFCCs), and model checkpoints. A typical architecture might use a data lake for raw audio files, allowing flexible storage of diverse audio formats, while processed features are stored in a more structured data warehouse for efficient access during training.

KWS systems benefit significantly from feature stores, particularly for managing pre-computed audio features. For example, commonly used audio representations can be computed once and stored for reuse across different experiments or model versions. The feature store must handle both batch access for training and real-time access for inference, often implementing a dual storage architecture, which includes an offline store for training data and an online store for low-latency inference.

In production, KWS systems require careful consideration of edge storage requirements. The models must be compact enough to fit on resource-constrained devices while maintaining quick access to necessary parameters for real-time wake word detection. This often involves optimized storage formats and careful caching strategies to balance between memory usage and inference speed.

## Data Governance {#sec-data-engineering-data-governance-f561}

Data governance is a significant component in the development and deployment of ML systems. It encompasses a set of practices and policies that ensure data is accurate, secure, compliant, and ethically used throughout the ML lifecycle. As ML systems become increasingly integral to decision-making processes across various domains, the importance of robust data governance has grown significantly.

One of the central challenges of data governance is addressing the unique complexities posed by ML workflows. These workflows often involve opaque processes, such as feature engineering and model training, which can obscure how data is being used. As shown in @fig-data-governance-pillars, governance practices aim to tackle these issues by focusing on maintaining data privacy, ensuring fairness, and providing transparency in decision-making processes. These practices go beyond traditional data management to address the evolving needs of ML systems.

::: {#fig-data-governance-pillars fig-env="figure" fig-pos="htb"}
```{.tikz}
\resizebox{.8\textwidth}{!}{
\begin{tikzpicture}[line join=round,font=\usefont{T1}{phv}{m}{n}]
% Macro for drawing a gear
% #1 number of teeths
% #2 radius intern
% #3 radius extern
% #4 angle from start to end of the first arc
% #5 angle to decale the second arc from the first
% #6 inner radius to cut off
\newcommand{\gear}[6]{%
  (0:#2)
  \foreach \i [evaluate=\i as \n using {(\i-1)*360/#1}] in {1,...,#1}{%
    arc (\n:\n+#4:#2) {[rounded corners=1.5pt] -- (\n+#4+#5:#3)
    arc (\n+#4+#5:\n+360/#1-#5:#3)} --  (\n+360/#1:#2)
  }%
  (0,0) circle[radius=#6]
}
% Styles for planets, satellites, and arrows
\tikzset{%
planet/.style = {circle, draw=none,
semithick, fill=blue!30,
%ball color=cyan!70,shading angle=-15,
fill=cyan!80!black!60,
text width=27mm, inner sep=1mm,align=center},
satellite/.style = {circle, draw=none, semithick, fill=#1!30,
text width=18mm, inner sep=1pt, align=flush center,minimum size=21mm},
arr/.style = {-{Triangle[length=3mm,width=6mm]}, color=#1,
line width=3mm, shorten <=1mm, shorten >=1mm}
}
% Outer circle and central planet
\node[draw=BackLine!50,line width=5pt,circle,minimum size=216.8]{};
\node (p) [planet] {\bfseries Data\\ Governance };
% Satellites around the planet
\foreach \i [count=\k] in {red,cyan, purple, green!80!black!70!, orange, yellow,brown!80!,violet}
{
\node (s\k) [satellite=\i] at (\k*45:3.8) {};
}
% Arcs around satellites
\def\ra{24mm}
\foreach \i [count=\k] in{-45,0,45,90,135,180,225,270}{
\pgfmathtruncatemacro{\newX}{\i + 180}
\draw[BrownLine, line width=0.75pt,{Circle[BrownLine,length=4pt]}-{Circle[BrownLine,length=4pt]}]
   (s\k)+(\i:0.5*\ra) arc[start angle=\i, end angle=\newX, radius=0.5*\ra];
}
% Gears decoration
\begin{scope}[local bounding box=GEARS,shift={($(s4)+(0.2,0.3)$)},scale=1.5, every node/.append style={transform shape}]
\begin{scope}[scale=0.1, every node/.append style={transform shape}]
\fill[red,even odd rule] \gear{10}{1.9}{1.4}{10}{2}{0.6}coordinate(GER2);
\end{scope}
\begin{scope}[scale=0.15, every node/.append style={transform shape},
shift={(-1.8,-2.08)}]
\fill[red!90,even odd rule] \gear{10}{1.9}{1.4}{11}{2}{0.6}coordinate(GER2);
\end{scope}
\end{scope}
% Persons icons
\begin{scope}[shift={($(s2)+(-0.1,0.23)$)},scale=0.65,line width=1.0pt]
\begin{scope}[shift={(0.3,0.3)}]%person2-back
\coordinate (head-center) at (0,0);
\coordinate (top) at ([yshift=-2mm]head-center);
\coordinate (left) at ([yshift=-10mm,xshift=-7mm]head-center);
\coordinate (right) at ([yshift=-10mm,xshift=7mm]head-center);
\draw[rounded corners=1.5mm,fill=Green!70]
  (top) to [out=-10,in=100]
  (right) to [bend left=15]
  (left) to [out=80,in=190]
  (top);
 \draw[fill=yellow] (head-center) circle (0.35);
\end{scope}
\begin{scope}%person1
\coordinate (head-center) at (0,0);
\coordinate (top) at ([yshift=-2mm]head-center);
\coordinate (left) at ([yshift=-10mm,xshift=-7mm]head-center);
\coordinate (right) at ([yshift=-10mm,xshift=7mm]head-center);
\draw[rounded corners=1.5mm,fill=Green!70]
  (top) to [out=-10,in=100]
  (right) to [bend left=15]
  (left) to [out=80,in=190]
  (top);
  \draw[fill=yellow] (head-center) circle (0.35);
\end{scope}
\end{scope}
% Padlock icon
\begin{scope}[shift={($(s3)+(-0.42,0.15)$)},scale=0.30,line width=1.0pt]
\colorlet{black}{violet}
\draw[fill=black,draw=black](0,0)--(2.7,0)--++(270:1.6)to[out=270,in=0](1.85,-2.45)
--++(180:1.1)to[out=180,in=270](0,-1.3)--cycle;
\draw[draw=none,fill=white](1.32,-0.9)+(230:0.3)
arc[start angle=230, end angle=-50, radius=0.3]--++(280:0.75)--++(180:0.62)--cycle;
\path[red](0.27,0)circle(1pt)coordinate(K1);
\path[red](0.57,0)circle(1pt)coordinate(K2);
\path[red](2.10,0)circle(1pt)coordinate(K3);
\path[red](2.4,0)circle(1pt)coordinate(K4);
\path[green](K1)--++(90:0.6)coordinate(KK1);
\path[green](K2)--++(90:0.5)coordinate(KK2);
\path[green](K4)--++(90:0.6)coordinate(KK4);
\path[green](K3)--++(90:0.5)coordinate(KK3);
\draw[fill=black,draw=black](K1)--(KK1)to[out=90,in=90,distance=37](KK4)--(K4)
--(K3)--(KK3)to[out=90,in=90,distance=29](KK2)--(K2)--cycle;
\end{scope}
% Cloud icon
\colorlet{red}{red}
\begin{scope}[shift={($(s6)+(-0.54,-0.49)$)},scale=0.70,line width=1.0pt]
\draw[red,line width=1.5pt](0,0)to[out=170,in=180,distance=11](0.1,0.61)
to[out=90,in=105,distance=17](1.07,0.71)
to[out=20,in=75,distance=7](1.48,0.36)
to[out=350,in=0,distance=7](1.48,0)--(0,0);
\draw[red,line width=1.5pt](0.27,0.71)to[bend left=25](0.49,0.96);
\draw[red,line width=1.5pt](0.67,1.21)to[out=55,in=90,distance=13](1.5,0.96)
to[out=360,in=30,distance=9](1.68,0.42);
\end{scope}
% Data quality block
\begin{scope}[shift={($(s5)+(0.0,-0.03)$)},scale=0.70,every node/.append style={transform shape}]
 \node[draw, minimum width  =20mm, minimum height = 12mm, inner sep      = 0pt,
        rounded corners,draw = BlueLine, fill=cyan!10, line width=2.0pt](COM){};
 \draw[draw = BlueLine,line width=1.0pt]
 ($(COM.north west)!0.85!(COM.south west)$)-- ($(COM.north east)!0.85!(COM.south east)$);
\node[GreenLine](CB1) at ($(COM.north west)!0.25!(COM.south west)+(0.3,0)$){
\mbox{\ooalign{$\checkmark$\cr\hidewidth$\square$\hidewidth\cr}}};
\node[GreenLine](CB2) at ($(COM.north west)!0.6!(COM.south west)+(0.3,0)$){
\makebox[0pt][l]{$\square$}\raisebox{.15ex}{\hspace{0.1em}$\checkmark$}};
 \draw[GreenLine,decoration={zigzag,segment length=4pt, amplitude=0.5pt},decorate]($(CB1)+(0.3,0.05)$)--++(0:1.3);
\draw[GreenLine,decoration={zigzag,segment length=4pt, amplitude=0.5pt},decorate]($(CB1)+(0.3,-0.12)$)--++(0:1.0);
  \draw[GreenLine,decoration={zigzag,segment length=4pt, amplitude=0.5pt},decorate]($(CB2)+(0.3,0.05)$)--++(0:1.3);
\draw[GreenLine,decoration={zigzag,segment length=4pt, amplitude=0.5pt},decorate]($(CB2)+(0.3,-0.12)$)--++(0:1.0);
\end{scope}
% Data cylinders icon
\tikzset{mycylinder/.style={cylinder, shape border rotate=90, aspect=1.3, draw, fill=white,
minimum width=25mm,minimum height=11mm,line width=\Linewidth,node distance=-0.15},
pics/data/.style = {
        code = {
        \pgfkeys{/channel/.cd, #1}
\begin{scope}[local bounding box=STREAMING,scale=\scalefac, every node/.append style={transform shape}]
\node[mycylinder,fill=\channelcolor!50] (A) {};
\node[mycylinder, above=of A,fill=\channelcolor!30] (B) {};
\node[mycylinder, above=of B,fill=\channelcolor!10] (C) {};
 \end{scope}
     }
  }
}
\pgfkeys{
  /channel/.cd,
  channelcolor/.store in=\channelcolor,
  drawchannelcolor/.store in=\drawchannelcolor,
  scalefac/.store in=\scalefac,
  Linewidth/.store in=\Linewidth,
  picname/.store in=\picname,
  channelcolor=BrownLine,
  drawchannelcolor=BrownLine,
  scalefac=1,
  Linewidth=1.6pt,
  picname=C
}
% Data element placement
\begin{scope}[shift={($(s8)+(0.03,-0.43)$)},scale=0.40,every node/.append style={transform shape}]
\pic[shift={(0,0)}] at  (0,0){data={scalefac=1,picname=1,channelcolor=BlueLine, Linewidth=0.7pt}};
\end{scope}
% Policies block with checkmarks
\newcommand{\tikzxmark}{%
\tikz[scale=0.18] {
    \draw[line width=0.7,line cap=round] (0,0) to [bend left=6] (1,1);
    \draw[line width=0.7,line cap=round] (0.2,0.95) to [bend right=3] (0.8,0.05);
}}
\begin{scope}[shift={($(s1)+(0.04,-0.03)$)},scale=0.70,every node/.append style={transform shape}]
 \node[draw, minimum width  =15mm, minimum height = 20mm, inner sep = 0pt,
        rounded corners,draw = BrownLine, fill=BrownL!10, line width=1.0pt](COM){};
\node[GreenLine](CB1) at ($(COM.north west)!0.25!(COM.south west)+(0.3,0)$){
\mbox{\ooalign{$\checkmark$\cr\hidewidth$\square$\hidewidth\cr}}};
\node[GreenLine](CB2) at ($(COM.north west)!0.5!(COM.south west)+(0.3,0)$){
\makebox[0pt][l]{$\square$}\raisebox{.15ex}{\hspace{0.1em}\tikzxmark}};
\node[GreenLine](CB3) at ($(COM.north west)!0.75!(COM.south west)+(0.3,0)$){
\makebox[0pt][l]{$\square$}\raisebox{.15ex}{\hspace{0.1em}\tikzxmark}};
\draw[GreenLine,decoration={zigzag,segment length=4pt, amplitude=0.5pt},decorate]($(CB1)+(0.3,0.05)$)--++(0:0.8);
\draw[GreenLine,decoration={zigzag,segment length=4pt, amplitude=0.5pt},decorate]($(CB1)+(0.3,-0.12)$)--++(0:0.7);
\draw[GreenLine,decoration={zigzag,segment length=4pt, amplitude=0.5pt},decorate]($(CB2)+(0.3,0.05)$)--++(0:0.8);
\draw[GreenLine,decoration={zigzag,segment length=4pt, amplitude=0.5pt},decorate]($(CB2)+(0.3,-0.12)$)--++(0:0.6);
\draw[GreenLine,decoration={zigzag,segment length=4pt, amplitude=0.5pt},decorate]($(CB3)+(0.3,0.05)$)--++(0:0.8);
\draw[GreenLine,decoration={zigzag,segment length=4pt, amplitude=0.5pt},decorate]($(CB3)+(0.3,-0.12)$)--++(0:0.6);
\end{scope}
% Bar chart icon
\begin{scope}[shift={($(s7)+(-0.35,-0.51)$)},scale=0.50,every node/.append style={transform shape}]
\draw[line width=2.0pt,RedLine](-0.20,0)--(2,0);
\draw[line width=2.0pt,RedLine](-0.20,0)--(-0.20,2);
\foreach \i/\vi in {0/10,0.5/17,1/9,1.5/5}{
\node[draw, minimum width  =4mm, minimum height = \vi mm, inner sep = 0pt,
      draw = RedLine, fill=RedL!40, line width=1.0pt,anchor=south west](COM)at(\i,0.2){};
}
\end{scope}
% Labels for satellites
\node[above=5pt of s2]{Organization};
\node[left=5pt of s3]{Data Security};
\node[left=5pt of s4]{Data Operations};
\node[left=5pt of s5,align=center]{Data quality \&\\ master Data};
\node[below=5pt of s6]{Data Sourcing};
\node[right=5pt of s7,align=center]{Data  \& \\ analytic definitions};
\node[right=5pt of s8]{Data  catalogs};
\node[right=5pt of s1]{Policies};
\end{tikzpicture}
}
```
**Data Governance Pillars**: Robust data governance establishes ethical and reliable machine learning systems by prioritizing privacy, fairness, transparency, and accountability throughout the data lifecycle. These interconnected pillars address unique challenges in ML workflows, ensuring responsible data usage and auditable decision-making processes.
:::

Security and access control form an essential aspect of data governance. Implementing measures to protect data from unauthorized access or breaches is critical in ML systems, which often deal with sensitive or proprietary information. For instance, a healthcare application may require granular access controls to ensure that only authorized personnel can view patient data. Encrypting data both at rest and in transit is another common approach to safeguarding information while enabling secure collaboration among ML teams.

Privacy protection is another key pillar of data governance. As ML models often rely on large-scale datasets, there is a risk of infringing on individual privacy rights. Techniques such as differential privacy can address this concern by adding carefully calibrated noise to the data. This ensures that individual identities are protected while preserving the statistical patterns necessary for model training. These techniques allow ML systems to benefit from data-driven insights without compromising ethical considerations [@dwork2008differential], which we will learn more about in the Responsible AI chapter.

Regulatory compliance is a critical area where data governance plays a central role. Laws such as the GDPR in Europe and the HIPAA in the United States impose strict requirements on data handling. Compliance with these regulations often involves implementing features like the ability to delete data upon request or providing individuals with copies of their data, and a "right to explanation" on decisions made by algorithms [@wachter2017counterfactual]. Standardized documentation frameworks like data cards (@fig-data-card) provide structured approaches to document dataset characteristics, limitations, and compliance requirements. These measures not only protect individuals but also ensure organizations avoid legal and reputational risks.

::: {#fig-data-card fig-env="figure" fig-pos="t!"}
```{.tikz}
\begin{tikzpicture}[font=\footnotesize\usefont{T1}{phv}{m}{n},line width=0.75pt]
\newcommand\Warning[1][1.4]{%
 \makebox[#1em][c]{%
 \makebox[0pt][c]{\raisebox{.3em}{\fontsize{7pt}{7}\selectfont\bfseries !}}%
 \makebox[0pt][c]{\color{red}\LARGE$\bigtriangleup$}}}%

\colorlet{BlueD}{blue!50!black}

\newcommand\barrow{%
\begin{tikzpicture}
\begin{scope}[local bounding box=BARROW,scale=0.6, every node/.append style={transform shape}]
\node[fill=white,draw=BlueD,line width=0.75pt,rectangle,minimum width=4mm,
minimum height=4mm,inner sep=0pt](RS1){};
\draw[shorten >=1pt,shorten <=-1.5pt,draw=BlueD,line width=0.75pt,
-{Latex[length=2pt, width=3pt]}](RS1.center)--(RS1.north east);
\end{scope}
\end{tikzpicture}
     }
 \tikzset{%
    Text/.style={align=flush left},
    TextB1/.style={align=flush left,font=\fontsize{11pt}{13}\selectfont\usefont{T1}{phv}{m}{n}\bfseries},
    TextB2/.style={align=flush left,font=\fontsize{10pt}{11}\selectfont\usefont{T1}{phv}{m}{n}\bfseries},
    TextB3/.style={align=flush left,font=\fontsize{9pt}{10}\selectfont\usefont{T1}{phv}{m}{n}\bfseries},
    TextBLUE/.style={BlueD},
    TextF/.style={align=flush left,font=\fontsize{6.5pt}{8}\selectfont\usefont{T1}{phv}{m}{n}},
  Box/.style={%
    draw=BrownLine,
    line width=0.75pt,
    rounded corners=3pt,
    fill=BrownL!40,
    minimum height=5mm
  },
}

\node[TextB1](N11){Open Images Extended - More \\ Inclusively Annotated People (MIAP)};
\node[TextBLUE,below=1mm of N11.south west,anchor=north west](N12){
Dataset Download~\barrow â¢ Related Publication~\barrow};
\node[TextF,text width=92mm,right=14mm of N12.south east,anchor=south west](N13){This dataset was created for
fairness research and fairness evaluations
in person detection. This dataset contains 100,000 images sampled from
Open Images V6 with additional annotations added. Annotations include the
image coordinates of bounding boxes for each visible person. Each box is annotated
with attributes for perceived gender presentation and
age range presentation. It can be used in conjunction with Open Images V6.};
%
\scoped[on background layer]
\node[draw=none,fit=(N11)(N13)](BB1){};
%%%%%%%2
\node[TextB2,below=of N11.south west,anchor=north west](N21){Authorship};
\node[TextBLUE,below=0mm of N21.south west,anchor=north west](N22){PUBLISHER(S)};
\node[TextB3,below=0mm of N22.south west,anchor=north west](N23){Google LLC};
\node[TextBLUE,right=16mm of N22.east,anchor=west](N24){INDUSTRY TYPE};
\node[TextF,below=0mm of N24.south west,anchor=north west](N25){Corporate - Tech};
\node[TextBLUE,right=32mm of N24.east,anchor=west](N26){DATASET AUTHORS};
\node[TextF,below=0mm of N26.south west,anchor=north west](N27){Candice Schumann, Google, 2021 \\
Susanna Ricco, Google, 2021 \\ Utsav Prabhu, Google, 2021 \\ Vittorio Ferrari, Google, 2021\\
Caroline Pantofaru, Google, 2021};
%
\node[TextBLUE,below=16mm of N22.south west,anchor=north west](N28){PUBLISHER(S)};
\node[TextB3,below=0mm of N28.south west,anchor=north west](N29){Google LLC};
\path[red](N28)-|coordinate(S21)(N25.south west);
\node[TextBLUE,anchor=west](N24)at(S21){FUNDING TYPE};
\node[TextF,below=0mm of N24.south west,anchor=north west](N210){Private Funding};
\path[red](N28)-|coordinate(S22)(N26);
\node[TextBLUE](N211)at(S22){DATASET CONTACT};
\node[TextF,Box,text=BlueD,below=0mm of N211.south west,anchor=north west,
xshift=1.5mm](N212){open-images-extended@google.com};
%
%%%% 3
\node[TextB2,below=36mm of N21.south west,anchor=north west](N31){Motivations};
\node[TextBLUE,below=1mm of N31.south west,anchor=north west](N32){DATASET PURPOSE(S)};
\node[TextB3,below=0mm of N32.south west,anchor=north west](N33){Research Purposes\\[1ex]
Machine Learning};
\node[TextF,below=0mm of N33.south west,anchor=north west](N33a){Training, testing, and validation};
\path[red](N32)-|coordinate(S30)(N24.south west);
\node[TextBLUE,anchor=west](N34)at(S30){KEY APPLICATION(S)};
\node[TextF,Box,below=0mm of N34.south west,anchor=north west,xshift=1.5mm](N3212){Machine Learning};
\node[TextF,Box,right=2mm of N3212.east,anchor=west,xshift=1.5mm](N3213){Object Recognition};
\node[TextF,Box,below=8mm of N34.south west,anchor=north west,xshift=1.5mm](N3212){Machine Learning Fairness};
%
\path[red](N32)-|coordinate(S300)(N211.south west);
\node[TextBLUE,anchor=west](N36)at(S300){PROBLEM SPACE};
\node[TextF,below=0mm of N36.south west,anchor=north west](N37){This dataset was created for fairness research
and\\ fairness evaluation with respect   to person detection.};
\node[TextBLUE,below=0mm of N37.south west,anchor=north west](N35){
See accompanying article~\barrow};
%
\node[TextBLUE,below=18mm of N32.south west,anchor=north west](N38){};
\node[TextB3,below=0mm of N38.south west,anchor=north west](N39){};
\path[red](N38)-|coordinate(S31)(N34.south west);
\node[TextBLUE,anchor=west](N39)at(S31){PRIMARY MOTIVATION(S)};
\node[TextF,below=0mm of N39.south west,anchor=north west,text width=50mm, align=flush left](N310){%
\leftmargini=9pt\vspace*{-4mm}
\begin{itemize} \itemsep=-3pt
\item Provide more complete ground-truth for bounding boxes around people.
\item Provide a standard fairness evaluation set for the broader fairness community.
\end{itemize}};
%
\path[red](N38)-|coordinate(S32)(N35.south west);
\node[TextBLUE,anchor=west](N34)at(S32){INTENDED AND/OR SUITABLE USE CASE(S)};
\node[TextF,below=0mm of N34.south west,anchor=north west,text width=72mm, align=flush left](N310){%
\leftmargini=9pt\vspace*{-4mm}
\begin{itemize} \itemsep=-3pt
\item \textbf{ML Model Evaluation for:} person detection, fairness evaluation
\item \textbf{ML Model Training for:} person detection, Object detection
\end{itemize}\vspace*{-1mm}
Also: \\\vspace*{-2mm}
\leftmargini=9pt
\begin{itemize} \itemsep=-3pt
\item \textbf{Person detection:} Without specifying gender or age presentations\\
\item \textbf{Fairness evaluations:} Over gender and age presentations\\
\item \textbf{Fairness research:} Without building gender presentation or age classifiers
\end{itemize}
};
\path[red](N38)-|coordinate(S32)(N36);
%%%%%%%%%%4
\node[TextB2,below=58mm of N31.south west,anchor=north west](N41){Use of Dataset};
\node[TextBLUE,below=0mm of N41.south west,anchor=north west](N42){SAFETY OF USE};
\node[TextB3,below=0mm of N42.south west,anchor=north west](N43){Conditional Use};
\node[TextF,below=0mm of N43.south west,anchor=north west](N431){There are some known\\ unsafe applications.};
%
\path[red](N42)-|coordinate(S40)(N39.south west);
\node[TextBLUE,anchor=west](N44)at(S40){UNSAFE APPLICATION(S)};
\node[TextF,below=0mm of N44.south west,anchor=north west,xshift=1.0mm,yshift=1mm](N441){
\Warning};
\node[TextF,Box,right=-1mm of N441.east,anchor=west,xshift=1.5mm](N442){Gender classificationn};
\node[TextF,Box,right=0mm of N442.east,anchor=west,xshift=1.5mm](N443){Age classification};
%
\path[red](N42)-|coordinate(S401)(N310.south west);
\node[TextBLUE,anchor=west](N46)at(S401){UNSAFE USE CASE(S)};
\node[TextF,below=0mm of N46.south west,anchor=north west,text width=72mm](N47){This dataset should not be used to create gender or age classifiers. The intention of perceived gender and age labels is to capture gender and age presentation as assessed by a third party based on visual cues alone, rather than an individual's self-identified gender or actual age.};
%
\node[TextBLUE,below=15mm of N42.south west,anchor=north west](N48){CONJUNCTIONAL USE};
\node[TextB3,below=0mm of N48.south west,anchor=north west](N49){Safe to use with\\ other datasets};
\path[red](N48)-|coordinate(S41)(N44.south west);
\node[TextBLUE,anchor=west](N44)at(S41){KNOWN CONJUNCTIONAL DATASET(S)};
\node[TextF,below=0mm of N44.south west,anchor=north west,text width=55mm](N410){%
\leftmargini=9pt\vspace*{-4mm}
\begin{itemize} \itemsep=-3pt
\item The data in this dataset can be combined with \textcolor{BlueD}{Open Images V6}
\end{itemize}};
\path[red](N48)-|coordinate(S42)(N46.south west);
\node[TextBLUE,anchor=west](N411)at(S42){KNOWN CONJUNCTIONAL USES};
\node[TextF,below=0mm of N411.south west,anchor=north west,
](N412){Analyzing bounding box annotations not annotated under\\ the Open Images V6 procedure.};
%%%%%%%%%%%%%%%%5
\node[TextBLUE,below=36mm of N41.south west,anchor=north west](N52){METHOD};
\node[TextB3,below=0mm of N52.south west,anchor=north west](N53){Object Detection};
%
\path[red](N52)-|coordinate(S50)(N44.south west);
\node[TextBLUE,anchor=west](N54)at(S50){SUMMARY};
\node[TextF,below=0mm of N54.south west,anchor=north west](N510){A person object detector can be trained using\\ the Object Detection API in Tensorflow.};
%
\path[red](N52)-|coordinate(S501)(N411.south west);
\node[TextBLUE,anchor=west](N56)at(S501){KNOWN CAVEATS};
\node[TextF,below=0mm of N56.south west,anchor=north west,text width=72mm](N57){
If this dataset is used in conjunction with the original Open Images dataset, negative examples
of people should only be pulled from images with an explicit negative person image level label.
\medskip
The dataset does not contain any examples not annotated as containing at least one person
by the original Open Images annotation procedure.};
%
\node[TextBLUE,below=19mm of N52.south west,anchor=north west](N58){METHOD};
\node[TextB3,below=0mm of N58.south west,anchor=north west](N59){Fairness Evaluation};
\path[red](N58)-|coordinate(S51)(N54.south west);
\node[TextBLUE,anchor=west](N54)at(S51){SUMMARY};
\node[TextF,below=0mm of N54.south west,anchor=north west](N510){Fairness evaluations can be run over the splits \\
of gender presentation and age presentation.};
\path[red](N58)-|coordinate(S52)(N56.south west);
\node[TextBLUE,anchor=west](N511)at(S52){KNOWN CAVEATS};
\node[TextF,below=0mm of N511.south west,anchor=north west,text width=72mm](N512){There still
exists a gender presentation skew towards unknown and predominantly masculine, as well as an
age presentation range skew towards middle.};
%
\node[draw=none,fit=(N52)(N512)](BB5){};
\scoped[on background layer]
\node[draw=BrownLine,inner xsep=0mm,inner ysep=0mm,yshift=0mm,
      fill=BrownL!10,fit=(BB1)(BB5),line width=0.75pt](BB){};
\foreach \i in{0.097,0.298,0.60,0.80}{
\draw[BrownLine,line width=0.75pt]($(BB.north west)!\i!(BB.south west)$)--($(BB.north east)!\i!(BB.south east)$);
}
\foreach \i in{0.097}{
\draw[BrownLine,line width=2.75pt]($(BB.north west)!\i!(BB.south west)$)--($(BB.north east)!\i!(BB.south east)$);
}
\end{tikzpicture}
```
**Data Governance Documentation**: Data cards standardize critical dataset information, enabling transparency and accountability required for regulatory compliance with laws like GDPR and HIPAA. By providing a structured overview of dataset characteristics, intended uses, and potential risks, data cards facilitate responsible AI practices and support data subject rights.
:::

Documentation and metadata management, which are often less discussed, are just as important for transparency and reproducibility in ML systems. Clear records of data lineage[^fn-data-lineage], including how data flows and transforms throughout the ML pipeline, are essential for accountability.

[^fn-data-lineage]: **Data Lineage Systems**: Track data from source to consumption across complex ML pipelines. Apache Atlas (originally Hortonworks, now Apache, 2015) and DataHub (LinkedIn, 2020) enable lineage tracking at enterprise scale. Critical for regulatory complianceâGDPR Article 30 requires detailed records of data processing activities, making lineage essential for demonstrating compliance. Standardized documentation frameworks, such as Data Cards proposed by @pushkarna2022data, offer a structured way to document the characteristics, limitations, and potential biases of datasets.

Audit trails[^fn-audit-trails] are another important component of data governance. These detailed logs track data access and usage throughout the lifecycle of ML models, from collection to deployment.

[^fn-audit-trails]: **ML Audit Requirements**: SOX compliance requires immutable audit logs for financial ML models, while HIPAA mandates detailed access logs for healthcare AI. Modern systems generate terabytes of audit dataâUber's ML platform logs 50+ billion events daily for compliance and debugging purposes. Comprehensive audit trails are invaluable for troubleshooting and accountability, especially in cases of data breaches or unexpected model behavior. They help organizations understand what actions were taken and why, providing a clear path for resolving issues and ensuring compliance.

Consider a hypothetical ML system designed to predict patient outcomes in a hospital. Such a system would need to address several governance challenges. It would need to ensure that patient data is securely stored and accessed only by authorized personnel, with privacy-preserving techniques in place to protect individual identities. The system would also need to comply with healthcare regulations governing the use of patient data, including detailed documentation of how data is processed and transformed. Comprehensive audit logs would be necessary to track data usage and ensure accountability.

As ML systems grow more complex and influential, the challenges of data governance will continue to evolve. Emerging trends, such as blockchain-inspired technologies[^fn-blockchain-governance] for tamper-evident logs and automated governance tools, offer promising solutions for real-time monitoring and issue detection.

[^fn-blockchain-governance]: **Blockchain for ML Governance**: Immutable ledgers provide tamper-proof audit trails for ML model decisions. Ocean Protocol and other projects use blockchain to track data provenance and usage rights. While promising for high-stakes applications like healthcare AI, blockchain's energy costs and complexity limit widespread adoption. By adopting robust data governance practices, including tools like Data Cards, organizations can build ML systems that are transparent, ethical, and trustworthy.

## Fallacies and Pitfalls

Data engineering forms the foundation of every ML system, yet it remains one of the most underestimated aspects of ML development. The complexity of managing data pipelines, ensuring quality, and maintaining governance creates numerous opportunities for costly mistakes that can undermine even the most sophisticated models.

**Fallacy:** _More data always leads to better model performance._

This widespread belief drives teams to collect massive datasets without considering data quality or relevance. While more data can improve performance when properly curated, raw quantity often introduces noise, inconsistencies, and irrelevant examples that degrade model performance. A smaller, high-quality dataset with proper labeling and representative coverage typically outperforms a larger dataset with quality issues. The computational costs and storage requirements of massive datasets also create practical constraints that limit experimentation and deployment options. Effective data engineering prioritizes data quality and representativeness over sheer volume.

**Pitfall:** _Treating data labeling as a simple mechanical task that can be outsourced without oversight._

Organizations often view data labeling as low-skill work that can be completed quickly by external teams or crowdsourcing platforms. This approach ignores the domain expertise, consistency requirements, and quality control necessary for reliable labels. Poor labeling guidelines, inadequate worker training, and insufficient quality validation lead to noisy labels that fundamentally limit model performance. The cost of correcting labeling errors after they affect model training far exceeds the investment in proper labeling infrastructure and oversight.

**Fallacy:** _Data engineering is a one-time setup that can be completed before model development begins._

This misconception treats data pipelines as static infrastructure rather than evolving systems that require continuous maintenance and adaptation. Real-world data sources change over time through schema evolution, quality degradation, and distribution shifts. Models deployed in production encounter new data patterns that require pipeline updates and quality checks. Teams that view data engineering as completed infrastructure rather than ongoing engineering practice often experience system failures when their pipelines cannot adapt to changing requirements.

**Fallacy:** _Training and test data splitting is sufficient to ensure model generalization._

While proper train/test splitting prevents overfitting to training data, it doesn't guarantee real-world performance. Production data often differs significantly from development datasets due to temporal shifts, geographic variations, or demographic changes. A model achieving 95% accuracy on a carefully curated test set may fail catastrophically when deployed to new regions or time periods. Robust evaluation requires understanding data collection biases, implementing continuous monitoring, and maintaining representative validation sets that reflect actual deployment conditions.

**Pitfall:** _Building data pipelines without considering failure modes and recovery mechanisms._

Data pipelines are often designed for the happy path where everything works correctly, ignoring the reality that data sources fail, formats change, and quality degrades. Teams discover these issues only when production systems crash or silently produce incorrect results. A pipeline processing financial transactions that lacks proper error handling for malformed data could lose critical records or duplicate transactions. Robust data engineering requires explicit handling of failures including data validation, checkpointing, rollback capabilities, and alerting mechanisms that detect anomalies before they impact downstream systems.

## Summary {#sec-data-engineering-summary-9702}

Data engineering serves as the foundational infrastructure that transforms raw information into the foundation of machine learning systems, determining not just model performance but also system reliability, ethical compliance, and long-term maintainability. This chapter revealed how every stage of the data pipeline, from initial problem definition through acquisition, storage, and governance, requires careful engineering decisions that cascade through the entire ML lifecycle. The seemingly straightforward task of "getting data ready" actually encompasses complex trade-offs between data quality and acquisition cost, real-time processing and batch efficiency, storage flexibility and query performance, and privacy protection and data utility.

The technical architecture of data systems demonstrates how engineering decisions compound across the pipeline to create either robust, scalable foundations or brittle, maintenance-heavy technical debt. Data acquisition strategies must navigate the reality that perfect datasets rarely exist in nature, requiring sophisticated approaches ranging from crowdsourcing and synthetic generation to careful curation and active learning. Storage architectures from traditional databases to modern data lakes and feature stores represent fundamental choices about how data flows through the system, affecting everything from training speed to serving latency. The emergence of streaming data processing and real-time feature stores reflects the growing demand for ML systems that can adapt continuously to changing environments while maintaining consistency and reliability.

::: {.callout-important title="Key Takeaways"}
* Data quality decisions made early in the pipeline have multiplicative effects on downstream model performance and system reliability
* The choice between batch and streaming processing architectures fundamentally shapes system capabilities and operational complexity
* Feature stores and data governance frameworks are becoming essential infrastructure for production ML systems at scale
* Effective data labeling requires both technical systems and human process design to ensure quality and ethical compliance
:::

The integration of robust data governance practices throughout the pipeline ensures that ML systems remain trustworthy, compliant, and transparent as they scale in complexity and impact. Data cards, lineage tracking, and automated monitoring create the observability needed to detect data drift, privacy violations, and quality degradation before they affect model behavior. These engineering foundations enable the distributed training strategies in @sec-ai-training, model optimization techniques in @sec-model-optimizations, and MLOps practices in @sec-ml-operations, where reliable data infrastructure becomes the prerequisite for scaling ML systems effectively.
