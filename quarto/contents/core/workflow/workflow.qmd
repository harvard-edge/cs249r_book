---
bibliography: workflow.bib
quiz: workflow_quizzes.json
concepts: workflow_concepts.yml
glossary: workflow_glossary.json
crossrefs: workflow_xrefs.json
---

# AI Workflow {#sec-ai-workflow}

::: {layout-narrow}
::: {.column-margin}
*DALL·E 3 Prompt: Create a rectangular illustration of a stylized flowchart representing the AI workflow/pipeline. From left to right, depict the stages as follows: 'Data Collection' with a database icon, 'Data Preprocessing' with a filter icon, 'Model Design' with a brain icon, 'Training' with a weight icon, 'Evaluation' with a checkmark, and 'Deployment' with a rocket. Connect each stage with arrows to guide the viewer horizontally through the AI processes, emphasizing these steps' sequential and interconnected nature.*
:::

\noindent
![](images/png/cover_ai_workflow.png)

:::

## Purpose {.unnumbered}

_How do structured workflows transform machine learning development from ad-hoc experimentation into reliable, reproducible engineering processes?_

Machine learning development often begins as exploratory data analysis and experimental model training, but production systems demand systematic, repeatable processes. Structured workflows transform this ad-hoc experimentation by establishing standardized stages for data collection, model development, validation, and deployment. These workflows address critical engineering challenges: ensuring data quality and consistency, managing model versioning and experimentation, automating testing and validation, and coordinating deployment across different environments. Systematic workflows enable teams to build reproducible systems, reduce development cycles, and maintain quality standards. This transformation from experimental prototyping to engineering discipline forms the operational backbone that supports reliable production deployments.

::: {.callout-tip title="Learning Objectives"}

- Understand the ML lifecycle and gain insights into the structured approach and stages of developing, deploying, and maintaining machine learning models.

- Identify the unique challenges and distinctions between lifecycles for traditional machine learning and specialized applications.

- Explore the various people and roles involved in ML projects.

- Examine the importance of system-level considerations, including resource constraints, infrastructure, and deployment environments.

- Appreciate the iterative nature of ML lifecycles and how feedback loops drive continuous improvement in real-world applications.

:::

## Overview {#sec-ai-workflow-overview-97fb}

The machine learning lifecycle is a systematic, interconnected process that guides the transformation of raw data into actionable models deployed in real-world applications. Each stage builds upon the outcomes of the previous one, creating an iterative cycle of refinement and improvement that supports robust, scalable, and reliable systems.

@fig-ml-lifecycle illustrates the lifecycle as a series of stages connected through continuous feedback loops. The process begins with data collection, which ensures a steady input of raw data from various sources. The collected data progresses to data ingestion, where it is prepared for downstream machine learning applications. Subsequently, data analysis and curation involve inspecting and selecting the most appropriate data for the task at hand. Following this, data labeling and data validation, which nowadays involves both humans and AI itself, ensure that the data is properly annotated and verified for usability before advancing further.

::: {#fig-ml-lifecycle fig-env="figure" fig-pos="htb"}
```{.tikz}
\begin{tikzpicture}[font=\usefont{T1}{phv}{m}{n}]
%
\tikzset{
  Box/.style={align=flush center,
    inner xsep=2pt,
    node distance=1.6,
    draw=GreenLine,
    line width=0.75pt,
    fill=GreenL,
    text width=25mm,
    minimum width=25mm, minimum height=23mm
  },
  Box1/.style={Box, node distance=3.7
  },
 Line/.style={line width=1.0pt,black!50,text=black},
 Text/.style={font=\usefont{T1}{phv}{m}{n}\footnotesize,align=center
  },
DLine/.style={draw=violet!60, line width=2pt, -{Triangle[length=3mm, bend]},
shorten >=1.1mm, shorten <=1.15mm},
}
%
\node[Box](B1){\textbf{Data Collection}\\\small Continuous input stream};
\node[Box,right=of B1](B2){\textbf{Data Ingestion}\\\small Prep data for downstream ML apps};
\node[Box, right=of B2](B3){\textbf{Data Analysis, Curation}\\\small  Inspect/select the right data};
\node[Box, right=of B3](B4){\textbf{Data Labeling}\\\small  Annotate data};
\node[Box, right=of B4](B5){\textbf{Data Validation}\\\small Verify data is usable through pipeline};
\node[Box, right=of B5](B6){\textbf{Data Preparation}\\\small Prep data for ML uses (split, versioning)};
%
\node[Box1,below=of B2,,fill=BlueL,draw=BlueLine](2B2){\textbf{ML System Deployment}\\\small  Deploy ML system to production};
\node[Box, right=of 2B2,,fill=BlueL,draw=BlueLine](2B3){\textbf{ML System Validation}\\\small Validate ML system for deployment};
\node[Box, right=of 2B3,,fill=BlueL,draw=BlueLine](2B4){\textbf{Model Evaluation}\\\small Compute model KPIs};
\node[Box, right=of 2B4,,fill=BlueL,draw=BlueLine](2B5){\textbf{Model Training}\\\small Use ML algos to create models};

\coordinate(S) at ($(B4.south)!0.5!(2B4.north)$);

\begin{scope}[local bounding box=AR,shift={($(S)+(-6,-0.7)$)},anchor=center]
% Dimensions
\def\w{6cm}
\def\h{15mm}
\def\r{6mm} % radius
\def\gap{4mm} % break lengths

\draw[cyan!90, -{Latex[length=10pt,width=22pt]},line width=10pt]
  (\w,\h-\r) -- (\w,\r)
  arc[start angle=0, end angle=-90, radius=\r]
  -- (\gap,0);

  \draw[green!50!black, -{Latex[length=10pt,width=22pt]},line width=10pt]
  (0,\r) -- (0,\h-\r)
  arc[start angle=180, end angle=90, radius=\r]
  -- ({\w-\gap},\h);
\end{scope}
%%%
\draw[Line,-latex](B1)--node[below,Text]{Raw\\ data}(B2);
\draw[Line,-latex](B2)--node[below,Text]{Indexed\\ data}(B3);
\draw[Line,-latex](B3)--node[below,Text]{Selected\\ data}(B4);
\draw[Line,-latex](B4)--node[below,Text]{Labeled\\ data}(B5);
\draw[Line,-latex](B5)--node[below,Text]{Validated\\ data}(B6);
\draw[Line,-latex](B6)|-node[left,Text,pos=0.2]{ML ready\\ Datasets}(2B5);
\draw[Line,-latex](2B5)--node[below,Text]{Models}(2B4);
\draw[Line,-latex](2B4)--node[below,Text]{KPIs}(2B3);
\draw[Line,-latex](2B3)--node[below,Text]{Validated\\ ML System}
node[above,Text]{ML\\ Certificate}(2B2);
\draw[Line,-latex](2B2)-|node[below,Text,pos=0.2]{Online\\ ML System}
node[right,Text,pos=0.8]{Online\\ Performance}(B1);

\draw[DLine,distance=44](B3.north)to[out=120,in=80]
node[below]{Data fixes}(B1.north);
\draw[DLine,distance=44](B5.north)to[out=120,in=80]
node[below]{Data needs}(B3.north);
\end{tikzpicture}
```
**ML Lifecycle Stages**: Iterative data processing and model refinement drive the development of machine learning systems, with continuous feedback loops enabling improvement across each stage, from initial data collection to final model deployment and monitoring. This cyclical process ensures models adapt to changing data and maintain performance in real-world applications.
:::

The data then enters the preparation stage, where it is transformed into machine learning-ready datasets through processes such as splitting and versioning. These datasets are used in the model training stage, where machine learning algorithms are applied to create predictive models. The resulting models are rigorously tested in the model evaluation stage, where performance metrics, such as key performance indicators (KPIs), are computed to assess reliability and effectiveness. Modern ML teams increasingly rely on experiment tracking systems[^fn-mlflow] to manage the complexity of iterative model development and comparison. The validated models move to the ML system validation phase, where they are verified for deployment readiness. Once validated, these models are integrated into production systems during the ML system deployment stage, ensuring alignment with operational requirements. The final stage tracks the performance of deployed systems in real time, enabling continuous adaptation to new data and evolving conditions.

[^fn-mlflow]: **Experiment Tracking Evolution**: MLflow, open-sourced by Databricks in 2018, was one of the first comprehensive experiment tracking platforms, addressing the "ML experiment management crisis" where data scientists were losing track of model versions and results. Similar platforms like Weights & Biases (2017) and Neptune (2019) emerged to solve what the industry calls "the reproducibility crisis"; studies found that only 15% of ML papers could be reproduced by other researchers, largely due to poor experiment tracking.

This general lifecycle forms the backbone of machine learning systems, with each stage contributing to the creation, validation, and maintenance of scalable and efficient solutions. While the lifecycle provides a detailed view of the interconnected processes in machine learning systems, it can be distilled into a simplified framework for practical implementation.

Each stage aligns with one of the following overarching categories:

- **Data Collection and Preparation** ensures the availability of high-quality, representative datasets.

- **Model Development and Training** focuses on creating accurate and efficient models tailored to the problem at hand.

- **Evaluation and Validation** rigorously tests models to ensure reliability and robustness in real-world conditions.

- **Deployment and Integration** translates models into production-ready systems that align with operational realities.

- **Monitoring and Maintenance** ensures ongoing system performance and adaptability in dynamic environments.

A defining feature of this framework is its iterative and dynamic nature. Feedback loops, such as those derived from monitoring that guide data collection improvements or deployment adjustments, ensure that machine learning systems maintain effectiveness and relevance over time. This adaptability is critical for addressing challenges such as shifting data distributions, operational constraints, and evolving user requirements. These challenges are compounded in production ML systems, where continuous integration and deployment practices[^fn-cicd-ml] must account for both code changes and data evolution.

[^fn-cicd-ml]: **CI/CD for Machine Learning**: Traditional continuous integration assumes deterministic builds—the same code produces the same output. ML systems violate this assumption because model behavior depends on training data, random initialization, and hardware differences. Google's TFX (TensorFlow Extended) and similar platforms had to reinvent CI/CD principles for ML, introducing concepts like "model validation" and "data validation" that have no equivalent in traditional software. Survey data shows that 78% of ML teams report that their traditional DevOps tools are inadequate for ML workflows.

By studying this framework, we establish a solid foundation for exploring specialized topics such as data engineering, model optimization, and deployment strategies in subsequent chapters. Viewing the ML lifecycle as an integrated and iterative process promotes a deeper understanding of how systems are designed, implemented, and maintained over time. To that end, this chapter focuses on the machine learning lifecycle as a systems-level framework, providing a high-level overview that bridges theoretical concepts with practical implementation. Through an examination of the lifecycle in its entirety, we gain insight into the interdependencies among its stages and the iterative processes that ensure long-term system scalability and relevance.

### Definition {#sec-ai-workflow-definition-d707}

The machine learning (ML) lifecycle is a structured, iterative process that guides the development, evaluation, and continual improvement of machine learning systems. Integrating ML into broader software engineering practices introduces unique challenges that necessitate systematic approaches to experimentation, evaluation, and adaptation over time [@amershi2019software]. This systematic approach builds upon decades of structured development methodologies[^fn-crisp-dm] that have evolved to address the unique challenges of data-driven systems.

[^fn-crisp-dm]: **CRISP-DM (Cross-Industry Standard Process for Data Mining)**: Developed in 1996 by a consortium of companies including IBM, SPSS, and Daimler, CRISP-DM was one of the first structured methodologies for data projects. Its six phases (Business Understanding, Data Understanding, Data Preparation, Modeling, Evaluation, Deployment) laid the groundwork for modern ML lifecycles, though today's approaches emphasize continuous iteration and monitoring that wasn't central to the original framework.

::: {.callout-definition title="Definition of the Machine Learning Lifecycle"}

**The Machine Learning (ML) Lifecycle** is a _structured, iterative process_ that defines the _key stages_ involved in the _development, deployment, and refinement_ of ML systems. It encompasses _interconnected steps_ such as _problem formulation, data collection, model training, evaluation, deployment, and monitoring_. The lifecycle emphasizes _feedback loops and continuous improvement_, ensuring that systems remain _robust, scalable, and responsive_ to _changing requirements and real-world conditions_.

:::

Rather than prescribing a fixed methodology, the ML lifecycle focuses on achieving specific objectives at each stage. This flexibility allows practitioners to adapt the process to the unique constraints and goals of individual projects. Typical stages include problem formulation, data acquisition and preprocessing, model development and training, evaluation, deployment, and ongoing optimization. Modern practitioners often use interactive development environments[^fn-jupyter] that support this iterative, experimental approach to ML system development.

[^fn-jupyter]: **Jupyter Notebooks**: Created by Fernando Pérez in 2001 as IPython, and later evolved into Project Jupyter in 2014. The name "Jupyter" comes from the core languages it supports: Julia, Python, and R. These notebooks revolutionized data science by allowing code, visualizations, and explanatory text in a single document, making the experimental nature of ML development more transparent and reproducible. Netflix estimates over 150,000 notebooks are created daily across the industry.

Although these stages may appear sequential, they are frequently revisited, creating a dynamic and interconnected process. The iterative nature of the lifecycle encourages feedback loops, whereby insights from later stages, including deployment, can inform earlier phases, including data preparation or model architecture design. This adaptability is essential for managing the uncertainties and complexities inherent in real-world ML applications.

From an instructional standpoint, the ML lifecycle provides a clear framework for organizing the study of machine learning systems. By decomposing the field into well-defined stages, students can engage more systematically with its core components. This structure mirrors industrial practice while supporting deeper conceptual understanding.

It is important to distinguish between the ML lifecycle and machine learning operations (MLOps), as the two are often conflated. The ML lifecycle, as presented in this chapter, emphasizes the stages and evolution of ML systems—the "what" and "why" of system development. In contrast, MLOps, which will be discussed in the [MLOps Chapter](../ops/ops.qmd), addresses the "how," focusing on tools, processes, and automation that support efficient implementation and maintenance. Introducing the lifecycle first provides a conceptual foundation for understanding the operational aspects that follow.

### Traditional vs. AI Lifecycles {#sec-ai-workflow-traditional-vs-ai-lifecycles-4f66}

Software development lifecycles have evolved through decades of engineering practice, establishing well-defined patterns for system development. Traditional lifecycles consist of sequential phases: requirements gathering, system design, implementation, testing, and deployment. Each phase produces specific artifacts that serve as inputs to subsequent phases. In financial software development, for instance, the requirements phase produces detailed specifications for transaction processing, security protocols, and regulatory compliance—specifications that directly translate into system behavior through explicit programming.

Machine learning systems require a fundamentally different approach to this traditional lifecycle model. The deterministic nature of conventional software, where behavior is explicitly programmed, contrasts sharply with the probabilistic nature of ML systems. Consider financial transaction processing: traditional systems follow predetermined rules (if account balance > transaction amount, then allow transaction), while ML-based fraud detection systems learn to recognize suspicious patterns from historical transaction data. This shift from explicit programming to learned behavior significantly reshapes the development lifecycle.

The unique characteristics of machine learning systems, characterized by data dependency, probabilistic outputs, and evolving performance, introduce new dynamics that alter how lifecycle stages interact. These systems require ongoing refinement, with insights from later stages frequently feeding back into earlier ones. Unlike traditional systems, where lifecycle stages aim to produce stable outputs, machine learning systems are inherently dynamic and must adapt to changing data distributions and objectives.

The key distinctions are summarized in @tbl-sw-ml-cycles below. These differences reflect the core challenge of working with data as a first-class citizen in system design, something traditional software engineering methodologies were not designed to handle[^fn-workflow-data-versioning].

[^fn-workflow-data-versioning]: **Data Versioning Challenges**: Unlike code, which changes through discrete edits, data can change gradually through drift, suddenly through schema changes, or subtly through quality degradation. Traditional version control systems like Git struggle with large datasets, leading to specialized tools like Git LFS (Large File Storage, 2015) and DVC (Data Version Control, 2017). Studies show that 87% of ML projects fail due to data issues, not algorithmic problems—highlighting why ML workflows must treat data with the same rigor as code.

+----------------------+------------------------------------------+--------------------------------------------------+
| Aspect               | Traditional Software Lifecycles          | Machine Learning Lifecycles                      |
+:=====================+:=========================================+:=================================================+
| Problem Definition   | Precise functional specifications are    | Performance-driven objectives evolve as          |
|                      | defined upfront.                         | the problem space is explored.                   |
+----------------------+------------------------------------------+--------------------------------------------------+
| Development Process  | Linear progression of feature            | Iterative experimentation with data, features    |
|                      | implementation.                          | and models.                                      |
+----------------------+------------------------------------------+--------------------------------------------------+
| Testing and          | Deterministic, binary pass/fail          | Statistical validation and metrics that          |
| Validation           | testing criteria.                        | involve uncertainty.                             |
+----------------------+------------------------------------------+--------------------------------------------------+
| Deployment           | Behavior remains static until            | Performance may change over time due             |
|                      | explicitly updated.                      | to shifts in data distributions.                 |
+----------------------+------------------------------------------+--------------------------------------------------+
| Maintenance          | Maintenance involves modifying code      | Continuous monitoring, updating data             |
|                      | to address bugs or add features.         | pipelines, retraining models, and adapting       |
|                      |                                          | to new data distributions.                       |
+----------------------+------------------------------------------+--------------------------------------------------+
| Feedback Loops       | Minimal; later stages rarely impact      | Frequent; insights from deployment and           |
|                      | earlier phases.                          | monitoring often refine earlier stages like      |
|                      |                                          | data preparation and model design.               |
+----------------------+------------------------------------------+--------------------------------------------------+

: **Traditional vs ML Development**: Traditional software and machine learning systems diverge in their development processes due to the data-driven and iterative nature of ML. Machine learning lifecycles emphasize experimentation and evolving objectives, requiring feedback loops between stages, whereas traditional software follows a linear progression with predefined specifications. {#tbl-sw-ml-cycles}

These differences underline the need for a robust ML lifecycle framework that can accommodate iterative development, dynamic behavior, and data-driven decision-making. This lifecycle ensures that machine learning systems remain effective not only at launch but throughout their operational lifespan, even as environments evolve.

## Lifecycle Stages {#sec-ai-workflow-lifecycle-stages-3032}

The AI lifecycle consists of several interconnected stages, each essential to the development and maintenance of effective machine learning systems. While the specific implementation details may vary across projects and organizations, @fig-lifecycle-overview provides a high-level illustration of the ML system development lifecycle. This chapter focuses on the overview, with subsequent chapters diving into the implementation aspects of each stage.

::: {#fig-lifecycle-overview fig-env="figure" fig-pos="htb"}
```{.tikz}
\begin{tikzpicture}[font=\small\usefont{T1}{phv}{m}{n}]
%
\tikzset{
  Box/.style={align=flush center,
    inner xsep=2pt,
    node distance=0.7,
    draw=GreenLine,
    line width=0.75pt,
    fill=GreenL,
    text width=20mm,
    minimum width=20mm, minimum height=14mm
  },
 Line/.style={line width=1.0pt,black!50,text=black},
 Text/.style={%
    inner sep=6pt,
    draw=none,
    line width=0.75pt,
    fill=TextColor,
    text=black,
    font=\footnotesize\usefont{T1}{phv}{m}{n},
    align=flush center,
    minimum width=7mm, minimum height=5mm
  },
}
%
\node[Box,fill=BlueL,draw=BlueLine](B1){Problem\\ Definition};
\node[Box,right=of B1](B2){Data Collection \& Preparation};
\node[Box, right=of B2](B3){Model Development \& Training};
\node[Box, right=of B3](B4){Evaluation\\ \& Validation};
\node[Box, right=of B4](B5){Deployment \& Integration};
\node[Box, right=of B5](B6){Monitoring \& Maintenance};
%
\foreach \i/\j in {B1/B2, B2/B3, B3/B4, B4/B5,B5/B6} {
    \draw[Line,-latex] (\i) -- (\j);
}
\draw[Line,-latex](B6)--++(270:1.6)-|node[Text,pos=0.25]{Feedback Loop}(B2);
\end{tikzpicture}
```
**ML System Lifecycle**: Iterative development defines successful machine learning systems, progressing through problem definition, data preparation, model building, evaluation, deployment, and ongoing monitoring for continuous improvement. Each stage informs subsequent iterations, enabling refinement and adaptation to changing requirements and data distributions.
:::

**Problem Definition and Requirements**: The first stage involves clearly defining the problem to be solved, establishing measurable performance objectives, and identifying key constraints. Precise problem definition ensures alignment between the system's goals and the desired outcomes.

**Data Collection and Preparation**: This stage includes gathering relevant data, cleaning it, and preparing it for model training. This process often involves curating diverse datasets, ensuring high-quality labeling, and developing preprocessing pipelines to address variations in the data.

**Model Development and Training**: In this stage, researchers select appropriate algorithms, design model architectures, and train models using the prepared data. Success depends on choosing techniques suited to the problem and iterating on the model design for optimal performance.

**Evaluation and Validation**: Evaluation involves rigorously testing the model's performance against predefined metrics and validating its behavior in different scenarios. This stage ensures the model is not only accurate but also reliable and robust in real-world conditions.

**Deployment and Integration**: Once validated, the trained model is integrated into production systems and workflows. This stage requires addressing practical challenges such as system compatibility, scalability, and operational constraints.

**Monitoring and Maintenance**: The final stage focuses on continuously monitoring the system's performance in real-world environments and maintaining or updating it as necessary. Effective monitoring ensures the system remains relevant and accurate over time, adapting to changes in data, requirements, or external conditions.

**A Case Study in Medical AI**:
To further ground our discussion on these stages, we will explore Google's Diabetic Retinopathy (DR) screening project as a case study. This project exemplifies the transformative potential of machine learning in medical imaging analysis, an area where the synergy between algorithmic innovation and robust systems engineering plays a pivotal role. Building upon the foundational work by @gulshan2016deep, which demonstrated the effectiveness of deep learning algorithms in detecting diabetic retinopathy from retinal fundus photographs, the project progressed from research to real-world deployment, revealing the complex challenges that characterize modern ML systems.

Diabetic retinopathy, a leading cause of preventable blindness worldwide, can be detected through regular screening of retinal photographs[^fn-dr-statistics]. @fig-eye-dr illustrates examples of such images: (A) a healthy retina and (B) a retina with diabetic retinopathy, marked by hemorrhages (dark red spots). The goal is to train a model to detect the hemorrhages.

[^fn-dr-statistics]: **Diabetic Retinopathy Global Impact**: Affects over 103 million people worldwide, with 28.5% of diabetic patients developing some form of retinopathy. In developing countries, up to 90% of vision loss from diabetes is preventable with early detection, but access to ophthalmologists remains severely limited—rural areas in India have one ophthalmologist per 120,000 people, compared to the WHO recommendation of 1 per 20,000. This stark disparity makes AI-assisted screening not just convenient but potentially life-changing for millions.

![**Retinal Hemorrhages**: Diabetic retinopathy causes visible hemorrhages in retinal images, providing a key visual indicator for model training and evaluation in medical image analysis. these images represent the input data used to develop algorithms that automatically detect and classify retinal diseases, ultimately assisting in early diagnosis and treatment. Source: Google.](images/png/eye-dr.png){#fig-eye-dr width=90%}

On the surface, the goal appears straightforward: develop an AI system that could analyze retinal images and identify signs of DR with accuracy comparable to expert ophthalmologists. However, as the project progressed from research to real-world deployment, it revealed the complex challenges that characterize modern ML systems.

The initial results in controlled settings were promising. The system achieved performance comparable to expert ophthalmologists in detecting DR from high-quality retinal photographs. Yet, when the team attempted to deploy the system in rural clinics across Thailand and India (based on Google's documented deployment experiences), they encountered a series of challenges that spanned the entire ML lifecycle, from data collection through deployment and maintenance. These deployment challenges reflect broader issues in healthcare AI[^fn-healthcare-ai-challenges] that affect most real-world medical ML applications.

[^fn-healthcare-ai-challenges]: **Healthcare AI Deployment Reality**: Studies show that 85% of healthcare AI projects never reach clinical deployment, with the majority failing not due to algorithmic issues but due to integration challenges, regulatory hurdles, and workflow disruption. The "AI chasm" between research success and clinical adoption is particularly wide in healthcare—while medical AI papers show 95%+ accuracy rates, real-world implementation studies report significant performance drops due to data drift, equipment variations, and user acceptance issues.

This case study will serve as a recurring thread throughout this chapter to illustrate how success in machine learning systems depends on more than just model accuracy. It requires careful orchestration of data pipelines, training infrastructure, deployment systems, and monitoring frameworks. Additionally, the project highlights the iterative nature of ML system development, where real-world deployment often necessitates revisiting and refining earlier stages.

While this narrative is inspired by Google's documented experiences in Thailand and India, certain aspects have been embellished to emphasize specific challenges frequently encountered in real-world healthcare ML deployments. These enhancements are to provide a richer understanding of the complexities involved while maintaining credibility and relevance to practical applications.

## Problem Definition {#sec-ai-workflow-problem-definition-87d9}

The development of machine learning systems begins with a critical challenge that fundamentally differs from traditional software development: defining not just what the system should do, but how it should learn to do it. Unlike conventional software, where requirements directly translate into implementation rules, ML systems require teams to consider how the system will learn from data while operating within real-world constraints[^fn-problem-definition]. This stage lays the foundation for all subsequent phases in the ML lifecycle.

[^fn-problem-definition]: **ML vs. Traditional Problem Definition**: Traditional software problems are defined by deterministic specifications ("if input X, then output Y"), but ML problems are defined by examples and desired behaviors. This shift means that 73% of ML project failures occur during problem definition, compared to only 32% for traditional software. The challenge lies in translating business objectives into learning objectives—something that didn't exist in software engineering until the rise of data-driven systems in the 2000s.

In our case study, diabetic retinopathy is a problem that blends technical complexity with global healthcare implications. With 415 million diabetic patients at risk of blindness worldwide and limited access to specialists in underserved regions, defining the problem required balancing technical goals, such as expert-level diagnostic accuracy, with practical constraints. The system needed to prioritize cases for early intervention while operating effectively in resource-limited settings. These constraints showcased how problem definition must integrate learning capabilities with operational needs to deliver actionable and sustainable solutions.

### Requirements and System Impact {#sec-ai-workflow-requirements-system-impact-db7f}

Defining an ML problem involves more than specifying desired performance metrics. It requires a deep understanding of the broader context in which the system will operate. For instance, developing a system to detect DR with expert-level accuracy might initially appear to be a straightforward classification task. After all, one might assume that training a model on a sufficiently large dataset of labeled retinal images and evaluating its performance against standard metrics would suffice.

However, real-world challenges complicate this picture. ML systems must function effectively in diverse environments, where factors like computational constraints, data variability, and integration requirements play significant roles. For example, the DR system needed to detect subtle features like microaneurysms, hemorrhages, and hard exudates across retinal images of varying quality while operating within the limitations of hardware in rural clinics. A model that performs well in isolation may falter if it cannot handle operational realities, such as inconsistent imaging conditions or time-sensitive clinical workflows. Addressing these factors requires aligning learning objectives with system constraints, ensuring the system's long-term viability in its intended context.

### Definition Workflow {#sec-ai-workflow-definition-workflow-baa6}

Establishing clear and actionable problem definitions involves a multi-step workflow that bridges technical, operational, and user considerations. The process begins with identifying the core objective of the system—what tasks it must perform and what constraints it must satisfy. Teams collaborate with stakeholders to gather domain knowledge, outline requirements, and anticipate challenges that may arise in real-world deployment.

In the DR project, this phase involved close collaboration with clinicians to determine the diagnostic needs of rural clinics. Key decisions, such as balancing model complexity with hardware limitations and ensuring interpretability for healthcare providers, were made during this phase. The team's iterative approach also accounted for regulatory considerations, such as patient privacy and compliance with healthcare standards. This collaborative process ensured that the problem definition aligned with both technical feasibility and clinical relevance.

### Scale and Distribution {#sec-ai-workflow-scale-distribution-e38e}

As ML systems scale, their problem definitions must adapt to new operational challenges. For example, the DR project initially focused on a limited number of clinics with consistent imaging setups. However, as the system expanded to include clinics with varying equipment, staff expertise, and patient demographics, the original problem definition required adjustments to accommodate these variations.

Scaling also introduces data challenges. Larger datasets may include more diverse edge cases, which can expose weaknesses in the initial model design. In the DR project, for instance, expanding the deployment to new regions introduced variations in imaging equipment and patient populations that required further tuning of the system. Defining a problem that accommodates such diversity from the outset ensures the system can handle future expansion without requiring a complete redesign.

### Systems Thinking {#sec-ai-workflow-systems-thinking-24d3}

Problem definition, viewed through a systems lens, connects deeply with every stage of the ML lifecycle. Choices made during this phase shape how data is collected, how models are developed, and how systems are deployed and maintained. A poorly defined problem can lead to inefficiencies or failures in later stages, emphasizing the need for a holistic perspective.

Feedback loops are central to effective problem definition. As the system evolves, real-world feedback from deployment and monitoring often reveals new constraints or requirements that necessitate revisiting the problem definition. For example, feedback from clinicians about system usability or patient outcomes may guide refinements in the original goals. In the DR project, the need for interpretable outputs that clinicians could trust and act upon influenced both model development and deployment strategies.

Emergent behaviors also play a role. A system that was initially designed to detect retinopathy might reveal additional use cases, such as identifying other conditions like diabetic macular edema, which can reshape the problem's scope and requirements. In the DR project, insights from deployment highlighted potential extensions to other imaging modalities, such as Optical Coherence Tomography (OCT).

Resource dependencies further highlight the interconnectedness of problem definition. Decisions about model complexity, for instance, directly affect infrastructure needs, data collection strategies, and deployment feasibility. Balancing these dependencies requires careful planning during the problem definition phase, ensuring that early decisions do not create bottlenecks in later stages.

### Lifecycle Implications {#sec-ai-workflow-lifecycle-implications-5cef}

The problem definition phase is foundational, influencing every subsequent stage of the lifecycle. A well-defined problem ensures that data collection focuses on the most relevant features, that models are developed with the right constraints in mind, and that deployment strategies align with operational realities.

In the DR project, defining the problem with scalability and adaptability in mind enabled the team to anticipate future challenges, such as accommodating new imaging devices or expanding to additional clinics. For instance, early considerations of diverse imaging conditions and patient demographics reduced the need for costly redesigns later in the lifecycle. This forward-thinking approach ensured the system's long-term success and adaptability in dynamic healthcare environments.

By embedding lifecycle thinking into problem definition, teams can create systems that not only meet initial requirements but also adapt and evolve in response to changing conditions. This ensures that ML systems remain effective, scalable, and impactful over time.

## Data Collection {#sec-ai-workflow-data-collection-ab2c}

Data is the foundation of machine learning systems, yet collecting and preparing data for ML applications introduces challenges that extend far beyond gathering enough training examples. Modern ML systems often need to handle terabytes of data, which range from raw, unstructured inputs to carefully annotated datasets, while maintaining quality, diversity, and relevance for model training. For medical systems like DR screening, data preparation must meet the highest standards to ensure diagnostic accuracy.

In the DR project, data collection involved a development dataset of 128,000 retinal fundus photographs evaluated by a panel of 54 ophthalmologists, with each image reviewed by 3-7 experts[^fn-medical-annotation]. This collaborative effort ensured high-quality labels that captured clinically relevant features like microaneurysms, hemorrhages, and hard exudates. Additionally, clinical validation datasets comprising 12,000 images provided an independent benchmark to test the model's robustness against real-world variability, illustrating the importance of rigorous and representative data collection. The scale and complexity of this effort highlight how domain expertise and interdisciplinary collaboration are critical to building datasets for high-stakes ML systems.

[^fn-medical-annotation]: **Medical Data Annotation Costs**: Expert medical annotation is extraordinarily expensive—ophthalmologists charge $200-500 per hour, meaning the DR dataset's annotation cost exceeded $2.7 million in expert time alone. This represents one of the highest annotation costs per sample in ML history. For comparison, ImageNet's 14 million images cost approximately $50,000 to annotate using crowdsourcing, while medical datasets can cost 100-1000x more per image. This cost disparity explains why medical AI often relies on transfer learning and why synthetic data generation is becoming crucial for healthcare applications.

### Data Requirements and Impact {#sec-ai-workflow-data-requirements-impact-6975}

The requirements for data collection and preparation emerge from the dual perspectives of machine learning and operational constraints. In the DR project, high-quality retinal images annotated by experts were a foundational need to train accurate models. However, real-world conditions quickly revealed additional complexities. Images were collected from rural clinics using different camera equipment, operated by staff with varying levels of expertise, and often under conditions of limited network connectivity.

These operational realities shaped the system architecture in significant ways. The volume and size of high-resolution images necessitated local storage and preprocessing capabilities at clinics, as centralizing all data collection was impractical due to unreliable internet access. Furthermore, patient privacy regulations required secure data handling at every stage, from image capture to model training[^fn-medical-privacy]. Coordinating expert annotations also introduced logistical challenges, necessitating systems that could bridge the physical distance between clinics and ophthalmologists while maintaining workflow efficiency.

[^fn-medical-privacy]: **Medical AI Privacy Complexity**: Healthcare data crosses jurisdictional boundaries with different privacy laws—HIPAA in the US, GDPR in Europe, and various national regulations elsewhere. Medical AI systems must implement techniques like differential privacy, federated learning, and homomorphic encryption, adding 40-60% to development costs. The "data cannot leave the country" requirements in many regions have led to the rise of federated learning architectures, where models travel to data rather than data traveling to models—a paradigm shift that Google's DR project helped establish in healthcare AI.

These considerations demonstrate how data collection requirements influence the entire ML lifecycle. Infrastructure design, annotation pipelines, and privacy protocols all play critical roles in ensuring that collected data aligns with both technical and operational goals.

### Data Infrastructure {#sec-ai-workflow-data-infrastructure-5088}

The flow of data through the system highlights critical infrastructure requirements at every stage. In the DR project, the journey of a single retinal image offers a glimpse into these complexities. From its capture on a retinal camera, where image quality is paramount, the data moves through local clinic systems for initial storage and preprocessing. Eventually, it must reach central systems where it is aggregated with data from other clinics for model training and validation.

At each step, the system must balance local needs with centralized aggregation requirements. Clinics with reliable high-speed internet could transmit data in real-time, but many rural locations relied on store-and-forward systems, where data was queued locally and transmitted in bulk when connectivity permitted. These differences necessitated flexible infrastructure that could adapt to varying conditions while maintaining data consistency and integrity across the lifecycle. This adaptability ensured that the system could function reliably despite the diverse operational environments of the clinics.

### Scale and Distribution {#sec-ai-workflow-scale-distribution-7fc8}

As ML systems scale, the challenges of data collection grow exponentially. In the DR project, scaling from an initial few clinics to a broader network introduced significant variability in equipment, workflows, and operating conditions. Each clinic effectively became an independent data node, yet the system needed to ensure consistent performance and reliability across all locations.

This scaling effort also brought increasing data volumes, as higher-resolution imaging devices became standard, generating larger and more detailed images. These advances amplified the demands on storage and processing infrastructure, requiring optimizations to maintain efficiency without compromising quality. Differences in patient demographics, clinic workflows, and connectivity patterns further underscored the need for robust design to handle these variations gracefully.

Scaling challenges highlight how decisions made during the data collection phase ripple through the lifecycle, impacting subsequent stages like model development, deployment, and monitoring. For instance, accommodating higher-resolution data during collection directly influences computational requirements for training and inference, emphasizing the need for lifecycle thinking even at this early stage.

### Data Validation {#sec-ai-workflow-data-validation-5359}

Quality assurance is an integral part of the data collection process, ensuring that data meets the requirements for downstream stages. In the DR project, automated checks at the point of collection flagged issues like poor focus or incorrect framing, allowing clinic staff to address problems immediately. These proactive measures ensured that low-quality data was not propagated through the pipeline.

Validation systems extended these efforts by verifying not just image quality but also proper labeling, patient association, and compliance with privacy regulations. Operating at both local and centralized levels, these systems ensured data reliability and robustness, safeguarding the integrity of the entire ML pipeline.

### Systems Thinking {#sec-ai-workflow-systems-thinking-0dca}

Viewing data collection and preparation through a lifecycle lens reveals the interconnected nature of these processes. Each decision made during this phase influences subsequent stages of the ML system. For instance, choices about camera equipment and image preprocessing affect not only the quality of the training dataset but also the computational requirements for model development and the accuracy of predictions during deployment.

@fig-ml-lifecycle-feedback illustrates the key feedback loops that characterize the ML lifecycle, with particular relevance to data collection and preparation. Looking at the left side of the diagram, we see how monitoring and maintenance activities feed back to both data collection and preparation stages. For example, when monitoring reveals data quality issues in production (shown by the "Data Quality Issues" feedback arrow), this triggers refinements in our data preparation pipelines. Similarly, performance insights from deployment might highlight gaps in our training data distribution (indicated by the "Performance Insights" loop back to data collection), prompting the collection of additional data to cover underrepresented cases. In the DR project, this manifested when monitoring revealed that certain demographic groups were underrepresented in the training data, leading to targeted data collection efforts to improve model fairness and accuracy across all populations.

::: {#fig-ml-lifecycle-feedback fig-env="figure" fig-pos="htb"}
```{.tikz}
\begin{tikzpicture}[font=\small\usefont{T1}{phv}{m}{n}]
\tikzset{%
  Line/.style={line width=1.0pt,black!50,text=black},
  Box/.style={align=flush center,
    inner xsep=2pt,
    node distance=1,
    draw=GreenLine,
    line width=0.75pt,
    fill=GreenL,
    text width=20mm,
    minimum width=20mm, minimum height=10mm
  },
 Text/.style={%
    inner sep=4pt,
    draw=none,
    line width=0.75pt,
    fill=TextColor!80,
    text=black,
    font=\footnotesize\usefont{T1}{phv}{m}{n},
    align=flush center,
    minimum width=7mm, minimum height=5mm
  },
}
%
\node[Box](B1){Data Preparation};
\node[Box,node distance=5,right=of B1](B2){Model Evaluation};
\node[Box,node distance=2.5, right=of B2](B3){Monitoring \& Maintenance};
\node[Box,below left=0.1 and 0.25 of B1](DB1){Data Collection};
\node[Box,above right=0.3 and 0.25 of B1](GB1){Model Training};
\node[Box,above right=0.3 and 0.25 of B2](GB2){Model Deployment};
%
\draw[Line,-latex](DB1)|-(B1);
\draw[Line,-latex](B1.60)|-(GB1);
\draw[Line,-latex](B2)|-node[Text,pos=0.7]{Data gaps}(DB1.10);
\draw[Line,-latex](B2)-|node[Text,pos=0.25]{Validation Issues}(GB1);
\draw[Line,-latex,](B3)|-node[Text,pos=0.6]{Performance Insights}(DB1.345);
\draw[Line,-latex](B2)-|(GB2);
\draw[Line,-latex](GB2)-|(B3.130);
\draw[Line,-latex](B3)--++(90:2.4)-|node[Text,pos=0.2]{Model Updates}(GB1);
\draw[Line,-latex](B3.50)--++(90:2.5)-|node[Text,pos=0.35]{Data Quality Issues}(B1.120);
\draw[Line,-latex](GB1.340)-|(B2);
\draw[Line,-latex](GB2.170)--node[Text,pos=0.5]{Deployment Constraints}(GB1.10);
\end{tikzpicture}
```
**ML Lifecycle Dependencies**: Iterative feedback loops connect data collection, preparation, model training, evaluation, and monitoring, emphasizing that each stage informs and influences subsequent stages in a continuous process. Effective machine learning system development requires acknowledging these dependencies to refine data, retrain models, and maintain performance over time.
:::

Feedback loops are another critical aspect of this lifecycle perspective. Insights from model performance often lead to adjustments in data collection strategies, creating an iterative improvement process. For example, in the DR project, patterns observed during model evaluation influenced updates to preprocessing pipelines, ensuring that new data aligned with the system's evolving requirements.

The scaling of data collection introduces emergent behaviors that must be managed holistically. While individual clinics may function well in isolation, the simultaneous operation of multiple clinics can lead to system-wide patterns like network congestion or storage bottlenecks. These behaviors reinforce the importance of considering data collection as a system-level challenge rather than a discrete, isolated task.

In the following chapters, we will step through each of the major stages of the lifecycle shown in @fig-ml-lifecycle-feedback. We will consider several key questions like what influences data source selection, how feedback loops can be systematically incorporated, and how emergent behaviors can be anticipated and managed holistically.

In addition, by adopting a systems thinking approach, we emphasize the iterative and interconnected nature of the ML lifecycle. How do choices in data collection and preparation ripple through the entire pipeline? What mechanisms ensure that monitoring insights and performance evaluations effectively inform improvements at earlier stages? And how can governance frameworks and infrastructure design evolve to meet the challenges of scaling while maintaining fairness and efficiency? These questions will guide our exploration of the lifecycle, offering a foundation for designing robust and adaptive ML systems.

### Lifecycle Implications {#sec-ai-workflow-lifecycle-implications-e952}

The success of ML systems depends on how effectively data collection integrates with the entire lifecycle. Decisions made in this stage affect not only the quality of the initial model but also the system's ability to evolve and adapt. For instance, data distribution shifts or changes in imaging equipment over time require the system to handle new inputs without compromising performance.

In the DR project, embedding lifecycle thinking into data management strategies ensured the system remained robust and scalable as it expanded to new clinics and regions. By proactively addressing variability and quality during data collection, the team minimized the need for costly downstream adjustments, aligning the system with long-term goals and operational realities.

## Model Development {#sec-ai-workflow-model-development-dfdc}

Model development and training form the core of machine learning systems, yet this stage presents unique challenges that extend far beyond selecting algorithms and tuning hyperparameters. It involves designing architectures suited to the problem, optimizing for computational efficiency, and iterating on models to balance performance with deployability. In high-stakes domains like healthcare, the stakes are particularly high, as every design decision impacts clinical outcomes.

For DR detection, the model needed to achieve expert-level accuracy while handling the high resolution and variability of retinal images. Using a machine learning model trained on their meticulously labeled dataset, the team achieved an F-score of 0.95, slightly exceeding the median score of the consulted ophthalmologists (0.91). This outcome highlights the effectiveness of advanced machine learning approaches[^fn-workflow-transfer-learning] and the importance of interdisciplinary collaboration between data scientists and medical experts to refine features and interpret model outputs.

[^fn-workflow-transfer-learning]: **Transfer Learning**: A technique where models pre-trained on large datasets (like ImageNet's 14 million images) are adapted for specific tasks, dramatically reducing training time and data requirements. Introduced by Yann LeCun's team in the 1990s and popularized by the 2014 ImageNet competition, transfer learning became the foundation for most practical computer vision applications. Instead of training from scratch, practitioners can achieve expert-level performance with thousands rather than millions of training examples.

### Model Requirements and Impact {#sec-ai-workflow-model-requirements-impact-6470}

The requirements for model development emerge not only from the specific learning task but also from broader system constraints. In the DR project, the model needed high sensitivity and specificity to detect different stages of retinopathy. However, achieving this purely from an ML perspective was not sufficient. The system had to meet operational constraints, including running on limited hardware in rural clinics, producing results quickly enough to fit into clinical workflows, and being interpretable enough for healthcare providers to trust its outputs.

These requirements shaped decisions during model development. While state-of-the-art accuracy might favor the largest and most complex models, such approaches were infeasible given hardware and workflow constraints. The team focused on designing architectures that balanced accuracy with efficiency, exploring lightweight models that could perform well on constrained devices. For example, model optimization techniques were employed to optimize the models for resource-limited environments, ensuring compatibility with rural clinic infrastructure.

This balancing act influenced every part of the system lifecycle. Decisions about model architecture affected data preprocessing, shaped the training infrastructure, and determined deployment strategies. For example, choosing to use multiple smaller models instead of a single large model altered data processing during training, required changes to inference pipelines, and introduced complexities in how model updates were managed in production.

### Development Workflow {#sec-ai-workflow-development-workflow-b547}

The model development workflow reflects the complex interplay between data, compute resources, and human expertise. In the DR project, this process began with data exploration and feature engineering, where data scientists collaborated with ophthalmologists to identify image characteristics indicative of retinopathy.

This initial stage required tools capable of handling large medical images and facilitating experimentation with preprocessing techniques. The team needed an environment that supported collaboration, visualization, and rapid iteration while managing the sheer scale of high-resolution data.

As the project advanced to model design and training, computational demands escalated. Training deep learning models on high-resolution images required extensive GPU resources and sophisticated infrastructure. The team implemented scalable training systems that could scale across multiple machines while managing large datasets, tracking experiments, and ensuring reproducibility. These systems also supported experiment comparison, enabling rapid evaluation of different architectures, hyperparameters, and preprocessing pipelines.

Model development was inherently iterative, with each cycle, involving adjustments to DNN architectures, refinements of hyperparameters, or incorporations of new data, producing extensive metadata, including checkpoints, validation results, and performance metrics. Managing this information across the team required robust tools for experiment tracking and version control to ensure that progress remained organized and reproducible.

### Scale and Distribution {#sec-ai-workflow-scale-distribution-56d9}

As ML systems scale in both data volume and model complexity, the challenges of model development grow exponentially. The DR project's evolution from prototype models to production-ready systems highlights these hurdles. Expanding datasets, more sophisticated models, and concurrent experiments demanded increasingly powerful computational resources and meticulous organization.

Large-scale training infrastructure became essential to meet these demands. While it significantly reduced training time, it introduced complexities in data synchronization, gradient aggregation, and fault tolerance. The team relied on advanced frameworks to optimize GPU clusters, manage network latency, and address hardware failures, ensuring training processes remained efficient and reliable. These frameworks included automated failure recovery mechanisms, which helped maintain progress even in the event of hardware interruptions.

The need for continuous experimentation and improvement compounded these challenges. Over time, the team managed an expanding repository of model versions, training datasets, and experimental results. This growth required scalable systems for tracking experiments, versioning models, and analyzing results to maintain consistency and focus across the project.

### Systems Thinking {#sec-ai-workflow-systems-thinking-bef9}

Approaching model development through a systems perspective reveals its connections to every other stage of the ML lifecycle. Decisions about model architecture ripple through the system, influencing preprocessing requirements, deployment strategies, and clinical workflows. For instance, adopting a complex model might improve accuracy but increase memory usage, complicating deployment in resource-constrained environments.

Feedback loops are inherent to this stage. Insights from deployment inform adjustments to models, while performance on test sets guides future data collection and annotation. Understanding these cycles is critical for iterative improvement and long-term success.

Scaling model development introduces emergent behaviors, such as bottlenecks in shared resources or unexpected interactions between multiple training experiments. Addressing these behaviors requires robust planning and the ability to anticipate system-wide patterns that might arise from local changes.

The boundaries between model development and other lifecycle stages often blur. Feature engineering overlaps with data preparation, while optimization for inference spans both development and deployment. Navigating these overlaps effectively requires careful coordination and clear interface definitions.

### Lifecycle Implications {#sec-ai-workflow-lifecycle-implications-3540}

Model development is not an isolated task; it exists within the broader ML lifecycle. Decisions made here influence data preparation strategies, training infrastructure, and deployment feasibility. The iterative nature of this stage ensures that insights gained feed back into data collection and system optimization, reinforcing the interconnectedness of the lifecycle.

In subsequent chapters, we will explore key questions that arise during model development:

- How can scalable training infrastructures be designed for large-scale ML models?

- What frameworks and tools help manage the complexity of distributed training?

- How can model reproducibility and version control be ensured in evolving projects?

- What trade-offs must be made to balance accuracy with operational constraints?

- How can continual learning and updates be handled in production systems?

These questions highlight how model development sits at the core of ML systems, with decisions in this stage resonating throughout the entire lifecycle.

## Deployment {#sec-ai-workflow-deployment-2839}

Once validated, the trained model is integrated into production systems and workflows. Deployment requires addressing practical challenges such as system compatibility, scalability, and operational constraints. Successful integration hinges on ensuring that the model's predictions are not only accurate but also actionable in real-world settings, where resource limitations and workflow disruptions can pose significant barriers.

In the DR project, deployment strategies were shaped by the diverse environments in which the system would operate. Edge deployment enabled local processing of retinal images in rural clinics with intermittent connectivity, while automated quality checks flagged poor-quality images for recapture, ensuring reliable predictions. These measures demonstrate how deployment must bridge technological sophistication with usability and scalability across varied clinical settings.

### Deployment Requirements and Impact {#sec-ai-workflow-deployment-requirements-impact-2ef2}

The requirements for deployment stem from both the technical specifications of the model and the operational constraints of its intended environment. In the DR project, the model needed to operate in rural clinics with limited computational resources and intermittent internet connectivity. Additionally, it had to fit seamlessly into the existing clinical workflow, which required rapid, interpretable results that could assist healthcare providers without causing disruption.

These requirements influenced deployment strategies significantly. A cloud-based deployment, while technically simpler, was not feasible due to unreliable connectivity in many clinics. Instead, the team opted for edge deployment, where models ran locally on clinic hardware. This approach required optimizing the model for smaller, less powerful devices while maintaining high accuracy. Model optimization techniques were employed to reduce resource demands without sacrificing performance.

Integration with existing systems posed additional challenges. The ML system had to interface with hospital information systems (HIS) for accessing patient records and storing results. Privacy regulations mandated secure data handling at every step, further shaping deployment decisions. These considerations ensured that the system adhered to clinical and legal standards while remaining practical for daily use.

### Deployment Workflow {#sec-ai-workflow-deployment-workflow-9bd5}

The deployment and integration workflow in the DR project highlighted the interplay between model functionality, infrastructure, and user experience. The process began with thorough testing in simulated environments that replicated the technical constraints and workflows of the target clinics. These simulations helped identify potential bottlenecks and incompatibilities early, allowing the team to refine the deployment strategy before full-scale rollout.

Once the deployment strategy was finalized, the team implemented a phased rollout. Initial deployments were limited to a few pilot sites, allowing for controlled testing in real-world conditions. This approach provided valuable feedback from clinicians and technical staff, helping to identify issues that hadn't surfaced during simulations.

Integration efforts focused on ensuring seamless interaction between the ML system and existing tools. For example, the DR system had to pull patient information from the HIS, process retinal images from connected cameras, and return results in a format that clinicians could easily interpret. These tasks required the development of robust APIs, real-time data processing pipelines, and user-friendly interfaces tailored to the needs of healthcare providers.

### Scale and Distribution {#sec-ai-workflow-scale-distribution-86c2}

Scaling deployment across multiple locations introduced new complexities. Each clinic had unique infrastructure, ranging from differences in imaging equipment to variations in network reliability. These differences necessitated flexible deployment strategies that could adapt to diverse environments while ensuring consistent performance.

Despite achieving high performance metrics during development, the DR system faced unexpected challenges in real-world deployment. For example, in rural clinics, variations in imaging equipment and operator expertise led to inconsistencies in image quality that the model struggled to handle. These issues underscored the gap between laboratory success and operational reliability, prompting iterative refinements in both the model and the deployment strategy. Feedback from clinicians further revealed that initial system interfaces were not intuitive enough for widespread adoption, leading to additional redesigns.

Distribution challenges extended beyond infrastructure variability. The team needed to maintain synchronized updates across all deployment sites to ensure that improvements in model performance or system features were universally applied. This required implementing centralized version control systems and automated update pipelines that minimized disruption to clinical operations.

Despite achieving high performance metrics during development, the DR system faced unexpected challenges in real-world deployment. As illustrated in @fig-ml-lifecycle-feedback, these challenges create multiple feedback paths---"Deployment Constraints" flowing back to model training to trigger optimizations, while "Performance Insights" from monitoring could necessitate new data collection. For example, when the system struggled with images from older camera models, this triggered both model optimizations and targeted data collection to improve performance under these conditions.

Another critical scaling challenge was training and supporting end-users. Clinicians and staff needed to understand how to operate the system, interpret its outputs, and provide feedback. The team developed comprehensive training programs and support channels to facilitate this transition, recognizing that user trust and proficiency were essential for system adoption.

### Robustness and Reliability {#sec-ai-workflow-robustness-reliability-cce1}

In a clinical context, reliability is paramount. The DR system needed to function seamlessly under a wide range of conditions, from high patient volumes to suboptimal imaging setups. To ensure robustness, the team implemented fail-safes that could detect and handle common issues, such as incomplete or poor-quality data. These mechanisms included automated image quality checks and fallback workflows for cases where the system encountered errors.

Testing played a central role in ensuring reliability. The team conducted extensive stress testing to simulate peak usage scenarios, validating that the system could handle high throughput without degradation in performance. Redundancy was built into critical components to minimize the risk of downtime, and all interactions with external systems, such as the HIS, were rigorously tested for compatibility and security.

### Systems Thinking {#sec-ai-workflow-systems-thinking-cebf}

Deployment and integration, viewed through a systems lens, reveal deep connections to every other stage of the ML lifecycle. Decisions made during model development influence deployment architecture, while choices about data handling affect integration strategies. Monitoring requirements often dictate how deployment pipelines are structured, ensuring compatibility with real-time feedback loops.

Feedback loops are integral to deployment and integration. Real-world usage generates valuable insights that inform future iterations of model development and evaluation. For example, clinician feedback on system usability during the DR project highlighted the need for clearer interfaces and more interpretable outputs, prompting targeted refinements in design and functionality.

Emergent behaviors frequently arise during deployment. In the DR project, early adoption revealed unexpected patterns, such as clinicians using the system for edge cases or non-critical diagnostics. These behaviors, which were not predicted during development, necessitated adjustments to both the system's operational focus and its training programs.

Deployment introduces significant resource dependencies. Running ML models on edge devices required balancing computational efficiency with accuracy, while ensuring other clinic operations were not disrupted. These trade-offs extended to the broader system, influencing everything from hardware requirements to scheduling updates without affecting clinical workflows.

The boundaries between deployment and other lifecycle stages are fluid. Optimization efforts for edge devices often overlapped with model development, while training programs for clinicians fed directly into monitoring and maintenance. Navigating these overlaps required clear communication and collaboration between teams, ensuring seamless integration and ongoing system adaptability.

By applying a systems perspective to deployment and integration, we can better anticipate challenges, design robust solutions, and maintain the flexibility needed to adapt to evolving operational and technical demands. This approach ensures that ML systems not only achieve initial success but remain effective and reliable in real-world applications.

### Lifecycle Implications {#sec-ai-workflow-lifecycle-implications-0431}

Deployment and integration are not terminal stages; they are the point at which an ML system becomes operationally active and starts generating real-world feedback. This feedback loops back into earlier stages, informing data collection strategies, model improvements, and evaluation protocols. By embedding lifecycle thinking into deployment, teams can design systems that are not only operationally effective but also adaptable and resilient to evolving needs.

In subsequent chapters, we will explore key questions related to deployment and integration:

- How can deployment strategies balance computational constraints with performance needs?

- What frameworks support scalable, synchronized deployments across diverse environments?

- How can systems be designed for seamless integration with existing workflows and tools?

- What are best practices for ensuring user trust and proficiency in operating ML systems?

- How do deployment insights feed back into the ML lifecycle to drive continuous improvement?

These questions emphasize the interconnected nature of deployment and integration within the lifecycle, highlighting the importance of aligning technical and operational priorities to create systems that deliver meaningful, lasting impact.

## Maintenance {#sec-ai-workflow-maintenance-e184}

Monitoring and maintenance represent the ongoing, critical processes that ensure the continued effectiveness and reliability of deployed machine learning systems. Unlike traditional software, ML systems must account for shifts in data distributions, changing usage patterns, and evolving operational requirements[^fn-model-drift]. Monitoring provides the feedback necessary to adapt to these challenges, while maintenance ensures the system evolves to meet new needs.

[^fn-model-drift]: **Model Drift Phenomenon**: ML models degrade over time without any code changes—a phenomenon unknown in traditional software. Studies show that 70% of production ML models experience significant performance degradation within 6 months due to data drift, concept drift, or infrastructure drift. This "silent failure" problem led to the development of specialized monitoring tools like Evidently AI (2020) and Fiddler (2018), creating an entirely new category of ML infrastructure that has no equivalent in traditional software engineering.

As shown in @fig-ml-lifecycle-feedback, monitoring serves as a central hub for system improvement, generating three critical feedback loops: "Performance Insights" flowing back to data collection to address gaps, "Data Quality Issues" triggering refinements in data preparation, and "Model Updates" initiating retraining when performance drifts. In the DR project, these feedback loops enabled continuous system improvement, from identifying underrepresented patient demographics (triggering new data collection) to detecting image quality issues (improving preprocessing) and addressing model drift (initiating retraining).

For DR screening, continuous monitoring tracked system performance across diverse clinics, detecting issues such as changing patient demographics or new imaging technologies that could impact accuracy. Proactive maintenance included plans to incorporate 3D imaging modalities like OCT, expanding the system's capabilities to diagnose a wider range of conditions. This highlights the importance of designing systems that can adapt to future challenges while maintaining compliance with rigorous healthcare regulations.

### Monitoring Requirements and Impact {#sec-ai-workflow-monitoring-requirements-impact-104b}

The requirements for monitoring and maintenance emerged from both technical needs and operational realities. In the DR project, the technical perspective required continuous tracking of model performance, data quality, and system resource usage. However, operational constraints added layers of complexity: monitoring systems had to align with clinical workflows, detect shifts in patient demographics, and provide actionable insights to both technical teams and healthcare providers.

Initial deployment highlighted several areas where the system failed to meet real-world needs, such as decreased accuracy in clinics with outdated equipment or lower-quality images. Monitoring systems detected performance drops in specific subgroups, such as patients with less common retinal conditions, demonstrating that even a well-trained model could face blind spots in practice[^fn-deployment-reality-gap]. These insights informed maintenance strategies, including targeted updates to address specific challenges and expanded training datasets to cover edge cases.

[^fn-deployment-reality-gap]: **The Lab-to-Clinic Performance Gap**: Medical AI systems typically see 10-30% performance drops when deployed in real-world settings, a phenomenon known as the "deployment reality gap." This occurs because training data, despite best efforts, cannot capture the full diversity of real-world conditions—different camera models, varying image quality, diverse patient populations, and operator skill levels all contribute to this gap. The gap is so consistent that regulatory bodies like the FDA now require "real-world performance studies" for medical AI approval, acknowledging that laboratory performance is insufficient to predict clinical utility.

These requirements influenced system design significantly. The critical nature of the DR system's function demanded real-time monitoring capabilities rather than periodic offline evaluations. To support this, the team implemented advanced logging and analytics pipelines to process large amounts of operational data from clinics without disrupting diagnostic workflows. Secure and efficient data handling was essential to transmit data across multiple clinics while preserving patient confidentiality.

Monitoring requirements also affected model design, as the team incorporated mechanisms for granular performance tracking and anomaly detection. Even the system's user interface was influenced, needing to present monitoring data in a clear, actionable manner for clinical and technical staff alike.

### Maintenance Workflow {#sec-ai-workflow-maintenance-workflow-ed45}

The monitoring and maintenance workflow in the DR project revealed the intricate interplay between automated systems, human expertise, and evolving healthcare practices. The process began with defining a comprehensive monitoring framework, establishing key performance indicators (KPIs), and implementing dashboards and alert systems. This framework had to balance depth of monitoring with system performance and privacy considerations, collecting sufficient data to detect issues without overburdening the system or violating patient confidentiality.

As the system matured, maintenance became an increasingly dynamic process. Model updates driven by new medical knowledge or performance improvements required careful validation and controlled rollouts. The team employed A/B testing frameworks to evaluate updates in real-world conditions and implemented rollback mechanisms to address issues quickly when they arose.

Monitoring and maintenance formed an iterative cycle rather than discrete phases. Insights from monitoring informed maintenance activities, while maintenance efforts often necessitated updates to monitoring strategies. The team developed workflows to transition seamlessly from issue detection to resolution, involving collaboration across technical and clinical domains.

### Scale and Distribution {#sec-ai-workflow-scale-distribution-0f2e}

As the DR project scaled from pilot sites to widespread deployment, monitoring and maintenance complexities grew exponentially. Each additional clinic added to the volume of operational data and introduced new environmental variables, such as differing hardware configurations or demographic patterns.

The need to monitor both global performance metrics and site-specific behaviors required sophisticated infrastructure. While global metrics provided an overview of system health, localized issues, including a hardware malfunction at a specific clinic or unexpected patterns in patient data, needed targeted monitoring. Advanced analytics systems processed data from all clinics to identify these localized anomalies while maintaining a system-wide perspective.

Continuous adaptation added further complexity. Real-world usage exposed the system to an ever-expanding range of scenarios. Capturing insights from these scenarios and using them to drive system updates required efficient mechanisms for integrating new data into training pipelines and deploying improved models without disrupting clinical workflows.

### Proactive Maintenance {#sec-ai-workflow-proactive-maintenance-2b10}

Reactive maintenance alone was insufficient for the DR project's dynamic operating environment. Proactive strategies became essential to anticipate and prevent issues before they affected clinical operations.

The team implemented predictive maintenance models to identify potential problems based on patterns in operational data. Continuous learning pipelines allowed the system to retrain and adapt based on new data, ensuring its relevance as clinical practices or patient demographics evolved. These capabilities required careful balancing to ensure safety and reliability while maintaining system performance.

Metrics assessing adaptability and resilience became as important as accuracy, reflecting the system's ability to evolve alongside its operating environment. Proactive maintenance ensured the system could handle future challenges without sacrificing reliability.

### Systems Thinking {#sec-ai-workflow-systems-thinking-b75b}

Monitoring and maintenance, viewed through a systems lens, reveal their deep integration with every other stage of the ML lifecycle. Changes in data collection affect model behavior, which influences monitoring thresholds. Maintenance actions can alter system availability or performance, impacting users and clinical workflows.

Feedback loops are central to these processes. Monitoring insights drive updates to models and workflows, while user feedback informs maintenance priorities. These loops ensure the system remains responsive to both technical and clinical needs.

Emergent behaviors often arise in distributed deployments. The DR team identified subtle system-wide shifts in diagnostic patterns that were invisible in individual clinics but evident in aggregated data. Managing these behaviors required sophisticated analytics and a holistic view of the system.

Resource dependencies also presented challenges. Real-time monitoring competed with diagnostic functions for computational resources, while maintenance activities required skilled personnel and occasional downtime. Effective resource planning was critical to balancing these demands.

### Lifecycle Implications {#sec-ai-workflow-lifecycle-implications-c19a}

Monitoring and maintenance are not isolated stages but integral parts of the ML lifecycle. Insights gained from these activities feed back into data collection, model development, and evaluation, ensuring the system evolves in response to real-world challenges. This lifecycle perspective emphasizes the need for strategies that not only address immediate concerns but also support long-term adaptability and improvement.

In subsequent chapters, we will explore critical questions related to monitoring and maintenance:

- How can monitoring systems detect subtle degradations in ML performance across diverse environments?

- What strategies support efficient maintenance of ML systems deployed at scale?

- How can continuous learning pipelines ensure relevance without compromising safety?

- What tools facilitate proactive maintenance and minimize disruption in production systems?

- How do monitoring and maintenance processes influence the design of future ML models?

These questions highlight the interconnected nature of monitoring and maintenance, where success depends on creating a framework that ensures both immediate reliability and long-term viability in complex, dynamic environments.

## AI Lifecycle Roles {#sec-ai-workflow-ai-lifecycle-roles-8d83}

Building effective and resilient machine learning systems is far more than a solo pursuit; it's a collaborative endeavor that thrives on the diverse expertise of a multidisciplinary team[^fn-ml-team-evolution]. Each role in this intricate dance brings unique skills and insights, supporting different phases of the AI development process. Understanding who these players are, what they contribute, and how they interconnect is crucial to navigating the complexities of modern AI systems.

[^fn-ml-team-evolution]: **ML Team Role Evolution**: The "data scientist" role only emerged around 2008 (coined by DJ Patil and Jeff Hammerbacher at Facebook and LinkedIn), while "ML engineer" became common around 2015 as companies realized that research models need production engineering. "MLOps engineer" appeared around 2018, and "AI ethics officer" became standard at major tech companies by 2020. This rapid role specialization reflects ML's evolution from research curiosity to production necessity—modern enterprise ML teams average 8-12 distinct roles compared to 2-3 in traditional software teams.

### Collaboration in AI {#sec-ai-workflow-collaboration-ai-2ccb}

At the heart of any AI project is a team of data scientists. These innovative thinkers focus on model creation, experiment with architectures, and refine the algorithms that will become the neural networks driving insights from data. In our DR project, data scientists were instrumental in architecting neural networks capable of identifying retinal anomalies, advancing through iterations to fine-tune a balance between accuracy and computational efficiency.

Behind the scenes, data engineers work tirelessly to design robust data pipelines, ensuring that vast amounts of data are ingested, transformed, and stored effectively. They play a crucial role in the DR project, handling data from various clinics and automating quality checks to guarantee that the training inputs were standardized and reliable.

Meanwhile, machine learning engineers take the baton to integrate these models into production settings. They guarantee that models are nimble, scalable, and fit the constraints of the deployment environment. In rural clinics where computational resources can be scarce, their work in optimizing models was pivotal to enabling on-the-spot diagnosis.

Domain experts, such as ophthalmologists in the DR project, infuse technical progress with practical relevance. Their insights shape early problem definitions and ensure that AI tools align closely with real-world needs, offering a measure of validation that keeps the outcome aligned with clinical and operational realities.

MLOps engineers are the guardians of workflow automation, orchestrating the continuous integration and monitoring systems that keep AI models up and running. They crafted centralized monitoring frameworks in the DR project, ensuring that updates were streamlined and model performance remained optimal across different deployment sites.

Ethicists and compliance officers remind us of the larger responsibility that accompanies AI deployment, ensuring adherence to ethical standards and legal requirements. Their oversight in the DR initiative safeguarded patient privacy amidst strict healthcare regulations.

Project managers weave together these diverse strands, orchestrating timelines, resources, and communication streams to maintain project momentum and alignment with objectives. They acted as linchpins within the project, harmonizing efforts between tech teams, clinical practitioners, and policy makers.

### Role Interplay {#sec-ai-workflow-role-interplay-7f0a}

The synergy between these roles fuels the AI machinery toward successful outcomes. Data engineers establish a solid foundation for data scientists' creative model-building endeavors. As models transition into real-world applications, ML engineers ensure compatibility and efficiency. Meanwhile, feedback loops between MLOps engineers and data scientists foster continuous improvement, enabling quick adaptation to data-driven discoveries.

Ultimately, the success of the DR project underscores the irreplaceable value of interdisciplinary collaboration. From bridging clinical insights with technical prowess to ensuring ethical deployment, this collective effort exemplifies how AI initiatives can be both technically successful and socially impactful.

This interconnected approach underlines why our exploration in later chapters will delve into various aspects of AI development, including those that may be seen as outside an individual's primary expertise. Understanding these diverse roles will equip us to build more robust, well-rounded AI solutions. By comprehending the broader context and the interplay of roles, you'll be better prepared to address challenges and collaborate effectively, paving the way for innovative and responsible AI systems.

## Fallacies and Pitfalls

**Fallacy:** _ML development can follow traditional software engineering workflows without modification._

This misconception leads teams to apply conventional software development practices directly to machine learning projects. Traditional software development assumes deterministic outputs from given inputs, enabling predictable testing and validation strategies. Machine learning introduces fundamental uncertainties through data variability, algorithmic randomness, and evolving model performance. Attempting to force ML projects into rigid waterfall or even standard agile methodologies often results in missed deadlines, inadequate model validation, and deployment failures. Successful ML workflows require specialized stages for data validation, experiment tracking, and iterative model refinement.

**Pitfall:** _Treating data preparation as a one-time preprocessing step._

Many practitioners view data collection and preprocessing as initial workflow stages that, once completed, remain static throughout the project lifecycle. This approach fails to account for the dynamic nature of real-world data, where distribution shifts, quality changes, and new data sources continuously emerge. Production systems require ongoing data validation, monitoring for drift, and adaptive preprocessing pipelines. Teams that treat data preparation as a completed milestone often encounter unexpected model degradation when deployed systems encounter data that differs from training conditions.

**Fallacy:** _Model performance in development environments accurately predicts production performance._

This belief assumes that achieving good metrics during development ensures successful deployment. Development environments typically use clean, well-curated datasets and controlled computational resources, creating artificial conditions that rarely match production realities. Production systems face data quality issues, latency constraints, resource limitations, and adversarial inputs not present during development. Models that excel in development can fail catastrophically in production due to these environmental differences, requiring workflow stages specifically designed to bridge this gap.

**Pitfall:** _Skipping systematic validation stages to accelerate development timelines._

Under pressure to deliver quickly, teams often bypass thorough validation, testing, and documentation stages. This approach treats validation as overhead rather than essential engineering discipline. Inadequate validation leads to models with hidden biases, poor generalization, or unexpected failure modes that only manifest in production. The cost of fixing these issues after deployment far exceeds the time investment required for systematic validation. Robust workflows embed validation throughout the development process rather than treating it as a final checkpoint.

## Summary {#sec-ai-workflow-summary-84ad}

AI development follows systematic workflows that provide structured methodologies for building machine learning systems across diverse application domains. These workflows encompass problem definition, data collection and preparation, model development and training, validation and testing, deployment, and ongoing maintenance phases. While specific implementations vary across domains from healthcare to finance to autonomous systems, the core workflow stages remain consistent, providing a universal framework for managing the complexity inherent in AI system development.

The interconnected nature of AI workflows creates complex feedback loops where decisions in one stage significantly impact all others. Data quality constraints influence model architecture choices, deployment environment limitations affect training strategies, and real-world performance metrics drive iterative refinement of the entire pipeline. This systems perspective recognizes that optimizing individual components in isolation often fails to achieve overall system objectives, requiring holistic approaches that consider the entire development lifecycle.

::: {.callout-important title="Key Takeaways"}
* AI workflows provide systematic frameworks applicable across diverse domains, from healthcare diagnostics to fraud detection to predictive maintenance
* Interconnected workflow stages create feedback loops where data quality, model performance, deployment constraints, and usage patterns mutually influence each other
* Systems thinking becomes essential for AI development success, requiring optimization of the entire pipeline rather than individual components
* Effective workflows balance technical performance with practical constraints including computational resources, deployment environments, and ethical considerations
:::

Modern AI development increasingly demands sophisticated workflow orchestration that addresses growing system complexity, ensures adaptability to changing requirements, and maintains alignment with ethical and regulatory considerations. Success requires understanding not just the technical aspects of machine learning, but also the organizational, operational, and societal contexts in which AI systems operate. This holistic approach enables the creation of AI solutions that are technically sound, operationally viable, and aligned with real-world needs across the expanding spectrum of AI applications.

[^fn-systems-thinking]: **Systems Thinking in AI**: The concept of "systems thinking" originated with biologist Ludwig von Bertalanffy in the 1940s and was later applied to engineering by Jay Forrester at MIT in the 1950s. Its application to AI became critical around 2018 when companies realized that optimizing individual ML models wasn't enough—success required optimizing the entire ML pipeline. This shift explains why "ML systems engineering" emerged as a distinct discipline, separate from both traditional software engineering and machine learning research, with its own conferences (MLSys, first held in 2020) and academic programs.
