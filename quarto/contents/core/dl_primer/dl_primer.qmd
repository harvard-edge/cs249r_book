---
bibliography: dl_primer.bib
quiz: dl_primer_quizzes.json
concepts: dl_primer_concepts.yml
glossary: dl_primer_glossary.json
crossrefs: dl_primer_xrefs.json
---

# DL Primer {#sec-dl-primer}

::: {layout-narrow}
::: {.column-margin}
_DALL·E 3 Prompt: A rectangular illustration divided into two halves on a clean white background. The left side features a detailed and colorful depiction of a biological neural network, showing interconnected neurons with glowing synapses and dendrites. The right side displays a sleek and modern artificial neural network, represented by a grid of interconnected nodes and edges resembling a digital circuit. The transition between the two sides is distinct but harmonious, with each half clearly illustrating its respective theme: biological on the left and artificial on the right._
:::

\noindent
![](images/png/cover_nn_primer.png)

:::

## Purpose {.unnumbered}

_Why do machine learning systems engineers need deep mathematical understanding of neural network fundamentals rather than treating them as black-box components?_

Modern machine learning systems rely on neural networks as their core computational engine, but successful systems engineering requires understanding the mathematical foundations that govern their behavior. Neural network mathematics determines memory requirements, computational complexity, convergence properties, and optimization landscapes that directly impact system design decisions. Without grasping concepts like gradient flow, activation functions, loss landscapes, and backpropagation mechanics, engineers cannot predict system behavior, diagnose training failures, optimize resource allocation, or design efficient architectures. Each mathematical operation translates to specific hardware requirements: a matrix multiplication may require gigabytes per second of memory bandwidth, while an activation function choice may determine whether a model can run on a mobile processor. Understanding these fundamentals transforms neural networks from opaque components into predictable, engineerable systems that can be systematically optimized for real-world constraints across the deployment spectrum from cloud to edge to tiny devices.

::: {.callout-tip title="Learning Objectives"}

* Understand the biological inspiration for artificial neural networks and how this foundation informs their design and function.

* Examine the structure of neural networks, including neurons, layers, and connections.

* Examine the processes of forward propagation, backward propagation, and optimization as the core mechanisms of learning.

* Understand the complete machine learning pipeline, from pre-processing through neural computation to post-processing.

* Compare and contrast training and inference phases, understanding their distinct computational requirements and optimizations.

* Learn how neural networks process data to extract patterns and make predictions, bridging theoretical concepts with computational implementations.

:::

## Overview {#sec-dl-primer-overview-9e60}

Neural networks are computational models inspired by biological neural systems, combining mathematics, computation, and system design to solve complex AI problems. These models fit within the broader hierarchy of AI and machine learning. @fig-ai-ml-dl provides a visual representation of this context. AI encompasses all computational methods that mimic human cognitive functions. Within AI, machine learning enables systems to learn patterns from data. Neural networks, a subset of ML, form the backbone of deep learning models by capturing complex relationships through interconnected computational units.

![**AI Hierarchy**: Neural networks form a core component of deep learning within machine learning and artificial intelligence by modeling patterns in large datasets. Machine learning algorithms enable systems to learn from data as a subset of the broader AI field.](images/png/ai_dl_progress_nvidia.png){#fig-ai-ml-dl}

The emergence of neural networks reflects three key shifts in how AI systems process information:

* **Data**: From manually structured and rule based datasets to raw, high dimensional data. Neural networks excel at learning from complex and unstructured data, enabling tasks involving images, speech, and text.

* **Algorithms**: From explicitly programmed rules to adaptive systems capable of learning patterns directly from data. Neural networks eliminate the need for manual feature engineering by discovering representations automatically through layers of interconnected units.

* **Computation**: From simple, sequential operations to massively parallel computations. The scalability of neural networks has driven demand for advanced hardware, such as GPUs[^fn-gpu-parallel], that can efficiently process large models and datasets.

[^fn-gpu-parallel]: **GPU (Graphics Processing Unit)**: Originally designed for rendering 3D graphics in 1999 by NVIDIA, GPUs excel at parallel computation with thousands of simple cores (compared to CPUs' 4-16 complex cores). A modern GPU like the NVIDIA A100 contains 6,912 CUDA cores and can perform 312 TFLOPS for FP16 Tensor operations—roughly 20× faster than CPUs for neural network training. This massive parallelism perfectly matches the matrix multiplication operations that dominate deep learning computations.

Understanding neural networks requires examining both their mathematical foundations and their practical implementation in real-world AI systems. Their development and deployment demand careful consideration of computational efficiency, data processing workflows, and hardware optimization. @sec-ml-systems provides an overview of how neural networks integrate into broader system architectures across cloud, edge, and embedded deployment scenarios. This chapter focuses on the core principles of neural networks, exploring their structure, functionality, and learning mechanisms. These basics prepare readers for @sec-dnn-architectures and their systems level implications.

## The Evolution to Deep Learning {#sec-dl-primer-evolution-deep-learning-fb02}

The current era of AI represents the latest stage in an evolution from rule-based programming through classical machine learning to modern neural networks. Understanding this progression reveals how each approach builds upon and addresses the limitations of its predecessors.

### Rule-Based Programming {#sec-dl-primer-rulebased-programming-16ec}

Traditional programming requires developers to explicitly define rules that tell computers how to process inputs and produce outputs. Consider a simple game like Breakout, shown in @fig-breakout. The program needs explicit rules for every interaction: when the ball hits a brick, the code must specify that the brick should be removed and the ball's direction should be reversed. While this approach functions effectively for games with clear physics and limited states, it demonstrates an inherent limitation of rule based systems.

::: {#fig-breakout fig-env="figure" fig-pos="htb"}
```{.tikz}
\scalebox{0.85}{%
\begin{tikzpicture}[line join=round,font=\small\usefont{T1}{phv}{m}{n}]
\definecolor{BlueGreen}{RGB}{20,188,188}
\definecolor{Cerulean}{RGB}{0,173,231}
\definecolor{Dandelion}{RGB}{255,185,76}
\definecolor{Goldenrod}{RGB}{255,219,87}
\definecolor{Lavender}{RGB}{253,160,204}
\definecolor{LimeGreen}{RGB}{136,201,70}
\definecolor{Maroon}{RGB}{186,49,50}
\definecolor{OrangeRed}{RGB}{255,46,88}
\definecolor{Peach}{RGB}{255,147,88}
\definecolor{Thistle}{RGB}{222,132,191}

\def\columns{5}
\def\rows{3}
\def\cellsize{25mm}
\def\cellheight{7mm}
\def\rowone{Peach,BlueGreen,OrangeRed,Thistle,Dandelion}
\def\rowtwo{brown!50,lime,teal,pink,lightgray}
\def\rowthree{Lavender,Goldenrod,Cerulean,Maroon,LimeGreen}
%
\foreach \x in {1,...,\columns}{
    \foreach \y in {1,...,\rows}{
        %
        \node[draw=black, fill=green!30, minimum width=\cellsize,
                    minimum height=\cellheight, line width=0.25pt] (cell-\x-\y) at (\x*\cellsize,-\y*\cellheight) {};
    }
}
\foreach \color [count=\x] in \rowone {
    \node[fill=\color,draw=black,line width=0.25pt, minimum size=\cellsize,
    minimum height=\cellheight] at (cell-\x-1) {};
}
%
\foreach \color [count=\x] in \rowtwo {
    \node[fill=\color,draw=black,line width=0.25pt, minimum size=\cellsize,
               minimum height=\cellheight] at (cell-\x-2) {};
}
%
\foreach \color [count=\x] in \rowthree {
    \node[fill=\color,draw=black,line width=0.25pt, minimum size=\cellsize,
               minimum height=\cellheight] at (cell-\x-3) {};
}
\begin{scope}[shift={($(cell-4-3)+(0,-1.7)$)}]
\node[align=left,font=\small\ttfamily]at(0,0){if (ball.collide(brick)) \{ \\
\qquad    removeBrick();\\
\qquad    ball.dx = 1.1 * (ball.dx);\\
\qquad    ball.dy = -1 * (ball.dy);\\
\}};
\end{scope}
\node[draw,rectangle,minimum width=40mm,minimum height=4mm,fill=Sepia!50!black!]
at($(cell-3-3.south west)+(0,-2.8)$)(R){};

\node[draw,circle,minimum size=5mm,fill=Sepia!50!black!,anchor=north]
at($(cell-1-3.south west)!0.8!(cell-1-3.south east)$)(C){};
\draw[thick,-latex,dash pattern={on 5pt off 2pt on 1pt off 3pt}](R)--(C)--++(225:2);
\end{tikzpicture}}
```
**Rule-Based System**: Traditional programming relies on explicitly defined rules to map inputs to outputs, limiting adaptability to complex or uncertain environments as every possible scenario must be anticipated and coded. This approach contrasts with machine learning, where systems learn patterns from data instead of relying on pre-programmed logic.
:::

Beyond individual applications, this rule based paradigm extends to all traditional programming, as illustrated in @fig-traditional. The program takes both rules for processing and input data to produce outputs. Early artificial intelligence research explored whether this approach could scale to solve complex problems by encoding sufficient rules to capture intelligent behavior.

::: {#fig-traditional fig-env="figure" fig-pos="htb"}
```{.tikz}
\resizebox{.65\textwidth}{!}{%
\begin{tikzpicture}[font=\small\usefont{T1}{phv}{m}{n}]
%
\tikzset{%
Line/.style={line width=1.0pt,black!50,text=black},
  Box/.style={inner xsep=2pt,
    node distance=1,
    draw=GreenLine, line width=0.75pt,
    fill=GreenL,
    text width=22mm,align=flush center,
    minimum width=22mm, minimum height=8mm
  },
  Box1/.style={Box, draw=RedLine, fill=RedL,
    text width=36mm, minimum width=40mm
  },
}
%
 \node[Box1](B1){Traditional Programming};
 \node[Box,right=of B1](B2){Answers};
 \node[Box,above left=0.2 and 1 of B1](B3){Rules};
 \node[Box, below left=0.2 and 1 of B1](B4){Data};
 \draw[-latex,Line](B1)--(B2);
 \draw[-latex,Line](B3)-|(B1);
 \draw[-latex,Line](B4)-|(B1);
\end{tikzpicture}}
```
**Rule-Based Programming**: Traditional programs operate on data using explicitly defined rules, forming the basis for early AI systems but lacking the adaptability of modern machine learning approaches. This paradigm contrasts with data-driven learning, where the system infers rules from examples rather than relying on pre-programmed logic.
:::

Despite their apparent simplicity, rule-based limitations become evident with complex real-world tasks. Recognizing human activities (@fig-activity-rules) illustrates this challenge: classifying movement below 4 mph as walking seems straightforward until real-world complexity emerges—speed variations, transitions between activities, and boundary cases each demand additional rules, creating unwieldy decision trees. Computer vision tasks compound these difficulties: detecting cats requires rules about ears, whiskers, and body shapes, while accounting for viewing angles, lighting, occlusions, and natural variations. Early systems achieved success only in controlled environments with well-defined constraints.

![**Rule-Based Programming**: Traditional programs rely on explicitly defined rules to operate on data, forming the basis for early AI systems but lacking adaptability in complex tasks.](images/png/activities.png){#fig-activity-rules}

Recognizing these limitations, the knowledge engineering approach that characterized artificial intelligence research in the 1970s and 1980s attempted to systematize rule creation. Expert systems[^fn-expert-systems] encoded domain knowledge as explicit rules, showing promise in specific domains with well defined parameters but struggling with tasks humans perform naturally, such as object recognition, speech understanding, or natural language interpretation. These limitations highlighted a significant challenge: many aspects of intelligent behavior rely on implicit knowledge that resists explicit rule based representation.

[^fn-expert-systems]: **Expert Systems**: Rule-based AI programs that encoded human domain expertise, prominent from 1970-1990. Notable examples include MYCIN (Stanford, 1976) for medical diagnosis, which outperformed human doctors in some antibiotics selection tasks, and XCON (DEC, 1980) for computer configuration, which saved the company $40 million annually. Despite early success, expert systems required extensive manual knowledge engineering—extracting and encoding rules from human experts—and struggled with uncertainty and common-sense reasoning that humans handle naturally.

### Classical Machine Learning {#sec-dl-primer-classical-machine-learning-0bc8}

Confronting the scalability barriers of rule based systems, researchers began exploring different approaches that could learn from data. Machine learning offered a promising direction: instead of writing rules for every situation, researchers could write programs that identified patterns in examples. However, the success of these methods still depended heavily on human insight to define relevant patterns, a process known as feature engineering.

This paradigm introduced feature engineering: transforming raw data into representations that expose patterns to learning algorithms. The Histogram of Oriented Gradients (HOG) [@dalal2005histograms][^fn-hog-method] method (@fig-hog) exemplifies this approach, identifying edges where brightness changes sharply, dividing images into cells, and measuring edge orientations within each cell. This transforms raw pixels into shape descriptors robust to lighting variations and small positional changes.

[^fn-hog-method]: **Histogram of Oriented Gradients (HOG)**: Developed by Navneet Dalal and Bill Triggs in 2005, HOG became the gold standard for object detection before deep learning. It achieved near-perfect accuracy on pedestrian detection—a breakthrough that enabled practical computer vision applications. HOG works by computing gradients (edge directions) in 8×8 pixel cells, then creating histograms of 9 orientation bins. This clever abstraction captures object shape while ignoring texture details, making it robust to lighting changes but requiring expert knowledge to design.

![**HOG Method**: Identifies edges in images to create a histogram of gradients, transforming pixel values into shape descriptors that are invariant to lighting changes.](images/png/hog.png){#fig-hog}

Complementary methods like SIFT [@lowe1999object][^fn-sift] (Scale-Invariant Feature Transform) and Gabor filters[^fn-gabor-filters] captured different visual patterns—SIFT detected keypoints stable across scale and orientation changes, while Gabor filters identified textures and frequencies. Each encoded domain expertise about visual pattern recognition.

[^fn-sift]: **Scale-Invariant Feature Transform (SIFT)**: Invented by David Lowe at University of British Columbia in 1999, SIFT revolutionized computer vision by detecting "keypoints" that remain stable across different viewpoints, scales, and lighting conditions. A typical image yields 1,000-2,000 SIFT keypoints, each described by a 128-dimensional vector. Before deep learning, SIFT was the backbone of applications like Google Street View's image matching and early smartphone augmented reality. The algorithm's 4-step process (scale-space extrema detection, keypoint localization, orientation assignment, and descriptor generation) required deep expertise to implement effectively.

[^fn-gabor-filters]: **Gabor Filters**: Named after Dennis Gabor (1971 Nobel Prize in Physics for holography), these mathematical filters detect edges and textures by analyzing frequency and orientation simultaneously. Used extensively in computer vision from 1980-2010, Gabor filters mimic how the human visual cortex processes images—different neurons respond to specific orientations and spatial frequencies. A typical Gabor filter bank contains 40+ filters (8 orientations × 5 frequencies) to capture texture patterns, making them ideal for applications like fingerprint recognition and fabric quality inspection before deep learning made manual filter design obsolete.

The culmination of these engineering efforts enabled significant advances in computer vision during the 2000s. Systems could now recognize objects with some robustness to real world variations, leading to applications in face detection, pedestrian detection, and object recognition. Despite these successes, the approach had inherent limitations. Experts needed to carefully design feature extractors for each new problem, and the resulting features might miss important patterns that were not anticipated in their design.

### Neural Networks and Representation Learning {#sec-dl-primer-neural-networks-representation-learning-0d1a}

Neural networks represent a shift in how we approach problem solving with computers, establishing a new programming paradigm that learns from data rather than following explicit rules. This shift becomes particularly evident when considering tasks like computer vision, specifically identifying objects in images.

::: {.callout-definition title="Definition of Deep Learning"}

Deep learning is a _subfield_ of machine learning that utilizes _artificial neural networks with multiple layers_ to _automatically learn hierarchical representations_ from data. This approach enables the extraction of _complex patterns_ from large datasets, facilitating tasks like _image recognition, natural language processing, and speech recognition_ without explicit feature engineering. Deep learning's effectiveness arises from its ability to _learn features directly_ from raw data, _adapt to diverse data structures_, and _scale with increasing data volume_.

:::

As defined in the formal definition above, deep learning's ability to automatically learn hierarchical representations eliminates the need for manual feature engineering while scaling effectively with data volume.

This capability distinguishes deep learning from previous approaches. Deep learning differs by learning directly from raw data. Traditional programming, as we saw earlier in @fig-traditional, required both rules and data as inputs to produce answers. Machine learning inverts this relationship, as shown in @fig-deeplearning. Instead of writing rules, we provide examples (data) and their correct answers to discover the underlying rules automatically. This shift eliminates the need for humans to specify what patterns are important.

::: {#fig-deeplearning fig-env="figure" fig-pos="htb"}
```{.tikz}
\resizebox{.65\textwidth}{!}{%
\begin{tikzpicture}[font=\small\usefont{T1}{phv}{m}{n}]
%
\tikzset{%
Line/.style={line width=1.0pt,black!50,text=black},
  Box/.style={inner xsep=2pt,
    node distance=1,
    draw=GreenLine, line width=0.75pt,
    fill=GreenL,
    text width=22mm,align=flush center,
    minimum width=22mm, minimum height=8mm
  },
  Box1/.style={Box,  draw=RedLine,
    fill=RedL, text width=36mm,
    minimum width=40mm
  },
}
%
 \node[Box1](B1){MachineLearning};
 \node[Box,right=of B1](B2){Rules};
 \node[Box,above left=0.2 and 1 of B1](B3){Answers};
 \node[Box, below left=0.2 and 1 of B1](B4){Data};
 \draw[-latex,Line](B1)--(B2);
 \draw[-latex,Line](B3)-|(B1);
 \draw[-latex,Line](B4)-|(B1);
\end{tikzpicture}}
```
**Data-Driven Rule Discovery**: Deep learning models learn patterns and relationships directly from data, eliminating the need for manually specified rules and enabling automated feature extraction from raw inputs. This contrasts with traditional programming, where both rules and data are required to generate outputs, and machine learning, where rules are inferred from labeled data.
:::

Through this automated approach, the system discovers these patterns automatically from examples. When shown millions of images of cats, the system learns to identify increasingly complex visual patterns, from simple edges to more sophisticated combinations that make up cat like features. This parallels how human visual systems operate, building understanding from basic visual elements to complex objects.

Building on this hierarchical learning principle, deep networks learn hierarchical representations where complex patterns emerge from simpler ones. Each layer learns increasingly abstract features: edges → shapes → objects → concepts. Deeper networks can express exponentially more functions with only polynomially more parameters, which is why "deep" matters theoretically. The compositionality principle explains why deep learning works: complex real-world patterns often have hierarchical structure that matches the network's representational bias.

This hierarchical structure creates a crucial advantage: unlike traditional approaches where performance often plateaus with more data and computation, deep learning models continue to improve as we provide more resources. More training examples help the system recognize more variations and nuances. More computational power enables the system to discover more subtle patterns. This scalability has led to dramatic improvements in performance. For example, the accuracy of image recognition systems has improved from 74% in 2012 to over 95% today[^fn-imagenet-progress].

[^fn-imagenet-progress]: **ImageNet Competition Progress**: The ImageNet Large Scale Visual Recognition Challenge (ILSVRC) tracked computer vision progress from 2010-2017. Error rates dropped dramatically: traditional methods achieved ~28% error in 2010, AlexNet [@krizhevsky2012imagenet] (first deep learning winner) achieved 15.3% in 2012, and ResNet [@he2016deep] achieved 3.6% in 2015—surpassing estimated human performance of 5.1%. This rapid improvement demonstrated deep learning's superiority over hand-crafted features, triggering the modern AI revolution. The competition ended in 2017 when further improvements became incremental.

Neural network performance follows predictable scaling relationships that directly impact system design. These scaling laws explain why modern AI systems prioritize larger models over longer training: GPT-4 has ~1000× more parameters than GPT-1 but uses similar training time. Memory bandwidth and storage capacity consequently become the primary constraints rather than raw computational power. The detailed mathematical formulations of these scaling laws and their quantitative analysis are covered in @sec-ai-training and @sec-model-optimizations.

Beyond performance improvements, this approach has profound implications for AI system construction. Deep learning's ability to learn directly from raw data eliminates the need for manual feature engineering while introducing new demands. Sophisticated infrastructure is required to handle massive datasets, powerful computers to process this data, and specialized hardware to perform complex mathematical calculations efficiently. The computational requirements of deep learning have driven the development of specialized computer chips optimized for these calculations.

The empirical evidence strongly supports these claims. The success of deep learning in computer vision exemplifies how this approach, when given sufficient data and computation, can surpass traditional methods. This pattern has repeated across many domains, from speech recognition to game playing, establishing deep learning as a transformative approach to artificial intelligence.

However, this transformation comes with significant trade-offs: deep learning's computational demands reshape system requirements fundamentally. Understanding these requirements provides essential context for the technical details of neural networks that follow.

### Neural System Implications {#sec-dl-primer-neural-system-implications-1ef5}

The progression from traditional programming to deep learning represents not just a shift in how we solve problems, but a transformation in computing system requirements that directly impacts every aspect of ML systems design. This transformation becomes critical when we consider the full spectrum of ML systems, from massive cloud deployments to resource constrained Tiny ML devices.

Traditional programs follow predictable patterns. They execute sequential instructions, access memory in regular patterns, and utilize computing resources in well understood ways. A typical rule based image processing system might scan through pixels methodically, applying fixed operations with modest and predictable computational and memory requirements. These characteristics made traditional programs relatively straightforward to deploy across different computing platforms.

+------------------+-------------------------+---------------------------+--------------------------------+
| System Aspect    | Traditional Programming | ML with Features          | Deep Learning                  |
+:=================+:========================+:==========================+:===============================+
| Computation      | Sequential,             | Structured parallel       | Massive matrix                 |
|                  | predictable paths       | operations                | parallelism                    |
+------------------+-------------------------+---------------------------+--------------------------------+
| Memory Access    | Small, predictable      | Medium,                   | Large, complex                 |
|                  | patterns                | batch-oriented            | hierarchical patterns          |
+------------------+-------------------------+---------------------------+--------------------------------+
| Data Movement    | Simple input/output     | Structured batch          | Intensive cross-system         |
|                  | flows                   | processing                | movement                       |
+------------------+-------------------------+---------------------------+--------------------------------+
| Hardware Needs   | CPU-centric             | CPU with vector           | Specialized                    |
|                  |                         | units                     | accelerators                   |
+------------------+-------------------------+---------------------------+--------------------------------+
| Resource Scaling | Fixed requirements      | Linear with data          | Exponential with               |
|                  |                         | size                      | complexity                     |
+------------------+-------------------------+---------------------------+--------------------------------+

: **System Resource Evolution**: Programming paradigms shift system demands from sequential computation to structured parallelism with feature engineering, and finally to massive matrix operations and complex memory hierarchies in deep learning. This table clarifies how deep learning fundamentally alters system requirements compared to traditional programming and machine learning with engineered features, impacting computation and memory access patterns. {#tbl-evolution}

As we moved toward data-driven approaches, machine learning with engineered features introduced new complexities. Feature extraction algorithms required more intensive computation and structured data movement. The HOG feature extractor discussed earlier, for instance, requires multiple passes over image data, computing gradients and constructing histograms. While this increased both computational demands and memory complexity, the resource requirements remained relatively predictable and scalable across platforms.

Deep learning, however, reshapes system requirements across multiple dimensions, as illustrated in @tbl-evolution. Understanding these evolutionary changes becomes crucial as differences manifest in several critical ways, with implications across the entire ML systems spectrum.

#### Computation Patterns {#sec-dl-primer-computation-patterns-0254}

The computational paradigm shift becomes immediately apparent when comparing these approaches. Traditional programs follow sequential logic flows. In stark contrast, deep learning requires massive parallel operations on matrices. This shift explains why conventional CPUs, designed for sequential processing, prove inefficient for neural network computations.

This parallel computational model creates new bottlenecks. Neural network parallelism is constrained by the memory wall: while compute scales linearly with parallel units, memory bandwidth scales as sqrt(area). Modern accelerators address this through hierarchical memory systems: L1 caches (1-10MB, 1000 GB/s), L2 caches (10-100MB, 500 GB/s), and specialized scratchpad memories that enable data reuse. TPU v4's 32MB on-chip memory provides 9TB/s bandwidth versus 1TB/s off-chip—a 9x advantage that enables efficient systolic array operation.

These different memory levels create distinct performance bottlenecks that determine neural network system design. Successful neural network accelerators maximize data reuse within higher levels of the memory hierarchy, often achieving 80%+ cache hit rates through careful dataflow scheduling. The detailed quantitative analysis of memory bandwidth and latency characteristics across different memory levels is covered in @sec-ai-acceleration.

Addressing these memory hierarchy challenges, the need for parallel processing has driven the adoption of specialized hardware architectures, ranging from powerful cloud GPUs to specialized mobile processors to Tiny ML accelerators. The specific hardware architectures and their trade-offs for ML workloads are explored in depth in @sec-ai-acceleration, which covers how different processors optimize matrix operations and memory access patterns crucial for neural network performance.

#### Memory Systems {#sec-dl-primer-memory-systems-5119}

The memory requirements present another significant shift. Traditional programs typically maintain small, fixed memory footprints. In contrast, deep learning models must manage parameters across complex memory hierarchies. Memory bandwidth often becomes the primary performance bottleneck, creating particular challenges for resource-constrained systems.

This memory-intensive nature creates performance bottlenecks that don't exist in traditional computing. Matrix multiplication, the core operation in neural networks, is memory bandwidth-bound rather than compute-bound. A typical GEMM operation achieves only 5-15% of peak FLOPS[^fn-dlprimer-flops] on CPUs due to data movement costs. Loading a single weight requires 4 bytes (FP32) but performs only one multiplication (6 FLOPS for multiply-accumulate), yielding 1.5 FLOPS/byte. Modern processors provide 10-1000 FLOPS/byte computational capability, creating a 10-1000x imbalance.

[^fn-dlprimer-flops]: **FLOPS**: Floating Point Operations Per Second measures computational throughput by counting mathematical operations like addition, subtraction, multiplication, and division of decimal numbers. Modern supercomputers achieve exascale performance (10^18 FLOPS), while neural network training requires petascale to exascale compute. For perspective, training GPT-3 [@brown2020language] required approximately 3.14 × 10^23 FLOPS—more computation than was available to the entire world before 1960.

GPUs achieve 40-60% efficiency through higher memory bandwidth (1TB/s vs 100GB/s for CPUs) and massive parallelism (thousands of cores vs 16-64 for CPUs). This bandwidth wall explains why AI accelerators focus on reducing data movement through techniques like systolic arrays and near-memory computing. Energy consumption in neural networks is dominated by data movement, not computation. Moving a 32-bit value from DRAM consumes ~640pJ, while a 32-bit multiplication consumes ~3.7pJ, a 173x difference. This energy hierarchy explains why mobile neural processors use techniques like quantization (8-bit operations consume 4x less energy than 32-bit), pruning (eliminating 90%+ of parameters), and specialized on-chip memory architectures to minimize DRAM access.

To illustrate these energy costs quantitatively: a typical smartphone neural processor like the Apple A17 Pro consumes approximately 2-4 watts during inference, while training the same model on a cloud GPU like NVIDIA H100 consumes 300-700 watts. For perspective, training a small CNN on CIFAR-10 for one epoch requires roughly 0.1 kWh on a V100 GPU, while running inference on 1 million images requires only 0.001 kWh on the same hardware. This 100:1 training-to-inference energy ratio drives the industry focus on efficient deployment architectures.

This drives different optimization strategies across the ML systems spectrum, ranging from memory-rich cloud deployments to heavily optimized Tiny ML implementations. These memory optimization strategies are detailed in @sec-model-optimizations, which covers techniques like quantization and pruning that reduce memory requirements while preserving model accuracy.

#### System Scaling {#sec-dl-primer-system-scaling-49fe}

Researchers discovered deep learning changes how systems scale and the importance of efficiency. Traditional programs have relatively fixed resource requirements with predictable performance characteristics. Deep learning models can consume exponentially more resources as they grow in complexity. This relationship between model capability and resource consumption makes system efficiency a concern. @sec-efficient-ai provides coverage of techniques to optimize this relationship, including methods to reduce computational requirements while maintaining model performance.

Bridging algorithmic concepts with hardware realities becomes essential. While traditional programs map relatively straightforwardly to standard computer architectures, deep learning requires careful consideration of:

* How to efficiently map matrix operations to physical hardware (@sec-ai-acceleration covers hardware-specific optimization strategies)
* Ways to minimize data movement across memory hierarchies
* Methods to balance computational capability with resource constraints (@sec-efficient-ai explores scaling laws and efficiency trade-offs)
* Techniques to optimize both algorithm and system-level efficiency (@sec-model-optimizations provides model compression techniques)

These shifts explain why deep learning has spurred innovations across the entire computing stack. From specialized hardware accelerators to new memory architectures to sophisticated software frameworks, the demands of deep learning continue to reshape computer system design.

Having established the systems landscape that deep learning operates within—from the evolution of programming paradigms to the computational demands that drive modern hardware design—we now turn to the foundational question: what are these neural networks actually computing? Understanding neural computation begins with examining the biological systems that inspired them. While the previous sections established why neural networks matter for systems engineering, the following sections reveal how they work, translating biological principles into mathematical operations and ultimately into the silicon implementations that power modern AI systems.

## Biological to Artificial Neurons {#sec-dl-primer-biological-artificial-neurons-bb4f}

Artificial intelligence development draws heavily from biological intelligence, particularly the human brain. The brain represents the most sophisticated information processing system known, capable of learning, adapting, and solving complex problems while maintaining remarkable energy efficiency. Understanding and replicating this energy efficiency in artificial systems is a key focus of @sec-sustainable-ai, which explores the environmental implications and optimization strategies for energy-efficient ML systems.

From a systems perspective, biological neural networks offer compelling solutions to the computational challenges we've just discussed: they achieve massive parallelism, efficient memory usage, and adaptive learning while consuming minimal energy. The way our brains function has provided insights that continue to shape both our algorithms and our system architectures.

### Biological Intelligence {#sec-dl-primer-biological-intelligence-7528}

When we observe biological intelligence, several key principles emerge. The brain demonstrates an extraordinary ability to learn from experience, constantly modifying its neural connections based on new information and interactions with the environment. This adaptability is key; every experience potentially alters the brain's structure and refines its responses for future situations. This biological capability directly inspired one of the core principles of machine learning: the ability to learn and improve from data rather than following fixed, pre-programmed rules.

Complementing this adaptive learning capability, another striking feature of biological intelligence is its parallel processing capability. The brain processes vast amounts of information simultaneously, with different regions specializing in specific functions while working in concert. This distributed, parallel architecture stands in stark contrast to traditional sequential computing and has significantly influenced modern AI system design. The brain's ability to efficiently coordinate these parallel processes while maintaining coherent function represents a level of sophistication we're still working to fully understand and replicate.

Beyond parallel processing, the brain's pattern recognition capabilities are particularly noteworthy. Biological systems excel at identifying patterns in complex, noisy data, whether it is recognizing faces in a crowd, understanding speech in a noisy environment, or identifying objects from partial information. This remarkable ability has inspired numerous AI applications, particularly in computer vision and speech recognition systems. The brain accomplishes these tasks with an efficiency that artificial systems are still striving to match.

Perhaps most remarkably, biological systems achieve all this sophisticated processing with incredible energy efficiency. As detailed in the System Requirements section, the human brain's 20-watt power consumption[^fn-brain-efficiency] creates a stark efficiency gap that artificial systems are still striving to bridge.

[^fn-brain-efficiency]: **Brain Energy Efficiency**: The human brain contains approximately 86-100 billion neurons and performs roughly 10^16 operations per second on just 20 watts—equivalent to running a single LED light bulb. Deep learning requires training GPT-3 [@brown2020language] consumed about 1,287 megawatt-hours of electricity [@strubell2019energy]. This stark efficiency gap drives research into neuromorphic computing and inspired the development of specialized AI chips designed to mimic brain-like processing.

These biological principles have led to two distinct but complementary approaches in artificial intelligence. The first attempts to directly mimic neural structure and function, leading to artificial neural networks and deep learning architectures that structurally resemble biological neural networks. The second takes a more abstract approach, adapting biological principles to work efficiently within the constraints of computer hardware without necessarily copying biological structures exactly. The following sections examine how these approaches manifest in practice, beginning with the basic building block of neural networks: the neuron itself.

### Transition to Artificial Neurons {#sec-dl-primer-transition-artificial-neurons-f44f}

Translating these high-level principles into practical implementation requires examining the basic unit of biological information processing: the neuron. This cellular building block provides the blueprint for its artificial counterpart and reveals how complex neural networks emerge from simple components working in concert.

In biological systems, the neuron (or cell) represents the basic functional unit of the nervous system. Understanding its structure is crucial for drawing parallels to artificial systems. @fig-bio_nn2ai_nn illustrates the structure of a biological neuron.

![**Biological Neuron Mapping**: Artificial neurons abstract key functions from their biological counterparts, receiving weighted inputs at dendrites, summing them in the cell body, and producing an output via the axon, analogous to activation functions in artificial neural networks. This abstraction enables the construction of complex artificial neural networks capable of sophisticated information processing. Source: geeksforgeeks.](images/png/bio_nn2ai_nn.png){#fig-bio_nn2ai_nn}

A biological neuron consists of several key components. The central part is the cell body, or soma, which contains the nucleus and performs the cell's basic life processes. Extending from the soma are branch-like structures called dendrites, which act as receivers for incoming signals from other neurons. The connections between neurons occur at synapses[^fn-synapses], which modulate the strength of the transmitted signals. Finally, a long, slender projection called the axon conducts electrical impulses away from the cell body to other neurons.

[^fn-synapses]: **Synapses**: From the Greek word "synaptein" meaning "to clasp together," synapses are the connection points between neurons where chemical or electrical signals are transmitted. A typical neuron has 1,000-10,000 synaptic connections, and the human brain contains roughly 100 trillion synapses. The strength of synaptic connections can change through experience, forming the biological basis of learning and memory—a principle directly mimicked by adjustable weights in artificial neural networks.

Integrating these structural components, the neuron functions as follows: Dendrites act as receivers, collecting input signals from other neurons. Synapses at these connections modulate the strength of each signal, determining how much influence each input has. The soma integrates these weighted signals and decides whether to trigger an output signal. If triggered, the axon transmits this signal to other neurons.

Each element of a biological neuron has a computational analog in artificial systems, reflecting the principles of learning, adaptability, and efficiency found in nature. To better understand how biological intelligence informs artificial systems, @tbl-bio_nn2ai_nn captures the mapping between the components of biological and artificial neurons. This should be viewed alongside @fig-bio_nn2ai_nn for a complete picture. Together, they show the biological-to-artificial neuron mapping.

+-----------------------+-----------------------+
| Biological Neuron     | Artificial Neuron     |
+:======================+:======================+
| Cell                  | Neuron / Node         |
+-----------------------+-----------------------+
| Dendrites             | Inputs                |
+-----------------------+-----------------------+
| Synapses              | Weights               |
+-----------------------+-----------------------+
| Soma                  | Net Input             |
+-----------------------+-----------------------+
| Axon                  | Output                |
+-----------------------+-----------------------+

: **Neuron Correspondence**: Biological neurons inspire artificial neuron design through analogous components—dendrites map to inputs (receiving signals), synapses map to weights (modulating connection strength), the soma to net input, and the axon to output—establishing a foundation for computational modeling of intelligence. This table clarifies how key functions of biological neurons are abstracted and implemented in artificial neural networks, enabling learning and information processing. {#tbl-bio_nn2ai_nn}

Understanding these correspondences proves crucial for grasping how artificial systems approximate biological intelligence. Each component serves a similar function through different mechanisms, with specific implications for artificial neural networks.

1. **Cell $\longleftrightarrow$ Neuron/Node**: The artificial neuron or node serves as the basic computational unit, mirroring the cell's role in biological systems.

2. **Dendrites $\longleftrightarrow$ Inputs**: Dendrites in biological neurons receive incoming signals from other neurons, analogous to how inputs feed into artificial neurons. They act as the signal receivers, like antennas collecting information.

3. **Synapses $\longleftrightarrow$ Weights**: Synapses modulate the strength of connections between neurons, directly analogous to weights in artificial neurons. These weights are adjustable, enabling learning and optimization over time by controlling how much influence each input has.

4. **Soma $\longleftrightarrow$ Net Input**:  The net input in artificial neurons sums weighted inputs to determine activation, similar to how the soma integrates signals in biological neurons.

5. **Axon $\longleftrightarrow$ Output**: The output of an artificial neuron passes processed information to subsequent network layers, much like an axon transmits signals to other neurons.

This mapping illustrates how artificial neural networks simplify and abstract biological processes while preserving their essential computational principles. Understanding individual neurons represents only the beginning. The true power of neural networks emerges from how these basic units work together in larger systems.

From a systems engineering perspective, this biological-to-artificial translation reveals why neural networks have such demanding computational requirements. Each simple biological process maps to intensive mathematical operations that must be executed millions or billions of times in parallel.

### Artificial Intelligence {#sec-dl-primer-artificial-intelligence-cfb8}

Bridging the gap from biological inspiration to practical implementation, the translation from biological principles to artificial computation requires a deep appreciation of what makes biological neural networks so effective at both the cellular and network levels, and why replicating these capabilities in silicon presents such significant systems challenges. The brain processes information through distributed computation across billions of neurons, each operating relatively slowly compared to silicon transistors. A biological neuron fires at approximately 200 Hz, while modern processors operate at gigahertz frequencies. Despite this speed limitation, the brain's parallel architecture enables sophisticated real-time processing of complex sensory input, decision-making, and control of behavior.

Despite the apparent speed disadvantage, this computational efficiency emerges from the brain's basic organizational principles. Each neuron acts as a simple processing unit, integrating inputs from thousands of other neurons and producing a binary output signal based on whether this integrated input exceeds a threshold. The connection strengths between neurons, mediated by synapses, are continuously modified through experience. This synaptic plasticity forms the basis for learning and adaptation in biological neural networks.

Replicating biological efficiency in artificial systems requires navigating fundamental trade-offs. While the brain operates with ~100 billion neurons using only 20 watts, a comparable artificial neural network would require orders of magnitude more power. Current large language models like GPT-4 consume ~1-10 MW during training (50,000-500,000× more power than the brain) and 1-10 kW during inference (50-500× more than the brain). This efficiency gap drives the engineering focus on specialized hardware, quantization techniques, and architectural innovations.

Drawing from these organizational insights, these biological principles suggest key computational elements needed in artificial neural systems:

* Simple processing units that integrate multiple inputs
* Adjustable connection strengths between units
* Nonlinear activation based on input thresholds
* Parallel processing architecture
* Learning through modification of connection strengths

### Computational Translation {#sec-dl-primer-computational-translation-1c53}

Translating biological insights into practical systems, we face the challenge of capturing the essence of neural computation within the rigid framework of digital systems. The implementation of biological principles in artificial neural systems represents a nuanced balance between biological fidelity and computational efficiency. At its core, an artificial neuron captures the essential computational properties of its biological counterpart through mathematical operations that can be efficiently executed on digital hardware.

@tbl-bio2comp provides a systematic view of how key biological features map to their computational counterparts. Each biological feature has an analog in computational systems, revealing both the possibilities and limitations of digital neural implementation, which we will discuss later.

Translating these biological principles into practical computational systems requires significant abstraction while preserving essential learning mechanisms. The basic computational unit in artificial neural networks, the artificial neuron, simplifies the complex electrochemical processes of biological neurons into three key operations. First, input signals are weighted, mimicking how biological synapses modulate incoming signals with different strengths. Second, these weighted inputs are summed together, analogous to how a biological neuron integrates incoming signals in its cell body. Finally, the summed input passes through an activation function that determines the neuron's output, similar to how a biological neuron fires based on whether its membrane potential exceeds a threshold.

+---------------------+---------------------------+
| Biological Feature  | Computational Translation |
+:====================+:==========================+
| Neuron firing       | Activation function       |
+---------------------+---------------------------+
| Synaptic strength   | Weighted connections      |
+---------------------+---------------------------+
| Signal integration  | Summation operation       |
+---------------------+---------------------------+
| Distributed memory  | Weight matrices           |
+---------------------+---------------------------+
| Parallel processing | Concurrent computation    |
+---------------------+---------------------------+

: **Biological-Computational Analogies**: Artificial neurons abstract key principles of biological neural systems, mapping neuron firing to activation functions, synaptic strength to weighted connections, and signal integration to summation operations—establishing a foundation for digital neural implementation. Distributed memory and parallel processing in biological systems find computational counterparts in weight matrices and concurrent computation, respectively, highlighting both the power and limitations of this abstraction. {#tbl-bio2comp}

This mathematical abstraction preserves key computational principles while enabling efficient digital implementation. The weighting of inputs allows the network to learn which connections are important, just as biological neural networks strengthen or weaken synaptic connections through experience. The summation operation captures how biological neurons integrate multiple inputs into a single decision. The activation function introduces nonlinearity essential for learning complex patterns, much like the threshold-based firing of biological neurons.

This abstraction has a computational cost. What happens effortlessly in biology requires intensive mathematical computation in artificial systems. As discussed in the Memory Systems section, these operations create significant computational demands due to memory bandwidth limitations.

Memory in artificial neural networks takes a markedly different form from biological systems. While biological memories are distributed across synaptic connections and neural patterns, artificial networks store information in discrete weights and parameters. This architectural difference reflects the constraints of current computing hardware, where memory and processing are physically separated rather than integrated as in biological systems. Despite these implementation differences, artificial neural networks achieve similar functional capabilities in pattern recognition and learning.

The brain's massive parallelism represents a challenge in artificial implementation. While biological neural networks process information through billions of neurons operating simultaneously, artificial systems approximate this parallelism through specialized hardware like GPUs and tensor processing units. These devices efficiently compute the matrix operations that form the mathematical foundation of artificial neural networks, achieving parallel processing at a different scale and granularity than biological systems.

### System Requirements {#sec-dl-primer-system-requirements-fdc2}

The computational translation of neural principles creates specific demands on the underlying computing infrastructure. These requirements emerge from the key differences between biological and artificial implementations of neural processing, shaping how we design and build systems capable of supporting artificial neural networks.

@tbl-comp2sys shows how each computational element drives particular system requirements. From this mapping, we can see how the choices made in computational translation directly influence the hardware and system architecture needed for implementation.

+-----------------------+---------------------------------+
| Computational Element | System Requirements             |
+:======================+:================================+
| Activation functions  | Fast nonlinear operation units  |
+-----------------------+---------------------------------+
| Weight operations     | High-bandwidth memory access    |
+-----------------------+---------------------------------+
| Parallel computation  | Specialized parallel processors |
+-----------------------+---------------------------------+
| Weight storage        | Large-scale memory systems      |
+-----------------------+---------------------------------+
| Learning algorithms   | Gradient computation hardware   |
+-----------------------+---------------------------------+

: **Computational Demands**: Artificial neural network design directly translates into specific system requirements; for example, efficient activation functions necessitate fast nonlinear operation units and large-scale weight storage demands high-bandwidth memory access. Understanding this mapping guides hardware and system architecture choices for effective implementation of artificial intelligence. {#tbl-comp2sys}

Storage architecture represents a critical requirement, driven by the key difference in how biological and artificial systems handle memory. In biological systems, memory and processing are intrinsically integrated—synapses both store connection strengths and process signals. Artificial systems, however, must maintain a clear separation between processing units and memory. This creates a need for both high-capacity storage to hold millions or billions of connection weights and high-bandwidth pathways to move this data quickly between storage and processing units. The efficiency of this data movement often becomes a critical bottleneck that biological systems do not face.

The learning process itself imposes distinct requirements on artificial systems. While biological networks modify synaptic strengths through local chemical processes, artificial networks must coordinate weight updates across the entire network. This creates substantial computational and memory demands during training, as systems must not only store current weights but also maintain space for gradients and intermediate calculations. The requirement to backpropagate error signals, with no real biological analog, further complicates the system architecture. Additionally, securing these large models and protecting sensitive training data introduces complex requirements addressed in @sec-security-privacy, which covers strategies for maintaining privacy and security in ML systems.

Energy efficiency emerges as a final critical requirement, highlighting perhaps the starkest contrast between biological and artificial implementations. The human brain's remarkable energy efficiency, which operates on approximately 20 watts, stands in sharp contrast to the substantial power demands of artificial neural networks. Current systems often require orders of magnitude more energy to implement similar capabilities. This gap drives ongoing research in more efficient hardware architectures and has profound implications for the practical deployment of neural networks, particularly in resource-constrained environments like mobile devices or edge computing systems. The environmental impact of this energy consumption and strategies for sustainable AI development are explored in @sec-sustainable-ai.

These system requirements directly drive the architectural choices we make in building ML systems, from the specialized hardware accelerators covered in @sec-ai-acceleration to the distributed training systems discussed in @sec-ai-training-distributed-systems-8fe8. Understanding why these requirements exist, rooted in the key differences between biological and artificial computation, is essential for making informed decisions about system design and optimization.

### Evolution and Impact {#sec-dl-primer-evolution-impact-ae30}

We can appreciate how the field of deep learning evolved to meet these challenges through advances in hardware and algorithms. This journey began with early artificial neural networks in the 1950s, marked by the introduction of the Perceptron [@rosenblatt1958perceptron][^fn-perceptron]. While groundbreaking in concept, these early systems were severely limited by the computational capabilities of their era, primarily mainframe computers that lacked both the processing power and memory capacity needed for complex networks.

[^fn-perceptron]: **Perceptron**: Invented by Frank Rosenblatt in 1957 at Cornell, the perceptron was the first artificial neural network capable of learning. The New York Times famously reported it would be "the embryo of an electronic computer that [the Navy] expects will be able to walk, talk, see, write, reproduce itself and be conscious of its existence." While overly optimistic, this breakthrough laid the foundation for all modern neural networks.

The development of backpropagation algorithms in the 1980s [@rumelhart1986learning], , was a theoretical breakthrough[^fn-dlprimer-backpropagation] and provided a systematic way to train multi-layer networks. The computational demands of this algorithm far exceeded available hardware capabilities. Training even modest networks could take weeks, making experimentation and practical applications challenging. This mismatch between algorithmic requirements and hardware capabilities contributed to a period of reduced interest in neural networks.

[^fn-dlprimer-backpropagation]: **Backpropagation**: Published by Rumelhart, Hinton, and Williams in 1986, backpropagation solved the "credit assignment problem"—how to determine which weights in a multi-layer network were responsible for errors. This algorithm, based on the mathematical chain rule, enabled training of deep networks and directly led to the modern AI revolution. Remarkably, a similar algorithm was discovered by Paul Werbos in 1974 but went largely unnoticed.

![**Computational Growth**: Exponential increases in computational power—initially at a 1.4× rate from 1952–2010, then accelerating to a doubling every 3.4 months from 2012–2022—enabled the scaling of deep learning models. this trend, coupled with a 10-month doubling cycle for large-scale models after 2015, directly addresses the historical bottleneck of training complex neural networks and fueled the recent advances in the field. Source: [@epochai2023trends].](images/png/trends_65d82031.png){#fig-trends fig-pos='htb'}

While we've established the technical foundations of deep learning in earlier sections, the term itself gained prominence in the 2010s, coinciding with significant advances in computational power and data accessibility. The field has grown exponentially, as illustrated in @fig-trends. The graph reveals two remarkable trends: computational capabilities measured in the number of Floating Point Operations per Second (FLOPS)[^fn-dlprimer-flops] initially followed a $1.4\times$ improvement pattern from 1952 to 2010, then accelerated to a 3.4-month doubling cycle from 2012 to 2022. Perhaps more striking is the emergence of large-scale models between 2015 and 2022 (not explicitly shown or easily seen in the figure), which scaled 2 to 3 orders of magnitude faster than the general trend, following an aggressive 10-month doubling cycle.

The evolutionary trends were driven by parallel advances across three dimensions: data availability, algorithmic innovations, and computing infrastructure. These three factors, namely, data, algorithms, and infrastructure, reinforced each other in a virtuous cycle that continues to drive progress in the field today. As @fig-virtuous-cycle shows, more powerful computing infrastructure enabled processing larger datasets. Larger datasets drove algorithmic innovations. Better algorithms demanded more sophisticated computing systems. This virtuous cycle continues to drive progress in the field today.

::: {#fig-virtuous-cycle fig-env="figure" fig-pos="htb"}
```{.tikz}
\resizebox{.7\textwidth}{!}{%
\begin{tikzpicture}[font=\small\usefont{T1}{phv}{m}{n}]
%
\tikzset{%
Line/.style={line width=1.0pt,black!50,text=black},
  Box/.style={inner xsep=2pt,
    node distance=1.2,
    draw=GreenLine, line width=0.75pt,
    fill=GreenL,text width=25mm,align=flush center,
    minimum width=25mm, minimum height=10mm
  }
}
%
\node[Box](B1){Data\\ Availability};
\node[Box,right=of B1,fill=BlueL,draw=BlueLine](B2){Algorithmic Innovations};
\node[Box, right=of B2,fill=RedL,draw=RedLine](B3){Computing Infrastructure};
\draw[Line,-latex](B1)--(B2);
\draw[Line,-latex](B2)--(B3);
\draw[Line,-latex](B3)--++(270:1)-|(B1);
%
\scoped[on background layer]
\node[draw=BackLine,inner xsep=9mm,inner ysep=8mm,yshift=0.5mm,
            fill=BackColor! 70,fit=(B1)(B3),line width=0.75pt](BB){};
\node[below=3pt of  BB.north east,anchor=north east]{Key Breakthroughs};
\end{tikzpicture}}
```
:::

The data revolution transformed what was possible with neural networks. The rise of the internet and digital devices created unprecedented access to training data. Image sharing platforms provided millions of labeled images. Digital text collections enabled language processing at scale. Sensor networks and IoT devices generated continuous streams of real-world data. This abundance of data provided the raw material needed for neural networks to learn complex patterns effectively.

Algorithmic innovations made it possible to use this data effectively. New methods for initializing networks and controlling learning rates made training more stable. Techniques for preventing overfitting[^fn-overfitting] allowed models to generalize better to new data. Researchers discovered researchers discovered that neural network performance scaled predictably with model size, computation, and data quantity, leading to increasingly ambitious architectures.

[^fn-overfitting]: **Overfitting**: When a model memorizes training examples instead of learning generalizable patterns—like a student who memorizes answers instead of understanding concepts. The model performs perfectly on training data but fails on new examples. Common signs include training accuracy continuing to improve while validation accuracy plateaus or decreases. Think of it as becoming an "expert" on a practice test who panics when facing slightly different questions on the real exam.

Computing infrastructure evolved to meet these growing demands. On the hardware side, graphics processing units (GPUs) provided the parallel processing capabilities needed for efficient neural network computation. Specialized AI accelerators like TPUs[^fn-dlprimer-tpu] [@jouppi2017datacenter] pushed performance further. High-bandwidth memory systems and fast interconnects addressed data movement challenges. Equally important were software advances—frameworks and libraries[^fn-dl-frameworks] that made it easier to build and train networks, distributed computing systems that enabled training at scale, and tools for optimizing model deployment.

[^fn-dlprimer-tpu]: **Tensor Processing Unit (TPU)**: Google's custom silicon designed specifically for tensor operations, the mathematical building blocks of neural networks. First deployed internally in 2015, TPUs can perform matrix multiplications up to 30× faster than 2015-era GPUs while using less power. The name reflects their optimization for tensor operations—multi-dimensional arrays that represent data flowing through neural networks. Google has since made TPUs available through cloud services, democratizing access to this specialized AI hardware.

[^fn-dl-frameworks]: **Deep Learning Frameworks**: TensorFlow [@abadi2016tensorflow] (released by Google in 2015) and PyTorch [@paszke2019pytorch] (released by Facebook in 2016) democratized deep learning by handling the complex mathematics automatically. Before these frameworks, implementing backpropagation required writing hundreds of lines of error-prone calculus code. Now, a complete neural network can be defined in 10-20 lines. TensorFlow emphasizes production deployment and has been downloaded over 180 million times, while PyTorch dominates research with its dynamic computation graphs. These frameworks automatically compute gradients, optimize GPU memory usage, and distribute training across multiple machines.

The convergence of data availability, algorithmic innovation, and computational infrastructure created the foundation for modern deep learning. Building effective ML systems requires understanding the computational operations that drive infrastructure requirements. Simple mathematical operations, when scaled across millions of parameters and billions of training examples, create the massive computational demands that shaped this evolution.

## Neural Network Fundamentals {#sec-dl-primer-neural-network-fundamentals-68cd}

Neural networks, from simple classifiers to large language models, share a common architectural foundation built upon basic computational units and principles. Each mathematical operation translates to specific computational and systems requirements, making these concepts essential for understanding ML systems. The latest developments in neural architectures and emerging paradigms that build upon these foundations are explored in @sec-agi-systems.

We begin by examining how individual artificial neurons process information, how they organize into layers, and how these layers connect to form complete networks. These core concepts provide the foundation for understanding more complex architectures and their applications.

Neural networks have evolved significantly since the perceptron's introduction in the 1950s. After declining in popularity due to computational and theoretical limitations, the field experienced a resurgence in the 2000s driven by hardware advances (particularly GPUs) and deep learning innovations. These breakthroughs enabled training networks with millions of parameters, making previously impossible applications feasible.

Before examining specific architectural components, it's crucial to understand how computational demands scale with model complexity, as this fundamentally shapes system design decisions. Deep learning's computational demands scale super-linearly with model size. Training GPT-3 (175B parameters) is estimated to have required ~3,640 petaflop-days and cost approximately $4.6M in compute. Rule of thumb: doubling model size increases training time by 4-8x and memory requirements by 2-4x. These scaling laws critically inform system design: a model that works on a laptop with 1M parameters may be completely infeasible at 100M parameters without distributed training infrastructure.

### Basic Architecture {#sec-dl-primer-basic-architecture-0553}

The architecture of a neural network determines how information flows through the system, from input to output. While modern networks can be tremendously complex, they all build upon a few key organizational principles that directly impact system design. Understanding these principles is necessary for both implementing neural networks and appreciating why they require the computational infrastructure we've discussed.

Driving practical system design, each architectural choice—from how neurons are connected to how layers are organized—creates specific computational patterns that must be efficiently mapped to hardware. This mapping between network architecture and computational requirements is crucial for building scalable ML systems.

#### Neurons and Activations {#sec-dl-primer-neurons-activations-622e}

At the heart of all neural architectures lies a basic building block. The Perceptron is the basic unit or node that serves as the building block for more complex structures. From a systems perspective, understanding the perceptron's mathematical operations is crucial because these simple operations, when replicated millions of times across a network, create the computational bottlenecks we discussed earlier. It functions by taking multiple inputs, each representing a feature of the object under analysis, such as the characteristics of a home for predicting its price or the attributes of a song to forecast its popularity in music streaming services. These inputs are denoted as $x_1, x_2, ..., x_n$.

This multiplication process reveals the computational complexity beneath apparently simple operations. From a computational standpoint, each input requires storage in memory and retrieval during processing. When multiplied across millions of neurons in a deep network, these memory access patterns become a primary performance bottleneck. This is why the memory hierarchy and bandwidth considerations we discussed earlier are so critical to neural network performance.

Understanding this weighted summation process, a perceptron can be configured to perform either regression or classification tasks. For regression, the actual numerical output $\hat{y}$ is used. For classification, the output depends on whether $\hat{y}$ crosses a certain threshold. If $\hat{y}$ exceeds this threshold, the perceptron might output one class (e.g., 'yes'), and if it does not, another class (e.g., 'no').

::: {#fig-perceptron fig-env="figure" fig-pos="htb"}
```{.tikz}
\scalebox{0.85}{%
\begin{tikzpicture}[font=\usefont{T1}{phv}{m}{n}]
\tikzset{%
  Line/.style={line width=1.0pt,black!50,text=black},
  Box/.style={inner xsep=3pt,
    node distance=0.2,circle,
    draw=GreenLine, line width=0.75pt,
    fill=GreenL,
    align=flush center,
    minimum width=8mm,
  },
}
%
\node[Box](B1){$w_{1j}$};
\node[Box,below=of B1](B2){$w_{2j}$};
\node[Box,below=of B2](B3){$w_{3j}$};
\node[Box,node distance=1.0,below=of B3](B4){$w_{ij}$};
\node[rotate=90,font=\tiny]at($(B3)!0.5!(B4)$){$\bullet$ $\bullet$ $\bullet$};
\foreach \x in{1,...,3}{
\draw[Line,latex-](B\x)--++(180:2)node[left](X\x){$x_\x$};
}
\node[above=0.1 of B1,font=\usefont{T1}{phv}{m}{n}\small](WE){Weights};
\path[](WE)-|coordinate(IN)(X1);
\node[font=\usefont{T1}{phv}{m}{n}\small]at(IN){Inputs};

\draw[Line,latex-](B4)--++(180:2)node[left](X4){$x_i$};
\node[rotate=90,font=\tiny]at($(X3)!0.5!(X4)$){$\bullet$ $\bullet$ $\bullet$};
\node[Box,minimum width=12mm,right=2of $(B1)!0.5!(B4)$,
            fill=RedL,draw=RedLine](B5){$\sum$};
\foreach \x in{1,...,4}{
\draw[Line,-latex](B\x)--(B5);
}
%
\node[Box,node distance=1.3,rectangle, right=of B5,fill=BlueL,
            draw=BlueLine, minimum width=11mm, minimum height=11mm,
            font=\usefont{T1}{phv}{m}{n}\huge](SI){$\sigma$};
\draw[Line,-latex](B5)--node[above]{$z$}(SI);
\draw[Line,latex-,font=\usefont{T1}{phv}{m}{n}\small](B5)--
           node[right]{$b$}++(270:1.75)node[below,]{Bias};
\draw[Line,-latex](SI)--++(0:1.75)node[right](OU){$\hat{y}$};
\node[above=0.4 of OU,font=\usefont{T1}{phv}{m}{n}\small]{Output};
\node[below=0.3 of SI,font=\usefont{T1}{phv}{m}{n}\small,align=center]{Activation\\ function};
\end{tikzpicture}}
```
**Weighted Input Summation**: Perceptrons compute a weighted sum of multiple inputs, representing feature values, and pass the result to an activation function to produce an output. each input $x_i$ is multiplied by a corresponding weight $w_{ij}$ before being aggregated, forming the basis for learning complex patterns from data. using this figure.
:::

Visualizing these mathematical concepts, @fig-perceptron illustrates the core building blocks of a perceptron, which serves as the foundation for more complex neural networks. A perceptron can be thought of as a miniature decision-maker, utilizing its weights, bias, and activation function to process inputs and generate outputs based on learned parameters. This concept forms the basis for understanding more intricate neural network architectures, such as multilayer perceptrons.

Scaling beyond individual units, in these advanced structures, layers of perceptrons work in concert, with each layer's output serving as the input for the subsequent layer. This hierarchical arrangement creates a deep learning model capable of comprehending and modeling complex, abstract patterns within data. By stacking these simple units, neural networks gain the ability to tackle increasingly sophisticated tasks, from image recognition to natural language processing.

Breaking down the computational mechanics, each input $x_i$ has a corresponding weight $w_{ij}$, and the perceptron simply multiplies each input by its matching weight. This operation is similar to linear regression, where the intermediate output, $z$, is computed as the sum of the products of inputs and their weights:
$$
z = \sum (x_i \cdot w_{ij})
$$

The apparent simplicity of this mathematical expression masks its computational complexity. From a systems perspective, this seemingly simple operation creates significant computational demands. Each multiply-accumulate operation requires retrieving both the input value and weight from memory, performing the multiplication, and accumulating the result. When scaled across millions of neurons and billions of parameters, these memory access patterns become the dominant performance bottleneck in neural network computation.

Enhancing the model's flexibility, to this intermediate calculation, a bias term $b$ is added, allowing the model to better fit the data by shifting the linear output function up or down. Thus, the intermediate linear combination computed by the perceptron including the bias becomes:
$$
z = \sum (x_i \cdot w_{ij}) + b
$$

This mathematical formulation directly drives the hardware requirements we discussed earlier. The summation requires accumulator units, the multiplications demand high-throughput arithmetic units, and the memory accesses necessitate high-bandwidth memory systems. Understanding this connection between mathematical operations and hardware requirements is crucial for designing efficient ML systems.

Beyond linear transformations, activation functions are critical nonlinear transformations that enable neural networks to learn complex patterns by converting linear weighted sums into nonlinear outputs. Without activation functions, multiple linear layers would collapse into a single linear transformation, severely limiting the network's expressive power. The choice of activation function has profound implications for both learning effectiveness and computational efficiency. Common activation functions include:

* **ReLU (Rectified Linear Unit)** [@nair2010rectified][^fn-relu-function]: Defined as $f(x) = \max(0,x)$, it introduces sparsity and accelerates convergence in deep networks. From a systems perspective, ReLU is computationally efficient, requiring only a simple comparison and conditional assignment rather than expensive exponential calculations. Its simplicity and effectiveness have made it the default choice in many modern architectures.

[^fn-relu-function]: **ReLU (Rectified Linear Unit)**: A piecewise linear activation function that outputs the input directly if positive, otherwise outputs zero. Introduced by Nair and Hinton in 2010, ReLU solved the vanishing gradient problem and became the default activation function in modern deep learning due to its computational simplicity and biological inspiration from neuron firing patterns.

* **Sigmoid**: Defined as $f(x) = \frac{1}{1 + e^{-x}}$, this function maps inputs to a range between 0 and 1 but is prone to vanishing gradients[^fn-vanishing] in deeper architectures. The exponential calculation makes sigmoid computationally expensive compared to ReLU, contributing to slower training and inference. It's particularly useful in binary classification problems where probabilities are needed.

[^fn-vanishing]: **Vanishing Gradients**: When gradients become exponentially small as they propagate backward through many layers, learning effectively stops in early layers. This problem is addressed in detail in @sec-ai-training.

* **Tanh**: Defined as $f(x) = \frac{e^x - e^{-x}}{e^x + e^{-x}}$ (or equivalently $\tanh(x)$), this function maps inputs to a range of $-1$ to 1, centering the data. Like sigmoid, tanh requires expensive exponential operations but often leads to faster convergence in practice due to its zero-centered output.

These computational differences explain why modern accelerators include specialized function units for common activations and why ReLU's simplicity contributed significantly to the practical success of deep learning.

::: {.callout-note title="Systems Perspective: Activation Functions and Hardware"}
**Why ReLU Dominates in Practice**: Beyond mathematical benefits like avoiding vanishing gradients, ReLU's hardware efficiency explains its widespread adoption. Computing $\max(0,x)$ requires a single comparison operation, while sigmoid and tanh require computing exponentials—operations that are orders of magnitude more expensive in both time and energy. This computational simplicity means ReLU can be executed faster on any processor and consumes significantly less power, a critical consideration for battery-powered devices. The systems principle: even simple mathematical choices cascade into real hardware constraints that determine where models can deploy.
:::

::: {#fig-nonlinear fig-env="figure" fig-pos="htb"}
```{.tikz}
\scalebox{0.75}{%
 \begin{tikzpicture}[line join=round,font=\small\usefont{T1}{phv}{m}{n}]
\definecolor{Cerulean}{RGB}{0,173,231}
\definecolor{LimeGreen}{RGB}{136,201,70}
\newcounter{point}
\tikzset{
  circ/.pic={
    \pgfkeys{/circ/.cd, #1}
%red
\foreach \x/\y in{0.4/0.77,0.39/1.46,0.39/2.02,0.37/2.52,0.37/2.95,0.47/3.35,
0.84/0.42,0.68/1.09,0.7/1.72,0.74/2.36,0.77/2.78,0.85/3.18,1.16/3.44,
1.37/0.36,1.14/0.82,1.08/1.47,1.02/1.97,1.45/2.10,1.16/2.39,1.26/2.9,1.56/3.3,
1.89/2.37,1.64/2.7,2.09/2.96,2.00/3.37,
2.57/2.33,3.08/2.2,3.42/2.42,3.25/3.06,2.96/2.75,2.48/2.73,2.71/3.13,
2.44/3.44,3.07/3.48
}{
 \stepcounter{point} % We increment the counter for each iteration
\fill[draw=none,fill=\bballcolor](\x,\y)circle[radius=5.5pt];
%\node[font=\tiny,blue] at (\x,\y) {C\arabic{point}}; % We add a label for each point
\coordinate(C\arabic{point})at(\x,\y);
}
%blue
\foreach \x/\y in {1.83/0.36,2.29/0.35,2.71/0.35,3.38/0.35,
3.4/0.8,3.39/1.35,3.41/1.90,3.07/1.62,2.59/1.82,2.19/1.98,
1.79/1.67,1.52/1.25,1.66/0.80,2.13/0.72,2.63/0.74,3.04/0.59,
3.02/0.99,2.72/1.28,2.29/1.48,1.95/1.15,2.36/1.07}
{
    \stepcounter{point} %We increment the counter for each iteration
\fill[draw=none,fill=\bballcolorr](\x,\y)circle[radius=5.5pt];
%\node[font=\tiny,red] at (\x,\y) {P\arabic{point}}; % We add a label for each point
\coordinate(P\arabic{point})at(\x,\y);
}  } }

\pgfkeys{
  /circ/.cd,
  bballcolor/.store in=\bballcolor,
  bballcolorr/.store in=\bballcolorr,
  bballcolor=red,      % derfault ball1 color
  bballcolorr=blue,      % derfault ball2 color
}
%LEFT
\begin{scope}[local bounding box=CIRC1,shift={(0,0)}]
\pic at (0,0) {circ={bballcolor=Cerulean,bballcolorr=LimeGreen}};
%fitting
\node[draw=black,inner xsep=4mm,inner ysep=3mm,
fill=none,fit=(C6)(P38)(C34),line width=1.75pt](BB1){};
\draw[red,line width=2pt]($(BB1.south west)!0.18!(BB1.south east)$)--
             ($(BB1.north east)!0.12!(BB1.south east)$);
\node[align=center,below=0.1 of BB1]{Neural Network without\\ an Activation Function};
\end{scope}
%RIGHT
\begin{scope}[local bounding box=CIRC2,shift={(6,0)}]
\pic at (0,0) {circ={bballcolor=Cerulean,bballcolorr=LimeGreen}};
%fitting
\node[draw=black,inner xsep=4mm,inner ysep=3mm,
yshift=0mm,fill=none,fit=(C61)(P93)(C89),line width=1.75pt](BB2){};
\draw[red,line width=2pt]($(BB2.south west)!0.4!(BB2.south east)$)
to[out=90,in=270]($(C69)!0.5!(P90)$)
to[out=90,in=280]($(C70)!0.5!(P102)$)
to[out=110,in=240]($(C71)!0.5!(P101)$)
to[out=70,in=220]($(C73)!0.5!(P100)$)
to[out=30,in=200]($(C77)!0.5!(P99)$)
to[out=30,in=160]($(C81)!0.5!(P98)$)
to[out=340,in=220]($(C82)!0.5!(P96)$)
to[out=60,in=210]($(C83)!0.5!(P96)$)
to($(BB2.north east)!0.4!(BB2.south east)$);
\node[align=center,below=0.1 of BB2]{Neural Network with\\ an Activation Function};
\end{scope}
\end{tikzpicture}}
```
**Non-Linear Activation**: Neural networks model complex relationships by applying non-linear activation functions to weighted sums of inputs, enabling the representation of non-linear decision boundaries. These functions transform input values, creating the capacity to learn intricate patterns beyond linear combinations via the arrangement of points. Source: Medium, sachin kaushik.
:::

As detailed in the activation function section above, these nonlinear transformations convert the linear input sum into a non-linear output:
$$
\hat{y} = \sigma(z)
$$

Thus, the final output of the perceptron, including the activation function, can be expressed as:

@fig-nonlinear shows an example where data exhibit a nonlinear pattern that could not be adequately modeled with a linear approach, demonstrating why the nonlinear activation functions discussed earlier are essential for complex pattern recognition.

The universal approximation theorem[^fn-universal-approximation] establishes that neural networks with activation functions can approximate arbitrary functions. This theoretical foundation, combined with the computational and optimization characteristics of specific activation functions like ReLU and sigmoid discussed above, explains neural networks' practical effectiveness in complex tasks.

[^fn-universal-approximation]: **Universal Approximation Theorem**: Proven by George Cybenko (1989) and Kurt Hornik (1991), this theorem states that neural networks with just one hidden layer containing enough neurons can approximate any continuous function to arbitrary accuracy. However, the theorem doesn't specify how many neurons are needed (could be exponentially many) or how to find the right weights. This explains why neural networks are theoretically powerful but doesn't guarantee practical learnability—a key distinction that drove the development of deep learning architectures and better training algorithms.

Combining the linear combination with the activation function, the complete perceptron computation is:
$$
\hat{y} = \sigma\left(\sum (x_i \cdot w_{ij}) + b\right)
$$

#### Layers and Connections {#sec-dl-primer-layers-connections-fa4f}

While a single perceptron can model simple decisions, the power of neural networks comes from combining multiple neurons into layers. A layer is a collection of neurons that process information in parallel. Each neuron in a layer operates independently on the same input but with its own set of weights and bias, allowing the layer to learn different features or patterns from the same input data.

In a typical neural network, we organize these layers hierarchically:

1. **Input Layer**: Receives the raw data features
2. **Hidden Layers**: Process and transform the data through multiple stages
3. **Output Layer**: Produces the final prediction or decision

@fig-layers illustrates this layered architecture. When data flows through these layers, each successive layer transforms the representation of the data, gradually building more complex and abstract features. This hierarchical processing is what gives deep neural networks their remarkable ability to learn complex patterns.

::: {#fig-layers fig-env="figure" fig-pos="htb"}
```{.tikz}
 \begin{tikzpicture}[line join=round,font=\usefont{T1}{phv}{m}{n}]
\definecolor{Cerulean}{RGB}{0,173,231}
\definecolor{Thistle}{RGB}{222,132,191}
\definecolor{Dandelion}{RGB}{255,185,76}
\tikzset{%
Line/.style={line width=0.35pt,black!60,-latex}
}

\tikzset{
  box/.pic={
    \pgfkeys{/box/.cd, #1}
\foreach \x in {1,...,\columns}{
    \foreach \y in {1,...,\rows}{
        %
        \node[draw=black, fill=\ffill, minimum width=\cellsize,
                    minimum height=\cellheight, line width=\linewidth] (cell-\x-\y\br) at (\x*\cellsize,-\y*\cellheight) {};
    }
}
 } }

\pgfkeys{
  /box/.cd,
  cellsize/.store in=\cellsize,
  linewidth/.store in=\linewidth,
  cellheight/.store in=\cellheight,
  columns/.store in=\columns,
  br/.store in=\br,
  ffill/.store in=\ffill,
  rows/.store in=\rows,
  columns=1,
  rows=3,
  br=A,
  ffill=red,
  cellsize=8mm,
  cellheight=8mm,
  linewidth=0.75pt
}

\pic at (0,0) {box={columns=1,rows=4,br=A,ffill=Thistle!30,linewidth=2.0pt}};
\pic at (3,24mm) {box={columns=1,rows=10,br=B,ffill=Dandelion!40}};
\pic at (6,16mm) {box={columns=1,rows=8,br=C,ffill=red!20}};
\pic at (9,32mm) {box={columns=1,rows=12,br=D,ffill=Cerulean!40}};
\pic at (13,16mm) {box={columns=1,rows=8,br=E,ffill=green!30}};
\pic at (16,-8mm) {box={columns=1,rows=2,br=F,ffill=Thistle!30,linewidth=2.0pt}};

\foreach \x in {1,...,4}{
    \foreach \y in {1,...,10}{
\draw[Line](cell-1-\x A.east)--(cell-1-\y B.west);
}}

\foreach \x in {1,...,10}{
    \foreach \y in {1,...,8}{
\draw[Line](cell-1-\x B.east)--(cell-1-\y C.west);
}}

\foreach \x in {1,...,8}{
    \foreach \y in {1,...,12}{
\draw[Line](cell-1-\x C.east)--(cell-1-\y D.west);
}}

\foreach \x in {1,...,8}{
    \foreach \y in {1,...,2}{
\draw[Line](cell-1-\x E.east)--(cell-1-\y F.west);
}}

\node[font=\huge]at($(cell-1-6D.south east)!0.5!(cell-1-4E.south west)$){$\bullet$ $\bullet$ $\bullet$};
\path[](cell-1-1B.north west)--++(90:1)coordinate(L)-|coordinate(D)(cell-1-1E.north east);
\draw[thick,decoration={brace,amplitude=11pt},decorate](L)--node[above=9pt](HL){Hidden layers}(D);
\path(HL)-|node[]{Input layer}(cell-1-1A);
\path(HL)-|node[]{Output layer}(cell-1-1F);

%\fill[red](cell-1-1B)circle(2pt);
\end{tikzpicture}
```
**Layered Network Architecture**: Deep neural networks transform data through successive layers, enabling the extraction of increasingly complex features and patterns. each layer applies non-linear transformations to the outputs of the previous layer, ultimately mapping raw inputs to desired outputs. Source: brunellon.
:::

{{< margin-video "https://youtu.be/aircAruvnKk?si=P7aT71L_uGT4xUz6" "Neural Network" "3Blue1Brown" >}}

#### Data Flow and Transformations {#sec-dl-primer-data-flow-transformations-5260}

As data flows through the network, it is transformed at each layer (l) to extract meaningful patterns. Each layer combines the input data using learned weights and biases, then applies an activation function to introduce non-linearity. This process can be written mathematically as:
$$
\mathbf{z}^{(l)} = \mathbf{W}^{(l)}\mathbf{x}^{(l-1)} + \mathbf{b}^{(l)}
$$
Where:

* $\mathbf{x}^{(l-1)}$ is the input vector from the previous layer

* $\mathbf{W}^{(l)}$ is the weight matrix for the current layer

* $\mathbf{b}^{(l)}$ is the bias vector

* $\mathbf{z}^{(l)}$ is the pre-activation output

Now that we have covered the basics, let's look at how these concepts come together in practice. Neural networks excel at tasks like handwritten digit recognition, where they learn to identify patterns in pixel data and classify images into different categories. This practical example introduces some new concepts that we will explore in more depth soon.

### Weights and Biases {#sec-dl-primer-weights-biases-71bc}

The learnable parameters of neural networks consist primarily of weights and biases, which together determine how information flows through the network and how transformations are applied to input data. This section examines how these parameters are organized and structured within neural networks. We explore weight matrices that connect layers, connection patterns that define network topology, bias terms that provide flexibility in transformations, and parameter organization strategies that enable efficient computation.

#### Weight Matrices {#sec-dl-primer-weight-matrices-709c}

Weights in neural networks determine how strongly inputs influence the output of a neuron. While we first discussed weights for a single perceptron, in larger networks, weights are organized into matrices for efficient computation across entire layers. For example, in a layer with $n$ input features and $m$ neurons, the weights form a matrix $\mathbf{W} \in \mathbb{R}^{n \times m}$. Each column in this matrix represents the weights for a single neuron in the layer. This organization allows the network to process multiple inputs simultaneously, an essential feature for handling real-world data efficiently.

Let's consider how this extends our previous perceptron equations to handle multiple neurons simultaneously. For a layer of $m$ neurons, instead of computing each neuron's output separately:
$$
z_j = \sum_{i=1}^n (x_i \cdot w_{ij}) + b_j
$$

We can compute all outputs at once using matrix multiplication:
$$
\mathbf{z} = \mathbf{x}^T\mathbf{W} + \mathbf{b}
$$

This matrix organization is more than just mathematical convenience; it reflects how modern neural networks are implemented for efficiency. Each weight $w_{ij}$ represents the strength of the connection between input feature $i$ and neuron $j$ in the layer.

#### Connection Patterns {#sec-dl-primer-connection-patterns-a389}

In the simplest and most common case, each neuron in a layer is connected to every neuron in the previous layer, forming what we call a "dense" or "fully-connected" layer. This pattern means that each neuron has the opportunity to learn from all available features from the previous layer.

@fig-connections illustrates these dense connections between layers. For a network with layers of sizes $(n_1, n_2, n_3)$, the weight matrices would have dimensions:

* Between first and second layer: $\mathbf{W}^{(1)} \in \mathbb{R}^{n_1 \times n_2}$
* Between second and third layer: $\mathbf{W}^{(2)} \in \mathbb{R}^{n_2 \times n_3}$

::: {#fig-connections fig-env="figure" fig-pos="htb"}
```{.tikz}
\begin{tikzpicture}[font=\small\usefont{T1}{phv}{m}{n}]
\definecolor{Blue}{RGB}{0,195,240}
  \tikzstyle{neuron}=[rectangle,draw=none,fill=blue!10,minimum size=10mm,inner sep=0pt]
  \tikzstyle{input neuron}=[neuron, fill=green!90!red!70];
  \tikzstyle{output neuron}=[neuron, fill=red!50];
  \tikzstyle{hidden neuron}=[neuron, fill=Blue,node distance=0.9];
  \tikzstyle{annot} = [sloped,text centered,text=black,midway,fill=white,inner sep=2pt,
                    font=\fontsize{7pt}{7}\selectfont\usefont{T1}{phv}{m}{n}]
  \tikzstyle{arrowR} = [line width=1.0pt,-latex,olive]
  \tikzstyle{arrowB} = [line width=1.0pt,-latex,RedLine]
  \tikzstyle{unutra} = [draw=yellow,regular polygon,line width=0.75pt, regular polygon sides=7,
                                     minimum size=10mm]
\tikzstyle{arrowC} = [line width=1.0pt,latex-,olive]
%
\node[hidden neuron] (H1) {.8337};
\node[hidden neuron,below=of H1] (H2) {.8764};
\node[hidden neuron,below=of H2] (H3) {.9087};
\node[hidden neuron,below=of H3] (H4) {.9329};
\node[input neuron,left=5 of $(H1)!0.25!(H2)$] (0H1) {1.0};
\node[input neuron,left=5 of $(H2)!0.5!(H3)$] (0H2) {5.0};
\node[input neuron,left=5 of $(H3)!0.75!(H4)$] (0H3) {9.0};

\node[output neuron,right=5 of $(H1)!0.5!(H2)$] (3H1) {.4886};
\node[output neuron,right=5 of $(H3)!0.5!(H4)$] (3H2) {.5114};
%
\draw[arrowR](0H1)--node[annot]{ihWeight\textsubscript{00} = 0.01} (H1);
\draw[arrowR](0H1)--node[annot,pos=0.15]{0.02}(H2);
\draw[arrowR](0H1)--node[annot,pos=0.15]{0.03}(H3);
\draw[arrowR](0H1)--node[annot,pos=0.1]{0.04}(H4);
%
\draw[arrowR](0H2)--node[annot,pos=0.12]{0.05}(H1);
\draw[arrowR](0H2)--node[annot,pos=0.15]{0.06}(H2);
\draw[arrowR](0H2)--node[annot,pos=0.15]{0.07}(H3);
\draw[arrowR](0H2)--node[annot,pos=0.12]{0.08}(H4);
%
\draw[arrowR](0H3)--node[annot,pos=0.12]{0.09}(H1);
\draw[arrowR](0H3)--node[annot,pos=0.12]{0.10}(H2);
\draw[arrowR](0H3)--node[annot,pos=0.12]{0.11}(H3);
\draw[arrowR](0H3)--node[annot,pos=0.5]{ihWeight\textsubscript{23} = 0.12}(H4);
%
\draw[arrowB](H1)--node[annot]{hoWeight\textsubscript{00} = 017} (3H1);
\draw[arrowB](H1)--node[annot,pos=0.12]{018} (3H2);
%
\draw[arrowB](H2)--node[annot,pos=0.12]{019} (3H1);
\draw[arrowB](H2)--node[annot,pos=0.12]{020} (3H2);
%
\draw[arrowB](H3)--node[annot,pos=0.12]{021} (3H1);
\draw[arrowB](H3)--node[annot,pos=0.12]{022} (3H2);
%
\draw[arrowB](H4)--node[annot,pos=0.12]{023} (3H1);
\draw[arrowB](H4)--node[annot,pos=0.5]{hoWeight\textsubscript{31} = 024} (3H2);
%%
\draw[arrowC](H1.150)--++(160:0.35)
   node[left,inner sep=1pt,text=black,
    font=\fontsize{7pt}{7}\selectfont\usefont{T1}{phv}{m}{n}]{hBias\textsubscript{0} = 0.13};
\draw[arrowC](H2.80)--++(130:0.35)
   node[above,inner sep=1pt,text=black,
    font=\fontsize{7pt}{7}\selectfont\usefont{T1}{phv}{m}{n}]{0.14};
\draw[arrowC](H3.80)--++(130:0.35)
    node[above,inner sep=1pt,text=black,
     font=\fontsize{7pt}{7}\selectfont\usefont{T1}{phv}{m}{n}]{0.15};
\draw[arrowC](H4.210)--++(200:0.35)
    node[left,inner sep=1pt,text=black,
     font=\fontsize{7pt}{7}\selectfont\usefont{T1}{phv}{m}{n}]{hBias\textsubscript{3} = 0.16};
%
\draw[arrowC,RedLine](3H1.70)--++(60:0.35)
              node[above,inner sep=1pt,text=black,
               font=\fontsize{7pt}{7}\selectfont\usefont{T1}{phv}{m}{n}]{oBias\textsubscript{0} = 0.25};
\draw[arrowC,RedLine](3H2.70)--++(60:0.35)
              node[above,inner sep=1pt,text=black,
               font=\fontsize{7pt}{7}\selectfont\usefont{T1}{phv}{m}{n}]{oBias\textsubscript{1} = 0.26};
%
\node[above=0.3 of H1,BlueLine](HL){Hidden layer};
\path[red](HL)-|coordinate(OL)(3H1);
\path[red](HL)-|coordinate(IL)(0H1);
\node[green!40!black!90]at(IL){Input layer};
\node[red]at(OL){Output layer};
\end{tikzpicture}
```
**Fully-Connected Layers**: Multilayer perceptrons (MLPs) utilize dense connections between layers, enabling each neuron to integrate information from all neurons in the preceding layer. The weight matrices defining these connections—$\mathbf{W}^{(1)} \in \mathbb{R}^{n_1 \times n_2}$ and $\mathbf{W}^{(2)} \in \mathbb{R}^{n_2 \times n_3}$—determine the strength of these integrations and facilitate learning complex patterns from input data. Source: J. McCaffrey.
:::

#### Bias Terms {#sec-dl-primer-bias-terms-367a}

Each neuron in a layer also has an associated bias term. While weights determine the relative importance of inputs, biases allow neurons to shift their activation functions. This shifting is crucial for learning, as it gives the network flexibility to fit more complex patterns.

For a layer with $m$ neurons, the bias terms form a vector $\mathbf{b} \in \mathbb{R}^m$. When we compute the layer's output, this bias vector is added to the weighted sum of inputs:
$$
\mathbf{z} = \mathbf{x}^T\mathbf{W} + \mathbf{b}
$$

The bias terms effectively allow each neuron to have a different "threshold" for activation, making the network more expressive.

#### Parameter Organization {#sec-dl-primer-parameter-organization-6d17}

The organization of weights and biases across a neural network follows a systematic pattern. For a network with $L$ layers, we maintain:

* A weight matrix $\mathbf{W}^{(l)}$ for each layer $l$

* A bias vector $\mathbf{b}^{(l)}$ for each layer $l$

* Activation functions $f^{(l)}$ for each layer $l$

This gives us the complete layer computation:
$$
\mathbf{h}^{(l)} = f^{(l)}(\mathbf{z}^{(l)}) = f^{(l)}(\mathbf{h}^{(l-1)T}\mathbf{W}^{(l)} + \mathbf{b}^{(l)})
$$
Where $\mathbf{h}^{(l)}$ represents the layer's output after applying the activation function.

### Network Topology {#sec-dl-primer-network-topology-11ff}

Network topology describes how individual neurons organize into layers and connect to form complete neural networks. Building intuition begins with a simple problem that became famous in AI history[^fn-xor-problem].

[^fn-xor-problem]: **XOR Problem**: The exclusive-or function became famous in AI history when Marvin Minsky and Seymour Papert proved in 1969 that single-layer perceptrons could never learn it, contributing to the "AI winter" of the 1970s. XOR requires non-linear decision boundaries—something impossible with linear models. The solution requires at least one hidden layer, demonstrating why "deep" networks (with hidden layers) are essential for learning complex patterns. This simple 2-input, 1-output problem helped establish the theoretical foundation for multi-layer neural networks.

::: {.callout-example title="Building Intuition: The XOR Problem"}
Consider a network learning the XOR function—a classic problem that requires non-linearity. With inputs $x_1$ and $x_2$ that can be 0 or 1, XOR outputs 1 when inputs differ and 0 when they're the same.

**Network Structure**: 2 inputs → 2 hidden neurons → 1 output

**Forward Pass Example**: For inputs $(1, 0)$:

- Hidden neuron 1: $h_1 = \text{ReLU}(1 \cdot w_{11} + 0 \cdot w_{12} + b_1)$
- Hidden neuron 2: $h_2 = \text{ReLU}(1 \cdot w_{21} + 0 \cdot w_{22} + b_2)$
- Output: $y = \text{sigmoid}(h_1 \cdot w_{31} + h_2 \cdot w_{32} + b_3)$

This simple network demonstrates how hidden layers enable learning non-linear patterns—something a single layer cannot achieve.
:::

The XOR example established the fundamental three-layer architecture, but real-world networks require systematic consideration of design constraints and computational scale. Recognizing handwritten digits using the MNIST [@lecun1998gradient][^fn-mnist-dataset] dataset illustrates how problem structure determines network dimensions while hidden layer configuration remains a critical design decision.

[^fn-mnist-dataset]: **MNIST Dataset**: Created by Yann LeCun and colleagues in 1998, MNIST (Modified National Institute of Standards and Technology) contains 70,000 images of handwritten digits—60,000 for training and 10,000 for testing. Each image is 28×28 pixels in grayscale, totaling 784 features per digit. MNIST became the "hello world" of computer vision, with error rates dropping from 12% with traditional methods in 1998 to 0.23% with modern deep learning. Despite being "solved," MNIST remains invaluable for teaching because it's large enough to be realistic yet small enough to train quickly on any computer.

#### Basic Structure {#sec-dl-primer-basic-structure-56bb}

Applying the three-layer architecture to MNIST reveals how data characteristics and task requirements constrain network design. As shown in @fig-mnist-topology-1$\text{a)}$, a $28\times 28$ pixel grayscale image of a handwritten digit must be processed through input, hidden, and output layers to produce a classification output.

The input layer's width is directly determined by our data format. As shown in @fig-mnist-topology-1$\text{b)}$, for a $28\times 28$ pixel image, each pixel becomes an input feature, requiring 784 input neurons $(28\times 28 = 784)$. We can think of this either as a 2D grid of pixels or as a flattened vector of 784 values, where each value represents the intensity of one pixel.

The output layer's structure is determined by our task requirements. For digit classification, we use 10 output neurons, one for each possible digit (0-9). When presented with an image, the network produces a value for each output neuron, where higher values indicate greater confidence that the image represents that particular digit.

Between these fixed input and output layers, we have flexibility in designing the hidden layer topology. The choice of hidden layer structure, including the number of layers to use and their respective widths, represents one of the key design decisions in neural networks. Additional layers increase the network's depth, allowing it to learn more abstract features through successive transformations. The width of each layer provides capacity for learning different features at each level of abstraction.

::: {#fig-mnist-topology-1 fig-env="figure" fig-pos="htb"}
```{.tikz}
 \begin{tikzpicture}[line join=round,font=\usefont{T1}{phv}{m}{n}]
 \tikzset{%
   mycycleR/.style={circle, draw=none, fill=Red, minimum width=8mm,node distance=0.4},
   LineA/.style={line width=2pt,violet!30,text=black,{Triangle[width=1.1*6pt,length=2.0*6pt]}-{Triangle[width=1.1*6pt,length=2.0*6pt]}},
   Line/.style={line width=0.5pt,BrownLine!50}
}
%circles sty
\tikzset{
circles/.pic={
\pgfkeys{/channel/.cd, #1}
\node[circle,draw=\channelcolor,line width=1pt,fill=\channelcolor!10,
minimum size=3mm](\picname){};
        }
}

\tikzset{
channel/.pic={
\pgfkeys{/channel/.cd, #1}
\begin{scope}[yscale=\scalefac,xscale=\scalefac,every node/.append style={scale=\scalefac}]
\node[rectangle,draw=\channelcolor,line width=1pt,fill=\channelcolor!10,
minimum width=46,minimum height=56](\picname){};
\end{scope}
        }
}

\pgfkeys{
  /channel/.cd,
  channelcolor/.store in=\channelcolor,
  scalefac/.store in=\scalefac,
  picname/.store in=\picname,
  channelcolor=BrownLine,
  scalefac=1,
  picname=C
}
  \def\data{
    % Ovde ide 28×28 = 784 vrednosti piksela (ovde samo primer sa 8×8)
       0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,
       0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,
       0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,
       0,0,0,0,0,0,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,0,0,0,0,0,0,0,
       0,0,0,0,0,0,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,0,0,0,0,0,0,0,
       0,0,0,0,0,0,255,255,0,0,0,0,0,0,0,0,0,0,255,255,255,255,0,0,0,0,0,0,0,
       0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,255,255,255,255,0,0,0,0,0,0,0,0,
       0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,255,255,255,255,0,0,0,0,0,0,0,0,0,
       0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,255,255,255,255,0,0,0,0,0,0,0,0,0,0,
       0,0,0,0,0,0,0,0,0,0,0,0,0,0,255,255,255,255,0,0,0,0,0,0,0,0,0,0,0,
       0,0,0,0,0,0,0,0,0,0,0,0,0,255,255,255,255,0,0,0,0,0,0,0,0,0,0,0,0,
       0,0,0,0,0,0,0,0,0,0,0,0,255,255,255,255,0,0,0,0,0,0,0,0,0,0,0,0,0,
       0,0,0,0,0,0,0,0,0,0,0,255,255,255,255,0,0,0,0,0,0,0,0,0,0,0,0,0,0,
       0,0,0,0,0,0,0,0,0,0,255,255,255,255,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,
       0,0,0,0,0,0,0,0,0,255,255,255,255,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,
       0,0,0,0,0,0,0,0,255,255,255,255,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,
       0,0,0,0,0,0,0,255,255,255,255,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,
       0,0,0,0,0,0,255,255,255,255,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,
       0,0,0,0,0,0,255,255,255,255,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,
       0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,
       0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
  }
%%%%
\begin{scope}[local bounding box=AFIG]
\begin{scope}[local bounding box=BIT,scale=7.5]
  \pgfmathsetmacro{\w}{29} % širina
  \pgfmathsetmacro{\h}{20} % visina
  \foreach \i [count=\n from 0] in \data {
    \pgfmathtruncatemacro{\x}{mod(\n,\w)}
    \pgfmathtruncatemacro{\y}{\h - 1 - int(\n/\w)}
    \pgfmathsetmacro{\percent}{100 - (\i / 255.0 * 100)} % skala u [0,100]
    %\fill[black!\percent!white] (\x,\y) rectangle ++(1,-1);
\def\px{0.01} % veličina jednog piksela
\def\py{0.013} % veličina jednog piksela

\fill[black!\percent!white] ({\x*\px},{\y*\py}) rectangle ++(\px,-\py)coordinate(P\n);
  }

\fill[green](P39)circle(0.1pt);
\end{scope}

\draw[LineA,shorten <=-3mm,shorten >=-3mm]([yshift=5mm]BIT.north west)--node[above]{28 px}([yshift=5mm]BIT.north east);
\draw[LineA,shorten <=-3mm,shorten >=-3mm]([xshift=-5mm]BIT.north west)--node[left]{28 px}([xshift=-5mm]BIT.south west);

\begin{scope}[local bounding box=CIRCLES1,shift={($(0,0)+(3.3,0.4)$)}]
\foreach \i in {1,...,20} {
  \pgfmathsetmacro{\y}{(11.5-\i)*0.5}
  \pic at (0,\y) {circles={channelcolor=VioletLine,picname=1CI\i,}};
}
\end{scope}

\begin{scope}[local bounding box=CIRCLES1,shift={($(0,0)+(6.5,0)$)}]
\foreach \i in {1,...,10} {
  \pgfmathsetmacro{\y}{(6.5-\i)*0.5}
  \pic at (0,\y) {circles={channelcolor=VioletLine,picname=2CI\i}};
}
\foreach \i in {1,...,7,9,10} {
 \node[right=1mm of 2CI\i]{0};
  \node[right=1mm of 2CI8]{1};
 }
\end{scope}

\foreach \i in {1,...,20} {
\foreach \j in {1,...,10} {
\draw[Line](1CI\i)--(2CI\j);
}
}
\end{scope}
\node[below=2mm of AFIG,font=\Large]{a)};
%%%%%%%%%%%
%RIGHT
%%%%%%%%%%%

\begin{scope}[local bounding box=BFIG,shift={($(AFIG)+(8.8,0)$)}]
 \begin{scope}[local bounding box=PIXG,shift={(0,5)}]
 \def\rows{18}
  \def\cols{0}
  \def\lastrows{18}  % number of rows in last column
  \def\xsize{0.55}
  \def\ysize{0.4}

  \foreach \j in {0,...,\cols} {
      \foreach \i in {0,...,\rows} {
        % Random blend: 30–60
        \pgfmathsetmacro\blend{rnd*99 + 0}
        %
          \draw[draw=black,fill=black!\blend!white] (\j*\xsize, -\i*\ysize) rectangle ++(\xsize, -\ysize);
    }
  }
 \coordinate (1topLeft) at (0, 0);
\pgfmathsetmacro\ycoord{-\rows*\ysize - \ysize}
\pgfmathsetmacro\xcoord{\cols*\xsize + \xsize}
\coordinate (1bottomLeft) at (0,{\ycoord});
\coordinate (1topRight) at ({\xcoord},0);
\coordinate (1bottomRight) at (\xcoord,\ycoord);
\end{scope}
 \begin{scope}[local bounding box=PIXD,shift={(0,-3.5)}]
 \def\rows{3}
  \def\cols{0}
  \def\lastrows{19}
  \def\xsize{0.55}
  \def\ysize{0.4}

  \foreach \j in {0,...,\cols} {
      \foreach \i in {0,...,\rows} {
        % Random blend: 0–99
        \pgfmathsetmacro\blend{rnd*99 + 0}
        %
          \draw[draw=black,fill=black!\blend!white] (\j*\xsize, -\i*\ysize) rectangle ++(\xsize, -\ysize);
    }
  }
\coordinate (2topLeft) at (0, 0);
\pgfmathsetmacro\ycoord{-\rows*\ysize - \ysize}
\pgfmathsetmacro\xcoord{\cols*\xsize + \xsize}
\coordinate (2bottomLeft) at (0,{\ycoord});
\coordinate (2topRight) at ({\xcoord},0);
\coordinate (2bottomRight) at (\xcoord,\ycoord);
\end{scope}

\draw[LineA,shorten <=-3mm,shorten >=-3mm]([yshift=3mm]1topLeft)--node[above]{}([yshift=3mm]1topRight);
\draw[LineA,shorten <=-3mm,shorten >=-3mm]([xshift=-5mm]1topLeft)--node[left]{784}([xshift=-5mm]2bottomLeft);
%%
\begin{scope}[local bounding box=CIRCLES2,shift={($(0,0)+(3.3,-0.55)$)}]
\foreach \i in {1,...,20} {
  \pgfmathsetmacro{\y}{(11.5-\i)*0.5}
  \pic at (0,\y) {circles={channelcolor=VioletLine,picname=1CI\i,}};
}
\end{scope}

\begin{scope}[local bounding box=CIRCLES22,shift={($(0,0)+(6.5,0)$)}]
\foreach \i in {1,...,10} {
  \pgfmathsetmacro{\y}{(6.5-\i)*0.5}
  \pic at (0,\y) {circles={channelcolor=VioletLine,picname=2CI\i}};
}
\foreach \i in {1,...,7,9,10} {
 \node[right=1mm of 2CI\i]{0};
  \node[right=1mm of 2CI8]{1};
 }
\end{scope}

\foreach \i in {1,...,20} {
\foreach \j in {1,...,10} {
\draw[Line](1CI\i)--(2CI\j);
}
}
\end{scope}
\node[below=0.6mm of BFIG,font=\Large]{b)};
\node[single arrow, draw=VioletLine, fill=VioletLine!50,
      minimum width = 10pt, single arrow head extend=3pt,
      minimum height=17mm]at([xshift=-15mm]CIRCLES2){};
\end{tikzpicture}
```
$\text{a)}$ A neural network topology for classifying MNIST digits, showing how a $28\times 28$ pixel image is processed. The image on the left shows the original digit, with dimensions labeled. The network on the right shows how each pixel connects to the hidden layers, ultimately producing 10 outputs for digit classification.
\smallskip\newline
$\text{b)}$ Alternative visualization of the MNIST network topology, showing how the 2D image is flattened into a 784-dimensional vector before being processed by the network. This representation emphasizes how spatial data is transformed into a format suitable for neural network processing.
:::

These basic topological choices have significant implications for both the network's capabilities and its computational requirements. Each additional layer or neuron increases the number of parameters that must be stored and computed during both training and inference. However, without sufficient depth or width, the network may lack the capacity to learn complex patterns in the data.

#### Design Trade-offs {#sec-dl-primer-design-tradeoffs-adee}

The design of neural network topology centers on three key decisions: the number of layers (depth), the size of each layer (width), and how these layers connect. Each choice affects both the network's learning capability and its computational requirements.

Network depth determines the level of abstraction the network can achieve. Each layer transforms its input into a new representation, and stacking multiple layers allows the network to build increasingly complex features. In our MNIST example, a deeper network might first learn to detect edges, then combine these edges into strokes, and finally assemble strokes into complete digit patterns. However, adding layers isn't always beneficial—deeper networks increase computational cost substantially, can be harder to train due to vanishing gradients, and may require more sophisticated training techniques.

The width of each layer, which is determined by the number of neurons it contains, controls how much information the network can process in parallel at each stage. Wider layers can learn more features simultaneously but require proportionally more parameters and computation. For instance, if a hidden layer is processing edge features in our digit recognition task, its width determines how many different edge patterns it can detect simultaneously.

A very important consideration in topology design is the total parameter count. For a network with layers of size $(n_1, n_2, \ldots, n_L)$, each pair of adjacent layers $l$ and $l+1$ requires $n_l \times n_{l+1}$ weight parameters, plus $n_{l+1}$ bias parameters. These parameters must be stored in memory and updated during training, making the parameter count a key constraint in practical applications.

Network design requires balancing learning capacity, computational efficiency, and training tractability. While the basic approach connects every neuron to every neuron in the next layer (fully connected), this does not always represent the most effective strategy. Using fewer but more strategic connections, as seen in specialized architectures, can achieve superior results with reduced computation. Consider the MNIST example: humans recognize digits by identifying meaningful patterns such as lines and curves rather than analyzing every pixel independently. Similarly, network architectures can focus on local patterns in images rather than treating each pixel as completely independent.

Information flow through the network represents another important consideration. While the basic flow proceeds from input to output, some network designs include additional paths such as skip connections or residual connections. These alternative paths facilitate training and improve effectiveness at learning complex patterns by functioning as shortcuts that enable more direct information flow when needed, analogous to how the human brain combines detailed and general impressions during object recognition.

These design decisions have significant practical implications including memory usage for storing network parameters, computational costs during both training and inference, training behavior and convergence, and the network's ability to generalize to new examples. The optimal balance of these trade-offs depends heavily on the specific problem, available computational resources, and dataset characteristics. Successful network design requires careful consideration of these factors against practical constraints.

::: {.callout-note title="Systems Perspective: Architecture Shapes Deployment Feasibility"}
**From Design to Deployment**: Every architectural decision—number of layers, layer widths, connection patterns—directly determines memory requirements and computational cost. A network with 1 million parameters requires roughly 4MB of memory just to store weights, before considering activations during inference. As models grow deeper and wider, their memory footprint and computational demands grow quadratically, not linearly. This mathematical relationship between architecture and resource requirements explains why the same architectural patterns cannot deploy uniformly across all platforms. Systems engineering insight emerges: architectural design must consider target deployment constraints from the outset, as post-hoc compression only partially recovers from architecture-resource mismatches.
:::

#### Connection Patterns {#sec-dl-primer-connection-patterns-c46f}

Neural networks can be structured with different connection patterns between layers, each offering distinct advantages for learning and computation. Understanding these patterns provides insight into how networks process information and learn representations from data.

Dense connectivity represents the standard pattern where each neuron connects to every neuron in the subsequent layer. In our MNIST example, connecting our 784-dimensional input layer to a hidden layer of 100 neurons requires 78,400 weight parameters. This full connectivity enables the network to learn arbitrary relationships between inputs and outputs, but the number of parameters scales quadratically with layer width.

Sparse connectivity patterns introduce purposeful restrictions in how neurons connect between layers. Rather than maintaining all possible connections, neurons connect to only a subset of neurons in the adjacent layer. This approach draws inspiration from biological neural systems, where neurons typically form connections with a limited number of other neurons. In visual processing tasks like our MNIST example, neurons might connect only to inputs representing nearby pixels, reflecting the local nature of visual features.

As networks grow deeper, the path from input to output becomes longer, potentially complicating the learning process. Skip connections address this by adding direct paths between non-adjacent layers. These connections provide alternative routes for information flow, supplementing the standard layer-by-layer progression. In our digit recognition example, skip connections might allow later layers to reference both high-level patterns and the original pixel values directly.

These connection patterns have significant implications for both the theoretical capabilities and practical implementation of neural networks. Dense connections maximize learning flexibility at the cost of computational efficiency. Sparse connections can reduce computational requirements while potentially improving the network's ability to learn structured patterns. Skip connections help maintain effective information flow in deeper networks.

#### Parameter Considerations {#sec-dl-primer-parameter-considerations-10f3}

The arrangement of parameters (weights and biases) in a neural network determines both its learning capacity and computational requirements. While topology defines the network's structure, the initialization and organization of parameters plays a crucial role in learning and performance.

Parameter count grows with network width and depth. For our MNIST example, consider a network with a 784-dimensional input layer, two hidden layers of 100 neurons each, and a 10-neuron output layer. The first layer requires 78,400 weights and 100 biases, the second layer 10,000 weights and 100 biases, and the output layer 1,000 weights and 10 biases, totaling 89,610 parameters. Each must be stored in memory and updated during learning.

Parameter initialization is critical to network behavior. Setting all parameters to zero would cause neurons in a layer to behave identically, preventing diverse feature learning. Instead, weights are typically initialized randomly, while biases often start at small constant values or even zeros. The scale of these initial values matters significantly, as values that are too large or too small can lead to poor learning dynamics.

The distribution of parameters affects information flow through layers. In digit recognition, if weights are too small, important input details might not propagate to later layers. If too large, the network might amplify noise. Biases help adjust the activation threshold of each neuron, enabling the network to learn optimal decision boundaries.

Different architectures may impose specific constraints on parameter organization. Some share weights across network regions to encode position-invariant pattern recognition. Others might restrict certain weights to zero, implementing sparse connectivity patterns.

## Learning Process {#sec-dl-primer-learning-process-38a0}

Neural networks learn to perform tasks through a process of training on examples. This process transforms the network from its initial state, where its weights are randomly initialized, to a trained state where the weights encode meaningful patterns from the training data. Understanding this process is essential to both the theoretical foundations and practical implementations of deep learning models.

### Training Overview {#sec-dl-primer-training-overview-1171}

Building on our architectural foundation, the core principle of neural network training is supervised learning from labeled examples. Consider our MNIST digit recognition task: we have a dataset of 60,000 training images, each a $28\times 28$ pixel grayscale image paired with its correct digit label. The network must learn the relationship between these images and their corresponding digits through an iterative process of prediction and weight adjustment. Ensuring the quality and integrity of training data is essential to model success, with data preparation strategies covered in @sec-data-engineering.

This relationship between inputs and outputs drives the training methodology. Training operates as a loop, where each iteration involves processing a subset of training examples called a batch[^fn-batch-processing]. For each batch, the network performs several key operations:

[^fn-batch-processing]: **Batch Processing**: Processing multiple examples simultaneously, typically 32-256 samples per batch. Larger batches provide more stable gradient estimates and better utilize parallel hardware but require more memory. The optimal batch size depends on available GPU memory and the specific model architecture.

* Forward computation through the network layers to generate predictions
* Evaluation of prediction accuracy using a loss function
* Computation of weight adjustments based on prediction errors
* Update of network weights to improve future predictions

Formalizing this iterative approach, this process can be expressed mathematically. Given an input image $x$ and its true label $y$, the network computes its prediction:
$$
\hat{y} = f(x; \theta)
$$
where $f$ represents the neural network function and $\theta$ represents all trainable parameters (weights and biases, which we discussed earlier). The network's error is measured by a loss function $L$:
$$
\text{loss} = L(\hat{y}, y)
$$

This quantification of prediction quality becomes the foundation for learning. This error measurement drives the adjustment of network parameters through a process called "backpropagation," which we will examine in detail later.

Scaling beyond individual examples, in practice, training operates on batches of examples rather than individual inputs. For the MNIST dataset, each training iteration might process, for example, 32, 64, or 128 images simultaneously. This batch processing serves two purposes: it enables efficient use of modern computing hardware through parallel processing, and it provides more stable parameter updates by averaging errors across multiple examples.

This batch-based approach creates both computational efficiency and training stability. The training cycle continues until the network achieves sufficient accuracy or reaches a predetermined number of iterations. Throughout this process, the loss function serves as a guide, with its minimization indicating improved network performance. Establishing proper metrics and evaluation protocols is crucial for assessing training effectiveness, as discussed in @sec-benchmarking-ai, which covers approaches to measuring ML system performance.

### Forward Propagation {#sec-dl-primer-forward-propagation-d412}

Forward propagation, as illustrated in @fig-forward-propagation, is the core computational process in a neural network, where input data flows through the network's layers to generate predictions. Understanding this process is essential as it underlies both network inference and training. Let's examine how forward propagation works using our MNIST digit recognition example.

::: {#fig-forward-propagation fig-env="figure" fig-pos="htb"}
```{.tikz}
\begin{tikzpicture}[font=\usefont{T1}{phv}{m}{n}\small]
\definecolor{Red}{RGB}{227,48,103}
\definecolor{Green}{RGB}{102,187,120}
\tikzset{%
  mycycleR/.style={circle, draw=none, fill=Red, minimum width=8mm,node distance=0.4},
  mycycleB/.style={circle, draw=none, fill=Green, minimum width=8mm,node distance=0.4},
  mycycleD/.style={circle, draw=none, fill=BlueLine, minimum width=8mm,node distance=0.4},
  mylineD/.style={line width=0.75pt,draw=black!70,dashed},
  myelipse/.style={ellipse,draw = brown,fill = cyan!20,minimum width = 20mm,
                             minimum height = 12mm,align=flush center,line width=0.75pt},
%
  Box/.style={
    inner xsep=2pt,
    draw=RedLine,
    line width=0.75pt,
    fill=RedL!20,
    align=flush center,
    text width=22mm,
    minimum width=22mm, minimum height=10mm
  },
%
Line/.style={line width=0.5pt,black!60,text=black},
Line2/.style={line width=0.85pt,black!60,text=black}
}

\begin{scope}[local bounding box = CIRC2]
\node[mycycleR] (2C1) {};
\node[mycycleR,below=of 2C1] (2C2) {};
\node[mycycleR,below=of 2C2] (2C3) {};
\node[mycycleR,below=of 2C3] (2C4) {};
%%
\node[mycycleD,left=1.6 of $(2C1)!0.5!(2C2)$] (1C1) {};
\node[mycycleD,left=1.6 of $(2C2)!0.5!(2C3)$] (1C2) {};
\node[mycycleD,left=1.6 of $(2C3)!0.5!(2C4)$] (1C3) {};
\foreach \x in {1,2,3} {
\draw[latex-,line width=0.75pt](1C\x)--++(180:1)node[left](X\x ){X\textsubscript{\x}};
}
\node[rotate=90,font=\Large\bfseries]at($(X2)!0.5!(X3)$){...};
\end{scope}
\begin{scope}[local bounding box = CIRC3,shift={(2.1,0)}]
\node[mycycleB] (3C1) {};
\node[mycycleB,below=of 3C1] (3C2) {};
\node[mycycleB,below=of 3C2] (3C3) {};
\node[mycycleB,below=of 3C3] (3C4) {};
\end{scope}
  \foreach \i in {1,2,3,4} {
        \foreach \j in {1,2,3,4} {
            \draw[Line,-latex] (2C\i) -- (3C\j);
        }
    }
      \foreach \i in {1,2,3} {
        \foreach \j in {1,2,3,4} {
            \draw[Line,-latex] (1C\i) --node(L\i\j){} (2C\j);
        }
    }
%
\node[mycycleD,fill=violet,right=1.5 of $(3C2)!0.5!(3C3)$] (4C1) {};
        \foreach \i in {1,2,3,4} {
            \draw[Line,-latex] (3C\i) --(4C1);
        }
\node[Box,right=1.5of 4C1](B1){Prediction\\ $(\hat{y})$};
\node[Box,right=of B1,fill=VioletL2,draw=VioletLine2](B2){True Value\\ $(y)$};
\node[myelipse,below=1.5 of $(B1)!0.5!(B2)$,fill=BlueL,draw=BlueLine,
            ](B3){Loss\\ Function L};
\node[Box,below left=0.5 and 0.25 of B3,fill=BrownL,draw=BrownLine](B4){Loss Score};
\node[myelipse,left=0.8 of B4,fill=BlueL,draw=BlueLine](B5){Optimizer};
\node[Box,left=2.3 of B5,fill=BrownL,draw=BrownLine](B6){Weights\\ \& bias};
%
\draw[Line2,-latex](4C1)--(B1);
\draw[Line2,-latex](B1)--(B3);
\draw[Line2,-latex](B2)--(B3);
\draw[Line2,-latex](B3)|-(B4);
\draw[Line2,-latex](B4)--(B5);
\draw[Line2,-latex](B5)--node[above]{Parameters}node[below]{update}(B6);
\draw[mylineD,-latex,shorten >=2pt](B6)--(L34.10);
\draw[mylineD,-latex,shorten >=2pt](B6)--(L33.180);
%%
\path[](1C1.west)--++(90:1.5)coordinate(A)-|coordinate(B)(4C1.east);
\path[](1C3.west)--++(270:3.5)coordinate(C)-|coordinate(D)(4C1.east);

\draw[RedLine, -{Triangle[width = 12pt, length = 5pt]}, line width = 5pt]
(A)--node[above,text=black]{Forward Propagation}(B);
\draw[OrangeLine, -{Triangle[width = 12pt, length = 5pt]}, line width = 5pt]
(D)--node[below,text=black]{Backward Propagation}(C);
\end{tikzpicture}
```
**Forward Propagation Process**: Neural networks transform input data into predictions by sequentially applying weighted sums and activation functions across interconnected layers, enabling complex pattern recognition. This layered computation forms the basis for both making inferences and updating model parameters during training.
:::

When an image of a handwritten digit enters our network, it undergoes a series of transformations through the layers. Each transformation combines the weighted inputs with learned patterns to progressively extract relevant features. In our MNIST example, a $28\times 28$ pixel image is processed through multiple layers to ultimately produce probabilities for each possible digit (0-9).

The process begins with the input layer, where each pixel's grayscale value becomes an input feature. For MNIST, this means 784 input values $(28\times 28 = 784)$, each normalized between 0 and 1. These values then propagate forward through the hidden layers, where each neuron combines its inputs according to its learned weights and applies a nonlinear activation function.

From a computational perspective, each forward pass through a typical MNIST network requires substantial matrix operations. Consider a network with layers of sizes 784 → 512 → 256 → 10: the first layer alone performs 784 × 512 = 401,408 multiply-accumulate operations per sample. With a batch size of 32, this becomes ~12.8 million operations, requiring approximately 51MB of memory bandwidth (assuming FP32 weights) and consuming roughly 0.15 milliseconds on a modern GPU.

#### Layer Computation {#sec-dl-primer-layer-computation-5636}

The forward computation through a neural network proceeds systematically, with each layer transforming its inputs into increasingly abstract representations. In our MNIST network, this transformation process occurs in distinct stages.

At each layer, the computation involves two key steps: a linear transformation of inputs followed by a nonlinear activation. The linear transformation combines all inputs to a neuron using learned weights and a bias term. For a single neuron receiving inputs from the previous layer, this computation takes the form:
$$
z = \sum_{i=1}^n w_ix_i + b
$$
where $w_i$ represents the weights, $x_i$ the inputs, and $b$ the bias term. For an entire layer of neurons, we can express this more efficiently using matrix operations:
$$
\mathbf{Z}^{(l)} = \mathbf{W}^{(l)}\mathbf{A}^{(l-1)} + \mathbf{b}^{(l)}
$$

Here, $\mathbf{W}^{(l)}$ represents the weight matrix for layer $l$, $\mathbf{A}^{(l-1)}$ contains the activations from the previous layer, and $\mathbf{b}^{(l)}$ is the bias vector.

Following this linear transformation, each layer applies a nonlinear activation function $f$:
$$
\mathbf{A}^{(l)} = f(\mathbf{Z}^{(l)})
$$

This process repeats at each layer, creating a chain of transformations:

Input → Linear Transform → Activation → Linear Transform → Activation → ... → Output

In our MNIST example, the pixel values first undergo a transformation by the first hidden layer's weights, converting the 784-dimensional input into an intermediate representation. Each subsequent layer further transforms this representation, ultimately producing a 10-dimensional output vector representing the network's confidence in each possible digit.

#### Mathematical Representation {#sec-dl-primer-mathematical-representation-6c2a}

The complete forward propagation process can be expressed as a composition of functions, each representing a layer's transformation. Formalizing this mathematically builds on the MNIST example.

For a network with $L$ layers, we can express the full forward computation as:
$$
\mathbf{A}^{(L)} = f^{(L)}\Big(\mathbf{W}^{(L)}f^{(L-1)}\Big(\mathbf{W}^{(L-1)}\cdots\big(f^{(1)}(\mathbf{W}^{(1)}\mathbf{X} + \mathbf{b}^{(1)})\big)\cdots + \mathbf{b}^{(L-1)}\Big) + \mathbf{b}^{(L)}\Big)
$$

While this nested expression captures the complete process, we typically compute it step by step:

1. First layer:
$$
\mathbf{Z}^{(1)} = \mathbf{W}^{(1)}\mathbf{X} + \mathbf{b}^{(1)}
$$
$$
\mathbf{A}^{(1)} = f^{(1)}(\mathbf{Z}^{(1)})
$$

2. Hidden layers $(l = 2,\ldots, L-1)$:
$$
\mathbf{Z}^{(l)} = \mathbf{W}^{(l)}\mathbf{A}^{(l-1)} + \mathbf{b}^{(l)}
$$
$$
\mathbf{A}^{(l)} = f^{(l)}(\mathbf{Z}^{(l)})
$$

3. Output layer:
$$
\mathbf{Z}^{(L)} = \mathbf{W}^{(L)}\mathbf{A}^{(L-1)} + \mathbf{b}^{(L)}
$$
$$
\mathbf{A}^{(L)} = f^{(L)}(\mathbf{Z}^{(L)})
$$

In our MNIST example, if we have a batch of $B$ images, the dimensions of these operations are:

* Input $\mathbf{X}$: $B \times 784$
* First layer weights $\mathbf{W}^{(1)}$: $n_1\times 784$
* Hidden layer weights $\mathbf{W}^{(l)}$: $n_l\times n_{l-1}$
* Output layer weights $\mathbf{W}^{(L)}$: $n_{L-1}\times 10$

#### Computational Process {#sec-dl-primer-computational-process-a092}

Understanding how these mathematical operations translate into actual computation requires examining the forward propagation process for a batch of MNIST images. This process illustrates how data transforms from raw pixel values to digit predictions.

Consider a batch of 32 images entering our network. Each image starts as a $28\times 28$ grid of pixel values, which we flatten into a 784-dimensional vector. For the entire batch, this gives us an input matrix $\mathbf{X}$ of size $32\times 784$, where each row represents one image. The values are typically normalized to lie between 0 and 1.

The transformation at each layer proceeds as follows:

* **Input Layer Processing**: The network takes our input matrix $\mathbf{X}$ $(32\times 784)$ and transforms it using the first layer's weights. If our first hidden layer has 128 neurons, $\mathbf{W}^{(1)}$ is a $784\times 128$ matrix. The resulting computation $\mathbf{X}\mathbf{W}^{(1)}$ produces a $32\times 128$ matrix.

* **Hidden Layer Transformations**: Each element in this matrix then has its corresponding bias added and passes through an activation function. For example, with a ReLU activation, any negative values become zero while positive values remain unchanged. This nonlinear transformation enables the network to learn complex patterns in the data.

* **Output Generation**: The final layer transforms its inputs into a $32\times 10$ matrix, where each row contains 10 values corresponding to the network's confidence scores for each possible digit. Often, these scores are converted to probabilities using a softmax function:
$$
P(\text{digit } j) = \frac{e^{z_j}}{\sum_{k=1}^{10} e^{z_k}}
$$

For each image in the batch, this produces a probability distribution over the possible digits. The digit with the highest probability represents the network's prediction.

#### Practical Considerations {#sec-dl-primer-practical-considerations-6167}

The implementation of forward propagation requires careful attention to several practical aspects that affect both computational efficiency and memory usage. These considerations become particularly important when processing large batches of data or working with deep networks.

Memory management plays an important role during forward propagation. Each layer's activations must be stored for potential use in the backward pass during training. For our MNIST example with a batch size of 32, if we have three hidden layers of sizes 128, 256, and 128, the activation storage requirements are:

* First hidden layer: $32\times 128 = 4,096$ values
* Second hidden layer: $32\times 256 = 8,192$ values
* Third hidden layer: $32\times 128 = 4,096$ values
* Output layer: $32\times 10 = 320$ values

This produces a total of 16,704 values that must be maintained in memory for each batch during training. The memory requirements scale linearly with batch size and become substantial for larger networks.

Batch processing introduces important trade-offs. Larger batches enable more efficient matrix operations and better hardware utilization but require more memory. For example, doubling the batch size to 64 would double the memory requirements for activations. This relationship between batch size, memory usage, and computational efficiency guides the choice of batch size in practice.

The organization of computations also affects performance. Matrix operations can be optimized through careful memory layout and specialized libraries. The choice of activation functions affects both the network's learning capabilities and computational efficiency, as some functions (like ReLU) require less computation than others (like tanh or sigmoid).

To put things into perspective, the MNIST forward pass requires ~200,000 multiply-accumulate operations for our example network. On a 3GHz CPU with 16 cores achieving 10% efficiency for this workload, this takes ~1.25ms. A GPU with 5000 cores at 40% efficiency completes the same computation in 0.01ms, achieving a 125x speedup. Specialized AI accelerators achieve further improvements through reduced precision (INT8 vs FP32 provides 4x speedup), systolic arrays (eliminating weight memory access overhead), and dataflow architectures optimized for neural network computation patterns.

The energy cost varies dramatically across these platforms. A CPU consumes ~0.3 microjoules (μJ) for this MNIST inference (25W × 1.25ms), while a GPU uses ~2.0 μJ (200W × 0.01ms) despite being faster. Edge accelerators like Google's Edge TPU achieve ~0.05 μJ through specialized dataflow and 8-bit quantization. For perspective, the human brain processes similar visual recognition tasks using ~20 picojoules per operation—10,000× more energy efficient than our best silicon. This efficiency gap drives research into neuromorphic computing, spiking neural networks, and novel memory technologies that could approach biological energy efficiency.

These considerations form the foundation for understanding the system requirements of neural networks, which we will explore in more detail @sec-dnn-architectures.

### Loss Functions {#sec-dl-primer-loss-functions-d892}

Neural networks learn by measuring and minimizing their prediction errors. Loss functions provide the algorithmic structure for quantifying these errors, serving as the essential feedback mechanism that guides the learning process. Through loss functions, researchers can convert the abstract goal of "making good predictions" into a concrete optimization problem.

To understand the role of loss functions, let's continue with our MNIST digit recognition example. When the network processes a handwritten digit image, it outputs ten numbers representing its confidence in each possible digit (0-9). The loss function measures how far these predictions deviate from the true answer. For instance, if an image displays a "7", the network should exhibit high confidence for digit "7" and low confidence for all other digits. The loss function penalizes the network when its prediction deviates from this target.

Consider a concrete example: if the network sees an image of "7" and outputs confidences:
$$
\text{\texttt{[0.1, 0.1, 0.1, 0.0, 0.0, 0.0, 0.2, 0.3, 0.1, 0.1]}}
$$

The highest confidence (0.3) is assigned to digit "7", but this confidence is quite low, indicating uncertainty in the prediction. A good loss function would produce a high loss value here, signaling that the network needs significant improvement. Conversely, if the network outputs:
$$
\text{\texttt{[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9, 0.0, 0.1]}}
$$

The loss function should produce a lower value, as this prediction is much closer to ideal. This illustrates how loss functions guide network improvement by providing feedback on prediction quality.

#### Basic Concepts {#sec-dl-primer-basic-concepts-397d}

A loss function measures how far the network's predictions are from the correct answers. This difference is expressed as a single number: a lower loss means the predictions are more accurate, while a higher loss indicates the network needs improvement. During training, the loss function guides the network by helping it adjust its weights to make better predictions. For example, in recognizing handwritten digits, the loss will penalize predictions that assign low confidence to the correct digit.

Mathematically, a loss function $L$ takes two inputs: the network's predictions $\hat{y}$ and the true values $y$. For a single training example in our MNIST task:
$$
L(\hat{y}, y) = \text{measure of discrepancy between prediction and truth}
$$

When training with batches of data, we typically compute the average loss across all examples in the batch:
$$
L_{\text{batch}} = \frac{1}{B}\sum_{i=1}^B L(\hat{y}_i, y_i)
$$
where $B$ is the batch size and $(\hat{y}_i, y_i)$ represents the prediction and truth for the $i$-th example.

The choice of loss function depends on the type of task. For our MNIST classification problem, we need a loss function that can:

1. Handle probability distributions over multiple classes
2. Provide meaningful gradients for learning
3. Penalize wrong predictions effectively
4. Scale well with batch processing

#### Classification Losses {#sec-dl-primer-classification-losses-9278}

For classification tasks like MNIST digit recognition, "cross-entropy" [@shannon1948mathematical][^fn-cross-entropy] loss has emerged as the standard choice. This loss function is particularly well-suited for comparing predicted probability distributions with true class labels.

[^fn-cross-entropy]: **Cross-Entropy Loss**: Derived from information theory by Claude Shannon in 1948, cross-entropy measures the "surprise" when predicting incorrectly. If a model is 99% confident about the wrong answer, the loss is much higher than being 60% confident about the wrong answer. This mathematical property naturally encourages the model to be both accurate and calibrated (confident when right, uncertain when unsure). Cross-entropy loss works perfectly with softmax outputs and provides strong gradients even when predictions are very wrong, making it ideal for classification tasks.

For a single digit image, our network outputs a probability distribution over the ten possible digits. We represent the true label as a one-hot vector where all entries are 0 except for a 1 at the correct digit's position. For instance, if the true digit is "7", the label would be:
$$
y = \big[0, 0, 0, 0, 0, 0, 0, 1, 0, 0\big]
$$

The cross-entropy loss for this example is:
$$
L(\hat{y}, y) = -\sum_{j=1}^{10} y_j \log(\hat{y}_j)
$$
where $\hat{y}_j$ represents the network's predicted probability for digit j. Given our one-hot encoding, this simplifies to:
$$
L(\hat{y}, y) = -\log(\hat{y}_c)
$$
where $c$ is the index of the correct class. This means the loss depends only on the predicted probability for the correct digit—the network is penalized based on how confident it is in the right answer.

For example, if our network predicts the following probabilities for an image of "7":

```
Predicted: [0.1, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.8, 0.0, 0.1]
True: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0]
```

The loss would be $-\log(0.8)$, which is approximately 0.223. If the network were more confident and predicted 0.9 for the correct digit, the loss would decrease to approximately 0.105.

#### Loss Computation {#sec-dl-primer-loss-computation-b815}

The practical computation of loss involves considerations for both numerical stability and batch processing. When working with batches of data, we compute the average loss across all examples in the batch.

For a batch of B examples, the cross-entropy loss becomes:
$$
L_{\text{batch}} = -\frac{1}{B}\sum_{i=1}^B \sum_{j=1}^{10} y_{ij} \log(\hat{y}_{ij})
$$

Computing this loss efficiently requires careful consideration of numerical precision. Taking the logarithm of very small probabilities can lead to numerical instability. Consider a case where our network predicts a probability of 0.0001 for the correct class. Computing $\log(0.0001)$ directly might cause underflow or result in imprecise values.

To address this, we typically implement the loss computation with two key modifications:

1. Add a small epsilon to prevent taking log of zero:
$$
L = -\log(\hat{y} + \epsilon)
$$

2. Apply the log-sum-exp trick for numerical stability:
$$
\text{softmax}(z_i) = \frac{\exp\big(z_i - \max(z)\big)}{\sum_j \exp\big(z_j - \max(z)\big)}
$$

For our MNIST example with a batch size of 32, this means:

* Processing 32 sets of 10 probabilities
* Computing 32 individual loss values
* Averaging these values to produce the final batch loss

#### Training Implications {#sec-dl-primer-training-implications-e004}

Understanding how loss functions influence training helps explain key implementation decisions in deep learning models.

During each training iteration, the loss value serves multiple purposes:

1. Performance Metric: It quantifies current network accuracy
2. Optimization Target: Its gradients guide weight updates
3. Convergence Signal: Its trend indicates training progress

For our MNIST classifier, monitoring the loss during training reveals the network's learning trajectory. A typical pattern might show:

* Initial high loss ($\sim 2.3$, equivalent to random guessing among 10 classes)
* Rapid decrease in early training iterations
* Gradual improvement as the network fine-tunes its predictions
* Eventually stabilizing at a lower loss ($\sim 0.1$, indicating confident correct predictions)

The loss function's gradients with respect to the network's outputs provide the initial error signal that drives backpropagation. For cross-entropy loss, these gradients have a particularly simple form: the difference between predicted and true probabilities. This mathematical property makes cross-entropy loss especially suitable for classification tasks, as it provides strong gradients even when predictions are very wrong.

The choice of loss function also influences other training decisions:

* Learning rate selection (larger loss gradients might require smaller learning rates)
* Batch size (loss averaging across batches affects gradient stability)
* Optimization algorithm behavior
* Convergence criteria

### Backward Propagation and Optimization {#sec-dl-primer-backward-propagation-9a49}

Backward propagation, often called backpropagation, is the algorithmic cornerstone of neural network training that enables systematic weight adjustment through gradient-based optimization. This section presents the complete optimization framework, from gradient computation through practical training implementation.

#### The Backpropagation Algorithm {#sec-dl-primer-backpropagation-algorithm}

While forward propagation computes predictions, backward propagation determines how to adjust the network's weights to improve these predictions. To understand this process, consider our MNIST example where the network predicts a "3" for an image of "7". Backward propagation provides a systematic way to adjust weights throughout the network to make this mistake less likely in the future by calculating how each weight contributed to the error.

The process begins at the network's output, where we compare predicted digit probabilities with the true label. This error then flows backward through the network, with each layer's weights receiving an update signal based on their contribution to the final prediction. The computation follows the chain rule of calculus, breaking down the complex relationship between weights and final error into manageable steps.

The mathematical foundations of backpropagation provide the theoretical basis for training neural networks, but practical implementation requires sophisticated software frameworks. Modern frameworks like PyTorch and TensorFlow implement automatic differentiation systems that handle gradient computation automatically, eliminating the need for manual derivative implementation. The systems engineering aspects of these frameworks, including computation graphs and optimization strategies, are covered comprehensively in @sec-ai-frameworks.

{{< margin-video "https://youtu.be/IHZwWFHWa-w?si=_MpUFVskdVHYztkz" "Gradient descent – Part 1" "3Blue1Brown" >}}

{{< margin-video "https://youtu.be/Ilg3gGewQ5U?si=YXVP3tm_ZBY9R-Hg" "Gradient descent – Part 2" "3Blue1Brown" >}}

#### Gradient Flow {#sec-dl-primer-gradient-flow-66f2}

The flow of gradients through a neural network follows a path opposite to the forward propagation. Starting from the loss at the output layer, gradients propagate backwards, computing how each layer, and ultimately each weight, influenced the final prediction error.

In our MNIST example, consider what happens when the network misclassifies a "7" as a "3". The loss function generates an initial error signal at the output layer, essentially indicating that the probability for "7" should increase while the probability for "3" should decrease. This error signal then propagates backward through the network layers.

For a network with L layers, the gradient flow can be expressed mathematically. At each layer l, we compute how the layer's output affected the final loss:
$$
\frac{\partial L}{\partial \mathbf{A}^{(l)}} = \frac{\partial L}{\partial \mathbf{A}^{(l+1)}} \frac{\partial \mathbf{A}^{(l+1)}}{\partial \mathbf{A}^{(l)}}
$$

This computation cascades backward through the network, with each layer's gradients depending on the gradients computed in the layer previous to it. The process reveals how each layer's transformation contributed to the final prediction error. For instance, if certain weights in an early layer strongly influenced a misclassification, they will receive larger gradient values, indicating a need for more substantial adjustment.

However, this process faces important challenges in deep networks. As gradients flow backward through many layers, they can either vanish or explode. When gradients are repeatedly multiplied through many layers, they can become exponentially small, particularly with sigmoid or tanh activation functions. This causes early layers to learn very slowly or not at all, as they receive negligible (vanishing) updates. Conversely, if gradient values are consistently greater than 1, they can grow exponentially, leading to unstable training and destructive weight updates.

#### Gradient Computation {#sec-dl-primer-gradient-computation-b46b}

The actual computation of gradients involves calculating several partial derivatives at each layer. For each layer, we need to determine how changes in weights, biases, and activations affect the final loss. These computations follow directly from the chain rule of calculus but must be implemented efficiently for practical neural network training.

At each layer $l$, we compute three main gradient components:

1. Weight Gradients:
$$
\frac{\partial L}{\partial \mathbf{W}^{(l)}} = \frac{\partial L}{\partial \mathbf{Z}^{(l)}} {\mathbf{A}^{(l-1)}}^T
$$

2. Bias Gradients:
$$
\frac{\partial L}{\partial \mathbf{b}^{(l)}} = \frac{\partial L}{\partial \mathbf{Z}^{(l)}}
$$

3. Input Gradients (for propagating to previous layer):
$$
\frac{\partial L}{\partial \mathbf{A}^{(l-1)}} = {\mathbf{W}^{(l)}}^T \frac{\partial L}{\partial \mathbf{Z}^{(l)}}
$$

In our MNIST example, consider the final layer where the network outputs digit probabilities. If the network predicted $[0.1, 0.2, 0.5,\ldots, 0.05]$ for an image of "7", the gradient computation would:

1. Start with the error in these probabilities
2. Compute how weight adjustments would affect this error
3. Propagate these gradients backward to help adjust earlier layer weights

While these mathematical formulations precisely describe gradient computation, the systems breakthrough lies in how frameworks automatically implement these calculations. Consider a simple operation like matrix multiplication followed by ReLU activation: `output = torch.relu(input @ weight)`. The mathematical gradient involves computing the derivative of ReLU (0 for negative inputs, 1 for positive) and applying the chain rule for matrix multiplication. However, the framework handles this automatically by:

1. Recording the operation in a computation graph during forward pass
2. Storing necessary intermediate values (pre-ReLU activations for gradient computation)
3. Automatically generating the backward pass function for each operation
4. Optimizing memory usage and computation order across the entire graph

This automation transforms gradient computation from a manual, error-prone process requiring deep mathematical expertise into a reliable system capability that enables rapid experimentation and deployment. The framework ensures correctness while optimizing for computational efficiency, memory usage, and hardware utilization.

#### Implementation Aspects {#sec-dl-primer-implementation-aspects-411b}

The practical implementation of backward propagation requires careful consideration of computational resources and memory management. These implementation details significantly impact training efficiency and scalability.

Memory requirements during backward propagation stem from two main sources. First, we need to store the intermediate activations from the forward pass, as these are required for computing gradients. For our MNIST network with a batch size of 32, each layer's activations must be maintained:

* Input layer: $32\times 784$ values (100KB in FP32)
* Hidden layer 1: $32\times 512$ values (64KB in FP32)
* Hidden layer 2: $32\times 256$ values (32KB in FP32)
* Output layer: $32\times 10$ values (1.3KB in FP32)

Second, we must store gradients for each parameter during backward propagation. For our example network with 784×512 + 512×256 + 256×10 = 533,504 total parameters, this requires an additional 2.1MB of memory for gradients, plus 2.1MB for momentum terms in optimizers like Adam[^fn-adam-optimizer], totaling approximately 4.4MB for a modest MNIST network.

[^fn-adam-optimizer]: **Adam Optimizer**: Introduced by Diederik Kingma and Jimmy Ba in 2014, Adam (Adaptive Moment Estimation) combines the benefits of two other optimizers: AdaGrad's adaptive learning rates and RMSprop's exponential moving averages. Adam maintains separate learning rates for each parameter and adapts them based on first and second moments of gradients. It requires 2× memory overhead (storing momentum and velocity for each parameter) but typically converges 2-5× faster than basic SGD. Adam became the default optimizer for most deep learning applications due to its robustness across different problems and minimal hyperparameter tuning requirements.

The memory bandwidth requirements scale with model size and batch size. Each training step for our MNIST network requires loading 533,504 parameters (2.1MB in FP32), storing gradients (2.1MB), and accessing activations (~400KB for batch size 32). At 100 training steps per second, this generates 460MB/s of memory traffic—well within DRAM bandwidth but approaching L3 cache limits. Large transformer models like GPT-3 with 175B parameters require 350GB for weights alone in FP16, saturating even HBM systems at 1-2TB/s during training.

Second, we need storage for the gradients themselves. For each layer, we must maintain gradients of similar dimensions to the weights and biases. Taking our previous example of a network with hidden layers of size 128, 256, and 128, this means storing:

* First layer gradients: $784\times 128$ values
* Second layer gradients: $128\times 256$ values
* Third layer gradients: $256\times 128$ values
* Output layer gradients: $128\times 10$ values

The computational pattern of backward propagation follows a specific sequence:

1. Compute gradients at current layer
2. Update stored gradients
3. Propagate error signal to previous layer
4. Repeat until input layer is reached

For batch processing, these computations are performed simultaneously across all examples in the batch, enabling efficient use of matrix operations and parallel processing capabilities.

Modern frameworks handle these computations through sophisticated autograd engines. When you call `loss.backward()` in PyTorch, the framework automatically manages memory allocation, operation scheduling, and gradient accumulation across the computation graph. The system tracks which tensors require gradients, optimizes memory usage through gradient checkpointing when needed, and schedules operations to maximize hardware utilization. This automated management allows practitioners to focus on model design rather than the intricate details of gradient computation implementation.

### Optimization Process {#sec-dl-primer-optimization-process-5160}

Training neural networks requires systematic adjustment of weights and biases to minimize prediction errors through an iterative optimization process. This section explores the core mechanisms of neural network optimization, from basic gradient descent principles to practical training implementations. We examine how gradients guide parameter updates, how batch processing improves efficiency, the structure of training loops, and practical considerations for effective optimization.

#### Gradient-Based Optimization {#sec-dl-primer-gradient-descent-basics-903a}

The optimization process adjusts network weights through gradient descent[^fn-gradient-descent], a systematic method that calculates how each weight contributes to the error and updates parameters to reduce loss. This iterative process gradually refines the network's predictive ability through repeated parameter adjustments.

[^fn-gradient-descent]: **Gradient Descent**: Think of gradient descent as finding the bottom of a valley while blindfolded—you feel the slope under your feet and take steps downhill. Mathematically, the gradient points in the direction of steepest increase, so we move in the opposite direction to minimize our loss function. The name comes from the Latin "gradus" (step) and was first formalized by Cauchy in 1847 for solving systems of equations, though the modern machine learning version was developed much later.

The fundamental update rule combines backpropagation's gradient computation with parameter adjustment:
$$
\theta_{\text{new}} = \theta_{\text{old}} - \alpha \nabla_{\theta}L
$$
where $\theta$ represents any network parameter (weights or biases), $\alpha$ is the learning rate, and $\nabla_{\theta}L$ is the gradient computed through backpropagation.

For our MNIST example, this means adjusting weights to improve digit classification accuracy. If the network frequently confuses "7"s with "1"s, gradient descent will modify weights to better distinguish between these digits. The learning rate $\alpha$[^fn-learning-rate] controls adjustment magnitude—too large values cause overshooting optimal parameters, while too small values result in slow convergence.

[^fn-learning-rate]: **Learning Rate**: Often called the most important hyperparameter in deep learning, the learning rate determines the step size in optimization. Think of it like the gas pedal on a car—too much acceleration and you'll crash past your destination, too little and you'll never get there. Typical values range from 0.1 to 0.0001, and getting this right can mean the difference between a model that learns in hours versus one that never converges.

{{< margin-video "https://youtu.be/tIeHLnjs5U8?si=Uckr8YPwwAZ_UI6t" "Backpropagation" "3Blue1Brown" >}}

#### Why Gradient Descent Works: Non-Convex Optimization Theory {#sec-dl-primer-why-gradient-descent-works-a834}

Understanding why gradient descent succeeds in training neural networks requires examining the theoretical foundations of non-convex optimization. Unlike convex functions that have a single global minimum, neural network loss landscapes are highly non-convex with multiple local minima, saddle points, and plateaus. This creates a puzzle: why does gradient descent, which only guarantees convergence to local minima, consistently find good solutions in practice?

Recent theoretical insights provide partial answers. The lottery ticket hypothesis [@frankle2018lottery] suggests that randomly initialized neural networks contain sparse subnetworks that can achieve comparable accuracy when trained in isolation. This implies that gradient descent succeeds because it naturally discovers these "winning tickets" within the larger network. The implicit bias of gradient descent also plays a crucial role: when multiple solutions exist that fit the training data, gradient descent exhibits an implicit preference for solutions with certain properties, often favoring simpler or more generalizable representations [@neyshabur2017exploring].

From an information-theoretic viewpoint, successful learning occurs when networks compress training data into useful representations while retaining the information necessary for generalization. During training, neural networks appear to undergo two phases: first, they fit the training data (memorization), then they discover generalizable patterns (compression). The information bottleneck principle[^fn-information-bottleneck] [@tishby2015deep] suggests that this compression process naturally emerges from gradient-based optimization, helping explain why networks learn hierarchical representations that transfer across tasks.

[^fn-information-bottleneck]: **Information Bottleneck Principle**: Proposed by Naftali Tishby, Pereira, and Bialek (1999), this principle suggests that optimal learning involves finding representations that retain information relevant to the task while discarding irrelevant details. Neural networks naturally implement this: early layers compress inputs by removing noise and irrelevant features, while later layers retain task-specific information. This explains why trained networks often generalize—they've learned to ignore distracting variations and focus on essential patterns. The principle also explains why over-parameterized networks don't always overfit: they can learn to compress effectively even with excess capacity.

The universal approximation theorem [@cybenko1989approximation] proves that even single-hidden-layer networks can approximate any continuous function given sufficient width. However, the practical success of deep networks stems from their ability to represent complex functions efficiently with exponentially fewer parameters than shallow alternatives [@telgarsky2016benefits]. Deep networks naturally capture compositional structure through hierarchical features, making them particularly well-suited for real-world data like images and language that exhibit such hierarchical organization.

Modern deep networks often contain more parameters than training examples, yet they generalize well—contradicting classical statistical learning theory. The double descent phenomenon [@nakkiran2019deep] reveals that test error can decrease even as model complexity increases beyond the point of overfitting. This suggests that overparameterized models can interpolate training data while still learning generalizable patterns, possibly because gradient descent finds solutions that lie in regions of parameter space with good generalization properties.

#### Batch Processing {#sec-dl-primer-batch-processing-89d7}

Neural networks typically process multiple examples simultaneously during training, an approach known as mini-batch gradient descent. Rather than updating weights after each individual image, we compute the average gradient over a batch of examples before performing the update.

For a batch of size $B$, the loss gradient becomes:
$$
\nabla_{\theta}L_{\text{batch}} = \frac{1}{B}\sum_{i=1}^B \nabla_{\theta}L_i
$$

In our MNIST training, with a typical batch size of 32, this means:

1. Process 32 images through forward propagation
2. Compute loss for all 32 predictions
3. Average the gradients across all 32 examples
4. Update weights using this averaged gradient

::: {.callout-note title="Systems Perspective: Batch Size and Hardware Utilization"}
**The Batch Size Trade-off**: Larger batches improve hardware efficiency because matrix operations can process multiple examples with similar computational cost to processing one. However, each example in the batch requires memory to store its activations, creating a fundamental trade-off: larger batches use hardware more efficiently but demand more memory. Available memory thus becomes a hard constraint on batch size, which in turn affects how efficiently the hardware can be utilized. This relationship between algorithm design (batch size) and hardware capability (memory) exemplifies why ML systems engineering requires thinking about both simultaneously.
:::

#### Training Loop {#sec-dl-primer-training-loop-e2e1}

The complete training process combines forward propagation, backward propagation, and weight updates into a systematic training loop. This loop repeats until the network achieves satisfactory performance or reaches a predetermined number of iterations.

A single pass through the entire training dataset is called an epoch[^fn-epoch-training]. For MNIST, with 60,000 training images and a batch size of 32, each epoch consists of 1,875 batch iterations. The training loop structure is:

[^fn-epoch-training]: **Epoch**: From the Greek word "epoche" meaning "fixed point in time," an epoch represents one complete cycle through all training data. Deep learning models typically require 10-200 epochs to converge, depending on dataset size and complexity. Modern large language models like GPT-3 train on only 1 epoch over massive datasets (300 billion tokens), while smaller models might train for 100+ epochs on limited data. The term was borrowed from astronomy, where it marks a specific moment for measuring celestial positions—fitting for the iterative refinement process of neural network training.

1. For each epoch:
   * Shuffle training data to prevent learning order-dependent patterns
   * For each batch:
     * Perform forward propagation
     * Compute loss
     * Execute backward propagation
     * Update weights using gradient descent
   * Evaluate network performance

During training, we monitor several key metrics:

* Training loss: average loss over recent batches
* Validation accuracy: performance on held-out test data
* Learning progress: how quickly the network improves

For our digit recognition task, we might observe the network's accuracy improve from 10% (random guessing) to over 95% through multiple epochs of training.

#### Practical Considerations {#sec-dl-primer-practical-considerations-accc}

The successful implementation of neural network training requires attention to several key practical aspects that significantly impact learning effectiveness. These considerations bridge the gap between theoretical understanding and practical implementation.

Learning rate selection is perhaps the most critical parameter affecting training. For our MNIST network, the choice of learning rate dramatically influences the training dynamics. A large learning rate of 0.1 might cause unstable training where the loss oscillates or explodes as weight updates overshoot optimal values. Conversely, a very small learning rate of 0.0001 might result in extremely slow convergence, requiring many more epochs to achieve good performance. A moderate learning rate of 0.01 often provides a good balance between training speed and stability, allowing the network to make steady progress while maintaining stable learning.

Convergence monitoring provides crucial feedback during the training process. As training progresses, we typically observe the loss value stabilizing around a particular value, indicating the network is approaching a local optimum. The validation accuracy often plateaus as well, suggesting the network has extracted most of the learnable patterns from the data. The gap between training and validation performance offers insights into whether the network is overfitting or generalizing well to new examples. The operational aspects of monitoring models in production environments, including detecting model degradation and performance drift, are comprehensively covered in @sec-ml-operations.

Resource requirements become increasingly important as we scale neural network training. The memory footprint must accommodate both model parameters and the intermediate computations needed for backpropagation. Computation scales linearly with batch size, affecting training speed and hardware utilization. Modern training often leverages GPU acceleration, making efficient use of parallel computing capabilities crucial for practical implementation.

Training neural networks also presents several challenges. Overfitting occurs when the network becomes too specialized to the training data, performing well on seen examples but poorly on new ones. Gradient instability can manifest as either vanishing or exploding gradients, making learning difficult. The interplay between batch size, available memory, and computational resources often requires careful balancing to achieve efficient training while working within hardware constraints.

\vspace*{-1mm}
## Prediction Phase {#sec-dl-primer-prediction-phase-4204}

Having explored the training process in detail, we now turn to the operational phase of neural networks. Neural networks serve two distinct purposes: learning from data during training and making predictions during inference. While we've explored how networks learn through forward propagation, backward propagation, and weight updates, the prediction phase operates differently. During inference, networks use their learned parameters to transform inputs into outputs without the need for learning mechanisms. This simpler computational process still requires careful consideration of how data flows through the network and how system resources are utilized. Understanding the prediction phase is crucial as it represents how neural networks are actually deployed to solve real-world problems, from classifying images to generating text predictions.

\needspace{8\baselineskip}
### Inference Basics {#sec-dl-primer-inference-basics-47d7}

\needspace{6\baselineskip}
#### Training vs Inference {#sec-dl-primer-training-vs-inference-098e}

Neural network operation divides into two fundamentally distinct phases that impose markedly different computational requirements and system constraints. Training requires both forward and backward passes through the network to compute gradients and update weights, while inference involves only forward pass computation. This architectural simplification means that each layer performs only one set of operations during inference, transforming inputs to outputs using learned weights without tracking intermediate values for gradient computation, as illustrated in @fig-training-vs-inference.

These computational differences manifest directly in hardware requirements and deployment strategies. Training clusters typically employ high-memory GPUs (80GB+ for large models) with substantial cooling infrastructure, consuming 300-700W per GPU. Inference deployments prioritize latency and energy efficiency across diverse platforms: mobile devices utilize 2-4W neural processors, edge servers deploy specialized inference accelerators like Google's Edge TPU (4 TOPS/W), and cloud services employ inference-optimized instances with reduced precision arithmetic (INT8/FP16) for increased throughput. Production inference systems serving millions of requests daily require sophisticated infrastructure including load balancing, auto-scaling, and failover mechanisms typically unnecessary in training environments.

::: {#fig-training-vs-inference fig-env="figure" fig-pos="htb"}
```{.tikz}
\begin{tikzpicture}[line join=round,font=\usefont{T1}{phv}{m}{n}]
\tikzset{
 Line/.style={line width=0.75pt,black!50,text=black},
 LineD/.style={line width=0.5pt,black!50,text=black},
 LineE/.style={line width=1.95pt,brown!50,text=black},
}
\newif\ifbox@dashed
\box@dashedfalse % default: not dashed

\tikzset{
circles/.pic={
\pgfkeys{/channel/.cd, #1}
\node[circle,draw=\channelcolor,line width=1pt,fill=\channelcolor!10,
minimum size=4.5mm,\ifbox@dashed dashed\fi](\picname){};
        }
}

\tikzset{
channel/.pic={
\pgfkeys{/channel/.cd, #1}
\node[rectangle,draw=\drawchannelcolor,line width=1pt,fill=\channelcolor!10,
minimum height=13mm,minimum width=22mm,\ifbox@dashed dashed\fi](\picname){};
        }
}

\pgfkeys{
  /channel/.cd,
  channelcolor/.store in=\channelcolor,
  drawchannelcolor/.store in=\drawchannelcolor,
  scalefac/.store in=\scalefac,
  picname/.store in=\picname,
  channelcolor=BrownLine,
  drawchannelcolor=BrownLine,
  scalefac=1,
  dashed/.is if=box@dashed,
  dashed/.default=true,
  picname=C
}

\tikzset{
  man/.pic={
  \pgfkeys{/man/.cd, #1}
     % tie
    \draw[draw=\tiecolor,fill=\tiecolor] (0.0,-1.1)--(0.16,-0.87)--(0.09,-0.46)--(0.13,-0.37)--(0.0,-0.28)
                   --(-0.13,-0.37)--(-0.09,-0.46)--(-0.16,-0.87)--cycle;
    % ears
    \draw[fill=black] (0.74,0.95) to[out=20,in=80](0.86,0.80) to[out=250,in=330](0.65,0.65) to[out=70,in=260] cycle;
    \draw[fill=black] (-0.76,0.96) to[out=170,in=110](-0.85,0.80) to[out=290,in=190](-0.65,0.65) to[out=110,in=290] cycle;

    % head
    \fill[fill=black] (0,0) to[out=180,in=290](-0.72,0.84) to[out=110,in=190](-0.56,1.67)
                      to[out=70,in=110](0.68,1.58) to[out=320,in=80](0.72,0.84) to[out=250,in=0] cycle;
    % face
    \fill[fill=white] (0,0.11) to[out=175,in=290](-0.53,0.65) to[out=110,in=265](-0.61,1.22)
                      to[out=80,in=235](-0.50,1.45) to[out=340,in=215](0.50,1.47)
                      to[out=310,in=85](0.60,0.92) to[out=260,in=2] cycle;
    \draw[fill=black] (-0.50,1.45) to[out=315,in=195](0.40,1.25) to[out=340,in=10](0.37,1.32)
                      to[out=190,in=310](-0.40,1.49) -- cycle;
    % neck
    \draw[line width=1.25pt] (-0.62,-0.2) to[out=50,in=290] (-0.5,0.42);
    \draw[line width=1.25pt] (0.62,-0.2) to[out=130,in=250] (0.5,0.42);
    % body
    \draw[draw=\bodycolor,fill=\bodycolor] (0.0,-1.0) to[out=150,in=290](-0.48,-0.14) to[out=200,in=50](-1.28,-0.44)
                   to[out=240,in=80](-1.55,-1.76) -- (1.55,-1.76)
                   to[out=100,in=300](1.28,-0.44) to[out=130,in=340](0.49,-0.14)
                   to[out=245,in=30] cycle;
    % right stet
    \draw[line width=2pt,\stetcolor] (0.8,-0.21) to[bend left=7](0.78,-0.64)
         to[out=350,in=80](0.98,-1.35) to[out=250,in=330](0.72,-1.60);
    \draw[line width=2pt,\stetcolor] (0.43,-1.53) to[out=180,in=240](0.3,-1.15)
         to[out=60,in=170](0.78,-0.64);
    % left stet
    \draw[line width=2pt,\stetcolor] (-0.75,-0.21) to[bend right=20](-0.65,-1.45);
    \node[fill=\stetcolor,circle,minimum size=5pt] at (-0.65,-1.45) {};
    % eyes
    \node[circle,fill=black,inner sep=2pt] at (0.28,0.94) {};
    \node[circle,fill=black,inner sep=2pt] at (-0.28,0.94) {};
     % mouth
    \draw[line width=1.0pt] (-0.25,0.5) to[bend right=\emotion](0.25,0.5);
  },
}
\pgfkeys{
  /man/.cd,
  tiecolor/.store in=\tiecolor,
  bodycolor/.store in=\bodycolor,
  stetcolor/.store in=\stetcolor,
  emotion/.store in=\emotion,
  emotion=40
  tiecolor=red,      % derfault tie color
  bodycolor=blue!30  % derfault body color
  stetcolor=green  % derfault stet color
}
%%%%
\begin{scope}[local bounding box=DOWN,shift={(0,0)}]
\begin{scope}[local bounding box=CHANEL1,shift={(0,0)}]
\foreach \i/\clr/\da in {1/BrownLine/dashed,2/BrownLine/dashed,3/BrownLine/dashed,4/BrownLine/,5/green/} {
\pic at ({-\i*0.25}, {-0.25*\i}) {channel={picname=\i-CH1,channelcolor=\clr,\da}};
}
\end{scope}
\node[below=0.1 of 5-CH1]{\textbf{Inference}};
\draw[Line,latex-latex]([xshift=3mm]5-CH1.south east)--node[pos=0.37, right=12pt, align=left] {Smaller,\\ varied N}
([xshift=3mm]1-CH1.south east);

\begin{scope}[local bounding box=MAN,shift={($(5-CH1.north west)!0.5!(5-CH1.south east)$)},
scale=0.3, every node/.append style={transform shape}]
\pic at (0,0) {man={tiecolor=brown, bodycolor=BlueLine,stetcolor=BlueLine,emotion=50}};
\end{scope}

\begin{scope}[local bounding box=CIRCLE1,shift={($(CHANEL1)+(6.8,-0.3)$)}]
\foreach \i in {1,...,5} {
  \pgfmathsetmacro{\y}{(2-\i)*0.6+0.7}
  \pic at (0,\y) {circles={channelcolor=RedLine,picname=1CD\i}};
}
%middle -3 neurons
\foreach \j in {1,2,3} {
  \pgfmathsetmacro{\y}{(1-\j)*0.6 + 0.7}
  \pic at (1.5,\y) {circles={channelcolor=RedLine,picname=2CD\j}};
}
%right -3 neurons
\foreach \j in {1,2,3} {
  \pgfmathsetmacro{\y}{(1-\j)*0.6 + 0.7}
  \pic at (3.0,\y) {circles={channelcolor=RedLine,picname=3CD\j}};
}
\end{scope}
\coordinate(DL)at($(1CD3)+(-2,0)$);
\coordinate(DD)at($(3CD2)+(2.5,0)$);
\foreach \i in {1,2,3,4,5}{
  \foreach \j in {1,2,3}{
\draw[LineD](1CD\i)--(2CD\j);
}}

\foreach \i in {1,2,3}{
  \foreach \j in {1,2,3}{
\draw[LineD](2CD\i)--(3CD\j);
}}

\draw[LineE,-latex](DL)--(DD)node[above left]{forward}node[right]{"person"};
\end{scope}
%%%%%%%%%%
%ABOVE
%%%%%%%%%%
\begin{scope}[local bounding box=ABOVE,shift={(0,3.8)}]
\begin{scope}[local bounding box=CHANEL1,shift={(0.6,0)}]
\foreach \i/\clr/\da in {1/BrownLine/,2/BrownLine/,3/BrownLine/,4/BrownLine/,5/BrownLine/,6/BrownLine/,7/yellow!70!/} {
\pic at ({-\i*0.25}, {-0.25*\i}) {channel={picname=\i-CH1,channelcolor=\clr,\da}};
}
\end{scope}
\node[below=0.1 of 7-CH1]{\textbf{Training}};
\draw[Line,latex-latex]([xshift=3mm]7-CH1.south east)--node[pos=0.37, right=12pt, align=left] {Large N}
([xshift=3mm]1-CH1.south east);
%person
\begin{scope}[local bounding box=MAN,shift={($(7-CH1.north west)!0.5!(7-CH1.south east)$)},
scale=0.3, every node/.append style={transform shape}]
\pic at (0,0) {man={tiecolor=green!70!black, bodycolor=VioletLine,stetcolor=VioletLine,emotion=-50}};
\end{scope}

\begin{scope}[local bounding box=CIRCLE1,shift={($(CHANEL1)+(6.44,-0.1)$)}]
\foreach \i in {1,...,5} {
  \pgfmathsetmacro{\y}{(2-\i)*0.6+0.7}
  \pic at (0,\y) {circles={channelcolor=RedLine,picname=1CD\i}};
}
%middle -3 neurons
\foreach \j in {1,2,3} {
  \pgfmathsetmacro{\y}{(1-\j)*0.6 + 0.7}
  \pic at (1.5,\y) {circles={channelcolor=RedLine,picname=2CD\j}};
}
%right -3 neurons
\foreach \j in {1,2,3} {
  \pgfmathsetmacro{\y}{(1-\j)*0.6 + 0.7}
  \pic at (3.0,\y) {circles={channelcolor=RedLine,picname=3CD\j}};
}
\end{scope}
\coordinate(DL)at($(1CD2)+(-2,0)$);
\coordinate(DD)at($(3CD1)+(2.5,0)$);
\coordinate(DL2)at($(1CD4)+(-2,0)$);
\coordinate(DD2)at($(3CD3)+(2.5,0)$);
\foreach \i in {1,2,3,4,5}{
  \foreach \j in {1,2,3}{
\draw[LineD](1CD\i)--(2CD\j);
}}

\foreach \i in {1,2,3}{
  \foreach \j in {1,2,3}{
\draw[LineD](2CD\i)--(3CD\j);
}}

\draw[LineE,-latex](DL)--(DD)node[above left]{forward}node(PE)[right]{"person"};
\draw[LineE,latex-](DL2)--(DD2)node[below left]{backward}node[right]{};
\draw[LineE,-latex,red](PE)|-node[fill=white,pos=0.2]{error}(DD2);
\end{scope}
\end{tikzpicture}
```
**Inference vs. Training Flow**: During inference, neural networks utilize learned weights for forward pass computation only, simplifying the data flow and reducing computational cost compared to training, which requires both forward and backward passes for weight updates. This streamlined process enables efficient deployment of trained models for real-time predictions.
:::

Parameter freezing represents another major distinction between training and inference phases. During training, weights and biases continuously update to minimize the loss function. In inference, these parameters remain fixed, acting as static transformations learned from the training data. This freezing of parameters not only simplifies computation but also enables optimizations impossible during training, such as weight quantization or pruning.

The structural difference between training loops and inference passes significantly impacts system design. Training operates in an iterative loop, processing multiple batches of data repeatedly across many epochs to refine the network's parameters. Inference, in contrast, typically processes each input just once, generating predictions in a single forward pass. This shift from iterative refinement to single-pass prediction influences how we architect systems for deployment.

These structural differences create substantially different memory and computation requirements between training and inference. Training demands considerable memory to store intermediate activations for backpropagation, gradients for weight updates, and optimization states. Inference eliminates these memory-intensive requirements, needing only enough memory to store the model parameters and compute a single forward pass. This reduction in memory footprint, coupled with simpler computation patterns, enables inference to run efficiently on a broader range of devices, from powerful servers to resource-constrained edge devices.

In general, the training phase requires more computational resources and memory for learning, while inference is streamlined for efficient prediction. @tbl-train-vs-inference summarizes the key differences between training and inference.

+-------------------------+------------------------------+--------------------------------+
| Aspect                  | Training                     | Inference                      |
+:========================+:=============================+:===============================+
| Computation Flow        | Forward and backward passes, | Forward pass only,             |
|                         | gradient computation         | direct input to output         |
+-------------------------+------------------------------+--------------------------------+
| Parameters              | Continuously updated         | Fixed/frozen weights           |
|                         | weights and biases           | and biases                     |
+-------------------------+------------------------------+--------------------------------+
| Processing Pattern      | Iterative loops over         | Single pass through            |
|                         | multiple epochs              | the network                    |
+-------------------------+------------------------------+--------------------------------+
| Memory Requirements     | High – stores activations,   | Lower– stores only model      |
|                         | gradients, optimizer state   | parameters and current input   |
+-------------------------+------------------------------+--------------------------------+
| Computational Needs     | Heavy – gradient updates,    | Lighter – matrix               |
|                         | backpropagation              | multiplication only            |
+-------------------------+------------------------------+--------------------------------+
| Hardware Requirements   | GPUs/specialized hardware    | Can run on simpler devices,    |
|                         | for efficient training       | including mobile/edge          |
+-------------------------+------------------------------+--------------------------------+

: **Training vs. Inference**: Neural networks transition from a computationally intensive training phase—requiring both forward and backward passes with updated parameters—to an efficient inference phase using fixed parameters and solely forward passes. This distinction enables deployment on resource-constrained devices by minimizing memory requirements and computational load during prediction. {#tbl-train-vs-inference}

This stark contrast between training and inference phases highlights why system architectures often differ significantly between development and deployment environments. While training requires substantial computational resources and specialized hardware, inference can be optimized for efficiency and deployed across a broader range of devices.

Training and inference enable different architectural optimizations. Training requires high-precision arithmetic and backward pass computation, driving specialized hardware adoption with flexible compute units. Inference allows for various efficiency optimizations and specialized architectures that take advantage of the simpler computational flow. These differences explain why specialized inference processors can achieve much higher energy efficiency compared to general-purpose training hardware.

Memory usage patterns also differ dramatically: training stores all activations for backpropagation (requiring 2-3x more memory), while inference can discard activations immediately after use.

#### Basic Pipeline {#sec-dl-primer-basic-pipeline-8598}

The implementation of neural networks in practical applications requires a complete processing pipeline that extends beyond the network itself. This pipeline, which is illustrated in @fig-inference-pipeline transforms raw inputs into meaningful outputs through a series of distinct stages, each essential for the system's operation. Understanding this complete pipeline provides critical insights into the design and deployment of machine learning systems.

::: {#fig-inference-pipeline fig-env="figure" fig-pos="htb"}
```{.tikz}
\begin{tikzpicture}[,font=\small\usefont{T1}{phv}{m}{n},line width=0.75pt]
\tikzset{%
  Line/.style={line width=1.0pt,black!50,text=black},
  Box/.style={inner xsep=3pt,
    node distance=0.6,
    draw=GreenLine, line width=0.75pt,
    fill=GreenL,
    align=flush center,
    minimum width=15mm,
    minimum height=10mm
  },
}
%
\node[Box](B1){Raw\\ Input};
\node[Box,right=of B1](B2){Pre-processing};
\node[Box,node distance=1, right=of B2,fill=BlueL,draw=BlueLine](B3){Neural\\ Network};
\node[Box,node distance=1, right=of B3,fill=VioletL2,draw=VioletLine2](B4){Raw\\ Output};
\node[Box,right=of B4,fill=VioletL2,draw=VioletLine2](B5){Post-processing};
\node[Box, right=of B5,fill=VioletL2,draw=VioletLine2](B6){Final\\ Output};
%
\draw[Line,-latex](B1)--(B2);
\draw[Line,-latex](B2)--(B3);
\draw[Line,-latex](B3)--(B4);
\draw[Line,-latex](B4)--(B5);
\draw[Line,-latex](B5)--(B6);
%
\scoped[on background layer]
\node[draw=BackLine,inner xsep=3mm,inner ysep=5mm,yshift=2mm,
            fill=BackColor,fit=(B1)(B2),line width=0.75pt](BB){};
\node[below=3pt of  BB.north,anchor=north]{Traditional Computing};
%
\scoped[on background layer]
\node[draw=OrangeLine,inner xsep=4mm,inner ysep=5mm,yshift=2mm,
            fill=OrangeL!70!red!10,fit=(B3),line width=0.75pt](BB){};
\node[below=3pt of  BB.north,anchor=north]{Deep Learning};
%
\scoped[on background layer]
\node[draw=BackLine,inner xsep=3mm,inner ysep=5mm,yshift=2mm,
            fill=BackColor,fit=(B4)(B6),line width=0.75pt](BB){};
\node[below=3pt of  BB.north,anchor=north]{Traditional Computing};
\end{tikzpicture}
```
**Inference Pipeline**: Machine learning systems transform raw inputs into final outputs through a series of sequential stages—preprocessing, neural network computation, and post-processing—each critical for accurate prediction and deployment. This pipeline emphasizes the distinction between model architecture and the complete system required for real-world application.
:::

The key thing to notice from the figure is that machine learning systems operate as hybrid architectures that combine conventional computing operations with neural network computations. The neural network component, focused on learned transformations through matrix operations, represents just one element within a broader computational framework. This framework encompasses both the preparation of input data and the interpretation of network outputs, processes that rely primarily on traditional computing methods.

Consider how data flows through the pipeline in @fig-inference-pipeline:

1. Raw inputs arrive in their original form, which might be images, text, sensor readings, or other data types
2. Pre-processing transforms these inputs into a format suitable for neural network consumption
3. The neural network performs its learned transformations
4. Raw outputs emerge from the network, often in numerical form
5. Post-processing converts these outputs into meaningful, actionable results

This pipeline structure reveals several key characteristics of machine learning systems. The neural network, despite its computational sophistication, functions as a component within a larger system. Performance bottlenecks may arise at any stage of the pipeline, not exclusively within the neural network computation. System optimization must therefore consider the entire pipeline rather than focusing solely on the neural network's operation.

The hybrid nature of this architecture has significant implications for system implementation. While neural network computations may benefit from specialized hardware accelerators, pre- and post-processing operations typically execute on conventional processors. This distribution of computation across heterogeneous hardware resources represents a fundamental consideration in system design.

### Pre-processing {#sec-dl-primer-preprocessing-992d}

The pre-processing stage transforms raw inputs into a format suitable for neural network computation. While often overlooked in theoretical discussions, this stage forms a critical bridge between real-world data and neural network operations. Consider our MNIST digit recognition example: before a handwritten digit image can be processed by the neural network we designed earlier, it must undergo several transformations. Raw images of handwritten digits arrive in various formats, sizes, and pixel value ranges. For instance, in @fig-handwritten, we see that the digits are all of different sizes, and even the number 6 is written differently by the same person.

![**Handwritten Digit Variability**: Real-world data exhibits substantial variation in style, size, and orientation, necessitating robust pre-processing techniques for reliable machine learning performance. these images exemplify the challenges of digit recognition, where even seemingly simple inputs require normalization and feature extraction before they can be effectively processed by a neural network. Source: o. augereau.](images/png/handwritten_digits.png){#fig-handwritten  width=55%}

The pre-processing stage standardizes these inputs through conventional computing operations:

* Image scaling to the required $28\times 28$ pixel dimensions, camera images are usually large(r).
* Pixel value normalization from $[0,255]$ to $[0,1]$, most cameras generate colored images.
* Flattening the 2D image array into a 784-dimensional vector, preparing it for the neural network.
* Basic validation to ensure data integrity, making sure the network predicted correctly.

What distinguishes pre-processing from neural network computation is its reliance on traditional computing operations rather than learned transformations. While the neural network learns to recognize digits through training, pre-processing operations remain fixed, deterministic transformations. This distinction has important system implications: pre-processing operates on conventional CPUs rather than specialized neural network hardware, and its performance characteristics follow traditional computing patterns.

The effectiveness of pre-processing directly impacts system performance. Poor normalization can lead to reduced accuracy, inconsistent scaling can introduce artifacts, and inefficient implementation can create bottlenecks. Understanding these implications helps in designing robust machine learning systems that perform well in real-world conditions.

### Inference {#sec-dl-primer-inference-dd7f}

The inference phase represents the operational state of a neural network, where learned parameters are used to transform inputs into predictions. Unlike the training phase we discussed earlier, inference focuses solely on forward computation with fixed parameters.

#### Network Initialization {#sec-dl-primer-network-initialization-3d95}

Before processing any inputs, the neural network must be properly initialized for inference. This initialization phase involves loading the model parameters learned during training into memory. For our MNIST digit recognition network, this means loading specific weight matrices and bias vectors for each layer. Let's examine the exact memory requirements for our architecture:

* Input to first hidden layer:
  * Weight matrix: $784\times 100 = 78,400$ parameters
  * Bias vector: 100 parameters

* First to second hidden layer:
  * Weight matrix: $100\times 100 = 10,000$ parameters
  * Bias vector: 100 parameters

* Second hidden layer to output:
  * Weight matrix: $100\times 10 = 1,000$ parameters
  * Bias vector: 10 parameters

This architecture's complete parameter requirements are detailed in the Resource Requirements section below. For processing a single image, this means allocating space for:

* First hidden layer activations: 100 values
* Second hidden layer activations: 100 values
* Output layer activations: 10 values

This memory allocation pattern differs significantly from training, where additional memory was needed for gradients, optimizer states, and backpropagation computations.

Real-world inference deployments employ various memory optimization techniques to reduce resource requirements while maintaining acceptable accuracy. Systems may combine multiple requests together to better utilize hardware capabilities while meeting response time requirements. For resource-constrained deployments, various model compression approaches help models fit within available memory while preserving functionality.

#### Forward Pass Computation {#sec-dl-primer-forward-pass-computation-81c3}

During inference, data propagates through the network's layers using the initialized parameters. This forward propagation process, while similar in structure to its training counterpart, operates with different computational constraints and optimizations. The computation follows a deterministic path from input to output, transforming the data at each layer using learned parameters.

For our MNIST digit recognition network, consider the precise computations at each layer. The network processes a pre-processed image represented as a 784-dimensional vector through successive transformations:

1. First Hidden Layer Computation:
   * Input transformation: 784 inputs combine with 78,400 weights through matrix multiplication
   * Linear computation: $\mathbf{z}^{(1)} = \mathbf{x}\mathbf{W}^{(1)} + \mathbf{b}^{(1)}$
   * Activation: $\mathbf{a}^{(1)} = \text{ReLU}(\mathbf{z}^{(1)})$
   * Output: 100-dimensional activation vector

2. Second Hidden Layer Computation:
   * Input transformation: 100 values combine with 10,000 weights
   * Linear computation: $\mathbf{z}^{(2)} = \mathbf{a}^{(1)}\mathbf{W}^{(2)} + \mathbf{b}^{(2)}$
   * Activation: $\mathbf{a}^{(2)} = \text{ReLU}(\mathbf{z}^{(2)})$
   * Output: 100-dimensional activation vector

3. Output Layer Computation:
   * Final transformation: 100 values combine with 1,000 weights
   * Linear computation: $\mathbf{z}^{(3)} = \mathbf{a}^{(2)}\mathbf{W}^{(3)} + \mathbf{b}^{(3)}$
   * Activation: $\mathbf{a}^{(3)} = \text{softmax}(\mathbf{z}^{(3)})$
   * Output: 10 probability values

@tbl-forward-pass shows how these computations, while mathematically identical to training-time forward propagation, show important operational differences:

+----------------------+--------------------------------+--------------------------------+
| Characteristic       | Training Forward Pass          | Inference Forward Pass         |
+:=====================+:===============================+:===============================+
| Activation Storage   | Maintains complete activation  | Retains only current layer     |
|                      | history for backpropagation    | activations                    |
+----------------------+--------------------------------+--------------------------------+
| Memory Pattern       | Preserves intermediate states  | Releases memory after layer    |
|                      | throughout forward pass        | computation completes          |
+----------------------+--------------------------------+--------------------------------+
| Computational Flow   | Structured for gradient        | Optimized for direct output    |
|                      | computation preparation        | generation                     |
+----------------------+--------------------------------+--------------------------------+
| Resource Profile     | Higher memory requirements     | Minimized memory footprint     |
|                      | for training operations        | for efficient execution        |
+----------------------+--------------------------------+--------------------------------+

: **Forward Pass Optimization**: During inference, neural networks prioritize computational efficiency by retaining only current layer activations and releasing intermediate states, unlike training where complete activation history is maintained for backpropagation. This optimization streamlines output generation by focusing resources on immediate computations rather than gradient preparation. {#tbl-forward-pass}

This streamlined computation pattern enables efficient inference while maintaining the network's learned capabilities. The reduction in memory requirements and simplified computational flow make inference particularly suitable for deployment in resource-constrained environments, such as Mobile ML and Tiny ML.

#### Resource Requirements {#sec-dl-primer-resource-requirements-9f9b}

Neural networks consume computational resources differently during inference compared to training. During inference, resource utilization focuses primarily on efficient forward pass computation and minimal memory overhead. Examining the specific requirements for the MNIST digit recognition network reveals:

Memory requirements during inference can be precisely quantified:

1. Static Memory (Model Parameters):
   * Layer 1: 78,400 weights + 100 biases
   * Layer 2: 10,000 weights + 100 biases
   * Layer 3: 1,000 weights + 10 biases
   * Total: 89,610 parameters ($\approx 358.44$ KB at 32-bit floating point precision[^fn-floating-point])

[^fn-floating-point]: **32-bit Floating Point Precision**: Also called "single precision" or FP32, this IEEE 754 standard uses 32 bits to represent real numbers: 1 bit for sign, 8 bits for exponent, and 23 bits for mantissa. While neural network training typically requires FP32 precision to maintain gradient stability, inference often works with FP16 (half precision) or even INT8 (8-bit integers), reducing memory usage by 2× to 4×. Modern AI chips like Google's TPU v4 support "bfloat16" (brain floating point), Google's custom 16-bit format that maintains FP32's range while halving memory requirements.

2. Dynamic Memory (Activations):
   * Layer 1 output: 100 values
   * Layer 2 output: 100 values
   * Layer 3 output: 10 values
   * Total: 210 values ($\approx 0.84$ KB at 32-bit floating point precision)

Computational requirements follow a fixed pattern for each input:

* First layer: 78,400 multiply-adds
* Second layer: 10,000 multiply-adds
* Output layer: 1,000 multiply-adds
* Total: 89,400 multiply-add operations per inference

This resource profile stands in stark contrast to training requirements, where additional memory for gradients and computational overhead for backpropagation significantly increase resource demands. The predictable, streamlined nature of inference computations enables various optimization opportunities and efficient hardware utilization.

#### Optimization Opportunities {#sec-dl-primer-optimization-opportunities-d5ec}

The fixed nature of inference computation presents several opportunities for optimization that are not available during training. Once a neural network's parameters are frozen, the predictable pattern of computation allows for systematic improvements in both memory usage and computational efficiency.

Batch size selection represents a key trade-off in inference optimization. During training, large batches were necessary for stable gradient computation, but inference offers more flexibility. Processing single inputs minimizes latency, making it ideal for real-time applications where immediate responses are crucial. However, batch processing can significantly improve throughput by better utilizing parallel computing capabilities, particularly on GPUs. For our MNIST network, consider the memory implications: processing a single image requires storing 210 activation values, while a batch of 32 images requires 6,720 activation values but can process images up to 32 times faster on parallel hardware.

Memory management during inference can be significantly more efficient than during training. Since intermediate values are only needed for forward computation, memory buffers can be carefully managed and reused. The activation values from each layer need only exist until the next layer's computation is complete. This enables in-place operations where possible, reducing the total memory footprint. The fixed nature of inference allows for precise memory alignment and access patterns optimized for the underlying hardware architecture.

Hardware-specific optimizations become particularly important during inference. On CPUs, computations can be organized to maximize cache utilization and take advantage of parallel processing capabilities where the same operation is applied to multiple data elements simultaneously. GPU deployments benefit from optimized matrix multiplication routines and efficient memory transfer patterns. These optimizations extend beyond pure computational efficiency, as they can significantly impact power consumption and hardware utilization, critical factors in real-world deployments.

The predictable nature of inference also enables optimizations like reduced numerical precision. While training typically requires full floating-point precision to maintain stable learning, inference can often operate with reduced precision while maintaining acceptable accuracy. For our MNIST network, such optimizations could significantly reduce the memory footprint with corresponding improvements in computational efficiency.

These optimization principles, while illustrated through our simple MNIST feedforward network, represent only the foundation of neural network optimization. More sophisticated architectures introduce additional considerations and opportunities, including specialized designs for spatial data processing, sequential computation, and attention-based computation patterns. These architectural variations and their optimizations will be explored in detail in @sec-dnn-architectures, @sec-model-optimizations, and @sec-efficient-ai deep learning architectures, model optimizations, and efficient AI implementations.

### Post-processing {#sec-dl-primer-postprocessing-93d9}

The transformation of neural network outputs into actionable predictions requires a return to traditional computing paradigms. Just as pre-processing bridges real-world data to neural computation, post-processing bridges neural outputs back to conventional computing systems. This completes the hybrid computing pipeline we examined earlier, where neural and traditional computing operations work in concert to solve real-world problems.

The complexity of post-processing extends beyond simple mathematical transformations. Real-world systems must handle uncertainty, validate outputs, and integrate with larger computing systems. In our MNIST example, a digit recognition system might require not just the most likely digit, but also confidence measures to determine when human intervention is needed. This introduces additional computational steps: confidence thresholds, secondary prediction checks, and error handling logic, all of which are implemented in traditional computing frameworks.

The computational requirements of post-processing differ significantly from neural network inference. While inference benefits from parallel processing and specialized hardware, post-processing typically runs on conventional CPUs and follows sequential logic patterns. This return to traditional computing brings both advantages and constraints. Operations are more flexible and easier to modify than neural computations, but they may become bottlenecks if not carefully implemented. For instance, computing softmax probabilities for a batch of predictions requires different optimization strategies than the matrix multiplications of neural network layers.

System integration considerations often dominate post-processing design. Output formats must match downstream system requirements, error handling must align with broader system protocols, and performance must meet system-level constraints. In a complete mail sorting system, the post-processing stage must not only identify digits but also format these predictions for the sorting machinery, handle uncertainty cases appropriately, and maintain processing speeds that match physical mail flow rates.

This return to traditional computing paradigms completes the hybrid nature of machine learning systems. Just as pre-processing prepared real-world data for neural computation, post-processing adapts neural outputs for real-world use. Understanding this hybrid nature, the interplay between neural and traditional computing, is essential for designing and implementing effective machine learning systems.

## Historical Case Study: USPS Postal Service {#sec-dl-primer-case-study-usps-postal-service-aa64}

The theoretical foundations of neural networks find concrete expression in systems that solve real-world problems at scale. The USPS handwritten digit recognition system, deployed in the 1990s, exemplifies this translation from theory to practice. This early production deployment established many principles still relevant in modern ML systems: the importance of robust preprocessing pipelines, the need for confidence thresholds in automated decision-making, and the challenge of maintaining system performance under varying real-world conditions. While today's systems deploy vastly more sophisticated architectures on more capable hardware, examining this foundational case study reveals how the mathematical principles of gradient descent, backpropagation, and activation functions combine to create production systems—lessons that scale from 1990s mail sorting to 2025's edge AI deployments.

### Real-world Problem {#sec-dl-primer-realworld-problem-5233}

The United States Postal Service (USPS) processes over 100 million pieces of mail daily, each requiring accurate routing based on handwritten ZIP codes. In the early 1990s, human operators primarily performed this task, making it one of the largest manual data entry operations worldwide. The automation of this process through neural networks represents an early and successful large-scale deployment of artificial intelligence, embodying many of the principles explored in this chapter.

The complexity of this task becomes evident: a ZIP code recognition system must process images of handwritten digits captured under varying conditions—different writing styles, pen types, paper colors, and environmental factors (@fig-usps-digit-examples). It must make accurate predictions within milliseconds to maintain mail processing speeds. Errors in recognition can lead to significant delays and costs from misrouted mail. This real-world constraint meant the system needed not just high accuracy, but also reliable measures of prediction confidence to identify when human intervention was necessary.

![**Handwritten Digit Variability**: Real-world handwritten digits exhibit significant variations in stroke width, slant, and character formation, posing challenges for automated recognition systems like those used by the USPS. These examples demonstrate the need for robust feature extraction and model generalization to achieve high accuracy in optical character recognition (OCR) tasks.](images/jpg/usps_examples_new.jpg){#fig-usps-digit-examples width=90%}

This challenging environment presented requirements spanning every aspect of neural network implementation we've discussed, from biological inspiration to practical deployment considerations. The success or failure of the system would depend not just on the neural network's accuracy, but on the entire pipeline from image capture through to final sorting decisions.

### System Development {#sec-dl-primer-system-development-02bf}

The development of the USPS digit recognition system required careful consideration at every stage, from data collection to deployment. This process illustrates how theoretical principles of neural networks translate into practical engineering decisions.

Data collection presented the first major challenge. Unlike controlled laboratory environments, postal facilities needed to process mail pieces with tremendous variety. The training dataset had to capture this diversity. Digits written by people of different ages, educational backgrounds, and writing styles formed just part of the challenge. Envelopes came in varying colors and textures, and images were captured under different lighting conditions and orientations. This extensive data collection effort later contributed to the creation of the MNIST database we've used in our examples.

The network architecture design required balancing multiple constraints. While deeper networks might achieve higher accuracy, they would also increase processing time and computational requirements. Processing $28\times 28$ pixel images of individual digits needed to complete within strict time constraints while running reliably on available hardware. The network had to maintain consistent accuracy across varying conditions, from well-written digits to hurried scrawls.

Training the network introduced additional complexity. The system needed to achieve high accuracy not just on a test dataset, but on the endless variety of real-world handwriting styles. Careful preprocessing normalized input images to account for variations in size and orientation. Data augmentation techniques increased the variety of training samples. The team validated performance across different demographic groups and tested under actual operating conditions to ensure robust performance.

The engineering team faced a critical decision regarding confidence thresholds. Setting these thresholds too high would route too many pieces to human operators, defeating the purpose of automation. Setting them too low would risk delivery errors. The solution emerged from analyzing the confidence distributions of correct versus incorrect predictions. This analysis established thresholds that optimized the tradeoff between automation rate and error rate, ensuring efficient operation while maintaining acceptable accuracy.

### Complete Pipeline {#sec-dl-primer-complete-pipeline-2253}

Following a single piece of mail through the USPS recognition system illustrates how the concepts we've discussed integrate into a complete solution. The journey from physical mail piece to sorted letter demonstrates the interplay between traditional computing, neural network inference, and physical machinery.

The process begins when an envelope reaches the imaging station. High-speed cameras capture the ZIP code region at rates exceeding several pieces of mail (e.g. 10) pieces per second. This image acquisition process must adapt to varying envelope colors, handwriting styles, and environmental conditions. The system must maintain consistent image quality despite the speed of operation, as motion blur and proper illumination present significant engineering challenges.

Pre-processing transforms these raw camera images into a format suitable for neural network analysis. The system must locate the ZIP code region, segment individual digits, and normalize each digit image. This stage employs traditional computer vision techniques: image thresholding adapts to envelope background color, connected component analysis identifies individual digits, and size normalization produces standard $28\times 28$ pixel images. Speed remains critical; these operations must complete within milliseconds to maintain throughput.

The neural network then processes each normalized digit image. The trained network, with its 89,610 parameters (as we detailed earlier), performs forward propagation to generate predictions. Each digit passes through two hidden layers of 100 neurons each, ultimately producing ten output values representing digit probabilities. This inference process, while computationally intensive, benefits from the optimizations we discussed in the previous section.

Post-processing converts these neural network outputs into sorting decisions. The system applies confidence thresholds to each digit prediction. A complete ZIP code requires high confidence in all five digits, a single uncertain digit flags the entire piece for human review. When confidence meets thresholds, the system transmits sorting instructions to mechanical systems that physically direct the mail piece to its appropriate bin.

The entire pipeline operates under strict timing constraints. From image capture to sorting decision, processing must complete before the mail piece reaches its sorting point. The system maintains multiple pieces in various pipeline stages simultaneously, requiring careful synchronization between computing and mechanical systems. This real-time operation illustrates why the optimizations we discussed in inference and post-processing become crucial in practical applications.

### Results and Impact {#sec-dl-primer-results-impact-bb54}

The implementation of neural network-based ZIP code recognition transformed USPS mail processing operations. By 2000, several facilities across the country utilized this technology, processing millions of mail pieces daily. This real-world deployment demonstrated both the potential and limitations of neural network systems in mission-critical applications.

Performance metrics revealed interesting patterns that validate many of the principles discussed earlier in this chapter. The system achieved its highest accuracy on clearly written digits, similar to those in the training data. However, performance varied significantly with real-world factors. Lighting conditions affected pre-processing effectiveness. Unusual writing styles occasionally confused the neural network. Environmental vibrations could also impact image quality. These challenges led to continuous refinements in both the physical system and the neural network pipeline.

The economic impact proved substantial. Prior to automation, manual sorting required operators to read and key in ZIP codes at an average rate of one piece per second. The neural network system processed pieces at ten times this rate while reducing labor costs and error rates. However, the system didn't eliminate human operators entirely; their role shifted to handling uncertain cases and maintaining system performance. This hybrid approach, combining artificial and human intelligence, became a model for other automation projects.

The system also revealed important lessons about deploying neural networks in production environments. Training data quality proved crucial; the network performed best on digit styles well-represented in its training set. Regular retraining helped adapt to evolving handwriting styles. Maintenance required both hardware specialists and machine learning experts, introducing new operational considerations. These insights influenced subsequent deployments of neural networks in other industrial applications.

Researchers discovered this implementation demonstrated how theoretical principles translate into practical constraints. The biological inspiration of neural networks provided the foundation for digit recognition, but successful deployment required careful consideration of system-level factors: processing speed, error handling, maintenance requirements, and integration with existing infrastructure. These lessons continue to inform modern machine learning deployments, where similar challenges of scale, reliability, and integration persist.

### Key Takeaways {#sec-dl-primer-key-takeaways-64ec}

The USPS ZIP code recognition system exemplifies the journey from biological inspiration to practical neural network deployment explored in this chapter. It demonstrates how the basic principles of neural computation, from preprocessing through inference to postprocessing, combine to solve real-world problems.

The system's development shows why understanding both the theoretical foundations and practical considerations is crucial. While the biological visual system processes handwritten digits effortlessly, translating this capability into an artificial system required careful consideration of network architecture, training procedures, and system integration.

The success of this early large-scale neural network deployment helped establish many practices we now consider standard: the importance of thorough training data, the need for confidence metrics, the role of pre- and post-processing, and the critical nature of system-level optimization.

The principles demonstrated by the USPS system—robust preprocessing, confidence-based decision making, and hybrid human-AI workflows—remain foundational in modern deployments, though the scale and sophistication have transformed dramatically. Where USPS deployed networks with ~100K parameters processing images at 10 pieces/second on specialized hardware consuming 50-100W, today's mobile devices deploy models with 1-10M parameters processing 30+ frames/second for real-time vision tasks on neural processors consuming <2W. Edge AI systems in 2025—from smartphone face recognition to autonomous vehicle perception—face analogous challenges of balancing accuracy against computational constraints, but operate under far tighter power budgets (milliwatts vs watts) and stricter latency requirements (milliseconds vs tens of milliseconds). The core systems engineering principles remain constant: understanding the mathematical operations enables hardware-software co-design, preprocessing pipelines determine robustness to real-world variations, and confidence thresholding separates cases requiring human judgment from automated processing. This historical case study thus provides not merely historical context but a template for reasoning about modern ML systems deployment across the entire spectrum from cloud to edge to tiny devices.

## Fallacies and Pitfalls

Deep learning represents a paradigm shift from explicit programming to learning from data, which creates unique misconceptions about when and how to apply these powerful but complex systems. The mathematical foundations and statistical nature of neural networks often lead to misunderstandings about their capabilities, limitations, and appropriate use cases.

**Fallacy:** _Neural networks are "black boxes" that cannot be understood or debugged._

While neural networks lack the explicit rule-based transparency of traditional algorithms, multiple techniques enable understanding and debugging their behavior. Activation visualization reveals what patterns neurons respond to, gradient analysis shows how inputs affect outputs, and attention mechanisms highlight which features influence decisions. Layer-wise relevance propagation traces decision paths through the network, while ablation studies identify critical components. The perception of inscrutability often stems from attempting to understand neural networks through traditional programming paradigms rather than statistical and visual analysis methods. Modern interpretability tools provide insights into network behavior, though admittedly different from line-by-line code debugging.

**Fallacy:** _Deep learning eliminates the need for domain expertise and careful feature engineering._

The promise of automatic feature learning has led to the misconception that deep learning operates independently of domain knowledge. In reality, successful deep learning applications require extensive domain expertise to design appropriate architectures (convolutional layers for spatial data, recurrent structures for sequences), select meaningful training objectives, create representative datasets, and interpret model outputs within context. The USPS digit recognition system succeeded precisely because it incorporated postal service expertise about mail handling, digit writing patterns, and operational constraints. Domain knowledge guides critical decisions about data augmentation strategies, validation metrics, and deployment requirements that determine real-world success.

**Pitfall:** _Using complex deep learning models for problems solvable with simpler methods._

Teams frequently deploy sophisticated neural networks for tasks where linear models or decision trees would suffice, introducing unnecessary complexity, computational cost, and maintenance burden. A linear regression model requiring milliseconds to train may outperform a neural network requiring hours when data is limited or relationships are truly linear. Before employing deep learning, establish baseline performance with simple models. If a logistic regression achieves 95% accuracy on your classification task, the marginal improvement from a neural network rarely justifies the increased complexity. Reserve deep learning for problems exhibiting hierarchical patterns, non-linear relationships, or high-dimensional interactions that simpler models cannot capture.

**Pitfall:** _Training neural networks without understanding the underlying data distribution._

Many practitioners treat neural network training as a mechanical process of feeding data through standard architectures, ignoring critical data characteristics that determine success. Networks trained on imbalanced datasets will exhibit poor performance on minority classes unless addressed through resampling or loss weighting. Non-stationary distributions require continuous retraining or adaptive mechanisms. Outliers can dominate gradient updates, preventing convergence. The USPS system required careful analysis of digit frequency distributions, writing style variations, and image quality factors before achieving production-ready performance. Successful training demands thorough exploratory data analysis, understanding of statistical properties, and continuous monitoring of data quality metrics throughout the training process.

**Pitfall:** _Assuming research-grade models can be deployed directly into production systems without system-level considerations._

Many teams treat model development as separate from system deployment, leading to failures when research prototypes encounter production constraints. A neural network achieving excellent accuracy on clean datasets may fail when integrated with real-time data pipelines, legacy databases, or distributed serving infrastructure. Production systems require consideration of latency budgets, memory constraints, concurrent user loads, and fault tolerance mechanisms that rarely appear in research environments. The transformation from research code to production systems demands careful attention to data preprocessing pipelines, model serialization formats, serving infrastructure scalability, and monitoring systems for detecting performance degradation. Successful deployment requires early collaboration between data science and systems engineering teams to align model requirements with operational constraints.

## Summary {#sec-dl-primer-summary-19d0}

Neural networks transform computational approaches by replacing rule-based programming with adaptive systems that learn patterns from data. These systems draw inspiration from biological neural networks while creating practical artificial implementations that process complex information and improve performance through experience. The fundamental building blocks include artificial neurons that combine weighted inputs, apply activation functions, and propagate signals through interconnected layers to solve increasingly sophisticated tasks.

Building on these individual components, neural network architecture demonstrates hierarchical processing, where each layer extracts progressively more abstract patterns from raw data. Training adjusts connection weights through iterative optimization to minimize prediction errors, while inference applies learned knowledge to make predictions on new data. This separation between learning and application phases creates distinct system requirements for computational resources, memory usage, and processing latency that shape system design and deployment strategies. The operational aspects of managing these distinct phases in production environments are covered in @sec-ml-operations, which addresses deployment pipelines, system monitoring, and the critical production concerns of model drift detection and incident response for production ML systems.

::: {.callout-important title="Key Takeaways"}
* Neural networks replace hand-coded rules with adaptive patterns discovered from data through hierarchical processing architectures
* Training and inference represent distinct operational phases with different computational demands and system design requirements
* Complete processing pipelines integrate traditional computing with learning components across preprocessing, inference, and postprocessing stages
* System-level considerations—from activation function selection to batch size configuration to network topology—directly determine deployment feasibility across cloud, edge, and tiny devices
* Real-world deployment requires careful consideration of data quality, system integration, and performance optimization across the entire pipeline
:::

This chapter established the foundational mathematics and systems implications of neural networks through fully-connected architectures and basic training principles. However, the multilayer perceptrons explored here represent just the beginning of neural network design space. Real-world problems exhibit structure—images have spatial locality, text has sequential dependencies, graphs have relational patterns, and time-series data has temporal dynamics—that generic fully-connected networks cannot efficiently exploit.

The next chapter (@sec-dnn-architectures) addresses this limitation by introducing specialized architectures that encode problem structure directly into network design. Convolutional neural networks exploit spatial locality and translational invariance for computer vision, achieving state-of-the-art performance with 10-100× fewer parameters than fully-connected alternatives. Recurrent neural networks capture temporal dependencies for sequential data, enabling applications from speech recognition to language modeling. Transformers enable parallel processing of sequential data through attention mechanisms, revolutionizing natural language processing while introducing new systems challenges around memory usage scaling quadratically with sequence length.

Each architectural innovation brings distinct systems engineering challenges that build directly on the foundations established in this chapter. Convolutional layers demand different memory access patterns (spatial locality enables cache optimization), recurrent networks face different parallelization constraints (sequential dependencies limit batch processing), and attention mechanisms create new computational bottlenecks (quadratic complexity in sequence length). The mathematical operations remain matrix multiplications and non-linear activations, but their organization fundamentally changes the systems engineering trade-offs.

Understanding how these specialized architectures translate biological inspirations and mathematical abstractions into silicon implementations represents the next step in ML systems engineering. The principles of forward propagation, gradient descent, and activation functions remain constant, but their manifestation in specialized architectures requires new systems thinking about memory hierarchies, computational dataflow, and hardware-software co-design across the deployment spectrum.
