{
  "metadata": {
    "chapter": "generative_ai",
    "version": "1.0.0",
    "generated": "2025-09-15T14:08:03.497552",
    "total_terms": 31,
    "standardized": true,
    "last_updated": "2025-09-15T15:01:37.287109"
  },
  "terms": [
    {
      "term": "adversarial_training",
      "definition": "A training technique where two neural networks compete against each other, with one generating data and another trying to distinguish real from generated content.",
      "chapter_source": "generative_ai",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "attention_mechanism",
      "definition": "A neural network component that allows models to focus on different parts of input sequences when generating outputs, crucial for transformer architectures.",
      "chapter_source": "generative_ai",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "autoencoder",
      "definition": "A neural network architecture that learns to compress data into a lower-dimensional representation and then reconstruct the original data, often used as a building block for generative models.",
      "chapter_source": "generative_ai",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "autoregressive_model",
      "definition": "A generative model that produces output sequences one element at a time, where each new element depends on all previously generated elements.",
      "chapter_source": "generative_ai",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "conditional_generation",
      "definition": "The process of generating new data samples based on specific input conditions or constraints, such as generating images from text descriptions.",
      "chapter_source": "generative_ai",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "denoising_diffusion",
      "definition": "A generative modeling approach that learns to reverse a gradual noise-adding process, starting from pure noise and iteratively removing it to create realistic data.",
      "chapter_source": "generative_ai",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "diffusion_model",
      "definition": "A class of generative models that create data by learning to reverse a diffusion process that gradually adds noise to training data.",
      "chapter_source": "generative_ai",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "discriminator",
      "definition": "The component in a generative adversarial network that learns to distinguish between real data and generated samples, providing feedback to improve the generator.",
      "chapter_source": "generative_ai",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "fine_tuning",
      "definition": "The process of adapting a pre-trained generative model to a specific task or domain by training on a smaller, task-specific dataset.",
      "chapter_source": "generative_ai",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "foundation_model",
      "definition": "A large-scale machine learning model trained on broad data that can be adapted for various downstream generative tasks through fine-tuning or prompting.",
      "chapter_source": "generative_ai",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "gan",
      "definition": "Generative Adversarial Network, an architecture consisting of two competing neural networks that learn to generate realistic data through adversarial training.",
      "chapter_source": "generative_ai",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "generative_ai",
      "definition": "Artificial intelligence systems capable of creating new content such as text, images, audio, or code that resembles human-created content.",
      "chapter_source": "generative_ai",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "generative_model",
      "definition": "A machine learning model that learns the underlying distribution of training data to generate new, similar samples.",
      "chapter_source": "generative_ai",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "generator",
      "definition": "The component in a generative adversarial network responsible for creating new data samples that attempt to fool the discriminator.",
      "chapter_source": "generative_ai",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "hallucination",
      "definition": "The phenomenon where generative models produce outputs that appear plausible but are factually incorrect or inconsistent with reality.",
      "chapter_source": "generative_ai",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "in_context_learning",
      "definition": "The ability of large language models to perform new tasks by providing examples or instructions within the input prompt, without additional training.",
      "chapter_source": "generative_ai",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "large_language_model",
      "definition": "A neural network with billions of parameters trained on vast amounts of text data, capable of generating human-like text and performing various language tasks.",
      "chapter_source": "generative_ai",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "latent_space",
      "definition": "A lower-dimensional representation space where generative models encode the essential features of data, enabling manipulation and generation of new samples.",
      "chapter_source": "generative_ai",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "mode_collapse",
      "definition": "A failure mode in generative models where the model generates only a limited variety of samples, failing to capture the full diversity of the training data.",
      "chapter_source": "generative_ai",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "multimodal_generation",
      "definition": "The ability to generate content across different data types, such as creating images from text descriptions or generating captions for images.",
      "chapter_source": "generative_ai",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "neural_language_model",
      "definition": "A neural network trained to predict the probability of word sequences, forming the foundation for text generation and understanding tasks.",
      "chapter_source": "generative_ai",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "prompt_engineering",
      "definition": "The practice of crafting input prompts to guide generative models toward producing desired outputs, particularly important for large language models.",
      "chapter_source": "generative_ai",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "retrieval_augmented_generation",
      "definition": "A technique that combines generative models with information retrieval systems to ground generated content in factual knowledge from external sources.",
      "chapter_source": "generative_ai",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "sampling_strategy",
      "definition": "Methods for selecting outputs from generative models, such as temperature scaling, top-k sampling, or nucleus sampling, affecting the diversity and quality of generated content.",
      "chapter_source": "generative_ai",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "self_attention",
      "definition": "A mechanism allowing each position in a sequence to attend to all positions in the same sequence, enabling models to capture long-range dependencies.",
      "chapter_source": "generative_ai",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "sequence_to_sequence",
      "definition": "A neural network architecture that transforms input sequences into output sequences, commonly used for tasks like machine translation and text summarization.",
      "chapter_source": "generative_ai",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "text_to_image",
      "definition": "Generative models capable of creating images based on natural language descriptions, combining computer vision and natural language processing.",
      "chapter_source": "generative_ai",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "tokenization",
      "definition": "The process of converting text into discrete units (tokens) that can be processed by machine learning models, fundamental for text generation systems.",
      "chapter_source": "generative_ai",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "transformer",
      "definition": "A neural network architecture based entirely on attention mechanisms, serving as the foundation for most modern large language models and many generative AI systems.",
      "chapter_source": "generative_ai",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "variational_autoencoder",
      "definition": "A type of autoencoder that learns a probabilistic latent representation, enabling both data compression and generation of new samples from the learned distribution.",
      "chapter_source": "generative_ai",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "zero_shot_learning",
      "definition": "The ability of generative models to perform tasks they were not explicitly trained for, using only natural language instructions or examples.",
      "chapter_source": "generative_ai",
      "aliases": [],
      "see_also": []
    }
  ]
}