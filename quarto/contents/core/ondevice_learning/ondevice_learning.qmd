---
bibliography: ondevice_learning.bib
quiz: ondevice_learning_quizzes.json
concepts: ondevice_learning_concepts.yml
glossary: ondevice_learning_glossary.json
crossrefs: ondevice_learning_xrefs.json
---

# On-Device Learning {#sec-ondevice-learning}

::: {layout-narrow}
::: {.column-margin}
_DALL·E 3 Prompt: Drawing of a smartphone with its internal components exposed, revealing diverse miniature engineers of different genders and skin tones actively working on the ML model. The engineers, including men, women, and non-binary individuals, are tuning parameters, repairing connections, and enhancing the network on the fly. Data flows into the ML model, being processed in real-time, and generating output inferences._
:::

\noindent
![](images/png/cover_ondevice_learning.png)

:::

## Purpose {.unnumbered}

_What design principles enable machine learning systems to learn and adapt locally on resource-constrained devices rather than relying on centralized training?_

Traditional machine learning systems separate training and inference, but modern applications require local adaptation capabilities on deployment devices. On-device learning represents a shift from centralized training to distributed, local adaptation that enables personalization, preserves privacy through local data processing, reduces network dependencies, and allows rapid response to changing conditions. This paradigm shift requires overcoming severe constraints: limited model complexity, restricted training data availability, and minimal computational resources. Specialized techniques enable practical implementation: federated learning coordinates distributed training, transfer learning leverages pre-trained models for efficient adaptation, and continual learning prevents catastrophic forgetting during updates. Understanding on-device learning principles enables designing adaptive systems that balance personalization benefits with deployment constraints.

::: {.callout-tip title="Learning Objectives"}

- Understand on-device learning principles and how they differ from cloud-based training approaches

- Recognize benefits and limitations of on-device learning in resource-constrained environments

- Examine strategies to adapt models through complexity reduction, optimization, and data compression

- Understand related concepts including federated learning, transfer learning, and continual learning

- Analyze security implications of on-device learning and evaluate mitigation strategies

- Design on-device ML systems that balance personalization with computational constraints

:::

## Overview {#sec-ondevice-learning-overview-c195}

Machine learning systems have traditionally treated model training and model inference as distinct phases, often separated by both time and infrastructure. Training occurs in the cloud, leveraging large-scale compute clusters and curated datasets, while inference is performed downstream on deployed models, typically on user devices or edge servers. This separation is beginning to erode. Devices are being equipped not just to run inference, but to adapt, personalize, and improve models locally.

On-device learning refers to the process of training or adapting machine learning models directly on the device where they are deployed[^fn-a11-bionic-breakthrough]. : **On-Device Learning Evolution**: While edge inference emerged in the 2000s with mobile GPUs, on-device training didn't become feasible until Apple's A11 Bionic chip (2017) and Google's Pixel Visual Core (2017) provided sufficient TOPS for gradient computation. The shift from "smart inference" to "smart training" marked a core paradigm change in mobile AI.

[^fn-a11-bionic-breakthrough]: **A11 Bionic Breakthrough**: Apple's A11 Bionic (2017) was the first mobile chip with sufficient computational power for on-device training, delivering 0.6 TOPS compared to the previous A10's 0.2 TOPS. This 3x improvement, combined with 4.3 billion transistors and a dual-core Neural Engine, allowed gradient computation for the first time on mobile devices. Google's Pixel Visual Core achieved similar capabilities with 8 custom Image Processing Units optimized for machine learning workloads.

On-device learning builds upon the optimization techniques from @sec-model-optimizations and @sec-efficient-ai, applying extreme compression and efficiency strategies to enable training within device constraints. The hardware limitations discussed in @sec-ai-acceleration become primary design drivers, while the deployment and monitoring challenges from @sec-ml-operations extend to managing thousands of independently learning devices.

This capability opens the door to systems that can personalize models in response to user behavior, operate without cloud connectivity, and respect stringent privacy constraints by keeping data local. It also introduces a new set of challenges: devices have limited memory, computational power, and energy, as detailed in the hardware constraints covered in @sec-ai-acceleration. Training data is sparse, noisy, and non-independent across users, presenting data engineering challenges similar to those explored in @sec-data-engineering. These limitations necessitate fundamental rethinking of training algorithms, system architecture, and deployment strategies.

::: {.callout-definition title="Definition of On-Device Learning"}

**On-Device Learning** is the _local adaptation or training_ of machine learning models directly on deployed hardware devices, without reliance on continuous connectivity to centralized servers. It allows _personalization, privacy preservation, and autonomous operation_ by leveraging user-specific data collected in situ. On-device learning systems must operate under _tight constraints on compute, memory, energy, and data availability_, requiring specialized methods for model optimization, training efficiency, and data representation. As on-device learning matures, it increasingly incorporates _federated collaboration, lifelong adaptation, and secure execution_, expanding the frontier of intelligent edge computing.<!-- .-->

<!--: **Edge Computing Origins**: The term "edge computing" was coined by Akamai in the late 1990s for content delivery networks, but the modern concept emerged from Cisco's "fog computing" initiative in 2012. The realization that data was growing faster than network bandwidth could handle led to the "bring compute to the data" philosophy that defines edge AI today.-->

:::

This chapter explores the principles and systems design considerations underpinning on-device learning. It begins by examining the motivating applications that necessitate learning on the device, followed by a discussion of the unique hardware constraints introduced by embedded and mobile environments. The chapter then develops a taxonomy of strategies for adapting models, algorithms, and data pipelines to these constraints. : **Privacy Awakening**: The Cambridge Analytica scandal (2018) [@cadwalladr2018cambridge] and GDPR enforcement (2018) created a "privacy reckoning" that pushed tech companies toward on-device processing. Apple's "What happens on your iPhone, stays on your iPhone" campaign wasn't just marketing—it reflected a core shift in how companies thought about user data.

Particular emphasis is placed on distributed and collaborative methods, such as federated learning[^fn-federated-birth], which allow decentralized training without direct data sharing. The chapter concludes with an analysis of outstanding challenges, including issues related to reliability, system validation, and the heterogeneity of deployment environments.

[^fn-federated-birth]: **Federated Learning Birth**: Google's Brendan McMahan coined "federated learning" in 2016 [@mcmahan2017communication], but the concept emerged from their Gboard team's frustration with keyboard personalization. They realized they needed user-specific data to improve predictions, but couldn't collect keystrokes due to privacy concerns. This led to the "train where the data lives" philosophy that defined federated learning.

## Deployment Drivers {#sec-ondevice-learning-deployment-drivers-e37e}

Machine learning systems have traditionally relied on centralized training pipelines, where models are developed and refined using large, curated datasets and powerful cloud-based infrastructure [@dean2012large]. Once trained, these models are deployed to client devices for inference. While this separation has served most use cases well, it imposes limitations in settings where local data is dynamic, private, or personalized. On-device learning challenges this model by enabling systems to train or adapt directly on the device, without relying on constant connectivity to the cloud.

### On-Device Learning Benefits {#sec-ondevice-learning-ondevice-learning-benefits-37a9}

Traditional machine learning systems rely on a clear division of labor between model training and inference. Training is performed in centralized environments with access to high-performance compute resources and large-scale datasets. Once trained, models are distributed to client devices, where they operate in a static inference-only mode. While this centralized paradigm has been effective in many deployments, it introduces limitations where data is user-specific, behavior is dynamic, or connectivity is intermittent.

On-device learning refers to the capability of a deployed device to perform model adaptation using locally available data. This shift from centralized to decentralized learning is motivated by four key considerations: personalization, latency and availability, privacy, and infrastructure efficiency [@li2020federated].

Personalization is a primary motivation. Deployed models often encounter usage patterns and data distributions that differ substantially from their training environments. Local adaptation allows models to refine behavior in response to user-specific data—capturing linguistic preferences, physiological baselines, sensor characteristics, or environmental conditions. This is important in applications with high inter-user variability, where a single global model fails to serve all users equally well.

Latency and availability further justify local learning. In edge computing scenarios, connectivity to centralized infrastructure may be unreliable, delayed, or intentionally limited to preserve bandwidth or reduce energy usage. These deployment challenges mirror the operational considerations discussed in @sec-ml-operations. On-device learning allows autonomous improvement of models even in fully offline or delay-sensitive contexts, where round-trip updates to the cloud are infeasible.

Privacy is another important factor. Many applications involve sensitive or regulated data, including biometric measurements, typed input, location traces, or health information. Transmitting such data to the cloud introduces privacy risks and compliance burdens. Local learning mitigates these concerns by keeping raw data on the device and operating within privacy-preserving boundaries, potentially aiding adherence to regulations such as GDPR[^fn-gdpr-impact], HIPAA [@hipaa1996health], or region-specific data sovereignty laws.

[^fn-gdpr-impact]: **GDPR's ML Impact**: When GDPR took effect in May 2018 [@gdpr2016regulation], it made centralized ML training illegal for personal data without explicit consent. The "right to be forgotten" also meant models trained on personal data could be legally required to "unlearn" specific users—technically impossible with traditional training. This drove massive investment in privacy-preserving ML techniques.

Infrastructure efficiency also plays a role. Centralized training pipelines require substantial backend infrastructure to collect, store, and process user data. At scale, this introduces bottlenecks in bandwidth, compute capacity, and energy consumption, highlighting the importance of the efficient AI principles from @sec-efficient-ai. By shifting learning to the edge, systems can reduce communication costs and distribute training workloads across the deployment fleet, relieving pressure on centralized resources.

On-device learning is not always the optimal solution. In many production scenarios, simpler alternatives achieve comparable results with lower complexity. Feature engineering and caching suffice for personalization, storing user preferences and recent interactions to provide effective customization without model updates. Centralized models with user-specific lookup tables can handle many personalization needs through simple retrieval rather than adaptation. Cloud-based fine-tuning during off-peak hours, where user data is processed in batches with proper privacy controls, offers better accuracy than constrained on-device updates. The decision to implement on-device learning should be driven by clear requirements: true data privacy constraints (not just preferences), genuine network limitations (not occasional connectivity issues), quantitative latency budgets that preclude cloud round-trips, or demonstrable performance improvements that justify the operational complexity. For applications requiring camera processing under 33ms, voice response under 500ms, AR/VR motion-to-photon latency under 20ms, or safety-critical control under 10ms, network round-trip times (typically 50-200ms) make cloud-based alternatives architecturally infeasible regardless of other considerations. Teams should exhaust simpler solutions before committing to the significant engineering investment that on-device learning requires.

These motivations are grounded in the broader concept of knowledge transfer, where a pretrained model transfers useful representations to a new task or domain. As depicted in @fig-transfer-conceptual, knowledge transfer can occur between closely related tasks (e.g., playing different board games or musical instruments), or across domains that share structure (e.g., from riding a bicycle to driving a scooter). In the context of on-device learning, this means leveraging a model pretrained in the cloud and adapting it efficiently to a new context using only local data and limited updates. The figure highlights the key idea: pretrained knowledge allows fast adaptation without relearning from scratch, even when the new task diverges in input modality or goal.

![**Knowledge Transfer**: Pretrained models accelerate learning on new tasks by leveraging existing representations, as seen by adapting skills between related board games or musical instruments. This transfer extends across domains like bicycle riding and scooter operation, where shared underlying structures allow efficient adaptation with limited new data.](images/png/ondevice_transfer_learning_apps.png){#fig-transfer-conceptual}

This conceptual shift, which is allowed by transfer learning and adaptation, is important for real-world on-device applications. Whether adapting a language model for personal typing preferences, adjusting gesture recognition to an individual's movement patterns, or recalibrating a sensor model in a changing environment, on-device learning allows systems to remain responsive, efficient, and user-aligned over time.

### Application Domains {#sec-ondevice-learning-application-domains-7ae7}

The motivations for on-device learning are most clearly illustrated by examining the application domains where its benefits are both tangible and necessary. These domains span consumer technologies, healthcare, industrial systems, and embedded applications, each presenting scenarios where local adaptation is preferable, or even required, for effective machine learning deployment.

Mobile input prediction is a mature example of on-device learning in action. In systems such as smartphone keyboards, predictive text and autocorrect features benefit substantially from continuous local adaptation. User typing patterns are personalized and evolve dynamically, making centralized static models insufficient. On-device learning allows language models to finetune their predictions directly on the device, without transmitting keystroke data to external servers. This approach not only supports personalization but also aligns with privacy-preserving design principles.

For instance, Google's Gboard employs federated learning to improve shared models across a large population of users while keeping raw data local to each device [@hard2018federated]. : **Gboard as Federated Pioneer**: Gboard became the first major commercial federated learning deployment in 2017, processing updates from over 1 billion devices. The technical challenge was immense: aggregating model updates while ensuring no individual user's typing patterns could be inferred. Google's success with Gboard proved federated learning could work at planetary scale.

As shown in @fig-ondevice-gboard, different prediction strategies illustrate how local adaptation operates in real time: next-word prediction (NWP) suggests likely continuations based on prior text, while Smart Compose uses on-the-fly rescoring to offer dynamic completions, demonstrating the sophistication of local inference mechanisms.

![**On-Device Prediction Strategies**: Gboard employs both next-word prediction and smart compose with on-the-fly rescoring to adapt to user typing patterns locally, enhancing personalization and preserving privacy. These techniques demonstrate how machine learning models can refine predictions in real time without transmitting data to a central server, enabling efficient and private mobile input experiences.](images/png/ondevice_gboard_example.png){#fig-ondevice-gboard}

Wearable and health monitoring devices present strong use cases. These systems rely on real time data from accelerometers, heart rate sensors, and electrodermal activity monitors. Physiological baselines vary between individuals. On-device learning allows models to adapt to these baselines over time, improving the accuracy of activity recognition, stress detection, and sleep staging. In regulated healthcare environments, patient data must remain localized due to privacy laws, reinforcing the need for edge-local adaptation.

Wake-word detection and voice interfaces illustrate another important scenario. Devices such as smart speakers and earbuds must recognize voice commands quickly and accurately, even in noisy or dynamic acoustic environments. These systems face strict latency requirements: voice interfaces must maintain end-to-end response times under 500ms to preserve natural conversation flow, with wake-word detection requiring sub-100ms response times to avoid user frustration. Local training allows models to adapt to the user's voice profile and ambient context, reducing false positives and missed detections while meeting these demanding performance constraints. This kind of adaptation is valuable in far-field audio settings, where microphone configurations and room acoustics vary across deployments.

Industrial IoT and remote monitoring systems also benefit from local learning capabilities. In applications such as agricultural sensing, pipeline monitoring, or environmental surveillance, connectivity to centralized infrastructure may be limited or costly. On-device learning allows these systems to detect anomalies, adjust thresholds, or adapt to seasonal trends without continuous communication with the cloud. This capability is important for maintaining autonomy and reliability in edge-deployed sensor networks.

Embedded computer vision systems, including those in robotics, AR/VR, and smart cameras, present additional opportunities with stringent timing constraints. Camera applications must process frames within 33ms to maintain 30 FPS real-time performance, while AR/VR systems demand motion-to-photon latencies under 20ms to prevent nausea and maintain immersion. Safety-critical control systems require even tighter bounds, typically under 10ms, where delayed decisions can have severe consequences. These systems operate in novel or evolving environments that differ from training conditions. On-device adaptation allows models to recalibrate to new lighting conditions, object appearances, or motion patterns while meeting these critical latency budgets that drive the fundamental decision between on-device versus cloud-based processing.

Each of these domains highlights a common pattern: the deployment environment introduces variation or uncertainty that cannot be fully anticipated during centralized training. On-device learning offers a mechanism for adapting models in place, enabling systems to improve continuously in response to local conditions. These examples also reveal a important design requirement: learning must be performed efficiently, privately, and reliably under significant resource constraints. The following section formalizes these constraints and outlines the system-level considerations that shape the design of on-device learning solutions.

### Training Paradigms {#sec-ondevice-learning-training-paradigms-170f}

Most machine learning systems today follow a centralized learning paradigm. Models are trained in data centers using large-scale, curated datasets aggregated from many sources, as described in the traditional training workflows of @sec-ai-training. Once trained, these models are deployed to client devices in a static form, where they perform inference without further modification. Updates to model parameters, either to incorporate new data or to improve generalization, are handled periodically through offline retraining, often using newly collected or labeled data sent back from the field.

This centralized model of learning offers numerous advantages: high-performance computing infrastructure, access to diverse data distributions, and robust debugging and validation pipelines detailed in @sec-ml-operations. It also depends on reliable data transfer, trust in data custodianship, and infrastructure capable of managing global updates across a fleet of devices. As machine learning is deployed into increasingly diverse and distributed environments, the limitations of this approach become more apparent.

In contrast, on-device learning is inherently decentralized. Each device maintains its own copy of a model and adapts it locally using data that is typically unavailable to centralized infrastructure. Training occurs on-device, often asynchronously and under varying resource conditions. Data never leaves the device, reducing exposure but also complicating coordination. Devices may differ in their hardware capabilities, runtime environments, and patterns of use, making the learning process heterogeneous and difficult to standardize. These hardware variations are explored in depth in @sec-ai-acceleration.

This decentralized nature introduces unique systems challenges. Devices may operate with different versions of the model, leading to inconsistencies in behavior. Evaluation and validation become more complex, as there is no central point from which to measure performance [@mcmahan2017communication]. Model updates must be carefully managed to prevent degradation, and safety guarantees become harder to enforce in the absence of centralized testing.

The operational reality of managing thousands of heterogeneous edge devices exceeds typical distributed systems complexity. Device heterogeneity extends beyond hardware differences to encompass varying operating system versions, security patches, network configurations, and power management policies. Update orchestration faces fundamental challenges: at any given time, 20-40% of devices are offline [@bonawitz2019towards], while others have been disconnected for weeks or months. When these devices reconnect, they require state reconciliation to avoid version conflicts. Update verification becomes critical as devices can silently fail to apply updates or report success while running outdated models. Robust systems implement multi-stage verification: cryptographic signatures confirm update integrity, functional tests validate model behavior, and telemetry confirms deployment success. Rollback strategies must handle partial deployments where some devices received updates while others remain on previous versions, requiring orchestration to maintain system consistency during failure recovery.

At the same time, decentralization introduces opportunities. It allows for personalization without centralized oversight, supports learning in disconnected or bandwidth-limited environments, and reduces the cost of infrastructure for model updates. It also raises important questions of how to coordinate learning across devices, whether through periodic synchronization, federated aggregation, or hybrid approaches that combine local and global objectives.

The move from centralized to decentralized learning represents more than a shift in deployment architecture; it fundamentally reshapes the design space for machine learning systems. In centralized training, data is aggregated from many sources and processed in large-scale data centers, where models are trained, validated, and then deployed in a static form to edge devices. In contrast, on-device learning introduces a decentralized paradigm: models are updated directly on client devices using local data, often asynchronously and under diverse hardware conditions. This change reduces reliance on cloud infrastructure while enhancing personalization and privacy, introducing new coordination and validation challenges.

On-device learning emerges as a response to the limitations of centralized machine learning workflows. As illustrated in @fig-centralized-vs-decentralized, the traditional paradigm (A) involves training a model on aggregated cloud-based data before pushing it to client devices for static inference. This architecture works well when centralized data collection is feasible, network connectivity is reliable, and model generalization across users is sufficient. It falls short in scenarios where data is personalized, privacy-sensitive, or collected in environments with limited connectivity.

In contrast, once the model is deployed, local differences begin to emerge. Region B depicts the process by which each device collects its own data stream, which is often non-IID[^fn-non-iid] and noisy, and adapts the model to better reflect its specific operating context. This marks the shift from global generalization to local specialization, highlighting the autonomy and variability introduced by decentralized learning.

[^fn-non-iid]: **Non-IID (Non-Independent and Identically Distributed)**: In machine learning, data is IID when samples are drawn independently from the same distribution. Non-IID violates this assumption, common in federated learning where each device collects data from different users, environments, or use cases. For example, smartphone keyboard data varies dramatically between users (languages, writing styles, autocorrect needs), making personalized model training essential but challenging for convergence.

@fig-centralized-vs-decentralized illustrates this shift. In region A, centralized learning begins with cloud-based training on aggregated data, followed by deployment to client devices. Region B marks the transition to local learning: devices begin collecting data, which is frequently non-IID, noisy, and unlabeled, and adapting their models based on individual usage patterns. Finally, region C depicts federated learning, in which client updates are periodically synchronized via aggregated model updates rather than raw data transfer, enabling privacy-preserving global refinement.

This shift from centralized training to decentralized, adaptive learning reshapes how ML systems are designed and deployed. It allows learning in settings where connectivity is intermittent, data is user-specific, and personalization is important, while introducing new challenges in update coordination, evaluation, and system robustness.

::: {#fig-centralized-vs-decentralized fig-env="figure" fig-pos="htb"}
```{.tikz}
\begin{tikzpicture}[line join=round,font=\usefont{T1}{phv}{m}{n}\small]
\tikzset{
 LineD/.style={line width=1.85pt,violet!50,text=black,dashed,dash pattern=on 5pt off 3pt,
-{Triangle[width=1.8*6pt,length=2.2*6pt]}},
Line/.style={line width=1.85pt,violet!50,text=black,-{Triangle[width=1.8*6pt,length=2.2*6pt]}},
SArrow/.style={red,line width=4pt,-{Triangle[width=1.8*6pt,length=2.2*6pt]}},
circleB/.style={circle,fill=red,minimum size=7mm},
BoxSTAR/.style={star, minimum width=10mm, star points=5, star point ratio=2.5, fill=blue, draw},
BoxG/.style={rectangle,fill=green!70!black!90,minimum size=7mm},
BoxB/.style={rectangle,fill=orange,rounded rectangle,minimum width=15mm,minimum height=5mm},
BoxR/.style={rectangle,fill=OliveLine,minimum width=11mm,minimum height=5mm},
pics/cloud/.style = {
        code = {
\colorlet{red}{BrownLine}
\begin{scope}[local bounding box=CLO,scale=1.8, every node/.append style={transform shape}]
\draw[red,line width=1.25pt,fill=yellow!10](0,0)to[out=170,in=180,distance=11](0.1,0.61)
to[out=90,in=105,distance=17](1.07,0.71)
to[out=20,in=75,distance=7](1.48,0.36)
to[out=350,in=0,distance=7](1.48,0)--(0,0);
\end{scope}
}
}
}
\tikzset {
pics/mobile/.style = {
        code = {
        \pgfkeys{/channel/.cd, #1}
\colorlet{red}{BrownLine}
\begin{scope}[local bounding box=MOB,scale=\scalefac, every node/.append style={transform shape}]
\node[rectangle,draw=red,minimum height=94,minimum width=57,
            rounded corners=6,thick,fill=red!10](\picname-R1){};
\node[rectangle,draw=red,minimum height=67,minimum width=44,thick,fill=white](\picname-R2){};
\node[circle,minimum size=8,below= 2pt of \picname-R2,inner sep=0pt,thick,fill=red!90]{};
\node[rectangle,fill=red,minimum height=2,minimum width=20,above= 4pt of \picname-R2,inner sep=0pt,thick]{};
%
 \end{scope}
     }
  }
}

\tikzset {
pics/handmobile/.style = {
        code = {
        \pgfkeys{/channel/.cd, #1}
\colorlet{red}{BrownLine}
\begin{scope}[local bounding box=MOB,scale=\scalefac, every node/.append style={transform shape}]
\node[rectangle,draw=red,minimum height=94,minimum width=57,
            rounded corners=6,thick,fill=red!10](\picname-R1){};
\node[rectangle,draw=red,minimum height=67,minimum width=44,thick,fill=white](\picname-R2){};
\node[circle,minimum size=8,below= 2pt of \picname-R2,inner sep=0pt,thick,fill=red!90]{};
\node[rectangle,fill=red,minimum height=2,minimum width=20,above= 4pt of \picname-R2,inner sep=0pt,thick]{};
%
%hand
\draw[thick,fill=orange!20](-1.62,-1.59)coordinate(D)--(-1.01,-2.12)to[out=320,in=190] (-0.03,-1.65)--++(180:0.3)
to[out=170,in=270] (-0.83,-1.0)--(-0.82,-0.25)to[out=50,in=50,distance=13] (-1.14,0.6)
to[out=230,in=70] (-1.29,-0.1)to[out=250,in=70] (-1.55,-0.71)to[out=250,in=150] (D);

\node[rectangle,draw,minimum height=12,minimum width=23,thick,
            rounded corners=4,rotate=35,fill=orange!20]at(0.96,-0.03)(PR2){};
\node[rectangle,draw,minimum height=12,minimum width=23,thick,
            rounded corners=4,rotate=35,fill=orange!20]at(0.96,-0.56)(PR3){};
\node[rectangle,draw,minimum height=12,minimum width=19,thick,
            rounded corners=4,rotate=35,fill=orange!20]at(0.96,-1.09)(PR4){};
\draw[thick,fill=orange!20](1.0,0.80)--(1.13,0.80)to[out=355,in=5,distance=9] (1.13,0.36)--(1.0,0.36)--cycle;
 \end{scope}
     }
  }
}

\tikzset{
channel/.pic={
\pgfkeys{/channel/.cd, #1}
\begin{scope}[local bounding box=MOB,scale=\scalefac, every node/.append style={transform shape}]
\node[rectangle,draw=\drawchannelcolor,line width=1pt,fill=\channelcolor!10,
minimum height=20mm,minimum width=20mm](\picname){};
\end{scope}
        }
}
\pgfkeys{
  /channel/.cd,
  channelcolor/.store in=\channelcolor,
  drawchannelcolor/.store in=\drawchannelcolor,
  scalefac/.store in=\scalefac,
  Linewidth/.store in=\Linewidth,
  picname/.store in=\picname,
  channelcolor=BrownLine,
  drawchannelcolor=BrownLine,
  scalefac=1,
  Linewidth=0.6pt,
  picname=C
}
\begin{scope}[local bounding box=MOBILE1,shift={($(0,0)+(0,0)$)},
scale=1, every node/.append style={transform shape}]

 \foreach \i/\sf in {1/0.6,2/0.7,3/0.8,4/0.9,5/1} {
  \pgfmathsetmacro{\y}{(1.5-\i)*0.83 + 0.7}
 \pic[shift={(0,0)}] at  ({-\i*0.2}, {0.4*\i}){mobile={scalefac=\sf,picname=1\i}};
  }
\end{scope}

\begin{scope}[local bounding box=MOBILE2,shift={($(MOBILE1)+(3.25,-1.52)$)},
scale=1, every node/.append style={transform shape}]

 \foreach \i/\sf in {1/0.6,2/0.7,3/0.8,4/0.9,5/1} {
  \pgfmathsetmacro{\y}{(1.5-\i)*0.83 + 0.7}
 \pic[shift={(0,0)}] at  ({-\i*0.17}, {0.4*\i}){mobile={scalefac=\sf,picname=2\i}};
  }
\end{scope}
\begin{scope}[local bounding box=MOBILE3,shift={($(MOBILE2)+(2.5,-1.52)$)},
scale=1, every node/.append style={transform shape}]

 \foreach \i/\sf in {1/0.6,2/0.7,3/0.8,4/0.9,5/1} {
  \pgfmathsetmacro{\y}{(1.5-\i)*0.83 + 0.7}
 \pic[shift={(0,0)}] at  ({-\i*0.02}, {0.4*\i}){mobile={scalefac=\sf,picname=3\i}};
  }
\end{scope}
\begin{scope}[local bounding box=MOBILE4,shift={($(MOBILE3)+(3.2,-1.52)$)},
scale=1, every node/.append style={transform shape}]

 \foreach \i/\sf in {1/0.6,2/0.7,3/0.8,4/0.9,5/1} {
  \pgfmathsetmacro{\y}{(1.5-\i)*0.83 + 0.7}
 \pic[shift={(0,0)}] at  ({-\i*0.17}, {0.4*\i}){mobile={scalefac=\sf,picname=4\i}};
  }
\end{scope}
\begin{scope}[local bounding box=MOBILE5,shift={($(MOBILE4)+(3.2,-1.52)$)},
scale=1, every node/.append style={transform shape}]

 \foreach \i/\sf in {1/0.6,2/0.7,3/0.8,4/0.9,5/1} {
  \pgfmathsetmacro{\y}{(1.5-\i)*0.83 + 0.7}
 \pic[shift={(0,0)}] at  ({-\i*0.17}, {0.4*\i}){mobile={scalefac=\sf,picname=5\i}};
  }
\end{scope}
\foreach \i in {1,2,3,4,5}{
\node[circleB](CI\i)at(\i5-R2){};
}
%%%
\begin{scope}[local bounding box=HMOBILE,shift={($(MOBILE1)+(-5,0.5)$)},
scale=1, every node/.append style={transform shape}]
\pic[shift={(0,0)}] at  (0,0) {handmobile={scalefac=1,picname=H}};
\node[circleB](CIA)at(H-R2){};
\end{scope}
%%%squars
\begin{scope}[local bounding box=CHANNELS,shift={($(MOBILE1)+(0.2,-1.5)$)}]
\begin{scope}[local bounding box=CHANEL1,shift={($(0,0)+(1.5,-3.5)$)}]
\foreach \i/\sf in {1/0.3,2/0.4,3/0.5,4/0.6,5/0.7,6/0.8,7/0.9,8/1} {
\pic at ({-\i*0.23}, {-0.23*\i}) {channel={scalefac=\sf,picname=\i-CH1,channelcolor=BrownLine}};
}
\end{scope}
\begin{scope}[local bounding box=CHANEL2,shift={($(CHANEL1)+(4.3,1.38)$)}]
\foreach \i/\sf in {7/0.9,8/1} {
\pic at ({-\i*0.23}, {-0.23*\i}) {channel={scalefac=\sf,picname=\i-CH2,channelcolor=BrownLine}};
}
\end{scope}
\begin{scope}[local bounding box=CHANEL3,shift={($(CHANEL2)+(4.3,1.77)$)}]
\foreach \i/\sf in {8/1} {
\pic at ({-\i*0.23}, {-0.23*\i}) {channel={scalefac=\sf,picname=\i-CH3,channelcolor=BrownLine}};
}
\end{scope}
\begin{scope}[local bounding box=CHANEL4,shift={($(CHANEL3)+(1.0,1.83)$)}]
\foreach \i/\sf in {3/0.5,4/0.6,5/0.7,6/0.8,7/0.9,8/1} {
\pic at ({\i*0.23}, {-0.23*\i}) {channel={scalefac=\sf,picname=\i-CH4,channelcolor=BrownLine}};
}
\end{scope}
\begin{scope}[local bounding box=CHANEL5,shift={($(CHANEL4)+(1.2,1.52)$)}]
\foreach \i/\sf in {5/0.7,6/0.8,7/0.9,8/1} {
\pic at ({\i*0.23}, {-0.23*\i}) {channel={scalefac=\sf,picname=\i-CH5,channelcolor=BrownLine}};
}
\end{scope}
\end{scope}

\begin{scope}[local bounding box=CHANEL6,shift={($(MOBILE5)+(7.7,2.35)$)},]
\foreach \i/\sf in {8/1} {
\pic at ({-\i*0.23}, {-0.23*\i}) {channel={scalefac=\sf,picname=\i-CH6,channelcolor=BrownLine}};
}
\end{scope}
%
\path[red](CHANEL6)|-coordinate(DD)(8-CH5);
\path[red](HMOBILE)|-coordinate(DL)(8-CH1);
%
\pic at (DL) {channel={scalefac=1,picname=DL-CH,channelcolor=BrownLine}};
\pic at ($(MOBILE3)+(0,5)$) {channel={scalefac=1,picname=GO-CH,channelcolor=BrownLine}};
%
\begin{scope}[local bounding box=CLOUD1,shift={($(DD)+(-1.25,-0.5)$)},
scale=1.2, every node/.append style={transform shape}]
\pic[shift={(0,0)}] at (0,0) {cloud};
\end{scope}
%
\node[circleB](CI)at(GO-CH){};
\node[BoxG]at(DL-CH){};
\node[BoxG,rotate=45]at(CHANEL6){};
\node[BoxG,rotate=45]at($(CLOUD1)+(0,-0.4)$){};
%
\node[BoxG]at(8-CH1){};
\node[BoxG,rotate=45]at(8-CH2){};
\node[BoxSTAR]at(8-CH3){};
\node[BoxG,rotate=45,fill=violet]at(8-CH4){};
\node[BoxB]at(8-CH5){};
%
\begin{scope}[local bounding box=CIRCLE,shift={($(HMOBILE)+(0.15,2.5)$)}]
\def\ra{22mm}
\draw [SArrow](70:\ra)  arc (70:12:\ra);
\draw [SArrow](170:\ra)  arc (170:110:\ra);
\draw [SArrow](230:\ra)  arc (230:190:\ra);
\draw [red,dashed,line width=2pt](320:\ra)  arc (320:345:\ra);
\node[BoxG]at(0:\ra){};
\node[BoxB]at(180:\ra){};
\node[BoxR]at(90:\ra){};
\end{scope}
\draw[LineD,shorten >=4pt](HMOBILE.south)--(DL-CH);
\draw[LineD,shorten <=4pt,shorten >=4pt](DL-CH)--(8-CH1);
\draw[Line,shorten <=4pt,shorten >=4pt](8-CH5)--++(3,0);
\path[Line,shorten <=4pt,shorten >=4pt](8-CH6)--++(0,-5.5)coordinate(CL4);
\draw[Line,shorten <=4pt,shorten >=4pt](8-CH6)--(CL4);
\draw[Line,shorten <=4pt,shorten >=4pt](8-CH6)--++(-4.5,0);
\draw[Line,shorten <=4pt](GO-CH)--++(0,-2.7);
\draw[LineD](CI1.center)--++(-3.2,0);
\draw[Line]($(31-R1)+(0,-1.2)$)coordinate(DO)--++(0,-2.5);
\draw[Line]($(DO)+(-1.3,0)$)--++(250:2.5);
\draw[Line]($(DO)+(1.3,0)$)--++(290:2.5);
%
\node[above=8pt of CHANEL6]{\huge C.};
\node[below=8pt of 8-CH3]{\huge B.};
\node[left=38pt of CIA]{\huge A.};
\end{tikzpicture}

```
**Decentralized vs. Centralized Learning**: On-device learning shifts model training from aggregated cloud data to individual devices, enabling personalization and reducing reliance on network connectivity. This paradigm contrasts with centralized training, where a single global model is deployed to all devices after cloud-based optimization.
:::

## Design Constraints {#sec-ondevice-learning-design-constraints-c776}

Enabling learning on the device requires rethinking conventional assumptions about where and how machine learning systems operate. In centralized environments, models are trained with access to extensive compute infrastructure, large and curated datasets, and generous memory and energy budgets. At the edge, none of these assumptions hold. Instead, on-device learning must navigate a constrained design space shaped by the structure of the model, the nature of the available data, and the computational capabilities of the deployment platform.

These three dimensions, the model, the data, and the computational resources, form the foundation of any on-device learning system. Each imposes distinct limitations that influence algorithmic design and system architecture. The model must be compact enough to fit within memory and storage bounds, yet expressive enough to support adaptation. The data is local, often sparse, unlabeled, and non-IID, requiring robust and efficient learning procedures. The compute environment is resource-constrained, often lacking support for floating-point operations or backpropagation primitives. These constraints are not merely technical; they reflect the realities of deploying machine learning systems in the wild. Devices may be battery-powered, have limited connectivity, and operate in unpredictable environments. They may also be heterogeneous, with different hardware capabilities and software stacks. As a result, on-device learning must be designed to accommodate these variations while still delivering reliable performance.

@fig-ondevice-pretraining illustrates a pipeline that combines offline pre-training with online adaptive learning on resource-constrained IoT devices. The system first undergoes meta-training with generic data. During deployment, device-specific constraints such as data availability, compute, and memory shape the adaptation strategy by ranking and selecting layers and channels to update. This allows efficient on-device learning within limited resource envelopes.

::: {#fig-ondevice-pretraining fig-env="figure" fig-pos="htb"}
```{.tikz}
\begin{tikzpicture}[line join=round,font=\usefont{T1}{phv}{m}{n}\small]
\tikzset{
 Line/.style={line width=0.35pt,black!50,text=black,-latex},
 LineD/.style={line width=0.75pt,black!50,text=black,dashed,dash pattern=on 5pt off 3pt},
   Box/.style={inner xsep=2pt,
   node distance=0.45,
    draw=BlueLine,
    line width=0.75pt,
    fill=BlueL,
    minimum width=15mm, minimum height=7mm
  },
 circles/.pic={
\pgfkeys{/channel/.cd, #1}
\node[circle,draw=\channelcolor,line width=\Linewidth,fill=\channelcolor!10,
minimum size=6.5mm](\picname){};
        }
}
\pgfkeys{
  /channel/.cd,
  channelcolor/.store in=\channelcolor,
  drawchannelcolor/.store in=\drawchannelcolor,
  scalefac/.store in=\scalefac,
  Linewidth/.store in=\Linewidth,
  picname/.store in=\picname,
  channelcolor=BrownLine,
  drawchannelcolor=BrownLine,
  scalefac=1,
  Linewidth=0.6pt,
  picname=C
}

\begin{scope}[local bounding box=CIRCLE1,shift={($(0,0)+(0,0)$)},
scale=1, every node/.append style={transform shape}]
%1 column
\foreach \j in {1,2,3} {
  \pgfmathsetmacro{\y}{(1.5-\j)*0.83 + 0.7}
  \pic at (-1.35,\y) {circles={channelcolor=BrownLine,picname=1CD\j}};
}
%2 column
\foreach \i in {1,...,4} {
  \pgfmathsetmacro{\y}{(2-\i)*0.83+0.7}
  \pic at (0,\y) {circles={channelcolor=BrownLine, picname=2CD\i}};
}
%3 column
\foreach \j in {1,2} {
  \pgfmathsetmacro{\y}{(1-\j)*0.83 + 0.7}
  \pic at (1.35,\y) {circles={channelcolor=BrownLine,picname=3CD\j}};
}
\foreach \i in {1,2,3}{
  \foreach \j in {1,2,3,4}{
\draw[Line](1CD\i)--(2CD\j);
}}
\foreach \i in {1,2,3,4}{
  \foreach \j in {1,2}{
\draw[Line](2CD\i)--(3CD\j);
}}
\end{scope}

\begin{scope}[local bounding box=CIRCLE2,shift={($(CIRCLE1.east)+(4.0,-0.3)$)},
scale=1, every node/.append style={transform shape}]
%1 column
\foreach \j/\col in {1/RedLine,2/RedLine!40!,3/RedLine} {
  \pgfmathsetmacro{\y}{(1.5-\j)*0.83 + 0.7}
  \pic at (-1.35,\y) {circles={channelcolor=\col,picname=1CD\j}};
}
%2 column
\foreach \i/\col in {1/RedLine,2/BrownLine,3/BrownLine,4/RedLine!40!} {
  \pgfmathsetmacro{\y}{(2-\i)*0.83+0.7}
  \pic at (0,\y) {circles={channelcolor=\col, picname=2CD\i}};
}
%3 column
\foreach \j/\col in {1/RedLine,2/BrownLine} {
  \pgfmathsetmacro{\y}{(1-\j)*0.83 + 0.7}
  \pic at (1.35,\y) {circles={channelcolor=\col,picname=3CD\j}};
}
\foreach \i in {1,2,3}{
  \foreach \j in {1,2,3,4}{
\draw[Line](1CD\i)--(2CD\j);
}}
\foreach \i in {1,2,3,4}{
  \foreach \j in {1,2}{
\draw[Line](2CD\i)--(3CD\j);
}}
\end{scope}

\begin{scope}[local bounding box=CIRCLE3,shift={($(CIRCLE2.east)+(3.2,-0.3)$)},
scale=1, every node/.append style={transform shape}]
%1 column
\foreach \j/\col in {1/green!60!black!90!,2/green!60!black!90!,3/green!60!black!90!} {
  \pgfmathsetmacro{\y}{(1.5-\j)*0.83 + 0.7}
  \pic at (-1.35,\y) {circles={channelcolor=\col,picname=1CD\j}};
}
%2 column
\foreach \i/\col in {1/green!60!black!90!,2/BrownLine,3/BrownLine,4/green!60!black!90!} {
  \pgfmathsetmacro{\y}{(2-\i)*0.83+0.7}
  \pic at (0,\y) {circles={channelcolor=\col, picname=2CD\i}};
}
%3 column
\foreach \j/\col in {1/green!60!black!90!,2/BrownLine} {
  \pgfmathsetmacro{\y}{(1-\j)*0.83 + 0.7}
  \pic at (1.35,\y) {circles={channelcolor=\col,picname=3CD\j}};
}
\foreach \i in {1,2,3}{
  \foreach \j in {1,2,3,4}{
\draw[Line](1CD\i)--(2CD\j);
}}
\foreach \i in {1,2,3,4}{
  \foreach \j in {1,2}{
\draw[Line](2CD\i)--(3CD\j);
}}
\end{scope}
%\node[single arrow,fill=red!80!black!90, inner ysep=2pt,
    %  minimum width = 12pt, single arrow head extend=2pt,
      %minimum height=12mm] at($(CIRCLE2.east)!0.5!(CIRCLE3.west)$){};
\draw[brown!70,line width=6pt,shorten <=4pt,shorten >=4pt,
-{Triangle[width=1.8*6pt,length=0.9*6pt]}](CIRCLE2.east)
-- coordinate(SR1) (CIRCLE3.west);
\draw[brown!70,line width=6pt,shorten <=4pt,shorten >=4pt,
-{Triangle[width=1.8*6pt,length=0.9*6pt]}](CIRCLE1.east)
--coordinate(SR) (CIRCLE2.west);
\draw[brown!70,line width=6pt,shorten <=4pt,shorten >=4pt,
{Triangle[width=1.8*6pt,length=0.9*6pt]}-](CIRCLE1.west)
--node[above=6pt,align=center,text=black](PTB){Pre-trained\\ backbone}++(-2,0);

\begin{scope}[local bounding box=BOXX,shift={($(CIRCLE2.north)+(-1.1,1.5)$)}]
\node[Box](2B1)at(0,0){Data};
\node[Box,right=of 2B1](2B2){Compute};
\node[Box,right=of 2B2](2B3){Memory};
\node[draw=BackLine,inner xsep=3mm,line width=0.75pt,inner ysep=5mm,
fill=none,yshift=-2mm,fit=(2B1)(2B3)](BB){};
\node[below=1pt of BB.south,anchor=south]{Device Specific};
\end{scope}
\draw[brown!70,line width=6pt,shorten <=4pt,shorten >=4pt,
-{Triangle[width=1.8*6pt,length=0.9*6pt]}](BB.west)-|
node[right=2pt,pos=0.85,text=black]{$S_i$}(SR);
\draw[brown!70,line width=6pt,shorten <=4pt,shorten >=4pt,
{Triangle[width=1.8*6pt,length=0.9*6pt]}-](SR1)--++(0,2);
%%
\node[below=4pt of CIRCLE1,align=center](T1){Meta-training with generic\\
     data (e.g. MiniImageNet)};
\node[below=4pt of CIRCLE2,align=center](T2){Rank the channels and layers based\\
     on the multi-objective metric $S_i$};
\node[below=4pt of CIRCLE3,align=center](T3){Train the selected\\ layers and channels};
%
\scoped[on background layer]
\node[LineD,draw=BrownLine,inner xsep=5mm,inner ysep=5mm,minimum height=74mm,
fill=BrownL!10,yshift=4mm,fit=(T2)(BB)(T3)](BB2){};
\node[below=1pt of BB2.north,anchor=north]{Online Adaptive Learning on loT Devices};
\scoped[on background layer]
\node[LineD,draw=BackLine,inner xsep=3mm,inner ysep=5mm,
minimum height=74mm,
fill=yellow!05,yshift=14.7mm,fit=(T1)(PTB)(CIRCLE1)](BB2){};
\node[below=1pt of BB2.north,anchor=north]{Online Pre-training};
\end{tikzpicture}
```
**On-Device Adaptation Pipeline**: Resource-constrained devices use a two-stage learning process: offline pre-training establishes initial model weights, followed by online adaptation that selectively updates layers based on available data, compute, and memory. This approach balances model performance with the practical limitations of edge deployment, enabling continuous learning in real-world environments.
:::

### Model Constraints {#sec-ondevice-learning-model-constraints-9232}

The structure and size of the machine learning model directly influence the feasibility of on-device training. Unlike cloud-deployed models that can span billions of parameters and rely on multi-gigabyte memory budgets, models intended for on-device learning must conform to tight constraints on memory, storage, and computational complexity. These constraints apply not only at inference time, but also during training, where additional resources are needed for gradient computation, parameter updates, and optimizer state.

For example, the MobileNetV2 architecture, commonly used in mobile vision tasks, requires approximately 14 MB of storage in its standard configuration. While this is feasible for modern smartphones, it far exceeds the memory available on embedded microcontrollers such as the Arduino Nano 33 BLE Sense[^fn-arduino-constraints], which provides only 256 KB of SRAM and 1 MB of flash storage. This constraint necessitates the model compression techniques detailed in @sec-model-optimizations. In such platforms, even a single layer of a typical convolutional neural network may exceed available RAM during training due to the need to store intermediate feature maps.

[^fn-arduino-constraints]: **Arduino Edge Computing Reality**: The Arduino Nano 33 BLE Sense represents typical microcontroller constraints—256KB SRAM is roughly 65,000 times smaller than a modern smartphone's 16GB RAM. To put this in perspective, storing just one 224×224×3 RGB image (150KB) would consume 60% of available memory. Training requires 3-5x more memory for gradients and activations, making even tiny models challenging. The 1MB flash storage can hold only the smallest quantized models, forcing designers to use 8-bit or even 4-bit representations.

In addition to storage constraints, the training process itself expands the effective memory footprint. Standard backpropagation requires caching activations for each layer during the forward pass, which are then reused during gradient computation in the backward pass. For a 10-layer convolutional model processing $64 \times 64$ images, the required memory may exceed 1–2 MB—well beyond the SRAM capacity of most embedded systems.

Model complexity also affects runtime energy consumption and thermal limits. In systems such as smartwatches or battery-powered wearables, sustained model training can deplete energy reserves or trigger thermal throttling. Training a full model using floating-point operations on these devices is often infeasible. This limitation has motivated the development of ultra-lightweight model variants, such as MLPerf Tiny benchmark networks [@banbury2021mlperf], which fit within 100–200 KB and can be adapted using only partial gradient updates. These models typically employ the quantization and pruning strategies from @sec-model-optimizations to achieve such compact representations.

The practical implications of battery and thermal constraints extend beyond just limiting training duration. Mobile devices must carefully balance training opportunities with user experience—aggressive on-device training can cause noticeable device heating and rapid battery drain, leading to user dissatisfaction and potential app uninstalls. Modern smartphones typically limit sustained workloads to 2-3W to prevent thermal discomfort, while training even modest models can easily exceed 5W power draw. This reality necessitates intelligent scheduling strategies: training during charging periods when thermal dissipation is improved, utilizing low-power cores for gradient computation when possible, and implementing thermal-aware duty cycling that pauses training when temperature thresholds are exceeded. Some systems even leverage device usage patterns, scheduling intensive adaptation only during overnight charging when the device is idle and connected to power.

The model architecture itself must also be designed with on-device learning in mind. Many conventional architectures, such as transformers or large convolutional networks, are not well-suited for on-device adaptation due to their size and complexity. Instead, lightweight architectures such as MobileNets[^fn-mobilenet-innovation], SqueezeNet [@iandola2016squeezenet], and EfficientNet [@tan2019efficientnet] have been developed specifically for resource-constrained environments. These architectures leverage the efficiency principles and architectural optimizations detailed in @sec-model-optimizations. These models use techniques such as depthwise separable convolutions[^fn-depthwise-separable], bottleneck layers, and quantization to reduce memory and compute requirements while maintaining performance. The quantization techniques are extensively covered in @sec-model-optimizations, while the architectural design principles follow from @sec-efficient-ai.

[^fn-mobilenet-innovation]: **MobileNet Innovation**: Google's MobileNet family revolutionized mobile AI by achieving 10-20x parameter reduction compared to traditional CNNs. MobileNetV1 (2017) used depthwise separable convolutions to reduce floating-point operations (FLOPs) by 8-9x, while MobileNetV2 (2018) added inverted residuals and linear bottlenecks. The breakthrough allowed real-time inference on smartphones: MobileNetV2 runs ImageNet classification in ~75ms on a Pixel phone versus 1.8 seconds for ResNet-50 [@he2016deep].

[^fn-depthwise-separable]: **Depthwise Separable Convolutions**: This technique decomposes standard convolution into two operations: depthwise convolution (applies single filter per input channel) and pointwise convolution (1×1 conv to combine channels). For a 3×3 conv with 512 input/output channels, standard convolution requires 2.4M parameters while depthwise separable needs only 13.8K—a 174x reduction. The computational savings are similarly dramatic, making real-time inference possible on mobile CPUs.

These architectures are often designed to be modular, allowing for easy adaptation and fine-tuning. For example, MobileNets [@howard2017mobilenets] can be configured with different width multipliers and resolution settings to balance performance and resource usage. Concretely, MobileNetV2 with α=1.0 requires 3.4M parameters (13.6MB in FP32), but with α=0.5 this drops to 0.7M parameters (2.8MB)—enabling deployment on devices with just 4MB available RAM. This flexibility is important for on-device learning, where the model must adapt to the specific constraints of the deployment environment.

### Data Constraints {#sec-ondevice-learning-data-constraints-303e}

The nature of data available to on-device ML systems differs from the large, curated, and centrally managed datasets used in cloud-based training. At the edge, data is locally collected, temporally sparse, and often unstructured or unlabeled. These characteristics introduce challenges in volume, quality, and statistical distribution, all of which affect the reliability and generalizability of learning on the device.

Data volume is limited due to storage constraints and the nature of user interaction. For example, a smart fitness tracker may collect motion data only during physical activity, generating relatively few labeled samples per day. If a user wears the device for just 30 minutes of exercise, only a few hundred data points might be available for training, compared to the thousands typically required for supervised learning in controlled environments.

On-device data is frequently non-IID (non-independent and identically distributed) [@zhao2018federated]. Consider a voice assistant deployed in different households: one user may issue commands in English with a strong regional accent, while another might speak a different language entirely. The local data distribution is user-specific and may differ from the training distribution of the initial model. This heterogeneity complicates both model convergence and the design of update mechanisms that generalize well across devices.

Label scarcity presents an additional obstacle. Most edge-collected data is unlabeled by default. In a smartphone camera, for instance, the device may capture thousands of images, but only a few are associated with user actions (e.g., tagging or favoriting), which could serve as implicit labels. In many applications, including detecting anomalies in sensor data and adapting gesture recognition models, labels may be entirely unavailable, making traditional supervised learning infeasible without additional methods.

Noise and variability further degrade data quality. Embedded systems such as environmental sensors or automotive ECUs may experience fluctuations in sensor calibration, environmental interference, or mechanical wear, leading to corrupted or drifting input signals over time. Without centralized validation, these errors may silently degrade learning performance if not detected and filtered appropriately.

Finally, data privacy and security concerns are paramount in many on-device learning applications. Sensitive information, such as health data or user interactions, must be protected from unauthorized access. This requirement often precludes the use of traditional data-sharing methods, such as uploading raw data to a central server for training. Instead, on-device learning must rely on techniques that allow for local adaptation without exposing sensitive information.

### Compute Constraints {#sec-ondevice-learning-compute-constraints-4d6d}

On-device learning must operate within the computational envelope of the target hardware platform, which ranges from low-power embedded microcontrollers to mobile-class processors found in smartphones and wearables. These systems differ from the large-scale GPU or TPU infrastructure used in cloud-based training. They impose strict limits on instruction throughput, parallelism, and architectural support for training-specific operations, all of which shape the design of feasible learning strategies.

On the embedded end of the spectrum, devices such as the STM32F4[^fn-stm32-constraints] or ESP32[^fn-esp32-capabilities] microcontrollers offer only a few hundred kilobytes of SRAM and lack hardware support for floating-point operations [@lai2020tinyml]. These constraints represent the fundamental limitations of edge hardware explored in @sec-ai-acceleration. These constraints preclude the use of conventional deep learning libraries and require models to be carefully designed for integer arithmetic and minimal runtime memory allocation. In such cases, even small models require tailored techniques, including quantization-aware training[^fn-quantization-aware] and selective parameter updates, to execute training loops without exceeding memory or power budgets. For example, the STM32F4 microcontroller can run a simple linear regression model with a few hundred parameters, but training even a small convolutional neural network would exceed its memory capacity. In these environments, training is often limited to simple algorithms such as stochastic gradient descent (SGD)[^fn-sgd] or $k$-means clustering, which can be implemented using integer arithmetic and minimal memory overhead.

[^fn-quantization-aware]: **Quantization-Aware Training**: Unlike post-training quantization which converts trained FP32 models to INT8, quantization-aware training simulates low-precision arithmetic during training itself. This allows the model to learn robust representations despite reduced precision. Critical for edge devices where INT8 operations consume 4x less power and enable 4x faster inference compared to FP32, while maintaining 95-99% of original accuracy.

[^fn-sgd]: **Stochastic Gradient Descent (SGD)**: The fundamental optimization algorithm for neural networks, updating parameters using gradients computed on small batches (or single samples). Unlike full-batch gradient descent, SGD's randomness helps escape local minima while requiring minimal memory—storing only current parameters and gradients. This simplicity makes SGD ideal for microcontrollers where advanced optimizers like Adam would exceed memory budgets.

[^fn-stm32-constraints]: **STM32F4 Microcontroller Reality**: The STM32F4 represents the harsh reality of embedded computing—192KB SRAM (roughly the size of a small JPEG image) and 1MB flash storage, running at 168MHz without floating-point hardware acceleration. Integer arithmetic is 10-100x slower than dedicated floating-point units found in mobile chips. Power consumption is ~100mW during active processing, requiring careful duty-cycling to preserve battery life. These constraints make even simple neural networks challenging—a 10-neuron hidden layer requires ~40KB for weights alone in FP32.

[^fn-esp32-capabilities]: **ESP32 Edge Computing**: The ESP32 provides 520KB SRAM and dual-core processing at 240MHz, making it more capable than STM32F4 but still severely constrained. Its key advantage is built-in WiFi and Bluetooth for federated learning scenarios. However, the lack of hardware floating-point support means all ML operations must use integer quantization. Real-world deployments show 8-bit quantized models can achieve 95% of FP32 accuracy while fitting in ~50KB memory, enabling basic on-device training for simple tasks like sensor anomaly detection.

In contrast, mobile-class hardware, including the Qualcomm Snapdragon, Apple Neural Engine[^fn-ondevice-neural-engine], and Google Tensor SoC[^fn-tensor-soc], provides significantly more compute power, often with dedicated AI accelerators and optimized support for 8-bit or mixed-precision[^fn-mixed-precision] matrix operations. These accelerators, their capabilities, and their programming models are detailed in @sec-ai-acceleration. These platforms can support more complex training routines, including full backpropagation over compact models, though they still fall short of the computational throughput and memory bandwidth available in centralized data centers. For instance, training a lightweight transformer[^fn-transformer-mobile] on a smartphone is feasible but must be tightly bounded in both time and energy consumption to avoid degrading the user experience.

[^fn-mixed-precision]: **Mixed-Precision Training**: Uses different numerical precisions for different operations—typically FP16 for forward/backward passes and FP32 for parameter updates. This halves memory usage and doubles throughput on modern hardware with Tensor Cores, while maintaining training stability through automatic loss scaling. Mobile implementations often use INT8 for inference and FP16 for gradient computation, balancing accuracy with hardware constraints.

[^fn-transformer-mobile]: **Lightweight Transformers**: Mobile-optimized transformer architectures like MobileBERT [@sun2020mobilebert] and DistilBERT [@sanh2019distilbert] achieve 4-6x speedup over full models through techniques like knowledge distillation, layer reduction, and attention head pruning. MobileBERT retains 97% of BERT-base accuracy while running inference in ~40ms on mobile CPUs versus 160ms for full BERT. Key optimizations include bottleneck attention mechanisms and specialized mobile-friendly layer configurations.

[^fn-ondevice-neural-engine]: **Apple Neural Engine Evolution**: Apple's Neural Engine has evolved dramatically since the A11 Bionic. The A17 Pro (2023) features a 16-core Neural Engine delivering 35 TOPS—roughly equivalent to an NVIDIA GTX 1080 Ti. This represents a 58x improvement over the original A11. The Neural Engine specializes in matrix operations with dedicated 8-bit and 16-bit arithmetic units, enabling efficient on-device training. Real-world performance: fine-tuning a MobileNet classifier takes ~2 seconds versus 45 seconds on CPU alone, while consuming only ~500mW additional power.

[^fn-tensor-soc]: **Google Tensor SoC Architecture**: Google's Tensor chips (starting with Pixel 6 in 2021) feature a custom TPU v1-derived Edge TPU optimized for ML workloads. Unlike Apple's Neural Engine, Tensor optimizes for Google's specific models (speech recognition, computational photography). The TPU provides efficient 8-bit integer operations while consuming only 2W, making it highly efficient for federated learning scenarios where devices train locally on speech or image data.

Compute constraints are especially salient in real-time or battery-operated systems as discussed previously in @sec-ml-systems, where specific latency budgets dictate architectural choices. Camera applications processing at 30 FPS cannot exceed 33ms per frame, voice interfaces require sub-500ms response times for natural interaction, AR/VR systems demand sub-20ms motion-to-photon latency to prevent user discomfort, and safety-critical control systems must respond within 10ms to ensure operational safety. These quantitative constraints directly determine whether on-device learning is feasible or whether cloud-based alternatives become necessary. In a smartphone-based speech recognizer, on-device adaptation must not interfere with inference latency or system responsiveness. Similarly, in wearable medical monitors, training must occur opportunistically, during periods of low activity or charging, to preserve battery life and avoid thermal issues.

The architectural implications of these hardware constraints extend beyond computational power. Training operations exhibit different memory access patterns than inference: backpropagation requires 3-5x higher memory bandwidth due to gradient computation and activation caching. Modern edge accelerators address these challenges through specialized hardware features. Adaptive precision datapaths allow dynamic switching between INT4 for forward passes and FP16 for gradient computation, optimizing both accuracy and efficiency. Sparse computation units accelerate selective parameter updates by skipping zero gradients—critical for bias-only and LoRA adaptations. Near-memory compute architectures[^fn-near-memory-compute] reduce data movement costs by performing gradient updates directly adjacent to weight storage. Most current edge accelerators remain optimized for inference workloads, creating hardware-software co-design opportunities for future on-device training accelerators that handle the demands of local adaptation.

[^fn-near-memory-compute]: **Near-Memory Computing**: Places processing units directly adjacent to or within memory arrays, dramatically reducing data movement costs. Traditional von Neumann architectures spend 100-1000x more energy moving data than computing on it. Near-memory designs can perform matrix operations with 10-100x better energy efficiency by eliminating costly memory bus transfers. Critical for edge training where gradient computations require intensive memory access patterns that overwhelm traditional cache hierarchies.

### Hardware-Software Co-Design Constraints {#sec-ondevice-learning-hardware-codesign-constraints}

On-device learning systems must navigate mobile computing physics: power dissipation, thermal limits, and energy budgets. These constraints are fundamental design drivers that determine the feasible space of on-device learning algorithms. Understanding these quantitative constraints enables design decisions that balance learning capabilities with system sustainability.

#### Energy and Thermal Constraint Analysis

Mobile devices operate under power budgets that determine feasible model complexity. The thermal design power (TDP) of mobile processors creates constraints that shape on-device learning strategies. Modern smartphones typically maintain a sustained TDP of 5W, but can burst to 10W for brief periods before thermal throttling reduces performance by 50% or more. This thermal cycling behavior forces training algorithms to operate in managed burst modes, utilizing peak performance for only 10-30 seconds before backing off to sustainable power levels.

**Mobile Power Budget Hierarchy:**
- **Smartphone sustained processing:** 2-3W to prevent user-noticeable heating and maintain battery life
- **Peak training burst mode:** 10W sustainable for 10-30 seconds before thermal throttling
- **Neural processing units:** 0.5-2W for dedicated AI workloads with optimized power efficiency
- **CPU AI processing:** 3-5W requiring aggressive thermal management and duty cycling

Power consumption scales with model size and training complexity. Training operations consume 10-50x more power than inference due to the computational overhead of gradient computation (consuming 40-70% of training power), weight updates (20-30%), and increased data movement between memory hierarchies (10-30%). Mobile devices typically budget only 500-1000mW for sustained ML training to preserve battery life and user experience, effectively limiting training sessions to 10-100 minutes daily under normal usage patterns. This constraint shifts the design priority from computational throughput to power efficiency, requiring co-optimization of algorithms and hardware utilization patterns.

The thermal implications extend beyond power limits. Training workloads generate localized heat that can trigger protective throttling in specific processor cores or accelerator units. Modern mobile SoCs implement sophisticated thermal management, including dynamic voltage and frequency scaling (DVFS)[^fn-dvfs-mobile], core migration between efficiency and performance clusters, and selective shutdown of non-essential processing units. On-device learning systems must integrate with these thermal management frameworks, scheduling training bursts during optimal thermal windows and degrading when thermal limits are approached.

[^fn-dvfs-mobile]: **Dynamic Voltage and Frequency Scaling (DVFS)**: Modern mobile processors continuously adjust operating voltage and clock frequency based on workload and thermal conditions. During ML training, DVFS can reduce clock speeds by 30-50% when temperature exceeds 70°C, directly impacting training throughput. Effective on-device learning systems monitor thermal state and proactively reduce batch sizes or training intensity to maintain consistent performance rather than experiencing sudden throttling events.

#### Memory Hierarchy Optimization

On-device deployment requires fitting models within constrained memory hierarchies that vary across device categories. These constraints affect model storage and dynamic memory requirements during training, where activation caching and gradient computation expand memory footprint by 3-5x compared to inference-only deployment.

**Device Memory Hierarchy:**
- **iPhone 15 Pro:** 8GB total system memory with approximately 2-4GB available for application workloads
- **Budget Android devices:** 4GB total system memory with 1-2GB available for ML workloads after OS overhead
- **IoT embedded systems:** 64MB-1GB total memory, often shared between system tasks and application data
- **Microcontrollers:** 256KB-2MB SRAM requiring extreme optimization and careful memory management

Memory requirements expand during training beyond static model storage. Standard backpropagation requires caching intermediate activations for each layer during the forward pass, which are then reused during gradient computation in the backward pass. A MobileNetV2 model requiring 14MB for inference needs 50-70MB during training, exceeding the available memory budget on many mobile devices. This expansion necessitates model compression techniques that compound multiplicatively: INT8 quantization provides 4x memory reduction, structured pruning achieves 10x parameter reduction, and knowledge distillation enables 5x model size reduction while maintaining accuracy within 2-5% of the original model.

Cache optimization becomes critical with constrained memory pools. Modern mobile SoCs feature complex memory hierarchies with L1 cache (32-64KB), L2 cache (1-8MB), and system memory (4-16GB) exhibiting 10-100x latency differences between levels. Training workloads that exceed cache capacity face performance degradation due to memory bandwidth bottlenecks. Successful on-device learning systems design data access patterns to maximize cache hit rates, often requiring specialized memory layouts that group related parameters, carefully sized mini-batches that fit within cache constraints, and gradient accumulation strategies that minimize memory bus traffic.

The memory bandwidth limitations become particularly acute during training. While inference workloads primarily read model weights sequentially, training requires bidirectional data flow for gradient computation and weight updates. This increased memory traffic can saturate the memory subsystem, creating bottlenecks that limit training throughput regardless of computational capacity. Advanced implementations employ techniques such as gradient checkpointing[^fn-gradient-checkpointing] to trade computation for memory, and mixed-precision training to reduce bandwidth requirements while maintaining numerical stability.

[^fn-gradient-checkpointing]: **Gradient Checkpointing**: A memory optimization technique that trades computation for memory by recomputing intermediate activations during the backward pass instead of storing them. This can reduce memory requirements by 50-80% at the cost of 20-30% additional computation. Particularly valuable for on-device training where memory is more constrained than compute capacity, enabling training of larger models within fixed memory budgets.

#### Specialized On-Device Architecture Analysis

Different mobile platforms provide distinct acceleration capabilities that determine not only achievable model complexity but also feasible learning paradigms. The architectural differences between these accelerators fundamentally shape the design space for on-device training algorithms, influencing everything from numerical precision choices to gradient computation strategies.

**Current Generation Mobile Accelerators:**
- **Apple Neural Engine (A17 Pro):** 35 TOPS peak performance specialized for 8-bit and 16-bit operations, optimized for CoreML inference patterns with limited training support
- **Qualcomm Hexagon DSP (Snapdragon 8 Gen 3):** 45 TOPS with flexible precision support and programmable vector units, enabling mixed-precision training workflows
- **Google Tensor TPU (Pixel 8):** Optimized for TensorFlow Lite operations with strong INT8 performance and federated learning integration
- **Energy efficiency comparison:** Dedicated NPUs achieve 1-5 TOPS/W versus general-purpose CPU at 0.1-0.2 TOPS/W

These accelerators determine not just raw performance but feasible learning paradigms and algorithmic approaches. Apple's Neural Engine excels at fixed-precision inference workloads but provides limited support for the dynamic precision requirements of gradient computation, making it more suitable for inference-heavy adaptation techniques like few-shot learning. Qualcomm's Hexagon DSP offers greater training flexibility through its programmable vector units and support for mixed-precision arithmetic, enabling more sophisticated on-device training including full backpropagation on compact models. Google's Tensor TPU integrates tightly with federated learning frameworks and provides optimized communication primitives for distributed training scenarios.

The architectural implications extend beyond computational throughput to memory access patterns and data flow optimization. Training workloads exhibit fundamentally different characteristics than inference: gradient computation requires 3-5x higher memory bandwidth, weight updates create write-heavy access patterns, and optimizer state management demands additional memory allocation. Modern edge accelerators are beginning to address these challenges through specialized hardware features including adaptive precision datapaths that dynamically switch between INT4 for forward passes and FP16 for gradient computation, sparse computation units that accelerate selective parameter updates by skipping zero gradients, and near-memory compute architectures that reduce data movement costs by performing gradient updates directly adjacent to weight storage.

However, most current edge accelerators remain primarily optimized for inference workloads, creating a significant hardware-software co-design opportunity. Future on-device training accelerators will need to efficiently handle the unique demands of local adaptation, including support for dynamic precision scaling, efficient gradient accumulation, and specialized memory hierarchies optimized for the bidirectional data flow patterns characteristic of training workloads. Architecture selection influences everything from model quantization strategies and gradient computation approaches to federated communication protocols and thermal management policies.

### From Constraints to Solutions: A Systems Approach

The constraints outlined above define a complex design space where traditional machine learning approaches are fundamentally inadequate. However, these constraints also reveal clear solution pathways when approached systematically. Each constraint category maps directly to specific technical strategies that enable practical on-device learning.

**Model complexity constraints** drive the need for selective parameter updating strategies. Rather than training entire models, successful on-device systems adapt minimal subsets of parameters through techniques like bias-only updates, low-rank adaptations, and sparse parameter selection. These approaches reduce memory footprint by 10-100x while preserving model expressiveness.

**Data scarcity and quality issues** necessitate sample-efficient learning approaches. Transfer learning leverages pre-trained representations to enable adaptation from few examples, while experience replay and data augmentation stretch limited local datasets. Streaming and few-shot learning strategies accommodate the sparse, temporal nature of edge data collection.

**Computational and energy limitations** require algorithmic efficiency through multiple dimensions. Quantized training reduces precision requirements from 32-bit to 8-bit or lower. Gradient approximation methods eliminate expensive backpropagation. Opportunistic training schedules balance adaptation with user experience and thermal constraints.

**Distributed coordination needs** are addressed through federated learning protocols that enable privacy-preserving collaboration without raw data sharing. These approaches aggregate local updates from thousands of devices while respecting individual privacy and communication constraints.

The following sections develop each solution category systematically, showing how theoretical approaches translate into practical implementations that respect the harsh realities of edge deployment. This progression moves from individual device adaptations to coordinated learning across device populations, building a complete systems view of on-device learning.

## Model Adaptation {#sec-ondevice-learning-model-adaptation-6a82}

Having established the fundamental constraints that make traditional training infeasible on edge devices, we now turn to the first category of solutions: selective parameter adaptation. These techniques directly address the model complexity constraints by dramatically reducing the scope of parameter updates while preserving learning capability.

The central insight behind model adaptation is that complete model retraining is neither necessary nor feasible for on-device learning. Instead, we can leverage the power of pre-trained representations and adapt only the minimal parameter subset required to capture local variations. This approach transforms the optimization problem from updating millions of parameters to updating hundreds or thousands, making training feasible within device memory and compute constraints.

Model adaptation strategies operate on a simple principle: preserve what works globally, adapt what matters locally. The pre-trained backbone captures general-purpose representations learned from large-scale datasets, while lightweight adaptation components specialize these representations for local conditions, user preferences, or environmental factors.

This section examines three complementary adaptation strategies, each targeting different constraint profiles:

- **Weight freezing** addresses memory limitations by updating only bias terms or final layers
- **Structured updates** use low-rank and residual adaptations to balance expressiveness with efficiency
- **Sparse updates** enable selective parameter modification based on importance or gradients

These approaches build on the architectural principles from @sec-dnn-architectures while applying the optimization strategies from @sec-model-optimizations. Each technique represents a different point in the accuracy-efficiency tradeoff space, enabling deployment across the full spectrum of edge hardware capabilities.

### Weight Freezing {#sec-ondevice-learning-weight-freezing-3407}

One of the simplest and most effective strategies for reducing the cost of on-device learning is to freeze the majority of a model's parameters and adapt only a minimal subset. A widely used approach is bias-only adaptation, in which all weights are fixed and only the bias terms, which are typically scalar offsets applied after linear or convolutional layers, are updated during training. This significantly reduces the number of trainable parameters, simplifies memory management during backpropagation, and helps mitigate overfitting when data is sparse or noisy.

Consider a standard neural network layer:
$$
y = W x + b
$$
where $W \in \mathbb{R}^{m \times n}$ is the weight matrix, $b \in \mathbb{R}^m$ is the bias vector, and $x \in \mathbb{R}^n$ is the input. In full training, gradients are computed for both $W$ and $b$. In bias-only adaptation, we constrain:
$$
\frac{\partial \mathcal{L}}{\partial W} = 0, \quad \frac{\partial \mathcal{L}}{\partial b} \neq 0
$$
so that only the bias is updated via gradient descent:
$$
b \leftarrow b - \eta \frac{\partial \mathcal{L}}{\partial b}
$$

This reduces the number of stored gradients and optimizer states, enabling training to proceed under memory-constrained conditions. On embedded devices that lack floating-point units, this reduction enables on-device learning.

The code snippet in @lst-bias-adaptation demonstrates how to implement bias-only adaptation in PyTorch.

::: {#lst-bias-adaptation lst-cap="**Bias-Only Adaptation**: Freezes model parameters except for biases to reduce memory usage and allow on-device learning."}
```{.python}
# Freeze all parameters
for name, param in model.named_parameters():
    param.requires_grad = False

# Enable gradients for bias parameters only
for name, param in model.named_parameters():
    if 'bias' in name:
        param.requires_grad = True
```
:::

This pattern ensures that only bias terms participate in the backward pass and optimizer update. It is particularly useful when adapting pretrained models to user-specific or device-local data.

This technique underpins TinyTL, a framework explicitly designed to allow efficient adaptation of deep neural networks on microcontrollers and other memory-limited platforms. Rather than updating all network parameters during training, TinyTL freezes both the convolutional weights and the batch normalization statistics, training only the bias terms and, in some cases, lightweight residual components. This architectural shift reduces memory usage during backpropagation, since the largest tensors, which are intermediate activations, no longer need to be stored for gradient computation.

@fig-tinytl-architecture illustrates the architectural differences between a standard model and the TinyTL approach. In the conventional baseline architecture, all layers are trainable, and backpropagation requires storing intermediate activations for the full network. This increases the memory footprint, which becomes infeasible on edge devices with only a few hundred kilobytes of SRAM.

::: {#fig-tinytl-architecture fig-env="figure" fig-pos="htb"}
```{.tikz}
\begin{tikzpicture}[line cap=round,line join=round,font=\small\usefont{T1}{phv}{m}{n}]
\tikzset{
Line/.style={violet!50, line width=1.1pt,shorten <=1pt,shorten >=2pt},
LineA/.style={black!50, line width=1.1pt,{-{Triangle[width=0.9*6pt,length=1.2*6pt]}}},
ALine/.style={black!50, line width=1.1pt,{{Triangle[width=0.9*6pt,length=1.2*6pt]}-}},
Larrow/.style={fill=violet!50, single arrow,  inner sep=2pt, single arrow head extend=3pt,
            single arrow head indent=0pt,minimum height=7mm, minimum width=3pt},
Box/.style={ inner xsep=2pt, draw=OrangeLine, line width=1.0pt, fill=OrangeLine!30,
 minimum width=13mm, minimum height=7mm
  },
}
\tikzset{
pics/square/.style = {
        code = {
        \pgfkeys{/channel/.cd, #1}
\begin{scope}[local bounding box=SQUARE,scale=\scalefac,every node/.append style={transform shape}]
% Right Face
\draw[draw=\drawchannelcolor,fill=\channelcolor!70,line width=\Linewidth]
(\Width,0,0)coordinate(\picname-ZDD)--(\Width,\Height,0)--(\Width,\Height,\Depth)--(\Width,0,\Depth)--cycle;
% Front Face
\draw[draw=\drawchannelcolor,fill=\channelcolor!40,line width=\Linewidth]
(0,0,\Depth)coordinate(\picname-DL)--(0,\Height,\Depth)coordinate(\picname-GL)--
(\Width,\Height,\Depth)coordinate(\picname-GD)--(\Width,0,\Depth)coordinate(\picname-DD)--(0,0,\Depth);
% Top Face
\draw[draw=\drawchannelcolor,fill=\channelcolor!20,line width=\Linewidth]
(0,\Height,0)coordinate(\picname-ZGL)--(0,\Height,\Depth)--
(\Width,\Height,\Depth)--(\Width,\Height,0)coordinate(\picname-ZGD)--cycle;
\end{scope}
    }
  }
}

\tikzset{
pics/trapez/.style = {
        code = {
        \pgfkeys{/channel/.cd, #1}
\begin{scope}[local bounding box=TRAP,scale=\scalefac,every node/.append style={transform shape}]
% Right Face
\draw[draw=\drawchannelcolor,fill=\channelcolor!40,line width=\Linewidth]
(0,-0.5)coordinate(\picname-DL)--(2,-1.5)coordinate(\picname-DD)--(2,1.5)coordinate(\picname-GD)--
(0,0.5)coordinate(\picname-GL)--cycle;
\end{scope}
    }
  }
}
\pgfkeys{
  /channel/.cd,
  Depth/.store in=\Depth,
  Height/.store in=\Height,
  Width/.store in=\Width,
  channelcolor/.store in=\channelcolor,
  drawchannelcolor/.store in=\drawchannelcolor,
  scalefac/.store in=\scalefac,
  Linewidth/.store in=\Linewidth,
  picname/.store in=\picname,
  Depth=1.0,
  Height=1.5,
  Width=0.8,
  channelcolor=BrownLine,
  drawchannelcolor=black,
  scalefac=1,
  Linewidth=0.5pt,
  picname=C
}
%%%%%%%%%%%%%%
%Above
%%%%%%%%%%%%%%
\begin{scope}[local bounding box=above,scale=\scalefac,every node/.append style={transform shape}]
%cuboid smaller 1
\pic[shift={(0,0.25)}] at (0,0){square={scalefac=1,picname=B1,channelcolor=BlueLine,
Depth=0.5,Height=0.4,Width=1.0,Linewidth=0.5pt}};
\node[]at($(B1-GL)!0.5!(B1-DD)$){C};
\node[below=1pt of $(B1-DL)!0.5!(B1-DD)$]{R};
\path[red](B1-DD)-|coordinate(PB1)(B1-ZGD);
\node[anchor=east](PO)at($($(B1-GL)!0.5!(B1-DL)$)+(-0.05,0)$){\tiny $\bullet$ $\bullet$ $\bullet$};
%trapez 1
\pic[shift={(0,0)}] at (2.2,0.35){trapez={scalefac=0.6,picname=B2,channelcolor=OrangeLine!80!,
drawchannelcolor=OrangeLine!40,Linewidth=0pt}};
\coordinate(PL1)at($(B2-DL)+(0,-0.8)$);
%rectangle 1
\pic[shift={(0,0)}] at (PL1){square={picname=P1,drawchannelcolor=OrangeLine!60,channelcolor=OrangeLine!80!,
Linewidth=0pt,Depth=0,Height=0.07,Width=1.2}};
%cuboid 1
\pic[shift={(1.5,-0.375)}] at ($(B2-DD)!0.5!(B2-GD)$){square={scalefac=1,picname=B3,channelcolor=BlueLine, Linewidth=0.5pt}};
\node[]at($(B3-GL)!0.5!(B3-DD)$){6C};
\node[below=1pt of $(B3-DL)!0.5!(B3-DD)$]{R};
%areas 1
\foreach \i [count=\c] in {0.5,1,1.5,2}{
\pic[shift={(1.75,-0.7)}] at ($(B3-DD)!0.5!(B3-GD)+(0,0.8*\i)$){square={picname=\c B4,drawchannelcolor=OrangeLine!60,
channelcolor=OrangeLine!60!, Linewidth=0pt,Depth=0.7,Height=0.03,Width=1.5}};
}
\coordinate(PL2)at($(1B4-DL)+(0,-0.52)$);
%rectangle 1
\pic[shift={(0,0)}] at (PL2){square={picname=P2,drawchannelcolor=OrangeLine!60,channelcolor=OrangeLine!80!,
Linewidth=0pt,Depth=0,Height=0.07,Width=1.75}};
%cuboid 2
\pic[shift={(1.65,-0.55)}] at ($(4B4-ZGD)!0.5!(1B4-GD)$){square={scalefac=1,picname=B5,channelcolor=BlueLine, Linewidth=0.5pt}};
\node[]at($(B5-GL)!0.5!(B5-DD)$){6C};
\node[below=1pt of $(B5-DL)!0.5!(B5-DD)$]{R};
%trapez 2
\pic[shift={(1.4,0)}] at ($(B5-DD)!0.5!(B5-GD)$){trapez={scalefac=0.6,picname=B6,channelcolor=OrangeLine!80!,
drawchannelcolor=OrangeLine!40,Linewidth=0pt}};
\coordinate(PL3)at($(B6-DL)+(0,-0.8)$);
%rectangle 2
\pic[shift={(0,0)}] at (PL3){square={picname=P3,drawchannelcolor=OrangeLine!60,channelcolor=OrangeLine!80!,
Linewidth=0pt,Depth=0,Height=0.07,Width=1.2}};
%cuboid smaller 2
\pic[shift={(1.45,-0.11)}] at ($(B6-DD)!0.5!(B6-GD)$){square={scalefac=1,picname=B7,channelcolor=BlueLine,
Depth=0.5,Height=0.4,Width=1.0,Linewidth=0.5pt}};
\node[]at($(B7-GL)!0.5!(B7-DD)$){C};
\node[below=1pt of $(B7-DL)!0.5!(B7-DD)$]{R};
%text above
\node[above =14pt of $(B2-GL)!0.5!(B2-GD)$](TT1){1 $\times$ 1 Conv};
\path[red](TT1)-|coordinate(GS1)($(4B4-ZGL)!0.5!(4B4-ZGD)$);
\node[](DWC)at(GS1){Depth-wise Conv};
\path[red](TT1)-|coordinate(GS2)($(B6-GL)!0.5!(B6-GD)$);
\node[](TT2)at(GS2){1 $\times$ 1 Conv};
%arrows
\coordinate(SR1)at($($(B1-ZGD)!0.5!(PB1)$)!0.5!($(B2-GL)!0.5!(B2-DL)$)$);
\node[Larrow](AR1)at(SR1){};
\coordinate(SR2)at($($(B2-DD)!0.5!(B2-GD)$)!0.5!($(B3-GL)!0.5!(B3-DL)$)$);
\node[Larrow](AR2)at(SR2){};
\draw[Line](AR2)|-($(P1-DD)!0.5!(P1-GD)$);
\coordinate(SR3)at($($(B3-DD)!0.5!(B3-GD)$)!0.57!($(2B4-GL)!0.5!(2B4-ZGL)$)$);
\node[Larrow](AR3)at(SR3){};
\coordinate(SR4)at($($(2B4-GD)!0.5!(2B4-ZGD)$)!0.5!($(B5-DL)!0.5!(B5-GL)$)$);
\node[Larrow](AR4)at(SR4){};
\draw[Line](AR4)|-($(P2-DD)!0.5!(P2-GD)$);
\coordinate(SR5)at($($(B5-DD)!0.5!(B5-GD)$)!0.6!($(B6-GL)!0.5!(B6-DL)$)$);
\node[Larrow](AR5)at(SR5){};
\coordinate(SR6)at($($(B6-DD)!0.55!(B6-GD)$)!0.5!($(B7-GL)!0.5!(B7-DL)$)$);
\node[Larrow](AR6)at(SR6){};
\draw[Line](AR6)|-($(P3-DD)!0.5!(P3-GD)$);
\coordinate(SR7)at($($(B7-DD)!0.5!(B7-GD)$)+(0.7,0.1)$);
\node[Larrow](AR7)at(SR7){};
%fitting
\scoped[on background layer]
\node[draw=none,inner xsep=3mm,inner ysep=2mm,
yshift=-1mm,fill=BrownLine!10,fit=(TT1)(P1-DL)(AR6)(P3-DD),line width=0.75pt](BB1){};
\node[below=13pt of  BB1.south,inner sep=0pt, anchor=south](FT){a) Fine-tune the full network (Conventional)};
\scoped[on background layer]
\node[draw=none,inner xsep=3mm,inner ysep=2mm,
yshift=-1mm,fill=green!5,fit=(BB1)(FT)(AR7)(PO),line width=0.75pt](BB2){};
\scoped[on background layer]
\node[draw=none,inner xsep=1mm,inner ysep=2mm,
yshift=-1mm,fill=magenta!5,fit=(TT1)(P1-DL)(AR6)(P3-DD),line width=0.75pt](BB1){};\%
%Legend
\node[Box,above=4pt of BB2.north west,anchor=south west,draw=OrangeLine!30](L1){};
\node[right=3pt of L1](TL1){learnable params};
\node[Box,above=4pt of L1.north west,anchor=south west,fill=BlueLine!30,draw=BlueLine!30](L11){};
\node[right=3pt of L11](TL11){fmap in memory};
%
\node[Box,right=5 of L1.east,anchor=east,fill=white,draw=OrangeLine](L2){};
\node[right=3pt of L2](TL2){fixed params};
\node[Box,above=4pt of L2.north west,anchor=south west,fill=white,draw=BlueLine](L22){};
\node[right=3pt of L22](TL22){fmap not in memory};
%
\node[Box,right=4 of L22.east,anchor=west,fill=magenta!10,draw=magenta!10](L33){};
\node[right=10pt of L33,inner sep=0pt](TL33){$i$\textsuperscript{th} mobile inverted bottleneck block};
\pic[shift={(-0.40,-0.7)}] at (L33){square={picname=LB,drawchannelcolor=OrangeLine!90,
channelcolor=white!, Linewidth=0pt,Depth=0.7,Height=0.03,Width=1.15}};
\path[red]($(LB-DD)!0.5!(LB-ZDD)$)-|coordinate(TEX1)(TL33.south west);
\node[anchor=west,inner sep=0pt](TL3)at(TEX1){weight};
%
\pic[shift={(1,-0.03)}] at (TL3.east){square={picname=LP,drawchannelcolor=OrangeLine!60,channelcolor=OrangeLine!80!,
Linewidth=0pt,Depth=0,Height=0.07,Width=1.2}};
\node[anchor=west,inner sep=8pt](TL4)at($(LP-DD)!0.8!(LP-GD)$){bias};
\end{scope}
%%%%%%%%%%%%%%%%
%below
%%%%%%%%%%%%%%%%
\begin{scope}[local bounding box=below,shift={($(0,0)+(0,-4.2)$)},scale=\scalefac,every node/.append style={transform shape}]
%cuboid smaller 1
\pic[shift={(0,0.25)}] at (0,0){square={scalefac=1,picname=B1,channelcolor=white, drawchannelcolor=BlueLine,
Depth=0.5,Height=0.4,Width=1.0,Linewidth=0.5pt}};
\path[red](B1-DD)-|coordinate(PB1)(B1-ZGD);
\node[anchor=east](PO)at($($(B1-GL)!0.5!(B1-DL)$)+(-0.05,0)$){\tiny $\bullet$ $\bullet$ $\bullet$};
%trapez 1
\pic[shift={(0,0)}] at (2.2,0.35){trapez={scalefac=0.6,picname=B2,channelcolor=white,
drawchannelcolor=OrangeLine,Linewidth=0pt}};
\coordinate(PL1)at($(B2-DL)+(0,-0.8)$);
%rectangle 1
\pic[shift={(0,0)}] at (PL1){square={picname=P1,drawchannelcolor=OrangeLine!60,channelcolor=OrangeLine!80!,
Linewidth=0pt,Depth=0,Height=0.07,Width=1.2}};
%cuboid 1
\pic[shift={(1.5,-0.375)}] at ($(B2-DD)!0.5!(B2-GD)$){square={scalefac=1,picname=B3,drawchannelcolor=BlueLine,
channelcolor=white, Linewidth=0.5pt}};
%areas 1
\foreach \i [count=\c] in {0.5,1,1.5,2}{
\pic[shift={(1.75,-0.7)}] at ($(B3-DD)!0.5!(B3-GD)+(0,0.8*\i)$){square={picname=\c B4,drawchannelcolor=OrangeLine,
channelcolor=white!, Linewidth=0pt,Depth=0.7,Height=0.03,Width=1.5}};
}
\coordinate(PL2)at($(1B4-DL)+(0,-0.52)$);
%rectangle 1
\pic[shift={(0,0)}] at (PL2){square={picname=P2,drawchannelcolor=OrangeLine!60,channelcolor=OrangeLine!80!,
Linewidth=0pt,Depth=0,Height=0.07,Width=1.75}};
%cuboid 2
\pic[shift={(1.65,-0.55)}] at ($(4B4-ZGD)!0.5!(1B4-GD)$){square={scalefac=1,picname=B5,channelcolor=white,
drawchannelcolor=BlueLine,Linewidth=0.5pt}};
%trapez 2
\pic[shift={(1.4,0)}] at ($(B5-DD)!0.5!(B5-GD)$){trapez={scalefac=0.6,picname=B6,channelcolor=white!,
drawchannelcolor=OrangeLine,Linewidth=0pt}};
\coordinate(PL3)at($(B6-DL)+(0,-0.8)$);
%rectangle 2
\pic[shift={(0,0)}] at (PL3){square={picname=P3,drawchannelcolor=OrangeLine!60,channelcolor=OrangeLine!80!,
Linewidth=0pt,Depth=0,Height=0.07,Width=1.2}};
%cuboid smaller 2
\pic[shift={(1.45,-0.11)}] at ($(B6-DD)!0.5!(B6-GD)$){square={scalefac=1,picname=B7,channelcolor=white,
drawchannelcolor=BlueLine,Depth=0.5,Height=0.4,Width=1.0,Linewidth=0.5pt}};
%%%%%%%%%%%%%%%%%%%%%
%The second row
%%%%%%%%%%%%%%%%%%%%%
%cuboid smaller 11
\pic[shift={(0.32,-3.3)}] at ($(B1-DL)$){square={scalefac=1,picname=B11,channelcolor=BlueLine!80!, drawchannelcolor=black,
Depth=0.5,Height=0.4,Width=0.5,Linewidth=0.5pt}};
\node[left=1pt of $(B11-GL)!0.5!(B11-DL)$]{C};
\node[below=1pt of $(B11-DL)!0.5!(B11-DD)$]{0.5 R};
%cuboid smaller 77
\pic[shift={(0.32,-3.3)}] at ($(B7-DL)$){square={scalefac=1,picname=B77,channelcolor=white, drawchannelcolor=BlueLine,
Depth=0.5,Height=0.4,Width=0.5,Linewidth=0.5pt}};
%cuboid smaller 33
\pic[shift={(0,0)}] at ($(B11-ZDD)!0.5!(B77-ZDD)$){square={scalefac=1,picname=B33,channelcolor=BlueLine!80!, drawchannelcolor=black,
Depth=0.5,Height=0.4,Width=0.5,Linewidth=0.5pt}};
\node[below=1pt of $(B33-DL)!0.5!(B33-DD)$]{0.5 R};
%cuboid Group Conv
\pic[shift={(0,-0.4)}] at ($(B11-ZDD)!0.45!(B33-ZDD)$){square={scalefac=1,picname=B22D,channelcolor=OrangeLine!80!, drawchannelcolor=black,
Depth=0.5,Height=0.4,Width=1.5,Linewidth=0.5pt}};
\pic[shift={(0,0.4)}] at ($(B11-ZDD)!0.45!(B33-ZDD)$){square={scalefac=1,picname=B22G,channelcolor=OrangeLine!80!, drawchannelcolor=black,
Depth=0.5,Height=0.4,Width=1.5,Linewidth=0.5pt}};
%rectangle 11
\coordinate(PL11)at($(B22D-DL)+(0,-0.4)$);
\pic[shift={(0,0)}] at (PL11){square={picname=P11,drawchannelcolor=OrangeLine!60,channelcolor=OrangeLine!80!,
Linewidth=0pt,Depth=0,Height=0.07,Width=1.55}};
%1x1Conv
\node[draw,rectangle,minimum size=9mm,fill=OrangeLine!30,line width=0.75pt,yshift=1mm](RE) at ($(B33-ZDD)!0.4!(B77-ZDD)$){};
\path[red]($(P11-DD)!0.5!(P11-GD)$)-|coordinate(X22)(RE.south west);
\pic[shift={(0,0)}] at (X22){square={picname=P22,drawchannelcolor=OrangeLine!60,channelcolor=OrangeLine!80!,
Linewidth=0pt,Depth=0,Height=0.07,Width=0.9}};
%text above
\node[above =14pt of $(B2-GL)!0.5!(B2-GD)$](TT1){1 $\times$ 1 Conv};
\path[red](TT1)-|coordinate(GS1)($(4B4-ZGL)!0.5!(4B4-ZGD)$);
\node[](DWC)at(GS1){Depth-wise Conv};
\path[red](TT1)-|coordinate(GS2)($(B6-GL)!0.5!(B6-GD)$);
\node[](TT2)at(GS2){1 $\times$ 1 Conv};
%
\node[above =8pt of $(B22G-GL)!0.6!(B22G-GD)$](TT11){Group Conv};
\path[red](TT11)-|coordinate(GS2)(RE.north);
\node[](DWC)at(GS2){1 $\times$ 1 Conv};
%arrows
\coordinate(SR1)at($($(B1-ZGD)!0.5!(PB1)$)!0.5!($(B2-GL)!0.5!(B2-DL)$)$);
\node[Larrow](AR1)at(SR1){};
\coordinate(SR2)at($($(B2-DD)!0.5!(B2-GD)$)!0.5!($(B3-GL)!0.5!(B3-DL)$)$);
\node[Larrow](AR2)at(SR2){};
\draw[Line](AR2)|-($(P1-DD)!0.5!(P1-GD)$);
\coordinate(SR3)at($($(B3-DD)!0.5!(B3-GD)$)!0.57!($(2B4-GL)!0.5!(2B4-ZGL)$)$);
\node[Larrow](AR3)at(SR3){};
\coordinate(SR4)at($($(2B4-GD)!0.5!(2B4-ZGD)$)!0.5!($(B5-DL)!0.5!(B5-GL)$)$);
\node[Larrow](AR4)at(SR4){};
\draw[Line](AR4)|-($(P2-DD)!0.5!(P2-GD)$);
\coordinate(SR5)at($($(B5-DD)!0.5!(B5-GD)$)!0.6!($(B6-GL)!0.5!(B6-DL)$)$);
\node[Larrow](AR5)at(SR5){};
\coordinate(SR6)at($($(B6-DD)!0.55!(B6-GD)$)!0.5!($(B7-GL)!0.5!(B7-DL)$)$);
\node[Larrow](AR6)at(SR6){};
\draw[Line](AR6)|-($(P3-DD)!0.5!(P3-GD)$);
\coordinate(SR7)at($($(B7-DD)!0.5!(B7-GD)$)+(0.7,0.1)$);
\node[Larrow](AR7)at(SR7){};
%
\coordinate(SR00)at($($(B1-DD)!0.5!(B1-DL)$)!0.5!($(B11-GL)!0.5!(B11-GD)$)$);
\node[Larrow,minimum height=22mm,rotate=270](AR00)at(SR00){};
\node[above right=-15pt and 5pt of AR00]{Downsample};
\coordinate(SR000)at($($(B7-DD)!0.5!(B7-DL)$)!0.5!($(B77-GL)!0.5!(B77-GD)$)$);
\node[Larrow,minimum height=22mm,rotate=90](AR000)at(SR000){};
\node[above left =-15pt and 5pt of AR000]{Upsample};
\coordinate(SR11)at($($(B11-ZGD)!0.5!(B11-ZDD)$)!0.5!($(B22D-GL)!0.5!(B22G-DL)$)$);
\node[Larrow,minimum height=22mm](AR11)at(SR11){};
\coordinate(SR22)at($($(B33-ZGD)!0.4!(B33-ZDD)$)!0.6!($(B22D-GD)!0.5!(B22G-DD)$)$);
\node[Larrow,minimum height=15mm](AR22)at(SR22){};
\draw[Line](AR22)|-($(P11-DD)!0.5!(P11-GD)$);
\coordinate(SR33)at($($(B33-ZGD)!0.65!(B33-ZDD)$)!0.5!(RE.west)$);
\node[Larrow,minimum height=15mm](AR33)at(SR33){};
\coordinate(SR44)at($($(B77-ZGD)!0.65!(B77-ZDD)$)!0.6!(RE.east)$);
\node[Larrow,minimum height=20mm](AR44)at(SR44){};
\draw[Line](AR44)|-($(P22-DD)!0.5!(P22-GD)$);
%fitting
\scoped[on background layer]
\node[draw=none,inner xsep=3mm,inner ysep=2mm,
yshift=-1mm,fill=BrownLine!10,fit=(TT1)(P1-DL)(AR6)(P3-DD),line width=0.75pt](BB1){};
\node[below=13pt of  BB1.south,inner sep=0pt, anchor=south](FT){b) Fine-tune bias only};
%
\scoped[on background layer]
\node[draw=none,inner xsep=3mm,inner ysep=4mm,
yshift=-3mm,fill=cyan!5,fit=(BB1)(FT)(AR7)(PO)(PL11),line width=0.75pt](BB2){};
\node[above=3pt of  BB2.south,inner sep=0pt, anchor=south](FT){c) Lite residual learning};
\scoped[on background layer]
\node[draw=none,inner xsep=1mm,inner ysep=2mm,
yshift=-1mm,fill=magenta!5,fit=(TT1)(P1-DL)(AR6)(P3-DD),line width=0.75pt](BB1){};
\end{scope}
\end{tikzpicture}
```
**Memory-Efficient Adaptation**: Tinytl reduces on-device training costs by freezing convolutional weights and batch normalization, updating only bias terms and lightweight residual connections to minimize memory usage during backpropagation. This approach allows deployment of deep neural networks on resource-constrained edge devices with limited SRAM, facilitating efficient model adaptation without requiring full parameter updates.
:::

In contrast, the TinyTL architecture freezes all weights and updates only the bias terms inserted after convolutional layers. These bias modules are lightweight and require minimal memory, enabling efficient training with a drastically reduced memory footprint. The frozen convolutional layers act as a fixed feature extractor, and only the trainable bias components are involved in adaptation. By avoiding storage of full activation maps and limiting the number of updated parameters, TinyTL allows on-device training under severe resource constraints.

Because the base model remains unchanged, TinyTL assumes that the pretrained features are sufficiently expressive for downstream tasks. The bias terms allow for minor but meaningful shifts in model behavior, particularly for personalization tasks. When domain shift is more significant, TinyTL can optionally incorporate small residual adapters to improve expressivity, all while preserving the system's tight memory and energy profile.

These design choices allow TinyTL to reduce training memory usage by 10×. For instance, adapting a MobileNetV2 model using TinyTL can reduce the number of updated parameters from over 3 million to fewer than 50,000[^fn-tinytl-efficiency]. Combined with quantization, this allows local adaptation on devices with only a few hundred kilobytes of memory—making on-device learning truly feasible in constrained environments.

[^fn-tinytl-efficiency]: **TinyTL Memory Breakthrough**: TinyTL's 60x parameter reduction (3M to 50K) translates to dramatic memory savings. In FP32, MobileNetV2 requires ~12MB for weights plus ~8MB for activation caching during training—exceeding most microcontroller capabilities. TinyTL reduces this to ~200KB weights plus ~400KB activations, fitting comfortably within a 1MB memory budget. Real deployments on STM32H7 achieve 85% of full fine-tuning accuracy while using 15x less memory and completing updates in ~30 seconds versus 8 minutes for full training.

### Residual and Low-Rank Updates {#sec-ondevice-learning-residual-lowrank-updates-930d}

Bias-only updates offer a lightweight path for on-device learning, but they are limited in representational flexibility. When the frozen model does not align well with the target distribution, it may be necessary to allow more expressive adaptation—without incurring the full cost of weight updates. One solution is to introduce residual adaptation modules [@houlsby2019parameter], or low-rank parameterizations, which provide a middle ground between static backbones and full fine-tuning [@hu2021lora].

These methods extend a frozen model by adding trainable layers, which are typically small and computationally inexpensive, that allow the network to respond to new data. The main body of the network remains fixed, while only the added components are optimized. This modularity makes the approach well-suited for on-device adaptation in constrained settings, where small updates must deliver meaningful changes.

#### Adapter-Based Adaptation {#sec-ondevice-learning-adapterbased-adaptation-99c7}

A common implementation involves inserting adapters, which are small residual bottleneck layers, between existing layers in a pretrained model. Consider a hidden representation $h$ passed between layers. A residual adapter introduces a transformation:
$$
h' = h + A(h)
$$
where $A(\cdot)$ is a trainable function, typically composed of two linear layers with a nonlinearity:
$$
A(h) = W_2 \, \sigma(W_1 h)
$$
with $W_1 \in \mathbb{R}^{r \times d}$ and $W_2 \in \mathbb{R}^{d \times r}$, where $r \ll d$. This bottleneck design ensures that only a small number of parameters are introduced per layer.

The adapters act as learnable perturbations on top of a frozen backbone. Because they are small and sparsely applied, they add negligible memory overhead, yet they allow the model to shift its predictions in response to new inputs.

#### Low-Rank Techniques {#sec-ondevice-learning-lowrank-techniques-e65d}

Another efficient strategy is to constrain weight updates themselves to a low-rank structure. Rather than updating a full matrix $W$, we approximate the update as:
$$
\Delta W \approx U V^\top
$$
where $U \in \mathbb{R}^{m \times r}$ and $V \in \mathbb{R}^{n \times r}$, with $r \ll \min(m,n)$. This reduces the number of trainable parameters from $mn$ to $r(m + n)$.

The mathematical intuition behind this decomposition connects to fundamental linear algebra principles: any matrix can be expressed as a sum of rank-one matrices through singular value decomposition. By constraining our updates to low rank (typically $r = 4$ to $16$), we capture the most significant modes of variation while reducing parameters. For a typical transformer layer with dimensions $768 \times 768$, full fine-tuning requires updating 589,824 parameters. With rank-4 decomposition, we update only $768 \times 4 \times 2 = 6,144$ parameters, a 96% reduction, while empirically retaining 85-90% of the adaptation quality.

During adaptation, the new weight is computed as:
$$
W_{\text{adapted}} = W_{\text{frozen}} + U V^\top
$$

This formulation is commonly used in LoRA (Low-Rank Adaptation)[^fn-lora] techniques, originally developed for transformer models [@hu2021lora] but broadly applicable across architectures. Low-rank updates can be implemented efficiently on edge devices, particularly when $U$ and $V$ are small and fixed-point representations are supported (@lst-lowrank-adapter).

[^fn-lora]: **LoRA (Low-Rank Adaptation)**: Introduced by Microsoft in 2021, LoRA enables efficient fine-tuning by learning low-rank decomposition matrices rather than updating full weight matrices. For a weight matrix W, LoRA learns rank-r matrices A and B such that the update is BA (where r << original dimensions). This reduces trainable parameters by 100-10000x while maintaining 90-95% adaptation quality. LoRA has become the standard for parameter-efficient fine-tuning in large language models.

::: {#lst-lowrank-adapter lst-cap="**Low-Rank Adapter**: The code implements a low-rank adapter module by approximating weight updates using matrices \(u\) and \(v\), reducing parameter count while enabling efficient model adaptation on edge devices."}
```{.python}
class Adapter(nn.Module):
    def __init__(self, dim, bottleneck_dim):
        super().__init__()
        self.down = nn.Linear(dim, bottleneck_dim)
        self.up = nn.Linear(bottleneck_dim, dim)
        self.activation = nn.ReLU()

    def forward(self, x):
        return x + self.up(self.activation(self.down(x)))
```
:::

This adapter adds a small residual transformation to a frozen layer. When inserted into a larger model, only the adapter parameters are trained.

#### Edge Personalization {#sec-ondevice-learning-edge-personalization-b2b9}

Adapters are useful when a global model is deployed to many devices and must adapt to device-specific input distributions. In smartphone camera pipelines, environmental lighting, user preferences, or lens distortion vary between users [@rebuffi2017learning]. A shared model can be frozen and fine-tuned per-device using a few residual modules, allowing lightweight personalization without risking catastrophic forgetting. In voice-based systems, adapter modules have been shown to reduce word error rates in personalized speech recognition without retraining the full acoustic model. They also allow easy rollback or switching between user-specific versions.

#### Tradeoffs {#sec-ondevice-learning-tradeoffs-25ee}

Residual and low-rank updates strike a balance between expressivity and efficiency. Compared to bias-only learning, they can model more substantial deviations from the pretrained task. However, they require more memory and compute for training and inference.

When considering residual and low-rank updates for on-device learning, several important tradeoffs emerge. First, these methods consistently demonstrate superior adaptation quality compared to bias-only approaches, particularly when deployed in scenarios involving significant distribution shifts from the original training data [@quinonero2009dataset]. This improved adaptability stems from their increased parameter capacity and ability to learn more complex transformations.

This enhanced adaptability comes at a cost. The introduction of additional layers or parameters inevitably increases both memory requirements and computational latency during forward and backward passes. While these increases are modest compared to full model training, they must be considered when deploying to resource-constrained devices.

Implementing these adaptation techniques requires system-level support for dynamic computation graphs and the ability to selectively inject trainable parameters. Not all deployment environments or inference engines support such capabilities out of the box.

Residual adaptation techniques have proven valuable in mobile and edge computing scenarios where devices have sufficient computational resources. Modern smartphones and tablets can accommodate these adaptations while maintaining acceptable performance characteristics. This makes residual adaptation a practical choice for applications requiring personalization without the overhead of full model retraining.

### Sparse Updates {#sec-ondevice-learning-sparse-updates-879b}

Even when adaptation is restricted to a small number of parameters, including biases or adapter modules, training remains resource-intensive on constrained devices. One promising approach is to selectively update only a task-relevant subset of model parameters, rather than modifying the entire network or introducing new modules. This approach is known as task-adaptive sparse updating [@zhang2020efficient].

The key insight is that not all layers of a deep model contribute equally to performance gains on a new task or dataset. If we can identify a *minimal subset of parameters* that are most impactful for adaptation, we can train only those, reducing memory and compute costs while still achieving meaningful personalization.

#### Sparse Update Design {#sec-ondevice-learning-sparse-update-design-ee7c}

Let a neural network be defined by parameters $\theta = \{\theta_1, \theta_2, \ldots, \theta_L\}$ across $L$ layers. In standard fine-tuning, we compute gradients and perform updates on all parameters:
$$
\theta_i \leftarrow \theta_i - \eta \frac{\partial \mathcal{L}}{\partial \theta_i}, \quad \text{for } i = 1, \ldots, L
$$

In task-adaptive sparse updates, we select a small subset $\mathcal{S} \subset \{1, \ldots, L\}$ such that only parameters in $\mathcal{S}$ are updated:
$$
\theta_i \leftarrow
\begin{cases}
\theta_i - \eta \frac{\partial \mathcal{L}}{\partial \theta_i}, & \text{if } i \in \mathcal{S} \\
\theta_i, & \text{otherwise}
\end{cases}
$$

The challenge lies in selecting the optimal subset $\mathcal{S}$ given memory and compute constraints.

#### Layer Selection {#sec-ondevice-learning-layer-selection-ab3c}

A principled strategy for selecting $\mathcal{S}$ is to use contribution analysis—an empirical method that estimates how much each layer contributes to downstream performance improvement. For example, one can measure the marginal gain from updating each layer independently:

1. Freeze the entire model.
2. Unfreeze one candidate layer.
3. Finetune briefly and evaluate improvement in validation accuracy.
4. Rank layers by performance gain per unit cost (e.g., per KB of trainable memory).

This layer-wise profiling yields a ranking from which $\mathcal{S}$ can be constructed subject to a memory budget.

A concrete example is TinyTrain, a method designed to allow rapid adaptation on-device [@deng2022tinytrain]. TinyTrain pretrains a model along with meta-gradients that capture which layers are most sensitive to new tasks. At runtime, the system dynamically selects layers to update based on task characteristics and available resources.

#### Code Fragment: Selective Layer Updating (PyTorch) {#sec-ondevice-learning-code-fragment-selective-layer-updating-pytorch-3f91}

This pattern can be extended with profiling logic to select layers based on contribution scores or hardware profiles, as shown in @lst-selective-update.

::: {#lst-selective-update lst-cap="**Selective Layer Updating**: This technique allows fine-tuning specific layers of a pre-trained model while keeping others frozen, optimizing computational resources for targeted improvements. *Source: PyTorch Documentation*"}
```{.python}
## Assume model has named layers: ['conv1', 'conv2', 'fc']
## We selectively update only conv2 and fc

for name, param in model.named_parameters():
    if 'conv2' in name or 'fc' in name:
        param.requires_grad = True
    else:
        param.requires_grad = False
```
:::

#### TinyTrain Personalization {#sec-ondevice-learning-tinytrain-personalization-66f2}

Consider a scenario where a user wears an augmented reality headset that performs real-time object recognition. As lighting and environments shift, the system must adapt to maintain accuracy—but training must occur during brief idle periods or while charging.

TinyTrain allows this by using meta-training during offline preparation: the model learns not only to perform the task, but also which parameters are most important to adapt. Then, at deployment, the device performs task-adaptive sparse updates, modifying only a few layers that are most relevant for its current environment. This keeps adaptation fast, energy-efficient, and memory-aware.

#### Tradeoffs {#sec-ondevice-learning-tradeoffs-9186}

Task-adaptive sparse updates introduce several important system-level considerations that must be carefully balanced. First, the overhead of contribution analysis, although primarily incurred during pretraining or initial profiling, represents a non-trivial computational cost. This overhead is typically acceptable since it occurs offline, but it must be factored into the overall system design and deployment pipeline.

Second, the stability of the adaptation process becomes important when working with sparse updates. If too few parameters are selected for updating, the model may underfit the target distribution, failing to capture important local variations. This suggests the need for careful validation of the selected parameter subset before deployment, potentially incorporating minimum thresholds for adaptation capacity.

Third, the selection of updateable parameters must account for hardware-specific characteristics of the target platform. Beyond just considering gradient magnitudes, the system must evaluate the actual execution cost of updating specific layers on the deployed hardware. Some parameters might show high contribution scores but prove expensive to update on certain architectures, requiring a more nuanced selection strategy that balances statistical utility with runtime efficiency.

Despite these tradeoffs, task-adaptive sparse updates provide a powerful mechanism to scale adaptation to diverse deployment contexts, from microcontrollers to mobile devices [@diao2023sparse].

#### Adaptation Strategy Comparison {#sec-ondevice-learning-adaptation-strategy-comparison-1faf}

Each adaptation strategy for on-device learning offers a distinct balance between expressivity, resource efficiency, and implementation complexity. Understanding these tradeoffs is important when designing systems for diverse deployment targets—from ultra-low-power microcontrollers to feature-rich mobile processors.

Bias-only adaptation is the most lightweight approach, updating only scalar offsets in each layer while freezing all other parameters. This significantly reduces memory requirements and computational burden, making it suitable for devices with tight memory and energy budgets. However, its limited expressivity means it is best suited to applications where the pretrained model already captures most of the relevant task features and only minor local calibration is required.

Residual adaptation, often implemented via adapter modules, introduces a small number of trainable parameters into the frozen backbone of a neural network. This allows for greater flexibility than bias-only updates, while still maintaining control over the adaptation cost. Because the backbone remains fixed, training can be performed efficiently and safely under constrained conditions. This method supports modular personalization across tasks and users, making it a favorable choice for mobile settings where moderate adaptation capacity is needed.

Task-adaptive sparse updates offer the greatest potential for task-specific finetuning by selectively updating only a subset of layers or parameters based on their contribution to downstream performance. While this method allows expressive local adaptation, it requires a mechanism for layer selection, through profiling, contribution analysis, or meta-training, which introduces additional complexity. Nonetheless, when deployed carefully, it allows for dynamic tradeoffs between accuracy and efficiency, particularly in systems that experience large domain shifts or evolving input conditions.

These three approaches form a spectrum of tradeoffs. Their relative suitability depends on application domain, available hardware, latency constraints, and expected distribution shift. @tbl-adaptation-strategies summarizes their characteristics:

+---------------------------+------------------------------+----------------------+-------------------------+----------------------------------------+-----------------------------------------+
| Technique                 | Trainable Parameters         | Memory Overhead      | Expressivity            | Use Case Suitability                   | System Requirements                     |
+:==========================+:=============================+:=====================+:========================+:=======================================+:========================================+
| Bias-Only Updates         | Bias terms only              | Minimal              | Low                     | Simple personalization; low variance   | Extreme memory/compute limits           |
+---------------------------+------------------------------+----------------------+-------------------------+----------------------------------------+-----------------------------------------+
| Residual Adapters         | Adapter modules              | Moderate             | Moderate to High        | User-specific tuning on mobile         | Mobile-class SoCs with runtime support  |
+---------------------------+------------------------------+----------------------+-------------------------+----------------------------------------+-----------------------------------------+
| Sparse Layer Updates      | Selective parameter subsets  | Variable             | High (task-adaptive)    | Real-time adaptation; domain shift     | Requires profiling or meta-training     |
+---------------------------+------------------------------+----------------------+-------------------------+----------------------------------------+-----------------------------------------+

: **Adaptation Strategy Trade-Offs**: Table entries characterize three approaches to model adaptation—bias-only updates, selective layer updates, and full finetuning—by quantifying their impact on trainable parameters, memory overhead, expressivity, suitability for different use cases, and system requirements. These characteristics reveal the inherent trade-offs between model flexibility, computational cost, and performance when deploying machine learning systems in dynamic environments. {#tbl-adaptation-strategies}

## Data Efficiency {#sec-ondevice-learning-data-efficiency-c701}

While model adaptation addresses the parameter complexity constraints, the second major challenge is **data scarcity and quality**. On-device learning systems must extract maximum learning value from minimal, noisy, and often unlabeled data streams. This represents a fundamental departure from the data-abundant assumptions of centralized machine learning.

The data constraints we established earlier create a specific set of challenges that require specialized algorithmic solutions:

**Limited volume**: Edge devices rarely observe sufficient data for traditional training. A fitness tracker might collect minutes of labeled activity per day, compared to the hours needed for robust model updates.

**Sparse labeling**: Most edge data lacks supervision. User interactions provide only implicit feedback, and manual labeling is impractical on resource-constrained devices.

**Temporal sparsity**: Data arrives irregularly and may cluster around specific usage patterns, creating imbalanced learning opportunities.

**Distribution shift**: Local data often differs dramatically from pre-training distributions, requiring robust adaptation mechanisms.

These constraints necessitate **sample-efficient learning approaches** that can extract maximal signal from minimal data. Rather than requiring large labeled datasets, on-device systems must leverage transfer learning, few-shot adaptation, and memory-based techniques to enable continuous improvement despite data limitations.

: **Few-Shot Learning Origins**: The concept traces back to human cognition research in the 1940s, but modern few-shot learning emerged from Li Fei-Fei's work at Stanford (2006), who observed that children learn new object categories from just 1-2 examples while machine learning models needed thousands. This "sample efficiency gap" became a defining challenge of practical AI.

This section examines four complementary data efficiency strategies:

- **Few-shot learning** enables adaptation from minimal labeled examples
- **Streaming updates** accommodate data that arrives incrementally over time
- **Experience replay** maximizes learning from limited data through intelligent reuse
- **Data compression** reduces memory requirements while preserving learning signals

Each technique addresses different aspects of the data constraint problem, enabling robust learning even when traditional supervised learning would fail.

### Few-Shot and Streaming {#sec-ondevice-learning-fewshot-streaming-8637}

In conventional machine learning workflows, effective training typically requires large labeled datasets, carefully curated and preprocessed to ensure sufficient diversity and balance. On-device learning, by contrast, must often proceed from only a handful of local examples—collected passively through user interaction or ambient sensing, and rarely labeled in a supervised fashion. These constraints motivate two complementary adaptation strategies: few-shot learning, in which models generalize from a small, static set of examples, and streaming adaptation, where updates occur continuously as data arrives.

Few-shot adaptation is particularly relevant when the device observes a small number of labeled or weakly labeled instances for a new task or user condition [@wang2020generalizing]. In such settings, it is often infeasible to perform full finetuning of all model parameters without overfitting. Instead, methods such as bias-only updates, adapter modules, or prototype-based classification are employed to make use of limited data while minimizing capacity for memorization. Let $D = \{(x_i, y_i)\}_{i=1}^K$ denote a $K$-shot dataset of labeled examples collected on-device. The goal is to update the model parameters $\theta$ to improve task performance under constraints such as:

- Limited number of gradient steps: $T \ll 100$
- Constrained memory footprint: $\|\theta_{\text{updated}}\| \ll \|\theta\|$
- Preservation of prior task knowledge (to avoid catastrophic forgetting)

Keyword spotting (KWS) systems offer a concrete example of few-shot adaptation in a real-world, on-device deployment [@warden2018speech]. These models are used to detect fixed phrases, including phrases like "Hey Siri"[^fn-hey-siri-constraints] or "OK Google", with low latency and high reliability. A typical KWS model consists of a pretrained acoustic encoder (e.g., a small convolutional or recurrent network that transforms input audio into an embedding space) followed by a lightweight classifier. In commercial systems, the encoder is trained centrally using thousands of hours of labeled speech across multiple languages and speakers. However, supporting custom wake words (e.g., "Hey Jarvis") or adapting to underrepresented accents and dialects is often infeasible via centralized training due to data scarcity and privacy concerns.

[^fn-hey-siri-constraints]: **"Hey Siri" Technical Reality**: Apple's "Hey Siri" system operates under extreme constraints—detection must complete within 100ms to feel responsive, while consuming less than 1mW power when listening continuously. The always-on processor monitors audio using a 192KB model running at ~0.5 TOPS. False positive rate must be under 0.001% (less than once per day) while maintaining >95% true positive rate across accents, background noise, and speaking styles. The system processes 16kHz audio in 200ms windows, extracting Mel-frequency features for classification.

Few-shot adaptation solves this problem by finetuning only the output classifier or a small subset of parameters, including bias terms, using just a few example utterances collected directly on the device. For example, a user might provide 5–10 recordings of their custom wake word. These samples are then used to update the model locally, while the main encoder remains frozen to preserve generalization and reduce memory overhead. This allows personalization without requiring additional labeled data or transmitting private audio to the cloud.

Such an approach is not only computationally efficient, but also aligned with privacy-preserving design principles. Because only the output layer is updated, often involving a simple gradient step or prototype computation, the total memory footprint and runtime compute are compatible with mobile-class devices or even microcontrollers. This makes KWS a canonical case study for few-shot learning at the edge, where the system must operate under tight constraints while delivering user-specific performance.

Beyond static few-shot learning, many on-device scenarios benefit from streaming adaptation, where models must learn incrementally as new data arrives [@hayes2020remind]. Streaming adaptation generalizes this idea to continuous, asynchronous settings where data arrives incrementally over time. Let $\{x_t\}_{t=1}^{\infty}$ represent a stream of observations. In streaming settings, the model must update itself after observing each new input, typically without access to prior data, and under bounded memory and compute. The model update can be written generically as:
$$
\theta_{t+1} = \theta_t - \eta_t \nabla \mathcal{L}(x_t; \theta_t)
$$
where $\eta_t$ is the learning rate at time $t$. This form of adaptation is sensitive to noise and drift in the input distribution, and thus often incorporates mechanisms such as learning rate decay, meta-learned initialization, or update gating to improve stability.

Aside from KWS, practical examples of these strategies abound. In wearable health devices, a model that classifies physical activities may begin with a generic classifier and adapt to user-specific motion patterns using only a few labeled activity segments. In smart assistants, user voice profiles are finetuned over time using ongoing speech input, even when explicit supervision is unavailable. In such cases, local feedback, including correction, repetition, or downstream task success, can serve as implicit signals to guide learning.

Few-shot and streaming adaptation highlight the shift from traditional training pipelines to data-efficient, real-time learning under uncertainty. They form a foundation for more advanced memory and replay strategies, which we turn to next.

### Experience Replay {#sec-ondevice-learning-experience-replay-737e}

On-device ML systems face a core tension between continuous adaptation and limited data availability. One common approach to alleviating this tension is experience replay—a memory-based strategy that allows models to retrain on past examples. Originally developed in the context of reinforcement learning and continual learning, replay buffers help prevent catastrophic forgetting and stabilize training in non-stationary environments.

Unlike server-side replay strategies that rely on large datasets and extensive compute, on-device replay must operate with extremely limited capacity, often with tens or hundreds of samples, and must avoid interfering with user experience [@rolnick2019experience]. Buffers may store only compressed features or distilled summaries, and updates must occur opportunistically (e.g., during idle cycles or charging). These system-level constraints reshape how replay is implemented and evaluated in the context of embedded ML.

Let $\mathcal{M}$ represent a memory buffer that retains a fixed-size subset of training examples. At time step $t$, the model receives a new data point $(x_t, y_t)$ and appends it to $\mathcal{M}$. A replay-based update then samples a batch $\{(x_i, y_i)\}_{i=1}^{k}$ from $\mathcal{M}$ and applies a gradient step:
$$
\theta_{t+1} = \theta_t - \eta \nabla_\theta \left[ \frac{1}{k} \sum_{i=1}^{k} \mathcal{L}(x_i, y_i; \theta_t) \right]
$$
where $\theta_t$ are the model parameters, $\eta$ is the learning rate, and $\mathcal{L}$ is the loss function. Over time, this replay mechanism allows the model to reinforce prior knowledge while incorporating new information.

A practical on-device implementation might use a ring buffer to store a small set of compressed feature vectors rather than full input examples. The pseudocode as shown in @lst-replay-buffer illustrates a minimal replay buffer designed for constrained environments.

::: {#lst-replay-buffer lst-cap="**Replay Buffer**: Implements a circular storage mechanism for efficient memory management in constrained environments. This approach allows models to efficiently retain and sample from recent data points, balancing the need to use historical information while incorporating new insights."}
```{.python}
# Replay Buffer Techniques
class ReplayBuffer:
    def __init__(self, capacity):
        self.capacity = capacity
        self.buffer = []
        self.index = 0

    def store(self, feature_vec, label):
        if len(self.buffer) < self.capacity:
            self.buffer.append((feature_vec, label))
        else:
            self.buffer[self.index] = (feature_vec, label)
        self.index = (self.index + 1) % self.capacity

    def sample(self, k):
        return random.sample(self.buffer, min(k, len(self.buffer)))
```
:::

This implementation maintains a fixed-capacity cyclic buffer, storing compressed representations (e.g., last-layer embeddings) and associated labels. Such buffers are useful for replaying adaptation updates without violating memory or energy budgets.

In TinyML applications[^fn-tinyml-scale], experience replay has been applied to problems such as gesture recognition, where devices must continuously improve predictions while observing a small number of events per day. Instead of training directly on the streaming data, the device stores representative feature vectors from recent gestures and uses them to finetune classification boundaries periodically. Similarly, in on-device keyword spotting, replaying past utterances can improve wake-word detection accuracy without the need to transmit audio data off-device.

[^fn-tinyml-scale]: **TinyML Market Reality**: The TinyML market reached $2.4 billion in 2023 and is projected to grow to $23.3 billion by 2030. Over 100 billion microcontrollers ship annually, but fewer than 1% currently support on-device learning due to memory and power constraints. Successful TinyML deployments typically consume <1mW power, use <256KB memory, and cost under $1 per chip. Applications include predictive maintenance (vibration sensors), health monitoring (heart rate variability), and smart agriculture (soil moisture prediction).

While experience replay improves stability in data-sparse or non-stationary environments, it introduces several tradeoffs. Storing raw inputs may breach privacy constraints or exceed storage budgets, especially in vision and audio applications. Replaying from feature vectors reduces memory usage but may limit the richness of gradients for upstream layers. Write cycles to persistent flash memory, which are frequently necessary for long-term storage on embedded devices, can also raise wear-leveling concerns. These constraints require careful co-design of memory usage policies, replay frequency, and feature selection strategies, particularly in continuous deployment scenarios.

### Data Compression {#sec-ondevice-learning-data-compression-8b40}

In many on-device learning scenarios, the raw training data may be too large, noisy, or redundant to store and process effectively. This motivates the use of compressed data representations, where the original inputs are transformed into lower-dimensional embeddings or compact encodings that preserve salient information while minimizing memory and compute costs.

Compressed representations serve two complementary goals. First, they reduce the footprint of stored data, allowing devices to maintain longer histories or replay buffers under tight memory budgets [@sanh2019distilbert]. Second, they simplify the learning task by projecting raw inputs into more structured feature spaces, often learned via pretraining or meta-learning, in which efficient adaptation is possible with minimal supervision.

One common approach is to encode data points using a pretrained feature extractor and discard the original high-dimensional input. For example, an image $x_i$ might be passed through a convolutional neural network (CNN) to produce an embedding vector $z_i = f(x_i)$, where $f(\cdot)$ is a fixed feature encoder. This embedding captures visual structure (e.g., shape, texture, or spatial layout) in a compact representation, usually ranging from 64 to 512 dimensions, suitable for lightweight downstream adaptation.

Mathematically, training can proceed over compressed samples $(z_i, y_i)$ using a lightweight decoder or projection head. Let $\theta$ represent the trainable parameters of this decoder model, which is typically a small neural network that maps from compressed representations to output predictions. As each example is presented, the model parameters are updated using gradient descent:
$$
\theta_{t+1} = \theta_t - \eta \nabla_\theta \mathcal{L}\big(g(z_i; \theta), y_i\big)
$$
Here:

- $z_i$ is the compressed representation of the $i$-th input,
- $y_i$ is the corresponding label or supervision signal,
- $g(z_i; \theta)$ is the decoder's prediction,
- $\mathcal{L}$ is the loss function measuring prediction error,
- $\eta$ is the learning rate, and
- $\nabla_\theta$ denotes the gradient with respect to the parameters $\theta$.

This formulation highlights how only a compact decoder model, which has the parameter set $\theta$, needs to be trained, making the learning process feasible even when memory and compute are limited.

Advanced approaches go beyond fixed encoders by learning discrete or sparse dictionaries that represent data using low-rank or sparse coefficient matrices. For instance, a dataset of sensor traces can be factorized as $X \approx DC$, where $D$ is a dictionary of basis patterns and $C$ is a block-sparse coefficient matrix indicating which patterns are active in each example. By updating only a small number of dictionary atoms or coefficients, the model can adapt with minimal overhead.

Compressed representations are particularly useful in privacy-sensitive settings, as they allow raw data to be discarded or obfuscated after encoding. Furthermore, compression acts as an implicit regularizer, smoothing the learning process and mitigating overfitting when only a few training examples are available.

In practice, these strategies have been applied in domains such as keyword spotting, where raw audio signals are first transformed into Mel-frequency cepstral coefficients (MFCCs)[^fn-mfcc]—a compact, lossy representation of the power spectrum of speech. These MFCC vectors serve as compressed inputs for downstream models, enabling local adaptation using only a few kilobytes of memory.

[^fn-mfcc]: **Mel-Frequency Cepstral Coefficients (MFCCs)**: Audio features that mimic human auditory perception by applying mel-scale frequency warping (emphasizing lower frequencies where speech information concentrates) followed by cepstral analysis. A typical MFCC extraction converts 16kHz audio windows into 12-13 coefficients, reducing a 320-sample window (20ms) from 640 bytes to ~50 bytes while preserving speech intelligibility. Widely used in speech recognition since the 1980s due to robustness against noise and computational efficiency. Instead of storing raw audio waveforms, which are large and computationally expensive to process, devices store and learn from these compressed feature vectors directly. Similarly, in low-power computer vision systems, embeddings extracted from lightweight CNNs are retained and reused for few-shot learning. These examples illustrate how representation learning and compression serve as foundational tools for scaling on-device learning to memory- and bandwidth-constrained environments.

### Tradeoffs Summary {#sec-ondevice-learning-tradeoffs-summary-934b}

Each of the techniques introduced in this section, few-shot learning, experience replay, and compressed data representations, offers a strategy for adapting models on-device when data is scarce or streaming. However, they operate under different assumptions and constraints, and their effectiveness depends on system-level factors such as memory capacity, data availability, task structure, and privacy requirements.

Few-shot adaptation excels when a small but informative set of labeled examples is available, especially when personalization or rapid task-specific tuning is required. It minimizes compute and data needs, but its effectiveness hinges on the quality of pretrained representations and the alignment between the initial model and the local task.

Experience replay addresses continual adaptation by mitigating forgetting and improving stability, especially in non-stationary environments. It allows reuse of past data, but requires memory to store examples and compute cycles for periodic updates. Replay buffers may also raise privacy or longevity concerns, especially on devices with limited storage or flash write cycles.

Compressed data representations reduce the footprint of learning by transforming raw data into compact feature spaces. This approach supports longer retention of experience and efficient finetuning, particularly when only lightweight heads are trainable. However, compression can introduce information loss, and fixed encoders may fail to capture task-relevant variability if they are not well-aligned with deployment conditions. @tbl-ondevice-techniques summarizes key tradeoffs:

+-----------------------------+-----------------------------+----------------------------+---------------------------+
| Technique                   | Data Requirements           | Memory/Compute Overhead    | Use Case Fit              |
+:============================+:============================+:===========================+:==========================+
| Few-Shot Adaptation         | Small labeled set (K-shots) | Low                        | Personalization, quick    |
|                             |                             |                            | on-device finetuning      |
+-----------------------------+-----------------------------+----------------------------+---------------------------+
| Experience Replay           | Streaming data              | Moderate (buffer & update) | Non-stationary data,      |
|                             |                             |                            | stability under drift     |
+-----------------------------+-----------------------------+----------------------------+---------------------------+
| Compressed Representations  | Unlabeled or encoded data   | Low to Moderate            | Memory-limited devices,   |
|                             |                             |                            | privacy-sensitive contexts|
+-----------------------------+-----------------------------+----------------------------+---------------------------+

: **On-Device Learning Trade-Offs**: Few-shot adaptation balances data efficiency with model personalization by leveraging small labeled datasets, but requires careful consideration of memory and compute constraints for deployment on resource-limited devices. The table summarizes key considerations for selecting appropriate on-device learning techniques based on application requirements and available resources. {#tbl-ondevice-techniques}

In practice, these methods are not mutually exclusive. Many real-world systems combine them to achieve robust, efficient adaptation. For example, a keyword spotting system may use compressed audio features (e.g., MFCCs), finetune a few parameters from a small support set, and maintain a replay buffer of past embeddings for continual refinement.

Together, these strategies embody the core challenge of on-device learning: achieving reliable model improvement under persistent constraints on data, compute, and memory.

## Federated Learning {#sec-ondevice-learning-federated-learning-6e7e}

The model adaptation and data efficiency techniques we've explored enable individual devices to learn effectively within their resource constraints. However, these local solutions create a new challenge: **distributed coordination**. How can thousands of independently learning devices share knowledge while preserving privacy and respecting communication limitations?

Individual on-device learning, while powerful, faces fundamental limitations when devices operate in isolation:

**Data heterogeneity**: Each device observes only a narrow slice of the full data distribution, limiting generalization.

**Resource inequality**: Device capabilities vary dramatically, creating learning imbalances across the population.

**Knowledge isolation**: Valuable insights learned on one device cannot benefit others, reducing overall system intelligence.

**Quality degradation**: Without coordination, models may diverge or degrade over time due to local biases.

**Federated learning emerges as the solution to distributed coordination constraints**. It enables privacy-preserving collaboration where devices contribute to collective intelligence without sharing raw data. This approach transforms the constraint of data locality from a limitation into a privacy feature, allowing systems to learn from population-scale data while keeping individual information secure.

The privacy requirements here directly connect to security and privacy principles that become crucial in production deployments. Rather than viewing individual device learning and coordinated learning as separate paradigms, federated learning represents the natural evolution of on-device systems when deployed at scale.

::: {.callout-definition title="Definition of Federated Learning"}

**Federated Learning** is a _decentralized machine learning approach_ in which training occurs across a population of distributed devices, each using its _private, locally collected data_. Rather than transmitting raw data to a central server, devices share only _model updates_, including gradients and weight changes, which are then aggregated to improve a shared global model. This approach _preserves data privacy_ while enabling _collective intelligence across diverse environments_. As federated learning matures, it integrates _privacy-enhancing technologies, communication-efficient protocols, and personalization strategies_, making it foundational for scalable, privacy-conscious ML systems.

:::

To better understand the role of federated learning, it is useful to contrast it with other learning paradigms. @fig-learning-paradigms illustrates the distinction between offline learning, on-device learning, and federated learning. In traditional offline learning, all data is collected and processed centrally. The model is trained in the cloud using curated datasets and is then deployed to edge devices without further adaptation. In contrast, on-device learning allows local model adaptation using data generated on the device itself, supporting personalization but in isolation—without sharing insights across users. Federated learning bridges these two extremes by enabling localized training while coordinating updates globally. It retains data privacy by keeping raw data local, yet benefits from distributed model improvements by aggregating updates from many devices.

::: {#fig-learning-paradigms fig-env="figure" fig-pos="htb"}
```{.tikz}
\begin{tikzpicture}[line join=round,font=\usefont{T1}{phv}{m}{n}\small]
\tikzset{
 LineD/.style={line width=0.2pt,black!50,text=black},
 pics/cloud/.style = {
        code = {
\colorlet{red}{BrownLine}
\begin{scope}[local bounding box=CLO,scale=1.8, every node/.append style={transform shape}]
\draw[red,line width=1.25pt,fill=yellow!10](0,0)to[out=170,in=180,distance=11](0.1,0.61)
to[out=90,in=105,distance=17](1.07,0.71)
to[out=20,in=75,distance=7](1.48,0.36)
to[out=350,in=0,distance=7](1.48,0)--(0,0);
\end{scope}
}
},
circles/.pic={
\pgfkeys{/channel/.cd, #1}
\node[circle,draw=\channelcolor,line width=\linewidth,fill=\channelcolor!10,
minimum size=6.5mm](\picname){};
        }
}
\pgfkeys{
  /channel/.cd,
  channelcolor/.store in=\channelcolor,
  drawchannelcolor/.store in=\drawchannelcolor,
  scalefac/.store in=\scalefac,
  linewidth/.store in=\linewidth,
  picname/.store in=\picname,
  channelcolor=BrownLine,
  drawchannelcolor=BrownLine,
  scalefac=1,
  linewidth=0.6pt,
  dashed/.is if=box@dashed,
  dashed/.default=true,
  picname=C
}

\tikzset {
  pics/server/.style = {
    code = {
      \colorlet{red}{BlueLine}
      \begin{scope}[anchor=center, transform shape,scale=0.8, every node/.append style={transform shape}]
        \draw[red,line width=1.25pt,fill=white](-0.55,-0.5) rectangle (0.55,0.5);
\foreach \i in {-0.25,0,0.25} {
                \draw[cyan,line width=1.25pt]( -0.55,\i) -- (0.55, \i);
}
        \foreach \i in {-0.375, -0.125, 0.125, 0.375} {
          \draw[cyan!50!black!90,line width=1.25pt](-0.45,\i)--(0,\i);
          \fill[cyan!50!black!90](0.35,\i) circle (1.5pt);
        }

        \draw[red,line width=1.75pt](0,-0.53) |- (-0.55,-0.7);
        \draw[red,line width=1.75pt](0,-0.53) |- (0.55,-0.7);
      \end{scope}
    }
  }
}

\tikzset {
pics/mobile/.style = {
        code = {
\colorlet{red}{BrownLine}
\begin{scope}[local bounding box=MOB,scale=0.4, every node/.append style={transform shape}]
\node[rectangle,draw=red,minimum height=94,minimum width=57,
            rounded corners=6,thick,fill=red!10](R1){};
\node[rectangle,draw=red,minimum height=67,minimum width=44,thick,fill=white](R2){};
\node[circle,minimum size=8,below= 2pt of R2,inner sep=0pt,thick,fill=red!90]{};
\node[rectangle,fill=red,minimum height=2,minimum width=20,above= 4pt of R2,inner sep=0pt,thick]{};
%
 \end{scope}
     }
  }
}

\newcommand{\mobilni}[1]{
\colorlet{red}{#1}
\begin{scope}[local bounding box=MOB,scale=0.4, every node/.append style={transform shape}]
\node[rectangle,draw=red,minimum height=94,minimum width=57,
            rounded corners=6,thick,fill=red!10](R1){};
\node[rectangle,draw=red,minimum height=67,minimum width=44,thick,fill=white](R2){};
\node[circle,minimum size=8,below= 2pt of R2,inner sep=0pt,thick,fill=red!90]{};
\node[rectangle,fill=red,minimum height=2,minimum width=20,above= 4pt of R2,inner sep=0pt,thick]{};
 \end{scope}
}
\newcommand{\krugovi}[2]{
\begin{scope}[local bounding box=CIRCLE1,shift={($(0,0)+(6.0,0)$)},
scale=#1, every node/.append style={transform shape}]
%1 column
\foreach \j in {1,2} {
  \pgfmathsetmacro{\y}{(1-\j)*0.83 + 0.7}
  \pic at (-2.8,\y) {circles={channelcolor=RedLine, linewidth=#2pt,picname=1CD\j}};
}
%2 column
\foreach \j in {1,2,3} {
  \pgfmathsetmacro{\y}{(1.5-\j)*0.83 + 0.7}
  \pic at (-1.5,\y) {circles={channelcolor=VioletLine, linewidth=#2pt,picname=2CD\j}};
}
%3 column
\foreach \i in {1,...,4} {
  \pgfmathsetmacro{\y}{(2-\i)*0.83+0.7}
  \pic at (0,\y) {circles={channelcolor=BrownLine, linewidth=#2pt,picname=3CD\i}};
}
%4 column
\foreach \j in {1,2,3,4} {
  \pgfmathsetmacro{\y}{(2-\j)*0.83 + 0.7}
  \pic at (1.5,\y) {circles={channelcolor=BlueLine, linewidth=#2pt,picname=4CD\j}};
}
%5 column
\foreach \j in {1,2} {
  \pgfmathsetmacro{\y}{(1-\j)*0.83 + 0.7}
  \pic at (2.8,\y) {circles={channelcolor=RedLine, linewidth=#2pt,picname=5CD\j}};
}
\foreach \i in {1,2}{
  \foreach \j in {1,2,3}{
\draw[LineD](1CD\i)--(2CD\j);
}}
\foreach \i in {1,2,3}{
  \foreach \j in {1,2,3,4}{
\draw[LineD](2CD\i)--(3CD\j);
}}
\foreach \i in {1,2,3,4}{
  \foreach \j in {1,2,3,4}{
\draw[LineD](3CD\i)--(4CD\j);
}}
\foreach \i in {1,2,3,4}{
  \foreach \j in {1,2}{
\draw[LineD](4CD\i)--(5CD\j);
}}
\end{scope}
}

\krugovi{0.6}{0.6}
\node[above=6pt of CIRCLE1](OL){\textbf{Offline learning}};

\begin{scope}[local bounding box=CLOUD1,shift={($(CIRCLE1)+(-1.4,-4.95)$)},
scale=1.2, every node/.append style={transform shape}]
\pic[shift={(0,0)}] at (0,0) {cloud};
\end{scope}

\begin{scope}[local bounding box=SERVER1,shift={($(CLOUD1)+(0,0)$)},
scale=1.2, every node/.append style={transform shape}]
\pic[shift={(-0.1,-0.1)}] at (0,0) {server};
\end{scope}
\node[below=2pt of SERVER1]{Data};
\draw[VioletLine,line width=1pt,-latex,shorten >=2pt,shorten <=-4pt](CLOUD1.80)--(CIRCLE1.288);
\draw[VioletLine,line width=1pt,latex-,shorten >=2pt,shorten <=-4pt](CLOUD1.110)--(CIRCLE1.245);
%%%
%center
\begin{scope}[local bounding box=CENTER]
\begin{scope}[local bounding box=MOBILE1,shift={($(CLOUD1)+(4.5,1.9)$)},
scale=3.35, every node/.append style={transform shape}]
 \pic[shift={(0,0)}] at (0,0) {mobile};
\end{scope}
\begin{scope}[local bounding box=KRUGOVI,shift={($(MOBILE1)+(-6,0)$)},
scale=1, every node/.append style={transform shape}]
\krugovi{0.3}{0.3}
\end{scope}
\node[draw=GreenLine, line width=0.75pt, fill=GreenL!44, align=center,
minimum height=10mm](AM)at($(MOBILE1.east)+(2.5,0)$){Adapt model\\ based on\\ local data};
\draw[VioletLine,line width=1pt,latex-,shorten <=4pt](AM.165)--++(180:1.3);
\draw[VioletLine,line width=1pt,-latex,shorten <=4pt](AM.195)--++(180:1.3);
\end{scope}
\node[below=12pt of CENTER,align=center](LAO){Locally adapt once to a few samples\\ (e.g., few shot learning)
     or continuously\\ (e.g., unsupervised learning)};
%%%%
%RIGHT
\begin{scope}[local bounding box=RIGHT,shift={($(CENTER)+(5.2,0)$)}]
\begin{scope}[local bounding box=CLOUD2,shift={($(0,0)+(0,1.0)$)},
scale=0.65, every node/.append style={transform shape}]
\pic[shift={(0,0)}] at (0,0) {cloud};
\end{scope}
\node[]at(0.8,1.35){Coordinator};

\begin{scope}[local bounding box=MOBILE2,shift={($(CLOUD2)+(-1.25,-2.9)$)},
scale=1.2, every node/.append style={transform shape}]
 \mobilni{OliveLine}
\end{scope}
\begin{scope}[local bounding box=KRUGOVI,shift={($(MOBILE2)+(-3,0)$)},
scale=0.5, every node/.append style={transform shape}]
\krugovi{0.2}{0.2}
\end{scope}
%
\begin{scope}[local bounding box=MOBILE3,shift={($(CLOUD2)+(1.25,-2.9)$)},
scale=1.2, every node/.append style={transform shape}]
 \mobilni{OliveLine}
\end{scope}
\begin{scope}[local bounding box=KRUGOVI,shift={($(MOBILE3)+(-3,0)$)},
scale=0.5, every node/.append style={transform shape}]
\krugovi{0.2}{0.2}
\end{scope}
\node[below=3pt of MOBILE2]{Worker};
\node[below=3pt of MOBILE3]{Worker};
\node[draw=RedLine, line width=0.75pt, fill=RedL!44, align=center,
minimum height=10mm](TD)at($(MOBILE3.east)+(2.25,0)$){Training on \\ local data};
\end{scope}
\node[]at($(MOBILE2)!0.5!(MOBILE3)$){$\bullet$ $\bullet$ $\bullet$};

\draw[VioletLine,line width=1pt,latex-,shorten <=4pt](TD.165)--++(180:1.2);
\draw[VioletLine,line width=1pt,-latex,shorten <=4pt](TD.195)--++(180:1.2);
\draw[VioletLine,line width=1pt,latex-latex,shorten <=1pt](MOBILE2)--
       node[left=4pt,align=center,text=black]{Model \\ updates}(CLOUD2);
       \draw[VioletLine,line width=1pt,latex-latex,shorten <=1pt](MOBILE3)--
       node[right=4pt,align=center,text=black]{5G \\ connectivity}(CLOUD2);
%
\path[red](OL)-|coordinate(MO)(MOBILE1);
\path[red](OL)-|coordinate(CO)(CLOUD2);
\path[red](LAO)-|coordinate(COD)(CLOUD2);
\node[align=center]at($(COD)+(1.5,0)$){Aggregate model updates across \\
multiple users to globally improve\\ model from more diverse data};
\node[]at(MO){\textbf{On-device learning}};
\node[]at(CO){\textbf{Federated learning}};
\end{tikzpicture}
```
**Learning Paradigm Comparison**: Federated learning balances data privacy with collective model improvement by coordinating local training across distributed devices, unlike offline learning’s centralized approach or on-device learning’s isolated adaptation. This figure contrasts how each paradigm handles data location and model update strategies, revealing the trade-offs between personalization, data security, and global knowledge sharing.
:::

This section explores the principles and practical considerations of federated learning in the context of mobile and embedded systems. It begins by outlining the canonical FL protocols and their system implications. It then discusses device participation constraints, communication-efficient update mechanisms, and strategies for personalized learning. Throughout, the emphasis remains on how federated methods can extend the reach of on-device learning by enabling distributed model training across diverse and resource-constrained hardware platforms.

### Federated Learning Motivation {#sec-ondevice-learning-federated-learning-motivation-3143}

Federated learning (FL) is a decentralized paradigm for training machine learning models across a population of devices without transferring raw data to a central server [@mcmahan2017communication]. Unlike traditional centralized training pipelines, which require aggregating all training data in a single location, federated learning distributes the training process itself. Each participating device computes updates based on its local data and contributes to a global model through an aggregation protocol, typically coordinated by a central server. This shift in training architecture aligns closely with the needs of mobile, edge, and embedded systems, where privacy, communication cost, and system heterogeneity impose significant constraints on centralized approaches.

The relevance of federated learning appears in several practical domains. In mobile keyboard applications, such as Google's Gboard, the system must continuously improve text prediction models based on user-specific input patterns [@hard2018federated]. Federated learning allows the system to train on device-local keystroke data, while maintaining privacy, while still contributing to a shared model that benefits all users. Wearable health monitors collect biometric signals that vary greatly between individuals. Training models centrally on such data would require uploading sensitive physiological traces, raising both ethical and regulatory concerns. FL mitigates these issues by enabling model updates to be computed directly on the wearable device.

In the context of smart assistants and voice interfaces, devices must adapt to individual voice profiles while minimizing false activations. Wake-word models, for instance, can be personalized locally and periodically synchronized through federated updates, avoiding the need to transmit raw voice recordings. Industrial and environmental sensors, deployed in remote locations or operating under severe bandwidth limitations, benefit from federated learning by enabling local adaptation and global coordination without constant connectivity.

These examples illustrate how federated learning bridges the gap between model improvement and system-level constraints. It allows personalization without compromising user privacy, supports learning under limited connectivity, and distributes computation across a diverse and heterogeneous device fleet. However, these benefits come with new challenges. Federated ML systems must account for client variability, communication efficiency, and the non-IID nature of local data distributions. They must ensure robustness to adversarial behavior and provide guarantees on model performance despite partial participation or dropout.

The remainder of this section explores the key techniques and tradeoffs that define federated learning in on-device settings. We begin by examining the core learning protocols that govern coordination across devices, and proceed to investigate strategies for scheduling, communication efficiency, and personalization.

### Learning Protocols {#sec-ondevice-learning-learning-protocols-139a}

Federated learning protocols define the rules and mechanisms by which devices collaborate to train a shared model. These protocols govern how local updates are computed, aggregated, and communicated, as well as how devices participate in the training process. The choice of protocol has significant implications for system performance, communication overhead, and model convergence.

In this section, we outline the core components of federated learning protocols, including local training, aggregation methods, and communication strategies. We also discuss the tradeoffs associated with different approaches and their implications for on-device ML systems.

#### Local Training {#sec-ondevice-learning-local-training-e73d}

Local training refers to the process by which individual devices compute model updates based on their local data. This step is critical in federated learning, as it allows devices to adapt the shared model to their specific contexts without transferring raw data. The local training process involves the following steps:

1. **Model Initialization**: Each device initializes its local model parameters, often by downloading the latest global model from the server.
2. **Local Data Sampling**: The device samples a subset of its local data for training. This data may be non-IID, meaning that it may not be uniformly distributed across devices.
3. **Local Training**: The device performs a number of training iterations on its local data, updating the model parameters based on the computed gradients.
4. **Model Update**: After local training, the device computes a model update (e.g., the difference between the updated and initial parameters) and prepares to send it to the server.
5. **Communication**: The device transmits the model update to the server, typically using a secure communication channel to protect user privacy.
6. **Model Aggregation**: The server aggregates the updates from multiple devices to produce a new global model, which is then distributed back to the participating devices.

This process is repeated iteratively, with devices periodically downloading the latest global model and performing local training. The frequency of these updates can vary based on system constraints, device availability, and communication costs.

#### Protocols Overview {#sec-ondevice-learning-protocols-overview-4582}

At the heart of federated learning is a coordination mechanism that allows many devices, each having access to only a small, local dataset, to collaboratively train a shared model. This is achieved through a protocol where client devices perform local training and transmit model updates to a central server. The server aggregates these updates to refine a global model, which is then redistributed to clients for the next training round. This cyclical procedure decouples the learning process from centralized data collection, making it well-suited to mobile and edge environments where user data is private, bandwidth is constrained, and device participation is sporadic.

The most widely used baseline for this process is Federated Averaging (FedAvg)[^fn-fedavg], which has become a canonical algorithm for federated learning [@mcmahan2017communication]. In FedAvg, each device trains its local copy of the model using stochastic gradient descent (SGD) on its private data.

[^fn-fedavg]: **Federated Averaging (FedAvg)**: Introduced by Google in 2017, FedAvg revolutionized distributed ML by averaging model weights rather than gradients. Each client performs multiple local SGD steps (typically 1-20) before sending weights to the server, reducing communication by 10-100x compared to distributed SGD. The key insight: local updates contain richer information than single gradients, enabling convergence with far fewer communication rounds. FedAvg powers production systems like Gboard, processing billions of devices. After a fixed number of local steps, each device sends its updated model parameters to the server. The server computes a weighted average of these parameters, which are weighted according to the number of data samples on each device, and updates the global model accordingly. This updated model is then sent back to the devices, completing one round of training.

Formally, let $\mathcal{D}_k$ denote the local dataset on client $k$, and let $\theta_k^t$ be the parameters of the model on client $k$ at round $t$. Each client performs $E$ steps of SGD on its local data, yielding an update $\theta_k^{t+1}$. The central server then aggregates these updates as:
$$
\theta^{t+1} = \sum_{k=1}^{K} \frac{n_k}{n} \theta_k^{t+1}
$$
where $n_k = |\mathcal{D}_k|$ is the number of samples on device $k$, $n = \sum_k n_k$ is the total number of samples across participating clients, and $K$ is the number of active devices in the current round.

This basic structure introduces a number of design choices and tradeoffs. The number of local steps $E$ impacts the balance between computation and communication: larger $E$ reduces communication frequency but risks divergence if local data distributions vary too much. The selection of participating clients affects convergence stability and fairness. In real-world deployments, not all devices are available at all times, and hardware capabilities may differ substantially, requiring robust participation scheduling and failure tolerance.

#### Client Scheduling {#sec-ondevice-learning-client-scheduling-f675}

Federated learning operates under the assumption that clients, devices, which hold local data, periodically become available for participation in training rounds. In real-world systems, client availability is intermittent and variable. Devices may be turned off, disconnected from power, lacking network access, or otherwise unable to participate at any given time. As a result, client scheduling plays a central role in the effectiveness and efficiency of distributed learning.

At a baseline level, federated ML systems define eligibility criteria for participation. Devices must meet minimum requirements such as being plugged in, connected to Wi-Fi, and idle, to avoid interfering with user experience or depleting battery resources. These criteria determine which subset of the total population is considered “available" for any given training round.

Beyond these operational filters, devices also differ in their hardware capabilities, data availability, and network conditions. Some smartphones contain many recent examples relevant to the current task, while others have outdated or irrelevant data. Network bandwidth and upload speed may vary widely depending on geography and carrier infrastructure. As a result, selecting clients at random can lead to poor coverage of the underlying data distribution and unstable model convergence.

Availability-driven selection introduces participation bias: clients with favorable conditions, including frequent charging, high-end hardware, and consistent connectivity, are more likely to participate repeatedly, while others are systematically underrepresented. This can skew the resulting model toward behaviors and preferences of a privileged subset of the population, raising both fairness and generalization concerns.

The severity of participation bias becomes apparent when examining real deployment statistics. Studies of federated learning deployments show that the most active 10% of devices can contribute to over 50% of training rounds, while the bottom 50% of devices may never participate at all. This creates a feedback loop: models become increasingly optimized for users with high-end devices and stable connectivity, potentially degrading performance for resource-constrained users who need adaptation the most. A keyboard prediction model might become biased toward the typing patterns of users with flagship phones who charge overnight, missing important linguistic variations from users with budget devices or irregular charging patterns.

To address these challenges, systems must balance scheduling efficiency with client diversity. A key approach involves using stratified or quota-based sampling to ensure representative client participation across different groups. Some systems implement "fairness budgets" that track cumulative participation and actively prioritize underrepresented devices when they become available. Others use importance sampling techniques to reweight contributions based on estimated population statistics rather than raw participation rates. For instance, asynchronous buffer-based techniques allow participating clients to contribute model updates independently, without requiring synchronized coordination in every round [@fedbuff]. This model has been extended to incorporate staleness awareness [@fedstale] and fairness mechanisms [@fedstaleweight], preventing bias from over-active clients who might otherwise dominate the training process.

To address these challenges, federated ML systems implement adaptive client selection strategies. These include prioritizing clients with underrepresented data types, targeting geographies or demographics that are less frequently sampled, and using historical participation data to enforce fairness constraints. Systems incorporate predictive modeling to anticipate future client availability or success rates, improving training throughput.

Selected clients perform one or more local training steps on their private data and transmit their model updates to a central server. These updates are aggregated to form a new global model. Typically, this aggregation is weighted, where the contributions of each client are scaled, for example, by the number of local examples used during training, before averaging. This ensures that clients with more representative or larger datasets exert proportional influence on the global model.

These scheduling decisions directly impact system performance. They affect convergence rate, model generalization, energy consumption, and overall user experience. Poor scheduling can result in excessive stragglers, overfitting to narrow client segments, or wasted computation. As a result, client scheduling is not merely a logistical concern; it is a core component of system design in federated learning, demanding both algorithmic insight and infrastructure-level coordination.

#### Efficient Communication {#sec-ondevice-learning-efficient-communication-d452}

One of the principal bottlenecks in federated ML systems is the cost of communication between edge clients and the central server. Transmitting full model weights or gradients after every training round can overwhelm bandwidth and energy budgets, particularly for mobile or embedded devices operating over constrained wireless links[^fn-wireless-constraints]. To address this, a range of techniques have been developed to reduce communication overhead while preserving learning efficacy.

[^fn-wireless-constraints]: **Wireless Communication Reality**: Mobile devices face severe bandwidth and energy constraints for federated learning. LTE uploads average 5-10 Mbps versus 50+ Mbps downloads, creating asymmetric bottlenecks. Transmitting a 50MB model update consumes ~100mAh battery (2-3% of typical capacity) and takes 40-80 seconds. WiFi improves throughput but isn't always available. Low-power devices using LoRaWAN or NB-IoT face even harsher limits—LoRaWAN maxes at 50kbps with 1% duty cycle restrictions, making frequent updates impractical without aggressive compression.

These techniques fall into three primary categories: model compression, selective update sharing, and architectural partitioning.

Model compression methods aim to reduce the size of transmitted updates through quantization[^fn-gradient-quantization], sparsification, or subsampling. Instead of sending full-precision gradients, a client transmits 8-bit quantized updates or communicates only the top-$k$ gradient elements[^fn-gradient-sparsification] with highest magnitude.

[^fn-gradient-quantization]: **Gradient Quantization**: Reduces communication by converting FP32 gradients to lower precision (INT8, INT4, or even 1-bit). Advanced techniques like signSGD use only gradient signs, achieving 32x compression. Error compensation methods accumulate quantization errors for later transmission, maintaining convergence quality. Real deployments achieve 8-16x communication reduction with <1% accuracy loss.

[^fn-gradient-sparsification]: **Gradient Sparsification**: Transmits only the largest gradients by magnitude (typically top 1-10%), dramatically reducing communication. Gradient accumulation stores untransmitted gradients locally until they become large enough to send. This technique exploits the observation that most gradients are small and contribute minimally to convergence, achieving 10-100x compression ratios while maintaining training effectiveness. These techniques reduce transmission size with limited impact on convergence when applied carefully.

Selective update sharing further reduces communication by transmitting only subsets of model parameters or updates. In layer-wise selective sharing, clients update only certain layers, typically the final classifier or adapter modules, while keeping the majority of the backbone frozen. This reduces both upload cost and the risk of overfitting shared representations to non-representative client data.

Split models and architectural partitioning divide the model into a shared global component and a private local component. Clients train and maintain their private modules independently while synchronizing only the shared parts with the server. This allows for user-specific personalization with minimal communication and privacy leakage.

All of these approaches operate within the context of a federated aggregation protocol. A standard baseline for aggregation is Federated Averaging (FedAvg), in which the server updates the global model by computing a weighted average of the client updates received in a given round. Let $\mathcal{K}_t$ denote the set of participating clients in round $t$, and let $\theta_k^t$ represent the locally updated model parameters from client $k$. The server computes the new global model $\theta^{t+1}$ as:
$$
\theta^{t+1} = \sum_{k \in \mathcal{K}_t} \frac{n_k}{n_{\mathcal{K}_t}} \theta_k^t
$$

Here, $n_k$ is the number of local training examples at client $k$, and $n_{\mathcal{K}_t} = \sum_{k \in \mathcal{K}_t} n_k$ is the total number of training examples across all participating clients. This data-weighted aggregation ensures that clients with more training data exert a proportionally larger influence on the global model, while also accounting for partial participation and heterogeneous data volumes.

However, communication-efficient updates can introduce tradeoffs. Compression may degrade gradient fidelity, selective updates can limit model capacity, and split architectures may complicate coordination. As a result, effective federated learning requires careful balancing of bandwidth constraints, privacy concerns, and convergence dynamics—a balance that depends heavily on the capabilities and variability of the client population.

#### Federated Personalization {#sec-ondevice-learning-federated-personalization-3c73}

While compression and communication strategies improve scalability, they do not address a important limitation of the global federated learning paradigm—its inability to capture user-specific variation. In real-world deployments, devices often observe distinct and heterogeneous data distributions. A one-size-fits-all global model may underperform when applied uniformly across diverse users. This motivates the need for personalized federated learning, where local models are adapted to user-specific data without compromising the benefits of global coordination.

Let $\theta_k$ denote the model parameters on client $k$, and $\theta_{\text{global}}$ the aggregated global model. Traditional FL seeks to minimize a global objective:
$$
\min_\theta \sum_{k=1}^K w_k \mathcal{L}_k(\theta)
$$
where $\mathcal{L}_k(\theta)$ is the local loss on client $k$, and $w_k$ is a weighting factor (e.g., proportional to local dataset size). However, this formulation assumes that a single model $\theta$ can serve all users well. In practice, local loss landscapes $\mathcal{L}_k$ often differ significantly across clients, reflecting non-IID data distributions and varying task requirements.

Personalization modifies this objective to allow each client to maintain its own adapted parameters $\theta_k$, optimized with respect to both the global model and local data:
$$
\min_{\theta_1, \ldots, \theta_K} \sum_{k=1}^K \left( \mathcal{L}_k(\theta_k) + \lambda \cdot \mathcal{R}(\theta_k, \theta_{\text{global}}) \right)
$$

Here, $\mathcal{R}$ is a regularization term that penalizes deviation from the global model, and $\lambda$ controls the strength of this penalty. This formulation allows local models to deviate as needed, while still benefiting from global coordination.

Real-world use cases illustrate the importance of this approach. Consider a wearable health monitor that tracks physiological signals to classify physical activities. While a global model may perform reasonably well across the population, individual users exhibit unique motion patterns, gait signatures, or sensor placements. Personalized finetuning of the final classification layer or low-rank adapters allows improved accuracy, particularly for rare or user-specific classes.

Several personalization strategies have emerged to address the tradeoffs between compute overhead, privacy, and adaptation speed. One widely used approach is local finetuning, in which each client downloads the latest global model and performs a small number of gradient steps using its private data. While this method is simple and preserves privacy, it may yield suboptimal results when the global model is poorly aligned with the client's data distribution or when the local dataset is extremely limited.

Another effective technique involves personalization layers, where the model is partitioned into a shared backbone and a lightweight, client-specific head—typically the final classification layer [@arivazhagan2019federated]. Only the head is updated on-device, significantly reducing memory usage and training time. This approach is particularly well-suited for scenarios in which the primary variation across clients lies in output categories or decision boundaries.

Clustered federated learning offers an alternative by grouping clients according to similarities in their data or performance characteristics, and training separate models for each cluster. This strategy can enhance accuracy within homogeneous subpopulations but introduces additional system complexity and may require exchanging metadata to determine group membership.

Finally, meta-learning approaches, such as Model-Agnostic Meta-Learning (MAML), aim to produce a global model initialization that can be quickly adapted to new tasks with just a few local updates [@finn2017model]. This technique is especially useful when clients have limited data or operate in environments with frequent distributional shifts.
Each of these strategies reflects a different point in the tradeoff space. These strategies vary in their system implications, including compute overhead, privacy guarantees, and adaptation latency. @tbl-personalization-strategies summarizes the tradeoffs.

+---------------------------+----------------------------+--------------------+-------------------------+----------------------------+
| Strategy                  | Personalization Mechanism  | Compute Overhead   | Privacy Preservation    | Adaptation Speed           |
+:==========================+:===========================+:===================+:========================+:===========================+
| Local Finetuning          | Gradient descent on local  | Low to Moderate    | High (no data sharing)  | Fast (few steps)           |
|                           | loss post-aggregation      |                    |                         |                            |
+---------------------------+----------------------------+--------------------+-------------------------+----------------------------+
| Personalization Layers    | Split model: shared base + | Moderate           | High                    | Fast (train small head)    |
|                           | user-specific head         |                    |                         |                            |
+---------------------------+----------------------------+--------------------+-------------------------+----------------------------+
| Clustered FL              | Group clients by data      | Moderate to High   | Medium (group metadata) | Medium                     |
|                           | similarity, train per group|                    |                         |                            |
+---------------------------+----------------------------+--------------------+-------------------------+----------------------------+
| Meta-Learning             | Train for fast adaptation  | High               | High                    | Very Fast (few-shot)       |
|                           | across tasks/devices       | (meta-objective)   |                         |                            |
+---------------------------+----------------------------+--------------------+-------------------------+----------------------------+

: **Personalization Trade-Offs**: Federated learning strategies balance personalization with system costs, impacting compute overhead, privacy preservation, and adaptation speed for diverse client populations. This table summarizes how local finetuning, clustered learning, and meta-learning each navigate this trade-off space, enabling tailored models while considering practical deployment constraints. {#tbl-personalization-strategies}

Selecting the appropriate personalization method depends on deployment constraints, data characteristics, and the desired balance between accuracy, privacy, and computational efficiency. In practice, hybrid approaches that combine elements of multiple strategies, including local finetuning atop a personalized head, are often employed to achieve robust performance across heterogeneous devices.

#### Federated Privacy {#sec-ondevice-learning-federated-privacy-a1ed}

While federated learning is often motivated by privacy concerns, as it involves keeping raw data localized instead of transmitting it to a central server, the paradigm introduces its own set of security and privacy risks. Although devices do not share their raw data, the transmitted model updates (such as gradients or weight changes) can inadvertently leak information about the underlying private data. Techniques such as model inversion attacks and membership inference attacks demonstrate that adversaries may partially reconstruct or infer properties of local datasets by analyzing these updates.

To mitigate such risks, modern federated ML systems commonly employ protective measures. Secure Aggregation protocols ensure that individual model updates are encrypted and aggregated in a way that the server only observes the combined result, not any individual client's contribution. Differential Privacy[^fn-differential-privacy] techniques inject carefully calibrated noise into updates to mathematically bound the information that can be inferred about any single client's data.

[^fn-differential-privacy]: **Differential Privacy**: Provides mathematical privacy guarantees by adding calibrated noise to model updates, ensuring that participation by any individual cannot be distinguished. Developed by Dwork in 2006, it's now the gold standard for privacy-preserving ML. The privacy budget ε (epsilon) controls the privacy-utility tradeoff: smaller ε means stronger privacy but noisier results. Apple uses local differential privacy with ε=4-14 for iOS telemetry, while Google's federated learning deployments use ε≈10.

While these techniques enhance privacy, they introduce additional system complexity and tradeoffs between model utility, communication cost, and robustness. A deeper exploration of these attacks, defenses, and their implications requires dedicated coverage of security principles in distributed ML systems.

### Distributed Systems Coordination {#sec-ondevice-learning-distributed-coordination}

Federated learning transforms machine learning into a massive distributed systems challenge that extends far beyond traditional algorithmic considerations. Coordinating thousands or millions of heterogeneous devices with intermittent connectivity requires sophisticated distributed systems protocols that handle Byzantine failures, network partitions, and communication efficiency at unprecedented scale. These challenges fundamentally differ from the controlled environments of data center distributed training, where high-bandwidth networks and reliable infrastructure enable straightforward coordination protocols.

#### Communication Requirements Analysis

The communication bottleneck represents the primary scalability constraint in federated learning systems. Understanding the quantitative transfer requirements enables principled design decisions about model architectures, update compression strategies, and client participation policies that determine system viability.

**Federated Communication Hierarchy:**
- **Full model synchronization:** 10-500MB per round for typical deep learning models, prohibitive for mobile networks with limited upload bandwidth
- **Gradient compression:** 10-100x reduction achievable through quantization, sparsification, and selective gradient transmission
- **Practical deployments:** 100-1000x compression ratios required, reducing 100MB models to 100KB-1MB updates for mobile viability
- **Communication frequency:** Critical trade-off between model update freshness and network efficiency constraints

Network infrastructure constraints directly impact participation rates and overall system viability. Modern 4G networks provide upload speeds averaging 5-50 Mbps under optimal conditions, meaning an 8MB model update requires 1.3-13 seconds of sustained transmission. However, real-world mobile networks exhibit extreme variability: rural areas may experience 1 Mbps upload speeds while urban 5G deployments enable 100+ Mbps. This 100x variance in network capability necessitates adaptive communication strategies that optimize for lowest-common-denominator connectivity while enabling high-capability devices to contribute more effectively.

The relationship between communication requirements and participation rates exhibits sharp threshold effects. Empirical studies demonstrate that federated learning systems requiring model transfers exceeding 10MB achieve less than 10% sustained client participation, while systems maintaining updates below 1MB can sustain 40-60% participation rates across diverse mobile populations. This communication efficiency directly translates to model quality improvements: higher participation rates provide better statistical diversity and more robust gradient estimates for global model updates.

Advanced compression techniques become essential for practical deployment. Gradient quantization reduces precision from FP32 to INT8 or even binary representations, achieving 4-32x compression with minimal accuracy loss. Sparsification techniques transmit only the largest gradient components, leveraging the natural sparsity in neural network updates. Top-k gradient selection further reduces communication by transmitting only the most significant parameter updates, while error accumulation ensures that small gradients are not permanently lost.

#### Distributed Coordination Challenges

Federated learning operates at the complex intersection of distributed systems and machine learning, inheriting fundamental challenges from both domains while introducing unique complications that arise from the mobile, heterogeneous, and unreliable nature of edge devices.

**Byzantine Fault Tolerance Requirements:**

- **Device failures:** Clients may crash, lose power, or disconnect during training rounds due to battery depletion or network issues
- **Malicious updates:** Adversarial clients can provide corrupted gradients designed to degrade global model performance or extract private information
- **Robust aggregation:** Byzantine-resilient averaging protocols ensure system reliability despite compromised or unreliable participants
- **Consensus mechanisms:** Coordinate millions of unreliable participants without the overhead of traditional distributed consensus protocols

Network partitions pose particularly acute challenges for federated coordination protocols. Unlike traditional distributed systems operating within reliable data center networks, federated learning must gracefully handle prolonged client disconnection events where devices may remain offline for hours or days while traveling, in poor coverage areas, or simply powered down. Asynchronous coordination protocols enable continued training progress despite missing participants, but must carefully balance staleness (accepting potentially outdated contributions) against freshness (prioritizing recent but potentially sparse updates).

**Fault Recovery and Resilience Strategies:**

- **Checkpoint synchronization:** Periodic global model snapshots enable recovery from server failures and provide rollback points for corrupted training rounds
- **Partial update handling:** Systems must gracefully handle incomplete training rounds when significant subsets of clients fail or disconnect
- **State reconciliation:** Clients rejoining after extended offline periods require efficient resynchronization protocols that minimize communication overhead
- **Dynamic load balancing:** Uneven client availability patterns create computational hotspots that require intelligent load redistribution across available participants

The asynchronous nature of federated coordination introduces additional complexity in maintaining training convergence guarantees. Traditional synchronous training assumes all participants complete each round, but federated systems must handle stragglers and dropouts gracefully. Techniques such as FedAsync[^fn-fedasync] enable asynchronous aggregation where the server continuously updates the global model as client updates arrive, while bounded staleness mechanisms prevent extremely outdated updates from corrupting recent progress.

[^fn-fedasync]: **Asynchronous Federated Learning (FedAsync)**: Enables continuous model updates without waiting for slow or unreliable clients. The server maintains a global model that gets updated immediately when client contributions arrive, using staleness-aware weighting to reduce the influence of outdated updates. This approach can improve convergence speed by 2-5x in heterogeneous environments while maintaining model quality within 1-3% of synchronous training.

#### Scale and Heterogeneity Management

Real-world federated learning deployments exhibit extreme heterogeneity across multiple dimensions simultaneously: hardware capabilities, network conditions, data distributions, and availability patterns. This multi-dimensional heterogeneity fundamentally challenges traditional distributed machine learning assumptions about homogeneous participants operating under similar conditions.

**Multi-Dimensional Device Heterogeneity:**
- **Computational variation:** 1000x differences in processing power between flagship smartphones and IoT microcontrollers
- **Memory constraints:** 100-10,000x differences in available RAM across device categories, from 256KB microcontrollers to 16GB smartphones
- **Energy limitations:** Training sessions must be carefully scheduled around charging patterns, thermal constraints, and battery preservation requirements
- **Network diversity:** WiFi, 4G, 5G, and satellite connectivity exhibit orders-of-magnitude performance differences in bandwidth, latency, and reliability

Adaptive coordination protocols address this heterogeneity through sophisticated tiered participation strategies that optimize resource utilization across the device spectrum. High-capability devices such as flagship smartphones can perform complex local training with large batch sizes and multiple epochs, while resource-constrained IoT devices contribute through lightweight updates, specialized subtasks, or even simple data aggregation. This creates a natural computational hierarchy where powerful devices act as "super-peers" performing disproportionate computation, while edge devices contribute specialized local knowledge and coverage.

The scale challenges extend far beyond device heterogeneity to fundamental coordination overhead limitations. Traditional distributed consensus algorithms such as Raft or PBFT are designed for dozens of nodes in controlled environments, but federated learning requires coordination among millions of participants across unreliable networks. This necessitates hierarchical coordination architectures where regional aggregation servers reduce communication overhead by performing local consensus before contributing to global aggregation. Edge computing infrastructure provides natural hierarchical coordination points, enabling federated learning systems to leverage existing content delivery networks (CDNs) and mobile edge computing (MEC) deployments for efficient gradient aggregation.

Modern federated systems implement sophisticated client selection strategies that balance statistical diversity with practical constraints. Random sampling ensures unbiased representation but may select many low-capability devices, while capability-based selection improves training efficiency but risks statistical bias. Hybrid approaches use stratified sampling across device tiers, ensuring both statistical representativeness and computational efficiency. These selection strategies must also consider temporal patterns: office workers' devices may be available during specific hours, while IoT sensors provide continuous but limited computational resources.

## Practical System Design {#sec-ondevice-learning-practical-system-design-7619}

Having established the fundamental constraint-solution patterns for on-device learning, we now turn to **systems integration**: how individual techniques combine into cohesive, deployable systems. This section bridges the gap between algorithmic solutions and engineering practice, showing how the theoretical approaches developed earlier translate into robust production systems.

The transition from individual techniques to complete systems introduces new challenges that cut across the constraint categories we've explored:

**Integration complexity**: Model adaptation, data efficiency, and federated coordination must work together seamlessly, not as independent components.

**Resource orchestration**: Different learning strategies have varying computational and memory profiles that must be coordinated within overall device budgets.

**Temporal coordination**: Training, inference, and communication must be scheduled carefully to avoid interference with user experience and system stability.

**Quality assurance**: Unlike centralized systems with observable training loops, on-device learning requires distributed validation and failure detection mechanisms.

This section examines how successful systems address these integration challenges through **principled system design patterns** that ensure scalable, reliable on-device learning deployment. We focus on practical engineering decisions that determine whether theoretical techniques succeed in real-world deployment.

### Efficient Learning Under Resource Constraints {#sec-ondevice-learning-efficient-learning-theory}

The constraints of on-device learning mirror fundamental challenges solved by biological intelligence systems, offering theoretical insights into efficient learning design. Understanding these connections enables principled approaches to resource-constrained machine learning that leverage billions of years of evolutionary optimization.

#### Biological Intelligence Comparison

The human brain operates at approximately 20 watts while continuously learning from limited supervision—precisely the efficiency target for on-device learning systems. This remarkable efficiency emerges from several architectural principles that directly inform edge learning design.

**Brain Efficiency Characteristics:**
- **Power consumption:** 20W total, with ~10W for active learning and memory consolidation
- **Memory efficiency:** Sparse, distributed representations with only 1-2% of neurons active simultaneously
- **Learning efficiency:** Few-shot learning from single exposures, continuous adaptation without catastrophic forgetting
- **Hierarchical processing:** Multi-scale representations from sensory input to abstract reasoning

Biological learning exhibits several features that on-device systems must replicate. Sparse representations ensure efficient use of limited neural resources—only a tiny fraction of brain neurons fire during any cognitive task. This sparsity directly parallels the selective parameter updates and pruned architectures essential for mobile deployment. Event-driven processing minimizes energy consumption by activating computation only when sensory input changes, analogous to opportunistic training during device idle periods.

#### Self-Supervised Learning Opportunities

Mobile devices continuously collect rich sensor streams ideal for self-supervised learning: visual data from cameras, temporal patterns from accelerometers, spatial patterns from GPS, and interaction patterns from touchscreen usage. This abundant unlabeled data enables sophisticated representation learning without external supervision.

**Sensor Data Potential:**
- **Visual streams:** 30fps video provides 2.6M frames daily for contrastive learning
- **Motion data:** 100Hz accelerometer generates 8.6M samples daily for temporal pattern detection
- **Location traces:** GPS trajectories enable spatial representation learning and behavioral prediction
- **Interaction patterns:** Touch, typing, and app usage create rich behavioral embeddings

Contrastive learning from temporal correlations offers particularly promising opportunities. Consecutive frames from mobile cameras naturally provide positive pairs for visual representation learning, while augmentation techniques create negative examples. Audio streams from microphones enable self-supervised speech representation learning through masking and prediction tasks. Even device orientation and motion data can be used for self-supervised pretraining of activity recognition models.

The biological inspiration extends to continual learning without forgetting. Brains continuously integrate new experiences while retaining decades of memories through mechanisms like synaptic consolidation and replay. On-device systems must implement analogous mechanisms: elastic weight consolidation prevents catastrophic forgetting, experience replay maintains stability during adaptation, and progressive neural architectures expand model capacity as new tasks emerge.

#### Continual Learning Requirements

Real-world on-device deployment demands continual adaptation to changing environments, user behavior, and task requirements. This presents the fundamental challenge of stability-plasticity tradeoff: models must remain stable enough to preserve existing knowledge while plastic enough to learn new patterns.

**Continual Learning Challenges:**
- **Catastrophic forgetting:** New learning overwrites previously acquired knowledge
- **Task interference:** Multiple learning objectives compete for limited model capacity
- **Data distribution shift:** Deployment environments differ significantly from training conditions
- **Resource constraints:** Limited memory prevents storing all historical data for replay

Meta-learning approaches address these challenges by learning learning algorithms themselves. Model-Agnostic Meta-Learning (MAML) trains models to quickly adapt to new tasks with minimal data—exactly the capability required for personalized on-device adaptation. Few-shot learning techniques enable rapid specialization from small user-specific datasets, while maintaining general capabilities learned during pretraining.

The theoretical foundation suggests that optimal on-device learning systems will combine sparse representations, self-supervised pretraining on sensor data, and meta-learning for rapid adaptation. These principles directly influence practical system design: sparse model architectures reduce memory and compute requirements, self-supervised objectives utilize abundant unlabeled sensor data, and meta-learning enables efficient personalization from limited user interactions.

A key principle in building practical systems is to minimize the adaptation footprint. Full-model fine-tuning is typically infeasible on edge platforms, instead, localized update strategies, including bias-only optimization, residual adapters, and lightweight task-specific heads, should be prioritized. These approaches allow model specialization under resource constraints while mitigating the risks of overfitting or instability.

The feasibility of lightweight adaptation depends importantly on the strength of offline pretraining [@bommasani2021opportunities]. Pretrained models should encapsulate generalizable feature representations that allow efficient adaptation from limited local data. Shifting the burden of feature extraction to centralized training reduces the complexity and energy cost of on-device updates, while improving convergence stability in data-sparse environments.

Even when adaptation is lightweight, opportunistic scheduling remains important to preserve system responsiveness and user experience. Local updates should be deferred to periods when the device is idle, connected to external power, and operating on a reliable network. Such policies minimize the impact of background training on latency, battery consumption, and thermal performance.

The sensitivity of local training artifacts necessitates careful data security measures. Replay buffers, support sets, adaptation logs, and model update metadata must be protected against unauthorized access or tampering. Lightweight encryption or hardware-backed secure storage can mitigate these risks without imposing prohibitive resource costs on edge platforms.

However, security measures alone do not guarantee model robustness. As models adapt locally, monitoring adaptation dynamics becomes important. Lightweight validation techniques, including confidence scoring, drift detection heuristics, and shadow model evaluation, can help identify divergence early, enabling systems to trigger rollback mechanisms before severe degradation occurs [@gama2014survey].

Robust rollback procedures depend on retaining trusted model checkpoints. Every deployment should preserve a known-good baseline version of the model that can be restored if adaptation leads to unacceptable behavior. This principle is especially important in safety-important and regulated domains, where failure recovery must be provable and rapid.

In decentralized or federated learning contexts, communication efficiency becomes a first-order design constraint. Compression techniques such as quantized gradient updates, sparsified parameter sets, and selective model transmission must be employed to allow scalable coordination across large, heterogeneous fleets of devices without overwhelming bandwidth or energy budgets [@konevcny2016federated].

Moreover, when personalization is required, systems should aim for localized adaptation wherever possible. Restricting updates to lightweight components, including final classification heads or modular adapters, constrains the risk of catastrophic forgetting, reduces memory overhead, and accelerates adaptation without destabilizing core model representations.

Finally, throughout the system lifecycle, privacy and compliance requirements must be architected into adaptation pipelines. Mechanisms to support user consent, data minimization, retention limits, and the right to erasure must be considered core aspects of model design, not post-hoc adjustments. Meeting regulatory obligations at scale demands that on-device learning workflows align inherently with principles of auditable autonomy.

The flowchart in @fig-odl-design-flow summarizes key decision points in designing practical, scalable, and resilient on-device ML systems.

::: {#fig-odl-design-flow fig-env="figure" fig-pos="htb"}
```{.tikz}
\begin{tikzpicture}[line join=round,font=\usefont{T1}{phv}{m}{n}\small]
\tikzset{%
  Line/.style={line width=0.75pt,black!50,text=black},
  Box/.style={align=flush center,
    inner xsep=2pt,
    node distance=0.45,
    draw=BlueLine,
    line width=0.75pt,
    top color=white, bottom color=BlueL,
    text width=35mm,
    minimum width=35mm, minimum height=9mm
  },
 decision/.style = {Box,diamond, aspect=1.95, inner xsep=7pt,inner ysep=-2ex, bottom color=VioletL2, draw=VioletLine},
  startstop/.style = {Box,rounded rectangle, top color=white, bottom color=RedL, draw=RedLine},
}
\node[startstop](B1){Start: Deploying On-Device Learning};
\node[decision,below=of B1](B2){Need Heavy\\ Adaptation?};
\node[Box,below left=0.7 and 0.9 of B2](B3){Use Bias-Only or Lightweight Updates};
\node[Box,below right=0.7 and 0.9 of B2](B4){Add Residual Adapters or Small Heads};
\node[decision,below=of B4](B5){Sufficient Compute\\ and Energy?};
\node[Box,below=of B5](B6){Allow Partial or Full Fine-Tuning};
\node[decision,below=of B6](B7){Is Data Valuable\\ Across Devices?};
\node[Box,below left=1 and 0.9 of B7](B8){Use Federated Learning + Privacy Measures};
\node[Box,below right=1 and 0.9 of B7](B9){Stay Localized and Monitor Drift};
%
\draw[Line,-latex](B1)--(B2);
\draw[Line,-latex](B2.west)node[above left]{No}-|(B3);
\draw[Line,-latex](B2.east)node[above right]{Yes}-|(B4);
\draw[Line,-latex](B4)--(B5);
\draw[Line,-latex](B5)--node[right]{Yes}(B6);
\draw[Line,-latex](B5.west)node[above left]{No}-|(B3);
\draw[Line,-latex](B6)--(B7);
\draw[Line,-latex](B7.west)node[above left]{Yes}-|(B8);
\draw[Line,-latex](B7.east)node[above right]{No}-|(B9);
\end{tikzpicture}
```
**On-Device Learning Design**: This flowchart guides the systematic development of practical on-device ML systems by outlining key decision points related to data management, model selection, and privacy considerations throughout the system lifecycle. Integrating privacy and compliance requirements—such as user consent and data minimization—into the design process ensures auditable autonomy and scalable deployment of on-device intelligence.
:::

## Challenges {#sec-ondevice-learning-challenges-b30f}

While on-device learning holds significant promise for enabling adaptive, private, and efficient machine learning at the edge, its practical deployment introduces a range of challenges that extend beyond algorithm design. Unlike conventional centralized systems, where training occurs in controlled environments with uniform hardware and curated datasets, edge systems must contend with heterogeneity in devices, fragmentation in data, and the absence of centralized validation infrastructure. These factors give rise to new systems-level tradeoffs and open questions concerning reliability, safety, and maintainability. Moreover, regulatory and operational constraints complicate the deployment of self-updating models in real-world applications. This section explores these limitations, emphasizing the systemic barriers that must be addressed to make on-device learning robust, scalable, and trustworthy.

### Heterogeneity {#sec-ondevice-learning-heterogeneity-500f}

Federated and on-device ML systems must operate across a vast and diverse ecosystem of devices, ranging from smartphones and wearables to IoT sensors and microcontrollers. This heterogeneity spans multiple dimensions: hardware capabilities, software stacks, network connectivity, and power availability. Unlike cloud-based systems, where environments can be standardized and controlled, edge deployments encounter a wide distribution of system configurations and constraints. These variations introduce significant complexity in algorithm design, resource scheduling, and model deployment.

At the hardware level, devices differ in terms of memory capacity, processor architecture (e.g., ARM Cortex-M vs. A-series)[^fn-arm-cortex-spectrum], instruction set support (e.g., availability of SIMD or floating-point units), and the presence or absence of AI accelerators. Some clients may possess powerful NPUs capable of running small training loops, while others may rely solely on low-frequency CPUs with minimal RAM. These differences affect the feasible size of models, the choice of training algorithm, and the frequency of updates.

[^fn-arm-cortex-spectrum]: **ARM Cortex Architecture Spectrum**: The ARM Cortex family spans 6 orders of magnitude in capabilities. Cortex-M0+ (IoT sensors) runs at 48MHz with 32KB RAM and no floating-point, consuming ~10µW. Cortex-M7 (embedded systems) reaches 400MHz with 1MB RAM and single-precision FPU, consuming ~100mW. Cortex-A78 (smartphones) delivers 3GHz performance with multi-core processing, NEON SIMD, and advanced branch prediction, consuming 1-5W. This diversity means federated learning must adapt algorithms dynamically—quantized inference on M0+, lightweight training on M7, and full backpropagation on A78.

Software heterogeneity compounds the challenge. Devices may run different versions of operating systems, kernel-level drivers, and runtime libraries. Some environments support optimized ML runtimes like TensorFlow Lite Micro or ONNX Runtime Mobile, while others rely on custom inference stacks or restricted APIs. These discrepancies can lead to subtle inconsistencies in behavior, especially when models are compiled differently or when floating-point precision varies across platforms.

In addition to computational heterogeneity, devices exhibit variation in connectivity and uptime. Some are intermittently connected, plugged in only occasionally, or operate under strict bandwidth constraints. Others may have continuous power and reliable networking, but still prioritize user-facing responsiveness over background learning. These differences complicate the orchestration of coordinated learning and the scheduling of updates.

Finally, system fragmentation affects reproducibility and testing. With such a wide range of execution environments, it is difficult to ensure consistent model behavior or to debug failures reliably. This makes monitoring, validation, and rollback mechanisms more important—but also more difficult to implement uniformly across the fleet.

Consider a federated learning deployment for mobile keyboards. A high-end smartphone might feature 8 GB of RAM, a dedicated AI accelerator, and continuous Wi-Fi access. In contrast, a budget device may have just 2 GB of RAM, no hardware acceleration, and rely on intermittent mobile data. These disparities influence how long training runs can proceed, how frequently models can be updated, and even whether training is feasible at all. To support such a range, the system must dynamically adjust training schedules, model formats, and compression strategies—ensuring equitable model improvement across users while respecting each device's limitations.

### Data Fragmentation {#sec-ondevice-learning-data-fragmentation-a0d4}

In centralized machine learning, data can be aggregated, shuffled, and curated to approximate independent and identically distributed (IID) samples—a key assumption underlying many learning algorithms. In contrast, on-device and federated learning systems must contend with highly fragmented and non-IID data. Each device collects data specific to its user, context, and usage patterns. These data distributions are often skewed, sparse, and dynamically shifting over time.

From a statistical standpoint, the non-IID nature of on-device data leads to challenges in both optimization and generalization. Gradients computed on one device may conflict with those from another, slowing convergence or destabilizing training. Local updates can cause models to overfit to the idiosyncrasies of individual clients, reducing performance when aggregated globally. Moreover, the diversity of data across clients complicates evaluation and model validation: there is no single test set that reflects the true deployment distribution.

The fragmentation also limits the representativeness of any single client's data. Many clients may observe only a narrow slice of the input space or task distribution, making it difficult to learn robust or generalizable representations. Devices might also encounter new classes or tasks not seen during centralized pretraining, requiring mechanisms for out-of-distribution detection and continual adaptation.

These challenges demand algorithms that are robust to heterogeneity and resilient to imbalanced participation. Techniques such as personalization layers, importance weighting, and adaptive aggregation schemes attempt to mitigate these issues, but there is no universally optimal solution. The degree and nature of non-IID data varies widely across applications, making this one of the most persistent and core challenges in decentralized learning.

A common example of data fragmentation arises in speech recognition systems deployed on personal assistants. Each user exhibits a unique voice profile, accent, and speaking style, which results in significant differences across local datasets. Some users may issue frequent, clearly enunciated commands, while others speak infrequently or in noisy environments. These variations cause device-specific gradients to diverge, especially when training wake-word detectors or adapting language models locally.

In federated learning deployments for virtual keyboards, the problem is further amplified. One user might primarily type in English, another in Hindi, and a third may switch fluidly between multiple languages. The resulting training data is highly non-IID—not only in language but also in vocabulary, phrasing, and typing cadence. A global model trained on aggregated updates may degrade if it fails to capture these localized differences, highlighting the need for adaptive, data-aware strategies that accommodate heterogeneity without sacrificing collective performance.

### Monitoring and Validation {#sec-ondevice-learning-monitoring-validation-c1b8}

Unlike centralized machine learning systems, where model updates can be continuously evaluated against a held-out validation set, on-device learning introduces a core shift in visibility and observability. Once deployed, models operate in highly diverse and often disconnected environments, where internal updates may proceed without external monitoring. This creates significant challenges for ensuring that model adaptation is both beneficial and safe.

A core difficulty lies in the absence of centralized validation data. In traditional workflows, models are trained and evaluated using curated datasets that serve as proxies for deployment conditions. On-device learners, by contrast, adapt in response to local inputs, which are rarely labeled and may not be systematically collected. As a result, the quality and direction of updates, whether they enhance generalization or cause drift, are difficult to assess without interfering with the user experience or violating privacy constraints. The broader challenges of monitoring distributed ML systems and debugging production deployments are addressed in @sec-ml-operations, while framework-specific considerations for edge deployment (TensorFlow Lite[^fn-tflite], ONNX Runtime, PyTorch Mobile) are covered in @sec-ai-frameworks.

[^fn-tflite]: **TensorFlow Lite**: Google's framework for mobile and embedded ML inference, optimized for ARM processors and mobile GPUs. TFLite reduces model size by 75% through quantization and pruning, while achieving 3x faster inference than full TensorFlow. The framework supports 16-bit and 8-bit quantization, with specialized kernels for mobile CPUs and GPUs. TFLite Micro targets microcontrollers with <1MB memory, enabling ML on Arduino and other embedded platforms.

The risk of model drift is especially pronounced in streaming settings, where continual adaptation may cause a slow degradation in performance. For instance, a voice recognition model that adapts too aggressively to background noise may eventually overfit to transient acoustic conditions, reducing accuracy on the target task. Without visibility into the evolution of model parameters or outputs, such degradations can remain undetected until they become severe.

Mitigating this problem requires mechanisms for on-device validation and update gating. One approach is to interleave adaptation steps with lightweight performance checks—using proxy objectives or self-supervised signals to approximate model confidence [@deng2021adaptive]. For example, a keyword spotting system might track detection confidence across recent utterances and suspend updates if confidence consistently drops below a threshold. Alternatively, shadow evaluation can be employed, where multiple model variants are maintained on the device and evaluated in parallel on incoming data streams, allowing the system to compare the adapted model's behavior against a stable baseline.

Another strategy involves periodic checkpointing and rollback, where snapshots of the model state are saved before adaptation. If subsequent performance degrades, as determined by downstream metrics or user feedback, the system can revert to a known good state. This approach has been used in health monitoring devices, where incorrect predictions could lead to user distrust or safety concerns. However, it introduces storage and compute overhead, especially in memory-constrained environments.

In some cases, federated validation offers a partial solution. Devices can share anonymized model updates or summary statistics with a central server, which aggregates them across users to identify global patterns of drift or failure. While this preserves some degree of privacy, it introduces communication overhead and may not capture rare or user-specific failures.

Ultimately, update monitoring and validation in on-device learning require a rethinking of traditional evaluation practices. Instead of centralized test sets, systems must rely on implicit signals, runtime feedback, and conservative adaptation policies to ensure robustness. The absence of global observability is not merely a technical limitation—it reflects a deeper systems challenge in aligning local adaptation with global reliability.

### Resource Management {#sec-ondevice-learning-resource-management-bf40}

On-device learning introduces resource contention modes absent in conventional inference-only deployments. Many edge devices are provisioned to run pretrained models efficiently but are rarely designed with training workloads in mind. Local adaptation therefore competes for scarce resources, including compute cycles, memory bandwidth, energy, and thermal headroom, with other system processes and user-facing applications.

The most direct constraint is compute availability. Training involves additional forward and backward passes through the model, which can exceed the cost of inference. Even when only a small subset of parameters is updated, for instance, in bias-only or head-only adaptation, backpropagation must still traverse the relevant layers, triggering increased instruction counts and memory traffic. On devices with shared compute units (e.g., mobile SoCs or embedded CPUs), this demand can delay interactive tasks, reduce frame rates, or impair sensor processing.

Energy consumption compounds this problem. Adaptation typically involves sustained computation over multiple input samples, which taxes battery-powered systems and may lead to rapid energy depletion. For instance, performing a single epoch of adaptation on a microcontroller-class device can consume several millijoules[^fn-microcontroller-power]—an appreciable fraction of the energy budget for a duty-cycled system operating on harvested power. This necessitates careful scheduling, such that learning occurs only during idle periods, when energy reserves are high and user latency constraints are relaxed.

[^fn-microcontroller-power]: **Microcontroller Power Budget Reality**: A typical microcontroller consuming 100mW during training exhausts 3.6 joules per hour, equivalent to a 1000mAh battery in 2.8 hours. Energy harvesting systems collect only 10-100mW continuously (solar panels in indoor light), making sustained training impossible. Real deployments use duty cycling: train for 10 seconds every hour, consuming ~1 joule total. This constrains training to 100-1000 gradient steps maximum, requiring extremely efficient algorithms and careful energy budgeting between sensing, computation, and communication.

From a memory perspective, training incurs higher peak usage than inference, due to the need to cache intermediate activations[^fn-activation-caching], gradients, and optimizer state [@lin2020mcunet].

[^fn-activation-caching]: **Activation Caching**: During backpropagation, forward pass activations must be stored to compute gradients, dramatically increasing memory usage. For a typical CNN, activation memory can be 3-5x larger than model weights. Modern techniques like gradient checkpointing trade computation for memory by recomputing activations during backward pass, reducing memory by 80% at the cost of ~30% more compute time. Critical for training on memory-constrained devices where activation storage often exceeds available RAM. These requirements may exceed the static memory footprint anticipated during model deployment, particularly when adaptation involves multiple layers or gradient accumulation. In highly constrained systems, for example, systems with less than 512 KB of RAM, this may preclude certain types of adaptation altogether, unless additional optimization techniques (e.g., checkpointing or low-rank updates) are employed.

These resource demands must also be balanced against quality of service (QoS) goals. Users expect edge devices to respond reliably and consistently, regardless of whether learning is occurring in the background. Any observable degradation, including dropped audio in a wake-word detector or lag in a wearable display, can erode user trust. These system reliability concerns parallel the operational challenges discussed in @sec-ml-operations. As such, many systems adopt opportunistic learning policies, where adaptation is suspended during foreground activity and resumed only when system load is low.

In some deployments, adaptation is further gated by cost constraints imposed by networked infrastructure. For instance, devices may offload portions of the learning workload to nearby gateways or cloudlets, introducing bandwidth and communication trade-offs. These hybrid models raise additional questions of task placement and scheduling: should the update occur locally, or be deferred until a high-throughput link is available?

In summary, the cost of on-device learning is not solely measured in FLOPs or memory usage. It manifests as a complex interplay of system load, user experience, energy availability, and infrastructure capacity. Addressing these challenges requires co-design across algorithmic, runtime, and hardware layers, ensuring that adaptation remains unobtrusive, efficient, and sustainable under real-world constraints.

### Common Failure Modes {#sec-ondevice-learning-failure-modes}

Understanding potential failure modes in on-device learning helps prevent costly deployment mistakes. Based on documented challenges in federated learning research [@kairouz2021advances] and known risks in adaptive systems, several categories of failures warrant careful consideration.

The most fundamental risk in on-device learning is unbounded adaptation drift, where continuous learning without constraints causes models to gradually diverge from their intended behavior. Consider a hypothetical keyboard prediction system that learns from all user inputs including corrections—it might begin incorporating typos as valid suggestions, leading to progressively degraded predictions. This risk becomes acute in health monitoring applications where gradual changes in user baselines could be learned as "normal," potentially causing the system to miss important anomalies that would have been detected by a static model. The insidious nature of this drift is that it occurs slowly and locally, making detection difficult without proper monitoring infrastructure.

Beyond individual device drift, federated learning systems face the challenge of participation bias amplification at the population level. Devices with reliable power and connectivity participate more frequently in federated rounds [@li2020federated]. This uneven participation creates scenarios where models become increasingly optimized for users with high-end devices while performance degrades for those with limited resources. The resulting feedback loop exacerbates digital inequality: better-served users receive increasingly better models, while underserved populations experience declining performance, reducing their engagement and further diminishing their representation in training rounds [@wang2021field]. These fairness and bias amplification concerns highlight the ethical implications of distributed learning systems.

These systematic biases interact with data quality issues to create autocorrection feedback loops, particularly in text-based applications. When systems cannot distinguish between intended inputs and corrections, they may develop unexpected behaviors. Frequently corrected domain-specific terminology might be incorrectly learned as errors, leading to inappropriate suggestions in professional contexts. This problem compounds the drift issue: not only do models adapt to individual quirks, but they may also learn from their own mistakes when users accept autocorrections without realizing the system is learning from these interactions.

The interconnected nature of these failure modes, from individual drift to population bias to data quality degradation, underscores the importance of implementing comprehensive safety mechanisms. Successful deployments require bounded adaptation ranges to prevent unbounded drift, stratified sampling to address participation bias, careful data filtering to avoid learning from corrections as ground truth, and shadow evaluation against static baselines to detect degradation. While specific production incidents are rarely publicized due to competitive and privacy concerns, the research community has identified these patterns as critical areas requiring systematic mitigation strategies [@li2020federated; @kairouz2021advances].

### Deployment Risks {#sec-ondevice-learning-deployment-risks-ee18}

The deployment of adaptive models on edge devices introduces challenges that extend beyond technical feasibility. In domains where compliance, auditability, and regulatory approval are necessary, including healthcare, finance, and safety-important systems, on-device learning poses a core tension between system autonomy and control.

In traditional machine learning pipelines, all model updates are centrally managed, versioned, and validated. The training data, model checkpoints, and evaluation metrics are typically recorded in reproducible workflows that support traceability. When learning occurs on the device itself, however, this visibility is lost. Each device may independently evolve its model parameters, influenced by unique local data streams that are never observed by the developer or system maintainer.

This autonomy creates a validation gap. Without access to the input data or the exact update trajectory, it becomes difficult to verify that the learned model still adheres to its original specification or performance guarantees. This is especially problematic in regulated industries, where certification depends on demonstrating that a system behaves consistently across defined operational boundaries. A device that updates itself in response to real-world usage may drift outside those bounds, triggering compliance violations without any external signal.

Moreover, the lack of centralized oversight complicates rollback and failure recovery. If a model update degrades performance, it may not be immediately detectable, particularly in offline scenarios or systems without telemetry. By the time failure is observed, the system's internal state may have diverged significantly from any known checkpoint, making diagnosis and recovery more complex than in static deployments. This necessitates robust safety mechanisms, such as conservative update thresholds, rollback caches, or dual-model architectures that retain a verified baseline.

In addition to compliance challenges, on-device learning introduces new security vulnerabilities. Because model adaptation occurs locally and relies on device-specific, potentially untrusted data streams, adversaries may attempt to manipulate the learning process by tampering with stored data, such as replay buffers, or by injecting poisoned examples during adaptation, to degrade model performance or introduce vulnerabilities. Furthermore, any locally stored adaptation data, such as feature embeddings or few-shot examples, must be secured against unauthorized access to prevent unintended information leakage.

Maintaining model integrity over time is particularly difficult in decentralized settings, where central monitoring and validation are limited. Autonomous updates could, without external visibility, cause models to drift into unsafe or biased states. These risks are compounded by compliance obligations such as the GDPR's right to erasure: if user data subtly influences a model through adaptation, tracking and reversing that influence becomes complex.

The security and integrity of self-adapting models, particularly at the edge, pose important open challenges. A comprehensive treatment of these threats and corresponding mitigation strategies requires specialized security frameworks for distributed ML systems.

Privacy regulations also interact with on-device learning in nontrivial ways. While local adaptation can reduce the need to transmit sensitive data, it may still require storage and processing of personal information, including sensor traces or behavioral logs, on the device itself. These privacy considerations require careful attention to security frameworks and regulatory compliance. Depending on jurisdiction, this may invoke additional requirements for data retention, user consent, and auditability. Systems must be designed to satisfy these requirements without compromising adaptation effectiveness, which often involves encrypting stored data, enforcing retention limits, or implementing user-controlled reset mechanisms.

Lastly, the emergence of edge learning raises open questions about accountability and liability [@brakerski2022federated]. When a model adapts autonomously, who is responsible for its behavior? If an adapted model makes a faulty decision, such as misdiagnosing a health condition or misinterpreting a voice command, the root cause may lie in local data drift, poor initialization, or insufficient safeguards. Without standardized mechanisms for capturing and analyzing these failure modes, responsibility may be difficult to assign, and regulatory approval harder to obtain.

Addressing these deployment and compliance risks requires new tooling, protocols, and design practices that support auditable autonomy—the ability of a system to adapt in place while still satisfying external requirements for traceability, reproducibility, and user protection. As on-device learning becomes more prevalent, these challenges will become central to both system architecture and governance frameworks.

### Challenges Summary {#sec-ondevice-learning-challenges-summary-269a}

Designing on-device ML systems involves navigating a complex landscape of technical and practical constraints. While localized adaptation allows personalization, privacy, and responsiveness, it also introduces a range of challenges that span hardware heterogeneity, data fragmentation, observability, and regulatory compliance.

System heterogeneity complicates deployment and optimization by introducing variation in compute, memory, and runtime environments. Non-IID data distributions challenge learning stability and generalization, especially when models are trained on-device without access to global context. The absence of centralized monitoring makes it difficult to validate updates or detect performance regressions, and training activity must often compete with core device functionality for energy and compute. Finally, post-deployment learning introduces complications in model governance, from auditability and rollback to privacy assurance.

These challenges are not isolated—they interact in ways that influence the viability of different adaptation strategies. @tbl-ondevice-challenges summarizes the primary challenges and their implications for ML systems deployed at the edge.

+-------------------------------------+-----------------------------------------------------------+------------------------------------------------------+
| Challenge                           | Root Cause                                                | System-Level Implications                            |
+:====================================+:==========================================================+:=====================================================+
| System Heterogeneity                | Diverse hardware, software, and toolchains                | Limits portability; requires platform-specific tuning|
+-------------------------------------+-----------------------------------------------------------+------------------------------------------------------+
| Non-IID and Fragmented Data         | Localized, user-specific data distributions               | Hinders generalization; increases risk of drift      |
+-------------------------------------+-----------------------------------------------------------+------------------------------------------------------+
| Limited Observability and Feedback  | No centralized testing or logging                         | Makes update validation and debugging difficult      |
+-------------------------------------+-----------------------------------------------------------+------------------------------------------------------+
| Resource Contention and Scheduling  | Competing demands for memory, compute, and battery        | Requires dynamic scheduling and budget-aware learning|
+-------------------------------------+-----------------------------------------------------------+------------------------------------------------------+
| Deployment and Compliance Risk      | Learning continues post-deployment                        | Complicates model versioning, auditing, and rollback |
+-------------------------------------+-----------------------------------------------------------+------------------------------------------------------+

: **On-Device Learning Challenges**: System heterogeneity, non-IID data, and limited resources introduce unique challenges for deploying and adapting machine learning models on edge devices, impacting portability, stability, and governance. The table details root causes of these challenges and their system-level implications, highlighting trade-offs between model performance and resource constraints. {#tbl-ondevice-challenges}

## Fallacies and Pitfalls

On-device learning operates in a fundamentally different environment from cloud-based training, with severe resource constraints and privacy requirements that challenge traditional machine learning assumptions. The appeal of local adaptation and privacy preservation can obscure the significant technical limitations and implementation challenges that determine whether on-device learning provides net benefits over simpler alternatives.

**Fallacy:** _On-device learning provides the same adaptation capabilities as cloud-based training._

This misconception leads teams to expect that local learning can achieve the same model improvements as centralized training with abundant computational resources. On-device learning operates under severe constraints including limited memory, restricted computational power, and minimal energy budgets that fundamentally limit adaptation capabilities. Local datasets are typically small, biased, and non-representative, making it impossible to achieve the same generalization performance as centralized training. Effective on-device learning requires accepting these limitations and designing adaptation strategies that provide meaningful improvements within practical constraints rather than attempting to replicate cloud-scale learning capabilities. This necessitates the efficiency-first mindset and optimization techniques detailed in @sec-efficient-ai.

**Pitfall:** _Assuming that federated learning automatically preserves privacy without additional safeguards._

Many practitioners believe that keeping data on local devices inherently provides privacy protection without considering the information that can be inferred from model updates. Gradient and parameter updates can leak significant information about local training data through various inference attacks. Device participation patterns, update frequencies, and model convergence behaviors can reveal sensitive information about users and their activities. True privacy preservation requires additional mechanisms like differential privacy, secure aggregation, and careful communication protocols rather than relying solely on data locality.

**Fallacy:** _Resource-constrained adaptation always produces better personalized models than generic models._

This belief assumes that any local adaptation is beneficial regardless of the quality or quantity of local data available. On-device learning with insufficient, noisy, or biased local data can actually degrade model performance compared to well-trained generic models. Small datasets may not provide enough signal for meaningful learning, while adaptation to local noise can harm generalization. Effective on-device learning systems must include mechanisms to detect when local adaptation is beneficial and fall back to generic models when local data is inadequate for reliable learning.

**Pitfall:** _Ignoring the heterogeneity challenges across different device types and capabilities._

Teams often design on-device learning systems assuming uniform hardware capabilities across deployment devices. Real-world deployments span diverse hardware with varying computational power, memory capacity, energy constraints, and networking capabilities. A learning algorithm that works well on high-end smartphones may fail catastrophically on resource-constrained IoT devices. This heterogeneity affects not only individual device performance but also federated learning coordination where slow or unreliable devices can bottleneck the entire system. Successful on-device learning requires adaptive algorithms that adjust to device capabilities and robust coordination mechanisms that handle device heterogeneity gracefully. The development and deployment of such systems benefits from robust engineering practices that handle uncertainty and failure gracefully.

**Pitfall:** _Underestimating the complexity of orchestrating learning across distributed edge systems._

Many teams focus on individual device optimization without considering the system-level challenges of coordinating learning across thousands or millions of edge devices. Edge systems orchestration must handle intermittent connectivity, varying power states, different time zones, and unpredictable device availability patterns that create complex scheduling and synchronization challenges. Device clustering, federated rounds coordination, model versioning across diverse deployment contexts, and handling partial participation from unreliable devices require sophisticated infrastructure beyond simple aggregation servers. Additionally, real-world edge deployments involve multiple stakeholders with different incentives, security requirements, and operational procedures that must be balanced against learning objectives. Effective edge learning systems require robust orchestration frameworks that can maintain system coherence despite constant device churn, network partitions, and operational disruptions.

## Summary {#sec-ondevice-learning-summary-0af9}

On-device learning represents a fundamental shift from static, centralized training to dynamic, local adaptation directly on deployment devices. This paradigm enables machine learning systems to personalize experiences while preserving privacy, reduce network dependencies, and respond rapidly to changing local conditions. Success requires integrating the optimization principles from @sec-model-optimizations, hardware constraints from @sec-ai-acceleration, and operational practices from @sec-ml-operations. The transition from traditional cloud-based training to edge-based learning requires overcoming severe computational, memory, and energy constraints that fundamentally reshape how models are designed and adapted, applying the efficiency principles and sustainable practices explored throughout this textbook.

The technical strategies that enable practical on-device learning span multiple dimensions of system design. Adaptation techniques range from lightweight bias-only updates to selective parameter tuning, each offering different tradeoffs between expressivity and resource efficiency. Data efficiency becomes paramount when learning from limited local examples, driving innovations in few-shot learning[^fn-few-shot-learning], streaming adaptation, and memory-based replay mechanisms.

[^fn-few-shot-learning]: **Few-Shot Learning**: Machine learning paradigm that learns new concepts from only a few (typically 1-10) labeled examples. Originally inspired by human learning capabilities—humans can recognize new objects from just one or two examples. In ML, few-shot learning leverages pre-trained representations and meta-learning to quickly adapt to new tasks. Critical for on-device scenarios where collecting large labeled datasets is impractical. Techniques include prototypical networks, model-agnostic meta-learning (MAML), and metric learning approaches that achieve 80-90% accuracy with just 5 examples per class. Federated learning emerges as a crucial coordination mechanism, allowing devices to collaborate while maintaining data locality and privacy guarantees.

::: {.callout-important title="Key Takeaways"}
* On-device learning shifts machine learning from static deployment to dynamic local adaptation, enabling personalization while preserving privacy
* Resource constraints drive specialized techniques: bias-only updates, adapter modules, sparse parameter updates, and compressed data representations
* Federated learning coordinates distributed training across heterogeneous devices while maintaining privacy and handling non-IID data distributions
* Success requires co-designing algorithms with hardware constraints, balancing adaptation capability against memory, energy, and computational limitations
:::

Real-world applications demonstrate both the potential and challenges of on-device learning, from keyword spotting systems that adapt to user voices to recommendation engines that personalize without transmitting user data. As machine learning expands into mobile, embedded, and wearable environments, the ability to learn locally while maintaining efficiency and reliability becomes essential for next-generation intelligent systems that operate seamlessly across diverse deployment contexts.
