{"title":"Image Classification","markdown":{"headingText":"Image Classification","headingAttr":{"id":"","classes":["unnumbered"],"keyvalue":[]},"containsRefs":false,"markdown":"\n![*DALL·E prompt - 1950s style cartoon illustration based on a real image by Marcelo Rovai*](./images/png/ini-dalle.png)\n\n## Overview {#sec-image-classification-overview-9a37}\n\nWe are increasingly facing an artificial intelligence (AI) revolution, where, as [Gartner](https://www.researchgate.net/figure/Gartner-2023-Artificial-intelligence-emerging-technologies-impact-radar-T-Nguyen-2023_fig1_372048156) states, **Edge AI and Computer Vision** have a very high impact potential, and **it is for now**!\n\n\\clearpage\n\nWhen we look into Machine Learning (ML) applied to vision, the first concept that greets us is **Image Classification**, a kind of ML's *Hello World* that is both simple and profound!\n\nThe Seeed Studio XIAOML Kit provides a comprehensive hardware solution centered around the[ XIAO ESP32-S3 Sense](https://www.seeedstudio.com/xiao-series-page), featuring an integrated **OV3660** camera and SD card support. Those features make the XIAO ESP32S3 Sense an excellent starting point for exploring TinyML vision AI.\n\nIn this Lab, we will explore Image Classification using the non-code tool **SenseCraft AI** and explore a more detailed development with **Edge Impulse Studio** and **Arduino IDE**.\n\n::: callout-tip\n\n## Learning Objectives {#sec-image-classification-learning-objectives-ec6a}\n\n- **Deploy Pre-trained Models** using SenseCraft AI Studio for immediate computer vision applications\n\n- **Collect and Manage Image Datasets** for custom classification tasks with proper data organization\n\n- **Train Custom Image Classification Models** using transfer learning with MobileNet V2 architecture\n\n- **Optimize Models for Edge Deployment** through quantization and memory-efficient preprocessing\n\n- **Implement Post-processing Pipelines,** including GPIO control and real-time inference integration\n\n- **Compare Development Approaches** between no-code and advanced ML platforms for embedded applications\n:::\n\n## Image Classification {#sec-image-classification-image-classification-d726}\n\nImage classification is a fundamental task in computer vision that involves categorizing entire images into one of several predefined classes. This process entails analyzing the visual content of an image and assigning it a label from a fixed set of categories based on the dominant object or scene it depicts.\n\nImage classification is crucial in various applications, ranging from organizing and searching through large databases of images in digital libraries and social media platforms to enabling autonomous systems to comprehend their surroundings. Common architectures that have significantly advanced the field of image classification include Convolutional Neural Networks (CNNs), such as AlexNet, VGGNet, and ResNet. These models have demonstrated remarkable accuracy on challenging datasets, such as [ImageNet](https://www.image-net.org/index.php), by learning hierarchical representations of visual data.\n\nAs the cornerstone of many computer vision systems, image classification drives innovation, laying the groundwork for more complex tasks like object detection and image segmentation, and facilitating a deeper understanding of visual data across various industries. So, let's start exploring the [Person Classification](https://sensecraft.seeed.cc/ai/view-model/60768-person-classification?tab=public) model (\"Person - No Person\"), a ready-to-use computer vision application on the **[SenseCraft AI](https://sensecraft.seeed.cc/ai/device/local/32)**.\n\n\\noindent\n![](./images/png/person-class.png){width=85% fig-align=\"center\"}\n\n### Image Classification on the SenseCraft AI Workspace {#sec-image-classification-image-classification-sensecraft-ai-workspace-68a8}\n\nStart by connecting the XIAOML Kit (or just the XIAO ESP32S3 Sense, disconnected from the Expansion Board) to the computer via USB-C, and then open the [SenseCraft AI Workspace](https://sensecraft.seeed.cc/ai/device/local/32) to connect it.\n\n\\noindent\n![](./images/png/connection.png){width=85% fig-align=\"center\"}\n\nOnce connected, select the option `[Select Model...]` and enter in the search window: \"*Person Classification*\". From the options available, select the one trained over the MobileNet V2 (passing the mouse over the models will open a pop-up window with its main characteristics).\n\n\\noindent\n![](./images/png/model_selection.png){width=85% fig-align=\"center\"}\n\nClick on the chosen model and confirm the deployment. A new firmware for the model should start uploading to our device.\n\n> Note that the percentage of models downloaded and firmware uploaded will be displayed. If not, try disconnecting the device, then reconnect it and press the boot button.\n\nAfter the model is uploaded successfully, we can view the live feed from the XIAO camera and the classification result (`Person` or `Not a Person`) in the **Preview** area, along with the inference details displayed in the **Device Logger**.\n\n> Note that we can also select our **Inference Frame Interval**, from \"Real-Time\" (Default) to 10 seconds, and the **Mode** (UART, I2C, etc) as the data is shared by the device (the default is UART via USB).\n\n\\noindent\n![](./images/png/person-detect-inference.png){width=85% fig-align=\"center\"}\n\nAt the Device Logger, we can see that the latency of the model is from 52 to 78 ms for pre-processing and around 532ms for inference, which will give us a total time of a little less than 600ms, or about **1.7 Frames per second (FPS)**.\n\n> To run the Mobilenet V2 0.35, the XIAO had a peak current of 160mA at 5.23V, resulting in a **power consumption of 830mW**.\n\n### Post-Processing {#sec-image-classification-postprocessing-c011}\n\nAn essential step in an Image Classification project pipeline is to define what we want to do with the inference result. So, imagine that we will use the XIAO to automatically turn on the room lights if a person is detected.\n\n\\noindent\n![](./images/png/pipeline.png){width=75% fig-align=\"center\"}\n\nWith the SebseCraft AI, we can do it on the `Output -> GPIO` section. Click on the Add icon to trigger the action when event conditions are met. A pop-up window will open, where you can define the action to be taken. For example, if a person is detected with a confidence of more than 60% the internal `LED` should be ON. In a real scenario, a GPIO, for example, `D0`, `D1`, `D2`, `D11`, or `D12`, would be used to trigger a relay to turn on a light.\n\n\\noindent\n![](./images/png/action.png){width=65% fig-align=\"center\"}\n\nOnce confirmed, the created **Trigger Action** will be shown. Press `Send` to upload the command to the XIAO.\n\n\\noindent\n![](./images/png/trigger.png){width=85% fig-align=\"center\"}\n\nNow, pointing the XIAO at a person will make the internal LED go ON.\n\n\\noindent\n![](./images/png/person-trigger.png){width=65% fig-align=\"center\"}\n\n> We will explore more trigger actions and post-processing techniques further in this lab.\n\n## An Image Classification Project {#sec-image-classification-image-classification-project-a2cf}\n\nLet's create a simple Image Classification project using SenseCraft AI Studio. Below, we can see a typical machine learning pipeline that will be used in our project.\n\n\\noindent\n![](./images/png/pipeline-sensecraft.png){width=85% fig-align=\"center\"}\n\nOn SenseCraft AI Studio: Let's open the tab [Training](https://sensecraft.seeed.cc/ai/training):\n\n\\noindent\n![](./images/png/img-class-project.png){width=85% fig-align=\"center\"}\n\nThe default is to train a `Classification` model with a WebCam if it is available. Let's select the `XIAOESP32S3 Sense` instead. Pressing the green button `[Connect]` will cause a Pop-Up window to appear. Select the corresponding Port and press the blue button `[Connect]`.\n\n\\noindent\n![](./images/png/train-img-class.png){width=85% fig-align=\"center\"}\n\nThe image streamed from the Grove Vision AI V2 will be displayed.\n\n### The Goal {#sec-image-classification-goal-8443}\n\nThe first step, as we can see in the ML pipeline, is to define a goal. Let's imagine that we have an industrial installation that should automatically sort wheels and boxes.\n\n\\noindent\n![](./images/png/distr-line.png){width=75% fig-align=\"center\"}\n\nSo, let's simulate it, classifying, for example, a toy `box` and a toy `wheel`. We should also include a 3rd class of images, `background`, where there are no objects in the scene.\n\n\\noindent\n![](./images/png/classes_img_class.png){width=75% fig-align=\"center\"}\n\n### Data Collection {#sec-image-classification-data-collection-2d1a}\n\nLet's create the classes, following, for example, an alphabetical order:\n\n- Class1: background\n- Class 2: box\n- Class 3: wheel\n\n\\noindent\n![](./images/png/classes.png){width=75% fig-align=\"center\"}\n\nSelect one of the classes and keep pressing the green button (`Hold to Record`) under the preview area. The collected images (and their counting) will appear on the Image Samples Screen. Carefully and slowly, move the camera to capture different angles of the object. To modify the position or interfere with the image, release the green button, rearrange the object, and then hold it again to resume the capture.\n\n\\noindent\n![](./images/png/collect-imaages.png){width=85% fig-align=\"center\"}\n\nAfter collecting the images, review them and delete any incorrect ones.\n\n\\noindent\n![](./images/png/clean_dataset.png){width=75% fig-align=\"center\"}\n\nCollect around **50 images** from each class and go to Training Step.\n\n> Note that it is possible to download the collected images to be used in another application, for example, with the Edge Impulse Studio.\n\n### Training {#sec-image-classification-training-afb2}\n\nConfirm if the correct device is selected (`XIAO ESP32S3 Sense`) and press `[Start Training]`\n\n\\noindent\n![](./images/png/train-setup.png){width=85% fig-align=\"center\"}\n\n### Test {#sec-image-classification-test-6c2f}\n\nAfter training, the inference result can be previewed.\n\n>  Note that the model is not running on the device. We are, in fact, only capturing the images with the device and performing a **live preview** using the training model, which is running in the Studio.\n\n\\noindent\n![](./images/png/img-class-infer.png){width=85% fig-align=\"center\"}\n\nNow is the time to really deploy the model in the device.\n\n### Deployment {#sec-image-classification-deployment-b07e}\n\nSelect the trained model and  `XIAO ESP32S3 Sense` at the `Supported Devices` window. And press `[Deploy to device]`.\n\n\\noindent\n![](./images/png/select-to-deploy.png){width=85% fig-align=\"center\"}\n\nThe SeneCrafit AI will redirect us to the **Vision Workplace** tab. `Confirm` the deployment, select the Port, and `Connect` it.\n\n\\noindent\n![](./images/png/upload-model.png){width=85% fig-align=\"center\"}\n\nThe model will be flashed into the device. After an automatic reset, the model will start running on the device. On the Device Logger, we can see that the inference has a **latency of approximately 426 ms**, plus a **pre-processing of around 110ms**, corresponding to a **frame rate of 1.8 frames per second (FPS)**.\n\nAlso, note that in **Settings**,  it is possible to adjust the model's confidence.\n\n\\noindent\n![](./images/png/infer.png){width=85% fig-align=\"center\"}\n\n> To run the Image Classification Model, the XIAO ESP32S3 had a peak current of 14mA at 5.23V, resulting in a **power consumption of 730mW**.\n\nAs before, in the **Output --> GPIO**, we can turn the GPIOs or the Internal LED ON based on the detected class. For example, the LED will be turned ON when the wheel is detected.\n\n\\noindent\n![](./images/png/led-wheel.png){width=65% fig-align=\"center\"}\n\n### Saving the Model {#sec-image-classification-saving-model-01c7}\n\nIt is possible to save the model in the SenseCraft AI Studio. The Studio will retain all our models for later deployment. For that, return to the `Training` tab and select the button `[Save to SenseCraft`]:\n\n\\noindent\n![](./images/png/save-model.png){width=75% fig-align=\"center\"}\n\nFollow the instructions to enter the model's name, description, image, and other details.\n\n\\noindent\n![](./images/png/model-saved.png){width=85% fig-align=\"center\"}\n\nNote that the trained model (an Int8 MobileNet V2 with a size of 320KB) can be downloaded for further use or even analysis, for example, using [Netron](https://github.com/lutzroeder/netron). Note that the model uses images of size 224x224x3 as its Input Tensor. In the next step, we will use different hyperparameters on the Edge Impulse Studio.\n\n\\noindent\n![](./images/png/netron.png){width=85% fig-align=\"center\"}\n\nAlso, the model can be deployed again to the device at any time. Automatically, the **Workspace** will be open on the SenseCraft AI.\n\n## Image Classification Project from a Dataset {#sec-image-classification-image-classification-project-dataset-8079}\n\nThe primary objective of our project is to train a model and perform inference on the XIAO ESP32S3 Sense. For training, we should find some data **(in fact, tons of data!)**.\n\n*But as we already know, first of all, we need a goal! What do we want to classify?*\n\nWith TinyML, a set of techniques associated with machine learning inference on embedded devices, we should limit the classification to three or four categories due to limitations (mainly memory). We can, for example, train the images captured for the Box versus Wheel, which can be downloaded from the SenseCraft AI Studio.\n\n> Alternatively, we can use a completely new dataset, such as one that differentiates apples from bananas and potatoes, or other categories. If possible, try finding a specific dataset that includes images from those categories. [Kaggle fruit-and-vegetable-image-recognition](https://www.kaggle.com/kritikseth/fruit-and-vegetable-image-recognition) is a good start.\n\nLet's download the dataset captured in the previous section. Open the menu (3 dots) on each of the captured classes and select `Export Data`.\n\n\\noindent\n![](./images/png/export-dataset.png){width=75% fig-align=\"center\"}\n\nThe dataset will be downloaded to the computer as a .ZIP file, with one file for each class. Save them in your working folder and unzip them. You should have three folders, one for each class.\n\n\\noindent\n![](./images/png/dataset-pc.png){width=85% fig-align=\"center\"}\n\n> Optionally, you can add some fresh images, using, for example, the code discussed in the setup lab.\n\n## Training the model with Edge Impulse Studio {#sec-image-classification-training-model-edge-impulse-studio-f530}\n\nWe will use the Edge Impulse Studio to train our model. [Edge Impulse](https://www.edgeimpulse.com/) is a leading development platform for machine learning on edge devices.\n\nEnter your account credentials (or create a free account) at Edge Impulse. Next, create a new project:\n\n### Data Acquisition {#sec-image-classification-data-acquisition-e25e}\n\nNext, go to the **Data acquisition** section and there, select `+ Add data`. A pop-up window will appear.  Select `UPLOAD DATA`.\n\n\\noindent\n![](./images/png/get-dataset.png){width=85% fig-align=\"center\"}\n\nAfter selection, a new Pop-Up window will appear, asking to update the data.\n\n- In Upload mode: `select a folder` and press `[Choose Files]`.\n- Go to the folder that contains one of the classes and press `[Upload]`\n\n\\noindent\n![](./images/png/select-folder.png){width=85% fig-align=\"center\"}\n\n- You will return automatically to the Upload data window.\n- Select `Automatically split between training and testing`\n- And enter the label of the images that are in the folder.\n- Select `[Upload data]`\n- At this point, the files will start to be uploaded, and after that, another Pop-Up window will appear asking if you are building an object detection project. Select `[no]`\n\n\\noindent\n![](./images/png/up-data-2.png){width=85% fig-align=\"center\"}\n\nRepeat the procedure for all classes. **Do not forget to change the label's name**. If you forget and the images are uploaded, please note that they will be mixed in the Studio. Do not worry, you can manually move the data between classes further.\n\nClose the Upload Data window and return to the **Data acquisition** page. We can see that all dataset was uploaded. Note that on the upper panel, we can see that we have 158 items, all of which are balanced. Also, 19% of the images were left for testing.\n\n\\noindent\n![](./images/png/dataset.png){width=85% fig-align=\"center\"}\n\n### Impulse Design {#sec-image-classification-impulse-design-4bfa}\n\n> An impulse takes raw data (in this case, images), extracts features (resizes pictures), and then uses a learning block to classify new data.\n\nClassifying images is the most common application of deep learning, but a substantial amount of data is required to accomplish this task. We have around 50 images for each category. Is this number enough? Not at all! We will need thousands of images to \"teach\" or \"model\" each class, allowing us to differentiate them. However, we can resolve this issue by retraining a previously trained model using thousands of images. We refer to this technique as **\"Transfer Learning\" (TL)**. With TL, we can fine-tune a pre-trained image classification model on our data, achieving good performance even with relatively small image datasets, as in our case.\n\n\\noindent\n![](./images/png/tl.png){width=85% fig-align=\"center\"}\n\nWith TL, we can fine-tune a pre-trained image classification model on our data, performing well even with relatively small image datasets (our case).\n\nSo, starting from the raw images, we will resize them $(96\\times 96)$ Pixels are fed to our Transfer Learning block. Let's create an Impulse.\n\n> At this point, we can also define our target device to monitor our \"budget\" (memory and latency). The XIAO ESP32S3 is not officially supported by Edge Impulse, so let's consider the Espressif ESP-EYE, which is similar but slower.\n\n\\noindent\n![](./images/png/impulse.png){width=85% fig-align=\"center\"}\n\nSave the Impulse, as shown above, and go to the **Image** section.\n\n### Pre-processing (Feature Generation) {#sec-image-classification-preprocessing-feature-generation-ee2e}\n\nBesides resizing the images, we can convert them to grayscale or retain their original RGB color depth. Let's select `[RGB]` in the `Image` section. Doing that, each data sample will have a dimension of 27,648 features (96x96x3). Pressing `[Save Parameters]` will open a new tab, `Generate Features`. Press the button `[Generate Features]`to generate the features.\n\n### Model Design, Training, and Test {#sec-image-classification-model-design-training-test-5f50}\n\nIn 2007, Google introduced [MobileNetV1](https://research.googleblog.com/2017/06/mobilenets-open-source-models-for.html). In 2018, [MobileNetV2: Inverted Residuals and Linear Bottlenecks](https://arxiv.org/abs/1801.04381), was launched, and, in 2019, the V3. The Mobilinet is a family of general-purpose computer vision neural networks explicitly designed for mobile devices to support classification, detection, and other applications. MobileNets are small, low-latency, low-power models parameterized to meet the resource constraints of various use cases.\n\nAlthough the base MobileNet architecture is already compact and has low latency, a specific use case or application may often require the model to be even smaller and faster. MobileNets introduce a straightforward parameter, **α** (alpha), called the width multiplier to construct these smaller, less computationally expensive models. The role of the width multiplier α is to thin a network uniformly at each layer.\n\nEdge Impulse Studio has available MobileNet V1 (96x96 images) and V2 (96x96 and 160x160 images), with several different **α** values (from 0.05 to 1.0). For example, you will get the highest accuracy with V2, 160x160 images, and α=1.0. Of course, there is a trade-off. The higher the accuracy, the more memory (around 1.3M RAM and 2.6M ROM) will be needed to run the model, implying more latency. The smaller footprint will be obtained at another extreme with MobileNet V1 and α=0.10 (around 53.2K RAM and 101K ROM).\n\n> We will use the **MobileNet V2 0.35** as our base model (but a model with a greater alpha can be used here). The final layer of our model, preceding the output layer, will have 16 neurons with a 10% dropout rate for preventing overfitting.\n\nAnother necessary technique to use with deep learning is **data augmentation**. Data augmentation is a method that can help improve the accuracy of machine learning models by creating additional artificial data. A data augmentation system makes small, random changes to your training data during the training process (such as flipping, cropping, or rotating the images).\n\nUnder the hood, here you can see how Edge Impulse implements a data Augmentation policy on your data:\n\n```cpp\n# Implements the data augmentation policy\ndef augment_image(image, label):\n    # Flips the image randomly\n    image = tf.image.random_flip_left_right(image)\n\n    # Increase the image size, then randomly crop it down to\n    # the original dimensions\n    resize_factor = random.uniform(1, 1.2)\n    new_height = math.floor(resize_factor * INPUT_SHAPE[0])\n    new_width = math.floor(resize_factor * INPUT_SHAPE[1])\n    image = tf.image.resize_with_crop_or_pad(image, new_height,\n                                             new_width)\n    image = tf.image.random_crop(image, size=INPUT_SHAPE)\n\n    # Vary the brightness of the image\n    image = tf.image.random_brightness(image, max_delta=0.2)\n\n    return image, label\n```\n\nNow, let's us define the hyperparameters:\n\n- Epochs: 20,\n- Bach Size: 32\n- Learning Rate: 0.0005\n- Validation size: 20%\n\nAnd, so, we have as a training result:\n\n\\noindent\n![](./images/png/train-result.png){width=85% fig-align=\"center\"}\n\nThe model profile predicts **233 KB of RAM and 546 KB of Flash**, indicating no problem with the Xiao ESP32S3, which has 8 MB of PSRAM. Additionally, the Studio indicates a **latency of around 1160 ms**, which is very high. However, this is to be expected, given that we are using the ESP-EYE, whose CPU is an Extensa LX6, and the ESP32S3 uses a newer and more powerful Xtensa LX7.\n\n> With the test data, we also achieved 100% accuracy, even with a quantized INT8 model. This result is not typical in real projects, but our project here is relatively simple, with two objects that are very distinctive from each other.\n\n## Model Deployment {#sec-image-classification-model-deployment-068b}\n\nWe can deploy the trained model:\n\n-  As `.TFLITE` to be used on the **SenseCraft AI  **\n- As an `Arduino Library` in the **Edge Impulse Studio**.\n\nLet's start with the SenseCraft, which is more straightforward and more intuitive.\n\n### Model Deployment on the SenseCraft AI {#sec-image-classification-model-deployment-sensecraft-ai-fe21}\n\nOn the **Dashboard**, it is possible to download the trained model in several different formats. Let's download `TensorFlow Lite (int8 quantized)`, which has a size of 623KB.\n\n\\noindent\n![](./images/png/deploy-1.png){width=85% fig-align=\"center\"}\n\nOn **SenseCraft AI Studio**, go to the `Workspace` tab, select `XIAO ESP32S3`, the corresponding Port, and connect the device.\n\nYou should see the last model that was uploaded to the device. Select the green button `[Upload Model]`. A pop-up window will prompt you to enter the model name, the model file, and the class names (**objects**).  We should use labels in alphabetical order: `0: background`, `1: box`, and `2: wheel`, and then press `[Send]`.\n\n\\noindent\n![](./images/png/model-tflite-infer.png){width=85% fig-align=\"center\"}\n\nAfter a few seconds, the model will be uploaded (\"flashed\") to our device, and the camera image will appear in real-time on the **Preview** Sector. The Classification result will be displayed under the image preview. It is also possible to select the `Confidence Threshold` of your inference using the cursor on **Settings**.\n\nOn the **Device Logger**, we can view the Serial Monitor, where we can observe the latency, which is approximately 81 ms for pre-processing and 205 ms for inference, **corresponding to a frame rate of 3.4 frames per second (FPS)**, what is double of we got, training the model on SenseCraft, because we are working with smaller images (96x96 versus 224x224).\n\n> The total latency is around **4 times faster** than the estimation made in Edge Impulse Studio on an Xtensa LX6 CPU; now we are performing the inference on an Xtensa LX7 CPU.\n\n\\noindent\n![](./images/png/tfmodel-infer.png){width=85% fig-align=\"center\"}\n\n#### Post-Processing {#sec-image-classification-postprocessing-3981}\n\nIt is possible to obtain the output of a model inference, including Latency, Class ID, and Confidence, as shown on the Device Logger in SenseCraft AI. This allows us to utilize the **XIAO ESP32S3 Sense as an AI sensor**. In other words, we can retrieve the model data using different communication protocols such as MQTT, UART, I2C, or SPI, depending on our project requirements.\n\n> The idea is similar to what we have done on the [Seeed Grove Vision AI V2 Image Classification Post-Processing Lab](https://www.mlsysbook.ai/contents/labs/seeed/grove_vision_ai_v2/image_classification/image_classification#sec-image-classification-postprocessing-9610).\n\nBelow is an example of a connection using the I2C bus.\n\n<!--\n![As a Sensor | Seeed Studio Wiki](images/png/iic.png){width=75% fig-align=\"center\"}\n--->\nPlease refer to the [Seeed Studio Wiki](https://wiki.seeedstudio.com/sensecraft-ai/tutorials/sensecraft-ai-output-libraries-xiao/) for more information.\n\n### Model Deployment as an Arduino Library at EI Studio {#sec-image-classification-model-deployment-arduino-library-ei-studio-5579}\n\nOn the **Deploy** section at Edge Impulse Studio, Select `Arduino library`, `TensorFlow Lite`, `Quantized(int8)`, and press `[Build]`. The trained model will be downloaded as a .zip Arduino library:\n\n\\noindent\n![](./images/png/ard-deploy.png){width=80% fig-align=\"center\"}\n\nOpen your Arduino IDE, and under **Sketch,** go to **Include Library** and **add .ZIP Library.** Next, select the file downloaded from Edge Impulse Studio and press `[Open]`.\n\n\\noindent\n![](./images/png/ard-install_lib.png){width=80% fig-align=\"center\"}\n\nGo to the Arduino IDE `Examples` and look for the project by its name (in this case: \"Box_versus_Whell_...Interfering\". Open `esp32` ->  `esp32_camera`. The sketch `esp32_camera.ino` will be downloaded to the IDE.\n\nThis sketch was developed for the standard ESP32 and will not work with the XIAO ESP32S3 Sense. It should be modified. Let's download the modified one from the project GitHub: [Image_class_XIAOML-Kit.ino](https://github.com/Mjrovai/XIAO-ESP32S3-Sense/blob/main/XIAOML_Kit_code/image_class_XIAOML-Kit/image_class_XIAOML-Kit.ino).\n\n#### XIAO ESP32S3 Image Classification Code Explained {#sec-image-classification-xiao-esp32s3-image-classification-code-explained-5865}\n\nThe code captures images from the onboard camera, processes them, and classifies them (in this case, \"Box\", \"Wheel\", or \"Background\") using the trained model on EI Studio. It runs continuously, performing real-time inference on the edge device.\n\nIn short,:\n\nCamera → JPEG Image → RGB888 Conversion → Resize to 96x96 →\nNeural Network → Classification Results → Serial Output\n\n##### Key Components {#sec-image-classification-key-components-5a0e}\n\n1. **Library Includes and Dependencies**\n\n```cpp\n#include <Box_versus_Wheel_-_XIAO_ESP32S3_inferencing.h>\n#include \"edge-impulse-sdk/dsp/image/image.hpp\"\n#include \"esp_camera.h\"\n```\n\n- **Edge Impulse Inference Library**: Contains our trained model and inference engine\n- **Image Processing**: Provides functions for image manipulation\n- **ESP Camera**: Hardware interface for the camera module\n\n2. **Camera Pin Configurations**\n\nThe XIAO ESP32S3 Sense can work with different camera sensors (OV2640 or OV3660), which may have different pin configurations. The code defines three possible configurations:\n\n```cpp\n// Configuration 1: Most common OV2640 configuration\n#define CONFIG_1_XCLK_GPIO_NUM    10\n#define CONFIG_1_SIOD_GPIO_NUM    40\n#define CONFIG_1_SIOC_GPIO_NUM    39\n// ... more pins\n```\n\nThis flexibility allows the code to automatically try different pin mappings if the first one doesn't work, making it more robust across different hardware revisions.\n\n3. **Memory Management Settings**\n\n```cpp\n#define EI_CAMERA_RAW_FRAME_BUFFER_COLS   320\n#define EI_CAMERA_RAW_FRAME_BUFFER_ROWS   240\n#define EI_CLASSIFIER_ALLOCATION_HEAP      1\n```\n\n- **Frame Buffer Size**: Defines the raw image size (320x240 pixels)\n- **Heap Allocation**: Uses dynamic memory allocation for flexibility\n- **PSRAM Support**: The ESP32S3 has 8MB of PSRAM for storing large data like images\n\n##### `setup()` - Initialization {#sec-image-classification-setup-initialization-b8f7}\n\n```cpp\nvoid setup() {\n    Serial.begin(115200);\n    while (!Serial);\n\n    if (ei_camera_init() == false) {\n        ei_printf(\"Failed to initialize Camera!\\r\\n\");\n    } else {\n        ei_printf(\"Camera initialized\\r\\n\");\n    }\n\n    ei_sleep(2000);  // Wait 2 seconds before starting\n}\n```\n\nThis function:\n\n1. Initializes serial communication for debugging output\n2. Initializes the camera with automatic configuration detection\n3. Waits 2 seconds before starting continuous inference\n\n##### `loop()` - Main Processing Loop {#sec-image-classification-loop-main-processing-loop-f973}\n\nThe loop performs these steps continuously:\n\n**Step 1: Memory Allocation**\n\n```cpp\nsnapshot_buf = (uint8_t*)ps_malloc(EI_CAMERA_RAW_FRAME_BUFFER_COLS *\n                                   EI_CAMERA_RAW_FRAME_BUFFER_ROWS *\n                                   EI_CAMERA_FRAME_BYTE_SIZE);\n```\n\nAllocates memory for the image buffer, preferring PSRAM (faster external RAM) but falling back to regular heap if needed.\n\n**Step 2: Image Capture**\n\n```cpp\nif (ei_camera_capture((size_t)EI_CLASSIFIER_INPUT_WIDTH,\n                     (size_t)EI_CLASSIFIER_INPUT_HEIGHT,\n                     snapshot_buf) == false) {\n    ei_printf(\"Failed to capture image\\r\\n\");\n    free(snapshot_buf);\n    return;\n}\n```\n\nCaptures an image from the camera and stores it in the buffer.\n\n**Step 3: Run Inference**\n\n```cpp\nei_impulse_result_t result = { 0 };\nEI_IMPULSE_ERROR err = run_classifier(&signal, &result, false);\n```\n\nRuns the machine learning model on the captured image.\n\n**Step 4: Output Results**\n\n```cpp\nfor (uint16_t i = 0; i < EI_CLASSIFIER_LABEL_COUNT; i++) {\n    ei_printf(\"  %s: %.5f\\r\\n\",\n              ei_classifier_inferencing_categories[i],\n              result.classification[i].value);\n}\n```\n\nPrints the classification results showing confidence scores for each category.\n\n##### `ei_camera_init()` - Smart Camera Initialization {#sec-image-classification-ei_camera_init-smart-camera-initialization-e11e}\n\nThis function implements an intelligent initialization sequence:\n\n```cpp\nbool ei_camera_init(void) {\n    // Try Configuration 1 (OV2640 common)\n    update_camera_config(1);\n    esp_err_t err = esp_camera_init(&camera_config);\n    if (err == ESP_OK) goto camera_init_success;\n\n    // Try Configuration 2 (OV3660)\n    esp_camera_deinit();\n    update_camera_config(2);\n    err = esp_camera_init(&camera_config);\n    if (err == ESP_OK) goto camera_init_success;\n\n    // Continue trying other configurations...\n}\n```\n\nThe function:\n\n1. Tries multiple pin configurations\n2. Tests different clock frequencies (10MHz or 16MHz)\n3. Attempts PSRAM first, then falls back to DRAM\n4. Applies sensor-specific settings based on detected hardware\n\n##### `ei_camera_capture()` - Image Processing Pipeline {#sec-image-classification-ei_camera_capture-image-processing-pipeline-da03}\n\n```cpp\nbool ei_camera_capture(uint32_t img_width, uint32_t img_height, uint8_t *out_buf) {\n    // 1. Get frame from camera\n    camera_fb_t *fb = esp_camera_fb_get();\n\n    // 2. Convert JPEG to RGB888 format\n    bool converted = fmt2rgb888(fb->buf, fb->len, PIXFORMAT_JPEG, snapshot_buf);\n\n    // 3. Return frame buffer to camera driver\n    esp_camera_fb_return(fb);\n\n    // 4. Resize if needed\n    if (do_resize) {\n        ei::image::processing::crop_and_interpolate_rgb888(...);\n    }\n}\n```\n\nThis function:\n\n1. Captures a JPEG image from the camera\n2. Converts it to RGB888 format (required by the ML model)\n3. Resizes the image to match the model's input size (96x96 pixels)\n\n### Inference {#sec-image-classification-inference-add8}\n\n- Upload the code to the XIAO ESP32S3 Sense.\n\n> ⚠️ **Attention**\n>\n> - The Xiao ESP32S3 **MUST** have the PSRAM enabled. You can check it on the Arduino IDE upper menu: `Tools`--> `PSRAM:OPI PSRAM`\n> - The Arduino Boards package (`esp32 by Espressif Systems`) should be **version 2.017**. Do not update it\n\n\\noindent\n![](./images/png/ard-ide.png){width=90% fig-align=\"center\"}\n\n- Open the Serial Monitor\n- Point the camera at the objects, and check the result on the Serial Monitor.\n\n\\noindent\n![](./images/png/ard-inf.png){width=90% fig-align=\"center\"}\n\n### Post-Processing {#sec-image-classification-postprocessing-dcc8}\n\nIn edge AI applications, the inference result is only as valuable as our ability to act upon it. While serial output provides detailed information for debugging and development, real-world deployments require immediate, human-readable feedback that doesn't depend on external monitors or connections.\n\nThe XIAOML Kit tiny 0.42\" OLED display (72×40 pixels) serves as a crucial post-processing component that transforms raw ML inference results into immediate, human-readable feedback—displaying detected class names and confidence levels directly on the device, eliminating the need for external monitors and enabling truly standalone edge AI deployment in industrial, agricultural, or retail environments where instant visual confirmation of AI predictions is essential.\n\nSo, let's modify the sketch to automatically adapt to the model trained on Edge Impulse by reading the class names and count directly from the model. The display will show abbreviated class names (3 letters) with larger fonts for better visibility on the tiny 72x40 pixel display. Download the code from the GitHub: [XIAOML-Kit-Img_Class_OLED_Gen](https://github.com/Mjrovai/XIAO-ESP32S3-Sense/tree/main/XIAOML_Kit_code/XIAOML-Kit-Img_Class_OLED_Gen).\n\nRunning the code, we can see the result:\n\n\\noindent\n![](./images/png/ard-inher-gen-oled.png){width=90% fig-align=\"center\"}\n\n## Summary {#sec-image-classification-summary-f24b}\n\nThe XIAO ESP32S3 Sense is a remarkably capable and flexible platform for image classification applications. Through this lab, we've explored two distinct development approaches that cater to different skill levels and project requirements.\n\n- The **SenseCraft AI Studio** provides an accessible entry point with its **no-code interface**, enabling rapid prototyping and deployment of pre-trained models like person detection. With real-time inference and integrated post-processing capabilities, it demonstrates how AI can be deployed without extensive programming or ML knowledge.\n\n- For more advanced applications, **Edge Impulse Studio** offers comprehensive machine learning pipeline tools, including custom dataset management, transfer learning with several pre-trained models, such as MobileNet, and model optimization.\n\nKey insights from this lab include the importance of image resolution trade-offs, the effectiveness of transfer learning for small datasets, and the practical considerations of edge AI deployment, such as power consumption and memory constraints.\n\nThe Lab demonstrates fundamental TinyML principles that extend beyond this specific hardware: resource-constrained inference, real-time processing requirements, and the complete pipeline from data collection through model deployment to practical applications. With built-in post-processing capabilities including GPIO control and communication protocols, the XIAO serves as more than just an inference engine—it becomes a complete AI sensor platform.\n\nThis foundation in image classification prepares you for more complex computer vision tasks while showcasing how modern edge AI makes sophisticated computer vision accessible, cost-effective, and deployable in real-world embedded applications ranging from industrial automation to smart home systems.\n\n## Resources {#sec-image-classification-resources-1cf2}\n\n- [Getting Started with the XIAO ESP32S3](https://wiki.seeedstudio.com/xiao_esp32s3_getting_started/)\n- [SenseCraft AI Studio Home](https://sensecraft.seeed.cc/ai/home)\n- [SenseCraft Vision Workspace](https://sensecraft.seeed.cc/ai/device/local/32)\n- [Dataset example](https://www.kaggle.com/kritikseth/fruit-and-vegetable-image-recognition)\n- [Edge Impulse Project](https://studio.edgeimpulse.com/public/757065/live)\n- [XIAO as an AI Sensor](https://wiki.seeedstudio.com/sensecraft-ai/tutorials/sensecraft-ai-output-libraries-xiao/)\n- [Seeed Arduino SSCMA Library](https://github.com/Seeed-Studio/Seeed_Arduino_SSCMA)\n- [XIAOML Kit Code](https://github.com/Mjrovai/XIAO-ESP32S3-Sense/tree/main/XIAOML_Kit_code)\n","srcMarkdownNoYaml":""},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"markdown"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"wrap","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":true,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","toc":true,"toc-depth":3,"number-sections":false,"highlight-style":"github","include-after-body":[{"text":"<script src=\"assets/scripts/subscribe-modal.js\" defer></script>\n"}],"output-file":"image_classification.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.8.25","theme":{"light":["default","../../../../assets/styles/style.scss"],"dark":["default","../../../../assets/styles/style.scss","../../../../assets/styles/dark-mode.scss"]},"respect-user-color-scheme":true,"code-copy":true,"anchor-sections":true},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}