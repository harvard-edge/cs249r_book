{"title":"Motion Classification and Anomaly Detection","markdown":{"headingText":"Motion Classification and Anomaly Detection","headingAttr":{"id":"","classes":["unnumbered"],"keyvalue":[]},"containsRefs":false,"markdown":"\n![DALLÂ·E prompt - 1950s style cartoon illustration set in a vintage audio lab. Scientists, dressed in classic attire with white lab coats, are intently analyzing audio data on large chalkboards. The boards display intricate FFT (Fast Fourier Transform) graphs and time-domain curves. Antique audio equipment is scattered around, but the data representations are clear and detailed, indicating their focus on audio analysis.](./images/jpeg/ini.jpg)\n\n## Overview {#sec-motion-classification-anomaly-detection-overview-cb1f}\n\nTransportation is the backbone of global commerce. Millions of containers are transported daily via various means, such as ships, trucks, and trains, to destinations worldwide. Ensuring the safe and efficient transit of these containers is a monumental task that requires leveraging modern technology, and TinyML is undoubtedly one of the key solutions.\n\nIn this hands-on lab, we will work to solve real-world problems related to transportation. We will develop a Motion Classification and Anomaly Detection system using the XIAOML Kit, the Arduino IDE, and the Edge Impulse Studio. This project will help us understand how containers experience different forces and motions during various phases of transportation, including terrestrial and maritime transit, vertical movement via forklifts, and periods of stationary storage in warehouses.\n\n::: callout-tip\n\n## Learning Objectives {#sec-motion-classification-anomaly-detection-learning-objectives-f4ee}\n\n- Setting up the XIAOML Kit\n- Data Collection and Preprocessing\n- Building the Motion Classification Model\n- Implementing Anomaly Detection\n- Real-world Testing and Analysis\n:::\n\nBy the end of this lab, youâ€™ll have a working prototype that can classify different types of motion and detect anomalies during the transportation of containers. This knowledge can serve as a stepping stone to more advanced projects in the burgeoning field of TinyML, particularly those involving vibration.\n\n## Installing the IMU {#sec-motion-classification-anomaly-detection-installing-imu-cac2}\n\nThe XIAOML Kit comes with a built-in LSM6DS3TR-C 6-axis IMU sensor on the expansion board, eliminating the need for external sensor connections. This integrated approach offers a clean and reliable platform for motion-based machine learning applications.\n\nThe LSM6DS3TR-C combines a 3-axis accelerometer and 3-axis gyroscope in a single package, connected via I2C to the XIAO ESP32S3 at address 0x6A that provides:\n\n- **Accelerometer ranges**: Â±2/Â±4/Â±8/Â±16 g (we'll use Â±2g by default)\n- **Gyroscope ranges**: Â±125/Â±250/Â±500/Â±1000/Â±2000 dps (we'll use Â±250 dps by default)\n- **Resolution**: 16-bit ADC\n- **Communication**: I2C interface at address 0x6A\n- **Power**: Ultra-low power design\n\n\\noindent\n![](./images/png/imu-directions.png){width=\"85%\" fig-align=\"center\"}\n\n**Coordinate System:** The sensor operates within a right-handed coordinate system. When looking at the expansion board from the bottom (where you can see the IMU sensor with the point mark):\n\n- **X-axis**: Points to the right\n- **Y-axis**: Points forward (away from you)\n- **Z-axis**: Points upward (out of the board)\n\n### Setting Up the Hardware {#sec-motion-classification-anomaly-detection-setting-hardware-66a1}\n\nSince the XIAOML Kit comes pre-assembled with the expansion board, no additional hardware connections are required. The LSM6DS3TR-C IMU is already properly connected via I2C.\n\n**What's Already Connected:**\n\n- LSM6DS3TR-C IMU â†’ I2C (SDA/SCL) â†’ XIAO ESP32S3\n- I2C Address: 0x6A\n- Power: 3.3V from XIAO ESP32S3\n\n**Required Library:** You should have the library installed during the Setup. If not,  install the Seeed Arduino LSM6DS3 library following the steps:\n\n1. Open Arduino IDE Library Manager\n2. Search for \"LSM6DS3\"\n3. Install **\"Seeed Arduino LSM6DS3\"** by Seeed Studio\n4. **Important**: Do NOT install \"Arduino_LSM6DS3 by Arduino\" - that's for different boards!\n\n### Testing the IMU Sensor {#sec-motion-classification-anomaly-detection-testing-imu-sensor-67b3}\n\nLet's start with a simple test to verify the IMU is working correctly. Upload this code to test the sensor:\n\n```cpp\n#include <LSM6DS3.h>\n#include <Wire.h>\n\n// Create IMU object using I2C interface\nLSM6DS3 myIMU(I2C_MODE, 0x6A);\n\nfloat accelX, accelY, accelZ;\nfloat gyroX, gyroY, gyroZ;\n\nvoid setup() {\n  Serial.begin(115200);\n  while (!Serial) delay(10);\n\n  Serial.println(\"XIAOML Kit IMU Test\");\n  Serial.println(\"LSM6DS3TR-C 6-Axis IMU\");\n  Serial.println(\"====================\");\n\n  // Initialize the IMU\n  if (myIMU.begin() != 0) {\n      Serial.println(\"ERROR: IMU initialization failed!\");\n      while(1) delay(1000);\n  } else {\n      Serial.println(\"âœ“ IMU initialized successfully\");\n      Serial.println(\"Data Format: AccelX,AccelY,AccelZ,\"\n                    \"GyroX,GyroY,GyroZ\");\n      Serial.println(\"Units: g-force, degrees/second\");\n      Serial.println();\n  }\n}\n\nvoid loop() {\n  // Read accelerometer data (in g-force)\n  accelX = myIMU.readFloatAccelX();\n  accelY = myIMU.readFloatAccelY();\n  accelZ = myIMU.readFloatAccelZ();\n\n  // Read gyroscope data (in degrees per second)\n  gyroX = myIMU.readFloatGyroX();\n  gyroY = myIMU.readFloatGyroY();\n  gyroZ = myIMU.readFloatGyroZ();\n\n  // Print readable format\n  Serial.print(\"Accel (g): X=\"); Serial.print(accelX, 3);\n  Serial.print(\" Y=\"); Serial.print(accelY, 3);\n  Serial.print(\" Z=\"); Serial.print(accelZ, 3);\n  Serial.print(\" | Gyro (Â°/s): X=\"); Serial.print(gyroX, 2);\n  Serial.print(\" Y=\"); Serial.print(gyroY, 2);\n  Serial.print(\" Z=\"); Serial.println(gyroZ, 2);\n\n  delay(100); // 10 Hz update rate\n}\n```\n\nWhen the kit is resting flat on a table, you should see:\n\n- Z-axis acceleration around +1.0g (gravity)\n- X and Y acceleration near 0.0g\n- All gyroscope values near 0.0Â°/s\n\nMove the kit around to see the values change accordingly.\n\n## The TinyML Motion Classification Project {#sec-motion-classification-anomaly-detection-tinyml-motion-classification-project-90f9}\n\nWe will simulate container (or, more accurately, package) transportation through various scenarios to make this tutorial more relatable and practical.\n\n\\noindent\n![](./images/jpeg/classes.jpg){width=\"85%\" fig-align=\"center\"}\n\nUsing the accelerometer of the XIAOML Kit, we'll capture motion data by manually simulating the conditions of:\n\n- **Maritime** (pallets on boats) - Movement in all axes with wave-like patterns\n- **Terrestrial** (pallets on trucks/trains) - Primarily horizontal movement\n- **Lift** (pallets being moved by forklift) - Primarily vertical movement\n- **Idle** (pallets in storage) - Minimal movement\n\nFrom the above image, we can define for our simulation that primarily horizontal movements ($x$ or $y$ axis) should be associated with the \"Terrestrial class. \" Vertical movements ($z$-axis) with the \"Lift Class,\" no activity with the \"Idle class,\" and movement on all three axes to [Maritime class.](https://www.containerhandbuch.de/chb_e/stra/index.html?/chb_e/stra/stra_02_03_03.htm)\n\n\\noindent\n![](./images/jpeg/classes_mov_def.jpg){width=\"85%\" fig-align=\"center\"}\n\n## Data Collection {#sec-motion-classification-anomaly-detection-data-collection-d54c}\n\nFor data collection, we have several options available. In a real-world scenario, we can have our device, for example, connected directly to one container, and the collected data stored in a file (for example, CSV) on an SD card. Data can also be sent remotely to a nearby repository, such as a mobile phone, using Wi-Fi or Bluetooth (as demonstrated in this project: [Sensor DataLogger](https://www.hackster.io/mjrobot/sensor-datalogger-50e44d)). Once your dataset is collected and stored as a .CSV file, it can be uploaded to the Studio using the [CSV Wizard tool](https://docs.edgeimpulse.com/docs/edge-impulse-studio/data-acquisition/csv-wizard).\n\n> In this [video](https://youtu.be/2KBPq_826WM), you can learn alternative ways to send data to the Edge Impulse Studio.\n\n### Preparing the Data Collection Code {#sec-motion-classification-anomaly-detection-preparing-data-collection-code-da6b}\n\nIn this lab, we will connect the Kit directly to the Edge Impulse Studio, which will also be used for data pre-processing, model training, testing, and deployment.\n\nFor data collection, we should first connect the Kit to Edge Impulse Studio, which will also be used for data pre-processing, model training, testing, and deployment.\n\n> Follow the instructions [here](https://docs.edgeimpulse.com/docs/edge-impulse-cli/cli-installation) to install [Node.js](https://nodejs.org/en/) and Edge Impulse CLI on your computer.\n\nOnce the XIAOML Kit is not a fully supported development board by Edge Impulse, we should, for example, use the [CLI Data Forwarder](https://docs.edgeimpulse.com/docs/edge-impulse-cli/cli-data-forwarder) to capture data from our sensor and send it to the Studio, as shown in this diagram:\n\n\\noindent\n![](./images/jpeg/data-forw.jpg){width=\"75%\" fig-align=\"center\"}\n\nWe'll modify our test code to output data in a format suitable for Edge Impulse:\n\n```cpp\n#include <LSM6DS3.h>\n#include <Wire.h>\n\n#define FREQUENCY_HZ        50\n#define INTERVAL_MS         (1000 / (FREQUENCY_HZ + 1))\n\nLSM6DS3 myIMU(I2C_MODE, 0x6A);\nstatic unsigned long last_interval_ms = 0;\n\nvoid setup() {\n  Serial.begin(115200);\n  while (!Serial) delay(10);\n\n  Serial.println(\"XIAOML Kit - Motion Data Collection\");\n  Serial.println(\"LSM6DS3TR-C IMU Sensor\");\n\n  // Initialize IMU\n  if (myIMU.begin() != 0) {\n      Serial.println(\"ERROR: IMU initialization failed!\");\n      while(1) delay(1000);\n  }\n\n  delay(2000);\n  Serial.println(\"Starting data collection in 3 seconds...\");\n  delay(3000);\n}\n\nvoid loop() {\n  if (millis() > last_interval_ms + INTERVAL_MS) {\n      last_interval_ms = millis();\n\n      // Read accelerometer data\n      float ax = myIMU.readFloatAccelX();\n      float ay = myIMU.readFloatAccelY();\n      float az = myIMU.readFloatAccelZ();\n\n      // Convert to m/sÂ² (multiply by 9.81)\n      float ax_ms2 = ax * 9.81;\n      float ay_ms2 = ay * 9.81;\n      float az_ms2 = az * 9.81;\n\n      // Output in Edge Impulse format\n      Serial.print(ax_ms2);\n      Serial.print(\"\\t\");\n      Serial.print(ay_ms2);\n      Serial.print(\"\\t\");\n      Serial.println(az_ms2);\n  }\n}\n```\n\nUpload the code to the Arduino IDE. We should see the accelerometer values (converted to m/sÂ²) at the Serial Monitor:\n\n\\noindent\n![](./images/png/serial_monitor.png){width=\"85%\" fig-align=\"center\"}\n\n> Keep the code running, but **turn off the Serial Monitor**. The data generated by the Kit will be sent to the Edge Impulse Studio via Serial Connection.\n\n### Connecting to Edge Impulse for Data Collection {#sec-motion-classification-anomaly-detection-connecting-edge-impulse-data-collection-c41a}\n\n**Create an Edge Impulse Project**\n-  Go to Edge Impulse Studio and create a new project\n- Choose a descriptive name (keep under 63 characters for Arduino library compatibility)\n\n\\noindent\n![](./images/png/eistudio-ini.png){width=\"85%\" fig-align=\"center\"}\n\n**Set up CLI Data Forwarder**\n- Install Edge Impulse CLI on your computer\n- Confirm that the XIAOML Kit is connected to the computer, **the code is running and the Serial Monitor is OFF**, otherwise we can get an error.\n- On the Computer Terminal, run: `edge-impulse-data-forwarder --clean`\n- Enter your Edge Impulse credentials\n- Select your project and configure device settings\n\n\\noindent\n![](./images/png/terminal-cli.png){width=\"85%\" fig-align=\"center\"}\n\n- Go to the Edge Impulse Studio Project. On the `Device` section is possible to verify if the kit is correctly connected (the dot should be green).\n\n\\noindent\n![](./images/png/ei-device.png){width=\"85%\" fig-align=\"center\"}\n\n## Data Collection at the Studio {#sec-motion-classification-anomaly-detection-data-collection-studio-5ca4}\n\nAs discussed before, we should capture data from all four **Transportation Classes.** Imagine that you have a container with a built-in accelerometer (In this case, our XIAOML Kit). Now imagine your container is on a boat, facing an angry ocean:\n\n\\noindent\n![](./images/png/boat.png){width=70% fig-align=\"center\"}\n\nOr in a Truck, travelling on a road, or being moved with a forklift, etc.\n\n### Movement Simulation {#sec-motion-classification-anomaly-detection-movement-simulation-2e54}\n\n**Maritime Class:**\n\n- Hold the kit and simulate boat movement\n- Move in all three axes with wave-like, undulating motions\n- Include gentle rolling and pitching movements\n\n**Terrestrial Class:**\n\n- Move the kit horizontally in straight lines (left to right and vice versa)\n- Simulate truck/train vibrations with small horizontal shakes\n- Occasional gentle bumps and turns\n\n**Lift Class:**\n\n- Move the kit primarily in vertical directions (up and down)\n- Simulate forklift operations: up, pause, down\n- Include some short horizontal positioning movements\n\n**Idle Class:**\n\n- Place the kit on a stable surface\n- Minimal to no movement\n- Capture environmental vibrations and sensor noise\n\n### Data Acquisition {#sec-motion-classification-anomaly-detection-data-acquisition-e277}\n\nOn the `Data Acquisition` section, you should see that your board `[xiaoml-kit]` is connected. The sensor is available: `[sensor with 3 axes (accX, accY, accZ)]` with a sampling frequency of `[50 Hz]`. The Studio suggests a sample length of `[10000]` ms (10 s). The last thing left is defining the sample label. Let's start, for example, with`[terrestrial]`.\n\nPress `[Start Sample]`and move your kit horizontally (left to right), keeping it in one direction. After 10 seconds, our data will be uploaded to the Studio.\n\nBelow is one sample (raw data) of 10 seconds of collected data. It is notable that the ondulatory movement predominantly occurs along the Y-axis (left-right). The other axes are almost stationary (the X-axis is centered around zero, and the Z-axis is centered around 9.8 msÂ² due to gravity).\n\n\\noindent\n![](./images/png/terrestrial-raw_data.png){width=70% fig-align=\"center\"}\n\nYou should capture, for example, around 2 minutes (ten to twelve samples of 10 seconds each) for each of the four classes. Using the `3 dots` after each sample, select two and move them to the **Test set**. Alternatively, you can use the Automatic `Train/Test Split` tool on the **Danger Zone** of the `Dashboard` tab. Below, it is possible to see the result datasets:\n\n\\noindent\n![](./images/png/datasets.png){width=85% fig-align=\"center\"}\n\n## Data Pre-Processing {#sec-motion-classification-anomaly-detection-data-preprocessing-2269}\n\nThe raw data type captured by the accelerometer is a \"time series\" and should be converted to \"tabular data\". We can do this conversion using a sliding window over the sample data. For example, in the below figure,\n\n\\noindent\n![](./images/png/features.png){width=85% fig-align=\"center\"}\n\nWe can see 10 seconds of accelerometer data captured with a sample rate (SR) of 50 Hz. A 2-second window will capture 300 data points (3 axes $\\times$ 2 seconds $\\times$ 50 samples). We will slide this window every 200ms, creating a larger dataset where each instance has 300 raw features.\n\n> You should use the best SR for your case, considering Nyquist's theorem, which states that a periodic signal must be sampled at more than twice the signal's highest frequency component.\n\nData preprocessing is a challenging area for embedded machine learning. Still, Edge Impulse helps overcome this with its digital signal processing (DSP) preprocessing step and, more specifically, the Spectral Features.\n\nOn the Studio, this dataset will be the input of a Spectral Analysis block, which is excellent for analyzing repetitive motion, such as data from accelerometers. This block will perform a DSP (Digital Signal Processing), extracting features such as \"FFT\" or \"Wavelets\". In the most common case, FFT, the **Time Domain Statistical features** per axis/channel are:\n\n- RMS\n- Skewness\n- Kurtosis\n\n![](./images/png/time-dom.png){width=85% fig-align=\"center\"}\n\nAnd the **Frequency Domain Spectral features** per axis/channel are:\n\n- Spectral Power\n- Skewness\n- Kurtosis\n\n\\noindent\n![](./images/png/freq-dom.png){width=85% fig-align=\"center\"}\n\nFor example, for an FFT length of 32 points, the Spectral Analysis Block's resulting output will be 21 features per axis (a total of 63 features).\n\nThose 63 features will serve as the input tensor for a Neural Network Classifier and the Anomaly Detection model (K-Means).\n\n> You can learn more by digging into the lab [DSP Spectral Features](@sec-dsp-spectral-features-overview-a7be)\n\n## Model Design {#sec-motion-classification-anomaly-detection-model-design-d2d4}\n\nOur classifier will be a Dense Neural Network (DNN) that will have 63 neurons on its input layer, two hidden layers with 20 and 10 neurons, and an output layer with four neurons (one per each class), as shown here:\n\n\\noindent\n![](./images/png/model.png){width=85% fig-align=\"center\"}\n\n## Impulse Design {#sec-motion-classification-anomaly-detection-impulse-design-18e9}\n\nAn impulse takes raw data, uses signal processing to extract features, and then uses a learning block (**Dense model**) to classify new data.\n\nWe also utilize a second model, the **K-means**, which can be used for Anomaly Detection. If we imagine that we could have our known classes as clusters, any sample that cannot fit into one of these clusters could be an outlier, an anomaly (for example, a container rolling out of a ship on the ocean or being upside down on the floor).\n\n\\noindent\n![](./images/jpeg/anomaly.jpg){width=70% fig-align=\"center\"}\n\n> Imagine our XIAOML Kit rolling or moving upside-down, on a movement complement different from the one trained on.\n\n\\noindent\n![](./images/png/models.png){width=85% fig-align=\"center\"}\n\nBelow the final Impulse design:\n\n\\noindent\n![](./images/png/impulse.png){width=85% fig-align=\"center\"}\n\n## Generating features {#sec-motion-classification-anomaly-detection-generating-features-cf4b}\n\nAt this point in our project, we have defined the pre-processing method, and the model has been designed. Now, it is time to have the job done. First, let's convert the raw data (time-series type) into tabular data. Go to the `Spectral Features` tab and select `[Save Parameters]`. Alternatively, instead of using the default values, we can select the `[Autotune parameters]` button. In this case, the Studio will define new hyperparameters, as the filter design and FFT length, based on the raw data.\n\n\\noindent\n![](./images/png/features-2.png){width=85% fig-align=\"center\"}\n\nAt the top menu, select the `Generate features` tab, and there, select the options, `Calculate feature importance`, `Normalize features,` and press the `[Generate features]` button. Each 2-second window of data (300 datapoints) will be converted into a single tabular data point with 63 features.\n\n> The Feature Explorer will display this data in 2D using [UMAP.](https://umap-learn.readthedocs.io/en/latest/) Uniform Manifold Approximation and Projection (UMAP) is a dimensionality reduction technique that can be used for visualization, similar to t-SNE, but also for general non-linear dimensionality reduction.\n\nThe visualization enables one to verify that the classes present an excellent separation, indicating that the classifier should perform well.\n\n\\noindent\n![](./images/png/feat-ger.png){width=85% fig-align=\"center\"}\n\nOptionally, you can analyze the relative importance of each feature for one class compared with other classes.\n\n## Training {#sec-motion-classification-anomaly-detection-training-d832}\n\nOur classifier will be a Dense Neural Network (DNN) that will have 63 neurons on its input layer, two hidden layers with 20 and 10 neurons, and an output layer with four neurons (one per each class), as shown here:\n\n\\noindent\n![](./images/png/model-2.png){width=85% fig-align=\"center\"}\n\nAs hyperparameters, we will use a Learning Rate of 0.005 and 20% of the data for validation for 30 epochs. After training, we can see that the accuracy is 100%.\n\n\\noindent\n![](./images/png/train.png){width=85% fig-align=\"center\"}\n\nFor anomaly detection, we should choose the suggested features that are precisely the most important in feature extraction. The number of clusters will be 32, as suggested by the Studio. After training, we can select some data for testing, such as maritime data. The resulting Anomaly score was `min: -0.1642, max: 0.0738, avg: -0.0867`.\n\nWhen changing the data, it is possible to realize that small or negative Anomaly Scores indicate that the data are normal.\n\n\\noindent\n![](./images/png/anomaly-train.png){width=70% fig-align=\"center\"}\n\n## Testing {#sec-motion-classification-anomaly-detection-testing-c2bb}\n\nUsing 20% of the data left behind during the data capture phase, we can verify how our model will behave with unknown data; if not 100% (what is expected), the result was very good (8%).\n\n\\noindent\n![](./images/png/test.png){width=85% fig-align=\"center\"}\n\nYou should also use your kit (which is still connected to the Studio) and perform some Live Classification. For example, let's test some \"terrestrial\" movement:\n\n\\noindent\n![](./images/png/live-test.png){width=85% fig-align=\"center\"}\n\n> Be aware that here, you will capture real data with your device and upload it to the Studio, where an inference will be made using the trained model (note that the model is not on your device).\n\n## Deploy {#sec-motion-classification-anomaly-detection-deploy-aa7e}\n\nNow it is time for magic! The Studio will package all the needed libraries, preprocessing functions, and trained models, downloading them to your computer. You should select the Arduino Library option, and then, at the bottom, choose Quantized (Int8) and click `[Build]`. A ZIP file will be created and downloaded to your computer.\n\n\\noindent\n![](./images/png/deploy.png){width=85% fig-align=\"center\"}\n\nOn your Arduino IDE, go to the Sketch tab, select the option Add.ZIP Library, and Choose the.zip file downloaded by the Studio:\n\n\\noindent\n![](./images/png/lib.png){width=85% fig-align=\"center\"}\n\n## Inference {#sec-motion-classification-anomaly-detection-inference-8351}\n\nNow, it is time for a real test. We will make inferences that are wholly disconnected from the Studio. Let's change one of the code examples created when you deploy the Arduino Library.\n\nIn your Arduino IDE, go to the `File/Examples` tab and look for your project, and in examples, select `nano_ble_sense_accelerometer`:\n\nOf course, this is not your board, but we can have the code working with only a few changes.\n\nFor example, at the beginning of the code, you have the library related to Arduino Sense IMU:\n\n```cpp\n/* Includes -------------------------------------------- */\n#include <XIAOML_Kit_Motion_Class_-_AD_inferencing.h>\n#include <Arduino_LSM9DS1.h>\n```\n\nChange the \"includes\" portion with the code related to the IMU:\n\n```cpp\n#include <XIAOML_Kit_Motion_Class_-_AD_inferencing.h>\n#include <LSM6DS3.h>\n#include <Wire.h>\n```\n\nChange the Constant Defines\n\n```cpp\n// IMU setup\nLSM6DS3 myIMU(I2C_MODE, 0x6A);\n\n// Inference settings\n#define CONVERT_G_TO_MS2    9.81f\n#define MAX_ACCEPTED_RANGE  2.0f * CONVERT_G_TO_MS2\n```\n\nOn the setup function, initiate the IMU:\n\n```cpp\n   // Initialize IMU\n    if (myIMU.begin() != 0) {\n        Serial.println(\"ERROR: IMU initialization failed!\");\n        return;\n    }\n```\n\nAt the loop function, the buffers buffer[ix], buffer[ix + 1], and  buffer[ix + 2] will receive the 3-axis data captured by the accelerometer. In the original code, you have the line:\n\n```cpp\nIMU.readAcceleration(buffer[ix], buffer[ix + 1], buffer[ix + 2]);\n```\n\nChange it with this block of code:\n\n```cpp\n// Read IMU data\nfloat x = myIMU.readFloatAccelX();\nfloat y = myIMU.readFloatAccelY();\nfloat z = myIMU.readFloatAccelZ();\n```\n\nYou should reorder the following two blocks of code. First, you make the conversion to raw data to \"Meters per squared second (m/s^2^)\", followed by the test regarding the maximum acceptance range (that here is in m/s^2^, but on Arduino, was in Gs):\n\n```cpp\n  // Convert to m/sÂ²\n  buffer[i + 0] = x * CONVERT_G_TO_MS2;\n  buffer[i + 1] = y * CONVERT_G_TO_MS2;\n  buffer[i + 2] = z * CONVERT_G_TO_MS2;\n\n  // Apply range limiting\n  for (int j = 0; j < 3; j++) {\n      if (fabs(buffer[i + j]) > MAX_ACCEPTED_RANGE) {\n          buffer[i + j] = copysign(MAX_ACCEPTED_RANGE, buffer[i + j]);\n      }\n  }\n```\n\nAnd this is enough. We can also adjust how the inference is displayed in the Serial Monitor. You can now upload the complete code below to your device and proceed with the inferences.\n\n```cpp\n// Motion Classification with LSM6DS3TR-C IMU\n#include <XIAOML_Kit_Motion_Class_-_AD_inferencing.h>\n#include <LSM6DS3.h>\n#include <Wire.h>\n\n// IMU setup\nLSM6DS3 myIMU(I2C_MODE, 0x6A);\n\n// Inference settings\n#define CONVERT_G_TO_MS2    9.81f\n#define MAX_ACCEPTED_RANGE  2.0f * CONVERT_G_TO_MS2\n\nstatic bool debug_nn = false;\nstatic float buffer[EI_CLASSIFIER_DSP_INPUT_FRAME_SIZE] = { 0 };\nstatic float inference_buffer[EI_CLASSIFIER_DSP_INPUT_FRAME_SIZE];\n\nvoid setup() {\n    Serial.begin(115200);\n    while (!Serial) delay(10);\n\n    Serial.println(\"XIAOML Kit - Motion Classification\");\n    Serial.println(\"LSM6DS3TR-C IMU Inference\");\n\n    // Initialize IMU\n    if (myIMU.begin() != 0) {\n        Serial.println(\"ERROR: IMU initialization failed!\");\n        return;\n    }\n\n    Serial.println(\"âœ“ IMU initialized\");\n\n    if (EI_CLASSIFIER_RAW_SAMPLES_PER_FRAME != 3) {\n        Serial.println(\"ERROR: EI_CLASSIFIER_RAW_SAMPLES_PER_FRAME\"\n                       \"should be 3\");\n        return;\n    }\n\n    Serial.println(\"âœ“ Model loaded\");\n    Serial.println(\"Starting motion classification...\");\n}\n\nvoid loop() {\n    ei_printf(\"\\nStarting inferencing in 2 seconds...\\n\");\n    delay(2000);\n\n    ei_printf(\"Sampling...\\n\");\n\n    // Clear buffer\n    for (size_t i = 0; i < EI_CLASSIFIER_DSP_INPUT_FRAME_SIZE; i++) {\n        buffer[i] = 0.0f;\n    }\n\n    // Collect accelerometer data\n    for (int i = 0; i < EI_CLASSIFIER_DSP_INPUT_FRAME_SIZE; i += 3) {\n        uint64_t next_tick = micros() +\n          (EI_CLASSIFIER_INTERVAL_MS * 1000);\n\n        // Read IMU data\n        float x = myIMU.readFloatAccelX();\n        float y = myIMU.readFloatAccelY();\n        float z = myIMU.readFloatAccelZ();\n\n        // Convert to m/sÂ²\n        buffer[i + 0] = x * CONVERT_G_TO_MS2;\n        buffer[i + 1] = y * CONVERT_G_TO_MS2;\n        buffer[i + 2] = z * CONVERT_G_TO_MS2;\n\n        // Apply range limiting\n        for (int j = 0; j < 3; j++) {\n            if (fabs(buffer[i + j]) > MAX_ACCEPTED_RANGE) {\n                buffer[i + j] = copysign(MAX_ACCEPTED_RANGE,\n                                         buffer[i + j]);\n            }\n        }\n\n        delayMicroseconds(next_tick - micros());\n    }\n\n    // Copy to inference buffer\n    for (int i = 0; i < EI_CLASSIFIER_DSP_INPUT_FRAME_SIZE; i++) {\n        inference_buffer[i] = buffer[i];\n    }\n\n    // Create signal from buffer\n    signal_t signal;\n    int err = numpy::signal_from_buffer(inference_buffer,\n              EI_CLASSIFIER_DSP_INPUT_FRAME_SIZE, &signal);\n    if (err != 0) {\n        ei_printf(\"ERROR: Failed to create signal from buffer (%d)\\n\",\n                  err);\n        return;\n    }\n\n    // Run the classifier\n    ei_impulse_result_t result = { 0 };\n    err = run_classifier(&signal, &result, debug_nn);\n    if (err != EI_IMPULSE_OK) {\n        ei_printf(\"ERROR: Failed to run classifier (%d)\\n\", err);\n        return;\n    }\n\n    // Print predictions\n    ei_printf(\"Predictions (DSP: %d ms, Classification: %d ms, \"\n              \"Anomaly: %d ms):\\n\",\n        result.timing.dsp, result.timing.classification, result.timing.anomaly);\n\n    for (size_t ix = 0; ix < EI_CLASSIFIER_LABEL_COUNT; ix++) {\n        ei_printf(\"    %s: %.5f\\n\", result.classification[ix].label,\n                  result.classification[ix].value);\n    }\n\n    // Print anomaly score\n#if EI_CLASSIFIER_HAS_ANOMALY == 1\n    ei_printf(\"Anomaly score: %.3f\\n\", result.anomaly);\n#endif\n\n    // Determine prediction\n    float max_confidence = 0.0;\n    String predicted_class = \"unknown\";\n\n    for (size_t ix = 0; ix < EI_CLASSIFIER_LABEL_COUNT; ix++) {\n        if (result.classification[ix].value > max_confidence) {\n            max_confidence = result.classification[ix].value;\n            predicted_class = String(result.classification[ix].label);\n        }\n    }\n\n    // Display result with confidence threshold\n    if (max_confidence > 0.6) {\n        ei_printf(\"\\nðŸŽ¯ PREDICTION: %s (%.1f%% confidence)\\n\",\n                 predicted_class.c_str(), max_confidence * 100);\n    } else {\n        ei_printf(\"\\nâ“ UNCERTAIN: Highest confidence is %s (%.1f%%)\\n\",\n                 predicted_class.c_str(), max_confidence * 100);\n    }\n\n    // Check for anomaly\n#if EI_CLASSIFIER_HAS_ANOMALY == 1\n    if (result.anomaly > 0.5) {\n        ei_printf(\"âš ï¸ ANOMALY DETECTED! Score: %.3f\\n\", result.anomaly);\n    }\n#endif\n\n    delay(1000);\n}\n\nvoid ei_printf(const char *format, ...) {\n    static char print_buf[1024] = { 0 };\n    va_list args;\n    va_start(args, format);\n    int r = vsnprintf(print_buf, sizeof(print_buf), format, args);\n    va_end(args);\n    if (r > 0) {\n        Serial.write(print_buf);\n    }\n}\n\n```\n\n> The complete code is available on the [Lab's GitHub](https://github.com/Mjrovai/XIAO-ESP32S3-Sense/tree/main/XIAOML_Kit_code/motion_class_ad_inference).\n\nNow you should try your movements, seeing the result of the inference of each class on the images:\n\n\\noindent\n![](./images/png/inf-idle.png){width=90% fig-align=\"center\"}\n\n\\noindent\n![](./images/png/inf-ter.png){width=90% fig-align=\"center\"}\n\n\\noindent\n![](./images/png/inf-lift.png){width=90% fig-align=\"center\"}\n\n\\noindent\n![](./images/png/inf-mar.png){width=90% fig-align=\"center\"}\n\nAnd, of course, some \"anomaly\", for example, putting the XIAO upside-down. The anomaly score will be over 0.5:\n\n\\noindent\n![](./images/png/inf-ano.png){width=90% fig-align=\"center\"}\n\n## Post-Processing {#sec-motion-classification-anomaly-detection-postprossessing-ef66}\n\nNow that we know the model is working, we suggest modifying the code to see the result with the Kit completely offline (disconnected from the PC and powered by a battery, a power bank, or an independent 5V power supply).\n\nThe idea is that if a specific movement is detected, a corresponding message will appear on the OLED display.\n\n![](./images/png/inf-oled.png)\n\nThe modified inference code to have the OLED display is available on the [Lab's GitHub](https://github.com/Mjrovai/XIAO-ESP32S3-Sense/tree/main/XIAOML_Kit_code/motion_class_ad_inference_oled).\n\n## Summary {#sec-motion-classification-anomaly-detection-summary-35d1}\n\nThis lab demonstrated how to build a complete motion classification system using the XIAOML Kit's built-in LSM6DS3TR-C IMU sensor. Key achievements include:\n\n**Technical Implementation:**\n\n- Utilized the integrated 6-axis IMU for motion sensing\n- Collected labeled training data for four transportation scenarios\n- Implemented spectral feature extraction for time-series analysis\n- Deployed a neural network classifier optimized for microcontroller inference\n- Added anomaly detection for identifying unusual movements\n\n**Machine Learning Pipeline:**\n\n- Data collection directly from embedded sensors\n- Feature engineering using frequency domain analysis\n- Model training and optimization in Edge Impulse\n- Real-time inference on resource-constrained hardware\n- Performance monitoring and validation\n\n**Practical Applications:** The techniques learned apply directly to real-world scenarios, including:\n\n- Asset tracking and logistics monitoring\n- Predictive maintenance for machinery\n- Human activity recognition\n- Vehicle and equipment monitoring\n- IoT sensor networks for smart cities\n\n**Key Learnings:**\n\n- Working with IMU coordinate systems and sensor fusion\n- Balancing model accuracy with inference speed on edge devices\n- Implementing robust data collection and preprocessing pipelines\n- Deploying machine learning models to embedded systems\n- Integrating multiple sensors (IMU + display) for complete solutions\n\nThe integration of motion classification with the XIAOML Kit demonstrates how modern embedded systems can perform sophisticated AI tasks locally, enabling real-time decision-making without reliance on the cloud. This approach is fundamental to the future of edge AI in industrial IoT, autonomous systems, and smart device applications.\n\n## Resources {#sec-motion-classification-anomaly-detection-resources-cd54}\n\n- [XIAOML KIT Code](https://github.com/Mjrovai/XIAO-ESP32S3-Sense/tree/main/XIAOML_Kit_code)\n- [DSP Spectral Features](@sec-dsp-spectral-features-overview-a7be)\n- [Edge Impulse Project](https://studio.edgeimpulse.com/public/750061/live)\n- [Edge Impulse Spectral Features Block Colab Notebook](https://colab.research.google.com/github/Mjrovai/Arduino_Nicla_Vision/blob/main/Motion_Classification/Edge_Impulse_Spectral_Features_Block.ipynb)\n- [Edge Impulse Documentation](https://docs.edgeimpulse.com/)\n- [Edge Impulse Spectral Features](https://docs.edgeimpulse.com/docs/edge-impulse-studio/processing-blocks/spectral-features)\n- [Seeed Studio LSM6DS3 Library](https://github.com/Seeed-Studio/Seeed_Arduino_LSM6DS3)\n\n<!-- This is here to make sure that quizzes are inserted properly before a part begins. -->\n::: { .quiz-end }\n:::\n\n```{=latex}\n\\part{key:grove}\n```\n","srcMarkdownNoYaml":""},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"markdown"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"wrap","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":true,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","toc":true,"toc-depth":3,"number-sections":false,"highlight-style":"github","include-after-body":[{"text":"<script src=\"assets/scripts/subscribe-modal.js\" defer></script>\n"}],"output-file":"motion_classification.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.8.25","theme":{"light":["default","../../../../assets/styles/style.scss"],"dark":["default","../../../../assets/styles/style.scss","../../../../assets/styles/dark-mode.scss"]},"respect-user-color-scheme":true,"code-copy":true,"anchor-sections":true},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}