{"title":"Keyword Spotting (KWS)","markdown":{"headingText":"Keyword Spotting (KWS)","headingAttr":{"id":"","classes":["unnumbered"],"keyvalue":[]},"containsRefs":false,"markdown":"\n![*DALL·E prompt - 1950s style cartoon illustration based on a real image by Marcelo Rovai*](./images/png/kws_ini_kit.png)\n\n## Overview {#sec-keyword-spotting-kws-overview-4373}\n\nKeyword Spotting (KWS) is integral to many voice recognition systems, enabling devices to respond to specific words or phrases. While this technology underpins popular devices like Google Assistant or Amazon Alexa, it's equally applicable and achievable on smaller, low-power devices. This lab will guide you through implementing a KWS system using TinyML on the XIAO ESP32S3 microcontroller board.\n\nThe XIAO ESP32S3, equipped with Espressif's ESP32-S3 chip, is a compact and potent microcontroller offering a dual-core Xtensa LX7 processor, integrated Wi-Fi, and Bluetooth. Its balance of computational power, energy efficiency, and versatile connectivity makes it a fantastic platform for TinyML applications. Also, with its expansion board, we will have access to the \"sense\" part of the device, which has a camera, an SD card slot, and a **digital microphone**. The integrated microphone and the SD card will be essential in this project.\n\nWe will use the [Edge Impulse Studio](https://www.edgeimpulse.com/), a powerful, user-friendly platform that simplifies creating and deploying machine learning models onto edge devices. We'll train a KWS model step-by-step, optimizing and deploying it onto the XIAO ESP32S3 Sense.\n\nOur model will be designed to recognize keywords that can trigger device wake-up or specific actions (in the case of \"YES\"), bringing your projects to life with voice-activated commands.\n\nLeveraging our experience with TensorFlow Lite for Microcontrollers (the engine \"under the hood\" on the EI Studio), we'll create a KWS system capable of real-time machine learning on the device.\n\nAs we progress through the lab, we'll break down each process stage – from data collection and preparation to model training and deployment – to provide a comprehensive understanding of implementing a KWS system on a microcontroller.\n\n::: callout-tip\n\n## Learning Objectives {#sec-keyword-spotting-kws-learning-objectives-0e1a}\n\n- **Understand Voice Assistant Architecture** including cascaded detection systems and the role of edge-based keyword spotting as the first stage of voice processing pipelines\n- **Master Audio Data Collection Techniques** using both offline methods (XIAO ESP32S3 microphone with SD card storage) and online methods (smartphone integration with Edge Impulse Studio)\n- **Implement Digital Signal Processing for Audio** including I2S protocol fundamentals, audio sampling at 16kHz/16-bit, and conversion between time-domain audio signals and frequency-domain features using MFCC\n- **Train Convolutional Neural Networks for Audio Classification** using transfer learning techniques, data augmentation strategies, and model optimization for four-class classification (YES, NO, NOISE, UNKNOWN)\n- **Deploy Optimized Models on Microcontrollers** through quantization (INT8), memory management with PSRAM, and real-time inference optimization for embedded systems\n- **Develop Complete Post-Processing Pipelines** including confidence thresholding, GPIO control for external devices, OLED display integration, and creating standalone AI sensor systems\n- **Compare Development Workflows** between no-code platforms (Edge Impulse Studio) and traditional embedded programming (Arduino IDE) for TinyML applications\n:::\n\n## The KWS Project {#sec-keyword-spotting-kws-kws-project-639f}\n\n### How does a voice assistant work? {#sec-keyword-spotting-kws-voice-assistant-work-dbbe}\n\nKeyword Spotting (KWS) is critical to many voice assistants, enabling devices to respond to specific words or phrases. To start, it is essential to realize that Voice Assistants on the market, like Google Home or Amazon Echo-Dot, only react to humans when they are “waked up\" by particular keywords such as “ Hey Google\" on the first one and “Alexa\" on the second.\n\n\\noindent\n![](images/png/1_3n44ykL.png){width=80% fig-align=\"center\"}\n\nIn other words, recognizing voice commands is based on a multi-stage model or Cascade Detection.\n\n\\noindent\n![](images/png/image.png)\n\n**Stage 1**: A smaller microprocessor inside the Echo Dot or Google Home **continuously** listens to the sound, waiting for the keyword to be spotted. For such detection, a TinyML model at the edge is used  (KWS application).\n\n**Stage 2**: Only when triggered by the KWS application on Stage 1 is the data sent to the cloud and processed on a larger model.\n\nThe video below shows an example where I emulate a Google Assistant on a Raspberry Pi (Stage 2), having an Arduino Nano 33 BLE as the tinyML device (Stage 1).\n\n::: {.content-visible when-format=\"html:js\"}\n<iframe class=\"react-editor-embed react-editor-embed-override\" src=\"https://www.youtube.com/embed/e_OPgcnsyvM\" style=\"box-sizing: border-box; align-self: center; flex: 1 1 0%; height: 363.068px; max-height: 100%; max-width: 100%; overflow: hidden; width: 645.455px; z-index: 1; border: none;\"></iframe>\n:::\n\n> If you want to go deeper on the full project, please see my tutorial: [Building an Intelligent Voice Assistant From Scratch](https://www.hackster.io/mjrobot/building-an-intelligent-voice-assistant-from-scratch-2199c3).\n\nIn this lab, we will focus on Stage 1 (KWS or Keyword Spotting), where we will use the XIAO ESP2S3 Sense, which has a digital microphone for spotting the keyword.\n\n### The Inference Pipeline {#sec-keyword-spotting-kws-inference-pipeline-df53}\n\nThe diagram below will give an idea of how the final KWS application should work (during inference):\n\n\\noindent\n![](images/png/image_2.png)\n\nOur KWS application will recognize four classes of sound:\n\n- **YES** (Keyword 1)\n- **NO** (Keyword 2)\n- **NOISE** (no keywords spoken, only background noise is present)\n- **UNKNOWN** (a mix of different words than YES and NO)\n\n> Optionally for real-world projects, it is always advised to include different words than keywords, such as \"Noise\" (or Background) and \"Unknown.\"\n\n### The Machine Learning workflow {#sec-keyword-spotting-kws-machine-learning-workflow-17f6}\n\nThe main component of the KWS application is its model. So, we must train such a model with our specific keywords, noise, and other words (the \"unknown\"):\n\n\\noindent\n![](images/png/image_3.png)\n\n## Dataset {#sec-keyword-spotting-kws-dataset-6ea2}\n\nThe critical component of Machine Learning Workflow is the **dataset**. Once we have decided on specific keywords (*YES* and NO), we can take advantage of the dataset developed by Pete Warden, [\"Speech Commands: A Dataset for Limited-Vocabulary Speech Recognition](https://arxiv.org/pdf/1804.03209.pdf).\" This dataset has 35 keywords (with +1,000 samples each), such as yes, no, stop, and go. In other words, we can get 1,500 samples of *yes* and *no*.\n\nYou can download a small portion of the dataset from Edge Studio ([Keyword spotting pre-built dataset](https://docs.edgeimpulse.com/docs/pre-built-datasets/keyword-spotting)), which includes samples from the four classes we will use in this project: yes, no, noise, and background. For this, follow the steps below:\n\n- Download the [keywords dataset.](https://cdn.edgeimpulse.com/datasets/keywords2.zip)\n- Unzip the file in a location of your choice.\n\nAlthough we have a lot of data from Pete's dataset, collecting some words spoken by us is advised. When working with accelerometers, creating a dataset with data captured by the same type of sensor was essential. In the case of *sound*, the classification differs because it involves, in reality, *audio* data.\n\n> The key difference between sound and audio is their form of energy. Sound is mechanical wave energy (longitudinal sound waves) that propagate through a medium causing variations in pressure within the medium. Audio is made of electrical energy (analog or digital signals) that represent sound electrically.\n\nThe sound waves should be converted to audio data when we speak a keyword. The conversion should be done by sampling the signal generated by the microphone in 16 kHz with a 16-bit depth.\n\nSo, any device that can generate audio data with this basic specification (16 kHz/16 bits) will work fine. As a device, we can use the proper XIAO ESP32S3 Sense, a computer, or even your mobile phone.\n\n\\noindent\n![](images/png/sound-audio.png)\n\n**Capturing online Audio Data with Edge Impulse and a smartphone**\n\nIn the lab Motion Classification and Anomaly Detection, we connect our device directly to Edge Impulse Studio for data capturing (having a sampling frequency of 50 Hz to 100 Hz). For such low frequency, we could use the EI CLI function *Data Forwarder,* but according to Jan Jongboom, Edge Impulse CTO, *audio (*16 kHz) *goes too fast for the data forwarder to be captured.* So, once we have the digital data captured by the microphone, we can turn *it into a WAV file* to be sent to the Studio via Data Uploader (same as we will do with Pete's dataset)*.*\n\n> If we want to collect audio data directly on the Studio, we can use any smartphone connected online with it. We will not explore this option here, but you can easily follow EI [documentation](https://docs.edgeimpulse.com/docs/development-platforms/using-your-mobile-phone).\n\n### Capturing (offline) Audio Data with the XIAO ESP32S3 Sense {#sec-keyword-spotting-kws-capturing-offline-audio-data-xiao-esp32s3-sense-5dbc}\n\nThe built-in microphone is the [MSM261D3526H1CPM](https://files.seeedstudio.com/wiki/XIAO-BLE/mic-MSM261D3526H1CPM-ENG.pdf), a PDM digital output MEMS microphone with Multi-modes. Internally, it is connected to the ESP32S3 via an I2S bus using pins IO41 (Clock) and IO41 (Data).\n\n\\noindent\n![](images/png/pasted_graphic_62.png)\n\n**What is I2S?**\n\nI2S, or Inter-IC Sound, is a standard protocol for transmitting digital audio from one device to another. It was initially developed by Philips Semiconductor (now NXP Semiconductors). It is commonly used in audio devices such as digital signal processors, digital audio processors, and, more recently, microcontrollers with digital audio capabilities (our case here).\n\nThe I2S protocol consists of at least three lines:\n\n\\noindent\n![](images/png/image_4.png){width=60% fig-align=\"center\"}\n\n**1. Bit (or Serial) clock line (BCLK or CLK)**: This line toggles to indicate the start of a new bit of data (pin IO42).\n\n**2. Word select line (WS)**: This line toggles to indicate the start of a new word (left channel or right channel). The Word select clock (WS) frequency defines the sample rate. In our case, L/R on the microphone is set to ground, meaning that we will use only the left channel (mono).\n\n**3. Data line (SD)**: This line carries the audio data (pin IO41)\n\nIn an I2S data stream, the data is sent as a sequence of frames, each containing a left-channel word and a right-channel word. This makes I2S particularly suited for transmitting stereo audio data. However, it can also be used for mono or multichannel audio with additional data lines.\n\nLet's start understanding how to capture raw data using the microphone. Go to the [GitHub project](https://github.com/Mjrovai/XIAO-ESP32S3-Sense) and download the sketch: [XIAOEsp2s3_Mic_Test](https://github.com/Mjrovai/XIAO-ESP32S3-Sense/tree/main/Mic_Test/XiaoEsp32s3_Mic_Test):\n\n> ⚠️ **Attention**\n>\n> - The Xiao ESP32S3 **MUST** have the PSRAM enabled. You can check it on the Arduino IDE upper menu: `Tools`--> `PSRAM:OPI PSRAM`\n> - The Arduino Library (`esp32 by Espressif Systems` should be **version 2.017**. Please do not update it.\n\n```cpp\n/*\n  XIAO ESP32S3 Simple Mic Test\n*/\n\n#include <I2S.h>\n\nvoid setup() {\n  Serial.begin(115200);\n  while (!Serial) {\n  }\n\n  // start I2S at 16 kHz with 16-bits per sample\n  I2S.setAllPins(-1, 42, 41, -1, -1);\n  if (!I2S.begin(PDM_MONO_MODE, 16000, 16)) {\n    Serial.println(\"Failed to initialize I2S!\");\n    while (1); // do nothing\n  }\n}\n\nvoid loop() {\n  // read a sample\n  int sample = I2S.read();\n\n  if (sample && sample != -1 && sample != 1) {\n    Serial.println(sample);\n  }\n}\n```\n\nThis code is a simple microphone test for the XIAO ESP32S3 using the I2S (Inter-IC Sound) interface. It sets up the I2S interface to capture audio data at a sample rate of 16 kHz with 16 bits per sample and then continuously reads samples from the microphone and prints them to the serial monitor.\n\nLet's dig into the code's main parts:\n\n- Include the I2S library: This library provides functions to configure and use the [I2S interface](https://espressif-docs.readthedocs-hosted.com/projects/arduino-esp32/en/latest/api/i2s.html), which is a standard for connecting digital audio devices.\n- I2S.setAllPins(–1, 42, 41, –1, –1): This sets up the I2S pins. The parameters are (–1, 42, 41, –1, –1), where the second parameter (42) is the PIN for the I2S clock (CLK), and the third parameter (41) is the PIN for the I2S data (DATA) line. The other parameters are set to –1, meaning those pins are not used.\n- I2S.begin(PDM_MONO_MODE, 16000, 16): This initializes the I2S interface in Pulse Density Modulation (PDM) mono mode, with a sample rate of 16 kHz and 16 bits per sample. If the initialization fails, an error message is printed, and the program halts.\n- int sample = I2S.read(): This reads an audio sample from the I2S interface.\n\nIf the sample is valid, it is printed on the Serial Monitor and Plotter.\n\nBelow is a test \"whispering\" in two different tones.\n\n\\noindent\n![](images/png/plotter.png)\n\n### Save Recorded Sound Samples {#sec-keyword-spotting-kws-save-recorded-sound-samples-3d1d}\n\nLet's use the onboard SD Card reader to save .wav audio files; we must habilitate the XIAO PSRAM first.\n\n> ESP32-S3 has only a few hundred kilobytes of internal RAM on the MCU chip. It can be insufficient for some purposes so that ESP32-S3 can use up to 16 MB of external PSRAM (Pseudo-static RAM) connected in parallel with the SPI flash chip. The external memory is incorporated in the memory map and, with certain restrictions, is usable in the same way as internal data RAM.\n\nFor a start, Insert the SD Card on the XIAO as shown in the photo below (the SD Card should be formatted to FAT32).\n\n\\noindent\n![](images/png/image_5.png){width=95% fig-align=\"center\"}\n\nTurn the PSRAM function of the ESP-32 chip on (Arduino IDE): Tools>PSRAM: \"OPI PSRAM\">OPI PSRAM\n\n\\noindent\n![](images/png/image_6.png){width=95% fig-align=\"center\"}\n\n- Download the sketch [Wav_Record_dataset](https://github.com/Mjrovai/XIAO-ESP32S3-Sense/tree/main/Wav_Record_dataset), which you can find on the project's GitHub.\n\nThis code records audio using the I2S interface of the Seeed XIAO ESP32S3 Sense board, saves the recording as a.wav file on an SD card, and allows for control of the recording process through commands sent from the serial monitor. The name of the audio file is customizable (it should be the class labels to be used with the training), and multiple recordings can be made, each saved in a new file. The code also includes functionality to increase the volume of the recordings.\n\nLet's break down the most essential parts of it:\n\n```cpp\n#include <I2S.h>\n#include \"FS.h\"\n#include \"SD.h\"\n#include \"SPI.h\"\n```\n\nThose are the necessary libraries for the program. I2S.h allows for audio input, FS.h provides file system handling capabilities, SD.h enables the program to interact with an SD card, and SPI.h handles the SPI communication with the SD card.\n\n```cpp\n#define RECORD_TIME   10\n#define SAMPLE_RATE 16000U\n#define SAMPLE_BITS 16\n#define WAV_HEADER_SIZE 44\n#define VOLUME_GAIN 2\n```\n\nHere, various constants are defined for the program.\n\n- **RECORD_TIME** specifies the length of the audio recording in seconds.\n- **SAMPLE_RATE** and **SAMPLE_BITS** define the audio quality of the recording.\n- **WAV_HEADER_SIZE** specifies the size of the .wav file header.\n- **VOLUME_GAIN** is used to increase the volume of the recording.\n\n```cpp\nint fileNumber = 1;\nString baseFileName;\nbool isRecording = false;\n```\n\nThese variables keep track of the current file number (to create unique file names), the base file name, and whether the system is currently recording.\n\n```cpp\nvoid setup() {\n  Serial.begin(115200);\n  while (!Serial);\n\n  I2S.setAllPins(-1, 42, 41, -1, -1);\n  if (!I2S.begin(PDM_MONO_MODE, SAMPLE_RATE, SAMPLE_BITS)) {\n    Serial.println(\"Failed to initialize I2S!\");\n    while (1);\n  }\n\n  if(!SD.begin(21)){\n    Serial.println(\"Failed to mount SD Card!\");\n    while (1);\n  }\n  Serial.printf(\"Enter with the label name\\n\");\n}\n```\n\nThe setup function initializes the serial communication, I2S interface for audio input, and SD card interface. If the I2S did not initialize or the SD card fails to mount, it will print an error message and halt execution.\n\n```cpp\nvoid loop() {\n  if (Serial.available() > 0) {\n    String command = Serial.readStringUntil('\\n');\n    command.trim();\n    if (command == \"rec\") {\n      isRecording = true;\n    } else {\n      baseFileName = command;\n      fileNumber = 1; //reset file number each time a new\n                        basefile name is set\n      Serial.printf(\"Send rec for starting recording label \\n\");\n    }\n  }\n  if (isRecording && baseFileName != \"\") {\n    String fileName = \"/\" + baseFileName + \".\"\n                      + String(fileNumber) + \".wav\";\n    fileNumber++;\n    record_wav(fileName);\n    delay(1000); // delay to avoid recording multiple files\n                    at once\n    isRecording = false;\n  }\n}\n```\n\nIn the main loop, the program waits for a command from the serial monitor. If the command is rec, the program starts recording. Otherwise, the command is assumed to be the base name for the .wav files. If it's currently recording and a base file name is set, it records the audio and saves it as a.wav file. The file names are generated by appending the file number to the base file name.\n\n```cpp\nvoid record_wav(String fileName)\n{\n  ...\n\n  File file = SD.open(fileName.c_str(), FILE_WRITE);\n  ...\n  rec_buffer = (uint8_t *)ps_malloc(record_size);\n  ...\n\n  esp_i2s::i2s_read(esp_i2s::I2S_NUM_0,\n                    rec_buffer,\n                    record_size,\n                    &sample_size,\n                    portMAX_DELAY);\n  ...\n}\n```\n\nThis function records audio and saves it as a.wav file with the given name. It starts by initializing the sample_size and record_size variables. record_size is calculated based on the sample rate, size, and desired recording time. Let's dig into the essential sections;\n\n```cpp\nFile file = SD.open(fileName.c_str(), FILE_WRITE);\n// Write the header to the WAV file\nuint8_t wav_header[WAV_HEADER_SIZE];\ngenerate_wav_header(wav_header, record_size, SAMPLE_RATE);\nfile.write(wav_header, WAV_HEADER_SIZE);\n```\n\nThis section of the code opens the file on the SD card for writing and then generates the .wav file header using the generate_wav_header function. It then writes the header to the file.\n\n```cpp\n// PSRAM malloc for recording\nrec_buffer = (uint8_t *)ps_malloc(record_size);\nif (rec_buffer == NULL) {\n  Serial.printf(\"malloc failed!\\n\");\n  while(1) ;\n}\nSerial.printf(\"Buffer: %d bytes\\n\", ESP.getPsramSize()\n               - ESP.getFreePsram());\n```\n\nThe ps_malloc function allocates memory in the PSRAM for the recording. If the allocation fails (i.e., rec_buffer is NULL), it prints an error message and halts execution.\n\n```cpp\n// Start recording\nesp_i2s::i2s_read(esp_i2s::I2S_NUM_0,\n         rec_buffer,\n         record_size,\n         &sample_size,\n         portMAX_DELAY);\nif (sample_size == 0) {\n  Serial.printf(\"Record Failed!\\n\");\n} else {\n    Serial.printf(\"Record %d bytes\\n\", sample_size);\n  }\n```\n\nThe i2s_read function reads audio data from the microphone into rec_buffer. It prints an error message if no data is read (sample_size is 0).\n\n```cpp\n// Increase volume\nfor (uint32_t i = 0; i < sample_size; i += SAMPLE_BITS/8) {\n  (*(uint16_t *)(rec_buffer+i)) <<= VOLUME_GAIN;\n}\n```\n\nThis section of the code increases the recording volume by shifting the sample values by VOLUME_GAIN.\n\n```cpp\n// Write data to the WAV file\nSerial.printf(\"Writing to the file ...\\n\");\nif (file.write(rec_buffer, record_size) != record_size)\n  Serial.printf(\"Write file Failed!\\n\");\n\nfree(rec_buffer);\nfile.close();\nSerial.printf(\"Recording complete: \\n\");\nSerial.printf(\"Send rec for a new sample or enter\n                a new label\\n\\n\");\n```\n\nFinally, the audio data is written to the .wav file. If the write operation fails, it prints an error message. After writing, the memory allocated for rec_buffer is freed, and the file is closed. The function finishes by printing a completion message and prompting the user to send a new command.\n\n```cpp\nvoid generate_wav_header(uint8_t *wav_header,\n             uint32_t wav_size,\n             uint32_t sample_rate)\n{\n  ...\n  memcpy(wav_header, set_wav_header, sizeof(set_wav_header));\n}\n```\n\nThe generate_wav_header function creates a.wav file header based on the parameters (wav_size and sample_rate). It generates an array of bytes according to the .wav file format, which includes fields for the file size, audio format, number of channels, sample rate, byte rate, block alignment, bits per sample, and data size. The generated header is then copied into the wav_header array passed to the function.\n\nNow, upload the code to the XIAO and get samples from the keywords (yes and no). You can also capture noise and other words.\n\nThe Serial monitor will prompt you to receive the label to be recorded.\n\n\\noindent\n![](images/png/pasted_graphic.png){width=72% fig-align=\"center\"}\n\nSend the label (for example, yes). The program will wait for another command: rec\n\n\\noindent\n![](images/png/pasted_graphic_2.png){width=72% fig-align=\"center\"}\n\nAnd the program will start recording new samples every time a command rec is sent. The files will be saved as yes.1.wav, yes.2.wav, yes.3.wav, etc., until a new label (for example, no) is sent. In this case, you should send the command rec for each new sample, which will be saved as no.1.wav, no.2.wav, no.3.wav, etc.\n\n\\noindent\n![](images/png/pasted_graphic_4.png){width=72% fig-align=\"center\"}\n\nUltimately, we will get the saved files on the SD card.\n\n\\noindent\n![](images/png/image_7.png){width=90% fig-align=\"center\"}\n\nThe files are ready to be uploaded to Edge Impulse Studio\n\n### Capturing (offline) Audio Data Apps {#sec-keyword-spotting-kws-capturing-offline-audio-data-apps-8900}\n\nThere are many ways to capture audio data; the simplest one is to use a mobile phone or a PC as a **connected device** on the [Edge Impulse Studio](https://docs.edgeimpulse.com/docs/edge-ai-hardware/using-your-mobile-phone).\n\n> The PC or smartphone should capture audio data with a sampling frequency of 16 kHz and a bit depth of 16 Bits.\n\nAnother alternative is to use dedicated apps. A good app for that is [*Voice Recorder Pro*](https://www.bejbej.ca/app/voicerecordpro) [(](https://www.bejbej.ca/app/voicerecordpro)IOS). You should save your records as .wav files and send them to your computer.\n\n\\noindent\n![](images/png/image_8.png){width=90% fig-align=\"center\"}\n\n## Training model with Edge Impulse Studio {#sec-keyword-spotting-kws-training-model-edge-impulse-studio-804e}\n\n### Uploading the Data {#sec-keyword-spotting-kws-uploading-data-36c8}\n\nWhen the raw dataset is defined and collected (Pete's dataset + recorded keywords), we should initiate a new project at Edge Impulse Studio:\n\n\\noindent\n![](images/png/pasted_graphic_44.png){width=80% fig-align=\"center\"}\n\nOnce the project is created, select the Upload Existing Data tool in the Data Acquisition section. Choose the files to be uploaded:\n\n\\noindent\n![](images/png/pasted_graphic_48.png){width=80% fig-align=\"center\"}\n\nAnd upload them to the Studio (You can automatically split data in train/test). Repeat to all classes and all raw data.\n\n\\noindent\n![](images/png/pasted_graphic_46.png){width=80% fig-align=\"center\"}\n\nThe samples will now appear in the Data acquisition section.\n\n\\noindent\n![](images/png/pasted_graphic_49.png){width=70% fig-align=\"center\"}\n\nAll data on Pete's dataset have a 1 s length, but the samples recorded in the previous section have 10 s and must be split into 1s samples to be compatible.\n\nClick on three dots after the sample name and select Split sample.\n\n\\noindent\n![](images/png/image_9.png){width=70% fig-align=\"center\"}\n\nOnce inside the tool, split the data into 1-second records. If necessary, add or remove segments:\n\n\\noindent\n![](images/png/image_10.png){width=70% fig-align=\"center\"}\n\nThis procedure should be repeated for all samples.\n\n> Note: For longer audio files (minutes), first, split into 10-second segments and after that, use the tool again to get the final 1-second splits.\n\nSuppose we do not split data automatically in train/test during upload. In that case, we can do it manually (using the three dots menu, moving samples individually) or using Perform Train / Test Split on Dashboard – Danger Zone.\n\n> We can optionally check all datasets using the tab Data Explorer.\n\n### Creating Impulse (Pre-Process / Model definition) {#sec-keyword-spotting-kws-creating-impulse-preprocess-model-definition-15b3}\n\n*An* **impulse** *takes raw data, uses signal processing to extract features, and then uses a learning block to classify new data.*\n\n\\noindent\n![](images/png/pasted_graphic_51.png){width=70% fig-align=\"center\"}\n\nFirst, we will take the data points with a 1-second window, augmenting the data, sliding that window each 500 ms. Note that the option zero-pad data is set. It is essential to fill with zeros samples smaller than 1 second (in some cases, I reduced the 1000 ms window on the split tool to avoid noises and spikes).\n\nEach 1-second audio sample should be pre-processed and converted to an image (for example, $13\\times 49\\times 1$). We will use MFCC, which extracts features from audio signals using [Mel Frequency Cepstral Coefficients](https://en.wikipedia.org/wiki/Mel-frequency_cepstrum), which are great for the human voice.\n\n\\noindent\n![](images/png/image_11.png){width=70% fig-align=\"center\"}\n\nNext, we select KERAS for classification and build our model from scratch by doing Image Classification using Convolution Neural Network).\n\n### Pre-Processing (MFCC) {#sec-keyword-spotting-kws-preprocessing-mfcc-3788}\n\nThe next step is to create the images to be trained in the next phase:\n\nWe can keep the default parameter values or take advantage of the DSP Autotuneparameters option, which we will do.\n\n\\noindent\n![](images/png/image_12.png){width=70% fig-align=\"center\"}\n\nThe result will not spend much memory to pre-process data (only 16KB). Still, the estimated processing time is high, 675 ms for an Espressif ESP-EYE (the closest reference available), with a 240 kHz clock (same as our device), but with a smaller CPU (XTensa LX6, versus the LX7 on the ESP32S). The real inference time should be smaller.\n\nSuppose we need to reduce the inference time later. In that case, we should return to the pre-processing stage and, for example, reduce the FFT length to 256, change the Number of coefficients, or another parameter.\n\nFor now, let's keep the parameters defined by the Autotuning tool. Save parameters and generate the features.\n\n\\noindent\n![](images/png/pasted_graphic_54.png){width=70% fig-align=\"center\"}\n\n> If you want to go further with converting temporal serial data into images using FFT, Spectrogram, etc., you can play with this CoLab: [Audio Raw Data Analysis.](https://colab.research.google.com/github/Mjrovai/UNIFEI-IESTI01-TinyML-2022.1/blob/main/00_Curse_Folder/2_Applications_Deploy/Class_24/IESTI01_Audio_Raw_Data_Analisys.ipynb)\n\n### Model Design and Training {#sec-keyword-spotting-kws-model-design-training-48a1}\n\nWe will use a Convolution Neural Network (CNN) model. The basic architecture is defined with two blocks of  Conv1D + MaxPooling (with 8 and 16 neurons, respectively) and a 0.25 Dropout. And on the last layer, after Flattening four neurons, one for each class:\n\n\\noindent\n![](images/png/image_13.png)\n\nAs hyper-parameters, we will have a Learning Rate of 0.005 and a model that  will be trained by 100 epochs. We will also include data augmentation, as some noise. The result seems OK:\n\n\\noindent\n![](images/png/image_14.png){width=75% fig-align=\"center\"}\n\nIf you want to understand what is happening \"under the hood, \" you can download the dataset and run a Jupyter Notebook playing with the code. For example, you can analyze the accuracy by each epoch:\n\n\\noindent\n![](images/png/image_15.png)\n\nThis CoLab Notebook can explain how you can go further: [KWS Classifier Project - Looking “Under the hood](https://colab.research.google.com/github/Mjrovai/XIAO-ESP32S3-Sense/blob/main/KWS) Training/xiao_esp32s3_keyword_spotting_project_nn_classifier.ipynb).\"\n\n## Testing {#sec-keyword-spotting-kws-testing-8a1c}\n\nTesting the model with the data put apart before training (Test Data), we got an accuracy of approximately 87%.\n\n\\noindent\n![](images/png/pasted_graphic_58.png){width=90% fig-align=\"center\"}\n\nInspecting the F1 score, we can see that for YES, we got 0.95, an excellent result once we used this keyword to \"trigger\" our postprocessing stage (turn on the built-in LED). Even for NO, we got 0.90. The worst result is for unknown, what is OK.\n\nWe can proceed with the project, but it is possible to perform Live Classification using a smartphone before deployment on our device. Go to the Live Classification section and click on Connect a Development board:\n\n\\noindent\n![](images/png/image_16.png){width=80% fig-align=\"center\"}\n\nPoint your phone to the barcode and select the link.\n\n\\noindent\n![](images/png/image_17.png)\n\nYour phone will be connected to the Studio. Select the option Classification on the app, and when it is running, start testing your keywords, confirming that the model is working with live and real data:\n\n\\noindent\n![](images/png/image_18.png)\n\n## Deploy and Inference {#sec-keyword-spotting-kws-deploy-inference-6b44}\n\nThe Studio will package all the needed libraries, preprocessing functions, and trained models, downloading them to your computer. Select the Arduino Library option, then choose Quantized (Int8) from the bottom menu and press Build.\n\n\\noindent\n![](images/png/pasted_graphic_59.png){width=90% fig-align=\"center\"}\n\nNow it is time for a real test. We will make inferences wholly disconnected from the Studio. Let's change one of the ESP32 code examples created when you deploy the Arduino Library.\n\nIn your Arduino IDE, go to the File/Examples tab look for your project, and select esp32/esp32_microphone:\n\n\\noindent\n![](images/png/image_19.png){width=90% fig-align=\"center\"}\n\nThis code was created for the ESP-EYE built-in microphone, which should be adapted for our device.\n\nStart changing the libraries to handle the I2S bus:\n\n\\noindent\n![](images/png/image_20.png){width=60% fig-align=\"center\"}\n\nBy:\n\n```cpp\n#include <I2S.h>\n#define SAMPLE_RATE 16000U\n#define SAMPLE_BITS 16\n```\n\nInitialize the IS2 microphone at setup(), including the lines:\n\n```cpp\nvoid setup()\n{\n...\n    I2S.setAllPins(-1, 42, 41, -1, -1);\n    if (!I2S.begin(PDM_MONO_MODE, SAMPLE_RATE, SAMPLE_BITS)) {\n      Serial.println(\"Failed to initialize I2S!\");\n    while (1) ;\n...\n}\n```\n\nOn the static void capture_samples(void* arg) function, replace the line 153 that reads data from I2S mic:\n\n\\noindent\n![](images/png/image_21.png)\n\nBy:\n\n```cpp\n/* read data at once from i2s */\nesp_i2s::i2s_read(esp_i2s::I2S_NUM_0,\n                 (void*)sampleBuffer,\n                 i2s_bytes_to_read,\n                 &bytes_read, 100);\n```\n\nOn function static bool microphone_inference_start(uint32_t n_samples), we should comment or delete lines 198 to 200, where the microphone initialization function is called. This is unnecessary because the I2S microphone was already initialized during the setup().\n\n\\noindent\n![](images/png/image_22.png)\n\nFinally, on static void microphone_inference_end(void) function, replace line 243:\n\n\\noindent\n![](images/png/image_23.png){width=60% fig-align=\"center\"}\n\nBy:\n\n```cpp\nstatic void microphone_inference_end(void)\n{\n    free(sampleBuffer);\n    ei_free(inference.buffer);\n}\n```\n\nYou can find the complete code on the [project's GitHub](https://github.com/Mjrovai/XIAO-ESP32S3-Sense/tree/main/xiao_esp32s3_microphone). Upload the sketch to your board and test some real inferences:\n\n> ⚠️ **Attention**\n>\n> - The Xiao ESP32S3 **MUST** have the PSRAM enabled. You can check it on the Arduino IDE upper menu: `Tools`--> `PSRAM:OPI PSRAM`\n> - The Arduino Library (`esp32 by Espressif Systems` should be **version 2.017**. Please do not update it.\n\n\\noindent\n![](images/png/image_24.png){width=80% fig-align=\"center\"}\n\n## Postprocessing {#sec-keyword-spotting-kws-postprocessing-d5e4}\n\nIn edge AI applications, the inference result is only as valuable as our ability to act upon it. While serial output provides detailed information for debugging and development, real-world deployments require immediate, human-readable feedback that doesn't depend on external monitors or connections.\n\nLet's explore two post-processing approaches. Using the internal XIAO's LED and the OLED on the XIAOML Kit.\n\n### With LED {#sec-keyword-spotting-kws-led-6deb}\n\nNow that we know the model is working by detecting our keywords, let's modify the code to see the internal LED go on every time a YES is detected.\n\nYou should initialize the LED:\n\n```cpp\n#define LED_BUILT_IN 21\n...\nvoid setup()\n{\n...\n  pinMode(LED_BUILT_IN, OUTPUT); // Set the pin as output\n  digitalWrite(LED_BUILT_IN, HIGH); //Turn off\n...\n}\n```\n\nAnd change the // print the predictions portion of the previous code (on loop():\n\n```cpp\nint pred_index = 0;     // Initialize pred_index\nfloat pred_value = 0;   // Initialize pred_value\n\n// print the predictions\nei_printf(\"Predictions \");\nei_printf(\"(DSP: %d ms., Classification: %d ms., Anomaly: %d ms.)\",\n     result.timing.dsp, result.timing.classification,\n     result.timing.anomaly);\nei_printf(\": \\n\");\nfor (size_t ix = 0; ix < EI_CLASSIFIER_LABEL_COUNT; ix++) {\n      ei_printf(\"    %s: \", result.classification[ix].label);\n      ei_printf_float(result.classification[ix].value);\n      ei_printf(\"\\n\");\n\n      if (result.classification[ix].value > pred_value){\n         pred_index = ix;\n         pred_value = result.classification[ix].value;\n      }\n}\n\n// show the inference result on LED\nif (pred_index == 3){\n    digitalWrite(LED_BUILT_IN, LOW); //Turn on\n}\nelse{\n   digitalWrite(LED_BUILT_IN, HIGH); //Turn off\n}\n```\n\nYou can find the complete code on the [project's GitHub.](https://github.com/Mjrovai/XIAO-ESP32S3-Sense/tree/main/xiao_esp32s3_microphone_led) Upload the sketch to your board and test some real inferences:\n\n\\noindent\n![](images/png/image_25.png){width=80% fig-align=\"center\"}\n\nThe idea is that the LED will be ON whenever the keyword YES is detected. In the same way, instead of turning on an LED, this could be a \"trigger\" for an external device, as we saw in the introduction.\n\n::: {.content-visible when-format=\"html:js\"}\n<iframe class=\"react-editor-embed react-editor-embed-override\" src=\"https://www.youtube.com/embed/wjhtEzXt60Q\" style=\"box-sizing: border-box; align-self: center; flex: 1 1 0%; height: 363.068px; max-height: 100%; max-width: 100%; overflow: hidden; width: 645.455px; z-index: 1; border: none;\"></iframe>\n:::\n\n### With OLED Display {#sec-keyword-spotting-kws-oled-display-9676}\n\nThe XIAOML Kit tiny 0.42\" OLED display (72×40 pixels) serves as a crucial post-processing component that transforms raw ML inference results into immediate, human-readable feedback—displaying detected class names and confidence levels directly on the device, eliminating the need for external monitors and enabling truly standalone edge AI deployment in industrial, agricultural, or retail environments where instant visual confirmation of AI predictions is essential.\n\nSo, let's modify the sketch to automatically adapt to the model trained on Edge Impulse by reading the class names and count directly from the model. Download the code from GitHub: [xiaoml-kit_kws_oled](https://github.com/Mjrovai/XIAO-ESP32S3-Sense/tree/main/XIAOML_Kit_code/xiaoml-kit_kws_oled).\n\nRunning the code, we can see the result:\n\n\\noindent\n![](./images/png/kit-infer.png){width=90% fig-align=\"center\"}\n\n## Summary {#sec-keyword-spotting-kws-summary-b181}\n\nThis lab demonstrated the complete development cycle of a keyword spotting system using the XIAOML Kit, showcasing how modern TinyML platforms make sophisticated audio AI accessible on resource-constrained devices. Through hands-on implementation, we've bridged the gap between theoretical machine learning concepts and practical embedded AI deployment.\n\n**Technical Achievements:**\n\nThe project successfully implemented a complete audio processing pipeline from raw sound capture through real-time inference. Using the XIAO ESP32S3's integrated digital microphone, we captured audio data at professional quality (16kHz/16-bit) and processed it using Mel Frequency Cepstral Coefficients (MFCC) for feature extraction. The deployed CNN model achieved excellent accuracy in distinguishing between our target keywords (\"YES\", \"NO\") and background conditions (\"NOISE\", \"UNKNOWN\"), with inference times suitable for real-time applications.\n\n**Platform Integration:**\n\nEdge Impulse Studio proved invaluable as a comprehensive MLOps platform for embedded systems, handling everything from data collection and labeling through model training, optimization, and deployment. The seamless integration between cloud-based training and edge deployment exemplifies modern TinyML workflows, while the Arduino IDE provided the flexibility needed for custom post-processing implementations.\n\n**Real-World Applications:**\n\nThe techniques learned extend far beyond simple keyword detection. Voice-activated control systems, industrial safety monitoring through sound classification, medical applications for respiratory analysis, and environmental monitoring for wildlife or equipment sounds all leverage similar audio processing approaches. The cascaded detection architecture demonstrated here—using edge-based KWS to trigger more complex cloud processing—is fundamental to modern voice assistant systems.\n\n**Embedded AI Principles:**\n\nThis project highlighted crucial TinyML considerations, including power management, memory optimization through PSRAM utilization, and the trade-offs between model complexity and inference speed. The successful deployment of a neural network performing real-time audio analysis on a microcontroller demonstrates how AI capabilities, once requiring powerful desktop computers, can now operate on battery-powered devices.\n\n**Development Methodology:**\n\nWe explored multiple development pathways, from data collection strategies (offline SD card storage versus online streaming) to deployment options (Edge Impulse's automated library generation versus custom Arduino implementation). This flexibility is crucial for adapting to various project requirements and constraints.\n\n**Future Directions:**\n\nThe foundation established here enables the exploration of more advanced audio AI applications. Multi-keyword recognition, speaker identification, emotion detection from voice, and environmental sound classification all build upon the same core techniques. The integration capabilities demonstrated with OLED displays and GPIO control illustrate how KWS can serve as the intelligent interface for broader IoT systems.\n\nConsider that Sound Classification encompasses much more than just voice recognition. This project's techniques apply across numerous domains:\n\n- **Security Applications**: Broken glass detection, intrusion monitoring, gunshot detection\n- **Industrial IoT**: Machinery health monitoring, anomaly detection in manufacturing equipment\n- **Healthcare**: Sleep disorder monitoring, respiratory condition assessment, elderly care systems\n- **Environmental Monitoring**: Wildlife tracking, urban noise analysis, smart building acoustic management\n- **Smart Home Integration**: Multi-room voice control, appliance status monitoring through sound signatures\n\n**Key Takeaways:**\n\nThe XIAOML Kit proves that professional-grade AI development is achievable with accessible tools and modest budgets. The combination of capable hardware (ESP32S3 with PSRAM and integrated sensors), mature development platforms (Edge Impulse Studio), and comprehensive software libraries creates an environment where complex AI concepts become tangible, working systems.\n\nThis lab demonstrates that the future of AI isn't just in massive data centers, but in intelligent edge devices that can process, understand, and respond to their environment in real-time—opening possibilities for ubiquitous, privacy-preserving, and responsive artificial intelligence systems.\n\n## Resources {#sec-keyword-spotting-kws-resources-c867}\n\n- [XIAO ESP32S3 Codes](https://github.com/Mjrovai/XIAO-ESP32S3-Sense)\n\n- [XIAOML Kit Code](https://github.com/Mjrovai/XIAO-ESP32S3-Sense/tree/main/XIAOML_Kit_code)\n\n- [Subset of Google Speech Commands Dataset](https://cdn.edgeimpulse.com/datasets/keywords2.zip)\n\n- [KWS MFCC Analysis Colab Notebook](https://colab.research.google.com/github/Mjrovai/Arduino_Nicla_Vision/blob/main/KWS/KWS_MFCC_Analysis.ipynb)\n\n- [KWS CNN training Colab Notebook](https://colab.research.google.com/github/Mjrovai/Arduino_Nicla_Vision/blob/main/KWS/KWS_CNN_training.ipynb)\n\n- [XIAO ESP32S3 Post-processing Code](https://github.com/Mjrovai/XIAO-ESP32S3-Sense/tree/main/xiao_esp32s3_microphone_led)\n\n- [Edge Impulse Project](https://studio.edgeimpulse.com/public/230109/live)\n","srcMarkdownNoYaml":""},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"markdown"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"wrap","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":true,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","toc":true,"toc-depth":3,"number-sections":false,"highlight-style":"github","include-after-body":[{"text":"<script src=\"assets/scripts/subscribe-modal.js\" defer></script>\n"}],"output-file":"kws.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.8.25","theme":{"light":["default","../../../../assets/styles/style.scss"],"dark":["default","../../../../assets/styles/style.scss","../../../../assets/styles/dark-mode.scss"]},"respect-user-color-scheme":true,"code-copy":true,"anchor-sections":true},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}