{"title":"Hardware Kits","markdown":{"yaml":{"title":"Hardware Kits","subtitle":"Hands-On Embedded ML Labs for Real-World Deployment"},"headingText":"Laboratory Development","containsRefs":false,"markdown":"\n\n::: {.content-visible when-format=\"html\"}\n```{=html}\n<!-- Hardware Carousel -->\n<div id=\"hardwareCarousel\" class=\"carousel slide mb-4\" data-bs-ride=\"carousel\">\n  <div class=\"carousel-inner\">\n    <div class=\"carousel-item active\">\n      <img src=\"contents/seeed/xiao_esp32s3/images/jpeg/xiao_esp32s3_decked.jpeg\" class=\"d-block mx-auto\" alt=\"XIAO ESP32S3\">\n    </div>\n    <div class=\"carousel-item\">\n      <img src=\"contents/arduino/nicla_vision/images/jpg/nicla_vision.jpeg\" class=\"d-block mx-auto\" alt=\"Arduino Nicla Vision\">\n    </div>\n    <div class=\"carousel-item\">\n      <img src=\"contents/seeed/grove_vision_ai_v2/images/jpeg/grove_vision_ai_v2.jpeg\" class=\"d-block mx-auto\" alt=\"Grove Vision AI V2\">\n    </div>\n    <div class=\"carousel-item\">\n      <img src=\"contents/raspi/images/jpeg/raspis.jpg\" class=\"d-block mx-auto\" alt=\"Raspberry Pi\">\n    </div>\n  </div>\n  <button class=\"carousel-control-prev\" type=\"button\" data-bs-target=\"#hardwareCarousel\" data-bs-slide=\"prev\">\n    <span class=\"carousel-control-prev-icon\"></span>\n  </button>\n  <button class=\"carousel-control-next\" type=\"button\" data-bs-target=\"#hardwareCarousel\" data-bs-slide=\"next\">\n    <span class=\"carousel-control-next-icon\"></span>\n  </button>\n  <div class=\"carousel-indicators\">\n    <button type=\"button\" data-bs-target=\"#hardwareCarousel\" data-bs-slide-to=\"0\" class=\"active\"></button>\n    <button type=\"button\" data-bs-target=\"#hardwareCarousel\" data-bs-slide-to=\"1\"></button>\n    <button type=\"button\" data-bs-target=\"#hardwareCarousel\" data-bs-slide-to=\"2\"></button>\n    <button type=\"button\" data-bs-target=\"#hardwareCarousel\" data-bs-slide-to=\"3\"></button>\n  </div>\n</div>\n<div class=\"carousel-caption-bar\">\n  <p><strong>Embedded ML Hardware Platforms</strong></p>\n  <p>From $20 microcontrollers to powerful edge devices</p>\n</div>\n```\n:::\n\n::: {.content-visible when-format=\"pdf\"}\n![Hardware platforms for embedded ML labs](contents/seeed/grove_vision_ai_v2/images/jpeg/grove_vision_ai_v2.jpeg){width=80%}\n:::\n\nThese hands-on laboratories accompany the [Machine Learning Systems](https://mlsysbook.ai) textbook, bringing theory to life on real hardware. Deploy machine learning on embedded devices you can hold in your hand, from image classification to voice recognition to motion detection. Professional development boards costing $25-100 provide immediate, tangible feedback: LEDs light up, motors spin, and buzzers sound when your model runs successfully.\n\nWorking within the resource constraints of embedded devices (typically 2MB of RAM and 1MB of flash) forces you to confront the same engineering trade-offs that define large-scale ML systems, but in a tangible environment where every optimization decision has immediate, observable consequences.\n\n::: {.callout-note}\n\nThese hands-on laboratories were co-designed by [Prof. Vijay Janapa Reddi](https://vijay.seas.harvard.edu) and [Marcelo Rovai](https://github.com/Mjrovai), with Marcelo leading their development. His decades of embedded systems expertise shaped accessible, practical learning experiences that bridge theory with real-world implementation.\n:::\n\n## Hardware Platforms\n\n::: {.content-visible when-format=\"html\"}\n```{=html}\n<div class=\"row g-4 mb-4\">\n  <div class=\"col-md-6 col-lg-3\">\n    <a href=\"contents/seeed/grove_vision_ai_v2/grove_vision_ai_v2.html\" class=\"text-decoration-none\">\n      <div class=\"platform-card\">\n        <img src=\"contents/seeed/grove_vision_ai_v2/images/jpeg/grove_vision_ai_v2.jpeg\" alt=\"Grove Vision AI V2\">\n        <h4>Grove Vision AI V2</h4>\n        <p class=\"price\">~$25</p>\n        <p class=\"text-muted small\">Best for beginners. Plug & play vision AI.</p>\n      </div>\n    </a>\n  </div>\n  <div class=\"col-md-6 col-lg-3\">\n    <a href=\"contents/seeed/xiao_esp32s3/xiao_esp32s3.html\" class=\"text-decoration-none\">\n      <div class=\"platform-card\">\n        <img src=\"contents/seeed/xiao_esp32s3/images/jpeg/xiao_esp32s3_decked.jpeg\" alt=\"XIAO ESP32S3\">\n        <h4>XIAO ESP32S3</h4>\n        <p class=\"price\">~$40</p>\n        <p class=\"text-muted small\">Best value. Vision, audio, motion.</p>\n      </div>\n    </a>\n  </div>\n  <div class=\"col-md-6 col-lg-3\">\n    <a href=\"contents/raspi/raspi.html\" class=\"text-decoration-none\">\n      <div class=\"platform-card\">\n        <img src=\"contents/raspi/images/jpeg/raspis.jpg\" alt=\"Raspberry Pi\">\n        <h4>Raspberry Pi</h4>\n        <p class=\"price\">~$60-80</p>\n        <p class=\"text-muted small\">Advanced. LLMs, VLMs, edge AI.</p>\n      </div>\n    </a>\n  </div>\n  <div class=\"col-md-6 col-lg-3\">\n    <a href=\"contents/arduino/nicla_vision/nicla_vision.html\" class=\"text-decoration-none\">\n      <div class=\"platform-card\">\n        <img src=\"contents/arduino/nicla_vision/images/jpg/nicla_vision.jpeg\" alt=\"Arduino Nicla Vision\">\n        <h4>Nicla Vision</h4>\n        <p class=\"price\">~$95</p>\n        <p class=\"text-muted small\">Professional. Dual sensors, compact.</p>\n      </div>\n    </a>\n  </div>\n</div>\n```\n:::\n\n::: {.content-visible when-format=\"pdf\"}\n| Platform | Price | Best For | Capabilities |\n|----------|-------|----------|--------------|\n| Grove Vision AI V2 | ~$25 | Beginners | Vision, Plug & Play |\n| XIAO ESP32S3 | ~$40 | Best Value | Vision, Audio, Motion |\n| Raspberry Pi | ~$60-80 | Advanced | Vision, LLM, VLM |\n| Arduino Nicla Vision | ~$95 | Professional | Vision, Audio, Motion |\n\n: Hardware platform comparison {.striped .hover}\n:::\n\n## What You Will Build\n\n::: {.content-visible when-format=\"html\"}\n```{=html}\n<div class=\"capability-grid\">\n  <div class=\"capability-item\">\n    <h4>üëÅÔ∏è Computer Vision</h4>\n    <p>Image classification and object detection on microcontrollers. Train models to recognize objects, detect faces, or classify scenes.</p>\n  </div>\n  <div class=\"capability-item\">\n    <h4>üé§ Audio Processing</h4>\n    <p>Keyword spotting and voice command recognition. Build wake-word detectors and voice interfaces that run entirely on-device.</p>\n  </div>\n  <div class=\"capability-item\">\n    <h4>üèÉ Motion Classification</h4>\n    <p>Activity and gesture recognition from IMU data. Create wearable-style applications using accelerometer and gyroscope sensors.</p>\n  </div>\n  <div class=\"capability-item\">\n    <h4>ü§ñ Large Language Models</h4>\n    <p>Run LLMs and VLMs on edge devices. Experience the frontier of on-device AI with models that understand and generate text.</p>\n  </div>\n</div>\n```\n:::\n\n::: {.content-visible when-format=\"pdf\"}\n**Computer Vision:** Image classification and object detection on microcontrollers. Train models to recognize objects, detect faces, or classify scenes, then deploy them to devices running on battery power.\n\n**Audio Processing:** Keyword spotting and voice command recognition. Build wake-word detectors and simple voice interfaces that run entirely on-device without cloud connectivity.\n\n**Motion Classification:** Activity and gesture recognition from IMU data. Create wearable-style applications that detect walking, running, or custom gestures using accelerometer and gyroscope sensors.\n\n**Large Language Models:** Run LLMs and VLMs on edge devices using Raspberry Pi. Experience the frontier of on-device AI with models that can understand and generate text.\n:::\n\n## Getting Started\n\n1. **Choose Hardware:** Select a platform based on your budget and learning goals. See [Platforms](contents/platforms.qmd) for detailed comparisons.\n\n2. **Set Up Environment:** Install Arduino IDE or platform-specific tools. Follow the [IDE Setup Guide](contents/ide-setup.qmd) for step-by-step instructions.\n\n3. **Build & Deploy:** Work through the labs for your chosen platform. Start with [Getting Started](contents/getting-started.qmd) for an overview of available exercises.\n\n## Part of the MLSysBook Ecosystem\n\n::: {.content-visible when-format=\"html\"}\n```{=html}\n<div class=\"row g-3 mt-3\">\n  <div class=\"col-md-4\">\n    <div class=\"p-3 border rounded h-100 ecosystem-link\">\n      <h5><a href=\"https://mlsysbook.ai\">Textbook</a></h5>\n      <p class=\"small text-muted mb-0\">Comprehensive theory and concepts covering the full ML systems stack.</p>\n    </div>\n  </div>\n  <div class=\"col-md-4\">\n    <div class=\"p-3 border rounded h-100 bg-light ecosystem-current\">\n      <h5>Hardware Kits</h5>\n      <p class=\"small text-muted mb-0\">Hands-on embedded deployment. <strong>You are here.</strong></p>\n    </div>\n  </div>\n  <div class=\"col-md-4\">\n    <div class=\"p-3 border rounded h-100 ecosystem-link\">\n      <h5><a href=\"https://mlsysbook.ai/tinytorch\">TinyTorch</a></h5>\n      <p class=\"small text-muted mb-0\">Build your own ML framework from scratch.</p>\n    </div>\n  </div>\n</div>\n```\n:::\n\n::: {.content-visible when-format=\"pdf\"}\nThese hardware labs complement the broader ML Systems learning experience:\n\n- **Textbook:** Comprehensive theory and concepts covering the full ML systems stack\n- **Hardware Kits:** Hands-on embedded deployment (you are here)\n- **TinyTorch:** Build your own ML framework from scratch\n:::\n","srcMarkdownNoYaml":"\n\n::: {.content-visible when-format=\"html\"}\n```{=html}\n<!-- Hardware Carousel -->\n<div id=\"hardwareCarousel\" class=\"carousel slide mb-4\" data-bs-ride=\"carousel\">\n  <div class=\"carousel-inner\">\n    <div class=\"carousel-item active\">\n      <img src=\"contents/seeed/xiao_esp32s3/images/jpeg/xiao_esp32s3_decked.jpeg\" class=\"d-block mx-auto\" alt=\"XIAO ESP32S3\">\n    </div>\n    <div class=\"carousel-item\">\n      <img src=\"contents/arduino/nicla_vision/images/jpg/nicla_vision.jpeg\" class=\"d-block mx-auto\" alt=\"Arduino Nicla Vision\">\n    </div>\n    <div class=\"carousel-item\">\n      <img src=\"contents/seeed/grove_vision_ai_v2/images/jpeg/grove_vision_ai_v2.jpeg\" class=\"d-block mx-auto\" alt=\"Grove Vision AI V2\">\n    </div>\n    <div class=\"carousel-item\">\n      <img src=\"contents/raspi/images/jpeg/raspis.jpg\" class=\"d-block mx-auto\" alt=\"Raspberry Pi\">\n    </div>\n  </div>\n  <button class=\"carousel-control-prev\" type=\"button\" data-bs-target=\"#hardwareCarousel\" data-bs-slide=\"prev\">\n    <span class=\"carousel-control-prev-icon\"></span>\n  </button>\n  <button class=\"carousel-control-next\" type=\"button\" data-bs-target=\"#hardwareCarousel\" data-bs-slide=\"next\">\n    <span class=\"carousel-control-next-icon\"></span>\n  </button>\n  <div class=\"carousel-indicators\">\n    <button type=\"button\" data-bs-target=\"#hardwareCarousel\" data-bs-slide-to=\"0\" class=\"active\"></button>\n    <button type=\"button\" data-bs-target=\"#hardwareCarousel\" data-bs-slide-to=\"1\"></button>\n    <button type=\"button\" data-bs-target=\"#hardwareCarousel\" data-bs-slide-to=\"2\"></button>\n    <button type=\"button\" data-bs-target=\"#hardwareCarousel\" data-bs-slide-to=\"3\"></button>\n  </div>\n</div>\n<div class=\"carousel-caption-bar\">\n  <p><strong>Embedded ML Hardware Platforms</strong></p>\n  <p>From $20 microcontrollers to powerful edge devices</p>\n</div>\n```\n:::\n\n::: {.content-visible when-format=\"pdf\"}\n![Hardware platforms for embedded ML labs](contents/seeed/grove_vision_ai_v2/images/jpeg/grove_vision_ai_v2.jpeg){width=80%}\n:::\n\nThese hands-on laboratories accompany the [Machine Learning Systems](https://mlsysbook.ai) textbook, bringing theory to life on real hardware. Deploy machine learning on embedded devices you can hold in your hand, from image classification to voice recognition to motion detection. Professional development boards costing $25-100 provide immediate, tangible feedback: LEDs light up, motors spin, and buzzers sound when your model runs successfully.\n\nWorking within the resource constraints of embedded devices (typically 2MB of RAM and 1MB of flash) forces you to confront the same engineering trade-offs that define large-scale ML systems, but in a tangible environment where every optimization decision has immediate, observable consequences.\n\n::: {.callout-note}\n## Laboratory Development\n\nThese hands-on laboratories were co-designed by [Prof. Vijay Janapa Reddi](https://vijay.seas.harvard.edu) and [Marcelo Rovai](https://github.com/Mjrovai), with Marcelo leading their development. His decades of embedded systems expertise shaped accessible, practical learning experiences that bridge theory with real-world implementation.\n:::\n\n## Hardware Platforms\n\n::: {.content-visible when-format=\"html\"}\n```{=html}\n<div class=\"row g-4 mb-4\">\n  <div class=\"col-md-6 col-lg-3\">\n    <a href=\"contents/seeed/grove_vision_ai_v2/grove_vision_ai_v2.html\" class=\"text-decoration-none\">\n      <div class=\"platform-card\">\n        <img src=\"contents/seeed/grove_vision_ai_v2/images/jpeg/grove_vision_ai_v2.jpeg\" alt=\"Grove Vision AI V2\">\n        <h4>Grove Vision AI V2</h4>\n        <p class=\"price\">~$25</p>\n        <p class=\"text-muted small\">Best for beginners. Plug & play vision AI.</p>\n      </div>\n    </a>\n  </div>\n  <div class=\"col-md-6 col-lg-3\">\n    <a href=\"contents/seeed/xiao_esp32s3/xiao_esp32s3.html\" class=\"text-decoration-none\">\n      <div class=\"platform-card\">\n        <img src=\"contents/seeed/xiao_esp32s3/images/jpeg/xiao_esp32s3_decked.jpeg\" alt=\"XIAO ESP32S3\">\n        <h4>XIAO ESP32S3</h4>\n        <p class=\"price\">~$40</p>\n        <p class=\"text-muted small\">Best value. Vision, audio, motion.</p>\n      </div>\n    </a>\n  </div>\n  <div class=\"col-md-6 col-lg-3\">\n    <a href=\"contents/raspi/raspi.html\" class=\"text-decoration-none\">\n      <div class=\"platform-card\">\n        <img src=\"contents/raspi/images/jpeg/raspis.jpg\" alt=\"Raspberry Pi\">\n        <h4>Raspberry Pi</h4>\n        <p class=\"price\">~$60-80</p>\n        <p class=\"text-muted small\">Advanced. LLMs, VLMs, edge AI.</p>\n      </div>\n    </a>\n  </div>\n  <div class=\"col-md-6 col-lg-3\">\n    <a href=\"contents/arduino/nicla_vision/nicla_vision.html\" class=\"text-decoration-none\">\n      <div class=\"platform-card\">\n        <img src=\"contents/arduino/nicla_vision/images/jpg/nicla_vision.jpeg\" alt=\"Arduino Nicla Vision\">\n        <h4>Nicla Vision</h4>\n        <p class=\"price\">~$95</p>\n        <p class=\"text-muted small\">Professional. Dual sensors, compact.</p>\n      </div>\n    </a>\n  </div>\n</div>\n```\n:::\n\n::: {.content-visible when-format=\"pdf\"}\n| Platform | Price | Best For | Capabilities |\n|----------|-------|----------|--------------|\n| Grove Vision AI V2 | ~$25 | Beginners | Vision, Plug & Play |\n| XIAO ESP32S3 | ~$40 | Best Value | Vision, Audio, Motion |\n| Raspberry Pi | ~$60-80 | Advanced | Vision, LLM, VLM |\n| Arduino Nicla Vision | ~$95 | Professional | Vision, Audio, Motion |\n\n: Hardware platform comparison {.striped .hover}\n:::\n\n## What You Will Build\n\n::: {.content-visible when-format=\"html\"}\n```{=html}\n<div class=\"capability-grid\">\n  <div class=\"capability-item\">\n    <h4>üëÅÔ∏è Computer Vision</h4>\n    <p>Image classification and object detection on microcontrollers. Train models to recognize objects, detect faces, or classify scenes.</p>\n  </div>\n  <div class=\"capability-item\">\n    <h4>üé§ Audio Processing</h4>\n    <p>Keyword spotting and voice command recognition. Build wake-word detectors and voice interfaces that run entirely on-device.</p>\n  </div>\n  <div class=\"capability-item\">\n    <h4>üèÉ Motion Classification</h4>\n    <p>Activity and gesture recognition from IMU data. Create wearable-style applications using accelerometer and gyroscope sensors.</p>\n  </div>\n  <div class=\"capability-item\">\n    <h4>ü§ñ Large Language Models</h4>\n    <p>Run LLMs and VLMs on edge devices. Experience the frontier of on-device AI with models that understand and generate text.</p>\n  </div>\n</div>\n```\n:::\n\n::: {.content-visible when-format=\"pdf\"}\n**Computer Vision:** Image classification and object detection on microcontrollers. Train models to recognize objects, detect faces, or classify scenes, then deploy them to devices running on battery power.\n\n**Audio Processing:** Keyword spotting and voice command recognition. Build wake-word detectors and simple voice interfaces that run entirely on-device without cloud connectivity.\n\n**Motion Classification:** Activity and gesture recognition from IMU data. Create wearable-style applications that detect walking, running, or custom gestures using accelerometer and gyroscope sensors.\n\n**Large Language Models:** Run LLMs and VLMs on edge devices using Raspberry Pi. Experience the frontier of on-device AI with models that can understand and generate text.\n:::\n\n## Getting Started\n\n1. **Choose Hardware:** Select a platform based on your budget and learning goals. See [Platforms](contents/platforms.qmd) for detailed comparisons.\n\n2. **Set Up Environment:** Install Arduino IDE or platform-specific tools. Follow the [IDE Setup Guide](contents/ide-setup.qmd) for step-by-step instructions.\n\n3. **Build & Deploy:** Work through the labs for your chosen platform. Start with [Getting Started](contents/getting-started.qmd) for an overview of available exercises.\n\n## Part of the MLSysBook Ecosystem\n\n::: {.content-visible when-format=\"html\"}\n```{=html}\n<div class=\"row g-3 mt-3\">\n  <div class=\"col-md-4\">\n    <div class=\"p-3 border rounded h-100 ecosystem-link\">\n      <h5><a href=\"https://mlsysbook.ai\">Textbook</a></h5>\n      <p class=\"small text-muted mb-0\">Comprehensive theory and concepts covering the full ML systems stack.</p>\n    </div>\n  </div>\n  <div class=\"col-md-4\">\n    <div class=\"p-3 border rounded h-100 bg-light ecosystem-current\">\n      <h5>Hardware Kits</h5>\n      <p class=\"small text-muted mb-0\">Hands-on embedded deployment. <strong>You are here.</strong></p>\n    </div>\n  </div>\n  <div class=\"col-md-4\">\n    <div class=\"p-3 border rounded h-100 ecosystem-link\">\n      <h5><a href=\"https://mlsysbook.ai/tinytorch\">TinyTorch</a></h5>\n      <p class=\"small text-muted mb-0\">Build your own ML framework from scratch.</p>\n    </div>\n  </div>\n</div>\n```\n:::\n\n::: {.content-visible when-format=\"pdf\"}\nThese hardware labs complement the broader ML Systems learning experience:\n\n- **Textbook:** Comprehensive theory and concepts covering the full ML systems stack\n- **Hardware Kits:** Hands-on embedded deployment (you are here)\n- **TinyTorch:** Build your own ML framework from scratch\n:::\n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"markdown"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"wrap","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":true,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","toc":true,"toc-depth":3,"number-sections":false,"highlight-style":"github","include-after-body":[{"text":"<script src=\"assets/scripts/subscribe-modal.js\" defer></script>\n"}],"output-file":"index.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.8.25","theme":{"light":["default","assets/styles/style.scss"],"dark":["default","assets/styles/style.scss","assets/styles/dark-mode.scss"]},"respect-user-color-scheme":true,"code-copy":true,"anchor-sections":true,"title":"Hardware Kits","subtitle":"Hands-On Embedded ML Labs for Real-World Deployment"},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}