[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Hardware Kits",
    "section": "",
    "text": "These hands-on laboratories accompany the Machine Learning Systems textbook, bringing theory to life on real hardware. Deploy machine learning on embedded devices you can hold in your hand, from image classification to voice recognition to motion detection. Professional development boards costing $25-100 provide immediate, tangible feedback: LEDs light up, motors spin, and buzzers sound when your model runs successfully.\nWorking within the resource constraints of embedded devices (typically 2MB of RAM and 1MB of flash) forces you to confront the same engineering trade-offs that define large-scale ML systems, but in a tangible environment where every optimization decision has immediate, observable consequences.",
    "crumbs": [
      "Overview"
    ]
  },
  {
    "objectID": "index.html#hardware-platforms",
    "href": "index.html#hardware-platforms",
    "title": "Hardware Kits",
    "section": "Hardware Platforms",
    "text": "Hardware Platforms\n\n  \n    \n      \n        \n        Grove Vision AI V2\n        ~$25\n        Best for beginners. Plug & play vision AI.\n      \n    \n  \n  \n    \n      \n        \n        XIAO ESP32S3\n        ~$40\n        Best value. Vision, audio, motion.\n      \n    \n  \n  \n    \n      \n        \n        Raspberry Pi\n        ~$60-80\n        Advanced. LLMs, VLMs, edge AI.\n      \n    \n  \n  \n    \n      \n        \n        Nicla Vision\n        ~$95\n        Professional. Dual sensors, compact.",
    "crumbs": [
      "Overview"
    ]
  },
  {
    "objectID": "index.html#what-you-will-build",
    "href": "index.html#what-you-will-build",
    "title": "Hardware Kits",
    "section": "What You Will Build",
    "text": "What You Will Build\n\n  \n    üëÅÔ∏è Computer Vision\n    Image classification and object detection on microcontrollers. Train models to recognize objects, detect faces, or classify scenes.\n  \n  \n    üé§ Audio Processing\n    Keyword spotting and voice command recognition. Build wake-word detectors and voice interfaces that run entirely on-device.\n  \n  \n    üèÉ Motion Classification\n    Activity and gesture recognition from IMU data. Create wearable-style applications using accelerometer and gyroscope sensors.\n  \n  \n    ü§ñ Large Language Models\n    Run LLMs and VLMs on edge devices. Experience the frontier of on-device AI with models that understand and generate text.",
    "crumbs": [
      "Overview"
    ]
  },
  {
    "objectID": "index.html#getting-started",
    "href": "index.html#getting-started",
    "title": "Hardware Kits",
    "section": "Getting Started",
    "text": "Getting Started\n\nChoose Hardware: Select a platform based on your budget and learning goals. See Platforms for detailed comparisons.\nSet Up Environment: Install Arduino IDE or platform-specific tools. Follow the IDE Setup Guide for step-by-step instructions.\nBuild & Deploy: Work through the labs for your chosen platform. Start with Getting Started for an overview of available exercises.",
    "crumbs": [
      "Overview"
    ]
  },
  {
    "objectID": "index.html#part-of-the-mlsysbook-ecosystem",
    "href": "index.html#part-of-the-mlsysbook-ecosystem",
    "title": "Hardware Kits",
    "section": "Part of the MLSysBook Ecosystem",
    "text": "Part of the MLSysBook Ecosystem\n\n  \n    \n      Textbook\n      Comprehensive theory and concepts covering the full ML systems stack.\n    \n  \n  \n    \n      Hardware Kits\n      Hands-on embedded deployment. You are here.\n    \n  \n  \n    \n      TinyTorch\n      Build your own ML framework from scratch.",
    "crumbs": [
      "Overview"
    ]
  },
  {
    "objectID": "contents/seeed/xiao_esp32s3/motion_classification/motion_classification.html",
    "href": "contents/seeed/xiao_esp32s3/motion_classification/motion_classification.html",
    "title": "Motion Classification and Anomaly Detection",
    "section": "",
    "text": "DALL¬∑E prompt - 1950s style cartoon illustration set in a vintage audio lab. Scientists, dressed in classic attire with white lab coats, are intently analyzing audio data on large chalkboards. The boards display intricate FFT (Fast Fourier Transform) graphs and time-domain curves. Antique audio equipment is scattered around, but the data representations are clear and detailed, indicating their focus on audio analysis.\n\n\n\n\nTransportation is the backbone of global commerce. Millions of containers are transported daily via various means, such as ships, trucks, and trains, to destinations worldwide. Ensuring the safe and efficient transit of these containers is a monumental task that requires leveraging modern technology, and TinyML is undoubtedly one of the key solutions.\nIn this hands-on lab, we will work to solve real-world problems related to transportation. We will develop a Motion Classification and Anomaly Detection system using the XIAOML Kit, the Arduino IDE, and the Edge Impulse Studio. This project will help us understand how containers experience different forces and motions during various phases of transportation, including terrestrial and maritime transit, vertical movement via forklifts, and periods of stationary storage in warehouses.\n\n\n\n\n\n\nTipLearning Objectives\n\n\n\n\nSetting up the XIAOML Kit\nData Collection and Preprocessing\nBuilding the Motion Classification Model\nImplementing Anomaly Detection\nReal-world Testing and Analysis\n\n\n\nBy the end of this lab, you‚Äôll have a working prototype that can classify different types of motion and detect anomalies during the transportation of containers. This knowledge can serve as a stepping stone to more advanced projects in the burgeoning field of TinyML, particularly those involving vibration.\n\n\n\nThe XIAOML Kit comes with a built-in LSM6DS3TR-C 6-axis IMU sensor on the expansion board, eliminating the need for external sensor connections. This integrated approach offers a clean and reliable platform for motion-based machine learning applications.\nThe LSM6DS3TR-C combines a 3-axis accelerometer and 3-axis gyroscope in a single package, connected via I2C to the XIAO ESP32S3 at address 0x6A that provides:\n\nAccelerometer ranges: ¬±2/¬±4/¬±8/¬±16 g (we‚Äôll use ¬±2g by default)\nGyroscope ranges: ¬±125/¬±250/¬±500/¬±1000/¬±2000 dps (we‚Äôll use ¬±250 dps by default)\nResolution: 16-bit ADC\nCommunication: I2C interface at address 0x6A\nPower: Ultra-low power design\n\n \nCoordinate System: The sensor operates within a right-handed coordinate system. When looking at the expansion board from the bottom (where you can see the IMU sensor with the point mark):\n\nX-axis: Points to the right\nY-axis: Points forward (away from you)\nZ-axis: Points upward (out of the board)\n\n\n\nSince the XIAOML Kit comes pre-assembled with the expansion board, no additional hardware connections are required. The LSM6DS3TR-C IMU is already properly connected via I2C.\nWhat‚Äôs Already Connected:\n\nLSM6DS3TR-C IMU ‚Üí I2C (SDA/SCL) ‚Üí XIAO ESP32S3\nI2C Address: 0x6A\nPower: 3.3V from XIAO ESP32S3\n\nRequired Library: You should have the library installed during the Setup. If not, install the Seeed Arduino LSM6DS3 library following the steps:\n\nOpen Arduino IDE Library Manager\nSearch for ‚ÄúLSM6DS3‚Äù\nInstall ‚ÄúSeeed Arduino LSM6DS3‚Äù by Seeed Studio\nImportant: Do NOT install ‚ÄúArduino_LSM6DS3 by Arduino‚Äù - that‚Äôs for different boards!\n\n\n\n\nLet‚Äôs start with a simple test to verify the IMU is working correctly. Upload this code to test the sensor:\n#include &lt;LSM6DS3.h&gt;\n#include &lt;Wire.h&gt;\n\n// Create IMU object using I2C interface\nLSM6DS3 myIMU(I2C_MODE, 0x6A);\n\nfloat accelX, accelY, accelZ;\nfloat gyroX, gyroY, gyroZ;\n\nvoid setup() {\n  Serial.begin(115200);\n  while (!Serial) delay(10);\n\n  Serial.println(\"XIAOML Kit IMU Test\");\n  Serial.println(\"LSM6DS3TR-C 6-Axis IMU\");\n  Serial.println(\"====================\");\n\n  // Initialize the IMU\n  if (myIMU.begin() != 0) {\n      Serial.println(\"ERROR: IMU initialization failed!\");\n      while(1) delay(1000);\n  } else {\n      Serial.println(\"‚úì IMU initialized successfully\");\n      Serial.println(\"Data Format: AccelX,AccelY,AccelZ,\"\n                    \"GyroX,GyroY,GyroZ\");\n      Serial.println(\"Units: g-force, degrees/second\");\n      Serial.println();\n  }\n}\n\nvoid loop() {\n  // Read accelerometer data (in g-force)\n  accelX = myIMU.readFloatAccelX();\n  accelY = myIMU.readFloatAccelY();\n  accelZ = myIMU.readFloatAccelZ();\n\n  // Read gyroscope data (in degrees per second)\n  gyroX = myIMU.readFloatGyroX();\n  gyroY = myIMU.readFloatGyroY();\n  gyroZ = myIMU.readFloatGyroZ();\n\n  // Print readable format\n  Serial.print(\"Accel (g): X=\"); Serial.print(accelX, 3);\n  Serial.print(\" Y=\"); Serial.print(accelY, 3);\n  Serial.print(\" Z=\"); Serial.print(accelZ, 3);\n  Serial.print(\" | Gyro (¬∞/s): X=\"); Serial.print(gyroX, 2);\n  Serial.print(\" Y=\"); Serial.print(gyroY, 2);\n  Serial.print(\" Z=\"); Serial.println(gyroZ, 2);\n\n  delay(100); // 10 Hz update rate\n}\nWhen the kit is resting flat on a table, you should see:\n\nZ-axis acceleration around +1.0g (gravity)\nX and Y acceleration near 0.0g\nAll gyroscope values near 0.0¬∞/s\n\nMove the kit around to see the values change accordingly.\n\n\n\n\nWe will simulate container (or, more accurately, package) transportation through various scenarios to make this tutorial more relatable and practical.\n \nUsing the accelerometer of the XIAOML Kit, we‚Äôll capture motion data by manually simulating the conditions of:\n\nMaritime (pallets on boats) - Movement in all axes with wave-like patterns\nTerrestrial (pallets on trucks/trains) - Primarily horizontal movement\nLift (pallets being moved by forklift) - Primarily vertical movement\nIdle (pallets in storage) - Minimal movement\n\nFrom the above image, we can define for our simulation that primarily horizontal movements (\\(x\\) or \\(y\\) axis) should be associated with the ‚ÄúTerrestrial class.‚Äù Vertical movements (\\(z\\)-axis) with the ‚ÄúLift Class,‚Äù no activity with the ‚ÄúIdle class,‚Äù and movement on all three axes to Maritime class.\n \n\n\n\nFor data collection, we have several options available. In a real-world scenario, we can have our device, for example, connected directly to one container, and the collected data stored in a file (for example, CSV) on an SD card. Data can also be sent remotely to a nearby repository, such as a mobile phone, using Wi-Fi or Bluetooth (as demonstrated in this project: Sensor DataLogger). Once your dataset is collected and stored as a .CSV file, it can be uploaded to the Studio using the CSV Wizard tool.\n\nIn this video, you can learn alternative ways to send data to the Edge Impulse Studio.\n\n\n\nIn this lab, we will connect the Kit directly to the Edge Impulse Studio, which will also be used for data pre-processing, model training, testing, and deployment.\nFor data collection, we should first connect the Kit to Edge Impulse Studio, which will also be used for data pre-processing, model training, testing, and deployment.\n\nFollow the instructions here to install Node.js and Edge Impulse CLI on your computer.\n\nOnce the XIAOML Kit is not a fully supported development board by Edge Impulse, we should, for example, use the CLI Data Forwarder to capture data from our sensor and send it to the Studio, as shown in this diagram:\n \nWe‚Äôll modify our test code to output data in a format suitable for Edge Impulse:\n#include &lt;LSM6DS3.h&gt;\n#include &lt;Wire.h&gt;\n\n#define FREQUENCY_HZ        50\n#define INTERVAL_MS         (1000 / (FREQUENCY_HZ + 1))\n\nLSM6DS3 myIMU(I2C_MODE, 0x6A);\nstatic unsigned long last_interval_ms = 0;\n\nvoid setup() {\n  Serial.begin(115200);\n  while (!Serial) delay(10);\n\n  Serial.println(\"XIAOML Kit - Motion Data Collection\");\n  Serial.println(\"LSM6DS3TR-C IMU Sensor\");\n\n  // Initialize IMU\n  if (myIMU.begin() != 0) {\n      Serial.println(\"ERROR: IMU initialization failed!\");\n      while(1) delay(1000);\n  }\n\n  delay(2000);\n  Serial.println(\"Starting data collection in 3 seconds...\");\n  delay(3000);\n}\n\nvoid loop() {\n  if (millis() &gt; last_interval_ms + INTERVAL_MS) {\n      last_interval_ms = millis();\n\n      // Read accelerometer data\n      float ax = myIMU.readFloatAccelX();\n      float ay = myIMU.readFloatAccelY();\n      float az = myIMU.readFloatAccelZ();\n\n      // Convert to m/s¬≤ (multiply by 9.81)\n      float ax_ms2 = ax * 9.81;\n      float ay_ms2 = ay * 9.81;\n      float az_ms2 = az * 9.81;\n\n      // Output in Edge Impulse format\n      Serial.print(ax_ms2);\n      Serial.print(\"\\t\");\n      Serial.print(ay_ms2);\n      Serial.print(\"\\t\");\n      Serial.println(az_ms2);\n  }\n}\nUpload the code to the Arduino IDE. We should see the accelerometer values (converted to m/s¬≤) at the Serial Monitor:\n \n\nKeep the code running, but turn off the Serial Monitor. The data generated by the Kit will be sent to the Edge Impulse Studio via Serial Connection.\n\n\n\n\nCreate an Edge Impulse Project - Go to Edge Impulse Studio and create a new project - Choose a descriptive name (keep under 63 characters for Arduino library compatibility)\n \nSet up CLI Data Forwarder - Install Edge Impulse CLI on your computer - Confirm that the XIAOML Kit is connected to the computer, the code is running and the Serial Monitor is OFF, otherwise we can get an error. - On the Computer Terminal, run: edge-impulse-data-forwarder --clean - Enter your Edge Impulse credentials - Select your project and configure device settings\n \n\nGo to the Edge Impulse Studio Project. On the Device section is possible to verify if the kit is correctly connected (the dot should be green).\n\n \n\n\n\n\nAs discussed before, we should capture data from all four Transportation Classes. Imagine that you have a container with a built-in accelerometer (In this case, our XIAOML Kit). Now imagine your container is on a boat, facing an angry ocean:\n \nOr in a Truck, travelling on a road, or being moved with a forklift, etc.\n\n\nMaritime Class:\n\nHold the kit and simulate boat movement\nMove in all three axes with wave-like, undulating motions\nInclude gentle rolling and pitching movements\n\nTerrestrial Class:\n\nMove the kit horizontally in straight lines (left to right and vice versa)\nSimulate truck/train vibrations with small horizontal shakes\nOccasional gentle bumps and turns\n\nLift Class:\n\nMove the kit primarily in vertical directions (up and down)\nSimulate forklift operations: up, pause, down\nInclude some short horizontal positioning movements\n\nIdle Class:\n\nPlace the kit on a stable surface\nMinimal to no movement\nCapture environmental vibrations and sensor noise\n\n\n\n\nOn the Data Acquisition section, you should see that your board [xiaoml-kit] is connected. The sensor is available: [sensor with 3 axes (accX, accY, accZ)] with a sampling frequency of [50 Hz]. The Studio suggests a sample length of [10000] ms (10 s). The last thing left is defining the sample label. Let‚Äôs start, for example, with[terrestrial].\nPress [Start Sample]and move your kit horizontally (left to right), keeping it in one direction. After 10 seconds, our data will be uploaded to the Studio.\nBelow is one sample (raw data) of 10 seconds of collected data. It is notable that the ondulatory movement predominantly occurs along the Y-axis (left-right). The other axes are almost stationary (the X-axis is centered around zero, and the Z-axis is centered around 9.8 ms¬≤ due to gravity).\n \nYou should capture, for example, around 2 minutes (ten to twelve samples of 10 seconds each) for each of the four classes. Using the 3 dots after each sample, select two and move them to the Test set. Alternatively, you can use the Automatic Train/Test Split tool on the Danger Zone of the Dashboard tab. Below, it is possible to see the result datasets:\n \n\n\n\n\nThe raw data type captured by the accelerometer is a ‚Äútime series‚Äù and should be converted to ‚Äútabular data‚Äù. We can do this conversion using a sliding window over the sample data. For example, in the below figure,\n \nWe can see 10 seconds of accelerometer data captured with a sample rate (SR) of 50 Hz. A 2-second window will capture 300 data points (3 axes \\(\\times\\) 2 seconds \\(\\times\\) 50 samples). We will slide this window every 200ms, creating a larger dataset where each instance has 300 raw features.\n\nYou should use the best SR for your case, considering Nyquist‚Äôs theorem, which states that a periodic signal must be sampled at more than twice the signal‚Äôs highest frequency component.\n\nData preprocessing is a challenging area for embedded machine learning. Still, Edge Impulse helps overcome this with its digital signal processing (DSP) preprocessing step and, more specifically, the Spectral Features.\nOn the Studio, this dataset will be the input of a Spectral Analysis block, which is excellent for analyzing repetitive motion, such as data from accelerometers. This block will perform a DSP (Digital Signal Processing), extracting features such as ‚ÄúFFT‚Äù or ‚ÄúWavelets‚Äù. In the most common case, FFT, the Time Domain Statistical features per axis/channel are:\n\nRMS\nSkewness\nKurtosis\n\n\n\n\n\n\nAnd the Frequency Domain Spectral features per axis/channel are:\n\nSpectral Power\nSkewness\nKurtosis\n\n \nFor example, for an FFT length of 32 points, the Spectral Analysis Block‚Äôs resulting output will be 21 features per axis (a total of 63 features).\nThose 63 features will serve as the input tensor for a Neural Network Classifier and the Anomaly Detection model (K-Means).\n\nYou can learn more by digging into the lab DSP Spectral Features\n\n\n\n\nOur classifier will be a Dense Neural Network (DNN) that will have 63 neurons on its input layer, two hidden layers with 20 and 10 neurons, and an output layer with four neurons (one per each class), as shown here:\n \n\n\n\nAn impulse takes raw data, uses signal processing to extract features, and then uses a learning block (Dense model) to classify new data.\nWe also utilize a second model, the K-means, which can be used for Anomaly Detection. If we imagine that we could have our known classes as clusters, any sample that cannot fit into one of these clusters could be an outlier, an anomaly (for example, a container rolling out of a ship on the ocean or being upside down on the floor).\n \n\nImagine our XIAOML Kit rolling or moving upside-down, on a movement complement different from the one trained on.\n\n \nBelow the final Impulse design:\n \n\n\n\nAt this point in our project, we have defined the pre-processing method, and the model has been designed. Now, it is time to have the job done. First, let‚Äôs convert the raw data (time-series type) into tabular data. Go to the Spectral Features tab and select [Save Parameters]. Alternatively, instead of using the default values, we can select the [Autotune parameters] button. In this case, the Studio will define new hyperparameters, as the filter design and FFT length, based on the raw data.\n \nAt the top menu, select the Generate features tab, and there, select the options, Calculate feature importance, Normalize features, and press the [Generate features] button. Each 2-second window of data (300 datapoints) will be converted into a single tabular data point with 63 features.\n\nThe Feature Explorer will display this data in 2D using UMAP. Uniform Manifold Approximation and Projection (UMAP) is a dimensionality reduction technique that can be used for visualization, similar to t-SNE, but also for general non-linear dimensionality reduction.\n\nThe visualization enables one to verify that the classes present an excellent separation, indicating that the classifier should perform well.\n \nOptionally, you can analyze the relative importance of each feature for one class compared with other classes.\n\n\n\nOur classifier will be a Dense Neural Network (DNN) that will have 63 neurons on its input layer, two hidden layers with 20 and 10 neurons, and an output layer with four neurons (one per each class), as shown here:\n \nAs hyperparameters, we will use a Learning Rate of 0.005 and 20% of the data for validation for 30 epochs. After training, we can see that the accuracy is 100%.\n \nFor anomaly detection, we should choose the suggested features that are precisely the most important in feature extraction. The number of clusters will be 32, as suggested by the Studio. After training, we can select some data for testing, such as maritime data. The resulting Anomaly score was min: -0.1642, max: 0.0738, avg: -0.0867.\nWhen changing the data, it is possible to realize that small or negative Anomaly Scores indicate that the data are normal.\n \n\n\n\nUsing 20% of the data left behind during the data capture phase, we can verify how our model will behave with unknown data; if not 100% (what is expected), the result was very good (8%).\n \nYou should also use your kit (which is still connected to the Studio) and perform some Live Classification. For example, let‚Äôs test some ‚Äúterrestrial‚Äù movement:\n \n\nBe aware that here, you will capture real data with your device and upload it to the Studio, where an inference will be made using the trained model (note that the model is not on your device).\n\n\n\n\nNow it is time for magic! The Studio will package all the needed libraries, preprocessing functions, and trained models, downloading them to your computer. You should select the Arduino Library option, and then, at the bottom, choose Quantized (Int8) and click [Build]. A ZIP file will be created and downloaded to your computer.\n \nOn your Arduino IDE, go to the Sketch tab, select the option Add.ZIP Library, and Choose the.zip file downloaded by the Studio:\n \n\n\n\nNow, it is time for a real test. We will make inferences that are wholly disconnected from the Studio. Let‚Äôs change one of the code examples created when you deploy the Arduino Library.\nIn your Arduino IDE, go to the File/Examples tab and look for your project, and in examples, select nano_ble_sense_accelerometer:\nOf course, this is not your board, but we can have the code working with only a few changes.\nFor example, at the beginning of the code, you have the library related to Arduino Sense IMU:\n/* Includes -------------------------------------------- */\n#include &lt;XIAOML_Kit_Motion_Class_-_AD_inferencing.h&gt;\n#include &lt;Arduino_LSM9DS1.h&gt;\nChange the ‚Äúincludes‚Äù portion with the code related to the IMU:\n#include &lt;XIAOML_Kit_Motion_Class_-_AD_inferencing.h&gt;\n#include &lt;LSM6DS3.h&gt;\n#include &lt;Wire.h&gt;\nChange the Constant Defines\n// IMU setup\nLSM6DS3 myIMU(I2C_MODE, 0x6A);\n\n// Inference settings\n#define CONVERT_G_TO_MS2    9.81f\n#define MAX_ACCEPTED_RANGE  2.0f * CONVERT_G_TO_MS2\nOn the setup function, initiate the IMU:\n   // Initialize IMU\n    if (myIMU.begin() != 0) {\n        Serial.println(\"ERROR: IMU initialization failed!\");\n        return;\n    }\nAt the loop function, the buffers buffer[ix], buffer[ix + 1], and buffer[ix + 2] will receive the 3-axis data captured by the accelerometer. In the original code, you have the line:\nIMU.readAcceleration(buffer[ix], buffer[ix + 1], buffer[ix + 2]);\nChange it with this block of code:\n// Read IMU data\nfloat x = myIMU.readFloatAccelX();\nfloat y = myIMU.readFloatAccelY();\nfloat z = myIMU.readFloatAccelZ();\nYou should reorder the following two blocks of code. First, you make the conversion to raw data to ‚ÄúMeters per squared second (m/s2)‚Äù, followed by the test regarding the maximum acceptance range (that here is in m/s2, but on Arduino, was in Gs):\n  // Convert to m/s¬≤\n  buffer[i + 0] = x * CONVERT_G_TO_MS2;\n  buffer[i + 1] = y * CONVERT_G_TO_MS2;\n  buffer[i + 2] = z * CONVERT_G_TO_MS2;\n\n  // Apply range limiting\n  for (int j = 0; j &lt; 3; j++) {\n      if (fabs(buffer[i + j]) &gt; MAX_ACCEPTED_RANGE) {\n          buffer[i + j] = copysign(MAX_ACCEPTED_RANGE, buffer[i + j]);\n      }\n  }\nAnd this is enough. We can also adjust how the inference is displayed in the Serial Monitor. You can now upload the complete code below to your device and proceed with the inferences.\n// Motion Classification with LSM6DS3TR-C IMU\n#include &lt;XIAOML_Kit_Motion_Class_-_AD_inferencing.h&gt;\n#include &lt;LSM6DS3.h&gt;\n#include &lt;Wire.h&gt;\n\n// IMU setup\nLSM6DS3 myIMU(I2C_MODE, 0x6A);\n\n// Inference settings\n#define CONVERT_G_TO_MS2    9.81f\n#define MAX_ACCEPTED_RANGE  2.0f * CONVERT_G_TO_MS2\n\nstatic bool debug_nn = false;\nstatic float buffer[EI_CLASSIFIER_DSP_INPUT_FRAME_SIZE] = { 0 };\nstatic float inference_buffer[EI_CLASSIFIER_DSP_INPUT_FRAME_SIZE];\n\nvoid setup() {\n    Serial.begin(115200);\n    while (!Serial) delay(10);\n\n    Serial.println(\"XIAOML Kit - Motion Classification\");\n    Serial.println(\"LSM6DS3TR-C IMU Inference\");\n\n    // Initialize IMU\n    if (myIMU.begin() != 0) {\n        Serial.println(\"ERROR: IMU initialization failed!\");\n        return;\n    }\n\n    Serial.println(\"‚úì IMU initialized\");\n\n    if (EI_CLASSIFIER_RAW_SAMPLES_PER_FRAME != 3) {\n        Serial.println(\"ERROR: EI_CLASSIFIER_RAW_SAMPLES_PER_FRAME\"\n                       \"should be 3\");\n        return;\n    }\n\n    Serial.println(\"‚úì Model loaded\");\n    Serial.println(\"Starting motion classification...\");\n}\n\nvoid loop() {\n    ei_printf(\"\\nStarting inferencing in 2 seconds...\\n\");\n    delay(2000);\n\n    ei_printf(\"Sampling...\\n\");\n\n    // Clear buffer\n    for (size_t i = 0; i &lt; EI_CLASSIFIER_DSP_INPUT_FRAME_SIZE; i++) {\n        buffer[i] = 0.0f;\n    }\n\n    // Collect accelerometer data\n    for (int i = 0; i &lt; EI_CLASSIFIER_DSP_INPUT_FRAME_SIZE; i += 3) {\n        uint64_t next_tick = micros() +\n          (EI_CLASSIFIER_INTERVAL_MS * 1000);\n\n        // Read IMU data\n        float x = myIMU.readFloatAccelX();\n        float y = myIMU.readFloatAccelY();\n        float z = myIMU.readFloatAccelZ();\n\n        // Convert to m/s¬≤\n        buffer[i + 0] = x * CONVERT_G_TO_MS2;\n        buffer[i + 1] = y * CONVERT_G_TO_MS2;\n        buffer[i + 2] = z * CONVERT_G_TO_MS2;\n\n        // Apply range limiting\n        for (int j = 0; j &lt; 3; j++) {\n            if (fabs(buffer[i + j]) &gt; MAX_ACCEPTED_RANGE) {\n                buffer[i + j] = copysign(MAX_ACCEPTED_RANGE,\n                                         buffer[i + j]);\n            }\n        }\n\n        delayMicroseconds(next_tick - micros());\n    }\n\n    // Copy to inference buffer\n    for (int i = 0; i &lt; EI_CLASSIFIER_DSP_INPUT_FRAME_SIZE; i++) {\n        inference_buffer[i] = buffer[i];\n    }\n\n    // Create signal from buffer\n    signal_t signal;\n    int err = numpy::signal_from_buffer(inference_buffer,\n              EI_CLASSIFIER_DSP_INPUT_FRAME_SIZE, &signal);\n    if (err != 0) {\n        ei_printf(\"ERROR: Failed to create signal from buffer (%d)\\n\",\n                  err);\n        return;\n    }\n\n    // Run the classifier\n    ei_impulse_result_t result = { 0 };\n    err = run_classifier(&signal, &result, debug_nn);\n    if (err != EI_IMPULSE_OK) {\n        ei_printf(\"ERROR: Failed to run classifier (%d)\\n\", err);\n        return;\n    }\n\n    // Print predictions\n    ei_printf(\"Predictions (DSP: %d ms, Classification: %d ms, \"\n              \"Anomaly: %d ms):\\n\",\n        result.timing.dsp, result.timing.classification, result.timing.anomaly);\n\n    for (size_t ix = 0; ix &lt; EI_CLASSIFIER_LABEL_COUNT; ix++) {\n        ei_printf(\"    %s: %.5f\\n\", result.classification[ix].label,\n                  result.classification[ix].value);\n    }\n\n    // Print anomaly score\n#if EI_CLASSIFIER_HAS_ANOMALY == 1\n    ei_printf(\"Anomaly score: %.3f\\n\", result.anomaly);\n#endif\n\n    // Determine prediction\n    float max_confidence = 0.0;\n    String predicted_class = \"unknown\";\n\n    for (size_t ix = 0; ix &lt; EI_CLASSIFIER_LABEL_COUNT; ix++) {\n        if (result.classification[ix].value &gt; max_confidence) {\n            max_confidence = result.classification[ix].value;\n            predicted_class = String(result.classification[ix].label);\n        }\n    }\n\n    // Display result with confidence threshold\n    if (max_confidence &gt; 0.6) {\n        ei_printf(\"\\nüéØ PREDICTION: %s (%.1f%% confidence)\\n\",\n                 predicted_class.c_str(), max_confidence * 100);\n    } else {\n        ei_printf(\"\\n‚ùì UNCERTAIN: Highest confidence is %s (%.1f%%)\\n\",\n                 predicted_class.c_str(), max_confidence * 100);\n    }\n\n    // Check for anomaly\n#if EI_CLASSIFIER_HAS_ANOMALY == 1\n    if (result.anomaly &gt; 0.5) {\n        ei_printf(\"‚ö†Ô∏è ANOMALY DETECTED! Score: %.3f\\n\", result.anomaly);\n    }\n#endif\n\n    delay(1000);\n}\n\nvoid ei_printf(const char *format, ...) {\n    static char print_buf[1024] = { 0 };\n    va_list args;\n    va_start(args, format);\n    int r = vsnprintf(print_buf, sizeof(print_buf), format, args);\n    va_end(args);\n    if (r &gt; 0) {\n        Serial.write(print_buf);\n    }\n}\n\nThe complete code is available on the Lab‚Äôs GitHub.\n\nNow you should try your movements, seeing the result of the inference of each class on the images:\n \n \n \n \nAnd, of course, some ‚Äúanomaly‚Äù, for example, putting the XIAO upside-down. The anomaly score will be over 0.5:\n \n\n\n\nNow that we know the model is working, we suggest modifying the code to see the result with the Kit completely offline (disconnected from the PC and powered by a battery, a power bank, or an independent 5V power supply).\nThe idea is that if a specific movement is detected, a corresponding message will appear on the OLED display.\n\nThe modified inference code to have the OLED display is available on the Lab‚Äôs GitHub.\n\n\n\nThis lab demonstrated how to build a complete motion classification system using the XIAOML Kit‚Äôs built-in LSM6DS3TR-C IMU sensor. Key achievements include:\nTechnical Implementation:\n\nUtilized the integrated 6-axis IMU for motion sensing\nCollected labeled training data for four transportation scenarios\nImplemented spectral feature extraction for time-series analysis\nDeployed a neural network classifier optimized for microcontroller inference\nAdded anomaly detection for identifying unusual movements\n\nMachine Learning Pipeline:\n\nData collection directly from embedded sensors\nFeature engineering using frequency domain analysis\nModel training and optimization in Edge Impulse\nReal-time inference on resource-constrained hardware\nPerformance monitoring and validation\n\nPractical Applications: The techniques learned apply directly to real-world scenarios, including:\n\nAsset tracking and logistics monitoring\nPredictive maintenance for machinery\nHuman activity recognition\nVehicle and equipment monitoring\nIoT sensor networks for smart cities\n\nKey Learnings:\n\nWorking with IMU coordinate systems and sensor fusion\nBalancing model accuracy with inference speed on edge devices\nImplementing robust data collection and preprocessing pipelines\nDeploying machine learning models to embedded systems\nIntegrating multiple sensors (IMU + display) for complete solutions\n\nThe integration of motion classification with the XIAOML Kit demonstrates how modern embedded systems can perform sophisticated AI tasks locally, enabling real-time decision-making without reliance on the cloud. This approach is fundamental to the future of edge AI in industrial IoT, autonomous systems, and smart device applications.\n\n\n\n\nXIAOML KIT Code\nDSP Spectral Features\nEdge Impulse Project\nEdge Impulse Spectral Features Block Colab Notebook\nEdge Impulse Documentation\nEdge Impulse Spectral Features\nSeeed Studio LSM6DS3 Library",
    "crumbs": [
      "Seeed XIAO ESP32S3",
      "Motion Classification and Anomaly Detection"
    ]
  },
  {
    "objectID": "contents/seeed/xiao_esp32s3/motion_classification/motion_classification.html#sec-motion-classification-anomaly-detection-overview-cb1f",
    "href": "contents/seeed/xiao_esp32s3/motion_classification/motion_classification.html#sec-motion-classification-anomaly-detection-overview-cb1f",
    "title": "Motion Classification and Anomaly Detection",
    "section": "",
    "text": "Transportation is the backbone of global commerce. Millions of containers are transported daily via various means, such as ships, trucks, and trains, to destinations worldwide. Ensuring the safe and efficient transit of these containers is a monumental task that requires leveraging modern technology, and TinyML is undoubtedly one of the key solutions.\nIn this hands-on lab, we will work to solve real-world problems related to transportation. We will develop a Motion Classification and Anomaly Detection system using the XIAOML Kit, the Arduino IDE, and the Edge Impulse Studio. This project will help us understand how containers experience different forces and motions during various phases of transportation, including terrestrial and maritime transit, vertical movement via forklifts, and periods of stationary storage in warehouses.\n\n\n\n\n\n\nTipLearning Objectives\n\n\n\n\nSetting up the XIAOML Kit\nData Collection and Preprocessing\nBuilding the Motion Classification Model\nImplementing Anomaly Detection\nReal-world Testing and Analysis\n\n\n\nBy the end of this lab, you‚Äôll have a working prototype that can classify different types of motion and detect anomalies during the transportation of containers. This knowledge can serve as a stepping stone to more advanced projects in the burgeoning field of TinyML, particularly those involving vibration.",
    "crumbs": [
      "Seeed XIAO ESP32S3",
      "Motion Classification and Anomaly Detection"
    ]
  },
  {
    "objectID": "contents/seeed/xiao_esp32s3/motion_classification/motion_classification.html#sec-motion-classification-anomaly-detection-installing-imu-cac2",
    "href": "contents/seeed/xiao_esp32s3/motion_classification/motion_classification.html#sec-motion-classification-anomaly-detection-installing-imu-cac2",
    "title": "Motion Classification and Anomaly Detection",
    "section": "",
    "text": "The XIAOML Kit comes with a built-in LSM6DS3TR-C 6-axis IMU sensor on the expansion board, eliminating the need for external sensor connections. This integrated approach offers a clean and reliable platform for motion-based machine learning applications.\nThe LSM6DS3TR-C combines a 3-axis accelerometer and 3-axis gyroscope in a single package, connected via I2C to the XIAO ESP32S3 at address 0x6A that provides:\n\nAccelerometer ranges: ¬±2/¬±4/¬±8/¬±16 g (we‚Äôll use ¬±2g by default)\nGyroscope ranges: ¬±125/¬±250/¬±500/¬±1000/¬±2000 dps (we‚Äôll use ¬±250 dps by default)\nResolution: 16-bit ADC\nCommunication: I2C interface at address 0x6A\nPower: Ultra-low power design\n\n \nCoordinate System: The sensor operates within a right-handed coordinate system. When looking at the expansion board from the bottom (where you can see the IMU sensor with the point mark):\n\nX-axis: Points to the right\nY-axis: Points forward (away from you)\nZ-axis: Points upward (out of the board)\n\n\n\nSince the XIAOML Kit comes pre-assembled with the expansion board, no additional hardware connections are required. The LSM6DS3TR-C IMU is already properly connected via I2C.\nWhat‚Äôs Already Connected:\n\nLSM6DS3TR-C IMU ‚Üí I2C (SDA/SCL) ‚Üí XIAO ESP32S3\nI2C Address: 0x6A\nPower: 3.3V from XIAO ESP32S3\n\nRequired Library: You should have the library installed during the Setup. If not, install the Seeed Arduino LSM6DS3 library following the steps:\n\nOpen Arduino IDE Library Manager\nSearch for ‚ÄúLSM6DS3‚Äù\nInstall ‚ÄúSeeed Arduino LSM6DS3‚Äù by Seeed Studio\nImportant: Do NOT install ‚ÄúArduino_LSM6DS3 by Arduino‚Äù - that‚Äôs for different boards!\n\n\n\n\nLet‚Äôs start with a simple test to verify the IMU is working correctly. Upload this code to test the sensor:\n#include &lt;LSM6DS3.h&gt;\n#include &lt;Wire.h&gt;\n\n// Create IMU object using I2C interface\nLSM6DS3 myIMU(I2C_MODE, 0x6A);\n\nfloat accelX, accelY, accelZ;\nfloat gyroX, gyroY, gyroZ;\n\nvoid setup() {\n  Serial.begin(115200);\n  while (!Serial) delay(10);\n\n  Serial.println(\"XIAOML Kit IMU Test\");\n  Serial.println(\"LSM6DS3TR-C 6-Axis IMU\");\n  Serial.println(\"====================\");\n\n  // Initialize the IMU\n  if (myIMU.begin() != 0) {\n      Serial.println(\"ERROR: IMU initialization failed!\");\n      while(1) delay(1000);\n  } else {\n      Serial.println(\"‚úì IMU initialized successfully\");\n      Serial.println(\"Data Format: AccelX,AccelY,AccelZ,\"\n                    \"GyroX,GyroY,GyroZ\");\n      Serial.println(\"Units: g-force, degrees/second\");\n      Serial.println();\n  }\n}\n\nvoid loop() {\n  // Read accelerometer data (in g-force)\n  accelX = myIMU.readFloatAccelX();\n  accelY = myIMU.readFloatAccelY();\n  accelZ = myIMU.readFloatAccelZ();\n\n  // Read gyroscope data (in degrees per second)\n  gyroX = myIMU.readFloatGyroX();\n  gyroY = myIMU.readFloatGyroY();\n  gyroZ = myIMU.readFloatGyroZ();\n\n  // Print readable format\n  Serial.print(\"Accel (g): X=\"); Serial.print(accelX, 3);\n  Serial.print(\" Y=\"); Serial.print(accelY, 3);\n  Serial.print(\" Z=\"); Serial.print(accelZ, 3);\n  Serial.print(\" | Gyro (¬∞/s): X=\"); Serial.print(gyroX, 2);\n  Serial.print(\" Y=\"); Serial.print(gyroY, 2);\n  Serial.print(\" Z=\"); Serial.println(gyroZ, 2);\n\n  delay(100); // 10 Hz update rate\n}\nWhen the kit is resting flat on a table, you should see:\n\nZ-axis acceleration around +1.0g (gravity)\nX and Y acceleration near 0.0g\nAll gyroscope values near 0.0¬∞/s\n\nMove the kit around to see the values change accordingly.",
    "crumbs": [
      "Seeed XIAO ESP32S3",
      "Motion Classification and Anomaly Detection"
    ]
  },
  {
    "objectID": "contents/seeed/xiao_esp32s3/motion_classification/motion_classification.html#sec-motion-classification-anomaly-detection-tinyml-motion-classification-project-90f9",
    "href": "contents/seeed/xiao_esp32s3/motion_classification/motion_classification.html#sec-motion-classification-anomaly-detection-tinyml-motion-classification-project-90f9",
    "title": "Motion Classification and Anomaly Detection",
    "section": "",
    "text": "We will simulate container (or, more accurately, package) transportation through various scenarios to make this tutorial more relatable and practical.\n \nUsing the accelerometer of the XIAOML Kit, we‚Äôll capture motion data by manually simulating the conditions of:\n\nMaritime (pallets on boats) - Movement in all axes with wave-like patterns\nTerrestrial (pallets on trucks/trains) - Primarily horizontal movement\nLift (pallets being moved by forklift) - Primarily vertical movement\nIdle (pallets in storage) - Minimal movement\n\nFrom the above image, we can define for our simulation that primarily horizontal movements (\\(x\\) or \\(y\\) axis) should be associated with the ‚ÄúTerrestrial class.‚Äù Vertical movements (\\(z\\)-axis) with the ‚ÄúLift Class,‚Äù no activity with the ‚ÄúIdle class,‚Äù and movement on all three axes to Maritime class.",
    "crumbs": [
      "Seeed XIAO ESP32S3",
      "Motion Classification and Anomaly Detection"
    ]
  },
  {
    "objectID": "contents/seeed/xiao_esp32s3/motion_classification/motion_classification.html#sec-motion-classification-anomaly-detection-data-collection-d54c",
    "href": "contents/seeed/xiao_esp32s3/motion_classification/motion_classification.html#sec-motion-classification-anomaly-detection-data-collection-d54c",
    "title": "Motion Classification and Anomaly Detection",
    "section": "",
    "text": "For data collection, we have several options available. In a real-world scenario, we can have our device, for example, connected directly to one container, and the collected data stored in a file (for example, CSV) on an SD card. Data can also be sent remotely to a nearby repository, such as a mobile phone, using Wi-Fi or Bluetooth (as demonstrated in this project: Sensor DataLogger). Once your dataset is collected and stored as a .CSV file, it can be uploaded to the Studio using the CSV Wizard tool.\n\nIn this video, you can learn alternative ways to send data to the Edge Impulse Studio.\n\n\n\nIn this lab, we will connect the Kit directly to the Edge Impulse Studio, which will also be used for data pre-processing, model training, testing, and deployment.\nFor data collection, we should first connect the Kit to Edge Impulse Studio, which will also be used for data pre-processing, model training, testing, and deployment.\n\nFollow the instructions here to install Node.js and Edge Impulse CLI on your computer.\n\nOnce the XIAOML Kit is not a fully supported development board by Edge Impulse, we should, for example, use the CLI Data Forwarder to capture data from our sensor and send it to the Studio, as shown in this diagram:\n \nWe‚Äôll modify our test code to output data in a format suitable for Edge Impulse:\n#include &lt;LSM6DS3.h&gt;\n#include &lt;Wire.h&gt;\n\n#define FREQUENCY_HZ        50\n#define INTERVAL_MS         (1000 / (FREQUENCY_HZ + 1))\n\nLSM6DS3 myIMU(I2C_MODE, 0x6A);\nstatic unsigned long last_interval_ms = 0;\n\nvoid setup() {\n  Serial.begin(115200);\n  while (!Serial) delay(10);\n\n  Serial.println(\"XIAOML Kit - Motion Data Collection\");\n  Serial.println(\"LSM6DS3TR-C IMU Sensor\");\n\n  // Initialize IMU\n  if (myIMU.begin() != 0) {\n      Serial.println(\"ERROR: IMU initialization failed!\");\n      while(1) delay(1000);\n  }\n\n  delay(2000);\n  Serial.println(\"Starting data collection in 3 seconds...\");\n  delay(3000);\n}\n\nvoid loop() {\n  if (millis() &gt; last_interval_ms + INTERVAL_MS) {\n      last_interval_ms = millis();\n\n      // Read accelerometer data\n      float ax = myIMU.readFloatAccelX();\n      float ay = myIMU.readFloatAccelY();\n      float az = myIMU.readFloatAccelZ();\n\n      // Convert to m/s¬≤ (multiply by 9.81)\n      float ax_ms2 = ax * 9.81;\n      float ay_ms2 = ay * 9.81;\n      float az_ms2 = az * 9.81;\n\n      // Output in Edge Impulse format\n      Serial.print(ax_ms2);\n      Serial.print(\"\\t\");\n      Serial.print(ay_ms2);\n      Serial.print(\"\\t\");\n      Serial.println(az_ms2);\n  }\n}\nUpload the code to the Arduino IDE. We should see the accelerometer values (converted to m/s¬≤) at the Serial Monitor:\n \n\nKeep the code running, but turn off the Serial Monitor. The data generated by the Kit will be sent to the Edge Impulse Studio via Serial Connection.\n\n\n\n\nCreate an Edge Impulse Project - Go to Edge Impulse Studio and create a new project - Choose a descriptive name (keep under 63 characters for Arduino library compatibility)\n \nSet up CLI Data Forwarder - Install Edge Impulse CLI on your computer - Confirm that the XIAOML Kit is connected to the computer, the code is running and the Serial Monitor is OFF, otherwise we can get an error. - On the Computer Terminal, run: edge-impulse-data-forwarder --clean - Enter your Edge Impulse credentials - Select your project and configure device settings\n \n\nGo to the Edge Impulse Studio Project. On the Device section is possible to verify if the kit is correctly connected (the dot should be green).",
    "crumbs": [
      "Seeed XIAO ESP32S3",
      "Motion Classification and Anomaly Detection"
    ]
  },
  {
    "objectID": "contents/seeed/xiao_esp32s3/motion_classification/motion_classification.html#sec-motion-classification-anomaly-detection-data-collection-studio-5ca4",
    "href": "contents/seeed/xiao_esp32s3/motion_classification/motion_classification.html#sec-motion-classification-anomaly-detection-data-collection-studio-5ca4",
    "title": "Motion Classification and Anomaly Detection",
    "section": "",
    "text": "As discussed before, we should capture data from all four Transportation Classes. Imagine that you have a container with a built-in accelerometer (In this case, our XIAOML Kit). Now imagine your container is on a boat, facing an angry ocean:\n \nOr in a Truck, travelling on a road, or being moved with a forklift, etc.\n\n\nMaritime Class:\n\nHold the kit and simulate boat movement\nMove in all three axes with wave-like, undulating motions\nInclude gentle rolling and pitching movements\n\nTerrestrial Class:\n\nMove the kit horizontally in straight lines (left to right and vice versa)\nSimulate truck/train vibrations with small horizontal shakes\nOccasional gentle bumps and turns\n\nLift Class:\n\nMove the kit primarily in vertical directions (up and down)\nSimulate forklift operations: up, pause, down\nInclude some short horizontal positioning movements\n\nIdle Class:\n\nPlace the kit on a stable surface\nMinimal to no movement\nCapture environmental vibrations and sensor noise\n\n\n\n\nOn the Data Acquisition section, you should see that your board [xiaoml-kit] is connected. The sensor is available: [sensor with 3 axes (accX, accY, accZ)] with a sampling frequency of [50 Hz]. The Studio suggests a sample length of [10000] ms (10 s). The last thing left is defining the sample label. Let‚Äôs start, for example, with[terrestrial].\nPress [Start Sample]and move your kit horizontally (left to right), keeping it in one direction. After 10 seconds, our data will be uploaded to the Studio.\nBelow is one sample (raw data) of 10 seconds of collected data. It is notable that the ondulatory movement predominantly occurs along the Y-axis (left-right). The other axes are almost stationary (the X-axis is centered around zero, and the Z-axis is centered around 9.8 ms¬≤ due to gravity).\n \nYou should capture, for example, around 2 minutes (ten to twelve samples of 10 seconds each) for each of the four classes. Using the 3 dots after each sample, select two and move them to the Test set. Alternatively, you can use the Automatic Train/Test Split tool on the Danger Zone of the Dashboard tab. Below, it is possible to see the result datasets:",
    "crumbs": [
      "Seeed XIAO ESP32S3",
      "Motion Classification and Anomaly Detection"
    ]
  },
  {
    "objectID": "contents/seeed/xiao_esp32s3/motion_classification/motion_classification.html#sec-motion-classification-anomaly-detection-data-preprocessing-2269",
    "href": "contents/seeed/xiao_esp32s3/motion_classification/motion_classification.html#sec-motion-classification-anomaly-detection-data-preprocessing-2269",
    "title": "Motion Classification and Anomaly Detection",
    "section": "",
    "text": "The raw data type captured by the accelerometer is a ‚Äútime series‚Äù and should be converted to ‚Äútabular data‚Äù. We can do this conversion using a sliding window over the sample data. For example, in the below figure,\n \nWe can see 10 seconds of accelerometer data captured with a sample rate (SR) of 50 Hz. A 2-second window will capture 300 data points (3 axes \\(\\times\\) 2 seconds \\(\\times\\) 50 samples). We will slide this window every 200ms, creating a larger dataset where each instance has 300 raw features.\n\nYou should use the best SR for your case, considering Nyquist‚Äôs theorem, which states that a periodic signal must be sampled at more than twice the signal‚Äôs highest frequency component.\n\nData preprocessing is a challenging area for embedded machine learning. Still, Edge Impulse helps overcome this with its digital signal processing (DSP) preprocessing step and, more specifically, the Spectral Features.\nOn the Studio, this dataset will be the input of a Spectral Analysis block, which is excellent for analyzing repetitive motion, such as data from accelerometers. This block will perform a DSP (Digital Signal Processing), extracting features such as ‚ÄúFFT‚Äù or ‚ÄúWavelets‚Äù. In the most common case, FFT, the Time Domain Statistical features per axis/channel are:\n\nRMS\nSkewness\nKurtosis\n\n\n\n\n\n\nAnd the Frequency Domain Spectral features per axis/channel are:\n\nSpectral Power\nSkewness\nKurtosis\n\n \nFor example, for an FFT length of 32 points, the Spectral Analysis Block‚Äôs resulting output will be 21 features per axis (a total of 63 features).\nThose 63 features will serve as the input tensor for a Neural Network Classifier and the Anomaly Detection model (K-Means).\n\nYou can learn more by digging into the lab DSP Spectral Features",
    "crumbs": [
      "Seeed XIAO ESP32S3",
      "Motion Classification and Anomaly Detection"
    ]
  },
  {
    "objectID": "contents/seeed/xiao_esp32s3/motion_classification/motion_classification.html#sec-motion-classification-anomaly-detection-model-design-d2d4",
    "href": "contents/seeed/xiao_esp32s3/motion_classification/motion_classification.html#sec-motion-classification-anomaly-detection-model-design-d2d4",
    "title": "Motion Classification and Anomaly Detection",
    "section": "",
    "text": "Our classifier will be a Dense Neural Network (DNN) that will have 63 neurons on its input layer, two hidden layers with 20 and 10 neurons, and an output layer with four neurons (one per each class), as shown here:",
    "crumbs": [
      "Seeed XIAO ESP32S3",
      "Motion Classification and Anomaly Detection"
    ]
  },
  {
    "objectID": "contents/seeed/xiao_esp32s3/motion_classification/motion_classification.html#sec-motion-classification-anomaly-detection-impulse-design-18e9",
    "href": "contents/seeed/xiao_esp32s3/motion_classification/motion_classification.html#sec-motion-classification-anomaly-detection-impulse-design-18e9",
    "title": "Motion Classification and Anomaly Detection",
    "section": "",
    "text": "An impulse takes raw data, uses signal processing to extract features, and then uses a learning block (Dense model) to classify new data.\nWe also utilize a second model, the K-means, which can be used for Anomaly Detection. If we imagine that we could have our known classes as clusters, any sample that cannot fit into one of these clusters could be an outlier, an anomaly (for example, a container rolling out of a ship on the ocean or being upside down on the floor).\n \n\nImagine our XIAOML Kit rolling or moving upside-down, on a movement complement different from the one trained on.\n\n \nBelow the final Impulse design:",
    "crumbs": [
      "Seeed XIAO ESP32S3",
      "Motion Classification and Anomaly Detection"
    ]
  },
  {
    "objectID": "contents/seeed/xiao_esp32s3/motion_classification/motion_classification.html#sec-motion-classification-anomaly-detection-generating-features-cf4b",
    "href": "contents/seeed/xiao_esp32s3/motion_classification/motion_classification.html#sec-motion-classification-anomaly-detection-generating-features-cf4b",
    "title": "Motion Classification and Anomaly Detection",
    "section": "",
    "text": "At this point in our project, we have defined the pre-processing method, and the model has been designed. Now, it is time to have the job done. First, let‚Äôs convert the raw data (time-series type) into tabular data. Go to the Spectral Features tab and select [Save Parameters]. Alternatively, instead of using the default values, we can select the [Autotune parameters] button. In this case, the Studio will define new hyperparameters, as the filter design and FFT length, based on the raw data.\n \nAt the top menu, select the Generate features tab, and there, select the options, Calculate feature importance, Normalize features, and press the [Generate features] button. Each 2-second window of data (300 datapoints) will be converted into a single tabular data point with 63 features.\n\nThe Feature Explorer will display this data in 2D using UMAP. Uniform Manifold Approximation and Projection (UMAP) is a dimensionality reduction technique that can be used for visualization, similar to t-SNE, but also for general non-linear dimensionality reduction.\n\nThe visualization enables one to verify that the classes present an excellent separation, indicating that the classifier should perform well.\n \nOptionally, you can analyze the relative importance of each feature for one class compared with other classes.",
    "crumbs": [
      "Seeed XIAO ESP32S3",
      "Motion Classification and Anomaly Detection"
    ]
  },
  {
    "objectID": "contents/seeed/xiao_esp32s3/motion_classification/motion_classification.html#sec-motion-classification-anomaly-detection-training-d832",
    "href": "contents/seeed/xiao_esp32s3/motion_classification/motion_classification.html#sec-motion-classification-anomaly-detection-training-d832",
    "title": "Motion Classification and Anomaly Detection",
    "section": "",
    "text": "Our classifier will be a Dense Neural Network (DNN) that will have 63 neurons on its input layer, two hidden layers with 20 and 10 neurons, and an output layer with four neurons (one per each class), as shown here:\n \nAs hyperparameters, we will use a Learning Rate of 0.005 and 20% of the data for validation for 30 epochs. After training, we can see that the accuracy is 100%.\n \nFor anomaly detection, we should choose the suggested features that are precisely the most important in feature extraction. The number of clusters will be 32, as suggested by the Studio. After training, we can select some data for testing, such as maritime data. The resulting Anomaly score was min: -0.1642, max: 0.0738, avg: -0.0867.\nWhen changing the data, it is possible to realize that small or negative Anomaly Scores indicate that the data are normal.",
    "crumbs": [
      "Seeed XIAO ESP32S3",
      "Motion Classification and Anomaly Detection"
    ]
  },
  {
    "objectID": "contents/seeed/xiao_esp32s3/motion_classification/motion_classification.html#sec-motion-classification-anomaly-detection-testing-c2bb",
    "href": "contents/seeed/xiao_esp32s3/motion_classification/motion_classification.html#sec-motion-classification-anomaly-detection-testing-c2bb",
    "title": "Motion Classification and Anomaly Detection",
    "section": "",
    "text": "Using 20% of the data left behind during the data capture phase, we can verify how our model will behave with unknown data; if not 100% (what is expected), the result was very good (8%).\n \nYou should also use your kit (which is still connected to the Studio) and perform some Live Classification. For example, let‚Äôs test some ‚Äúterrestrial‚Äù movement:\n \n\nBe aware that here, you will capture real data with your device and upload it to the Studio, where an inference will be made using the trained model (note that the model is not on your device).",
    "crumbs": [
      "Seeed XIAO ESP32S3",
      "Motion Classification and Anomaly Detection"
    ]
  },
  {
    "objectID": "contents/seeed/xiao_esp32s3/motion_classification/motion_classification.html#sec-motion-classification-anomaly-detection-deploy-aa7e",
    "href": "contents/seeed/xiao_esp32s3/motion_classification/motion_classification.html#sec-motion-classification-anomaly-detection-deploy-aa7e",
    "title": "Motion Classification and Anomaly Detection",
    "section": "",
    "text": "Now it is time for magic! The Studio will package all the needed libraries, preprocessing functions, and trained models, downloading them to your computer. You should select the Arduino Library option, and then, at the bottom, choose Quantized (Int8) and click [Build]. A ZIP file will be created and downloaded to your computer.\n \nOn your Arduino IDE, go to the Sketch tab, select the option Add.ZIP Library, and Choose the.zip file downloaded by the Studio:",
    "crumbs": [
      "Seeed XIAO ESP32S3",
      "Motion Classification and Anomaly Detection"
    ]
  },
  {
    "objectID": "contents/seeed/xiao_esp32s3/motion_classification/motion_classification.html#sec-motion-classification-anomaly-detection-inference-8351",
    "href": "contents/seeed/xiao_esp32s3/motion_classification/motion_classification.html#sec-motion-classification-anomaly-detection-inference-8351",
    "title": "Motion Classification and Anomaly Detection",
    "section": "",
    "text": "Now, it is time for a real test. We will make inferences that are wholly disconnected from the Studio. Let‚Äôs change one of the code examples created when you deploy the Arduino Library.\nIn your Arduino IDE, go to the File/Examples tab and look for your project, and in examples, select nano_ble_sense_accelerometer:\nOf course, this is not your board, but we can have the code working with only a few changes.\nFor example, at the beginning of the code, you have the library related to Arduino Sense IMU:\n/* Includes -------------------------------------------- */\n#include &lt;XIAOML_Kit_Motion_Class_-_AD_inferencing.h&gt;\n#include &lt;Arduino_LSM9DS1.h&gt;\nChange the ‚Äúincludes‚Äù portion with the code related to the IMU:\n#include &lt;XIAOML_Kit_Motion_Class_-_AD_inferencing.h&gt;\n#include &lt;LSM6DS3.h&gt;\n#include &lt;Wire.h&gt;\nChange the Constant Defines\n// IMU setup\nLSM6DS3 myIMU(I2C_MODE, 0x6A);\n\n// Inference settings\n#define CONVERT_G_TO_MS2    9.81f\n#define MAX_ACCEPTED_RANGE  2.0f * CONVERT_G_TO_MS2\nOn the setup function, initiate the IMU:\n   // Initialize IMU\n    if (myIMU.begin() != 0) {\n        Serial.println(\"ERROR: IMU initialization failed!\");\n        return;\n    }\nAt the loop function, the buffers buffer[ix], buffer[ix + 1], and buffer[ix + 2] will receive the 3-axis data captured by the accelerometer. In the original code, you have the line:\nIMU.readAcceleration(buffer[ix], buffer[ix + 1], buffer[ix + 2]);\nChange it with this block of code:\n// Read IMU data\nfloat x = myIMU.readFloatAccelX();\nfloat y = myIMU.readFloatAccelY();\nfloat z = myIMU.readFloatAccelZ();\nYou should reorder the following two blocks of code. First, you make the conversion to raw data to ‚ÄúMeters per squared second (m/s2)‚Äù, followed by the test regarding the maximum acceptance range (that here is in m/s2, but on Arduino, was in Gs):\n  // Convert to m/s¬≤\n  buffer[i + 0] = x * CONVERT_G_TO_MS2;\n  buffer[i + 1] = y * CONVERT_G_TO_MS2;\n  buffer[i + 2] = z * CONVERT_G_TO_MS2;\n\n  // Apply range limiting\n  for (int j = 0; j &lt; 3; j++) {\n      if (fabs(buffer[i + j]) &gt; MAX_ACCEPTED_RANGE) {\n          buffer[i + j] = copysign(MAX_ACCEPTED_RANGE, buffer[i + j]);\n      }\n  }\nAnd this is enough. We can also adjust how the inference is displayed in the Serial Monitor. You can now upload the complete code below to your device and proceed with the inferences.\n// Motion Classification with LSM6DS3TR-C IMU\n#include &lt;XIAOML_Kit_Motion_Class_-_AD_inferencing.h&gt;\n#include &lt;LSM6DS3.h&gt;\n#include &lt;Wire.h&gt;\n\n// IMU setup\nLSM6DS3 myIMU(I2C_MODE, 0x6A);\n\n// Inference settings\n#define CONVERT_G_TO_MS2    9.81f\n#define MAX_ACCEPTED_RANGE  2.0f * CONVERT_G_TO_MS2\n\nstatic bool debug_nn = false;\nstatic float buffer[EI_CLASSIFIER_DSP_INPUT_FRAME_SIZE] = { 0 };\nstatic float inference_buffer[EI_CLASSIFIER_DSP_INPUT_FRAME_SIZE];\n\nvoid setup() {\n    Serial.begin(115200);\n    while (!Serial) delay(10);\n\n    Serial.println(\"XIAOML Kit - Motion Classification\");\n    Serial.println(\"LSM6DS3TR-C IMU Inference\");\n\n    // Initialize IMU\n    if (myIMU.begin() != 0) {\n        Serial.println(\"ERROR: IMU initialization failed!\");\n        return;\n    }\n\n    Serial.println(\"‚úì IMU initialized\");\n\n    if (EI_CLASSIFIER_RAW_SAMPLES_PER_FRAME != 3) {\n        Serial.println(\"ERROR: EI_CLASSIFIER_RAW_SAMPLES_PER_FRAME\"\n                       \"should be 3\");\n        return;\n    }\n\n    Serial.println(\"‚úì Model loaded\");\n    Serial.println(\"Starting motion classification...\");\n}\n\nvoid loop() {\n    ei_printf(\"\\nStarting inferencing in 2 seconds...\\n\");\n    delay(2000);\n\n    ei_printf(\"Sampling...\\n\");\n\n    // Clear buffer\n    for (size_t i = 0; i &lt; EI_CLASSIFIER_DSP_INPUT_FRAME_SIZE; i++) {\n        buffer[i] = 0.0f;\n    }\n\n    // Collect accelerometer data\n    for (int i = 0; i &lt; EI_CLASSIFIER_DSP_INPUT_FRAME_SIZE; i += 3) {\n        uint64_t next_tick = micros() +\n          (EI_CLASSIFIER_INTERVAL_MS * 1000);\n\n        // Read IMU data\n        float x = myIMU.readFloatAccelX();\n        float y = myIMU.readFloatAccelY();\n        float z = myIMU.readFloatAccelZ();\n\n        // Convert to m/s¬≤\n        buffer[i + 0] = x * CONVERT_G_TO_MS2;\n        buffer[i + 1] = y * CONVERT_G_TO_MS2;\n        buffer[i + 2] = z * CONVERT_G_TO_MS2;\n\n        // Apply range limiting\n        for (int j = 0; j &lt; 3; j++) {\n            if (fabs(buffer[i + j]) &gt; MAX_ACCEPTED_RANGE) {\n                buffer[i + j] = copysign(MAX_ACCEPTED_RANGE,\n                                         buffer[i + j]);\n            }\n        }\n\n        delayMicroseconds(next_tick - micros());\n    }\n\n    // Copy to inference buffer\n    for (int i = 0; i &lt; EI_CLASSIFIER_DSP_INPUT_FRAME_SIZE; i++) {\n        inference_buffer[i] = buffer[i];\n    }\n\n    // Create signal from buffer\n    signal_t signal;\n    int err = numpy::signal_from_buffer(inference_buffer,\n              EI_CLASSIFIER_DSP_INPUT_FRAME_SIZE, &signal);\n    if (err != 0) {\n        ei_printf(\"ERROR: Failed to create signal from buffer (%d)\\n\",\n                  err);\n        return;\n    }\n\n    // Run the classifier\n    ei_impulse_result_t result = { 0 };\n    err = run_classifier(&signal, &result, debug_nn);\n    if (err != EI_IMPULSE_OK) {\n        ei_printf(\"ERROR: Failed to run classifier (%d)\\n\", err);\n        return;\n    }\n\n    // Print predictions\n    ei_printf(\"Predictions (DSP: %d ms, Classification: %d ms, \"\n              \"Anomaly: %d ms):\\n\",\n        result.timing.dsp, result.timing.classification, result.timing.anomaly);\n\n    for (size_t ix = 0; ix &lt; EI_CLASSIFIER_LABEL_COUNT; ix++) {\n        ei_printf(\"    %s: %.5f\\n\", result.classification[ix].label,\n                  result.classification[ix].value);\n    }\n\n    // Print anomaly score\n#if EI_CLASSIFIER_HAS_ANOMALY == 1\n    ei_printf(\"Anomaly score: %.3f\\n\", result.anomaly);\n#endif\n\n    // Determine prediction\n    float max_confidence = 0.0;\n    String predicted_class = \"unknown\";\n\n    for (size_t ix = 0; ix &lt; EI_CLASSIFIER_LABEL_COUNT; ix++) {\n        if (result.classification[ix].value &gt; max_confidence) {\n            max_confidence = result.classification[ix].value;\n            predicted_class = String(result.classification[ix].label);\n        }\n    }\n\n    // Display result with confidence threshold\n    if (max_confidence &gt; 0.6) {\n        ei_printf(\"\\nüéØ PREDICTION: %s (%.1f%% confidence)\\n\",\n                 predicted_class.c_str(), max_confidence * 100);\n    } else {\n        ei_printf(\"\\n‚ùì UNCERTAIN: Highest confidence is %s (%.1f%%)\\n\",\n                 predicted_class.c_str(), max_confidence * 100);\n    }\n\n    // Check for anomaly\n#if EI_CLASSIFIER_HAS_ANOMALY == 1\n    if (result.anomaly &gt; 0.5) {\n        ei_printf(\"‚ö†Ô∏è ANOMALY DETECTED! Score: %.3f\\n\", result.anomaly);\n    }\n#endif\n\n    delay(1000);\n}\n\nvoid ei_printf(const char *format, ...) {\n    static char print_buf[1024] = { 0 };\n    va_list args;\n    va_start(args, format);\n    int r = vsnprintf(print_buf, sizeof(print_buf), format, args);\n    va_end(args);\n    if (r &gt; 0) {\n        Serial.write(print_buf);\n    }\n}\n\nThe complete code is available on the Lab‚Äôs GitHub.\n\nNow you should try your movements, seeing the result of the inference of each class on the images:\n \n \n \n \nAnd, of course, some ‚Äúanomaly‚Äù, for example, putting the XIAO upside-down. The anomaly score will be over 0.5:",
    "crumbs": [
      "Seeed XIAO ESP32S3",
      "Motion Classification and Anomaly Detection"
    ]
  },
  {
    "objectID": "contents/seeed/xiao_esp32s3/motion_classification/motion_classification.html#sec-motion-classification-anomaly-detection-postprossessing-ef66",
    "href": "contents/seeed/xiao_esp32s3/motion_classification/motion_classification.html#sec-motion-classification-anomaly-detection-postprossessing-ef66",
    "title": "Motion Classification and Anomaly Detection",
    "section": "",
    "text": "Now that we know the model is working, we suggest modifying the code to see the result with the Kit completely offline (disconnected from the PC and powered by a battery, a power bank, or an independent 5V power supply).\nThe idea is that if a specific movement is detected, a corresponding message will appear on the OLED display.\n\nThe modified inference code to have the OLED display is available on the Lab‚Äôs GitHub.",
    "crumbs": [
      "Seeed XIAO ESP32S3",
      "Motion Classification and Anomaly Detection"
    ]
  },
  {
    "objectID": "contents/seeed/xiao_esp32s3/motion_classification/motion_classification.html#sec-motion-classification-anomaly-detection-summary-35d1",
    "href": "contents/seeed/xiao_esp32s3/motion_classification/motion_classification.html#sec-motion-classification-anomaly-detection-summary-35d1",
    "title": "Motion Classification and Anomaly Detection",
    "section": "",
    "text": "This lab demonstrated how to build a complete motion classification system using the XIAOML Kit‚Äôs built-in LSM6DS3TR-C IMU sensor. Key achievements include:\nTechnical Implementation:\n\nUtilized the integrated 6-axis IMU for motion sensing\nCollected labeled training data for four transportation scenarios\nImplemented spectral feature extraction for time-series analysis\nDeployed a neural network classifier optimized for microcontroller inference\nAdded anomaly detection for identifying unusual movements\n\nMachine Learning Pipeline:\n\nData collection directly from embedded sensors\nFeature engineering using frequency domain analysis\nModel training and optimization in Edge Impulse\nReal-time inference on resource-constrained hardware\nPerformance monitoring and validation\n\nPractical Applications: The techniques learned apply directly to real-world scenarios, including:\n\nAsset tracking and logistics monitoring\nPredictive maintenance for machinery\nHuman activity recognition\nVehicle and equipment monitoring\nIoT sensor networks for smart cities\n\nKey Learnings:\n\nWorking with IMU coordinate systems and sensor fusion\nBalancing model accuracy with inference speed on edge devices\nImplementing robust data collection and preprocessing pipelines\nDeploying machine learning models to embedded systems\nIntegrating multiple sensors (IMU + display) for complete solutions\n\nThe integration of motion classification with the XIAOML Kit demonstrates how modern embedded systems can perform sophisticated AI tasks locally, enabling real-time decision-making without reliance on the cloud. This approach is fundamental to the future of edge AI in industrial IoT, autonomous systems, and smart device applications.",
    "crumbs": [
      "Seeed XIAO ESP32S3",
      "Motion Classification and Anomaly Detection"
    ]
  },
  {
    "objectID": "contents/seeed/xiao_esp32s3/motion_classification/motion_classification.html#sec-motion-classification-anomaly-detection-resources-cd54",
    "href": "contents/seeed/xiao_esp32s3/motion_classification/motion_classification.html#sec-motion-classification-anomaly-detection-resources-cd54",
    "title": "Motion Classification and Anomaly Detection",
    "section": "",
    "text": "XIAOML KIT Code\nDSP Spectral Features\nEdge Impulse Project\nEdge Impulse Spectral Features Block Colab Notebook\nEdge Impulse Documentation\nEdge Impulse Spectral Features\nSeeed Studio LSM6DS3 Library",
    "crumbs": [
      "Seeed XIAO ESP32S3",
      "Motion Classification and Anomaly Detection"
    ]
  },
  {
    "objectID": "contents/seeed/xiao_esp32s3/object_detection/object_detection.html",
    "href": "contents/seeed/xiao_esp32s3/object_detection/object_detection.html",
    "title": "Object Detection",
    "section": "",
    "text": "DALL¬∑E prompt - Cartoon styled after 1950s animations, showing a detailed board with sensors, particularly a camera, on a table with patterned cloth. Behind the board, a computer with a large back showcases the Arduino IDE. The IDE‚Äôs content hints at LED pin assignments and machine learning inference for detecting spoken commands. The Serial Monitor, in a distinct window, reveals outputs for the commands ‚Äòyes‚Äô and ‚Äòno‚Äô.\n\n\n\n\nIn the last section regarding Computer Vision (CV) and the XIAO ESP32S3, Image Classification, we learned how to set up and classify images with this remarkable development board. Continuing our CV journey, we will explore Object Detection on microcontrollers.\n\n\nThe main task with Image Classification models is to identify the most probable object category present on an image, for example, to classify between a cat or a dog, dominant ‚Äúobjects‚Äù in an image:\n \nBut what happens if there is no dominant category in the image?\n \nAn image classification model identifies the above image utterly wrong as an ‚Äúashcan,‚Äù possibly due to the color tonalities.\n\nThe model used in the previous images is MobileNet, which is trained with a large dataset, ImageNet, running on a Raspberry Pi.\n\nTo solve this issue, we need another type of model, where not only multiple categories (or labels) can be found but also where the objects are located on a given image.\nAs we can imagine, such models are much more complicated and bigger, for example, the MobileNetV2 SSD FPN-Lite 320x320, trained with the COCO dataset. This pre-trained object detection model is designed to locate up to 10 objects within an image, outputting a bounding box for each object detected. The below image is the result of such a model running on a Raspberry Pi:\n \nThose models used for object detection (such as the MobileNet SSD or YOLO) usually have several MB in size, which is OK for use with Raspberry Pi but unsuitable for use with embedded devices, where the RAM usually has, at most, a few MB as in the case of the XIAO ESP32S3.\n\n\n\nEdge Impulse launched in 2022, FOMO (Faster Objects, More Objects), a novel solution to perform object detection on embedded devices, such as the Nicla Vision and Portenta (Cortex M7), on Cortex M4F CPUs (Arduino Nano33 and OpenMV M4 series) as well the Espressif ESP32 devices (ESP-CAM, ESP-EYE and XIAO ESP32S3 Sense).\nIn this Hands-On project, we will explore Object Detection using FOMO.\n\nTo understand more about FOMO, you can go into the official FOMO announcement by Edge Impulse, where Louis Moreau and Mat Kelcey explain in detail how it works.\n\n\n\n\n\nAll Machine Learning projects need to start with a detailed goal. Let‚Äôs assume we are in an industrial or rural facility and must sort and count oranges (fruits) and particular frogs (bugs).\n \nIn other words, we should perform a multi-label classification, where each image can have three classes:\n\nBackground (No objects)\nFruit\nBug\n\nHere are some not labeled image samples that we should use to detect the objects (fruits and bugs):\n \nWe are interested in which object is in the image, its location (centroid), and how many we can find on it. The object‚Äôs size is not detected with FOMO, as with MobileNet SSD or YOLO, where the Bounding Box is one of the model outputs.\nWe will develop the project using the XIAO ESP32S3 for image capture and model inference. The ML project will be developed using the Edge Impulse Studio. But before starting the object detection project in the Studio, let‚Äôs create a raw dataset (not labeled) with images that contain the objects to be detected.\n\n\n\nYou can capture images using the XIAO, your phone, or other devices. Here, we will use the XIAO with code from the Arduino IDE ESP32 library.\n\n\nOpen the Arduino IDE and select the XIAO_ESP32S3 board (and the port where it is connected). On File &gt; Examples &gt; ESP32 &gt; Camera, select CameraWebServer.\nOn the BOARDS MANAGER panel, confirm that you have installed the latest ‚Äústable‚Äù package.\n\n‚ö†Ô∏è Attention\nAlpha versions (for example, 3.x-alpha) do not work correctly with the XIAO and Edge Impulse. Use the last stable version (for example, 2.0.11) instead.\n\nYou also should comment on all cameras‚Äô models, except the XIAO model pins:\n#define CAMERA_MODEL_XIAO_ESP32S3 // Has PSRAM\nAnd on Tools, enable the PSRAM. Enter your wifi credentials and upload the code to the device:\n \nIf the code is executed correctly, you should see the address on the Serial Monitor:\n \nCopy the address on your browser and wait for the page to be uploaded. Select the camera resolution (for example, QVGA) and select [START STREAM]. Wait for a few seconds/minutes, depending on your connection. You can save an image on your computer download area using the [Save] button.\n \nEdge impulse suggests that the objects should be similar in size and not overlapping for better performance. This is OK in an industrial facility, where the camera should be fixed, keeping the same distance from the objects to be detected. Despite that, we will also try using mixed sizes and positions to see the result.\n\nWe do not need to create separate folders for our images because each contains multiple labels.\n\nWe suggest using around 50 images to mix the objects and vary the number of each appearing on the scene. Try to capture different angles, backgrounds, and light conditions.\n\nThe stored images use a QVGA frame size of \\(320\\times 240\\) and RGB565 (color pixel format).\n\nAfter capturing your dataset, [Stop Stream] and move your images to a folder.\n\n\n\n\n\n\nGo to Edge Impulse Studio, enter your credentials at Login (or create an account), and start a new project.\n \n\nHere, you can clone the project developed for this hands-on: XIAO-ESP32S3-Sense-Object_Detection\n\nOn your Project Dashboard, go down and on Project info and select Bounding boxes (object detection) and Espressif ESP-EYE (most similar to our board) as your Target Device:\n \n\n\n\nOn Studio, go to the Data acquisition tab, and on the UPLOAD DATA section, upload files captured as a folder from your computer.\n \n\nYou can leave for the Studio to split your data automatically between Train and Test or do it manually. We will upload all of them as training.\n\n \nAll the not-labeled images (47) were uploaded but must be labeled appropriately before being used as a project dataset. The Studio has a tool for that purpose, which you can find in the link Labeling queue (47).\nThere are two ways you can use to perform AI-assisted labeling on the Edge Impulse Studio (free version):\n\nUsing yolov5\nTracking objects between frames\n\n\nEdge Impulse launched an auto-labeling feature for Enterprise customers, easing labeling tasks in object detection projects.\n\nOrdinary objects can quickly be identified and labeled using an existing library of pre-trained object detection models from YOLOv5 (trained with the COCO dataset). But since, in our case, the objects are not part of COCO datasets, we should select the option of tracking objects. With this option, once you draw bounding boxes and label the images in one frame, the objects will be tracked automatically from frame to frame, partially labeling the new ones (not all are correctly labeled).\n\nYou can use the EI uploader to import your data if you already have a labeled dataset containing bounding boxes.\n\n\n\n\nStarting with the first image of your unlabeled data, use your mouse to drag a box around an object to add a label. Then click Save labels to advance to the next item.\n \nContinue with this process until the queue is empty. At the end, all images should have the objects labeled as those samples below:\n \nNext, review the labeled samples on the Data acquisition tab. If one of the labels is wrong, you can edit it using the three dots menu after the sample name:\n \nYou will be guided to replace the wrong label and correct the dataset.\n \n\n\n\nAfter labeling all data, it was realized that the class fruit had many more samples than the bug. So, 11 new and additional bug images were collected (ending with 58 images). After labeling them, it is time to select some images and move them to the test dataset. You can do it using the three-dot menu after the image name. I selected six images, representing 13% of the total dataset.\n \n\n\n\n\nIn this phase, you should define how to:\n\nPre-processing consists of resizing the individual images from \\(320\\times 240\\) to \\(96\\times 96\\) and squashing them (squared form, without cropping). Afterward, the images are converted from RGB to Grayscale.\nDesign a Model, in this case, ‚ÄúObject Detection.‚Äù\n\n \n\n\nIn this section, select Color depth as Grayscale, suitable for use with FOMO models and Save parameters.\n \nThe Studio moves automatically to the next section, Generate features, where all samples will be pre-processed, resulting in a dataset with individual \\(96\\times 96\\times 1\\) images or 9,216 features.\n \nThe feature explorer shows that all samples evidence a good separation after the feature generation.\n\nSome samples seem to be in the wrong space, but clicking on them confirms the correct labeling.\n\n\n\n\n\nWe will use FOMO, an object detection model based on MobileNetV2 (alpha 0.35) designed to coarsely segment an image into a grid of background vs objects of interest (here, boxes and wheels).\nFOMO is an innovative machine learning model for object detection, which can use up to 30 times less energy and memory than traditional models like Mobilenet SSD and YOLOv5. FOMO can operate on microcontrollers with less than 200 KB of RAM. The main reason this is possible is that while other models calculate the object‚Äôs size by drawing a square around it (bounding box), FOMO ignores the size of the image, providing only the information about where the object is located in the image through its centroid coordinates.\n\n\nFOMO takes the image in grayscale and divides it into blocks of pixels using a factor of 8. For the input of \\(96\\times 96\\), the grid would be \\(12\\times 12\\) \\((96/8=12)\\). Next, FOMO will run a classifier through each pixel block to calculate the probability that there is a box or a wheel in each of them and, subsequently, determine the regions that have the highest probability of containing the object (If a pixel block has no objects, it will be classified as background). From the overlap of the final region, the FOMO provides the coordinates (related to the image dimensions) of the centroid of this region.\n \nFor training, we should select a pre-trained model. Let‚Äôs use the FOMO (Faster Objects, More Objects) MobileNetV2 0.35. This model uses around 250 KB of RAM and 80 KB of ROM (Flash), which suits well with our board.\n \nRegarding the training hyper-parameters, the model will be trained with:\n\nEpochs: 60\nBatch size: 32\nLearning Rate: 0.001.\n\nFor validation during training, 20% of the dataset (validation_dataset) will be spared. For the remaining 80% (train_dataset), we will apply Data Augmentation, which will randomly flip, change the size and brightness of the image, and crop them, artificially increasing the number of samples on the dataset for training.\nAs a result, the model ends with an overall F1 score of 85%, similar to the result when using the test data (83%).\n\nNote that FOMO automatically added a 3rd label background to the two previously defined (box and wheel).\n\n \n\nIn object detection tasks, accuracy is generally not the primary evaluation metric. Object detection involves classifying objects and providing bounding boxes around them, making it a more complex problem than simple classification. The issue is that we do not have the bounding box, only the centroids. In short, using accuracy as a metric could be misleading and may not provide a complete understanding of how well the model is performing. Because of that, we will use the F1 score.\n\n\n\n\nOnce our model is trained, we can test it using the Live Classification tool. On the correspondent section, click on Connect a development board icon (a small MCU) and scan the QR code with your phone.\n \nOnce connected, you can use the smartphone to capture actual images to be tested by the trained model on Edge Impulse Studio.\n \nOne thing to be noted is that the model can produce false positives and negatives. This can be minimized by defining a proper Confidence Threshold (use the Three dots menu for the setup). Try with 0.8 or more.\n\n\n\n\nSelect the Arduino Library and Quantized (int8) model, enable the EON Compiler on the Deploy Tab, and press [Build].\n \nOpen your Arduino IDE, and under Sketch, go to Include Library and add.ZIP Library. Select the file you download from Edge Impulse Studio, and that‚Äôs it!\n \nUnder the Examples tab on Arduino IDE, you should find a sketch code (esp32 &gt; esp32_camera) under your project name.\n \nYou should change lines 32 to 75, which define the camera model and pins, using the data related to our model. Copy and paste the below lines, replacing the lines 32-75:\n#define PWDN_GPIO_NUM     -1\n#define RESET_GPIO_NUM    -1\n#define XCLK_GPIO_NUM     10\n#define SIOD_GPIO_NUM     40\n#define SIOC_GPIO_NUM     39\n#define Y9_GPIO_NUM       48\n#define Y8_GPIO_NUM       11\n#define Y7_GPIO_NUM       12\n#define Y6_GPIO_NUM       14\n#define Y5_GPIO_NUM       16\n#define Y4_GPIO_NUM       18\n#define Y3_GPIO_NUM       17\n#define Y2_GPIO_NUM       15\n#define VSYNC_GPIO_NUM    38\n#define HREF_GPIO_NUM     47\n#define PCLK_GPIO_NUM     13\nHere you can see the resulting code:\n \nUpload the code to your XIAO ESP32S3 Sense, and you should be OK to start detecting fruits and bugs. You can check the result on Serial Monitor.\n\n\n \n\n\n\n \n\n\n\n \nNote that the model latency is 143 ms, and the frame rate per second is around 7 fps (similar to what we got with the Image Classification project). This happens because FOMO is cleverly built over a CNN model, not with an object detection model like the SSD MobileNet. For example, when running a MobileNetV2 SSD FPN-Lite \\(320\\times 320\\) model on a Raspberry Pi 4, the latency is around five times higher (around 1.5 fps).\n\n\n\n\nAs discussed in the Image Classification chapter, verifying inference with Image models on Arduino IDE is very challenging because we can not see what the camera focuses on. Again, let‚Äôs use the SenseCraft-Web Toolkit.\nFollow the following steps to start the SenseCraft-Web-Toolkit:\n\nOpen the SenseCraft-Web-Toolkit website.\nConnect the XIAO to your computer:\n\n\nHaving the XIAO connected, select it as below:\n\n \n\nSelect the device/Port and press [Connect]:\n\n \n\nYou can try several Computer Vision models previously uploaded by Seeed Studio. Try them and have fun!\n\nIn our case, we will use the blue button at the bottom of the page: [Upload Custom AI Model].\nBut first, we must download from Edge Impulse Studio our quantized .tflite model.\n\nGo to your project at Edge Impulse Studio, or clone this one:\n\n\nXIAO-ESP32S3-CAM-Fruits-vs-Veggies-v1-ESP-NN\n\n\nOn Dashboard, download the model (‚Äúblock output‚Äù): Object Detection model - TensorFlow Lite (int8 quantized)\n\n \n\nOn SenseCraft-Web-Toolkit, use the blue button at the bottom of the page: [Upload Custom AI Model]. A window will pop up. Enter the Model file that you downloaded to your computer from Edge Impulse Studio, choose a Model Name, and enter with labels (ID: Object):\n\n \n\nNote that you should use the labels trained on EI Studio and enter them in alphabetic order (in our case, background, bug, fruit).\n\nAfter a few seconds (or minutes), the model will be uploaded to your device, and the camera image will appear in real-time on the Preview Sector:\n \nThe detected objects will be marked (the centroid). You can select the Confidence of your inference cursor Confidence and IoU, which is used to assess the accuracy of predicted bounding boxes compared to truth bounding boxes.\nClicking on the top button (Device Log), you can open a Serial Monitor to follow the inference, as we did with the Arduino IDE.\n \nOn Device Log, you will get information as:\n\nPreprocess time (image capture and Crop): 3 ms,\nInference time (model latency): 115 ms,\nPostprocess time (display of the image and marking objects): 1 ms.\nOutput tensor (boxes), for example, one of the boxes: [[30,150, 20, 20,97, 2]]; where 30,150, 20, 20 are the coordinates of the box (around the centroid); 97 is the inference result, and 2 is the class (in this case 2: fruit).\n\n\nNote that in the above example, we got 5 boxes because none of the fruits got 3 centroids. One solution will be post-processing, where we can aggregate close centroids in one.\n\nHere are other screenshots:\n \n\n\n\nFOMO is a significant leap in the image processing space, as Louis Moreau and Mat Kelcey put it during its launch in 2022:\n\nFOMO is a ground-breaking algorithm that brings real-time object detection, tracking, and counting to microcontrollers for the first time.\n\nMultiple possibilities exist for exploring object detection (and, more precisely, counting them) on embedded devices.\n\n\n\n\nEdge Impulse Project",
    "crumbs": [
      "Seeed XIAO ESP32S3",
      "Object Detection"
    ]
  },
  {
    "objectID": "contents/seeed/xiao_esp32s3/object_detection/object_detection.html#sec-object-detection-overview-d035",
    "href": "contents/seeed/xiao_esp32s3/object_detection/object_detection.html#sec-object-detection-overview-d035",
    "title": "Object Detection",
    "section": "",
    "text": "In the last section regarding Computer Vision (CV) and the XIAO ESP32S3, Image Classification, we learned how to set up and classify images with this remarkable development board. Continuing our CV journey, we will explore Object Detection on microcontrollers.\n\n\nThe main task with Image Classification models is to identify the most probable object category present on an image, for example, to classify between a cat or a dog, dominant ‚Äúobjects‚Äù in an image:\n \nBut what happens if there is no dominant category in the image?\n \nAn image classification model identifies the above image utterly wrong as an ‚Äúashcan,‚Äù possibly due to the color tonalities.\n\nThe model used in the previous images is MobileNet, which is trained with a large dataset, ImageNet, running on a Raspberry Pi.\n\nTo solve this issue, we need another type of model, where not only multiple categories (or labels) can be found but also where the objects are located on a given image.\nAs we can imagine, such models are much more complicated and bigger, for example, the MobileNetV2 SSD FPN-Lite 320x320, trained with the COCO dataset. This pre-trained object detection model is designed to locate up to 10 objects within an image, outputting a bounding box for each object detected. The below image is the result of such a model running on a Raspberry Pi:\n \nThose models used for object detection (such as the MobileNet SSD or YOLO) usually have several MB in size, which is OK for use with Raspberry Pi but unsuitable for use with embedded devices, where the RAM usually has, at most, a few MB as in the case of the XIAO ESP32S3.\n\n\n\nEdge Impulse launched in 2022, FOMO (Faster Objects, More Objects), a novel solution to perform object detection on embedded devices, such as the Nicla Vision and Portenta (Cortex M7), on Cortex M4F CPUs (Arduino Nano33 and OpenMV M4 series) as well the Espressif ESP32 devices (ESP-CAM, ESP-EYE and XIAO ESP32S3 Sense).\nIn this Hands-On project, we will explore Object Detection using FOMO.\n\nTo understand more about FOMO, you can go into the official FOMO announcement by Edge Impulse, where Louis Moreau and Mat Kelcey explain in detail how it works.",
    "crumbs": [
      "Seeed XIAO ESP32S3",
      "Object Detection"
    ]
  },
  {
    "objectID": "contents/seeed/xiao_esp32s3/object_detection/object_detection.html#sec-object-detection-object-detection-project-goal-d1c5",
    "href": "contents/seeed/xiao_esp32s3/object_detection/object_detection.html#sec-object-detection-object-detection-project-goal-d1c5",
    "title": "Object Detection",
    "section": "",
    "text": "All Machine Learning projects need to start with a detailed goal. Let‚Äôs assume we are in an industrial or rural facility and must sort and count oranges (fruits) and particular frogs (bugs).\n \nIn other words, we should perform a multi-label classification, where each image can have three classes:\n\nBackground (No objects)\nFruit\nBug\n\nHere are some not labeled image samples that we should use to detect the objects (fruits and bugs):\n \nWe are interested in which object is in the image, its location (centroid), and how many we can find on it. The object‚Äôs size is not detected with FOMO, as with MobileNet SSD or YOLO, where the Bounding Box is one of the model outputs.\nWe will develop the project using the XIAO ESP32S3 for image capture and model inference. The ML project will be developed using the Edge Impulse Studio. But before starting the object detection project in the Studio, let‚Äôs create a raw dataset (not labeled) with images that contain the objects to be detected.",
    "crumbs": [
      "Seeed XIAO ESP32S3",
      "Object Detection"
    ]
  },
  {
    "objectID": "contents/seeed/xiao_esp32s3/object_detection/object_detection.html#sec-object-detection-data-collection-897c",
    "href": "contents/seeed/xiao_esp32s3/object_detection/object_detection.html#sec-object-detection-data-collection-897c",
    "title": "Object Detection",
    "section": "",
    "text": "You can capture images using the XIAO, your phone, or other devices. Here, we will use the XIAO with code from the Arduino IDE ESP32 library.\n\n\nOpen the Arduino IDE and select the XIAO_ESP32S3 board (and the port where it is connected). On File &gt; Examples &gt; ESP32 &gt; Camera, select CameraWebServer.\nOn the BOARDS MANAGER panel, confirm that you have installed the latest ‚Äústable‚Äù package.\n\n‚ö†Ô∏è Attention\nAlpha versions (for example, 3.x-alpha) do not work correctly with the XIAO and Edge Impulse. Use the last stable version (for example, 2.0.11) instead.\n\nYou also should comment on all cameras‚Äô models, except the XIAO model pins:\n#define CAMERA_MODEL_XIAO_ESP32S3 // Has PSRAM\nAnd on Tools, enable the PSRAM. Enter your wifi credentials and upload the code to the device:\n \nIf the code is executed correctly, you should see the address on the Serial Monitor:\n \nCopy the address on your browser and wait for the page to be uploaded. Select the camera resolution (for example, QVGA) and select [START STREAM]. Wait for a few seconds/minutes, depending on your connection. You can save an image on your computer download area using the [Save] button.\n \nEdge impulse suggests that the objects should be similar in size and not overlapping for better performance. This is OK in an industrial facility, where the camera should be fixed, keeping the same distance from the objects to be detected. Despite that, we will also try using mixed sizes and positions to see the result.\n\nWe do not need to create separate folders for our images because each contains multiple labels.\n\nWe suggest using around 50 images to mix the objects and vary the number of each appearing on the scene. Try to capture different angles, backgrounds, and light conditions.\n\nThe stored images use a QVGA frame size of \\(320\\times 240\\) and RGB565 (color pixel format).\n\nAfter capturing your dataset, [Stop Stream] and move your images to a folder.",
    "crumbs": [
      "Seeed XIAO ESP32S3",
      "Object Detection"
    ]
  },
  {
    "objectID": "contents/seeed/xiao_esp32s3/object_detection/object_detection.html#sec-object-detection-edge-impulse-studio-e39e",
    "href": "contents/seeed/xiao_esp32s3/object_detection/object_detection.html#sec-object-detection-edge-impulse-studio-e39e",
    "title": "Object Detection",
    "section": "",
    "text": "Go to Edge Impulse Studio, enter your credentials at Login (or create an account), and start a new project.\n \n\nHere, you can clone the project developed for this hands-on: XIAO-ESP32S3-Sense-Object_Detection\n\nOn your Project Dashboard, go down and on Project info and select Bounding boxes (object detection) and Espressif ESP-EYE (most similar to our board) as your Target Device:\n \n\n\n\nOn Studio, go to the Data acquisition tab, and on the UPLOAD DATA section, upload files captured as a folder from your computer.\n \n\nYou can leave for the Studio to split your data automatically between Train and Test or do it manually. We will upload all of them as training.\n\n \nAll the not-labeled images (47) were uploaded but must be labeled appropriately before being used as a project dataset. The Studio has a tool for that purpose, which you can find in the link Labeling queue (47).\nThere are two ways you can use to perform AI-assisted labeling on the Edge Impulse Studio (free version):\n\nUsing yolov5\nTracking objects between frames\n\n\nEdge Impulse launched an auto-labeling feature for Enterprise customers, easing labeling tasks in object detection projects.\n\nOrdinary objects can quickly be identified and labeled using an existing library of pre-trained object detection models from YOLOv5 (trained with the COCO dataset). But since, in our case, the objects are not part of COCO datasets, we should select the option of tracking objects. With this option, once you draw bounding boxes and label the images in one frame, the objects will be tracked automatically from frame to frame, partially labeling the new ones (not all are correctly labeled).\n\nYou can use the EI uploader to import your data if you already have a labeled dataset containing bounding boxes.\n\n\n\n\nStarting with the first image of your unlabeled data, use your mouse to drag a box around an object to add a label. Then click Save labels to advance to the next item.\n \nContinue with this process until the queue is empty. At the end, all images should have the objects labeled as those samples below:\n \nNext, review the labeled samples on the Data acquisition tab. If one of the labels is wrong, you can edit it using the three dots menu after the sample name:\n \nYou will be guided to replace the wrong label and correct the dataset.\n \n\n\n\nAfter labeling all data, it was realized that the class fruit had many more samples than the bug. So, 11 new and additional bug images were collected (ending with 58 images). After labeling them, it is time to select some images and move them to the test dataset. You can do it using the three-dot menu after the image name. I selected six images, representing 13% of the total dataset.",
    "crumbs": [
      "Seeed XIAO ESP32S3",
      "Object Detection"
    ]
  },
  {
    "objectID": "contents/seeed/xiao_esp32s3/object_detection/object_detection.html#sec-object-detection-impulse-design-f04a",
    "href": "contents/seeed/xiao_esp32s3/object_detection/object_detection.html#sec-object-detection-impulse-design-f04a",
    "title": "Object Detection",
    "section": "",
    "text": "In this phase, you should define how to:\n\nPre-processing consists of resizing the individual images from \\(320\\times 240\\) to \\(96\\times 96\\) and squashing them (squared form, without cropping). Afterward, the images are converted from RGB to Grayscale.\nDesign a Model, in this case, ‚ÄúObject Detection.‚Äù\n\n \n\n\nIn this section, select Color depth as Grayscale, suitable for use with FOMO models and Save parameters.\n \nThe Studio moves automatically to the next section, Generate features, where all samples will be pre-processed, resulting in a dataset with individual \\(96\\times 96\\times 1\\) images or 9,216 features.\n \nThe feature explorer shows that all samples evidence a good separation after the feature generation.\n\nSome samples seem to be in the wrong space, but clicking on them confirms the correct labeling.",
    "crumbs": [
      "Seeed XIAO ESP32S3",
      "Object Detection"
    ]
  },
  {
    "objectID": "contents/seeed/xiao_esp32s3/object_detection/object_detection.html#sec-object-detection-model-design-training-test-6104",
    "href": "contents/seeed/xiao_esp32s3/object_detection/object_detection.html#sec-object-detection-model-design-training-test-6104",
    "title": "Object Detection",
    "section": "",
    "text": "We will use FOMO, an object detection model based on MobileNetV2 (alpha 0.35) designed to coarsely segment an image into a grid of background vs objects of interest (here, boxes and wheels).\nFOMO is an innovative machine learning model for object detection, which can use up to 30 times less energy and memory than traditional models like Mobilenet SSD and YOLOv5. FOMO can operate on microcontrollers with less than 200 KB of RAM. The main reason this is possible is that while other models calculate the object‚Äôs size by drawing a square around it (bounding box), FOMO ignores the size of the image, providing only the information about where the object is located in the image through its centroid coordinates.\n\n\nFOMO takes the image in grayscale and divides it into blocks of pixels using a factor of 8. For the input of \\(96\\times 96\\), the grid would be \\(12\\times 12\\) \\((96/8=12)\\). Next, FOMO will run a classifier through each pixel block to calculate the probability that there is a box or a wheel in each of them and, subsequently, determine the regions that have the highest probability of containing the object (If a pixel block has no objects, it will be classified as background). From the overlap of the final region, the FOMO provides the coordinates (related to the image dimensions) of the centroid of this region.\n \nFor training, we should select a pre-trained model. Let‚Äôs use the FOMO (Faster Objects, More Objects) MobileNetV2 0.35. This model uses around 250 KB of RAM and 80 KB of ROM (Flash), which suits well with our board.\n \nRegarding the training hyper-parameters, the model will be trained with:\n\nEpochs: 60\nBatch size: 32\nLearning Rate: 0.001.\n\nFor validation during training, 20% of the dataset (validation_dataset) will be spared. For the remaining 80% (train_dataset), we will apply Data Augmentation, which will randomly flip, change the size and brightness of the image, and crop them, artificially increasing the number of samples on the dataset for training.\nAs a result, the model ends with an overall F1 score of 85%, similar to the result when using the test data (83%).\n\nNote that FOMO automatically added a 3rd label background to the two previously defined (box and wheel).\n\n \n\nIn object detection tasks, accuracy is generally not the primary evaluation metric. Object detection involves classifying objects and providing bounding boxes around them, making it a more complex problem than simple classification. The issue is that we do not have the bounding box, only the centroids. In short, using accuracy as a metric could be misleading and may not provide a complete understanding of how well the model is performing. Because of that, we will use the F1 score.\n\n\n\n\nOnce our model is trained, we can test it using the Live Classification tool. On the correspondent section, click on Connect a development board icon (a small MCU) and scan the QR code with your phone.\n \nOnce connected, you can use the smartphone to capture actual images to be tested by the trained model on Edge Impulse Studio.\n \nOne thing to be noted is that the model can produce false positives and negatives. This can be minimized by defining a proper Confidence Threshold (use the Three dots menu for the setup). Try with 0.8 or more.",
    "crumbs": [
      "Seeed XIAO ESP32S3",
      "Object Detection"
    ]
  },
  {
    "objectID": "contents/seeed/xiao_esp32s3/object_detection/object_detection.html#sec-object-detection-deploying-model-arduino-ide-3755",
    "href": "contents/seeed/xiao_esp32s3/object_detection/object_detection.html#sec-object-detection-deploying-model-arduino-ide-3755",
    "title": "Object Detection",
    "section": "",
    "text": "Select the Arduino Library and Quantized (int8) model, enable the EON Compiler on the Deploy Tab, and press [Build].\n \nOpen your Arduino IDE, and under Sketch, go to Include Library and add.ZIP Library. Select the file you download from Edge Impulse Studio, and that‚Äôs it!\n \nUnder the Examples tab on Arduino IDE, you should find a sketch code (esp32 &gt; esp32_camera) under your project name.\n \nYou should change lines 32 to 75, which define the camera model and pins, using the data related to our model. Copy and paste the below lines, replacing the lines 32-75:\n#define PWDN_GPIO_NUM     -1\n#define RESET_GPIO_NUM    -1\n#define XCLK_GPIO_NUM     10\n#define SIOD_GPIO_NUM     40\n#define SIOC_GPIO_NUM     39\n#define Y9_GPIO_NUM       48\n#define Y8_GPIO_NUM       11\n#define Y7_GPIO_NUM       12\n#define Y6_GPIO_NUM       14\n#define Y5_GPIO_NUM       16\n#define Y4_GPIO_NUM       18\n#define Y3_GPIO_NUM       17\n#define Y2_GPIO_NUM       15\n#define VSYNC_GPIO_NUM    38\n#define HREF_GPIO_NUM     47\n#define PCLK_GPIO_NUM     13\nHere you can see the resulting code:\n \nUpload the code to your XIAO ESP32S3 Sense, and you should be OK to start detecting fruits and bugs. You can check the result on Serial Monitor.\n\n\n \n\n\n\n \n\n\n\n \nNote that the model latency is 143 ms, and the frame rate per second is around 7 fps (similar to what we got with the Image Classification project). This happens because FOMO is cleverly built over a CNN model, not with an object detection model like the SSD MobileNet. For example, when running a MobileNetV2 SSD FPN-Lite \\(320\\times 320\\) model on a Raspberry Pi 4, the latency is around five times higher (around 1.5 fps).",
    "crumbs": [
      "Seeed XIAO ESP32S3",
      "Object Detection"
    ]
  },
  {
    "objectID": "contents/seeed/xiao_esp32s3/object_detection/object_detection.html#sec-object-detection-deploying-model-sensecraftwebtoolkit-968c",
    "href": "contents/seeed/xiao_esp32s3/object_detection/object_detection.html#sec-object-detection-deploying-model-sensecraftwebtoolkit-968c",
    "title": "Object Detection",
    "section": "",
    "text": "As discussed in the Image Classification chapter, verifying inference with Image models on Arduino IDE is very challenging because we can not see what the camera focuses on. Again, let‚Äôs use the SenseCraft-Web Toolkit.\nFollow the following steps to start the SenseCraft-Web-Toolkit:\n\nOpen the SenseCraft-Web-Toolkit website.\nConnect the XIAO to your computer:\n\n\nHaving the XIAO connected, select it as below:\n\n \n\nSelect the device/Port and press [Connect]:\n\n \n\nYou can try several Computer Vision models previously uploaded by Seeed Studio. Try them and have fun!\n\nIn our case, we will use the blue button at the bottom of the page: [Upload Custom AI Model].\nBut first, we must download from Edge Impulse Studio our quantized .tflite model.\n\nGo to your project at Edge Impulse Studio, or clone this one:\n\n\nXIAO-ESP32S3-CAM-Fruits-vs-Veggies-v1-ESP-NN\n\n\nOn Dashboard, download the model (‚Äúblock output‚Äù): Object Detection model - TensorFlow Lite (int8 quantized)\n\n \n\nOn SenseCraft-Web-Toolkit, use the blue button at the bottom of the page: [Upload Custom AI Model]. A window will pop up. Enter the Model file that you downloaded to your computer from Edge Impulse Studio, choose a Model Name, and enter with labels (ID: Object):\n\n \n\nNote that you should use the labels trained on EI Studio and enter them in alphabetic order (in our case, background, bug, fruit).\n\nAfter a few seconds (or minutes), the model will be uploaded to your device, and the camera image will appear in real-time on the Preview Sector:\n \nThe detected objects will be marked (the centroid). You can select the Confidence of your inference cursor Confidence and IoU, which is used to assess the accuracy of predicted bounding boxes compared to truth bounding boxes.\nClicking on the top button (Device Log), you can open a Serial Monitor to follow the inference, as we did with the Arduino IDE.\n \nOn Device Log, you will get information as:\n\nPreprocess time (image capture and Crop): 3 ms,\nInference time (model latency): 115 ms,\nPostprocess time (display of the image and marking objects): 1 ms.\nOutput tensor (boxes), for example, one of the boxes: [[30,150, 20, 20,97, 2]]; where 30,150, 20, 20 are the coordinates of the box (around the centroid); 97 is the inference result, and 2 is the class (in this case 2: fruit).\n\n\nNote that in the above example, we got 5 boxes because none of the fruits got 3 centroids. One solution will be post-processing, where we can aggregate close centroids in one.\n\nHere are other screenshots:",
    "crumbs": [
      "Seeed XIAO ESP32S3",
      "Object Detection"
    ]
  },
  {
    "objectID": "contents/seeed/xiao_esp32s3/object_detection/object_detection.html#sec-object-detection-summary-3edc",
    "href": "contents/seeed/xiao_esp32s3/object_detection/object_detection.html#sec-object-detection-summary-3edc",
    "title": "Object Detection",
    "section": "",
    "text": "FOMO is a significant leap in the image processing space, as Louis Moreau and Mat Kelcey put it during its launch in 2022:\n\nFOMO is a ground-breaking algorithm that brings real-time object detection, tracking, and counting to microcontrollers for the first time.\n\nMultiple possibilities exist for exploring object detection (and, more precisely, counting them) on embedded devices.",
    "crumbs": [
      "Seeed XIAO ESP32S3",
      "Object Detection"
    ]
  },
  {
    "objectID": "contents/seeed/xiao_esp32s3/object_detection/object_detection.html#sec-object-detection-resources-7d20",
    "href": "contents/seeed/xiao_esp32s3/object_detection/object_detection.html#sec-object-detection-resources-7d20",
    "title": "Object Detection",
    "section": "",
    "text": "Edge Impulse Project",
    "crumbs": [
      "Seeed XIAO ESP32S3",
      "Object Detection"
    ]
  },
  {
    "objectID": "contents/seeed/xiao_esp32s3/xiao_esp32s3.html",
    "href": "contents/seeed/xiao_esp32s3/xiao_esp32s3.html",
    "title": "Overview",
    "section": "",
    "text": "These labs provide a unique opportunity to gain practical experience with machine learning (ML) systems. Unlike working with large models that require data center-scale resources, these exercises enable you to directly interact with hardware and software using TinyML. This hands-on approach provides a tangible understanding of the challenges and opportunities in deploying AI, albeit on a small scale. However, the principles are largely the same as what you would encounter when working with larger systems.\n\n\n\nThe XIAOML Kit bundles the XIAO ESP32S3 Sense with an expansion board, IMU, OLED display, and SD card toolkit:\n\nXIAOML Kit (Seeed Studio) (~$40)\n\nIndividual components are also available separately from Seeed Studio.\n\n\n\n\nThe XIAOML Kit:\n\nXIAO ESP32S3 Sense board\nExpansion board with a 6-axis IMU and 0.42‚Äù OLED display.\nSD card toolkit:\n\nSD Card and USB adapter for data storage\nUSB-C Cable for connecting the board to your computer.\n\n\nNetwork: With internet access for downloading the necessary software.\n\n\n\n\n\nSetup the XIAOML Kit\n\n\n\n\n\n\n\nModality\nTask\nDescription\nLink\n\n\n\n\nVision\nImage Classification\nLearn to classify images\nLink\n\n\nVision\nObject Detection\nImplement object detection\nLink\n\n\nSound\nKeyword Spotting\nExplore voice recognition systems\nLink\n\n\nIMU\nMotion Classification and Anomaly Detection\nClassify motion data and detect anomalies\nLink",
    "crumbs": [
      "Seeed XIAO ESP32S3",
      "Overview"
    ]
  },
  {
    "objectID": "contents/seeed/xiao_esp32s3/xiao_esp32s3.html#sec-overview-where-to-buy",
    "href": "contents/seeed/xiao_esp32s3/xiao_esp32s3.html#sec-overview-where-to-buy",
    "title": "Overview",
    "section": "",
    "text": "The XIAOML Kit bundles the XIAO ESP32S3 Sense with an expansion board, IMU, OLED display, and SD card toolkit:\n\nXIAOML Kit (Seeed Studio) (~$40)\n\nIndividual components are also available separately from Seeed Studio.",
    "crumbs": [
      "Seeed XIAO ESP32S3",
      "Overview"
    ]
  },
  {
    "objectID": "contents/seeed/xiao_esp32s3/xiao_esp32s3.html#sec-overview-prerequisites-f092",
    "href": "contents/seeed/xiao_esp32s3/xiao_esp32s3.html#sec-overview-prerequisites-f092",
    "title": "Overview",
    "section": "",
    "text": "The XIAOML Kit:\n\nXIAO ESP32S3 Sense board\nExpansion board with a 6-axis IMU and 0.42‚Äù OLED display.\nSD card toolkit:\n\nSD Card and USB adapter for data storage\nUSB-C Cable for connecting the board to your computer.\n\n\nNetwork: With internet access for downloading the necessary software.",
    "crumbs": [
      "Seeed XIAO ESP32S3",
      "Overview"
    ]
  },
  {
    "objectID": "contents/seeed/xiao_esp32s3/xiao_esp32s3.html#sec-overview-setup-2491",
    "href": "contents/seeed/xiao_esp32s3/xiao_esp32s3.html#sec-overview-setup-2491",
    "title": "Overview",
    "section": "",
    "text": "Setup the XIAOML Kit",
    "crumbs": [
      "Seeed XIAO ESP32S3",
      "Overview"
    ]
  },
  {
    "objectID": "contents/seeed/xiao_esp32s3/xiao_esp32s3.html#sec-overview-exercises-f0f7",
    "href": "contents/seeed/xiao_esp32s3/xiao_esp32s3.html#sec-overview-exercises-f0f7",
    "title": "Overview",
    "section": "",
    "text": "Modality\nTask\nDescription\nLink\n\n\n\n\nVision\nImage Classification\nLearn to classify images\nLink\n\n\nVision\nObject Detection\nImplement object detection\nLink\n\n\nSound\nKeyword Spotting\nExplore voice recognition systems\nLink\n\n\nIMU\nMotion Classification and Anomaly Detection\nClassify motion data and detect anomalies\nLink",
    "crumbs": [
      "Seeed XIAO ESP32S3",
      "Overview"
    ]
  },
  {
    "objectID": "contents/seeed/grove_vision_ai_v2/setup_and_no_code_apps/setup_and_no_code_apps.html",
    "href": "contents/seeed/grove_vision_ai_v2/setup_and_no_code_apps/setup_and_no_code_apps.html",
    "title": "Setup and No-Code Applications",
    "section": "",
    "text": "In this Lab, we will explore computer vision (CV) applications using the Seeed Studio Grove Vision AI Module V2, a powerful yet compact device specifically designed for embedded machine learning applications. Based on the Himax WiseEye2 chip, this module is designed to enable AI capabilities on edge devices, making it an ideal tool for Edge Machine Learning (ML) applications.\n\n\n\n\n\n\n\n\n\nThe Grove Vision AI (V2) is an MCU-based vision AI module that utilizes a Himax WiseEye2 HX6538 processor featuring a dual-core Arm Cortex-M55 and an integrated ARM Ethos-U55 neural network unit. The Arm Ethos-U55 is a machine learning (ML) processor class, specifically designed as a microNPU, to accelerate ML inference in area-constrained embedded and IoT devices. The Ethos-U55, combined with the AI-capable Cortex-M55 processor, provides a 480x uplift in ML performance over existing Cortex-M-based systems. Its clock frequency is 400 MHz, and its internal system memory (SRAM) is configurable, with a maximum capacity of 2.4 MB.\n\n\n\n\n\n\nNote: Based on Seeed Studio documentation, besides the Himax internal memory of 2.5MB (2.4MB SRAM + 64KB ROM), the Grove Vision AI (V2) is also equipped with a 16MB/133 MHz external flash.\n\n\n\n\n\n\nBelow is a block Diagram of the Grove Vision AI (V2) system, including a camera and a master controller.\n\n\n\n\n\nWith interfaces like IIC, UART, SPI, and Type-C, the Grove Vision AI (V2) can be easily connected to devices such as XIAO, Raspberry Pi, BeagleBoard, and ESP-based products for further development. For instance, integrating Grove Vision AI V2 with one of the devices from the XIAO family makes it easy to access the data resulting from inference on the device through the Arduino IDE or MicroPython, and conveniently connect to the cloud or dedicated servers, such as Home Assistance.\n\nUsing the I2C Grove connector, the Grove Vision AI V2 can be easily connected with any Master Device.\n\n\n\n\n\n\nBesides performance, another area to comment on is Power Consumption. For example, in a comparative test against the XIAO ESP32S3 Sense, running Swift-YOLO Tiny 96x96, despite achieving higher performance (30 FPS vs.¬†5.5 FPS), the Grove Vision AI V2 exhibited lower power consumption (0.35 W vs.¬†0.45 W) when compared with the XIAO ESP32S3 Sense.\n\n\n\n\n\n\nThe above comparison (and with other devices) can be found in the article 2024 MCU AI Vision Boards: Performance Comparison, which confirms the power of Grove Vision AI (V2).\n\n\n\n\nHaving the Grove Vision AI (V2) and camera ready, you can connect, for example, a Raspberry Pi OV5647 Camera Module via the CSI cable.\n\nWhen connecting, please pay attention to the direction of the row of pins and ensure they are plugged in correctly, not in the opposite direction.\n\n\n\n\n\n\n\n\n\n\nThe SenseCraft AI Studio is a robust platform that offers a wide range of AI models compatible with various devices, including the XIAO ESP32S3 Sense and the Grove Vision AI V2. In this lab, we will walk through the process of using an AI model with the Grove Vision AI V2 and preview the model‚Äôs output. We will also explore some key concepts, settings, and how to optimize the model‚Äôs performance.\n\n\n\n\n\nModels can also be deployed using the SenseCraft Web Toolkit, a simplified version of the SenseCraft AI Studio.\n\nWe can start using the SenseCraft Web Toolkit for simplicity, or go directly to the SenseCraft AI Studio, which has more resources.\n\n\n\nThe SenseCraft Web Toolkit is a visual model deployment tool included in the SSCMA(Seeed SenseCraft Model Assistant). This tool enables us to deploy models to various platforms with ease through simple operations. The tool offers a user-friendly interface and does not require any coding.\nThe SenseCraft Web Toolkit is based on the Himax AI Web Toolkit, which can (optionally) be downloaded from here. Once downloaded and unzipped to the local PC, double-click index.html to run it locally.\n\n\n\n\n\nBut in our case, let‚Äôs follow the steps below to start the SenseCraft-Web-Toolkit:\n\nOpen the SenseCraft-Web-Toolkit website on a web browser as Chrome.\nConnect Grove Vision AI (V2) to your computer using a Type-C cable.\nHaving the XIAO connected, select it as below:\n\n\n\n\n\n\n\nSelect the device/Port and press [Connect]:\n\n\n\n\n\n\n\nNote: The WebUSB tool may not function correctly in certain browsers, such as Safari. Use Chrome instead.\n\nWe can try several Basic Computer Vision models previously uploaded by Seeed Studio. Passing the cursor over the AI models, we can have some information about them, such as name, description, category (Image Classification, Object Detection, or Pose/Keypoint Detection), the algorithm (like YOLO V5 or V8, FOMO, MobileNet V2, etc.) and metrics (Accuracy or mAP).\n\n\n\n\n\nWe can choose one of those ready-to-use AI models by clicking on it and pressing the [Send] button, or upload our model.\nFor the SenseCraft AI platform, follow the instructions here.\n\n\n\n\n\n\nObject detection is a pivotal technology in computer vision that focuses on identifying and locating objects within digital images or video frames. Unlike image classification, which categorizes an entire image into a single label, object detection recognizes multiple objects within the image and determines their precise locations, typically represented by bounding boxes. This capability is crucial for a wide range of applications, including autonomous vehicles, security, surveillance systems, and augmented reality, where understanding the context and content of the visual environment is essential.\nCommon architectures that have set the benchmark in object detection include the YOLO (You Only Look Once), SSD (Single Shot MultiBox Detector), FOMO (Faster Objects, More Objects), and Faster R-CNN (Region-based Convolutional Neural Networks) models.\nLet‚Äôs choose one of the ready-to-use AI models, such as Person Detection, which was trained using the Swift-YOLO algorithm.\n\n\n\n\n\nOnce the model is uploaded successfully, you can see the live feed from the Grove Vision AI (V2) camera in the Preview area on the right. Also, the inference details can be shown on the Serial Monitor by clicking on the [Device Log] button at the top.\n\n\n\n\n\n\nIn the SenseCraft AI Studio, the Device Logger is always on the screen.\n\nPointing the camera at me, only one person was detected, so that the model output will be a single ‚Äúbox‚Äù. Looking in detail, the module sends continuously two lines of information:\n\n\n\n\n\nperf (Performance), displays latency in milliseconds.\n\nPreprocess time (image capture and Crop): 7ms;\nInference time (model latency): 76ms (13 fps)\nPostprocess time (display of the image and inclusion of data): less than 0ms.\n\nboxes: Show the objects detected in the image. In this case, only one.\n\nThe box has the x, y, w, and h coordinates of (245, 292,449,392), and the object (person, label 0) was captured with a value of .89.\n\nIf we point the camera at an image with several people, we will get one box for each person (object):\n\n\n\n\n\n\nOn the SenseCraft AI Studio, the inference latency (48ms) is lower than on the SenseCraft ToolKit (76ms), due to a distinct deployment implementation.\n\n\n\n\n\n\nPower Consumption\nThe peak power consumption running this Swift-YOLO model was 410 milliwatts.\nPreview Settings\nWe can see that in the Settings, two settings options can be adjusted to optimize the model‚Äôs recognition accuracy.\n\nConfidence: Refers to the level of certainty or probability assigned to its predictions by a model. This value determines the minimum confidence level required for the model to consider a detection as valid. A higher confidence threshold will result in fewer detections but with higher certainty, while a lower threshold will allow more detections but may include some false positives.\nIoU: Used to assess the accuracy of predicted bounding boxes compared to truth bounding boxes. IoU is a metric that measures the overlap between the predicted bounding box and the ground truth bounding box. It is used to determine the accuracy of the object detection. The IoU threshold sets the minimum IoU value required for a detection to be considered a true positive. Adjusting this threshold can help in fine-tuning the model‚Äôs precision and recall.\n\n\n\n\n\n\n\nExperiment with different values for the Confidence Threshold and IoU Threshold to find the optimal balance between detecting persons accurately and minimizing false positives. The best settings may vary depending on our specific application and the characteristics of the images or video feed.\n\n\n\n\nPose or keypoint detection is a sophisticated area within computer vision that focuses on identifying specific points of interest within an image or video frame, often related to human bodies, faces, or other objects of interest. This technology can detect and map out the various keypoints of a subject, such as the joints on a human body or the features of a face, enabling the analysis of postures, movements, and gestures. This has profound implications for various applications, including augmented reality, human-computer interaction, sports analytics, and healthcare monitoring, where understanding human motion and activity is crucial.\nUnlike general object detection, which identifies and locates objects, pose detection drills down to a finer level of detail, capturing the nuanced positions and orientations of specific parts. Leading architectures in this field include OpenPose, AlphaPose, and PoseNet, each designed to tackle the challenges of pose estimation with varying degrees of complexity and precision. Through advancements in deep learning and neural networks, pose detection has become increasingly accurate and efficient, offering real-time insights into the intricate dynamics of subjects captured in visual data.\nSo, let‚Äôs explore this popular CV application, Pose/Keypoint Detection.\n\n\n\n\n\nStop the current model inference by pressing [Stop] in the Preview area. Select the model and press [Send]. Once the model is uploaded successfully, you can view the live feed from the Grove Vision AI (V2) camera in the Preview area on the right, along with the inference details displayed in the Serial Monitor (accessible by clicking the [Device Log] button at the top).\n\n\n\n\n\nThe YOLOV8 Pose model was trained using the COCO-Pose Dataset, which contains 200K images labeled with 17 keypoints for pose estimation tasks.\nLet‚Äôs look at a single screenshot of the inference (to simplify, let‚Äôs analyse an image with a single person in it). We can note that we have two lines, one with the inference performance in milliseconds (121 ms) and a second line with the keypoints as below:\n\n1 box of info, the same as we got with the object detection example (box coordinates (113, 119, 67, 208), inference result (90), label (0).\n17 groups of 4 numbers represent the 17 ‚Äújoints‚Äù of the body, where ‚Äò0‚Äô is the nose, ‚Äò1‚Äô and ‚Äò2‚Äô are the eyes, ‚Äò15‚Äô and‚Äô 16‚Äô are the feet, and so on.\n\n\n\n\n\n\n\nTo understand a pose estimation project more deeply, please refer to the tutorial: Exploring AI at the Edge! - Pose Estimation.\n\n\n\n\nImage classification is a foundational task within computer vision aimed at categorizing entire images into one of several predefined classes. This process involves analyzing the visual content of an image and assigning it a label from a fixed set of categories based on the predominant object or scene it contains.\nImage classification is crucial in various applications, ranging from organizing and searching through large databases of images in digital libraries and social media platforms to enabling autonomous systems to comprehend their surroundings. Common architectures that have significantly advanced the field of image classification include Convolutional Neural Networks (CNNs), such as AlexNet, VGGNet, and ResNet. These models have demonstrated remarkable accuracy on challenging datasets, such as ImageNet, by learning hierarchical representations of visual data.\nAs the cornerstone of many computer vision systems, image classification drives innovation, laying the groundwork for more complex tasks like object detection and image segmentation, and facilitating a deeper understanding of visual data across various industries. So, let‚Äôs also explore this computer vision application.\n\n\n\n\n\n\nThis example is available on the SenseCraft ToolKit, but not in the SenseCraft AI Studio. In the last one, it is possible to find other examples of Image Classification.\n\nAfter the model is uploaded successfully, we can view the live feed from the Grove Vision AI (V2) camera in the Preview area on the right, along with the inference details displayed in the Serial Monitor (by clicking the [Device Log] button at the top).\n\n\n\n\n\nAs a result, we will receive a score and the class as output.\n\n\n\n\n\nFor example, [99, 1] means class: 1 (Person) with a score of 0.99. Once this model is a binary classification, class 0 will be ‚ÄúNo Person‚Äù (or Background). The Inference latency is 15ms or around 70fps.\n\n\nTo run the Mobilenet V2 0.35, the Grove Vision AI V2 had a peak current of 80mA at 5.24V, resulting in a power consumption of 420mW.\nRunning the same model on XIAO ESP32S3 Sense, the power consumption was 523mW with a latency of 291ms.\n\n\n\n\n\n\n\n\n\nSeveral public AI models can also be downloaded from the SenseCraft AI WebPage. For example, you can run a Swift-YOLO model, detecting traffic lights as shown here:\n\n\n\n\n\nThe latency of this model is approximately 86 ms, with an average power consumption of 420 mW.\n\n\n\n\nLet‚Äôs create a complete Image Classification project, using the SenseCraft AI Studio.\n\n\n\n\n\nOn SenseCraft AI Studio: Let‚Äôs open the tab Training:\n\n\n\n\n\nThe default is to train a Classification model with a WebCam if it is available. Let‚Äôs select the Grove Vision AI V2 instead. Pressing the green button[Connect], a Pop-Up window will appear. Select the corresponding Port and press the blue button [Connect].\n\n\n\n\n\nThe image streamed from the Grove Vision AI V2 will be displayed.\n\n\nThe first step is always to define a goal. Let‚Äôs classify, for example, two simple objects‚Äîfor instance, a toy box and a toy wheel. We should also include a 3rd class of images, background, where no object is in the scene.\n\n\n\n\n\n\n\n\nLet‚Äôs create the classes, following, for example, an alphabetical order:\n\nClass1: background\nClass 2: box\nClass 3: wheel\n\n\n\n\n\n\nSelect one of the classes and keep pressing the green button under the preview area. The collected images will appear on the Image Samples Screen.\n\n\n\n\n\nAfter collecting the images, review them and delete any incorrect ones.\n\n\n\n\n\nCollect around 50 images from each class and go to Training Step:\n\n\n\nConfirm if the correct device is selected (Grove Vision AI V2) and press [Start Training]\n\n\n\n\n\n\n\n\nAfter training, the inference result can be previewed.\n\nNote that the model is not running on the device. We are, in fact, only capturing the images with the device and performing a live preview using the training model, which is running in the Studio.\n\n\n\n\n\n\nNow is time to really deploy the model in the device:\n\n\n\nSelect the trained model on [Deploy to device], select the Grove Vision AI V2:\n\n\n\n\n\nThe Studio will redirect us to the Vision Workplace tab. Confirm the deployment, select the appropriate Port, and connect it:\n\n\n\n\n\nThe model will be flashed into the device. After an automatic reset, the model will start running on the device. On the Device Logger, we can see that the inference has a latency of approximately 8 ms, corresponding to a frame rate of 125 frames per second (FPS).\nAlso, note that it is possible to adjust the model‚Äôs confidence.\n\n\n\n\n\n\nTo run the Image Classification Model, the Grove Vision AI V2 had a peak current of 80mA at 5.24V, resulting in a power consumption of 420mW.\n\n\n\n\nIt is possible to save the model in the SenseCraft AI Studio. The Studio will keep all our models, which can be deployed later. For that, return to the Training tab and select the button [Save to SenseCraft]:\n\n\n\n\n\n\n\n\n\nIn this lab, we explored several computer vision (CV) applications using the Seeed Studio Grove Vision AI Module V2, demonstrating its exceptional capabilities as a powerful yet compact device specifically designed for embedded machine learning applications.\nPerformance Excellence: The Grove Vision AI V2 demonstrated remarkable performance across multiple computer vision tasks. With its Himax WiseEye2 chip featuring a dual-core Arm Cortex-M55 and integrated ARM Ethos-U55 neural network unit, the device delivered:\n\nImage Classification: 15 ms inference time (67 FPS)\nObject Detection (Person): 48 ms to 76 ms inference time (21 FPS to 13 FPS)\nPose Detection: 121 ms real-time keypoint detection with 17-joint tracking (8 FPS)\n\nPower Efficiency Leadership: One of the most compelling advantages of the Grove Vision AI V2 is its superior power efficiency. Comparative testing revealed significant improvements over traditional embedded platforms:\n\nGrove Vision AI V2: 80 mA (410 mW) peak consumption (60+ FPS)\nXIAO ESP32S3: Performing similar CV tasks (Image Classification) 523 mW (3+ FPS)\n\nPractical Implementation: The device‚Äôs versatility was demonstrated through a comprehensive end-to-end project, encompassing dataset creation, model training, deployment, and offline inference.\nDeveloper-Friendly Ecosystem: The SenseCraft AI Studio, with its no-code deployment and integration capabilities for custom applications, makes the Grove Vision AI V2 accessible to both beginners and advanced developers. The extensive library of pre-trained models and support for custom model deployment provide flexibility for diverse applications.\nThe Grove Vision AI V2 represents a significant advancement in edge AI hardware, offering professional-grade computer vision capabilities in a compact, energy-efficient package that democratizes AI deployment for embedded applications across industrial, IoT, and educational domains.\nKey Takeaways\nThis Lab demonstrates that sophisticated computer vision applications are not limited to cloud-based solutions or power-hungry hardware, as the Raspberry Pi or Jetson Nanos ‚Äì they can now be deployed effectively at the edge with remarkable efficiency and performance.\nOptionally, we can have the XIAO Vision AI Camera. This innovative vision solution seamlessly combines the Grove Vision AI V2 module, XIAO ESP32-C3 controller, and an OV5647 camera, all housed in a custom 3D-printed enclosure:\n\n\n\n\n\n\n\n\nSenseCraft AI Studio Instructions.\nSenseCraft-Web-Toolkit website.\nSenseCraft AI Studio\nHimax AI Web Toolkit\nHimax examples",
    "crumbs": [
      "Grove Vision AI V2",
      "Setup and No-Code Applications"
    ]
  },
  {
    "objectID": "contents/seeed/grove_vision_ai_v2/setup_and_no_code_apps/setup_and_no_code_apps.html#sec-setup-nocode-applications-introduction-b740",
    "href": "contents/seeed/grove_vision_ai_v2/setup_and_no_code_apps/setup_and_no_code_apps.html#sec-setup-nocode-applications-introduction-b740",
    "title": "Setup and No-Code Applications",
    "section": "",
    "text": "The Grove Vision AI (V2) is an MCU-based vision AI module that utilizes a Himax WiseEye2 HX6538 processor featuring a dual-core Arm Cortex-M55 and an integrated ARM Ethos-U55 neural network unit. The Arm Ethos-U55 is a machine learning (ML) processor class, specifically designed as a microNPU, to accelerate ML inference in area-constrained embedded and IoT devices. The Ethos-U55, combined with the AI-capable Cortex-M55 processor, provides a 480x uplift in ML performance over existing Cortex-M-based systems. Its clock frequency is 400 MHz, and its internal system memory (SRAM) is configurable, with a maximum capacity of 2.4 MB.\n\n\n\n\n\n\nNote: Based on Seeed Studio documentation, besides the Himax internal memory of 2.5MB (2.4MB SRAM + 64KB ROM), the Grove Vision AI (V2) is also equipped with a 16MB/133 MHz external flash.\n\n\n\n\n\n\nBelow is a block Diagram of the Grove Vision AI (V2) system, including a camera and a master controller.\n\n\n\n\n\nWith interfaces like IIC, UART, SPI, and Type-C, the Grove Vision AI (V2) can be easily connected to devices such as XIAO, Raspberry Pi, BeagleBoard, and ESP-based products for further development. For instance, integrating Grove Vision AI V2 with one of the devices from the XIAO family makes it easy to access the data resulting from inference on the device through the Arduino IDE or MicroPython, and conveniently connect to the cloud or dedicated servers, such as Home Assistance.\n\nUsing the I2C Grove connector, the Grove Vision AI V2 can be easily connected with any Master Device.\n\n\n\n\n\n\nBesides performance, another area to comment on is Power Consumption. For example, in a comparative test against the XIAO ESP32S3 Sense, running Swift-YOLO Tiny 96x96, despite achieving higher performance (30 FPS vs.¬†5.5 FPS), the Grove Vision AI V2 exhibited lower power consumption (0.35 W vs.¬†0.45 W) when compared with the XIAO ESP32S3 Sense.\n\n\n\n\n\n\nThe above comparison (and with other devices) can be found in the article 2024 MCU AI Vision Boards: Performance Comparison, which confirms the power of Grove Vision AI (V2).\n\n\n\n\nHaving the Grove Vision AI (V2) and camera ready, you can connect, for example, a Raspberry Pi OV5647 Camera Module via the CSI cable.\n\nWhen connecting, please pay attention to the direction of the row of pins and ensure they are plugged in correctly, not in the opposite direction.",
    "crumbs": [
      "Grove Vision AI V2",
      "Setup and No-Code Applications"
    ]
  },
  {
    "objectID": "contents/seeed/grove_vision_ai_v2/setup_and_no_code_apps/setup_and_no_code_apps.html#sec-setup-nocode-applications-sensecraft-ai-studio-04c9",
    "href": "contents/seeed/grove_vision_ai_v2/setup_and_no_code_apps/setup_and_no_code_apps.html#sec-setup-nocode-applications-sensecraft-ai-studio-04c9",
    "title": "Setup and No-Code Applications",
    "section": "",
    "text": "The SenseCraft AI Studio is a robust platform that offers a wide range of AI models compatible with various devices, including the XIAO ESP32S3 Sense and the Grove Vision AI V2. In this lab, we will walk through the process of using an AI model with the Grove Vision AI V2 and preview the model‚Äôs output. We will also explore some key concepts, settings, and how to optimize the model‚Äôs performance.\n\n\n\n\n\nModels can also be deployed using the SenseCraft Web Toolkit, a simplified version of the SenseCraft AI Studio.\n\nWe can start using the SenseCraft Web Toolkit for simplicity, or go directly to the SenseCraft AI Studio, which has more resources.\n\n\n\nThe SenseCraft Web Toolkit is a visual model deployment tool included in the SSCMA(Seeed SenseCraft Model Assistant). This tool enables us to deploy models to various platforms with ease through simple operations. The tool offers a user-friendly interface and does not require any coding.\nThe SenseCraft Web Toolkit is based on the Himax AI Web Toolkit, which can (optionally) be downloaded from here. Once downloaded and unzipped to the local PC, double-click index.html to run it locally.\n\n\n\n\n\nBut in our case, let‚Äôs follow the steps below to start the SenseCraft-Web-Toolkit:\n\nOpen the SenseCraft-Web-Toolkit website on a web browser as Chrome.\nConnect Grove Vision AI (V2) to your computer using a Type-C cable.\nHaving the XIAO connected, select it as below:\n\n\n\n\n\n\n\nSelect the device/Port and press [Connect]:\n\n\n\n\n\n\n\nNote: The WebUSB tool may not function correctly in certain browsers, such as Safari. Use Chrome instead.\n\nWe can try several Basic Computer Vision models previously uploaded by Seeed Studio. Passing the cursor over the AI models, we can have some information about them, such as name, description, category (Image Classification, Object Detection, or Pose/Keypoint Detection), the algorithm (like YOLO V5 or V8, FOMO, MobileNet V2, etc.) and metrics (Accuracy or mAP).\n\n\n\n\n\nWe can choose one of those ready-to-use AI models by clicking on it and pressing the [Send] button, or upload our model.\nFor the SenseCraft AI platform, follow the instructions here.",
    "crumbs": [
      "Grove Vision AI V2",
      "Setup and No-Code Applications"
    ]
  },
  {
    "objectID": "contents/seeed/grove_vision_ai_v2/setup_and_no_code_apps/setup_and_no_code_apps.html#sec-setup-nocode-applications-exploring-cv-ai-models-90ce",
    "href": "contents/seeed/grove_vision_ai_v2/setup_and_no_code_apps/setup_and_no_code_apps.html#sec-setup-nocode-applications-exploring-cv-ai-models-90ce",
    "title": "Setup and No-Code Applications",
    "section": "",
    "text": "Object detection is a pivotal technology in computer vision that focuses on identifying and locating objects within digital images or video frames. Unlike image classification, which categorizes an entire image into a single label, object detection recognizes multiple objects within the image and determines their precise locations, typically represented by bounding boxes. This capability is crucial for a wide range of applications, including autonomous vehicles, security, surveillance systems, and augmented reality, where understanding the context and content of the visual environment is essential.\nCommon architectures that have set the benchmark in object detection include the YOLO (You Only Look Once), SSD (Single Shot MultiBox Detector), FOMO (Faster Objects, More Objects), and Faster R-CNN (Region-based Convolutional Neural Networks) models.\nLet‚Äôs choose one of the ready-to-use AI models, such as Person Detection, which was trained using the Swift-YOLO algorithm.\n\n\n\n\n\nOnce the model is uploaded successfully, you can see the live feed from the Grove Vision AI (V2) camera in the Preview area on the right. Also, the inference details can be shown on the Serial Monitor by clicking on the [Device Log] button at the top.\n\n\n\n\n\n\nIn the SenseCraft AI Studio, the Device Logger is always on the screen.\n\nPointing the camera at me, only one person was detected, so that the model output will be a single ‚Äúbox‚Äù. Looking in detail, the module sends continuously two lines of information:\n\n\n\n\n\nperf (Performance), displays latency in milliseconds.\n\nPreprocess time (image capture and Crop): 7ms;\nInference time (model latency): 76ms (13 fps)\nPostprocess time (display of the image and inclusion of data): less than 0ms.\n\nboxes: Show the objects detected in the image. In this case, only one.\n\nThe box has the x, y, w, and h coordinates of (245, 292,449,392), and the object (person, label 0) was captured with a value of .89.\n\nIf we point the camera at an image with several people, we will get one box for each person (object):\n\n\n\n\n\n\nOn the SenseCraft AI Studio, the inference latency (48ms) is lower than on the SenseCraft ToolKit (76ms), due to a distinct deployment implementation.\n\n\n\n\n\n\nPower Consumption\nThe peak power consumption running this Swift-YOLO model was 410 milliwatts.\nPreview Settings\nWe can see that in the Settings, two settings options can be adjusted to optimize the model‚Äôs recognition accuracy.\n\nConfidence: Refers to the level of certainty or probability assigned to its predictions by a model. This value determines the minimum confidence level required for the model to consider a detection as valid. A higher confidence threshold will result in fewer detections but with higher certainty, while a lower threshold will allow more detections but may include some false positives.\nIoU: Used to assess the accuracy of predicted bounding boxes compared to truth bounding boxes. IoU is a metric that measures the overlap between the predicted bounding box and the ground truth bounding box. It is used to determine the accuracy of the object detection. The IoU threshold sets the minimum IoU value required for a detection to be considered a true positive. Adjusting this threshold can help in fine-tuning the model‚Äôs precision and recall.\n\n\n\n\n\n\n\nExperiment with different values for the Confidence Threshold and IoU Threshold to find the optimal balance between detecting persons accurately and minimizing false positives. The best settings may vary depending on our specific application and the characteristics of the images or video feed.\n\n\n\n\nPose or keypoint detection is a sophisticated area within computer vision that focuses on identifying specific points of interest within an image or video frame, often related to human bodies, faces, or other objects of interest. This technology can detect and map out the various keypoints of a subject, such as the joints on a human body or the features of a face, enabling the analysis of postures, movements, and gestures. This has profound implications for various applications, including augmented reality, human-computer interaction, sports analytics, and healthcare monitoring, where understanding human motion and activity is crucial.\nUnlike general object detection, which identifies and locates objects, pose detection drills down to a finer level of detail, capturing the nuanced positions and orientations of specific parts. Leading architectures in this field include OpenPose, AlphaPose, and PoseNet, each designed to tackle the challenges of pose estimation with varying degrees of complexity and precision. Through advancements in deep learning and neural networks, pose detection has become increasingly accurate and efficient, offering real-time insights into the intricate dynamics of subjects captured in visual data.\nSo, let‚Äôs explore this popular CV application, Pose/Keypoint Detection.\n\n\n\n\n\nStop the current model inference by pressing [Stop] in the Preview area. Select the model and press [Send]. Once the model is uploaded successfully, you can view the live feed from the Grove Vision AI (V2) camera in the Preview area on the right, along with the inference details displayed in the Serial Monitor (accessible by clicking the [Device Log] button at the top).\n\n\n\n\n\nThe YOLOV8 Pose model was trained using the COCO-Pose Dataset, which contains 200K images labeled with 17 keypoints for pose estimation tasks.\nLet‚Äôs look at a single screenshot of the inference (to simplify, let‚Äôs analyse an image with a single person in it). We can note that we have two lines, one with the inference performance in milliseconds (121 ms) and a second line with the keypoints as below:\n\n1 box of info, the same as we got with the object detection example (box coordinates (113, 119, 67, 208), inference result (90), label (0).\n17 groups of 4 numbers represent the 17 ‚Äújoints‚Äù of the body, where ‚Äò0‚Äô is the nose, ‚Äò1‚Äô and ‚Äò2‚Äô are the eyes, ‚Äò15‚Äô and‚Äô 16‚Äô are the feet, and so on.\n\n\n\n\n\n\n\nTo understand a pose estimation project more deeply, please refer to the tutorial: Exploring AI at the Edge! - Pose Estimation.\n\n\n\n\nImage classification is a foundational task within computer vision aimed at categorizing entire images into one of several predefined classes. This process involves analyzing the visual content of an image and assigning it a label from a fixed set of categories based on the predominant object or scene it contains.\nImage classification is crucial in various applications, ranging from organizing and searching through large databases of images in digital libraries and social media platforms to enabling autonomous systems to comprehend their surroundings. Common architectures that have significantly advanced the field of image classification include Convolutional Neural Networks (CNNs), such as AlexNet, VGGNet, and ResNet. These models have demonstrated remarkable accuracy on challenging datasets, such as ImageNet, by learning hierarchical representations of visual data.\nAs the cornerstone of many computer vision systems, image classification drives innovation, laying the groundwork for more complex tasks like object detection and image segmentation, and facilitating a deeper understanding of visual data across various industries. So, let‚Äôs also explore this computer vision application.\n\n\n\n\n\n\nThis example is available on the SenseCraft ToolKit, but not in the SenseCraft AI Studio. In the last one, it is possible to find other examples of Image Classification.\n\nAfter the model is uploaded successfully, we can view the live feed from the Grove Vision AI (V2) camera in the Preview area on the right, along with the inference details displayed in the Serial Monitor (by clicking the [Device Log] button at the top).\n\n\n\n\n\nAs a result, we will receive a score and the class as output.\n\n\n\n\n\nFor example, [99, 1] means class: 1 (Person) with a score of 0.99. Once this model is a binary classification, class 0 will be ‚ÄúNo Person‚Äù (or Background). The Inference latency is 15ms or around 70fps.\n\n\nTo run the Mobilenet V2 0.35, the Grove Vision AI V2 had a peak current of 80mA at 5.24V, resulting in a power consumption of 420mW.\nRunning the same model on XIAO ESP32S3 Sense, the power consumption was 523mW with a latency of 291ms.\n\n\n\n\n\n\n\n\n\nSeveral public AI models can also be downloaded from the SenseCraft AI WebPage. For example, you can run a Swift-YOLO model, detecting traffic lights as shown here:\n\n\n\n\n\nThe latency of this model is approximately 86 ms, with an average power consumption of 420 mW.",
    "crumbs": [
      "Grove Vision AI V2",
      "Setup and No-Code Applications"
    ]
  },
  {
    "objectID": "contents/seeed/grove_vision_ai_v2/setup_and_no_code_apps/setup_and_no_code_apps.html#sec-setup-nocode-applications-image-classification-project-0e45",
    "href": "contents/seeed/grove_vision_ai_v2/setup_and_no_code_apps/setup_and_no_code_apps.html#sec-setup-nocode-applications-image-classification-project-0e45",
    "title": "Setup and No-Code Applications",
    "section": "",
    "text": "Let‚Äôs create a complete Image Classification project, using the SenseCraft AI Studio.\n\n\n\n\n\nOn SenseCraft AI Studio: Let‚Äôs open the tab Training:\n\n\n\n\n\nThe default is to train a Classification model with a WebCam if it is available. Let‚Äôs select the Grove Vision AI V2 instead. Pressing the green button[Connect], a Pop-Up window will appear. Select the corresponding Port and press the blue button [Connect].\n\n\n\n\n\nThe image streamed from the Grove Vision AI V2 will be displayed.\n\n\nThe first step is always to define a goal. Let‚Äôs classify, for example, two simple objects‚Äîfor instance, a toy box and a toy wheel. We should also include a 3rd class of images, background, where no object is in the scene.\n\n\n\n\n\n\n\n\nLet‚Äôs create the classes, following, for example, an alphabetical order:\n\nClass1: background\nClass 2: box\nClass 3: wheel\n\n\n\n\n\n\nSelect one of the classes and keep pressing the green button under the preview area. The collected images will appear on the Image Samples Screen.\n\n\n\n\n\nAfter collecting the images, review them and delete any incorrect ones.\n\n\n\n\n\nCollect around 50 images from each class and go to Training Step:\n\n\n\nConfirm if the correct device is selected (Grove Vision AI V2) and press [Start Training]\n\n\n\n\n\n\n\n\nAfter training, the inference result can be previewed.\n\nNote that the model is not running on the device. We are, in fact, only capturing the images with the device and performing a live preview using the training model, which is running in the Studio.\n\n\n\n\n\n\nNow is time to really deploy the model in the device:\n\n\n\nSelect the trained model on [Deploy to device], select the Grove Vision AI V2:\n\n\n\n\n\nThe Studio will redirect us to the Vision Workplace tab. Confirm the deployment, select the appropriate Port, and connect it:\n\n\n\n\n\nThe model will be flashed into the device. After an automatic reset, the model will start running on the device. On the Device Logger, we can see that the inference has a latency of approximately 8 ms, corresponding to a frame rate of 125 frames per second (FPS).\nAlso, note that it is possible to adjust the model‚Äôs confidence.\n\n\n\n\n\n\nTo run the Image Classification Model, the Grove Vision AI V2 had a peak current of 80mA at 5.24V, resulting in a power consumption of 420mW.\n\n\n\n\nIt is possible to save the model in the SenseCraft AI Studio. The Studio will keep all our models, which can be deployed later. For that, return to the Training tab and select the button [Save to SenseCraft]:",
    "crumbs": [
      "Grove Vision AI V2",
      "Setup and No-Code Applications"
    ]
  },
  {
    "objectID": "contents/seeed/grove_vision_ai_v2/setup_and_no_code_apps/setup_and_no_code_apps.html#sec-setup-nocode-applications-summary-d118",
    "href": "contents/seeed/grove_vision_ai_v2/setup_and_no_code_apps/setup_and_no_code_apps.html#sec-setup-nocode-applications-summary-d118",
    "title": "Setup and No-Code Applications",
    "section": "",
    "text": "In this lab, we explored several computer vision (CV) applications using the Seeed Studio Grove Vision AI Module V2, demonstrating its exceptional capabilities as a powerful yet compact device specifically designed for embedded machine learning applications.\nPerformance Excellence: The Grove Vision AI V2 demonstrated remarkable performance across multiple computer vision tasks. With its Himax WiseEye2 chip featuring a dual-core Arm Cortex-M55 and integrated ARM Ethos-U55 neural network unit, the device delivered:\n\nImage Classification: 15 ms inference time (67 FPS)\nObject Detection (Person): 48 ms to 76 ms inference time (21 FPS to 13 FPS)\nPose Detection: 121 ms real-time keypoint detection with 17-joint tracking (8 FPS)\n\nPower Efficiency Leadership: One of the most compelling advantages of the Grove Vision AI V2 is its superior power efficiency. Comparative testing revealed significant improvements over traditional embedded platforms:\n\nGrove Vision AI V2: 80 mA (410 mW) peak consumption (60+ FPS)\nXIAO ESP32S3: Performing similar CV tasks (Image Classification) 523 mW (3+ FPS)\n\nPractical Implementation: The device‚Äôs versatility was demonstrated through a comprehensive end-to-end project, encompassing dataset creation, model training, deployment, and offline inference.\nDeveloper-Friendly Ecosystem: The SenseCraft AI Studio, with its no-code deployment and integration capabilities for custom applications, makes the Grove Vision AI V2 accessible to both beginners and advanced developers. The extensive library of pre-trained models and support for custom model deployment provide flexibility for diverse applications.\nThe Grove Vision AI V2 represents a significant advancement in edge AI hardware, offering professional-grade computer vision capabilities in a compact, energy-efficient package that democratizes AI deployment for embedded applications across industrial, IoT, and educational domains.\nKey Takeaways\nThis Lab demonstrates that sophisticated computer vision applications are not limited to cloud-based solutions or power-hungry hardware, as the Raspberry Pi or Jetson Nanos ‚Äì they can now be deployed effectively at the edge with remarkable efficiency and performance.\nOptionally, we can have the XIAO Vision AI Camera. This innovative vision solution seamlessly combines the Grove Vision AI V2 module, XIAO ESP32-C3 controller, and an OV5647 camera, all housed in a custom 3D-printed enclosure:",
    "crumbs": [
      "Grove Vision AI V2",
      "Setup and No-Code Applications"
    ]
  },
  {
    "objectID": "contents/seeed/grove_vision_ai_v2/setup_and_no_code_apps/setup_and_no_code_apps.html#sec-setup-nocode-applications-resources-85aa",
    "href": "contents/seeed/grove_vision_ai_v2/setup_and_no_code_apps/setup_and_no_code_apps.html#sec-setup-nocode-applications-resources-85aa",
    "title": "Setup and No-Code Applications",
    "section": "",
    "text": "SenseCraft AI Studio Instructions.\nSenseCraft-Web-Toolkit website.\nSenseCraft AI Studio\nHimax AI Web Toolkit\nHimax examples",
    "crumbs": [
      "Grove Vision AI V2",
      "Setup and No-Code Applications"
    ]
  },
  {
    "objectID": "contents/seeed/grove_vision_ai_v2/grove_vision_ai_v2.html",
    "href": "contents/seeed/grove_vision_ai_v2/grove_vision_ai_v2.html",
    "title": "Overview",
    "section": "",
    "text": "These labs offer an opportunity to gain practical experience with machine learning (ML) systems on a high-end, yet compact, embedded device, the Seeed Studio Grove Vision AI V2. Unlike working with large models requiring data center-scale resources, these labs allow you to interact with hardware and software using TinyML directly. This hands-on approach provides a tangible understanding of the challenges and opportunities in deploying AI, albeit on a small scale. However, the principles are essentially the same as what you would encounter when working with larger or even smaller systems.\nThe Grove Vision AI V2 occupies a unique position in the embedded AI landscape, bridging the gap between basic microcontroller solutions, such as the Seeed XIAO ESP32S3 Sense or Arduino Nicla Vision, and more powerful single-board computers, like the Raspberry Pi. At its heart lies the Himax WiseEye2 HX6538 processor, featuring a dual-core Arm Cortex-M55 and an integrated ARM Ethos-U55 neural network unit.\nThe Arm Ethos-U55 represents a specialized machine learning processor class, specifically designed as a microNPU to accelerate ML inference in area-constrained embedded and IoT devices. This powerful combination of the Ethos-U55 with the AI-capable Cortex-M55 processor delivers a remarkable 480x uplift in ML performance over existing Cortex-M-based systems. Operating at 400 MHz with configurable internal system memory (SRAM) up to 2.4 MB, the Grove Vision AI V2 offers professional-grade computer vision capabilities while maintaining the power efficiency and compact form factor essential for edge applications.\nThis positioning makes it an ideal platform for learning advanced TinyML concepts, offering the simplicity and reduced power requirements of smaller systems while providing capabilities that far exceed those of traditional microcontroller-based solutions.\n\n\n\nGrove - Vision AI Module V2. Source: SEEED Studio.\n\n\n\n\nThe Grove Vision AI V2 is available from Seeed Studio:\n\nGrove Vision AI V2 (Seeed Studio) (~$25)\n\nYou will also need a compatible camera module (Raspberry Pi OV5647) and optionally a master controller like the XIAO ESP32S3.\n\n\n\n\nGrove Vision AI V2 Board: Ensure you have the Grove Vision AI V2 Board.\nRaspberry Pi OV5647 Camera Module: The camera should be connected to the Grove Vision AI V2 Board for image capture.\nMaster Controller: Can be a Seeed XIAO ESP32S3, a XIAO ESP32C6, or other devices.\nUSB-C Cable: This is for connecting the board to your computer.\nNetwork: With internet access for downloading the necessary software.\nXIAO Expansion Board Base: This helps connect the Master Device to the Physical World (optional).\n\n\n\n\n\nSetup and No-Code Apps\n\n\n\n\n\n\n\nModality\nTask\nDescription\nLink\n\n\n\n\nVision\nImage Classification\nLearn to classify images\nLink\n\n\nVision\nObject Detection\nImplement object detection\nTBD",
    "crumbs": [
      "Grove Vision AI V2",
      "Overview"
    ]
  },
  {
    "objectID": "contents/seeed/grove_vision_ai_v2/grove_vision_ai_v2.html#sec-overview-where-to-buy",
    "href": "contents/seeed/grove_vision_ai_v2/grove_vision_ai_v2.html#sec-overview-where-to-buy",
    "title": "Overview",
    "section": "",
    "text": "The Grove Vision AI V2 is available from Seeed Studio:\n\nGrove Vision AI V2 (Seeed Studio) (~$25)\n\nYou will also need a compatible camera module (Raspberry Pi OV5647) and optionally a master controller like the XIAO ESP32S3.",
    "crumbs": [
      "Grove Vision AI V2",
      "Overview"
    ]
  },
  {
    "objectID": "contents/seeed/grove_vision_ai_v2/grove_vision_ai_v2.html#sec-overview-prerequisites-7b1e",
    "href": "contents/seeed/grove_vision_ai_v2/grove_vision_ai_v2.html#sec-overview-prerequisites-7b1e",
    "title": "Overview",
    "section": "",
    "text": "Grove Vision AI V2 Board: Ensure you have the Grove Vision AI V2 Board.\nRaspberry Pi OV5647 Camera Module: The camera should be connected to the Grove Vision AI V2 Board for image capture.\nMaster Controller: Can be a Seeed XIAO ESP32S3, a XIAO ESP32C6, or other devices.\nUSB-C Cable: This is for connecting the board to your computer.\nNetwork: With internet access for downloading the necessary software.\nXIAO Expansion Board Base: This helps connect the Master Device to the Physical World (optional).",
    "crumbs": [
      "Grove Vision AI V2",
      "Overview"
    ]
  },
  {
    "objectID": "contents/seeed/grove_vision_ai_v2/grove_vision_ai_v2.html#sec-overview-setup-nocode-applications-e70f",
    "href": "contents/seeed/grove_vision_ai_v2/grove_vision_ai_v2.html#sec-overview-setup-nocode-applications-e70f",
    "title": "Overview",
    "section": "",
    "text": "Setup and No-Code Apps",
    "crumbs": [
      "Grove Vision AI V2",
      "Overview"
    ]
  },
  {
    "objectID": "contents/seeed/grove_vision_ai_v2/grove_vision_ai_v2.html#sec-overview-exercises-e8a6",
    "href": "contents/seeed/grove_vision_ai_v2/grove_vision_ai_v2.html#sec-overview-exercises-e8a6",
    "title": "Overview",
    "section": "",
    "text": "Modality\nTask\nDescription\nLink\n\n\n\n\nVision\nImage Classification\nLearn to classify images\nLink\n\n\nVision\nObject Detection\nImplement object detection\nTBD",
    "crumbs": [
      "Grove Vision AI V2",
      "Overview"
    ]
  },
  {
    "objectID": "contents/arduino/nicla_vision/image_classification/image_classification.html",
    "href": "contents/arduino/nicla_vision/image_classification/image_classification.html",
    "title": "Image Classification",
    "section": "",
    "text": "DALL¬∑E 3 Prompt: Cartoon in a 1950s style featuring a compact electronic device with a camera module placed on a wooden table. The screen displays blue robots on one side and green periquitos on the other. LED lights on the device indicate classifications, while characters in retro clothing observe with interest.\n\n\n\n\nAs we initiate our studies into embedded machine learning or TinyML, it‚Äôs impossible to overlook the transformative impact of Computer Vision (CV) and Artificial Intelligence (AI) in our lives. These two intertwined disciplines redefine what machines can perceive and accomplish, from autonomous vehicles and robotics to healthcare and surveillance.\nMore and more, we are facing an artificial intelligence (AI) revolution where, as stated by Gartner, Edge AI has a very high impact potential, and it is for now!\n \nIn the ‚Äúbullseye‚Äù of the Radar is the Edge Computer Vision, and when we talk about Machine Learning (ML) applied to vision, the first thing that comes to mind is Image Classification, a kind of ML ‚ÄúHello World‚Äù!\nThis lab will explore a computer vision project utilizing Convolutional Neural Networks (CNNs) for real-time image classification. Leveraging TensorFlow‚Äôs robust ecosystem, we‚Äôll implement a pre-trained MobileNet model and adapt it for edge deployment. The focus will be optimizing the model to run efficiently on resource-constrained hardware without sacrificing accuracy.\nWe‚Äôll employ techniques like quantization and pruning to reduce the computational load. By the end of this tutorial, you‚Äôll have a working prototype capable of classifying images in real-time, all running on a low-power embedded system based on the Arduino Nicla Vision board.\n\n\n\nAt its core, computer vision enables machines to interpret and make decisions based on visual data from the world, essentially mimicking the capability of the human optical system. Conversely, AI is a broader field encompassing machine learning, natural language processing, and robotics, among other technologies. When you bring AI algorithms into computer vision projects, you supercharge the system‚Äôs ability to understand, interpret, and react to visual stimuli.\nWhen discussing Computer Vision projects applied to embedded devices, the most common applications that come to mind are Image Classification and Object Detection.\n \nBoth models can be implemented on tiny devices like the Arduino Nicla Vision and used on real projects. In this chapter, we will cover Image Classification.\n\n\n\nThe first step in any ML project is to define the goal. In this case, the goal is to detect and classify two specific objects present in one image. For this project, we will use two small toys: a robot and a small Brazilian parrot (named Periquito). We will also collect images of a background where those two objects are absent.\n \n\n\n\nOnce we have defined our Machine Learning project goal, the next and most crucial step is collecting the dataset. For image capturing, we can use:\n\nWeb Serial Camera tool,\nEdge Impulse Studio,\nOpenMV IDE,\nA smartphone.\n\nHere, we will use the OpenMV IDE.\n\n\nFirst, we should create a folder on our computer where the data will be saved, for example, ‚Äúdata.‚Äù Next, on the OpenMV IDE, we go to Tools &gt; Dataset Editor and select New Dataset to start the dataset collection:\n \nThe IDE will ask us to open the file where the data will be saved. Choose the ‚Äúdata‚Äù folder that was created. Note that new icons will appear on the Left panel.\n \nUsing the upper icon (1), enter with the first class name, for example, ‚Äúperiquito‚Äù:\n \nRunning the dataset_capture_script.py and clicking on the camera icon (2) will start capturing images:\n \nRepeat the same procedure with the other classes.\n \n\nWe suggest around 50 to 60 images from each category. Try to capture different angles, backgrounds, and light conditions.\n\nThe stored images use a QVGA frame size of \\(320\\times 240\\) and the RGB565 (color pixel format).\nAfter capturing the dataset, close the Dataset Editor Tool on the Tools &gt; Dataset Editor.\nWe will end up with a dataset on our computer that contains three classes: periquito, robot, and background.\n \nWe should return to Edge Impulse Studio and upload the dataset to our created project.\n\n\n\n\nWe will use the Edge Impulse Studio to train our model. Enter the account credentials and create a new project:\n \n\nHere, you can clone a similar project: NICLA-Vision_Image_Classification.\n\n\n\n\nUsing the EI Studio (or Studio), we will go over four main steps to have our model ready for use on the Nicla Vision board: Dataset, Impulse, Tests, and Deploy (on the Edge Device, in this case, the NiclaV).\n \nRegarding the Dataset, it is essential to point out that our Original Dataset, captured with the OpenMV IDE, will be split into Training, Validation, and Test. The Test Set will be spared from the beginning and reserved for use only in the Test phase after training. The Validation Set will be used during training.\n\nThe EI Studio will take a percentage of training data to be used for validation\n\n \nOn Studio, go to the Data acquisition tab, and on the UPLOAD DATA section, upload the chosen categories files from your computer:\n \nLeave to the Studio the splitting of the original dataset into train and test and choose the label about that specific data:\n \nRepeat the procedure for all three classes.\n\nSelecting a folder and upload all the files at once is possible.\n\nAt the end, you should see your ‚Äúraw data‚Äù in the Studio:\n \nNote that when you start to upload the data, a pop-up window can appear, asking if you are building an Object Detection project. Select [NO].\n\n\n\n\n\nWe can always change it in the Dashboard section: One label per data item (Image Classification):\n\n\n\n\n\nOptionally, the Studio allows us to explore the data, showing a complete view of all the data in the project. We can clear, inspect, or change labels by clicking on individual data items. In our case, the data seems OK.\n \n\n\n\nIn this phase, we should define how to:\n\nPre-process our data, which consists of resizing the individual images and determining the color depth to use (be it RGB or Grayscale) and\nSpecify a Model, in this case, it will be the Transfer Learning (Images) to fine-tune a pre-trained MobileNet V2 image classification model on our data. This method performs well even with relatively small image datasets (around 150 images in our case).\n\n \nTransfer Learning with MobileNet offers a streamlined approach to model training, which is especially beneficial for resource-constrained environments and projects with limited labeled data. MobileNet, known for its lightweight architecture, is a pre-trained model that has already learned valuable features from a large dataset (ImageNet).\n \nBy leveraging these learned features, you can train a new model for your specific task with fewer data and computational resources and yet achieve competitive accuracy.\n \nThis approach significantly reduces training time and computational cost, making it ideal for quick prototyping and deployment on embedded devices where efficiency is paramount.\nGo to the Impulse Design Tab and create the impulse, defining an image size of 96x96 and squashing them (squared form, without cropping). Select Image and Transfer Learning blocks. Save the Impulse.\n \n\n\nAll the input QVGA/RGB565 images will be converted to 27,640 features \\((96\\times 96\\times 3)\\).\n \nPress [Save parameters] and Generate all features:\n \n\n\n\nIn 2007, Google introduced MobileNetV1, a family of general-purpose computer vision neural networks designed with mobile devices in mind to support classification, detection, and more. MobileNets are small, low-latency, low-power models parameterized to meet the resource constraints of various use cases. in 2018, Google launched MobileNetV2: Inverted Residuals and Linear Bottlenecks.\nMobileNet V1 and MobileNet V2 aim at mobile efficiency and embedded vision applications but differ in architectural complexity and performance. While both use depthwise separable convolutions to reduce the computational cost, MobileNet V2 introduces Inverted Residual Blocks and Linear Bottlenecks to improve performance. These new features allow V2 to capture more complex features using fewer parameters, making it computationally more efficient and generally more accurate than its predecessor. Additionally, V2 employs a non-linear activation in the intermediate expansion layer. It still uses a linear activation for the bottleneck layer, a design choice found to preserve important information through the network. MobileNet V2 offers an optimized architecture for higher accuracy and efficiency and will be used in this project.\nAlthough the base MobileNet architecture is already tiny and has low latency, many times, a specific use case or application may require the model to be even smaller and faster. MobileNets introduces a straightforward parameter \\(\\alpha\\) (alpha) called width multiplier to construct these smaller, less computationally expensive models. The role of the width multiplier \\(\\alpha\\) is that of thinning a network uniformly at each layer.\nEdge Impulse Studio can use both MobileNetV1 (\\(96\\times 96\\) images) and V2 (\\(96\\times 96\\) or \\(160\\times 160\\) images), with several different \\(\\alpha\\) values (from 0.05 to 1.0). For example, you will get the highest accuracy with V2, \\(160\\times 160\\) images, and \\(\\alpha=1.0\\). Of course, there is a trade-off. The higher the accuracy, the more memory (around 1.3 MB RAM and 2.6 MB ROM) will be needed to run the model, implying more latency. The smaller footprint will be obtained at the other extreme with MobileNetV1 and \\(\\alpha=0.10\\) (around 53.2 K RAM and 101 K ROM).\n \nWe will use MobileNetV2 96x96 0.1 ( or 0.05) for this project, with an estimated memory cost of 265.3 KB in RAM. This model should be OK for the Nicla Vision with 1MB of SRAM. On the Transfer Learning Tab, select this model:\n \n\n\n\n\nAnother valuable technique to be used with Deep Learning is Data Augmentation. Data augmentation is a method to improve the accuracy of machine learning models by creating additional artificial data. A data augmentation system makes small, random changes to your training data during the training process (such as flipping, cropping, or rotating the images).\nLooking under the hood, here you can see how Edge Impulse implements a data Augmentation policy on your data:\n# Implements the data augmentation policy\ndef augment_image(image, label):\n    # Flips the image randomly\n    image = tf.image.random_flip_left_right(image)\n\n    # Increase the image size, then randomly crop it down to\n    # the original dimensions\n    resize_factor = random.uniform(1, 1.2)\n    new_height = math.floor(resize_factor * INPUT_SHAPE[0])\n    new_width = math.floor(resize_factor * INPUT_SHAPE[1])\n    image = tf.image.resize_with_crop_or_pad(image, new_height,\n                                             new_width)\n    image = tf.image.random_crop(image, size=INPUT_SHAPE)\n\n    # Vary the brightness of the image\n    image = tf.image.random_brightness(image, max_delta=0.2)\n\n    return image, label\nExposure to these variations during training can help prevent your model from taking shortcuts by ‚Äúmemorizing‚Äù superficial clues in your training data, meaning it may better reflect the deep underlying patterns in your dataset.\nThe final layer of our model will have 12 neurons with a 15% dropout for overfitting prevention. Here is the Training result:\n \nThe result is excellent, with 77 ms of latency (estimated), which should result in around 13 fps (frames per second) during inference.\n\n\n\n \nNow, we should take the data set put aside at the start of the project and run the trained model using it as input:\n \nThe result is, again, excellent.\n \n\n\n\nAt this point, we can deploy the trained model as a firmware (FW) and use the OpenMV IDE to run it using MicroPython, or we can deploy it as a C/C++ or an Arduino library.\n \n\n\nFirst, Let‚Äôs deploy it as an Arduino Library:\n \nWe should install the library as.zip on the Arduino IDE and run the sketch nicla_vision_camera.ino available in Examples under the library name.\n\nNote that Arduino Nicla Vision has, by default, 512 KB of RAM allocated for the M7 core and an additional 244 KB on the M4 address space. In the code, this allocation was changed to 288 kB to guarantee that the model will run on the device (malloc_addblock((void*)0x30000000, 288 * 1024);).\n\nThe result is good, with 86 ms of measured latency.\n \nHere is a short video showing the inference results: \n\n\n\nIt is possible to deploy the trained model to be used with OpenMV in two ways: as a library and as a firmware (FW). Choosing FW, the Edge Impulse Studio generates optimized models, libraries, and frameworks needed to make the inference. Let‚Äôs explore this option.\nSelect OpenMV Firmware on the Deploy Tab and press [Build].\n \nOn the computer, we will find a ZIP file. Open it:\n \nUse the Bootloader tool on the OpenMV IDE to load the FW on your board (1):\n \nSelect the appropriate file (.bin for Nicla-Vision):\n \nAfter the download is finished, press OK:\n \nIf a message says that the FW is outdated, DO NOT UPGRADE. Select [NO].\n \nNow, open the script ei_image_classification.py that was downloaded from the Studio and the.bin file for the Nicla.\n \nRun it. Pointing the camera to the objects we want to classify, the inference result will be displayed on the Serial Terminal.\n \nThe classification result will appear at the Serial Terminal. If it is difficult to read the result, include a new line in the code to add some delay:\nimport time\nWhile True:\n...\n    time.sleep_ms(200)  # Delay for .2 second\n\n\nThe code provided by Edge Impulse can be modified so that we can see, for test reasons, the inference result directly on the image displayed on the OpenMV IDE.\nUpload the code from GitHub, or modify it as below:\n# Marcelo Rovai - NICLA Vision - Image Classification\n# Adapted from Edge Impulse - OpenMV Image Classification Example\n# @24March25\n\nimport sensor\nimport time\nimport ml\n\nsensor.reset()  # Reset and initialize the sensor.\n# Set pixel format to RGB565 (or GRAYSCALE)\nsensor.set_pixformat(sensor.RGB565)\n# Set frame size to QVGA (320x240)\nsensor.set_framesize(sensor.QVGA)\nsensor.set_windowing((240, 240))  # Set 240x240 window.\nsensor.skip_frames(time=2000)  # Let the camera adjust.\n\nmodel = ml.Model(\"trained\")#mobilenet, load_to_fb=True)\n\nclock = time.clock()\n\nwhile True:\n    clock.tick()\n    img = sensor.snapshot()\n\n    fps = clock.fps()\n    lat = clock.avg()\n    print(\"**********\\nPrediction:\")\n    # Combines labels & confidence into a list of tuples and then\n    # sorts that list by the confidence values.\n    sorted_list = sorted(\n        zip(model.labels, model.predict([img])[0].flatten().tolist()),\n      key=lambda x: x[1], reverse=True\n    )\n\n    # Print only the class with the highest probability\n    max_val = sorted_list[0][1]\n    max_lbl = sorted_list[0][0]\n\n    if max_val &lt; 0.5:\n        max_lbl = 'uncertain'\n\n    print(\"{} with a prob of {:.2f}\".format(max_lbl, max_val))\n    print(\"FPS: {:.2f} fps ==&gt; latency: {:.0f} ms\".format(fps, lat))\n\n    # Draw the label with the highest probability to the image viewer\n    img.draw_string(\n    10, 10,\n    max_lbl + \"\\n{:.2f}\".format(max_val),\n    mono_space = False,\n    scale=3\n    )\n\n    time.sleep_ms(500)  # Delay for .5 second\nHere you can see the result:\n \nNote that the latency (136 ms) is almost double of what we got directly with the Arduino IDE. This is because we are using the IDE as an interface and also the time to wait for the camera to be ready. If we start the clock just before the inference, the latency should drop to around 70 ms.\n\nThe NiclaV runs about half as fast when connected to the IDE. The FPS should increase once disconnected.\n\n\n\n\nWhen working with embedded machine learning, we are looking for devices that can continually proceed with the inference and result, taking some action directly on the physical world and not displaying the result on a connected computer. To simulate this, we will light up a different LED for each possible inference result.\n \nTo accomplish that, we should upload the code from GitHub or change the last code to include the LEDs:\n# Marcelo Rovai - NICLA Vision - Image Classification with LEDs\n# Adapted from Edge Impulse - OpenMV Image Classification Example\n# @24Aug23\n\nimport sensor, time, ml\nfrom machine import LED\n\nledRed = LED(\"LED_RED\")\nledGre = LED(\"LED_GREEN\")\nledBlu = LED(\"LED_BLUE\")\n\nsensor.reset()  # Reset and initialize the sensor.\n# Set pixel format to RGB565 (or GRAYSCALE)\nsensor.set_pixformat(sensor.RGB565)\n# Set frame size to QVGA (320x240)\nsensor.set_framesize(sensor.QVGA)\nsensor.set_windowing((240, 240))  # Set 240x240 window.\nsensor.skip_frames(time=2000)  # Let the camera adjust.\n\nmodel = ml.Model(\"trained\")#mobilenet, load_to_fb=True)\n\nledRed.off()\nledGre.off()\nledBlu.off()\n\nclock = time.clock()\n\ndef setLEDs(max_lbl):\n    if max_lbl == 'uncertain‚Äô:\n        ledRed.on()\n        ledGre.off()\n        ledBlu.off()\n\n    if max_lbl == 'periquito‚Äô:\n        ledRed.off()\n        ledGre.on()\n        ledBlu.off()\n\n    if max_lbl == 'robot‚Äô:\n        ledRed.off()\n        ledGre.off()\n        ledBlu.on()\n\n    if max_lbl == 'background‚Äô:\n        ledRed.off()\n        ledGre.off()\n        ledBlu.off()\n\n  while True:\n    img = sensor.snapshot()\n\n    clock.tick()\n    fps = clock.fps()\n    lat = clock.avg()\n    print(\"**********\\nPrediction:\")\n    sorted_list = sorted(\n        zip(model.labels, model.predict([img])[0].flatten().tolist()),\n      key=lambda x: x[1], reverse=True\n    )\n\n    # Print only the class with the highest probability\n    max_val = sorted_list[0][1]\n    max_lbl = sorted_list[0][0]\n\n    if max_val &lt; 0.5:\n        max_lbl = 'uncertain'\n\n    print(\"{} with a prob of {:.2f}\".format(max_lbl, max_val))\n    print(\"FPS: {:.2f} fps ==&gt; latency: {:.0f} ms\".format(fps, lat))\n\n    # Draw the label with the highest probability to the image viewer\n    img.draw_string(\n    10, 10,\n    max_lbl + \"\\n{:.2f}\".format(max_val),\n    mono_space = False,\n    scale=3\n    )\n\n    setLEDs(max_lbl)\n    time.sleep_ms(200)  # Delay for .2 second\nNow, each time that a class scores a result greater than 0.8, the correspondent LED will be lit:\n\nLed Red 0n: Uncertain (no class is over 0.8)\nLed Green 0n: Periquito &gt; 0.8\nLed Blue 0n: Robot &gt; 0.8\nAll LEDs Off: Background &gt; 0.8\n\nHere is the result:\n \nIn more detail\n \n\n\n\n\n\nSeveral development boards can be used for embedded machine learning (TinyML), and the most common ones for Computer Vision applications (consuming low energy), are the ESP32 CAM, the Seeed XIAO ESP32S3 Sense, the Arduino Nicla Vison, and the Arduino Portenta.\n \nCatching the opportunity, the same trained model was deployed on the ESP-CAM, the XIAO, and the Portenta (in this one, the model was trained again, using grayscaled images to be compatible with its camera). Here is the result, deploying the models as Arduino‚Äôs Library:\n \n\n\n\nBefore we finish, consider that Computer Vision is more than just image classification. For example, you can develop Edge Machine Learning projects around vision in several areas, such as:\n\nAutonomous Vehicles: Use sensor fusion, lidar data, and computer vision algorithms to navigate and make decisions.\nHealthcare: Automated diagnosis of diseases through MRI, X-ray, and CT scan image analysis\nRetail: Automated checkout systems that identify products as they pass through a scanner.\nSecurity and Surveillance: Facial recognition, anomaly detection, and object tracking in real-time video feeds.\nAugmented Reality: Object detection and classification to overlay digital information in the real world.\nIndustrial Automation: Visual inspection of products, predictive maintenance, and robot and drone guidance.\nAgriculture: Drone-based crop monitoring and automated harvesting.\nNatural Language Processing: Image captioning and visual question answering.\nGesture Recognition: For gaming, sign language translation, and human-machine interaction.\nContent Recommendation: Image-based recommendation systems in e-commerce.\n\n\n\n\n\nMicropython codes\nDataset\nEdge Impulse Project",
    "crumbs": [
      "Arduino Nicla Vision",
      "Image Classification"
    ]
  },
  {
    "objectID": "contents/arduino/nicla_vision/image_classification/image_classification.html#sec-image-classification-overview-7420",
    "href": "contents/arduino/nicla_vision/image_classification/image_classification.html#sec-image-classification-overview-7420",
    "title": "Image Classification",
    "section": "",
    "text": "As we initiate our studies into embedded machine learning or TinyML, it‚Äôs impossible to overlook the transformative impact of Computer Vision (CV) and Artificial Intelligence (AI) in our lives. These two intertwined disciplines redefine what machines can perceive and accomplish, from autonomous vehicles and robotics to healthcare and surveillance.\nMore and more, we are facing an artificial intelligence (AI) revolution where, as stated by Gartner, Edge AI has a very high impact potential, and it is for now!\n \nIn the ‚Äúbullseye‚Äù of the Radar is the Edge Computer Vision, and when we talk about Machine Learning (ML) applied to vision, the first thing that comes to mind is Image Classification, a kind of ML ‚ÄúHello World‚Äù!\nThis lab will explore a computer vision project utilizing Convolutional Neural Networks (CNNs) for real-time image classification. Leveraging TensorFlow‚Äôs robust ecosystem, we‚Äôll implement a pre-trained MobileNet model and adapt it for edge deployment. The focus will be optimizing the model to run efficiently on resource-constrained hardware without sacrificing accuracy.\nWe‚Äôll employ techniques like quantization and pruning to reduce the computational load. By the end of this tutorial, you‚Äôll have a working prototype capable of classifying images in real-time, all running on a low-power embedded system based on the Arduino Nicla Vision board.",
    "crumbs": [
      "Arduino Nicla Vision",
      "Image Classification"
    ]
  },
  {
    "objectID": "contents/arduino/nicla_vision/image_classification/image_classification.html#sec-image-classification-computer-vision-f732",
    "href": "contents/arduino/nicla_vision/image_classification/image_classification.html#sec-image-classification-computer-vision-f732",
    "title": "Image Classification",
    "section": "",
    "text": "At its core, computer vision enables machines to interpret and make decisions based on visual data from the world, essentially mimicking the capability of the human optical system. Conversely, AI is a broader field encompassing machine learning, natural language processing, and robotics, among other technologies. When you bring AI algorithms into computer vision projects, you supercharge the system‚Äôs ability to understand, interpret, and react to visual stimuli.\nWhen discussing Computer Vision projects applied to embedded devices, the most common applications that come to mind are Image Classification and Object Detection.\n \nBoth models can be implemented on tiny devices like the Arduino Nicla Vision and used on real projects. In this chapter, we will cover Image Classification.",
    "crumbs": [
      "Arduino Nicla Vision",
      "Image Classification"
    ]
  },
  {
    "objectID": "contents/arduino/nicla_vision/image_classification/image_classification.html#sec-image-classification-image-classification-project-goal-4eaa",
    "href": "contents/arduino/nicla_vision/image_classification/image_classification.html#sec-image-classification-image-classification-project-goal-4eaa",
    "title": "Image Classification",
    "section": "",
    "text": "The first step in any ML project is to define the goal. In this case, the goal is to detect and classify two specific objects present in one image. For this project, we will use two small toys: a robot and a small Brazilian parrot (named Periquito). We will also collect images of a background where those two objects are absent.",
    "crumbs": [
      "Arduino Nicla Vision",
      "Image Classification"
    ]
  },
  {
    "objectID": "contents/arduino/nicla_vision/image_classification/image_classification.html#sec-image-classification-data-collection-d6bb",
    "href": "contents/arduino/nicla_vision/image_classification/image_classification.html#sec-image-classification-data-collection-d6bb",
    "title": "Image Classification",
    "section": "",
    "text": "Once we have defined our Machine Learning project goal, the next and most crucial step is collecting the dataset. For image capturing, we can use:\n\nWeb Serial Camera tool,\nEdge Impulse Studio,\nOpenMV IDE,\nA smartphone.\n\nHere, we will use the OpenMV IDE.\n\n\nFirst, we should create a folder on our computer where the data will be saved, for example, ‚Äúdata.‚Äù Next, on the OpenMV IDE, we go to Tools &gt; Dataset Editor and select New Dataset to start the dataset collection:\n \nThe IDE will ask us to open the file where the data will be saved. Choose the ‚Äúdata‚Äù folder that was created. Note that new icons will appear on the Left panel.\n \nUsing the upper icon (1), enter with the first class name, for example, ‚Äúperiquito‚Äù:\n \nRunning the dataset_capture_script.py and clicking on the camera icon (2) will start capturing images:\n \nRepeat the same procedure with the other classes.\n \n\nWe suggest around 50 to 60 images from each category. Try to capture different angles, backgrounds, and light conditions.\n\nThe stored images use a QVGA frame size of \\(320\\times 240\\) and the RGB565 (color pixel format).\nAfter capturing the dataset, close the Dataset Editor Tool on the Tools &gt; Dataset Editor.\nWe will end up with a dataset on our computer that contains three classes: periquito, robot, and background.\n \nWe should return to Edge Impulse Studio and upload the dataset to our created project.",
    "crumbs": [
      "Arduino Nicla Vision",
      "Image Classification"
    ]
  },
  {
    "objectID": "contents/arduino/nicla_vision/image_classification/image_classification.html#sec-image-classification-training-model-edge-impulse-studio-3f42",
    "href": "contents/arduino/nicla_vision/image_classification/image_classification.html#sec-image-classification-training-model-edge-impulse-studio-3f42",
    "title": "Image Classification",
    "section": "",
    "text": "We will use the Edge Impulse Studio to train our model. Enter the account credentials and create a new project:\n \n\nHere, you can clone a similar project: NICLA-Vision_Image_Classification.",
    "crumbs": [
      "Arduino Nicla Vision",
      "Image Classification"
    ]
  },
  {
    "objectID": "contents/arduino/nicla_vision/image_classification/image_classification.html#sec-image-classification-dataset-db33",
    "href": "contents/arduino/nicla_vision/image_classification/image_classification.html#sec-image-classification-dataset-db33",
    "title": "Image Classification",
    "section": "",
    "text": "Using the EI Studio (or Studio), we will go over four main steps to have our model ready for use on the Nicla Vision board: Dataset, Impulse, Tests, and Deploy (on the Edge Device, in this case, the NiclaV).\n \nRegarding the Dataset, it is essential to point out that our Original Dataset, captured with the OpenMV IDE, will be split into Training, Validation, and Test. The Test Set will be spared from the beginning and reserved for use only in the Test phase after training. The Validation Set will be used during training.\n\nThe EI Studio will take a percentage of training data to be used for validation\n\n \nOn Studio, go to the Data acquisition tab, and on the UPLOAD DATA section, upload the chosen categories files from your computer:\n \nLeave to the Studio the splitting of the original dataset into train and test and choose the label about that specific data:\n \nRepeat the procedure for all three classes.\n\nSelecting a folder and upload all the files at once is possible.\n\nAt the end, you should see your ‚Äúraw data‚Äù in the Studio:\n \nNote that when you start to upload the data, a pop-up window can appear, asking if you are building an Object Detection project. Select [NO].\n\n\n\n\n\nWe can always change it in the Dashboard section: One label per data item (Image Classification):\n\n\n\n\n\nOptionally, the Studio allows us to explore the data, showing a complete view of all the data in the project. We can clear, inspect, or change labels by clicking on individual data items. In our case, the data seems OK.",
    "crumbs": [
      "Arduino Nicla Vision",
      "Image Classification"
    ]
  },
  {
    "objectID": "contents/arduino/nicla_vision/image_classification/image_classification.html#sec-image-classification-impulse-design-fb27",
    "href": "contents/arduino/nicla_vision/image_classification/image_classification.html#sec-image-classification-impulse-design-fb27",
    "title": "Image Classification",
    "section": "",
    "text": "In this phase, we should define how to:\n\nPre-process our data, which consists of resizing the individual images and determining the color depth to use (be it RGB or Grayscale) and\nSpecify a Model, in this case, it will be the Transfer Learning (Images) to fine-tune a pre-trained MobileNet V2 image classification model on our data. This method performs well even with relatively small image datasets (around 150 images in our case).\n\n \nTransfer Learning with MobileNet offers a streamlined approach to model training, which is especially beneficial for resource-constrained environments and projects with limited labeled data. MobileNet, known for its lightweight architecture, is a pre-trained model that has already learned valuable features from a large dataset (ImageNet).\n \nBy leveraging these learned features, you can train a new model for your specific task with fewer data and computational resources and yet achieve competitive accuracy.\n \nThis approach significantly reduces training time and computational cost, making it ideal for quick prototyping and deployment on embedded devices where efficiency is paramount.\nGo to the Impulse Design Tab and create the impulse, defining an image size of 96x96 and squashing them (squared form, without cropping). Select Image and Transfer Learning blocks. Save the Impulse.\n \n\n\nAll the input QVGA/RGB565 images will be converted to 27,640 features \\((96\\times 96\\times 3)\\).\n \nPress [Save parameters] and Generate all features:\n \n\n\n\nIn 2007, Google introduced MobileNetV1, a family of general-purpose computer vision neural networks designed with mobile devices in mind to support classification, detection, and more. MobileNets are small, low-latency, low-power models parameterized to meet the resource constraints of various use cases. in 2018, Google launched MobileNetV2: Inverted Residuals and Linear Bottlenecks.\nMobileNet V1 and MobileNet V2 aim at mobile efficiency and embedded vision applications but differ in architectural complexity and performance. While both use depthwise separable convolutions to reduce the computational cost, MobileNet V2 introduces Inverted Residual Blocks and Linear Bottlenecks to improve performance. These new features allow V2 to capture more complex features using fewer parameters, making it computationally more efficient and generally more accurate than its predecessor. Additionally, V2 employs a non-linear activation in the intermediate expansion layer. It still uses a linear activation for the bottleneck layer, a design choice found to preserve important information through the network. MobileNet V2 offers an optimized architecture for higher accuracy and efficiency and will be used in this project.\nAlthough the base MobileNet architecture is already tiny and has low latency, many times, a specific use case or application may require the model to be even smaller and faster. MobileNets introduces a straightforward parameter \\(\\alpha\\) (alpha) called width multiplier to construct these smaller, less computationally expensive models. The role of the width multiplier \\(\\alpha\\) is that of thinning a network uniformly at each layer.\nEdge Impulse Studio can use both MobileNetV1 (\\(96\\times 96\\) images) and V2 (\\(96\\times 96\\) or \\(160\\times 160\\) images), with several different \\(\\alpha\\) values (from 0.05 to 1.0). For example, you will get the highest accuracy with V2, \\(160\\times 160\\) images, and \\(\\alpha=1.0\\). Of course, there is a trade-off. The higher the accuracy, the more memory (around 1.3 MB RAM and 2.6 MB ROM) will be needed to run the model, implying more latency. The smaller footprint will be obtained at the other extreme with MobileNetV1 and \\(\\alpha=0.10\\) (around 53.2 K RAM and 101 K ROM).\n \nWe will use MobileNetV2 96x96 0.1 ( or 0.05) for this project, with an estimated memory cost of 265.3 KB in RAM. This model should be OK for the Nicla Vision with 1MB of SRAM. On the Transfer Learning Tab, select this model:",
    "crumbs": [
      "Arduino Nicla Vision",
      "Image Classification"
    ]
  },
  {
    "objectID": "contents/arduino/nicla_vision/image_classification/image_classification.html#sec-image-classification-model-training-1ea7",
    "href": "contents/arduino/nicla_vision/image_classification/image_classification.html#sec-image-classification-model-training-1ea7",
    "title": "Image Classification",
    "section": "",
    "text": "Another valuable technique to be used with Deep Learning is Data Augmentation. Data augmentation is a method to improve the accuracy of machine learning models by creating additional artificial data. A data augmentation system makes small, random changes to your training data during the training process (such as flipping, cropping, or rotating the images).\nLooking under the hood, here you can see how Edge Impulse implements a data Augmentation policy on your data:\n# Implements the data augmentation policy\ndef augment_image(image, label):\n    # Flips the image randomly\n    image = tf.image.random_flip_left_right(image)\n\n    # Increase the image size, then randomly crop it down to\n    # the original dimensions\n    resize_factor = random.uniform(1, 1.2)\n    new_height = math.floor(resize_factor * INPUT_SHAPE[0])\n    new_width = math.floor(resize_factor * INPUT_SHAPE[1])\n    image = tf.image.resize_with_crop_or_pad(image, new_height,\n                                             new_width)\n    image = tf.image.random_crop(image, size=INPUT_SHAPE)\n\n    # Vary the brightness of the image\n    image = tf.image.random_brightness(image, max_delta=0.2)\n\n    return image, label\nExposure to these variations during training can help prevent your model from taking shortcuts by ‚Äúmemorizing‚Äù superficial clues in your training data, meaning it may better reflect the deep underlying patterns in your dataset.\nThe final layer of our model will have 12 neurons with a 15% dropout for overfitting prevention. Here is the Training result:\n \nThe result is excellent, with 77 ms of latency (estimated), which should result in around 13 fps (frames per second) during inference.",
    "crumbs": [
      "Arduino Nicla Vision",
      "Image Classification"
    ]
  },
  {
    "objectID": "contents/arduino/nicla_vision/image_classification/image_classification.html#sec-image-classification-model-testing-ca42",
    "href": "contents/arduino/nicla_vision/image_classification/image_classification.html#sec-image-classification-model-testing-ca42",
    "title": "Image Classification",
    "section": "",
    "text": "Now, we should take the data set put aside at the start of the project and run the trained model using it as input:\n \nThe result is, again, excellent.",
    "crumbs": [
      "Arduino Nicla Vision",
      "Image Classification"
    ]
  },
  {
    "objectID": "contents/arduino/nicla_vision/image_classification/image_classification.html#sec-image-classification-deploying-model-8f73",
    "href": "contents/arduino/nicla_vision/image_classification/image_classification.html#sec-image-classification-deploying-model-8f73",
    "title": "Image Classification",
    "section": "",
    "text": "At this point, we can deploy the trained model as a firmware (FW) and use the OpenMV IDE to run it using MicroPython, or we can deploy it as a C/C++ or an Arduino library.\n \n\n\nFirst, Let‚Äôs deploy it as an Arduino Library:\n \nWe should install the library as.zip on the Arduino IDE and run the sketch nicla_vision_camera.ino available in Examples under the library name.\n\nNote that Arduino Nicla Vision has, by default, 512 KB of RAM allocated for the M7 core and an additional 244 KB on the M4 address space. In the code, this allocation was changed to 288 kB to guarantee that the model will run on the device (malloc_addblock((void*)0x30000000, 288 * 1024);).\n\nThe result is good, with 86 ms of measured latency.\n \nHere is a short video showing the inference results: \n\n\n\nIt is possible to deploy the trained model to be used with OpenMV in two ways: as a library and as a firmware (FW). Choosing FW, the Edge Impulse Studio generates optimized models, libraries, and frameworks needed to make the inference. Let‚Äôs explore this option.\nSelect OpenMV Firmware on the Deploy Tab and press [Build].\n \nOn the computer, we will find a ZIP file. Open it:\n \nUse the Bootloader tool on the OpenMV IDE to load the FW on your board (1):\n \nSelect the appropriate file (.bin for Nicla-Vision):\n \nAfter the download is finished, press OK:\n \nIf a message says that the FW is outdated, DO NOT UPGRADE. Select [NO].\n \nNow, open the script ei_image_classification.py that was downloaded from the Studio and the.bin file for the Nicla.\n \nRun it. Pointing the camera to the objects we want to classify, the inference result will be displayed on the Serial Terminal.\n \nThe classification result will appear at the Serial Terminal. If it is difficult to read the result, include a new line in the code to add some delay:\nimport time\nWhile True:\n...\n    time.sleep_ms(200)  # Delay for .2 second\n\n\nThe code provided by Edge Impulse can be modified so that we can see, for test reasons, the inference result directly on the image displayed on the OpenMV IDE.\nUpload the code from GitHub, or modify it as below:\n# Marcelo Rovai - NICLA Vision - Image Classification\n# Adapted from Edge Impulse - OpenMV Image Classification Example\n# @24March25\n\nimport sensor\nimport time\nimport ml\n\nsensor.reset()  # Reset and initialize the sensor.\n# Set pixel format to RGB565 (or GRAYSCALE)\nsensor.set_pixformat(sensor.RGB565)\n# Set frame size to QVGA (320x240)\nsensor.set_framesize(sensor.QVGA)\nsensor.set_windowing((240, 240))  # Set 240x240 window.\nsensor.skip_frames(time=2000)  # Let the camera adjust.\n\nmodel = ml.Model(\"trained\")#mobilenet, load_to_fb=True)\n\nclock = time.clock()\n\nwhile True:\n    clock.tick()\n    img = sensor.snapshot()\n\n    fps = clock.fps()\n    lat = clock.avg()\n    print(\"**********\\nPrediction:\")\n    # Combines labels & confidence into a list of tuples and then\n    # sorts that list by the confidence values.\n    sorted_list = sorted(\n        zip(model.labels, model.predict([img])[0].flatten().tolist()),\n      key=lambda x: x[1], reverse=True\n    )\n\n    # Print only the class with the highest probability\n    max_val = sorted_list[0][1]\n    max_lbl = sorted_list[0][0]\n\n    if max_val &lt; 0.5:\n        max_lbl = 'uncertain'\n\n    print(\"{} with a prob of {:.2f}\".format(max_lbl, max_val))\n    print(\"FPS: {:.2f} fps ==&gt; latency: {:.0f} ms\".format(fps, lat))\n\n    # Draw the label with the highest probability to the image viewer\n    img.draw_string(\n    10, 10,\n    max_lbl + \"\\n{:.2f}\".format(max_val),\n    mono_space = False,\n    scale=3\n    )\n\n    time.sleep_ms(500)  # Delay for .5 second\nHere you can see the result:\n \nNote that the latency (136 ms) is almost double of what we got directly with the Arduino IDE. This is because we are using the IDE as an interface and also the time to wait for the camera to be ready. If we start the clock just before the inference, the latency should drop to around 70 ms.\n\nThe NiclaV runs about half as fast when connected to the IDE. The FPS should increase once disconnected.\n\n\n\n\nWhen working with embedded machine learning, we are looking for devices that can continually proceed with the inference and result, taking some action directly on the physical world and not displaying the result on a connected computer. To simulate this, we will light up a different LED for each possible inference result.\n \nTo accomplish that, we should upload the code from GitHub or change the last code to include the LEDs:\n# Marcelo Rovai - NICLA Vision - Image Classification with LEDs\n# Adapted from Edge Impulse - OpenMV Image Classification Example\n# @24Aug23\n\nimport sensor, time, ml\nfrom machine import LED\n\nledRed = LED(\"LED_RED\")\nledGre = LED(\"LED_GREEN\")\nledBlu = LED(\"LED_BLUE\")\n\nsensor.reset()  # Reset and initialize the sensor.\n# Set pixel format to RGB565 (or GRAYSCALE)\nsensor.set_pixformat(sensor.RGB565)\n# Set frame size to QVGA (320x240)\nsensor.set_framesize(sensor.QVGA)\nsensor.set_windowing((240, 240))  # Set 240x240 window.\nsensor.skip_frames(time=2000)  # Let the camera adjust.\n\nmodel = ml.Model(\"trained\")#mobilenet, load_to_fb=True)\n\nledRed.off()\nledGre.off()\nledBlu.off()\n\nclock = time.clock()\n\ndef setLEDs(max_lbl):\n    if max_lbl == 'uncertain‚Äô:\n        ledRed.on()\n        ledGre.off()\n        ledBlu.off()\n\n    if max_lbl == 'periquito‚Äô:\n        ledRed.off()\n        ledGre.on()\n        ledBlu.off()\n\n    if max_lbl == 'robot‚Äô:\n        ledRed.off()\n        ledGre.off()\n        ledBlu.on()\n\n    if max_lbl == 'background‚Äô:\n        ledRed.off()\n        ledGre.off()\n        ledBlu.off()\n\n  while True:\n    img = sensor.snapshot()\n\n    clock.tick()\n    fps = clock.fps()\n    lat = clock.avg()\n    print(\"**********\\nPrediction:\")\n    sorted_list = sorted(\n        zip(model.labels, model.predict([img])[0].flatten().tolist()),\n      key=lambda x: x[1], reverse=True\n    )\n\n    # Print only the class with the highest probability\n    max_val = sorted_list[0][1]\n    max_lbl = sorted_list[0][0]\n\n    if max_val &lt; 0.5:\n        max_lbl = 'uncertain'\n\n    print(\"{} with a prob of {:.2f}\".format(max_lbl, max_val))\n    print(\"FPS: {:.2f} fps ==&gt; latency: {:.0f} ms\".format(fps, lat))\n\n    # Draw the label with the highest probability to the image viewer\n    img.draw_string(\n    10, 10,\n    max_lbl + \"\\n{:.2f}\".format(max_val),\n    mono_space = False,\n    scale=3\n    )\n\n    setLEDs(max_lbl)\n    time.sleep_ms(200)  # Delay for .2 second\nNow, each time that a class scores a result greater than 0.8, the correspondent LED will be lit:\n\nLed Red 0n: Uncertain (no class is over 0.8)\nLed Green 0n: Periquito &gt; 0.8\nLed Blue 0n: Robot &gt; 0.8\nAll LEDs Off: Background &gt; 0.8\n\nHere is the result:\n \nIn more detail",
    "crumbs": [
      "Arduino Nicla Vision",
      "Image Classification"
    ]
  },
  {
    "objectID": "contents/arduino/nicla_vision/image_classification/image_classification.html#sec-image-classification-image-classification-nonofficial-benchmark-f84a",
    "href": "contents/arduino/nicla_vision/image_classification/image_classification.html#sec-image-classification-image-classification-nonofficial-benchmark-f84a",
    "title": "Image Classification",
    "section": "",
    "text": "Several development boards can be used for embedded machine learning (TinyML), and the most common ones for Computer Vision applications (consuming low energy), are the ESP32 CAM, the Seeed XIAO ESP32S3 Sense, the Arduino Nicla Vison, and the Arduino Portenta.\n \nCatching the opportunity, the same trained model was deployed on the ESP-CAM, the XIAO, and the Portenta (in this one, the model was trained again, using grayscaled images to be compatible with its camera). Here is the result, deploying the models as Arduino‚Äôs Library:",
    "crumbs": [
      "Arduino Nicla Vision",
      "Image Classification"
    ]
  },
  {
    "objectID": "contents/arduino/nicla_vision/image_classification/image_classification.html#sec-image-classification-summary-aea6",
    "href": "contents/arduino/nicla_vision/image_classification/image_classification.html#sec-image-classification-summary-aea6",
    "title": "Image Classification",
    "section": "",
    "text": "Before we finish, consider that Computer Vision is more than just image classification. For example, you can develop Edge Machine Learning projects around vision in several areas, such as:\n\nAutonomous Vehicles: Use sensor fusion, lidar data, and computer vision algorithms to navigate and make decisions.\nHealthcare: Automated diagnosis of diseases through MRI, X-ray, and CT scan image analysis\nRetail: Automated checkout systems that identify products as they pass through a scanner.\nSecurity and Surveillance: Facial recognition, anomaly detection, and object tracking in real-time video feeds.\nAugmented Reality: Object detection and classification to overlay digital information in the real world.\nIndustrial Automation: Visual inspection of products, predictive maintenance, and robot and drone guidance.\nAgriculture: Drone-based crop monitoring and automated harvesting.\nNatural Language Processing: Image captioning and visual question answering.\nGesture Recognition: For gaming, sign language translation, and human-machine interaction.\nContent Recommendation: Image-based recommendation systems in e-commerce.",
    "crumbs": [
      "Arduino Nicla Vision",
      "Image Classification"
    ]
  },
  {
    "objectID": "contents/arduino/nicla_vision/image_classification/image_classification.html#sec-image-classification-resources-7c21",
    "href": "contents/arduino/nicla_vision/image_classification/image_classification.html#sec-image-classification-resources-7c21",
    "title": "Image Classification",
    "section": "",
    "text": "Micropython codes\nDataset\nEdge Impulse Project",
    "crumbs": [
      "Arduino Nicla Vision",
      "Image Classification"
    ]
  },
  {
    "objectID": "contents/arduino/nicla_vision/kws/kws.html",
    "href": "contents/arduino/nicla_vision/kws/kws.html",
    "title": "Keyword Spotting (KWS)",
    "section": "",
    "text": "DALL¬∑E 3 Prompt: 1950s style cartoon scene set in a vintage audio research room. Two Afro-American female scientists are at the center. One holds a magnifying glass, closely examining ancient circuitry, while the other takes notes. On their wooden table, there are multiple boards with sensors, notably featuring a microphone. Behind these boards, a computer with a large, rounded back displays the Arduino IDE. The IDE showcases code for LED pin assignments and machine learning inference for voice command detection. A distinct window in the IDE, the Serial Monitor, reveals outputs indicating the spoken commands ‚Äòyes‚Äô and ‚Äòno‚Äô. The room ambiance is nostalgic with vintage lamps, classic audio analysis tools, and charts depicting FFT graphs and time-domain curves.\n\n\n\n\nHaving already explored the Nicla Vision board in the Image Classification and Object Detection applications, we are now shifting our focus to voice-activated applications with a project on Keyword Spotting (KWS).\nAs introduced in the Feature Engineering for Audio Classification Hands-On tutorial, Keyword Spotting (KWS) is integrated into many voice recognition systems, enabling devices to respond to specific words or phrases. While this technology underpins popular devices like Google Assistant or Amazon Alexa, it‚Äôs equally applicable and feasible on smaller, low-power devices. This tutorial will guide you through implementing a KWS system using TinyML on the Nicla Vision development board equipped with a digital microphone.\nOur model will be designed to recognize keywords that can trigger device wake-up or specific actions, bringing them to life with voice-activated commands.\n\n\n\nAs said, voice assistants on the market, like Google Home or Amazon Echo-Dot, only react to humans when they are ‚Äúwaked up‚Äù by particular keywords such as ‚Äù Hey Google‚Äù on the first one and ‚ÄúAlexa‚Äù on the second.\n \nIn other words, recognizing voice commands is based on a multi-stage model or Cascade Detection.\n \nStage 1: A small microprocessor inside the Echo Dot or Google Home continuously listens, waiting for the keyword to be spotted, using a TinyML model at the edge (KWS application).\nStage 2: Only when triggered by the KWS application on Stage 1 is the data sent to the cloud and processed on a larger model.\nThe video below shows an example of a Google Assistant being programmed on a Raspberry Pi (Stage 2), with an Arduino Nano 33 BLE as the TinyML device (Stage 1).\n\n\nTo explore the above Google Assistant project, please see the tutorial: Building an Intelligent Voice Assistant From Scratch.\n\nIn this KWS project, we will focus on Stage 1 (KWS or Keyword Spotting), where we will use the Nicla Vision, which has a digital microphone that will be used to spot the keyword.\n\n\n\nThe diagram below gives an idea of how the final KWS application should work (during inference):\n \nOur KWS application will recognize four classes of sound:\n\nYES (Keyword 1)\nNO (Keyword 2)\nNOISE (no words spoken; only background noise is present)\nUNKNOWN (a mix of different words than YES and NO)\n\n\nFor real-world projects, it is always advisable to include other sounds besides the keywords, such as ‚ÄúNoise‚Äù (or Background) and ‚ÄúUnknown.‚Äù\n\n\n\nThe main component of the KWS application is its model. So, we must train such a model with our specific keywords, noise, and other words (the ‚Äúunknown‚Äù):\n \n\n\n\n\nThe critical component of any Machine Learning Workflow is the dataset. Once we have decided on specific keywords, in our case (YES and NO), we can take advantage of the dataset developed by Pete Warden, ‚ÄúSpeech Commands: A Dataset for Limited-Vocabulary Speech Recognition.‚Äù This dataset has 35 keywords (with +1,000 samples each), such as yes, no, stop, and go. In words such as yes and no, we can get 1,500 samples.\nYou can download a small portion of the dataset from Edge Studio (Keyword spotting pre-built dataset), which includes samples from the four classes we will use in this project: yes, no, noise, and background. For this, follow the steps below:\n\nDownload the keywords dataset.\nUnzip the file to a location of your choice.\n\n\n\nInitiate a new project at Edge Impulse Studio (EIS) and select the Upload Existing Data tool in the Data Acquisition section. Choose the files to be uploaded:\n \nDefine the Label, select Automatically split between train and test, and Upload data to the EIS. Repeat for all classes.\n \nThe dataset will now appear in the Data acquisition section. Note that the approximately 6,000 samples (1,500 for each class) are split into Train (4,800) and Test (1,200) sets.\n \n\n\n\nAlthough we have a lot of data from Pete‚Äôs dataset, collecting some words spoken by us is advised. When working with accelerometers, creating a dataset with data captured by the same type of sensor is essential. In the case of sound, this is optional because what we will classify is, in reality, audio data.\n\nThe key difference between sound and audio is the type of energy. Sound is mechanical perturbation (longitudinal sound waves) that propagate through a medium, causing variations of pressure in it. Audio is an electrical (analog or digital) signal representing sound.\n\nWhen we pronounce a keyword, the sound waves should be converted to audio data. The conversion should be done by sampling the signal generated by the microphone at a 16 KHz frequency with 16-bit per sample amplitude.\nSo, any device that can generate audio data with this basic specification (16 KHz/16 bits) will work fine. As a device, we can use the NiclaV, a computer, or even your mobile phone.\n \n\n\nAs we learned in the chapter Setup Nicla Vision, EIS officially supports the Nicla Vision, which simplifies the capture of the data from its sensors, including the microphone. So, please create a new project on EIS and connect the Nicla to it, following these steps:\n\nDownload the last updated EIS Firmware and unzip it.\nOpen the zip file on your computer and select the uploader corresponding to your OS:\n\n \n\nPut the NiclaV in Boot Mode by pressing the reset button twice.\nUpload the binary arduino-nicla-vision.bin to your board by running the batch code corresponding to your OS.\n\nGo to your project on EIS, and on the Data Acquisition tab, select WebUSB. A window will pop up; choose the option that shows that the Nicla is paired and press [Connect].\nYou can choose which sensor data to pick in the Collect Data section on the Data Acquisition tab. Select: Built-in microphone, define your label (for example, yes), the sampling Frequency[16000Hz], and the Sample length (in milliseconds), for example [10s]. Start sampling.\n \nData on Pete‚Äôs dataset have a length of 1s, but the recorded samples are 10s long and must be split into 1s samples. Click on three dots after the sample name and select Split sample.\nA window will pop up with the Split tool.\n \nOnce inside the tool, split the data into 1-second (1000 ms) records. If necessary, add or remove segments. This procedure should be repeated for all new samples.\n\n\n\nYou can also use your PC or smartphone to capture audio data, using a sampling frequency of 16 KHz and a bit depth of 16.\nGo to Devices, scan the QR Code using your phone, and click on the link. A data Collection app will appear in your browser. Select Collecting Audio, and define your Label, data capture Length, and Category.\n \nRepeat the same procedure used with the NiclaV.\n\nNote that any app, such as Audacity, can be used for audio recording, provided you use 16 KHz/16-bit depth samples.\n\n\n\n\n\n\nAn impulse takes raw data, uses signal processing to extract features, and then uses a learning block to classify new data.\n\n\n \nFirst, we will take the data points with a 1-second window, augmenting the data and sliding that window in 500 ms intervals. Note that the option zero-pad data is set. It is essential to fill with ‚Äòzeros‚Äô samples smaller than 1 second (in some cases, some samples can result smaller than the 1000 ms window on the split tool to avoid noise and spikes).\nEach 1-second audio sample should be pre-processed and converted to an image (for example, \\(13\\times 49\\times 1\\)). As discussed in the Feature Engineering for Audio Classification Hands-On tutorial, we will use Audio (MFCC), which extracts features from audio signals using Mel Frequency Cepstral Coefficients, which are well suited for the human voice, our case here.\nNext, we select the Classification block to build our model from scratch using a Convolution Neural Network (CNN).\n\nAlternatively, you can use the Transfer Learning (Keyword Spotting) block, which fine-tunes a pre-trained keyword spotting model on your data. This approach has good performance with relatively small keyword datasets.\n\n\n\n\nThe following step is to create the features to be trained in the next phase:\nWe could keep the default parameter values, but we will use the DSP Autotune parameters option.\n \nWe will take the Raw features (our 1-second, 16 KHz sampled audio data) and use the MFCC processing block to calculate the Processed features. For every 16,000 raw features (16,000 \\(\\times\\) 1 second), we will get 637 processed features \\((13\\times 49)\\).\n \nThe result shows that we only used a small amount of memory to pre-process data (16 KB) and a latency of 34 ms, which is excellent. For example, on an Arduino Nano (Cortex-M4f @ 64 MHz), the same pre-process will take around 480 ms. The parameters chosen, such as the FFT length [512], will significantly impact the latency.\nNow, let‚Äôs Save parameters and move to the Generated features tab, where the actual features will be generated. Using UMAP, a dimension reduction technique, the Feature explorer shows how the features are distributed on a two-dimensional plot.\n \nThe result seems OK, with a visually clear separation between yes features (in red) and no features (in blue). The unknown features seem nearer to the no space than the yes. This suggests that the keyword no has more propensity to false positives.\n\n\n\nTo understand better how the raw sound is preprocessed, look at the Feature Engineering for Audio Classification chapter. You can play with the MFCC features generation by downloading this notebook from GitHub or [Opening it In Colab]\n\n\n\n\nWe will use a simple Convolution Neural Network (CNN) model, tested with 1D and 2D convolutions. The basic architecture has two blocks of Convolution + MaxPooling ([8] and [16] filters, respectively) and a Dropout of [0.25] for the 1D and [0.5] for the 2D. For the last layer, after Flattening, we have [4] neurons, one for each class:\n \nAs hyper-parameters, we will have a Learning Rate of [0.005] and a model trained by [100] epochs. We will also include a data augmentation method based on SpecAugment. We trained the 1D and the 2D models with the same hyperparameters. The 1D architecture had a better overall result (90.5% accuracy when compared with 88% of the 2D, so we will use the 1D.\n \n\nUsing 1D convolutions is more efficient because it requires fewer parameters than 2D convolutions, making them more suitable for resource-constrained environments.\n\nIt is also interesting to pay attention to the 1D Confusion Matrix. The F1 Score for yes is 95%, and for no, 91%. That was expected by what we saw with the Feature Explorer (no and unknown at close distance). In trying to improve the result, you can inspect closely the results of the samples with an error.\n \nListen to the samples that went wrong. For example, for yes, most of the mistakes were related to a yes pronounced as ‚Äúyeh‚Äù. You can acquire additional samples and then retrain your model.\n\n\nIf you want to understand what is happening ‚Äúunder the hood,‚Äù you can download the pre-processed dataset (MFCC training data) from the Dashboard tab and run this Jupyter Notebook, playing with the code or [Opening it In Colab]. For example, you can analyze the accuracy by each epoch:\n \n\n\n\n\nTesting the model with the data reserved for training (Test Data), we got an accuracy of approximately 76%.\n \nInspecting the F1 score, we can see that for YES, we got 0.90, an excellent result since we expect to use this keyword as the primary ‚Äútrigger‚Äù for our KWS project. The worst result (0.70) is for UNKNOWN, which is OK.\nFor NO, we got 0.72, which was expected, but to improve this result, we can move the samples that were not correctly classified to the training dataset and then repeat the training process.\n\n\nWe can proceed to the project‚Äôs next step but also consider that it is possible to perform Live Classification using the NiclaV or a smartphone to capture live samples, testing the trained model before deployment on our device.\n\n\n\n\nThe EIS will package all the needed libraries, preprocessing functions, and trained models, downloading them to your computer. Go to the Deployment section, select Arduino Library, and at the bottom, choose Quantized (Int8) and press Build.\n \nWhen the Build button is selected, a zip file will be created and downloaded to your computer. On your Arduino IDE, go to the Sketch tab, select the option Add .ZIP Library, and Choose the .zip file downloaded by EIS:\n \nNow, it is time for a real test. We will make inferences while completely disconnected from the EIS. Let‚Äôs use the NiclaV code example created when we deployed the Arduino Library.\nIn your Arduino IDE, go to the File/Examples tab, look for your project, and select nicla-vision/nicla-vision_microphone (or nicla-vision_microphone_continuous)\n \nPress the reset button twice to put the NiclaV in boot mode, upload the sketch to your board, and test some real inferences:\n \n\n\n\nNow that we know the model is working since it detects our keywords, let‚Äôs modify the code to see the result with the NiclaV completely offline (disconnected from the PC and powered by a battery, a power bank, or an independent 5V power supply).\nThe idea is that whenever the keyword YES is detected, the Green LED will light; if a NO is heard, the Red LED will light, if it is a UNKNOWN, the Blue LED will light; and in the presence of noise (No Keyword), the LEDs will be OFF.\nWe should modify one of the code examples. Let‚Äôs do it now with the nicla-vision_microphone_continuous.\nStart with initializing the LEDs:\n...\nvoid setup()\n{\n        // Once you finish debugging your code, you can\n        // comment or delete the Serial part of the code\n    Serial.begin(115200);\n    while (!Serial);\n    Serial.println(\"Inferencing - Nicla Vision KWS with LEDs\");\n\n    // Pins for the built-in RGB LEDs on the Arduino NiclaV\n    pinMode(LEDR, OUTPUT);\n    pinMode(LEDG, OUTPUT);\n    pinMode(LEDB, OUTPUT);\n\n    // Ensure the LEDs are OFF by default.\n    // Note: The RGB LEDs on the Arduino Nicla Vision\n    // are ON when the pin is LOW, OFF when HIGH.\n    digitalWrite(LEDR, HIGH);\n    digitalWrite(LEDG, HIGH);\n    digitalWrite(LEDB, HIGH);\n...\n}\nCreate two functions, turn_off_leds() function , to turn off all RGB LEDs\n/*\n * @brief      turn_off_leds function - turn-off all RGB LEDs\n */\nvoid turn_off_leds(){\n    digitalWrite(LEDR, HIGH);\n    digitalWrite(LEDG, HIGH);\n    digitalWrite(LEDB, HIGH);\n}\nAnother turn_on_led() function is used to turn on the RGB LEDs according to the most probable result of the classifier.\n/*\n * @brief     turn_on_leds function used to turn on the RGB LEDs\n * @param[in] pred_index\n *            no:       [0] ==&gt; Red ON\n *            noise:    [1] ==&gt; ALL OFF\n *            unknown:  [2] ==&gt; Blue ON\n *            Yes:      [3] ==&gt; Green ON\n */\nvoid turn_on_leds(int pred_index) {\n  switch (pred_index)\n  {\n    case 0:\n      turn_off_leds();\n      digitalWrite(LEDR, LOW);\n      break;\n\n    case 1:\n      turn_off_leds();\n      break;\n\n    case 2:\n      turn_off_leds();\n      digitalWrite(LEDB, LOW);\n      break;\n\n    case 3:\n      turn_off_leds();\n      digitalWrite(LEDG, LOW);\n      break;\n  }\n}\nAnd change the // print the predictions portion of the code on loop():\n...\n\n if (++print_results &gt;= (EI_CLASSIFIER_SLICES_PER_MODEL_WINDOW)) {\n     // print the predictions\n     ei_printf(\"Predictions \");\n     ei_printf(\"(DSP: %d ms., Classification: %d ms.,\n                 Anomaly: %d ms.)\",\n         result.timing.dsp, result.timing.classification,\n         result.timing.anomaly);\n     ei_printf(\": \\n\");\n     int pred_index = 0;     // Initialize pred_index\n     float pred_value = 0;   // Initialize pred_value\n     for (size_t ix = 0; ix &lt; EI_CLASSIFIER_LABEL_COUNT; ix++) {\n         if (result.classification[ix].value &gt; pred_value){\n             pred_index = ix;\n             pred_value = result.classification[ix].value;\n         }\n         // ei_printf(\"    %s: \",\n         // result.classification[ix].label);\n         // ei_printf_float(result.classification[ix].value);\n         // ei_printf(\"\\n\");\n     }\n     ei_printf(\"  PREDICTION: ==&gt; %s with probability %.2f\\n\",\n               result.classification[pred_index].label,\n               pred_value);\n     turn_on_leds (pred_index);\n\n#if EI_CLASSIFIER_HAS_ANOMALY == 1\n        ei_printf(\"    anomaly score: \");\n        ei_printf_float(result.anomaly);\n        ei_printf(\"\\n\");\n#endif\n\n        print_results = 0;\n    }\n}\n\n...\nYou can find the complete code on the project‚Äôs GitHub.\nUpload the sketch to your board and test some real inferences. The idea is that the Green LED will be ON whenever the keyword YES is detected, the Red will lit for a NO, and any other word will turn on the Blue LED. All the LEDs should be off if silence or background noise is present. Remember that the same procedure can ‚Äútrigger‚Äù an external device to perform a desired action instead of turning on an LED, as we saw in the introduction.\n\n\n\n\n\nYou will find the notebooks and code used in this hands-on tutorial on the GitHub repository.\n\nBefore we finish, consider that Sound Classification is more than just voice. For example, you can develop TinyML projects around sound in several areas, such as:\n\nSecurity (Broken Glass detection, Gunshot)\nIndustry (Anomaly Detection)\nMedical (Snore, Cough, Pulmonary diseases)\nNature (Beehive control, insect sound, pouching mitigation)\n\n\n\n\n\nSubset of Google Speech Commands Dataset\nKWS MFCC Analysis Colab Notebook\nKWS_CNN_training Colab Notebook\nArduino Post-processing Code\nEdge Impulse Project",
    "crumbs": [
      "Arduino Nicla Vision",
      "Keyword Spotting (KWS)"
    ]
  },
  {
    "objectID": "contents/arduino/nicla_vision/kws/kws.html#sec-keyword-spotting-kws-overview-0ae6",
    "href": "contents/arduino/nicla_vision/kws/kws.html#sec-keyword-spotting-kws-overview-0ae6",
    "title": "Keyword Spotting (KWS)",
    "section": "",
    "text": "Having already explored the Nicla Vision board in the Image Classification and Object Detection applications, we are now shifting our focus to voice-activated applications with a project on Keyword Spotting (KWS).\nAs introduced in the Feature Engineering for Audio Classification Hands-On tutorial, Keyword Spotting (KWS) is integrated into many voice recognition systems, enabling devices to respond to specific words or phrases. While this technology underpins popular devices like Google Assistant or Amazon Alexa, it‚Äôs equally applicable and feasible on smaller, low-power devices. This tutorial will guide you through implementing a KWS system using TinyML on the Nicla Vision development board equipped with a digital microphone.\nOur model will be designed to recognize keywords that can trigger device wake-up or specific actions, bringing them to life with voice-activated commands.",
    "crumbs": [
      "Arduino Nicla Vision",
      "Keyword Spotting (KWS)"
    ]
  },
  {
    "objectID": "contents/arduino/nicla_vision/kws/kws.html#sec-keyword-spotting-kws-voice-assistant-work-7c16",
    "href": "contents/arduino/nicla_vision/kws/kws.html#sec-keyword-spotting-kws-voice-assistant-work-7c16",
    "title": "Keyword Spotting (KWS)",
    "section": "",
    "text": "As said, voice assistants on the market, like Google Home or Amazon Echo-Dot, only react to humans when they are ‚Äúwaked up‚Äù by particular keywords such as ‚Äù Hey Google‚Äù on the first one and ‚ÄúAlexa‚Äù on the second.\n \nIn other words, recognizing voice commands is based on a multi-stage model or Cascade Detection.\n \nStage 1: A small microprocessor inside the Echo Dot or Google Home continuously listens, waiting for the keyword to be spotted, using a TinyML model at the edge (KWS application).\nStage 2: Only when triggered by the KWS application on Stage 1 is the data sent to the cloud and processed on a larger model.\nThe video below shows an example of a Google Assistant being programmed on a Raspberry Pi (Stage 2), with an Arduino Nano 33 BLE as the TinyML device (Stage 1).\n\n\nTo explore the above Google Assistant project, please see the tutorial: Building an Intelligent Voice Assistant From Scratch.\n\nIn this KWS project, we will focus on Stage 1 (KWS or Keyword Spotting), where we will use the Nicla Vision, which has a digital microphone that will be used to spot the keyword.",
    "crumbs": [
      "Arduino Nicla Vision",
      "Keyword Spotting (KWS)"
    ]
  },
  {
    "objectID": "contents/arduino/nicla_vision/kws/kws.html#sec-keyword-spotting-kws-kws-handson-project-d8cd",
    "href": "contents/arduino/nicla_vision/kws/kws.html#sec-keyword-spotting-kws-kws-handson-project-d8cd",
    "title": "Keyword Spotting (KWS)",
    "section": "",
    "text": "The diagram below gives an idea of how the final KWS application should work (during inference):\n \nOur KWS application will recognize four classes of sound:\n\nYES (Keyword 1)\nNO (Keyword 2)\nNOISE (no words spoken; only background noise is present)\nUNKNOWN (a mix of different words than YES and NO)\n\n\nFor real-world projects, it is always advisable to include other sounds besides the keywords, such as ‚ÄúNoise‚Äù (or Background) and ‚ÄúUnknown.‚Äù\n\n\n\nThe main component of the KWS application is its model. So, we must train such a model with our specific keywords, noise, and other words (the ‚Äúunknown‚Äù):",
    "crumbs": [
      "Arduino Nicla Vision",
      "Keyword Spotting (KWS)"
    ]
  },
  {
    "objectID": "contents/arduino/nicla_vision/kws/kws.html#sec-keyword-spotting-kws-dataset-bd0d",
    "href": "contents/arduino/nicla_vision/kws/kws.html#sec-keyword-spotting-kws-dataset-bd0d",
    "title": "Keyword Spotting (KWS)",
    "section": "",
    "text": "The critical component of any Machine Learning Workflow is the dataset. Once we have decided on specific keywords, in our case (YES and NO), we can take advantage of the dataset developed by Pete Warden, ‚ÄúSpeech Commands: A Dataset for Limited-Vocabulary Speech Recognition.‚Äù This dataset has 35 keywords (with +1,000 samples each), such as yes, no, stop, and go. In words such as yes and no, we can get 1,500 samples.\nYou can download a small portion of the dataset from Edge Studio (Keyword spotting pre-built dataset), which includes samples from the four classes we will use in this project: yes, no, noise, and background. For this, follow the steps below:\n\nDownload the keywords dataset.\nUnzip the file to a location of your choice.\n\n\n\nInitiate a new project at Edge Impulse Studio (EIS) and select the Upload Existing Data tool in the Data Acquisition section. Choose the files to be uploaded:\n \nDefine the Label, select Automatically split between train and test, and Upload data to the EIS. Repeat for all classes.\n \nThe dataset will now appear in the Data acquisition section. Note that the approximately 6,000 samples (1,500 for each class) are split into Train (4,800) and Test (1,200) sets.\n \n\n\n\nAlthough we have a lot of data from Pete‚Äôs dataset, collecting some words spoken by us is advised. When working with accelerometers, creating a dataset with data captured by the same type of sensor is essential. In the case of sound, this is optional because what we will classify is, in reality, audio data.\n\nThe key difference between sound and audio is the type of energy. Sound is mechanical perturbation (longitudinal sound waves) that propagate through a medium, causing variations of pressure in it. Audio is an electrical (analog or digital) signal representing sound.\n\nWhen we pronounce a keyword, the sound waves should be converted to audio data. The conversion should be done by sampling the signal generated by the microphone at a 16 KHz frequency with 16-bit per sample amplitude.\nSo, any device that can generate audio data with this basic specification (16 KHz/16 bits) will work fine. As a device, we can use the NiclaV, a computer, or even your mobile phone.\n \n\n\nAs we learned in the chapter Setup Nicla Vision, EIS officially supports the Nicla Vision, which simplifies the capture of the data from its sensors, including the microphone. So, please create a new project on EIS and connect the Nicla to it, following these steps:\n\nDownload the last updated EIS Firmware and unzip it.\nOpen the zip file on your computer and select the uploader corresponding to your OS:\n\n \n\nPut the NiclaV in Boot Mode by pressing the reset button twice.\nUpload the binary arduino-nicla-vision.bin to your board by running the batch code corresponding to your OS.\n\nGo to your project on EIS, and on the Data Acquisition tab, select WebUSB. A window will pop up; choose the option that shows that the Nicla is paired and press [Connect].\nYou can choose which sensor data to pick in the Collect Data section on the Data Acquisition tab. Select: Built-in microphone, define your label (for example, yes), the sampling Frequency[16000Hz], and the Sample length (in milliseconds), for example [10s]. Start sampling.\n \nData on Pete‚Äôs dataset have a length of 1s, but the recorded samples are 10s long and must be split into 1s samples. Click on three dots after the sample name and select Split sample.\nA window will pop up with the Split tool.\n \nOnce inside the tool, split the data into 1-second (1000 ms) records. If necessary, add or remove segments. This procedure should be repeated for all new samples.\n\n\n\nYou can also use your PC or smartphone to capture audio data, using a sampling frequency of 16 KHz and a bit depth of 16.\nGo to Devices, scan the QR Code using your phone, and click on the link. A data Collection app will appear in your browser. Select Collecting Audio, and define your Label, data capture Length, and Category.\n \nRepeat the same procedure used with the NiclaV.\n\nNote that any app, such as Audacity, can be used for audio recording, provided you use 16 KHz/16-bit depth samples.",
    "crumbs": [
      "Arduino Nicla Vision",
      "Keyword Spotting (KWS)"
    ]
  },
  {
    "objectID": "contents/arduino/nicla_vision/kws/kws.html#sec-keyword-spotting-kws-creating-impulse-preprocess-model-definition-5c3e",
    "href": "contents/arduino/nicla_vision/kws/kws.html#sec-keyword-spotting-kws-creating-impulse-preprocess-model-definition-5c3e",
    "title": "Keyword Spotting (KWS)",
    "section": "",
    "text": "An impulse takes raw data, uses signal processing to extract features, and then uses a learning block to classify new data.\n\n\n \nFirst, we will take the data points with a 1-second window, augmenting the data and sliding that window in 500 ms intervals. Note that the option zero-pad data is set. It is essential to fill with ‚Äòzeros‚Äô samples smaller than 1 second (in some cases, some samples can result smaller than the 1000 ms window on the split tool to avoid noise and spikes).\nEach 1-second audio sample should be pre-processed and converted to an image (for example, \\(13\\times 49\\times 1\\)). As discussed in the Feature Engineering for Audio Classification Hands-On tutorial, we will use Audio (MFCC), which extracts features from audio signals using Mel Frequency Cepstral Coefficients, which are well suited for the human voice, our case here.\nNext, we select the Classification block to build our model from scratch using a Convolution Neural Network (CNN).\n\nAlternatively, you can use the Transfer Learning (Keyword Spotting) block, which fine-tunes a pre-trained keyword spotting model on your data. This approach has good performance with relatively small keyword datasets.\n\n\n\n\nThe following step is to create the features to be trained in the next phase:\nWe could keep the default parameter values, but we will use the DSP Autotune parameters option.\n \nWe will take the Raw features (our 1-second, 16 KHz sampled audio data) and use the MFCC processing block to calculate the Processed features. For every 16,000 raw features (16,000 \\(\\times\\) 1 second), we will get 637 processed features \\((13\\times 49)\\).\n \nThe result shows that we only used a small amount of memory to pre-process data (16 KB) and a latency of 34 ms, which is excellent. For example, on an Arduino Nano (Cortex-M4f @ 64 MHz), the same pre-process will take around 480 ms. The parameters chosen, such as the FFT length [512], will significantly impact the latency.\nNow, let‚Äôs Save parameters and move to the Generated features tab, where the actual features will be generated. Using UMAP, a dimension reduction technique, the Feature explorer shows how the features are distributed on a two-dimensional plot.\n \nThe result seems OK, with a visually clear separation between yes features (in red) and no features (in blue). The unknown features seem nearer to the no space than the yes. This suggests that the keyword no has more propensity to false positives.\n\n\n\nTo understand better how the raw sound is preprocessed, look at the Feature Engineering for Audio Classification chapter. You can play with the MFCC features generation by downloading this notebook from GitHub or [Opening it In Colab]",
    "crumbs": [
      "Arduino Nicla Vision",
      "Keyword Spotting (KWS)"
    ]
  },
  {
    "objectID": "contents/arduino/nicla_vision/kws/kws.html#sec-keyword-spotting-kws-model-design-training-1c97",
    "href": "contents/arduino/nicla_vision/kws/kws.html#sec-keyword-spotting-kws-model-design-training-1c97",
    "title": "Keyword Spotting (KWS)",
    "section": "",
    "text": "We will use a simple Convolution Neural Network (CNN) model, tested with 1D and 2D convolutions. The basic architecture has two blocks of Convolution + MaxPooling ([8] and [16] filters, respectively) and a Dropout of [0.25] for the 1D and [0.5] for the 2D. For the last layer, after Flattening, we have [4] neurons, one for each class:\n \nAs hyper-parameters, we will have a Learning Rate of [0.005] and a model trained by [100] epochs. We will also include a data augmentation method based on SpecAugment. We trained the 1D and the 2D models with the same hyperparameters. The 1D architecture had a better overall result (90.5% accuracy when compared with 88% of the 2D, so we will use the 1D.\n \n\nUsing 1D convolutions is more efficient because it requires fewer parameters than 2D convolutions, making them more suitable for resource-constrained environments.\n\nIt is also interesting to pay attention to the 1D Confusion Matrix. The F1 Score for yes is 95%, and for no, 91%. That was expected by what we saw with the Feature Explorer (no and unknown at close distance). In trying to improve the result, you can inspect closely the results of the samples with an error.\n \nListen to the samples that went wrong. For example, for yes, most of the mistakes were related to a yes pronounced as ‚Äúyeh‚Äù. You can acquire additional samples and then retrain your model.\n\n\nIf you want to understand what is happening ‚Äúunder the hood,‚Äù you can download the pre-processed dataset (MFCC training data) from the Dashboard tab and run this Jupyter Notebook, playing with the code or [Opening it In Colab]. For example, you can analyze the accuracy by each epoch:",
    "crumbs": [
      "Arduino Nicla Vision",
      "Keyword Spotting (KWS)"
    ]
  },
  {
    "objectID": "contents/arduino/nicla_vision/kws/kws.html#sec-keyword-spotting-kws-testing-bd97",
    "href": "contents/arduino/nicla_vision/kws/kws.html#sec-keyword-spotting-kws-testing-bd97",
    "title": "Keyword Spotting (KWS)",
    "section": "",
    "text": "Testing the model with the data reserved for training (Test Data), we got an accuracy of approximately 76%.\n \nInspecting the F1 score, we can see that for YES, we got 0.90, an excellent result since we expect to use this keyword as the primary ‚Äútrigger‚Äù for our KWS project. The worst result (0.70) is for UNKNOWN, which is OK.\nFor NO, we got 0.72, which was expected, but to improve this result, we can move the samples that were not correctly classified to the training dataset and then repeat the training process.\n\n\nWe can proceed to the project‚Äôs next step but also consider that it is possible to perform Live Classification using the NiclaV or a smartphone to capture live samples, testing the trained model before deployment on our device.",
    "crumbs": [
      "Arduino Nicla Vision",
      "Keyword Spotting (KWS)"
    ]
  },
  {
    "objectID": "contents/arduino/nicla_vision/kws/kws.html#sec-keyword-spotting-kws-deploy-inference-152d",
    "href": "contents/arduino/nicla_vision/kws/kws.html#sec-keyword-spotting-kws-deploy-inference-152d",
    "title": "Keyword Spotting (KWS)",
    "section": "",
    "text": "The EIS will package all the needed libraries, preprocessing functions, and trained models, downloading them to your computer. Go to the Deployment section, select Arduino Library, and at the bottom, choose Quantized (Int8) and press Build.\n \nWhen the Build button is selected, a zip file will be created and downloaded to your computer. On your Arduino IDE, go to the Sketch tab, select the option Add .ZIP Library, and Choose the .zip file downloaded by EIS:\n \nNow, it is time for a real test. We will make inferences while completely disconnected from the EIS. Let‚Äôs use the NiclaV code example created when we deployed the Arduino Library.\nIn your Arduino IDE, go to the File/Examples tab, look for your project, and select nicla-vision/nicla-vision_microphone (or nicla-vision_microphone_continuous)\n \nPress the reset button twice to put the NiclaV in boot mode, upload the sketch to your board, and test some real inferences:",
    "crumbs": [
      "Arduino Nicla Vision",
      "Keyword Spotting (KWS)"
    ]
  },
  {
    "objectID": "contents/arduino/nicla_vision/kws/kws.html#sec-keyword-spotting-kws-postprocessing-ce62",
    "href": "contents/arduino/nicla_vision/kws/kws.html#sec-keyword-spotting-kws-postprocessing-ce62",
    "title": "Keyword Spotting (KWS)",
    "section": "",
    "text": "Now that we know the model is working since it detects our keywords, let‚Äôs modify the code to see the result with the NiclaV completely offline (disconnected from the PC and powered by a battery, a power bank, or an independent 5V power supply).\nThe idea is that whenever the keyword YES is detected, the Green LED will light; if a NO is heard, the Red LED will light, if it is a UNKNOWN, the Blue LED will light; and in the presence of noise (No Keyword), the LEDs will be OFF.\nWe should modify one of the code examples. Let‚Äôs do it now with the nicla-vision_microphone_continuous.\nStart with initializing the LEDs:\n...\nvoid setup()\n{\n        // Once you finish debugging your code, you can\n        // comment or delete the Serial part of the code\n    Serial.begin(115200);\n    while (!Serial);\n    Serial.println(\"Inferencing - Nicla Vision KWS with LEDs\");\n\n    // Pins for the built-in RGB LEDs on the Arduino NiclaV\n    pinMode(LEDR, OUTPUT);\n    pinMode(LEDG, OUTPUT);\n    pinMode(LEDB, OUTPUT);\n\n    // Ensure the LEDs are OFF by default.\n    // Note: The RGB LEDs on the Arduino Nicla Vision\n    // are ON when the pin is LOW, OFF when HIGH.\n    digitalWrite(LEDR, HIGH);\n    digitalWrite(LEDG, HIGH);\n    digitalWrite(LEDB, HIGH);\n...\n}\nCreate two functions, turn_off_leds() function , to turn off all RGB LEDs\n/*\n * @brief      turn_off_leds function - turn-off all RGB LEDs\n */\nvoid turn_off_leds(){\n    digitalWrite(LEDR, HIGH);\n    digitalWrite(LEDG, HIGH);\n    digitalWrite(LEDB, HIGH);\n}\nAnother turn_on_led() function is used to turn on the RGB LEDs according to the most probable result of the classifier.\n/*\n * @brief     turn_on_leds function used to turn on the RGB LEDs\n * @param[in] pred_index\n *            no:       [0] ==&gt; Red ON\n *            noise:    [1] ==&gt; ALL OFF\n *            unknown:  [2] ==&gt; Blue ON\n *            Yes:      [3] ==&gt; Green ON\n */\nvoid turn_on_leds(int pred_index) {\n  switch (pred_index)\n  {\n    case 0:\n      turn_off_leds();\n      digitalWrite(LEDR, LOW);\n      break;\n\n    case 1:\n      turn_off_leds();\n      break;\n\n    case 2:\n      turn_off_leds();\n      digitalWrite(LEDB, LOW);\n      break;\n\n    case 3:\n      turn_off_leds();\n      digitalWrite(LEDG, LOW);\n      break;\n  }\n}\nAnd change the // print the predictions portion of the code on loop():\n...\n\n if (++print_results &gt;= (EI_CLASSIFIER_SLICES_PER_MODEL_WINDOW)) {\n     // print the predictions\n     ei_printf(\"Predictions \");\n     ei_printf(\"(DSP: %d ms., Classification: %d ms.,\n                 Anomaly: %d ms.)\",\n         result.timing.dsp, result.timing.classification,\n         result.timing.anomaly);\n     ei_printf(\": \\n\");\n     int pred_index = 0;     // Initialize pred_index\n     float pred_value = 0;   // Initialize pred_value\n     for (size_t ix = 0; ix &lt; EI_CLASSIFIER_LABEL_COUNT; ix++) {\n         if (result.classification[ix].value &gt; pred_value){\n             pred_index = ix;\n             pred_value = result.classification[ix].value;\n         }\n         // ei_printf(\"    %s: \",\n         // result.classification[ix].label);\n         // ei_printf_float(result.classification[ix].value);\n         // ei_printf(\"\\n\");\n     }\n     ei_printf(\"  PREDICTION: ==&gt; %s with probability %.2f\\n\",\n               result.classification[pred_index].label,\n               pred_value);\n     turn_on_leds (pred_index);\n\n#if EI_CLASSIFIER_HAS_ANOMALY == 1\n        ei_printf(\"    anomaly score: \");\n        ei_printf_float(result.anomaly);\n        ei_printf(\"\\n\");\n#endif\n\n        print_results = 0;\n    }\n}\n\n...\nYou can find the complete code on the project‚Äôs GitHub.\nUpload the sketch to your board and test some real inferences. The idea is that the Green LED will be ON whenever the keyword YES is detected, the Red will lit for a NO, and any other word will turn on the Blue LED. All the LEDs should be off if silence or background noise is present. Remember that the same procedure can ‚Äútrigger‚Äù an external device to perform a desired action instead of turning on an LED, as we saw in the introduction.",
    "crumbs": [
      "Arduino Nicla Vision",
      "Keyword Spotting (KWS)"
    ]
  },
  {
    "objectID": "contents/arduino/nicla_vision/kws/kws.html#sec-keyword-spotting-kws-summary-06f5",
    "href": "contents/arduino/nicla_vision/kws/kws.html#sec-keyword-spotting-kws-summary-06f5",
    "title": "Keyword Spotting (KWS)",
    "section": "",
    "text": "You will find the notebooks and code used in this hands-on tutorial on the GitHub repository.\n\nBefore we finish, consider that Sound Classification is more than just voice. For example, you can develop TinyML projects around sound in several areas, such as:\n\nSecurity (Broken Glass detection, Gunshot)\nIndustry (Anomaly Detection)\nMedical (Snore, Cough, Pulmonary diseases)\nNature (Beehive control, insect sound, pouching mitigation)",
    "crumbs": [
      "Arduino Nicla Vision",
      "Keyword Spotting (KWS)"
    ]
  },
  {
    "objectID": "contents/arduino/nicla_vision/kws/kws.html#sec-keyword-spotting-kws-resources-a202",
    "href": "contents/arduino/nicla_vision/kws/kws.html#sec-keyword-spotting-kws-resources-a202",
    "title": "Keyword Spotting (KWS)",
    "section": "",
    "text": "Subset of Google Speech Commands Dataset\nKWS MFCC Analysis Colab Notebook\nKWS_CNN_training Colab Notebook\nArduino Post-processing Code\nEdge Impulse Project",
    "crumbs": [
      "Arduino Nicla Vision",
      "Keyword Spotting (KWS)"
    ]
  },
  {
    "objectID": "contents/arduino/nicla_vision/nicla_vision.html",
    "href": "contents/arduino/nicla_vision/nicla_vision.html",
    "title": "Overview",
    "section": "",
    "text": "These labs provide a unique opportunity to gain practical experience with machine learning (ML) systems. Unlike working with large models requiring data center-scale resources, these exercises allow you to directly interact with hardware and software using TinyML. This hands-on approach gives you a tangible understanding of the challenges and opportunities in deploying AI, albeit at a tiny scale. However, the principles are largely the same as what you would encounter when working with larger systems.\n\n\n\nNicla Vision. Source: Arduino.\n\n\n\n\nThe Arduino Nicla Vision is available from the official Arduino Store:\n\nArduino Store (~$95)\n\n\n\n\n\nNicla Vision Board: Ensure you have the Nicla Vision board.\nUSB Cable: For connecting the board to your computer.\nNetwork: With internet access for downloading necessary software.\n\n\n\n\n\nSetup Nicla Vision\n\n\n\n\n\n\n\nModality\nTask\nDescription\nLink\n\n\n\n\nVision\nImage Classification\nLearn to classify images\nLink\n\n\nVision\nObject Detection\nImplement object detection\nLink\n\n\nSound\nKeyword Spotting\nExplore voice recognition systems\nLink\n\n\nIMU\nMotion Classification and Anomaly Detection\nClassify motion data and detect anomalies\nLink",
    "crumbs": [
      "Arduino Nicla Vision",
      "Overview"
    ]
  },
  {
    "objectID": "contents/arduino/nicla_vision/nicla_vision.html#sec-overview-where-to-buy",
    "href": "contents/arduino/nicla_vision/nicla_vision.html#sec-overview-where-to-buy",
    "title": "Overview",
    "section": "",
    "text": "The Arduino Nicla Vision is available from the official Arduino Store:\n\nArduino Store (~$95)",
    "crumbs": [
      "Arduino Nicla Vision",
      "Overview"
    ]
  },
  {
    "objectID": "contents/arduino/nicla_vision/nicla_vision.html#sec-overview-prerequisites-b2d4",
    "href": "contents/arduino/nicla_vision/nicla_vision.html#sec-overview-prerequisites-b2d4",
    "title": "Overview",
    "section": "",
    "text": "Nicla Vision Board: Ensure you have the Nicla Vision board.\nUSB Cable: For connecting the board to your computer.\nNetwork: With internet access for downloading necessary software.",
    "crumbs": [
      "Arduino Nicla Vision",
      "Overview"
    ]
  },
  {
    "objectID": "contents/arduino/nicla_vision/nicla_vision.html#sec-overview-setup-cde0",
    "href": "contents/arduino/nicla_vision/nicla_vision.html#sec-overview-setup-cde0",
    "title": "Overview",
    "section": "",
    "text": "Setup Nicla Vision",
    "crumbs": [
      "Arduino Nicla Vision",
      "Overview"
    ]
  },
  {
    "objectID": "contents/arduino/nicla_vision/nicla_vision.html#sec-overview-exercises-f4f3",
    "href": "contents/arduino/nicla_vision/nicla_vision.html#sec-overview-exercises-f4f3",
    "title": "Overview",
    "section": "",
    "text": "Modality\nTask\nDescription\nLink\n\n\n\n\nVision\nImage Classification\nLearn to classify images\nLink\n\n\nVision\nObject Detection\nImplement object detection\nLink\n\n\nSound\nKeyword Spotting\nExplore voice recognition systems\nLink\n\n\nIMU\nMotion Classification and Anomaly Detection\nClassify motion data and detect anomalies\nLink",
    "crumbs": [
      "Arduino Nicla Vision",
      "Overview"
    ]
  },
  {
    "objectID": "contents/getting-started.html",
    "href": "contents/getting-started.html",
    "title": "Getting Started",
    "section": "",
    "text": "This guide walks you through selecting hardware, configuring your development environment, and running your first embedded ML application. Most students complete setup in under an hour.\n\n\nYour choice depends on budget, learning objectives, and the types of applications you want to build.\nFor beginners or budget-conscious learners:\n\n\n\nPlatform\nCost\nWhy Choose It\n\n\n\n\nGrove Vision AI V2\n~$25\nNo-code interface, fastest path to running models\n\n\nXIAO ESP32S3\n~$40\nBest value, supports vision, audio, and motion\n\n\n\nFor advanced applications:\n\n\n\nPlatform\nCost\nWhy Choose It\n\n\n\n\nRaspberry Pi\n~$60-80\nFull Linux environment, LLMs and VLMs\n\n\nNicla Vision\n~$95\nProfessional-grade, ultra-low power design\n\n\n\nFor detailed specifications and technical comparisons, see Platforms.\n\n\n\nDevelopment environment configuration is platform-dependent but follows a common pattern: install software tools, configure communication with hardware, and verify the setup works.\nTime estimate: 30-60 minutes depending on platform and internet speed.\nFollow the IDE Setup Guide for complete procedures covering:\n\nSystem requirements for your development computer\nArduino IDE installation for microcontroller platforms\nPython environment configuration for Raspberry Pi\nSenseCraft AI web interface for Grove Vision AI V2\nSerial communication and hardware verification\n\n\n\n\nEach platform supports different exercise categories. Select labs that match both your hardware and learning goals.\n\nExercise availability by platform\n\n\nLab Category\nGrove Vision\nXIAO\nNicla\nRaspberry Pi\n\n\n\n\nImage Classification\n‚úì\n‚úì\n‚úì\n‚úì\n\n\nObject Detection\n‚úì\n‚úì\n‚úì\n‚úì\n\n\nKeyword Spotting\n\n‚úì\n‚úì\n\n\n\nMotion Classification\n\n‚úì\n‚úì\n\n\n\nLarge Language Models\n\n\n\n‚úì\n\n\nVision Language Models\n\n\n\n‚úì\n\n\n\n\n\n\nGrove Vision AI V2: Begin with Setup and No-Code Apps. You‚Äôll deploy a pre-trained model in minutes using the visual interface.\nXIAO ESP32S3: Start with Setup, then proceed to Image Classification to train and deploy your first custom model.\nNicla Vision: Complete Setup to configure your board, then try Image Classification.\nRaspberry Pi: Follow Setup, then choose your path: - Image Classification for computer vision fundamentals - LLM Deployment to run language models on edge hardware\n\n\n\nThese labs assume:\n\nProgramming: Proficiency in Python. Familiarity with C/C++ is helpful for microcontroller platforms but not required.\nMathematics: Working knowledge of linear algebra and basic probability at the undergraduate level.\nHardware: No prior embedded systems experience. Each lab includes complete setup and troubleshooting procedures.\n\n\n\n\nThese laboratories complement specific chapters in the ML Systems textbook:\n\nImage Classification labs reinforce concepts from the Computer Vision and Model Optimization chapters\nKeyword Spotting labs connect to Audio Processing and Real-time Inference\nMotion Classification labs demonstrate Sensor Fusion and Time-series Analysis\nLLM/VLM labs extend Large Model Deployment to resource-constrained environments\n\nEach lab identifies relevant textbook sections for deeper theoretical understanding.",
    "crumbs": [
      "Getting Started"
    ]
  },
  {
    "objectID": "contents/getting-started.html#step-1-select-your-platform",
    "href": "contents/getting-started.html#step-1-select-your-platform",
    "title": "Getting Started",
    "section": "",
    "text": "Your choice depends on budget, learning objectives, and the types of applications you want to build.\nFor beginners or budget-conscious learners:\n\n\n\nPlatform\nCost\nWhy Choose It\n\n\n\n\nGrove Vision AI V2\n~$25\nNo-code interface, fastest path to running models\n\n\nXIAO ESP32S3\n~$40\nBest value, supports vision, audio, and motion\n\n\n\nFor advanced applications:\n\n\n\nPlatform\nCost\nWhy Choose It\n\n\n\n\nRaspberry Pi\n~$60-80\nFull Linux environment, LLMs and VLMs\n\n\nNicla Vision\n~$95\nProfessional-grade, ultra-low power design\n\n\n\nFor detailed specifications and technical comparisons, see Platforms.",
    "crumbs": [
      "Getting Started"
    ]
  },
  {
    "objectID": "contents/getting-started.html#step-2-set-up-your-environment",
    "href": "contents/getting-started.html#step-2-set-up-your-environment",
    "title": "Getting Started",
    "section": "",
    "text": "Development environment configuration is platform-dependent but follows a common pattern: install software tools, configure communication with hardware, and verify the setup works.\nTime estimate: 30-60 minutes depending on platform and internet speed.\nFollow the IDE Setup Guide for complete procedures covering:\n\nSystem requirements for your development computer\nArduino IDE installation for microcontroller platforms\nPython environment configuration for Raspberry Pi\nSenseCraft AI web interface for Grove Vision AI V2\nSerial communication and hardware verification",
    "crumbs": [
      "Getting Started"
    ]
  },
  {
    "objectID": "contents/getting-started.html#step-3-choose-your-first-lab",
    "href": "contents/getting-started.html#step-3-choose-your-first-lab",
    "title": "Getting Started",
    "section": "",
    "text": "Each platform supports different exercise categories. Select labs that match both your hardware and learning goals.\n\nExercise availability by platform\n\n\nLab Category\nGrove Vision\nXIAO\nNicla\nRaspberry Pi\n\n\n\n\nImage Classification\n‚úì\n‚úì\n‚úì\n‚úì\n\n\nObject Detection\n‚úì\n‚úì\n‚úì\n‚úì\n\n\nKeyword Spotting\n\n‚úì\n‚úì\n\n\n\nMotion Classification\n\n‚úì\n‚úì\n\n\n\nLarge Language Models\n\n\n\n‚úì\n\n\nVision Language Models\n\n\n\n‚úì",
    "crumbs": [
      "Getting Started"
    ]
  },
  {
    "objectID": "contents/getting-started.html#step-4-start-your-first-lab",
    "href": "contents/getting-started.html#step-4-start-your-first-lab",
    "title": "Getting Started",
    "section": "",
    "text": "Grove Vision AI V2: Begin with Setup and No-Code Apps. You‚Äôll deploy a pre-trained model in minutes using the visual interface.\nXIAO ESP32S3: Start with Setup, then proceed to Image Classification to train and deploy your first custom model.\nNicla Vision: Complete Setup to configure your board, then try Image Classification.\nRaspberry Pi: Follow Setup, then choose your path: - Image Classification for computer vision fundamentals - LLM Deployment to run language models on edge hardware",
    "crumbs": [
      "Getting Started"
    ]
  },
  {
    "objectID": "contents/getting-started.html#prerequisites",
    "href": "contents/getting-started.html#prerequisites",
    "title": "Getting Started",
    "section": "",
    "text": "These labs assume:\n\nProgramming: Proficiency in Python. Familiarity with C/C++ is helpful for microcontroller platforms but not required.\nMathematics: Working knowledge of linear algebra and basic probability at the undergraduate level.\nHardware: No prior embedded systems experience. Each lab includes complete setup and troubleshooting procedures.",
    "crumbs": [
      "Getting Started"
    ]
  },
  {
    "objectID": "contents/getting-started.html#connection-to-ml-systems-textbook",
    "href": "contents/getting-started.html#connection-to-ml-systems-textbook",
    "title": "Getting Started",
    "section": "",
    "text": "These laboratories complement specific chapters in the ML Systems textbook:\n\nImage Classification labs reinforce concepts from the Computer Vision and Model Optimization chapters\nKeyword Spotting labs connect to Audio Processing and Real-time Inference\nMotion Classification labs demonstrate Sensor Fusion and Time-series Analysis\nLLM/VLM labs extend Large Model Deployment to resource-constrained environments\n\nEach lab identifies relevant textbook sections for deeper theoretical understanding.",
    "crumbs": [
      "Getting Started"
    ]
  },
  {
    "objectID": "contents/shared/shared.html",
    "href": "contents/shared/shared.html",
    "title": "Overview",
    "section": "",
    "text": "Overview\nThe labs in this section cover topics and techniques that are applicable across different hardware platforms. These labs are designed to be independent of specific boards, allowing you to focus on the fundamental concepts and algorithms used in (tiny) ML applications.\nBy exploring these shared labs, you‚Äôll gain a deeper understanding of the common challenges and solutions in embedded machine learning. The knowledge and skills acquired here will be valuable regardless of the specific hardware you work with in the future.\n\n\n\nExercise\nNicla Vision\nXIAO ESP32S3\n\n\n\n\nKWS Feature Engineering\n‚úî Link\n‚úî Link\n\n\nDSP Spectral Features Block\n‚úî Link\n‚úî Link\n\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "Shared Resources",
      "Overview"
    ]
  },
  {
    "objectID": "contents/platforms.html",
    "href": "contents/platforms.html",
    "title": "Hardware Platforms",
    "section": "",
    "text": "This chapter provides detailed technical specifications for the four hardware platforms used in these laboratories. Each platform represents a different point along the spectrum of embedded computing capabilities, from ultra-low-power microcontrollers to full-featured edge computers.\nThese platforms were selected because they illustrate distinct engineering trade-offs in power consumption, computational capability, and development complexity. All are widely used in commercial applications, ensuring that skills developed here transfer directly to professional embedded systems work.\n\n\n\n\n\nComplete XIAOML Kit with all components\n\n\nThe XIAOML Kit is the most recent addition to our educational hardware platforms (released on July 31st, 2025). It offers a comprehensive TinyML development environment for learning about ML systems, featuring integrated wireless connectivity, a camera, multiple sensors, and extensive documentation. This compact board exemplifies how contemporary embedded systems can efficiently provide advanced machine learning capabilities within a cost-effective framework.\n\n\n\nOur curriculum features four carefully selected platforms that span the full spectrum of embedded computing capabilities. Each platform shown in Table¬†1 has been chosen to illustrate specific engineering trade-offs and learning objectives.\n\n\n\nTable¬†1: Platform selection strategy table.\n\n\n\n\n\n\n\n\n\n\n\n\nPlatform\nPrimary Learning Focus\nCost\nPower Profile\nBest For\n\n\n\n\nXIAOML Kit\nIoT & Wireless ML\n~$40\nLow Power\nCost-sensitive deployments\n\n\nArduino Nicla\nUltra-low Power Design\n~$95\nUltra-low\nBattery-powered devices\n\n\nGrove Vision AI\nHardware Acceleration\n~$25\nMedium\nIndustrial applications\n\n\nRaspberry Pi\nFull ML Frameworks\n$60-145\nHigh\nAdvanced edge computing\n\n\n\n\n\n\n\n\n\nTable¬†2 provides a comprehensive technical comparison of all four platforms.\n\n\n\nTable¬†2: Platform comparison matrix.\n\n\n\n\n\n\n\n\n\n\n\n\nCharacteristic\nXIAOML Kit\nRaspberry Pi\nArduino Nicla\nGrove Vision AI V2\n\n\n\n\nCost Range (USD)\n~$40\n$60-145\n~$95\n~$25\n\n\nPower Consumption\nLow\nHigh\nUltra-low\nMedium\n\n\nProcessing Power\nMedium\nVery High\nLow\nHigh (NPU)\n\n\nMemory Capacity\n8MB\n1-16GB\n2MB\n16MB\n\n\nPrimary Use Case\nIoT networks\nEdge computing\nBattery devices\nIndustrial AI\n\n\nML Framework\nTF Lite\nTensorFlow, PyTorch\nTensorFlow Lite\nSenseCraft AI\n\n\nDevelopment Env.\nArduino/ PlatformIO\nPython/Linux\nArduino IDE\nVisual/Code\n\n\n\n\n\n\n\n\n\nSelecting the appropriate platform depends on specific learning objectives and project requirements. Table¬†3 provides a systematic mapping to guide these decisions.\n\n\n\nTable¬†3: Platform capabilities matrix.\n\n\n\n\n\n\n\n\n\n\n\n\nLearning Objective/Application\nXIAOML Kit\nRas Pi\nArduino Nicla\nGrove Vision AI V2\n\n\n\n\nEmbedded Systems Basics\n‚úì\nLimited\n‚úì\n‚úì\n\n\nWireless Connectivity\n‚úì\n‚úì\n\n‚úì\n\n\nUltra-Low Power Design\n\n\n‚úì\n\n\n\nFull ML Frameworks\n\n‚úì\n\n\n\n\nHardware Acceleration\n\n\n\n‚úì\n\n\nReal-time Vision\nLimited\n‚úì\n‚úì\n‚úì\n\n\nEdge-Cloud Integration\n‚úì\n‚úì\n\n‚úì\n\n\nProduction Deployment\n‚úì\n\n‚úì\n‚úì\n\n\n\n\n\n\n\n\n\nThis section provides detailed technical specifications for each platform, including processor architecture, memory hierarchy, sensor capabilities, and development toolchain requirements.\n\n\n\n\n\n\n\n\nTipBest For: IoT & Wireless ML\n\n\n\nThe XIAOML Kit excels at wireless connectivity and cost-sensitive deployments. It‚Äôs perfect for learning IoT sensor networks, remote monitoring systems, and wireless ML inference where you need reliable connectivity in a compact, affordable package.\n\n\nThe XIAO ESP32S3 represents the category of ultra-compact, wireless-enabled microcontrollers optimized for IoT applications. The name ‚ÄúXIAO‚Äù (Â∞è) translates to ‚Äútiny‚Äù in Chinese, reflecting the board‚Äôs 21√ó17.5mm form factor.\n\n\n\nXIAO ESP32S3 development board\n\n\nProcessor Architecture: ESP32-S3 dual-core Xtensa LX7 running at 240MHz\nMemory Hierarchy: 8MB PSRAM and 8MB Flash storage\nConnectivity: WiFi 802.11 b/g/n and Bluetooth 5.0\nIntegrated Sensors: OV2640 camera sensor, digital microphone, 6-axis inertial measurement unit\nPower Characteristics: 3.3V operation with multiple low-power modes\nDevelopment Environment: Arduino IDE and PlatformIO support with extensive library ecosystem. Supports C/C++ programming with Arduino-style abstractions and direct ESP-IDF for advanced users.\nApplication Focus: IoT sensor networks, remote monitoring systems, wireless ML inference, cost-sensitive deployments\n\n\n\n\n\n\n\n\n\nTipBest For: Ultra-Low Power Design\n\n\n\nThe Arduino Nicla Vision is optimized for battery-powered devices and always-on sensing applications. It‚Äôs ideal for learning ultra-low power design, image classification systems, and object detection applications where battery life is measured in months, not hours.\n\n\nThe Nicla Vision exemplifies professional-grade embedded vision systems built around the STM32H7 microcontroller. This platform demonstrates how specialized hardware design enables sophisticated ML inference within severe resource constraints.\n\n\n\nArduino Nicla Vision with camera module\n\n\nProcessor Architecture: STM32H747 dual-core ARM Cortex-M7/M4 running at 480MHz\nMemory Hierarchy: 2MB integrated RAM and 16MB Flash storage\nIntegrated Sensors: GC2145 camera sensor, MP34DT05 digital microphone, 6-axis IMU\nPower Characteristics: 3.3V operation optimized for battery-powered deployment\nDevelopment Environment: Arduino IDE and OpenMV IDE support with specialized computer vision libraries. MicroPython support for rapid prototyping alongside C/C++ for production deployments.\nApplication Focus: Battery-powered devices, image classification systems, object detection applications, always-on sensing\n\n\n\n\n\n\n\n\n\nTipBest For: Hardware Acceleration\n\n\n\nThe Grove Vision AI V2 features dedicated neural processing hardware for orders-of-magnitude performance improvements. It‚Äôs perfect for learning industrial inspection systems, real-time video analytics, and advanced object detection where you need NPU-accelerated inference capabilities.\n\n\nThe Grove Vision AI V2 incorporates dedicated neural processing hardware (NPU) to demonstrate hardware-accelerated ML inference. This platform illustrates how specialized AI processors achieve orders-of-magnitude performance improvements over software-only implementations.\n\n\n\nGrove Vision AI V2 with NPU\n\n\nProcessor Architecture: ARM Cortex-M55 with integrated Ethos-U55 NPU\nMemory Hierarchy: 16MB external memory for model and data storage\nNeural Processing Unit: Dedicated hardware accelerator for ML inference\nCamera Interface: Standard CSI connector supporting various camera modules\nAudio Input: Onboard digital microphone\nDevelopment Environment: SenseCraft AI visual programming platform for no-code development, with Arduino IDE support for custom applications. Supports both graphical programming and traditional C/C++ development workflows.\nApplication Focus: Industrial inspection systems, real-time video analytics, advanced object detection, NPU-accelerated inference\n\n\n\n\n\n\n\n\n\nTipBest For: Full ML Frameworks\n\n\n\nThe Raspberry Pi bridges embedded systems and traditional computing, providing a complete Linux environment for advanced ML applications. It‚Äôs ideal for learning edge AI gateways, advanced computer vision systems, language model deployment, and multi-modal AI applications where you need full computing capabilities.\n\n\nThe Raspberry Pi family bridges embedded systems and traditional computing, providing a full Linux environment while maintaining educational accessibility. This platform demonstrates how increased computational resources enable sophisticated ML applications.\n\n\n\nRaspberry Pi 5 and Pi Zero 2W comparison\n\n\nProcessor Architecture: ARM Cortex-A76 (Pi 5) or Cortex-A53 (Zero 2W)\nMemory Hierarchy: 1-16GB DDR4 RAM depending on model\nStorage: MicroSD card primary storage with USB 3.0 expansion\nConnectivity: Gigabit Ethernet, WiFi, Bluetooth, multiple USB ports\nCamera Interface: Dedicated CSI connector plus USB camera support\nOperating System: Debian-based Raspberry Pi OS (full Linux distribution)\nDevelopment Environment: Full Linux development environment with native Python, C/C++, and JavaScript support. Package managers (apt, pip) provide access to extensive ML libraries including TensorFlow, PyTorch, and OpenCV.\nApplication Focus: Edge AI gateways, advanced computer vision systems, language model deployment, multi-modal AI applications\n\n\n\n\nTo get started with the hardware kits used in this course, you can purchase them directly from the following official sources:\n\nSeeed Studio ‚Äì XIAOML Kit and Grove Vision AI V2 Module\nArduino Store ‚Äì Nicla Vision\nRaspberry Pi Foundation ‚Äì Boards and Kits\nDigiKey, Mouser, SparkFun ‚Äî Alternative distributors for a variety of components and kits\n\nCheck each site for educational discounts, bundles, and regional availability. Most kits are available as starter packages that include the board and basic accessories.",
    "crumbs": [
      "Platforms"
    ]
  },
  {
    "objectID": "contents/platforms.html#sec-hardware-kits-featured-platform-73d8",
    "href": "contents/platforms.html#sec-hardware-kits-featured-platform-73d8",
    "title": "Hardware Platforms",
    "section": "",
    "text": "Complete XIAOML Kit with all components\n\n\nThe XIAOML Kit is the most recent addition to our educational hardware platforms (released on July 31st, 2025). It offers a comprehensive TinyML development environment for learning about ML systems, featuring integrated wireless connectivity, a camera, multiple sensors, and extensive documentation. This compact board exemplifies how contemporary embedded systems can efficiently provide advanced machine learning capabilities within a cost-effective framework.",
    "crumbs": [
      "Platforms"
    ]
  },
  {
    "objectID": "contents/platforms.html#sec-hardware-kits-hardware-platform-overview-9f77",
    "href": "contents/platforms.html#sec-hardware-kits-hardware-platform-overview-9f77",
    "title": "Hardware Platforms",
    "section": "",
    "text": "Our curriculum features four carefully selected platforms that span the full spectrum of embedded computing capabilities. Each platform shown in Table¬†1 has been chosen to illustrate specific engineering trade-offs and learning objectives.\n\n\n\nTable¬†1: Platform selection strategy table.\n\n\n\n\n\n\n\n\n\n\n\n\nPlatform\nPrimary Learning Focus\nCost\nPower Profile\nBest For\n\n\n\n\nXIAOML Kit\nIoT & Wireless ML\n~$40\nLow Power\nCost-sensitive deployments\n\n\nArduino Nicla\nUltra-low Power Design\n~$95\nUltra-low\nBattery-powered devices\n\n\nGrove Vision AI\nHardware Acceleration\n~$25\nMedium\nIndustrial applications\n\n\nRaspberry Pi\nFull ML Frameworks\n$60-145\nHigh\nAdvanced edge computing",
    "crumbs": [
      "Platforms"
    ]
  },
  {
    "objectID": "contents/platforms.html#sec-hardware-kits-platform-comparison-2ad7",
    "href": "contents/platforms.html#sec-hardware-kits-platform-comparison-2ad7",
    "title": "Hardware Platforms",
    "section": "",
    "text": "Table¬†2 provides a comprehensive technical comparison of all four platforms.\n\n\n\nTable¬†2: Platform comparison matrix.\n\n\n\n\n\n\n\n\n\n\n\n\nCharacteristic\nXIAOML Kit\nRaspberry Pi\nArduino Nicla\nGrove Vision AI V2\n\n\n\n\nCost Range (USD)\n~$40\n$60-145\n~$95\n~$25\n\n\nPower Consumption\nLow\nHigh\nUltra-low\nMedium\n\n\nProcessing Power\nMedium\nVery High\nLow\nHigh (NPU)\n\n\nMemory Capacity\n8MB\n1-16GB\n2MB\n16MB\n\n\nPrimary Use Case\nIoT networks\nEdge computing\nBattery devices\nIndustrial AI\n\n\nML Framework\nTF Lite\nTensorFlow, PyTorch\nTensorFlow Lite\nSenseCraft AI\n\n\nDevelopment Env.\nArduino/ PlatformIO\nPython/Linux\nArduino IDE\nVisual/Code",
    "crumbs": [
      "Platforms"
    ]
  },
  {
    "objectID": "contents/platforms.html#sec-hardware-kits-platform-selection-guidelines-325e",
    "href": "contents/platforms.html#sec-hardware-kits-platform-selection-guidelines-325e",
    "title": "Hardware Platforms",
    "section": "",
    "text": "Selecting the appropriate platform depends on specific learning objectives and project requirements. Table¬†3 provides a systematic mapping to guide these decisions.\n\n\n\nTable¬†3: Platform capabilities matrix.\n\n\n\n\n\n\n\n\n\n\n\n\nLearning Objective/Application\nXIAOML Kit\nRas Pi\nArduino Nicla\nGrove Vision AI V2\n\n\n\n\nEmbedded Systems Basics\n‚úì\nLimited\n‚úì\n‚úì\n\n\nWireless Connectivity\n‚úì\n‚úì\n\n‚úì\n\n\nUltra-Low Power Design\n\n\n‚úì\n\n\n\nFull ML Frameworks\n\n‚úì\n\n\n\n\nHardware Acceleration\n\n\n\n‚úì\n\n\nReal-time Vision\nLimited\n‚úì\n‚úì\n‚úì\n\n\nEdge-Cloud Integration\n‚úì\n‚úì\n\n‚úì\n\n\nProduction Deployment\n‚úì\n\n‚úì\n‚úì",
    "crumbs": [
      "Platforms"
    ]
  },
  {
    "objectID": "contents/platforms.html#sec-hardware-kits-hardware-platform-specifications-01ae",
    "href": "contents/platforms.html#sec-hardware-kits-hardware-platform-specifications-01ae",
    "title": "Hardware Platforms",
    "section": "",
    "text": "This section provides detailed technical specifications for each platform, including processor architecture, memory hierarchy, sensor capabilities, and development toolchain requirements.\n\n\n\n\n\n\n\n\nTipBest For: IoT & Wireless ML\n\n\n\nThe XIAOML Kit excels at wireless connectivity and cost-sensitive deployments. It‚Äôs perfect for learning IoT sensor networks, remote monitoring systems, and wireless ML inference where you need reliable connectivity in a compact, affordable package.\n\n\nThe XIAO ESP32S3 represents the category of ultra-compact, wireless-enabled microcontrollers optimized for IoT applications. The name ‚ÄúXIAO‚Äù (Â∞è) translates to ‚Äútiny‚Äù in Chinese, reflecting the board‚Äôs 21√ó17.5mm form factor.\n\n\n\nXIAO ESP32S3 development board\n\n\nProcessor Architecture: ESP32-S3 dual-core Xtensa LX7 running at 240MHz\nMemory Hierarchy: 8MB PSRAM and 8MB Flash storage\nConnectivity: WiFi 802.11 b/g/n and Bluetooth 5.0\nIntegrated Sensors: OV2640 camera sensor, digital microphone, 6-axis inertial measurement unit\nPower Characteristics: 3.3V operation with multiple low-power modes\nDevelopment Environment: Arduino IDE and PlatformIO support with extensive library ecosystem. Supports C/C++ programming with Arduino-style abstractions and direct ESP-IDF for advanced users.\nApplication Focus: IoT sensor networks, remote monitoring systems, wireless ML inference, cost-sensitive deployments\n\n\n\n\n\n\n\n\n\nTipBest For: Ultra-Low Power Design\n\n\n\nThe Arduino Nicla Vision is optimized for battery-powered devices and always-on sensing applications. It‚Äôs ideal for learning ultra-low power design, image classification systems, and object detection applications where battery life is measured in months, not hours.\n\n\nThe Nicla Vision exemplifies professional-grade embedded vision systems built around the STM32H7 microcontroller. This platform demonstrates how specialized hardware design enables sophisticated ML inference within severe resource constraints.\n\n\n\nArduino Nicla Vision with camera module\n\n\nProcessor Architecture: STM32H747 dual-core ARM Cortex-M7/M4 running at 480MHz\nMemory Hierarchy: 2MB integrated RAM and 16MB Flash storage\nIntegrated Sensors: GC2145 camera sensor, MP34DT05 digital microphone, 6-axis IMU\nPower Characteristics: 3.3V operation optimized for battery-powered deployment\nDevelopment Environment: Arduino IDE and OpenMV IDE support with specialized computer vision libraries. MicroPython support for rapid prototyping alongside C/C++ for production deployments.\nApplication Focus: Battery-powered devices, image classification systems, object detection applications, always-on sensing\n\n\n\n\n\n\n\n\n\nTipBest For: Hardware Acceleration\n\n\n\nThe Grove Vision AI V2 features dedicated neural processing hardware for orders-of-magnitude performance improvements. It‚Äôs perfect for learning industrial inspection systems, real-time video analytics, and advanced object detection where you need NPU-accelerated inference capabilities.\n\n\nThe Grove Vision AI V2 incorporates dedicated neural processing hardware (NPU) to demonstrate hardware-accelerated ML inference. This platform illustrates how specialized AI processors achieve orders-of-magnitude performance improvements over software-only implementations.\n\n\n\nGrove Vision AI V2 with NPU\n\n\nProcessor Architecture: ARM Cortex-M55 with integrated Ethos-U55 NPU\nMemory Hierarchy: 16MB external memory for model and data storage\nNeural Processing Unit: Dedicated hardware accelerator for ML inference\nCamera Interface: Standard CSI connector supporting various camera modules\nAudio Input: Onboard digital microphone\nDevelopment Environment: SenseCraft AI visual programming platform for no-code development, with Arduino IDE support for custom applications. Supports both graphical programming and traditional C/C++ development workflows.\nApplication Focus: Industrial inspection systems, real-time video analytics, advanced object detection, NPU-accelerated inference\n\n\n\n\n\n\n\n\n\nTipBest For: Full ML Frameworks\n\n\n\nThe Raspberry Pi bridges embedded systems and traditional computing, providing a complete Linux environment for advanced ML applications. It‚Äôs ideal for learning edge AI gateways, advanced computer vision systems, language model deployment, and multi-modal AI applications where you need full computing capabilities.\n\n\nThe Raspberry Pi family bridges embedded systems and traditional computing, providing a full Linux environment while maintaining educational accessibility. This platform demonstrates how increased computational resources enable sophisticated ML applications.\n\n\n\nRaspberry Pi 5 and Pi Zero 2W comparison\n\n\nProcessor Architecture: ARM Cortex-A76 (Pi 5) or Cortex-A53 (Zero 2W)\nMemory Hierarchy: 1-16GB DDR4 RAM depending on model\nStorage: MicroSD card primary storage with USB 3.0 expansion\nConnectivity: Gigabit Ethernet, WiFi, Bluetooth, multiple USB ports\nCamera Interface: Dedicated CSI connector plus USB camera support\nOperating System: Debian-based Raspberry Pi OS (full Linux distribution)\nDevelopment Environment: Full Linux development environment with native Python, C/C++, and JavaScript support. Package managers (apt, pip) provide access to extensive ML libraries including TensorFlow, PyTorch, and OpenCV.\nApplication Focus: Edge AI gateways, advanced computer vision systems, language model deployment, multi-modal AI applications",
    "crumbs": [
      "Platforms"
    ]
  },
  {
    "objectID": "contents/platforms.html#sec-hardware-kits-getting-started-b984",
    "href": "contents/platforms.html#sec-hardware-kits-getting-started-b984",
    "title": "Hardware Platforms",
    "section": "",
    "text": "To get started with the hardware kits used in this course, you can purchase them directly from the following official sources:\n\nSeeed Studio ‚Äì XIAOML Kit and Grove Vision AI V2 Module\nArduino Store ‚Äì Nicla Vision\nRaspberry Pi Foundation ‚Äì Boards and Kits\nDigiKey, Mouser, SparkFun ‚Äî Alternative distributors for a variety of components and kits\n\nCheck each site for educational discounts, bundles, and regional availability. Most kits are available as starter packages that include the board and basic accessories.",
    "crumbs": [
      "Platforms"
    ]
  },
  {
    "objectID": "contents/raspi/object_detection/object_detection.html",
    "href": "contents/raspi/object_detection/object_detection.html",
    "title": "Object Detection",
    "section": "",
    "text": "DALL¬∑E prompt - A cover image for an ‚ÄòObject Detection‚Äô chapter in a Raspberry Pi tutorial, designed in the same vintage 1950s electronics lab style as previous covers. The scene should prominently feature wheels and cubes, similar to those provided by the user, placed on a workbench in the foreground. A Raspberry Pi with a connected camera module should be capturing an image of these objects. Surround the scene with classic lab tools like soldering irons, resistors, and wires. The lab background should include vintage equipment like oscilloscopes and tube radios, maintaining the detailed and nostalgic feel of the era. No text or logos should be included.\n\n\n\n\nBuilding upon our exploration of image classification, we now turn our attention to a more advanced computer vision task: object detection. While image classification assigns a single label to an entire image, object detection goes further by identifying and locating multiple objects within a single image. This capability opens up many new applications and challenges, particularly in edge computing and IoT devices like the Raspberry Pi.\nObject detection combines the tasks of classification and localization. It not only determines what objects are present in an image but also pinpoints their locations by, for example, drawing bounding boxes around them. This added complexity makes object detection a more powerful tool for understanding visual scenes, but it also requires more sophisticated models and training techniques.\nIn edge AI, where we work with constrained computational resources, implementing efficient object detection models becomes crucial. The challenges we faced with image classification‚Äîbalancing model size, inference speed, and accuracy‚Äîare amplified in object detection. However, the rewards are also more significant, as object detection enables more nuanced and detailed visual data analysis.\nSome applications of object detection on edge devices include:\n\nSurveillance and security systems\nAutonomous vehicles and drones\nIndustrial quality control\nWildlife monitoring\nAugmented reality applications\n\nAs we put our hands into object detection, we‚Äôll build upon the concepts and techniques we explored in image classification. We‚Äôll examine popular object detection architectures designed for efficiency, such as:\n\nSingle Stage Detectors, such as MobileNet and EfficientDet,\nFOMO (Faster Objects, More Objects), and\nYOLO (You Only Look Once).\n\n\nTo learn more about object detection models, follow the tutorial A Gentle Introduction to Object Recognition With Deep Learning.\n\nWe will explore those object detection models using\n\nTensorFlow Lite Runtime (now changed to LiteRT),\nEdge Impulse Linux Python SDK and\nUltralytics\n\n \nThroughout this lab, we‚Äôll cover the fundamentals of object detection and how it differs from image classification. We‚Äôll also learn how to train, fine-tune, test, optimize, and deploy popular object detection architectures using a dataset created from scratch.\n\n\nObject detection builds upon the foundations of image classification but extends its capabilities significantly. To understand object detection, it‚Äôs crucial first to recognize its key differences from image classification:\n\n\nImage Classification:\n\nAssigns a single label to an entire image\nAnswers the question: ‚ÄúWhat is this image‚Äôs primary object or scene?‚Äù\nOutputs a single class prediction for the whole image\n\nObject Detection:\n\nIdentifies and locates multiple objects within an image\nAnswers the questions: ‚ÄúWhat objects are in this image, and where are they located?‚Äù\nOutputs multiple predictions, each consisting of a class label and a bounding box\n\nTo visualize this difference, let‚Äôs consider an example:\n \nThis diagram illustrates the critical difference: image classification provides a single label for the entire image, while object detection identifies multiple objects, their classes, and their locations within the image.\n\n\n\nObject detection systems typically consist of two main components:\n\nObject Localization: This component identifies where objects are located in the image. It typically outputs bounding boxes, rectangular regions encompassing each detected object.\nObject Classification: This component determines the class or category of each detected object, similar to image classification but applied to each localized region.\n\n\n\n\nObject detection presents several challenges beyond those of image classification:\n\nMultiple objects: An image may contain multiple objects of various classes, sizes, and positions.\nVarying scales: Objects can appear at different sizes within the image.\nOcclusion: Objects may be partially hidden or overlapping.\nBackground clutter: Distinguishing objects from complex backgrounds can be challenging.\nReal-time performance: Many applications require fast inference times, especially on edge devices.\n\n\n\n\nThere are two main approaches to object detection:\n\nTwo-stage detectors: These first propose regions of interest and then classify each region. Examples include R-CNN and its variants (Fast R-CNN, Faster R-CNN).\nSingle-stage detectors: These predict bounding boxes (or centroids) and class probabilities in one forward pass of the network. Examples include YOLO (You Only Look Once), EfficientDet, SSD (Single Shot Detector), and FOMO (Faster Objects, More Objects). These are often faster and more suitable for edge devices like Raspberry Pi.\n\n\n\n\nObject detection uses different metrics compared to image classification:\n\nIntersection over Union (IoU): Measures the overlap between predicted and ground truth bounding boxes.\nMean Average Precision (mAP): Combines precision and recall across all classes and IoU thresholds.\nFrames Per Second (FPS): Measures detection speed, crucial for real-time applications on edge devices.\n\n\n\n\n\n\nAs we saw in the introduction, given an image or a video stream, an object detection model can identify which of a known set of objects might be present and provide information about their positions within the image.\n\nYou can test some common models online by visiting Object Detection - MediaPipe Studio\n\nOn Kaggle, we can find the most common pre-trained tflite models to use with the Raspi, ssd_mobilenet_v1, and EfficientDet. Those models were trained on the COCO (Common Objects in Context) dataset, with over 200,000 labeled images in 91 categories. Go, download the models, and upload them to the ./models folder in the Raspi.\n\nAlternatively, you can find the models and the COCO labels on GitHub.\n\nFor the first part of this lab, we will focus on a pre-trained \\(300\\times 300\\) SSD-Mobilenet V1 model and compare it with the \\(320\\times 320\\) EfficientDet-lite0, also trained using the COCO 2017 dataset. Both models were converted to a TensorFlow Lite format (4.2 MB for the SSD Mobilenet and 4.6 MB for the EfficientDet).\n\nSSD-Mobilenet V2 or V3 is recommended for transfer learning projects, but once the V1 TFLite model is publicly available, we will use it for this overview.\n\n \n\n\nWe should confirm the steps done on the last Hands-On Lab, Image Classification, as follows:\n\nUpdating the Raspberry Pi\nInstalling Required Libraries\nSetting up a Virtual Environment (Optional but Recommended)\n\nsource ~/tflite/bin/activate\n\nInstalling TensorFlow Lite Runtime\nInstalling Additional Python Libraries (inside the environment)\n\n\n\n\nConsidering that we have created the Documents/TFLITE folder in the last Lab, let‚Äôs now create the specific folders for this object detection lab:\ncd Documents/TFLITE/\nmkdir OBJ_DETECT\ncd OBJ_DETECT\nmkdir images\nmkdir models\ncd models\n\n\n\nLet‚Äôs start a new notebook to follow all the steps to detect objects on an image:\nImport the needed libraries:\nimport time\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom PIL import Image\nimport tflite_runtime.interpreter as tflite\nLoad the TFLite model and allocate tensors:\nmodel_path = \"./models/ssd-mobilenet-v1-tflite-default-v1.tflite\"\ninterpreter = tflite.Interpreter(model_path=model_path)\ninterpreter.allocate_tensors()\nGet input and output tensors.\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\nInput details will inform us how the model should be fed with an image. The shape of (1, 300, 300, 3) with a dtype of uint8 tells us that a non-normalized (pixel value range from 0 to 255) image with dimensions \\((300\\times 300\\times 3)\\) should be input one by one (Batch Dimension: 1).\nThe output details include not only the labels (‚Äúclasses‚Äù) and probabilities (‚Äúscores‚Äù) but also the relative window position of the bounding boxes (‚Äúboxes‚Äù) about where the object is located on the image and the number of detected objects (‚Äúnum_detections‚Äù). The output details also tell us that the model can detect a maximum of 10 objects in the image.\n \nSo, for the above example, using the same cat image used with the Image Classification Lab looking for the output, we have a 76% probability of having found an object with a class ID of 16 on an area delimited by a bounding box of [0.028011084, 0.020121813, 0.9886069, 0.802299]. Those four numbers are related to ymin, xmin, ymax and xmax, the box coordinates.\nTaking into consideration that y goes from the top (ymin) to the bottom (ymax) and x goes from left (xmin) to the right (xmax), we have, in fact, the coordinates of the top/left corner and the bottom/right one. With both edges and knowing the shape of the picture, it is possible to draw a rectangle around the object, as shown in the figure below:\n \nNext, we should find what class ID equal to 16 means. Opening the file coco_labels.txt, as a list, each element has an associated index, and inspecting index 16, we get, as expected, cat. The probability is the value returning from the score.\nLet‚Äôs now upload some images with multiple objects on it for testing.\nimg_path = \"./images/cat_dog.jpeg\"\norig_img = Image.open(img_path)\n\n# Display the image\nplt.figure(figsize=(8, 8))\nplt.imshow(orig_img)\nplt.title(\"Original Image\")\nplt.show()\n \nBased on the input details, let‚Äôs pre-process the image, changing its shape and expanding its dimension:\nimg = orig_img.resize(\n    (input_details[0][\"shape\"][1], input_details[0][\"shape\"][2])\n)\ninput_data = np.expand_dims(img, axis=0)\ninput_data.shape, input_data.dtype\nThe new input_data shape is(1, 300, 300, 3) with a dtype of uint8, which is compatible with what the model expects.\nUsing the input_data, let‚Äôs run the interpreter, measure the latency, and get the output:\nstart_time = time.time()\ninterpreter.set_tensor(input_details[0][\"index\"], input_data)\ninterpreter.invoke()\nend_time = time.time()\ninference_time = (\n    end_time - start_time\n) * 1000  # Convert to milliseconds\nprint(\"Inference time: {:.1f}ms\".format(inference_time))\nWith a latency of around 800 ms, we can get 4 distinct outputs:\nboxes = interpreter.get_tensor(output_details[0][\"index\"])[0]\nclasses = interpreter.get_tensor(output_details[1][\"index\"])[0]\nscores = interpreter.get_tensor(output_details[2][\"index\"])[0]\nnum_detections = int(\n    interpreter.get_tensor(output_details[3][\"index\"])[0]\n)\nOn a quick inspection, we can see that the model detected 2 objects with a score over 0.5:\nfor i in range(num_detections):\n    if scores[i] &gt; 0.5:  # Confidence threshold\n        print(f\"Object {i}:\")\n        print(f\"  Bounding Box: {boxes[i]}\")\n        print(f\"  Confidence: {scores[i]}\")\n        print(f\"  Class: {classes[i]}\")\n \nAnd we can also visualize the results:\nplt.figure(figsize=(12, 8))\nplt.imshow(orig_img)\nfor i in range(num_detections):\n    if scores[i] &gt; 0.5:  # Adjust threshold as needed\n        ymin, xmin, ymax, xmax = boxes[i]\n        (left, right, top, bottom) = (\n            xmin * orig_img.width,\n            xmax * orig_img.width,\n            ymin * orig_img.height,\n            ymax * orig_img.height,\n        )\n        rect = plt.Rectangle(\n            (left, top),\n            right - left,\n            bottom - top,\n            fill=False,\n            color=\"red\",\n            linewidth=2,\n        )\n        plt.gca().add_patch(rect)\n        class_id = int(classes[i])\n        class_name = labels[class_id]\n        plt.text(\n            left,\n            top - 10,\n            f\"{class_name}: {scores[i]:.2f}\",\n            color=\"red\",\n            fontsize=12,\n            backgroundcolor=\"white\",\n        )\n \n\n\n\nEfficientDet is not technically an SSD (Single Shot Detector) model, but it shares some similarities and builds upon ideas from SSD and other object detection architectures:\n\nEfficientDet:\n\nDeveloped by Google researchers in 2019\nUses EfficientNet as the backbone network\nEmploys a novel bi-directional feature pyramid network (BiFPN)\nIt uses compound scaling to scale the backbone network and the object detection components efficiently.\n\nSimilarities to SSD:\n\nBoth are single-stage detectors, meaning they perform object localization and classification in a single forward pass.\nBoth use multi-scale feature maps to detect objects at different scales.\n\nKey differences:\n\nBackbone: SSD typically uses VGG or MobileNet, while EfficientDet uses EfficientNet.\nFeature fusion: SSD uses a simple feature pyramid, while EfficientDet uses the more advanced BiFPN.\nScaling method: EfficientDet introduces compound scaling for all components of the network\n\nAdvantages of EfficientDet:\n\nGenerally achieves better accuracy-efficiency trade-offs than SSD and many other object detection models.\nMore flexible scaling allows for a family of models with different size-performance trade-offs.\n\n\nWhile EfficientDet is not an SSD model, it can be seen as an evolution of single-stage detection architectures, incorporating more advanced techniques to improve efficiency and accuracy. When using EfficientDet, we can expect similar output structures to SSD (e.g., bounding boxes and class scores).\n\nOn GitHub, you can find another notebook exploring the EfficientDet model that we did with SSD MobileNet.\n\n\n\n\n\nNow, we will develop a complete Image Classification project from data collection to training and deployment. As we did with the Image Classification project, the trained and converted model will be used for inference.\nWe will use the same dataset to train 3 models: SSD-MobileNet V2, FOMO, and YOLO.\n\n\nAll Machine Learning projects need to start with a goal. Let‚Äôs assume we are in an industrial facility and must sort and count wheels and special boxes.\n \nIn other words, we should perform a multi-label classification, where each image can have three classes:\n\nBackground (no objects)\nBox\nWheel\n\n\n\n\nOnce we have defined our Machine Learning project goal, the next and most crucial step is collecting the dataset. We can use a phone, the Raspi, or a mix to create the raw dataset (with no labels). Let‚Äôs use the simple web app on our Raspberry Pi to view the QVGA (320 x 240) captured images in a browser.\nFrom GitHub, get the Python script get_img_data.py and open it in the terminal:\npython3 get_img_data.py\nAccess the web interface:\n\nOn the Raspberry Pi itself (if you have a GUI): Open a web browser and go to http://localhost:5000\nFrom another device on the same network: Open a web browser and go to http://&lt;raspberry_pi_ip&gt;:5000 (Replace &lt;raspberry_pi_ip&gt; with your Raspberry Pi‚Äôs IP address). For example: http://192.168.4.210:5000/\n\n \nThe Python script creates a web-based interface for capturing and organizing image datasets using a Raspberry Pi and its camera. It‚Äôs handy for machine learning projects that require labeled image data or not, as in our case here.\nAccess the web interface from a browser, enter a generic label for the images you want to capture, and press Start Capture.\n \n\nNote that the images to be captured will have multiple labels that should be defined later.\n\nUse the live preview to position the camera and click Capture Image to save images under the current label (in this case, box-wheel.\n \nWhen we have enough images, we can press Stop Capture. The captured images are saved on the folder dataset/box-wheel:\n \n\nGet around 60 images. Try to capture different angles, backgrounds, and light conditions. Filezilla can transfer the created raw dataset to your main computer.\n\n\n\n\nThe next step in an Object Detect project is to create a labeled dataset. We should label the raw dataset images, creating bounding boxes around each picture‚Äôs objects (box and wheel). We can use labeling tools like LabelImg, CVAT, Roboflow, or even the Edge Impulse Studio. Once we have explored the Edge Impulse tool in other labs, let‚Äôs use Roboflow here.\n\nWe are using Roboflow (free version) here for two main reasons. 1) We can have auto-labeler, and 2) The annotated dataset is available in several formats and can be used both on Edge Impulse Studio (we will use it for MobileNet V2 and FOMO train) and on CoLab (YOLOv8 train), for example. Having the annotated dataset on Edge Impulse (Free account), it is not possible to use it for training on other platforms.\n\nWe should upload the raw dataset to Roboflow. Create a free account there and start a new project, for example, (‚Äúbox-versus-wheel‚Äù).\n \n\nWe will not enter in deep details about the Roboflow process once many tutorials are available.\n\n\n\nOnce the project is created and the dataset is uploaded, you should make the annotations using the ‚ÄúAuto-Label‚Äù Tool. Note that you can also upload images with only a background, which should be saved w/o any annotations.\n \nOnce all images are annotated, you should split them into training, validation, and testing.\n \n\n\n\nThe last step with the dataset is preprocessing to generate a final version for training. Let‚Äôs resize all images to \\(320\\times 320\\) and generate augmented versions of each image (augmentation) to create new training examples from which our model can learn.\nFor augmentation, we will rotate the images (+/-15o), crop, and vary the brightness and exposure.\n \nAt the end of the process, we will have 153 images.\n \nNow, you should export the annotated dataset in a format that Edge Impulse, Ultralytics, and other frameworks/tools understand, for example, YOLOv8. Let‚Äôs download a zipped version of the dataset to our desktop.\n \nHere, it is possible to review how the dataset was structured\n \nThere are 3 separate folders, one for each split (train/test/valid). For each of them, there are 2 subfolders, images, and labels. The pictures are stored as image_id.jpg and images_id.txt, where ‚Äúimage_id‚Äù is unique for every picture.\nThe labels file format will be class_id bounding box coordinates, where in our case, class_id will be 0 for box and 1 for wheel. The numerical id (o, 1, 2‚Ä¶) will follow the alphabetical order of the class name.\nThe data.yaml file has info about the dataset as the classes‚Äô names (names: ['box', 'wheel']) following the YOLO format.\nAnd that‚Äôs it! We are ready to start training using the Edge Impulse Studio (as we will do in the following step), Ultralytics (as we will when discussing YOLO), or even training from scratch on CoLab (as we did with the Cifar-10 dataset on the Image Classification lab).\n\nThe pre-processed dataset can be found at the Roboflow site.\n\n\n\n\n\n\nGo to Edge Impulse Studio, enter your credentials at Login (or create an account), and start a new project.\n\nHere, you can clone the project developed for this hands-on lab: Raspi - Object Detection.\n\nOn the Project Dashboard tab, go down and on Project info, and for Labeling method select Bounding boxes (object detection)\n\n\nOn Studio, go to the Data acquisition tab, and on the UPLOAD DATA section, upload from your computer the raw dataset.\nWe can use the option Select a folder, choosing, for example, the folder train in your computer, which contains two sub-folders, images, and labels. Select the Image label format, ‚ÄúYOLO TXT‚Äù, upload into the category Training, and press Upload data.\n \nRepeat the process for the test data (upload both folders, test, and validation). At the end of the upload process, you should end with the annotated dataset of 153 images split in the train/test (84%/16%).\n\nNote that labels will be stored at the labels files 0 and 1 , which are equivalent to box and wheel.\n\n \n\n\n\nThe first thing to define when we enter the Create impulse step is to describe the target device for deployment. A pop-up window will appear. We will select Raspberry 4, an intermediary device between the Raspi-Zero and the Raspi-5.\n\nThis choice will not interfere with the training; it will only give us an idea about the latency of the model on that specific target.\n\n \nIn this phase, you should define how to:\n\nPre-processing consists of resizing the individual images. In our case, the images were pre-processed on Roboflow, to 320x320 , so let‚Äôs keep it. The resize will not matter here because the images are already squared. If you upload a rectangular image, squash it (squared form, without cropping). Afterward, you could define if the images are converted from RGB to Grayscale or not.\nDesign a Model, in this case, ‚ÄúObject Detection.‚Äù\n\n \n\n\n\nIn the section Image, select Color depth as RGB, and press Save parameters.\n \nThe Studio moves automatically to the next section, Generate features, where all samples will be pre-processed, resulting in 480 objects: 207 boxes and 273 wheels.\n \nThe feature explorer shows that all samples evidence a good separation after the feature generation.\n\n\n\nFor training, we should select a pre-trained model. Let‚Äôs use the MobileNetV2 SSD FPN-Lite (320x320 only) . It is a pre-trained object detection model designed to locate up to 10 objects within an image, outputting a bounding box for each object detected. The model is around 3.7 MB in size. It supports an RGB input at \\(320\\times 320\\) px.\nRegarding the training hyper-parameters, the model will be trained with:\n\nEpochs: 25\nBatch size: 32\nLearning Rate: 0.15.\n\nFor validation during training, 20% of the dataset (validation_dataset) will be spared.\n \nAs a result, the model ends with an overall precision score (based on COCO mAP) of 88.8%, higher than the result when using the test data (83.3%).\n\n\n\nWe have two ways to deploy our model:\n\nTFLite model, which lets deploy the trained model as .tflite for the Raspi to run it using Python.\nLinux (AARCH64), a binary for Linux (AARCH64), implements the Edge Impulse Linux protocol, which lets us run our models on any Linux-based development board, with SDKs for Python, for example. See the documentation for more information and setup instructions.\n\nLet‚Äôs deploy the TFLite model. On the Dashboard tab, go to Transfer learning model (int8 quantized) and click on the download icon:\n \nTransfer the model from your computer to the Raspi folder./models and capture or get some images for inference and save them in the folder ./images.\n\n\n\nThe inference can be made as discussed in the Pre-Trained Object Detection Models Overview. Let‚Äôs start a new notebook to follow all the steps to detect cubes and wheels on an image.\nImport the needed libraries:\nimport time\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as patches\nfrom PIL import Image\nimport tflite_runtime.interpreter as tflite\nDefine the model path and labels:\nmodel_path = \"./models/ei-raspi-object-detection-SSD-\\\n              MobileNetv2-320x0320-int8.lite\"\nlabels = [\"box\", \"wheel\"]\n\nRemember that the model will output the class ID as values (0 and 1), following an alphabetic order regarding the class names.\n\nLoad the model, allocate the tensors, and get the input and output tensor details:\n# Load the TFLite model\ninterpreter = tflite.Interpreter(model_path=model_path)\ninterpreter.allocate_tensors()\n\n# Get input and output tensors\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\nOne crucial difference to note is that the dtype of the input details of the model is now int8, which means that the input values go from ‚Äì128 to +127, while each pixel of our raw image goes from 0 to 256. This means that we should pre-process the image to match it. We can check here:\ninput_dtype = input_details[0][\"dtype\"]\ninput_dtype\nnumpy.int8\nSo, let‚Äôs open the image and show it:\n# Load the image\nimg_path = \"./images/box_2_wheel_2.jpg\"\norig_img = Image.open(img_path)\n\n# Display the image\nplt.figure(figsize=(6, 6))\nplt.imshow(orig_img)\nplt.title(\"Original Image\")\nplt.show()\n \nAnd perform the pre-processing:\nscale, zero_point = input_details[0][\"quantization\"]\nimg = orig_img.resize(\n    (input_details[0][\"shape\"][1], input_details[0][\"shape\"][2])\n)\nimg_array = np.array(img, dtype=np.float32) / 255.0\nimg_array = (\n    (img_array / scale + zero_point).clip(-128, 127).astype(np.int8)\n)\ninput_data = np.expand_dims(img_array, axis=0)\nChecking the input data, we can verify that the input tensor is compatible with what is expected by the model:\ninput_data.shape, input_data.dtype\n((1, 320, 320, 3), dtype('int8'))\nNow, it is time to perform the inference. Let‚Äôs also calculate the latency of the model:\n# Inference on Raspi-Zero\nstart_time = time.time()\ninterpreter.set_tensor(input_details[0][\"index\"], input_data)\ninterpreter.invoke()\nend_time = time.time()\ninference_time = (\n    end_time - start_time\n) * 1000  # Convert to milliseconds\nprint(\"Inference time: {:.1f}ms\".format(inference_time))\nThe model will take around 600ms to perform the inference in the Raspi-Zero, which is around 5 times longer than a Raspi-5.\nNow, we can get the output classes of objects detected, its bounding boxes coordinates, and probabilities.\nboxes = interpreter.get_tensor(output_details[1][\"index\"])[0]\nclasses = interpreter.get_tensor(output_details[3][\"index\"])[0]\nscores = interpreter.get_tensor(output_details[0][\"index\"])[0]\nnum_detections = int(\n    interpreter.get_tensor(output_details[2][\"index\"])[0]\n)\nfor i in range(num_detections):\n    if scores[i] &gt; 0.5:  # Confidence threshold\n        print(f\"Object {i}:\")\n        print(f\"  Bounding Box: {boxes[i]}\")\n        print(f\"  Confidence: {scores[i]}\")\n        print(f\"  Class: {classes[i]}\")\n \nFrom the results, we can see that 4 objects were detected: two with class ID 0 (box)and two with class ID 1 (wheel), what is correct!\nLet‚Äôs visualize the result for a threshold of 0.5\nthreshold = 0.5\nplt.figure(figsize=(6, 6))\nplt.imshow(orig_img)\nfor i in range(num_detections):\n    if scores[i] &gt; threshold:\n        ymin, xmin, ymax, xmax = boxes[i]\n        (left, right, top, bottom) = (\n            xmin * orig_img.width,\n            xmax * orig_img.width,\n            ymin * orig_img.height,\n            ymax * orig_img.height,\n        )\n        rect = plt.Rectangle(\n            (left, top),\n            right - left,\n            bottom - top,\n            fill=False,\n            color=\"red\",\n            linewidth=2,\n        )\n        plt.gca().add_patch(rect)\n        class_id = int(classes[i])\n        class_name = labels[class_id]\n        plt.text(\n            left,\n            top - 10,\n            f\"{class_name}: {scores[i]:.2f}\",\n            color=\"red\",\n            fontsize=12,\n            backgroundcolor=\"white\",\n        )\n \nBut what happens if we reduce the threshold to 0.3, for example?\n \nWe start to see false positives and multiple detections, where the model detects the same object multiple times with different confidence levels and slightly different bounding boxes.\nCommonly, sometimes, we need to adjust the threshold to smaller values to capture all objects, avoiding false negatives, which would lead to multiple detections.\nTo improve the detection results, we should implement Non-Maximum Suppression (NMS), which helps eliminate overlapping bounding boxes and keeps only the most confident detection.\nFor that, let‚Äôs create a general function named non_max_suppression(), with the role of refining object detection results by eliminating redundant and overlapping bounding boxes. It achieves this by iteratively selecting the detection with the highest confidence score and removing other significantly overlapping detections based on an Intersection over Union (IoU) threshold.\ndef non_max_suppression(boxes, scores, threshold):\n    # Convert to corner coordinates\n    x1 = boxes[:, 0]\n    y1 = boxes[:, 1]\n    x2 = boxes[:, 2]\n    y2 = boxes[:, 3]\n\n    areas = (x2 - x1 + 1) * (y2 - y1 + 1)\n    order = scores.argsort()[::-1]\n\n    keep = []\n    while order.size &gt; 0:\n        i = order[0]\n        keep.append(i)\n        xx1 = np.maximum(x1[i], x1[order[1:]])\n        yy1 = np.maximum(y1[i], y1[order[1:]])\n        xx2 = np.minimum(x2[i], x2[order[1:]])\n        yy2 = np.minimum(y2[i], y2[order[1:]])\n\n        w = np.maximum(0.0, xx2 - xx1 + 1)\n        h = np.maximum(0.0, yy2 - yy1 + 1)\n        inter = w * h\n        ovr = inter / (areas[i] + areas[order[1:]] - inter)\n\n        inds = np.where(ovr &lt;= threshold)[0]\n        order = order[inds + 1]\n\n    return keep\nHow it works:\n\nSorting: It starts by sorting all detections by their confidence scores, highest to lowest.\nSelection: It selects the highest-scoring box and adds it to the final list of detections.\nComparison: This selected box is compared with all remaining lower-scoring boxes.\nElimination: Any box that overlaps significantly (above the IoU threshold) with the selected box is eliminated.\nIteration: This process repeats with the next highest-scoring box until all boxes are processed.\n\nNow, we can define a more precise visualization function that will take into consideration an IoU threshold, detecting only the objects that were selected by the non_max_suppression function:\ndef visualize_detections(\n    image, boxes, classes, scores, labels, threshold, iou_threshold\n):\n    if isinstance(image, Image.Image):\n        image_np = np.array(image)\n    else:\n        image_np = image\n    height, width = image_np.shape[:2]\n    # Convert normalized coordinates to pixel coordinates\n    boxes_pixel = boxes * np.array([height, width, height, width])\n    # Apply NMS\n    keep = non_max_suppression(boxes_pixel, scores, iou_threshold)\n    # Set the figure size to 12x8 inches\n    fig, ax = plt.subplots(1, figsize=(12, 8))\n    ax.imshow(image_np)\n    for i in keep:\n        if scores[i] &gt; threshold:\n            ymin, xmin, ymax, xmax = boxes[i]\n            rect = patches.Rectangle(\n                (xmin * width, ymin * height),\n                (xmax - xmin) * width,\n                (ymax - ymin) * height,\n                linewidth=2,\n                edgecolor=\"r\",\n                facecolor=\"none\",\n            )\n\n            ax.add_patch(rect)\n            class_name = labels[int(classes[i])]\n            ax.text(\n                xmin * width,\n                ymin * height - 10,\n                f\"{class_name}: {scores[i]:.2f}\",\n                color=\"red\",\n                fontsize=12,\n                backgroundcolor=\"white\",\n            )\n    plt.show()\nNow we can create a function that will call the others, performing inference on any image:\ndef detect_objects(img_path, conf=0.5, iou=0.5):\n    orig_img = Image.open(img_path)\n    scale, zero_point = input_details[0][\"quantization\"]\n    img = orig_img.resize(\n        (input_details[0][\"shape\"][1], input_details[0][\"shape\"][2])\n    )\n    img_array = np.array(img, dtype=np.float32) / 255.0\n    img_array = (\n        (img_array / scale + zero_point)\n        .clip(-128, 127)\n        .astype(np.int8)\n    )\n    input_data = np.expand_dims(img_array, axis=0)\n\n    # Inference on Raspi-Zero\n    start_time = time.time()\n    interpreter.set_tensor(input_details[0][\"index\"], input_data)\n    interpreter.invoke()\n    end_time = time.time()\n    inference_time = (\n        end_time - start_time\n    ) * 1000  # Convert to milliseconds\n\n    print(\"Inference time: {:.1f}ms\".format(inference_time))\n\n    # Extract the outputs\n    boxes = interpreter.get_tensor(output_details[1][\"index\"])[0]\n    classes = interpreter.get_tensor(output_details[3][\"index\"])[0]\n    scores = interpreter.get_tensor(output_details[0][\"index\"])[0]\n    num_detections = int(\n        interpreter.get_tensor(output_details[2][\"index\"])[0]\n    )\n\n    visualize_detections(\n        orig_img,\n        boxes,\n        classes,\n        scores,\n        labels,\n        threshold=conf,\n        iou_threshold=iou,\n    )\nNow, running the code, having the same image again with a confidence threshold of 0.3, but with a small IoU:\nimg_path = \"./images/box_2_wheel_2.jpg\"\ndetect_objects(img_path, conf=0.3, iou=0.05)\n \n\n\n\n\nThe inference with the SSD MobileNet model worked well, but the latency was significantly high. The inference varied from 0.5 to 1.3 seconds on a Raspi-Zero, which means around or less than 1 FPS (1 frame per second). One alternative to speed up the process is to use FOMO (Faster Objects, More Objects).\nThis novel machine learning algorithm lets us count multiple objects and find their location in an image in real-time using up to \\(30\\times\\) less processing power and memory than MobileNet SSD or YOLO. The main reason this is possible is that while other models calculate the object‚Äôs size by drawing a square around it (bounding box), FOMO ignores the size of the image, providing only the information about where the object is located in the image through its centroid coordinates.\n\n\nIn a typical object detection pipeline, the first stage is extracting features from the input image. FOMO leverages MobileNetV2 to perform this task. MobileNetV2 processes the input image to produce a feature map that captures essential characteristics, such as textures, shapes, and object edges, in a computationally efficient way.\n \nOnce these features are extracted, FOMO‚Äôs simpler architecture, focused on center-point detection, interprets the feature map to determine where objects are located in the image. The output is a grid of cells, where each cell represents whether or not an object center is detected. The model outputs one or more confidence scores for each cell, indicating the likelihood of an object being present.\nLet‚Äôs see how it works on an image.\nFOMO divides the image into blocks of pixels using a factor of 8. For the input of \\(96\\times 96\\), the grid would be \\(12\\times 12\\) \\((96/8=12)\\). For a \\(160\\times 160\\), the grid will be \\(20\\times 20\\), and so on. Next, FOMO will run a classifier through each pixel block to calculate the probability that there is a box or a wheel in each of them and, subsequently, determine the regions that have the highest probability of containing the object (If a pixel block has no objects, it will be classified as background). From the overlap of the final region, the FOMO provides the coordinates (related to the image dimensions) of the centroid of this region.\n \nTrade-off Between Speed and Precision:\n\nGrid Resolution: FOMO uses a grid of fixed resolution, meaning each cell can detect if an object is present in that part of the image. While it doesn‚Äôt provide high localization accuracy, it makes a trade-off by being fast and computationally light, which is crucial for edge devices.\nMulti-Object Detection: Since each cell is independent, FOMO can detect multiple objects simultaneously in an image by identifying multiple centers.\n\n\n\n\nReturn to Edge Impulse Studio, and in the Experiments tab, create another impulse. Now, the input images should be \\(160\\times 160\\) (this is the expected input size for MobilenetV2).\n \nOn the Image tab, generate the features and go to the Object detection tab.\nWe should select a pre-trained model for training. Let‚Äôs use the FOMO (Faster Objects, More Objects) MobileNetV2 0.35.\n \nRegarding the training hyper-parameters, the model will be trained with:\n\nEpochs: 30\nBatch size: 32\nLearning Rate: 0.001.\n\nFor validation during training, 20% of the dataset (validation_dataset) will be spared. We will not apply Data Augmentation for the remaining 80% (train_dataset) because our dataset was already augmented during the labeling phase at Roboflow.\nAs a result, the model ends with an overall F1 score of 93.3% with an impressive latency of 8 ms (Raspi-4), around \\(60\\times\\) less than we got with the SSD MovileNetV2.\n \n\nNote that FOMO automatically added a third label background to the two previously defined boxes (0) and wheels (1).\n\nOn the Model testing tab, we can see that the accuracy was 94%. Here is one of the test sample results:\n \n\nIn object detection tasks, accuracy is generally not the primary evaluation metric. Object detection involves classifying objects and providing bounding boxes around them, making it a more complex problem than simple classification. The issue is that we do not have the bounding box, only the centroids. In short, using accuracy as a metric could be misleading and may not provide a complete understanding of how well the model is performing.\n\n\n\n\nAs we did in the previous section, we can deploy the trained model as TFLite or Linux (AARCH64). Let‚Äôs do it now as Linux (AARCH64), a binary that implements the Edge Impulse Linux protocol.\nEdge Impulse for Linux models is delivered in .eim format. This executable contains our ‚Äúfull impulse‚Äù created in Edge Impulse Studio. The impulse consists of the signal processing block(s) and any learning and anomaly block(s) we added and trained. It is compiled with optimizations for our processor or GPU (e.g., NEON instructions on ARM cores), plus a straightforward IPC layer (over a Unix socket).\nAt the Deploy tab, select the option Linux (AARCH64), the int8model and press Build.\n \nThe model will be automatically downloaded to your computer.\nOn our Raspi, let‚Äôs create a new working area:\ncd ~\ncd Documents\nmkdir EI_Linux\ncd EI_Linux\nmkdir models\nmkdir images\nRename the model for easy identification:\nFor example, raspi-object-detection-linux-aarch64-FOMO-int8.eim and transfer it to the new Raspi folder./models and capture or get some images for inference and save them in the folder ./images.\n\n\n\nThe inference will be made using the Linux Python SDK. This library lets us run machine learning models and collect sensor data on Linux machines using Python. The SDK is open source and hosted on GitHub: edgeimpulse/linux-sdk-python.\nLet‚Äôs set up a Virtual Environment for working with the Linux Python SDK\npython3 -m venv ~/eilinux\nsource ~/eilinux/bin/activate\nAnd Install the all the libraries needed:\nsudo apt-get update\nsudo apt-get install libatlas-base-dev\\\n                     libportaudio0 libportaudio2\nsudo apt-get installlibportaudiocpp0 portaudio19-dev\n\npip3 install edge_impulse_linux -i https://pypi.python.org/simple\npip3 install Pillow matplotlib pyaudio opencv-contrib-python\n\nsudo apt-get install portaudio19-dev\npip3 install pyaudio\npip3 install opencv-contrib-python\nPermit our model to be executable.\nchmod +x raspi-object-detection-linux-aarch64-FOMO-int8.eim\nInstall the Jupiter Notebook on the new environment\npip3 install jupyter\nRun a notebook locally (on the Raspi-4 or 5 with desktop)\njupyter notebook\nor on the browser on your computer:\njupyter notebook --ip=192.168.4.210 --no-browser\nLet‚Äôs start a new notebook by following all the steps to detect cubes and wheels on an image using the FOMO model and the Edge Impulse Linux Python SDK.\nImport the needed libraries:\nimport sys, time\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as patches\nfrom PIL import Image\nimport cv2\nfrom edge_impulse_linux.image import ImageImpulseRunner\nDefine the model path and labels:\nmodel_file = \"raspi-object-detection-linux-aarch64-int8.eim\"\nmodel_path = \"models/\" + model_file  # Trained ML model from\n# Edge Impulse\nlabels = [\"box\", \"wheel\"]\n\nRemember that the model will output the class ID as values (0 and 1), following an alphabetic order regarding the class names.\n\nLoad and initialize the model:\n# Load the model file\nrunner = ImageImpulseRunner(model_path)\n\n# Initialize model\nmodel_info = runner.init()\nThe model_info will contain critical information about our model. However, unlike the TFLite interpreter, the EI Linux Python SDK library will now prepare the model for inference.\nSo, let‚Äôs open the image and show it (Now, for compatibility, we will use OpenCV, the CV Library used internally by EI. OpenCV reads the image as BGR, so we will need to convert it to RGB :\n# Load the image\nimg_path = \"./images/1_box_1_wheel.jpg\"\norig_img = cv2.imread(img_path)\nimg_rgb = cv2.cvtColor(orig_img, cv2.COLOR_BGR2RGB)\n\n# Display the image\nplt.imshow(img_rgb)\nplt.title(\"Original Image\")\nplt.show()\n \nNow we will get the features and the preprocessed image (cropped) using the runner:\nfeatures, cropped = (\n    runner.get_features_from_image_auto_studio_settings(img_rgb)\n)\nAnd perform the inference. Let‚Äôs also calculate the latency of the model:\nres = runner.classify(features)\nLet‚Äôs get the output classes of objects detected, their bounding boxes centroids, and probabilities.\nprint(\n    \"Found %d bounding boxes (%d ms.)\"\n    % (\n        len(res[\"result\"][\"bounding_boxes\"]),\n        res[\"timing\"][\"dsp\"] + res[\"timing\"][\"classification\"],\n    )\n)\nfor bb in res[\"result\"][\"bounding_boxes\"]:\n    print(\n        \"\\t%s (%.2f): x=%d y=%d w=%d h=%d\"\n        % (\n            bb[\"label\"],\n            bb[\"value\"],\n            bb[\"x\"],\n            bb[\"y\"],\n            bb[\"width\"],\n            bb[\"height\"],\n        )\n    )\nFound 2 bounding boxes (29 ms.)\n    1 (0.91): x=112 y=40 w=16 h=16\n    0 (0.75): x=48 y=56 w=8 h=8\nThe results show that two objects were detected: one with class ID 0 (box) and one with class ID 1 (wheel), which is correct!\nLet‚Äôs visualize the result (The threshold is 0.5, the default value set during the model testing on the Edge Impulse Studio).\nprint(\n    \"\\tFound %d bounding boxes (latency: %d ms)\"\n    % (\n        len(res[\"result\"][\"bounding_boxes\"]),\n        res[\"timing\"][\"dsp\"] + res[\"timing\"][\"classification\"],\n    )\n)\nplt.figure(figsize=(5, 5))\nplt.imshow(cropped)\n\n# Go through each of the returned bounding boxes\nbboxes = res[\"result\"][\"bounding_boxes\"]\nfor bbox in bboxes:\n\n    # Get the corners of the bounding box\n    left = bbox[\"x\"]\n    top = bbox[\"y\"]\n    width = bbox[\"width\"]\n    height = bbox[\"height\"]\n\n    # Draw a circle centered on the detection\n    circ = plt.Circle(\n        (left + width // 2, top + height // 2),\n        5,\n        fill=False,\n        color=\"red\",\n        linewidth=3,\n    )\n    plt.gca().add_patch(circ)\n    class_id = int(bbox[\"label\"])\n    class_name = labels[class_id]\n    plt.text(\n        left,\n        top - 10,\n        f'{class_name}: {bbox[\"value\"]:.2f}',\n        color=\"red\",\n        fontsize=12,\n        backgroundcolor=\"white\",\n    )\nplt.show()\n \n\n\n\n\nFor this lab, we will explore YOLOv8. Ultralytics YOLOv8 is a version of the acclaimed real-time object detection and image segmentation model, YOLO. YOLOv8 is built on cutting-edge advancements in deep learning and computer vision, offering unparalleled performance in terms of speed and accuracy. Its streamlined design makes it suitable for various applications and easily adaptable to different hardware platforms, from edge devices to cloud APIs.\n\n\nThe YOLO (You Only Look Once) model is a highly efficient and widely used object detection algorithm known for its real-time processing capabilities. Unlike traditional object detection systems that repurpose classifiers or localizers to perform detection, YOLO frames the detection problem as a single regression task. This innovative approach enables YOLO to simultaneously predict multiple bounding boxes and their class probabilities from full images in one evaluation, significantly boosting its speed.\n\n\n\nSingle Network Architecture:\n\nYOLO employs a single neural network to process the entire image. This network divides the image into a grid and, for each grid cell, directly predicts bounding boxes and associated class probabilities. This end-to-end training improves speed and simplifies the model architecture.\n\nReal-Time Processing:\n\nOne of YOLO‚Äôs standout features is its ability to perform object detection in real-time. Depending on the version and hardware, YOLO can process images at high frames per second (FPS). This makes it ideal for applications requiring quick and accurate object detection, such as video surveillance, autonomous driving, and live sports analysis.\n\nEvolution of Versions:\n\nOver the years, YOLO has undergone significant improvements, from YOLOv1 to the latest YOLOv10. Each iteration has introduced enhancements in accuracy, speed, and efficiency. YOLOv8, for instance, incorporates advancements in network architecture, improved training methodologies, and better support for various hardware, ensuring a more robust performance.\nAlthough YOLOv10 is the family‚Äôs newest member with an encouraging performance based on its paper, it was just released (May 2024) and is not fully integrated with the Ultralytics library. Conversely, the precision-recall curve analysis suggests that YOLOv8 generally outperforms YOLOv9, capturing a higher proportion of true positives while minimizing false positives more effectively (for more details, see this article). So, this lab is based on the YOLOv8n.\n\n \nAccuracy and Efficiency:\n\nWhile early versions of YOLO traded off some accuracy for speed, recent versions have made substantial strides in balancing both. The newer models are faster and more accurate, detecting small objects (such as bees) and performing well on complex datasets.\n\nWide Range of Applications:\n\nYOLO‚Äôs versatility has led to its adoption in numerous fields. It is used in traffic monitoring systems to detect and count vehicles, security applications to identify potential threats and agricultural technology to monitor crops and livestock. Its application extends to any domain requiring efficient and accurate object detection.\n\nCommunity and Development:\n\nYOLO continues to evolve and is supported by a strong community of developers and researchers (being the YOLOv8 very strong). Open-source implementations and extensive documentation have made it accessible for customization and integration into various projects. Popular deep learning frameworks like Darknet, TensorFlow, and PyTorch support YOLO, further broadening its applicability.\nUltralytics YOLOv8 can not only Detect (our case here) but also Segment and Pose models pre-trained on the COCO dataset and YOLOv8 Classify models pre-trained on the ImageNet dataset. Track mode is available for all Detect, Segment, and Pose models.\n\n\n\n\nUltralytics YOLO supported tasks\n\n\n\n\n\n\n\nOn our Raspi, let‚Äôs deactivate the current environment to create a new working area:\ndeactivate\ncd ~\ncd Documents/\nmkdir YOLO\ncd YOLO\nmkdir models\nmkdir images\nLet‚Äôs set up a Virtual Environment for working with the Ultralytics YOLOv8\npython3 -m venv ~/yolo\nsource ~/yolo/bin/activate\nAnd install the Ultralytics packages for local inference on the Raspi\n\nUpdate the packages list, install pip, and upgrade to the latest:\n\nsudo apt update\nsudo apt install python3-pip -y\npip install -U pip\n\nInstall the ultralytics pip package with optional dependencies:\n\npip install ultralytics[export]\n\nReboot the device:\n\nsudo reboot\n\n\n\nAfter the Raspi-Zero booting, let‚Äôs activate the yolo env, go to the working directory,\nsource ~/yolo/bin/activate\ncd /Documents/YOLO\n and run inference on an image that will be downloaded from the Ultralytics website, using the YOLOV8n model (the smallest in the family) at the Terminal (CLI):\nyolo predict model='yolov8n' \\\n     source='https://ultralytics.com/images/bus.jpg'\n\nThe YOLO model family is pre-trained with the COCO dataset.\n\nThe inference result will appear in the terminal. In the image (bus.jpg), 4 persons, 1 bus, and 1 stop signal were detected:\n \nAlso, we got a message that Results saved to runs/detect/predict. Inspecting that directory, we can see a new image saved (bus.jpg). Let‚Äôs download it from the Raspi-Zero to our desktop for inspection:\n \nSo, the Ultralytics YOLO is correctly installed on our Raspi. But, on the Raspi-Zero, an issue is the high latency for this inference, around 18 seconds, even with the most miniature model of the family (YOLOv8n).\n\n\n\nDeploying computer vision models on edge devices with limited computational power, such as the Raspi-Zero, can cause latency issues. One alternative is to use a format optimized for optimal performance. This ensures that even devices with limited processing power can handle advanced computer vision tasks well.\nOf all the model export formats supported by Ultralytics, the NCNN is a high-performance neural network inference computing framework optimized for mobile platforms. From the beginning of the design, NCNN was deeply considerate about deployment and use on mobile phones and did not have third-party dependencies. It is cross-platform and runs faster than all known open-source frameworks (such as TFLite).\nNCNN delivers the best inference performance when working with Raspberry Pi devices. NCNN is highly optimized for mobile embedded platforms (such as ARM architecture).\nSo, let‚Äôs convert our model and rerun the inference:\n\nExport a YOLOv8n PyTorch model to NCNN format, creating: ‚Äò/yolov8n_ncnn_model‚Äô\n\nyolo export model=yolov8n.pt format=ncnn\n\nRun inference with the exported model (now the source could be the bus.jpg image that was downloaded from the website to the current directory on the last inference):\n\nyolo predict model='./yolov8n_ncnn_model' source='bus.jpg'\n\nThe first inference, when the model is loaded, usually has a high latency (around 17s), but from the 2nd, it is possible to note that the inference goes down to around 2s.\n\n\n\n\nTo start, let‚Äôs call the Python Interpreter so we can explore how the YOLO model works, line by line:\npython3\nNow, we should call the YOLO library from Ultralytics and load the model:\nfrom ultralytics import YOLO\n\nmodel = YOLO(\"yolov8n_ncnn_model\")\nNext, run inference over an image (let‚Äôs use again bus.jpg):\nimg = \"bus.jpg\"\nresult = model.predict(img, save=True, imgsz=640, conf=0.5, iou=0.3)\n \nWe can verify that the result is almost identical to the one we get running the inference at the terminal level (CLI), except that the bus stop was not detected with the reduced NCNN model. Note that the latency was reduced.\nLet‚Äôs analyze the ‚Äúresult‚Äù content.\nFor example, we can see result[0].boxes.data, showing us the main inference result, which is a tensor shape (4, 6). Each line is one of the objects detected, being the 4 first columns, the bounding boxes coordinates, the 5th, the confidence, and the 6th, the class (in this case, 0: person and 5: bus):\n \nWe can access several inference results separately, as the inference time, and have it printed in a better format:\ninference_time = int(result[0].speed[\"inference\"])\nprint(f\"Inference Time: {inference_time} ms\")\nOr we can have the total number of objects detected:\nprint(f\"Number of objects: {len (result[0].boxes.cls)}\")\n \nWith Python, we can create a detailed output that meets our needs (See Model Prediction with Ultralytics YOLO for more details). Let‚Äôs run a Python script instead of manually entering it line by line in the interpreter, as shown below. Let‚Äôs use nano as our text editor. First, we should create an empty Python script named, for example, yolov8_tests.py:\nnano yolov8_tests.py\nEnter with the code lines:\nfrom ultralytics import YOLO\n\n# Load the YOLOv8 model\nmodel = YOLO(\"yolov8n_ncnn_model\")\n\n# Run inference\nimg = \"bus.jpg\"\nresult = model.predict(img, save=False, imgsz=640, conf=0.5, iou=0.3)\n\n# print the results\ninference_time = int(result[0].speed[\"inference\"])\nprint(f\"Inference Time: {inference_time} ms\")\nprint(f\"Number of objects: {len (result[0].boxes.cls)}\")\n \nAnd enter with the commands: [CTRL+O] + [ENTER] +[CTRL+X] to save the Python script.\nRun the script:\npython yolov8_tests.py\nThe result is the same as running the inference at the terminal level (CLI) and with the built-in Python interpreter.\n\nCalling the YOLO library and loading the model for inference for the first time takes a long time, but the inferences after that will be much faster. For example, the first single inference can take several seconds, but after that, the inference time should be reduced to less than 1 second.\n\n\n\n\nReturn to our ‚ÄúBox versus Wheel‚Äù dataset, labeled on Roboflow. On the Download Dataset, instead of Download a zip to computer option done for training on Edge Impulse Studio, we will opt for Show download code. This option will open a pop-up window with a code snippet that should be pasted into our training notebook.\n \nFor training, let‚Äôs adapt one of the public examples available from Ultralytics and run it on Google Colab. Below, you can find mine to be adapted in your project:\n\nYOLOv8 Box versus Wheel Dataset Training [Open In Colab]\n\n\n\n\nRun it with GPU (the NVidia T4 is free)\nInstall Ultralytics using PIP.\n \nNow, you can import the YOLO and upload your dataset to the CoLab, pasting the Download code that we get from Roboflow. Note that our dataset will be mounted under /content/datasets/:\n\n \n\nIt is essential to verify and change the file data.yaml with the correct path for the images (copy the path on each images folder).\n\nnames:\n\n- box\n- wheel\nnc: 2\nroboflow:\n  license: CC BY 4.0\n  project: box-versus-wheel-auto-dataset\n  url: https://universe.roboflow.com/marcelo-rovai-riila/ \\\n     box-versus-wheel-auto-dataset/dataset/5\n  version: 5\n  workspace: marcelo-rovai-riila\ntest: /content/datasets/Box-versus-Wheel-auto-dataset-5/ \\\n      test/images\ntrain: /content/datasets/Box-versus-Wheel-auto-dataset-5/ \\\n       train/images\nval: /content/datasets/Box-versus-Wheel-auto-dataset-5/ \\\n     valid/images\n\nDefine the main hyperparameters that you want to change from default, for example:\nMODEL = 'yolov8n.pt'\nIMG_SIZE = 640\nEPOCHS = 25 # For a final project, you should consider\n            # at least 100 epochs\nRun the training (using CLI):\n!yolo task=detect mode=train model={MODEL} \\\n  data={dataset.location}/data.yaml \\\n  epochs={EPOCHS}\n  imgsz={IMG_SIZE} plots=True\n\n\n\n\nimage-20240910111319804\n\n\nThe model took a few minutes to be trained and has an excellent result (mAP50 of 0.995). At the end of the training, all results are saved in the folder listed, for example: /runs/detect/train/. There, you can find, for example, the confusion matrix.\n \n\nNote that the trained model (best.pt) is saved in the folder /runs/detect/train/weights/. Now, you should validate the trained model with the valid/images.\n\n!yolo task=detect mode=val model={HOME}/runs/detect/train/\\\n       weights/best.pt data={dataset.location}/data.yaml\nThe results were similar to training.\n\nNow, we should perform inference on the images left aside for testing\n\n!yolo task=detect mode=predict model={HOME}/runs/detect/train/\\\n     weights/best.pt conf=0.25 source={dataset.location}/test/\\\n     images save=True\nThe inference results are saved in the folder runs/detect/predict. Let‚Äôs see some of them:\n \n\nIt is advised to export the train, validation, and test results for a Drive at Google. To do so, we should mount the drive.\nfrom google.colab import drive\ndrive.mount('/content/gdrive')\nand copy the content of /runs folder to a folder that you should create in your Drive, for example:\n!scp -r /content/runs '/content/gdrive/MyDrive/\\\n     10_UNIFEI/Box_vs_Wheel_Project'\n\n\n\n\n\nDownload the trained model /runs/detect/train/weights/best.pt to your computer. Using the FileZilla FTP, let‚Äôs transfer the best.pt to the Raspi models folder (before the transfer, you may change the model name, for example, box_wheel_320_yolo.pt).\nUsing the FileZilla FTP, let‚Äôs transfer a few images from the test dataset to .\\YOLO\\images:\nLet‚Äôs return to the YOLO folder and use the Python Interpreter:\ncd ..\npython\nAs before, we will import the YOLO library and define our converted model to detect bees:\nfrom ultralytics import YOLO\n\nmodel = YOLO(\"./models/box_wheel_320_yolo.pt\")\nNow, let‚Äôs define an image and call the inference (we will save the image result this time to external verification):\nimg = \"./images/1_box_1_wheel.jpg\"\nresult = model.predict(img, save=True, imgsz=320, conf=0.5, iou=0.3)\nLet‚Äôs repeat for several images. The inference result is saved on the variable result, and the processed image on runs/detect/predict8\n \nUsing FileZilla FTP, we can send the inference result to our Desktop for verification:\n \nWe can see that the inference result is excellent! The model was trained based on the smaller base model of the YOLOv8 family (YOLOv8n). The issue is the latency, around 1 second (or 1 FPS on the Raspi-Zero). Of course, we can reduce this latency and convert the model to TFLite or NCNN.\n\n\n\n\nAll the models explored in this lab can detect objects in real-time using a camera. The captured image should be the input for the trained and converted model. For the Raspi-4 or 5 with a desktop, OpenCV can capture the frames and display the inference result.\nHowever, creating a live stream with a webcam to detect objects in real-time is also possible. For example, let‚Äôs start with the script developed for the Image Classification app and adapt it for a Real-Time Object Detection Web Application Using TensorFlow Lite and Flask.\nThis app version will work for all TFLite models. Verify if the model is in its correct folder, for example:\nmodel_path = \"./models/ssd-mobilenet-v1-tflite-default-v1.tflite\"\nDownload the Python script object_detection_app.py from GitHub.\nAnd on the terminal, run:\npython3 object_detection_app.py\nAnd access the web interface:\n\nOn the Raspberry Pi itself (if you have a GUI): Open a web browser and go to http://localhost:5000\nFrom another device on the same network: Open a web browser and go to http://&lt;raspberry_pi_ip&gt;:5000 (Replace &lt;raspberry_pi_ip&gt; with your Raspberry Pi‚Äôs IP address). For example: http://192.168.4.210: 5000/\n\nHere are some screenshots of the app running on an external desktop\n \nLet‚Äôs see a technical description of the key modules used in the object detection application:\n\nTensorFlow Lite (tflite_runtime):\n\nPurpose: Efficient inference of machine learning models on edge devices.\nWhy: TFLite offers reduced model size and optimized performance compared to full TensorFlow, which is crucial for resource-constrained devices like Raspberry Pi. It supports hardware acceleration and quantization, further improving efficiency.\nKey functions: Interpreter for loading and running the model, get_input_details(), and get_output_details() for interfacing with the model.\n\nFlask:\n\nPurpose: Lightweight web framework for creating the backend server.\nWhy: Flask‚Äôs simplicity and flexibility make it ideal for rapidly developing and deploying web applications. It‚Äôs less resource-intensive than larger frameworks suitable for edge devices.\nKey components: route decorators for defining API endpoints, Response objects for streaming video, render_template_string for serving dynamic HTML.\n\nPicamera2:\n\nPurpose: Interface with the Raspberry Pi camera module.\nWhy: Picamera2 is the latest library for controlling Raspberry Pi cameras, offering improved performance and features over the original Picamera library.\nKey functions: create_preview_configuration() for setting up the camera, capture_file() for capturing frames.\n\nPIL (Python Imaging Library):\n\nPurpose: Image processing and manipulation.\nWhy: PIL provides a wide range of image processing capabilities. It‚Äôs used here to resize images, draw bounding boxes, and convert between image formats.\nKey classes: Image for loading and manipulating images, ImageDraw for drawing shapes and text on images.\n\nNumPy:\n\nPurpose: Efficient array operations and numerical computing.\nWhy: NumPy‚Äôs array operations are much faster than pure Python lists, which is crucial for efficiently processing image data and model inputs/outputs.\nKey functions: array() for creating arrays, expand_dims() for adding dimensions to arrays.\n\nThreading:\n\nPurpose: Concurrent execution of tasks.\nWhy: Threading allows simultaneous frame capture, object detection, and web server operation, crucial for maintaining real-time performance.\nKey components: Thread class creates separate execution threads, and Lock is used for thread synchronization.\n\nio.BytesIO:\n\nPurpose: In-memory binary streams.\nWhy: Allows efficient handling of image data in memory without needing temporary files, improving speed and reducing I/O operations.\n\ntime:\n\nPurpose: Time-related functions.\nWhy: Used for adding delays (time.sleep()) to control frame rate and for performance measurements.\n\njQuery (client-side):\n\nPurpose: Simplified DOM manipulation and AJAX requests.\nWhy: It makes it easy to update the web interface dynamically and communicate with the server without page reloads.\nKey functions: .get() and .post() for AJAX requests, DOM manipulation methods for updating the UI.\n\n\nRegarding the main app system architecture:\n\nMain Thread: Runs the Flask server, handling HTTP requests and serving the web interface.\nCamera Thread: Continuously captures frames from the camera.\nDetection Thread: Processes frames through the TFLite model for object detection.\nFrame Buffer: Shared memory space (protected by locks) storing the latest frame and detection results.\n\nAnd the app data flow, we can describe in short:\n\nCamera captures frame ‚Üí Frame Buffer\nDetection thread reads from Frame Buffer ‚Üí Processes through TFLite model ‚Üí Updates detection results in Frame Buffer\nFlask routes access Frame Buffer to serve the latest frame and detection results\nWeb client receives updates via AJAX and updates UI\n\nThis architecture allows for efficient, real-time object detection while maintaining a responsive web interface running on a resource-constrained edge device like a Raspberry Pi. Threading and efficient libraries like TFLite and PIL enable the system to process video frames in real-time, while Flask and jQuery provide a user-friendly way to interact with them.\nYou can test the app with another pre-processed model, such as the EfficientDet, changing the app line:\nmodel_path = \"./models/lite-model_efficientdet_lite0_\\\n   detection_metadata_1.tflite\"\n\nIf we want to use the app for the SSD-MobileNetV2 model, trained on Edge Impulse Studio with the ‚ÄúBox versus Wheel‚Äù dataset, the code should also be adapted depending on the input details, as we have explored on its notebook.\n\n\n\n\nThis lab has explored the implementation of object detection on edge devices like the Raspberry Pi, demonstrating the power and potential of running advanced computer vision tasks on resource-constrained hardware. We‚Äôve covered several vital aspects:\n\nModel Comparison: We examined different object detection models, including SSD-MobileNet, EfficientDet, FOMO, and YOLO, comparing their performance and trade-offs on edge devices.\nTraining and Deployment: Using a custom dataset of boxes and wheels (labeled on Roboflow), we walked through the process of training models using Edge Impulse Studio and Ultralytics and deploying them on Raspberry Pi.\nOptimization Techniques: To improve inference speed on edge devices, we explored various optimization methods, such as model quantization (TFLite int8) and format conversion (e.g., to NCNN).\nReal-time Applications: The lab exemplified a real-time object detection web application, demonstrating how these models can be integrated into practical, interactive systems.\nPerformance Considerations: Throughout the lab, we discussed the balance between model accuracy and inference speed, a critical consideration for edge AI applications.\n\nThe ability to perform object detection on edge devices opens up numerous possibilities across various domains, from precision agriculture, industrial automation, and quality control to smart home applications and environmental monitoring. By processing data locally, these systems can offer reduced latency, improved privacy, and operation in environments with limited connectivity.\nLooking ahead, potential areas for further exploration include:\n\nImplementing multi-model pipelines for more complex tasks\nExploring hardware acceleration options for Raspberry Pi\nIntegrating object detection with other sensors for more comprehensive edge AI systems\nDeveloping edge-to-cloud solutions that leverage both local processing and cloud resources\n\nObject detection on edge devices can create intelligent, responsive systems that bring the power of AI directly into the physical world, opening up new frontiers in how we interact with and understand our environment.\n\n\n\n\nDataset (‚ÄúBox versus Wheel‚Äù)\nSSD-MobileNet Notebook on a Raspi\nEfficientDet Notebook on a Raspi\nFOMO - EI Linux Notebook on a Raspi\nYOLOv8 Box versus Wheel Dataset Training on CoLab\nEdge Impulse Project - SSD MobileNet and FOMO\nPython Scripts\nModels",
    "crumbs": [
      "Raspberry Pi",
      "Object Detection"
    ]
  },
  {
    "objectID": "contents/raspi/object_detection/object_detection.html#sec-object-detection-overview-1133",
    "href": "contents/raspi/object_detection/object_detection.html#sec-object-detection-overview-1133",
    "title": "Object Detection",
    "section": "",
    "text": "Building upon our exploration of image classification, we now turn our attention to a more advanced computer vision task: object detection. While image classification assigns a single label to an entire image, object detection goes further by identifying and locating multiple objects within a single image. This capability opens up many new applications and challenges, particularly in edge computing and IoT devices like the Raspberry Pi.\nObject detection combines the tasks of classification and localization. It not only determines what objects are present in an image but also pinpoints their locations by, for example, drawing bounding boxes around them. This added complexity makes object detection a more powerful tool for understanding visual scenes, but it also requires more sophisticated models and training techniques.\nIn edge AI, where we work with constrained computational resources, implementing efficient object detection models becomes crucial. The challenges we faced with image classification‚Äîbalancing model size, inference speed, and accuracy‚Äîare amplified in object detection. However, the rewards are also more significant, as object detection enables more nuanced and detailed visual data analysis.\nSome applications of object detection on edge devices include:\n\nSurveillance and security systems\nAutonomous vehicles and drones\nIndustrial quality control\nWildlife monitoring\nAugmented reality applications\n\nAs we put our hands into object detection, we‚Äôll build upon the concepts and techniques we explored in image classification. We‚Äôll examine popular object detection architectures designed for efficiency, such as:\n\nSingle Stage Detectors, such as MobileNet and EfficientDet,\nFOMO (Faster Objects, More Objects), and\nYOLO (You Only Look Once).\n\n\nTo learn more about object detection models, follow the tutorial A Gentle Introduction to Object Recognition With Deep Learning.\n\nWe will explore those object detection models using\n\nTensorFlow Lite Runtime (now changed to LiteRT),\nEdge Impulse Linux Python SDK and\nUltralytics\n\n \nThroughout this lab, we‚Äôll cover the fundamentals of object detection and how it differs from image classification. We‚Äôll also learn how to train, fine-tune, test, optimize, and deploy popular object detection architectures using a dataset created from scratch.\n\n\nObject detection builds upon the foundations of image classification but extends its capabilities significantly. To understand object detection, it‚Äôs crucial first to recognize its key differences from image classification:\n\n\nImage Classification:\n\nAssigns a single label to an entire image\nAnswers the question: ‚ÄúWhat is this image‚Äôs primary object or scene?‚Äù\nOutputs a single class prediction for the whole image\n\nObject Detection:\n\nIdentifies and locates multiple objects within an image\nAnswers the questions: ‚ÄúWhat objects are in this image, and where are they located?‚Äù\nOutputs multiple predictions, each consisting of a class label and a bounding box\n\nTo visualize this difference, let‚Äôs consider an example:\n \nThis diagram illustrates the critical difference: image classification provides a single label for the entire image, while object detection identifies multiple objects, their classes, and their locations within the image.\n\n\n\nObject detection systems typically consist of two main components:\n\nObject Localization: This component identifies where objects are located in the image. It typically outputs bounding boxes, rectangular regions encompassing each detected object.\nObject Classification: This component determines the class or category of each detected object, similar to image classification but applied to each localized region.\n\n\n\n\nObject detection presents several challenges beyond those of image classification:\n\nMultiple objects: An image may contain multiple objects of various classes, sizes, and positions.\nVarying scales: Objects can appear at different sizes within the image.\nOcclusion: Objects may be partially hidden or overlapping.\nBackground clutter: Distinguishing objects from complex backgrounds can be challenging.\nReal-time performance: Many applications require fast inference times, especially on edge devices.\n\n\n\n\nThere are two main approaches to object detection:\n\nTwo-stage detectors: These first propose regions of interest and then classify each region. Examples include R-CNN and its variants (Fast R-CNN, Faster R-CNN).\nSingle-stage detectors: These predict bounding boxes (or centroids) and class probabilities in one forward pass of the network. Examples include YOLO (You Only Look Once), EfficientDet, SSD (Single Shot Detector), and FOMO (Faster Objects, More Objects). These are often faster and more suitable for edge devices like Raspberry Pi.\n\n\n\n\nObject detection uses different metrics compared to image classification:\n\nIntersection over Union (IoU): Measures the overlap between predicted and ground truth bounding boxes.\nMean Average Precision (mAP): Combines precision and recall across all classes and IoU thresholds.\nFrames Per Second (FPS): Measures detection speed, crucial for real-time applications on edge devices.",
    "crumbs": [
      "Raspberry Pi",
      "Object Detection"
    ]
  },
  {
    "objectID": "contents/raspi/object_detection/object_detection.html#sec-object-detection-pretrained-object-detection-models-overview-b4a3",
    "href": "contents/raspi/object_detection/object_detection.html#sec-object-detection-pretrained-object-detection-models-overview-b4a3",
    "title": "Object Detection",
    "section": "",
    "text": "As we saw in the introduction, given an image or a video stream, an object detection model can identify which of a known set of objects might be present and provide information about their positions within the image.\n\nYou can test some common models online by visiting Object Detection - MediaPipe Studio\n\nOn Kaggle, we can find the most common pre-trained tflite models to use with the Raspi, ssd_mobilenet_v1, and EfficientDet. Those models were trained on the COCO (Common Objects in Context) dataset, with over 200,000 labeled images in 91 categories. Go, download the models, and upload them to the ./models folder in the Raspi.\n\nAlternatively, you can find the models and the COCO labels on GitHub.\n\nFor the first part of this lab, we will focus on a pre-trained \\(300\\times 300\\) SSD-Mobilenet V1 model and compare it with the \\(320\\times 320\\) EfficientDet-lite0, also trained using the COCO 2017 dataset. Both models were converted to a TensorFlow Lite format (4.2 MB for the SSD Mobilenet and 4.6 MB for the EfficientDet).\n\nSSD-Mobilenet V2 or V3 is recommended for transfer learning projects, but once the V1 TFLite model is publicly available, we will use it for this overview.\n\n \n\n\nWe should confirm the steps done on the last Hands-On Lab, Image Classification, as follows:\n\nUpdating the Raspberry Pi\nInstalling Required Libraries\nSetting up a Virtual Environment (Optional but Recommended)\n\nsource ~/tflite/bin/activate\n\nInstalling TensorFlow Lite Runtime\nInstalling Additional Python Libraries (inside the environment)\n\n\n\n\nConsidering that we have created the Documents/TFLITE folder in the last Lab, let‚Äôs now create the specific folders for this object detection lab:\ncd Documents/TFLITE/\nmkdir OBJ_DETECT\ncd OBJ_DETECT\nmkdir images\nmkdir models\ncd models\n\n\n\nLet‚Äôs start a new notebook to follow all the steps to detect objects on an image:\nImport the needed libraries:\nimport time\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom PIL import Image\nimport tflite_runtime.interpreter as tflite\nLoad the TFLite model and allocate tensors:\nmodel_path = \"./models/ssd-mobilenet-v1-tflite-default-v1.tflite\"\ninterpreter = tflite.Interpreter(model_path=model_path)\ninterpreter.allocate_tensors()\nGet input and output tensors.\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\nInput details will inform us how the model should be fed with an image. The shape of (1, 300, 300, 3) with a dtype of uint8 tells us that a non-normalized (pixel value range from 0 to 255) image with dimensions \\((300\\times 300\\times 3)\\) should be input one by one (Batch Dimension: 1).\nThe output details include not only the labels (‚Äúclasses‚Äù) and probabilities (‚Äúscores‚Äù) but also the relative window position of the bounding boxes (‚Äúboxes‚Äù) about where the object is located on the image and the number of detected objects (‚Äúnum_detections‚Äù). The output details also tell us that the model can detect a maximum of 10 objects in the image.\n \nSo, for the above example, using the same cat image used with the Image Classification Lab looking for the output, we have a 76% probability of having found an object with a class ID of 16 on an area delimited by a bounding box of [0.028011084, 0.020121813, 0.9886069, 0.802299]. Those four numbers are related to ymin, xmin, ymax and xmax, the box coordinates.\nTaking into consideration that y goes from the top (ymin) to the bottom (ymax) and x goes from left (xmin) to the right (xmax), we have, in fact, the coordinates of the top/left corner and the bottom/right one. With both edges and knowing the shape of the picture, it is possible to draw a rectangle around the object, as shown in the figure below:\n \nNext, we should find what class ID equal to 16 means. Opening the file coco_labels.txt, as a list, each element has an associated index, and inspecting index 16, we get, as expected, cat. The probability is the value returning from the score.\nLet‚Äôs now upload some images with multiple objects on it for testing.\nimg_path = \"./images/cat_dog.jpeg\"\norig_img = Image.open(img_path)\n\n# Display the image\nplt.figure(figsize=(8, 8))\nplt.imshow(orig_img)\nplt.title(\"Original Image\")\nplt.show()\n \nBased on the input details, let‚Äôs pre-process the image, changing its shape and expanding its dimension:\nimg = orig_img.resize(\n    (input_details[0][\"shape\"][1], input_details[0][\"shape\"][2])\n)\ninput_data = np.expand_dims(img, axis=0)\ninput_data.shape, input_data.dtype\nThe new input_data shape is(1, 300, 300, 3) with a dtype of uint8, which is compatible with what the model expects.\nUsing the input_data, let‚Äôs run the interpreter, measure the latency, and get the output:\nstart_time = time.time()\ninterpreter.set_tensor(input_details[0][\"index\"], input_data)\ninterpreter.invoke()\nend_time = time.time()\ninference_time = (\n    end_time - start_time\n) * 1000  # Convert to milliseconds\nprint(\"Inference time: {:.1f}ms\".format(inference_time))\nWith a latency of around 800 ms, we can get 4 distinct outputs:\nboxes = interpreter.get_tensor(output_details[0][\"index\"])[0]\nclasses = interpreter.get_tensor(output_details[1][\"index\"])[0]\nscores = interpreter.get_tensor(output_details[2][\"index\"])[0]\nnum_detections = int(\n    interpreter.get_tensor(output_details[3][\"index\"])[0]\n)\nOn a quick inspection, we can see that the model detected 2 objects with a score over 0.5:\nfor i in range(num_detections):\n    if scores[i] &gt; 0.5:  # Confidence threshold\n        print(f\"Object {i}:\")\n        print(f\"  Bounding Box: {boxes[i]}\")\n        print(f\"  Confidence: {scores[i]}\")\n        print(f\"  Class: {classes[i]}\")\n \nAnd we can also visualize the results:\nplt.figure(figsize=(12, 8))\nplt.imshow(orig_img)\nfor i in range(num_detections):\n    if scores[i] &gt; 0.5:  # Adjust threshold as needed\n        ymin, xmin, ymax, xmax = boxes[i]\n        (left, right, top, bottom) = (\n            xmin * orig_img.width,\n            xmax * orig_img.width,\n            ymin * orig_img.height,\n            ymax * orig_img.height,\n        )\n        rect = plt.Rectangle(\n            (left, top),\n            right - left,\n            bottom - top,\n            fill=False,\n            color=\"red\",\n            linewidth=2,\n        )\n        plt.gca().add_patch(rect)\n        class_id = int(classes[i])\n        class_name = labels[class_id]\n        plt.text(\n            left,\n            top - 10,\n            f\"{class_name}: {scores[i]:.2f}\",\n            color=\"red\",\n            fontsize=12,\n            backgroundcolor=\"white\",\n        )\n \n\n\n\nEfficientDet is not technically an SSD (Single Shot Detector) model, but it shares some similarities and builds upon ideas from SSD and other object detection architectures:\n\nEfficientDet:\n\nDeveloped by Google researchers in 2019\nUses EfficientNet as the backbone network\nEmploys a novel bi-directional feature pyramid network (BiFPN)\nIt uses compound scaling to scale the backbone network and the object detection components efficiently.\n\nSimilarities to SSD:\n\nBoth are single-stage detectors, meaning they perform object localization and classification in a single forward pass.\nBoth use multi-scale feature maps to detect objects at different scales.\n\nKey differences:\n\nBackbone: SSD typically uses VGG or MobileNet, while EfficientDet uses EfficientNet.\nFeature fusion: SSD uses a simple feature pyramid, while EfficientDet uses the more advanced BiFPN.\nScaling method: EfficientDet introduces compound scaling for all components of the network\n\nAdvantages of EfficientDet:\n\nGenerally achieves better accuracy-efficiency trade-offs than SSD and many other object detection models.\nMore flexible scaling allows for a family of models with different size-performance trade-offs.\n\n\nWhile EfficientDet is not an SSD model, it can be seen as an evolution of single-stage detection architectures, incorporating more advanced techniques to improve efficiency and accuracy. When using EfficientDet, we can expect similar output structures to SSD (e.g., bounding boxes and class scores).\n\nOn GitHub, you can find another notebook exploring the EfficientDet model that we did with SSD MobileNet.",
    "crumbs": [
      "Raspberry Pi",
      "Object Detection"
    ]
  },
  {
    "objectID": "contents/raspi/object_detection/object_detection.html#sec-object-detection-object-detection-project-22de",
    "href": "contents/raspi/object_detection/object_detection.html#sec-object-detection-object-detection-project-22de",
    "title": "Object Detection",
    "section": "",
    "text": "Now, we will develop a complete Image Classification project from data collection to training and deployment. As we did with the Image Classification project, the trained and converted model will be used for inference.\nWe will use the same dataset to train 3 models: SSD-MobileNet V2, FOMO, and YOLO.\n\n\nAll Machine Learning projects need to start with a goal. Let‚Äôs assume we are in an industrial facility and must sort and count wheels and special boxes.\n \nIn other words, we should perform a multi-label classification, where each image can have three classes:\n\nBackground (no objects)\nBox\nWheel\n\n\n\n\nOnce we have defined our Machine Learning project goal, the next and most crucial step is collecting the dataset. We can use a phone, the Raspi, or a mix to create the raw dataset (with no labels). Let‚Äôs use the simple web app on our Raspberry Pi to view the QVGA (320 x 240) captured images in a browser.\nFrom GitHub, get the Python script get_img_data.py and open it in the terminal:\npython3 get_img_data.py\nAccess the web interface:\n\nOn the Raspberry Pi itself (if you have a GUI): Open a web browser and go to http://localhost:5000\nFrom another device on the same network: Open a web browser and go to http://&lt;raspberry_pi_ip&gt;:5000 (Replace &lt;raspberry_pi_ip&gt; with your Raspberry Pi‚Äôs IP address). For example: http://192.168.4.210:5000/\n\n \nThe Python script creates a web-based interface for capturing and organizing image datasets using a Raspberry Pi and its camera. It‚Äôs handy for machine learning projects that require labeled image data or not, as in our case here.\nAccess the web interface from a browser, enter a generic label for the images you want to capture, and press Start Capture.\n \n\nNote that the images to be captured will have multiple labels that should be defined later.\n\nUse the live preview to position the camera and click Capture Image to save images under the current label (in this case, box-wheel.\n \nWhen we have enough images, we can press Stop Capture. The captured images are saved on the folder dataset/box-wheel:\n \n\nGet around 60 images. Try to capture different angles, backgrounds, and light conditions. Filezilla can transfer the created raw dataset to your main computer.\n\n\n\n\nThe next step in an Object Detect project is to create a labeled dataset. We should label the raw dataset images, creating bounding boxes around each picture‚Äôs objects (box and wheel). We can use labeling tools like LabelImg, CVAT, Roboflow, or even the Edge Impulse Studio. Once we have explored the Edge Impulse tool in other labs, let‚Äôs use Roboflow here.\n\nWe are using Roboflow (free version) here for two main reasons. 1) We can have auto-labeler, and 2) The annotated dataset is available in several formats and can be used both on Edge Impulse Studio (we will use it for MobileNet V2 and FOMO train) and on CoLab (YOLOv8 train), for example. Having the annotated dataset on Edge Impulse (Free account), it is not possible to use it for training on other platforms.\n\nWe should upload the raw dataset to Roboflow. Create a free account there and start a new project, for example, (‚Äúbox-versus-wheel‚Äù).\n \n\nWe will not enter in deep details about the Roboflow process once many tutorials are available.\n\n\n\nOnce the project is created and the dataset is uploaded, you should make the annotations using the ‚ÄúAuto-Label‚Äù Tool. Note that you can also upload images with only a background, which should be saved w/o any annotations.\n \nOnce all images are annotated, you should split them into training, validation, and testing.\n \n\n\n\nThe last step with the dataset is preprocessing to generate a final version for training. Let‚Äôs resize all images to \\(320\\times 320\\) and generate augmented versions of each image (augmentation) to create new training examples from which our model can learn.\nFor augmentation, we will rotate the images (+/-15o), crop, and vary the brightness and exposure.\n \nAt the end of the process, we will have 153 images.\n \nNow, you should export the annotated dataset in a format that Edge Impulse, Ultralytics, and other frameworks/tools understand, for example, YOLOv8. Let‚Äôs download a zipped version of the dataset to our desktop.\n \nHere, it is possible to review how the dataset was structured\n \nThere are 3 separate folders, one for each split (train/test/valid). For each of them, there are 2 subfolders, images, and labels. The pictures are stored as image_id.jpg and images_id.txt, where ‚Äúimage_id‚Äù is unique for every picture.\nThe labels file format will be class_id bounding box coordinates, where in our case, class_id will be 0 for box and 1 for wheel. The numerical id (o, 1, 2‚Ä¶) will follow the alphabetical order of the class name.\nThe data.yaml file has info about the dataset as the classes‚Äô names (names: ['box', 'wheel']) following the YOLO format.\nAnd that‚Äôs it! We are ready to start training using the Edge Impulse Studio (as we will do in the following step), Ultralytics (as we will when discussing YOLO), or even training from scratch on CoLab (as we did with the Cifar-10 dataset on the Image Classification lab).\n\nThe pre-processed dataset can be found at the Roboflow site.",
    "crumbs": [
      "Raspberry Pi",
      "Object Detection"
    ]
  },
  {
    "objectID": "contents/raspi/object_detection/object_detection.html#sec-object-detection-training-ssd-mobilenet-model-edge-impulse-studio-b43a",
    "href": "contents/raspi/object_detection/object_detection.html#sec-object-detection-training-ssd-mobilenet-model-edge-impulse-studio-b43a",
    "title": "Object Detection",
    "section": "",
    "text": "Go to Edge Impulse Studio, enter your credentials at Login (or create an account), and start a new project.\n\nHere, you can clone the project developed for this hands-on lab: Raspi - Object Detection.\n\nOn the Project Dashboard tab, go down and on Project info, and for Labeling method select Bounding boxes (object detection)\n\n\nOn Studio, go to the Data acquisition tab, and on the UPLOAD DATA section, upload from your computer the raw dataset.\nWe can use the option Select a folder, choosing, for example, the folder train in your computer, which contains two sub-folders, images, and labels. Select the Image label format, ‚ÄúYOLO TXT‚Äù, upload into the category Training, and press Upload data.\n \nRepeat the process for the test data (upload both folders, test, and validation). At the end of the upload process, you should end with the annotated dataset of 153 images split in the train/test (84%/16%).\n\nNote that labels will be stored at the labels files 0 and 1 , which are equivalent to box and wheel.\n\n \n\n\n\nThe first thing to define when we enter the Create impulse step is to describe the target device for deployment. A pop-up window will appear. We will select Raspberry 4, an intermediary device between the Raspi-Zero and the Raspi-5.\n\nThis choice will not interfere with the training; it will only give us an idea about the latency of the model on that specific target.\n\n \nIn this phase, you should define how to:\n\nPre-processing consists of resizing the individual images. In our case, the images were pre-processed on Roboflow, to 320x320 , so let‚Äôs keep it. The resize will not matter here because the images are already squared. If you upload a rectangular image, squash it (squared form, without cropping). Afterward, you could define if the images are converted from RGB to Grayscale or not.\nDesign a Model, in this case, ‚ÄúObject Detection.‚Äù\n\n \n\n\n\nIn the section Image, select Color depth as RGB, and press Save parameters.\n \nThe Studio moves automatically to the next section, Generate features, where all samples will be pre-processed, resulting in 480 objects: 207 boxes and 273 wheels.\n \nThe feature explorer shows that all samples evidence a good separation after the feature generation.\n\n\n\nFor training, we should select a pre-trained model. Let‚Äôs use the MobileNetV2 SSD FPN-Lite (320x320 only) . It is a pre-trained object detection model designed to locate up to 10 objects within an image, outputting a bounding box for each object detected. The model is around 3.7 MB in size. It supports an RGB input at \\(320\\times 320\\) px.\nRegarding the training hyper-parameters, the model will be trained with:\n\nEpochs: 25\nBatch size: 32\nLearning Rate: 0.15.\n\nFor validation during training, 20% of the dataset (validation_dataset) will be spared.\n \nAs a result, the model ends with an overall precision score (based on COCO mAP) of 88.8%, higher than the result when using the test data (83.3%).\n\n\n\nWe have two ways to deploy our model:\n\nTFLite model, which lets deploy the trained model as .tflite for the Raspi to run it using Python.\nLinux (AARCH64), a binary for Linux (AARCH64), implements the Edge Impulse Linux protocol, which lets us run our models on any Linux-based development board, with SDKs for Python, for example. See the documentation for more information and setup instructions.\n\nLet‚Äôs deploy the TFLite model. On the Dashboard tab, go to Transfer learning model (int8 quantized) and click on the download icon:\n \nTransfer the model from your computer to the Raspi folder./models and capture or get some images for inference and save them in the folder ./images.\n\n\n\nThe inference can be made as discussed in the Pre-Trained Object Detection Models Overview. Let‚Äôs start a new notebook to follow all the steps to detect cubes and wheels on an image.\nImport the needed libraries:\nimport time\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as patches\nfrom PIL import Image\nimport tflite_runtime.interpreter as tflite\nDefine the model path and labels:\nmodel_path = \"./models/ei-raspi-object-detection-SSD-\\\n              MobileNetv2-320x0320-int8.lite\"\nlabels = [\"box\", \"wheel\"]\n\nRemember that the model will output the class ID as values (0 and 1), following an alphabetic order regarding the class names.\n\nLoad the model, allocate the tensors, and get the input and output tensor details:\n# Load the TFLite model\ninterpreter = tflite.Interpreter(model_path=model_path)\ninterpreter.allocate_tensors()\n\n# Get input and output tensors\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\nOne crucial difference to note is that the dtype of the input details of the model is now int8, which means that the input values go from ‚Äì128 to +127, while each pixel of our raw image goes from 0 to 256. This means that we should pre-process the image to match it. We can check here:\ninput_dtype = input_details[0][\"dtype\"]\ninput_dtype\nnumpy.int8\nSo, let‚Äôs open the image and show it:\n# Load the image\nimg_path = \"./images/box_2_wheel_2.jpg\"\norig_img = Image.open(img_path)\n\n# Display the image\nplt.figure(figsize=(6, 6))\nplt.imshow(orig_img)\nplt.title(\"Original Image\")\nplt.show()\n \nAnd perform the pre-processing:\nscale, zero_point = input_details[0][\"quantization\"]\nimg = orig_img.resize(\n    (input_details[0][\"shape\"][1], input_details[0][\"shape\"][2])\n)\nimg_array = np.array(img, dtype=np.float32) / 255.0\nimg_array = (\n    (img_array / scale + zero_point).clip(-128, 127).astype(np.int8)\n)\ninput_data = np.expand_dims(img_array, axis=0)\nChecking the input data, we can verify that the input tensor is compatible with what is expected by the model:\ninput_data.shape, input_data.dtype\n((1, 320, 320, 3), dtype('int8'))\nNow, it is time to perform the inference. Let‚Äôs also calculate the latency of the model:\n# Inference on Raspi-Zero\nstart_time = time.time()\ninterpreter.set_tensor(input_details[0][\"index\"], input_data)\ninterpreter.invoke()\nend_time = time.time()\ninference_time = (\n    end_time - start_time\n) * 1000  # Convert to milliseconds\nprint(\"Inference time: {:.1f}ms\".format(inference_time))\nThe model will take around 600ms to perform the inference in the Raspi-Zero, which is around 5 times longer than a Raspi-5.\nNow, we can get the output classes of objects detected, its bounding boxes coordinates, and probabilities.\nboxes = interpreter.get_tensor(output_details[1][\"index\"])[0]\nclasses = interpreter.get_tensor(output_details[3][\"index\"])[0]\nscores = interpreter.get_tensor(output_details[0][\"index\"])[0]\nnum_detections = int(\n    interpreter.get_tensor(output_details[2][\"index\"])[0]\n)\nfor i in range(num_detections):\n    if scores[i] &gt; 0.5:  # Confidence threshold\n        print(f\"Object {i}:\")\n        print(f\"  Bounding Box: {boxes[i]}\")\n        print(f\"  Confidence: {scores[i]}\")\n        print(f\"  Class: {classes[i]}\")\n \nFrom the results, we can see that 4 objects were detected: two with class ID 0 (box)and two with class ID 1 (wheel), what is correct!\nLet‚Äôs visualize the result for a threshold of 0.5\nthreshold = 0.5\nplt.figure(figsize=(6, 6))\nplt.imshow(orig_img)\nfor i in range(num_detections):\n    if scores[i] &gt; threshold:\n        ymin, xmin, ymax, xmax = boxes[i]\n        (left, right, top, bottom) = (\n            xmin * orig_img.width,\n            xmax * orig_img.width,\n            ymin * orig_img.height,\n            ymax * orig_img.height,\n        )\n        rect = plt.Rectangle(\n            (left, top),\n            right - left,\n            bottom - top,\n            fill=False,\n            color=\"red\",\n            linewidth=2,\n        )\n        plt.gca().add_patch(rect)\n        class_id = int(classes[i])\n        class_name = labels[class_id]\n        plt.text(\n            left,\n            top - 10,\n            f\"{class_name}: {scores[i]:.2f}\",\n            color=\"red\",\n            fontsize=12,\n            backgroundcolor=\"white\",\n        )\n \nBut what happens if we reduce the threshold to 0.3, for example?\n \nWe start to see false positives and multiple detections, where the model detects the same object multiple times with different confidence levels and slightly different bounding boxes.\nCommonly, sometimes, we need to adjust the threshold to smaller values to capture all objects, avoiding false negatives, which would lead to multiple detections.\nTo improve the detection results, we should implement Non-Maximum Suppression (NMS), which helps eliminate overlapping bounding boxes and keeps only the most confident detection.\nFor that, let‚Äôs create a general function named non_max_suppression(), with the role of refining object detection results by eliminating redundant and overlapping bounding boxes. It achieves this by iteratively selecting the detection with the highest confidence score and removing other significantly overlapping detections based on an Intersection over Union (IoU) threshold.\ndef non_max_suppression(boxes, scores, threshold):\n    # Convert to corner coordinates\n    x1 = boxes[:, 0]\n    y1 = boxes[:, 1]\n    x2 = boxes[:, 2]\n    y2 = boxes[:, 3]\n\n    areas = (x2 - x1 + 1) * (y2 - y1 + 1)\n    order = scores.argsort()[::-1]\n\n    keep = []\n    while order.size &gt; 0:\n        i = order[0]\n        keep.append(i)\n        xx1 = np.maximum(x1[i], x1[order[1:]])\n        yy1 = np.maximum(y1[i], y1[order[1:]])\n        xx2 = np.minimum(x2[i], x2[order[1:]])\n        yy2 = np.minimum(y2[i], y2[order[1:]])\n\n        w = np.maximum(0.0, xx2 - xx1 + 1)\n        h = np.maximum(0.0, yy2 - yy1 + 1)\n        inter = w * h\n        ovr = inter / (areas[i] + areas[order[1:]] - inter)\n\n        inds = np.where(ovr &lt;= threshold)[0]\n        order = order[inds + 1]\n\n    return keep\nHow it works:\n\nSorting: It starts by sorting all detections by their confidence scores, highest to lowest.\nSelection: It selects the highest-scoring box and adds it to the final list of detections.\nComparison: This selected box is compared with all remaining lower-scoring boxes.\nElimination: Any box that overlaps significantly (above the IoU threshold) with the selected box is eliminated.\nIteration: This process repeats with the next highest-scoring box until all boxes are processed.\n\nNow, we can define a more precise visualization function that will take into consideration an IoU threshold, detecting only the objects that were selected by the non_max_suppression function:\ndef visualize_detections(\n    image, boxes, classes, scores, labels, threshold, iou_threshold\n):\n    if isinstance(image, Image.Image):\n        image_np = np.array(image)\n    else:\n        image_np = image\n    height, width = image_np.shape[:2]\n    # Convert normalized coordinates to pixel coordinates\n    boxes_pixel = boxes * np.array([height, width, height, width])\n    # Apply NMS\n    keep = non_max_suppression(boxes_pixel, scores, iou_threshold)\n    # Set the figure size to 12x8 inches\n    fig, ax = plt.subplots(1, figsize=(12, 8))\n    ax.imshow(image_np)\n    for i in keep:\n        if scores[i] &gt; threshold:\n            ymin, xmin, ymax, xmax = boxes[i]\n            rect = patches.Rectangle(\n                (xmin * width, ymin * height),\n                (xmax - xmin) * width,\n                (ymax - ymin) * height,\n                linewidth=2,\n                edgecolor=\"r\",\n                facecolor=\"none\",\n            )\n\n            ax.add_patch(rect)\n            class_name = labels[int(classes[i])]\n            ax.text(\n                xmin * width,\n                ymin * height - 10,\n                f\"{class_name}: {scores[i]:.2f}\",\n                color=\"red\",\n                fontsize=12,\n                backgroundcolor=\"white\",\n            )\n    plt.show()\nNow we can create a function that will call the others, performing inference on any image:\ndef detect_objects(img_path, conf=0.5, iou=0.5):\n    orig_img = Image.open(img_path)\n    scale, zero_point = input_details[0][\"quantization\"]\n    img = orig_img.resize(\n        (input_details[0][\"shape\"][1], input_details[0][\"shape\"][2])\n    )\n    img_array = np.array(img, dtype=np.float32) / 255.0\n    img_array = (\n        (img_array / scale + zero_point)\n        .clip(-128, 127)\n        .astype(np.int8)\n    )\n    input_data = np.expand_dims(img_array, axis=0)\n\n    # Inference on Raspi-Zero\n    start_time = time.time()\n    interpreter.set_tensor(input_details[0][\"index\"], input_data)\n    interpreter.invoke()\n    end_time = time.time()\n    inference_time = (\n        end_time - start_time\n    ) * 1000  # Convert to milliseconds\n\n    print(\"Inference time: {:.1f}ms\".format(inference_time))\n\n    # Extract the outputs\n    boxes = interpreter.get_tensor(output_details[1][\"index\"])[0]\n    classes = interpreter.get_tensor(output_details[3][\"index\"])[0]\n    scores = interpreter.get_tensor(output_details[0][\"index\"])[0]\n    num_detections = int(\n        interpreter.get_tensor(output_details[2][\"index\"])[0]\n    )\n\n    visualize_detections(\n        orig_img,\n        boxes,\n        classes,\n        scores,\n        labels,\n        threshold=conf,\n        iou_threshold=iou,\n    )\nNow, running the code, having the same image again with a confidence threshold of 0.3, but with a small IoU:\nimg_path = \"./images/box_2_wheel_2.jpg\"\ndetect_objects(img_path, conf=0.3, iou=0.05)",
    "crumbs": [
      "Raspberry Pi",
      "Object Detection"
    ]
  },
  {
    "objectID": "contents/raspi/object_detection/object_detection.html#sec-object-detection-training-fomo-model-edge-impulse-studio-d41c",
    "href": "contents/raspi/object_detection/object_detection.html#sec-object-detection-training-fomo-model-edge-impulse-studio-d41c",
    "title": "Object Detection",
    "section": "",
    "text": "The inference with the SSD MobileNet model worked well, but the latency was significantly high. The inference varied from 0.5 to 1.3 seconds on a Raspi-Zero, which means around or less than 1 FPS (1 frame per second). One alternative to speed up the process is to use FOMO (Faster Objects, More Objects).\nThis novel machine learning algorithm lets us count multiple objects and find their location in an image in real-time using up to \\(30\\times\\) less processing power and memory than MobileNet SSD or YOLO. The main reason this is possible is that while other models calculate the object‚Äôs size by drawing a square around it (bounding box), FOMO ignores the size of the image, providing only the information about where the object is located in the image through its centroid coordinates.\n\n\nIn a typical object detection pipeline, the first stage is extracting features from the input image. FOMO leverages MobileNetV2 to perform this task. MobileNetV2 processes the input image to produce a feature map that captures essential characteristics, such as textures, shapes, and object edges, in a computationally efficient way.\n \nOnce these features are extracted, FOMO‚Äôs simpler architecture, focused on center-point detection, interprets the feature map to determine where objects are located in the image. The output is a grid of cells, where each cell represents whether or not an object center is detected. The model outputs one or more confidence scores for each cell, indicating the likelihood of an object being present.\nLet‚Äôs see how it works on an image.\nFOMO divides the image into blocks of pixels using a factor of 8. For the input of \\(96\\times 96\\), the grid would be \\(12\\times 12\\) \\((96/8=12)\\). For a \\(160\\times 160\\), the grid will be \\(20\\times 20\\), and so on. Next, FOMO will run a classifier through each pixel block to calculate the probability that there is a box or a wheel in each of them and, subsequently, determine the regions that have the highest probability of containing the object (If a pixel block has no objects, it will be classified as background). From the overlap of the final region, the FOMO provides the coordinates (related to the image dimensions) of the centroid of this region.\n \nTrade-off Between Speed and Precision:\n\nGrid Resolution: FOMO uses a grid of fixed resolution, meaning each cell can detect if an object is present in that part of the image. While it doesn‚Äôt provide high localization accuracy, it makes a trade-off by being fast and computationally light, which is crucial for edge devices.\nMulti-Object Detection: Since each cell is independent, FOMO can detect multiple objects simultaneously in an image by identifying multiple centers.\n\n\n\n\nReturn to Edge Impulse Studio, and in the Experiments tab, create another impulse. Now, the input images should be \\(160\\times 160\\) (this is the expected input size for MobilenetV2).\n \nOn the Image tab, generate the features and go to the Object detection tab.\nWe should select a pre-trained model for training. Let‚Äôs use the FOMO (Faster Objects, More Objects) MobileNetV2 0.35.\n \nRegarding the training hyper-parameters, the model will be trained with:\n\nEpochs: 30\nBatch size: 32\nLearning Rate: 0.001.\n\nFor validation during training, 20% of the dataset (validation_dataset) will be spared. We will not apply Data Augmentation for the remaining 80% (train_dataset) because our dataset was already augmented during the labeling phase at Roboflow.\nAs a result, the model ends with an overall F1 score of 93.3% with an impressive latency of 8 ms (Raspi-4), around \\(60\\times\\) less than we got with the SSD MovileNetV2.\n \n\nNote that FOMO automatically added a third label background to the two previously defined boxes (0) and wheels (1).\n\nOn the Model testing tab, we can see that the accuracy was 94%. Here is one of the test sample results:\n \n\nIn object detection tasks, accuracy is generally not the primary evaluation metric. Object detection involves classifying objects and providing bounding boxes around them, making it a more complex problem than simple classification. The issue is that we do not have the bounding box, only the centroids. In short, using accuracy as a metric could be misleading and may not provide a complete understanding of how well the model is performing.\n\n\n\n\nAs we did in the previous section, we can deploy the trained model as TFLite or Linux (AARCH64). Let‚Äôs do it now as Linux (AARCH64), a binary that implements the Edge Impulse Linux protocol.\nEdge Impulse for Linux models is delivered in .eim format. This executable contains our ‚Äúfull impulse‚Äù created in Edge Impulse Studio. The impulse consists of the signal processing block(s) and any learning and anomaly block(s) we added and trained. It is compiled with optimizations for our processor or GPU (e.g., NEON instructions on ARM cores), plus a straightforward IPC layer (over a Unix socket).\nAt the Deploy tab, select the option Linux (AARCH64), the int8model and press Build.\n \nThe model will be automatically downloaded to your computer.\nOn our Raspi, let‚Äôs create a new working area:\ncd ~\ncd Documents\nmkdir EI_Linux\ncd EI_Linux\nmkdir models\nmkdir images\nRename the model for easy identification:\nFor example, raspi-object-detection-linux-aarch64-FOMO-int8.eim and transfer it to the new Raspi folder./models and capture or get some images for inference and save them in the folder ./images.\n\n\n\nThe inference will be made using the Linux Python SDK. This library lets us run machine learning models and collect sensor data on Linux machines using Python. The SDK is open source and hosted on GitHub: edgeimpulse/linux-sdk-python.\nLet‚Äôs set up a Virtual Environment for working with the Linux Python SDK\npython3 -m venv ~/eilinux\nsource ~/eilinux/bin/activate\nAnd Install the all the libraries needed:\nsudo apt-get update\nsudo apt-get install libatlas-base-dev\\\n                     libportaudio0 libportaudio2\nsudo apt-get installlibportaudiocpp0 portaudio19-dev\n\npip3 install edge_impulse_linux -i https://pypi.python.org/simple\npip3 install Pillow matplotlib pyaudio opencv-contrib-python\n\nsudo apt-get install portaudio19-dev\npip3 install pyaudio\npip3 install opencv-contrib-python\nPermit our model to be executable.\nchmod +x raspi-object-detection-linux-aarch64-FOMO-int8.eim\nInstall the Jupiter Notebook on the new environment\npip3 install jupyter\nRun a notebook locally (on the Raspi-4 or 5 with desktop)\njupyter notebook\nor on the browser on your computer:\njupyter notebook --ip=192.168.4.210 --no-browser\nLet‚Äôs start a new notebook by following all the steps to detect cubes and wheels on an image using the FOMO model and the Edge Impulse Linux Python SDK.\nImport the needed libraries:\nimport sys, time\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as patches\nfrom PIL import Image\nimport cv2\nfrom edge_impulse_linux.image import ImageImpulseRunner\nDefine the model path and labels:\nmodel_file = \"raspi-object-detection-linux-aarch64-int8.eim\"\nmodel_path = \"models/\" + model_file  # Trained ML model from\n# Edge Impulse\nlabels = [\"box\", \"wheel\"]\n\nRemember that the model will output the class ID as values (0 and 1), following an alphabetic order regarding the class names.\n\nLoad and initialize the model:\n# Load the model file\nrunner = ImageImpulseRunner(model_path)\n\n# Initialize model\nmodel_info = runner.init()\nThe model_info will contain critical information about our model. However, unlike the TFLite interpreter, the EI Linux Python SDK library will now prepare the model for inference.\nSo, let‚Äôs open the image and show it (Now, for compatibility, we will use OpenCV, the CV Library used internally by EI. OpenCV reads the image as BGR, so we will need to convert it to RGB :\n# Load the image\nimg_path = \"./images/1_box_1_wheel.jpg\"\norig_img = cv2.imread(img_path)\nimg_rgb = cv2.cvtColor(orig_img, cv2.COLOR_BGR2RGB)\n\n# Display the image\nplt.imshow(img_rgb)\nplt.title(\"Original Image\")\nplt.show()\n \nNow we will get the features and the preprocessed image (cropped) using the runner:\nfeatures, cropped = (\n    runner.get_features_from_image_auto_studio_settings(img_rgb)\n)\nAnd perform the inference. Let‚Äôs also calculate the latency of the model:\nres = runner.classify(features)\nLet‚Äôs get the output classes of objects detected, their bounding boxes centroids, and probabilities.\nprint(\n    \"Found %d bounding boxes (%d ms.)\"\n    % (\n        len(res[\"result\"][\"bounding_boxes\"]),\n        res[\"timing\"][\"dsp\"] + res[\"timing\"][\"classification\"],\n    )\n)\nfor bb in res[\"result\"][\"bounding_boxes\"]:\n    print(\n        \"\\t%s (%.2f): x=%d y=%d w=%d h=%d\"\n        % (\n            bb[\"label\"],\n            bb[\"value\"],\n            bb[\"x\"],\n            bb[\"y\"],\n            bb[\"width\"],\n            bb[\"height\"],\n        )\n    )\nFound 2 bounding boxes (29 ms.)\n    1 (0.91): x=112 y=40 w=16 h=16\n    0 (0.75): x=48 y=56 w=8 h=8\nThe results show that two objects were detected: one with class ID 0 (box) and one with class ID 1 (wheel), which is correct!\nLet‚Äôs visualize the result (The threshold is 0.5, the default value set during the model testing on the Edge Impulse Studio).\nprint(\n    \"\\tFound %d bounding boxes (latency: %d ms)\"\n    % (\n        len(res[\"result\"][\"bounding_boxes\"]),\n        res[\"timing\"][\"dsp\"] + res[\"timing\"][\"classification\"],\n    )\n)\nplt.figure(figsize=(5, 5))\nplt.imshow(cropped)\n\n# Go through each of the returned bounding boxes\nbboxes = res[\"result\"][\"bounding_boxes\"]\nfor bbox in bboxes:\n\n    # Get the corners of the bounding box\n    left = bbox[\"x\"]\n    top = bbox[\"y\"]\n    width = bbox[\"width\"]\n    height = bbox[\"height\"]\n\n    # Draw a circle centered on the detection\n    circ = plt.Circle(\n        (left + width // 2, top + height // 2),\n        5,\n        fill=False,\n        color=\"red\",\n        linewidth=3,\n    )\n    plt.gca().add_patch(circ)\n    class_id = int(bbox[\"label\"])\n    class_name = labels[class_id]\n    plt.text(\n        left,\n        top - 10,\n        f'{class_name}: {bbox[\"value\"]:.2f}',\n        color=\"red\",\n        fontsize=12,\n        backgroundcolor=\"white\",\n    )\nplt.show()",
    "crumbs": [
      "Raspberry Pi",
      "Object Detection"
    ]
  },
  {
    "objectID": "contents/raspi/object_detection/object_detection.html#sec-object-detection-exploring-yolo-model-using-ultralitics-943a",
    "href": "contents/raspi/object_detection/object_detection.html#sec-object-detection-exploring-yolo-model-using-ultralitics-943a",
    "title": "Object Detection",
    "section": "",
    "text": "For this lab, we will explore YOLOv8. Ultralytics YOLOv8 is a version of the acclaimed real-time object detection and image segmentation model, YOLO. YOLOv8 is built on cutting-edge advancements in deep learning and computer vision, offering unparalleled performance in terms of speed and accuracy. Its streamlined design makes it suitable for various applications and easily adaptable to different hardware platforms, from edge devices to cloud APIs.\n\n\nThe YOLO (You Only Look Once) model is a highly efficient and widely used object detection algorithm known for its real-time processing capabilities. Unlike traditional object detection systems that repurpose classifiers or localizers to perform detection, YOLO frames the detection problem as a single regression task. This innovative approach enables YOLO to simultaneously predict multiple bounding boxes and their class probabilities from full images in one evaluation, significantly boosting its speed.\n\n\n\nSingle Network Architecture:\n\nYOLO employs a single neural network to process the entire image. This network divides the image into a grid and, for each grid cell, directly predicts bounding boxes and associated class probabilities. This end-to-end training improves speed and simplifies the model architecture.\n\nReal-Time Processing:\n\nOne of YOLO‚Äôs standout features is its ability to perform object detection in real-time. Depending on the version and hardware, YOLO can process images at high frames per second (FPS). This makes it ideal for applications requiring quick and accurate object detection, such as video surveillance, autonomous driving, and live sports analysis.\n\nEvolution of Versions:\n\nOver the years, YOLO has undergone significant improvements, from YOLOv1 to the latest YOLOv10. Each iteration has introduced enhancements in accuracy, speed, and efficiency. YOLOv8, for instance, incorporates advancements in network architecture, improved training methodologies, and better support for various hardware, ensuring a more robust performance.\nAlthough YOLOv10 is the family‚Äôs newest member with an encouraging performance based on its paper, it was just released (May 2024) and is not fully integrated with the Ultralytics library. Conversely, the precision-recall curve analysis suggests that YOLOv8 generally outperforms YOLOv9, capturing a higher proportion of true positives while minimizing false positives more effectively (for more details, see this article). So, this lab is based on the YOLOv8n.\n\n \nAccuracy and Efficiency:\n\nWhile early versions of YOLO traded off some accuracy for speed, recent versions have made substantial strides in balancing both. The newer models are faster and more accurate, detecting small objects (such as bees) and performing well on complex datasets.\n\nWide Range of Applications:\n\nYOLO‚Äôs versatility has led to its adoption in numerous fields. It is used in traffic monitoring systems to detect and count vehicles, security applications to identify potential threats and agricultural technology to monitor crops and livestock. Its application extends to any domain requiring efficient and accurate object detection.\n\nCommunity and Development:\n\nYOLO continues to evolve and is supported by a strong community of developers and researchers (being the YOLOv8 very strong). Open-source implementations and extensive documentation have made it accessible for customization and integration into various projects. Popular deep learning frameworks like Darknet, TensorFlow, and PyTorch support YOLO, further broadening its applicability.\nUltralytics YOLOv8 can not only Detect (our case here) but also Segment and Pose models pre-trained on the COCO dataset and YOLOv8 Classify models pre-trained on the ImageNet dataset. Track mode is available for all Detect, Segment, and Pose models.\n\n\n\n\nUltralytics YOLO supported tasks\n\n\n\n\n\n\n\nOn our Raspi, let‚Äôs deactivate the current environment to create a new working area:\ndeactivate\ncd ~\ncd Documents/\nmkdir YOLO\ncd YOLO\nmkdir models\nmkdir images\nLet‚Äôs set up a Virtual Environment for working with the Ultralytics YOLOv8\npython3 -m venv ~/yolo\nsource ~/yolo/bin/activate\nAnd install the Ultralytics packages for local inference on the Raspi\n\nUpdate the packages list, install pip, and upgrade to the latest:\n\nsudo apt update\nsudo apt install python3-pip -y\npip install -U pip\n\nInstall the ultralytics pip package with optional dependencies:\n\npip install ultralytics[export]\n\nReboot the device:\n\nsudo reboot\n\n\n\nAfter the Raspi-Zero booting, let‚Äôs activate the yolo env, go to the working directory,\nsource ~/yolo/bin/activate\ncd /Documents/YOLO\n and run inference on an image that will be downloaded from the Ultralytics website, using the YOLOV8n model (the smallest in the family) at the Terminal (CLI):\nyolo predict model='yolov8n' \\\n     source='https://ultralytics.com/images/bus.jpg'\n\nThe YOLO model family is pre-trained with the COCO dataset.\n\nThe inference result will appear in the terminal. In the image (bus.jpg), 4 persons, 1 bus, and 1 stop signal were detected:\n \nAlso, we got a message that Results saved to runs/detect/predict. Inspecting that directory, we can see a new image saved (bus.jpg). Let‚Äôs download it from the Raspi-Zero to our desktop for inspection:\n \nSo, the Ultralytics YOLO is correctly installed on our Raspi. But, on the Raspi-Zero, an issue is the high latency for this inference, around 18 seconds, even with the most miniature model of the family (YOLOv8n).\n\n\n\nDeploying computer vision models on edge devices with limited computational power, such as the Raspi-Zero, can cause latency issues. One alternative is to use a format optimized for optimal performance. This ensures that even devices with limited processing power can handle advanced computer vision tasks well.\nOf all the model export formats supported by Ultralytics, the NCNN is a high-performance neural network inference computing framework optimized for mobile platforms. From the beginning of the design, NCNN was deeply considerate about deployment and use on mobile phones and did not have third-party dependencies. It is cross-platform and runs faster than all known open-source frameworks (such as TFLite).\nNCNN delivers the best inference performance when working with Raspberry Pi devices. NCNN is highly optimized for mobile embedded platforms (such as ARM architecture).\nSo, let‚Äôs convert our model and rerun the inference:\n\nExport a YOLOv8n PyTorch model to NCNN format, creating: ‚Äò/yolov8n_ncnn_model‚Äô\n\nyolo export model=yolov8n.pt format=ncnn\n\nRun inference with the exported model (now the source could be the bus.jpg image that was downloaded from the website to the current directory on the last inference):\n\nyolo predict model='./yolov8n_ncnn_model' source='bus.jpg'\n\nThe first inference, when the model is loaded, usually has a high latency (around 17s), but from the 2nd, it is possible to note that the inference goes down to around 2s.\n\n\n\n\nTo start, let‚Äôs call the Python Interpreter so we can explore how the YOLO model works, line by line:\npython3\nNow, we should call the YOLO library from Ultralytics and load the model:\nfrom ultralytics import YOLO\n\nmodel = YOLO(\"yolov8n_ncnn_model\")\nNext, run inference over an image (let‚Äôs use again bus.jpg):\nimg = \"bus.jpg\"\nresult = model.predict(img, save=True, imgsz=640, conf=0.5, iou=0.3)\n \nWe can verify that the result is almost identical to the one we get running the inference at the terminal level (CLI), except that the bus stop was not detected with the reduced NCNN model. Note that the latency was reduced.\nLet‚Äôs analyze the ‚Äúresult‚Äù content.\nFor example, we can see result[0].boxes.data, showing us the main inference result, which is a tensor shape (4, 6). Each line is one of the objects detected, being the 4 first columns, the bounding boxes coordinates, the 5th, the confidence, and the 6th, the class (in this case, 0: person and 5: bus):\n \nWe can access several inference results separately, as the inference time, and have it printed in a better format:\ninference_time = int(result[0].speed[\"inference\"])\nprint(f\"Inference Time: {inference_time} ms\")\nOr we can have the total number of objects detected:\nprint(f\"Number of objects: {len (result[0].boxes.cls)}\")\n \nWith Python, we can create a detailed output that meets our needs (See Model Prediction with Ultralytics YOLO for more details). Let‚Äôs run a Python script instead of manually entering it line by line in the interpreter, as shown below. Let‚Äôs use nano as our text editor. First, we should create an empty Python script named, for example, yolov8_tests.py:\nnano yolov8_tests.py\nEnter with the code lines:\nfrom ultralytics import YOLO\n\n# Load the YOLOv8 model\nmodel = YOLO(\"yolov8n_ncnn_model\")\n\n# Run inference\nimg = \"bus.jpg\"\nresult = model.predict(img, save=False, imgsz=640, conf=0.5, iou=0.3)\n\n# print the results\ninference_time = int(result[0].speed[\"inference\"])\nprint(f\"Inference Time: {inference_time} ms\")\nprint(f\"Number of objects: {len (result[0].boxes.cls)}\")\n \nAnd enter with the commands: [CTRL+O] + [ENTER] +[CTRL+X] to save the Python script.\nRun the script:\npython yolov8_tests.py\nThe result is the same as running the inference at the terminal level (CLI) and with the built-in Python interpreter.\n\nCalling the YOLO library and loading the model for inference for the first time takes a long time, but the inferences after that will be much faster. For example, the first single inference can take several seconds, but after that, the inference time should be reduced to less than 1 second.\n\n\n\n\nReturn to our ‚ÄúBox versus Wheel‚Äù dataset, labeled on Roboflow. On the Download Dataset, instead of Download a zip to computer option done for training on Edge Impulse Studio, we will opt for Show download code. This option will open a pop-up window with a code snippet that should be pasted into our training notebook.\n \nFor training, let‚Äôs adapt one of the public examples available from Ultralytics and run it on Google Colab. Below, you can find mine to be adapted in your project:\n\nYOLOv8 Box versus Wheel Dataset Training [Open In Colab]\n\n\n\n\nRun it with GPU (the NVidia T4 is free)\nInstall Ultralytics using PIP.\n \nNow, you can import the YOLO and upload your dataset to the CoLab, pasting the Download code that we get from Roboflow. Note that our dataset will be mounted under /content/datasets/:\n\n \n\nIt is essential to verify and change the file data.yaml with the correct path for the images (copy the path on each images folder).\n\nnames:\n\n- box\n- wheel\nnc: 2\nroboflow:\n  license: CC BY 4.0\n  project: box-versus-wheel-auto-dataset\n  url: https://universe.roboflow.com/marcelo-rovai-riila/ \\\n     box-versus-wheel-auto-dataset/dataset/5\n  version: 5\n  workspace: marcelo-rovai-riila\ntest: /content/datasets/Box-versus-Wheel-auto-dataset-5/ \\\n      test/images\ntrain: /content/datasets/Box-versus-Wheel-auto-dataset-5/ \\\n       train/images\nval: /content/datasets/Box-versus-Wheel-auto-dataset-5/ \\\n     valid/images\n\nDefine the main hyperparameters that you want to change from default, for example:\nMODEL = 'yolov8n.pt'\nIMG_SIZE = 640\nEPOCHS = 25 # For a final project, you should consider\n            # at least 100 epochs\nRun the training (using CLI):\n!yolo task=detect mode=train model={MODEL} \\\n  data={dataset.location}/data.yaml \\\n  epochs={EPOCHS}\n  imgsz={IMG_SIZE} plots=True\n\n\n\n\nimage-20240910111319804\n\n\nThe model took a few minutes to be trained and has an excellent result (mAP50 of 0.995). At the end of the training, all results are saved in the folder listed, for example: /runs/detect/train/. There, you can find, for example, the confusion matrix.\n \n\nNote that the trained model (best.pt) is saved in the folder /runs/detect/train/weights/. Now, you should validate the trained model with the valid/images.\n\n!yolo task=detect mode=val model={HOME}/runs/detect/train/\\\n       weights/best.pt data={dataset.location}/data.yaml\nThe results were similar to training.\n\nNow, we should perform inference on the images left aside for testing\n\n!yolo task=detect mode=predict model={HOME}/runs/detect/train/\\\n     weights/best.pt conf=0.25 source={dataset.location}/test/\\\n     images save=True\nThe inference results are saved in the folder runs/detect/predict. Let‚Äôs see some of them:\n \n\nIt is advised to export the train, validation, and test results for a Drive at Google. To do so, we should mount the drive.\nfrom google.colab import drive\ndrive.mount('/content/gdrive')\nand copy the content of /runs folder to a folder that you should create in your Drive, for example:\n!scp -r /content/runs '/content/gdrive/MyDrive/\\\n     10_UNIFEI/Box_vs_Wheel_Project'\n\n\n\n\n\nDownload the trained model /runs/detect/train/weights/best.pt to your computer. Using the FileZilla FTP, let‚Äôs transfer the best.pt to the Raspi models folder (before the transfer, you may change the model name, for example, box_wheel_320_yolo.pt).\nUsing the FileZilla FTP, let‚Äôs transfer a few images from the test dataset to .\\YOLO\\images:\nLet‚Äôs return to the YOLO folder and use the Python Interpreter:\ncd ..\npython\nAs before, we will import the YOLO library and define our converted model to detect bees:\nfrom ultralytics import YOLO\n\nmodel = YOLO(\"./models/box_wheel_320_yolo.pt\")\nNow, let‚Äôs define an image and call the inference (we will save the image result this time to external verification):\nimg = \"./images/1_box_1_wheel.jpg\"\nresult = model.predict(img, save=True, imgsz=320, conf=0.5, iou=0.3)\nLet‚Äôs repeat for several images. The inference result is saved on the variable result, and the processed image on runs/detect/predict8\n \nUsing FileZilla FTP, we can send the inference result to our Desktop for verification:\n \nWe can see that the inference result is excellent! The model was trained based on the smaller base model of the YOLOv8 family (YOLOv8n). The issue is the latency, around 1 second (or 1 FPS on the Raspi-Zero). Of course, we can reduce this latency and convert the model to TFLite or NCNN.",
    "crumbs": [
      "Raspberry Pi",
      "Object Detection"
    ]
  },
  {
    "objectID": "contents/raspi/object_detection/object_detection.html#sec-object-detection-object-detection-live-stream-7df0",
    "href": "contents/raspi/object_detection/object_detection.html#sec-object-detection-object-detection-live-stream-7df0",
    "title": "Object Detection",
    "section": "",
    "text": "All the models explored in this lab can detect objects in real-time using a camera. The captured image should be the input for the trained and converted model. For the Raspi-4 or 5 with a desktop, OpenCV can capture the frames and display the inference result.\nHowever, creating a live stream with a webcam to detect objects in real-time is also possible. For example, let‚Äôs start with the script developed for the Image Classification app and adapt it for a Real-Time Object Detection Web Application Using TensorFlow Lite and Flask.\nThis app version will work for all TFLite models. Verify if the model is in its correct folder, for example:\nmodel_path = \"./models/ssd-mobilenet-v1-tflite-default-v1.tflite\"\nDownload the Python script object_detection_app.py from GitHub.\nAnd on the terminal, run:\npython3 object_detection_app.py\nAnd access the web interface:\n\nOn the Raspberry Pi itself (if you have a GUI): Open a web browser and go to http://localhost:5000\nFrom another device on the same network: Open a web browser and go to http://&lt;raspberry_pi_ip&gt;:5000 (Replace &lt;raspberry_pi_ip&gt; with your Raspberry Pi‚Äôs IP address). For example: http://192.168.4.210: 5000/\n\nHere are some screenshots of the app running on an external desktop\n \nLet‚Äôs see a technical description of the key modules used in the object detection application:\n\nTensorFlow Lite (tflite_runtime):\n\nPurpose: Efficient inference of machine learning models on edge devices.\nWhy: TFLite offers reduced model size and optimized performance compared to full TensorFlow, which is crucial for resource-constrained devices like Raspberry Pi. It supports hardware acceleration and quantization, further improving efficiency.\nKey functions: Interpreter for loading and running the model, get_input_details(), and get_output_details() for interfacing with the model.\n\nFlask:\n\nPurpose: Lightweight web framework for creating the backend server.\nWhy: Flask‚Äôs simplicity and flexibility make it ideal for rapidly developing and deploying web applications. It‚Äôs less resource-intensive than larger frameworks suitable for edge devices.\nKey components: route decorators for defining API endpoints, Response objects for streaming video, render_template_string for serving dynamic HTML.\n\nPicamera2:\n\nPurpose: Interface with the Raspberry Pi camera module.\nWhy: Picamera2 is the latest library for controlling Raspberry Pi cameras, offering improved performance and features over the original Picamera library.\nKey functions: create_preview_configuration() for setting up the camera, capture_file() for capturing frames.\n\nPIL (Python Imaging Library):\n\nPurpose: Image processing and manipulation.\nWhy: PIL provides a wide range of image processing capabilities. It‚Äôs used here to resize images, draw bounding boxes, and convert between image formats.\nKey classes: Image for loading and manipulating images, ImageDraw for drawing shapes and text on images.\n\nNumPy:\n\nPurpose: Efficient array operations and numerical computing.\nWhy: NumPy‚Äôs array operations are much faster than pure Python lists, which is crucial for efficiently processing image data and model inputs/outputs.\nKey functions: array() for creating arrays, expand_dims() for adding dimensions to arrays.\n\nThreading:\n\nPurpose: Concurrent execution of tasks.\nWhy: Threading allows simultaneous frame capture, object detection, and web server operation, crucial for maintaining real-time performance.\nKey components: Thread class creates separate execution threads, and Lock is used for thread synchronization.\n\nio.BytesIO:\n\nPurpose: In-memory binary streams.\nWhy: Allows efficient handling of image data in memory without needing temporary files, improving speed and reducing I/O operations.\n\ntime:\n\nPurpose: Time-related functions.\nWhy: Used for adding delays (time.sleep()) to control frame rate and for performance measurements.\n\njQuery (client-side):\n\nPurpose: Simplified DOM manipulation and AJAX requests.\nWhy: It makes it easy to update the web interface dynamically and communicate with the server without page reloads.\nKey functions: .get() and .post() for AJAX requests, DOM manipulation methods for updating the UI.\n\n\nRegarding the main app system architecture:\n\nMain Thread: Runs the Flask server, handling HTTP requests and serving the web interface.\nCamera Thread: Continuously captures frames from the camera.\nDetection Thread: Processes frames through the TFLite model for object detection.\nFrame Buffer: Shared memory space (protected by locks) storing the latest frame and detection results.\n\nAnd the app data flow, we can describe in short:\n\nCamera captures frame ‚Üí Frame Buffer\nDetection thread reads from Frame Buffer ‚Üí Processes through TFLite model ‚Üí Updates detection results in Frame Buffer\nFlask routes access Frame Buffer to serve the latest frame and detection results\nWeb client receives updates via AJAX and updates UI\n\nThis architecture allows for efficient, real-time object detection while maintaining a responsive web interface running on a resource-constrained edge device like a Raspberry Pi. Threading and efficient libraries like TFLite and PIL enable the system to process video frames in real-time, while Flask and jQuery provide a user-friendly way to interact with them.\nYou can test the app with another pre-processed model, such as the EfficientDet, changing the app line:\nmodel_path = \"./models/lite-model_efficientdet_lite0_\\\n   detection_metadata_1.tflite\"\n\nIf we want to use the app for the SSD-MobileNetV2 model, trained on Edge Impulse Studio with the ‚ÄúBox versus Wheel‚Äù dataset, the code should also be adapted depending on the input details, as we have explored on its notebook.",
    "crumbs": [
      "Raspberry Pi",
      "Object Detection"
    ]
  },
  {
    "objectID": "contents/raspi/object_detection/object_detection.html#sec-object-detection-summary-680e",
    "href": "contents/raspi/object_detection/object_detection.html#sec-object-detection-summary-680e",
    "title": "Object Detection",
    "section": "",
    "text": "This lab has explored the implementation of object detection on edge devices like the Raspberry Pi, demonstrating the power and potential of running advanced computer vision tasks on resource-constrained hardware. We‚Äôve covered several vital aspects:\n\nModel Comparison: We examined different object detection models, including SSD-MobileNet, EfficientDet, FOMO, and YOLO, comparing their performance and trade-offs on edge devices.\nTraining and Deployment: Using a custom dataset of boxes and wheels (labeled on Roboflow), we walked through the process of training models using Edge Impulse Studio and Ultralytics and deploying them on Raspberry Pi.\nOptimization Techniques: To improve inference speed on edge devices, we explored various optimization methods, such as model quantization (TFLite int8) and format conversion (e.g., to NCNN).\nReal-time Applications: The lab exemplified a real-time object detection web application, demonstrating how these models can be integrated into practical, interactive systems.\nPerformance Considerations: Throughout the lab, we discussed the balance between model accuracy and inference speed, a critical consideration for edge AI applications.\n\nThe ability to perform object detection on edge devices opens up numerous possibilities across various domains, from precision agriculture, industrial automation, and quality control to smart home applications and environmental monitoring. By processing data locally, these systems can offer reduced latency, improved privacy, and operation in environments with limited connectivity.\nLooking ahead, potential areas for further exploration include:\n\nImplementing multi-model pipelines for more complex tasks\nExploring hardware acceleration options for Raspberry Pi\nIntegrating object detection with other sensors for more comprehensive edge AI systems\nDeveloping edge-to-cloud solutions that leverage both local processing and cloud resources\n\nObject detection on edge devices can create intelligent, responsive systems that bring the power of AI directly into the physical world, opening up new frontiers in how we interact with and understand our environment.",
    "crumbs": [
      "Raspberry Pi",
      "Object Detection"
    ]
  },
  {
    "objectID": "contents/raspi/object_detection/object_detection.html#sec-object-detection-resources-17bf",
    "href": "contents/raspi/object_detection/object_detection.html#sec-object-detection-resources-17bf",
    "title": "Object Detection",
    "section": "",
    "text": "Dataset (‚ÄúBox versus Wheel‚Äù)\nSSD-MobileNet Notebook on a Raspi\nEfficientDet Notebook on a Raspi\nFOMO - EI Linux Notebook on a Raspi\nYOLOv8 Box versus Wheel Dataset Training on CoLab\nEdge Impulse Project - SSD MobileNet and FOMO\nPython Scripts\nModels",
    "crumbs": [
      "Raspberry Pi",
      "Object Detection"
    ]
  },
  {
    "objectID": "contents/raspi/vlm/vlm.html",
    "href": "contents/raspi/vlm/vlm.html",
    "title": "Vision-Language Models (VLM)",
    "section": "",
    "text": "DALL¬∑E prompt - A Raspberry Pi setup featuring vision tasks. The image shows a Raspberry Pi connected to a camera, with various computer vision tasks displayed visually around it, including object detection, image captioning, segmentation, and visual grounding. The Raspberry Pi is placed on a desk, with a display showing bounding boxes and annotations related to these tasks. The background should be a home workspace, with tools and devices typically used by developers and hobbyists.\n\n\n\n\nIn this hands-on lab, we will continuously explore AI applications at the Edge, from the basic setup of the Florence-2, Microsoft‚Äôs state-of-the-art vision foundation model, to advanced implementations on devices like the Raspberry Pi. We will learn to use Vision Language Models (VLMs) for tasks such as captioning, object detection, grounding, segmentation, and OCR on a Raspberry Pi.\n\n\nFlorence-2 is a vision-language model open-sourced by Microsoft under the MIT license, which significantly advances vision-language models by combining a lightweight architecture with robust capabilities. Thanks to its training on the massive FLD-5B dataset, which contains 126 million images and 5.4 billion visual annotations, it achieves performance comparable to larger models. This makes Florence-2 ideal for deployment at the edge, where power and computational resources are limited.\nIn this tutorial, we will explore how to use Florence-2 for real-time computer vision applications, such as:\n\nImage captioning\nObject detection\nSegmentation\nVisual grounding\n\n\nVisual grounding involves linking textual descriptions to specific regions within an image. This enables the model to understand where particular objects or entities described in a prompt are in the image. For example, if the prompt is ‚Äúa red car,‚Äù the model will identify and highlight the region where the red car is found in the image. Visual grounding is helpful for applications where precise alignment between text and visual content is needed, such as human-computer interaction, image annotation, and interactive AI systems.\n\nIn the tutorial, we will walk through:\n\nSetting up Florence-2 on the Raspberry Pi\nRunning inference tasks such as object detection and captioning\nOptimizing the model to get the best performance from the edge device\nExploring practical, real-world applications with fine-tuning.\n\n\n\n\nFlorence-2 utilizes a unified, prompt-based representation to handle various vision-language tasks. The model architecture consists of two main components: an image encoder and a multi-modal transformer encoder-decoder.\n\\noindent\n\\begin{minipage}{\\linewidth}\n\\centering\n\\scalebox{0.8}{%\n\\begin{tikzpicture}[font=\\small\\usefont{T1}{phv}{m}{n}]\n\\tikzset{Line/.style={line width=1.0pt,black!50},\n  Box/.style={inner xsep=2pt,\n    draw=BlueLine,\n    node distance=0.6,\n    line width=0.75pt,\n    fill=BlueL,\n    anchor=west,\n    text width=36mm,\n    align=flush center,\n    minimum width=36mm,\n    minimum height=8mm\n  },\n}\n\n\\node[Box,draw=RedLine, fill=RedL](B1){Input};\n\\node[Box,below left=of B1](B2){Image};\n\\node[Box,below right =of B1](B3){Text Prompt};\n\\node[Box,below=of B2](B4){Image Encoder (DaVIT)};\n\\node[Box,below=of B3](B5){Text Tokenize};\n\\node[Box,below=of $(B4)!0.5!(B5)$](B6){Multimodality Encoder (Transformer)};\n\\node[Box,below=of B6](B7){Multimodality Decoder (Transformer)};\n\\node[Box,below=of B7,draw=RedLine, fill=RedL](B8){Output (Text/Coordinates)};\n%\n\\draw[Line,-latex](B1)-|(B2);\n\\draw[Line,-latex](B1)-|(B3);\n\\draw[Line,-latex](B2)--(B4);\n\\draw[Line,-latex](B3)--(B5);\n\\draw[Line,-latex](B4)|-(B6);\n\\draw[Line,-latex](B5)|-(B6);\n\\draw[Line,-latex](B6)--(B7);\n\\draw[Line,-latex](B7)--(B8);\n\\end{tikzpicture}}\n\\end{minipage}\n\nImage Encoder: The image encoder is based on the DaViT (Dual Attention Vision Transformers) architecture. It converts input images into a series of visual token embeddings. These embeddings serve as the foundational representations of the visual content, capturing both spatial and contextual information about the image.\nMulti-Modal Transformer Encoder-Decoder: Florence-2‚Äôs core is the multi-modal transformer encoder-decoder, which combines visual token embeddings from the image encoder with textual embeddings generated by a BERT-like model. This combination allows the model to simultaneously process visual and textual inputs, enabling a unified approach to tasks such as image captioning, object detection, and segmentation.\n\nThe model‚Äôs training on the extensive FLD-5B dataset ensures it can effectively handle diverse vision tasks without requiring task-specific modifications. Florence-2 uses textual prompts to activate specific tasks, making it highly flexible and capable of zero-shot generalization. For tasks like object detection or visual grounding, the model incorporates additional location tokens to represent regions within the image, ensuring a precise understanding of spatial relationships.\n\nFlorence-2‚Äôs compact architecture and innovative training approach allow it to perform computer vision tasks accurately, even on resource-constrained devices like the Raspberry Pi.\n\n\n\n\n\nFlorence-2 introduces several innovative features that set it apart:\n\n\n \n\nLightweight Design: Two variants available\n\nFlorence-2-Base: 232 million parameters\nFlorence-2-Large: 771 million parameters\n\nUnified Representation: Handles multiple vision tasks through a single architecture\nDaViT Vision Encoder: Converts images into visual token embeddings\nTransformer-based Multi-modal Encoder-Decoder: Processes combined visual and text embeddings\n\n\n\n\n \n\n126 million unique images\n5.4 billion comprehensive annotations, including:\n\n500M text annotations\n1.3B region-text annotations\n3.6B text-phrase-region annotations\n\nAutomated annotation pipeline using specialist models\nIterative refinement process for high-quality labels\n\n\n\n\nFlorence-2 excels in multiple vision tasks:\n\n\n\nImage Captioning: Achieves 135.6 CIDEr score on COCO\nVisual Grounding: 84.4% recall@1 on Flickr30k\nObject Detection: 37.5 mAP on COCO val2017\nReferring Expression: 67.0% accuracy on RefCOCO\n\n\n\n\n\nCompetitive with specialist models despite the smaller size\nOutperforms larger models in specific benchmarks\nEfficient adaptation to new tasks\n\n\n\n\n\nFlorence-2 can be applied across various domains:\n\nContent Understanding\n\nAutomated image captioning for accessibility\nVisual content moderation\nMedia asset management\n\nE-commerce\n\nProduct image analysis\nVisual search\nAutomated product tagging\n\nHealthcare\n\nMedical image analysis\nDiagnostic assistance\nResearch data processing\n\nSecurity & Surveillance\n\nObject detection and tracking\nAnomaly detection\nScene understanding\n\n\n\n\n\nFlorence-2 stands out from other visual language models due to its impressive zero-shot capabilities. Unlike models like Google PaliGemma, which rely on extensive fine-tuning to adapt to various tasks, Florence-2 works right out of the box, as we will see in this lab. It can also compete with larger models like GPT-4V and Flamingo, which often have many more parameters but only sometimes match Florence-2‚Äôs performance. For example, Florence-2 achieves better zero-shot results than Kosmos-2 despite having over twice the parameters.\nIn benchmark tests, Florence-2 has shown remarkable performance in tasks like COCO captioning and referring expression comprehension. It outperformed models like PolyFormer and UNINEXT in object detection and segmentation tasks on the COCO dataset. It is a highly competitive choice for real-world applications where both performance and resource efficiency are crucial.\n\n\n\n\nOur choice of edge device is the Raspberry Pi 5 (Raspi-5). Its robust platform is equipped with the Broadcom BCM2712, a 2.4 GHz quad-core 64-bit Arm Cortex-A76 CPU featuring Cryptographic Extension and enhanced caching capabilities. It boasts a VideoCore VII GPU, dual 4Kp60 HDMI¬Æ outputs with HDR, and a 4Kp60 HEVC decoder. Memory options include 4 GB and 8 GB of high-speed LPDDR4X SDRAM, with 8 GB being our choice to run Florence-2. It also features expandable storage via a microSD card slot and a PCIe 2.0 interface for fast peripherals such as M.2 SSDs (Solid State Drives).\n\nFor real applications, SSDs are a better option than SD cards.\n\nWe suggest installing an Active Cooler, a dedicated clip-on cooling solution for Raspberry Pi 5 (Raspi-5), for this lab. It combines an aluminum heat sink with a temperature-controlled blower fan to keep the Raspi-5 operating comfortably under heavy loads, such as running Florense-2.\n \n\n\nTo run Microsoft Florense-2 on the Raspberry Pi 5, we‚Äôll need a few libraries:\n\nTransformers:\n\nFlorence-2 uses the transformers library from Hugging Face for model loading and inference. This library provides the architecture for working with pre-trained vision-language models, making it easy to perform tasks like image captioning, object detection, and more. Essentially, transformers helps in interacting with the model, processing input prompts, and obtaining outputs.\n\nPyTorch:\n\nPyTorch is a deep learning framework that provides the infrastructure needed to run the Florence-2 model, which includes tensor operations, GPU acceleration (if a GPU is available), and model training/inference functionalities. The Florence-2 model is trained in PyTorch, and we need it to leverage its functions, layers, and computation capabilities to perform inferences on the Raspberry Pi.\n\nTimm (PyTorch Image Models):\n\nFlorence-2 uses timm to access efficient implementations of vision models and pre-trained weights. Specifically, the timm library is utilized for the image encoder part of Florence-2, particularly for managing the DaViT architecture. It provides model definitions and optimized code for common vision tasks and allows the easy integration of different backbones that are lightweight and suitable for edge devices.\n\nEinops:\n\nEinops is a library for flexible and powerful tensor operations. It makes it easy to reshape and manipulate tensor dimensions, which is especially important for the multi-modal processing done in Florence-2. Vision-language models like Florence-2 often need to rearrange image data, text embeddings, and visual embeddings to align correctly for the transformer blocks, and einops simplifies these complex operations, making the code more readable and concise.\n\n\nIn short, these libraries enable different essential components of Florence-2:\n\nTransformers and PyTorch are needed to load the model and run the inference.\nTimm is used to access and efficiently implement the vision encoder.\nEinops helps reshape data, facilitating the integration of visual and text features.\n\nAll these components work together to help Florence-2 run seamlessly on our Raspberry Pi, allowing it to perform complex vision-language tasks relatively quickly.\nConsidering that the Raspberry Pi already has its OS installed, let‚Äôs use SSH to reach it from another computer:\nssh mjrovai@raspi-5.local\nAnd check the IP allocated to it:\nhostname -I\n192.168.4.209\n \nUpdating the Raspberry Pi\nFirst, ensure your Raspberry Pi is up to date:\nsudo apt update\nsudo apt upgrade -y\nInitial setup for using PIP:\nsudo apt install python3-pip\nsudo rm /usr/lib/python3.11/EXTERNALLY-MANAGED\npip3 install --upgrade pip\nInstall Dependencies\nsudo apt-get install libjpeg-dev libopenblas-dev libopenmpi-dev \\\n    libomp-dev\nLet‚Äôs set up and activate a Virtual Environment for working with Florence-2:\npython3 -m venv ~/florence\nsource ~/florence/bin/activate\nInstall PyTorch\npip3 install setuptools numpy Cython\npip3 install requests\npip3 install torch torchvision \\\n    --index-url https://download.pytorch.org/whl/cpu\npip3 install torchaudio \\\n    --index-url https://download.pytorch.org/whl/cpu\nLet‚Äôs verify that PyTorch is correctly installed:\n \nInstall Transformers, Timm and Einops:\npip3 install transformers\npip3 install timm einops\nInstall the model:\npip3 install autodistill-florence-2\nJupyter Notebook and Python libraries\nInstalling a Jupyter Notebook to run and test our Python scripts is possible.\npip3 install jupyter\npip3 install numpy Pillow matplotlib\njupyter notebook --generate-config\n\n\n\nRunning the Jupyter Notebook on the remote computer\njupyter notebook --ip=192.168.4.209 --no-browser\nRunning the above command on the SSH terminal, we can see the local URL address to open the notebook:\n \nThe notebook with the code used on this initial test can be found on the Lab GitHub:\n\n10-florence2_test.ipynb\n\nWe can access it on the remote computer by entering the Raspberry Pi‚Äôs IP address and the provided token in a web browser (copy the entire URL from the terminal).\nFrom the Home page, create a new notebook [Python 3 (ipykernel) ] and copy and paste the example code from Hugging Face Hub.\nThe code is designed to run Florence-2 on a given image to perform object detection. It loads the model, processes an image and a prompt, and then generates a response to identify and describe the objects in the image.\n\nThe processor helps prepare text and image inputs.\nThe model takes the processed inputs to generate a meaningful response.\nThe post-processing step refines the generated output into a more interpretable form, like bounding boxes for detected objects.\n\n\nThis workflow leverages the versatility of Florence-2 to handle vision-language tasks and is implemented efficiently using PyTorch, Transformers, and related image-processing tools.\n\nimport requests\nfrom PIL import Image\nimport torch\nfrom transformers import AutoProcessor, AutoModelForCausalLM\n\ndevice = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\ntorch_dtype = (\n    torch.float16 if torch.cuda.is_available() else torch.float32\n)\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    \"microsoft/Florence-2-base\",\n    torch_dtype=torch_dtype,\n    trust_remote_code=True,\n).to(device)\nprocessor = AutoProcessor.from_pretrained(\n    \"microsoft/Florence-2-base\", trust_remote_code=True\n)\n\nprompt = \"&lt;OD&gt;\"\n\nurl = (\n    \"https://huggingface.co/datasets/huggingface/\"\n    \"documentation-images/resolve/main/transformers/\"\n    \"tasks/car.jpg?download=true\"\n)\nimage = Image.open(requests.get(url, stream=True).raw)\n\ninputs = processor(text=prompt, images=image, return_tensors=\"pt\").to(\n    device, torch_dtype\n)\n\ngenerated_ids = model.generate(\n    input_ids=inputs[\"input_ids\"],\n    pixel_values=inputs[\"pixel_values\"],\n    max_new_tokens=1024,\n    do_sample=False,\n    num_beams=3,\n)\ngenerated_text = processor.batch_decode(\n    generated_ids, skip_special_tokens=False\n)[0]\n\nparsed_answer = processor.post_process_generation(\n    generated_text,\n    task=\"&lt;OD&gt;\",\n    image_size=(image.width, image.height),\n)\n\nprint(parsed_answer)\nLet‚Äôs break down the provided code step by step:\n\n\nimport requests\nfrom PIL import Image\nimport torch\nfrom transformers import AutoProcessor, AutoModelForCausalLM\n\nrequests: Used to make HTTP requests. In this case, it downloads an image from a URL.\nPIL (Pillow): Provides tools for manipulating images. Here, it‚Äôs used to open the downloaded image.\ntorch: PyTorch is imported to handle tensor operations and determine the hardware availability (CPU or GPU).\ntransformers: This module provides easy access to Florence-2 by using AutoProcessor and AutoModelForCausalLM to load pre-trained models and process inputs.\n\n\n\n\ndevice = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n\ntorch_dtype = (\n    torch.float16 if torch.cuda.is_available() else torch.float32\n)\n\nDevice Setup: The code checks if a CUDA-enabled GPU is available (torch.cuda.is_available()). The device is set to ‚Äúcuda:0‚Äù if a GPU is available. Otherwise, it defaults to \"cpu\" (our case here).\nData Type Setup: If a GPU is available, torch.float16 is chosen, which uses half-precision floats to speed up processing and reduce memory usage. On the CPU, it defaults to torch.float32 to maintain compatibility.\n\n\n\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    \"microsoft/Florence-2-base\",\n    torch_dtype=torch_dtype,\n    trust_remote_code=True,\n).to(device)\n\nprocessor = AutoProcessor.from_pretrained(\n    \"microsoft/Florence-2-base\", trust_remote_code=True\n)\n\nModel Initialization:\n\nAutoModelForCausalLM.from_pretrained() loads the pre-trained Florence-2 model from Microsoft‚Äôs repository on Hugging Face. The torch_dtype is set according to the available hardware (GPU/CPU), and trust_remote_code=True allows the use of any custom code that might be provided with the model.\n.to(device) moves the model to the appropriate device (either CPU or GPU). In our case, it will be set to CPU.\n\nProcessor Initialization:\n\nAutoProcessor.from_pretrained() loads the processor for Florence-2. The processor is responsible for transforming text and image inputs into a format the model can work with (e.g., encoding text, normalizing images, etc.).\n\n\n\n\n\n\nprompt = \"&lt;OD&gt;\"\n\nPrompt Definition: The string \"&lt;OD&gt;\" is used as a prompt. This refers to ‚ÄúObject Detection‚Äù, instructing the model to detect objects on the image.\n\n\n\nurl = \"https://huggingface.co/datasets/huggingface/\"\n      \"documentation-images/resolve/main/transformers/\"\n      \"tasks/car.jpg?download=true\"\nimage = Image.open(requests.get(url, stream=True).raw)\n\nDownloading the Image: The requests.get() function fetches the image from the specified URL. The stream=True parameter ensures the image is streamed rather than downloaded completely at once.\nOpening the Image: Image.open() opens the image so the model can process it.\n\n\n\n\ninputs = processor(text=prompt, images=image, return_tensors=\"pt\").to(\n    device, torch_dtype\n)\n\nProcessing Input Data: The processor() function processes the text (prompt) and the image (image). The return_tensors=\"pt\" argument converts the processed data into PyTorch tensors, which are necessary for inputting data into the model.\nMoving Inputs to Device: .to(device, torch_dtype) moves the inputs to the correct device (CPU or GPU) and assigns the appropriate data type.\n\n\n\n\n\ngenerated_ids = model.generate(\n    input_ids=inputs[\"input_ids\"],\n    pixel_values=inputs[\"pixel_values\"],\n    max_new_tokens=1024,\n    do_sample=False,\n    num_beams=3,\n)\n\nModel Generation: model.generate() is used to generate the output based on the input data.\n\ninput_ids: Represents the tokenized form of the prompt.\npixel_values: Contains the processed image data.\nmax_new_tokens=1024: Specifies the maximum number of new tokens to be generated in the response. This limits the response length.\ndo_sample=False: Disables sampling; instead, the generation uses deterministic methods (beam search).\nnum_beams=3: Enables beam search with three beams, which improves output quality by considering multiple possibilities during generation.\n\n\n\n\ngenerated_text = processor.batch_decode(\n    generated_ids, skip_special_tokens=False\n)[0]\n\nBatch Decode: processor.batch_decode() decodes the generated IDs (tokens) into readable text. The skip_special_tokens=False parameter means that the output will include any special tokens that may be part of the response.\n\n\n\n\nparsed_answer = processor.post_process_generation(\n    generated_text,\n    task=\"&lt;OD&gt;\",\n    image_size=(image.width, image.height),\n)\n\nPost-Processing: processor.post_process_generation() is called to process the generated text further, interpreting it based on the task (\"&lt;OD&gt;\" for object detection) and the size of the image.\nThis function extracts specific information from the generated text, such as bounding boxes for detected objects, making the output more useful for visual tasks.\n\n\n\n\nprint(parsed_answer)\n\nFinally, print(parsed_answer) displays the output, which could include object detection results, such as bounding box coordinates and labels for the detected objects in the image.\n\n\n\n\nRunning the code, we get as the Parsed Answer:\n[{'&lt;OD&gt;': {\n   'bboxes': [\n     [34.23999786376953, 160.0800018310547, 597.4400024414062],\n     [371.7599792480469, 272.32000732421875, 241.67999267578125],\n     [303.67999267578125, 247.4399871826172, 454.0799865722656],\n     [276.7200012207031, 553.9199829101562, 370.79998779296875],\n     [96.31999969482422, 280.55999755859375, 198.0800018310547],\n     [371.2799987792969]\n    ],\n    'labels': ['car', 'door handle', 'wheel', 'wheel']\n}}]\nFirst, let‚Äôs inspect the image:\nimport matplotlib.pyplot as plt\n\nplt.figure(figsize=(8, 8))\nplt.imshow(image)\nplt.axis(\"off\")\nplt.show()\n \nBy the Object Detection result, we can see that:\n'labels': ['car', 'door handle', 'wheel', 'wheel']\nIt seems that at least a few objects were detected. We can also implement a code to draw the bounding boxes in the find objects:\ndef plot_bbox(image, data):\n    # Create a figure and axes\n    fig, ax = plt.subplots()\n\n    # Display the image\n    ax.imshow(image)\n\n    # Plot each bounding box\n    for bbox, label in zip(data[\"bboxes\"], data[\"labels\"]):\n        # Unpack the bounding box coordinates\n        x1, y1, x2, y2 = bbox\n        # Create a Rectangle patch\n        rect = patches.Rectangle(\n            (x1, y1),\n            x2 - x1,\n            y2 - y1,\n            linewidth=1,\n            edgecolor=\"r\",\n            facecolor=\"none\",\n        )\n        # Add the rectangle to the Axes\n        ax.add_patch(rect)\n        # Annotate the label\n        plt.text(\n            x1,\n            y1,\n            label,\n            color=\"white\",\n            fontsize=8,\n            bbox=dict(facecolor=\"red\", alpha=0.5),\n        )\n\n    # Remove the axis ticks and labels\n    ax.axis(\"off\")\n\n    # Show the plot\n    plt.show()\n\nBox (x0, y0, x1, y1): Location tokens correspond to the top-left and bottom-right corners of a box.\n\nAnd running\nplot_bbox(image, parsed_answer['&lt;OD&gt;'])\nWe get:\n \n\n\n\n\n\nFlorence-2 is designed to perform a variety of computer vision and vision-language tasks through prompts. These tasks can be activated by providing a specific textual prompt to the model, as we saw with &lt;OD&gt; (Object Detection).\nFlorence-2‚Äôs versatility comes from combining these prompts, allowing us to guide the model‚Äôs behavior to perform specific vision tasks. Changing the prompt allows us to adapt Florence-2 to different tasks without needing task-specific modifications in the architecture. This capability directly results from Florence-2‚Äôs unified model architecture and large-scale multi-task training on the FLD-5B dataset.\nHere are some of the key tasks that Florence-2 can perform, along with example prompts:\n\n\n\nPrompt: \"&lt;OD&gt;\"\nDescription: Identifies objects in an image and provides bounding boxes for each detected object. This task is helpful for applications like visual inspection, surveillance, and general object recognition.\n\n\n\n\n\nPrompt: \"&lt;CAPTION&gt;\"\nDescription: Generates a textual description for an input image. This task helps the model describe what is happening in the image, providing a human-readable caption for content understanding.\n\n\n\n\n\nPrompt: \"&lt;DETAILED_CAPTION&gt;\"\nDescription: Generates a more detailed caption with more nuanced information about the scene, such as the objects present and their relationships.\n\n\n\n\n\nPrompt: \"&lt;CAPTION_TO_PHRASE_GROUNDING&gt;\"\nDescription: Links a textual description to specific regions in an image. For example, given a prompt like ‚Äúa green car,‚Äù the model highlights where the green car is in the image. This is useful for human-computer interaction, where you must find specific objects based on text.\n\n\n\n\n\nPrompt: \"&lt;REFERRING_EXPRESSION_SEGMENTATION&gt;\"\nDescription: Performs segmentation based on a referring expression, such as ‚Äúthe blue cup.‚Äù The model identifies and segments the specific region containing the object mentioned in the prompt (all related pixels).\n\n\n\n\n\nPrompt: \"&lt;DENSE_REGION_CAPTION&gt;\"\nDescription: Provides captions for multiple regions within an image, offering a detailed breakdown of all visible areas, including different objects and their relationships.\n\n\n\n\n\nPrompt: \"&lt;OCR_WITH_REGION&gt;\"\nDescription: Performs Optical Character Recognition (OCR) on an image and provides bounding boxes for the detected text. This is useful for extracting and locating textual information in images, such as reading signs, labels, or other forms of text in images.\n\n\n\n\n\nPrompt: \"&lt;CAPTION_TO_PHRASE_GROUNDING&gt;\" along with a specific expression, such as \"a wine glass\".\nDescription: Locates the area in the image that corresponds to a specific textual phrase. This task allows for identifying particular objects or elements when prompted with a word or keyword.\n\n\n\n\n\nPrompt: \"&lt;OPEN_VOCABULARY_OD&gt;\"\nDescription: The model can detect objects without being restricted to a predefined list of classes, making it helpful in recognizing a broader range of items based on general visual understanding.\n\n\n\n\n\nFor exploration, all codes can be found on the GitHub:\n\n20-florence_2.ipynb\n\nLet‚Äôs use a couple of images created by Dall-E and upload them to the Rasp-5 (FileZilla can be used for that). The images will be saved on a sub-folder named images :\ndogs_cats = Image.open(\"./images/dogs-cats.jpg\")\ntable = Image.open(\"./images/table.jpg\")\n \nLet‚Äôs create a function to facilitate our exploration and to keep track of the latency of the model for different tasks:\ndef run_example(task_prompt, text_input=None, image=None):\n    start_time = time.perf_counter()  # Start timing\n    if text_input is None:\n        prompt = task_prompt\n    else:\n        prompt = task_prompt + text_input\n    inputs = processor(\n        text=prompt, images=image, return_tensors=\"pt\"\n    ).to(device)\n    generated_ids = model.generate(\n        input_ids=inputs[\"input_ids\"],\n        pixel_values=inputs[\"pixel_values\"],\n        max_new_tokens=1024,\n        early_stopping=False,\n        do_sample=False,\n        num_beams=3,\n    )\n    generated_text = processor.batch_decode(\n        generated_ids, skip_special_tokens=False\n    )[0]\n    parsed_answer = processor.post_process_generation(\n        generated_text,\n        task=task_prompt,\n        image_size=(image.width, image.height),\n    )\n\n    end_time = time.perf_counter()  # End timing\n    elapsed_time = end_time - start_time  # Calculate elapsed time\n    print(\n        f\" \\n[INFO] ==&gt; Florence-2-base ({task_prompt}), \\\n          took {elapsed_time:.1f} seconds to execute.\\n\"\n    )\n\n    return parsed_answer\n\n\n1. Dogs and Cats\nrun_example(task_prompt=\"&lt;CAPTION&gt;\", image=dogs_cats)\n[INFO] ==&gt; Florence-2-base (&lt;CAPTION&gt;), \\\ntook 16.1 seconds to execute.\n\n{'&lt;CAPTION&gt;': 'A group of dogs and cats sitting in a garden.'}\n2. Table\nrun_example(task_prompt=\"&lt;CAPTION&gt;\", image=table)\n[INFO] ==&gt; Florence-2-base (&lt;CAPTION&gt;), \\\ntook 16.5 seconds to execute.\n\n{'&lt;CAPTION&gt;': 'A wooden table topped with a plate of fruit \\\nand a glass of wine.'}\n\n\n\n1. Dogs and Cats\nrun_example(task_prompt=\"&lt;DETAILED_CAPTION&gt;\", image=dogs_cats)\n[INFO] ==&gt; Florence-2-base (&lt;DETAILED_CAPTION&gt;), \\\ntook 25.5 seconds to execute.\n\n{'&lt;DETAILED_CAPTION&gt;': 'The image shows a group of cats and \\\ndogs sitting on top of a lush green field, surrounded by plants \\\nwith flowers, trees, and a house in the background. The sky is \\\nvisible above them, creating a peaceful atmosphere.'}\n2. Table\nrun_example(task_prompt=\"&lt;DETAILED_CAPTION&gt;\", image=table)\n[INFO] ==&gt; Florence-2-base (&lt;DETAILED_CAPTION&gt;), \\\ntook 26.8 seconds to execute.\n\n{'&lt;DETAILED_CAPTION&gt;': 'The image shows a wooden table with \\\na bottle of wine and a glass of wine on it, surrounded by \\\na variety of fruits such as apples, oranges, and grapes. \\\nIn the background, there are chairs, plants, trees, and \\\na house, all slightly blurred.'}\n\n\n\n1. Dogs and Cats\nrun_example(task_prompt=\"&lt;MORE_DETAILED_CAPTION&gt;\", image=dogs_cats)\n[INFO] ==&gt; Florence-2-base (&lt;MORE_DETAILED_CAPTION&gt;), \\\ntook 49.8 seconds to execute.\n\n{'&lt;MORE_DETAILED_CAPTION&gt;': 'The image shows a group of four \\\ncats and a dog in a garden. The garden is filled with colorful \\\nflowers and plants, and there is a pathway leading up to \\\na house in the background. The main focus of the image is \\\na large German Shepherd dog standing on the left side of \\\nthe garden, with its tongue hanging out and its mouth open, \\\nas if it is panting. On the right side, there are \\\ntwo smaller cats, one orange and one gray, sitting on the \\\ngrass. In the background, there is another golden retriever \\\ndog sitting and looking at the camera. The sky is blue and \\\nthe sun is shining, creating a warm and inviting atmosphere.'}\n2. Table\nrun_example(task_prompt=\"&lt; MORE_DETAILED_CAPTION&gt;\", image=table)\nINFO] ==&gt; Florence-2-base (&lt;MORE_DETAILED_CAPTION&gt;), \\\ntook 32.4 seconds to execute.\n\n{'&lt;MORE_DETAILED_CAPTION&gt;': 'The image shows a wooden table \\\nwith a wooden tray on it. On the tray, there are various \\\nfruits such as grapes, oranges, apples, and grapes. There \\\nis also a bottle of red wine on the table. The background \\\nshows a garden with trees and a house. The overall mood \\\nof the image is peaceful and serene.'}\n\nWe can note that the more detailed the caption task, the longer the latency and the possibility of mistakes (like ‚ÄúThe image shows a group of four cats and a dog in a garden‚Äù, instead of two dogs and three cats).\n\n\n\n\nWe can run the same previous function for object detection using the prompt &lt;OD&gt;.\ntask_prompt = \"&lt;OD&gt;\"\nresults = run_example(task_prompt, image=dogs_cats)\nprint(results)\nLet‚Äôs see the result:\n[INFO] ==&gt; Florence-2-base (&lt;OD&gt;), took 20.9 seconds to execute.\n\n{'&lt;OD&gt;': {'bboxes': [\n  [737.79, 571.90, 1022.46, 980.48],\n  [0.51, 593.40, 211.45, 991.74],\n  [445.95, 721.40, 680.44, 850.43],\n  [39.42, 91.64, 491.00, 933.37],\n  [570.88, 184.83, 974.33, 782.84]\n  ],\n  'labels': ['cat', 'cat', 'cat', 'dog', 'dog']\n}}\nOnly by the labels ['cat,' 'cat,' 'cat,' 'dog,' 'dog'] is it possible to see that the main objects in the image were captured. Let‚Äôs apply the function used before to draw the bounding boxes:\nplot_bbox(dogs_cats, results[\"&lt;OD&gt;\"])\n \nLet‚Äôs also do it with the Table image:\ntask_prompt = \"&lt;OD&gt;\"\nresults = run_example(task_prompt, image=table)\nplot_bbox(table, results[\"&lt;OD&gt;\"])\n[INFO] ==&gt; Florence-2-base (&lt;OD&gt;), took 40.8 seconds to execute.\n \n\n\n\nIt is possible to mix the classic Object Detection with the Caption task in specific sub-regions of the image:\ntask_prompt = \"&lt;DENSE_REGION_CAPTION&gt;\"\n\nresults = run_example(task_prompt, image=dogs_cats)\nplot_bbox(dogs_cats, results[\"&lt;DENSE_REGION_CAPTION&gt;\"])\n\nresults = run_example(task_prompt, image=table)\nplot_bbox(table, results[\"&lt;DENSE_REGION_CAPTION&gt;\"])\n \n\n\n\nWith this task, we can enter with a caption, such as ‚Äúa wine glass‚Äù, ‚Äúa wine bottle,‚Äù or ‚Äúa half orange,‚Äù and Florence-2 will localize the object in the image:\ntask_prompt = \"&lt;CAPTION_TO_PHRASE_GROUNDING&gt;\"\n\nresults = run_example(\n    task_prompt, text_input=\"a wine bottle\", image=table\n)\nplot_bbox(table, results[\"&lt;CAPTION_TO_PHRASE_GROUNDING&gt;\"])\n\nresults = run_example(\n    task_prompt, text_input=\"a wine glass\", image=table\n)\nplot_bbox(table, results[\"&lt;CAPTION_TO_PHRASE_GROUNDING&gt;\"])\n\nresults = run_example(\n    task_prompt, text_input=\"a half orange\", image=table\n)\nplot_bbox(table, results[\"&lt;CAPTION_TO_PHRASE_GROUNDING&gt;\"])\n \n[INFO] ==&gt; Florence-2-base (&lt;CAPTION_TO_PHRASE_GROUNDING&gt;), \\\ntook 15.7 seconds to execute\neach task.\n\n\n\nWe can also enter the image caption as the input text to push Florence-2 to find more objects:\ntask_prompt = \"&lt;CAPTION&gt;\"\nresults = run_example(task_prompt, image=dogs_cats)\ntext_input = results[task_prompt]\ntask_prompt = \"&lt;CAPTION_TO_PHRASE_GROUNDING&gt;\"\nresults = run_example(task_prompt, text_input, image=dogs_cats)\nplot_bbox(dogs_cats, results[\"&lt;CAPTION_TO_PHRASE_GROUNDING&gt;\"])\nChanging the task_prompt among &lt;CAPTION,&gt; &lt;DETAILED_CAPTION&gt; and &lt;MORE_DETAILED_CAPTION&gt;, we will get more objects in the image.\n \n\n\n\n&lt;OPEN_VOCABULARY_DETECTION&gt; allows Florence-2 to detect recognizable objects in an image without relying on a predefined list of categories, making it a versatile tool for identifying various items that may not have been explicitly labeled during training. Unlike &lt;CAPTION_TO_PHRASE_GROUNDING&gt;, which requires a specific text phrase to locate and highlight a particular object in an image, &lt;OPEN_VOCABULARY_DETECTION&gt; performs a broad scan to find and classify all objects present.\nThis makes &lt;OPEN_VOCABULARY_DETECTION&gt; particularly useful for applications where you need a comprehensive overview of everything in an image without prior knowledge of what to expect. Enter with a text describing specific objects not previously detected, resulting in their detection. For example:\ntask_prompt = \"&lt;OPEN_VOCABULARY_DETECTION&gt;\"\n\ntext = [\n    \"a house\",\n    \"a tree\",\n    \"a standing cat at the left\",\n    \"a sleeping cat on the ground\",\n    \"a standing cat at the right\",\n    \"a yellow cat\",\n]\n\nfor txt in text:\n    results = run_example(\n        task_prompt, text_input=txt, image=dogs_cats\n    )\n\n    bbox_results = convert_to_od_format(\n        results[\"&lt;OPEN_VOCABULARY_DETECTION&gt;\"]\n    )\n\n    plot_bbox(dogs_cats, bbox_results)\n \n[INFO] ==&gt; Florence-2-base (&lt;OPEN_VOCABULARY_DETECTION&gt;), \\\ntook 15.1 seconds to execute\neach task.\n\nNote: Trying to use Florence-2 to find objects that were not found can leads to mistakes (see examples on the Notebook).\n\n\n\n\nWe can also segment a specific object in the image and give its description (caption), such as ‚Äúa wine bottle‚Äù on the table image or ‚Äúa German Sheppard‚Äù on the dogs_cats.\nReferring expression segmentation results format: {'&lt;REFERRING_EXPRESSION_SEGMENTATION&gt;': {'Polygons': [[[polygon]], ...], 'labels': ['', '', ...]}}, one object is represented by a list of polygons. each polygon is [x1, y1, x2, y2, ..., xn, yn].\n\nPolygon (x1, y1, ‚Ä¶, xn, yn): Location tokens represent the vertices of a polygon in clockwise order.\n\nSo, let‚Äôs first create a function to plot the segmentation:\nfrom PIL import Image, ImageDraw, ImageFont\nimport copy\nimport random\nimport numpy as np\n\ncolormap = [\n    \"blue\",\n    \"orange\",\n    \"green\",\n    \"purple\",\n    \"brown\",\n    \"pink\",\n    \"gray\",\n    \"olive\",\n    \"cyan\",\n    \"red\",\n    \"lime\",\n    \"indigo\",\n    \"violet\",\n    \"aqua\",\n    \"magenta\",\n    \"coral\",\n    \"gold\",\n    \"tan\",\n    \"skyblue\",\n]\n\n\ndef draw_polygons(image, prediction, fill_mask=False):\n    \"\"\"\n        Draws segmentation masks with polygons on an image.\n\n        Parameters:\n          - image_path: Path to the image file.\n          - prediction: Dictionary containing 'polygons' and 'labels'\n                  keys. 'polygons' is a list of lists, each\n                  containing vertices of a polygon. 'labels' is\n                  a list of labels corresponding to each polygon.\n    - fill_mask: Boolean indicating whether to fill the polygons\n                 with color.\n    \"\"\"\n    # Load the image\n\n    draw = ImageDraw.Draw(image)\n\n    # Set up scale factor if needed (use 1 if not scaling)\n    scale = 1\n\n    # Iterate over polygons and labels\n    for polygons, label in zip(\n        prediction[\"polygons\"], prediction[\"labels\"]\n    ):\n        color = random.choice(colormap)\n        fill_color = random.choice(colormap) if fill_mask else None\n\n        for _polygon in polygons:\n            _polygon = np.array(_polygon).reshape(-1, 2)\n            if len(_polygon) &lt; 3:\n                print(\"Invalid polygon:\", _polygon)\n                continue\n\n            _polygon = (_polygon * scale).reshape(-1).tolist()\n\n            # Draw the polygon\n            if fill_mask:\n                draw.polygon(_polygon, outline=color, fill=fill_color)\n            else:\n                draw.polygon(_polygon, outline=color)\n\n            # Draw the label text\n            draw.text(\n                (_polygon[0] + 8, _polygon[1] + 2), label, fill=color\n            )\n\n    # Save or display the image\n    # image.show()  # Display the image\n    display(image)\nNow we can run the functions:\ntask_prompt = \"&lt;REFERRING_EXPRESSION_SEGMENTATION&gt;\"\n\nresults = run_example(\n    task_prompt, text_input=\"a wine bottle\", image=table\n)\noutput_image = copy.deepcopy(table)\ndraw_polygons(\n    output_image,\n    results[\"&lt;REFERRING_EXPRESSION_SEGMENTATION&gt;\"],\n    fill_mask=True,\n)\n\nresults = run_example(\n    task_prompt, text_input=\"a german sheppard\", image=dogs_cats\n)\noutput_image = copy.deepcopy(dogs_cats)\ndraw_polygons(\n    output_image,\n    results[\"&lt;REFERRING_EXPRESSION_SEGMENTATION&gt;\"],\n    fill_mask=True,\n)\n \n[INFO] ==&gt; Florence-2-base\n(&lt;REFERRING_EXPRESSION_SEGMENTATION&gt;), took 207.0 seconds\nto execute each task.\n\n\n\nWith this task, it is also possible to give the object coordinates in the image to segment it. The input format is '&lt;loc_x1&gt;&lt;loc_y1&gt;&lt;loc_x2&gt;&lt;loc_y2&gt;', [x1, y1, x2, y2] , which is the quantized coordinates in [0, 999].\nFor example, when running the code:\ntask_prompt = \"&lt;CAPTION_TO_PHRASE_GROUNDING&gt;\"\nresults = run_example(\n    task_prompt, text_input=\"a half orange\", image=table\n)\nresults\nThe results were:\n{'&lt;CAPTION_TO_PHRASE_GROUNDING&gt;': {'bboxes': [[343.552001953125,\n    689.6640625,\n    530.9440307617188,\n    873.9840698242188]],\n  'labels': ['a half']}}\nUsing the bboxes rounded coordinates:\ntask_prompt = \"&lt;REGION_TO_SEGMENTATION&gt;\"\nresults = run_example(\n    task_prompt,\n    text_input=(\"&lt;loc_343&gt;&lt;loc_690&gt;\" \"&lt;loc_531&gt;&lt;loc_874&gt;\"),\n    image=table,\n)\noutput_image = copy.deepcopy(table)\ndraw_polygons(\n    output_image, results[\"&lt;REGION_TO_SEGMENTATION&gt;\"], fill_mask=True\n)\nWe got the segmentation of the object on those coordinates (Latency: 83 seconds):\n \n\n\n\nWe can also give the region (coordinates and ask for a caption):\ntask_prompt = \"&lt;REGION_TO_CATEGORY&gt;\"\nresults = run_example(\n    task_prompt,\n    text_input=(\"&lt;loc_343&gt;&lt;loc_690&gt;\" \"&lt;loc_531&gt;&lt;loc_874&gt;\"),\n    image=table,\n)\nresults\n[INFO] ==&gt; Florence-2-base (&lt;REGION_TO_CATEGORY&gt;), \\\ntook 14.3 seconds to execute.\n\n{{\n  '&lt;REGION_TO_CATEGORY&gt;':\n    'orange&lt;loc_343&gt;&lt;loc_690&gt;'\n    '&lt;loc_531&gt;&lt;loc_874&gt;'\n}\nThe model identified an orange in that region. Let‚Äôs ask for a description:\ntask_prompt = \"&lt;REGION_TO_DESCRIPTION&gt;\"\nresults = run_example(\n    task_prompt,\n    text_input=(\"&lt;loc_343&gt;&lt;loc_690&gt;\" \"&lt;loc_531&gt;&lt;loc_874&gt;\"),\n    image=table,\n)\nresults\n[INFO] ==&gt; Florence-2-base (&lt;REGION_TO_CATEGORY&gt;), \\\ntook 14.6 seconds to execute.\n\n{\n  '&lt;REGION_TO_CATEGORY&gt;':\n    'orange&lt;loc_343&gt;&lt;loc_690&gt;'\n    '&lt;loc_531&gt;&lt;loc_874&gt;'\n}\nIn this case, the description did not provide more details, but it could. Try another example.\n\n\n\nWith Florence-2, we can perform Optical Character Recognition (OCR) on an image, getting what is written on it (task_prompt = '&lt;OCR&gt;' and also get the bounding boxes (location) for the detected text (ask_prompt = '&lt;OCR_WITH_REGION&gt;'). Those tasks can help extract and locate textual information in images, such as reading signs, labels, or other forms of text in images.\nLet‚Äôs upload a flyer from a talk in Brazil to Raspi. Let‚Äôs test works in another language, here Portuguese):\nflayer = Image.open(\"./images/embarcados.jpg\")\n# Display the image\nplt.figure(figsize=(8, 8))\nplt.imshow(flayer)\nplt.axis(\"off\")\n# plt.title(\"Image\")\nplt.show()\n \nLet‚Äôs examine the image with '&lt;MORE_DETAILED_CAPTION&gt;' :\n[INFO] ==&gt; Florence-2-base (&lt;MORE_DETAILED_CAPTION&gt;), \\\ntook 85.2 seconds to execute.\n\n{'&lt;MORE_DETAILED_CAPTION&gt;': 'The image is a promotional poster \\\nfor an event called \"Machine Learning Embarcados\" hosted by \\\nMarcelo Roval. The poster has a black background with white \\\ntext. On the left side of the poster, there is a logo of a \\\ncoffee cup with the text \"Caf√© Com Embarcados\" above it. \\\nBelow the logo, it says \"25 de Setembro as 17th\" which \\\ntranslates to \"25th of September as 17\" in English. \\n\\nOn \\\nthe right side, there are two smaller text boxes with the names \\\nof the participants and their names. The first text box reads \\\n\"Democratizando a Intelig√™ncia Artificial para Paises em \\\nDesenvolvimento\" and the second text box says \"Toda \\\nquarta-feira\" which is Portuguese for \"Transmiss√£o via in \\\nPortuguese\".\\n\\nIn the center of the image, there has a photo \\\nof Marcelo, a man with a beard and glasses, smiling at the \\\ncamera. He is wearing a white hard hat and a white shirt. \\\nThe text boxes are in orange and yellow colors.'}\nThe description is very accurate. Let‚Äôs get to the more important words with the task OCR:\ntask_prompt = \"&lt;OCR&gt;\"\nrun_example(task_prompt, image=flayer)\n[INFO] ==&gt; Florence-2-base (&lt;OCR&gt;), took 37.7 seconds to execute.\n\n{'&lt;OCR&gt;':\n 'Machine Learning Caf√© com Embarcado Embarcados '\n 'Democratizando a Intelig√™ncia Artificial para Paises em '\n '25 de Setembro √†s 17h Desenvolvimento Toda quarta-feira '\n 'Marcelo Roval Professor na UNIFIEI e Transmiss√£o via in '\n 'Co-Director do TinyML4D'}\nLet‚Äôs locate the words in the flyer:\ntask_prompt = \"&lt;OCR_WITH_REGION&gt;\"\nresults = run_example(task_prompt, image=flayer)\nLet‚Äôs also create a function to draw bounding boxes around the detected words:\ndef draw_ocr_bboxes(image, prediction):\n    scale = 1\n    draw = ImageDraw.Draw(image)\n    bboxes = prediction[\"quad_boxes\"]\n    labels = prediction[\"labels\"]\n    for box, label in zip(bboxes, labels):\n        color = random.choice(colormap)\n        new_box = (np.array(box) * scale).tolist()\n        draw.polygon(new_box, width=3, outline=color)\n        draw.text(\n            (new_box[0] + 8, new_box[1] + 2),\n            \"{}\".format(label),\n            align=\"right\",\n            fill=color,\n        )\n    display(image)\noutput_image = copy.deepcopy(flayer)\ndraw_ocr_bboxes(output_image, results[\"&lt;OCR_WITH_REGION&gt;\"])\n \nWe can inspect the detected words:\nresults[\"&lt;OCR_WITH_REGION&gt;\"][\"labels\"]\n'&lt;/s&gt;Machine Learning',\n 'Caf√©',\n 'com',\n 'Embarcado',\n 'Embarcados',\n 'Democratizando a Intelig√™ncia',\n 'Artificial para Paises em',\n '25 de Setembro √°s 17h',\n 'Desenvolvimento',\n 'Toda quarta-feira',\n 'Marcelo Roval',\n 'Professor na UNIFIEI e',\n 'Transmiss√£o via',\n 'in',\n 'Co-Director do TinyML4D']\n\n\n\n\nThe latency observed for different tasks using Florence-2 on the Raspberry Pi (Raspi-5) varied depending on the complexity of the task:\n\nImage Captioning: It took approximately 16-17 seconds to generate a caption for an image.\nDetailed Captioning: Increased latency to around 25-27 seconds, requiring generating more nuanced scene descriptions.\nMore Detailed Captioning: It took about 32-50 seconds, and the latency increased as the description grew more complex.\nObject Detection: It took approximately 20-41 seconds, depending on the image‚Äôs complexity and the number of detected objects.\nVisual Grounding: Approximately 15-16 seconds to localize specific objects based on textual prompts.\nOCR (Optical Character Recognition): Extracting text from an image took around 37-38 seconds.\nSegmentation and Region to Segmentation: Segmentation tasks took considerably longer, with a latency of around 83-207 seconds, depending on the complexity and the number of regions to be segmented.\n\nThese latency times highlight the resource constraints of edge devices like the Raspberry Pi and emphasize the need to optimize the model and the environment to achieve real-time performance.\n \n\nRunning complex tasks can use all 8 GB of the Raspi-5‚Äôs memory. For example, the above screenshot during the Florence OD task shows 4 CPUs at full speed and over 5 GB of memory in use. Consider increasing the SWAP memory to 2 GB.\n\nChecking the CPU temperature with vcgencmd measure_temp , showed that temperature can go up to +80oC.\n\n\n\nAs explored in this lab, Florence supports many tasks out of the box, including captioning, object detection, OCR, and more. However, like other pre-trained foundational models, Florence-2 may need domain-specific knowledge. For example, it may need to improve with medical or satellite imagery. In such cases, fine-tuning with a custom dataset is necessary. The Roboflow tutorial, How to Fine-tune Florence-2 for Object Detection Tasks, shows how to fine-tune Florence-2 on object detection datasets to improve model performance for our specific use case.\nBased on the above tutorial, it is possible to fine-tune the Florence-2 model to detect boxes and wheels used in previous labs:\n \nIt is important to note that after fine-tuning, the model can still detect classes that don‚Äôt belong to our custom dataset, like cats, dogs, grapes, etc, as seen before).\nThe complete fine-tuning project using a previously annotated dataset in Roboflow and executed on CoLab can be found in the notebook:\n\n30-Finetune_florence_2_on_detection_dataset_box_vs_wheel.ipynb\n\nIn another example, in the post, Fine-tuning Florence-2 - Microsoft‚Äôs Cutting-edge Vision Language Models, the authors show an example of fine-tuning Florence on DocVQA. The authors report that Florence 2 can perform visual question answering (VQA), but the released models don‚Äôt include VQA capability.\n\n\n\nFlorence-2 offers a versatile and powerful approach to vision-language tasks at the edge, providing performance that rivals larger, task-specific models, such as YOLO for object detection, BERT/RoBERTa for text analysis, and specialized OCR models.\nThanks to its multi-modal transformer architecture, Florence-2 is more flexible than YOLO in terms of the tasks it can handle. These include object detection, image captioning, and visual grounding.\nUnlike BERT, which focuses purely on language, Florence-2 integrates vision and language, allowing it to excel in applications that require both modalities, such as image captioning and visual grounding.\nMoreover, while traditional OCR models such as Tesseract and EasyOCR are designed solely for recognizing and extracting text from images, Florence-2‚Äôs OCR capabilities are part of a broader framework that includes contextual understanding and visual-text alignment. This makes it particularly useful for scenarios that require both reading text and interpreting its context within images.\nOverall, Florence-2 stands out for its ability to seamlessly integrate various vision-language tasks into a unified model that is efficient enough to run on edge devices like the Raspberry Pi. This makes it a compelling choice for developers and researchers exploring AI applications at the edge.\n\n\n\nUnified Architecture\n\nSingle model handles multiple vision tasks vs.¬†specialized models (YOLO, BERT, Tesseract)\nEliminates the need for multiple model deployments and integrations\nConsistent API and interface across tasks\n\nPerformance Comparison\n\nObject Detection: Comparable to YOLOv8 (~37.5 mAP on COCO vs.¬†YOLOv8‚Äôs ~39.7 mAP) despite being general-purpose\nText Recognition: Handles multiple languages effectively like specialized OCR models (Tesseract, EasyOCR)\nLanguage Understanding: Integrates BERT-like capabilities for text processing while adding visual context\n\nResource Efficiency\n\nThe Base model (232M parameters) achieves strong results despite smaller size\nRuns effectively on edge devices (Raspberry Pi)\nSingle model deployment vs.¬†multiple specialized models\n\n\n\n\n\n\nPerformance vs.¬†Specialized Models\n\nYOLO series may offer faster inference for pure object detection\nSpecialized OCR models might handle complex document layouts better\nBERT/RoBERTa provide deeper language understanding for text-only tasks\n\nResource Requirements\n\nHigher latency on edge devices (15-200s depending on task)\nRequires careful memory management on Raspberry Pi\nIt may need optimization for real-time applications\n\nDeployment Considerations\n\nInitial setup is more complex than single-purpose models\nRequires understanding of multiple task types and prompts\nThe learning curve for optimal prompt engineering\n\n\n\n\n\n\nResource-Constrained Environments\n\nEdge devices requiring multiple vision capabilities\nSystems with limited storage/deployment capacity\nApplications needing flexible vision processing\n\nMulti-modal Applications\n\nContent moderation systems\nAccessibility tools\nDocument analysis workflows\n\nRapid Prototyping\n\nQuick deployment of vision capabilities\nTesting multiple vision tasks without separate models\nProof-of-concept development\n\n\n\n\n\n\nFlorence-2 represents a shift toward unified vision models that could eventually replace task-specific architectures in many applications. While specialized models maintain advantages in specific scenarios, the convenience and efficiency of unified models like Florence-2 make them increasingly attractive for real-world deployments.\nThe lab demonstrates Florence-2‚Äôs viability on edge devices, suggesting future IoT, mobile computing, and embedded systems applications where deploying multiple specialized models would be impractical.\n\n\n\n\n10-florence2_test.ipynb\n20-florence_2.ipynb\n30-Finetune_florence_2_on_detection_dataset_box_vs_wheel.ipynb",
    "crumbs": [
      "Raspberry Pi",
      "Vision-Language Models (VLM)"
    ]
  },
  {
    "objectID": "contents/raspi/vlm/vlm.html#sec-visionlanguage-models-vlm-introduction-4272",
    "href": "contents/raspi/vlm/vlm.html#sec-visionlanguage-models-vlm-introduction-4272",
    "title": "Vision-Language Models (VLM)",
    "section": "",
    "text": "In this hands-on lab, we will continuously explore AI applications at the Edge, from the basic setup of the Florence-2, Microsoft‚Äôs state-of-the-art vision foundation model, to advanced implementations on devices like the Raspberry Pi. We will learn to use Vision Language Models (VLMs) for tasks such as captioning, object detection, grounding, segmentation, and OCR on a Raspberry Pi.\n\n\nFlorence-2 is a vision-language model open-sourced by Microsoft under the MIT license, which significantly advances vision-language models by combining a lightweight architecture with robust capabilities. Thanks to its training on the massive FLD-5B dataset, which contains 126 million images and 5.4 billion visual annotations, it achieves performance comparable to larger models. This makes Florence-2 ideal for deployment at the edge, where power and computational resources are limited.\nIn this tutorial, we will explore how to use Florence-2 for real-time computer vision applications, such as:\n\nImage captioning\nObject detection\nSegmentation\nVisual grounding\n\n\nVisual grounding involves linking textual descriptions to specific regions within an image. This enables the model to understand where particular objects or entities described in a prompt are in the image. For example, if the prompt is ‚Äúa red car,‚Äù the model will identify and highlight the region where the red car is found in the image. Visual grounding is helpful for applications where precise alignment between text and visual content is needed, such as human-computer interaction, image annotation, and interactive AI systems.\n\nIn the tutorial, we will walk through:\n\nSetting up Florence-2 on the Raspberry Pi\nRunning inference tasks such as object detection and captioning\nOptimizing the model to get the best performance from the edge device\nExploring practical, real-world applications with fine-tuning.\n\n\n\n\nFlorence-2 utilizes a unified, prompt-based representation to handle various vision-language tasks. The model architecture consists of two main components: an image encoder and a multi-modal transformer encoder-decoder.\n\\noindent\n\\begin{minipage}{\\linewidth}\n\\centering\n\\scalebox{0.8}{%\n\\begin{tikzpicture}[font=\\small\\usefont{T1}{phv}{m}{n}]\n\\tikzset{Line/.style={line width=1.0pt,black!50},\n  Box/.style={inner xsep=2pt,\n    draw=BlueLine,\n    node distance=0.6,\n    line width=0.75pt,\n    fill=BlueL,\n    anchor=west,\n    text width=36mm,\n    align=flush center,\n    minimum width=36mm,\n    minimum height=8mm\n  },\n}\n\n\\node[Box,draw=RedLine, fill=RedL](B1){Input};\n\\node[Box,below left=of B1](B2){Image};\n\\node[Box,below right =of B1](B3){Text Prompt};\n\\node[Box,below=of B2](B4){Image Encoder (DaVIT)};\n\\node[Box,below=of B3](B5){Text Tokenize};\n\\node[Box,below=of $(B4)!0.5!(B5)$](B6){Multimodality Encoder (Transformer)};\n\\node[Box,below=of B6](B7){Multimodality Decoder (Transformer)};\n\\node[Box,below=of B7,draw=RedLine, fill=RedL](B8){Output (Text/Coordinates)};\n%\n\\draw[Line,-latex](B1)-|(B2);\n\\draw[Line,-latex](B1)-|(B3);\n\\draw[Line,-latex](B2)--(B4);\n\\draw[Line,-latex](B3)--(B5);\n\\draw[Line,-latex](B4)|-(B6);\n\\draw[Line,-latex](B5)|-(B6);\n\\draw[Line,-latex](B6)--(B7);\n\\draw[Line,-latex](B7)--(B8);\n\\end{tikzpicture}}\n\\end{minipage}\n\nImage Encoder: The image encoder is based on the DaViT (Dual Attention Vision Transformers) architecture. It converts input images into a series of visual token embeddings. These embeddings serve as the foundational representations of the visual content, capturing both spatial and contextual information about the image.\nMulti-Modal Transformer Encoder-Decoder: Florence-2‚Äôs core is the multi-modal transformer encoder-decoder, which combines visual token embeddings from the image encoder with textual embeddings generated by a BERT-like model. This combination allows the model to simultaneously process visual and textual inputs, enabling a unified approach to tasks such as image captioning, object detection, and segmentation.\n\nThe model‚Äôs training on the extensive FLD-5B dataset ensures it can effectively handle diverse vision tasks without requiring task-specific modifications. Florence-2 uses textual prompts to activate specific tasks, making it highly flexible and capable of zero-shot generalization. For tasks like object detection or visual grounding, the model incorporates additional location tokens to represent regions within the image, ensuring a precise understanding of spatial relationships.\n\nFlorence-2‚Äôs compact architecture and innovative training approach allow it to perform computer vision tasks accurately, even on resource-constrained devices like the Raspberry Pi.",
    "crumbs": [
      "Raspberry Pi",
      "Vision-Language Models (VLM)"
    ]
  },
  {
    "objectID": "contents/raspi/vlm/vlm.html#sec-visionlanguage-models-vlm-technical-overview-faee",
    "href": "contents/raspi/vlm/vlm.html#sec-visionlanguage-models-vlm-technical-overview-faee",
    "title": "Vision-Language Models (VLM)",
    "section": "",
    "text": "Florence-2 introduces several innovative features that set it apart:\n\n\n \n\nLightweight Design: Two variants available\n\nFlorence-2-Base: 232 million parameters\nFlorence-2-Large: 771 million parameters\n\nUnified Representation: Handles multiple vision tasks through a single architecture\nDaViT Vision Encoder: Converts images into visual token embeddings\nTransformer-based Multi-modal Encoder-Decoder: Processes combined visual and text embeddings\n\n\n\n\n \n\n126 million unique images\n5.4 billion comprehensive annotations, including:\n\n500M text annotations\n1.3B region-text annotations\n3.6B text-phrase-region annotations\n\nAutomated annotation pipeline using specialist models\nIterative refinement process for high-quality labels\n\n\n\n\nFlorence-2 excels in multiple vision tasks:\n\n\n\nImage Captioning: Achieves 135.6 CIDEr score on COCO\nVisual Grounding: 84.4% recall@1 on Flickr30k\nObject Detection: 37.5 mAP on COCO val2017\nReferring Expression: 67.0% accuracy on RefCOCO\n\n\n\n\n\nCompetitive with specialist models despite the smaller size\nOutperforms larger models in specific benchmarks\nEfficient adaptation to new tasks\n\n\n\n\n\nFlorence-2 can be applied across various domains:\n\nContent Understanding\n\nAutomated image captioning for accessibility\nVisual content moderation\nMedia asset management\n\nE-commerce\n\nProduct image analysis\nVisual search\nAutomated product tagging\n\nHealthcare\n\nMedical image analysis\nDiagnostic assistance\nResearch data processing\n\nSecurity & Surveillance\n\nObject detection and tracking\nAnomaly detection\nScene understanding\n\n\n\n\n\nFlorence-2 stands out from other visual language models due to its impressive zero-shot capabilities. Unlike models like Google PaliGemma, which rely on extensive fine-tuning to adapt to various tasks, Florence-2 works right out of the box, as we will see in this lab. It can also compete with larger models like GPT-4V and Flamingo, which often have many more parameters but only sometimes match Florence-2‚Äôs performance. For example, Florence-2 achieves better zero-shot results than Kosmos-2 despite having over twice the parameters.\nIn benchmark tests, Florence-2 has shown remarkable performance in tasks like COCO captioning and referring expression comprehension. It outperformed models like PolyFormer and UNINEXT in object detection and segmentation tasks on the COCO dataset. It is a highly competitive choice for real-world applications where both performance and resource efficiency are crucial.",
    "crumbs": [
      "Raspberry Pi",
      "Vision-Language Models (VLM)"
    ]
  },
  {
    "objectID": "contents/raspi/vlm/vlm.html#sec-visionlanguage-models-vlm-setup-installation-78de",
    "href": "contents/raspi/vlm/vlm.html#sec-visionlanguage-models-vlm-setup-installation-78de",
    "title": "Vision-Language Models (VLM)",
    "section": "",
    "text": "Our choice of edge device is the Raspberry Pi 5 (Raspi-5). Its robust platform is equipped with the Broadcom BCM2712, a 2.4 GHz quad-core 64-bit Arm Cortex-A76 CPU featuring Cryptographic Extension and enhanced caching capabilities. It boasts a VideoCore VII GPU, dual 4Kp60 HDMI¬Æ outputs with HDR, and a 4Kp60 HEVC decoder. Memory options include 4 GB and 8 GB of high-speed LPDDR4X SDRAM, with 8 GB being our choice to run Florence-2. It also features expandable storage via a microSD card slot and a PCIe 2.0 interface for fast peripherals such as M.2 SSDs (Solid State Drives).\n\nFor real applications, SSDs are a better option than SD cards.\n\nWe suggest installing an Active Cooler, a dedicated clip-on cooling solution for Raspberry Pi 5 (Raspi-5), for this lab. It combines an aluminum heat sink with a temperature-controlled blower fan to keep the Raspi-5 operating comfortably under heavy loads, such as running Florense-2.\n \n\n\nTo run Microsoft Florense-2 on the Raspberry Pi 5, we‚Äôll need a few libraries:\n\nTransformers:\n\nFlorence-2 uses the transformers library from Hugging Face for model loading and inference. This library provides the architecture for working with pre-trained vision-language models, making it easy to perform tasks like image captioning, object detection, and more. Essentially, transformers helps in interacting with the model, processing input prompts, and obtaining outputs.\n\nPyTorch:\n\nPyTorch is a deep learning framework that provides the infrastructure needed to run the Florence-2 model, which includes tensor operations, GPU acceleration (if a GPU is available), and model training/inference functionalities. The Florence-2 model is trained in PyTorch, and we need it to leverage its functions, layers, and computation capabilities to perform inferences on the Raspberry Pi.\n\nTimm (PyTorch Image Models):\n\nFlorence-2 uses timm to access efficient implementations of vision models and pre-trained weights. Specifically, the timm library is utilized for the image encoder part of Florence-2, particularly for managing the DaViT architecture. It provides model definitions and optimized code for common vision tasks and allows the easy integration of different backbones that are lightweight and suitable for edge devices.\n\nEinops:\n\nEinops is a library for flexible and powerful tensor operations. It makes it easy to reshape and manipulate tensor dimensions, which is especially important for the multi-modal processing done in Florence-2. Vision-language models like Florence-2 often need to rearrange image data, text embeddings, and visual embeddings to align correctly for the transformer blocks, and einops simplifies these complex operations, making the code more readable and concise.\n\n\nIn short, these libraries enable different essential components of Florence-2:\n\nTransformers and PyTorch are needed to load the model and run the inference.\nTimm is used to access and efficiently implement the vision encoder.\nEinops helps reshape data, facilitating the integration of visual and text features.\n\nAll these components work together to help Florence-2 run seamlessly on our Raspberry Pi, allowing it to perform complex vision-language tasks relatively quickly.\nConsidering that the Raspberry Pi already has its OS installed, let‚Äôs use SSH to reach it from another computer:\nssh mjrovai@raspi-5.local\nAnd check the IP allocated to it:\nhostname -I\n192.168.4.209\n \nUpdating the Raspberry Pi\nFirst, ensure your Raspberry Pi is up to date:\nsudo apt update\nsudo apt upgrade -y\nInitial setup for using PIP:\nsudo apt install python3-pip\nsudo rm /usr/lib/python3.11/EXTERNALLY-MANAGED\npip3 install --upgrade pip\nInstall Dependencies\nsudo apt-get install libjpeg-dev libopenblas-dev libopenmpi-dev \\\n    libomp-dev\nLet‚Äôs set up and activate a Virtual Environment for working with Florence-2:\npython3 -m venv ~/florence\nsource ~/florence/bin/activate\nInstall PyTorch\npip3 install setuptools numpy Cython\npip3 install requests\npip3 install torch torchvision \\\n    --index-url https://download.pytorch.org/whl/cpu\npip3 install torchaudio \\\n    --index-url https://download.pytorch.org/whl/cpu\nLet‚Äôs verify that PyTorch is correctly installed:\n \nInstall Transformers, Timm and Einops:\npip3 install transformers\npip3 install timm einops\nInstall the model:\npip3 install autodistill-florence-2\nJupyter Notebook and Python libraries\nInstalling a Jupyter Notebook to run and test our Python scripts is possible.\npip3 install jupyter\npip3 install numpy Pillow matplotlib\njupyter notebook --generate-config\n\n\n\nRunning the Jupyter Notebook on the remote computer\njupyter notebook --ip=192.168.4.209 --no-browser\nRunning the above command on the SSH terminal, we can see the local URL address to open the notebook:\n \nThe notebook with the code used on this initial test can be found on the Lab GitHub:\n\n10-florence2_test.ipynb\n\nWe can access it on the remote computer by entering the Raspberry Pi‚Äôs IP address and the provided token in a web browser (copy the entire URL from the terminal).\nFrom the Home page, create a new notebook [Python 3 (ipykernel) ] and copy and paste the example code from Hugging Face Hub.\nThe code is designed to run Florence-2 on a given image to perform object detection. It loads the model, processes an image and a prompt, and then generates a response to identify and describe the objects in the image.\n\nThe processor helps prepare text and image inputs.\nThe model takes the processed inputs to generate a meaningful response.\nThe post-processing step refines the generated output into a more interpretable form, like bounding boxes for detected objects.\n\n\nThis workflow leverages the versatility of Florence-2 to handle vision-language tasks and is implemented efficiently using PyTorch, Transformers, and related image-processing tools.\n\nimport requests\nfrom PIL import Image\nimport torch\nfrom transformers import AutoProcessor, AutoModelForCausalLM\n\ndevice = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\ntorch_dtype = (\n    torch.float16 if torch.cuda.is_available() else torch.float32\n)\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    \"microsoft/Florence-2-base\",\n    torch_dtype=torch_dtype,\n    trust_remote_code=True,\n).to(device)\nprocessor = AutoProcessor.from_pretrained(\n    \"microsoft/Florence-2-base\", trust_remote_code=True\n)\n\nprompt = \"&lt;OD&gt;\"\n\nurl = (\n    \"https://huggingface.co/datasets/huggingface/\"\n    \"documentation-images/resolve/main/transformers/\"\n    \"tasks/car.jpg?download=true\"\n)\nimage = Image.open(requests.get(url, stream=True).raw)\n\ninputs = processor(text=prompt, images=image, return_tensors=\"pt\").to(\n    device, torch_dtype\n)\n\ngenerated_ids = model.generate(\n    input_ids=inputs[\"input_ids\"],\n    pixel_values=inputs[\"pixel_values\"],\n    max_new_tokens=1024,\n    do_sample=False,\n    num_beams=3,\n)\ngenerated_text = processor.batch_decode(\n    generated_ids, skip_special_tokens=False\n)[0]\n\nparsed_answer = processor.post_process_generation(\n    generated_text,\n    task=\"&lt;OD&gt;\",\n    image_size=(image.width, image.height),\n)\n\nprint(parsed_answer)\nLet‚Äôs break down the provided code step by step:\n\n\nimport requests\nfrom PIL import Image\nimport torch\nfrom transformers import AutoProcessor, AutoModelForCausalLM\n\nrequests: Used to make HTTP requests. In this case, it downloads an image from a URL.\nPIL (Pillow): Provides tools for manipulating images. Here, it‚Äôs used to open the downloaded image.\ntorch: PyTorch is imported to handle tensor operations and determine the hardware availability (CPU or GPU).\ntransformers: This module provides easy access to Florence-2 by using AutoProcessor and AutoModelForCausalLM to load pre-trained models and process inputs.\n\n\n\n\ndevice = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n\ntorch_dtype = (\n    torch.float16 if torch.cuda.is_available() else torch.float32\n)\n\nDevice Setup: The code checks if a CUDA-enabled GPU is available (torch.cuda.is_available()). The device is set to ‚Äúcuda:0‚Äù if a GPU is available. Otherwise, it defaults to \"cpu\" (our case here).\nData Type Setup: If a GPU is available, torch.float16 is chosen, which uses half-precision floats to speed up processing and reduce memory usage. On the CPU, it defaults to torch.float32 to maintain compatibility.\n\n\n\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    \"microsoft/Florence-2-base\",\n    torch_dtype=torch_dtype,\n    trust_remote_code=True,\n).to(device)\n\nprocessor = AutoProcessor.from_pretrained(\n    \"microsoft/Florence-2-base\", trust_remote_code=True\n)\n\nModel Initialization:\n\nAutoModelForCausalLM.from_pretrained() loads the pre-trained Florence-2 model from Microsoft‚Äôs repository on Hugging Face. The torch_dtype is set according to the available hardware (GPU/CPU), and trust_remote_code=True allows the use of any custom code that might be provided with the model.\n.to(device) moves the model to the appropriate device (either CPU or GPU). In our case, it will be set to CPU.\n\nProcessor Initialization:\n\nAutoProcessor.from_pretrained() loads the processor for Florence-2. The processor is responsible for transforming text and image inputs into a format the model can work with (e.g., encoding text, normalizing images, etc.).\n\n\n\n\n\n\nprompt = \"&lt;OD&gt;\"\n\nPrompt Definition: The string \"&lt;OD&gt;\" is used as a prompt. This refers to ‚ÄúObject Detection‚Äù, instructing the model to detect objects on the image.\n\n\n\nurl = \"https://huggingface.co/datasets/huggingface/\"\n      \"documentation-images/resolve/main/transformers/\"\n      \"tasks/car.jpg?download=true\"\nimage = Image.open(requests.get(url, stream=True).raw)\n\nDownloading the Image: The requests.get() function fetches the image from the specified URL. The stream=True parameter ensures the image is streamed rather than downloaded completely at once.\nOpening the Image: Image.open() opens the image so the model can process it.\n\n\n\n\ninputs = processor(text=prompt, images=image, return_tensors=\"pt\").to(\n    device, torch_dtype\n)\n\nProcessing Input Data: The processor() function processes the text (prompt) and the image (image). The return_tensors=\"pt\" argument converts the processed data into PyTorch tensors, which are necessary for inputting data into the model.\nMoving Inputs to Device: .to(device, torch_dtype) moves the inputs to the correct device (CPU or GPU) and assigns the appropriate data type.\n\n\n\n\n\ngenerated_ids = model.generate(\n    input_ids=inputs[\"input_ids\"],\n    pixel_values=inputs[\"pixel_values\"],\n    max_new_tokens=1024,\n    do_sample=False,\n    num_beams=3,\n)\n\nModel Generation: model.generate() is used to generate the output based on the input data.\n\ninput_ids: Represents the tokenized form of the prompt.\npixel_values: Contains the processed image data.\nmax_new_tokens=1024: Specifies the maximum number of new tokens to be generated in the response. This limits the response length.\ndo_sample=False: Disables sampling; instead, the generation uses deterministic methods (beam search).\nnum_beams=3: Enables beam search with three beams, which improves output quality by considering multiple possibilities during generation.\n\n\n\n\ngenerated_text = processor.batch_decode(\n    generated_ids, skip_special_tokens=False\n)[0]\n\nBatch Decode: processor.batch_decode() decodes the generated IDs (tokens) into readable text. The skip_special_tokens=False parameter means that the output will include any special tokens that may be part of the response.\n\n\n\n\nparsed_answer = processor.post_process_generation(\n    generated_text,\n    task=\"&lt;OD&gt;\",\n    image_size=(image.width, image.height),\n)\n\nPost-Processing: processor.post_process_generation() is called to process the generated text further, interpreting it based on the task (\"&lt;OD&gt;\" for object detection) and the size of the image.\nThis function extracts specific information from the generated text, such as bounding boxes for detected objects, making the output more useful for visual tasks.\n\n\n\n\nprint(parsed_answer)\n\nFinally, print(parsed_answer) displays the output, which could include object detection results, such as bounding box coordinates and labels for the detected objects in the image.\n\n\n\n\nRunning the code, we get as the Parsed Answer:\n[{'&lt;OD&gt;': {\n   'bboxes': [\n     [34.23999786376953, 160.0800018310547, 597.4400024414062],\n     [371.7599792480469, 272.32000732421875, 241.67999267578125],\n     [303.67999267578125, 247.4399871826172, 454.0799865722656],\n     [276.7200012207031, 553.9199829101562, 370.79998779296875],\n     [96.31999969482422, 280.55999755859375, 198.0800018310547],\n     [371.2799987792969]\n    ],\n    'labels': ['car', 'door handle', 'wheel', 'wheel']\n}}]\nFirst, let‚Äôs inspect the image:\nimport matplotlib.pyplot as plt\n\nplt.figure(figsize=(8, 8))\nplt.imshow(image)\nplt.axis(\"off\")\nplt.show()\n \nBy the Object Detection result, we can see that:\n'labels': ['car', 'door handle', 'wheel', 'wheel']\nIt seems that at least a few objects were detected. We can also implement a code to draw the bounding boxes in the find objects:\ndef plot_bbox(image, data):\n    # Create a figure and axes\n    fig, ax = plt.subplots()\n\n    # Display the image\n    ax.imshow(image)\n\n    # Plot each bounding box\n    for bbox, label in zip(data[\"bboxes\"], data[\"labels\"]):\n        # Unpack the bounding box coordinates\n        x1, y1, x2, y2 = bbox\n        # Create a Rectangle patch\n        rect = patches.Rectangle(\n            (x1, y1),\n            x2 - x1,\n            y2 - y1,\n            linewidth=1,\n            edgecolor=\"r\",\n            facecolor=\"none\",\n        )\n        # Add the rectangle to the Axes\n        ax.add_patch(rect)\n        # Annotate the label\n        plt.text(\n            x1,\n            y1,\n            label,\n            color=\"white\",\n            fontsize=8,\n            bbox=dict(facecolor=\"red\", alpha=0.5),\n        )\n\n    # Remove the axis ticks and labels\n    ax.axis(\"off\")\n\n    # Show the plot\n    plt.show()\n\nBox (x0, y0, x1, y1): Location tokens correspond to the top-left and bottom-right corners of a box.\n\nAnd running\nplot_bbox(image, parsed_answer['&lt;OD&gt;'])\nWe get:",
    "crumbs": [
      "Raspberry Pi",
      "Vision-Language Models (VLM)"
    ]
  },
  {
    "objectID": "contents/raspi/vlm/vlm.html#sec-visionlanguage-models-vlm-florence2-tasks-bcbb",
    "href": "contents/raspi/vlm/vlm.html#sec-visionlanguage-models-vlm-florence2-tasks-bcbb",
    "title": "Vision-Language Models (VLM)",
    "section": "",
    "text": "Florence-2 is designed to perform a variety of computer vision and vision-language tasks through prompts. These tasks can be activated by providing a specific textual prompt to the model, as we saw with &lt;OD&gt; (Object Detection).\nFlorence-2‚Äôs versatility comes from combining these prompts, allowing us to guide the model‚Äôs behavior to perform specific vision tasks. Changing the prompt allows us to adapt Florence-2 to different tasks without needing task-specific modifications in the architecture. This capability directly results from Florence-2‚Äôs unified model architecture and large-scale multi-task training on the FLD-5B dataset.\nHere are some of the key tasks that Florence-2 can perform, along with example prompts:\n\n\n\nPrompt: \"&lt;OD&gt;\"\nDescription: Identifies objects in an image and provides bounding boxes for each detected object. This task is helpful for applications like visual inspection, surveillance, and general object recognition.\n\n\n\n\n\nPrompt: \"&lt;CAPTION&gt;\"\nDescription: Generates a textual description for an input image. This task helps the model describe what is happening in the image, providing a human-readable caption for content understanding.\n\n\n\n\n\nPrompt: \"&lt;DETAILED_CAPTION&gt;\"\nDescription: Generates a more detailed caption with more nuanced information about the scene, such as the objects present and their relationships.\n\n\n\n\n\nPrompt: \"&lt;CAPTION_TO_PHRASE_GROUNDING&gt;\"\nDescription: Links a textual description to specific regions in an image. For example, given a prompt like ‚Äúa green car,‚Äù the model highlights where the green car is in the image. This is useful for human-computer interaction, where you must find specific objects based on text.\n\n\n\n\n\nPrompt: \"&lt;REFERRING_EXPRESSION_SEGMENTATION&gt;\"\nDescription: Performs segmentation based on a referring expression, such as ‚Äúthe blue cup.‚Äù The model identifies and segments the specific region containing the object mentioned in the prompt (all related pixels).\n\n\n\n\n\nPrompt: \"&lt;DENSE_REGION_CAPTION&gt;\"\nDescription: Provides captions for multiple regions within an image, offering a detailed breakdown of all visible areas, including different objects and their relationships.\n\n\n\n\n\nPrompt: \"&lt;OCR_WITH_REGION&gt;\"\nDescription: Performs Optical Character Recognition (OCR) on an image and provides bounding boxes for the detected text. This is useful for extracting and locating textual information in images, such as reading signs, labels, or other forms of text in images.\n\n\n\n\n\nPrompt: \"&lt;CAPTION_TO_PHRASE_GROUNDING&gt;\" along with a specific expression, such as \"a wine glass\".\nDescription: Locates the area in the image that corresponds to a specific textual phrase. This task allows for identifying particular objects or elements when prompted with a word or keyword.\n\n\n\n\n\nPrompt: \"&lt;OPEN_VOCABULARY_OD&gt;\"\nDescription: The model can detect objects without being restricted to a predefined list of classes, making it helpful in recognizing a broader range of items based on general visual understanding.",
    "crumbs": [
      "Raspberry Pi",
      "Vision-Language Models (VLM)"
    ]
  },
  {
    "objectID": "contents/raspi/vlm/vlm.html#sec-visionlanguage-models-vlm-exploring-computer-vision-visionlanguage-tasks-5c1d",
    "href": "contents/raspi/vlm/vlm.html#sec-visionlanguage-models-vlm-exploring-computer-vision-visionlanguage-tasks-5c1d",
    "title": "Vision-Language Models (VLM)",
    "section": "",
    "text": "For exploration, all codes can be found on the GitHub:\n\n20-florence_2.ipynb\n\nLet‚Äôs use a couple of images created by Dall-E and upload them to the Rasp-5 (FileZilla can be used for that). The images will be saved on a sub-folder named images :\ndogs_cats = Image.open(\"./images/dogs-cats.jpg\")\ntable = Image.open(\"./images/table.jpg\")\n \nLet‚Äôs create a function to facilitate our exploration and to keep track of the latency of the model for different tasks:\ndef run_example(task_prompt, text_input=None, image=None):\n    start_time = time.perf_counter()  # Start timing\n    if text_input is None:\n        prompt = task_prompt\n    else:\n        prompt = task_prompt + text_input\n    inputs = processor(\n        text=prompt, images=image, return_tensors=\"pt\"\n    ).to(device)\n    generated_ids = model.generate(\n        input_ids=inputs[\"input_ids\"],\n        pixel_values=inputs[\"pixel_values\"],\n        max_new_tokens=1024,\n        early_stopping=False,\n        do_sample=False,\n        num_beams=3,\n    )\n    generated_text = processor.batch_decode(\n        generated_ids, skip_special_tokens=False\n    )[0]\n    parsed_answer = processor.post_process_generation(\n        generated_text,\n        task=task_prompt,\n        image_size=(image.width, image.height),\n    )\n\n    end_time = time.perf_counter()  # End timing\n    elapsed_time = end_time - start_time  # Calculate elapsed time\n    print(\n        f\" \\n[INFO] ==&gt; Florence-2-base ({task_prompt}), \\\n          took {elapsed_time:.1f} seconds to execute.\\n\"\n    )\n\n    return parsed_answer\n\n\n1. Dogs and Cats\nrun_example(task_prompt=\"&lt;CAPTION&gt;\", image=dogs_cats)\n[INFO] ==&gt; Florence-2-base (&lt;CAPTION&gt;), \\\ntook 16.1 seconds to execute.\n\n{'&lt;CAPTION&gt;': 'A group of dogs and cats sitting in a garden.'}\n2. Table\nrun_example(task_prompt=\"&lt;CAPTION&gt;\", image=table)\n[INFO] ==&gt; Florence-2-base (&lt;CAPTION&gt;), \\\ntook 16.5 seconds to execute.\n\n{'&lt;CAPTION&gt;': 'A wooden table topped with a plate of fruit \\\nand a glass of wine.'}\n\n\n\n1. Dogs and Cats\nrun_example(task_prompt=\"&lt;DETAILED_CAPTION&gt;\", image=dogs_cats)\n[INFO] ==&gt; Florence-2-base (&lt;DETAILED_CAPTION&gt;), \\\ntook 25.5 seconds to execute.\n\n{'&lt;DETAILED_CAPTION&gt;': 'The image shows a group of cats and \\\ndogs sitting on top of a lush green field, surrounded by plants \\\nwith flowers, trees, and a house in the background. The sky is \\\nvisible above them, creating a peaceful atmosphere.'}\n2. Table\nrun_example(task_prompt=\"&lt;DETAILED_CAPTION&gt;\", image=table)\n[INFO] ==&gt; Florence-2-base (&lt;DETAILED_CAPTION&gt;), \\\ntook 26.8 seconds to execute.\n\n{'&lt;DETAILED_CAPTION&gt;': 'The image shows a wooden table with \\\na bottle of wine and a glass of wine on it, surrounded by \\\na variety of fruits such as apples, oranges, and grapes. \\\nIn the background, there are chairs, plants, trees, and \\\na house, all slightly blurred.'}\n\n\n\n1. Dogs and Cats\nrun_example(task_prompt=\"&lt;MORE_DETAILED_CAPTION&gt;\", image=dogs_cats)\n[INFO] ==&gt; Florence-2-base (&lt;MORE_DETAILED_CAPTION&gt;), \\\ntook 49.8 seconds to execute.\n\n{'&lt;MORE_DETAILED_CAPTION&gt;': 'The image shows a group of four \\\ncats and a dog in a garden. The garden is filled with colorful \\\nflowers and plants, and there is a pathway leading up to \\\na house in the background. The main focus of the image is \\\na large German Shepherd dog standing on the left side of \\\nthe garden, with its tongue hanging out and its mouth open, \\\nas if it is panting. On the right side, there are \\\ntwo smaller cats, one orange and one gray, sitting on the \\\ngrass. In the background, there is another golden retriever \\\ndog sitting and looking at the camera. The sky is blue and \\\nthe sun is shining, creating a warm and inviting atmosphere.'}\n2. Table\nrun_example(task_prompt=\"&lt; MORE_DETAILED_CAPTION&gt;\", image=table)\nINFO] ==&gt; Florence-2-base (&lt;MORE_DETAILED_CAPTION&gt;), \\\ntook 32.4 seconds to execute.\n\n{'&lt;MORE_DETAILED_CAPTION&gt;': 'The image shows a wooden table \\\nwith a wooden tray on it. On the tray, there are various \\\nfruits such as grapes, oranges, apples, and grapes. There \\\nis also a bottle of red wine on the table. The background \\\nshows a garden with trees and a house. The overall mood \\\nof the image is peaceful and serene.'}\n\nWe can note that the more detailed the caption task, the longer the latency and the possibility of mistakes (like ‚ÄúThe image shows a group of four cats and a dog in a garden‚Äù, instead of two dogs and three cats).\n\n\n\n\nWe can run the same previous function for object detection using the prompt &lt;OD&gt;.\ntask_prompt = \"&lt;OD&gt;\"\nresults = run_example(task_prompt, image=dogs_cats)\nprint(results)\nLet‚Äôs see the result:\n[INFO] ==&gt; Florence-2-base (&lt;OD&gt;), took 20.9 seconds to execute.\n\n{'&lt;OD&gt;': {'bboxes': [\n  [737.79, 571.90, 1022.46, 980.48],\n  [0.51, 593.40, 211.45, 991.74],\n  [445.95, 721.40, 680.44, 850.43],\n  [39.42, 91.64, 491.00, 933.37],\n  [570.88, 184.83, 974.33, 782.84]\n  ],\n  'labels': ['cat', 'cat', 'cat', 'dog', 'dog']\n}}\nOnly by the labels ['cat,' 'cat,' 'cat,' 'dog,' 'dog'] is it possible to see that the main objects in the image were captured. Let‚Äôs apply the function used before to draw the bounding boxes:\nplot_bbox(dogs_cats, results[\"&lt;OD&gt;\"])\n \nLet‚Äôs also do it with the Table image:\ntask_prompt = \"&lt;OD&gt;\"\nresults = run_example(task_prompt, image=table)\nplot_bbox(table, results[\"&lt;OD&gt;\"])\n[INFO] ==&gt; Florence-2-base (&lt;OD&gt;), took 40.8 seconds to execute.\n \n\n\n\nIt is possible to mix the classic Object Detection with the Caption task in specific sub-regions of the image:\ntask_prompt = \"&lt;DENSE_REGION_CAPTION&gt;\"\n\nresults = run_example(task_prompt, image=dogs_cats)\nplot_bbox(dogs_cats, results[\"&lt;DENSE_REGION_CAPTION&gt;\"])\n\nresults = run_example(task_prompt, image=table)\nplot_bbox(table, results[\"&lt;DENSE_REGION_CAPTION&gt;\"])\n \n\n\n\nWith this task, we can enter with a caption, such as ‚Äúa wine glass‚Äù, ‚Äúa wine bottle,‚Äù or ‚Äúa half orange,‚Äù and Florence-2 will localize the object in the image:\ntask_prompt = \"&lt;CAPTION_TO_PHRASE_GROUNDING&gt;\"\n\nresults = run_example(\n    task_prompt, text_input=\"a wine bottle\", image=table\n)\nplot_bbox(table, results[\"&lt;CAPTION_TO_PHRASE_GROUNDING&gt;\"])\n\nresults = run_example(\n    task_prompt, text_input=\"a wine glass\", image=table\n)\nplot_bbox(table, results[\"&lt;CAPTION_TO_PHRASE_GROUNDING&gt;\"])\n\nresults = run_example(\n    task_prompt, text_input=\"a half orange\", image=table\n)\nplot_bbox(table, results[\"&lt;CAPTION_TO_PHRASE_GROUNDING&gt;\"])\n \n[INFO] ==&gt; Florence-2-base (&lt;CAPTION_TO_PHRASE_GROUNDING&gt;), \\\ntook 15.7 seconds to execute\neach task.\n\n\n\nWe can also enter the image caption as the input text to push Florence-2 to find more objects:\ntask_prompt = \"&lt;CAPTION&gt;\"\nresults = run_example(task_prompt, image=dogs_cats)\ntext_input = results[task_prompt]\ntask_prompt = \"&lt;CAPTION_TO_PHRASE_GROUNDING&gt;\"\nresults = run_example(task_prompt, text_input, image=dogs_cats)\nplot_bbox(dogs_cats, results[\"&lt;CAPTION_TO_PHRASE_GROUNDING&gt;\"])\nChanging the task_prompt among &lt;CAPTION,&gt; &lt;DETAILED_CAPTION&gt; and &lt;MORE_DETAILED_CAPTION&gt;, we will get more objects in the image.\n \n\n\n\n&lt;OPEN_VOCABULARY_DETECTION&gt; allows Florence-2 to detect recognizable objects in an image without relying on a predefined list of categories, making it a versatile tool for identifying various items that may not have been explicitly labeled during training. Unlike &lt;CAPTION_TO_PHRASE_GROUNDING&gt;, which requires a specific text phrase to locate and highlight a particular object in an image, &lt;OPEN_VOCABULARY_DETECTION&gt; performs a broad scan to find and classify all objects present.\nThis makes &lt;OPEN_VOCABULARY_DETECTION&gt; particularly useful for applications where you need a comprehensive overview of everything in an image without prior knowledge of what to expect. Enter with a text describing specific objects not previously detected, resulting in their detection. For example:\ntask_prompt = \"&lt;OPEN_VOCABULARY_DETECTION&gt;\"\n\ntext = [\n    \"a house\",\n    \"a tree\",\n    \"a standing cat at the left\",\n    \"a sleeping cat on the ground\",\n    \"a standing cat at the right\",\n    \"a yellow cat\",\n]\n\nfor txt in text:\n    results = run_example(\n        task_prompt, text_input=txt, image=dogs_cats\n    )\n\n    bbox_results = convert_to_od_format(\n        results[\"&lt;OPEN_VOCABULARY_DETECTION&gt;\"]\n    )\n\n    plot_bbox(dogs_cats, bbox_results)\n \n[INFO] ==&gt; Florence-2-base (&lt;OPEN_VOCABULARY_DETECTION&gt;), \\\ntook 15.1 seconds to execute\neach task.\n\nNote: Trying to use Florence-2 to find objects that were not found can leads to mistakes (see examples on the Notebook).\n\n\n\n\nWe can also segment a specific object in the image and give its description (caption), such as ‚Äúa wine bottle‚Äù on the table image or ‚Äúa German Sheppard‚Äù on the dogs_cats.\nReferring expression segmentation results format: {'&lt;REFERRING_EXPRESSION_SEGMENTATION&gt;': {'Polygons': [[[polygon]], ...], 'labels': ['', '', ...]}}, one object is represented by a list of polygons. each polygon is [x1, y1, x2, y2, ..., xn, yn].\n\nPolygon (x1, y1, ‚Ä¶, xn, yn): Location tokens represent the vertices of a polygon in clockwise order.\n\nSo, let‚Äôs first create a function to plot the segmentation:\nfrom PIL import Image, ImageDraw, ImageFont\nimport copy\nimport random\nimport numpy as np\n\ncolormap = [\n    \"blue\",\n    \"orange\",\n    \"green\",\n    \"purple\",\n    \"brown\",\n    \"pink\",\n    \"gray\",\n    \"olive\",\n    \"cyan\",\n    \"red\",\n    \"lime\",\n    \"indigo\",\n    \"violet\",\n    \"aqua\",\n    \"magenta\",\n    \"coral\",\n    \"gold\",\n    \"tan\",\n    \"skyblue\",\n]\n\n\ndef draw_polygons(image, prediction, fill_mask=False):\n    \"\"\"\n        Draws segmentation masks with polygons on an image.\n\n        Parameters:\n          - image_path: Path to the image file.\n          - prediction: Dictionary containing 'polygons' and 'labels'\n                  keys. 'polygons' is a list of lists, each\n                  containing vertices of a polygon. 'labels' is\n                  a list of labels corresponding to each polygon.\n    - fill_mask: Boolean indicating whether to fill the polygons\n                 with color.\n    \"\"\"\n    # Load the image\n\n    draw = ImageDraw.Draw(image)\n\n    # Set up scale factor if needed (use 1 if not scaling)\n    scale = 1\n\n    # Iterate over polygons and labels\n    for polygons, label in zip(\n        prediction[\"polygons\"], prediction[\"labels\"]\n    ):\n        color = random.choice(colormap)\n        fill_color = random.choice(colormap) if fill_mask else None\n\n        for _polygon in polygons:\n            _polygon = np.array(_polygon).reshape(-1, 2)\n            if len(_polygon) &lt; 3:\n                print(\"Invalid polygon:\", _polygon)\n                continue\n\n            _polygon = (_polygon * scale).reshape(-1).tolist()\n\n            # Draw the polygon\n            if fill_mask:\n                draw.polygon(_polygon, outline=color, fill=fill_color)\n            else:\n                draw.polygon(_polygon, outline=color)\n\n            # Draw the label text\n            draw.text(\n                (_polygon[0] + 8, _polygon[1] + 2), label, fill=color\n            )\n\n    # Save or display the image\n    # image.show()  # Display the image\n    display(image)\nNow we can run the functions:\ntask_prompt = \"&lt;REFERRING_EXPRESSION_SEGMENTATION&gt;\"\n\nresults = run_example(\n    task_prompt, text_input=\"a wine bottle\", image=table\n)\noutput_image = copy.deepcopy(table)\ndraw_polygons(\n    output_image,\n    results[\"&lt;REFERRING_EXPRESSION_SEGMENTATION&gt;\"],\n    fill_mask=True,\n)\n\nresults = run_example(\n    task_prompt, text_input=\"a german sheppard\", image=dogs_cats\n)\noutput_image = copy.deepcopy(dogs_cats)\ndraw_polygons(\n    output_image,\n    results[\"&lt;REFERRING_EXPRESSION_SEGMENTATION&gt;\"],\n    fill_mask=True,\n)\n \n[INFO] ==&gt; Florence-2-base\n(&lt;REFERRING_EXPRESSION_SEGMENTATION&gt;), took 207.0 seconds\nto execute each task.\n\n\n\nWith this task, it is also possible to give the object coordinates in the image to segment it. The input format is '&lt;loc_x1&gt;&lt;loc_y1&gt;&lt;loc_x2&gt;&lt;loc_y2&gt;', [x1, y1, x2, y2] , which is the quantized coordinates in [0, 999].\nFor example, when running the code:\ntask_prompt = \"&lt;CAPTION_TO_PHRASE_GROUNDING&gt;\"\nresults = run_example(\n    task_prompt, text_input=\"a half orange\", image=table\n)\nresults\nThe results were:\n{'&lt;CAPTION_TO_PHRASE_GROUNDING&gt;': {'bboxes': [[343.552001953125,\n    689.6640625,\n    530.9440307617188,\n    873.9840698242188]],\n  'labels': ['a half']}}\nUsing the bboxes rounded coordinates:\ntask_prompt = \"&lt;REGION_TO_SEGMENTATION&gt;\"\nresults = run_example(\n    task_prompt,\n    text_input=(\"&lt;loc_343&gt;&lt;loc_690&gt;\" \"&lt;loc_531&gt;&lt;loc_874&gt;\"),\n    image=table,\n)\noutput_image = copy.deepcopy(table)\ndraw_polygons(\n    output_image, results[\"&lt;REGION_TO_SEGMENTATION&gt;\"], fill_mask=True\n)\nWe got the segmentation of the object on those coordinates (Latency: 83 seconds):\n \n\n\n\nWe can also give the region (coordinates and ask for a caption):\ntask_prompt = \"&lt;REGION_TO_CATEGORY&gt;\"\nresults = run_example(\n    task_prompt,\n    text_input=(\"&lt;loc_343&gt;&lt;loc_690&gt;\" \"&lt;loc_531&gt;&lt;loc_874&gt;\"),\n    image=table,\n)\nresults\n[INFO] ==&gt; Florence-2-base (&lt;REGION_TO_CATEGORY&gt;), \\\ntook 14.3 seconds to execute.\n\n{{\n  '&lt;REGION_TO_CATEGORY&gt;':\n    'orange&lt;loc_343&gt;&lt;loc_690&gt;'\n    '&lt;loc_531&gt;&lt;loc_874&gt;'\n}\nThe model identified an orange in that region. Let‚Äôs ask for a description:\ntask_prompt = \"&lt;REGION_TO_DESCRIPTION&gt;\"\nresults = run_example(\n    task_prompt,\n    text_input=(\"&lt;loc_343&gt;&lt;loc_690&gt;\" \"&lt;loc_531&gt;&lt;loc_874&gt;\"),\n    image=table,\n)\nresults\n[INFO] ==&gt; Florence-2-base (&lt;REGION_TO_CATEGORY&gt;), \\\ntook 14.6 seconds to execute.\n\n{\n  '&lt;REGION_TO_CATEGORY&gt;':\n    'orange&lt;loc_343&gt;&lt;loc_690&gt;'\n    '&lt;loc_531&gt;&lt;loc_874&gt;'\n}\nIn this case, the description did not provide more details, but it could. Try another example.\n\n\n\nWith Florence-2, we can perform Optical Character Recognition (OCR) on an image, getting what is written on it (task_prompt = '&lt;OCR&gt;' and also get the bounding boxes (location) for the detected text (ask_prompt = '&lt;OCR_WITH_REGION&gt;'). Those tasks can help extract and locate textual information in images, such as reading signs, labels, or other forms of text in images.\nLet‚Äôs upload a flyer from a talk in Brazil to Raspi. Let‚Äôs test works in another language, here Portuguese):\nflayer = Image.open(\"./images/embarcados.jpg\")\n# Display the image\nplt.figure(figsize=(8, 8))\nplt.imshow(flayer)\nplt.axis(\"off\")\n# plt.title(\"Image\")\nplt.show()\n \nLet‚Äôs examine the image with '&lt;MORE_DETAILED_CAPTION&gt;' :\n[INFO] ==&gt; Florence-2-base (&lt;MORE_DETAILED_CAPTION&gt;), \\\ntook 85.2 seconds to execute.\n\n{'&lt;MORE_DETAILED_CAPTION&gt;': 'The image is a promotional poster \\\nfor an event called \"Machine Learning Embarcados\" hosted by \\\nMarcelo Roval. The poster has a black background with white \\\ntext. On the left side of the poster, there is a logo of a \\\ncoffee cup with the text \"Caf√© Com Embarcados\" above it. \\\nBelow the logo, it says \"25 de Setembro as 17th\" which \\\ntranslates to \"25th of September as 17\" in English. \\n\\nOn \\\nthe right side, there are two smaller text boxes with the names \\\nof the participants and their names. The first text box reads \\\n\"Democratizando a Intelig√™ncia Artificial para Paises em \\\nDesenvolvimento\" and the second text box says \"Toda \\\nquarta-feira\" which is Portuguese for \"Transmiss√£o via in \\\nPortuguese\".\\n\\nIn the center of the image, there has a photo \\\nof Marcelo, a man with a beard and glasses, smiling at the \\\ncamera. He is wearing a white hard hat and a white shirt. \\\nThe text boxes are in orange and yellow colors.'}\nThe description is very accurate. Let‚Äôs get to the more important words with the task OCR:\ntask_prompt = \"&lt;OCR&gt;\"\nrun_example(task_prompt, image=flayer)\n[INFO] ==&gt; Florence-2-base (&lt;OCR&gt;), took 37.7 seconds to execute.\n\n{'&lt;OCR&gt;':\n 'Machine Learning Caf√© com Embarcado Embarcados '\n 'Democratizando a Intelig√™ncia Artificial para Paises em '\n '25 de Setembro √†s 17h Desenvolvimento Toda quarta-feira '\n 'Marcelo Roval Professor na UNIFIEI e Transmiss√£o via in '\n 'Co-Director do TinyML4D'}\nLet‚Äôs locate the words in the flyer:\ntask_prompt = \"&lt;OCR_WITH_REGION&gt;\"\nresults = run_example(task_prompt, image=flayer)\nLet‚Äôs also create a function to draw bounding boxes around the detected words:\ndef draw_ocr_bboxes(image, prediction):\n    scale = 1\n    draw = ImageDraw.Draw(image)\n    bboxes = prediction[\"quad_boxes\"]\n    labels = prediction[\"labels\"]\n    for box, label in zip(bboxes, labels):\n        color = random.choice(colormap)\n        new_box = (np.array(box) * scale).tolist()\n        draw.polygon(new_box, width=3, outline=color)\n        draw.text(\n            (new_box[0] + 8, new_box[1] + 2),\n            \"{}\".format(label),\n            align=\"right\",\n            fill=color,\n        )\n    display(image)\noutput_image = copy.deepcopy(flayer)\ndraw_ocr_bboxes(output_image, results[\"&lt;OCR_WITH_REGION&gt;\"])\n \nWe can inspect the detected words:\nresults[\"&lt;OCR_WITH_REGION&gt;\"][\"labels\"]\n'&lt;/s&gt;Machine Learning',\n 'Caf√©',\n 'com',\n 'Embarcado',\n 'Embarcados',\n 'Democratizando a Intelig√™ncia',\n 'Artificial para Paises em',\n '25 de Setembro √°s 17h',\n 'Desenvolvimento',\n 'Toda quarta-feira',\n 'Marcelo Roval',\n 'Professor na UNIFIEI e',\n 'Transmiss√£o via',\n 'in',\n 'Co-Director do TinyML4D']",
    "crumbs": [
      "Raspberry Pi",
      "Vision-Language Models (VLM)"
    ]
  },
  {
    "objectID": "contents/raspi/vlm/vlm.html#sec-visionlanguage-models-vlm-latency-summary-fec8",
    "href": "contents/raspi/vlm/vlm.html#sec-visionlanguage-models-vlm-latency-summary-fec8",
    "title": "Vision-Language Models (VLM)",
    "section": "",
    "text": "The latency observed for different tasks using Florence-2 on the Raspberry Pi (Raspi-5) varied depending on the complexity of the task:\n\nImage Captioning: It took approximately 16-17 seconds to generate a caption for an image.\nDetailed Captioning: Increased latency to around 25-27 seconds, requiring generating more nuanced scene descriptions.\nMore Detailed Captioning: It took about 32-50 seconds, and the latency increased as the description grew more complex.\nObject Detection: It took approximately 20-41 seconds, depending on the image‚Äôs complexity and the number of detected objects.\nVisual Grounding: Approximately 15-16 seconds to localize specific objects based on textual prompts.\nOCR (Optical Character Recognition): Extracting text from an image took around 37-38 seconds.\nSegmentation and Region to Segmentation: Segmentation tasks took considerably longer, with a latency of around 83-207 seconds, depending on the complexity and the number of regions to be segmented.\n\nThese latency times highlight the resource constraints of edge devices like the Raspberry Pi and emphasize the need to optimize the model and the environment to achieve real-time performance.\n \n\nRunning complex tasks can use all 8 GB of the Raspi-5‚Äôs memory. For example, the above screenshot during the Florence OD task shows 4 CPUs at full speed and over 5 GB of memory in use. Consider increasing the SWAP memory to 2 GB.\n\nChecking the CPU temperature with vcgencmd measure_temp , showed that temperature can go up to +80oC.",
    "crumbs": [
      "Raspberry Pi",
      "Vision-Language Models (VLM)"
    ]
  },
  {
    "objectID": "contents/raspi/vlm/vlm.html#sec-visionlanguage-models-vlm-finetunning-80d7",
    "href": "contents/raspi/vlm/vlm.html#sec-visionlanguage-models-vlm-finetunning-80d7",
    "title": "Vision-Language Models (VLM)",
    "section": "",
    "text": "As explored in this lab, Florence supports many tasks out of the box, including captioning, object detection, OCR, and more. However, like other pre-trained foundational models, Florence-2 may need domain-specific knowledge. For example, it may need to improve with medical or satellite imagery. In such cases, fine-tuning with a custom dataset is necessary. The Roboflow tutorial, How to Fine-tune Florence-2 for Object Detection Tasks, shows how to fine-tune Florence-2 on object detection datasets to improve model performance for our specific use case.\nBased on the above tutorial, it is possible to fine-tune the Florence-2 model to detect boxes and wheels used in previous labs:\n \nIt is important to note that after fine-tuning, the model can still detect classes that don‚Äôt belong to our custom dataset, like cats, dogs, grapes, etc, as seen before).\nThe complete fine-tuning project using a previously annotated dataset in Roboflow and executed on CoLab can be found in the notebook:\n\n30-Finetune_florence_2_on_detection_dataset_box_vs_wheel.ipynb\n\nIn another example, in the post, Fine-tuning Florence-2 - Microsoft‚Äôs Cutting-edge Vision Language Models, the authors show an example of fine-tuning Florence on DocVQA. The authors report that Florence 2 can perform visual question answering (VQA), but the released models don‚Äôt include VQA capability.",
    "crumbs": [
      "Raspberry Pi",
      "Vision-Language Models (VLM)"
    ]
  },
  {
    "objectID": "contents/raspi/vlm/vlm.html#sec-visionlanguage-models-vlm-summary-012c",
    "href": "contents/raspi/vlm/vlm.html#sec-visionlanguage-models-vlm-summary-012c",
    "title": "Vision-Language Models (VLM)",
    "section": "",
    "text": "Florence-2 offers a versatile and powerful approach to vision-language tasks at the edge, providing performance that rivals larger, task-specific models, such as YOLO for object detection, BERT/RoBERTa for text analysis, and specialized OCR models.\nThanks to its multi-modal transformer architecture, Florence-2 is more flexible than YOLO in terms of the tasks it can handle. These include object detection, image captioning, and visual grounding.\nUnlike BERT, which focuses purely on language, Florence-2 integrates vision and language, allowing it to excel in applications that require both modalities, such as image captioning and visual grounding.\nMoreover, while traditional OCR models such as Tesseract and EasyOCR are designed solely for recognizing and extracting text from images, Florence-2‚Äôs OCR capabilities are part of a broader framework that includes contextual understanding and visual-text alignment. This makes it particularly useful for scenarios that require both reading text and interpreting its context within images.\nOverall, Florence-2 stands out for its ability to seamlessly integrate various vision-language tasks into a unified model that is efficient enough to run on edge devices like the Raspberry Pi. This makes it a compelling choice for developers and researchers exploring AI applications at the edge.\n\n\n\nUnified Architecture\n\nSingle model handles multiple vision tasks vs.¬†specialized models (YOLO, BERT, Tesseract)\nEliminates the need for multiple model deployments and integrations\nConsistent API and interface across tasks\n\nPerformance Comparison\n\nObject Detection: Comparable to YOLOv8 (~37.5 mAP on COCO vs.¬†YOLOv8‚Äôs ~39.7 mAP) despite being general-purpose\nText Recognition: Handles multiple languages effectively like specialized OCR models (Tesseract, EasyOCR)\nLanguage Understanding: Integrates BERT-like capabilities for text processing while adding visual context\n\nResource Efficiency\n\nThe Base model (232M parameters) achieves strong results despite smaller size\nRuns effectively on edge devices (Raspberry Pi)\nSingle model deployment vs.¬†multiple specialized models\n\n\n\n\n\n\nPerformance vs.¬†Specialized Models\n\nYOLO series may offer faster inference for pure object detection\nSpecialized OCR models might handle complex document layouts better\nBERT/RoBERTa provide deeper language understanding for text-only tasks\n\nResource Requirements\n\nHigher latency on edge devices (15-200s depending on task)\nRequires careful memory management on Raspberry Pi\nIt may need optimization for real-time applications\n\nDeployment Considerations\n\nInitial setup is more complex than single-purpose models\nRequires understanding of multiple task types and prompts\nThe learning curve for optimal prompt engineering\n\n\n\n\n\n\nResource-Constrained Environments\n\nEdge devices requiring multiple vision capabilities\nSystems with limited storage/deployment capacity\nApplications needing flexible vision processing\n\nMulti-modal Applications\n\nContent moderation systems\nAccessibility tools\nDocument analysis workflows\n\nRapid Prototyping\n\nQuick deployment of vision capabilities\nTesting multiple vision tasks without separate models\nProof-of-concept development",
    "crumbs": [
      "Raspberry Pi",
      "Vision-Language Models (VLM)"
    ]
  },
  {
    "objectID": "contents/raspi/vlm/vlm.html#sec-visionlanguage-models-vlm-future-implications-2fe6",
    "href": "contents/raspi/vlm/vlm.html#sec-visionlanguage-models-vlm-future-implications-2fe6",
    "title": "Vision-Language Models (VLM)",
    "section": "",
    "text": "Florence-2 represents a shift toward unified vision models that could eventually replace task-specific architectures in many applications. While specialized models maintain advantages in specific scenarios, the convenience and efficiency of unified models like Florence-2 make them increasingly attractive for real-world deployments.\nThe lab demonstrates Florence-2‚Äôs viability on edge devices, suggesting future IoT, mobile computing, and embedded systems applications where deploying multiple specialized models would be impractical.",
    "crumbs": [
      "Raspberry Pi",
      "Vision-Language Models (VLM)"
    ]
  },
  {
    "objectID": "contents/raspi/vlm/vlm.html#sec-visionlanguage-models-vlm-resources-fb6a",
    "href": "contents/raspi/vlm/vlm.html#sec-visionlanguage-models-vlm-resources-fb6a",
    "title": "Vision-Language Models (VLM)",
    "section": "",
    "text": "10-florence2_test.ipynb\n20-florence_2.ipynb\n30-Finetune_florence_2_on_detection_dataset_box_vs_wheel.ipynb",
    "crumbs": [
      "Raspberry Pi",
      "Vision-Language Models (VLM)"
    ]
  },
  {
    "objectID": "contents/raspi/llm/llm.html",
    "href": "contents/raspi/llm/llm.html",
    "title": "Small Language Models (SLM)",
    "section": "",
    "text": "DALL¬∑E prompt - A 1950s-style cartoon illustration showing a Raspberry Pi running a small language model at the edge. The Raspberry Pi is stylized in a retro-futuristic way with rounded edges and chrome accents, connected to playful cartoonish sensors and devices. Speech bubbles are floating around, representing language processing, and the background has a whimsical landscape of interconnected devices with wires and small gadgets, all drawn in a vintage cartoon style. The color palette uses soft pastel colors and bold outlines typical of 1950s cartoons, giving a fun and nostalgic vibe to the scene.\n\n\n\n\nIn the fast-growing area of artificial intelligence, edge computing presents an opportunity to decentralize capabilities traditionally reserved for powerful, centralized servers. This lab explores the practical integration of small versions of traditional large language models (LLMs) into a Raspberry Pi 5, transforming this edge device into an AI hub capable of real-time, on-site data processing.\nAs large language models grow in size and complexity, Small Language Models (SLMs) offer a compelling alternative for edge devices, striking a balance between performance and resource efficiency. By running these models directly on Raspberry Pi, we can create responsive, privacy-preserving applications that operate even in environments with limited or no internet connectivity.\nThis lab will guide you through setting up, optimizing, and leveraging SLMs on Raspberry Pi. We will explore the installation and utilization of Ollama. This open-source framework allows us to run LLMs locally on our machines (our desktops or edge devices such as the Raspberry Pis or NVidia Jetsons). Ollama is designed to be efficient, scalable, and easy to use, making it a good option for deploying AI models such as Microsoft Phi, Google Gemma, Meta Llama, and LLaVa (Multimodal). We will integrate some of those models into projects using Python‚Äôs ecosystem, exploring their potential in real-world scenarios (or at least point in this direction).\n \n\n\n\nWe could use any Raspi model in the previous labs, but here, the choice must be the Raspberry Pi 5 (Raspi-5). It is a robust platform that substantially upgrades the last version 4, equipped with the Broadcom BCM2712, a 2.4 GHz quad-core 64-bit Arm Cortex-A76 CPU featuring Cryptographic Extension and enhanced caching capabilities. It boasts a VideoCore VII GPU, dual 4Kp60 HDMI¬Æ outputs with HDR, and a 4Kp60 HEVC decoder. Memory options include 4 GB and 8 GB of high-speed LPDDR4X SDRAM, with 8GB being our choice to run SLMs. It also features expandable storage via a microSD card slot and a PCIe 2.0 interface for fast peripherals such as M.2 SSDs (Solid State Drives).\n\nFor real SSL applications, SSDs are a better option than SD cards.\n\nBy the way, as Alasdair Allan discussed, inferencing directly on the Raspberry Pi 5 CPU‚Äîwith no GPU acceleration‚Äîis now on par with the performance of the Coral TPU.\n \nFor more info, please see the complete article: Benchmarking TensorFlow and TensorFlow Lite on Raspberry Pi 5.\n\n\nWe suggest installing an Active Cooler, a dedicated clip-on cooling solution for Raspberry Pi 5 (Raspi-5), for this lab. It combines an aluminum heat sink with a temperature-controlled blower fan to keep the Raspi-5 operating comfortably under heavy loads, such as running SLMs.\n \nThe Active Cooler has pre-applied thermal pads for heat transfer and is mounted directly to the Raspberry Pi 5 board using spring-loaded push pins. The Raspberry Pi firmware actively manages it: at 60¬∞C, the blower‚Äôs fan will be turned on; at 67.5¬∞C, the fan speed will be increased; and finally, at 75¬∞C, the fan increases to full speed. The blower‚Äôs fan will spin down automatically when the temperature drops below these limits.\n \n\nTo prevent overheating, all Raspberry Pi boards begin to throttle the processor when the temperature reaches 80¬∞Cand throttle even further when it reaches the maximum temperature of 85¬∞C (more detail here).\n\n\n\n\n\nGenerative AI is an artificial intelligence system capable of creating new, original content across various mediums such as text, images, audio, and video. These systems learn patterns from existing data and use that knowledge to generate novel outputs that didn‚Äôt previously exist. Large Language Models (LLMs), Small Language Models (SLMs), and multimodal models can all be considered types of GenAI when used for generative tasks.\nGenAI provides the conceptual framework for AI-driven content creation, with LLMs serving as powerful general-purpose text generators. SLMs adapt this technology for edge computing, while multimodal models extend GenAI capabilities across different data types. Together, they represent a spectrum of generative AI technologies, each with its strengths and applications, collectively driving AI-powered content creation and understanding.\n\n\nLarge Language Models (LLMs) are advanced artificial intelligence systems that understand, process, and generate human-like text. These models are characterized by their massive scale in terms of the amount of data they are trained on and the number of parameters they contain. Critical aspects of LLMs include:\n\nSize: LLMs typically contain billions of parameters. For example, GPT-3 has 175 billion parameters, while some newer models exceed a trillion parameters.\nTraining Data: They are trained on vast amounts of text data, often including books, websites, and other diverse sources, amounting to hundreds of gigabytes or even terabytes of text.\nArchitecture: Most LLMs use transformer-based architectures, which allow them to process and generate text by paying attention to different parts of the input simultaneously.\nCapabilities: LLMs can perform a wide range of language tasks without specific fine-tuning, including:\n\nText generation\nTranslation\nSummarization\nQuestion answering\nCode generation\nLogical reasoning\n\nFew-shot Learning: They can often understand and perform new tasks with minimal examples or instructions.\nResource-Intensive: Due to their size, LLMs typically require significant computational resources to run, often needing powerful GPUs or TPUs.\nContinual Development: The field of LLMs is rapidly evolving, with new models and techniques constantly emerging.\nEthical Considerations: The use of LLMs raises important questions about bias, misinformation, and the environmental impact of training such large models.\nApplications: LLMs are used in various fields, including content creation, customer service, research assistance, and software development.\nLimitations: Despite their power, LLMs can produce incorrect or biased information and lack true understanding or reasoning capabilities.\n\nWe must note that we use large models beyond text, calling them multi-modal models. These models integrate and process information from multiple types of input simultaneously. They are designed to understand and generate content across various forms of data, such as text, images, audio, and video.\n\n\n\nClosed models, also called proprietary models, are AI models whose internal workings, code, and training data are not publicly disclosed. Examples: GPT-4 (by OpenAI), Claude (by Anthropic), Gemini (by Google).\nOpen models, also known as open-source models, are AI models whose underlying code, architecture, and often training data are publicly available and accessible. Examples: Gemma (by Google), LLaMA (by Meta) and Phi (by Microsoft).\nOpen models are particularly relevant for running models on edge devices like Raspberry Pi as they can be more easily adapted, optimized, and deployed in resource-constrained environments. Still, it is crucial to verify their Licenses. Open models come with various open-source licenses that may affect their use in commercial applications, while closed models have clear, albeit restrictive, terms of service.\n\n\n\nAdapted from arXiv\n\n\n\n\n\nIn the context of edge computing on devices like Raspberry Pi, full-scale LLMs are typically too large and resource-intensive to run directly. This limitation has driven the development of smaller, more efficient models, such as the Small Language Models (SLMs).\nSLMs are compact versions of LLMs designed to run efficiently on resource-constrained devices such as smartphones, IoT devices, and single-board computers like the Raspberry Pi. These models are significantly smaller in size and computational requirements than their larger counterparts while still retaining impressive language understanding and generation capabilities.\nKey characteristics of SLMs include:\n\nReduced parameter count: Typically ranging from a few hundred million to a few billion parameters, compared to two-digit billions in larger models.\nLower memory footprint: Requiring, at most, a few gigabytes of memory rather than tens or hundreds of gigabytes.\nFaster inference time: Can generate responses in milliseconds to seconds on edge devices.\nEnergy efficiency: Consuming less power, making them suitable for battery-powered devices.\nPrivacy-preserving: Enabling on-device processing without sending data to cloud servers.\nOffline functionality: Operating without an internet connection.\n\nSLMs achieve their compact size through various techniques such as knowledge distillation, model pruning, and quantization. While they may not match the broad capabilities of larger models, SLMs excel in specific tasks and domains, making them ideal for targeted applications on edge devices.\n\nWe will generally consider SLMs, language models with less than 5 billion parameters quantized to 4 bits.\n\nExamples of SLMs include compressed versions of models like Meta Llama, Microsoft PHI, and Google Gemma. These models enable a wide range of natural language processing tasks directly on edge devices, from text classification and sentiment analysis to question answering and limited text generation.\nFor more information on SLMs, the paper, LLM Pruning and Distillation in Practice: The Minitron Approach, provides an approach applying pruning and distillation to obtain SLMs from LLMs. And, SMALL LANGUAGE MODELS: SURVEY, MEASUREMENTS, AND INSIGHTS, presents a comprehensive survey and analysis of Small Language Models (SLMs), which are language models with 100 million to 5 billion parameters designed for resource-constrained devices.\n\n\n\n\n\n\n\nollama logo\n\n\nOllama is an open-source framework that allows us to run language models (LMs), large or small, locally on our machines. Here are some critical points about Ollama:\n\nLocal Model Execution: Ollama enables running LMs on personal computers or edge devices such as the Raspi-5, eliminating the need for cloud-based API calls.\nEase of Use: It provides a simple command-line interface for downloading, running, and managing different language models.\nModel Variety: Ollama supports various LLMs, including Phi, Gemma, Llama, Mistral, and other open-source models.\nCustomization: Users can create and share custom models tailored to specific needs or domains.\nLightweight: Designed to be efficient and run on consumer-grade hardware.\nAPI Integration: Offers an API that allows integration with other applications and services.\nPrivacy-Focused: By running models locally, it addresses privacy concerns associated with sending data to external servers.\nCross-Platform: Available for macOS, Windows, and Linux systems (our case, here).\nActive Development: Regularly updated with new features and model support.\nCommunity-Driven: Benefits from community contributions and model sharing.\n\nTo learn more about what Ollama is and how it works under the hood, you should see this short video from Matt Williams, one of the founders of Ollama:\n\n\nMatt has an entirely free course about Ollama that we recommend: \n\n\n\nLet‚Äôs set up and activate a Virtual Environment for working with Ollama:\npython3 -m venv ~/ollama\nsource ~/ollama/bin/activate\nAnd run the command to install Ollama:\ncurl -fsSL https://ollama.com/install.sh | sh\nAs a result, an API will run in the background on 127.0.0.1:11434. From now on, we can run Ollama via the terminal. For starting, let‚Äôs verify the Ollama version, which will also tell us that it is correctly installed:\nollama -v\n \nOn the Ollama Library page, we can find the models Ollama supports. For example, by filtering by Most popular, we can see Meta Llama, Google Gemma, Microsoft Phi, LLaVa, etc.\n\n\n\n\n\n\n\n\nLet‚Äôs install and run our first small language model, Llama 3.2 1B (and 3B). The Meta Llama 3.2 series comprises a set of multilingual generative language models available in 1 billion and 3 billion parameter sizes. These models are designed to process text input and generate text output. The instruction-tuned variants within this collection are specifically optimized for multilingual conversational applications, including tasks involving information retrieval and summarization with an agentic approach. When compared to many existing open-source and proprietary chat models, the Llama 3.2 instruction-tuned models demonstrate superior performance on widely-used industry benchmarks.\nThe 1B and 3B models were pruned from the Llama 8B, and then logits from the 8B and 70B models were used as token-level targets (token-level distillation). Knowledge distillation was used to recover performance (they were trained with 9 trillion tokens). The 1B model has 1,24B, quantized to integer (Q8_0), and the 3B, 3.12B parameters, with a Q4_0 quantization, which ends with a size of 1.3 GB and 2 GB, respectively. Its context window is 131,072 tokens.\n \nInstall and run the Model\nollama run llama3.2:1b\nRunning the model with the command before, we should have the Ollama prompt available for us to input a question and start chatting with the LLM model; for example,\n&gt;&gt;&gt; What is the capital of France?\nAlmost immediately, we get the correct answer:\nThe capital of France is Paris.\nUsing the option --verbose when calling the model will generate several statistics about its performance (The model will be polling only the first time we run the command).\n \nEach metric gives insights into how the model processes inputs and generates outputs. Here‚Äôs a breakdown of what each metric means:\n\nTotal Duration (2.620170326 s): This is the complete time taken from the start of the command to the completion of the response. It encompasses loading the model, processing the input prompt, and generating the response.\nLoad Duration (39.947908 ms): This duration indicates the time to load the model or necessary components into memory. If this value is minimal, it can suggest that the model was preloaded or that only a minimal setup was required.\nPrompt Eval Count (32 tokens): The number of tokens in the input prompt. In NLP, tokens are typically words or subwords, so this count includes all the tokens that the model evaluated to understand and respond to the query.\nPrompt Eval Duration (1.644773 s): This measures the model‚Äôs time to evaluate or process the input prompt. It accounts for the bulk of the total duration, implying that understanding the query and preparing a response is the most time-consuming part of the process.\nPrompt Eval Rate (19.46 tokens/s): This rate indicates how quickly the model processes tokens from the input prompt. It reflects the model‚Äôs speed in terms of natural language comprehension.\nEval Count (8 token(s)): This is the number of tokens in the model‚Äôs response, which in this case was, ‚ÄúThe capital of France is Paris.‚Äù\nEval Duration (889.941 ms): This is the time taken to generate the output based on the evaluated input. It‚Äôs much shorter than the prompt evaluation, suggesting that generating the response is less complex or computationally intensive than understanding the prompt.\nEval Rate (8.99 tokens/s): Similar to the prompt eval rate, this indicates the speed at which the model generates output tokens. It‚Äôs a crucial metric for understanding the model‚Äôs efficiency in output generation.\n\nThis detailed breakdown can help understand the computational demands and performance characteristics of running SLMs like Llama on edge devices like the Raspberry Pi 5. It shows that while prompt evaluation is more time-consuming, the actual generation of responses is relatively quicker. This analysis is crucial for optimizing performance and diagnosing potential bottlenecks in real-time applications.\nLoading and running the 3B model, we can see the difference in performance for the same prompt;\n \nThe eval rate is lower, 5.3 tokens/s versus 9 tokens/s with the smaller model.\nWhen question about\n&gt;&gt;&gt; What is the distance between Paris and Santiago, Chile?\nThe 1B model answered 9,841 kilometers (6,093 miles), which is inaccurate, and the 3B model answered 7,300 miles (11,700 km), which is close to the correct (11,642 km).\nLet‚Äôs ask for the Paris‚Äôs coordinates:\n&gt;&gt;&gt; what is the latitude and longitude of Paris?\nThe latitude and longitude of Paris are 48.8567¬∞ N (48¬∞55'\n42\" N) and 2.3510¬∞ E (2¬∞22' 8\" E), respectively.\n\n\n\n\n\nBoth 1B and 3B models gave correct answers.\n\n\n\nLet‚Äôs install Gemma 2, a high-performing and efficient model available in three sizes: 2B, 9B, and 27B. We will install Gemma 2 2B, a lightweight model trained with 2 trillion tokens that produces outsized results by learning from larger models through distillation. The model has 2.6 billion parameters and a Q4_0 quantization, which ends with a size of 1.6 GB. Its context window is 8,192 tokens.\n \nInstall and run the Model\nollama run gemma2:2b --verbose\nRunning the model with the command before, we should have the Ollama prompt available for us to input a question and start chatting with the LLM model; for example,\n&gt;&gt;&gt; What is the capital of France?\nAlmost immediately, we get the correct answer:\nThe capital of France is **Paris**. üóº\nAnd it‚Äô statistics.\n \nWe can see that Gemma 2:2B has around the same performance as Llama 3.2:3B, but having less parameters.\nOther examples:\n&gt;&gt;&gt; What is the distance between Paris and Santiago, Chile?\n\nThe distance between Paris, France and Santiago, Chile is\napproximately **7,000 miles (11,267 kilometers)**.\n\nKeep in mind that this is a straight-line distance, and actual\ntravel distance can vary depending on the chosen routes and any\nstops along the way. ‚úàÔ∏è`\nAlso, a good response but less accurate than Llama3.2:3B.\n&gt;&gt;&gt; what is the latitude and longitude of Paris?\n\nYou got it! Here are the latitudes and longitudes of Paris,\nFrance:\n\n* **Latitude**: 48.8566¬∞ N (north)\n* **Longitude**: 2.3522¬∞ E (east)\n\nLet me know if you'd like to explore more about Paris or its\nlocation! üóºüá´üá∑\nA good and accurate answer (a little more verbose than the Llama answers).\n\n\n\nLet‚Äôs pull a bigger (but still tiny) model, the PHI3.5, a 3.8B lightweight state-of-the-art open model by Microsoft. The model belongs to the Phi-3 model family and supports 128K token context length and the languages: Arabic, Chinese, Czech, Danish, Dutch, English, Finnish, French, German, Hebrew, Hungarian, Italian, Japanese, Korean, Norwegian, Polish, Portuguese, Russian, Spanish, Swedish, Thai, Turkish and Ukrainian.\nThe model size, in terms of bytes, will depend on the specific quantization format used. The size can go from 2-bit quantization (q2_k) of 1.4 GB (higher performance/lower quality) to 16-bit quantization (fp-16) of 7.6 GB (lower performance/higher quality).\nLet‚Äôs run the 4-bit quantization (Q4_0), which will need 2.2 GB of RAM, with an intermediary trade-off regarding output quality and performance.\nollama run phi3.5:3.8b --verbose\n\nYou can use run or pull to download the model. What happens is that Ollama keeps note of the pulled models, and once the PHI3 does not exist, before running it, Ollama pulls it.\n\nLet‚Äôs enter with the same prompt used before:\n&gt;&gt;&gt; What is the capital of France?\n\nThe capital of France is Paris. It' extradites significant\nhistorical, cultural, and political importance to the country as\nwell as being a major European city known for its art, fashion,\ngastronomy, and culture. Its influence extends beyond national\nborders, with millions of tourists visiting each year from around\nthe globe. The Seine River flows through Paris before it reaches\nthe broader English Channel at Le Havre. Moreover, France is one\nof Europe's leading economies with its capital playing a key role\n\n...\nThe answer was very ‚Äúverbose‚Äù, let‚Äôs specify a better prompt:\n \nIn this case, the answer was still longer than we expected, with an eval rate of 2.25 tokens/s, more than double that of Gemma and Llama.\n\nChoosing the most appropriate prompt is one of the most important skills to be used with LLMs, no matter its size.\n\nWhen we asked the same questions about distance and Latitude/Longitude, we did not get a good answer for a distance of 13,507 kilometers (8,429 miles), but it was OK for coordinates. Again, it could have been less verbose (more than 200 tokens for each answer).\nWe can use any model as an assistant since their speed is relatively decent, but on September 24 (2023), the Llama2:3B is a better choice. You should try other models, depending on your needs. ü§ó Open LLM Leaderboard can give you an idea about the best models in size, benchmark, license, etc.\n\nThe best model to use is the one fit for your specific necessity. Also, take into consideration that this field evolves with new models everyday.\n\n\n\n\nMultimodal models are artificial intelligence (AI) systems that can process and understand information from multiple sources, such as images, text, audio, and video. In our context, multimodal LLMs can process various inputs, including text, images, and audio, as prompts and convert those prompts into various outputs, not just the source type.\nWe will work here with LLaVA-Phi-3, a fine-tuned LLaVA model from Phi 3 Mini 4k. It has strong performance benchmarks that are on par with the original LLaVA (Large Language and Vision Assistant) model.\nThe LLaVA-Phi-3 is an end-to-end trained large multimodal model designed to understand and generate content based on visual inputs (images) and textual instructions. It combines the capabilities of a visual encoder and a language model to process and respond to multimodal inputs.\nLet‚Äôs install the model:\nollama run llava-phi3:3.8b --verbose\nLet‚Äôs start with a text input:\n&gt;&gt;&gt; You are a helpful AI assistant.\n    What is the capital of France?\n\nAs an AI language model, I can tell you that the capital of\nFrance is Paris. It's not only the largest city in the country\nbut also serves as its political and administrative center.\nParis is known for its iconic landmarks such as the Eiffel\nTower, Notre-Dame Cathedral, and the Louvre Museum. The city\nhas a rich history, beautiful architecture, and is widely\nconsidered to be one of the most romantic cities in the world.\nThe response took around 30 s, with an eval rate of 3.93 tokens/s! Not bad!\nBut let us know to enter with an image as input. For that, let‚Äôs create a directory for working:\ncd Documents/\nmkdir OLLAMA\ncd OLLAMA\nLet‚Äôs download a \\(640\\times 320\\) image from the internet, for example (Wikipedia: Paris, France):\n \nUsing FileZilla, for example, let‚Äôs upload the image to the OLLAMA folder at the Raspi-5 and name it image_test_1.jpg. We should have the whole image path (we can use pwd to get it).\n/home/mjrovai/Documents/OLLAMA/image_test_1.jpg\nIf you use a desktop, you can copy the image path by clicking the image with the mouse‚Äôs right button.\n \nLet‚Äôs enter with this prompt:\n&gt;&gt;&gt; Describe the image /home/mjrovai/Documents/OLLAMA/\\\n                        image_test_1.jpg\nThe result was great, but the overall latency was significant; almost 4 minutes to perform the inference.\n \n\n\n\nUsing htop, we can monitor the resources running on our device.\nhtop\nDuring the time that the model is running, we can inspect the resources:\n \nAll four CPUs run at almost 100% of their capacity, and the memory used with the model loaded is 3.24 GB. Exiting Ollama, the memory goes down to around 377 MB (with no desktop).\nIt is also essential to monitor the temperature. When running the Raspberry with a desktop, you can have the temperature shown on the taskbar:\n \nIf you are ‚Äúheadless‚Äù, the temperature can be monitored with the command:\nvcgencmd measure_temp\nIf you are doing nothing, the temperature is around 50¬∞C for CPUs running at 1%. During inference, with the CPUs at 100%, the temperature can rise to almost 70¬∞C. This is OK and means the active cooler is working, keeping the temperature below 80¬∞C / 85¬∞C (its limit).\n\n\n\n\nSo far, we have explored SLMs‚Äô chat capability using the command line on a terminal. However, we want to integrate those models into our projects, so Python seems to be the right path. The good news is that Ollama has such a library.\nThe Ollama Python library simplifies interaction with advanced LLM models, enabling more sophisticated responses and capabilities, besides providing the easiest way to integrate Python 3.8+ projects with Ollama.\nFor a better understanding of how to create apps using Ollama with Python, we can follow Matt Williams‚Äôs videos, as the one below:\n\nInstallation:\nIn the terminal, run the command:\npip install ollama\nWe will need a text editor or an IDE to create a Python script. If you run the Raspberry OS on a desktop, several options, such as Thonny and Geany, have already been installed by default (accessed by [Menu][Programming]). You can download other IDEs, such as Visual Studio Code, from [Menu][Recommended Software]. When the window pops up, go to [Programming], select the option of your choice, and press [Apply].\n \nIf you prefer using Jupyter Notebook for development:\npip install jupyter\njupyter notebook --generate-config\nTo run Jupyter Notebook, run the command (change the IP address for yours):\njupyter notebook --ip=192.168.4.209 --no-browser\nOn the terminal, you can see the local URL address to open the notebook:\n \nWe can access it from another computer by entering the Raspberry Pi‚Äôs IP address and the provided token in a web browser (we should copy it from the terminal).\nIn our working directory in the Raspi, we will create a new Python 3 notebook.\nLet‚Äôs enter with a very simple script to verify the installed models:\nimport ollama\n\nollama.list()\nAll the models will be printed as a dictionary, for example:\n  {'name': 'gemma2:2b',\n   'model': 'gemma2:2b',\n   'modified_at': '2024-09-24T19:30:40.053898094+01:00',\n   'size': 1629518495,\n   'digest': (\n     '8ccf136fdd5298f3ffe2d69862750ea7fb56555fa4d5b18c0'\n     '4e3fa4d82ee09d7'\n    ),\n\n   'details': {'parent_model': '',\n    'format': 'gguf',\n    'family': 'gemma2',\n    'families': ['gemma2'],\n    'parameter_size': '2.6B',\n    'quantization_level': 'Q4_0'}}]}\nLet‚Äôs repeat one of the questions that we did before, but now using ollama.generate() from Ollama python library. This API will generate a response for the given prompt with the provided model. This is a streaming endpoint, so there will be a series of responses. The final response object will include statistics and additional data from the request.\nMODEL = \"gemma2:2b\"\nPROMPT = \"What is the capital of France?\"\n\nres = ollama.generate(model=MODEL, prompt=PROMPT)\nprint(res)\nIn case you are running the code as a Python script, you should save it, for example, test_ollama.py. You can use the IDE to run it or do it directly on the terminal. Also, remember that you should always call the model and define it when running a stand-alone script.\npython test_ollama.py\nAs a result, we will have the model response in a JSON format:\n{\n  'model': 'gemma2:2b',\n  'created_at': '2024-09-25T14:43:31.869633807Z',\n  'response': 'The capital of France is **Paris**.\\n',\n  'done': True,\n  'done_reason': 'stop',\n  'context': [\n      106, 1645, 108, 1841, 603, 573, 6037, 576, 6081, 235336,\n      107, 108, 106, 2516, 108, 651, 6037, 576, 6081, 603, 5231,\n      29437, 168428, 235248, 244304, 241035, 235248, 108\n  ],\n  'total_duration': 24259469458,\n  'load_duration': 19830013859,\n  'prompt_eval_count': 16,\n  'prompt_eval_duration': 1908757000,\n  'eval_count': 14,\n  'eval_duration': 2475410000\n}\nAs we can see, several pieces of information are generated, such as:\n\nresponse: the main output text generated by the model in response to our prompt.\n\nThe capital of France is **Paris**. üá´üá∑\n\ncontext: the token IDs representing the input and context used by the model. Tokens are numerical representations of text used for processing by the language model.\n\n[106, 1645, 108, 1841, 603, 573, 6037, 576, 6081, 235336, 107, 108, 106, 2516, 108, 651, 6037, 576, 6081, 603, 5231, 29437, 168428, 235248, 244304, 241035, 235248, 108]\n\n\nThe Performance Metrics:\n\ntotal_duration: The total time taken for the operation in nanoseconds. In this case, approximately 24.26 seconds.\nload_duration: The time taken to load the model or components in nanoseconds. About 19.83 seconds.\nprompt_eval_duration: The time taken to evaluate the prompt in nanoseconds. Around 16 nanoseconds.\neval_count: The number of tokens evaluated during the generation. Here, 14 tokens.\neval_duration: The time taken for the model to generate the response in nanoseconds. Approximately 2.5 seconds.\n\nBut, what we want is the plain ‚Äòresponse‚Äô and, perhaps for analysis, the total duration of the inference, so let‚Äôs change the code to extract it from the dictionary:\nprint(f\"\\n{res['response']}\")\nprint(\n    f\"\\n [INFO] Total Duration: \"\n    f\"{res['total_duration']/1e9:.2f} seconds\"\n)\nNow, we got:\nThe capital of France is **Paris**. üá´üá∑\n\n [INFO] Total Duration: 24.26 seconds\nUsing Ollama.chat()\nAnother way to get our response is to use ollama.chat(), which generates the next message in a chat with a provided model. This is a streaming endpoint, so a series of responses will occur. Streaming can be disabled using \"stream\": false. The final response object will also include statistics and additional data from the request.\nPROMPT_1 = \"What is the capital of France?\"\n\nresponse = ollama.chat(\n    model=MODEL,\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": PROMPT_1,\n        },\n    ],\n)\nresp_1 = response[\"message\"][\"content\"]\nprint(f\"\\n{resp_1}\")\nprint(\n    f\"\\n [INFO] Total Duration: \"\n    f\"{(res['total_duration']/1e9):.2f} seconds\"\n)\nThe answer is the same as before.\nAn important consideration is that by using ollama.generate(), the response is ‚Äúclear‚Äù from the model‚Äôs ‚Äúmemory‚Äù after the end of inference (only used once), but If we want to keep a conversation, we must use ollama.chat(). Let‚Äôs see it in action:\nPROMPT_1 = \"What is the capital of France?\"\nresponse = ollama.chat(\n    model=MODEL,\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": PROMPT_1,\n        },\n    ],\n)\nresp_1 = response[\"message\"][\"content\"]\nprint(f\"\\n{resp_1}\")\nprint(\n    f\"\\n [INFO] Total Duration: \"\n    f\"{(response['total_duration']/1e9):.2f} seconds\"\n)\n\nPROMPT_2 = \"and of Italy?\"\nresponse = ollama.chat(\n    model=MODEL,\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": PROMPT_1,\n        },\n        {\n            \"role\": \"assistant\",\n            \"content\": resp_1,\n        },\n        {\n            \"role\": \"user\",\n            \"content\": PROMPT_2,\n        },\n    ],\n)\nresp_2 = response[\"message\"][\"content\"]\nprint(f\"\\n{resp_2}\")\nprint(\n    f\"\\n [INFO] Total Duration: \"\n    f\"{(response_2['total_duration']/1e9):.2f} seconds\"\n)\nIn the above code, we are running two queries, and the second prompt considers the result of the first one.\nHere is how the model responded:\nThe capital of France is **Paris**. üá´üá∑\n\n [INFO] Total Duration: 2.82 seconds\n\nThe capital of Italy is **Rome**. üáÆüáπ\n\n [INFO] Total Duration: 4.46 seconds\nGetting an image description:\nIn the same way that we have used the LlaVa-PHI-3 model with the command line to analyze an image, the same can be done here with Python. Let‚Äôs use the same image of Paris, but now with the ollama.generate():\nMODEL = \"llava-phi3:3.8b\"\nPROMPT = \"Describe this picture\"\n\nwith open(\"image_test_1.jpg\", \"rb\") as image_file:\n    img = image_file.read()\n\nresponse = ollama.generate(model=MODEL, prompt=PROMPT, images=[img])\nprint(f\"\\n{response['response']}\")\nprint(\n    f\"\\n [INFO] Total Duration: \"\n    f\"{(res['total_duration']/1e9):.2f} seconds\"\n)\nHere is the result:\nThis image captures the iconic cityscape of Paris, France. The\nvantage point is high, providing a panoramic view of the Seine\nRiver that meanders through the heart of the city. Several\nbridges arch gracefully over the river, connecting different\nparts of the city. The Eiffel Tower, an iron lattice structure\nwith a pointed top and two antennas on its summit, stands\ntall in the background, piercing the sky. It is painted in a\nlight gray color, contrasting against the blue sky speckled\nwith white clouds.\n\nThe buildings that line the river are predominantly white or\nbeige, their uniform color palette broken occasionally by red\nroofs peeking through. The Seine River itself appears calm\nand wide, reflecting the city's architectural beauty in its\nsurface. On either side of the river, trees add a touch of\ngreen to the urban landscape.\n\nThe image is taken from an elevated perspective, looking down\non the city. This viewpoint allows for a comprehensive view of\nParis's beautiful architecture and layout. The relative\npositions of the buildings, bridges, and other structures\ncreate a harmonious composition that showcases the city's charm.\n\nIn summary, this image presents a serene day in Paris, with its\narchitectural marvels - from the Eiffel Tower to the river-side\nbuildings - all bathed in soft colors under a clear sky.\n\n [INFO] Total Duration: 256.45 seconds\nThe model took about 4 minutes (256.45 s) to return with a detailed image description.\n\nIn the 10-Ollama_Python_Library notebook, it is possible to find the experiments with the Ollama Python library.\n\n\n\nSo far, we can observe that by using the model‚Äôs response into a variable, we can effectively incorporate it into real-world projects. However, a major issue arises when the model provides varying responses to the same input. For instance, let‚Äôs assume that we only need the name of a country‚Äôs capital and its coordinates as the model‚Äôs response in the previous examples, without any additional information, even when utilizing verbose models like Microsoft Phi. To ensure consistent responses, we can employ the ‚ÄòOllama function call,‚Äô which is fully compatible with the OpenAI API.\n\n\nIn modern artificial intelligence, function calling with Large Language Models (LLMs) allows these models to perform actions beyond generating text. By integrating with external functions or APIs, LLMs can access real-time data, automate tasks, and interact with various systems.\nFor instance, instead of merely responding to a query about the weather, an LLM can call a weather API to fetch the current conditions and provide accurate, up-to-date information. This capability enhances the relevance and accuracy of the model‚Äôs responses and makes it a powerful tool for driving workflows and automating processes, transforming it into an active participant in real-world applications.\nFor more details about Function Calling, please see this video made by Marvin Prison:\n\n\n\n\nWe want to create an app where the user enters a country‚Äôs name and gets, as an output, the distance in km from the capital city of such a country and the app‚Äôs location (for simplicity, We will use Santiago, Chile, as the app location).\n \nOnce the user enters a country name, the model will return the name of its capital city (as a string) and the latitude and longitude of such city (in float). Using those coordinates, we can use a simple Python library (haversine) to calculate the distance between those 2 points.\nThe idea of this project is to demonstrate a combination of language model interaction, structured data handling with Pydantic, and geospatial calculations using the Haversine formula (traditional computing).\nFirst, let us install some libraries. Besides Haversine, the main one is the OpenAI Python library, which provides convenient access to the OpenAI REST API from any Python 3.7+ application. The other one is Pydantic (and instructor), a robust data validation and settings management library engineered by Python to enhance the robustness and reliability of our codebase. In short, Pydantic will help ensure that our model‚Äôs response will always be consistent.\npip install haversine\npip install openai\npip install pydantic\npip install instructor\nNow, we should create a Python script designed to interact with our model (LLM) to determine the coordinates of a country‚Äôs capital city and calculate the distance from Santiago de Chile to that capital.\nLet‚Äôs go over the code:\n\n\n\n\nimport sys\nfrom haversine import haversine\nfrom openai import OpenAI\nfrom pydantic import BaseModel, Field\nimport instructor\n\nsys: Provides access to system-specific parameters and functions. It‚Äôs used to get command-line arguments.\nhaversine: A function from the haversine library that calculates the distance between two geographic points using the Haversine formula.\nopenAI: A module for interacting with the OpenAI API (although it‚Äôs used in conjunction with a local setup, Ollama). Everything is off-line here.\npydantic: Provides data validation and settings management using Python-type annotations. It‚Äôs used to define the structure of expected response data.\ninstructor: A module is used to patch the OpenAI client to work in a specific mode (likely related to structured data handling).\n\n\n\n\ncountry = sys.argv[1]  # Get the country from\n# command-line arguments\nMODEL = \"phi3.5:3.8b\"  # The name of the model to be used\nmylat = -33.33  # Latitude of Santiago de Chile\nmylon = -70.51  # Longitude of Santiago de Chile\n\ncountry: On a Python script, getting the country name from command-line arguments is possible. On a Jupyter notebook, we can enter its name, for example,\n\ncountry = \"France\"\n\nMODEL: Specifies the model being used, which is, in this example, the phi3.5.\nmylat and mylon: Coordinates of Santiago de Chile, used as the starting point for the distance calculation.\n\n\n\n\nclass CityCoord(BaseModel):\n    city: str = Field(..., description=\"Name of the city\")\n    lat: float = Field(\n        ..., description=\"Decimal Latitude of the city\"\n    )\n    lon: float = Field(\n        ..., description=\"Decimal Longitude of the city\"\n    )\n\nCityCoord: A Pydantic model that defines the expected structure of the response from the LLM. It expects three fields: city (name of the city), lat (latitude), and lon (longitude).\n\n\n\n\nclient = instructor.patch(\n    OpenAI(\n        base_url=\"http://localhost:11434/v1\",  # Local API base\n        # URL (Ollama)\n        api_key=\"ollama\",  # API key\n        # (not used)\n    ),\n    mode=instructor.Mode.JSON,  # Mode for\n    # structured\n    # JSON output\n)\n\nOpenAI: This setup initializes an OpenAI client with a local base URL and an API key (ollama). It uses a local server.\ninstructor.patch: Patches the OpenAI client to work in JSON mode, enabling structured output that matches the Pydantic model.\n\n\n\n\nresp = client.chat.completions.create(\n    model=MODEL,\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": f\"return the decimal latitude and \\\n            decimal longitude of the capital of the {country}.\",\n        }\n    ],\n    response_model=CityCoord,\n    max_retries=10,\n)\n\nclient.chat.completions.create: Calls the LLM to generate a response.\nmodel: Specifies the model to use (llava-phi3).\nmessages: Contains the prompt for the LLM, asking for the latitude and longitude of the capital city of the specified country.\nresponse_model: Indicates that the response should conform to the CityCoord model.\nmax_retries: The maximum number of retry attempts if the request fails.\n\n\n\n\ndistance = haversine((mylat, mylon), (resp.lat, resp.lon), unit=\"km\")\n\nprint(\n    f\"Santiago de Chile is about {int(round(distance, -1))} \"\n    f\"kilometers away from {resp.city}.\"\n)\n\nhaversine: Calculates the distance between Santiago de Chile and the capital city returned by the LLM using their respective coordinates.\n(mylat, mylon): Coordinates of Santiago de Chile.\nresp.city: Name of the country‚Äôs capital\n(resp.lat, resp.lon): Coordinates of the capital city are provided by the LLM response.\nunit = ‚Äòkm‚Äô: Specifies that the distance should be calculated in kilometers.\nprint: Outputs the distance, rounded to the nearest 10 kilometers, with thousands of separators for readability.\n\nRunning the code\nIf we enter different countries, for example, France, Colombia, and the United States, We can note that we always receive the same structured information:\nSantiago de Chile is about 8,060 kilometers away from\n    Washington, D.C..\nSantiago de Chile is about 4,250 kilometers away from Bogot√°.\nSantiago de Chile is about 11,630 kilometers away from Paris.\nIf you run the code as a script, the result will be printed on the terminal:\n \nAnd the calculations are pretty good!\n \n\nIn the 20-Ollama_Function_Calling notebook, it is possible to find experiments with all models installed.\n\n\n\n\nNow it is time to wrap up everything so far! Let‚Äôs modify the script so that instead of entering the country name (as a text), the user enters an image, and the application (based on SLM) returns the city in the image and its geographic location. With those data, we can calculate the distance as before.\n \nFor simplicity, we will implement this new code in two steps. First, the LLM will analyze the image and create a description (text). This text will be passed on to another instance, where the model will extract the information needed to pass along.\nWe will start importing the libraries\nimport sys\nimport time\nfrom haversine import haversine\nimport ollama\nfrom openai import OpenAI\nfrom pydantic import BaseModel, Field\nimport instructor\nWe can see the image if you run the code on the Jupyter Notebook. For that we need also import:\nimport matplotlib.pyplot as plt\nfrom PIL import Image\n\nThose libraries are unnecessary if we run the code as a script.\n\nNow, we define the model and the local coordinates:\nMODEL = \"llava-phi3:3.8b\"\nmylat = -33.33\nmylon = -70.51\nWe can download a new image, for example, Machu Picchu from Wikipedia. On the Notebook we can see it:\n# Load the image\nimg_path = \"image_test_3.jpg\"\nimg = Image.open(img_path)\n\n# Display the image\nplt.figure(figsize=(8, 8))\nplt.imshow(img)\nplt.axis(\"off\")\n# plt.title(\"Image\")\nplt.show()\n \nNow, let‚Äôs define a function that will receive the image and will return the decimal latitude and decimal longitude of the city in the image, its name, and what country it is located\ndef image_description(img_path):\n    with open(img_path, \"rb\") as file:\n        response = ollama.chat(\n            model=MODEL,\n            messages=[\n                {\n                    \"role\": \"user\",\n                    \"content\": \"\"\"return the decimal latitude and \\\n                 decimal longitude of the city in the image, \\\n                 its name, and what country it is located\"\"\",\n                    \"images\": [file.read()],\n                },\n            ],\n            options={\n                \"temperature\": 0,\n            },\n        )\n    # print(response['message']['content'])\n    return response[\"message\"][\"content\"]\n\nWe can print the entire response for debug purposes.\n\nThe image description generated for the function will be passed as a prompt for the model again.\nstart_time = time.perf_counter()  # Start timing\n\n\nclass CityCoord(BaseModel):\n    city: str = Field(\n        ..., description=\"Name of the city in the image\"\n    )\n    country: str = Field(\n        ...,\n        description=(\n            \"Name of the country where \"\n            \"the city in the image is located\"\n        ),\n    )\n    lat: float = Field(\n        ...,\n        description=(\"Decimal latitude of the city in \" \"the image\"),\n    )\n    lon: float = Field(\n        ...,\n        description=(\"Decimal longitude of the city in \" \"the image\"),\n    )\n\n\n# enables `response_model` in create call\nclient = instructor.patch(\n    OpenAI(base_url=\"http://localhost:11434/v1\", api_key=\"ollama\"),\n    mode=instructor.Mode.JSON,\n)\n\nimage_description = image_description(img_path)\n# Send this description to the model\nresp = client.chat.completions.create(\n    model=MODEL,\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": image_description,\n        }\n    ],\n    response_model=CityCoord,\n    max_retries=10,\n    temperature=0,\n)\nIf we print the image description , we will get:\nThe image shows the ancient city of Machu Picchu, located in\nPeru. The city is perched on a steep hillside and consists of\nvarious structures made of stone. It is surrounded by lush\ngreenery and towering mountains. The sky above is blue with\nscattered clouds.\n\nMachu Picchu's latitude is approximately 13.5086¬∞ S, and its\nlongitude is around 72.5494¬∞ W.\nAnd the second response from the model (resp) will be:\nCityCoord(city='Machu Picchu', country='Peru', lat=-13.5086,\n                lon=-72.5494)\nNow, we can do a ‚ÄúPost-Processing‚Äù, calculating the distance and preparing the final answer:\ndistance = haversine((mylat, mylon), (resp.lat, resp.lon), unit=\"km\")\n\nprint(\n    (\n        f\"\\nThe image shows {resp.city}, with lat: \"\n        f\"{round(resp.lat, 2)} and long: \"\n        f\"{round(resp.lon, 2)}, located in \"\n        f\"{resp.country} and about \"\n        f\"{int(round(distance, -1)):,} kilometers \"\n        f\"away from Santiago, Chile.\\n\"\n    )\n)\n\nend_time = time.perf_counter()  # End timing\nelapsed_time = end_time - start_time  # Calculate elapsed time\nprint(\n    f\"[INFO] ==&gt; The code (running {MODEL}), \"\n    f\"took {elapsed_time:.1f} seconds to execute.\\n\"\n)\nAnd we will get:\n The image shows Machu Picchu, with lat:-13.16 and long:\n -72.54, located in Peru  and about 2,250 kilometers away\n from Santiago, Chile.\n\nprint(\n    f\"[INFO] ==&gt; The code (running {MODEL}), \"\n    f\"took {elapsed_time:.1f} seconds \"\n    f\"to execute.\\n\"\n)\nIn the 30-Function_Calling_with_images notebook, it is possible to find the experiments with multiple images.\nLet‚Äôs now download the script calc_distance_image.py from the GitHub and run it on the terminal with the command:\npython calc_distance_image.py \\\n  /home/mjrovai/Documents/OLLAMA/image_test_3.jpg\nEnter with the Machu Picchu image full patch as an argument. We will get the same previous result.\n \nHow about Paris?\n \nOf course, there are many ways to optimize the code used here. Still, the idea is to explore the considerable potential of function calling with SLMs at the edge, allowing those models to integrate with external functions or APIs. Going beyond text generation, SLMs can access real-time data, automate tasks, and interact with various systems.\n\n\n\n\nLarge Language Models (LLMs) have revolutionized natural language processing, but their deployment and optimization come with unique challenges. One significant issue is the tendency for LLMs (and more, the SLMs) to generate plausible-sounding but factually incorrect information, a phenomenon known as hallucination. This occurs when models produce content that seems coherent but is not grounded in truth or real-world facts.\nOther challenges include the immense computational resources required for training and running these models, the difficulty in maintaining up-to-date knowledge within the model, and the need for domain-specific adaptations. Privacy concerns also arise when handling sensitive data during training or inference. Additionally, ensuring consistent performance across diverse tasks and maintaining ethical use of these powerful tools present ongoing challenges. Addressing these issues is crucial for the effective and responsible deployment of LLMs in real-world applications.\nThe fundamental techniques for enhancing LLM (and SLM) performance and efficiency are Fine-tuning, Prompt engineering, and Retrieval-Augmented Generation (RAG).\n\nFine-tuning, while more resource-intensive, offers a way to specialize LLMs for particular domains or tasks. This process involves further training the model on carefully curated datasets, allowing it to adapt its vast general knowledge to specific applications. Fine-tuning can lead to substantial improvements in performance, especially in specialized fields or for unique use cases.\nPrompt engineering is at the forefront of LLM optimization. By carefully crafting input prompts, we can guide models to produce more accurate and relevant outputs. This technique involves structuring queries that leverage the model‚Äôs pre-trained knowledge and capabilities, often incorporating examples or specific instructions to shape the desired response.\nRetrieval-Augmented Generation (RAG) represents another powerful approach to improving LLM performance. This method combines the vast knowledge embedded in pre-trained models with the ability to access and incorporate external, up-to-date information. By retrieving relevant data to supplement the model‚Äôs decision-making process, RAG can significantly enhance accuracy and reduce the likelihood of generating outdated or false information.\n\nFor edge applications, it is more beneficial to focus on techniques like RAG that can enhance model performance without needing on-device fine-tuning. Let‚Äôs explore it.\n\n\n\nIn a basic interaction between a user and a language model, the user asks a question, which is sent as a prompt to the model. The model generates a response based solely on its pre-trained knowledge. In a RAG process, there‚Äôs an additional step between the user‚Äôs question and the model‚Äôs response. The user‚Äôs question triggers a retrieval process from a knowledge base.\n \n\n\nHere are the steps to implement a basic Retrieval Augmented Generation (RAG):\n\nDetermine the type of documents you‚Äôll be using: The best types are documents from which we can get clean and unobscured text. PDFs can be problematic because they are designed for printing, not for extracting sensible text. To work with PDFs, we should get the source document or use tools to handle it.\nChunk the text: We can‚Äôt store the text as one long stream because of context size limitations and the potential for confusion. Chunking involves splitting the text into smaller pieces. Chunk text has many ways, such as character count, tokens, words, paragraphs, or sections. It is also possible to overlap chunks.\nCreate embeddings: Embeddings are numerical representations of text that capture semantic meaning. We create embeddings by passing each chunk of text through a particular embedding model. The model outputs a vector, the length of which depends on the embedding model used. We should pull one (or more) embedding models from Ollama, to perform this task. Here are some examples of embedding models available at Ollama.\n\n\n\nModel\nParameter Size\nEmbedding Size\n\n\n\n\nmxbai-embed-large\n334M\n1024\n\n\nnomic-embed-text\n137M\n768\n\n\nall-minilm\n23M\n384\n\n\n\n\nGenerally, larger embedding sizes capture more nuanced information about the input. Still, they also require more computational resources to process, and a higher number of parameters should increase the latency (but also the quality of the response).\n\nStore the chunks and embeddings in a vector database: We will need a way to efficiently find the most relevant chunks of text for a given prompt, which is where a vector database comes in. We will use Chromadb, an AI-native open-source vector database, which simplifies building RAGs by creating knowledge, facts, and skills pluggable for LLMs. Both the embedding and the source text for each chunk are stored.\nBuild the prompt: When we have a question, we create an embedding and query the vector database for the most similar chunks. Then, we select the top few results and include their text in the prompt.\n\nThe goal of RAG is to provide the model with the most relevant information from our documents, allowing it to generate more accurate and informative responses. So, let‚Äôs implement a simple example of an SLM incorporating a particular set of facts about bees (‚ÄúBee Facts‚Äù).\nInside the ollama env, enter the command in the terminal for Chromadb installation:\npip install ollama chromadb\nLet‚Äôs pull an intermediary embedding model, nomic-embed-text\nollama pull nomic-embed-text\nAnd create a working directory:\ncd Documents/OLLAMA/\nmkdir RAG-simple-bee\ncd RAG-simple-bee/\nLet‚Äôs create a new Jupyter notebook, 40-RAG-simple-bee for some exploration:\nImport the needed libraries:\nimport ollama\nimport chromadb\nimport time\nAnd define aor models:\nEMB_MODEL = \"nomic-embed-text\"\nMODEL = \"llama3.2:3B\"\nInitially, a knowledge base about bee facts should be created. This involves collecting relevant documents and converting them into vector embeddings. These embeddings are then stored in a vector database, allowing for efficient similarity searches later. Enter with the ‚Äúdocument,‚Äù a base of ‚Äúbee facts‚Äù as a list:\ndocuments = [\n    \"Bee-keeping, also known as apiculture, involves the \\\n    maintenance of bee colonies, typically in hives, by humans.\",\n    \"The most commonly kept species of bees is the European \\\n    honey bee (Apis  mellifera).\",\n\n    ...\n\n    \"There are another 20,000 different bee species in \\\n    the world.\",\n    \"Brazil alone has more than 300 different bee species, and \\\n    the vast majority, unlike western honey bees, don‚Äôt sting.\",\n    \"Reports written in 1577 by Hans Staden, mention three \\\n    native bees used by indigenous people in Brazil.\", \\\n    \"The indigenous people in Brazil used bees for medicine \\\n    and food purposes\",\n    \"From Hans Staden report: probable species: manda√ßaia \\\n    (Melipona quadrifasciata), mandaguari (Scaptotrigona \\\n    postica) and jata√≠-amarela (Tetragonisca angustula).\"\n]\n\nWe do not need to ‚Äúchunk‚Äù the document here because we will use each element of the list and a chunk.\n\nNow, we will create our vector embedding database bee_facts and store the document in it:\nclient = chromadb.Client()\ncollection = client.create_collection(name=\"bee_facts\")\n\n# store each document in a vector embedding database\nfor i, d in enumerate(documents):\n    response = ollama.embeddings(model=EMB_MODEL, prompt=d)\n    embedding = response[\"embedding\"]\n    collection.add(\n        ids=[str(i)], embeddings=[embedding], documents=[d]\n    )\nNow that we have our ‚ÄúKnowledge Base‚Äù created, we can start making queries, retrieving data from it:\n \nUser Query: The process begins when a user asks a question, such as ‚ÄúHow many bees are in a colony? Who lays eggs, and how much? How about common pests and diseases?‚Äù\nprompt = \"How many bees are in a colony? Who lays eggs and \\\n          how much? How about common pests and diseases?\"\nQuery Embedding: The user‚Äôs question is converted into a vector embedding using the same embedding model used for the knowledge base.\nresponse = ollama.embeddings(prompt=prompt, model=EMB_MODEL)\nRelevant Document Retrieval: The system searches the knowledge base using the query embedding to find the most relevant documents (in this case, the 5 more probable). This is done using a similarity search, which compares the query embedding to the document embeddings in the database.\nresults = collection.query(\n    query_embeddings=[response[\"embedding\"]], n_results=5\n)\ndata = results[\"documents\"]\nPrompt Augmentation: The retrieved relevant information is combined with the original user query to create an augmented prompt. This prompt now contains the user‚Äôs question and pertinent facts from the knowledge base.\nprompt = (\n    f\"Using this data: {data}. \" f\"Respond to this prompt: {prompt}\"\n)\nAnswer Generation: The augmented prompt is then fed into a language model, in this case, the llama3.2:3b model. The model uses this enriched context to generate a comprehensive answer. Parameters like temperature, top_k, and top_p are set to control the randomness and quality of the generated response.\noutput = ollama.generate(\n  model=MODEL,\n  prompt = (\n      f\"Using this data: {data}. \"\n      f\"Respond to this prompt: {prompt}\"\n  )\n\n  options={\n    \"temperature\": 0.0,\n    \"top_k\":10,\n    \"top_p\":0.5                          }\n)\nResponse Delivery: Finally, the system returns the generated answer to the user.\nprint(output[\"response\"])\nBased on the provided data, here are the answers to your \\\nquestions:\n\n1. How many bees are in a colony?\nA typical bee colony can contain between 20,000 and 80,000 bees.\n\n2. Who lays eggs and how much?\nThe queen bee lays up to 2,000 eggs per day during peak seasons.\n\n3. What about common pests and diseases?\nCommon pests and diseases that affect bees include varroa \\\nmites, hive beetles, and foulbrood.\nLet‚Äôs create a function to help answer new questions:\ndef rag_bees(prompt, n_results=5, temp=0.0, top_k=10, top_p=0.5):\n    start_time = time.perf_counter()  # Start timing\n\n    # generate an embedding for the prompt and retrieve the data\n    response = ollama.embeddings(\n      prompt=prompt,\n      model=EMB_MODEL\n    )\n\n    results = collection.query(\n      query_embeddings=[response[\"embedding\"]],\n      n_results=n_results\n    )\n    data = results['documents']\n\n    # generate a response combining the prompt and data retrieved\n    output = ollama.generate(\n      model=MODEL,\n      prompt = (\n         f\"Using this data: {data}. \"\n         f\"Respond to this prompt: {prompt}\"\n      )\n\n      options={\n        \"temperature\": temp,\n        \"top_k\": top_k,\n        \"top_p\": top_p                          }\n    )\n\n    print(output['response'])\n\n    end_time = time.perf_counter()  # End timing\n    elapsed_time = round(\n       (end_time - start_time), 1\n    )  # Calculate elapsed time\n\nprint(\n    f\"\\n[INFO] ==&gt; The code for model: {MODEL}, \"\n    f\"took {elapsed_time}s to generate the answer.\\n\"\n)\n\n    print(\n       f\"\\n[INFO] ==&gt; The code for model: {MODEL}, \"\n       f\"took {elapsed_time}s to generate the answer.\\n\"\n    )\nWe can now create queries and call the function:\nprompt = \"Are bees in Brazil?\"\nrag_bees(prompt)\nYes, bees are found in Brazil. According to the data, Brazil \\\nhas more than 300 different bee species, and indigenous people \\\nin Brazil used bees for medicine and food purposes. \\\nAdditionally, reports from 1577 mention three native bees \\\nused by indigenous people in Brazil.\n\n [INFO] ==&gt; The code for model: llama3.2:3b, took 22.7s to \\\n generate the answer.\nBy the way, if the model used supports multiple languages, we can use it (for example, Portuguese), even if the dataset was created in English:\nprompt = \"Existem abelhas no Brazil?\"\nrag_bees(prompt)\nSim, existem abelhas no Brasil! De acordo com o relato de Hans \\\nStaden, h√° tr√™s esp√©cies de abelhas nativas do Brasil que \\\nforam mencionadas: manda√ßaia (Melipona quadrifasciata), \\\nmandaguari (Scaptotrigona postica) e jata√≠-amarela \\\n(Tetragonisca angustula). Al√©m disso, o Brasil √© conhecido \\\npor ter mais de 300 esp√©cies diferentes de abelhas, a \\\nmaioria das quais n√£o √© agressiva e n√£o p√µe veneno.\n\n [INFO] ==&gt; The code for model: llama3.2:3b, took 54.6s to \\\n            generate the answer.\n\n\n\nThe small LLM models tested worked well at the edge, both in text and with images, but of course, they had high latency regarding the last one. A combination of specific and dedicated models can lead to better results; for example, in real cases, an Object Detection model (such as YOLO) can get a general description and count of objects on an image that, once passed to an LLM, can help extract essential insights and actions.\nAccording to Avi Baum, CTO at Hailo,\n\nIn the vast landscape of artificial intelligence (AI), one of the most intriguing journeys has been the evolution of AI on the edge. This journey has taken us from classic machine vision to the realms of discriminative AI, enhancive AI, and now, the groundbreaking frontier of generative AI. Each step has brought us closer to a future where intelligent systems seamlessly integrate with our daily lives, offering an immersive experience of not just perception but also creation at the palm of our hand.\n\n\n\n\n\n\nThis lab has demonstrated how a Raspberry Pi 5 can be transformed into a potent AI hub capable of running large language models (LLMs) for real-time, on-site data analysis and insights using Ollama and Python. The Raspberry Pi‚Äôs versatility and power, coupled with the capabilities of lightweight LLMs like Llama 3.2 and LLaVa-Phi-3-mini, make it an excellent platform for edge computing applications.\nThe potential of running LLMs on the edge extends far beyond simple data processing, as in this lab‚Äôs examples. Here are some innovative suggestions for using this project:\n1. Smart Home Automation:\n\nIntegrate SLMs to interpret voice commands or analyze sensor data for intelligent home automation. This could include real-time monitoring and control of home devices, security systems, and energy management, all processed locally without relying on cloud services.\n\n2. Field Data Collection and Analysis:\n\nDeploy SLMs on Raspberry Pi in remote or mobile setups for real-time data collection and analysis. This can be used in agriculture to monitor crop health, in environmental studies for wildlife tracking, or in disaster response for situational awareness and resource management.\n\n3. Educational Tools:\n\nCreate interactive educational tools that leverage SLMs to provide instant feedback, language translation, and tutoring. This can be particularly useful in developing regions with limited access to advanced technology and internet connectivity.\n\n4. Healthcare Applications:\n\nUse SLMs for medical diagnostics and patient monitoring. They can provide real-time analysis of symptoms and suggest potential treatments. This can be integrated into telemedicine platforms or portable health devices.\n\n5. Local Business Intelligence:\n\nImplement SLMs in retail or small business environments to analyze customer behavior, manage inventory, and optimize operations. The ability to process data locally ensures privacy and reduces dependency on external services.\n\n6. Industrial IoT:\n\nIntegrate SLMs into industrial IoT systems for predictive maintenance, quality control, and process optimization. The Raspberry Pi can serve as a localized data processing unit, reducing latency and improving the reliability of automated systems.\n\n7. Autonomous Vehicles:\n\nUse SLMs to process sensory data from autonomous vehicles, enabling real-time decision-making and navigation. This can be applied to drones, robots, and self-driving cars for enhanced autonomy and safety.\n\n8. Cultural Heritage and Tourism:\n\nImplement SLMs to provide interactive and informative cultural heritage sites and museum guides. Visitors can use these systems to get real-time information and insights, enhancing their experience without internet connectivity.\n\n9. Artistic and Creative Projects:\n\nUse SLMs to analyze and generate creative content, such as music, art, and literature. This can foster innovative projects in the creative industries and allow for unique interactive experiences in exhibitions and performances.\n\n10. Customized Assistive Technologies:\n\nDevelop assistive technologies for individuals with disabilities, providing personalized and adaptive support through real-time text-to-speech, language translation, and other accessible tools.\n\n\n\n\n\n10-Ollama_Python_Library notebook\n20-Ollama_Function_Calling notebook\n30-Function_Calling_with_images notebook\n40-RAG-simple-bee notebook\ncalc_distance_image python script",
    "crumbs": [
      "Raspberry Pi",
      "Small Language Models (SLM)"
    ]
  },
  {
    "objectID": "contents/raspi/llm/llm.html#sec-small-language-models-slm-overview-ef83",
    "href": "contents/raspi/llm/llm.html#sec-small-language-models-slm-overview-ef83",
    "title": "Small Language Models (SLM)",
    "section": "",
    "text": "In the fast-growing area of artificial intelligence, edge computing presents an opportunity to decentralize capabilities traditionally reserved for powerful, centralized servers. This lab explores the practical integration of small versions of traditional large language models (LLMs) into a Raspberry Pi 5, transforming this edge device into an AI hub capable of real-time, on-site data processing.\nAs large language models grow in size and complexity, Small Language Models (SLMs) offer a compelling alternative for edge devices, striking a balance between performance and resource efficiency. By running these models directly on Raspberry Pi, we can create responsive, privacy-preserving applications that operate even in environments with limited or no internet connectivity.\nThis lab will guide you through setting up, optimizing, and leveraging SLMs on Raspberry Pi. We will explore the installation and utilization of Ollama. This open-source framework allows us to run LLMs locally on our machines (our desktops or edge devices such as the Raspberry Pis or NVidia Jetsons). Ollama is designed to be efficient, scalable, and easy to use, making it a good option for deploying AI models such as Microsoft Phi, Google Gemma, Meta Llama, and LLaVa (Multimodal). We will integrate some of those models into projects using Python‚Äôs ecosystem, exploring their potential in real-world scenarios (or at least point in this direction).",
    "crumbs": [
      "Raspberry Pi",
      "Small Language Models (SLM)"
    ]
  },
  {
    "objectID": "contents/raspi/llm/llm.html#sec-small-language-models-slm-setup-ceea",
    "href": "contents/raspi/llm/llm.html#sec-small-language-models-slm-setup-ceea",
    "title": "Small Language Models (SLM)",
    "section": "",
    "text": "We could use any Raspi model in the previous labs, but here, the choice must be the Raspberry Pi 5 (Raspi-5). It is a robust platform that substantially upgrades the last version 4, equipped with the Broadcom BCM2712, a 2.4 GHz quad-core 64-bit Arm Cortex-A76 CPU featuring Cryptographic Extension and enhanced caching capabilities. It boasts a VideoCore VII GPU, dual 4Kp60 HDMI¬Æ outputs with HDR, and a 4Kp60 HEVC decoder. Memory options include 4 GB and 8 GB of high-speed LPDDR4X SDRAM, with 8GB being our choice to run SLMs. It also features expandable storage via a microSD card slot and a PCIe 2.0 interface for fast peripherals such as M.2 SSDs (Solid State Drives).\n\nFor real SSL applications, SSDs are a better option than SD cards.\n\nBy the way, as Alasdair Allan discussed, inferencing directly on the Raspberry Pi 5 CPU‚Äîwith no GPU acceleration‚Äîis now on par with the performance of the Coral TPU.\n \nFor more info, please see the complete article: Benchmarking TensorFlow and TensorFlow Lite on Raspberry Pi 5.\n\n\nWe suggest installing an Active Cooler, a dedicated clip-on cooling solution for Raspberry Pi 5 (Raspi-5), for this lab. It combines an aluminum heat sink with a temperature-controlled blower fan to keep the Raspi-5 operating comfortably under heavy loads, such as running SLMs.\n \nThe Active Cooler has pre-applied thermal pads for heat transfer and is mounted directly to the Raspberry Pi 5 board using spring-loaded push pins. The Raspberry Pi firmware actively manages it: at 60¬∞C, the blower‚Äôs fan will be turned on; at 67.5¬∞C, the fan speed will be increased; and finally, at 75¬∞C, the fan increases to full speed. The blower‚Äôs fan will spin down automatically when the temperature drops below these limits.\n \n\nTo prevent overheating, all Raspberry Pi boards begin to throttle the processor when the temperature reaches 80¬∞Cand throttle even further when it reaches the maximum temperature of 85¬∞C (more detail here).",
    "crumbs": [
      "Raspberry Pi",
      "Small Language Models (SLM)"
    ]
  },
  {
    "objectID": "contents/raspi/llm/llm.html#sec-small-language-models-slm-generative-ai-genai-fc75",
    "href": "contents/raspi/llm/llm.html#sec-small-language-models-slm-generative-ai-genai-fc75",
    "title": "Small Language Models (SLM)",
    "section": "",
    "text": "Generative AI is an artificial intelligence system capable of creating new, original content across various mediums such as text, images, audio, and video. These systems learn patterns from existing data and use that knowledge to generate novel outputs that didn‚Äôt previously exist. Large Language Models (LLMs), Small Language Models (SLMs), and multimodal models can all be considered types of GenAI when used for generative tasks.\nGenAI provides the conceptual framework for AI-driven content creation, with LLMs serving as powerful general-purpose text generators. SLMs adapt this technology for edge computing, while multimodal models extend GenAI capabilities across different data types. Together, they represent a spectrum of generative AI technologies, each with its strengths and applications, collectively driving AI-powered content creation and understanding.\n\n\nLarge Language Models (LLMs) are advanced artificial intelligence systems that understand, process, and generate human-like text. These models are characterized by their massive scale in terms of the amount of data they are trained on and the number of parameters they contain. Critical aspects of LLMs include:\n\nSize: LLMs typically contain billions of parameters. For example, GPT-3 has 175 billion parameters, while some newer models exceed a trillion parameters.\nTraining Data: They are trained on vast amounts of text data, often including books, websites, and other diverse sources, amounting to hundreds of gigabytes or even terabytes of text.\nArchitecture: Most LLMs use transformer-based architectures, which allow them to process and generate text by paying attention to different parts of the input simultaneously.\nCapabilities: LLMs can perform a wide range of language tasks without specific fine-tuning, including:\n\nText generation\nTranslation\nSummarization\nQuestion answering\nCode generation\nLogical reasoning\n\nFew-shot Learning: They can often understand and perform new tasks with minimal examples or instructions.\nResource-Intensive: Due to their size, LLMs typically require significant computational resources to run, often needing powerful GPUs or TPUs.\nContinual Development: The field of LLMs is rapidly evolving, with new models and techniques constantly emerging.\nEthical Considerations: The use of LLMs raises important questions about bias, misinformation, and the environmental impact of training such large models.\nApplications: LLMs are used in various fields, including content creation, customer service, research assistance, and software development.\nLimitations: Despite their power, LLMs can produce incorrect or biased information and lack true understanding or reasoning capabilities.\n\nWe must note that we use large models beyond text, calling them multi-modal models. These models integrate and process information from multiple types of input simultaneously. They are designed to understand and generate content across various forms of data, such as text, images, audio, and video.\n\n\n\nClosed models, also called proprietary models, are AI models whose internal workings, code, and training data are not publicly disclosed. Examples: GPT-4 (by OpenAI), Claude (by Anthropic), Gemini (by Google).\nOpen models, also known as open-source models, are AI models whose underlying code, architecture, and often training data are publicly available and accessible. Examples: Gemma (by Google), LLaMA (by Meta) and Phi (by Microsoft).\nOpen models are particularly relevant for running models on edge devices like Raspberry Pi as they can be more easily adapted, optimized, and deployed in resource-constrained environments. Still, it is crucial to verify their Licenses. Open models come with various open-source licenses that may affect their use in commercial applications, while closed models have clear, albeit restrictive, terms of service.\n\n\n\nAdapted from arXiv\n\n\n\n\n\nIn the context of edge computing on devices like Raspberry Pi, full-scale LLMs are typically too large and resource-intensive to run directly. This limitation has driven the development of smaller, more efficient models, such as the Small Language Models (SLMs).\nSLMs are compact versions of LLMs designed to run efficiently on resource-constrained devices such as smartphones, IoT devices, and single-board computers like the Raspberry Pi. These models are significantly smaller in size and computational requirements than their larger counterparts while still retaining impressive language understanding and generation capabilities.\nKey characteristics of SLMs include:\n\nReduced parameter count: Typically ranging from a few hundred million to a few billion parameters, compared to two-digit billions in larger models.\nLower memory footprint: Requiring, at most, a few gigabytes of memory rather than tens or hundreds of gigabytes.\nFaster inference time: Can generate responses in milliseconds to seconds on edge devices.\nEnergy efficiency: Consuming less power, making them suitable for battery-powered devices.\nPrivacy-preserving: Enabling on-device processing without sending data to cloud servers.\nOffline functionality: Operating without an internet connection.\n\nSLMs achieve their compact size through various techniques such as knowledge distillation, model pruning, and quantization. While they may not match the broad capabilities of larger models, SLMs excel in specific tasks and domains, making them ideal for targeted applications on edge devices.\n\nWe will generally consider SLMs, language models with less than 5 billion parameters quantized to 4 bits.\n\nExamples of SLMs include compressed versions of models like Meta Llama, Microsoft PHI, and Google Gemma. These models enable a wide range of natural language processing tasks directly on edge devices, from text classification and sentiment analysis to question answering and limited text generation.\nFor more information on SLMs, the paper, LLM Pruning and Distillation in Practice: The Minitron Approach, provides an approach applying pruning and distillation to obtain SLMs from LLMs. And, SMALL LANGUAGE MODELS: SURVEY, MEASUREMENTS, AND INSIGHTS, presents a comprehensive survey and analysis of Small Language Models (SLMs), which are language models with 100 million to 5 billion parameters designed for resource-constrained devices.",
    "crumbs": [
      "Raspberry Pi",
      "Small Language Models (SLM)"
    ]
  },
  {
    "objectID": "contents/raspi/llm/llm.html#sec-small-language-models-slm-ollama-0b03",
    "href": "contents/raspi/llm/llm.html#sec-small-language-models-slm-ollama-0b03",
    "title": "Small Language Models (SLM)",
    "section": "",
    "text": "ollama logo\n\n\nOllama is an open-source framework that allows us to run language models (LMs), large or small, locally on our machines. Here are some critical points about Ollama:\n\nLocal Model Execution: Ollama enables running LMs on personal computers or edge devices such as the Raspi-5, eliminating the need for cloud-based API calls.\nEase of Use: It provides a simple command-line interface for downloading, running, and managing different language models.\nModel Variety: Ollama supports various LLMs, including Phi, Gemma, Llama, Mistral, and other open-source models.\nCustomization: Users can create and share custom models tailored to specific needs or domains.\nLightweight: Designed to be efficient and run on consumer-grade hardware.\nAPI Integration: Offers an API that allows integration with other applications and services.\nPrivacy-Focused: By running models locally, it addresses privacy concerns associated with sending data to external servers.\nCross-Platform: Available for macOS, Windows, and Linux systems (our case, here).\nActive Development: Regularly updated with new features and model support.\nCommunity-Driven: Benefits from community contributions and model sharing.\n\nTo learn more about what Ollama is and how it works under the hood, you should see this short video from Matt Williams, one of the founders of Ollama:\n\n\nMatt has an entirely free course about Ollama that we recommend: \n\n\n\nLet‚Äôs set up and activate a Virtual Environment for working with Ollama:\npython3 -m venv ~/ollama\nsource ~/ollama/bin/activate\nAnd run the command to install Ollama:\ncurl -fsSL https://ollama.com/install.sh | sh\nAs a result, an API will run in the background on 127.0.0.1:11434. From now on, we can run Ollama via the terminal. For starting, let‚Äôs verify the Ollama version, which will also tell us that it is correctly installed:\nollama -v\n \nOn the Ollama Library page, we can find the models Ollama supports. For example, by filtering by Most popular, we can see Meta Llama, Google Gemma, Microsoft Phi, LLaVa, etc.\n\n\n\n\n\n\n\n\nLet‚Äôs install and run our first small language model, Llama 3.2 1B (and 3B). The Meta Llama 3.2 series comprises a set of multilingual generative language models available in 1 billion and 3 billion parameter sizes. These models are designed to process text input and generate text output. The instruction-tuned variants within this collection are specifically optimized for multilingual conversational applications, including tasks involving information retrieval and summarization with an agentic approach. When compared to many existing open-source and proprietary chat models, the Llama 3.2 instruction-tuned models demonstrate superior performance on widely-used industry benchmarks.\nThe 1B and 3B models were pruned from the Llama 8B, and then logits from the 8B and 70B models were used as token-level targets (token-level distillation). Knowledge distillation was used to recover performance (they were trained with 9 trillion tokens). The 1B model has 1,24B, quantized to integer (Q8_0), and the 3B, 3.12B parameters, with a Q4_0 quantization, which ends with a size of 1.3 GB and 2 GB, respectively. Its context window is 131,072 tokens.\n \nInstall and run the Model\nollama run llama3.2:1b\nRunning the model with the command before, we should have the Ollama prompt available for us to input a question and start chatting with the LLM model; for example,\n&gt;&gt;&gt; What is the capital of France?\nAlmost immediately, we get the correct answer:\nThe capital of France is Paris.\nUsing the option --verbose when calling the model will generate several statistics about its performance (The model will be polling only the first time we run the command).\n \nEach metric gives insights into how the model processes inputs and generates outputs. Here‚Äôs a breakdown of what each metric means:\n\nTotal Duration (2.620170326 s): This is the complete time taken from the start of the command to the completion of the response. It encompasses loading the model, processing the input prompt, and generating the response.\nLoad Duration (39.947908 ms): This duration indicates the time to load the model or necessary components into memory. If this value is minimal, it can suggest that the model was preloaded or that only a minimal setup was required.\nPrompt Eval Count (32 tokens): The number of tokens in the input prompt. In NLP, tokens are typically words or subwords, so this count includes all the tokens that the model evaluated to understand and respond to the query.\nPrompt Eval Duration (1.644773 s): This measures the model‚Äôs time to evaluate or process the input prompt. It accounts for the bulk of the total duration, implying that understanding the query and preparing a response is the most time-consuming part of the process.\nPrompt Eval Rate (19.46 tokens/s): This rate indicates how quickly the model processes tokens from the input prompt. It reflects the model‚Äôs speed in terms of natural language comprehension.\nEval Count (8 token(s)): This is the number of tokens in the model‚Äôs response, which in this case was, ‚ÄúThe capital of France is Paris.‚Äù\nEval Duration (889.941 ms): This is the time taken to generate the output based on the evaluated input. It‚Äôs much shorter than the prompt evaluation, suggesting that generating the response is less complex or computationally intensive than understanding the prompt.\nEval Rate (8.99 tokens/s): Similar to the prompt eval rate, this indicates the speed at which the model generates output tokens. It‚Äôs a crucial metric for understanding the model‚Äôs efficiency in output generation.\n\nThis detailed breakdown can help understand the computational demands and performance characteristics of running SLMs like Llama on edge devices like the Raspberry Pi 5. It shows that while prompt evaluation is more time-consuming, the actual generation of responses is relatively quicker. This analysis is crucial for optimizing performance and diagnosing potential bottlenecks in real-time applications.\nLoading and running the 3B model, we can see the difference in performance for the same prompt;\n \nThe eval rate is lower, 5.3 tokens/s versus 9 tokens/s with the smaller model.\nWhen question about\n&gt;&gt;&gt; What is the distance between Paris and Santiago, Chile?\nThe 1B model answered 9,841 kilometers (6,093 miles), which is inaccurate, and the 3B model answered 7,300 miles (11,700 km), which is close to the correct (11,642 km).\nLet‚Äôs ask for the Paris‚Äôs coordinates:\n&gt;&gt;&gt; what is the latitude and longitude of Paris?\nThe latitude and longitude of Paris are 48.8567¬∞ N (48¬∞55'\n42\" N) and 2.3510¬∞ E (2¬∞22' 8\" E), respectively.\n\n\n\n\n\nBoth 1B and 3B models gave correct answers.\n\n\n\nLet‚Äôs install Gemma 2, a high-performing and efficient model available in three sizes: 2B, 9B, and 27B. We will install Gemma 2 2B, a lightweight model trained with 2 trillion tokens that produces outsized results by learning from larger models through distillation. The model has 2.6 billion parameters and a Q4_0 quantization, which ends with a size of 1.6 GB. Its context window is 8,192 tokens.\n \nInstall and run the Model\nollama run gemma2:2b --verbose\nRunning the model with the command before, we should have the Ollama prompt available for us to input a question and start chatting with the LLM model; for example,\n&gt;&gt;&gt; What is the capital of France?\nAlmost immediately, we get the correct answer:\nThe capital of France is **Paris**. üóº\nAnd it‚Äô statistics.\n \nWe can see that Gemma 2:2B has around the same performance as Llama 3.2:3B, but having less parameters.\nOther examples:\n&gt;&gt;&gt; What is the distance between Paris and Santiago, Chile?\n\nThe distance between Paris, France and Santiago, Chile is\napproximately **7,000 miles (11,267 kilometers)**.\n\nKeep in mind that this is a straight-line distance, and actual\ntravel distance can vary depending on the chosen routes and any\nstops along the way. ‚úàÔ∏è`\nAlso, a good response but less accurate than Llama3.2:3B.\n&gt;&gt;&gt; what is the latitude and longitude of Paris?\n\nYou got it! Here are the latitudes and longitudes of Paris,\nFrance:\n\n* **Latitude**: 48.8566¬∞ N (north)\n* **Longitude**: 2.3522¬∞ E (east)\n\nLet me know if you'd like to explore more about Paris or its\nlocation! üóºüá´üá∑\nA good and accurate answer (a little more verbose than the Llama answers).\n\n\n\nLet‚Äôs pull a bigger (but still tiny) model, the PHI3.5, a 3.8B lightweight state-of-the-art open model by Microsoft. The model belongs to the Phi-3 model family and supports 128K token context length and the languages: Arabic, Chinese, Czech, Danish, Dutch, English, Finnish, French, German, Hebrew, Hungarian, Italian, Japanese, Korean, Norwegian, Polish, Portuguese, Russian, Spanish, Swedish, Thai, Turkish and Ukrainian.\nThe model size, in terms of bytes, will depend on the specific quantization format used. The size can go from 2-bit quantization (q2_k) of 1.4 GB (higher performance/lower quality) to 16-bit quantization (fp-16) of 7.6 GB (lower performance/higher quality).\nLet‚Äôs run the 4-bit quantization (Q4_0), which will need 2.2 GB of RAM, with an intermediary trade-off regarding output quality and performance.\nollama run phi3.5:3.8b --verbose\n\nYou can use run or pull to download the model. What happens is that Ollama keeps note of the pulled models, and once the PHI3 does not exist, before running it, Ollama pulls it.\n\nLet‚Äôs enter with the same prompt used before:\n&gt;&gt;&gt; What is the capital of France?\n\nThe capital of France is Paris. It' extradites significant\nhistorical, cultural, and political importance to the country as\nwell as being a major European city known for its art, fashion,\ngastronomy, and culture. Its influence extends beyond national\nborders, with millions of tourists visiting each year from around\nthe globe. The Seine River flows through Paris before it reaches\nthe broader English Channel at Le Havre. Moreover, France is one\nof Europe's leading economies with its capital playing a key role\n\n...\nThe answer was very ‚Äúverbose‚Äù, let‚Äôs specify a better prompt:\n \nIn this case, the answer was still longer than we expected, with an eval rate of 2.25 tokens/s, more than double that of Gemma and Llama.\n\nChoosing the most appropriate prompt is one of the most important skills to be used with LLMs, no matter its size.\n\nWhen we asked the same questions about distance and Latitude/Longitude, we did not get a good answer for a distance of 13,507 kilometers (8,429 miles), but it was OK for coordinates. Again, it could have been less verbose (more than 200 tokens for each answer).\nWe can use any model as an assistant since their speed is relatively decent, but on September 24 (2023), the Llama2:3B is a better choice. You should try other models, depending on your needs. ü§ó Open LLM Leaderboard can give you an idea about the best models in size, benchmark, license, etc.\n\nThe best model to use is the one fit for your specific necessity. Also, take into consideration that this field evolves with new models everyday.\n\n\n\n\nMultimodal models are artificial intelligence (AI) systems that can process and understand information from multiple sources, such as images, text, audio, and video. In our context, multimodal LLMs can process various inputs, including text, images, and audio, as prompts and convert those prompts into various outputs, not just the source type.\nWe will work here with LLaVA-Phi-3, a fine-tuned LLaVA model from Phi 3 Mini 4k. It has strong performance benchmarks that are on par with the original LLaVA (Large Language and Vision Assistant) model.\nThe LLaVA-Phi-3 is an end-to-end trained large multimodal model designed to understand and generate content based on visual inputs (images) and textual instructions. It combines the capabilities of a visual encoder and a language model to process and respond to multimodal inputs.\nLet‚Äôs install the model:\nollama run llava-phi3:3.8b --verbose\nLet‚Äôs start with a text input:\n&gt;&gt;&gt; You are a helpful AI assistant.\n    What is the capital of France?\n\nAs an AI language model, I can tell you that the capital of\nFrance is Paris. It's not only the largest city in the country\nbut also serves as its political and administrative center.\nParis is known for its iconic landmarks such as the Eiffel\nTower, Notre-Dame Cathedral, and the Louvre Museum. The city\nhas a rich history, beautiful architecture, and is widely\nconsidered to be one of the most romantic cities in the world.\nThe response took around 30 s, with an eval rate of 3.93 tokens/s! Not bad!\nBut let us know to enter with an image as input. For that, let‚Äôs create a directory for working:\ncd Documents/\nmkdir OLLAMA\ncd OLLAMA\nLet‚Äôs download a \\(640\\times 320\\) image from the internet, for example (Wikipedia: Paris, France):\n \nUsing FileZilla, for example, let‚Äôs upload the image to the OLLAMA folder at the Raspi-5 and name it image_test_1.jpg. We should have the whole image path (we can use pwd to get it).\n/home/mjrovai/Documents/OLLAMA/image_test_1.jpg\nIf you use a desktop, you can copy the image path by clicking the image with the mouse‚Äôs right button.\n \nLet‚Äôs enter with this prompt:\n&gt;&gt;&gt; Describe the image /home/mjrovai/Documents/OLLAMA/\\\n                        image_test_1.jpg\nThe result was great, but the overall latency was significant; almost 4 minutes to perform the inference.\n \n\n\n\nUsing htop, we can monitor the resources running on our device.\nhtop\nDuring the time that the model is running, we can inspect the resources:\n \nAll four CPUs run at almost 100% of their capacity, and the memory used with the model loaded is 3.24 GB. Exiting Ollama, the memory goes down to around 377 MB (with no desktop).\nIt is also essential to monitor the temperature. When running the Raspberry with a desktop, you can have the temperature shown on the taskbar:\n \nIf you are ‚Äúheadless‚Äù, the temperature can be monitored with the command:\nvcgencmd measure_temp\nIf you are doing nothing, the temperature is around 50¬∞C for CPUs running at 1%. During inference, with the CPUs at 100%, the temperature can rise to almost 70¬∞C. This is OK and means the active cooler is working, keeping the temperature below 80¬∞C / 85¬∞C (its limit).",
    "crumbs": [
      "Raspberry Pi",
      "Small Language Models (SLM)"
    ]
  },
  {
    "objectID": "contents/raspi/llm/llm.html#sec-small-language-models-slm-ollama-python-library-fea5",
    "href": "contents/raspi/llm/llm.html#sec-small-language-models-slm-ollama-python-library-fea5",
    "title": "Small Language Models (SLM)",
    "section": "",
    "text": "So far, we have explored SLMs‚Äô chat capability using the command line on a terminal. However, we want to integrate those models into our projects, so Python seems to be the right path. The good news is that Ollama has such a library.\nThe Ollama Python library simplifies interaction with advanced LLM models, enabling more sophisticated responses and capabilities, besides providing the easiest way to integrate Python 3.8+ projects with Ollama.\nFor a better understanding of how to create apps using Ollama with Python, we can follow Matt Williams‚Äôs videos, as the one below:\n\nInstallation:\nIn the terminal, run the command:\npip install ollama\nWe will need a text editor or an IDE to create a Python script. If you run the Raspberry OS on a desktop, several options, such as Thonny and Geany, have already been installed by default (accessed by [Menu][Programming]). You can download other IDEs, such as Visual Studio Code, from [Menu][Recommended Software]. When the window pops up, go to [Programming], select the option of your choice, and press [Apply].\n \nIf you prefer using Jupyter Notebook for development:\npip install jupyter\njupyter notebook --generate-config\nTo run Jupyter Notebook, run the command (change the IP address for yours):\njupyter notebook --ip=192.168.4.209 --no-browser\nOn the terminal, you can see the local URL address to open the notebook:\n \nWe can access it from another computer by entering the Raspberry Pi‚Äôs IP address and the provided token in a web browser (we should copy it from the terminal).\nIn our working directory in the Raspi, we will create a new Python 3 notebook.\nLet‚Äôs enter with a very simple script to verify the installed models:\nimport ollama\n\nollama.list()\nAll the models will be printed as a dictionary, for example:\n  {'name': 'gemma2:2b',\n   'model': 'gemma2:2b',\n   'modified_at': '2024-09-24T19:30:40.053898094+01:00',\n   'size': 1629518495,\n   'digest': (\n     '8ccf136fdd5298f3ffe2d69862750ea7fb56555fa4d5b18c0'\n     '4e3fa4d82ee09d7'\n    ),\n\n   'details': {'parent_model': '',\n    'format': 'gguf',\n    'family': 'gemma2',\n    'families': ['gemma2'],\n    'parameter_size': '2.6B',\n    'quantization_level': 'Q4_0'}}]}\nLet‚Äôs repeat one of the questions that we did before, but now using ollama.generate() from Ollama python library. This API will generate a response for the given prompt with the provided model. This is a streaming endpoint, so there will be a series of responses. The final response object will include statistics and additional data from the request.\nMODEL = \"gemma2:2b\"\nPROMPT = \"What is the capital of France?\"\n\nres = ollama.generate(model=MODEL, prompt=PROMPT)\nprint(res)\nIn case you are running the code as a Python script, you should save it, for example, test_ollama.py. You can use the IDE to run it or do it directly on the terminal. Also, remember that you should always call the model and define it when running a stand-alone script.\npython test_ollama.py\nAs a result, we will have the model response in a JSON format:\n{\n  'model': 'gemma2:2b',\n  'created_at': '2024-09-25T14:43:31.869633807Z',\n  'response': 'The capital of France is **Paris**.\\n',\n  'done': True,\n  'done_reason': 'stop',\n  'context': [\n      106, 1645, 108, 1841, 603, 573, 6037, 576, 6081, 235336,\n      107, 108, 106, 2516, 108, 651, 6037, 576, 6081, 603, 5231,\n      29437, 168428, 235248, 244304, 241035, 235248, 108\n  ],\n  'total_duration': 24259469458,\n  'load_duration': 19830013859,\n  'prompt_eval_count': 16,\n  'prompt_eval_duration': 1908757000,\n  'eval_count': 14,\n  'eval_duration': 2475410000\n}\nAs we can see, several pieces of information are generated, such as:\n\nresponse: the main output text generated by the model in response to our prompt.\n\nThe capital of France is **Paris**. üá´üá∑\n\ncontext: the token IDs representing the input and context used by the model. Tokens are numerical representations of text used for processing by the language model.\n\n[106, 1645, 108, 1841, 603, 573, 6037, 576, 6081, 235336, 107, 108, 106, 2516, 108, 651, 6037, 576, 6081, 603, 5231, 29437, 168428, 235248, 244304, 241035, 235248, 108]\n\n\nThe Performance Metrics:\n\ntotal_duration: The total time taken for the operation in nanoseconds. In this case, approximately 24.26 seconds.\nload_duration: The time taken to load the model or components in nanoseconds. About 19.83 seconds.\nprompt_eval_duration: The time taken to evaluate the prompt in nanoseconds. Around 16 nanoseconds.\neval_count: The number of tokens evaluated during the generation. Here, 14 tokens.\neval_duration: The time taken for the model to generate the response in nanoseconds. Approximately 2.5 seconds.\n\nBut, what we want is the plain ‚Äòresponse‚Äô and, perhaps for analysis, the total duration of the inference, so let‚Äôs change the code to extract it from the dictionary:\nprint(f\"\\n{res['response']}\")\nprint(\n    f\"\\n [INFO] Total Duration: \"\n    f\"{res['total_duration']/1e9:.2f} seconds\"\n)\nNow, we got:\nThe capital of France is **Paris**. üá´üá∑\n\n [INFO] Total Duration: 24.26 seconds\nUsing Ollama.chat()\nAnother way to get our response is to use ollama.chat(), which generates the next message in a chat with a provided model. This is a streaming endpoint, so a series of responses will occur. Streaming can be disabled using \"stream\": false. The final response object will also include statistics and additional data from the request.\nPROMPT_1 = \"What is the capital of France?\"\n\nresponse = ollama.chat(\n    model=MODEL,\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": PROMPT_1,\n        },\n    ],\n)\nresp_1 = response[\"message\"][\"content\"]\nprint(f\"\\n{resp_1}\")\nprint(\n    f\"\\n [INFO] Total Duration: \"\n    f\"{(res['total_duration']/1e9):.2f} seconds\"\n)\nThe answer is the same as before.\nAn important consideration is that by using ollama.generate(), the response is ‚Äúclear‚Äù from the model‚Äôs ‚Äúmemory‚Äù after the end of inference (only used once), but If we want to keep a conversation, we must use ollama.chat(). Let‚Äôs see it in action:\nPROMPT_1 = \"What is the capital of France?\"\nresponse = ollama.chat(\n    model=MODEL,\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": PROMPT_1,\n        },\n    ],\n)\nresp_1 = response[\"message\"][\"content\"]\nprint(f\"\\n{resp_1}\")\nprint(\n    f\"\\n [INFO] Total Duration: \"\n    f\"{(response['total_duration']/1e9):.2f} seconds\"\n)\n\nPROMPT_2 = \"and of Italy?\"\nresponse = ollama.chat(\n    model=MODEL,\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": PROMPT_1,\n        },\n        {\n            \"role\": \"assistant\",\n            \"content\": resp_1,\n        },\n        {\n            \"role\": \"user\",\n            \"content\": PROMPT_2,\n        },\n    ],\n)\nresp_2 = response[\"message\"][\"content\"]\nprint(f\"\\n{resp_2}\")\nprint(\n    f\"\\n [INFO] Total Duration: \"\n    f\"{(response_2['total_duration']/1e9):.2f} seconds\"\n)\nIn the above code, we are running two queries, and the second prompt considers the result of the first one.\nHere is how the model responded:\nThe capital of France is **Paris**. üá´üá∑\n\n [INFO] Total Duration: 2.82 seconds\n\nThe capital of Italy is **Rome**. üáÆüáπ\n\n [INFO] Total Duration: 4.46 seconds\nGetting an image description:\nIn the same way that we have used the LlaVa-PHI-3 model with the command line to analyze an image, the same can be done here with Python. Let‚Äôs use the same image of Paris, but now with the ollama.generate():\nMODEL = \"llava-phi3:3.8b\"\nPROMPT = \"Describe this picture\"\n\nwith open(\"image_test_1.jpg\", \"rb\") as image_file:\n    img = image_file.read()\n\nresponse = ollama.generate(model=MODEL, prompt=PROMPT, images=[img])\nprint(f\"\\n{response['response']}\")\nprint(\n    f\"\\n [INFO] Total Duration: \"\n    f\"{(res['total_duration']/1e9):.2f} seconds\"\n)\nHere is the result:\nThis image captures the iconic cityscape of Paris, France. The\nvantage point is high, providing a panoramic view of the Seine\nRiver that meanders through the heart of the city. Several\nbridges arch gracefully over the river, connecting different\nparts of the city. The Eiffel Tower, an iron lattice structure\nwith a pointed top and two antennas on its summit, stands\ntall in the background, piercing the sky. It is painted in a\nlight gray color, contrasting against the blue sky speckled\nwith white clouds.\n\nThe buildings that line the river are predominantly white or\nbeige, their uniform color palette broken occasionally by red\nroofs peeking through. The Seine River itself appears calm\nand wide, reflecting the city's architectural beauty in its\nsurface. On either side of the river, trees add a touch of\ngreen to the urban landscape.\n\nThe image is taken from an elevated perspective, looking down\non the city. This viewpoint allows for a comprehensive view of\nParis's beautiful architecture and layout. The relative\npositions of the buildings, bridges, and other structures\ncreate a harmonious composition that showcases the city's charm.\n\nIn summary, this image presents a serene day in Paris, with its\narchitectural marvels - from the Eiffel Tower to the river-side\nbuildings - all bathed in soft colors under a clear sky.\n\n [INFO] Total Duration: 256.45 seconds\nThe model took about 4 minutes (256.45 s) to return with a detailed image description.\n\nIn the 10-Ollama_Python_Library notebook, it is possible to find the experiments with the Ollama Python library.\n\n\n\nSo far, we can observe that by using the model‚Äôs response into a variable, we can effectively incorporate it into real-world projects. However, a major issue arises when the model provides varying responses to the same input. For instance, let‚Äôs assume that we only need the name of a country‚Äôs capital and its coordinates as the model‚Äôs response in the previous examples, without any additional information, even when utilizing verbose models like Microsoft Phi. To ensure consistent responses, we can employ the ‚ÄòOllama function call,‚Äô which is fully compatible with the OpenAI API.\n\n\nIn modern artificial intelligence, function calling with Large Language Models (LLMs) allows these models to perform actions beyond generating text. By integrating with external functions or APIs, LLMs can access real-time data, automate tasks, and interact with various systems.\nFor instance, instead of merely responding to a query about the weather, an LLM can call a weather API to fetch the current conditions and provide accurate, up-to-date information. This capability enhances the relevance and accuracy of the model‚Äôs responses and makes it a powerful tool for driving workflows and automating processes, transforming it into an active participant in real-world applications.\nFor more details about Function Calling, please see this video made by Marvin Prison:\n\n\n\n\nWe want to create an app where the user enters a country‚Äôs name and gets, as an output, the distance in km from the capital city of such a country and the app‚Äôs location (for simplicity, We will use Santiago, Chile, as the app location).\n \nOnce the user enters a country name, the model will return the name of its capital city (as a string) and the latitude and longitude of such city (in float). Using those coordinates, we can use a simple Python library (haversine) to calculate the distance between those 2 points.\nThe idea of this project is to demonstrate a combination of language model interaction, structured data handling with Pydantic, and geospatial calculations using the Haversine formula (traditional computing).\nFirst, let us install some libraries. Besides Haversine, the main one is the OpenAI Python library, which provides convenient access to the OpenAI REST API from any Python 3.7+ application. The other one is Pydantic (and instructor), a robust data validation and settings management library engineered by Python to enhance the robustness and reliability of our codebase. In short, Pydantic will help ensure that our model‚Äôs response will always be consistent.\npip install haversine\npip install openai\npip install pydantic\npip install instructor\nNow, we should create a Python script designed to interact with our model (LLM) to determine the coordinates of a country‚Äôs capital city and calculate the distance from Santiago de Chile to that capital.\nLet‚Äôs go over the code:\n\n\n\n\nimport sys\nfrom haversine import haversine\nfrom openai import OpenAI\nfrom pydantic import BaseModel, Field\nimport instructor\n\nsys: Provides access to system-specific parameters and functions. It‚Äôs used to get command-line arguments.\nhaversine: A function from the haversine library that calculates the distance between two geographic points using the Haversine formula.\nopenAI: A module for interacting with the OpenAI API (although it‚Äôs used in conjunction with a local setup, Ollama). Everything is off-line here.\npydantic: Provides data validation and settings management using Python-type annotations. It‚Äôs used to define the structure of expected response data.\ninstructor: A module is used to patch the OpenAI client to work in a specific mode (likely related to structured data handling).\n\n\n\n\ncountry = sys.argv[1]  # Get the country from\n# command-line arguments\nMODEL = \"phi3.5:3.8b\"  # The name of the model to be used\nmylat = -33.33  # Latitude of Santiago de Chile\nmylon = -70.51  # Longitude of Santiago de Chile\n\ncountry: On a Python script, getting the country name from command-line arguments is possible. On a Jupyter notebook, we can enter its name, for example,\n\ncountry = \"France\"\n\nMODEL: Specifies the model being used, which is, in this example, the phi3.5.\nmylat and mylon: Coordinates of Santiago de Chile, used as the starting point for the distance calculation.\n\n\n\n\nclass CityCoord(BaseModel):\n    city: str = Field(..., description=\"Name of the city\")\n    lat: float = Field(\n        ..., description=\"Decimal Latitude of the city\"\n    )\n    lon: float = Field(\n        ..., description=\"Decimal Longitude of the city\"\n    )\n\nCityCoord: A Pydantic model that defines the expected structure of the response from the LLM. It expects three fields: city (name of the city), lat (latitude), and lon (longitude).\n\n\n\n\nclient = instructor.patch(\n    OpenAI(\n        base_url=\"http://localhost:11434/v1\",  # Local API base\n        # URL (Ollama)\n        api_key=\"ollama\",  # API key\n        # (not used)\n    ),\n    mode=instructor.Mode.JSON,  # Mode for\n    # structured\n    # JSON output\n)\n\nOpenAI: This setup initializes an OpenAI client with a local base URL and an API key (ollama). It uses a local server.\ninstructor.patch: Patches the OpenAI client to work in JSON mode, enabling structured output that matches the Pydantic model.\n\n\n\n\nresp = client.chat.completions.create(\n    model=MODEL,\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": f\"return the decimal latitude and \\\n            decimal longitude of the capital of the {country}.\",\n        }\n    ],\n    response_model=CityCoord,\n    max_retries=10,\n)\n\nclient.chat.completions.create: Calls the LLM to generate a response.\nmodel: Specifies the model to use (llava-phi3).\nmessages: Contains the prompt for the LLM, asking for the latitude and longitude of the capital city of the specified country.\nresponse_model: Indicates that the response should conform to the CityCoord model.\nmax_retries: The maximum number of retry attempts if the request fails.\n\n\n\n\ndistance = haversine((mylat, mylon), (resp.lat, resp.lon), unit=\"km\")\n\nprint(\n    f\"Santiago de Chile is about {int(round(distance, -1))} \"\n    f\"kilometers away from {resp.city}.\"\n)\n\nhaversine: Calculates the distance between Santiago de Chile and the capital city returned by the LLM using their respective coordinates.\n(mylat, mylon): Coordinates of Santiago de Chile.\nresp.city: Name of the country‚Äôs capital\n(resp.lat, resp.lon): Coordinates of the capital city are provided by the LLM response.\nunit = ‚Äòkm‚Äô: Specifies that the distance should be calculated in kilometers.\nprint: Outputs the distance, rounded to the nearest 10 kilometers, with thousands of separators for readability.\n\nRunning the code\nIf we enter different countries, for example, France, Colombia, and the United States, We can note that we always receive the same structured information:\nSantiago de Chile is about 8,060 kilometers away from\n    Washington, D.C..\nSantiago de Chile is about 4,250 kilometers away from Bogot√°.\nSantiago de Chile is about 11,630 kilometers away from Paris.\nIf you run the code as a script, the result will be printed on the terminal:\n \nAnd the calculations are pretty good!\n \n\nIn the 20-Ollama_Function_Calling notebook, it is possible to find experiments with all models installed.\n\n\n\n\nNow it is time to wrap up everything so far! Let‚Äôs modify the script so that instead of entering the country name (as a text), the user enters an image, and the application (based on SLM) returns the city in the image and its geographic location. With those data, we can calculate the distance as before.\n \nFor simplicity, we will implement this new code in two steps. First, the LLM will analyze the image and create a description (text). This text will be passed on to another instance, where the model will extract the information needed to pass along.\nWe will start importing the libraries\nimport sys\nimport time\nfrom haversine import haversine\nimport ollama\nfrom openai import OpenAI\nfrom pydantic import BaseModel, Field\nimport instructor\nWe can see the image if you run the code on the Jupyter Notebook. For that we need also import:\nimport matplotlib.pyplot as plt\nfrom PIL import Image\n\nThose libraries are unnecessary if we run the code as a script.\n\nNow, we define the model and the local coordinates:\nMODEL = \"llava-phi3:3.8b\"\nmylat = -33.33\nmylon = -70.51\nWe can download a new image, for example, Machu Picchu from Wikipedia. On the Notebook we can see it:\n# Load the image\nimg_path = \"image_test_3.jpg\"\nimg = Image.open(img_path)\n\n# Display the image\nplt.figure(figsize=(8, 8))\nplt.imshow(img)\nplt.axis(\"off\")\n# plt.title(\"Image\")\nplt.show()\n \nNow, let‚Äôs define a function that will receive the image and will return the decimal latitude and decimal longitude of the city in the image, its name, and what country it is located\ndef image_description(img_path):\n    with open(img_path, \"rb\") as file:\n        response = ollama.chat(\n            model=MODEL,\n            messages=[\n                {\n                    \"role\": \"user\",\n                    \"content\": \"\"\"return the decimal latitude and \\\n                 decimal longitude of the city in the image, \\\n                 its name, and what country it is located\"\"\",\n                    \"images\": [file.read()],\n                },\n            ],\n            options={\n                \"temperature\": 0,\n            },\n        )\n    # print(response['message']['content'])\n    return response[\"message\"][\"content\"]\n\nWe can print the entire response for debug purposes.\n\nThe image description generated for the function will be passed as a prompt for the model again.\nstart_time = time.perf_counter()  # Start timing\n\n\nclass CityCoord(BaseModel):\n    city: str = Field(\n        ..., description=\"Name of the city in the image\"\n    )\n    country: str = Field(\n        ...,\n        description=(\n            \"Name of the country where \"\n            \"the city in the image is located\"\n        ),\n    )\n    lat: float = Field(\n        ...,\n        description=(\"Decimal latitude of the city in \" \"the image\"),\n    )\n    lon: float = Field(\n        ...,\n        description=(\"Decimal longitude of the city in \" \"the image\"),\n    )\n\n\n# enables `response_model` in create call\nclient = instructor.patch(\n    OpenAI(base_url=\"http://localhost:11434/v1\", api_key=\"ollama\"),\n    mode=instructor.Mode.JSON,\n)\n\nimage_description = image_description(img_path)\n# Send this description to the model\nresp = client.chat.completions.create(\n    model=MODEL,\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": image_description,\n        }\n    ],\n    response_model=CityCoord,\n    max_retries=10,\n    temperature=0,\n)\nIf we print the image description , we will get:\nThe image shows the ancient city of Machu Picchu, located in\nPeru. The city is perched on a steep hillside and consists of\nvarious structures made of stone. It is surrounded by lush\ngreenery and towering mountains. The sky above is blue with\nscattered clouds.\n\nMachu Picchu's latitude is approximately 13.5086¬∞ S, and its\nlongitude is around 72.5494¬∞ W.\nAnd the second response from the model (resp) will be:\nCityCoord(city='Machu Picchu', country='Peru', lat=-13.5086,\n                lon=-72.5494)\nNow, we can do a ‚ÄúPost-Processing‚Äù, calculating the distance and preparing the final answer:\ndistance = haversine((mylat, mylon), (resp.lat, resp.lon), unit=\"km\")\n\nprint(\n    (\n        f\"\\nThe image shows {resp.city}, with lat: \"\n        f\"{round(resp.lat, 2)} and long: \"\n        f\"{round(resp.lon, 2)}, located in \"\n        f\"{resp.country} and about \"\n        f\"{int(round(distance, -1)):,} kilometers \"\n        f\"away from Santiago, Chile.\\n\"\n    )\n)\n\nend_time = time.perf_counter()  # End timing\nelapsed_time = end_time - start_time  # Calculate elapsed time\nprint(\n    f\"[INFO] ==&gt; The code (running {MODEL}), \"\n    f\"took {elapsed_time:.1f} seconds to execute.\\n\"\n)\nAnd we will get:\n The image shows Machu Picchu, with lat:-13.16 and long:\n -72.54, located in Peru  and about 2,250 kilometers away\n from Santiago, Chile.\n\nprint(\n    f\"[INFO] ==&gt; The code (running {MODEL}), \"\n    f\"took {elapsed_time:.1f} seconds \"\n    f\"to execute.\\n\"\n)\nIn the 30-Function_Calling_with_images notebook, it is possible to find the experiments with multiple images.\nLet‚Äôs now download the script calc_distance_image.py from the GitHub and run it on the terminal with the command:\npython calc_distance_image.py \\\n  /home/mjrovai/Documents/OLLAMA/image_test_3.jpg\nEnter with the Machu Picchu image full patch as an argument. We will get the same previous result.\n \nHow about Paris?\n \nOf course, there are many ways to optimize the code used here. Still, the idea is to explore the considerable potential of function calling with SLMs at the edge, allowing those models to integrate with external functions or APIs. Going beyond text generation, SLMs can access real-time data, automate tasks, and interact with various systems.",
    "crumbs": [
      "Raspberry Pi",
      "Small Language Models (SLM)"
    ]
  },
  {
    "objectID": "contents/raspi/llm/llm.html#sec-small-language-models-slm-slms-optimization-techniques-f595",
    "href": "contents/raspi/llm/llm.html#sec-small-language-models-slm-slms-optimization-techniques-f595",
    "title": "Small Language Models (SLM)",
    "section": "",
    "text": "Large Language Models (LLMs) have revolutionized natural language processing, but their deployment and optimization come with unique challenges. One significant issue is the tendency for LLMs (and more, the SLMs) to generate plausible-sounding but factually incorrect information, a phenomenon known as hallucination. This occurs when models produce content that seems coherent but is not grounded in truth or real-world facts.\nOther challenges include the immense computational resources required for training and running these models, the difficulty in maintaining up-to-date knowledge within the model, and the need for domain-specific adaptations. Privacy concerns also arise when handling sensitive data during training or inference. Additionally, ensuring consistent performance across diverse tasks and maintaining ethical use of these powerful tools present ongoing challenges. Addressing these issues is crucial for the effective and responsible deployment of LLMs in real-world applications.\nThe fundamental techniques for enhancing LLM (and SLM) performance and efficiency are Fine-tuning, Prompt engineering, and Retrieval-Augmented Generation (RAG).\n\nFine-tuning, while more resource-intensive, offers a way to specialize LLMs for particular domains or tasks. This process involves further training the model on carefully curated datasets, allowing it to adapt its vast general knowledge to specific applications. Fine-tuning can lead to substantial improvements in performance, especially in specialized fields or for unique use cases.\nPrompt engineering is at the forefront of LLM optimization. By carefully crafting input prompts, we can guide models to produce more accurate and relevant outputs. This technique involves structuring queries that leverage the model‚Äôs pre-trained knowledge and capabilities, often incorporating examples or specific instructions to shape the desired response.\nRetrieval-Augmented Generation (RAG) represents another powerful approach to improving LLM performance. This method combines the vast knowledge embedded in pre-trained models with the ability to access and incorporate external, up-to-date information. By retrieving relevant data to supplement the model‚Äôs decision-making process, RAG can significantly enhance accuracy and reduce the likelihood of generating outdated or false information.\n\nFor edge applications, it is more beneficial to focus on techniques like RAG that can enhance model performance without needing on-device fine-tuning. Let‚Äôs explore it.",
    "crumbs": [
      "Raspberry Pi",
      "Small Language Models (SLM)"
    ]
  },
  {
    "objectID": "contents/raspi/llm/llm.html#sec-small-language-models-slm-rag-implementation-9fda",
    "href": "contents/raspi/llm/llm.html#sec-small-language-models-slm-rag-implementation-9fda",
    "title": "Small Language Models (SLM)",
    "section": "",
    "text": "In a basic interaction between a user and a language model, the user asks a question, which is sent as a prompt to the model. The model generates a response based solely on its pre-trained knowledge. In a RAG process, there‚Äôs an additional step between the user‚Äôs question and the model‚Äôs response. The user‚Äôs question triggers a retrieval process from a knowledge base.\n \n\n\nHere are the steps to implement a basic Retrieval Augmented Generation (RAG):\n\nDetermine the type of documents you‚Äôll be using: The best types are documents from which we can get clean and unobscured text. PDFs can be problematic because they are designed for printing, not for extracting sensible text. To work with PDFs, we should get the source document or use tools to handle it.\nChunk the text: We can‚Äôt store the text as one long stream because of context size limitations and the potential for confusion. Chunking involves splitting the text into smaller pieces. Chunk text has many ways, such as character count, tokens, words, paragraphs, or sections. It is also possible to overlap chunks.\nCreate embeddings: Embeddings are numerical representations of text that capture semantic meaning. We create embeddings by passing each chunk of text through a particular embedding model. The model outputs a vector, the length of which depends on the embedding model used. We should pull one (or more) embedding models from Ollama, to perform this task. Here are some examples of embedding models available at Ollama.\n\n\n\nModel\nParameter Size\nEmbedding Size\n\n\n\n\nmxbai-embed-large\n334M\n1024\n\n\nnomic-embed-text\n137M\n768\n\n\nall-minilm\n23M\n384\n\n\n\n\nGenerally, larger embedding sizes capture more nuanced information about the input. Still, they also require more computational resources to process, and a higher number of parameters should increase the latency (but also the quality of the response).\n\nStore the chunks and embeddings in a vector database: We will need a way to efficiently find the most relevant chunks of text for a given prompt, which is where a vector database comes in. We will use Chromadb, an AI-native open-source vector database, which simplifies building RAGs by creating knowledge, facts, and skills pluggable for LLMs. Both the embedding and the source text for each chunk are stored.\nBuild the prompt: When we have a question, we create an embedding and query the vector database for the most similar chunks. Then, we select the top few results and include their text in the prompt.\n\nThe goal of RAG is to provide the model with the most relevant information from our documents, allowing it to generate more accurate and informative responses. So, let‚Äôs implement a simple example of an SLM incorporating a particular set of facts about bees (‚ÄúBee Facts‚Äù).\nInside the ollama env, enter the command in the terminal for Chromadb installation:\npip install ollama chromadb\nLet‚Äôs pull an intermediary embedding model, nomic-embed-text\nollama pull nomic-embed-text\nAnd create a working directory:\ncd Documents/OLLAMA/\nmkdir RAG-simple-bee\ncd RAG-simple-bee/\nLet‚Äôs create a new Jupyter notebook, 40-RAG-simple-bee for some exploration:\nImport the needed libraries:\nimport ollama\nimport chromadb\nimport time\nAnd define aor models:\nEMB_MODEL = \"nomic-embed-text\"\nMODEL = \"llama3.2:3B\"\nInitially, a knowledge base about bee facts should be created. This involves collecting relevant documents and converting them into vector embeddings. These embeddings are then stored in a vector database, allowing for efficient similarity searches later. Enter with the ‚Äúdocument,‚Äù a base of ‚Äúbee facts‚Äù as a list:\ndocuments = [\n    \"Bee-keeping, also known as apiculture, involves the \\\n    maintenance of bee colonies, typically in hives, by humans.\",\n    \"The most commonly kept species of bees is the European \\\n    honey bee (Apis  mellifera).\",\n\n    ...\n\n    \"There are another 20,000 different bee species in \\\n    the world.\",\n    \"Brazil alone has more than 300 different bee species, and \\\n    the vast majority, unlike western honey bees, don‚Äôt sting.\",\n    \"Reports written in 1577 by Hans Staden, mention three \\\n    native bees used by indigenous people in Brazil.\", \\\n    \"The indigenous people in Brazil used bees for medicine \\\n    and food purposes\",\n    \"From Hans Staden report: probable species: manda√ßaia \\\n    (Melipona quadrifasciata), mandaguari (Scaptotrigona \\\n    postica) and jata√≠-amarela (Tetragonisca angustula).\"\n]\n\nWe do not need to ‚Äúchunk‚Äù the document here because we will use each element of the list and a chunk.\n\nNow, we will create our vector embedding database bee_facts and store the document in it:\nclient = chromadb.Client()\ncollection = client.create_collection(name=\"bee_facts\")\n\n# store each document in a vector embedding database\nfor i, d in enumerate(documents):\n    response = ollama.embeddings(model=EMB_MODEL, prompt=d)\n    embedding = response[\"embedding\"]\n    collection.add(\n        ids=[str(i)], embeddings=[embedding], documents=[d]\n    )\nNow that we have our ‚ÄúKnowledge Base‚Äù created, we can start making queries, retrieving data from it:\n \nUser Query: The process begins when a user asks a question, such as ‚ÄúHow many bees are in a colony? Who lays eggs, and how much? How about common pests and diseases?‚Äù\nprompt = \"How many bees are in a colony? Who lays eggs and \\\n          how much? How about common pests and diseases?\"\nQuery Embedding: The user‚Äôs question is converted into a vector embedding using the same embedding model used for the knowledge base.\nresponse = ollama.embeddings(prompt=prompt, model=EMB_MODEL)\nRelevant Document Retrieval: The system searches the knowledge base using the query embedding to find the most relevant documents (in this case, the 5 more probable). This is done using a similarity search, which compares the query embedding to the document embeddings in the database.\nresults = collection.query(\n    query_embeddings=[response[\"embedding\"]], n_results=5\n)\ndata = results[\"documents\"]\nPrompt Augmentation: The retrieved relevant information is combined with the original user query to create an augmented prompt. This prompt now contains the user‚Äôs question and pertinent facts from the knowledge base.\nprompt = (\n    f\"Using this data: {data}. \" f\"Respond to this prompt: {prompt}\"\n)\nAnswer Generation: The augmented prompt is then fed into a language model, in this case, the llama3.2:3b model. The model uses this enriched context to generate a comprehensive answer. Parameters like temperature, top_k, and top_p are set to control the randomness and quality of the generated response.\noutput = ollama.generate(\n  model=MODEL,\n  prompt = (\n      f\"Using this data: {data}. \"\n      f\"Respond to this prompt: {prompt}\"\n  )\n\n  options={\n    \"temperature\": 0.0,\n    \"top_k\":10,\n    \"top_p\":0.5                          }\n)\nResponse Delivery: Finally, the system returns the generated answer to the user.\nprint(output[\"response\"])\nBased on the provided data, here are the answers to your \\\nquestions:\n\n1. How many bees are in a colony?\nA typical bee colony can contain between 20,000 and 80,000 bees.\n\n2. Who lays eggs and how much?\nThe queen bee lays up to 2,000 eggs per day during peak seasons.\n\n3. What about common pests and diseases?\nCommon pests and diseases that affect bees include varroa \\\nmites, hive beetles, and foulbrood.\nLet‚Äôs create a function to help answer new questions:\ndef rag_bees(prompt, n_results=5, temp=0.0, top_k=10, top_p=0.5):\n    start_time = time.perf_counter()  # Start timing\n\n    # generate an embedding for the prompt and retrieve the data\n    response = ollama.embeddings(\n      prompt=prompt,\n      model=EMB_MODEL\n    )\n\n    results = collection.query(\n      query_embeddings=[response[\"embedding\"]],\n      n_results=n_results\n    )\n    data = results['documents']\n\n    # generate a response combining the prompt and data retrieved\n    output = ollama.generate(\n      model=MODEL,\n      prompt = (\n         f\"Using this data: {data}. \"\n         f\"Respond to this prompt: {prompt}\"\n      )\n\n      options={\n        \"temperature\": temp,\n        \"top_k\": top_k,\n        \"top_p\": top_p                          }\n    )\n\n    print(output['response'])\n\n    end_time = time.perf_counter()  # End timing\n    elapsed_time = round(\n       (end_time - start_time), 1\n    )  # Calculate elapsed time\n\nprint(\n    f\"\\n[INFO] ==&gt; The code for model: {MODEL}, \"\n    f\"took {elapsed_time}s to generate the answer.\\n\"\n)\n\n    print(\n       f\"\\n[INFO] ==&gt; The code for model: {MODEL}, \"\n       f\"took {elapsed_time}s to generate the answer.\\n\"\n    )\nWe can now create queries and call the function:\nprompt = \"Are bees in Brazil?\"\nrag_bees(prompt)\nYes, bees are found in Brazil. According to the data, Brazil \\\nhas more than 300 different bee species, and indigenous people \\\nin Brazil used bees for medicine and food purposes. \\\nAdditionally, reports from 1577 mention three native bees \\\nused by indigenous people in Brazil.\n\n [INFO] ==&gt; The code for model: llama3.2:3b, took 22.7s to \\\n generate the answer.\nBy the way, if the model used supports multiple languages, we can use it (for example, Portuguese), even if the dataset was created in English:\nprompt = \"Existem abelhas no Brazil?\"\nrag_bees(prompt)\nSim, existem abelhas no Brasil! De acordo com o relato de Hans \\\nStaden, h√° tr√™s esp√©cies de abelhas nativas do Brasil que \\\nforam mencionadas: manda√ßaia (Melipona quadrifasciata), \\\nmandaguari (Scaptotrigona postica) e jata√≠-amarela \\\n(Tetragonisca angustula). Al√©m disso, o Brasil √© conhecido \\\npor ter mais de 300 esp√©cies diferentes de abelhas, a \\\nmaioria das quais n√£o √© agressiva e n√£o p√µe veneno.\n\n [INFO] ==&gt; The code for model: llama3.2:3b, took 54.6s to \\\n            generate the answer.\n\n\n\nThe small LLM models tested worked well at the edge, both in text and with images, but of course, they had high latency regarding the last one. A combination of specific and dedicated models can lead to better results; for example, in real cases, an Object Detection model (such as YOLO) can get a general description and count of objects on an image that, once passed to an LLM, can help extract essential insights and actions.\nAccording to Avi Baum, CTO at Hailo,\n\nIn the vast landscape of artificial intelligence (AI), one of the most intriguing journeys has been the evolution of AI on the edge. This journey has taken us from classic machine vision to the realms of discriminative AI, enhancive AI, and now, the groundbreaking frontier of generative AI. Each step has brought us closer to a future where intelligent systems seamlessly integrate with our daily lives, offering an immersive experience of not just perception but also creation at the palm of our hand.",
    "crumbs": [
      "Raspberry Pi",
      "Small Language Models (SLM)"
    ]
  },
  {
    "objectID": "contents/raspi/llm/llm.html#sec-small-language-models-slm-summary-e350",
    "href": "contents/raspi/llm/llm.html#sec-small-language-models-slm-summary-e350",
    "title": "Small Language Models (SLM)",
    "section": "",
    "text": "This lab has demonstrated how a Raspberry Pi 5 can be transformed into a potent AI hub capable of running large language models (LLMs) for real-time, on-site data analysis and insights using Ollama and Python. The Raspberry Pi‚Äôs versatility and power, coupled with the capabilities of lightweight LLMs like Llama 3.2 and LLaVa-Phi-3-mini, make it an excellent platform for edge computing applications.\nThe potential of running LLMs on the edge extends far beyond simple data processing, as in this lab‚Äôs examples. Here are some innovative suggestions for using this project:\n1. Smart Home Automation:\n\nIntegrate SLMs to interpret voice commands or analyze sensor data for intelligent home automation. This could include real-time monitoring and control of home devices, security systems, and energy management, all processed locally without relying on cloud services.\n\n2. Field Data Collection and Analysis:\n\nDeploy SLMs on Raspberry Pi in remote or mobile setups for real-time data collection and analysis. This can be used in agriculture to monitor crop health, in environmental studies for wildlife tracking, or in disaster response for situational awareness and resource management.\n\n3. Educational Tools:\n\nCreate interactive educational tools that leverage SLMs to provide instant feedback, language translation, and tutoring. This can be particularly useful in developing regions with limited access to advanced technology and internet connectivity.\n\n4. Healthcare Applications:\n\nUse SLMs for medical diagnostics and patient monitoring. They can provide real-time analysis of symptoms and suggest potential treatments. This can be integrated into telemedicine platforms or portable health devices.\n\n5. Local Business Intelligence:\n\nImplement SLMs in retail or small business environments to analyze customer behavior, manage inventory, and optimize operations. The ability to process data locally ensures privacy and reduces dependency on external services.\n\n6. Industrial IoT:\n\nIntegrate SLMs into industrial IoT systems for predictive maintenance, quality control, and process optimization. The Raspberry Pi can serve as a localized data processing unit, reducing latency and improving the reliability of automated systems.\n\n7. Autonomous Vehicles:\n\nUse SLMs to process sensory data from autonomous vehicles, enabling real-time decision-making and navigation. This can be applied to drones, robots, and self-driving cars for enhanced autonomy and safety.\n\n8. Cultural Heritage and Tourism:\n\nImplement SLMs to provide interactive and informative cultural heritage sites and museum guides. Visitors can use these systems to get real-time information and insights, enhancing their experience without internet connectivity.\n\n9. Artistic and Creative Projects:\n\nUse SLMs to analyze and generate creative content, such as music, art, and literature. This can foster innovative projects in the creative industries and allow for unique interactive experiences in exhibitions and performances.\n\n10. Customized Assistive Technologies:\n\nDevelop assistive technologies for individuals with disabilities, providing personalized and adaptive support through real-time text-to-speech, language translation, and other accessible tools.",
    "crumbs": [
      "Raspberry Pi",
      "Small Language Models (SLM)"
    ]
  },
  {
    "objectID": "contents/raspi/llm/llm.html#sec-small-language-models-slm-resources-015b",
    "href": "contents/raspi/llm/llm.html#sec-small-language-models-slm-resources-015b",
    "title": "Small Language Models (SLM)",
    "section": "",
    "text": "10-Ollama_Python_Library notebook\n20-Ollama_Function_Calling notebook\n30-Function_Calling_with_images notebook\n40-RAG-simple-bee notebook\ncalc_distance_image python script",
    "crumbs": [
      "Raspberry Pi",
      "Small Language Models (SLM)"
    ]
  },
  {
    "objectID": "contents/raspi/setup/setup.html",
    "href": "contents/raspi/setup/setup.html",
    "title": "Setup",
    "section": "",
    "text": "DALL¬∑E prompt - An electronics laboratory environment inspired by the 1950s, with a cartoon style. The lab should have vintage equipment, large oscilloscopes, old-fashioned tube radios, and large, boxy computers. The Raspberry Pi 5 board is prominently displayed, accurately shown in its real size, similar to a credit card, on a workbench. The Pi board is surrounded by classic lab tools like a soldering iron, resistors, and wires. The overall scene should be vibrant, with exaggerated colors and playful details characteristic of a cartoon. No logos or text should be included.\n\n\nThis chapter will guide you through setting up Raspberry Pi Zero 2 W (Raspi-Zero) and Raspberry Pi 5 (Raspi-5) models. We‚Äôll cover hardware setup, operating system installation, initial configuration, and tests.\n\nThe general instructions for the Raspi-5 also apply to the older Raspberry Pi versions, such as the Raspi-3 and Raspi-4.\n\n\n\nThe Raspberry Pi is a powerful and versatile single-board computer that has become an essential tool for engineers across various disciplines. Developed by the Raspberry Pi Foundation, these compact devices offer a unique combination of affordability, computational power, and extensive GPIO (General Purpose Input/Output) capabilities, making them ideal for prototyping, embedded systems development, and advanced engineering projects.\n\n\n\nComputational Power: Despite their small size, Raspberry Pis offers significant processing capabilities, with the latest models featuring multi-core ARM processors and up to 8 GB of RAM.\nGPIO Interface: The 40-pin GPIO header allows direct interaction with sensors, actuators, and other electronic components, facilitating hardware-software integration projects.\nExtensive Connectivity: Built-in Wi-Fi, Bluetooth, Ethernet, and multiple USB ports enable diverse communication and networking projects.\nLow-Level Hardware Access: Raspberry Pis provides access to interfaces like I2C, SPI, and UART, allowing for detailed control and communication with external devices.\nReal-Time Capabilities: With proper configuration, Raspberry Pis can be used for soft real-time applications, making them suitable for control systems and signal processing tasks.\nPower Efficiency: Low power consumption enables battery-powered and energy-efficient designs, especially in models like the Pi Zero.\n\n\n\n\n\nRaspberry Pi Zero 2 W (Raspi-Zero):\n\nIdeal for: Compact embedded systems\nKey specs: 1 GHz single-core CPU (ARM Cortex-A53), 512 MB RAM, minimal power consumption\n\nRaspberry Pi 5 (Raspi-5):\n\nIdeal for: More demanding applications such as edge computing, computer vision, and edgeAI applications, including LLMs.\nKey specs: 2.4 GHz quad-core CPU (ARM Cortex A-76), up to 8 GB RAM, PCIe interface for expansions\n\n\n\n\n\n\nEmbedded Systems Design: Develop and prototype embedded systems for real-world applications.\nIoT and Networked Devices: Create interconnected devices and explore protocols like MQTT, CoAP, and HTTP/HTTPS.\nControl Systems: Implement feedback control loops, PID controllers, and interface with actuators.\nComputer Vision and AI: Utilize libraries like OpenCV and TensorFlow Lite for image processing and machine learning at the edge.\nData Acquisition and Analysis: Collect sensor data, perform real-time analysis, and create data logging systems.\nRobotics: Build robot controllers, implement motion planning algorithms, and interface with motor drivers.\nSignal Processing: Perform real-time signal analysis, filtering, and DSP applications.\nNetwork Security: Set up VPNs, firewalls, and explore network penetration testing.\n\nThis tutorial will guide you through setting up the most common Raspberry Pi models, enabling you to start on your machine learning project quickly. We‚Äôll cover hardware setup, operating system installation, and initial configuration, focusing on preparing your Pi for Machine Learning applications.\n\n\n\n\n\n\n \n\nProcessor: 1 GHz quad-core 64-bit Arm Cortex-A53 CPU\nRAM: 512 MB SDRAM\nWireless: 2.4 GHz 802.11 b/g/n wireless LAN, Bluetooth 4.2, BLE\nPorts: Mini HDMI, micro USB OTG, CSI-2 camera connector\nPower: 5 V via micro USB port\n\n\n\n\n \n\nProcessor:\n\nPi 5: Quad-core 64-bit Arm Cortex-A76 CPU @ 2.4 GHz\nPi 4: Quad-core Cortex-A72 (ARM v8) 64-bit SoC @ 1.5 GHz\n\nRAM: 2 GB, 4 GB, or 8 GB options (8 GB recommended for AI tasks)\nWireless: Dual-band 802.11ac wireless, Bluetooth 5.0\nPorts: 2 \\(\\times\\) micro HDMI ports, 2 \\(\\times\\) USB 3.0 ports, 2 \\(\\times\\) USB 2.0 ports, CSI camera port, DSI display port\nPower: 5 V DC via USB-C connector (3A)\n\n\nIn the labs, we will use different names to address the Raspberry: Raspi, Raspi-5, Raspi-Zero, etc. Usually, Raspi is used when the instructions or comments apply to every model.\n\n\n\n\n\n\n\nAn operating system (OS) is fundamental software that manages computer hardware and software resources, providing standard services for computer programs. It is the core software that runs on a computer, acting as an intermediary between hardware and application software. The OS manages the computer‚Äôs memory, processes, device drivers, files, and security protocols.\n\nKey functions:\n\nProcess management: Allocating CPU time to different programs\nMemory management: Allocating and freeing up memory as needed\nFile system management: Organizing and keeping track of files and directories\nDevice management: Communicating with connected hardware devices\nUser interface: Providing a way for users to interact with the computer\n\nComponents:\n\nKernel: The core of the OS that manages hardware resources\nShell: The user interface for interacting with the OS\nFile system: Organizes and manages data storage\nDevice drivers: Software that allows the OS to communicate with hardware\n\n\nThe Raspberry Pi runs a specialized version of Linux designed for embedded systems. This operating system, typically a variant of Debian called Raspberry Pi OS (formerly Raspbian), is optimized for the Pi‚Äôs ARM-based architecture and limited resources.\n\nThe latest version of Raspberry Pi OS is based on Debian Bookworm.\n\nKey features:\n\nLightweight: Tailored to run efficiently on the Pi‚Äôs hardware.\nVersatile: Supports a wide range of applications and programming languages.\nOpen-Source: Allows for customization and community-driven improvements.\nGPIO support: Enables interaction with sensors and other hardware through the Pi‚Äôs pins.\nRegular updates: Continuously improved for performance and security.\n\nEmbedded Linux on the Raspberry Pi provides a full-featured operating system in a compact package, making it ideal for projects ranging from simple IoT devices to more complex edge machine-learning applications. Its compatibility with standard Linux tools and libraries makes it a powerful platform for development and experimentation.\n\n\n\nTo use the Raspberry Pi, we will need an operating system. By default, Raspberry Pi checks for an operating system on any SD card inserted in the slot, so we should install an operating system using Raspberry Pi Imager.\nRaspberry Pi Imager is a tool for downloading and writing images on macOS, Windows, and Linux. It includes many popular operating system images for Raspberry Pi. We will also use the Imager to preconfigure credentials and remote access settings.\nFollow the steps to install the OS in your Raspi.\n\nDownload and install the Raspberry Pi Imager on your computer.\nInsert a microSD card into your computer (a 32GB SD card is recommended) .\nOpen Raspberry Pi Imager and select your Raspberry Pi model.\nChoose the appropriate operating system:\n\nFor Raspi-Zero: For example, you can select: Raspberry Pi OS Lite (64-bit).\n\n\n \n\nDue to its reduced SDRAM (512 MB), the recommended OS for the Raspi-Zero is the 32-bit version. However, to run some machine learning models, such as the YOLOv8 from Ultralytics, we should use the 64-bit version. Although Raspi-Zero can run a desktop, we will choose the LITE version (no Desktop) to reduce the RAM needed for regular operation.\n\n\nFor Raspi-5: We can select the full 64-bit version, which includes a desktop: Raspberry Pi OS (64-bit)\n\n \n\nSelect your microSD card as the storage device.\nClick on Next and then the gear icon to access advanced options.\nSet the hostname, the Raspi username and password, configure WiFi and enable SSH (Very important!)\n\n \n\nWrite the image to the microSD card.\n\n\nIn the examples here, we will use different hostnames depending on the device used: raspi, raspi-5, raspi-Zero, etc. It would help if you replaced it with the one you are using.\n\n\n\n\n\nInsert the microSD card into your Raspberry Pi.\nConnect power to boot up the Raspberry Pi.\nPlease wait for the initial boot process to complete (it may take a few minutes).\n\n\nYou can find the most common Linux commands to be used with the Raspi here or here.\n\n\n\n\n\n\n\nThe easiest way to interact with the Raspi-Zero is via SSH (‚ÄúHeadless‚Äù). You can use a Terminal (MAC/Linux), PuTTy (Windows), or any other.\n\nFind your Raspberry Pi‚Äôs IP address (for example, check your router).\nOn your computer, open a terminal and connect via SSH:\nssh username@[raspberry_pi_ip_address]\nAlternatively, if you do not have the IP address, you can try the following: bash ssh username@hostname.local for example, ssh mjrovai@rpi-5.local , ssh mjrovai@raspi.local , etc.\n\n\n\nimg\n\n\nWhen you see the prompt:\nmjrovai@rpi-5:~ $\nIt means that you are interacting remotely with your Raspi. It is a good practice to update/upgrade the system regularly. For that, you should run:\nsudo apt-get update\nsudo apt upgrade\nYou should confirm the Raspi IP address. On the terminal, you can use:\nhostname -I\n\n \n\n\n\nWhen you want to turn off your Raspberry Pi, there are better ideas than just pulling the power cord. This is because the Raspi may still be writing data to the SD card, in which case merely powering down may result in data loss or, even worse, a corrupted SD card.\nFor safety shut down, use the command line:\nsudo shutdown -h now\n\nTo avoid possible data loss and SD card corruption, before removing the power, you should wait a few seconds after shutdown for the Raspberry Pi‚Äôs LED to stop blinking and go dark. Once the LED goes out, it‚Äôs safe to power down.\n\n\n\n\nTransferring files between the Raspi and our main computer can be done using a pen drive, directly on the terminal (with scp), or an FTP program over the network.\n\n\n\n\nLet‚Äôs create a text file on our computer, for example, test.txt.\n \n\nYou can use any text editor. In the same terminal, an option is the nano.\n\nTo copy the file named test.txt from your personal computer to a user‚Äôs home folder on your Raspberry Pi, run the following command from the directory containing test.txt, replacing the &lt;username&gt; placeholder with the username you use to log in to your Raspberry Pi and the &lt;pi_ip_address&gt; placeholder with your Raspberry Pi‚Äôs IP address:\n$ scp test.txt &lt;username&gt;@&lt;pi_ip_address&gt;:~/\n\nNote that ~/ means that we will move the file to the ROOT of our Raspi. You can choose any folder in your Raspi. But you should create the folder before you run scp, since scp won‚Äôt create folders automatically.\n\nFor example, let‚Äôs transfer the file test.txt to the ROOT of my Raspi-zero, which has an IP of 192.168.4.210:\nscp test.txt mjrovai@192.168.4.210:~/\n \nI use a different profile to differentiate the terminals. The above action happens on your computer. Now, let‚Äôs go to our Raspi (using the SSH) and check if the file is there:\n \n\n\n\nTo copy a file named test.txt from a user‚Äôs home directory on a Raspberry Pi to the current directory on another computer, run the following command on your Host Computer:\n$ scp &lt;username&gt;@&lt;pi_ip_address&gt;:myfile.txt .\nFor example:\nOn the Raspi, let‚Äôs create a copy of the file with another name:\ncp test.txt test_2.txt\nAnd on the Host Computer (in my case, a Mac)\nscp mjrovai@192.168.4.210:test_2.txt .\n \n\n\n\n\nTransferring files using FTP, such as FileZilla FTP Client, is also possible. Follow the instructions, install the program for your Desktop OS, and use the Raspi IP address as the Host. For example:\nsftp://192.168.4.210\n and enter your Raspi username and password. Pressing Quickconnect will open two windows, one for your host computer desktop (right) and another for the Raspi (left).\n \n\n\n\n\n\nUsing htop, a cross-platform interactive process viewer, you can easily monitor the resources running on your Raspi, such as the list of processes, the running CPUs, and the memory used in real-time. To lunch hop, enter with the command on the terminal:\nhtop\n \nRegarding memory, among the devices in the Raspberry Pi family, the Raspi-Zero has the smallest amount of SRAM (500 MB), compared to a selection of 2 GB to 8 GB on the Raspis 4 or 5. For any Raspi, it is possible to increase the memory available to the system with ‚ÄúSwap.‚Äù Swap memory, also known as swap space, is a technique used in computer operating systems to temporarily store data from RAM (Random Access Memory) on the SD card when the physical RAM is fully utilized. This allows the operating system (OS) to continue running even when RAM is full, which can prevent system crashes or slowdowns.\nSwap memory benefits devices with limited RAM, such as the Raspi-Zero. Increasing swap can help run more demanding applications or processes, but it‚Äôs essential to balance this with the potential performance impact of frequent disk access.\nBy default, the Rapi-Zero‚Äôs SWAP (Swp) memory is only 100 MB, which is very small for running some more complex and demanding Machine Learning applications (for example, YOLO). Let‚Äôs increase it to 2 MB:\nFirst, turn off swap-file:\nsudo dphys-swapfile swapoff\nNext, you should open and change the file /etc/dphys-swapfile. For that, we will use the nano:\nsudo nano /etc/dphys-swapfile\nSearch for the CONF_SWAPSIZE variable (default is 200) and update it to 2000:\nCONF_SWAPSIZE=2000\nAnd save the file.\nNext, turn on the swapfile again and reboot the Raspi-zero:\nsudo dphys-swapfile setup\nsudo dphys-swapfile swapon\nsudo reboot\nWhen your device is rebooted (you should enter with the SSH again), you will realize that the maximum swap memory value shown on top is now something near 2 GB (in my case, 1.95 GB).\n\nTo keep the htop running, you should open another terminal window to interact continuously with your Raspi.\n\n\n\n\nThe Raspi is an excellent device for computer vision applications; a camera is needed for it. We can install a standard USB webcam on the micro-USB port using a USB OTG adapter (Raspi-Zero and Raspi-5) or a camera module connected to the Raspi CSI (Camera Serial Interface) port.\n\nUSB Webcams generally have inferior quality to the camera modules that connect to the CSI port. They can also not be controlled using the raspistill and raspivid commands in the terminal or the picamera recording package in Python. Nevertheless, there may be reasons why you want to connect a USB camera to your Raspberry Pi, such as because of the benefit that it is much easier to set up multiple cameras with a single Raspberry Pi, long cables, or simply because you have such a camera on hand.\n\n\n\n\nPower off the Raspi:\n\nsudo shutdown -h no\n\nConnect the USB Webcam (USB Camera Module 30 fps, \\(1280\\times 720\\)) to your Raspi (In this example, I am using the Raspi-Zero, but the instructions work for all Raspis).\n\n \n\nPower on again and run the SSH\nTo check if your USB camera is recognized, run:\n\nlsusb\nYou should see your camera listed in the output.\n \n\nTo take a test picture with your USB camera, use:\n\nfswebcam test_image.jpg\nThis will save an image named ‚Äútest_image.jpg‚Äù in your current directory.\n \n\nSince we are using SSH to connect to our Rapsi, we must transfer the image to our main computer so we can view it. We can use FileZilla or SCP for this:\n\nOpen a terminal on your host computer and run:\nscp mjrovai@raspi-zero.local:~/test_image.jpg .\n\nReplace ‚Äúmjrovai‚Äù with your username and ‚Äúraspi-zero‚Äù with Pi‚Äôs hostname.\n\n \n\nIf the image quality isn‚Äôt satisfactory, you can adjust various settings; for example, define a resolution that is suitable for YOLO \\((640x640)\\):\n\nfswebcam -r 640x640 --no-banner test_image_yolo.jpg\nThis captures a higher-resolution image without the default banner.\n \nAn ordinary USB Webcam can also be used:\n \nAnd verified using lsusb\n \n\n\nFor stream video (which is more resource-intensive), we can install and use mjpg-streamer:\nFirst, install Git:\nsudo apt install git\nNow, we should install the necessary dependencies for mjpg-streamer, clone the repository, and proceed with the installation:\nsudo apt install cmake libjpeg62-turbo-dev\ngit clone https://github.com/jacksonliam/mjpg-streamer.git\ncd mjpg-streamer/mjpg-streamer-experimental\nmake\nsudo make install\nThen start the stream with:\nmjpg_streamer -i \"input_uvc.so\" -o \"output_http.so -w ./www\"\nWe can then access the stream by opening a web browser and navigating to:\nhttp://&lt;your_pi_ip_address&gt;:8080. In my case: http://192.168.4.210:8080\nWe should see a webpage with options to view the stream. Click on the link that says ‚ÄúStream‚Äù or try accessing:\nhttp://&lt;raspberry_pi_ip_address&gt;:8080/?action=stream\n \n\n\n\n\nThere are now several Raspberry Pi camera modules. The original 5-megapixel model was released in 2013, followed by an 8-megapixel Camera Module 2 that was later released in 2016. The latest camera model is the 12-megapixel Camera Module 3, released in 2023.\nThe original 5 MP camera (Arducam OV5647) is no longer available from Raspberry Pi but can be found from several alternative suppliers. Below is an example of such a camera on a Raspi-Zero.\n \nHere is another example of a v2 Camera Module, which has a Sony IMX219 8-megapixel sensor:\n \nAny camera module will work on the Raspberry Pis, but for that, the configuration.txt file must be updated:\nsudo nano /boot/firmware/config.txt\nAt the bottom of the file, for example, to use the 5 MP Arducam OV5647 camera, add the line:\ndtoverlay=ov5647,cam0\nOr for the v2 module, which has the 8MP Sony IMX219 camera:\ndtoverlay=imx219,cam0\nSave the file (CTRL+O [ENTER] CRTL+X) and reboot the Raspi:\nSudo reboot\nAfter the boot, you can see if the camera is listed:\nlibcamera-hello --list-cameras\n \n \n\nlibcamera is an open-source software library that supports camera systems directly from the Linux operating system on Arm processors. It minimizes proprietary code running on the Broadcom GPU.\n\nLet‚Äôs capture a jpeg image with a resolution of \\(640\\times 480\\) for testing and save it to a file named test_cli_camera.jpg\nrpicam-jpeg --output test_cli_camera.jpg --width 640 --height 480\n if we want to see the file saved, we should use ls -f, which lists all current directory content in long format. As before, we can use scp to view the image:\n \n\n\n\n\nWhile we‚Äôve primarily interacted with the Raspberry Pi using terminal commands via SSH, we can access the whole graphical desktop environment remotely if we have installed the complete Raspberry Pi OS (for example, Raspberry Pi OS (64-bit). This can be particularly useful for tasks that benefit from a visual interface. To enable this functionality, we must set up a VNC (Virtual Network Computing) server on the Raspberry Pi. Here‚Äôs how to do it:\n\nEnable the VNC Server:\n\nConnect to your Raspberry Pi via SSH.\nRun the Raspberry Pi configuration tool by entering:\nsudo raspi-config\nNavigate to Interface Options using the arrow keys.\n\n\n \n\nSelect VNC and Yes to enable the VNC server.\n\n \n\nExit the configuration tool, saving changes when prompted.\n\n \n\nInstall a VNC Viewer on Your Computer:\n\nDownload and install a VNC viewer application on your main computer. Popular options include RealVNC Viewer, TightVNC, or VNC Viewer by RealVNC. We will install VNC Viewer by RealVNC.\n\nOnce installed, you should confirm the Raspi IP address. For example, on the terminal, you can use:\nhostname -I\n\n \n\nConnect to Your Raspberry Pi:\n\nOpen your VNC viewer application.\n\n\n \n\nEnter your Raspberry Pi‚Äôs IP address and hostname.\nWhen prompted, enter your Raspberry Pi‚Äôs username and password.\n\n \n\nThe Raspberry Pi 5 Desktop should appear on your computer monitor.\n\n \n\nAdjust Display Settings (if needed):\n\nOnce connected, adjust the display resolution for optimal viewing. This can be done through the Raspberry Pi‚Äôs desktop settings or by modifying the config.txt file.\nLet‚Äôs do it using the desktop settings. Reach the menu (the Raspberry Icon at the left upper corner) and select the best screen definition for your monitor:\n\n\n \n\n\n\n\nUpdate your system:\nsudo apt update && sudo apt upgrade -y\nInstall essential software:\nsudo apt install python3-pip -y\nEnable pip for Python projects:\nsudo rm /usr/lib/python3.11/EXTERNALLY-MANAGED\n\n\n\n\n\n\n\nLimited processing power, best for lightweight projects\nIt is better to use a headless setup (SSH) to conserve resources.\nConsider increasing swap space for memory-intensive tasks.\nIt can be used for Image Classification and Object Detection Labs but not for the LLM (SLM).\n\n\n\n\n\nSuitable for more demanding projects, including AI and machine learning.\nIt can run the whole desktop environment smoothly.\nRaspi-4 can be used for Image Classification and Object Detection Labs but will not work well with LLMs (SLM).\nFor Raspi-5, consider using an active cooler for temperature management during intensive tasks, as in the LLMs (SLMs) lab.\n\nRemember to adjust your project requirements based on the specific Raspberry Pi model you‚Äôre using. The Raspi-Zero is great for low-power, space-constrained projects, while the Raspi-4 or 5 models are better suited for more computationally intensive tasks.",
    "crumbs": [
      "Raspberry Pi",
      "Setup"
    ]
  },
  {
    "objectID": "contents/raspi/setup/setup.html#sec-setup-overview-0ec9",
    "href": "contents/raspi/setup/setup.html#sec-setup-overview-0ec9",
    "title": "Setup",
    "section": "",
    "text": "The Raspberry Pi is a powerful and versatile single-board computer that has become an essential tool for engineers across various disciplines. Developed by the Raspberry Pi Foundation, these compact devices offer a unique combination of affordability, computational power, and extensive GPIO (General Purpose Input/Output) capabilities, making them ideal for prototyping, embedded systems development, and advanced engineering projects.\n\n\n\nComputational Power: Despite their small size, Raspberry Pis offers significant processing capabilities, with the latest models featuring multi-core ARM processors and up to 8 GB of RAM.\nGPIO Interface: The 40-pin GPIO header allows direct interaction with sensors, actuators, and other electronic components, facilitating hardware-software integration projects.\nExtensive Connectivity: Built-in Wi-Fi, Bluetooth, Ethernet, and multiple USB ports enable diverse communication and networking projects.\nLow-Level Hardware Access: Raspberry Pis provides access to interfaces like I2C, SPI, and UART, allowing for detailed control and communication with external devices.\nReal-Time Capabilities: With proper configuration, Raspberry Pis can be used for soft real-time applications, making them suitable for control systems and signal processing tasks.\nPower Efficiency: Low power consumption enables battery-powered and energy-efficient designs, especially in models like the Pi Zero.\n\n\n\n\n\nRaspberry Pi Zero 2 W (Raspi-Zero):\n\nIdeal for: Compact embedded systems\nKey specs: 1 GHz single-core CPU (ARM Cortex-A53), 512 MB RAM, minimal power consumption\n\nRaspberry Pi 5 (Raspi-5):\n\nIdeal for: More demanding applications such as edge computing, computer vision, and edgeAI applications, including LLMs.\nKey specs: 2.4 GHz quad-core CPU (ARM Cortex A-76), up to 8 GB RAM, PCIe interface for expansions\n\n\n\n\n\n\nEmbedded Systems Design: Develop and prototype embedded systems for real-world applications.\nIoT and Networked Devices: Create interconnected devices and explore protocols like MQTT, CoAP, and HTTP/HTTPS.\nControl Systems: Implement feedback control loops, PID controllers, and interface with actuators.\nComputer Vision and AI: Utilize libraries like OpenCV and TensorFlow Lite for image processing and machine learning at the edge.\nData Acquisition and Analysis: Collect sensor data, perform real-time analysis, and create data logging systems.\nRobotics: Build robot controllers, implement motion planning algorithms, and interface with motor drivers.\nSignal Processing: Perform real-time signal analysis, filtering, and DSP applications.\nNetwork Security: Set up VPNs, firewalls, and explore network penetration testing.\n\nThis tutorial will guide you through setting up the most common Raspberry Pi models, enabling you to start on your machine learning project quickly. We‚Äôll cover hardware setup, operating system installation, and initial configuration, focusing on preparing your Pi for Machine Learning applications.",
    "crumbs": [
      "Raspberry Pi",
      "Setup"
    ]
  },
  {
    "objectID": "contents/raspi/setup/setup.html#sec-setup-hardware-overview-ed45",
    "href": "contents/raspi/setup/setup.html#sec-setup-hardware-overview-ed45",
    "title": "Setup",
    "section": "",
    "text": "Processor: 1 GHz quad-core 64-bit Arm Cortex-A53 CPU\nRAM: 512 MB SDRAM\nWireless: 2.4 GHz 802.11 b/g/n wireless LAN, Bluetooth 4.2, BLE\nPorts: Mini HDMI, micro USB OTG, CSI-2 camera connector\nPower: 5 V via micro USB port\n\n\n\n\n \n\nProcessor:\n\nPi 5: Quad-core 64-bit Arm Cortex-A76 CPU @ 2.4 GHz\nPi 4: Quad-core Cortex-A72 (ARM v8) 64-bit SoC @ 1.5 GHz\n\nRAM: 2 GB, 4 GB, or 8 GB options (8 GB recommended for AI tasks)\nWireless: Dual-band 802.11ac wireless, Bluetooth 5.0\nPorts: 2 \\(\\times\\) micro HDMI ports, 2 \\(\\times\\) USB 3.0 ports, 2 \\(\\times\\) USB 2.0 ports, CSI camera port, DSI display port\nPower: 5 V DC via USB-C connector (3A)\n\n\nIn the labs, we will use different names to address the Raspberry: Raspi, Raspi-5, Raspi-Zero, etc. Usually, Raspi is used when the instructions or comments apply to every model.",
    "crumbs": [
      "Raspberry Pi",
      "Setup"
    ]
  },
  {
    "objectID": "contents/raspi/setup/setup.html#sec-setup-installing-operating-system-7325",
    "href": "contents/raspi/setup/setup.html#sec-setup-installing-operating-system-7325",
    "title": "Setup",
    "section": "",
    "text": "An operating system (OS) is fundamental software that manages computer hardware and software resources, providing standard services for computer programs. It is the core software that runs on a computer, acting as an intermediary between hardware and application software. The OS manages the computer‚Äôs memory, processes, device drivers, files, and security protocols.\n\nKey functions:\n\nProcess management: Allocating CPU time to different programs\nMemory management: Allocating and freeing up memory as needed\nFile system management: Organizing and keeping track of files and directories\nDevice management: Communicating with connected hardware devices\nUser interface: Providing a way for users to interact with the computer\n\nComponents:\n\nKernel: The core of the OS that manages hardware resources\nShell: The user interface for interacting with the OS\nFile system: Organizes and manages data storage\nDevice drivers: Software that allows the OS to communicate with hardware\n\n\nThe Raspberry Pi runs a specialized version of Linux designed for embedded systems. This operating system, typically a variant of Debian called Raspberry Pi OS (formerly Raspbian), is optimized for the Pi‚Äôs ARM-based architecture and limited resources.\n\nThe latest version of Raspberry Pi OS is based on Debian Bookworm.\n\nKey features:\n\nLightweight: Tailored to run efficiently on the Pi‚Äôs hardware.\nVersatile: Supports a wide range of applications and programming languages.\nOpen-Source: Allows for customization and community-driven improvements.\nGPIO support: Enables interaction with sensors and other hardware through the Pi‚Äôs pins.\nRegular updates: Continuously improved for performance and security.\n\nEmbedded Linux on the Raspberry Pi provides a full-featured operating system in a compact package, making it ideal for projects ranging from simple IoT devices to more complex edge machine-learning applications. Its compatibility with standard Linux tools and libraries makes it a powerful platform for development and experimentation.\n\n\n\nTo use the Raspberry Pi, we will need an operating system. By default, Raspberry Pi checks for an operating system on any SD card inserted in the slot, so we should install an operating system using Raspberry Pi Imager.\nRaspberry Pi Imager is a tool for downloading and writing images on macOS, Windows, and Linux. It includes many popular operating system images for Raspberry Pi. We will also use the Imager to preconfigure credentials and remote access settings.\nFollow the steps to install the OS in your Raspi.\n\nDownload and install the Raspberry Pi Imager on your computer.\nInsert a microSD card into your computer (a 32GB SD card is recommended) .\nOpen Raspberry Pi Imager and select your Raspberry Pi model.\nChoose the appropriate operating system:\n\nFor Raspi-Zero: For example, you can select: Raspberry Pi OS Lite (64-bit).\n\n\n \n\nDue to its reduced SDRAM (512 MB), the recommended OS for the Raspi-Zero is the 32-bit version. However, to run some machine learning models, such as the YOLOv8 from Ultralytics, we should use the 64-bit version. Although Raspi-Zero can run a desktop, we will choose the LITE version (no Desktop) to reduce the RAM needed for regular operation.\n\n\nFor Raspi-5: We can select the full 64-bit version, which includes a desktop: Raspberry Pi OS (64-bit)\n\n \n\nSelect your microSD card as the storage device.\nClick on Next and then the gear icon to access advanced options.\nSet the hostname, the Raspi username and password, configure WiFi and enable SSH (Very important!)\n\n \n\nWrite the image to the microSD card.\n\n\nIn the examples here, we will use different hostnames depending on the device used: raspi, raspi-5, raspi-Zero, etc. It would help if you replaced it with the one you are using.\n\n\n\n\n\nInsert the microSD card into your Raspberry Pi.\nConnect power to boot up the Raspberry Pi.\nPlease wait for the initial boot process to complete (it may take a few minutes).\n\n\nYou can find the most common Linux commands to be used with the Raspi here or here.",
    "crumbs": [
      "Raspberry Pi",
      "Setup"
    ]
  },
  {
    "objectID": "contents/raspi/setup/setup.html#sec-setup-remote-access-edc8",
    "href": "contents/raspi/setup/setup.html#sec-setup-remote-access-edc8",
    "title": "Setup",
    "section": "",
    "text": "The easiest way to interact with the Raspi-Zero is via SSH (‚ÄúHeadless‚Äù). You can use a Terminal (MAC/Linux), PuTTy (Windows), or any other.\n\nFind your Raspberry Pi‚Äôs IP address (for example, check your router).\nOn your computer, open a terminal and connect via SSH:\nssh username@[raspberry_pi_ip_address]\nAlternatively, if you do not have the IP address, you can try the following: bash ssh username@hostname.local for example, ssh mjrovai@rpi-5.local , ssh mjrovai@raspi.local , etc.\n\n\n\nimg\n\n\nWhen you see the prompt:\nmjrovai@rpi-5:~ $\nIt means that you are interacting remotely with your Raspi. It is a good practice to update/upgrade the system regularly. For that, you should run:\nsudo apt-get update\nsudo apt upgrade\nYou should confirm the Raspi IP address. On the terminal, you can use:\nhostname -I\n\n \n\n\n\nWhen you want to turn off your Raspberry Pi, there are better ideas than just pulling the power cord. This is because the Raspi may still be writing data to the SD card, in which case merely powering down may result in data loss or, even worse, a corrupted SD card.\nFor safety shut down, use the command line:\nsudo shutdown -h now\n\nTo avoid possible data loss and SD card corruption, before removing the power, you should wait a few seconds after shutdown for the Raspberry Pi‚Äôs LED to stop blinking and go dark. Once the LED goes out, it‚Äôs safe to power down.\n\n\n\n\nTransferring files between the Raspi and our main computer can be done using a pen drive, directly on the terminal (with scp), or an FTP program over the network.\n\n\n\n\nLet‚Äôs create a text file on our computer, for example, test.txt.\n \n\nYou can use any text editor. In the same terminal, an option is the nano.\n\nTo copy the file named test.txt from your personal computer to a user‚Äôs home folder on your Raspberry Pi, run the following command from the directory containing test.txt, replacing the &lt;username&gt; placeholder with the username you use to log in to your Raspberry Pi and the &lt;pi_ip_address&gt; placeholder with your Raspberry Pi‚Äôs IP address:\n$ scp test.txt &lt;username&gt;@&lt;pi_ip_address&gt;:~/\n\nNote that ~/ means that we will move the file to the ROOT of our Raspi. You can choose any folder in your Raspi. But you should create the folder before you run scp, since scp won‚Äôt create folders automatically.\n\nFor example, let‚Äôs transfer the file test.txt to the ROOT of my Raspi-zero, which has an IP of 192.168.4.210:\nscp test.txt mjrovai@192.168.4.210:~/\n \nI use a different profile to differentiate the terminals. The above action happens on your computer. Now, let‚Äôs go to our Raspi (using the SSH) and check if the file is there:\n \n\n\n\nTo copy a file named test.txt from a user‚Äôs home directory on a Raspberry Pi to the current directory on another computer, run the following command on your Host Computer:\n$ scp &lt;username&gt;@&lt;pi_ip_address&gt;:myfile.txt .\nFor example:\nOn the Raspi, let‚Äôs create a copy of the file with another name:\ncp test.txt test_2.txt\nAnd on the Host Computer (in my case, a Mac)\nscp mjrovai@192.168.4.210:test_2.txt .\n \n\n\n\n\nTransferring files using FTP, such as FileZilla FTP Client, is also possible. Follow the instructions, install the program for your Desktop OS, and use the Raspi IP address as the Host. For example:\nsftp://192.168.4.210\n and enter your Raspi username and password. Pressing Quickconnect will open two windows, one for your host computer desktop (right) and another for the Raspi (left).",
    "crumbs": [
      "Raspberry Pi",
      "Setup"
    ]
  },
  {
    "objectID": "contents/raspi/setup/setup.html#sec-setup-increasing-swap-memory-1c93",
    "href": "contents/raspi/setup/setup.html#sec-setup-increasing-swap-memory-1c93",
    "title": "Setup",
    "section": "",
    "text": "Using htop, a cross-platform interactive process viewer, you can easily monitor the resources running on your Raspi, such as the list of processes, the running CPUs, and the memory used in real-time. To lunch hop, enter with the command on the terminal:\nhtop\n \nRegarding memory, among the devices in the Raspberry Pi family, the Raspi-Zero has the smallest amount of SRAM (500 MB), compared to a selection of 2 GB to 8 GB on the Raspis 4 or 5. For any Raspi, it is possible to increase the memory available to the system with ‚ÄúSwap.‚Äù Swap memory, also known as swap space, is a technique used in computer operating systems to temporarily store data from RAM (Random Access Memory) on the SD card when the physical RAM is fully utilized. This allows the operating system (OS) to continue running even when RAM is full, which can prevent system crashes or slowdowns.\nSwap memory benefits devices with limited RAM, such as the Raspi-Zero. Increasing swap can help run more demanding applications or processes, but it‚Äôs essential to balance this with the potential performance impact of frequent disk access.\nBy default, the Rapi-Zero‚Äôs SWAP (Swp) memory is only 100 MB, which is very small for running some more complex and demanding Machine Learning applications (for example, YOLO). Let‚Äôs increase it to 2 MB:\nFirst, turn off swap-file:\nsudo dphys-swapfile swapoff\nNext, you should open and change the file /etc/dphys-swapfile. For that, we will use the nano:\nsudo nano /etc/dphys-swapfile\nSearch for the CONF_SWAPSIZE variable (default is 200) and update it to 2000:\nCONF_SWAPSIZE=2000\nAnd save the file.\nNext, turn on the swapfile again and reboot the Raspi-zero:\nsudo dphys-swapfile setup\nsudo dphys-swapfile swapon\nsudo reboot\nWhen your device is rebooted (you should enter with the SSH again), you will realize that the maximum swap memory value shown on top is now something near 2 GB (in my case, 1.95 GB).\n\nTo keep the htop running, you should open another terminal window to interact continuously with your Raspi.",
    "crumbs": [
      "Raspberry Pi",
      "Setup"
    ]
  },
  {
    "objectID": "contents/raspi/setup/setup.html#sec-setup-installing-camera-d427",
    "href": "contents/raspi/setup/setup.html#sec-setup-installing-camera-d427",
    "title": "Setup",
    "section": "",
    "text": "The Raspi is an excellent device for computer vision applications; a camera is needed for it. We can install a standard USB webcam on the micro-USB port using a USB OTG adapter (Raspi-Zero and Raspi-5) or a camera module connected to the Raspi CSI (Camera Serial Interface) port.\n\nUSB Webcams generally have inferior quality to the camera modules that connect to the CSI port. They can also not be controlled using the raspistill and raspivid commands in the terminal or the picamera recording package in Python. Nevertheless, there may be reasons why you want to connect a USB camera to your Raspberry Pi, such as because of the benefit that it is much easier to set up multiple cameras with a single Raspberry Pi, long cables, or simply because you have such a camera on hand.\n\n\n\n\nPower off the Raspi:\n\nsudo shutdown -h no\n\nConnect the USB Webcam (USB Camera Module 30 fps, \\(1280\\times 720\\)) to your Raspi (In this example, I am using the Raspi-Zero, but the instructions work for all Raspis).\n\n \n\nPower on again and run the SSH\nTo check if your USB camera is recognized, run:\n\nlsusb\nYou should see your camera listed in the output.\n \n\nTo take a test picture with your USB camera, use:\n\nfswebcam test_image.jpg\nThis will save an image named ‚Äútest_image.jpg‚Äù in your current directory.\n \n\nSince we are using SSH to connect to our Rapsi, we must transfer the image to our main computer so we can view it. We can use FileZilla or SCP for this:\n\nOpen a terminal on your host computer and run:\nscp mjrovai@raspi-zero.local:~/test_image.jpg .\n\nReplace ‚Äúmjrovai‚Äù with your username and ‚Äúraspi-zero‚Äù with Pi‚Äôs hostname.\n\n \n\nIf the image quality isn‚Äôt satisfactory, you can adjust various settings; for example, define a resolution that is suitable for YOLO \\((640x640)\\):\n\nfswebcam -r 640x640 --no-banner test_image_yolo.jpg\nThis captures a higher-resolution image without the default banner.\n \nAn ordinary USB Webcam can also be used:\n \nAnd verified using lsusb\n \n\n\nFor stream video (which is more resource-intensive), we can install and use mjpg-streamer:\nFirst, install Git:\nsudo apt install git\nNow, we should install the necessary dependencies for mjpg-streamer, clone the repository, and proceed with the installation:\nsudo apt install cmake libjpeg62-turbo-dev\ngit clone https://github.com/jacksonliam/mjpg-streamer.git\ncd mjpg-streamer/mjpg-streamer-experimental\nmake\nsudo make install\nThen start the stream with:\nmjpg_streamer -i \"input_uvc.so\" -o \"output_http.so -w ./www\"\nWe can then access the stream by opening a web browser and navigating to:\nhttp://&lt;your_pi_ip_address&gt;:8080. In my case: http://192.168.4.210:8080\nWe should see a webpage with options to view the stream. Click on the link that says ‚ÄúStream‚Äù or try accessing:\nhttp://&lt;raspberry_pi_ip_address&gt;:8080/?action=stream\n \n\n\n\n\nThere are now several Raspberry Pi camera modules. The original 5-megapixel model was released in 2013, followed by an 8-megapixel Camera Module 2 that was later released in 2016. The latest camera model is the 12-megapixel Camera Module 3, released in 2023.\nThe original 5 MP camera (Arducam OV5647) is no longer available from Raspberry Pi but can be found from several alternative suppliers. Below is an example of such a camera on a Raspi-Zero.\n \nHere is another example of a v2 Camera Module, which has a Sony IMX219 8-megapixel sensor:\n \nAny camera module will work on the Raspberry Pis, but for that, the configuration.txt file must be updated:\nsudo nano /boot/firmware/config.txt\nAt the bottom of the file, for example, to use the 5 MP Arducam OV5647 camera, add the line:\ndtoverlay=ov5647,cam0\nOr for the v2 module, which has the 8MP Sony IMX219 camera:\ndtoverlay=imx219,cam0\nSave the file (CTRL+O [ENTER] CRTL+X) and reboot the Raspi:\nSudo reboot\nAfter the boot, you can see if the camera is listed:\nlibcamera-hello --list-cameras\n \n \n\nlibcamera is an open-source software library that supports camera systems directly from the Linux operating system on Arm processors. It minimizes proprietary code running on the Broadcom GPU.\n\nLet‚Äôs capture a jpeg image with a resolution of \\(640\\times 480\\) for testing and save it to a file named test_cli_camera.jpg\nrpicam-jpeg --output test_cli_camera.jpg --width 640 --height 480\n if we want to see the file saved, we should use ls -f, which lists all current directory content in long format. As before, we can use scp to view the image:",
    "crumbs": [
      "Raspberry Pi",
      "Setup"
    ]
  },
  {
    "objectID": "contents/raspi/setup/setup.html#sec-setup-running-raspi-desktop-remotely-83f4",
    "href": "contents/raspi/setup/setup.html#sec-setup-running-raspi-desktop-remotely-83f4",
    "title": "Setup",
    "section": "",
    "text": "While we‚Äôve primarily interacted with the Raspberry Pi using terminal commands via SSH, we can access the whole graphical desktop environment remotely if we have installed the complete Raspberry Pi OS (for example, Raspberry Pi OS (64-bit). This can be particularly useful for tasks that benefit from a visual interface. To enable this functionality, we must set up a VNC (Virtual Network Computing) server on the Raspberry Pi. Here‚Äôs how to do it:\n\nEnable the VNC Server:\n\nConnect to your Raspberry Pi via SSH.\nRun the Raspberry Pi configuration tool by entering:\nsudo raspi-config\nNavigate to Interface Options using the arrow keys.\n\n\n \n\nSelect VNC and Yes to enable the VNC server.\n\n \n\nExit the configuration tool, saving changes when prompted.\n\n \n\nInstall a VNC Viewer on Your Computer:\n\nDownload and install a VNC viewer application on your main computer. Popular options include RealVNC Viewer, TightVNC, or VNC Viewer by RealVNC. We will install VNC Viewer by RealVNC.\n\nOnce installed, you should confirm the Raspi IP address. For example, on the terminal, you can use:\nhostname -I\n\n \n\nConnect to Your Raspberry Pi:\n\nOpen your VNC viewer application.\n\n\n \n\nEnter your Raspberry Pi‚Äôs IP address and hostname.\nWhen prompted, enter your Raspberry Pi‚Äôs username and password.\n\n \n\nThe Raspberry Pi 5 Desktop should appear on your computer monitor.\n\n \n\nAdjust Display Settings (if needed):\n\nOnce connected, adjust the display resolution for optimal viewing. This can be done through the Raspberry Pi‚Äôs desktop settings or by modifying the config.txt file.\nLet‚Äôs do it using the desktop settings. Reach the menu (the Raspberry Icon at the left upper corner) and select the best screen definition for your monitor:",
    "crumbs": [
      "Raspberry Pi",
      "Setup"
    ]
  },
  {
    "objectID": "contents/raspi/setup/setup.html#sec-setup-updating-installing-software-8692",
    "href": "contents/raspi/setup/setup.html#sec-setup-updating-installing-software-8692",
    "title": "Setup",
    "section": "",
    "text": "Update your system:\nsudo apt update && sudo apt upgrade -y\nInstall essential software:\nsudo apt install python3-pip -y\nEnable pip for Python projects:\nsudo rm /usr/lib/python3.11/EXTERNALLY-MANAGED",
    "crumbs": [
      "Raspberry Pi",
      "Setup"
    ]
  },
  {
    "objectID": "contents/raspi/setup/setup.html#sec-setup-modelspecific-considerations-45de",
    "href": "contents/raspi/setup/setup.html#sec-setup-modelspecific-considerations-45de",
    "title": "Setup",
    "section": "",
    "text": "Limited processing power, best for lightweight projects\nIt is better to use a headless setup (SSH) to conserve resources.\nConsider increasing swap space for memory-intensive tasks.\nIt can be used for Image Classification and Object Detection Labs but not for the LLM (SLM).\n\n\n\n\n\nSuitable for more demanding projects, including AI and machine learning.\nIt can run the whole desktop environment smoothly.\nRaspi-4 can be used for Image Classification and Object Detection Labs but will not work well with LLMs (SLM).\nFor Raspi-5, consider using an active cooler for temperature management during intensive tasks, as in the LLMs (SLMs) lab.\n\nRemember to adjust your project requirements based on the specific Raspberry Pi model you‚Äôre using. The Raspi-Zero is great for low-power, space-constrained projects, while the Raspi-4 or 5 models are better suited for more computationally intensive tasks.",
    "crumbs": [
      "Raspberry Pi",
      "Setup"
    ]
  },
  {
    "objectID": "contents/raspi/raspi.html",
    "href": "contents/raspi/raspi.html",
    "title": "Overview",
    "section": "",
    "text": "These labs offer invaluable hands-on experience with machine learning systems, leveraging the versatility and accessibility of the Raspberry Pi platform. Unlike working with large-scale models that demand extensive cloud resources, these exercises allow you to directly interact with hardware and software in a compact yet powerful edge computing environment. You‚Äôll gain practical insights into deploying AI at the edge by utilizing Raspberry Pi‚Äôs capabilities, from the efficient Pi Zero to the more robust Pi 4 or Pi 5 models. This approach provides a tangible understanding of the challenges and opportunities in implementing machine learning solutions in resource-constrained settings. While we‚Äôre working at a smaller scale, the principles and techniques you‚Äôll learn are fundamentally similar to those used in larger systems. The Raspberry Pi‚Äôs ability to run a whole operating system and its extensive GPIO capabilities allow for a rich learning experience that bridges the gap between theoretical knowledge and real-world application. Through these labs, you‚Äôll grasp the intricacies of EdgeML and develop skills applicable to a wide range of AI deployment scenarios.\n\n\n\nRaspberry Pi Zero 2-W and Raspberry Pi 5 with Camera\n\n\n\n\nRaspberry Pi boards are available from authorized resellers worldwide:\n\nRaspberry Pi Products (Official site with reseller locator)\nRaspberry Pi Zero 2 W: ~$15\nRaspberry Pi 4 (4GB): ~$55\nRaspberry Pi 5 (4GB): ~$60\nRaspberry Pi 5 (8GB): ~$80\n\nCamera modules, power adapters, and SD cards are available from the same resellers.\n\n\n\n\nRaspberry Pi: Ensure you have at least one of the boards: the Raspberry Pi Zero 2 W, Raspberry Pi 4 or 5 for the Vision Labs, and the Raspberry 5 for the GenAi labs.\nPower Adapter: To Power on the boards.\n\nRaspberry Pi Zero 2-W: 2.5 W with a Micro-USB adapter\nRaspberry Pi 4 or 5: 3.5 W with a USB-C adapter\n\nNetwork: With internet access for downloading the necessary software and controlling the boards remotely.\nSD Card (32 GB minimum) and an SD card Adapter: For the Raspberry Pi OS.\n\n\n\n\n\nSetup Raspberry Pi\n\n\n\n\n\n\n\nModality\nTask\nDescription\nLink\n\n\n\n\nVision\nImage Classification\nLearn to classify images\nLink\n\n\nVision\nObject Detection\nImplement object detection\nLink\n\n\nGenAI\nSmall Language Models\nDeploy SLMs at the Edge\nLink\n\n\nGenAI\nVisual-Language Models\nDeploy VLMs at the Edge\nLink",
    "crumbs": [
      "Raspberry Pi",
      "Overview"
    ]
  },
  {
    "objectID": "contents/raspi/raspi.html#sec-overview-where-to-buy",
    "href": "contents/raspi/raspi.html#sec-overview-where-to-buy",
    "title": "Overview",
    "section": "",
    "text": "Raspberry Pi boards are available from authorized resellers worldwide:\n\nRaspberry Pi Products (Official site with reseller locator)\nRaspberry Pi Zero 2 W: ~$15\nRaspberry Pi 4 (4GB): ~$55\nRaspberry Pi 5 (4GB): ~$60\nRaspberry Pi 5 (8GB): ~$80\n\nCamera modules, power adapters, and SD cards are available from the same resellers.",
    "crumbs": [
      "Raspberry Pi",
      "Overview"
    ]
  },
  {
    "objectID": "contents/raspi/raspi.html#sec-overview-prerequisites-028f",
    "href": "contents/raspi/raspi.html#sec-overview-prerequisites-028f",
    "title": "Overview",
    "section": "",
    "text": "Raspberry Pi: Ensure you have at least one of the boards: the Raspberry Pi Zero 2 W, Raspberry Pi 4 or 5 for the Vision Labs, and the Raspberry 5 for the GenAi labs.\nPower Adapter: To Power on the boards.\n\nRaspberry Pi Zero 2-W: 2.5 W with a Micro-USB adapter\nRaspberry Pi 4 or 5: 3.5 W with a USB-C adapter\n\nNetwork: With internet access for downloading the necessary software and controlling the boards remotely.\nSD Card (32 GB minimum) and an SD card Adapter: For the Raspberry Pi OS.",
    "crumbs": [
      "Raspberry Pi",
      "Overview"
    ]
  },
  {
    "objectID": "contents/raspi/raspi.html#sec-overview-setup-02c7",
    "href": "contents/raspi/raspi.html#sec-overview-setup-02c7",
    "title": "Overview",
    "section": "",
    "text": "Setup Raspberry Pi",
    "crumbs": [
      "Raspberry Pi",
      "Overview"
    ]
  },
  {
    "objectID": "contents/raspi/raspi.html#sec-overview-exercises-6edf",
    "href": "contents/raspi/raspi.html#sec-overview-exercises-6edf",
    "title": "Overview",
    "section": "",
    "text": "Modality\nTask\nDescription\nLink\n\n\n\n\nVision\nImage Classification\nLearn to classify images\nLink\n\n\nVision\nObject Detection\nImplement object detection\nLink\n\n\nGenAI\nSmall Language Models\nDeploy SLMs at the Edge\nLink\n\n\nGenAI\nVisual-Language Models\nDeploy VLMs at the Edge\nLink",
    "crumbs": [
      "Raspberry Pi",
      "Overview"
    ]
  },
  {
    "objectID": "contents/raspi/image_classification/image_classification.html",
    "href": "contents/raspi/image_classification/image_classification.html",
    "title": "Image Classification",
    "section": "",
    "text": "DALL¬∑E prompt - A cover image for an ‚ÄòImage Classification‚Äô chapter in a Raspberry Pi tutorial, designed in the same vintage 1950s electronics lab style as previous covers. The scene should feature a Raspberry Pi connected to a camera module, with the camera capturing a photo of the small blue robot provided by the user. The robot should be placed on a workbench, surrounded by classic lab tools like soldering irons, resistors, and wires. The lab background should include vintage equipment like oscilloscopes and tube radios, maintaining the detailed and nostalgic feel of the era. No text or logos should be included.\n\n\n\n\nImage classification is a fundamental task in computer vision that involves categorizing an image into one of several predefined classes. It‚Äôs a cornerstone of artificial intelligence, enabling machines to interpret and understand visual information in a way that mimics human perception.\nImage classification refers to assigning a label or category to an entire image based on its visual content. This task is crucial in computer vision and has numerous applications across various industries. Image classification‚Äôs importance lies in its ability to automate visual understanding tasks that would otherwise require human intervention.\n\n\nImage classification has found its way into numerous real-world applications, revolutionizing various sectors:\n\nHealthcare: Assisting in medical image analysis, such as identifying abnormalities in X-rays or MRIs.\nAgriculture: Monitoring crop health and detecting plant diseases through aerial imagery.\nAutomotive: Enabling advanced driver assistance systems and autonomous vehicles to recognize road signs, pedestrians, and other vehicles.\nRetail: Powering visual search capabilities and automated inventory management systems.\nSecurity and Surveillance: Enhancing threat detection and facial recognition systems.\nEnvironmental Monitoring: Analyzing satellite imagery for deforestation, urban planning, and climate change studies.\n\n\n\n\nImplementing image classification on edge devices such as the Raspberry Pi offers several compelling advantages:\n\nLow Latency: Processing images locally eliminates the need to send data to cloud servers, significantly reducing response times.\nOffline Functionality: Classification can be performed without an internet connection, making it suitable for remote or connectivity-challenged environments.\nPrivacy and Security: Sensitive image data remains on the local device, addressing data privacy concerns and compliance requirements.\nCost-Effectiveness: Eliminates the need for expensive cloud computing resources, especially for continuous or high-volume classification tasks.\nScalability: Enables distributed computing architectures where multiple devices can work independently or in a network.\nEnergy Efficiency: Optimized models on dedicated hardware can be more energy-efficient than cloud-based solutions, which is crucial for battery-powered or remote applications.\nCustomization: Deploying specialized or frequently updated models tailored to specific use cases is more manageable.\n\nWe can create more responsive, secure, and efficient computer vision solutions by leveraging the power of edge devices like Raspberry Pi for image classification. This approach opens up new possibilities for integrating intelligent visual processing into various applications and environments.\nIn the following sections, we‚Äôll explore how to implement and optimize image classification on the Raspberry Pi, harnessing these advantages to create powerful and efficient computer vision systems.\n\n\n\n\n\n\nFirst, ensure your Raspberry Pi is up to date:\nsudo apt update\nsudo apt upgrade -y\n\n\n\nInstall the necessary libraries for image processing and machine learning:\nsudo apt install python3-pip\nsudo rm /usr/lib/python3.11/EXTERNALLY-MANAGED\npip3 install --upgrade pip\n\n\n\nCreate a virtual environment to manage dependencies:\npython3 -m venv ~/tflite\nsource ~/tflite/bin/activate\n\n\n\nWe are interested in performing inference, which refers to executing a TensorFlow Lite model on a device to make predictions based on input data. To perform an inference with a TensorFlow Lite model, we must run it through an interpreter. The TensorFlow Lite interpreter is designed to be lean and fast. The interpreter uses a static graph ordering and a custom (less-dynamic) memory allocator to ensure minimal load, initialization, and execution latency.\nWe‚Äôll use the TensorFlow Lite runtime for Raspberry Pi, a simplified library for running machine learning models on mobile and embedded devices, without including all TensorFlow packages.\npip install tflite_runtime --no-deps\n\nThe wheel installed: tflite_runtime-2.14.0-cp311-cp311-manylinux_2_34_aarch64.whl\n\n\n\n\nInstall required Python libraries for use with Image Classification:\nIf you have another version of Numpy installed, first uninstall it.\npip3 uninstall numpy\nInstall version 1.23.2, which is compatible with the tflite_runtime.\n pip3 install numpy==1.23.2\npip3 install Pillow matplotlib\n\n\n\nIf you are working on the Raspi-Zero with the minimum OS (No Desktop), you may not have a user-pre-defined directory tree (you can check it with ls. So, let‚Äôs create one:\nmkdir Documents\ncd Documents/\nmkdir TFLITE\ncd TFLITE/\nmkdir IMG_CLASS\ncd IMG_CLASS\nmkdir models\ncd models\n\nOn the Raspi-5, the /Documents should be there.\n\nGet a pre-trained Image Classification model:\nAn appropriate pre-trained model is crucial for successful image classification on resource-constrained devices like the Raspberry Pi. MobileNet is designed for mobile and embedded vision applications with a good balance between accuracy and speed. Versions: MobileNetV1, MobileNetV2, MobileNetV3. Let‚Äôs download the V2:\n# One long line, split with backslash \\\nwget https://storage.googleapis.com/download.tensorflow.org/\\\nmodels/tflite_11_05_08/mobilenet_v2_1.0_224_quant.tgz\n\ntar xzf mobilenet_v2_1.0_224_quant.tgz\nGet its labels:\nwget https://raw.githubusercontent.com/Mjrovai/EdgeML-with-Raspberry-Pi/refs/heads/\\\nmain/IMG_CLASS/models/labels.txt\nIn the end, you should have the models in its directory:\n \n\nWe will only need the mobilenet_v2_1.0_224_quant.tflite model and the labels.txt. You can delete the other files.\n\n\n\n\nIf you prefer using Jupyter Notebook for development:\npip3 install jupyter\njupyter notebook --generate-config\nTo run Jupyter Notebook, run the command (change the IP address for yours):\njupyter notebook --ip=192.168.4.210 --no-browser\nOn the terminal, you can see the local URL address to open the notebook:\n \nYou can access it from another device by entering the Raspberry Pi‚Äôs IP address and the provided token in a web browser (you can copy the token from the terminal).\n \nDefine your working directory in the Raspi and create a new Python 3 notebook.\n\n\n\nTest your setup by running a simple Python script:\nimport tflite_runtime.interpreter as tflite\nimport numpy as np\nfrom PIL import Image\n\nprint(\"NumPy:\", np.__version__)\nprint(\"Pillow:\", Image.__version__)\n\n# Try to create a TFLite Interpreter\nmodel_path = \"./models/mobilenet_v2_1.0_224_quant.tflite\"\ninterpreter = tflite.Interpreter(model_path=model_path)\ninterpreter.allocate_tensors()\nprint(\"TFLite Interpreter created successfully!\")\nYou can create the Python script using nano on the terminal, saving it with CTRL+0 + ENTER + CTRL+X\n \nAnd run it with the command:\n \nOr you can run it directly on the Notebook:\n \n\n\n\n\nIn the last section, we set up the environment, including downloading a popular pre-trained model, Mobilenet V2, trained on ImageNet‚Äôs \\(224\\times 224\\) images (1.2 million) for 1,001 classes (1,000 object categories plus 1 background). The model was converted to a compact 3.5 MB TensorFlow Lite format, making it suitable for the limited storage and memory of a Raspberry Pi.\n \nLet‚Äôs start a new notebook to follow all the steps to classify one image:\nImport the needed libraries:\nimport time\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom PIL import Image\nimport tflite_runtime.interpreter as tflite\nLoad the TFLite model and allocate tensors:\nmodel_path = \"./models/mobilenet_v2_1.0_224_quant.tflite\"\ninterpreter = tflite.Interpreter(model_path=model_path)\ninterpreter.allocate_tensors()\nGet input and output tensors.\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\nInput details will give us information about how the model should be fed with an image. The shape of (1, 224, 224, 3) informs us that an image with dimensions \\((224\\times 224\\times 3)\\) should be input one by one (Batch Dimension: 1).\n \nThe output details show that the inference will result in an array of 1,001 integer values. Those values result from the image classification, where each value is the probability of that specific label being related to the image.\n \nLet‚Äôs also inspect the dtype of input details of the model\ninput_dtype = input_details[0][\"dtype\"]\ninput_dtype\ndtype('uint8')\nThis shows that the input image should be raw pixels (0 - 255).\nLet‚Äôs get a test image. You can transfer it from your computer or download one for testing. Let‚Äôs first create a folder under our working directory:\nmkdir images\ncd images\nwget https://upload.wikimedia.org/wikipedia/commons/3/3a/Cat03.jpg\nLet‚Äôs load and display the image:\n# Load he image\nimg_path = \"./images/Cat03.jpg\"\nimg = Image.open(img_path)\n\n# Display the image\nplt.figure(figsize=(8, 8))\nplt.imshow(img)\nplt.title(\"Original Image\")\nplt.show()\n \nWe can see the image size running the command:\nwidth, height = img.size\nThat shows us that the image is an RGB image with a width of 1600 and a height of 1600 pixels. So, to use our model, we should reshape it to (224, 224, 3) and add a batch dimension of 1, as defined in input details: (1, 224, 224, 3). The inference result, as shown in output details, will be an array with a 1001 size, as shown below:\n \nSo, let‚Äôs reshape the image, add the batch dimension, and see the result:\nimg = img.resize(\n    (input_details[0][\"shape\"][1], input_details[0][\"shape\"][2])\n)\ninput_data = np.expand_dims(img, axis=0)\ninput_data.shape\nThe input_data shape is as expected: (1, 224, 224, 3)\nLet‚Äôs confirm the dtype of the input data:\ninput_data.dtype\ndtype('uint8')\nThe input data dtype is ‚Äòuint8‚Äô, which is compatible with the dtype expected for the model.\nUsing the input_data, let‚Äôs run the interpreter and get the predictions (output):\ninterpreter.set_tensor(input_details[0][\"index\"], input_data)\ninterpreter.invoke()\npredictions = interpreter.get_tensor(output_details[0][\"index\"])[0]\nThe prediction is an array with 1001 elements. Let‚Äôs get the Top-5 indices where their elements have high values:\ntop_k_results = 5\ntop_k_indices = np.argsort(predictions)[::-1][:top_k_results]\ntop_k_indices\nThe top_k_indices is an array with 5 elements: array([283, 286, 282])\nSo, 283, 286, 282, 288, and 479 are the image‚Äôs most probable classes. Having the index, we must find to what class it appoints (such as car, cat, or dog). The text file downloaded with the model has a label associated with each index from 0 to 1,000. Let‚Äôs use a function to load the .txt file as a list:\ndef load_labels(filename):\n    with open(filename, \"r\") as f:\n        return [line.strip() for line in f.readlines()]\nAnd get the list, printing the labels associated with the indexes:\nlabels_path = \"./models/labels.txt\"\nlabels = load_labels(labels_path)\n\nprint(labels[286])\nprint(labels[283])\nprint(labels[282])\nprint(labels[288])\nprint(labels[479])\nAs a result, we have:\nEgyptian cat\ntiger cat\ntabby\nlynx\ncarton\nAt least the four top indices are related to felines. The prediction content is the probability associated with each one of the labels. As we saw on output details, those values are quantized and should be dequantized and apply softmax.\nscale, zero_point = output_details[0][\"quantization\"]\ndequantized_output = (\n    predictions.astype(np.float32) - zero_point\n) * scale\nexp_output = np.exp(dequantized_output - np.max(dequantized_output))\nprobabilities = exp_output / np.sum(exp_output)\nLet‚Äôs print the top-5 probabilities:\nprint(probabilities[286])\nprint(probabilities[283])\nprint(probabilities[282])\nprint(probabilities[288])\nprint(probabilities[479])\n0.27741462\n0.3732285\n0.16919471\n0.10319158\n0.023410844\nFor clarity, let‚Äôs create a function to relate the labels with the probabilities:\nfor i in range(top_k_results):\n    print(\n        \"\\t{:20}: {}%\".format(\n            labels[top_k_indices[i]],\n            (int(probabilities[top_k_indices[i]] * 100)),\n        )\n    )\ntiger cat           : 37%\nEgyptian cat        : 27%\ntabby               : 16%\nlynx                : 10%\ncarton              : 2%\n\n\nLet‚Äôs create a general function to give an image as input, and we get the Top-5 possible classes:\ndef image_classification(\n    img_path, model_path, labels, top_k_results=5\n):\n    # load the image\n    img = Image.open(img_path)\n    plt.figure(figsize=(4, 4))\n    plt.imshow(img)\n    plt.axis(\"off\")\n\n    # Load the TFLite model\n    interpreter = tflite.Interpreter(model_path=model_path)\n    interpreter.allocate_tensors()\n\n    # Get input and output tensors\n    input_details = interpreter.get_input_details()\n    output_details = interpreter.get_output_details()\n\n    # Preprocess\n    img = img.resize(\n        (input_details[0][\"shape\"][1], input_details[0][\"shape\"][2])\n    )\n    input_data = np.expand_dims(img, axis=0)\n\n    # Inference on Raspi-Zero\n    interpreter.set_tensor(input_details[0][\"index\"], input_data)\n    interpreter.invoke()\n\n    # Obtain results and map them to the classes\n    predictions = interpreter.get_tensor(output_details[0][\"index\"])[\n        0\n    ]\n\n    # Get indices of the top k results\n    top_k_indices = np.argsort(predictions)[::-1][:top_k_results]\n\n    # Get quantization parameters\n    scale, zero_point = output_details[0][\"quantization\"]\n\n    # Dequantize the output and apply softmax\n    dequantized_output = (\n        predictions.astype(np.float32) - zero_point\n    ) * scale\n    exp_output = np.exp(\n        dequantized_output - np.max(dequantized_output)\n    )\n    probabilities = exp_output / np.sum(exp_output)\n\n    print(\"\\n\\t[PREDICTION]        [Prob]\\n\")\n    for i in range(top_k_results):\n        print(\n            \"\\t{:20}: {}%\".format(\n                labels[top_k_indices[i]],\n                (int(probabilities[top_k_indices[i]] * 100)),\n            )\n        )\nAnd loading some images for testing, we have:\n \n\n\n\nLet‚Äôs get a TFLite model trained from scratch. For that, you can follow the Notebook:\nCNN to classify Cifar-10 dataset\nIn the notebook, we trained a model using the CIFAR10 dataset, which contains 60,000 images from 10 classes of CIFAR (airplane, automobile, bird, cat, deer, dog, frog, horse, ship, and truck). CIFAR has \\(32\\times 32\\) color images (3 color channels) where the objects are not centered and can have the object with a background, such as airplanes that might have a cloudy sky behind them! In short, small but real images.\nThe CNN trained model (cifar10_model.keras) had a size of 2.0MB. Using the TFLite Converter, the model cifar10.tflite became with 674MB (around 1/3 of the original size).\n \nOn the notebook Cifar 10 - Image Classification on a Raspi with TFLite (which can be run over the Raspi), we can follow the same steps we did with the mobilenet_v2_1.0_224_quant.tflite. Below are examples of images using the General Function for Image Classification on a Raspi-Zero, as shown in the last section.\n \n\n\n\nPicamera2, a Python library for interacting with Raspberry Pi‚Äôs camera, is based on the libcamera camera stack, and the Raspberry Pi foundation maintains it. The Picamera2 library is supported on all Raspberry Pi models, from the Pi Zero to the RPi 5. It is already installed system-wide on the Raspi, but we should make it accessible within the virtual environment.\n\nFirst, activate the virtual environment if it‚Äôs not already activated:\nsource ~/tflite/bin/activate\nNow, let‚Äôs create a .pth file in your virtual environment to add the system site-packages path:\necho \"/usr/lib/python3/dist-packages\" &gt; \\\n $VIRTUAL_ENV/lib/python3.11/\nsite-packages/system_site_packages.pth\n\nNote: If your Python version differs, replace python3.11 with the appropriate version.\n\nAfter creating this file, try importing picamera2 in Python:\npython3\n&gt;&gt;&gt; import picamera2\n&gt;&gt;&gt; print(picamera2.__file__)\n\nThe above code will show the file location of the picamera2 module itself, proving that the library can be accessed from the environment.\n/home/mjrovai/tflite/lib/python3.11/site-packages/\\\npicamera2/__init__.py\nYou can also list the available cameras in the system:\n&gt;&gt;&gt; print(Picamera2.global_camera_info())\nIn my case, with a USB installed, I got:\n \nNow that we‚Äôve confirmed picamera2 is working in the environment with an index 0, let‚Äôs try a simple Python script to capture an image from your USB camera:\nfrom picamera2 import Picamera2\nimport time\n\n# Initialize the camera\npicam2 = Picamera2()  # default is index 0\n\n# Configure the camera\nconfig = picam2.create_still_configuration(main={\"size\": (640, 480)})\npicam2.configure(config)\n\n# Start the camera\npicam2.start()\n\n# Wait for the camera to warm up\ntime.sleep(2)\n\n# Capture an image\npicam2.capture_file(\"usb_camera_image.jpg\")\nprint(\"Image captured and saved as 'usb_camera_image.jpg'\")\n\n# Stop the camera\npicam2.stop()\nUse the Nano text editor, the Jupyter Notebook, or any other editor. Save this as a Python script (e.g., capture_image.py) and run it. This should capture an image from your camera and save it as ‚Äúusb_camera_image.jpg‚Äù in the same directory as your script.\n \nIf the Jupyter is open, you can see the captured image on your computer. Otherwise, transfer the file from the Raspi to your computer.\n \n\nIf you are working with a Raspi-5 with a whole desktop, you can open the file directly on the device.\n\n\n\n\n\nNow, we will develop a complete Image Classification project using the Edge Impulse Studio. As we did with the Movilinet V2, the trained and converted TFLite model will be used for inference.\n\n\nThe first step in any ML project is to define its goal. In this case, it is to detect and classify two specific objects present in one image. For this project, we will use two small toys: a robot and a small Brazilian parrot (named Periquito). We will also collect images of a background where those two objects are absent.\n \n\n\n\nOnce we have defined our Machine Learning project goal, the next and most crucial step is collecting the dataset. We can use a phone for the image capture, but we will use the Raspi here. Let‚Äôs set up a simple web server on our Raspberry Pi to view the QVGA (320 x 240) captured images in a browser.\n\nFirst, let‚Äôs install Flask, a lightweight web framework for Python:\npip3 install flask\nLet‚Äôs create a new Python script combining image capture with a web server. We‚Äôll call it get_img_data.py:\n\n\nfrom flask import Flask, Response, render_template_string,\n                  request, redirect, url_for\nfrom picamera2 import Picamera2\nimport io\nimport threading\nimport time\nimport os\nimport signal\n\napp = Flask(__name__)\n\n# Global variables\nbase_dir = \"dataset\"\npicam2 = None\nframe = None\nframe_lock = threading.Lock()\ncapture_counts = {}\ncurrent_label = None\nshutdown_event = threading.Event()\n\ndef initialize_camera():\n    global picam2\n    picam2 = Picamera2()\n    config = picam2.create_preview_configuration(\n             main={\"size\": (320, 240)}\n    )\n    picam2.configure(config)\n    picam2.start()\n    time.sleep(2)  # Wait for camera to warm up\n\ndef get_frame():\n    global frame\n    while not shutdown_event.is_set():\n        stream = io.BytesIO()\n        picam2.capture_file(stream, format='jpeg')\n        with frame_lock:\n            frame = stream.getvalue()\n        time.sleep(0.1)  # Adjust as needed for smooth preview\n\ndef generate_frames():\n    while not shutdown_event.is_set():\n        with frame_lock:\n            if frame is not None:\n                yield (b'--frame\\r\\n'\n                       b'Content-Type: image/jpeg\\r\\n\\r\\n' +\n                                       frame + b'\\r\\n')\n        time.sleep(0.1)  # Adjust as needed for smooth streaming\n\ndef shutdown_server():\n    shutdown_event.set()\n    if picam2:\n        picam2.stop()\n    # Give some time for other threads to finish\n    time.sleep(2)\n    # Send SIGINT to the main process\n    os.kill(os.getpid(), signal.SIGINT)\n\n@app.route('/', methods=['GET', 'POST'])\ndef index():\n    global current_label\n    if request.method == 'POST':\n        current_label = request.form['label']\n        if current_label not in capture_counts:\n            capture_counts[current_label] = 0\n        os.makedirs(os.path.join(base_dir, current_label),\n                                 exist_ok=True)\n        return redirect(url_for('capture_page'))\n    return render_template_string('''\n        &lt;!DOCTYPE html&gt;\n        &lt;html&gt;\n        &lt;head&gt;\n            &lt;title&gt;Dataset Capture - Label Entry&lt;/title&gt;\n        &lt;/head&gt;\n        &lt;body&gt;\n            &lt;h1&gt;Enter Label for Dataset&lt;/h1&gt;\n            &lt;form method=\"post\"&gt;\n                &lt;input type=\"text\" name=\"label\" required&gt;\n                &lt;input type=\"submit\" value=\"Start Capture\"&gt;\n            &lt;/form&gt;\n        &lt;/body&gt;\n        &lt;/html&gt;\n    ''')\n\n@app.route('/capture')\ndef capture_page():\n    return render_template_string('''\n        &lt;!DOCTYPE html&gt;\n        &lt;html&gt;\n        &lt;head&gt;\n            &lt;title&gt;Dataset Capture&lt;/title&gt;\n            &lt;script&gt;\n                var shutdownInitiated = false;\n                function checkShutdown() {\n                    if (!shutdownInitiated) {\n                        fetch('/check_shutdown')\n                            .then(response =&gt; response.json())\n                            .then(data =&gt; {\n                                if (data.shutdown) {\n                                    shutdownInitiated = true;\n                                    document.getElementById(\n                                          'video-feed').src = '';\n                                    document.getElementById(\n                                          'shutdown-message')\n                                    .style.display = 'block';\n                                }\n                            });\n                    }\n                }\n                setInterval(checkShutdown, 1000); // Check\n                                                     every second\n            &lt;/script&gt;\n        &lt;/head&gt;\n        &lt;body&gt;\n            &lt;h1&gt;Dataset Capture&lt;/h1&gt;\n            &lt;p&gt;Current Label: {{ label }}&lt;/p&gt;\n            &lt;p&gt;Images captured for this label: {{ capture_count\n                                                  }}&lt;/p&gt;\n            &lt;img id=\"video-feed\" src=\"{{ url_for('video_feed')\n                                         }}\" width=\"640\"\n            height=\"480\" /&gt;\n            &lt;div id=\"shutdown-message\" style=\"display: none;\n                                              color: red;\"&gt;\n                Capture process has been stopped.\n                You can close this window.\n            &lt;/div&gt;\n            &lt;form action=\"/capture_image\" method=\"post\"&gt;\n                &lt;input type=\"submit\" value=\"Capture Image\"&gt;\n            &lt;/form&gt;\n            &lt;form action=\"/stop\" method=\"post\"&gt;\n                &lt;input type=\"submit\" value=\"Stop Capture\"\n                style=\"background-color: #ff6666;\"&gt;\n            &lt;/form&gt;\n            &lt;form action=\"/\" method=\"get\"&gt;\n                &lt;input type=\"submit\" value=\"Change Label\"\n                style=\"background-color: #ffff66;\"&gt;\n            &lt;/form&gt;\n        &lt;/body&gt;\n        &lt;/html&gt;\n    ''', label=current_label, capture_count=capture_counts.get(\n                                            current_label, 0))\n\n@app.route('/video_feed')\ndef video_feed():\n    return Response(generate_frames(),\n                    mimetype='multipart/x-mixed-replace;\n                    boundary=frame')\n\n@app.route('/capture_image', methods=['POST'])\ndef capture_image():\n    global capture_counts\n    if current_label and not shutdown_event.is_set():\n        capture_counts[current_label] += 1\n        timestamp = time.strftime(\"%Y%m%d-%H%M%S\")\n        filename = f\"image_{timestamp}.jpg\"\n        full_path = os.path.join(base_dir, current_label,\n                                 filename)\n\n        picam2.capture_file(full_path)\n\n    return redirect(url_for('capture_page'))\n\n@app.route('/stop', methods=['POST'])\ndef stop():\n    summary = render_template_string('''\n        &lt;!DOCTYPE html&gt;\n        &lt;html&gt;\n        &lt;head&gt;\n            &lt;title&gt;Dataset Capture - Stopped&lt;/title&gt;\n        &lt;/head&gt;\n        &lt;body&gt;\n            &lt;h1&gt;Dataset Capture Stopped&lt;/h1&gt;\n            &lt;p&gt;The capture process has been stopped.\n               You can close this window.&lt;/p&gt;\n            &lt;p&gt;Summary of captures:&lt;/p&gt;\n            &lt;ul&gt;\n            {% for label, count in capture_counts.items() %}\n                &lt;li&gt;{{ label }}: {{ count }} images&lt;/li&gt;\n            {% endfor %}\n            &lt;/ul&gt;\n        &lt;/body&gt;\n        &lt;/html&gt;\n    ''', capture_counts=capture_counts)\n\n    # Start a new thread to shutdown the server\n    threading.Thread(target=shutdown_server).start()\n\n    return summary\n\n@app.route('/check_shutdown')\ndef check_shutdown():\n    return {'shutdown': shutdown_event.is_set()}\n\nif __name__ == '__main__':\n    initialize_camera()\n    threading.Thread(target=get_frame, daemon=True).start()\n    app.run(host='0.0.0.0', port=5000, threaded=True)\n\n\nRun this script:\n\n    python3 get_img_data.py\n\nAccess the web interface:\n\nOn the Raspberry Pi itself (if you have a GUI): Open a web browser and go to http://localhost:5000\nFrom another device on the same network: Open a web browser and go to http://&lt;raspberry_pi_ip&gt;:5000 (Replace &lt;raspberry_pi_ip&gt; with your Raspberry Pi‚Äôs IP address). For example: http://192.168.4.210:5000/\n\n\nThis Python script creates a web-based interface for capturing and organizing image datasets using a Raspberry Pi and its camera. It‚Äôs handy for machine learning projects that require labeled image data.\n\n\n\nWeb Interface: Accessible from any device on the same network as the Raspberry Pi.\nLive Camera Preview: This shows a real-time feed from the camera.\nLabeling System: Allows users to input labels for different categories of images.\nOrganized Storage: Automatically saves images in label-specific subdirectories.\nPer-Label Counters: Keeps track of how many images are captured for each label.\nSummary Statistics: Provides a summary of captured images when stopping the capture process.\n\n\n\n\n\nFlask Web Application: Handles routing and serves the web interface.\nPicamera2 Integration: Controls the Raspberry Pi camera.\nThreaded Frame Capture: Ensures smooth live preview.\nFile Management: Organizes captured images into labeled directories.\n\n\n\n\n\ninitialize_camera(): Sets up the Picamera2 instance.\nget_frame(): Continuously captures frames for the live preview.\ngenerate_frames(): Yields frames for the live video feed.\nshutdown_server(): Sets the shutdown event, stops the camera, and shuts down the Flask server\nindex(): Handles the label input page.\ncapture_page(): Displays the main capture interface.\nvideo_feed(): Shows a live preview to position the camera\ncapture_image(): Saves an image with the current label.\nstop(): Stops the capture process and displays a summary.\n\n\n\n\n\nStart the script on your Raspberry Pi.\nAccess the web interface from a browser.\nEnter a label for the images you want to capture and press Start Capture.\n\n \n\nUse the live preview to position the camera.\nClick Capture Image to save images under the current label.\n\n \n\nChange labels as needed for different categories, selecting Change Label.\nClick Stop Capture when finished to see a summary.\n\n \n\n\n\n\nThe script uses threading to handle concurrent frame capture and web serving.\nImages are saved with timestamps in their filenames for uniqueness.\nThe web interface is responsive and can be accessed from mobile devices.\n\n\n\n\n\nAdjust image resolution in the initialize_camera() function. Here we used QVGA \\((320\\times 240)\\).\nModify the HTML templates for a different look and feel.\nAdd additional image processing or analysis steps in the capture_image() function.\n\n\n\n\nGet around 60 images from each category (periquito, robot and background). Try to capture different angles, backgrounds, and light conditions. On the Raspi, we will end with a folder named dataset, witch contains 3 sub-folders periquito, robot, and background. one for each class of images.\nYou can use Filezilla to transfer the created dataset to your main computer.\n\n\n\n\n\nWe will use the Edge Impulse Studio to train our model. Go to the Edge Impulse Page, enter your account credentials, and create a new project:\n \n\nHere, you can clone a similar project: Raspi - Img Class.\n\n\n\nWe will walk through four main steps using the EI Studio (or Studio). These steps are crucial in preparing our model for use on the Raspi: Dataset, Impulse, Tests, and Deploy (on the Edge Device, in this case, the Raspi).\n\nRegarding the Dataset, it is essential to point out that our Original Dataset, captured with the Raspi, will be split into Training, Validation, and Test. The Test Set will be separated from the beginning and reserved for use only in the Test phase after training. The Validation Set will be used during training.\n\nOn Studio, follow the steps to upload the captured data:\n\nGo to the Data acquisition tab, and in the UPLOAD DATA section, upload the files from your computer in the chosen categories.\nLeave to the Studio the splitting of the original dataset into train and test and choose the label about\nRepeat the procedure for all three classes. At the end, you should see your ‚Äúraw data‚Äù in the Studio:\n\n \nThe Studio allows you to explore your data, showing a complete view of all the data in your project. You can clear, inspect, or change labels by clicking on individual data items. In our case, a straightforward project, the data seems OK.\n \n\n\n\n\nIn this phase, we should define how to:\n\nPre-process our data, which consists of resizing the individual images and determining the color depth to use (be it RGB or Grayscale) and\nSpecify a Model. In this case, it will be the Transfer Learning (Images) to fine-tune a pre-trained MobileNet V2 image classification model on our data. This method performs well even with relatively small image datasets (around 180 images in our case).\n\nTransfer Learning with MobileNet offers a streamlined approach to model training, which is especially beneficial for resource-constrained environments and projects with limited labeled data. MobileNet, known for its lightweight architecture, is a pre-trained model that has already learned valuable features from a large dataset (ImageNet).\n \nBy leveraging these learned features, we can train a new model for your specific task with fewer data and computational resources and achieve competitive accuracy.\n \nThis approach significantly reduces training time and computational cost, making it ideal for quick prototyping and deployment on embedded devices where efficiency is paramount.\nGo to the Impulse Design Tab and create the impulse, defining an image size of \\(160\\times 160\\) and squashing them (squared form, without cropping). Select Image and Transfer Learning blocks. Save the Impulse.\n \n\n\nAll the input QVGA/RGB565 images will be converted to 76,800 features \\((160\\times 160\\times 3)\\).\n\n\n\n\n\nPress Save parameters and select Generate features in the next tab.\n\n\n\nMobileNet is a family of efficient convolutional neural networks designed for mobile and embedded vision applications. The key features of MobileNet are:\n\nLightweight: Optimized for mobile devices and embedded systems with limited computational resources.\nSpeed: Fast inference times, suitable for real-time applications.\nAccuracy: Maintains good accuracy despite its compact size.\n\nMobileNetV2, introduced in 2018, improves the original MobileNet architecture. Key features include:\n\nInverted Residuals: Inverted residual structures are used where shortcut connections are made between thin bottleneck layers.\nLinear Bottlenecks: Removes non-linearities in the narrow layers to prevent the destruction of information.\nDepth-wise Separable Convolutions: Continues to use this efficient operation from MobileNetV1.\n\nIn our project, we will do a Transfer Learning with the MobileNetV2 160x160 1.0, which means that the images used for training (and future inference) should have an input Size of \\(160\\times 160\\) pixels and a Width Multiplier of 1.0 (full width, not reduced). This configuration balances between model size, speed, and accuracy.\n\n\n\nAnother valuable deep learning technique is Data Augmentation. Data augmentation improves the accuracy of machine learning models by creating additional artificial data. A data augmentation system makes small, random changes to the training data during the training process (such as flipping, cropping, or rotating the images).\nLooking under the hood, here you can see how Edge Impulse implements a data Augmentation policy on your data:\n# Implements the data augmentation policy\ndef augment_image(image, label):\n    # Flips the image randomly\n    image = tf.image.random_flip_left_right(image)\n\n    # Increase the image size, then randomly crop it down to\n    # the original dimensions\n    resize_factor = random.uniform(1, 1.2)\n    new_height = math.floor(resize_factor * INPUT_SHAPE[0])\n    new_width = math.floor(resize_factor * INPUT_SHAPE[1])\n    image = tf.image.resize_with_crop_or_pad(\n        image, new_height, new_width\n    )\n    image = tf.image.random_crop(image, size=INPUT_SHAPE)\n\n    # Vary the brightness of the image\n    image = tf.image.random_brightness(image, max_delta=0.2)\n\n    return image, label\nExposure to these variations during training can help prevent your model from taking shortcuts by ‚Äúmemorizing‚Äù superficial clues in your training data, meaning it may better reflect the deep underlying patterns in your dataset.\nThe final dense layer of our model will have 0 neurons with a 10% dropout for overfitting prevention. Here is the Training result:\n \nThe result is excellent, with a reasonable 35 ms of latency (for a Raspi-4), which should result in around 30 fps (frames per second) during inference. A Raspi-Zero should be slower, and the Raspi-5, faster.\n\n\n\nIf faster inference is needed, we should train the model using smaller alphas (0.35, 0.5, and 0.75) or even reduce the image input size, trading with accuracy. However, reducing the input image size and decreasing the alpha (width multiplier) can speed up inference for MobileNet V2, but they have different trade-offs. Let‚Äôs compare:\n\nReducing Image Input Size:\n\nPros:\n\nSignificantly reduces the computational cost across all layers.\nDecreases memory usage.\nIt often provides a substantial speed boost.\n\nCons:\n\nIt may reduce the model‚Äôs ability to detect small features or fine details.\nIt can significantly impact accuracy, especially for tasks requiring fine-grained recognition.\n\n\nReducing Alpha (Width Multiplier):\n\nPros:\n\nReduces the number of parameters and computations in the model.\nMaintains the original input resolution, potentially preserving more detail.\nIt can provide a good balance between speed and accuracy.\n\nCons:\n\nIt may not speed up inference as dramatically as reducing input size.\nIt can reduce the model‚Äôs capacity to learn complex features.\n\nComparison:\n\nSpeed Impact:\n\nReducing input size often provides a more substantial speed boost because it reduces computations quadratically (halving both width and height reduces computations by about 75%).\nReducing alpha provides a more linear reduction in computations.\n\nAccuracy Impact:\n\nReducing input size can severely impact accuracy, especially when detecting small objects or fine details.\nReducing alpha tends to have a more gradual impact on accuracy.\n\nModel Architecture:\n\nChanging input size doesn‚Äôt alter the model‚Äôs architecture.\nChanging alpha modifies the model‚Äôs structure by reducing the number of channels in each layer.\n\n\nRecommendation:\n\nIf our application doesn‚Äôt require detecting tiny details and can tolerate some loss in accuracy, reducing the input size is often the most effective way to speed up inference.\nReducing alpha might be preferable if maintaining the ability to detect fine details is crucial or if you need a more balanced trade-off between speed and accuracy.\nFor best results, you might want to experiment with both:\n\nTry MobileNet V2 with input sizes like \\(160\\times 160\\) or \\(92\\times 92\\)\nExperiment with alpha values like 1.0, 0.75, 0.5 or 0.35.\n\nAlways benchmark the different configurations on your specific hardware and with your particular dataset to find the optimal balance for your use case.\n\n\nRemember, the best choice depends on your specific requirements for accuracy, speed, and the nature of the images you‚Äôre working with. It‚Äôs often worth experimenting with combinations to find the optimal configuration for your particular use case.\n\n\n\n\nNow, you should take the data set aside at the start of the project and run the trained model using it as input. Again, the result is excellent (92.22%).\n\n\n\nAs we did in the previous section, we can deploy the trained model as .tflite and use Raspi to run it using Python.\nOn the Dashboard tab, go to Transfer learning model (int8 quantized) and click on the download icon:\n \n\nLet‚Äôs also download the float32 version for comparison\n\nTransfer the model from your computer to the Raspi (./models), for example, using FileZilla. Also, capture some images for inference (./images).\nImport the needed libraries:\nimport time\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom PIL import Image\nimport tflite_runtime.interpreter as tflite\nDefine the paths and labels:\nimg_path = \"./images/robot.jpg\"\nmodel_path = \"./models/ei-raspi-img-class-int8-quantized-\\\n                model.tflite\"\nlabels = [\"background\", \"periquito\", \"robot\"]\n\nNote that the models trained on the Edge Impulse Studio will output values with index 0, 1, 2, etc., where the actual labels will follow an alphabetic order.\n\nLoad the model, allocate the tensors, and get the input and output tensor details:\n# Load the TFLite model\ninterpreter = tflite.Interpreter(model_path=model_path)\ninterpreter.allocate_tensors()\n\n# Get input and output tensors\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\nOne important difference to note is that the dtype of the input details of the model is now int8, which means that the input values go from ‚Äì128 to +127, while each pixel of our image goes from 0 to 255. This means that we should pre-process the image to match it. We can check here:\ninput_dtype = input_details[0][\"dtype\"]\ninput_dtype\nnumpy.int8\nSo, let‚Äôs open the image and show it:\nimg = Image.open(img_path)\nplt.figure(figsize=(4, 4))\nplt.imshow(img)\nplt.axis(\"off\")\nplt.show()\n \nAnd perform the pre-processing:\nscale, zero_point = input_details[0][\"quantization\"]\nimg = img.resize(\n    (input_details[0][\"shape\"][1], input_details[0][\"shape\"][2])\n)\nimg_array = np.array(img, dtype=np.float32) / 255.0\nimg_array = (\n    (img_array / scale + zero_point).clip(-128, 127).astype(np.int8)\n)\ninput_data = np.expand_dims(img_array, axis=0)\nChecking the input data, we can verify that the input tensor is compatible with what is expected by the model:\ninput_data.shape, input_data.dtype\n((1, 160, 160, 3), dtype('int8'))\nNow, it is time to perform the inference. Let‚Äôs also calculate the latency of the model:\n# Inference on Raspi-Zero\nstart_time = time.time()\ninterpreter.set_tensor(input_details[0][\"index\"], input_data)\ninterpreter.invoke()\nend_time = time.time()\ninference_time = (end_time - start_time) * 1000  # Convert\n# to milliseconds\nprint(\"Inference time: {:.1f}ms\".format(inference_time))\nThe model will take around 125ms to perform the inference in the Raspi-Zero, which is 3 to 4 times longer than a Raspi-5.\nNow, we can get the output labels and probabilities. It is also important to note that the model trained on the Edge Impulse Studio has a softmax in its output (different from the original Movilenet V2), and we should use the model‚Äôs raw output as the ‚Äúprobabilities.‚Äù\n# Obtain results and map them to the classes\npredictions = interpreter.get_tensor(output_details[0][\"index\"])[0]\n\n# Get indices of the top k results\ntop_k_results = 3\ntop_k_indices = np.argsort(predictions)[::-1][:top_k_results]\n\n# Get quantization parameters\nscale, zero_point = output_details[0][\"quantization\"]\n\n# Dequantize the output\ndequantized_output = (\n    predictions.astype(np.float32) - zero_point\n) * scale\nprobabilities = dequantized_output\n\nprint(\"\\n\\t[PREDICTION]        [Prob]\\n\")\nfor i in range(top_k_results):\n    print(\n        \"\\t{:20}: {:.2f}%\".format(\n            labels[top_k_indices[i]],\n            probabilities[top_k_indices[i]] * 100,\n        )\n    )\n \nLet‚Äôs modify the function created before so that we can handle different type of models:\n\ndef image_classification(\n    img_path, model_path, labels, top_k_results=3, apply_softmax=False\n):\n    # Load the image\n    img = Image.open(img_path)\n    plt.figure(figsize=(4, 4))\n    plt.imshow(img)\n    plt.axis(\"off\")\n\n    # Load the TFLite model\n    interpreter = tflite.Interpreter(model_path=model_path)\n    interpreter.allocate_tensors()\n\n    # Get input and output tensors\n    input_details = interpreter.get_input_details()\n    output_details = interpreter.get_output_details()\n\n    # Preprocess\n    img = img.resize(\n        (input_details[0][\"shape\"][1], input_details[0][\"shape\"][2])\n    )\n\n    input_dtype = input_details[0][\"dtype\"]\n\n    if input_dtype == np.uint8:\n        input_data = np.expand_dims(np.array(img), axis=0)\n    elif input_dtype == np.int8:\n        scale, zero_point = input_details[0][\"quantization\"]\n        img_array = np.array(img, dtype=np.float32) / 255.0\n        img_array = (\n            (img_array / scale + zero_point)\n            .clip(-128, 127)\n            .astype(np.int8)\n        )\n        input_data = np.expand_dims(img_array, axis=0)\n    else:  # float32\n        input_data = (\n            np.expand_dims(np.array(img, dtype=np.float32), axis=0)\n            / 255.0\n        )\n\n    # Inference on Raspi-Zero\n    start_time = time.time()\n    interpreter.set_tensor(input_details[0][\"index\"], input_data)\n    interpreter.invoke()\n    end_time = time.time()\n    inference_time = (\n        end_time - start_time\n    ) * 1000  # Convert to milliseconds\n\n    # Obtain results\n    predictions = interpreter.get_tensor(output_details[0][\"index\"])[\n        0\n    ]\n\n    # Get indices of the top k results\n    top_k_indices = np.argsort(predictions)[::-1][:top_k_results]\n\n    # Handle output based on type\n    output_dtype = output_details[0][\"dtype\"]\n    if output_dtype in [np.int8, np.uint8]:\n        # Dequantize the output\n        scale, zero_point = output_details[0][\"quantization\"]\n        predictions = (\n            predictions.astype(np.float32) - zero_point\n        ) * scale\n\n    if apply_softmax:\n        # Apply softmax\n        exp_preds = np.exp(predictions - np.max(predictions))\n        probabilities = exp_preds / np.sum(exp_preds)\n    else:\n        probabilities = predictions\n\n    print(\"\\n\\t[PREDICTION]        [Prob]\\n\")\n    for i in range(top_k_results):\n        print(\n            \"\\t{:20}: {:.1f}%\".format(\n                labels[top_k_indices[i]],\n                probabilities[top_k_indices[i]] * 100,\n            )\n        )\n    print(\"\\n\\tInference time: {:.1f}ms\".format(inference_time))\n\nAnd test it with different images and the int8 quantized model (160x160 alpha =1.0).\n \nLet‚Äôs download a smaller model, such as the one trained for the Nicla Vision Lab (int8 quantized model, 96x96, alpha = 0.1), as a test. We can use the same function:\n \nThe model lost some accuracy, but it is still OK once our model does not look for many details. Regarding latency, we are around ten times faster on the Raspi-Zero.\n\n\n\n\nLet‚Äôs develop an app to capture images with the USB camera in real time, showing its classification.\nUsing the nano on the terminal, save the code below, such as img_class_live_infer.py.\n\nfrom flask import Flask, Response, render_template_string,\n                  request, jsonify\nfrom picamera2 import Picamera2\nimport io\nimport threading\nimport time\nimport numpy as np\nfrom PIL import Image\nimport tflite_runtime.interpreter as tflite\nfrom queue import Queue\n\napp = Flask(__name__)\n\n# Global variables\npicam2 = None\nframe = None\nframe_lock = threading.Lock()\nis_classifying = False\nconfidence_threshold = 0.8\nmodel_path = \"./models/ei-raspi-img-class-int8-quantized-\\\n                model.tflite\"\nlabels = ['background', 'periquito', 'robot']\ninterpreter = None\nclassification_queue = Queue(maxsize=1)\n\ndef initialize_camera():\n    global picam2\n    picam2 = Picamera2()\n    config = picam2.create_preview_configuration(\n        main={\"size\": (320, 240)}\n    )\n    picam2.configure(config)\n    picam2.start()\n    time.sleep(2)  # Wait for camera to warm up\n\ndef get_frame():\n    global frame\n    while True:\n        stream = io.BytesIO()\n        picam2.capture_file(stream, format='jpeg')\n        with frame_lock:\n            frame = stream.getvalue()\n        time.sleep(0.1)  # Capture frames more frequently\n\ndef generate_frames():\n    while True:\n        with frame_lock:\n            if frame is not None:\n                yield (\n                   b'--frame\\r\\n'\n                   b'Content-Type: image/jpeg\\r\\n\\r\\n'\n                   + frame + b'\\r\\n'\n                )\n        time.sleep(0.1)\n\ndef load_model():\n    global interpreter\n    if interpreter is None:\n        interpreter = tflite.Interpreter(model_path=model_path)\n        interpreter.allocate_tensors()\n    return interpreter\n\ndef classify_image(img, interpreter):\n    input_details = interpreter.get_input_details()\n    output_details = interpreter.get_output_details()\n\n    img = img.resize((input_details[0]['shape'][1],\n                      input_details[0]['shape'][2]))\n    input_data = np.expand_dims(np.array(img), axis=0)\\\n                             .astype(input_details[0]['dtype'])\n\n    interpreter.set_tensor(input_details[0]['index'], input_data)\n    interpreter.invoke()\n\n    predictions = interpreter.get_tensor(output_details[0]\n                                         ['index'])[0]\n    # Handle output based on type\n    output_dtype = output_details[0]['dtype']\n    if output_dtype in [np.int8, np.uint8]:\n        # Dequantize the output\n        scale, zero_point = output_details[0]['quantization']\n        predictions = (predictions.astype(np.float32) -\n                       zero_point) * scale\n    return predictions\n\ndef classification_worker():\n    interpreter = load_model()\n    while True:\n        if is_classifying:\n            with frame_lock:\n                if frame is not None:\n                    img = Image.open(io.BytesIO(frame))\n            predictions = classify_image(img, interpreter)\n            max_prob = np.max(predictions)\n            if max_prob &gt;= confidence_threshold:\n                label = labels[np.argmax(predictions)]\n            else:\n                label = 'Uncertain'\n            classification_queue.put({\n                 'label': label,\n                 'probability': float(max_prob)\n            })\n        time.sleep(0.1)  # Adjust based on your needs\n\n@app.route('/')\ndef index():\n   return render_template_string('''\n      &lt;!DOCTYPE html&gt;\n      &lt;html&gt;\n      &lt;head&gt;\n          &lt;title&gt;Image Classification&lt;/title&gt;\n          &lt;script\n            src=\"https://code.jquery.com/jquery-3.6.0.min.js\"&gt;\n          &lt;/script&gt;\n          &lt;script&gt;\n              function startClassification() {\n                  $.post('/start');\n                  $('#startBtn').prop('disabled', true);\n                  $('#stopBtn').prop('disabled', false);\n              }\n              function stopClassification() {\n                  $.post('/stop');\n                  $('#startBtn').prop('disabled', false);\n                  $('#stopBtn').prop('disabled', true);\n              }\n              function updateConfidence() {\n                  var confidence = $('#confidence').val();\n                  $.post('/update_confidence',\n                         {confidence: confidence}\n                        );\n              }\n              function updateClassification() {\n                  $.get('/get_classification', function(data) {\n                    $('#classification').text(data.label + ': '\n                    + data.probability.toFixed(2));\n                  });\n              }\n              $(document).ready(function() {\n                  setInterval(updateClassification, 100);\n                  // Update every 100ms\n              });\n          &lt;/script&gt;\n      &lt;/head&gt;\n      &lt;body&gt;\n          &lt;h1&gt;Image Classification&lt;/h1&gt;\n          &lt;img src=\"{{ url_for('video_feed') }}\"\n               width=\"640\"\n               height=\"480\" /&gt;\n\n          &lt;br&gt;\n          &lt;button id=\"startBtn\"\n                  onclick=\"startClassification()\"&gt;\n            Start Classification\n          &lt;/button&gt;\n\n          &lt;button id=\"stopBtn\"\n                  onclick=\"stopClassification()\"\n                  disabled&gt;\n            Stop Classification\n          &lt;/button&gt;\n\n          &lt;br&gt;\n          &lt;label for=\"confidence\"&gt;Confidence Threshold:&lt;/label&gt;\n          &lt;input type=\"number\"\n                 id=\"confidence\"\n                 name=\"confidence\"\n                 min=\"0\" max=\"1\"\n                 step=\"0.1\"\n                 value=\"0.8\"\n                 onchange=\"updateConfidence()\" /&gt;\n\n          &lt;br&gt;\n          &lt;div id=\"classification\"&gt;\n             Waiting for classification...\n          &lt;/div&gt;\n\n      &lt;/body&gt;\n      &lt;/html&gt;\n   ''')\n\n@app.route('/video_feed')\ndef video_feed():\n    return Response(\n       generate_frames(),\n       mimetype='multipart/x-mixed-replace; boundary=frame'\n    )\n\n@app.route('/start', methods=['POST'])\ndef start_classification():\n    global is_classifying\n    is_classifying = True\n    return '', 204\n\n@app.route('/stop', methods=['POST'])\ndef stop_classification():\n    global is_classifying\n    is_classifying = False\n    return '', 204\n\n@app.route('/update_confidence', methods=['POST'])\ndef update_confidence():\n    global confidence_threshold\n    confidence_threshold = float(request.form['confidence'])\n    return '', 204\n\n@app.route('/get_classification')\ndef get_classification():\n    if not is_classifying:\n        return jsonify({'label': 'Not classifying',\n                       'probability': 0})\n    try:\n        result = classification_queue.get_nowait()\n    except Queue.Empty:\n        result = {'label': 'Processing', 'probability': 0}\n    return jsonify(result)\n\nif __name__ == '__main__':\n    initialize_camera()\n    threading.Thread(target=get_frame, daemon=True).start()\n    threading.Thread(target=classification_worker,\n                     daemon=True).start()\n    app.run(host='0.0.0.0', port=5000, threaded=True)\n\nOn the terminal, run:\npython3 img_class_live_infer.py\nAnd access the web interface:\n\nOn the Raspberry Pi itself (if you have a GUI): Open a web browser and go to http://localhost:5000\nFrom another device on the same network: Open a web browser and go to http://&lt;raspberry_pi_ip&gt;:5000 (Replace &lt;raspberry_pi_ip&gt; with your Raspberry Pi‚Äôs IP address). For example: http://192.168.4.210:5000/\n\nHere are some screenshots of the app running on an external desktop\n \nHere, you can see the app running on the YouTube:\n\nThe code creates a web application for real-time image classification using a Raspberry Pi, its camera module, and a TensorFlow Lite model. The application uses Flask to serve a web interface where is possible to view the camera feed and see live classification results.\n\n\n\nFlask Web Application: Serves the user interface and handles requests.\nPiCamera2: Captures images from the Raspberry Pi camera module.\nTensorFlow Lite: Runs the image classification model.\nThreading: Manages concurrent operations for smooth performance.\n\n\n\n\n\nLive camera feed display\nReal-time image classification\nAdjustable confidence threshold\nStart/Stop classification on demand\n\n\n\n\n\nImports and Setup:\n\nFlask for web application\nPiCamera2 for camera control\nTensorFlow Lite for inference\nThreading and Queue for concurrent operations\n\nGlobal Variables:\n\nCamera and frame management\nClassification control\nModel and label information\n\nCamera Functions:\n\ninitialize_camera(): Sets up the PiCamera2\nget_frame(): Continuously captures frames\ngenerate_frames(): Yields frames for the web feed\n\nModel Functions:\n\nload_model(): Loads the TFLite model\nclassify_image(): Performs inference on a single image\n\nClassification Worker:\n\nRuns in a separate thread\nContinuously classifies frames when active\nUpdates a queue with the latest results\n\nFlask Routes:\n\n/: Serves the main HTML page\n/video_feed: Streams the camera feed\n/start and /stop: Controls classification\n/update_confidence: Adjusts the confidence threshold\n/get_classification: Returns the latest classification result\n\nHTML Template:\n\nDisplays camera feed and classification results\nProvides controls for starting/stopping and adjusting settings\n\nMain Execution:\n\nInitializes camera and starts necessary threads\nRuns the Flask application\n\n\n\n\n\n\nConcurrent Operations: Using threads to handle camera capture and classification separately from the web server.\nReal-time Updates: Frequent updates to the classification results without page reloads.\nModel Reuse: Loading the TFLite model once and reusing it for efficiency.\nFlexible Configuration: Allowing users to adjust the confidence threshold on the fly.\n\n\n\n\n\nEnsure all dependencies are installed.\nRun the script on a Raspberry Pi with a camera module.\nAccess the web interface from a browser using the Raspberry Pi‚Äôs IP address.\nStart classification and adjust settings as needed.\n\n\n\n\n\nImage classification has emerged as a powerful and versatile application of machine learning, with significant implications for various fields, from healthcare to environmental monitoring. This chapter has demonstrated how to implement a robust image classification system on edge devices like the Raspi-Zero and Raspi-5, showcasing the potential for real-time, on-device intelligence.\nWe‚Äôve explored the entire pipeline of an image classification project, from data collection and model training using Edge Impulse Studio to deploying and running inferences on a Raspi. The process highlighted several key points:\n\nThe importance of proper data collection and preprocessing for training effective models.\nThe power of transfer learning, allowing us to leverage pre-trained models like MobileNet V2 for efficient training with limited data.\nThe trade-offs between model accuracy and inference speed, especially crucial for edge devices.\nThe implementation of real-time classification using a web-based interface, demonstrating practical applications.\n\nThe ability to run these models on edge devices like the Raspi opens up numerous possibilities for IoT applications, autonomous systems, and real-time monitoring solutions. It allows for reduced latency, improved privacy, and operation in environments with limited connectivity.\nAs we‚Äôve seen, even with the computational constraints of edge devices, it‚Äôs possible to achieve impressive results in terms of both accuracy and speed. The flexibility to adjust model parameters, such as input size and alpha values, allows for fine-tuning to meet specific project requirements.\nLooking forward, the field of edge AI and image classification continues to evolve rapidly. Advances in model compression techniques, hardware acceleration, and more efficient neural network architectures promise to further expand the capabilities of edge devices in computer vision tasks.\nThis project serves as a foundation for more complex computer vision applications and encourages further exploration into the exciting world of edge AI and IoT. Whether it‚Äôs for industrial automation, smart home applications, or environmental monitoring, the skills and concepts covered here provide a solid starting point for a wide range of innovative projects.\n\n\n\n\nDataset Example\nSetup Test Notebook on a Raspi\nImage Classification Notebook on a Raspi\nCNN to classify Cifar-10 dataset at CoLab\nCifar 10 - Image Classification on a Raspi\nPython Scripts\nEdge Impulse Project",
    "crumbs": [
      "Raspberry Pi",
      "Image Classification"
    ]
  },
  {
    "objectID": "contents/raspi/image_classification/image_classification.html#sec-image-classification-overview-3e02",
    "href": "contents/raspi/image_classification/image_classification.html#sec-image-classification-overview-3e02",
    "title": "Image Classification",
    "section": "",
    "text": "Image classification is a fundamental task in computer vision that involves categorizing an image into one of several predefined classes. It‚Äôs a cornerstone of artificial intelligence, enabling machines to interpret and understand visual information in a way that mimics human perception.\nImage classification refers to assigning a label or category to an entire image based on its visual content. This task is crucial in computer vision and has numerous applications across various industries. Image classification‚Äôs importance lies in its ability to automate visual understanding tasks that would otherwise require human intervention.\n\n\nImage classification has found its way into numerous real-world applications, revolutionizing various sectors:\n\nHealthcare: Assisting in medical image analysis, such as identifying abnormalities in X-rays or MRIs.\nAgriculture: Monitoring crop health and detecting plant diseases through aerial imagery.\nAutomotive: Enabling advanced driver assistance systems and autonomous vehicles to recognize road signs, pedestrians, and other vehicles.\nRetail: Powering visual search capabilities and automated inventory management systems.\nSecurity and Surveillance: Enhancing threat detection and facial recognition systems.\nEnvironmental Monitoring: Analyzing satellite imagery for deforestation, urban planning, and climate change studies.\n\n\n\n\nImplementing image classification on edge devices such as the Raspberry Pi offers several compelling advantages:\n\nLow Latency: Processing images locally eliminates the need to send data to cloud servers, significantly reducing response times.\nOffline Functionality: Classification can be performed without an internet connection, making it suitable for remote or connectivity-challenged environments.\nPrivacy and Security: Sensitive image data remains on the local device, addressing data privacy concerns and compliance requirements.\nCost-Effectiveness: Eliminates the need for expensive cloud computing resources, especially for continuous or high-volume classification tasks.\nScalability: Enables distributed computing architectures where multiple devices can work independently or in a network.\nEnergy Efficiency: Optimized models on dedicated hardware can be more energy-efficient than cloud-based solutions, which is crucial for battery-powered or remote applications.\nCustomization: Deploying specialized or frequently updated models tailored to specific use cases is more manageable.\n\nWe can create more responsive, secure, and efficient computer vision solutions by leveraging the power of edge devices like Raspberry Pi for image classification. This approach opens up new possibilities for integrating intelligent visual processing into various applications and environments.\nIn the following sections, we‚Äôll explore how to implement and optimize image classification on the Raspberry Pi, harnessing these advantages to create powerful and efficient computer vision systems.",
    "crumbs": [
      "Raspberry Pi",
      "Image Classification"
    ]
  },
  {
    "objectID": "contents/raspi/image_classification/image_classification.html#sec-image-classification-setting-environment-fc34",
    "href": "contents/raspi/image_classification/image_classification.html#sec-image-classification-setting-environment-fc34",
    "title": "Image Classification",
    "section": "",
    "text": "First, ensure your Raspberry Pi is up to date:\nsudo apt update\nsudo apt upgrade -y\n\n\n\nInstall the necessary libraries for image processing and machine learning:\nsudo apt install python3-pip\nsudo rm /usr/lib/python3.11/EXTERNALLY-MANAGED\npip3 install --upgrade pip\n\n\n\nCreate a virtual environment to manage dependencies:\npython3 -m venv ~/tflite\nsource ~/tflite/bin/activate\n\n\n\nWe are interested in performing inference, which refers to executing a TensorFlow Lite model on a device to make predictions based on input data. To perform an inference with a TensorFlow Lite model, we must run it through an interpreter. The TensorFlow Lite interpreter is designed to be lean and fast. The interpreter uses a static graph ordering and a custom (less-dynamic) memory allocator to ensure minimal load, initialization, and execution latency.\nWe‚Äôll use the TensorFlow Lite runtime for Raspberry Pi, a simplified library for running machine learning models on mobile and embedded devices, without including all TensorFlow packages.\npip install tflite_runtime --no-deps\n\nThe wheel installed: tflite_runtime-2.14.0-cp311-cp311-manylinux_2_34_aarch64.whl\n\n\n\n\nInstall required Python libraries for use with Image Classification:\nIf you have another version of Numpy installed, first uninstall it.\npip3 uninstall numpy\nInstall version 1.23.2, which is compatible with the tflite_runtime.\n pip3 install numpy==1.23.2\npip3 install Pillow matplotlib\n\n\n\nIf you are working on the Raspi-Zero with the minimum OS (No Desktop), you may not have a user-pre-defined directory tree (you can check it with ls. So, let‚Äôs create one:\nmkdir Documents\ncd Documents/\nmkdir TFLITE\ncd TFLITE/\nmkdir IMG_CLASS\ncd IMG_CLASS\nmkdir models\ncd models\n\nOn the Raspi-5, the /Documents should be there.\n\nGet a pre-trained Image Classification model:\nAn appropriate pre-trained model is crucial for successful image classification on resource-constrained devices like the Raspberry Pi. MobileNet is designed for mobile and embedded vision applications with a good balance between accuracy and speed. Versions: MobileNetV1, MobileNetV2, MobileNetV3. Let‚Äôs download the V2:\n# One long line, split with backslash \\\nwget https://storage.googleapis.com/download.tensorflow.org/\\\nmodels/tflite_11_05_08/mobilenet_v2_1.0_224_quant.tgz\n\ntar xzf mobilenet_v2_1.0_224_quant.tgz\nGet its labels:\nwget https://raw.githubusercontent.com/Mjrovai/EdgeML-with-Raspberry-Pi/refs/heads/\\\nmain/IMG_CLASS/models/labels.txt\nIn the end, you should have the models in its directory:\n \n\nWe will only need the mobilenet_v2_1.0_224_quant.tflite model and the labels.txt. You can delete the other files.\n\n\n\n\nIf you prefer using Jupyter Notebook for development:\npip3 install jupyter\njupyter notebook --generate-config\nTo run Jupyter Notebook, run the command (change the IP address for yours):\njupyter notebook --ip=192.168.4.210 --no-browser\nOn the terminal, you can see the local URL address to open the notebook:\n \nYou can access it from another device by entering the Raspberry Pi‚Äôs IP address and the provided token in a web browser (you can copy the token from the terminal).\n \nDefine your working directory in the Raspi and create a new Python 3 notebook.\n\n\n\nTest your setup by running a simple Python script:\nimport tflite_runtime.interpreter as tflite\nimport numpy as np\nfrom PIL import Image\n\nprint(\"NumPy:\", np.__version__)\nprint(\"Pillow:\", Image.__version__)\n\n# Try to create a TFLite Interpreter\nmodel_path = \"./models/mobilenet_v2_1.0_224_quant.tflite\"\ninterpreter = tflite.Interpreter(model_path=model_path)\ninterpreter.allocate_tensors()\nprint(\"TFLite Interpreter created successfully!\")\nYou can create the Python script using nano on the terminal, saving it with CTRL+0 + ENTER + CTRL+X\n \nAnd run it with the command:\n \nOr you can run it directly on the Notebook:",
    "crumbs": [
      "Raspberry Pi",
      "Image Classification"
    ]
  },
  {
    "objectID": "contents/raspi/image_classification/image_classification.html#sec-image-classification-making-inferences-mobilenet-v2-d635",
    "href": "contents/raspi/image_classification/image_classification.html#sec-image-classification-making-inferences-mobilenet-v2-d635",
    "title": "Image Classification",
    "section": "",
    "text": "In the last section, we set up the environment, including downloading a popular pre-trained model, Mobilenet V2, trained on ImageNet‚Äôs \\(224\\times 224\\) images (1.2 million) for 1,001 classes (1,000 object categories plus 1 background). The model was converted to a compact 3.5 MB TensorFlow Lite format, making it suitable for the limited storage and memory of a Raspberry Pi.\n \nLet‚Äôs start a new notebook to follow all the steps to classify one image:\nImport the needed libraries:\nimport time\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom PIL import Image\nimport tflite_runtime.interpreter as tflite\nLoad the TFLite model and allocate tensors:\nmodel_path = \"./models/mobilenet_v2_1.0_224_quant.tflite\"\ninterpreter = tflite.Interpreter(model_path=model_path)\ninterpreter.allocate_tensors()\nGet input and output tensors.\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\nInput details will give us information about how the model should be fed with an image. The shape of (1, 224, 224, 3) informs us that an image with dimensions \\((224\\times 224\\times 3)\\) should be input one by one (Batch Dimension: 1).\n \nThe output details show that the inference will result in an array of 1,001 integer values. Those values result from the image classification, where each value is the probability of that specific label being related to the image.\n \nLet‚Äôs also inspect the dtype of input details of the model\ninput_dtype = input_details[0][\"dtype\"]\ninput_dtype\ndtype('uint8')\nThis shows that the input image should be raw pixels (0 - 255).\nLet‚Äôs get a test image. You can transfer it from your computer or download one for testing. Let‚Äôs first create a folder under our working directory:\nmkdir images\ncd images\nwget https://upload.wikimedia.org/wikipedia/commons/3/3a/Cat03.jpg\nLet‚Äôs load and display the image:\n# Load he image\nimg_path = \"./images/Cat03.jpg\"\nimg = Image.open(img_path)\n\n# Display the image\nplt.figure(figsize=(8, 8))\nplt.imshow(img)\nplt.title(\"Original Image\")\nplt.show()\n \nWe can see the image size running the command:\nwidth, height = img.size\nThat shows us that the image is an RGB image with a width of 1600 and a height of 1600 pixels. So, to use our model, we should reshape it to (224, 224, 3) and add a batch dimension of 1, as defined in input details: (1, 224, 224, 3). The inference result, as shown in output details, will be an array with a 1001 size, as shown below:\n \nSo, let‚Äôs reshape the image, add the batch dimension, and see the result:\nimg = img.resize(\n    (input_details[0][\"shape\"][1], input_details[0][\"shape\"][2])\n)\ninput_data = np.expand_dims(img, axis=0)\ninput_data.shape\nThe input_data shape is as expected: (1, 224, 224, 3)\nLet‚Äôs confirm the dtype of the input data:\ninput_data.dtype\ndtype('uint8')\nThe input data dtype is ‚Äòuint8‚Äô, which is compatible with the dtype expected for the model.\nUsing the input_data, let‚Äôs run the interpreter and get the predictions (output):\ninterpreter.set_tensor(input_details[0][\"index\"], input_data)\ninterpreter.invoke()\npredictions = interpreter.get_tensor(output_details[0][\"index\"])[0]\nThe prediction is an array with 1001 elements. Let‚Äôs get the Top-5 indices where their elements have high values:\ntop_k_results = 5\ntop_k_indices = np.argsort(predictions)[::-1][:top_k_results]\ntop_k_indices\nThe top_k_indices is an array with 5 elements: array([283, 286, 282])\nSo, 283, 286, 282, 288, and 479 are the image‚Äôs most probable classes. Having the index, we must find to what class it appoints (such as car, cat, or dog). The text file downloaded with the model has a label associated with each index from 0 to 1,000. Let‚Äôs use a function to load the .txt file as a list:\ndef load_labels(filename):\n    with open(filename, \"r\") as f:\n        return [line.strip() for line in f.readlines()]\nAnd get the list, printing the labels associated with the indexes:\nlabels_path = \"./models/labels.txt\"\nlabels = load_labels(labels_path)\n\nprint(labels[286])\nprint(labels[283])\nprint(labels[282])\nprint(labels[288])\nprint(labels[479])\nAs a result, we have:\nEgyptian cat\ntiger cat\ntabby\nlynx\ncarton\nAt least the four top indices are related to felines. The prediction content is the probability associated with each one of the labels. As we saw on output details, those values are quantized and should be dequantized and apply softmax.\nscale, zero_point = output_details[0][\"quantization\"]\ndequantized_output = (\n    predictions.astype(np.float32) - zero_point\n) * scale\nexp_output = np.exp(dequantized_output - np.max(dequantized_output))\nprobabilities = exp_output / np.sum(exp_output)\nLet‚Äôs print the top-5 probabilities:\nprint(probabilities[286])\nprint(probabilities[283])\nprint(probabilities[282])\nprint(probabilities[288])\nprint(probabilities[479])\n0.27741462\n0.3732285\n0.16919471\n0.10319158\n0.023410844\nFor clarity, let‚Äôs create a function to relate the labels with the probabilities:\nfor i in range(top_k_results):\n    print(\n        \"\\t{:20}: {}%\".format(\n            labels[top_k_indices[i]],\n            (int(probabilities[top_k_indices[i]] * 100)),\n        )\n    )\ntiger cat           : 37%\nEgyptian cat        : 27%\ntabby               : 16%\nlynx                : 10%\ncarton              : 2%\n\n\nLet‚Äôs create a general function to give an image as input, and we get the Top-5 possible classes:\ndef image_classification(\n    img_path, model_path, labels, top_k_results=5\n):\n    # load the image\n    img = Image.open(img_path)\n    plt.figure(figsize=(4, 4))\n    plt.imshow(img)\n    plt.axis(\"off\")\n\n    # Load the TFLite model\n    interpreter = tflite.Interpreter(model_path=model_path)\n    interpreter.allocate_tensors()\n\n    # Get input and output tensors\n    input_details = interpreter.get_input_details()\n    output_details = interpreter.get_output_details()\n\n    # Preprocess\n    img = img.resize(\n        (input_details[0][\"shape\"][1], input_details[0][\"shape\"][2])\n    )\n    input_data = np.expand_dims(img, axis=0)\n\n    # Inference on Raspi-Zero\n    interpreter.set_tensor(input_details[0][\"index\"], input_data)\n    interpreter.invoke()\n\n    # Obtain results and map them to the classes\n    predictions = interpreter.get_tensor(output_details[0][\"index\"])[\n        0\n    ]\n\n    # Get indices of the top k results\n    top_k_indices = np.argsort(predictions)[::-1][:top_k_results]\n\n    # Get quantization parameters\n    scale, zero_point = output_details[0][\"quantization\"]\n\n    # Dequantize the output and apply softmax\n    dequantized_output = (\n        predictions.astype(np.float32) - zero_point\n    ) * scale\n    exp_output = np.exp(\n        dequantized_output - np.max(dequantized_output)\n    )\n    probabilities = exp_output / np.sum(exp_output)\n\n    print(\"\\n\\t[PREDICTION]        [Prob]\\n\")\n    for i in range(top_k_results):\n        print(\n            \"\\t{:20}: {}%\".format(\n                labels[top_k_indices[i]],\n                (int(probabilities[top_k_indices[i]] * 100)),\n            )\n        )\nAnd loading some images for testing, we have:\n \n\n\n\nLet‚Äôs get a TFLite model trained from scratch. For that, you can follow the Notebook:\nCNN to classify Cifar-10 dataset\nIn the notebook, we trained a model using the CIFAR10 dataset, which contains 60,000 images from 10 classes of CIFAR (airplane, automobile, bird, cat, deer, dog, frog, horse, ship, and truck). CIFAR has \\(32\\times 32\\) color images (3 color channels) where the objects are not centered and can have the object with a background, such as airplanes that might have a cloudy sky behind them! In short, small but real images.\nThe CNN trained model (cifar10_model.keras) had a size of 2.0MB. Using the TFLite Converter, the model cifar10.tflite became with 674MB (around 1/3 of the original size).\n \nOn the notebook Cifar 10 - Image Classification on a Raspi with TFLite (which can be run over the Raspi), we can follow the same steps we did with the mobilenet_v2_1.0_224_quant.tflite. Below are examples of images using the General Function for Image Classification on a Raspi-Zero, as shown in the last section.\n \n\n\n\nPicamera2, a Python library for interacting with Raspberry Pi‚Äôs camera, is based on the libcamera camera stack, and the Raspberry Pi foundation maintains it. The Picamera2 library is supported on all Raspberry Pi models, from the Pi Zero to the RPi 5. It is already installed system-wide on the Raspi, but we should make it accessible within the virtual environment.\n\nFirst, activate the virtual environment if it‚Äôs not already activated:\nsource ~/tflite/bin/activate\nNow, let‚Äôs create a .pth file in your virtual environment to add the system site-packages path:\necho \"/usr/lib/python3/dist-packages\" &gt; \\\n $VIRTUAL_ENV/lib/python3.11/\nsite-packages/system_site_packages.pth\n\nNote: If your Python version differs, replace python3.11 with the appropriate version.\n\nAfter creating this file, try importing picamera2 in Python:\npython3\n&gt;&gt;&gt; import picamera2\n&gt;&gt;&gt; print(picamera2.__file__)\n\nThe above code will show the file location of the picamera2 module itself, proving that the library can be accessed from the environment.\n/home/mjrovai/tflite/lib/python3.11/site-packages/\\\npicamera2/__init__.py\nYou can also list the available cameras in the system:\n&gt;&gt;&gt; print(Picamera2.global_camera_info())\nIn my case, with a USB installed, I got:\n \nNow that we‚Äôve confirmed picamera2 is working in the environment with an index 0, let‚Äôs try a simple Python script to capture an image from your USB camera:\nfrom picamera2 import Picamera2\nimport time\n\n# Initialize the camera\npicam2 = Picamera2()  # default is index 0\n\n# Configure the camera\nconfig = picam2.create_still_configuration(main={\"size\": (640, 480)})\npicam2.configure(config)\n\n# Start the camera\npicam2.start()\n\n# Wait for the camera to warm up\ntime.sleep(2)\n\n# Capture an image\npicam2.capture_file(\"usb_camera_image.jpg\")\nprint(\"Image captured and saved as 'usb_camera_image.jpg'\")\n\n# Stop the camera\npicam2.stop()\nUse the Nano text editor, the Jupyter Notebook, or any other editor. Save this as a Python script (e.g., capture_image.py) and run it. This should capture an image from your camera and save it as ‚Äúusb_camera_image.jpg‚Äù in the same directory as your script.\n \nIf the Jupyter is open, you can see the captured image on your computer. Otherwise, transfer the file from the Raspi to your computer.\n \n\nIf you are working with a Raspi-5 with a whole desktop, you can open the file directly on the device.",
    "crumbs": [
      "Raspberry Pi",
      "Image Classification"
    ]
  },
  {
    "objectID": "contents/raspi/image_classification/image_classification.html#sec-image-classification-image-classification-project-2617",
    "href": "contents/raspi/image_classification/image_classification.html#sec-image-classification-image-classification-project-2617",
    "title": "Image Classification",
    "section": "",
    "text": "Now, we will develop a complete Image Classification project using the Edge Impulse Studio. As we did with the Movilinet V2, the trained and converted TFLite model will be used for inference.\n\n\nThe first step in any ML project is to define its goal. In this case, it is to detect and classify two specific objects present in one image. For this project, we will use two small toys: a robot and a small Brazilian parrot (named Periquito). We will also collect images of a background where those two objects are absent.\n \n\n\n\nOnce we have defined our Machine Learning project goal, the next and most crucial step is collecting the dataset. We can use a phone for the image capture, but we will use the Raspi here. Let‚Äôs set up a simple web server on our Raspberry Pi to view the QVGA (320 x 240) captured images in a browser.\n\nFirst, let‚Äôs install Flask, a lightweight web framework for Python:\npip3 install flask\nLet‚Äôs create a new Python script combining image capture with a web server. We‚Äôll call it get_img_data.py:\n\n\nfrom flask import Flask, Response, render_template_string,\n                  request, redirect, url_for\nfrom picamera2 import Picamera2\nimport io\nimport threading\nimport time\nimport os\nimport signal\n\napp = Flask(__name__)\n\n# Global variables\nbase_dir = \"dataset\"\npicam2 = None\nframe = None\nframe_lock = threading.Lock()\ncapture_counts = {}\ncurrent_label = None\nshutdown_event = threading.Event()\n\ndef initialize_camera():\n    global picam2\n    picam2 = Picamera2()\n    config = picam2.create_preview_configuration(\n             main={\"size\": (320, 240)}\n    )\n    picam2.configure(config)\n    picam2.start()\n    time.sleep(2)  # Wait for camera to warm up\n\ndef get_frame():\n    global frame\n    while not shutdown_event.is_set():\n        stream = io.BytesIO()\n        picam2.capture_file(stream, format='jpeg')\n        with frame_lock:\n            frame = stream.getvalue()\n        time.sleep(0.1)  # Adjust as needed for smooth preview\n\ndef generate_frames():\n    while not shutdown_event.is_set():\n        with frame_lock:\n            if frame is not None:\n                yield (b'--frame\\r\\n'\n                       b'Content-Type: image/jpeg\\r\\n\\r\\n' +\n                                       frame + b'\\r\\n')\n        time.sleep(0.1)  # Adjust as needed for smooth streaming\n\ndef shutdown_server():\n    shutdown_event.set()\n    if picam2:\n        picam2.stop()\n    # Give some time for other threads to finish\n    time.sleep(2)\n    # Send SIGINT to the main process\n    os.kill(os.getpid(), signal.SIGINT)\n\n@app.route('/', methods=['GET', 'POST'])\ndef index():\n    global current_label\n    if request.method == 'POST':\n        current_label = request.form['label']\n        if current_label not in capture_counts:\n            capture_counts[current_label] = 0\n        os.makedirs(os.path.join(base_dir, current_label),\n                                 exist_ok=True)\n        return redirect(url_for('capture_page'))\n    return render_template_string('''\n        &lt;!DOCTYPE html&gt;\n        &lt;html&gt;\n        &lt;head&gt;\n            &lt;title&gt;Dataset Capture - Label Entry&lt;/title&gt;\n        &lt;/head&gt;\n        &lt;body&gt;\n            &lt;h1&gt;Enter Label for Dataset&lt;/h1&gt;\n            &lt;form method=\"post\"&gt;\n                &lt;input type=\"text\" name=\"label\" required&gt;\n                &lt;input type=\"submit\" value=\"Start Capture\"&gt;\n            &lt;/form&gt;\n        &lt;/body&gt;\n        &lt;/html&gt;\n    ''')\n\n@app.route('/capture')\ndef capture_page():\n    return render_template_string('''\n        &lt;!DOCTYPE html&gt;\n        &lt;html&gt;\n        &lt;head&gt;\n            &lt;title&gt;Dataset Capture&lt;/title&gt;\n            &lt;script&gt;\n                var shutdownInitiated = false;\n                function checkShutdown() {\n                    if (!shutdownInitiated) {\n                        fetch('/check_shutdown')\n                            .then(response =&gt; response.json())\n                            .then(data =&gt; {\n                                if (data.shutdown) {\n                                    shutdownInitiated = true;\n                                    document.getElementById(\n                                          'video-feed').src = '';\n                                    document.getElementById(\n                                          'shutdown-message')\n                                    .style.display = 'block';\n                                }\n                            });\n                    }\n                }\n                setInterval(checkShutdown, 1000); // Check\n                                                     every second\n            &lt;/script&gt;\n        &lt;/head&gt;\n        &lt;body&gt;\n            &lt;h1&gt;Dataset Capture&lt;/h1&gt;\n            &lt;p&gt;Current Label: {{ label }}&lt;/p&gt;\n            &lt;p&gt;Images captured for this label: {{ capture_count\n                                                  }}&lt;/p&gt;\n            &lt;img id=\"video-feed\" src=\"{{ url_for('video_feed')\n                                         }}\" width=\"640\"\n            height=\"480\" /&gt;\n            &lt;div id=\"shutdown-message\" style=\"display: none;\n                                              color: red;\"&gt;\n                Capture process has been stopped.\n                You can close this window.\n            &lt;/div&gt;\n            &lt;form action=\"/capture_image\" method=\"post\"&gt;\n                &lt;input type=\"submit\" value=\"Capture Image\"&gt;\n            &lt;/form&gt;\n            &lt;form action=\"/stop\" method=\"post\"&gt;\n                &lt;input type=\"submit\" value=\"Stop Capture\"\n                style=\"background-color: #ff6666;\"&gt;\n            &lt;/form&gt;\n            &lt;form action=\"/\" method=\"get\"&gt;\n                &lt;input type=\"submit\" value=\"Change Label\"\n                style=\"background-color: #ffff66;\"&gt;\n            &lt;/form&gt;\n        &lt;/body&gt;\n        &lt;/html&gt;\n    ''', label=current_label, capture_count=capture_counts.get(\n                                            current_label, 0))\n\n@app.route('/video_feed')\ndef video_feed():\n    return Response(generate_frames(),\n                    mimetype='multipart/x-mixed-replace;\n                    boundary=frame')\n\n@app.route('/capture_image', methods=['POST'])\ndef capture_image():\n    global capture_counts\n    if current_label and not shutdown_event.is_set():\n        capture_counts[current_label] += 1\n        timestamp = time.strftime(\"%Y%m%d-%H%M%S\")\n        filename = f\"image_{timestamp}.jpg\"\n        full_path = os.path.join(base_dir, current_label,\n                                 filename)\n\n        picam2.capture_file(full_path)\n\n    return redirect(url_for('capture_page'))\n\n@app.route('/stop', methods=['POST'])\ndef stop():\n    summary = render_template_string('''\n        &lt;!DOCTYPE html&gt;\n        &lt;html&gt;\n        &lt;head&gt;\n            &lt;title&gt;Dataset Capture - Stopped&lt;/title&gt;\n        &lt;/head&gt;\n        &lt;body&gt;\n            &lt;h1&gt;Dataset Capture Stopped&lt;/h1&gt;\n            &lt;p&gt;The capture process has been stopped.\n               You can close this window.&lt;/p&gt;\n            &lt;p&gt;Summary of captures:&lt;/p&gt;\n            &lt;ul&gt;\n            {% for label, count in capture_counts.items() %}\n                &lt;li&gt;{{ label }}: {{ count }} images&lt;/li&gt;\n            {% endfor %}\n            &lt;/ul&gt;\n        &lt;/body&gt;\n        &lt;/html&gt;\n    ''', capture_counts=capture_counts)\n\n    # Start a new thread to shutdown the server\n    threading.Thread(target=shutdown_server).start()\n\n    return summary\n\n@app.route('/check_shutdown')\ndef check_shutdown():\n    return {'shutdown': shutdown_event.is_set()}\n\nif __name__ == '__main__':\n    initialize_camera()\n    threading.Thread(target=get_frame, daemon=True).start()\n    app.run(host='0.0.0.0', port=5000, threaded=True)\n\n\nRun this script:\n\n    python3 get_img_data.py\n\nAccess the web interface:\n\nOn the Raspberry Pi itself (if you have a GUI): Open a web browser and go to http://localhost:5000\nFrom another device on the same network: Open a web browser and go to http://&lt;raspberry_pi_ip&gt;:5000 (Replace &lt;raspberry_pi_ip&gt; with your Raspberry Pi‚Äôs IP address). For example: http://192.168.4.210:5000/\n\n\nThis Python script creates a web-based interface for capturing and organizing image datasets using a Raspberry Pi and its camera. It‚Äôs handy for machine learning projects that require labeled image data.\n\n\n\nWeb Interface: Accessible from any device on the same network as the Raspberry Pi.\nLive Camera Preview: This shows a real-time feed from the camera.\nLabeling System: Allows users to input labels for different categories of images.\nOrganized Storage: Automatically saves images in label-specific subdirectories.\nPer-Label Counters: Keeps track of how many images are captured for each label.\nSummary Statistics: Provides a summary of captured images when stopping the capture process.\n\n\n\n\n\nFlask Web Application: Handles routing and serves the web interface.\nPicamera2 Integration: Controls the Raspberry Pi camera.\nThreaded Frame Capture: Ensures smooth live preview.\nFile Management: Organizes captured images into labeled directories.\n\n\n\n\n\ninitialize_camera(): Sets up the Picamera2 instance.\nget_frame(): Continuously captures frames for the live preview.\ngenerate_frames(): Yields frames for the live video feed.\nshutdown_server(): Sets the shutdown event, stops the camera, and shuts down the Flask server\nindex(): Handles the label input page.\ncapture_page(): Displays the main capture interface.\nvideo_feed(): Shows a live preview to position the camera\ncapture_image(): Saves an image with the current label.\nstop(): Stops the capture process and displays a summary.\n\n\n\n\n\nStart the script on your Raspberry Pi.\nAccess the web interface from a browser.\nEnter a label for the images you want to capture and press Start Capture.\n\n \n\nUse the live preview to position the camera.\nClick Capture Image to save images under the current label.\n\n \n\nChange labels as needed for different categories, selecting Change Label.\nClick Stop Capture when finished to see a summary.\n\n \n\n\n\n\nThe script uses threading to handle concurrent frame capture and web serving.\nImages are saved with timestamps in their filenames for uniqueness.\nThe web interface is responsive and can be accessed from mobile devices.\n\n\n\n\n\nAdjust image resolution in the initialize_camera() function. Here we used QVGA \\((320\\times 240)\\).\nModify the HTML templates for a different look and feel.\nAdd additional image processing or analysis steps in the capture_image() function.\n\n\n\n\nGet around 60 images from each category (periquito, robot and background). Try to capture different angles, backgrounds, and light conditions. On the Raspi, we will end with a folder named dataset, witch contains 3 sub-folders periquito, robot, and background. one for each class of images.\nYou can use Filezilla to transfer the created dataset to your main computer.",
    "crumbs": [
      "Raspberry Pi",
      "Image Classification"
    ]
  },
  {
    "objectID": "contents/raspi/image_classification/image_classification.html#sec-image-classification-training-model-edge-impulse-studio-bbff",
    "href": "contents/raspi/image_classification/image_classification.html#sec-image-classification-training-model-edge-impulse-studio-bbff",
    "title": "Image Classification",
    "section": "",
    "text": "We will use the Edge Impulse Studio to train our model. Go to the Edge Impulse Page, enter your account credentials, and create a new project:\n \n\nHere, you can clone a similar project: Raspi - Img Class.\n\n\n\nWe will walk through four main steps using the EI Studio (or Studio). These steps are crucial in preparing our model for use on the Raspi: Dataset, Impulse, Tests, and Deploy (on the Edge Device, in this case, the Raspi).\n\nRegarding the Dataset, it is essential to point out that our Original Dataset, captured with the Raspi, will be split into Training, Validation, and Test. The Test Set will be separated from the beginning and reserved for use only in the Test phase after training. The Validation Set will be used during training.\n\nOn Studio, follow the steps to upload the captured data:\n\nGo to the Data acquisition tab, and in the UPLOAD DATA section, upload the files from your computer in the chosen categories.\nLeave to the Studio the splitting of the original dataset into train and test and choose the label about\nRepeat the procedure for all three classes. At the end, you should see your ‚Äúraw data‚Äù in the Studio:\n\n \nThe Studio allows you to explore your data, showing a complete view of all the data in your project. You can clear, inspect, or change labels by clicking on individual data items. In our case, a straightforward project, the data seems OK.",
    "crumbs": [
      "Raspberry Pi",
      "Image Classification"
    ]
  },
  {
    "objectID": "contents/raspi/image_classification/image_classification.html#sec-image-classification-impulse-design-0ca2",
    "href": "contents/raspi/image_classification/image_classification.html#sec-image-classification-impulse-design-0ca2",
    "title": "Image Classification",
    "section": "",
    "text": "In this phase, we should define how to:\n\nPre-process our data, which consists of resizing the individual images and determining the color depth to use (be it RGB or Grayscale) and\nSpecify a Model. In this case, it will be the Transfer Learning (Images) to fine-tune a pre-trained MobileNet V2 image classification model on our data. This method performs well even with relatively small image datasets (around 180 images in our case).\n\nTransfer Learning with MobileNet offers a streamlined approach to model training, which is especially beneficial for resource-constrained environments and projects with limited labeled data. MobileNet, known for its lightweight architecture, is a pre-trained model that has already learned valuable features from a large dataset (ImageNet).\n \nBy leveraging these learned features, we can train a new model for your specific task with fewer data and computational resources and achieve competitive accuracy.\n \nThis approach significantly reduces training time and computational cost, making it ideal for quick prototyping and deployment on embedded devices where efficiency is paramount.\nGo to the Impulse Design Tab and create the impulse, defining an image size of \\(160\\times 160\\) and squashing them (squared form, without cropping). Select Image and Transfer Learning blocks. Save the Impulse.\n \n\n\nAll the input QVGA/RGB565 images will be converted to 76,800 features \\((160\\times 160\\times 3)\\).\n\n\n\n\n\nPress Save parameters and select Generate features in the next tab.\n\n\n\nMobileNet is a family of efficient convolutional neural networks designed for mobile and embedded vision applications. The key features of MobileNet are:\n\nLightweight: Optimized for mobile devices and embedded systems with limited computational resources.\nSpeed: Fast inference times, suitable for real-time applications.\nAccuracy: Maintains good accuracy despite its compact size.\n\nMobileNetV2, introduced in 2018, improves the original MobileNet architecture. Key features include:\n\nInverted Residuals: Inverted residual structures are used where shortcut connections are made between thin bottleneck layers.\nLinear Bottlenecks: Removes non-linearities in the narrow layers to prevent the destruction of information.\nDepth-wise Separable Convolutions: Continues to use this efficient operation from MobileNetV1.\n\nIn our project, we will do a Transfer Learning with the MobileNetV2 160x160 1.0, which means that the images used for training (and future inference) should have an input Size of \\(160\\times 160\\) pixels and a Width Multiplier of 1.0 (full width, not reduced). This configuration balances between model size, speed, and accuracy.\n\n\n\nAnother valuable deep learning technique is Data Augmentation. Data augmentation improves the accuracy of machine learning models by creating additional artificial data. A data augmentation system makes small, random changes to the training data during the training process (such as flipping, cropping, or rotating the images).\nLooking under the hood, here you can see how Edge Impulse implements a data Augmentation policy on your data:\n# Implements the data augmentation policy\ndef augment_image(image, label):\n    # Flips the image randomly\n    image = tf.image.random_flip_left_right(image)\n\n    # Increase the image size, then randomly crop it down to\n    # the original dimensions\n    resize_factor = random.uniform(1, 1.2)\n    new_height = math.floor(resize_factor * INPUT_SHAPE[0])\n    new_width = math.floor(resize_factor * INPUT_SHAPE[1])\n    image = tf.image.resize_with_crop_or_pad(\n        image, new_height, new_width\n    )\n    image = tf.image.random_crop(image, size=INPUT_SHAPE)\n\n    # Vary the brightness of the image\n    image = tf.image.random_brightness(image, max_delta=0.2)\n\n    return image, label\nExposure to these variations during training can help prevent your model from taking shortcuts by ‚Äúmemorizing‚Äù superficial clues in your training data, meaning it may better reflect the deep underlying patterns in your dataset.\nThe final dense layer of our model will have 0 neurons with a 10% dropout for overfitting prevention. Here is the Training result:\n \nThe result is excellent, with a reasonable 35 ms of latency (for a Raspi-4), which should result in around 30 fps (frames per second) during inference. A Raspi-Zero should be slower, and the Raspi-5, faster.\n\n\n\nIf faster inference is needed, we should train the model using smaller alphas (0.35, 0.5, and 0.75) or even reduce the image input size, trading with accuracy. However, reducing the input image size and decreasing the alpha (width multiplier) can speed up inference for MobileNet V2, but they have different trade-offs. Let‚Äôs compare:\n\nReducing Image Input Size:\n\nPros:\n\nSignificantly reduces the computational cost across all layers.\nDecreases memory usage.\nIt often provides a substantial speed boost.\n\nCons:\n\nIt may reduce the model‚Äôs ability to detect small features or fine details.\nIt can significantly impact accuracy, especially for tasks requiring fine-grained recognition.\n\n\nReducing Alpha (Width Multiplier):\n\nPros:\n\nReduces the number of parameters and computations in the model.\nMaintains the original input resolution, potentially preserving more detail.\nIt can provide a good balance between speed and accuracy.\n\nCons:\n\nIt may not speed up inference as dramatically as reducing input size.\nIt can reduce the model‚Äôs capacity to learn complex features.\n\nComparison:\n\nSpeed Impact:\n\nReducing input size often provides a more substantial speed boost because it reduces computations quadratically (halving both width and height reduces computations by about 75%).\nReducing alpha provides a more linear reduction in computations.\n\nAccuracy Impact:\n\nReducing input size can severely impact accuracy, especially when detecting small objects or fine details.\nReducing alpha tends to have a more gradual impact on accuracy.\n\nModel Architecture:\n\nChanging input size doesn‚Äôt alter the model‚Äôs architecture.\nChanging alpha modifies the model‚Äôs structure by reducing the number of channels in each layer.\n\n\nRecommendation:\n\nIf our application doesn‚Äôt require detecting tiny details and can tolerate some loss in accuracy, reducing the input size is often the most effective way to speed up inference.\nReducing alpha might be preferable if maintaining the ability to detect fine details is crucial or if you need a more balanced trade-off between speed and accuracy.\nFor best results, you might want to experiment with both:\n\nTry MobileNet V2 with input sizes like \\(160\\times 160\\) or \\(92\\times 92\\)\nExperiment with alpha values like 1.0, 0.75, 0.5 or 0.35.\n\nAlways benchmark the different configurations on your specific hardware and with your particular dataset to find the optimal balance for your use case.\n\n\nRemember, the best choice depends on your specific requirements for accuracy, speed, and the nature of the images you‚Äôre working with. It‚Äôs often worth experimenting with combinations to find the optimal configuration for your particular use case.\n\n\n\n\nNow, you should take the data set aside at the start of the project and run the trained model using it as input. Again, the result is excellent (92.22%).\n\n\n\nAs we did in the previous section, we can deploy the trained model as .tflite and use Raspi to run it using Python.\nOn the Dashboard tab, go to Transfer learning model (int8 quantized) and click on the download icon:\n \n\nLet‚Äôs also download the float32 version for comparison\n\nTransfer the model from your computer to the Raspi (./models), for example, using FileZilla. Also, capture some images for inference (./images).\nImport the needed libraries:\nimport time\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom PIL import Image\nimport tflite_runtime.interpreter as tflite\nDefine the paths and labels:\nimg_path = \"./images/robot.jpg\"\nmodel_path = \"./models/ei-raspi-img-class-int8-quantized-\\\n                model.tflite\"\nlabels = [\"background\", \"periquito\", \"robot\"]\n\nNote that the models trained on the Edge Impulse Studio will output values with index 0, 1, 2, etc., where the actual labels will follow an alphabetic order.\n\nLoad the model, allocate the tensors, and get the input and output tensor details:\n# Load the TFLite model\ninterpreter = tflite.Interpreter(model_path=model_path)\ninterpreter.allocate_tensors()\n\n# Get input and output tensors\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\nOne important difference to note is that the dtype of the input details of the model is now int8, which means that the input values go from ‚Äì128 to +127, while each pixel of our image goes from 0 to 255. This means that we should pre-process the image to match it. We can check here:\ninput_dtype = input_details[0][\"dtype\"]\ninput_dtype\nnumpy.int8\nSo, let‚Äôs open the image and show it:\nimg = Image.open(img_path)\nplt.figure(figsize=(4, 4))\nplt.imshow(img)\nplt.axis(\"off\")\nplt.show()\n \nAnd perform the pre-processing:\nscale, zero_point = input_details[0][\"quantization\"]\nimg = img.resize(\n    (input_details[0][\"shape\"][1], input_details[0][\"shape\"][2])\n)\nimg_array = np.array(img, dtype=np.float32) / 255.0\nimg_array = (\n    (img_array / scale + zero_point).clip(-128, 127).astype(np.int8)\n)\ninput_data = np.expand_dims(img_array, axis=0)\nChecking the input data, we can verify that the input tensor is compatible with what is expected by the model:\ninput_data.shape, input_data.dtype\n((1, 160, 160, 3), dtype('int8'))\nNow, it is time to perform the inference. Let‚Äôs also calculate the latency of the model:\n# Inference on Raspi-Zero\nstart_time = time.time()\ninterpreter.set_tensor(input_details[0][\"index\"], input_data)\ninterpreter.invoke()\nend_time = time.time()\ninference_time = (end_time - start_time) * 1000  # Convert\n# to milliseconds\nprint(\"Inference time: {:.1f}ms\".format(inference_time))\nThe model will take around 125ms to perform the inference in the Raspi-Zero, which is 3 to 4 times longer than a Raspi-5.\nNow, we can get the output labels and probabilities. It is also important to note that the model trained on the Edge Impulse Studio has a softmax in its output (different from the original Movilenet V2), and we should use the model‚Äôs raw output as the ‚Äúprobabilities.‚Äù\n# Obtain results and map them to the classes\npredictions = interpreter.get_tensor(output_details[0][\"index\"])[0]\n\n# Get indices of the top k results\ntop_k_results = 3\ntop_k_indices = np.argsort(predictions)[::-1][:top_k_results]\n\n# Get quantization parameters\nscale, zero_point = output_details[0][\"quantization\"]\n\n# Dequantize the output\ndequantized_output = (\n    predictions.astype(np.float32) - zero_point\n) * scale\nprobabilities = dequantized_output\n\nprint(\"\\n\\t[PREDICTION]        [Prob]\\n\")\nfor i in range(top_k_results):\n    print(\n        \"\\t{:20}: {:.2f}%\".format(\n            labels[top_k_indices[i]],\n            probabilities[top_k_indices[i]] * 100,\n        )\n    )\n \nLet‚Äôs modify the function created before so that we can handle different type of models:\n\ndef image_classification(\n    img_path, model_path, labels, top_k_results=3, apply_softmax=False\n):\n    # Load the image\n    img = Image.open(img_path)\n    plt.figure(figsize=(4, 4))\n    plt.imshow(img)\n    plt.axis(\"off\")\n\n    # Load the TFLite model\n    interpreter = tflite.Interpreter(model_path=model_path)\n    interpreter.allocate_tensors()\n\n    # Get input and output tensors\n    input_details = interpreter.get_input_details()\n    output_details = interpreter.get_output_details()\n\n    # Preprocess\n    img = img.resize(\n        (input_details[0][\"shape\"][1], input_details[0][\"shape\"][2])\n    )\n\n    input_dtype = input_details[0][\"dtype\"]\n\n    if input_dtype == np.uint8:\n        input_data = np.expand_dims(np.array(img), axis=0)\n    elif input_dtype == np.int8:\n        scale, zero_point = input_details[0][\"quantization\"]\n        img_array = np.array(img, dtype=np.float32) / 255.0\n        img_array = (\n            (img_array / scale + zero_point)\n            .clip(-128, 127)\n            .astype(np.int8)\n        )\n        input_data = np.expand_dims(img_array, axis=0)\n    else:  # float32\n        input_data = (\n            np.expand_dims(np.array(img, dtype=np.float32), axis=0)\n            / 255.0\n        )\n\n    # Inference on Raspi-Zero\n    start_time = time.time()\n    interpreter.set_tensor(input_details[0][\"index\"], input_data)\n    interpreter.invoke()\n    end_time = time.time()\n    inference_time = (\n        end_time - start_time\n    ) * 1000  # Convert to milliseconds\n\n    # Obtain results\n    predictions = interpreter.get_tensor(output_details[0][\"index\"])[\n        0\n    ]\n\n    # Get indices of the top k results\n    top_k_indices = np.argsort(predictions)[::-1][:top_k_results]\n\n    # Handle output based on type\n    output_dtype = output_details[0][\"dtype\"]\n    if output_dtype in [np.int8, np.uint8]:\n        # Dequantize the output\n        scale, zero_point = output_details[0][\"quantization\"]\n        predictions = (\n            predictions.astype(np.float32) - zero_point\n        ) * scale\n\n    if apply_softmax:\n        # Apply softmax\n        exp_preds = np.exp(predictions - np.max(predictions))\n        probabilities = exp_preds / np.sum(exp_preds)\n    else:\n        probabilities = predictions\n\n    print(\"\\n\\t[PREDICTION]        [Prob]\\n\")\n    for i in range(top_k_results):\n        print(\n            \"\\t{:20}: {:.1f}%\".format(\n                labels[top_k_indices[i]],\n                probabilities[top_k_indices[i]] * 100,\n            )\n        )\n    print(\"\\n\\tInference time: {:.1f}ms\".format(inference_time))\n\nAnd test it with different images and the int8 quantized model (160x160 alpha =1.0).\n \nLet‚Äôs download a smaller model, such as the one trained for the Nicla Vision Lab (int8 quantized model, 96x96, alpha = 0.1), as a test. We can use the same function:\n \nThe model lost some accuracy, but it is still OK once our model does not look for many details. Regarding latency, we are around ten times faster on the Raspi-Zero.",
    "crumbs": [
      "Raspberry Pi",
      "Image Classification"
    ]
  },
  {
    "objectID": "contents/raspi/image_classification/image_classification.html#sec-image-classification-live-image-classification-aa5d",
    "href": "contents/raspi/image_classification/image_classification.html#sec-image-classification-live-image-classification-aa5d",
    "title": "Image Classification",
    "section": "",
    "text": "Let‚Äôs develop an app to capture images with the USB camera in real time, showing its classification.\nUsing the nano on the terminal, save the code below, such as img_class_live_infer.py.\n\nfrom flask import Flask, Response, render_template_string,\n                  request, jsonify\nfrom picamera2 import Picamera2\nimport io\nimport threading\nimport time\nimport numpy as np\nfrom PIL import Image\nimport tflite_runtime.interpreter as tflite\nfrom queue import Queue\n\napp = Flask(__name__)\n\n# Global variables\npicam2 = None\nframe = None\nframe_lock = threading.Lock()\nis_classifying = False\nconfidence_threshold = 0.8\nmodel_path = \"./models/ei-raspi-img-class-int8-quantized-\\\n                model.tflite\"\nlabels = ['background', 'periquito', 'robot']\ninterpreter = None\nclassification_queue = Queue(maxsize=1)\n\ndef initialize_camera():\n    global picam2\n    picam2 = Picamera2()\n    config = picam2.create_preview_configuration(\n        main={\"size\": (320, 240)}\n    )\n    picam2.configure(config)\n    picam2.start()\n    time.sleep(2)  # Wait for camera to warm up\n\ndef get_frame():\n    global frame\n    while True:\n        stream = io.BytesIO()\n        picam2.capture_file(stream, format='jpeg')\n        with frame_lock:\n            frame = stream.getvalue()\n        time.sleep(0.1)  # Capture frames more frequently\n\ndef generate_frames():\n    while True:\n        with frame_lock:\n            if frame is not None:\n                yield (\n                   b'--frame\\r\\n'\n                   b'Content-Type: image/jpeg\\r\\n\\r\\n'\n                   + frame + b'\\r\\n'\n                )\n        time.sleep(0.1)\n\ndef load_model():\n    global interpreter\n    if interpreter is None:\n        interpreter = tflite.Interpreter(model_path=model_path)\n        interpreter.allocate_tensors()\n    return interpreter\n\ndef classify_image(img, interpreter):\n    input_details = interpreter.get_input_details()\n    output_details = interpreter.get_output_details()\n\n    img = img.resize((input_details[0]['shape'][1],\n                      input_details[0]['shape'][2]))\n    input_data = np.expand_dims(np.array(img), axis=0)\\\n                             .astype(input_details[0]['dtype'])\n\n    interpreter.set_tensor(input_details[0]['index'], input_data)\n    interpreter.invoke()\n\n    predictions = interpreter.get_tensor(output_details[0]\n                                         ['index'])[0]\n    # Handle output based on type\n    output_dtype = output_details[0]['dtype']\n    if output_dtype in [np.int8, np.uint8]:\n        # Dequantize the output\n        scale, zero_point = output_details[0]['quantization']\n        predictions = (predictions.astype(np.float32) -\n                       zero_point) * scale\n    return predictions\n\ndef classification_worker():\n    interpreter = load_model()\n    while True:\n        if is_classifying:\n            with frame_lock:\n                if frame is not None:\n                    img = Image.open(io.BytesIO(frame))\n            predictions = classify_image(img, interpreter)\n            max_prob = np.max(predictions)\n            if max_prob &gt;= confidence_threshold:\n                label = labels[np.argmax(predictions)]\n            else:\n                label = 'Uncertain'\n            classification_queue.put({\n                 'label': label,\n                 'probability': float(max_prob)\n            })\n        time.sleep(0.1)  # Adjust based on your needs\n\n@app.route('/')\ndef index():\n   return render_template_string('''\n      &lt;!DOCTYPE html&gt;\n      &lt;html&gt;\n      &lt;head&gt;\n          &lt;title&gt;Image Classification&lt;/title&gt;\n          &lt;script\n            src=\"https://code.jquery.com/jquery-3.6.0.min.js\"&gt;\n          &lt;/script&gt;\n          &lt;script&gt;\n              function startClassification() {\n                  $.post('/start');\n                  $('#startBtn').prop('disabled', true);\n                  $('#stopBtn').prop('disabled', false);\n              }\n              function stopClassification() {\n                  $.post('/stop');\n                  $('#startBtn').prop('disabled', false);\n                  $('#stopBtn').prop('disabled', true);\n              }\n              function updateConfidence() {\n                  var confidence = $('#confidence').val();\n                  $.post('/update_confidence',\n                         {confidence: confidence}\n                        );\n              }\n              function updateClassification() {\n                  $.get('/get_classification', function(data) {\n                    $('#classification').text(data.label + ': '\n                    + data.probability.toFixed(2));\n                  });\n              }\n              $(document).ready(function() {\n                  setInterval(updateClassification, 100);\n                  // Update every 100ms\n              });\n          &lt;/script&gt;\n      &lt;/head&gt;\n      &lt;body&gt;\n          &lt;h1&gt;Image Classification&lt;/h1&gt;\n          &lt;img src=\"{{ url_for('video_feed') }}\"\n               width=\"640\"\n               height=\"480\" /&gt;\n\n          &lt;br&gt;\n          &lt;button id=\"startBtn\"\n                  onclick=\"startClassification()\"&gt;\n            Start Classification\n          &lt;/button&gt;\n\n          &lt;button id=\"stopBtn\"\n                  onclick=\"stopClassification()\"\n                  disabled&gt;\n            Stop Classification\n          &lt;/button&gt;\n\n          &lt;br&gt;\n          &lt;label for=\"confidence\"&gt;Confidence Threshold:&lt;/label&gt;\n          &lt;input type=\"number\"\n                 id=\"confidence\"\n                 name=\"confidence\"\n                 min=\"0\" max=\"1\"\n                 step=\"0.1\"\n                 value=\"0.8\"\n                 onchange=\"updateConfidence()\" /&gt;\n\n          &lt;br&gt;\n          &lt;div id=\"classification\"&gt;\n             Waiting for classification...\n          &lt;/div&gt;\n\n      &lt;/body&gt;\n      &lt;/html&gt;\n   ''')\n\n@app.route('/video_feed')\ndef video_feed():\n    return Response(\n       generate_frames(),\n       mimetype='multipart/x-mixed-replace; boundary=frame'\n    )\n\n@app.route('/start', methods=['POST'])\ndef start_classification():\n    global is_classifying\n    is_classifying = True\n    return '', 204\n\n@app.route('/stop', methods=['POST'])\ndef stop_classification():\n    global is_classifying\n    is_classifying = False\n    return '', 204\n\n@app.route('/update_confidence', methods=['POST'])\ndef update_confidence():\n    global confidence_threshold\n    confidence_threshold = float(request.form['confidence'])\n    return '', 204\n\n@app.route('/get_classification')\ndef get_classification():\n    if not is_classifying:\n        return jsonify({'label': 'Not classifying',\n                       'probability': 0})\n    try:\n        result = classification_queue.get_nowait()\n    except Queue.Empty:\n        result = {'label': 'Processing', 'probability': 0}\n    return jsonify(result)\n\nif __name__ == '__main__':\n    initialize_camera()\n    threading.Thread(target=get_frame, daemon=True).start()\n    threading.Thread(target=classification_worker,\n                     daemon=True).start()\n    app.run(host='0.0.0.0', port=5000, threaded=True)\n\nOn the terminal, run:\npython3 img_class_live_infer.py\nAnd access the web interface:\n\nOn the Raspberry Pi itself (if you have a GUI): Open a web browser and go to http://localhost:5000\nFrom another device on the same network: Open a web browser and go to http://&lt;raspberry_pi_ip&gt;:5000 (Replace &lt;raspberry_pi_ip&gt; with your Raspberry Pi‚Äôs IP address). For example: http://192.168.4.210:5000/\n\nHere are some screenshots of the app running on an external desktop\n \nHere, you can see the app running on the YouTube:\n\nThe code creates a web application for real-time image classification using a Raspberry Pi, its camera module, and a TensorFlow Lite model. The application uses Flask to serve a web interface where is possible to view the camera feed and see live classification results.\n\n\n\nFlask Web Application: Serves the user interface and handles requests.\nPiCamera2: Captures images from the Raspberry Pi camera module.\nTensorFlow Lite: Runs the image classification model.\nThreading: Manages concurrent operations for smooth performance.\n\n\n\n\n\nLive camera feed display\nReal-time image classification\nAdjustable confidence threshold\nStart/Stop classification on demand\n\n\n\n\n\nImports and Setup:\n\nFlask for web application\nPiCamera2 for camera control\nTensorFlow Lite for inference\nThreading and Queue for concurrent operations\n\nGlobal Variables:\n\nCamera and frame management\nClassification control\nModel and label information\n\nCamera Functions:\n\ninitialize_camera(): Sets up the PiCamera2\nget_frame(): Continuously captures frames\ngenerate_frames(): Yields frames for the web feed\n\nModel Functions:\n\nload_model(): Loads the TFLite model\nclassify_image(): Performs inference on a single image\n\nClassification Worker:\n\nRuns in a separate thread\nContinuously classifies frames when active\nUpdates a queue with the latest results\n\nFlask Routes:\n\n/: Serves the main HTML page\n/video_feed: Streams the camera feed\n/start and /stop: Controls classification\n/update_confidence: Adjusts the confidence threshold\n/get_classification: Returns the latest classification result\n\nHTML Template:\n\nDisplays camera feed and classification results\nProvides controls for starting/stopping and adjusting settings\n\nMain Execution:\n\nInitializes camera and starts necessary threads\nRuns the Flask application\n\n\n\n\n\n\nConcurrent Operations: Using threads to handle camera capture and classification separately from the web server.\nReal-time Updates: Frequent updates to the classification results without page reloads.\nModel Reuse: Loading the TFLite model once and reusing it for efficiency.\nFlexible Configuration: Allowing users to adjust the confidence threshold on the fly.\n\n\n\n\n\nEnsure all dependencies are installed.\nRun the script on a Raspberry Pi with a camera module.\nAccess the web interface from a browser using the Raspberry Pi‚Äôs IP address.\nStart classification and adjust settings as needed.",
    "crumbs": [
      "Raspberry Pi",
      "Image Classification"
    ]
  },
  {
    "objectID": "contents/raspi/image_classification/image_classification.html#sec-image-classification-summary-084d",
    "href": "contents/raspi/image_classification/image_classification.html#sec-image-classification-summary-084d",
    "title": "Image Classification",
    "section": "",
    "text": "Image classification has emerged as a powerful and versatile application of machine learning, with significant implications for various fields, from healthcare to environmental monitoring. This chapter has demonstrated how to implement a robust image classification system on edge devices like the Raspi-Zero and Raspi-5, showcasing the potential for real-time, on-device intelligence.\nWe‚Äôve explored the entire pipeline of an image classification project, from data collection and model training using Edge Impulse Studio to deploying and running inferences on a Raspi. The process highlighted several key points:\n\nThe importance of proper data collection and preprocessing for training effective models.\nThe power of transfer learning, allowing us to leverage pre-trained models like MobileNet V2 for efficient training with limited data.\nThe trade-offs between model accuracy and inference speed, especially crucial for edge devices.\nThe implementation of real-time classification using a web-based interface, demonstrating practical applications.\n\nThe ability to run these models on edge devices like the Raspi opens up numerous possibilities for IoT applications, autonomous systems, and real-time monitoring solutions. It allows for reduced latency, improved privacy, and operation in environments with limited connectivity.\nAs we‚Äôve seen, even with the computational constraints of edge devices, it‚Äôs possible to achieve impressive results in terms of both accuracy and speed. The flexibility to adjust model parameters, such as input size and alpha values, allows for fine-tuning to meet specific project requirements.\nLooking forward, the field of edge AI and image classification continues to evolve rapidly. Advances in model compression techniques, hardware acceleration, and more efficient neural network architectures promise to further expand the capabilities of edge devices in computer vision tasks.\nThis project serves as a foundation for more complex computer vision applications and encourages further exploration into the exciting world of edge AI and IoT. Whether it‚Äôs for industrial automation, smart home applications, or environmental monitoring, the skills and concepts covered here provide a solid starting point for a wide range of innovative projects.",
    "crumbs": [
      "Raspberry Pi",
      "Image Classification"
    ]
  },
  {
    "objectID": "contents/raspi/image_classification/image_classification.html#sec-image-classification-resources-e552",
    "href": "contents/raspi/image_classification/image_classification.html#sec-image-classification-resources-e552",
    "title": "Image Classification",
    "section": "",
    "text": "Dataset Example\nSetup Test Notebook on a Raspi\nImage Classification Notebook on a Raspi\nCNN to classify Cifar-10 dataset at CoLab\nCifar 10 - Image Classification on a Raspi\nPython Scripts\nEdge Impulse Project",
    "crumbs": [
      "Raspberry Pi",
      "Image Classification"
    ]
  },
  {
    "objectID": "contents/shared/kws_feature_eng/kws_feature_eng.html",
    "href": "contents/shared/kws_feature_eng/kws_feature_eng.html",
    "title": "KWS Feature Engineering",
    "section": "",
    "text": "DALL¬∑E 3 Prompt: 1950s style cartoon scene set in an audio research room. Two scientists, one holding a magnifying glass and the other taking notes, examine large charts pinned to the wall. These charts depict FFT graphs and time curves related to audio data analysis. The room has a retro ambiance, with wooden tables, vintage lamps, and classic audio analysis tools.\n\n\n\n\nIn this hands-on tutorial, the emphasis is on the critical role that feature engineering plays in optimizing the performance of machine learning models applied to audio classification tasks, such as speech recognition. It is essential to be aware that the performance of any machine learning model relies heavily on the quality of features used, and we will deal with ‚Äúunder-the-hood‚Äù mechanics of feature extraction, mainly focusing on Mel-frequency Cepstral Coefficients (MFCCs), a cornerstone in the field of audio signal processing.\nMachine learning models, especially traditional algorithms, don‚Äôt understand audio waves. They understand numbers arranged in some meaningful way, i.e., features. These features encapsulate the characteristics of the audio signal, making it easier for models to distinguish between different sounds.\n\nThis tutorial will deal with generating features specifically for audio classification. This can be particularly interesting for applying machine learning to a variety of audio data, whether for speech recognition, music categorization, insect classification based on wingbeat sounds, or other sound analysis tasks\n\n\n\n\nThe most common TinyML application is Keyword Spotting (KWS), a subset of the broader field of speech recognition. While general speech recognition transcribes all spoken words into text, Keyword Spotting focuses on detecting specific ‚Äúkeywords‚Äù or ‚Äúwake words‚Äù in a continuous audio stream. The system is trained to recognize these keywords as predefined phrases or words, such as yes or no. In short, KWS is a specialized form of speech recognition with its own set of challenges and requirements.\nHere a typical KWS Process using MFCC Feature Converter:\n \n\n\n\nVoice Assistants: In devices like Amazon‚Äôs Alexa or Google Home, KWS is used to detect the wake word (‚ÄúAlexa‚Äù or ‚ÄúHey Google‚Äù) to activate the device.\nVoice-Activated Controls: In automotive or industrial settings, KWS can be used to initiate specific commands like ‚ÄúStart engine‚Äù or ‚ÄúTurn off lights.‚Äù\nSecurity Systems: Voice-activated security systems may use KWS to authenticate users based on a spoken passphrase.\nTelecommunication Services: Customer service lines may use KWS to route calls based on spoken keywords.\n\n\n\n\n\nComputational Efficiency: KWS is usually designed to be less computationally intensive than full speech recognition, as it only needs to recognize a small set of phrases.\nReal-time Processing: KWS often operates in real-time and is optimized for low-latency detection of keywords.\nResource Constraints: KWS models are often designed to be lightweight, so they can run on devices with limited computational resources, like microcontrollers or mobile phones.\nFocused Task: While general speech recognition models are trained to handle a broad range of vocabulary and accents, KWS models are fine-tuned to recognize specific keywords, often in noisy environments accurately.\n\n\n\n\n\nUnderstanding the basic properties of audio signals is crucial for effective feature extraction and, ultimately, for successfully applying machine learning algorithms in audio classification tasks. Audio signals are complex waveforms that capture fluctuations in air pressure over time. These signals can be characterized by several fundamental attributes: sampling rate, frequency, and amplitude.\n\nFrequency and Amplitude: Frequency refers to the number of oscillations a waveform undergoes per unit time and is also measured in Hz. In the context of audio signals, different frequencies correspond to different pitches. Amplitude, on the other hand, measures the magnitude of the oscillations and correlates with the loudness of the sound. Both frequency and amplitude are essential features that capture audio signals‚Äô tonal and rhythmic qualities.\nSampling Rate: The sampling rate, often denoted in Hertz (Hz), defines the number of samples taken per second when digitizing an analog signal. A higher sampling rate allows for a more accurate digital representation of the signal but also demands more computational resources for processing. Typical sampling rates include 44.1 kHz for CD-quality audio and 16 kHz or 8 kHz for speech recognition tasks. Understanding the trade-offs in selecting an appropriate sampling rate is essential for balancing accuracy and computational efficiency. In general, with TinyML projects, we work with 16 kHz. Although music tones can be heard at frequencies up to 20¬†kHz, voice maxes out at 8 kHz. Traditional telephone systems use an 8¬†kHz sampling frequency.\n\n\nFor an accurate representation of the signal, the sampling rate must be at least twice the highest frequency present in the signal.\n\n\nTime Domain vs.¬†Frequency Domain: Audio signals can be analyzed in the time and frequency domains. In the time domain, a signal is represented as a waveform where the amplitude is plotted against time. This representation helps to observe temporal features like onset and duration but the signal‚Äôs tonal characteristics are not well evidenced. Conversely, a frequency domain representation provides a view of the signal‚Äôs constituent frequencies and their respective amplitudes, typically obtained via a Fourier Transform. This is invaluable for tasks that require understanding the signal‚Äôs spectral content, such as identifying musical notes or speech phonemes (our case).\n\nThe image below shows the words YES and NO with typical representations in the Time (Raw Audio) and Frequency domains:\n \n\n\nWhile using raw audio data directly for machine learning tasks may seem tempting, this approach presents several challenges that make it less suitable for building robust and efficient models.\nUsing raw audio data for Keyword Spotting (KWS), for example, on TinyML devices poses challenges due to its high dimensionality (using a 16 kHz sampling rate), computational complexity for capturing temporal features, susceptibility to noise, and lack of semantically meaningful features, making feature extraction techniques like MFCCs a more practical choice for resource-constrained applications.\nHere are some additional details of the critical issues associated with using raw audio:\n\nHigh Dimensionality: Audio signals, especially those sampled at high rates, result in large amounts of data. For example, a 1-second audio clip sampled at 16 kHz will have 16,000 individual data points. High-dimensional data increases computational complexity, leading to longer training times and higher computational costs, making it impractical for resource-constrained environments. Furthermore, the wide dynamic range of audio signals requires a significant amount of bits per sample, while conveying little useful information.\nTemporal Dependencies: Raw audio signals have temporal structures that simple machine learning models may find hard to capture. While recurrent neural networks like LSTMs can model such dependencies, they are computationally intensive and tricky to train on tiny devices.\nNoise and Variability: Raw audio signals often contain background noise and other non-essential elements affecting model performance. Additionally, the same sound can have different characteristics based on various factors such as distance from the microphone, the orientation of the sound source, and acoustic properties of the environment, adding to the complexity of the data.\nLack of Semantic Meaning: Raw audio doesn‚Äôt inherently contain semantically meaningful features for classification tasks. Features like pitch, tempo, and spectral characteristics, which can be crucial for speech recognition, are not directly accessible from raw waveform data.\nSignal Redundancy: Audio signals often contain redundant information, with certain portions of the signal contributing little to no value to the task at hand. This redundancy can make learning inefficient and potentially lead to overfitting.\n\nFor these reasons, feature extraction techniques such as Mel-frequency Cepstral Coefficients (MFCCs), Mel-Frequency Energies (MFEs), and simple Spectrograms are commonly used to transform raw audio data into a more manageable and informative format. These features capture the essential characteristics of the audio signal while reducing dimensionality and noise, facilitating more effective machine learning.\n\n\n\n\n\n\nMel-frequency Cepstral Coefficients (MFCCs) are a set of features derived from the spectral content of an audio signal. They are based on human auditory perceptions and are commonly used to capture the phonetic characteristics of an audio signal. The MFCCs are computed through a multi-step process that includes pre-emphasis, framing, windowing, applying the Fast Fourier Transform (FFT) to convert the signal to the frequency domain, and finally, applying the Discrete Cosine Transform (DCT). The result is a compact representation of the original audio signal‚Äôs spectral characteristics.\nThe image below shows the words YES and NO in their MFCC representation:\n \n\nThis video explains the Mel Frequency Cepstral Coefficients (MFCC) and how to compute them.\n\n\n\n\nMFCCs are crucial for several reasons, particularly in the context of Keyword Spotting (KWS) and TinyML:\n\nDimensionality Reduction: MFCCs capture essential spectral characteristics of the audio signal while significantly reducing the dimensionality of the data, making it ideal for resource-constrained TinyML applications.\nRobustness: MFCCs are less susceptible to noise and variations in pitch and amplitude, providing a more stable and robust feature set for audio classification tasks.\nHuman Auditory System Modeling: The Mel scale in MFCCs approximates the human ear‚Äôs response to different frequencies, making them practical for speech recognition where human-like perception is desired.\nComputational Efficiency: The process of calculating MFCCs is computationally efficient, making it well-suited for real-time applications on hardware with limited computational resources.\n\nIn summary, MFCCs offer a balance of information richness and computational efficiency, making them popular for audio classification tasks, particularly in constrained environments like TinyML.\n\n\n\nThe computation of Mel-frequency Cepstral Coefficients (MFCCs) involves several key steps. Let‚Äôs walk through these, which are particularly important for Keyword Spotting (KWS) tasks on TinyML devices.\n\nPre-emphasis: The first step is pre-emphasis, which is applied to accentuate the high-frequency components of the audio signal and balance the frequency spectrum. This is achieved by applying a filter that amplifies the difference between consecutive samples. The formula for pre-emphasis is: \\(y(t)=x(t)-\\alpha x(t-1)\\), where \\(\\alpha\\) is the pre-emphasis factor, typically around 0.97.\nFraming: Audio signals are divided into short frames (the frame length), usually 20 to 40 milliseconds. This is based on the assumption that frequencies in a signal are stationary over a short period. Framing helps in analyzing the signal in such small time slots. The frame stride (or step) will displace one frame and the adjacent. Those steps could be sequential or overlapped.\nWindowing: Each frame is then windowed to minimize the discontinuities at the frame boundaries. A commonly used window function is the Hamming window. Windowing prepares the signal for a Fourier transform by minimizing the edge effects. The image below shows three frames (10, 20, and 30) and the time samples after windowing (note that the frame length and frame stride are 20 ms):\n\n \n\nFast Fourier Transform (FFT) The Fast Fourier Transform (FFT) is applied to each windowed frame to convert it from the time domain to the frequency domain. The FFT gives us a complex-valued representation that includes both magnitude and phase information. However, for MFCCs, only the magnitude is used to calculate the Power Spectrum. The power spectrum is the square of the magnitude spectrum and measures the energy present at each frequency component.\n\n\nThe power spectrum \\(P(f)\\) of a signal \\(x(t)\\) is defined as \\(P(f)=|X(f)|^2\\), where \\(X(f)\\) is the Fourier Transform of \\(x(t)\\). By squaring the magnitude of the Fourier Transform, we emphasize stronger frequencies over weaker ones, thereby capturing more relevant spectral characteristics of the audio signal. This is important in applications like audio classification, speech recognition, and Keyword Spotting (KWS), where the focus is on identifying distinct frequency patterns that characterize different classes of audio or phonemes in speech.\n\n \n\nMel Filter Banks: The frequency domain is then mapped to the Mel scale, which approximates the human ear‚Äôs response to different frequencies. The idea is to extract more features (more filter banks) in the lower frequencies and less in the high frequencies. Thus, it performs well on sounds distinguished by the human ear. Typically, 20 to 40 triangular filters extract the Mel-frequency energies. These energies are then log-transformed to convert multiplicative factors into additive ones, making them more suitable for further processing.\n\n \n\nDiscrete Cosine Transform (DCT): The last step is to apply the Discrete Cosine Transform (DCT) to the log Mel energies. The DCT helps to decorrelate the energies, effectively compressing the data and retaining only the most discriminative features. Usually, the first 12-13 DCT coefficients are retained, forming the final MFCC feature vector.\n\n \n\n\n\n\nLet‚Äôs apply what we discussed while working on an actual audio sample. Open the notebook on Google CoLab and extract the MLCC features on your audio samples: [Open In Colab]\n\n\n\nWhat Feature Extraction technique should we use?\nMel-frequency Cepstral Coefficients (MFCCs), Mel-Frequency Energies (MFEs), or Spectrogram are techniques for representing audio data, which are often helpful in different contexts.\nIn general, MFCCs are more focused on capturing the envelope of the power spectrum, which makes them less sensitive to fine-grained spectral details but more robust to noise. This is often desirable for speech-related tasks. On the other hand, spectrograms or MFEs preserve more detailed frequency information, which can be advantageous in tasks that require discrimination based on fine-grained spectral content.\n\n\n\nSpeech Recognition: MFCCs are excellent for identifying phonetic content in speech signals.\nSpeaker Identification: They can be used to distinguish between different speakers based on voice characteristics.\nEmotion Recognition: MFCCs can capture the nuanced variations in speech indicative of emotional states.\nKeyword Spotting: Especially in TinyML, where low computational complexity and small feature size are crucial.\n\n\n\n\n\nMusic Analysis: Spectrograms can capture harmonic and timbral structures in music, which is essential for tasks like genre classification, instrument recognition, or music transcription.\nEnvironmental Sound Classification: In recognizing non-speech, environmental sounds (e.g., rain, wind, traffic), the full spectrogram can provide more discriminative features.\nBirdsong Identification: The intricate details of bird calls are often better captured using spectrograms.\nBioacoustic Signal Processing: In applications like dolphin or bat call analysis, the fine-grained frequency information in a spectrogram can be essential.\nAudio Quality Assurance: Spectrograms are often used in professional audio analysis to identify unwanted noises, clicks, or other artifacts.\n\n\n\n\n\n\nAudio_Data_Analysis Colab Notebook",
    "crumbs": [
      "Shared Resources",
      "KWS Feature Engineering"
    ]
  },
  {
    "objectID": "contents/shared/kws_feature_eng/kws_feature_eng.html#sec-kws-feature-engineering-overview-afdf",
    "href": "contents/shared/kws_feature_eng/kws_feature_eng.html#sec-kws-feature-engineering-overview-afdf",
    "title": "KWS Feature Engineering",
    "section": "",
    "text": "In this hands-on tutorial, the emphasis is on the critical role that feature engineering plays in optimizing the performance of machine learning models applied to audio classification tasks, such as speech recognition. It is essential to be aware that the performance of any machine learning model relies heavily on the quality of features used, and we will deal with ‚Äúunder-the-hood‚Äù mechanics of feature extraction, mainly focusing on Mel-frequency Cepstral Coefficients (MFCCs), a cornerstone in the field of audio signal processing.\nMachine learning models, especially traditional algorithms, don‚Äôt understand audio waves. They understand numbers arranged in some meaningful way, i.e., features. These features encapsulate the characteristics of the audio signal, making it easier for models to distinguish between different sounds.\n\nThis tutorial will deal with generating features specifically for audio classification. This can be particularly interesting for applying machine learning to a variety of audio data, whether for speech recognition, music categorization, insect classification based on wingbeat sounds, or other sound analysis tasks",
    "crumbs": [
      "Shared Resources",
      "KWS Feature Engineering"
    ]
  },
  {
    "objectID": "contents/shared/kws_feature_eng/kws_feature_eng.html#sec-kws-feature-engineering-kws-218a",
    "href": "contents/shared/kws_feature_eng/kws_feature_eng.html#sec-kws-feature-engineering-kws-218a",
    "title": "KWS Feature Engineering",
    "section": "",
    "text": "The most common TinyML application is Keyword Spotting (KWS), a subset of the broader field of speech recognition. While general speech recognition transcribes all spoken words into text, Keyword Spotting focuses on detecting specific ‚Äúkeywords‚Äù or ‚Äúwake words‚Äù in a continuous audio stream. The system is trained to recognize these keywords as predefined phrases or words, such as yes or no. In short, KWS is a specialized form of speech recognition with its own set of challenges and requirements.\nHere a typical KWS Process using MFCC Feature Converter:\n \n\n\n\nVoice Assistants: In devices like Amazon‚Äôs Alexa or Google Home, KWS is used to detect the wake word (‚ÄúAlexa‚Äù or ‚ÄúHey Google‚Äù) to activate the device.\nVoice-Activated Controls: In automotive or industrial settings, KWS can be used to initiate specific commands like ‚ÄúStart engine‚Äù or ‚ÄúTurn off lights.‚Äù\nSecurity Systems: Voice-activated security systems may use KWS to authenticate users based on a spoken passphrase.\nTelecommunication Services: Customer service lines may use KWS to route calls based on spoken keywords.\n\n\n\n\n\nComputational Efficiency: KWS is usually designed to be less computationally intensive than full speech recognition, as it only needs to recognize a small set of phrases.\nReal-time Processing: KWS often operates in real-time and is optimized for low-latency detection of keywords.\nResource Constraints: KWS models are often designed to be lightweight, so they can run on devices with limited computational resources, like microcontrollers or mobile phones.\nFocused Task: While general speech recognition models are trained to handle a broad range of vocabulary and accents, KWS models are fine-tuned to recognize specific keywords, often in noisy environments accurately.",
    "crumbs": [
      "Shared Resources",
      "KWS Feature Engineering"
    ]
  },
  {
    "objectID": "contents/shared/kws_feature_eng/kws_feature_eng.html#sec-kws-feature-engineering-overview-audio-signals-aeff",
    "href": "contents/shared/kws_feature_eng/kws_feature_eng.html#sec-kws-feature-engineering-overview-audio-signals-aeff",
    "title": "KWS Feature Engineering",
    "section": "",
    "text": "Understanding the basic properties of audio signals is crucial for effective feature extraction and, ultimately, for successfully applying machine learning algorithms in audio classification tasks. Audio signals are complex waveforms that capture fluctuations in air pressure over time. These signals can be characterized by several fundamental attributes: sampling rate, frequency, and amplitude.\n\nFrequency and Amplitude: Frequency refers to the number of oscillations a waveform undergoes per unit time and is also measured in Hz. In the context of audio signals, different frequencies correspond to different pitches. Amplitude, on the other hand, measures the magnitude of the oscillations and correlates with the loudness of the sound. Both frequency and amplitude are essential features that capture audio signals‚Äô tonal and rhythmic qualities.\nSampling Rate: The sampling rate, often denoted in Hertz (Hz), defines the number of samples taken per second when digitizing an analog signal. A higher sampling rate allows for a more accurate digital representation of the signal but also demands more computational resources for processing. Typical sampling rates include 44.1 kHz for CD-quality audio and 16 kHz or 8 kHz for speech recognition tasks. Understanding the trade-offs in selecting an appropriate sampling rate is essential for balancing accuracy and computational efficiency. In general, with TinyML projects, we work with 16 kHz. Although music tones can be heard at frequencies up to 20¬†kHz, voice maxes out at 8 kHz. Traditional telephone systems use an 8¬†kHz sampling frequency.\n\n\nFor an accurate representation of the signal, the sampling rate must be at least twice the highest frequency present in the signal.\n\n\nTime Domain vs.¬†Frequency Domain: Audio signals can be analyzed in the time and frequency domains. In the time domain, a signal is represented as a waveform where the amplitude is plotted against time. This representation helps to observe temporal features like onset and duration but the signal‚Äôs tonal characteristics are not well evidenced. Conversely, a frequency domain representation provides a view of the signal‚Äôs constituent frequencies and their respective amplitudes, typically obtained via a Fourier Transform. This is invaluable for tasks that require understanding the signal‚Äôs spectral content, such as identifying musical notes or speech phonemes (our case).\n\nThe image below shows the words YES and NO with typical representations in the Time (Raw Audio) and Frequency domains:\n \n\n\nWhile using raw audio data directly for machine learning tasks may seem tempting, this approach presents several challenges that make it less suitable for building robust and efficient models.\nUsing raw audio data for Keyword Spotting (KWS), for example, on TinyML devices poses challenges due to its high dimensionality (using a 16 kHz sampling rate), computational complexity for capturing temporal features, susceptibility to noise, and lack of semantically meaningful features, making feature extraction techniques like MFCCs a more practical choice for resource-constrained applications.\nHere are some additional details of the critical issues associated with using raw audio:\n\nHigh Dimensionality: Audio signals, especially those sampled at high rates, result in large amounts of data. For example, a 1-second audio clip sampled at 16 kHz will have 16,000 individual data points. High-dimensional data increases computational complexity, leading to longer training times and higher computational costs, making it impractical for resource-constrained environments. Furthermore, the wide dynamic range of audio signals requires a significant amount of bits per sample, while conveying little useful information.\nTemporal Dependencies: Raw audio signals have temporal structures that simple machine learning models may find hard to capture. While recurrent neural networks like LSTMs can model such dependencies, they are computationally intensive and tricky to train on tiny devices.\nNoise and Variability: Raw audio signals often contain background noise and other non-essential elements affecting model performance. Additionally, the same sound can have different characteristics based on various factors such as distance from the microphone, the orientation of the sound source, and acoustic properties of the environment, adding to the complexity of the data.\nLack of Semantic Meaning: Raw audio doesn‚Äôt inherently contain semantically meaningful features for classification tasks. Features like pitch, tempo, and spectral characteristics, which can be crucial for speech recognition, are not directly accessible from raw waveform data.\nSignal Redundancy: Audio signals often contain redundant information, with certain portions of the signal contributing little to no value to the task at hand. This redundancy can make learning inefficient and potentially lead to overfitting.\n\nFor these reasons, feature extraction techniques such as Mel-frequency Cepstral Coefficients (MFCCs), Mel-Frequency Energies (MFEs), and simple Spectrograms are commonly used to transform raw audio data into a more manageable and informative format. These features capture the essential characteristics of the audio signal while reducing dimensionality and noise, facilitating more effective machine learning.",
    "crumbs": [
      "Shared Resources",
      "KWS Feature Engineering"
    ]
  },
  {
    "objectID": "contents/shared/kws_feature_eng/kws_feature_eng.html#sec-kws-feature-engineering-overview-mfccs-3f21",
    "href": "contents/shared/kws_feature_eng/kws_feature_eng.html#sec-kws-feature-engineering-overview-mfccs-3f21",
    "title": "KWS Feature Engineering",
    "section": "",
    "text": "Mel-frequency Cepstral Coefficients (MFCCs) are a set of features derived from the spectral content of an audio signal. They are based on human auditory perceptions and are commonly used to capture the phonetic characteristics of an audio signal. The MFCCs are computed through a multi-step process that includes pre-emphasis, framing, windowing, applying the Fast Fourier Transform (FFT) to convert the signal to the frequency domain, and finally, applying the Discrete Cosine Transform (DCT). The result is a compact representation of the original audio signal‚Äôs spectral characteristics.\nThe image below shows the words YES and NO in their MFCC representation:\n \n\nThis video explains the Mel Frequency Cepstral Coefficients (MFCC) and how to compute them.\n\n\n\n\nMFCCs are crucial for several reasons, particularly in the context of Keyword Spotting (KWS) and TinyML:\n\nDimensionality Reduction: MFCCs capture essential spectral characteristics of the audio signal while significantly reducing the dimensionality of the data, making it ideal for resource-constrained TinyML applications.\nRobustness: MFCCs are less susceptible to noise and variations in pitch and amplitude, providing a more stable and robust feature set for audio classification tasks.\nHuman Auditory System Modeling: The Mel scale in MFCCs approximates the human ear‚Äôs response to different frequencies, making them practical for speech recognition where human-like perception is desired.\nComputational Efficiency: The process of calculating MFCCs is computationally efficient, making it well-suited for real-time applications on hardware with limited computational resources.\n\nIn summary, MFCCs offer a balance of information richness and computational efficiency, making them popular for audio classification tasks, particularly in constrained environments like TinyML.\n\n\n\nThe computation of Mel-frequency Cepstral Coefficients (MFCCs) involves several key steps. Let‚Äôs walk through these, which are particularly important for Keyword Spotting (KWS) tasks on TinyML devices.\n\nPre-emphasis: The first step is pre-emphasis, which is applied to accentuate the high-frequency components of the audio signal and balance the frequency spectrum. This is achieved by applying a filter that amplifies the difference between consecutive samples. The formula for pre-emphasis is: \\(y(t)=x(t)-\\alpha x(t-1)\\), where \\(\\alpha\\) is the pre-emphasis factor, typically around 0.97.\nFraming: Audio signals are divided into short frames (the frame length), usually 20 to 40 milliseconds. This is based on the assumption that frequencies in a signal are stationary over a short period. Framing helps in analyzing the signal in such small time slots. The frame stride (or step) will displace one frame and the adjacent. Those steps could be sequential or overlapped.\nWindowing: Each frame is then windowed to minimize the discontinuities at the frame boundaries. A commonly used window function is the Hamming window. Windowing prepares the signal for a Fourier transform by minimizing the edge effects. The image below shows three frames (10, 20, and 30) and the time samples after windowing (note that the frame length and frame stride are 20 ms):\n\n \n\nFast Fourier Transform (FFT) The Fast Fourier Transform (FFT) is applied to each windowed frame to convert it from the time domain to the frequency domain. The FFT gives us a complex-valued representation that includes both magnitude and phase information. However, for MFCCs, only the magnitude is used to calculate the Power Spectrum. The power spectrum is the square of the magnitude spectrum and measures the energy present at each frequency component.\n\n\nThe power spectrum \\(P(f)\\) of a signal \\(x(t)\\) is defined as \\(P(f)=|X(f)|^2\\), where \\(X(f)\\) is the Fourier Transform of \\(x(t)\\). By squaring the magnitude of the Fourier Transform, we emphasize stronger frequencies over weaker ones, thereby capturing more relevant spectral characteristics of the audio signal. This is important in applications like audio classification, speech recognition, and Keyword Spotting (KWS), where the focus is on identifying distinct frequency patterns that characterize different classes of audio or phonemes in speech.\n\n \n\nMel Filter Banks: The frequency domain is then mapped to the Mel scale, which approximates the human ear‚Äôs response to different frequencies. The idea is to extract more features (more filter banks) in the lower frequencies and less in the high frequencies. Thus, it performs well on sounds distinguished by the human ear. Typically, 20 to 40 triangular filters extract the Mel-frequency energies. These energies are then log-transformed to convert multiplicative factors into additive ones, making them more suitable for further processing.\n\n \n\nDiscrete Cosine Transform (DCT): The last step is to apply the Discrete Cosine Transform (DCT) to the log Mel energies. The DCT helps to decorrelate the energies, effectively compressing the data and retaining only the most discriminative features. Usually, the first 12-13 DCT coefficients are retained, forming the final MFCC feature vector.",
    "crumbs": [
      "Shared Resources",
      "KWS Feature Engineering"
    ]
  },
  {
    "objectID": "contents/shared/kws_feature_eng/kws_feature_eng.html#sec-kws-feature-engineering-handson-using-python-af55",
    "href": "contents/shared/kws_feature_eng/kws_feature_eng.html#sec-kws-feature-engineering-handson-using-python-af55",
    "title": "KWS Feature Engineering",
    "section": "",
    "text": "Let‚Äôs apply what we discussed while working on an actual audio sample. Open the notebook on Google CoLab and extract the MLCC features on your audio samples: [Open In Colab]",
    "crumbs": [
      "Shared Resources",
      "KWS Feature Engineering"
    ]
  },
  {
    "objectID": "contents/shared/kws_feature_eng/kws_feature_eng.html#sec-kws-feature-engineering-summary-2713",
    "href": "contents/shared/kws_feature_eng/kws_feature_eng.html#sec-kws-feature-engineering-summary-2713",
    "title": "KWS Feature Engineering",
    "section": "",
    "text": "What Feature Extraction technique should we use?\nMel-frequency Cepstral Coefficients (MFCCs), Mel-Frequency Energies (MFEs), or Spectrogram are techniques for representing audio data, which are often helpful in different contexts.\nIn general, MFCCs are more focused on capturing the envelope of the power spectrum, which makes them less sensitive to fine-grained spectral details but more robust to noise. This is often desirable for speech-related tasks. On the other hand, spectrograms or MFEs preserve more detailed frequency information, which can be advantageous in tasks that require discrimination based on fine-grained spectral content.\n\n\n\nSpeech Recognition: MFCCs are excellent for identifying phonetic content in speech signals.\nSpeaker Identification: They can be used to distinguish between different speakers based on voice characteristics.\nEmotion Recognition: MFCCs can capture the nuanced variations in speech indicative of emotional states.\nKeyword Spotting: Especially in TinyML, where low computational complexity and small feature size are crucial.\n\n\n\n\n\nMusic Analysis: Spectrograms can capture harmonic and timbral structures in music, which is essential for tasks like genre classification, instrument recognition, or music transcription.\nEnvironmental Sound Classification: In recognizing non-speech, environmental sounds (e.g., rain, wind, traffic), the full spectrogram can provide more discriminative features.\nBirdsong Identification: The intricate details of bird calls are often better captured using spectrograms.\nBioacoustic Signal Processing: In applications like dolphin or bat call analysis, the fine-grained frequency information in a spectrogram can be essential.\nAudio Quality Assurance: Spectrograms are often used in professional audio analysis to identify unwanted noises, clicks, or other artifacts.",
    "crumbs": [
      "Shared Resources",
      "KWS Feature Engineering"
    ]
  },
  {
    "objectID": "contents/shared/kws_feature_eng/kws_feature_eng.html#sec-kws-feature-engineering-resources-9922",
    "href": "contents/shared/kws_feature_eng/kws_feature_eng.html#sec-kws-feature-engineering-resources-9922",
    "title": "KWS Feature Engineering",
    "section": "",
    "text": "Audio_Data_Analysis Colab Notebook",
    "crumbs": [
      "Shared Resources",
      "KWS Feature Engineering"
    ]
  },
  {
    "objectID": "contents/shared/dsp_spectral_features_block/dsp_spectral_features_block.html",
    "href": "contents/shared/dsp_spectral_features_block/dsp_spectral_features_block.html",
    "title": "DSP Spectral Features",
    "section": "",
    "text": "DALL¬∑E 3 Prompt: 1950s style cartoon illustration of a Latin male and female scientist in a vibration research room. The man is using a calculus ruler to examine ancient circuitry. The woman is at a computer with complex vibration graphs. The wooden table has boards with sensors, prominently an accelerometer. A classic, rounded-back computer shows the Arduino IDE with code for LED pin assignments and machine learning algorithms for movement detection. The Serial Monitor displays FFT, classification, wavelets, and DSPs. Vintage lamps, tools, and charts with FFT and Wavelets graphs complete the scene.\n\n\n\n\nTinyML projects related to motion (or vibration) involve data from IMUs (usually accelerometers and Gyroscopes). These time-series type datasets should be preprocessed before inputting them into a Machine Learning model training, which is a challenging area for embedded machine learning. Still, Edge Impulse helps overcome this complexity with its digital signal processing (DSP) preprocessing step and, more specifically, the Spectral Features Block for Inertial sensors.\nBut how does it work under the hood? Let‚Äôs dig into it.\n\n\n\nExtracting features from a dataset captured with inertial sensors, such as accelerometers, involves processing and analyzing the raw data. Accelerometers measure the acceleration of an object along one or more axes (typically three, denoted as X, Y, and Z). These measurements can be used to understand various aspects of the object‚Äôs motion, such as movement patterns and vibrations. Here‚Äôs a high-level overview of the process:\nData collection: First, we need to gather data from the accelerometers. Depending on the application, data may be collected at different sampling rates. It‚Äôs essential to ensure that the sampling rate is high enough to capture the relevant dynamics of the studied motion (the sampling rate should be at least double the maximum relevant frequency present in the signal).\nData preprocessing: Raw accelerometer data can be noisy and contain errors or irrelevant information. Preprocessing steps, such as filtering and normalization, can help clean and standardize the data, making it more suitable for feature extraction.\n\nThe Studio does not perform normalization or standardization, so sometimes, when working with Sensor Fusion, it could be necessary to perform this step before uploading data to the Studio. This is particularly crucial in sensor fusion projects, as seen in this tutorial, Sensor Data Fusion with Spresense and CommonSense.\n\nSegmentation: Depending on the nature of the data and the application, dividing the data into smaller segments or windows may be necessary. This can help focus on specific events or activities within the dataset, making feature extraction more manageable and meaningful. The window size and overlap (window span) choice depend on the application and the frequency of the events of interest. As a rule of thumb, we should try to capture a couple of ‚Äúdata cycles.‚Äù\nFeature extraction: Once the data is preprocessed and segmented, you can extract features that describe the motion‚Äôs characteristics. Some typical features extracted from accelerometer data include:\n\nTime-domain features describe the data‚Äôs statistical properties within each segment, such as mean, median, standard deviation, skewness, kurtosis, and zero-crossing rate.\nFrequency-domain features are obtained by transforming the data into the frequency domain using techniques like the Fast Fourier Transform (FFT). Some typical frequency-domain features include the power spectrum, spectral energy, dominant frequencies (amplitude and frequency), and spectral entropy.\nTime-frequency domain features combine the time and frequency domain information, such as the Short-Time Fourier Transform (STFT) or the Discrete Wavelet Transform (DWT). They can provide a more detailed understanding of how the signal‚Äôs frequency content changes over time.\n\nIn many cases, the number of extracted features can be large, which may lead to overfitting or increased computational complexity. Feature selection techniques, such as mutual information, correlation-based methods, or principal component analysis (PCA), can help identify the most relevant features for a given application and reduce the dimensionality of the dataset. The Studio can help with such feature-relevant calculations.\nLet‚Äôs explore in more detail a typical TinyML Motion Classification project covered in this series of Hands-Ons.\n\n\n\n \nIn the hands-on project, Motion Classification and Anomaly Detection, we simulated mechanical stresses in transport, where our problem was to classify four classes of movement:\n\nMaritime (pallets in boats)\nTerrestrial (pallets in a Truck or Train)\nLift (pallets being handled by Fork-Lift)\nIdle (pallets in Storage houses)\n\nThe accelerometers provided the data on the pallet (or container).\n \nBelow is one sample (raw data) of 10 seconds, captured with a sampling frequency of 50 Hz:\n \n\nThe result is similar when this analysis is done over another dataset with the same principle, using a different sampling frequency, 62.5¬†Hz instead of 50 Hz.\n\n\n\n\nThe raw data captured by the accelerometer (a ‚Äútime series‚Äù data) should be converted to ‚Äútabular data‚Äù using one of the typical Feature Extraction methods described in the last section.\nWe should segment the data using a sliding window over the sample data for feature extraction. The project captured accelerometer data every 10 seconds with a sample rate of 62.5 Hz. A 2-second window captures 375 data points (3 axis \\(\\times\\) 2 seconds \\(\\times\\) 62.5 samples). The window is slid every 80 ms, creating a larger dataset where each instance has 375 ‚Äúraw features.‚Äù\n \nOn the Studio, the previous version (V1) of the Spectral Analysis Block extracted as time-domain features only the RMS, and for the frequency-domain, the peaks and frequency (using FFT) and the power characteristics (PSD) of the signal over time resulting in a fixed tabular dataset of 33 features (11 per each axis),\n \nThose 33 features were the Input tensor of a Neural Network Classifier.\nIn 2022, Edge Impulse released version 2 of the Spectral Analysis block, which we will explore here.\n\n\nIn Version 2, Time Domain Statistical features per axis/channel are:\n\nRMS\nSkewness\nKurtosis\n\nAnd the Frequency Domain Spectral features per axis/channel are:\n\nSpectral Power\nSkewness (in the next version)\nKurtosis (in the next version)\n\nIn this link, we can have more details about the feature extraction.\n\nClone the public project. You can also follow the explanation, playing with the code using my Google CoLab Notebook: Edge Impulse Spectral Analysis Block Notebook.\n\nStart importing the libraries:\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport math\nfrom scipy.stats import skew, kurtosis\nfrom scipy import signal\nfrom scipy.signal import welch\nfrom scipy.stats import entropy\nfrom sklearn import preprocessing\nimport pywt\n\nplt.rcParams['figure.figsize'] = (12, 6)\nplt.rcParams['lines.linewidth'] = 3\nFrom the studied project, let‚Äôs choose a data sample from accelerometers as below:\n\nWindow size of 2 seconds: [2,000] ms\nSample frequency: [62.5] Hz\nWe will choose the [None] filter (for simplicity) and a\nFFT length: [16].\n\nf =  62.5 # Hertz\nwind_sec = 2 # seconds\nFFT_Length = 16\naxis = ['accX', 'accY', 'accZ']\nn_sensors = len(axis)\n \nSelecting the Raw Features on the Studio Spectral Analysis tab, we can copy all 375 data points of a particular 2-second window to the clipboard.\n \nPaste the data points to a new variable data:\ndata = [\n    -5.6330,  0.2376,  9.8701,\n    -5.9442,  0.4830,  9.8701,\n    -5.4217, ...\n]\nNo_raw_features = len(data)\nN = int(No_raw_features/n_sensors)\nThe total raw features are 375, but we will work with each axis individually, where \\(N= 125\\) (number of samples per axis).\nWe aim to understand how Edge Impulse gets the processed features.\n \nSo, you should also past the processed features on a variable (to compare the calculated features in Python with the ones provided by the Studio) :\nfeatures = [\n    2.7322, -0.0978, -0.3813,\n    2.3980, 3.8924, 24.6841,\n    9.6303, ...\n]\nN_feat = len(features)\nN_feat_axis = int(N_feat/n_sensors)\nThe total number of processed features is 39, which means 13 features/axis.\nLooking at those 13 features closely, we will find 3 for the time domain (RMS, Skewness, and Kurtosis):\n\n[rms] [skew] [kurtosis]\n\nand 10 for the frequency domain (we will return to this later).\n\n[spectral skew][spectral kurtosis][Spectral Power 1] ... [Spectral Power 8]\n\nSplitting raw data per sensor\nThe data has samples from all axes; let‚Äôs split and plot them separately:\ndef plot_data(sensors, axis, title):\n    [plt.plot(x, label=y) for x,y in zip(sensors, axis)]\n    plt.legend(loc='lower right')\n    plt.title(title)\n    plt.xlabel('#Sample')\n    plt.ylabel('Value')\n    plt.box(False)\n    plt.grid()\n    plt.show()\n\naccX = data[0::3]\naccY = data[1::3]\naccZ = data[2::3]\nsensors = [accX, accY, accZ]\nplot_data(sensors, axis, 'Raw Features')\n \nSubtracting the mean\nNext, we should subtract the mean from the data. Subtracting the mean from a data set is a common data pre-processing step in statistics and machine learning. The purpose of subtracting the mean from the data is to center the data around zero. This is important because it can reveal patterns and relationships that might be hidden if the data is not centered.\nHere are some specific reasons why subtracting the mean can be helpful:\n\nIt simplifies analysis: By centering the data, the mean becomes zero, making some calculations simpler and easier to interpret.\nIt removes bias: If the data is biased, subtracting the mean can remove it and allow for a more accurate analysis.\nIt can reveal patterns: Centering the data can help uncover patterns that might be hidden if the data is not centered. For example, centering the data can help you identify trends over time if you analyze a time series dataset.\nIt can improve performance: In some machine learning algorithms, centering the data can improve performance by reducing the influence of outliers and making the data more easily comparable. Overall, subtracting the mean is a simple but powerful technique that can be used to improve the analysis and interpretation of data.\n\ndtmean = [\n    (sum(x) / len(x))\n    for x in sensors\n]\n\n[\n    print('mean_' + x + ' =', round(y, 4))\n    for x, y in zip(axis, dtmean)\n][0]\n\naccX = [(x - dtmean[0]) for x in accX]\naccY = [(x - dtmean[1]) for x in accY]\naccZ = [(x - dtmean[2]) for x in accZ]\nsensors = [accX, accY, accZ]\n\nplot_data(sensors, axis, 'Raw Features - Subtract the Mean')\n \n\n\n\n\nRMS Calculation\nThe RMS value of a set of values (or a continuous-time waveform) is the square root of the arithmetic mean of the squares of the values or the square of the function that defines the continuous waveform. In physics, the RMS value of an electrical current is defined as the ‚Äúvalue of the direct current that dissipates the same power in a resistor.‚Äù\nIn the case of a set of \\(n\\) values \\({ùë•_1, ùë•_2, \\ldots, ùë•_ùëõ}\\), the RMS is: \\[\nx_{\\mathrm{RMS}} = \\sqrt{\\frac{1}{n} \\left( x_1^2 + x_2^2 + \\cdots + x_n^2 \\right)}\n\\]\n\n\nNOTE that the RMS value is different for the original raw data, and after subtracting the mean\n\n# Using numpy and standardized data (subtracting mean)\nrms = [np.sqrt(np.mean(np.square(x))) for x in sensors]\nWe can compare the calculated RMS values here with the ones presented by Edge Impulse:\n[print('rms_'+x+'= ', round(y, 4)) for x,y in zip(axis, rms)][0]\nprint(\"\\nCompare with Edge Impulse result features\")\nprint(features[0:N_feat:N_feat_axis])\nrms_accX=  2.7322\nrms_accY=  0.7833\nrms_accZ=  0.1383\nCompared with Edge Impulse result features:\n[2.7322, 0.7833, 0.1383]\nSkewness and kurtosis calculation\nIn statistics, skewness and kurtosis are two ways to measure the shape of a distribution.\nHere, we can see the sensor values distribution:\nfig, axes = plt.subplots(nrows=1, ncols=3, figsize=(13, 4))\nsns.kdeplot(accX, fill=True, ax=axes[0])\nsns.kdeplot(accY, fill=True, ax=axes[1])\nsns.kdeplot(accZ, fill=True, ax=axes[2])\naxes[0].set_title('accX')\naxes[1].set_title('accY')\naxes[2].set_title('accZ')\nplt.suptitle('IMU Sensors distribution', fontsize=16, y=1.02)\nplt.show()\n \nSkewness is a measure of the asymmetry of a distribution. This value can be positive or negative.\n \n\nA negative skew indicates that the tail is on the left side of the distribution, which extends towards more negative values.\nA positive skew indicates that the tail is on the right side of the distribution, which extends towards more positive values.\nA zero value indicates no skewness in the distribution at all, meaning the distribution is perfectly symmetrical.\n\nskew = [skew(x, bias=False) for x in sensors]\n[print('skew_'+x+'= ', round(y, 4))\n  for x,y in zip(axis, skew)][0]\nprint(\"\\nCompare with Edge Impulse result features\")\nfeatures[1:N_feat:N_feat_axis]\nskew_accX=  -0.099\nskew_accY=  0.1756\nskew_accZ=  6.9463\nCompared with Edge Impulse result features:\n[-0.0978, 0.1735, 6.8629]\nKurtosis is a measure of whether or not a distribution is heavy-tailed or light-tailed relative to a normal distribution.\n \n\nThe kurtosis of a normal distribution is zero.\nIf a given distribution has a negative kurtosis, it is said to be playkurtic, which means it tends to produce fewer and less extreme outliers than the normal distribution.\nIf a given distribution has a positive kurtosis , it is said to be leptokurtic, which means it tends to produce more outliers than the normal distribution.\n\nkurt = [kurtosis(x, bias=False) for x in sensors]\n[print('kurt_'+x+'= ', round(y, 4))\n  for x,y in zip(axis, kurt)][0]\nprint(\"\\nCompare with Edge Impulse result features\")\nfeatures[2:N_feat:N_feat_axis]\nkurt_accX=  -0.3475\nkurt_accY=  1.2673\nkurt_accZ=  68.1123\nCompared with Edge Impulse result features:\n[-0.3813, 1.1696, 65.3726]\n\n\n\nThe filtered signal is passed to the Spectral power section, which computes the FFT to generate the spectral features.\nSince the sampled window is usually larger than the FFT size, the window will be broken into frames (or ‚Äúsub-windows‚Äù), and the FFT is calculated over each frame.\nFFT length - The FFT size. This determines the number of FFT bins and the resolution of frequency peaks that can be separated. A low number means more signals will average together in the same FFT bin, but it also reduces the number of features and model size. A high number will separate more signals into separate bins, generating a larger model.\n\nThe total number of Spectral Power features will vary depending on how you set the filter and FFT parameters. With No filtering, the number of features is 1/2 of the FFT Length.\n\nSpectral Power - Welch‚Äôs method\nWe should use Welch‚Äôs method to split the signal on the frequency domain in bins and calculate the power spectrum for each bin. This method divides the signal into overlapping segments, applies a window function to each segment, computes the periodogram of each segment using DFT, and averages them to obtain a smoother estimate of the power spectrum.\n# Function used by Edge Impulse instead of scipy.signal.welch().\ndef welch_max_hold(fx, sampling_freq, nfft, n_overlap):\n    n_overlap = int(n_overlap)\n    spec_powers = [0 for _ in range(nfft//2+1)]\n    ix = 0\n    while ix &lt;= len(fx):\n        # Slicing truncates if end_idx &gt; len,\n        # and rfft will auto-zero pad\n        fft_out = np.abs(np.fft.rfft(fx[ix:ix+nfft], nfft))\n        spec_powers = np.maximum(spec_powers, fft_out**2/nfft)\n        ix = ix + (nfft-n_overlap)\n    return np.fft.rfftfreq(nfft, 1/sampling_freq), spec_powers\nApplying the above function to 3 signals:\nfax,Pax = welch_max_hold(accX, fs, FFT_Length, 0)\nfay,Pay = welch_max_hold(accY, fs, FFT_Length, 0)\nfaz,Paz = welch_max_hold(accZ, fs, FFT_Length, 0)\nspecs = [Pax, Pay, Paz ]\nWe can plot the Power Spectrum P(f):\nplt.plot(fax,Pax, label='accX')\nplt.plot(fay,Pay, label='accY')\nplt.plot(faz,Paz, label='accZ')\nplt.legend(loc='upper right')\nplt.xlabel('Frequency (Hz)')\n#plt.ylabel('PSD [V**2/Hz]')\nplt.ylabel('Power')\nplt.title('Power spectrum P(f) using Welch's method')\nplt.grid()\nplt.box(False)\nplt.show()\n \nBesides the Power Spectrum, we can also include the skewness and kurtosis of the features in the frequency domain (should be available on a new version):\nspec_skew = [skew(x, bias=False) for x in specs]\nspec_kurtosis = [kurtosis(x, bias=False) for x in specs]\nLet‚Äôs now list all Spectral features per axis and compare them with EI:\nprint(\"EI Processed Spectral features (accX): \")\nprint(features[3:N_feat_axis][0:])\nprint(\"\\nCalculated features:\")\nprint (round(spec_skew[0],4))\nprint (round(spec_kurtosis[0],4))\n[print(round(x, 4)) for x in Pax[1:]][0]\nEI Processed Spectral features (accX):\n2.398, 3.8924, 24.6841, 9.6303, 8.4867, 7.7793, 2.9963, 5.6242, 3.4198, 4.2735\nCalculated features:\n2.9069 8.5569 24.6844 9.6304 8.4865 7.7794 2.9964 5.6242 3.4198 4.2736\nprint(\"EI Processed Spectral features (accY): \")\nprint(features[16:26][0:]) # 13: 3+N_feat_axis;\n                           # 26 = 2x N_feat_axis\nprint(\"\\nCalculated features:\")\nprint (round(spec_skew[1],4))\nprint (round(spec_kurtosis[1],4))\n[print(round(x, 4)) for x in Pay[1:]][0]\nEI Processed Spectral features (accY):\n0.9426, -0.8039, 5.429, 0.999, 1.0315, 0.9459, 1.8117, 0.9088, 1.3302, 3.112\nCalculated features:\n1.1426 -0.3886 5.4289 0.999 1.0315 0.9458 1.8116 0.9088 1.3301 3.1121\nprint(\"EI Processed Spectral features (accZ): \")\nprint(features[29:][0:]) #29: 3+(2*N_feat_axis);\nprint(\"\\nCalculated features:\")\nprint (round(spec_skew[2],4))\nprint (round(spec_kurtosis[2],4))\n[print(round(x, 4)) for x in Paz[1:]][0]\nEI Processed Spectral features (accZ):\n0.3117, -1.3812, 0.0606, 0.057, 0.0567, 0.0976, 0.194, 0.2574, 0.2083, 0.166\nCalculated features:\n0.3781 -1.4874 0.0606 0.057 0.0567 0.0976 0.194 0.2574 0.2083 0.166\n\n\n\n\n\nWavelet is a powerful technique for analyzing signals with transient features or abrupt changes, such as spikes or edges, which are difficult to interpret with traditional Fourier-based methods.\nWavelet transforms work by breaking down a signal into different frequency components and analyzing them individually. The transformation is achieved by convolving the signal with a wavelet function, a small waveform centered at a specific time and frequency. This process effectively decomposes the signal into different frequency bands, each of which can be analyzed separately.\nOne of the critical benefits of wavelet transforms is that they allow for time-frequency analysis, which means that they can reveal the frequency content of a signal as it changes over time. This makes them particularly useful for analyzing non-stationary signals, which vary over time.\nWavelets have many practical applications, including signal and image compression, denoising, feature extraction, and image processing.\nLet‚Äôs select Wavelet on the Spectral Features block in the same project:\n\nType: Wavelet\nWavelet Decomposition Level: 1\nWavelet: bior1.3\n\n \nThe Wavelet Function\nwavelet_name='bior1.3'\nnum_layer = 1\n\nwavelet = pywt.Wavelet(wavelet_name)\n[phi_d,psi_d,phi_r,psi_r,x] = wavelet.wavefun(level=5)\nplt.plot(x, psi_d, color='red')\nplt.title('Wavelet Function')\nplt.ylabel('Value')\nplt.xlabel('Time')\nplt.grid()\nplt.box(False)\nplt.show()\n \nAs we did before, let‚Äôs copy and past the Processed Features:\n \nfeatures = [\n    3.6251, 0.0615, 0.0615,\n    -7.3517, -2.7641, 2.8462,\n    5.0924, ...\n]\nN_feat = len(features)\nN_feat_axis = int(N_feat/n_sensors)\nEdge Impulse computes the Discrete Wavelet Transform (DWT) for each one of the Wavelet Decomposition levels selected. After that, the features will be extracted.\nIn the case of Wavelets, the extracted features are basic statistical values, crossing values, and entropy. There are, in total, 14 features per layer as below:\n\n[11] Statistical Features: n5, n25, n75, n95, mean, median, standard deviation (std), variance (var) root mean square (rms), kurtosis, and skewness (skew).\n[2] Crossing Features: Zero crossing rate (zcross) and mean crossing rate (mcross) are the times that the signal passes through the baseline \\((y = 0)\\) and the average level (y = u) per unit of time, respectively\n[1] Complexity Feature: Entropy is a characteristic measure of the complexity of the signal\n\nAll the above 14 values are calculated for each Layer (including L0, the original signal)\n\nThe total number of features varies depending on how you set the filter and the number of layers. For example, with [None] filtering and Level[1], the number of features per axis will be \\(14\\times 2\\) (L0 and L1) = 28. For the three axes, we will have a total of 84 features.\n\n\n\n\nWavelet analysis decomposes the signal (accX, accY, and accZ) into different frequency components using a set of filters, which separate these components into low-frequency (slowly varying parts of the signal containing long-term patterns), such as accX_l1, accY_l1, accZ_l1 and, high-frequency (rapidly varying parts of the signal containing short-term patterns) components, such as accX_d1, accY_d1, accZ_d1, permitting the extraction of features for further analysis or classification.\nOnly the low-frequency components (approximation coefficients, or cA) will be used. In this example, we assume only one level (Single-level Discrete Wavelet Transform), where the function will return a tuple. With a multilevel decomposition, the ‚ÄúMultilevel 1D Discrete Wavelet Transform‚Äù, the result will be a list (for detail, please see: Discrete Wavelet Transform (DWT) )\n(accX_l1, accX_d1) = pywt.dwt(accX, wavelet_name)\n(accY_l1, accY_d1) = pywt.dwt(accY, wavelet_name)\n(accZ_l1, accZ_d1) = pywt.dwt(accZ, wavelet_name)\nsensors_l1 = [accX_l1, accY_l1, accZ_l1]\n\n# Plot power spectrum versus frequency\nplt.plot(accX_l1, label='accX')\nplt.plot(accY_l1, label='accY')\nplt.plot(accZ_l1, label='accZ')\nplt.legend(loc='lower right')\nplt.xlabel('Time')\nplt.ylabel('Value')\nplt.title('Wavelet Approximation')\nplt.grid()\nplt.box(False)\nplt.show()\n \n\n\n\nLet‚Äôs start with the basic statistical features. Note that we apply the function for both the original signals and the resultant cAs from the DWT:\ndef calculate_statistics(signal):\n    n5 = np.percentile(signal, 5)\n    n25 = np.percentile(signal, 25)\n    n75 = np.percentile(signal, 75)\n    n95 = np.percentile(signal, 95)\n    median = np.percentile(signal, 50)\n    mean = np.mean(signal)\n    std = np.std(signal)\n    var = np.var(signal)\n    rms = np.sqrt(np.mean(np.square(signal)))\n    return [n5, n25, n75, n95, median, mean, std, var, rms]\n\nstat_feat_l0 = [calculate_statistics(x) for x in sensors]\nstat_feat_l1 = [calculate_statistics(x) for x in sensors_l1]\nThe Skelness and Kurtosis:\nskew_l0 = [skew(x, bias=False) for x in sensors]\nskew_l1 = [skew(x, bias=False) for x in sensors_l1]\nkurtosis_l0 = [kurtosis(x, bias=False) for x in sensors]\nkurtosis_l1 = [kurtosis(x, bias=False) for x in sensors_l1]\nZero crossing (zcross) is the number of times the wavelet coefficient crosses the zero axis. It can be used to measure the signal‚Äôs frequency content since high-frequency signals tend to have more zero crossings than low-frequency signals.\nMean crossing (mcross), on the other hand, is the number of times the wavelet coefficient crosses the mean of the signal. It can be used to measure the amplitude since high-amplitude signals tend to have more mean crossings than low-amplitude signals.\ndef getZeroCrossingRate(arr):\n    my_array = np.array(arr)\n    zcross = float(\n        \"{:.2f}\".format(\n          (((my_array[:-1] * my_array[1:]) &lt; 0).sum()) / len(arr)\n        )\n    )\n    return zcross\n\ndef getMeanCrossingRate(arr):\n    mcross = getZeroCrossingRate(np.array(arr) - np.mean(arr))\n    return mcross\n\ndef calculate_crossings(list):\n    zcross=[]\n    mcross=[]\n    for i in range(len(list)):\n        zcross_i = getZeroCrossingRate(list[i])\n        zcross.append(zcross_i)\n        mcross_i = getMeanCrossingRate(list[i])\n        mcross.append(mcross_i)\n    return zcross, mcross\n\ncross_l0 = calculate_crossings(sensors)\ncross_l1 = calculate_crossings(sensors_l1)\nIn wavelet analysis, entropy refers to the degree of disorder or randomness in the distribution of wavelet coefficients. Here, we used Shannon entropy, which measures a signal‚Äôs uncertainty or randomness. It is calculated as the negative sum of the probabilities of the different possible outcomes of the signal multiplied by their base 2 logarithm. In the context of wavelet analysis, Shannon entropy can be used to measure the complexity of the signal, with higher values indicating greater complexity.\ndef calculate_entropy(signal, base=None):\n    value, counts = np.unique(signal, return_counts=True)\n    return entropy(counts, base=base)\n\nentropy_l0 = [calculate_entropy(x) for x in sensors]\nentropy_l1 = [calculate_entropy(x) for x in sensors_l1]\nLet‚Äôs now list all the wavelet features and create a list by layers.\nL1_features_names = [\n    \"L1-n5\", \"L1-n25\", \"L1-n75\", \"L1-n95\", \"L1-median\",\n    \"L1-mean\", \"L1-std\", \"L1-var\", \"L1-rms\", \"L1-skew\",\n    \"L1-Kurtosis\", \"L1-zcross\", \"L1-mcross\", \"L1-entropy\"\n]\n\nL0_features_names = [\n    \"L0-n5\", \"L0-n25\", \"L0-n75\", \"L0-n95\", \"L0-median\",\n    \"L0-mean\", \"L0-std\", \"L0-var\", \"L0-rms\", \"L0-skew\",\n    \"L0-Kurtosis\", \"L0-zcross\", \"L0-mcross\", \"L0-entropy\"\n]\n\nall_feat_l0 = []\nfor i in range(len(axis)):\n    feat_l0 = (\n        stat_feat_l0[i]\n        + [skew_l0[i]]\n        + [kurtosis_l0[i]]\n        + [cross_l0[0][i]]\n        + [cross_l0[1][i]]\n        + [entropy_l0[i]]\n    )\n    [print(axis[i] + ' +x+= ', round(y, 4))\n       for x, y in zip(LO_features_names, feat_l0)][0]\n    all_feat_l0.append(feat_l0)\n\nall_feat_l0 = [\n    item\n    for sublist in all_feat_l0\n    for item in sublist\n]\nprint(f\"\\nAll L0 Features = {len(all_feat_l0)}\")\n\nall_feat_l1 = []\nfor i in range(len(axis)):\n    feat_l1 = (\n        stat_feat_l1[i]\n        + [skew_l1[i]]\n        + [kurtosis_l1[i]]\n        + [cross_l1[0][i]]\n        + [cross_l1[1][i]]\n        + [entropy_l1[i]]\n    )\n    [print(axis[i]+' '+x+'= ', round(y, 4))\n       for x,y in zip(L1_features_names, feat_l1)][0]\n    all_feat_l1.append(feat_l1)\n\nall_feat_l1 = [\n    item\n    for sublist in all_feat_l1\n    for item in sublist\n]\nprint(f\"\\nAll L1 Features = {len(all_feat_l1)}\")\n \n\n\n\n\nEdge Impulse Studio is a powerful online platform that can handle the pre-processing task for us. Still, given our engineering perspective, we want to understand what is happening under the hood. This knowledge will help us find the best options and hyper-parameters for tuning our projects.\nDaniel Situnayake wrote in his blog: ‚ÄúRaw sensor data is highly dimensional and noisy. Digital signal processing algorithms help us sift the signal from the noise. DSP is an essential part of embedded engineering, and many edge processors have on-board acceleration for DSP. As an ML engineer, learning basic DSP gives you superpowers for handling high-frequency time series data in your models.‚Äù I recommend you read Dan‚Äôs excellent post in its totality: nn to cpp: What you need to know about porting deep learning models to the edge.",
    "crumbs": [
      "Shared Resources",
      "DSP Spectral Features"
    ]
  },
  {
    "objectID": "contents/shared/dsp_spectral_features_block/dsp_spectral_features_block.html#sec-dsp-spectral-features-overview-a7be",
    "href": "contents/shared/dsp_spectral_features_block/dsp_spectral_features_block.html#sec-dsp-spectral-features-overview-a7be",
    "title": "DSP Spectral Features",
    "section": "",
    "text": "TinyML projects related to motion (or vibration) involve data from IMUs (usually accelerometers and Gyroscopes). These time-series type datasets should be preprocessed before inputting them into a Machine Learning model training, which is a challenging area for embedded machine learning. Still, Edge Impulse helps overcome this complexity with its digital signal processing (DSP) preprocessing step and, more specifically, the Spectral Features Block for Inertial sensors.\nBut how does it work under the hood? Let‚Äôs dig into it.",
    "crumbs": [
      "Shared Resources",
      "DSP Spectral Features"
    ]
  },
  {
    "objectID": "contents/shared/dsp_spectral_features_block/dsp_spectral_features_block.html#sec-dsp-spectral-features-extracting-features-review-b821",
    "href": "contents/shared/dsp_spectral_features_block/dsp_spectral_features_block.html#sec-dsp-spectral-features-extracting-features-review-b821",
    "title": "DSP Spectral Features",
    "section": "",
    "text": "Extracting features from a dataset captured with inertial sensors, such as accelerometers, involves processing and analyzing the raw data. Accelerometers measure the acceleration of an object along one or more axes (typically three, denoted as X, Y, and Z). These measurements can be used to understand various aspects of the object‚Äôs motion, such as movement patterns and vibrations. Here‚Äôs a high-level overview of the process:\nData collection: First, we need to gather data from the accelerometers. Depending on the application, data may be collected at different sampling rates. It‚Äôs essential to ensure that the sampling rate is high enough to capture the relevant dynamics of the studied motion (the sampling rate should be at least double the maximum relevant frequency present in the signal).\nData preprocessing: Raw accelerometer data can be noisy and contain errors or irrelevant information. Preprocessing steps, such as filtering and normalization, can help clean and standardize the data, making it more suitable for feature extraction.\n\nThe Studio does not perform normalization or standardization, so sometimes, when working with Sensor Fusion, it could be necessary to perform this step before uploading data to the Studio. This is particularly crucial in sensor fusion projects, as seen in this tutorial, Sensor Data Fusion with Spresense and CommonSense.\n\nSegmentation: Depending on the nature of the data and the application, dividing the data into smaller segments or windows may be necessary. This can help focus on specific events or activities within the dataset, making feature extraction more manageable and meaningful. The window size and overlap (window span) choice depend on the application and the frequency of the events of interest. As a rule of thumb, we should try to capture a couple of ‚Äúdata cycles.‚Äù\nFeature extraction: Once the data is preprocessed and segmented, you can extract features that describe the motion‚Äôs characteristics. Some typical features extracted from accelerometer data include:\n\nTime-domain features describe the data‚Äôs statistical properties within each segment, such as mean, median, standard deviation, skewness, kurtosis, and zero-crossing rate.\nFrequency-domain features are obtained by transforming the data into the frequency domain using techniques like the Fast Fourier Transform (FFT). Some typical frequency-domain features include the power spectrum, spectral energy, dominant frequencies (amplitude and frequency), and spectral entropy.\nTime-frequency domain features combine the time and frequency domain information, such as the Short-Time Fourier Transform (STFT) or the Discrete Wavelet Transform (DWT). They can provide a more detailed understanding of how the signal‚Äôs frequency content changes over time.\n\nIn many cases, the number of extracted features can be large, which may lead to overfitting or increased computational complexity. Feature selection techniques, such as mutual information, correlation-based methods, or principal component analysis (PCA), can help identify the most relevant features for a given application and reduce the dimensionality of the dataset. The Studio can help with such feature-relevant calculations.\nLet‚Äôs explore in more detail a typical TinyML Motion Classification project covered in this series of Hands-Ons.",
    "crumbs": [
      "Shared Resources",
      "DSP Spectral Features"
    ]
  },
  {
    "objectID": "contents/shared/dsp_spectral_features_block/dsp_spectral_features_block.html#sec-dsp-spectral-features-tinyml-motion-classification-project-7321",
    "href": "contents/shared/dsp_spectral_features_block/dsp_spectral_features_block.html#sec-dsp-spectral-features-tinyml-motion-classification-project-7321",
    "title": "DSP Spectral Features",
    "section": "",
    "text": "In the hands-on project, Motion Classification and Anomaly Detection, we simulated mechanical stresses in transport, where our problem was to classify four classes of movement:\n\nMaritime (pallets in boats)\nTerrestrial (pallets in a Truck or Train)\nLift (pallets being handled by Fork-Lift)\nIdle (pallets in Storage houses)\n\nThe accelerometers provided the data on the pallet (or container).\n \nBelow is one sample (raw data) of 10 seconds, captured with a sampling frequency of 50 Hz:\n \n\nThe result is similar when this analysis is done over another dataset with the same principle, using a different sampling frequency, 62.5¬†Hz instead of 50 Hz.",
    "crumbs": [
      "Shared Resources",
      "DSP Spectral Features"
    ]
  },
  {
    "objectID": "contents/shared/dsp_spectral_features_block/dsp_spectral_features_block.html#sec-dsp-spectral-features-data-preprocessing-78cc",
    "href": "contents/shared/dsp_spectral_features_block/dsp_spectral_features_block.html#sec-dsp-spectral-features-data-preprocessing-78cc",
    "title": "DSP Spectral Features",
    "section": "",
    "text": "The raw data captured by the accelerometer (a ‚Äútime series‚Äù data) should be converted to ‚Äútabular data‚Äù using one of the typical Feature Extraction methods described in the last section.\nWe should segment the data using a sliding window over the sample data for feature extraction. The project captured accelerometer data every 10 seconds with a sample rate of 62.5 Hz. A 2-second window captures 375 data points (3 axis \\(\\times\\) 2 seconds \\(\\times\\) 62.5 samples). The window is slid every 80 ms, creating a larger dataset where each instance has 375 ‚Äúraw features.‚Äù\n \nOn the Studio, the previous version (V1) of the Spectral Analysis Block extracted as time-domain features only the RMS, and for the frequency-domain, the peaks and frequency (using FFT) and the power characteristics (PSD) of the signal over time resulting in a fixed tabular dataset of 33 features (11 per each axis),\n \nThose 33 features were the Input tensor of a Neural Network Classifier.\nIn 2022, Edge Impulse released version 2 of the Spectral Analysis block, which we will explore here.\n\n\nIn Version 2, Time Domain Statistical features per axis/channel are:\n\nRMS\nSkewness\nKurtosis\n\nAnd the Frequency Domain Spectral features per axis/channel are:\n\nSpectral Power\nSkewness (in the next version)\nKurtosis (in the next version)\n\nIn this link, we can have more details about the feature extraction.\n\nClone the public project. You can also follow the explanation, playing with the code using my Google CoLab Notebook: Edge Impulse Spectral Analysis Block Notebook.\n\nStart importing the libraries:\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport math\nfrom scipy.stats import skew, kurtosis\nfrom scipy import signal\nfrom scipy.signal import welch\nfrom scipy.stats import entropy\nfrom sklearn import preprocessing\nimport pywt\n\nplt.rcParams['figure.figsize'] = (12, 6)\nplt.rcParams['lines.linewidth'] = 3\nFrom the studied project, let‚Äôs choose a data sample from accelerometers as below:\n\nWindow size of 2 seconds: [2,000] ms\nSample frequency: [62.5] Hz\nWe will choose the [None] filter (for simplicity) and a\nFFT length: [16].\n\nf =  62.5 # Hertz\nwind_sec = 2 # seconds\nFFT_Length = 16\naxis = ['accX', 'accY', 'accZ']\nn_sensors = len(axis)\n \nSelecting the Raw Features on the Studio Spectral Analysis tab, we can copy all 375 data points of a particular 2-second window to the clipboard.\n \nPaste the data points to a new variable data:\ndata = [\n    -5.6330,  0.2376,  9.8701,\n    -5.9442,  0.4830,  9.8701,\n    -5.4217, ...\n]\nNo_raw_features = len(data)\nN = int(No_raw_features/n_sensors)\nThe total raw features are 375, but we will work with each axis individually, where \\(N= 125\\) (number of samples per axis).\nWe aim to understand how Edge Impulse gets the processed features.\n \nSo, you should also past the processed features on a variable (to compare the calculated features in Python with the ones provided by the Studio) :\nfeatures = [\n    2.7322, -0.0978, -0.3813,\n    2.3980, 3.8924, 24.6841,\n    9.6303, ...\n]\nN_feat = len(features)\nN_feat_axis = int(N_feat/n_sensors)\nThe total number of processed features is 39, which means 13 features/axis.\nLooking at those 13 features closely, we will find 3 for the time domain (RMS, Skewness, and Kurtosis):\n\n[rms] [skew] [kurtosis]\n\nand 10 for the frequency domain (we will return to this later).\n\n[spectral skew][spectral kurtosis][Spectral Power 1] ... [Spectral Power 8]\n\nSplitting raw data per sensor\nThe data has samples from all axes; let‚Äôs split and plot them separately:\ndef plot_data(sensors, axis, title):\n    [plt.plot(x, label=y) for x,y in zip(sensors, axis)]\n    plt.legend(loc='lower right')\n    plt.title(title)\n    plt.xlabel('#Sample')\n    plt.ylabel('Value')\n    plt.box(False)\n    plt.grid()\n    plt.show()\n\naccX = data[0::3]\naccY = data[1::3]\naccZ = data[2::3]\nsensors = [accX, accY, accZ]\nplot_data(sensors, axis, 'Raw Features')\n \nSubtracting the mean\nNext, we should subtract the mean from the data. Subtracting the mean from a data set is a common data pre-processing step in statistics and machine learning. The purpose of subtracting the mean from the data is to center the data around zero. This is important because it can reveal patterns and relationships that might be hidden if the data is not centered.\nHere are some specific reasons why subtracting the mean can be helpful:\n\nIt simplifies analysis: By centering the data, the mean becomes zero, making some calculations simpler and easier to interpret.\nIt removes bias: If the data is biased, subtracting the mean can remove it and allow for a more accurate analysis.\nIt can reveal patterns: Centering the data can help uncover patterns that might be hidden if the data is not centered. For example, centering the data can help you identify trends over time if you analyze a time series dataset.\nIt can improve performance: In some machine learning algorithms, centering the data can improve performance by reducing the influence of outliers and making the data more easily comparable. Overall, subtracting the mean is a simple but powerful technique that can be used to improve the analysis and interpretation of data.\n\ndtmean = [\n    (sum(x) / len(x))\n    for x in sensors\n]\n\n[\n    print('mean_' + x + ' =', round(y, 4))\n    for x, y in zip(axis, dtmean)\n][0]\n\naccX = [(x - dtmean[0]) for x in accX]\naccY = [(x - dtmean[1]) for x in accY]\naccZ = [(x - dtmean[2]) for x in accZ]\nsensors = [accX, accY, accZ]\n\nplot_data(sensors, axis, 'Raw Features - Subtract the Mean')",
    "crumbs": [
      "Shared Resources",
      "DSP Spectral Features"
    ]
  },
  {
    "objectID": "contents/shared/dsp_spectral_features_block/dsp_spectral_features_block.html#sec-dsp-spectral-features-time-domain-statistical-features-0125",
    "href": "contents/shared/dsp_spectral_features_block/dsp_spectral_features_block.html#sec-dsp-spectral-features-time-domain-statistical-features-0125",
    "title": "DSP Spectral Features",
    "section": "",
    "text": "RMS Calculation\nThe RMS value of a set of values (or a continuous-time waveform) is the square root of the arithmetic mean of the squares of the values or the square of the function that defines the continuous waveform. In physics, the RMS value of an electrical current is defined as the ‚Äúvalue of the direct current that dissipates the same power in a resistor.‚Äù\nIn the case of a set of \\(n\\) values \\({ùë•_1, ùë•_2, \\ldots, ùë•_ùëõ}\\), the RMS is: \\[\nx_{\\mathrm{RMS}} = \\sqrt{\\frac{1}{n} \\left( x_1^2 + x_2^2 + \\cdots + x_n^2 \\right)}\n\\]\n\n\nNOTE that the RMS value is different for the original raw data, and after subtracting the mean\n\n# Using numpy and standardized data (subtracting mean)\nrms = [np.sqrt(np.mean(np.square(x))) for x in sensors]\nWe can compare the calculated RMS values here with the ones presented by Edge Impulse:\n[print('rms_'+x+'= ', round(y, 4)) for x,y in zip(axis, rms)][0]\nprint(\"\\nCompare with Edge Impulse result features\")\nprint(features[0:N_feat:N_feat_axis])\nrms_accX=  2.7322\nrms_accY=  0.7833\nrms_accZ=  0.1383\nCompared with Edge Impulse result features:\n[2.7322, 0.7833, 0.1383]\nSkewness and kurtosis calculation\nIn statistics, skewness and kurtosis are two ways to measure the shape of a distribution.\nHere, we can see the sensor values distribution:\nfig, axes = plt.subplots(nrows=1, ncols=3, figsize=(13, 4))\nsns.kdeplot(accX, fill=True, ax=axes[0])\nsns.kdeplot(accY, fill=True, ax=axes[1])\nsns.kdeplot(accZ, fill=True, ax=axes[2])\naxes[0].set_title('accX')\naxes[1].set_title('accY')\naxes[2].set_title('accZ')\nplt.suptitle('IMU Sensors distribution', fontsize=16, y=1.02)\nplt.show()\n \nSkewness is a measure of the asymmetry of a distribution. This value can be positive or negative.\n \n\nA negative skew indicates that the tail is on the left side of the distribution, which extends towards more negative values.\nA positive skew indicates that the tail is on the right side of the distribution, which extends towards more positive values.\nA zero value indicates no skewness in the distribution at all, meaning the distribution is perfectly symmetrical.\n\nskew = [skew(x, bias=False) for x in sensors]\n[print('skew_'+x+'= ', round(y, 4))\n  for x,y in zip(axis, skew)][0]\nprint(\"\\nCompare with Edge Impulse result features\")\nfeatures[1:N_feat:N_feat_axis]\nskew_accX=  -0.099\nskew_accY=  0.1756\nskew_accZ=  6.9463\nCompared with Edge Impulse result features:\n[-0.0978, 0.1735, 6.8629]\nKurtosis is a measure of whether or not a distribution is heavy-tailed or light-tailed relative to a normal distribution.\n \n\nThe kurtosis of a normal distribution is zero.\nIf a given distribution has a negative kurtosis, it is said to be playkurtic, which means it tends to produce fewer and less extreme outliers than the normal distribution.\nIf a given distribution has a positive kurtosis , it is said to be leptokurtic, which means it tends to produce more outliers than the normal distribution.\n\nkurt = [kurtosis(x, bias=False) for x in sensors]\n[print('kurt_'+x+'= ', round(y, 4))\n  for x,y in zip(axis, kurt)][0]\nprint(\"\\nCompare with Edge Impulse result features\")\nfeatures[2:N_feat:N_feat_axis]\nkurt_accX=  -0.3475\nkurt_accY=  1.2673\nkurt_accZ=  68.1123\nCompared with Edge Impulse result features:\n[-0.3813, 1.1696, 65.3726]",
    "crumbs": [
      "Shared Resources",
      "DSP Spectral Features"
    ]
  },
  {
    "objectID": "contents/shared/dsp_spectral_features_block/dsp_spectral_features_block.html#sec-dsp-spectral-features-spectral-features-5a1c",
    "href": "contents/shared/dsp_spectral_features_block/dsp_spectral_features_block.html#sec-dsp-spectral-features-spectral-features-5a1c",
    "title": "DSP Spectral Features",
    "section": "",
    "text": "The filtered signal is passed to the Spectral power section, which computes the FFT to generate the spectral features.\nSince the sampled window is usually larger than the FFT size, the window will be broken into frames (or ‚Äúsub-windows‚Äù), and the FFT is calculated over each frame.\nFFT length - The FFT size. This determines the number of FFT bins and the resolution of frequency peaks that can be separated. A low number means more signals will average together in the same FFT bin, but it also reduces the number of features and model size. A high number will separate more signals into separate bins, generating a larger model.\n\nThe total number of Spectral Power features will vary depending on how you set the filter and FFT parameters. With No filtering, the number of features is 1/2 of the FFT Length.\n\nSpectral Power - Welch‚Äôs method\nWe should use Welch‚Äôs method to split the signal on the frequency domain in bins and calculate the power spectrum for each bin. This method divides the signal into overlapping segments, applies a window function to each segment, computes the periodogram of each segment using DFT, and averages them to obtain a smoother estimate of the power spectrum.\n# Function used by Edge Impulse instead of scipy.signal.welch().\ndef welch_max_hold(fx, sampling_freq, nfft, n_overlap):\n    n_overlap = int(n_overlap)\n    spec_powers = [0 for _ in range(nfft//2+1)]\n    ix = 0\n    while ix &lt;= len(fx):\n        # Slicing truncates if end_idx &gt; len,\n        # and rfft will auto-zero pad\n        fft_out = np.abs(np.fft.rfft(fx[ix:ix+nfft], nfft))\n        spec_powers = np.maximum(spec_powers, fft_out**2/nfft)\n        ix = ix + (nfft-n_overlap)\n    return np.fft.rfftfreq(nfft, 1/sampling_freq), spec_powers\nApplying the above function to 3 signals:\nfax,Pax = welch_max_hold(accX, fs, FFT_Length, 0)\nfay,Pay = welch_max_hold(accY, fs, FFT_Length, 0)\nfaz,Paz = welch_max_hold(accZ, fs, FFT_Length, 0)\nspecs = [Pax, Pay, Paz ]\nWe can plot the Power Spectrum P(f):\nplt.plot(fax,Pax, label='accX')\nplt.plot(fay,Pay, label='accY')\nplt.plot(faz,Paz, label='accZ')\nplt.legend(loc='upper right')\nplt.xlabel('Frequency (Hz)')\n#plt.ylabel('PSD [V**2/Hz]')\nplt.ylabel('Power')\nplt.title('Power spectrum P(f) using Welch's method')\nplt.grid()\nplt.box(False)\nplt.show()\n \nBesides the Power Spectrum, we can also include the skewness and kurtosis of the features in the frequency domain (should be available on a new version):\nspec_skew = [skew(x, bias=False) for x in specs]\nspec_kurtosis = [kurtosis(x, bias=False) for x in specs]\nLet‚Äôs now list all Spectral features per axis and compare them with EI:\nprint(\"EI Processed Spectral features (accX): \")\nprint(features[3:N_feat_axis][0:])\nprint(\"\\nCalculated features:\")\nprint (round(spec_skew[0],4))\nprint (round(spec_kurtosis[0],4))\n[print(round(x, 4)) for x in Pax[1:]][0]\nEI Processed Spectral features (accX):\n2.398, 3.8924, 24.6841, 9.6303, 8.4867, 7.7793, 2.9963, 5.6242, 3.4198, 4.2735\nCalculated features:\n2.9069 8.5569 24.6844 9.6304 8.4865 7.7794 2.9964 5.6242 3.4198 4.2736\nprint(\"EI Processed Spectral features (accY): \")\nprint(features[16:26][0:]) # 13: 3+N_feat_axis;\n                           # 26 = 2x N_feat_axis\nprint(\"\\nCalculated features:\")\nprint (round(spec_skew[1],4))\nprint (round(spec_kurtosis[1],4))\n[print(round(x, 4)) for x in Pay[1:]][0]\nEI Processed Spectral features (accY):\n0.9426, -0.8039, 5.429, 0.999, 1.0315, 0.9459, 1.8117, 0.9088, 1.3302, 3.112\nCalculated features:\n1.1426 -0.3886 5.4289 0.999 1.0315 0.9458 1.8116 0.9088 1.3301 3.1121\nprint(\"EI Processed Spectral features (accZ): \")\nprint(features[29:][0:]) #29: 3+(2*N_feat_axis);\nprint(\"\\nCalculated features:\")\nprint (round(spec_skew[2],4))\nprint (round(spec_kurtosis[2],4))\n[print(round(x, 4)) for x in Paz[1:]][0]\nEI Processed Spectral features (accZ):\n0.3117, -1.3812, 0.0606, 0.057, 0.0567, 0.0976, 0.194, 0.2574, 0.2083, 0.166\nCalculated features:\n0.3781 -1.4874 0.0606 0.057 0.0567 0.0976 0.194 0.2574 0.2083 0.166",
    "crumbs": [
      "Shared Resources",
      "DSP Spectral Features"
    ]
  },
  {
    "objectID": "contents/shared/dsp_spectral_features_block/dsp_spectral_features_block.html#sec-dsp-spectral-features-timefrequency-domain-0a7d",
    "href": "contents/shared/dsp_spectral_features_block/dsp_spectral_features_block.html#sec-dsp-spectral-features-timefrequency-domain-0a7d",
    "title": "DSP Spectral Features",
    "section": "",
    "text": "Wavelet is a powerful technique for analyzing signals with transient features or abrupt changes, such as spikes or edges, which are difficult to interpret with traditional Fourier-based methods.\nWavelet transforms work by breaking down a signal into different frequency components and analyzing them individually. The transformation is achieved by convolving the signal with a wavelet function, a small waveform centered at a specific time and frequency. This process effectively decomposes the signal into different frequency bands, each of which can be analyzed separately.\nOne of the critical benefits of wavelet transforms is that they allow for time-frequency analysis, which means that they can reveal the frequency content of a signal as it changes over time. This makes them particularly useful for analyzing non-stationary signals, which vary over time.\nWavelets have many practical applications, including signal and image compression, denoising, feature extraction, and image processing.\nLet‚Äôs select Wavelet on the Spectral Features block in the same project:\n\nType: Wavelet\nWavelet Decomposition Level: 1\nWavelet: bior1.3\n\n \nThe Wavelet Function\nwavelet_name='bior1.3'\nnum_layer = 1\n\nwavelet = pywt.Wavelet(wavelet_name)\n[phi_d,psi_d,phi_r,psi_r,x] = wavelet.wavefun(level=5)\nplt.plot(x, psi_d, color='red')\nplt.title('Wavelet Function')\nplt.ylabel('Value')\nplt.xlabel('Time')\nplt.grid()\nplt.box(False)\nplt.show()\n \nAs we did before, let‚Äôs copy and past the Processed Features:\n \nfeatures = [\n    3.6251, 0.0615, 0.0615,\n    -7.3517, -2.7641, 2.8462,\n    5.0924, ...\n]\nN_feat = len(features)\nN_feat_axis = int(N_feat/n_sensors)\nEdge Impulse computes the Discrete Wavelet Transform (DWT) for each one of the Wavelet Decomposition levels selected. After that, the features will be extracted.\nIn the case of Wavelets, the extracted features are basic statistical values, crossing values, and entropy. There are, in total, 14 features per layer as below:\n\n[11] Statistical Features: n5, n25, n75, n95, mean, median, standard deviation (std), variance (var) root mean square (rms), kurtosis, and skewness (skew).\n[2] Crossing Features: Zero crossing rate (zcross) and mean crossing rate (mcross) are the times that the signal passes through the baseline \\((y = 0)\\) and the average level (y = u) per unit of time, respectively\n[1] Complexity Feature: Entropy is a characteristic measure of the complexity of the signal\n\nAll the above 14 values are calculated for each Layer (including L0, the original signal)\n\nThe total number of features varies depending on how you set the filter and the number of layers. For example, with [None] filtering and Level[1], the number of features per axis will be \\(14\\times 2\\) (L0 and L1) = 28. For the three axes, we will have a total of 84 features.\n\n\n\n\nWavelet analysis decomposes the signal (accX, accY, and accZ) into different frequency components using a set of filters, which separate these components into low-frequency (slowly varying parts of the signal containing long-term patterns), such as accX_l1, accY_l1, accZ_l1 and, high-frequency (rapidly varying parts of the signal containing short-term patterns) components, such as accX_d1, accY_d1, accZ_d1, permitting the extraction of features for further analysis or classification.\nOnly the low-frequency components (approximation coefficients, or cA) will be used. In this example, we assume only one level (Single-level Discrete Wavelet Transform), where the function will return a tuple. With a multilevel decomposition, the ‚ÄúMultilevel 1D Discrete Wavelet Transform‚Äù, the result will be a list (for detail, please see: Discrete Wavelet Transform (DWT) )\n(accX_l1, accX_d1) = pywt.dwt(accX, wavelet_name)\n(accY_l1, accY_d1) = pywt.dwt(accY, wavelet_name)\n(accZ_l1, accZ_d1) = pywt.dwt(accZ, wavelet_name)\nsensors_l1 = [accX_l1, accY_l1, accZ_l1]\n\n# Plot power spectrum versus frequency\nplt.plot(accX_l1, label='accX')\nplt.plot(accY_l1, label='accY')\nplt.plot(accZ_l1, label='accZ')\nplt.legend(loc='lower right')\nplt.xlabel('Time')\nplt.ylabel('Value')\nplt.title('Wavelet Approximation')\nplt.grid()\nplt.box(False)\nplt.show()\n \n\n\n\nLet‚Äôs start with the basic statistical features. Note that we apply the function for both the original signals and the resultant cAs from the DWT:\ndef calculate_statistics(signal):\n    n5 = np.percentile(signal, 5)\n    n25 = np.percentile(signal, 25)\n    n75 = np.percentile(signal, 75)\n    n95 = np.percentile(signal, 95)\n    median = np.percentile(signal, 50)\n    mean = np.mean(signal)\n    std = np.std(signal)\n    var = np.var(signal)\n    rms = np.sqrt(np.mean(np.square(signal)))\n    return [n5, n25, n75, n95, median, mean, std, var, rms]\n\nstat_feat_l0 = [calculate_statistics(x) for x in sensors]\nstat_feat_l1 = [calculate_statistics(x) for x in sensors_l1]\nThe Skelness and Kurtosis:\nskew_l0 = [skew(x, bias=False) for x in sensors]\nskew_l1 = [skew(x, bias=False) for x in sensors_l1]\nkurtosis_l0 = [kurtosis(x, bias=False) for x in sensors]\nkurtosis_l1 = [kurtosis(x, bias=False) for x in sensors_l1]\nZero crossing (zcross) is the number of times the wavelet coefficient crosses the zero axis. It can be used to measure the signal‚Äôs frequency content since high-frequency signals tend to have more zero crossings than low-frequency signals.\nMean crossing (mcross), on the other hand, is the number of times the wavelet coefficient crosses the mean of the signal. It can be used to measure the amplitude since high-amplitude signals tend to have more mean crossings than low-amplitude signals.\ndef getZeroCrossingRate(arr):\n    my_array = np.array(arr)\n    zcross = float(\n        \"{:.2f}\".format(\n          (((my_array[:-1] * my_array[1:]) &lt; 0).sum()) / len(arr)\n        )\n    )\n    return zcross\n\ndef getMeanCrossingRate(arr):\n    mcross = getZeroCrossingRate(np.array(arr) - np.mean(arr))\n    return mcross\n\ndef calculate_crossings(list):\n    zcross=[]\n    mcross=[]\n    for i in range(len(list)):\n        zcross_i = getZeroCrossingRate(list[i])\n        zcross.append(zcross_i)\n        mcross_i = getMeanCrossingRate(list[i])\n        mcross.append(mcross_i)\n    return zcross, mcross\n\ncross_l0 = calculate_crossings(sensors)\ncross_l1 = calculate_crossings(sensors_l1)\nIn wavelet analysis, entropy refers to the degree of disorder or randomness in the distribution of wavelet coefficients. Here, we used Shannon entropy, which measures a signal‚Äôs uncertainty or randomness. It is calculated as the negative sum of the probabilities of the different possible outcomes of the signal multiplied by their base 2 logarithm. In the context of wavelet analysis, Shannon entropy can be used to measure the complexity of the signal, with higher values indicating greater complexity.\ndef calculate_entropy(signal, base=None):\n    value, counts = np.unique(signal, return_counts=True)\n    return entropy(counts, base=base)\n\nentropy_l0 = [calculate_entropy(x) for x in sensors]\nentropy_l1 = [calculate_entropy(x) for x in sensors_l1]\nLet‚Äôs now list all the wavelet features and create a list by layers.\nL1_features_names = [\n    \"L1-n5\", \"L1-n25\", \"L1-n75\", \"L1-n95\", \"L1-median\",\n    \"L1-mean\", \"L1-std\", \"L1-var\", \"L1-rms\", \"L1-skew\",\n    \"L1-Kurtosis\", \"L1-zcross\", \"L1-mcross\", \"L1-entropy\"\n]\n\nL0_features_names = [\n    \"L0-n5\", \"L0-n25\", \"L0-n75\", \"L0-n95\", \"L0-median\",\n    \"L0-mean\", \"L0-std\", \"L0-var\", \"L0-rms\", \"L0-skew\",\n    \"L0-Kurtosis\", \"L0-zcross\", \"L0-mcross\", \"L0-entropy\"\n]\n\nall_feat_l0 = []\nfor i in range(len(axis)):\n    feat_l0 = (\n        stat_feat_l0[i]\n        + [skew_l0[i]]\n        + [kurtosis_l0[i]]\n        + [cross_l0[0][i]]\n        + [cross_l0[1][i]]\n        + [entropy_l0[i]]\n    )\n    [print(axis[i] + ' +x+= ', round(y, 4))\n       for x, y in zip(LO_features_names, feat_l0)][0]\n    all_feat_l0.append(feat_l0)\n\nall_feat_l0 = [\n    item\n    for sublist in all_feat_l0\n    for item in sublist\n]\nprint(f\"\\nAll L0 Features = {len(all_feat_l0)}\")\n\nall_feat_l1 = []\nfor i in range(len(axis)):\n    feat_l1 = (\n        stat_feat_l1[i]\n        + [skew_l1[i]]\n        + [kurtosis_l1[i]]\n        + [cross_l1[0][i]]\n        + [cross_l1[1][i]]\n        + [entropy_l1[i]]\n    )\n    [print(axis[i]+' '+x+'= ', round(y, 4))\n       for x,y in zip(L1_features_names, feat_l1)][0]\n    all_feat_l1.append(feat_l1)\n\nall_feat_l1 = [\n    item\n    for sublist in all_feat_l1\n    for item in sublist\n]\nprint(f\"\\nAll L1 Features = {len(all_feat_l1)}\")",
    "crumbs": [
      "Shared Resources",
      "DSP Spectral Features"
    ]
  },
  {
    "objectID": "contents/shared/dsp_spectral_features_block/dsp_spectral_features_block.html#sec-dsp-spectral-features-summary-6245",
    "href": "contents/shared/dsp_spectral_features_block/dsp_spectral_features_block.html#sec-dsp-spectral-features-summary-6245",
    "title": "DSP Spectral Features",
    "section": "",
    "text": "Edge Impulse Studio is a powerful online platform that can handle the pre-processing task for us. Still, given our engineering perspective, we want to understand what is happening under the hood. This knowledge will help us find the best options and hyper-parameters for tuning our projects.\nDaniel Situnayake wrote in his blog: ‚ÄúRaw sensor data is highly dimensional and noisy. Digital signal processing algorithms help us sift the signal from the noise. DSP is an essential part of embedded engineering, and many edge processors have on-board acceleration for DSP. As an ML engineer, learning basic DSP gives you superpowers for handling high-frequency time series data in your models.‚Äù I recommend you read Dan‚Äôs excellent post in its totality: nn to cpp: What you need to know about porting deep learning models to the edge.",
    "crumbs": [
      "Shared Resources",
      "DSP Spectral Features"
    ]
  },
  {
    "objectID": "contents/arduino/nicla_vision/setup/setup.html",
    "href": "contents/arduino/nicla_vision/setup/setup.html",
    "title": "Setup",
    "section": "",
    "text": "DALL¬∑E 3 Prompt: Illustration reminiscent of a 1950s cartoon where the Arduino NICLA VISION board, equipped with various sensors including a camera, is the focal point on an old-fashioned desk. In the background, a computer screen with rounded edges displays the Arduino IDE. The code is related to LED configurations and machine learning voice command detection. Outputs on the Serial Monitor explicitly display the words ‚Äòyes‚Äô and ‚Äòno‚Äô.\n\n\n\n\nThe Arduino Nicla Vision (sometimes called NiclaV) is a development board that includes two processors that can run tasks in parallel. It is part of a family of development boards with the same form factor but designed for specific tasks, such as the Nicla Sense ME and the Nicla Voice. The Niclas can efficiently run processes created with TensorFlow Lite. For example, one of the cores of the NiclaV runs a computer vision algorithm on the fly (inference). At the same time, the other executes low-level operations like controlling a motor and communicating or acting as a user interface. The onboard wireless module allows the simultaneous management of WiFi and Bluetooth Low Energy (BLE) connectivity.\n \n\n\n\n\n\nThe central processor is the dual-core STM32H747, including a Cortex M7 at 480 MHz and a Cortex M4 at 240 MHz. The two cores communicate via a Remote Procedure Call mechanism that seamlessly allows calling functions on the other processor. Both processors share all the on-chip peripherals and can run:\n\nArduino sketches on top of the Arm Mbed OS\nNative Mbed applications\nMicroPython / JavaScript via an interpreter\nTensorFlow Lite\n\n \n\n\n\nMemory is crucial for embedded machine learning projects. The NiclaV board can host up to 16 MB of QSPI Flash for storage. However, it is essential to consider that the MCU SRAM is the one to be used with machine learning inferences; the STM32H747 is only 1 MB, shared by both processors. This MCU also has incorporated 2 MB of FLASH, mainly for code storage.\n\n\n\n\nCamera: A GC2145 2 MP Color CMOS Camera.\nMicrophone: The MP34DT05 is an ultra-compact, low-power, omnidirectional, digital MEMS microphone built with a capacitive sensing element and the IC interface.\n6-Axis IMU: 3D gyroscope and 3D accelerometer data from the LSM6DSOX 6-axis IMU.\nTime of Flight Sensor: The VL53L1CBV0FY Time-of-Flight sensor adds accurate and low-power-ranging capabilities to Nicla Vision. The invisible near-infrared VCSEL laser (including the analog driver) is encapsulated with receiving optics in an all-in-one small module below the camera.\n\n\n\n\n\nStart connecting the board (micro USB) to your computer:\n \nInstall the Mbed OS core for Nicla boards in the Arduino IDE. Having the IDE open, navigate to Tools &gt; Board &gt; Board Manager, look for Arduino Nicla Vision on the search window, and install the board.\n \nNext, go to Tools &gt; Board &gt; Arduino Mbed OS Nicla Boards and select Arduino Nicla Vision. Having your board connected to the USB, you should see the Nicla on Port and select it.\n\nOpen the Blink sketch on Examples/Basic and run it using the IDE Upload button. You should see the Built-in LED (green RGB) blinking, which means the Nicla board is correctly installed and functional!\n\n\n\nOn Arduino IDE, go to Examples &gt; PDM &gt; PDMSerialPlotter, open it, and run the sketch. Open the Plotter and see the audio representation from the microphone:\n \n\nVary the frequency of the sound you generate and confirm that the mic is working correctly.\n\n\n\n\nBefore testing the IMU, it will be necessary to install the LSM6DSOX library. To do so, go to Library Manager and look for LSM6DSOX. Install the library provided by Arduino:\n \nNext, go to Examples &gt; Arduino_LSM6DSOX &gt; SimpleAccelerometer and run the accelerometer test (you can also run Gyro and board temperature):\n \n\n\n\nAs we did with IMU, installing the VL53L1X ToF library is necessary. To do that, go to Library Manager and look for VL53L1X. Install the library provided by Pololu:\n \nNext, run the sketch proximity_detection.ino:\n \nOn the Serial Monitor, you will see the distance from the camera to an object in front of it (max of 4 m).\n \n\n\n\nWe can also test the camera using, for example, the code provided on Examples &gt; Camera &gt; CameraCaptureRawBytes. We cannot see the image directly, but we can get the raw image data generated by the camera.\nWe can use the Web Serial Camera (API) to see the image generated by the camera. This web application streams the camera image over Web Serial from camera-equipped Arduino boards.\nThe Web Serial Camera example shows you how to send image data over the wire from your Arduino board and how to unpack the data in JavaScript for rendering. In addition, in the source code of the web application, we can find some example image filters that show us how to manipulate pixel data to achieve visual effects.\nThe Arduino sketch (CameraCaptureWebSerial) for sending the camera image data can be found here and is also directly available from the ‚ÄúExamples‚ÜíCamera‚Äù menu in the Arduino IDE when selecting the Nicla board.\nThe web application for displaying the camera image can be accessed here. We may also look at [this tutorial, which explains the setup in more detail.\n \n\n\n\n\nOpenMV IDE is the premier integrated development environment with OpenMV cameras, similar to the Nicla Vision. It features a powerful text editor, debug terminal, and frame buffer viewer with a histogram display. We will use MicroPython to program the camera.\nGo to the OpenMV IDE page, download the correct version for your Operating System, and follow the instructions for its installation on your computer.\n \nThe IDE should open, defaulting to the helloworld_1.py code on its Code Area. If not, you can open it from Files &gt; Examples &gt; HelloWord &gt; helloword.py\n \nAny messages sent through a serial connection (using print() or error messages) will be displayed on the Serial Terminal during run time. The image captured by a camera will be displayed in the Camera Viewer Area (or Frame Buffer) and in the Histogram area, immediately below the Camera Viewer.\n\n\nBefore connecting the Nicla to the OpenMV IDE, ensure you have the latest bootloader version. Go to your Arduino IDE, select the Nicla board, and open the sketch on Examples &gt; STM_32H747_System STM32H747_manageBootloader. Upload the code to your board. The Serial Monitor will guide you.\n\n\n\nAfter updating the bootloader, put the Nicla Vision in bootloader mode by double-pressing the reset button on the board. The built-in green LED will start fading in and out. Now return to the OpenMV IDE and click on the connect icon (Left ToolBar):\n \nA pop-up will tell you that a board in DFU mode was detected and ask how you would like to proceed. First, select Install the latest release firmware (vX.Y.Z). This action will install the latest OpenMV firmware on the Nicla Vision.\n \nYou can leave the option Erase internal file system unselected and click [OK].\nNicla‚Äôs green LED will start flashing while the OpenMV firmware is uploaded to the board, and a terminal window will then open, showing the flashing progress.\n \nWait until the green LED stops flashing and fading. When the process ends, you will see a message saying, ‚ÄúDFU firmware update complete!‚Äù. Press [OK].\n \nA green play button appears when the Nicla Vison connects to the Tool Bar.\n \nAlso, note that a drive named ‚ÄúNO NAME‚Äù will appear on your computer.\n \nEvery time you press the [RESET] button on the board, the main.py script stored on it automatically executes. You can load the main.py code on the IDE (File &gt; Open File...).\n \n\nThis code is the ‚ÄúBlink‚Äù code, confirming that the HW is OK.\n\n\n\n\nTo test the camera, let‚Äôs run helloword_1.py. For that, select the script on File &gt; Examples &gt; HelloWorld &gt; helloword.py,\nWhen clicking the green play button, the MicroPython script (helloworld.py) on the Code Area will be uploaded and run on the Nicla Vision. On-Camera Viewer, you will start to see the video streaming. The Serial Monitor will show us the FPS (Frames per second), which should be around 27fps.\n \nHere is the helloworld.py script:\nimport sensor, time\n\nsensor.reset()                      # Reset and initialize\n                                    # the sensor.\nsensor.set_pixformat(sensor.RGB565) # Set pixel format to RGB565\n                                    # (or GRAYSCALE)\nsensor.set_framesize(sensor.QVGA)   # Set frame size to\n                                    # QVGA (320x240)\nsensor.skip_frames(time = 2000)     # Wait for settings take\n                                    # effect.\nclock = time.clock()                # Create a clock object\n                                    # to track the FPS.\n\nwhile(True):\n    clock.tick()                    # Update the FPS clock.\n    img = sensor.snapshot()         # Take a picture and return\n                                    # the image.\n    print(clock.fps())\nIn GitHub, you can find the Python scripts used here.\nThe code can be split into two parts:\n\nSetup: Where the libraries are imported, initialized and the variables are defined and initiated.\nLoop: (while loop) part of the code that runs continually. The image (img variable) is captured (one frame). Each of those frames can be used for inference in Machine Learning Applications.\n\nTo interrupt the program execution, press the red [X] button.\n\nNote: OpenMV Cam runs about half as fast when connected to the IDE. The FPS should increase once disconnected.\n\nIn the GitHub, You can find other Python scripts. Try to test the onboard sensors.\n\n\n\n\nWe will need the Edge Impulse Studio later in other labs. Edge Impulse is a leading development platform for machine learning on edge devices.\nEdge Impulse officially supports the Nicla Vision. So, to start, please create a new project on the Studio and connect the Nicla to it. For that, follow the steps:\n\nDownload the Arduino CLI for your specific computer architecture (OS)\nDownload the most updated EI Firmware.\nUnzip both files and place all the files in the same folder.\nPut the Nicla-Vision on Boot Mode, pressing the reset button twice.\nRun the uploader (EI FW) corresponding to your OS:\n\n \n\nExecuting the specific batch code for your OS will upload the binary arduino-nicla-vision.bin to your board.\n\n\nUsing Chrome, WebUSB can be used to connect the Nicla to the EI Studio. The EI CLI is not needed.\n\nGo to your project on the Studio, and on the Data Acquisition tab, select WebUSB (1). A window will pop up; choose the option that shows that the Nicla is paired (2) and press [Connect] (3).\n \nYou can choose which sensor data to pick in the Collect Data section on the Data Acquisition tab.\n \nFor example. IMU data (inertial):\n \nOr Image (Camera):\n \nYou can also test an external sensor connected to the ADC (Nicla pin 0) and the other onboard sensors, such as the built-in microphone, the ToF (Proximity) or a combination of sensors (fusion).\n\n\n\nA last item to explore is that sometimes, during prototyping, it is essential to experiment with external sensors and devices. An excellent expansion to the Nicla is the Arduino MKR Connector Carrier (Grove compatible).\nThe shield has 14 Grove connectors: five single analog inputs (A0-A5), one double analog input (A5/A6), five single digital I/Os (D0-D4), one double digital I/O (D5/D6), one I2C (TWI), and one UART (Serial). All connectors are 5V compatible.\n\nNote that all 17 Nicla Vision pins will be connected to the Shield Groves, but some Grove connections remain disconnected.\n\n \nThis shield is MKR compatible and can be used with the Nicla Vision and Portenta.\n \nFor example, suppose that on a TinyML project, you want to send inference results using a LoRaWAN device and add information about local luminosity. Often, with offline operations, a local low-power display such as an OLED is advised. This setup can be seen here:\n \nThe Grove Light Sensor would be connected to one of the single Analog pins (A0/PC4), the LoRaWAN device to the UART, and the OLED to the I2C connector.\nThe Nicla Pins 3 (Tx) and 4 (Rx) are connected with the Serial Shield connector. The UART communication is used with the LoRaWan device. Here is a simple code to use the UART:\n# UART Test - By: marcelo_rovai - Sat Sep 23 2023\n\nimport time\nfrom pyb import UART\nfrom pyb import LED\n\nredLED = LED(1) # built-in red LED\n\n# Init UART object.\n# Nicla Vision's UART (TX/RX pins) is on \"LP1\"\nuart = UART(\"LP1\", 9600)\n\nwhile(True):\n    uart.write(\"Hello World!\\r\\n\")\n    redLED.toggle()\n    time.sleep_ms(1000)\nTo verify that the UART is working, you should, for example, connect another device as the Arduino UNO, displaying ‚ÄúHello Word‚Äù on the Serial Monitor. Here is the code.\n \nBelow is the Hello World code to be used with the I2C OLED. The MicroPython SSD1306 OLED driver (ssd1306.py), created by Adafruit, should also be uploaded to the Nicla (the ssd1306.py script can be found in GitHub).\n# Nicla_OLED_Hello_World - By: marcelo_rovai - Sat Sep 30 2023\n\n#Save on device: MicroPython SSD1306 OLED driver,\n# I2C and SPI interfaces created by Adafruit\nimport ssd1306\n\nfrom machine import I2C\ni2c = I2C(1)\n\noled_width = 128\noled_height = 64\noled = ssd1306.SSD1306_I2C(oled_width, oled_height, i2c)\n\noled.text('Hello, World', 10, 10)\noled.show()\nFinally, here is a simple script to read the ADC value on pin ‚ÄúPC4‚Äù (Nicla pin A0):\n\n# Light Sensor (A0) - By: marcelo_rovai - Wed Oct 4 2023\n\nimport pyb\nfrom time import sleep\n\nadc = pyb.ADC(pyb.Pin(\"PC4\"))   # create an analog object\n                                # from a pin\nval = adc.read()                # read an analog value\n\nwhile (True):\n\n    val = adc.read()\n    print (\"Light={}\".format (val))\n    sleep (1)\nThe ADC can be used for other sensor variables, such as Temperature.\n\nNote that the above scripts (downloaded from Github) introduce only how to connect external devices with the Nicla Vision board using MicroPython.\n\n\n\n\nThe Arduino Nicla Vision is an excellent tiny device for industrial and professional uses! However, it is powerful, trustworthy, low power, and has suitable sensors for the most common embedded machine learning applications such as vision, movement, sensor fusion, and sound.\n\nOn the GitHub repository, you will find the last version of all the code used or commented on in this hands-on lab.\n\n\n\n\n\nMicropython codes\nArduino Codes",
    "crumbs": [
      "Arduino Nicla Vision",
      "Setup"
    ]
  },
  {
    "objectID": "contents/arduino/nicla_vision/setup/setup.html#sec-setup-overview-dcdd",
    "href": "contents/arduino/nicla_vision/setup/setup.html#sec-setup-overview-dcdd",
    "title": "Setup",
    "section": "",
    "text": "The Arduino Nicla Vision (sometimes called NiclaV) is a development board that includes two processors that can run tasks in parallel. It is part of a family of development boards with the same form factor but designed for specific tasks, such as the Nicla Sense ME and the Nicla Voice. The Niclas can efficiently run processes created with TensorFlow Lite. For example, one of the cores of the NiclaV runs a computer vision algorithm on the fly (inference). At the same time, the other executes low-level operations like controlling a motor and communicating or acting as a user interface. The onboard wireless module allows the simultaneous management of WiFi and Bluetooth Low Energy (BLE) connectivity.",
    "crumbs": [
      "Arduino Nicla Vision",
      "Setup"
    ]
  },
  {
    "objectID": "contents/arduino/nicla_vision/setup/setup.html#sec-setup-hardware-3b70",
    "href": "contents/arduino/nicla_vision/setup/setup.html#sec-setup-hardware-3b70",
    "title": "Setup",
    "section": "",
    "text": "The central processor is the dual-core STM32H747, including a Cortex M7 at 480 MHz and a Cortex M4 at 240 MHz. The two cores communicate via a Remote Procedure Call mechanism that seamlessly allows calling functions on the other processor. Both processors share all the on-chip peripherals and can run:\n\nArduino sketches on top of the Arm Mbed OS\nNative Mbed applications\nMicroPython / JavaScript via an interpreter\nTensorFlow Lite\n\n \n\n\n\nMemory is crucial for embedded machine learning projects. The NiclaV board can host up to 16 MB of QSPI Flash for storage. However, it is essential to consider that the MCU SRAM is the one to be used with machine learning inferences; the STM32H747 is only 1 MB, shared by both processors. This MCU also has incorporated 2 MB of FLASH, mainly for code storage.\n\n\n\n\nCamera: A GC2145 2 MP Color CMOS Camera.\nMicrophone: The MP34DT05 is an ultra-compact, low-power, omnidirectional, digital MEMS microphone built with a capacitive sensing element and the IC interface.\n6-Axis IMU: 3D gyroscope and 3D accelerometer data from the LSM6DSOX 6-axis IMU.\nTime of Flight Sensor: The VL53L1CBV0FY Time-of-Flight sensor adds accurate and low-power-ranging capabilities to Nicla Vision. The invisible near-infrared VCSEL laser (including the analog driver) is encapsulated with receiving optics in an all-in-one small module below the camera.",
    "crumbs": [
      "Arduino Nicla Vision",
      "Setup"
    ]
  },
  {
    "objectID": "contents/arduino/nicla_vision/setup/setup.html#sec-setup-arduino-ide-installation-4c32",
    "href": "contents/arduino/nicla_vision/setup/setup.html#sec-setup-arduino-ide-installation-4c32",
    "title": "Setup",
    "section": "",
    "text": "Start connecting the board (micro USB) to your computer:\n \nInstall the Mbed OS core for Nicla boards in the Arduino IDE. Having the IDE open, navigate to Tools &gt; Board &gt; Board Manager, look for Arduino Nicla Vision on the search window, and install the board.\n \nNext, go to Tools &gt; Board &gt; Arduino Mbed OS Nicla Boards and select Arduino Nicla Vision. Having your board connected to the USB, you should see the Nicla on Port and select it.\n\nOpen the Blink sketch on Examples/Basic and run it using the IDE Upload button. You should see the Built-in LED (green RGB) blinking, which means the Nicla board is correctly installed and functional!\n\n\n\nOn Arduino IDE, go to Examples &gt; PDM &gt; PDMSerialPlotter, open it, and run the sketch. Open the Plotter and see the audio representation from the microphone:\n \n\nVary the frequency of the sound you generate and confirm that the mic is working correctly.\n\n\n\n\nBefore testing the IMU, it will be necessary to install the LSM6DSOX library. To do so, go to Library Manager and look for LSM6DSOX. Install the library provided by Arduino:\n \nNext, go to Examples &gt; Arduino_LSM6DSOX &gt; SimpleAccelerometer and run the accelerometer test (you can also run Gyro and board temperature):\n \n\n\n\nAs we did with IMU, installing the VL53L1X ToF library is necessary. To do that, go to Library Manager and look for VL53L1X. Install the library provided by Pololu:\n \nNext, run the sketch proximity_detection.ino:\n \nOn the Serial Monitor, you will see the distance from the camera to an object in front of it (max of 4 m).\n \n\n\n\nWe can also test the camera using, for example, the code provided on Examples &gt; Camera &gt; CameraCaptureRawBytes. We cannot see the image directly, but we can get the raw image data generated by the camera.\nWe can use the Web Serial Camera (API) to see the image generated by the camera. This web application streams the camera image over Web Serial from camera-equipped Arduino boards.\nThe Web Serial Camera example shows you how to send image data over the wire from your Arduino board and how to unpack the data in JavaScript for rendering. In addition, in the source code of the web application, we can find some example image filters that show us how to manipulate pixel data to achieve visual effects.\nThe Arduino sketch (CameraCaptureWebSerial) for sending the camera image data can be found here and is also directly available from the ‚ÄúExamples‚ÜíCamera‚Äù menu in the Arduino IDE when selecting the Nicla board.\nThe web application for displaying the camera image can be accessed here. We may also look at [this tutorial, which explains the setup in more detail.",
    "crumbs": [
      "Arduino Nicla Vision",
      "Setup"
    ]
  },
  {
    "objectID": "contents/arduino/nicla_vision/setup/setup.html#sec-setup-installing-openmv-ide-8fa1",
    "href": "contents/arduino/nicla_vision/setup/setup.html#sec-setup-installing-openmv-ide-8fa1",
    "title": "Setup",
    "section": "",
    "text": "OpenMV IDE is the premier integrated development environment with OpenMV cameras, similar to the Nicla Vision. It features a powerful text editor, debug terminal, and frame buffer viewer with a histogram display. We will use MicroPython to program the camera.\nGo to the OpenMV IDE page, download the correct version for your Operating System, and follow the instructions for its installation on your computer.\n \nThe IDE should open, defaulting to the helloworld_1.py code on its Code Area. If not, you can open it from Files &gt; Examples &gt; HelloWord &gt; helloword.py\n \nAny messages sent through a serial connection (using print() or error messages) will be displayed on the Serial Terminal during run time. The image captured by a camera will be displayed in the Camera Viewer Area (or Frame Buffer) and in the Histogram area, immediately below the Camera Viewer.\n\n\nBefore connecting the Nicla to the OpenMV IDE, ensure you have the latest bootloader version. Go to your Arduino IDE, select the Nicla board, and open the sketch on Examples &gt; STM_32H747_System STM32H747_manageBootloader. Upload the code to your board. The Serial Monitor will guide you.\n\n\n\nAfter updating the bootloader, put the Nicla Vision in bootloader mode by double-pressing the reset button on the board. The built-in green LED will start fading in and out. Now return to the OpenMV IDE and click on the connect icon (Left ToolBar):\n \nA pop-up will tell you that a board in DFU mode was detected and ask how you would like to proceed. First, select Install the latest release firmware (vX.Y.Z). This action will install the latest OpenMV firmware on the Nicla Vision.\n \nYou can leave the option Erase internal file system unselected and click [OK].\nNicla‚Äôs green LED will start flashing while the OpenMV firmware is uploaded to the board, and a terminal window will then open, showing the flashing progress.\n \nWait until the green LED stops flashing and fading. When the process ends, you will see a message saying, ‚ÄúDFU firmware update complete!‚Äù. Press [OK].\n \nA green play button appears when the Nicla Vison connects to the Tool Bar.\n \nAlso, note that a drive named ‚ÄúNO NAME‚Äù will appear on your computer.\n \nEvery time you press the [RESET] button on the board, the main.py script stored on it automatically executes. You can load the main.py code on the IDE (File &gt; Open File...).\n \n\nThis code is the ‚ÄúBlink‚Äù code, confirming that the HW is OK.\n\n\n\n\nTo test the camera, let‚Äôs run helloword_1.py. For that, select the script on File &gt; Examples &gt; HelloWorld &gt; helloword.py,\nWhen clicking the green play button, the MicroPython script (helloworld.py) on the Code Area will be uploaded and run on the Nicla Vision. On-Camera Viewer, you will start to see the video streaming. The Serial Monitor will show us the FPS (Frames per second), which should be around 27fps.\n \nHere is the helloworld.py script:\nimport sensor, time\n\nsensor.reset()                      # Reset and initialize\n                                    # the sensor.\nsensor.set_pixformat(sensor.RGB565) # Set pixel format to RGB565\n                                    # (or GRAYSCALE)\nsensor.set_framesize(sensor.QVGA)   # Set frame size to\n                                    # QVGA (320x240)\nsensor.skip_frames(time = 2000)     # Wait for settings take\n                                    # effect.\nclock = time.clock()                # Create a clock object\n                                    # to track the FPS.\n\nwhile(True):\n    clock.tick()                    # Update the FPS clock.\n    img = sensor.snapshot()         # Take a picture and return\n                                    # the image.\n    print(clock.fps())\nIn GitHub, you can find the Python scripts used here.\nThe code can be split into two parts:\n\nSetup: Where the libraries are imported, initialized and the variables are defined and initiated.\nLoop: (while loop) part of the code that runs continually. The image (img variable) is captured (one frame). Each of those frames can be used for inference in Machine Learning Applications.\n\nTo interrupt the program execution, press the red [X] button.\n\nNote: OpenMV Cam runs about half as fast when connected to the IDE. The FPS should increase once disconnected.\n\nIn the GitHub, You can find other Python scripts. Try to test the onboard sensors.",
    "crumbs": [
      "Arduino Nicla Vision",
      "Setup"
    ]
  },
  {
    "objectID": "contents/arduino/nicla_vision/setup/setup.html#sec-setup-connecting-nicla-vision-edge-impulse-studio-2a12",
    "href": "contents/arduino/nicla_vision/setup/setup.html#sec-setup-connecting-nicla-vision-edge-impulse-studio-2a12",
    "title": "Setup",
    "section": "",
    "text": "We will need the Edge Impulse Studio later in other labs. Edge Impulse is a leading development platform for machine learning on edge devices.\nEdge Impulse officially supports the Nicla Vision. So, to start, please create a new project on the Studio and connect the Nicla to it. For that, follow the steps:\n\nDownload the Arduino CLI for your specific computer architecture (OS)\nDownload the most updated EI Firmware.\nUnzip both files and place all the files in the same folder.\nPut the Nicla-Vision on Boot Mode, pressing the reset button twice.\nRun the uploader (EI FW) corresponding to your OS:\n\n \n\nExecuting the specific batch code for your OS will upload the binary arduino-nicla-vision.bin to your board.\n\n\nUsing Chrome, WebUSB can be used to connect the Nicla to the EI Studio. The EI CLI is not needed.\n\nGo to your project on the Studio, and on the Data Acquisition tab, select WebUSB (1). A window will pop up; choose the option that shows that the Nicla is paired (2) and press [Connect] (3).\n \nYou can choose which sensor data to pick in the Collect Data section on the Data Acquisition tab.\n \nFor example. IMU data (inertial):\n \nOr Image (Camera):\n \nYou can also test an external sensor connected to the ADC (Nicla pin 0) and the other onboard sensors, such as the built-in microphone, the ToF (Proximity) or a combination of sensors (fusion).",
    "crumbs": [
      "Arduino Nicla Vision",
      "Setup"
    ]
  },
  {
    "objectID": "contents/arduino/nicla_vision/setup/setup.html#sec-setup-expanding-nicla-vision-board-optional-6bc0",
    "href": "contents/arduino/nicla_vision/setup/setup.html#sec-setup-expanding-nicla-vision-board-optional-6bc0",
    "title": "Setup",
    "section": "",
    "text": "A last item to explore is that sometimes, during prototyping, it is essential to experiment with external sensors and devices. An excellent expansion to the Nicla is the Arduino MKR Connector Carrier (Grove compatible).\nThe shield has 14 Grove connectors: five single analog inputs (A0-A5), one double analog input (A5/A6), five single digital I/Os (D0-D4), one double digital I/O (D5/D6), one I2C (TWI), and one UART (Serial). All connectors are 5V compatible.\n\nNote that all 17 Nicla Vision pins will be connected to the Shield Groves, but some Grove connections remain disconnected.\n\n \nThis shield is MKR compatible and can be used with the Nicla Vision and Portenta.\n \nFor example, suppose that on a TinyML project, you want to send inference results using a LoRaWAN device and add information about local luminosity. Often, with offline operations, a local low-power display such as an OLED is advised. This setup can be seen here:\n \nThe Grove Light Sensor would be connected to one of the single Analog pins (A0/PC4), the LoRaWAN device to the UART, and the OLED to the I2C connector.\nThe Nicla Pins 3 (Tx) and 4 (Rx) are connected with the Serial Shield connector. The UART communication is used with the LoRaWan device. Here is a simple code to use the UART:\n# UART Test - By: marcelo_rovai - Sat Sep 23 2023\n\nimport time\nfrom pyb import UART\nfrom pyb import LED\n\nredLED = LED(1) # built-in red LED\n\n# Init UART object.\n# Nicla Vision's UART (TX/RX pins) is on \"LP1\"\nuart = UART(\"LP1\", 9600)\n\nwhile(True):\n    uart.write(\"Hello World!\\r\\n\")\n    redLED.toggle()\n    time.sleep_ms(1000)\nTo verify that the UART is working, you should, for example, connect another device as the Arduino UNO, displaying ‚ÄúHello Word‚Äù on the Serial Monitor. Here is the code.\n \nBelow is the Hello World code to be used with the I2C OLED. The MicroPython SSD1306 OLED driver (ssd1306.py), created by Adafruit, should also be uploaded to the Nicla (the ssd1306.py script can be found in GitHub).\n# Nicla_OLED_Hello_World - By: marcelo_rovai - Sat Sep 30 2023\n\n#Save on device: MicroPython SSD1306 OLED driver,\n# I2C and SPI interfaces created by Adafruit\nimport ssd1306\n\nfrom machine import I2C\ni2c = I2C(1)\n\noled_width = 128\noled_height = 64\noled = ssd1306.SSD1306_I2C(oled_width, oled_height, i2c)\n\noled.text('Hello, World', 10, 10)\noled.show()\nFinally, here is a simple script to read the ADC value on pin ‚ÄúPC4‚Äù (Nicla pin A0):\n\n# Light Sensor (A0) - By: marcelo_rovai - Wed Oct 4 2023\n\nimport pyb\nfrom time import sleep\n\nadc = pyb.ADC(pyb.Pin(\"PC4\"))   # create an analog object\n                                # from a pin\nval = adc.read()                # read an analog value\n\nwhile (True):\n\n    val = adc.read()\n    print (\"Light={}\".format (val))\n    sleep (1)\nThe ADC can be used for other sensor variables, such as Temperature.\n\nNote that the above scripts (downloaded from Github) introduce only how to connect external devices with the Nicla Vision board using MicroPython.",
    "crumbs": [
      "Arduino Nicla Vision",
      "Setup"
    ]
  },
  {
    "objectID": "contents/arduino/nicla_vision/setup/setup.html#sec-setup-summary-3e4b",
    "href": "contents/arduino/nicla_vision/setup/setup.html#sec-setup-summary-3e4b",
    "title": "Setup",
    "section": "",
    "text": "The Arduino Nicla Vision is an excellent tiny device for industrial and professional uses! However, it is powerful, trustworthy, low power, and has suitable sensors for the most common embedded machine learning applications such as vision, movement, sensor fusion, and sound.\n\nOn the GitHub repository, you will find the last version of all the code used or commented on in this hands-on lab.",
    "crumbs": [
      "Arduino Nicla Vision",
      "Setup"
    ]
  },
  {
    "objectID": "contents/arduino/nicla_vision/setup/setup.html#sec-setup-resources-4dce",
    "href": "contents/arduino/nicla_vision/setup/setup.html#sec-setup-resources-4dce",
    "title": "Setup",
    "section": "",
    "text": "Micropython codes\nArduino Codes",
    "crumbs": [
      "Arduino Nicla Vision",
      "Setup"
    ]
  },
  {
    "objectID": "contents/arduino/nicla_vision/object_detection/object_detection.html",
    "href": "contents/arduino/nicla_vision/object_detection/object_detection.html",
    "title": "Object Detection",
    "section": "",
    "text": "DALL¬∑E 3 Prompt: Cartoon in the style of the 1940s or 1950s showcasing a spacious industrial warehouse interior. A conveyor belt is prominently featured, carrying a mixture of toy wheels and boxes. The wheels are distinguishable with their bright yellow centers and black tires. The boxes are white cubes painted with alternating black and white patterns. At the end of the moving conveyor stands a retro-styled robot, equipped with tools and sensors, diligently classifying and counting the arriving wheels and boxes. The overall aesthetic is reminiscent of mid-century animation with bold lines and a classic color palette.\n\n\n\n\nThis continuation of Image Classification on Nicla Vision is now exploring Object Detection.\n \n\n\nThe main task with Image Classification models is to produce a list of the most probable object categories present on an image, for example, to identify a tabby cat just after his dinner:\n \nBut what happens when the cat jumps near the wine glass? The model still only recognizes the predominant category on the image, the tabby cat:\n \nAnd what happens if there is not a dominant category on the image?\n \nThe model identifies the above image utterly wrong as an ‚Äúashcan,‚Äù possibly due to the color tonalities.\n\nThe model used in all previous examples is MobileNet, which was trained with a large dataset, ImageNet.\n\nTo solve this issue, we need another type of model, where not only multiple categories (or labels) can be found but also where the objects are located on a given image.\nAs we can imagine, such models are much more complicated and bigger, for example, the MobileNetV2 SSD FPN-Lite 320x320, trained with the COCO dataset. This pre-trained object detection model is designed to locate up to 10 objects within an image, outputting a bounding box for each object detected. The below image is the result of such a model running on a Raspberry Pi:\n \nThose models used for object detection (such as the MobileNet SSD or YOLO) usually have several MB in size, which is OK for Raspberry Pi but unsuitable for use with embedded devices, where the RAM is usually lower than 1 Mbyte.\n\n\n\nEdge Impulse launched in 2022, FOMO (Faster Objects, More Objects), a novel solution for performing object detection on embedded devices, not only on the Nicla Vision (Cortex M7) but also on Cortex M4F CPUs (Arduino Nano33 and OpenMV M4 series) and the Espressif ESP32 devices (ESP-CAM and XIAO ESP32S3 Sense).\nIn this Hands-On lab, we will explore using FOMO with Object Detection, not entering many details about the model itself. To understand more about how the model works, you can go into the official FOMO announcement by Edge Impulse, where Louis Moreau and Mat Kelcey explain in detail how it works.\n\n\n\n\nAll Machine Learning projects need to start with a detailed goal. Let‚Äôs assume we are in an industrial facility and must sort and count wheels and special boxes.\n \nIn other words, we should perform a multi-label classification, where each image can have three classes:\n\nBackground (No objects)\nBox\nWheel\n\nHere are some not labeled image samples that we should use to detect the objects (wheels and boxes):\n \nWe are interested in which object is in the image, its location (centroid), and how many we can find on it. The object‚Äôs size is not detected with FOMO, as with MobileNet SSD or YOLO, where the Bounding Box is one of the model outputs.\nWe will develop the project using the Nicla Vision for image capture and model inference. The ML project will be developed using the Edge Impulse Studio. But before starting the object detection project in the Studio, let‚Äôs create a raw dataset (not labeled) with images that contain the objects to be detected.\n\n\n\nFor image capturing, we can use:\n\nWeb Serial Camera tool,\nEdge Impulse Studio,\nOpenMV IDE,\nA smartphone.\n\nHere, we will use the OpenMV IDE.\n\n\nFirst, we create a folder on the computer where the data will be saved, for example, ‚Äúdata.‚Äù Next, on the OpenMV IDE, we go to Tools &gt; Dataset Editor and select New Dataset to start the dataset collection:\n \nEdge impulse suggests that the objects should be similar in size and not overlap for better performance. This is OK in an industrial facility, where the camera should be fixed, keeping the same distance from the objects to be detected. Despite that, we will also try using mixed sizes and positions to see the result.\n\nWe will not create separate folders for our images because each contains multiple labels.\n\nConnect the Nicla Vision to the OpenMV IDE and run the dataset_capture_script.py. Clicking on the Capture Image button will start capturing images:\n \nWe suggest using around 50 images to mix the objects and vary the number of each appearing on the scene. Try to capture different angles, backgrounds, and light conditions.\n\nThe stored images use a QVGA frame size \\(320\\times 240\\) and RGB565 (color pixel format).\n\nAfter capturing your dataset, close the Dataset Editor Tool on the Tools &gt; Dataset Editor.\n\n\n\n\n\n\nGo to Edge Impulse Studio, enter your credentials at Login (or create an account), and start a new project.\n\nHere, you can clone the project developed for this hands-on: NICLA_Vision_Object_Detection.\n\nOn the Project Dashboard, go to Project info and select Bounding boxes (object detection), and at the right-top of the page, select Target, Arduino Nicla Vision (Cortex-M7).\n \n\n\n\nOn Studio, go to the Data acquisition tab, and on the UPLOAD DATA section, upload from your computer files captured.\n \n\nYou can leave for the Studio to split your data automatically between Train and Test or do it manually.\n\n \nAll the unlabeled images (51) were uploaded, but they still need to be labeled appropriately before being used as a dataset in the project. The Studio has a tool for that purpose, which you can find in the link Labeling queue (51).\nThere are two ways you can use to perform AI-assisted labeling on the Edge Impulse Studio (free version):\n\nUsing yolov5\nTracking objects between frames\n\n\nEdge Impulse launched an auto-labeling feature for Enterprise customers, easing labeling tasks in object detection projects.\n\nOrdinary objects can quickly be identified and labeled using an existing library of pre-trained object detection models from YOLOv5 (trained with the COCO dataset). But since, in our case, the objects are not part of COCO datasets, we should select the option of tracking objects. With this option, once you draw bounding boxes and label the images in one frame, the objects will be tracked automatically from frame to frame, partially labeling the new ones (not all are correctly labeled).\n\nIf you already have a labeled dataset containing bounding boxes, import your data using the EI uploader.\n\n\n\n\nStarting with the first image of your unlabeled data, use your mouse to drag a box around an object to add a label. Then click Save labels to advance to the next item.\n \nContinue with this process until the queue is empty. At the end, all images should have the objects labeled as those samples below:\n \nNext, review the labeled samples on the Data acquisition tab. If one of the labels is wrong, it can be edited using the three dots menu after the sample name:\n \nWe will be guided to replace the wrong label and correct the dataset.\n \n\n\n\n\nIn this phase, we should define how to:\n\nPre-processing consists of resizing the individual images from 320 x 240 to 96 x 96 and squashing them (squared form, without cropping). Afterward, the images are converted from RGB to Grayscale.\nDesign a Model, in this case, ‚ÄúObject Detection.‚Äù\n\n \n\n\nIn this section, select Color depth as Grayscale, suitable for use with FOMO models and Save parameters.\n \nThe Studio moves automatically to the next section, Generate features, where all samples will be pre-processed, resulting in a dataset with individual \\(96\\times 96\\times 1\\) images or 9,216 features.\n \nThe feature explorer shows that all samples evidence a good separation after the feature generation.\n\nOne of the samples (46) is apparently in the wrong space, but clicking on it confirms that the labeling is correct.\n\n\n\n\n\nWe will use FOMO, an object detection model based on MobileNetV2 (alpha 0.35) designed to coarsely segment an image into a grid of background vs objects of interest (here, boxes and wheels).\nFOMO is an innovative machine learning model for object detection, which can use up to 30 times less energy and memory than traditional models like Mobilenet SSD and YOLOv5. FOMO can operate on microcontrollers with less than 200 KB of RAM. The main reason this is possible is that while other models calculate the object‚Äôs size by drawing a square around it (bounding box), FOMO ignores the size of the image, providing only the information about where the object is located in the image, by means of its centroid coordinates.\n\n\nFOMO takes the image in grayscale and divides it into blocks of pixels using a factor of 8. For the input of 96x96, the grid would be \\(12\\times 12\\) \\((96/8=12)\\). Next, FOMO will run a classifier through each pixel block to calculate the probability that there is a box or a wheel in each of them and, subsequently, determine the regions that have the highest probability of containing the object (If a pixel block has no objects, it will be classified as background). From the overlap of the final region, the FOMO provides the coordinates (related to the image dimensions) of the centroid of this region.\n \nFor training, we should select a pre-trained model. Let‚Äôs use the FOMO (Faster Objects, More Objects) MobileNetV2 0.35. This model uses around 250 KB of RAM and 80 KB of ROM (Flash), which suits well with our board since it has 1 MB of RAM and ROM.\n \nRegarding the training hyper-parameters, the model will be trained with:\n\nEpochs: 60,\nBatch size: 32\nLearning Rate: 0.001.\n\nFor validation during training, 20% of the dataset (validation_dataset) will be spared. For the remaining 80% (train_dataset), we will apply Data Augmentation, which will randomly flip, change the size and brightness of the image, and crop them, artificially increasing the number of samples on the dataset for training.\nAs a result, the model ends with an F1 score of around 91% (validation) and 93% (test data).\n\nNote that FOMO automatically added a 3rd label background to the two previously defined (box and wheel).\n\n \n\nIn object detection tasks, accuracy is generally not the primary evaluation metric. Object detection involves classifying objects and providing bounding boxes around them, making it a more complex problem than simple classification. The issue is that we do not have the bounding box, only the centroids. In short, using accuracy as a metric could be misleading and may not provide a complete understanding of how well the model is performing. Because of that, we will use the F1 score.\n\n\n\n\nSince Edge Impulse officially supports the Nicla Vision, let‚Äôs connect it to the Studio. For that, follow the steps:\n\nDownload the last EI Firmware and unzip it.\nOpen the zip file on your computer and select the uploader related to your OS\nPut the Nicla-Vision on Boot Mode, pressing the reset button twice.\nExecute the specific batch code for your OS to upload the binary (arduino-nicla-vision.bin) to your board.\n\nGo to Live classification section at EI Studio, and using webUSB, connect your Nicla Vision:\n \nOnce connected, you can use the Nicla to capture actual images to be tested by the trained model on Edge Impulse Studio.\n \nOne thing to note is that the model can produce false positives and negatives. This can be minimized by defining a proper Confidence Threshold (use the three dots menu for the setup). Try with 0.8 or more.\n\n\n\n\nSelect OpenMV Firmware on the Deploy Tab and press [Build].\n \nWhen you try to connect the Nicla with the OpenMV IDE again, it will try to update its FW. Choose the option Load a specific firmware instead. Or go to `Tools &gt; Runs Bootloader (Load Firmware).\n \nYou will find a ZIP file on your computer from the Studio. Open it:\n \nLoad the .bin file to your board:\n \nAfter the download is finished, a pop-up message will be displayed. Press OK, and open the script ei_object_detection.py downloaded from the Studio.\n\nNote: If a Pop-up appears saying that the FW is out of date, press [NO], to upgrade it.\n\nBefore running the script, let‚Äôs change a few lines. Note that you can leave the window definition as \\(240\\times 240\\) and the camera capturing images as QVGA/RGB. The captured image will be pre-processed by the FW deployed from Edge Impulse\nimport sensor\nimport time\nimport ml\nfrom ml.utils import NMS\nimport math\nimport image\n\nsensor.reset()  # Reset and initialize the sensor.\n# Set pixel format (RGB565 or GRAYSCALE)\nsensor.set_pixformat(sensor.RGB565)\n# Set frame size to QVGA (320x240)\nsensor.set_framesize(sensor.QVGA)\nsensor.skip_frames(time=2000)  # Let the camera adjust.\nRedefine the minimum confidence, for example, to 0.8 to minimize false positives and negatives.\nmin_confidence = 0.8\nChange if necessary, the color of the circles that will be used to display the detected object‚Äôs centroid for a better contrast.\nthreshold_list = [(math.ceil(min_confidence * 255), 255)]\n\n# Load built-in model\nmodel = ml.Model(\"trained\")\nprint(model)\n\n# Alternatively, models can be loaded from the\n# filesystem storage.\n# model = ml.Model(\n#     '&lt;object_detection_modelwork&gt;.tflite',\n#     load_to_fb=True)\n# labels = [line.rstrip('\\n') for line in open(\"labels.txt\")]\n\ncolors = [ # Add more colors if you are detecting more\n           # than 7 types of classes at once.\n    (255, 255,   0), # background: yellow (not used)\n    (  0, 255,   0), # cube: green\n    (255,   0,   0), # wheel: red\n    (  0,   0, 255), # not used\n    (255,   0, 255), # not used\n    (  0, 255, 255), # not used\n    (255, 255, 255), # not used\n]\nKeep the remaining code as it is\n# FOMO outputs an image per class where each pixel in the\n# image is the centroid of the trained object. So, we will\n# get those output images and then run find_blobs() on them\n# to extract the centroids. We will also run get_stats() on\n# the detected blobs to determine their score.\n# The Non-Max-Suppression (NMS) object then filters out\n# overlapping detections and maps their position in the\n# output image back to the original input image. The\n# function then returns a list per class which each contain\n# a list of (rect, score) tuples representing the detected\n# objects.\n\n\ndef fomo_post_process(model, inputs, outputs):\n    n, oh, ow, oc = model.output_shape[0]\n    nms = NMS(ow, oh, inputs[0].roi)\n    for i in range(oc):\n        img = image.Image(outputs[0][0, :, :, i] * 255)\n        blobs = img.find_blobs(\n            threshold_list,\n            x_stride=1,\n            area_threshold=1,\n            pixels_threshold=1,\n        )\n        for b in blobs:\n            rect = b.rect()\n            x, y, w, h = rect\n            score = (\n                img.get_statistics(\n                    thresholds=threshold_list, roi=rect\n                ).l_mean()\n                / 255.0\n            )\n            nms.add_bounding_box(x, y, x + w, y + h, score, i)\n    return nms.get_bounding_boxes()\n\n\nclock = time.clock()\nwhile True:\n    clock.tick()\n\n    img = sensor.snapshot()\n\n    for i, detection_list in enumerate(\n        model.predict([img], callback=fomo_post_process)\n    ):\n        if i == 0:\n            continue  # background class\n        if len(detection_list) == 0:\n            continue  # no detections for this class?\n\n        print(\"********** %s **********\" % model.labels[i])\n        for (x, y, w, h), score in detection_list:\n            center_x = math.floor(x + (w / 2))\n            center_y = math.floor(y + (h / 2))\n            print(f\"x {center_x}\\ty {center_y}\\tscore {score}\")\n            img.draw_circle((center_x, center_y, 12), color=colors[i])\n\n    print(clock.fps(), \"fps\", end=\"\\n\")\nand press the green Play button to run the code:\n \nFrom the camera‚Äôs view, we can see the objects with their centroids marked with 12 pixel-fixed circles (each circle has a distinct color, depending on its class). On the Serial Terminal, the model shows the labels detected and their position on the image window \\((240\\times 240)\\).\n\nBe aware that the coordinate origin is in the upper left corner.\n\n \nNote that the frames per second rate is around 8 fps (similar to what we got with the Image Classification project). This happens because FOMO is cleverly built over a CNN model, not with an object detection model like the SSD MobileNet or YOLO. For example, when running a MobileNetV2 SSD FPN-Lite \\(320\\times 320\\) model on a Raspberry Pi 4, the latency is around 5 times higher (around 1.5 fps)\nHere is a short video showing the inference results: \n\n\n\nFOMO is a significant leap in the image processing space, as Louis Moreau and Mat Kelcey put it during its launch in 2022:\n\nFOMO is a ground-breaking algorithm that brings real-time object detection, tracking, and counting to microcontrollers for the first time.\n\nMultiple possibilities exist for exploring object detection (and, more precisely, counting them) on embedded devices. This can be very useful on projects counting bees, for example.\n \n\n\n\n\nEdge Impulse Project",
    "crumbs": [
      "Arduino Nicla Vision",
      "Object Detection"
    ]
  },
  {
    "objectID": "contents/arduino/nicla_vision/object_detection/object_detection.html#sec-object-detection-overview-9d59",
    "href": "contents/arduino/nicla_vision/object_detection/object_detection.html#sec-object-detection-overview-9d59",
    "title": "Object Detection",
    "section": "",
    "text": "This continuation of Image Classification on Nicla Vision is now exploring Object Detection.\n \n\n\nThe main task with Image Classification models is to produce a list of the most probable object categories present on an image, for example, to identify a tabby cat just after his dinner:\n \nBut what happens when the cat jumps near the wine glass? The model still only recognizes the predominant category on the image, the tabby cat:\n \nAnd what happens if there is not a dominant category on the image?\n \nThe model identifies the above image utterly wrong as an ‚Äúashcan,‚Äù possibly due to the color tonalities.\n\nThe model used in all previous examples is MobileNet, which was trained with a large dataset, ImageNet.\n\nTo solve this issue, we need another type of model, where not only multiple categories (or labels) can be found but also where the objects are located on a given image.\nAs we can imagine, such models are much more complicated and bigger, for example, the MobileNetV2 SSD FPN-Lite 320x320, trained with the COCO dataset. This pre-trained object detection model is designed to locate up to 10 objects within an image, outputting a bounding box for each object detected. The below image is the result of such a model running on a Raspberry Pi:\n \nThose models used for object detection (such as the MobileNet SSD or YOLO) usually have several MB in size, which is OK for Raspberry Pi but unsuitable for use with embedded devices, where the RAM is usually lower than 1 Mbyte.\n\n\n\nEdge Impulse launched in 2022, FOMO (Faster Objects, More Objects), a novel solution for performing object detection on embedded devices, not only on the Nicla Vision (Cortex M7) but also on Cortex M4F CPUs (Arduino Nano33 and OpenMV M4 series) and the Espressif ESP32 devices (ESP-CAM and XIAO ESP32S3 Sense).\nIn this Hands-On lab, we will explore using FOMO with Object Detection, not entering many details about the model itself. To understand more about how the model works, you can go into the official FOMO announcement by Edge Impulse, where Louis Moreau and Mat Kelcey explain in detail how it works.",
    "crumbs": [
      "Arduino Nicla Vision",
      "Object Detection"
    ]
  },
  {
    "objectID": "contents/arduino/nicla_vision/object_detection/object_detection.html#sec-object-detection-object-detection-project-goal-c7c4",
    "href": "contents/arduino/nicla_vision/object_detection/object_detection.html#sec-object-detection-object-detection-project-goal-c7c4",
    "title": "Object Detection",
    "section": "",
    "text": "All Machine Learning projects need to start with a detailed goal. Let‚Äôs assume we are in an industrial facility and must sort and count wheels and special boxes.\n \nIn other words, we should perform a multi-label classification, where each image can have three classes:\n\nBackground (No objects)\nBox\nWheel\n\nHere are some not labeled image samples that we should use to detect the objects (wheels and boxes):\n \nWe are interested in which object is in the image, its location (centroid), and how many we can find on it. The object‚Äôs size is not detected with FOMO, as with MobileNet SSD or YOLO, where the Bounding Box is one of the model outputs.\nWe will develop the project using the Nicla Vision for image capture and model inference. The ML project will be developed using the Edge Impulse Studio. But before starting the object detection project in the Studio, let‚Äôs create a raw dataset (not labeled) with images that contain the objects to be detected.",
    "crumbs": [
      "Arduino Nicla Vision",
      "Object Detection"
    ]
  },
  {
    "objectID": "contents/arduino/nicla_vision/object_detection/object_detection.html#sec-object-detection-data-collection-1466",
    "href": "contents/arduino/nicla_vision/object_detection/object_detection.html#sec-object-detection-data-collection-1466",
    "title": "Object Detection",
    "section": "",
    "text": "For image capturing, we can use:\n\nWeb Serial Camera tool,\nEdge Impulse Studio,\nOpenMV IDE,\nA smartphone.\n\nHere, we will use the OpenMV IDE.\n\n\nFirst, we create a folder on the computer where the data will be saved, for example, ‚Äúdata.‚Äù Next, on the OpenMV IDE, we go to Tools &gt; Dataset Editor and select New Dataset to start the dataset collection:\n \nEdge impulse suggests that the objects should be similar in size and not overlap for better performance. This is OK in an industrial facility, where the camera should be fixed, keeping the same distance from the objects to be detected. Despite that, we will also try using mixed sizes and positions to see the result.\n\nWe will not create separate folders for our images because each contains multiple labels.\n\nConnect the Nicla Vision to the OpenMV IDE and run the dataset_capture_script.py. Clicking on the Capture Image button will start capturing images:\n \nWe suggest using around 50 images to mix the objects and vary the number of each appearing on the scene. Try to capture different angles, backgrounds, and light conditions.\n\nThe stored images use a QVGA frame size \\(320\\times 240\\) and RGB565 (color pixel format).\n\nAfter capturing your dataset, close the Dataset Editor Tool on the Tools &gt; Dataset Editor.",
    "crumbs": [
      "Arduino Nicla Vision",
      "Object Detection"
    ]
  },
  {
    "objectID": "contents/arduino/nicla_vision/object_detection/object_detection.html#sec-object-detection-edge-impulse-studio-a4c2",
    "href": "contents/arduino/nicla_vision/object_detection/object_detection.html#sec-object-detection-edge-impulse-studio-a4c2",
    "title": "Object Detection",
    "section": "",
    "text": "Go to Edge Impulse Studio, enter your credentials at Login (or create an account), and start a new project.\n\nHere, you can clone the project developed for this hands-on: NICLA_Vision_Object_Detection.\n\nOn the Project Dashboard, go to Project info and select Bounding boxes (object detection), and at the right-top of the page, select Target, Arduino Nicla Vision (Cortex-M7).\n \n\n\n\nOn Studio, go to the Data acquisition tab, and on the UPLOAD DATA section, upload from your computer files captured.\n \n\nYou can leave for the Studio to split your data automatically between Train and Test or do it manually.\n\n \nAll the unlabeled images (51) were uploaded, but they still need to be labeled appropriately before being used as a dataset in the project. The Studio has a tool for that purpose, which you can find in the link Labeling queue (51).\nThere are two ways you can use to perform AI-assisted labeling on the Edge Impulse Studio (free version):\n\nUsing yolov5\nTracking objects between frames\n\n\nEdge Impulse launched an auto-labeling feature for Enterprise customers, easing labeling tasks in object detection projects.\n\nOrdinary objects can quickly be identified and labeled using an existing library of pre-trained object detection models from YOLOv5 (trained with the COCO dataset). But since, in our case, the objects are not part of COCO datasets, we should select the option of tracking objects. With this option, once you draw bounding boxes and label the images in one frame, the objects will be tracked automatically from frame to frame, partially labeling the new ones (not all are correctly labeled).\n\nIf you already have a labeled dataset containing bounding boxes, import your data using the EI uploader.\n\n\n\n\nStarting with the first image of your unlabeled data, use your mouse to drag a box around an object to add a label. Then click Save labels to advance to the next item.\n \nContinue with this process until the queue is empty. At the end, all images should have the objects labeled as those samples below:\n \nNext, review the labeled samples on the Data acquisition tab. If one of the labels is wrong, it can be edited using the three dots menu after the sample name:\n \nWe will be guided to replace the wrong label and correct the dataset.",
    "crumbs": [
      "Arduino Nicla Vision",
      "Object Detection"
    ]
  },
  {
    "objectID": "contents/arduino/nicla_vision/object_detection/object_detection.html#sec-object-detection-impulse-design-22bf",
    "href": "contents/arduino/nicla_vision/object_detection/object_detection.html#sec-object-detection-impulse-design-22bf",
    "title": "Object Detection",
    "section": "",
    "text": "In this phase, we should define how to:\n\nPre-processing consists of resizing the individual images from 320 x 240 to 96 x 96 and squashing them (squared form, without cropping). Afterward, the images are converted from RGB to Grayscale.\nDesign a Model, in this case, ‚ÄúObject Detection.‚Äù\n\n \n\n\nIn this section, select Color depth as Grayscale, suitable for use with FOMO models and Save parameters.\n \nThe Studio moves automatically to the next section, Generate features, where all samples will be pre-processed, resulting in a dataset with individual \\(96\\times 96\\times 1\\) images or 9,216 features.\n \nThe feature explorer shows that all samples evidence a good separation after the feature generation.\n\nOne of the samples (46) is apparently in the wrong space, but clicking on it confirms that the labeling is correct.",
    "crumbs": [
      "Arduino Nicla Vision",
      "Object Detection"
    ]
  },
  {
    "objectID": "contents/arduino/nicla_vision/object_detection/object_detection.html#sec-object-detection-model-design-training-test-c561",
    "href": "contents/arduino/nicla_vision/object_detection/object_detection.html#sec-object-detection-model-design-training-test-c561",
    "title": "Object Detection",
    "section": "",
    "text": "We will use FOMO, an object detection model based on MobileNetV2 (alpha 0.35) designed to coarsely segment an image into a grid of background vs objects of interest (here, boxes and wheels).\nFOMO is an innovative machine learning model for object detection, which can use up to 30 times less energy and memory than traditional models like Mobilenet SSD and YOLOv5. FOMO can operate on microcontrollers with less than 200 KB of RAM. The main reason this is possible is that while other models calculate the object‚Äôs size by drawing a square around it (bounding box), FOMO ignores the size of the image, providing only the information about where the object is located in the image, by means of its centroid coordinates.\n\n\nFOMO takes the image in grayscale and divides it into blocks of pixels using a factor of 8. For the input of 96x96, the grid would be \\(12\\times 12\\) \\((96/8=12)\\). Next, FOMO will run a classifier through each pixel block to calculate the probability that there is a box or a wheel in each of them and, subsequently, determine the regions that have the highest probability of containing the object (If a pixel block has no objects, it will be classified as background). From the overlap of the final region, the FOMO provides the coordinates (related to the image dimensions) of the centroid of this region.\n \nFor training, we should select a pre-trained model. Let‚Äôs use the FOMO (Faster Objects, More Objects) MobileNetV2 0.35. This model uses around 250 KB of RAM and 80 KB of ROM (Flash), which suits well with our board since it has 1 MB of RAM and ROM.\n \nRegarding the training hyper-parameters, the model will be trained with:\n\nEpochs: 60,\nBatch size: 32\nLearning Rate: 0.001.\n\nFor validation during training, 20% of the dataset (validation_dataset) will be spared. For the remaining 80% (train_dataset), we will apply Data Augmentation, which will randomly flip, change the size and brightness of the image, and crop them, artificially increasing the number of samples on the dataset for training.\nAs a result, the model ends with an F1 score of around 91% (validation) and 93% (test data).\n\nNote that FOMO automatically added a 3rd label background to the two previously defined (box and wheel).\n\n \n\nIn object detection tasks, accuracy is generally not the primary evaluation metric. Object detection involves classifying objects and providing bounding boxes around them, making it a more complex problem than simple classification. The issue is that we do not have the bounding box, only the centroids. In short, using accuracy as a metric could be misleading and may not provide a complete understanding of how well the model is performing. Because of that, we will use the F1 score.\n\n\n\n\nSince Edge Impulse officially supports the Nicla Vision, let‚Äôs connect it to the Studio. For that, follow the steps:\n\nDownload the last EI Firmware and unzip it.\nOpen the zip file on your computer and select the uploader related to your OS\nPut the Nicla-Vision on Boot Mode, pressing the reset button twice.\nExecute the specific batch code for your OS to upload the binary (arduino-nicla-vision.bin) to your board.\n\nGo to Live classification section at EI Studio, and using webUSB, connect your Nicla Vision:\n \nOnce connected, you can use the Nicla to capture actual images to be tested by the trained model on Edge Impulse Studio.\n \nOne thing to note is that the model can produce false positives and negatives. This can be minimized by defining a proper Confidence Threshold (use the three dots menu for the setup). Try with 0.8 or more.",
    "crumbs": [
      "Arduino Nicla Vision",
      "Object Detection"
    ]
  },
  {
    "objectID": "contents/arduino/nicla_vision/object_detection/object_detection.html#sec-object-detection-deploying-model-0320",
    "href": "contents/arduino/nicla_vision/object_detection/object_detection.html#sec-object-detection-deploying-model-0320",
    "title": "Object Detection",
    "section": "",
    "text": "Select OpenMV Firmware on the Deploy Tab and press [Build].\n \nWhen you try to connect the Nicla with the OpenMV IDE again, it will try to update its FW. Choose the option Load a specific firmware instead. Or go to `Tools &gt; Runs Bootloader (Load Firmware).\n \nYou will find a ZIP file on your computer from the Studio. Open it:\n \nLoad the .bin file to your board:\n \nAfter the download is finished, a pop-up message will be displayed. Press OK, and open the script ei_object_detection.py downloaded from the Studio.\n\nNote: If a Pop-up appears saying that the FW is out of date, press [NO], to upgrade it.\n\nBefore running the script, let‚Äôs change a few lines. Note that you can leave the window definition as \\(240\\times 240\\) and the camera capturing images as QVGA/RGB. The captured image will be pre-processed by the FW deployed from Edge Impulse\nimport sensor\nimport time\nimport ml\nfrom ml.utils import NMS\nimport math\nimport image\n\nsensor.reset()  # Reset and initialize the sensor.\n# Set pixel format (RGB565 or GRAYSCALE)\nsensor.set_pixformat(sensor.RGB565)\n# Set frame size to QVGA (320x240)\nsensor.set_framesize(sensor.QVGA)\nsensor.skip_frames(time=2000)  # Let the camera adjust.\nRedefine the minimum confidence, for example, to 0.8 to minimize false positives and negatives.\nmin_confidence = 0.8\nChange if necessary, the color of the circles that will be used to display the detected object‚Äôs centroid for a better contrast.\nthreshold_list = [(math.ceil(min_confidence * 255), 255)]\n\n# Load built-in model\nmodel = ml.Model(\"trained\")\nprint(model)\n\n# Alternatively, models can be loaded from the\n# filesystem storage.\n# model = ml.Model(\n#     '&lt;object_detection_modelwork&gt;.tflite',\n#     load_to_fb=True)\n# labels = [line.rstrip('\\n') for line in open(\"labels.txt\")]\n\ncolors = [ # Add more colors if you are detecting more\n           # than 7 types of classes at once.\n    (255, 255,   0), # background: yellow (not used)\n    (  0, 255,   0), # cube: green\n    (255,   0,   0), # wheel: red\n    (  0,   0, 255), # not used\n    (255,   0, 255), # not used\n    (  0, 255, 255), # not used\n    (255, 255, 255), # not used\n]\nKeep the remaining code as it is\n# FOMO outputs an image per class where each pixel in the\n# image is the centroid of the trained object. So, we will\n# get those output images and then run find_blobs() on them\n# to extract the centroids. We will also run get_stats() on\n# the detected blobs to determine their score.\n# The Non-Max-Suppression (NMS) object then filters out\n# overlapping detections and maps their position in the\n# output image back to the original input image. The\n# function then returns a list per class which each contain\n# a list of (rect, score) tuples representing the detected\n# objects.\n\n\ndef fomo_post_process(model, inputs, outputs):\n    n, oh, ow, oc = model.output_shape[0]\n    nms = NMS(ow, oh, inputs[0].roi)\n    for i in range(oc):\n        img = image.Image(outputs[0][0, :, :, i] * 255)\n        blobs = img.find_blobs(\n            threshold_list,\n            x_stride=1,\n            area_threshold=1,\n            pixels_threshold=1,\n        )\n        for b in blobs:\n            rect = b.rect()\n            x, y, w, h = rect\n            score = (\n                img.get_statistics(\n                    thresholds=threshold_list, roi=rect\n                ).l_mean()\n                / 255.0\n            )\n            nms.add_bounding_box(x, y, x + w, y + h, score, i)\n    return nms.get_bounding_boxes()\n\n\nclock = time.clock()\nwhile True:\n    clock.tick()\n\n    img = sensor.snapshot()\n\n    for i, detection_list in enumerate(\n        model.predict([img], callback=fomo_post_process)\n    ):\n        if i == 0:\n            continue  # background class\n        if len(detection_list) == 0:\n            continue  # no detections for this class?\n\n        print(\"********** %s **********\" % model.labels[i])\n        for (x, y, w, h), score in detection_list:\n            center_x = math.floor(x + (w / 2))\n            center_y = math.floor(y + (h / 2))\n            print(f\"x {center_x}\\ty {center_y}\\tscore {score}\")\n            img.draw_circle((center_x, center_y, 12), color=colors[i])\n\n    print(clock.fps(), \"fps\", end=\"\\n\")\nand press the green Play button to run the code:\n \nFrom the camera‚Äôs view, we can see the objects with their centroids marked with 12 pixel-fixed circles (each circle has a distinct color, depending on its class). On the Serial Terminal, the model shows the labels detected and their position on the image window \\((240\\times 240)\\).\n\nBe aware that the coordinate origin is in the upper left corner.\n\n \nNote that the frames per second rate is around 8 fps (similar to what we got with the Image Classification project). This happens because FOMO is cleverly built over a CNN model, not with an object detection model like the SSD MobileNet or YOLO. For example, when running a MobileNetV2 SSD FPN-Lite \\(320\\times 320\\) model on a Raspberry Pi 4, the latency is around 5 times higher (around 1.5 fps)\nHere is a short video showing the inference results:",
    "crumbs": [
      "Arduino Nicla Vision",
      "Object Detection"
    ]
  },
  {
    "objectID": "contents/arduino/nicla_vision/object_detection/object_detection.html#sec-object-detection-summary-b971",
    "href": "contents/arduino/nicla_vision/object_detection/object_detection.html#sec-object-detection-summary-b971",
    "title": "Object Detection",
    "section": "",
    "text": "FOMO is a significant leap in the image processing space, as Louis Moreau and Mat Kelcey put it during its launch in 2022:\n\nFOMO is a ground-breaking algorithm that brings real-time object detection, tracking, and counting to microcontrollers for the first time.\n\nMultiple possibilities exist for exploring object detection (and, more precisely, counting them) on embedded devices. This can be very useful on projects counting bees, for example.",
    "crumbs": [
      "Arduino Nicla Vision",
      "Object Detection"
    ]
  },
  {
    "objectID": "contents/arduino/nicla_vision/object_detection/object_detection.html#sec-object-detection-resources-93b2",
    "href": "contents/arduino/nicla_vision/object_detection/object_detection.html#sec-object-detection-resources-93b2",
    "title": "Object Detection",
    "section": "",
    "text": "Edge Impulse Project",
    "crumbs": [
      "Arduino Nicla Vision",
      "Object Detection"
    ]
  },
  {
    "objectID": "contents/arduino/nicla_vision/motion_classification/motion_classification.html",
    "href": "contents/arduino/nicla_vision/motion_classification/motion_classification.html",
    "title": "Motion Classification and Anomaly Detection",
    "section": "",
    "text": "DALL¬∑E 3 Prompt: 1950s style cartoon illustration depicting a movement research room. In the center of the room, there‚Äôs a simulated container used for transporting goods on trucks, boats, and forklifts. The container is detailed with rivets and markings typical of industrial cargo boxes. Around the container, the room is filled with vintage equipment, including an oscilloscope, various sensor arrays, and large paper rolls of recorded data. The walls are adorned with educational posters about transportation safety and logistics. The overall ambiance of the room is nostalgic and scientific, with a hint of industrial flair.\n\n\n\n\nTransportation is the backbone of global commerce. Millions of containers are transported daily via various means, such as ships, trucks, and trains, to destinations worldwide. Ensuring these containers‚Äô safe and efficient transit is a monumental task that requires leveraging modern technology, and TinyML is undoubtedly one of them.\nIn this hands-on tutorial, we will work to solve real-world problems related to transportation. We will develop a Motion Classification and Anomaly Detection system using the Arduino Nicla Vision board, the Arduino IDE, and the Edge Impulse Studio. This project will help us understand how containers experience different forces and motions during various phases of transportation, such as terrestrial and maritime transit, vertical movement via forklifts, and stationary periods in warehouses.\n\n\n\n\n\n\nTipLearning Objectives\n\n\n\n\nSetting up the Arduino Nicla Vision Board\nData Collection and Preprocessing\nBuilding the Motion Classification Model\nImplementing Anomaly Detection\nReal-world Testing and Analysis\n\n\n\nBy the end of this tutorial, you‚Äôll have a working prototype that can classify different types of motion and detect anomalies during the transportation of containers. This knowledge can be a stepping stone to more advanced projects in the burgeoning field of TinyML involving vibration.\n\n\n\nFor this project, we will use an accelerometer. As discussed in the Hands-On Tutorial, Setup Nicla Vision, the Nicla Vision Board has an onboard 6-axis IMU: 3D gyroscope and 3D accelerometer, the LSM6DSOX. Let‚Äôs verify if the LSM6DSOX IMU library is installed. If not, install it.\n \nNext, go to Examples &gt; Arduino_LSM6DSOX &gt; SimpleAccelerometer and run the accelerometer test. You can check if it works by opening the IDE Serial Monitor or Plotter. The values are in g (earth gravity), with a default range of +/- 4g:\n \n\n\nChoosing an appropriate sampling frequency is crucial for capturing the motion characteristics you‚Äôre interested in studying. The Nyquist-Shannon sampling theorem states that the sampling rate should be at least twice the highest frequency component in the signal to reconstruct it properly. In the context of motion classification and anomaly detection for transportation, the choice of sampling frequency would depend on several factors:\n\nNature of the Motion: Different types of transportation (terrestrial, maritime, etc.) may involve different ranges of motion frequencies. Faster movements may require higher sampling frequencies.\nHardware Limitations: The Arduino Nicla Vision board and any associated sensors may have limitations on how fast they can sample data.\nComputational Resources: Higher sampling rates will generate more data, which might be computationally intensive, especially critical in a TinyML environment.\nBattery Life: A higher sampling rate will consume more power. If the system is battery-operated, this is an important consideration.\nData Storage: More frequent sampling will require more storage space, another crucial consideration for embedded systems with limited memory.\n\nIn many human activity recognition tasks, sampling rates of around 50 Hz to 100 Hz are commonly used. Given that we are simulating transportation scenarios, which are generally not high-frequency events, a sampling rate in that range (50-100 Hz) might be a reasonable starting point.\nLet‚Äôs define a sketch that will allow us to capture our data with a defined sampling frequency (for example, 50 Hz):\n/*\n * Based on Edge Impulse Data Forwarder Example (Arduino)\n  - https://docs.edgeimpulse.com/docs/cli-data-forwarder\n * Developed by M.Rovai @11May23\n */\n\n/* Include ------------------------------------------- */\n#include &lt;Arduino_LSM6DSOX.h&gt;\n\n/* Constant defines ---------------------------------- */\n#define CONVERT_G_TO_MS2 9.80665f\n#define FREQUENCY_HZ        50\n#define INTERVAL_MS         (1000 / (FREQUENCY_HZ + 1))\n\nstatic unsigned long last_interval_ms = 0;\nfloat x, y, z;\n\nvoid setup() {\n  Serial.begin(9600);\n  while (!Serial);\n\n  if (!IMU.begin()) {\n    Serial.println(\"Failed to initialize IMU!\");\n    while (1);\n  }\n}\n\nvoid loop() {\n  if (millis() &gt; last_interval_ms + INTERVAL_MS) {\n    last_interval_ms = millis();\n\n    if (IMU.accelerationAvailable()) {\n      // Read raw acceleration measurements from the device\n      IMU.readAcceleration(x, y, z);\n\n      // converting to m/s2\n      float ax_m_s2 = x * CONVERT_G_TO_MS2;\n      float ay_m_s2 = y * CONVERT_G_TO_MS2;\n      float az_m_s2 = z * CONVERT_G_TO_MS2;\n\n      Serial.print(ax_m_s2);\n      Serial.print(\"\\t\");\n      Serial.print(ay_m_s2);\n      Serial.print(\"\\t\");\n      Serial.println(az_m_s2);\n    }\n  }\n}\nUploading the sketch and inspecting the Serial Monitor, we can see that we are capturing 50 samples per second.\n \n\nNote that with the Nicla board resting on a table (with the camera facing down), the \\(z\\)-axis measures around 9.8 m/s\\(^2\\), the expected earth acceleration.\n\n\n\n\n\nWe will simulate container (or better package) transportation through different scenarios to make this tutorial more relatable and practical. Using the built-in accelerometer of the Arduino Nicla Vision board, we‚Äôll capture motion data by manually simulating the conditions of:\n\nTerrestrial Transportation (by road or train)\nMaritime-associated Transportation\nVertical Movement via Fork-Lift\nStationary (Idle) period in a Warehouse\n\n \nFrom the above images, we can define for our simulation that primarily horizontal movements (\\(x\\) or \\(y\\) axis) should be associated with the ‚ÄúTerrestrial class,‚Äù Vertical movements (\\(z\\)-axis) with the ‚ÄúLift Class,‚Äù no activity with the ‚ÄúIdle class,‚Äù and movement on all three axes to Maritime class.\n \n\n\n\nFor data collection, we can have several options. In a real case, we can have our device, for example, connected directly to one container, and the data collected on a file (for example .CSV) and stored on an SD card (Via SPI connection) or an offline repo in your computer. Data can also be sent remotely to a nearby repository, such as a mobile phone, using Bluetooth (as done in this project: Sensor DataLogger). Once your dataset is collected and stored as a .CSV file, it can be uploaded to the Studio using the CSV Wizard tool.\n\nIn this video, you can learn alternative ways to send data to the Edge Impulse Studio.\n\n\n\nWe will connect the Nicla directly to the Edge Impulse Studio, which will also be used for data pre-processing, model training, testing, and deployment. For that, you have two options:\n\nDownload the latest firmware and connect it directly to the Data Collection section.\nUse the CLI Data Forwarder tool to capture sensor data from the sensor and send it to the Studio.\n\nOption 1 is more straightforward, as we saw in the Setup Nicla Vision hands-on, but option 2 will give you more flexibility regarding capturing your data, such as sampling frequency definition. Let‚Äôs do it with the last one.\nPlease create a new project on the Edge Impulse Studio (EIS) and connect the Nicla to it, following these steps:\n\nInstall the Edge Impulse CLI and the Node.js into your computer.\nUpload a sketch for data capture (the one discussed previously in this tutorial).\nUse the CLI Data Forwarder to capture data from the Nicla‚Äôs accelerometer and send it to the Studio, as shown in this diagram:\n\n \nStart the CLI Data Forwarder on your terminal, entering (if it is the first time) the following command:\n$ edge-impulse-data-forwarder --clean\nNext, enter your EI credentials and choose your project, variables (for example, accX, accY, and accZ), and device name (for example, NiclaV:\n \nGo to the Devices section on your EI Project and verify if the device is connected (the dot should be green):\n \n\nYou can clone the project developed for this hands-on: NICLA Vision Movement Classification.\n\n\n\n\nOn the Data Acquisition section, you should see that your board [NiclaV] is connected. The sensor is available: [sensor with 3 axes (accX, accY, accZ)] with a sampling frequency of [50 Hz]. The Studio suggests a sample length of [10000] ms (10 s). The last thing left is defining the sample label. Let‚Äôs start with[terrestrial]:\n \nTerrestrial (palettes in a Truck or Train), moving horizontally. Press [Start Sample]and move your device horizontally, keeping one direction over your table. After 10 s, your data will be uploaded to the studio. Here is how the sample was collected:\n \nAs expected, the movement was captured mainly in the \\(Y\\)-axis (green). In the blue, we see the \\(Z\\) axis, around -10 m/s\\(^2\\) (the Nicla has the camera facing up).\nAs discussed before, we should capture data from all four Transportation Classes. So, imagine that you have a container with a built-in accelerometer facing the following situations:\nMaritime (pallets in boats into an angry ocean). The movement is captured on all three axes:\n \nLift (Palettes being handled vertically by a Forklift). Movement captured only in the \\(Z\\)-axis:\n \nIdle (Palettes in a warehouse). No movement detected by the accelerometer:\n \nYou can capture, for example, 2 minutes (twelve samples of 10 seconds) for each of the four classes (a total of 8 minutes of data). Using the three dots menu after each one of the samples, select 2 of them, reserving them for the Test set. Alternatively, you can use the automatic Train/Test Split tool on the Danger Zone of Dashboard tab. Below, you can see the resulting dataset:\n \nOnce you have captured your dataset, you can explore it in more detail using the Data Explorer, a visual tool to find outliers or mislabeled data (helping to correct them). The data explorer first tries to extract meaningful features from your data (by applying signal processing and neural network embeddings) and then uses a dimensionality reduction algorithm such as PCA or t-SNE to map these features to a 2D space. This gives you a one-look overview of your complete dataset.\n \nIn our case, the dataset seems OK (good separation). But the PCA shows we can have issues between maritime (green) and lift (orange). This is expected, once on a boat, sometimes the movement can be only ‚Äúvertical‚Äù.\n\n\n\n\nThe next step is the definition of our Impulse, which takes the raw data and uses signal processing to extract features, passing them as the input tensor of a learning block to classify new data. Go to Impulse Design and Create Impulse. The Studio will suggest the basic design. Let‚Äôs also add a second Learning Block for Anomaly Detection.\n \nThis second model uses a K-means model. If we imagine that we could have our known classes as clusters, any sample that could not fit on that could be an outlier, an anomaly such as a container rolling out of a ship on the ocean or falling from a Forklift.\n \nThe sampling frequency should be automatically captured, if not, enter it: [50]Hz. The Studio suggests a Window Size of 2 seconds ([2000] ms) with a sliding window of [20]ms. What we are defining in this step is that we will pre-process the captured data (Time-Seres data), creating a tabular dataset features) that will be the input for a Neural Networks Classifier (DNN) and an Anomaly Detection model (K-Means), as shown below:\n \nLet‚Äôs dig into those steps and parameters to understand better what we are doing here.\n\n\nData pre-processing is extracting features from the dataset captured with the accelerometer, which involves processing and analyzing the raw data. Accelerometers measure the acceleration of an object along one or more axes (typically three, denoted as \\(X\\), \\(Y\\), and \\(Z\\)). These measurements can be used to understand various aspects of the object‚Äôs motion, such as movement patterns and vibrations.\nRaw accelerometer data can be noisy and contain errors or irrelevant information. Preprocessing steps, such as filtering and normalization, can clean and standardize the data, making it more suitable for feature extraction. In our case, we should divide the data into smaller segments or windows. This can help focus on specific events or activities within the dataset, making feature extraction more manageable and meaningful. The window size and overlap (window increase) choice depend on the application and the frequency of the events of interest. As a thumb rule, we should try to capture a couple of ‚Äúcycles of data‚Äù.\n\nWith a sampling rate (SR) of 50 Hz and a window size of 2 seconds, we will get 100 samples per axis, or 300 in total (3 axis \\(\\times\\) 2 seconds \\(\\times\\) 50 samples). We will slide this window every 200 ms, creating a larger dataset where each instance has 300 raw features.\n\n \nOnce the data is preprocessed and segmented, you can extract features that describe the motion‚Äôs characteristics. Some typical features extracted from accelerometer data include:\n\nTime-domain features describe the data‚Äôs statistical properties within each segment, such as mean, median, standard deviation, skewness, kurtosis, and zero-crossing rate.\nFrequency-domain features are obtained by transforming the data into the frequency domain using techniques like the Fast Fourier Transform (FFT). Some typical frequency-domain features include the power spectrum, spectral energy, dominant frequencies (amplitude and frequency), and spectral entropy.\nTime-frequency domain features combine the time and frequency domain information, such as the Short-Time Fourier Transform (STFT) or the Discrete Wavelet Transform (DWT). They can provide a more detailed understanding of how the signal‚Äôs frequency content changes over time.\n\nIn many cases, the number of extracted features can be large, which may lead to overfitting or increased computational complexity. Feature selection techniques, such as mutual information, correlation-based methods, or principal component analysis (PCA), can help identify the most relevant features for a given application and reduce the dimensionality of the dataset. The Studio can help with such feature importance calculations.\n\n\n\nData preprocessing is a challenging area for embedded machine learning, still, Edge Impulse helps overcome this with its digital signal processing (DSP) preprocessing step and, more specifically, the Spectral Features Block.\nOn the Studio, the collected raw dataset will be the input of a Spectral Analysis block, which is excellent for analyzing repetitive motion, such as data from accelerometers. This block will perform a DSP (Digital Signal Processing), extracting features such as FFT or Wavelets.\nFor our project, once the time signal is continuous, we should use FFT with, for example, a length of [32].\nThe per axis/channel Time Domain Statistical features are:\n\nRMS: 1 feature\nSkewness: 1 feature\nKurtosis: 1 feature\n\nThe per axis/channel Frequency Domain Spectral features are:\n\nSpectral Power: 16 features (FFT Length/2)\nSkewness: 1 feature\nKurtosis: 1 feature\n\nSo, for an FFT length of 32 points, the resulting output of the Spectral Analysis Block will be 21 features per axis (a total of 63 features).\n\nYou can learn more about how each feature is calculated by downloading the notebook Edge Impulse - Spectral Features Block Analysis TinyML under the hood: Spectral Analysis or opening it directly on Google CoLab.\n\n\n\n\nOnce we understand what the pre-processing does, it is time to finish the job. So, let‚Äôs take the raw data (time-series type) and convert it to tabular data. For that, go to the Spectral Features section on the Parameters tab, define the main parameters as discussed in the previous section ([FFT] with [32] points), and select[Save Parameters]:\n \nAt the top menu, select the Generate Features option and the Generate Features button. Each 2-second window data will be converted into one data point of 63 features.\n\nThe Feature Explorer will show those data in 2D using UMAP. Uniform Manifold Approximation and Projection (UMAP) is a dimension reduction technique that can be used for visualization similarly to t-SNE but is also applicable for general non-linear dimension reduction.\n\nThe visualization makes it possible to verify that after the feature generation, the classes present keep their excellent separation, which indicates that the classifier should work well. Optionally, you can analyze how important each one of the features is for one class compared with others.\n \n\n\n\n\nOur classifier will be a Dense Neural Network (DNN) that will have 63 neurons on its input layer, two hidden layers with 20 and 10 neurons, and an output layer with four neurons (one per each class), as shown here:\n \nAs hyperparameters, we will use a Learning Rate of [0.005], a Batch size of [32], and [20]% of data for validation for [30] epochs. After training, we can see that the accuracy is 98.5%. The cost of memory and latency is meager.\n \nFor Anomaly Detection, we will choose the suggested features that are precisely the most important ones in the Feature Extraction, plus the accZ RMS. The number of clusters will be [32], as suggested by the Studio:\n \n\n\n\nWe can verify how our model will behave with unknown data using 20% of the data left behind during the data capture phase. The result was almost 95%, which is good. You can always work to improve the results, for example, to understand what went wrong with one of the wrong results. If it is a unique situation, you can add it to the training dataset and then repeat it.\nThe default minimum threshold for a considered uncertain result is [0.6] for classification and [0.3] for anomaly. Once we have four classes (their output sum should be 1.0), you can also set up a lower threshold for a class to be considered valid (for example, 0.4). You can Set confidence thresholds on the three dots menu, besides the Classify all button.\n \nYou can also perform Live Classification with your device (which should still be connected to the Studio).\n\nBe aware that here, you will capture real data with your device and upload it to the Studio, where an inference will be taken using the trained model (But the model is NOT in your device).\n\n\n\n\nIt is time to deploy the preprocessing block and the trained model to the Nicla. The Studio will package all the needed libraries, preprocessing functions, and trained models, downloading them to your computer. You should select the option Arduino Library, and at the bottom, you can choose Quantized (Int8) or Unoptimized (float32) and [Build]. A Zip file will be created and downloaded to your computer.\n \nOn your Arduino IDE, go to the Sketch tab, select Add.ZIP Library, and Choose the.zip file downloaded by the Studio. A message will appear in the IDE Terminal: Library installed.\n\n\nNow, it is time for a real test. We will make inferences wholly disconnected from the Studio. Let‚Äôs change one of the code examples created when you deploy the Arduino Library.\nIn your Arduino IDE, go to the File/Examples tab and look for your project, and on examples, select Nicla_vision_fusion:\n \nNote that the code created by Edge Impulse considers a sensor fusion approach where the IMU (Accelerometer and Gyroscope) and the ToF are used. At the beginning of the code, you have the libraries related to our project, IMU and ToF:\n/* Includes ---------------------------------------------- */\n#include &lt;NICLA_Vision_Movement_Classification_inferencing.h&gt;\n#include &lt;Arduino_LSM6DSOX.h&gt; //IMU\n#include \"VL53L1X.h\" // ToF\n\nYou can keep the code this way for testing because the trained model will use only features pre-processed from the accelerometer. But consider that you will write your code only with the needed libraries for a real project.\n\nAnd that is it!\nYou can now upload the code to your device and proceed with the inferences. Press the Nicla [RESET] button twice to put it on boot mode (disconnect from the Studio if it is still connected), and upload the sketch to your board.\nNow you should try different movements with your board (similar to those done during data capture), observing the inference result of each class on the Serial Monitor:\n\nIdle and lift classes:\n\n \n\nMaritime and terrestrial:\n\n \nNote that in all situations above, the value of the anomaly score was smaller than 0.0. Try a new movement that was not part of the original dataset, for example, ‚Äúrolling‚Äù the Nicla, facing the camera upside-down, as a container falling from a boat or even a boat accident:\n\nAnomaly detection:\n\n \nIn this case, the anomaly is much bigger, over 1.00\n\n\n\nNow that we know the model is working since it detects the movements, we suggest that you modify the code to see the result with the NiclaV completely offline (disconnected from the PC and powered by a battery, a power bank, or an independent 5 V power supply).\nThe idea is to do the same as with the KWS project: if one specific movement is detected, a specific LED could be lit. For example, if terrestrial is detected, the Green LED will light; if maritime, the Red LED will light, if it is a lift, the Blue LED will light; and if no movement is detected (idle), the LEDs will be OFF. You can also add a condition when an anomaly is detected, in this case, for example, a white color can be used (all e LEDs light simultaneously).\n\n\n\n\n\nThe notebooks and codeused in this hands-on tutorial will be found on the GitHub repository.\n\nBefore we finish, consider that Movement Classification and Object Detection can be utilized in many applications across various domains. Here are some of the potential applications:\n\n\n\n\n\nPredictive Maintenance: Detecting anomalies in machinery motion to predict failures before they occur.\nQuality Control: Monitoring the motion of assembly lines or robotic arms for precision assessment and deviation detection from the standard motion pattern.\nWarehouse Logistics: Managing and tracking the movement of goods with automated systems that classify different types of motion and detect anomalies in handling.\n\n\n\n\n\nPatient Monitoring: Detecting falls or abnormal movements in the elderly or those with mobility issues.\nRehabilitation: Monitoring the progress of patients recovering from injuries by classifying motion patterns during physical therapy sessions.\nActivity Recognition: Classifying types of physical activity for fitness applications or patient monitoring.\n\n\n\n\n\nGesture Control: Interpreting specific motions to control devices, such as turning on lights with a hand wave.\nGaming: Enhancing gaming experiences with motion-controlled inputs.\n\n\n\n\n\nVehicle Telematics: Monitoring vehicle motion for unusual behavior such as hard braking, sharp turns, or accidents.\nCargo Monitoring: Ensuring the integrity of goods during transport by detecting unusual movements that could indicate tampering or mishandling.\n\n\n\n\n\nStructural Health Monitoring: Detecting vibrations or movements within structures that could indicate potential failures or maintenance needs.\nTraffic Management: Analyzing the flow of pedestrians or vehicles to improve urban mobility and safety.\n\n\n\n\n\nIntruder Detection: Detecting motion patterns typical of unauthorized access or other security breaches.\nWildlife Monitoring: Detecting poachers or abnormal animal movements in protected areas.\n\n\n\n\n\nEquipment Monitoring: Tracking the performance and usage of agricultural machinery.\nAnimal Behavior Analysis: Monitoring livestock movements to detect behaviors indicating health issues or stress.\n\n\n\n\n\nSeismic Activity: Detecting irregular motion patterns that precede earthquakes or other geologically relevant events.\nOceanography: Studying wave patterns or marine movements for research and safety purposes.\n\n\n\n\n\nFor real applications, as some described before, we can add a case to our device, and Eoin Jordan, from Edge Impulse, developed a great wearable and machine health case for the Nicla range of boards. It works with a 10mm magnet, 2M screws, and a 16mm strap for human and machine health use case scenarios. Here is the link: Arduino Nicla Voice and Vision Wearable Case.\n \nThe applications for motion classification and anomaly detection are extensive, and the Arduino Nicla Vision is well-suited for scenarios where low power consumption and edge processing are advantageous. Its small form factor and efficiency in processing make it an ideal choice for deploying portable and remote applications where real-time processing is crucial and connectivity may be limited.\n\n\n\n\n\nArduino Code\nEdge Impulse Spectral Features Block Colab Notebook\nEdge Impulse Project",
    "crumbs": [
      "Arduino Nicla Vision",
      "Motion Classification and Anomaly Detection"
    ]
  },
  {
    "objectID": "contents/arduino/nicla_vision/motion_classification/motion_classification.html#sec-motion-classification-anomaly-detection-overview-b1a8",
    "href": "contents/arduino/nicla_vision/motion_classification/motion_classification.html#sec-motion-classification-anomaly-detection-overview-b1a8",
    "title": "Motion Classification and Anomaly Detection",
    "section": "",
    "text": "Transportation is the backbone of global commerce. Millions of containers are transported daily via various means, such as ships, trucks, and trains, to destinations worldwide. Ensuring these containers‚Äô safe and efficient transit is a monumental task that requires leveraging modern technology, and TinyML is undoubtedly one of them.\nIn this hands-on tutorial, we will work to solve real-world problems related to transportation. We will develop a Motion Classification and Anomaly Detection system using the Arduino Nicla Vision board, the Arduino IDE, and the Edge Impulse Studio. This project will help us understand how containers experience different forces and motions during various phases of transportation, such as terrestrial and maritime transit, vertical movement via forklifts, and stationary periods in warehouses.\n\n\n\n\n\n\nTipLearning Objectives\n\n\n\n\nSetting up the Arduino Nicla Vision Board\nData Collection and Preprocessing\nBuilding the Motion Classification Model\nImplementing Anomaly Detection\nReal-world Testing and Analysis\n\n\n\nBy the end of this tutorial, you‚Äôll have a working prototype that can classify different types of motion and detect anomalies during the transportation of containers. This knowledge can be a stepping stone to more advanced projects in the burgeoning field of TinyML involving vibration.",
    "crumbs": [
      "Arduino Nicla Vision",
      "Motion Classification and Anomaly Detection"
    ]
  },
  {
    "objectID": "contents/arduino/nicla_vision/motion_classification/motion_classification.html#sec-motion-classification-anomaly-detection-imu-installation-testing-deff",
    "href": "contents/arduino/nicla_vision/motion_classification/motion_classification.html#sec-motion-classification-anomaly-detection-imu-installation-testing-deff",
    "title": "Motion Classification and Anomaly Detection",
    "section": "",
    "text": "For this project, we will use an accelerometer. As discussed in the Hands-On Tutorial, Setup Nicla Vision, the Nicla Vision Board has an onboard 6-axis IMU: 3D gyroscope and 3D accelerometer, the LSM6DSOX. Let‚Äôs verify if the LSM6DSOX IMU library is installed. If not, install it.\n \nNext, go to Examples &gt; Arduino_LSM6DSOX &gt; SimpleAccelerometer and run the accelerometer test. You can check if it works by opening the IDE Serial Monitor or Plotter. The values are in g (earth gravity), with a default range of +/- 4g:\n \n\n\nChoosing an appropriate sampling frequency is crucial for capturing the motion characteristics you‚Äôre interested in studying. The Nyquist-Shannon sampling theorem states that the sampling rate should be at least twice the highest frequency component in the signal to reconstruct it properly. In the context of motion classification and anomaly detection for transportation, the choice of sampling frequency would depend on several factors:\n\nNature of the Motion: Different types of transportation (terrestrial, maritime, etc.) may involve different ranges of motion frequencies. Faster movements may require higher sampling frequencies.\nHardware Limitations: The Arduino Nicla Vision board and any associated sensors may have limitations on how fast they can sample data.\nComputational Resources: Higher sampling rates will generate more data, which might be computationally intensive, especially critical in a TinyML environment.\nBattery Life: A higher sampling rate will consume more power. If the system is battery-operated, this is an important consideration.\nData Storage: More frequent sampling will require more storage space, another crucial consideration for embedded systems with limited memory.\n\nIn many human activity recognition tasks, sampling rates of around 50 Hz to 100 Hz are commonly used. Given that we are simulating transportation scenarios, which are generally not high-frequency events, a sampling rate in that range (50-100 Hz) might be a reasonable starting point.\nLet‚Äôs define a sketch that will allow us to capture our data with a defined sampling frequency (for example, 50 Hz):\n/*\n * Based on Edge Impulse Data Forwarder Example (Arduino)\n  - https://docs.edgeimpulse.com/docs/cli-data-forwarder\n * Developed by M.Rovai @11May23\n */\n\n/* Include ------------------------------------------- */\n#include &lt;Arduino_LSM6DSOX.h&gt;\n\n/* Constant defines ---------------------------------- */\n#define CONVERT_G_TO_MS2 9.80665f\n#define FREQUENCY_HZ        50\n#define INTERVAL_MS         (1000 / (FREQUENCY_HZ + 1))\n\nstatic unsigned long last_interval_ms = 0;\nfloat x, y, z;\n\nvoid setup() {\n  Serial.begin(9600);\n  while (!Serial);\n\n  if (!IMU.begin()) {\n    Serial.println(\"Failed to initialize IMU!\");\n    while (1);\n  }\n}\n\nvoid loop() {\n  if (millis() &gt; last_interval_ms + INTERVAL_MS) {\n    last_interval_ms = millis();\n\n    if (IMU.accelerationAvailable()) {\n      // Read raw acceleration measurements from the device\n      IMU.readAcceleration(x, y, z);\n\n      // converting to m/s2\n      float ax_m_s2 = x * CONVERT_G_TO_MS2;\n      float ay_m_s2 = y * CONVERT_G_TO_MS2;\n      float az_m_s2 = z * CONVERT_G_TO_MS2;\n\n      Serial.print(ax_m_s2);\n      Serial.print(\"\\t\");\n      Serial.print(ay_m_s2);\n      Serial.print(\"\\t\");\n      Serial.println(az_m_s2);\n    }\n  }\n}\nUploading the sketch and inspecting the Serial Monitor, we can see that we are capturing 50 samples per second.\n \n\nNote that with the Nicla board resting on a table (with the camera facing down), the \\(z\\)-axis measures around 9.8 m/s\\(^2\\), the expected earth acceleration.",
    "crumbs": [
      "Arduino Nicla Vision",
      "Motion Classification and Anomaly Detection"
    ]
  },
  {
    "objectID": "contents/arduino/nicla_vision/motion_classification/motion_classification.html#sec-motion-classification-anomaly-detection-case-study-simulated-container-transportation-1b32",
    "href": "contents/arduino/nicla_vision/motion_classification/motion_classification.html#sec-motion-classification-anomaly-detection-case-study-simulated-container-transportation-1b32",
    "title": "Motion Classification and Anomaly Detection",
    "section": "",
    "text": "We will simulate container (or better package) transportation through different scenarios to make this tutorial more relatable and practical. Using the built-in accelerometer of the Arduino Nicla Vision board, we‚Äôll capture motion data by manually simulating the conditions of:\n\nTerrestrial Transportation (by road or train)\nMaritime-associated Transportation\nVertical Movement via Fork-Lift\nStationary (Idle) period in a Warehouse\n\n \nFrom the above images, we can define for our simulation that primarily horizontal movements (\\(x\\) or \\(y\\) axis) should be associated with the ‚ÄúTerrestrial class,‚Äù Vertical movements (\\(z\\)-axis) with the ‚ÄúLift Class,‚Äù no activity with the ‚ÄúIdle class,‚Äù and movement on all three axes to Maritime class.",
    "crumbs": [
      "Arduino Nicla Vision",
      "Motion Classification and Anomaly Detection"
    ]
  },
  {
    "objectID": "contents/arduino/nicla_vision/motion_classification/motion_classification.html#sec-motion-classification-anomaly-detection-data-collection-aa04",
    "href": "contents/arduino/nicla_vision/motion_classification/motion_classification.html#sec-motion-classification-anomaly-detection-data-collection-aa04",
    "title": "Motion Classification and Anomaly Detection",
    "section": "",
    "text": "For data collection, we can have several options. In a real case, we can have our device, for example, connected directly to one container, and the data collected on a file (for example .CSV) and stored on an SD card (Via SPI connection) or an offline repo in your computer. Data can also be sent remotely to a nearby repository, such as a mobile phone, using Bluetooth (as done in this project: Sensor DataLogger). Once your dataset is collected and stored as a .CSV file, it can be uploaded to the Studio using the CSV Wizard tool.\n\nIn this video, you can learn alternative ways to send data to the Edge Impulse Studio.\n\n\n\nWe will connect the Nicla directly to the Edge Impulse Studio, which will also be used for data pre-processing, model training, testing, and deployment. For that, you have two options:\n\nDownload the latest firmware and connect it directly to the Data Collection section.\nUse the CLI Data Forwarder tool to capture sensor data from the sensor and send it to the Studio.\n\nOption 1 is more straightforward, as we saw in the Setup Nicla Vision hands-on, but option 2 will give you more flexibility regarding capturing your data, such as sampling frequency definition. Let‚Äôs do it with the last one.\nPlease create a new project on the Edge Impulse Studio (EIS) and connect the Nicla to it, following these steps:\n\nInstall the Edge Impulse CLI and the Node.js into your computer.\nUpload a sketch for data capture (the one discussed previously in this tutorial).\nUse the CLI Data Forwarder to capture data from the Nicla‚Äôs accelerometer and send it to the Studio, as shown in this diagram:\n\n \nStart the CLI Data Forwarder on your terminal, entering (if it is the first time) the following command:\n$ edge-impulse-data-forwarder --clean\nNext, enter your EI credentials and choose your project, variables (for example, accX, accY, and accZ), and device name (for example, NiclaV:\n \nGo to the Devices section on your EI Project and verify if the device is connected (the dot should be green):\n \n\nYou can clone the project developed for this hands-on: NICLA Vision Movement Classification.\n\n\n\n\nOn the Data Acquisition section, you should see that your board [NiclaV] is connected. The sensor is available: [sensor with 3 axes (accX, accY, accZ)] with a sampling frequency of [50 Hz]. The Studio suggests a sample length of [10000] ms (10 s). The last thing left is defining the sample label. Let‚Äôs start with[terrestrial]:\n \nTerrestrial (palettes in a Truck or Train), moving horizontally. Press [Start Sample]and move your device horizontally, keeping one direction over your table. After 10 s, your data will be uploaded to the studio. Here is how the sample was collected:\n \nAs expected, the movement was captured mainly in the \\(Y\\)-axis (green). In the blue, we see the \\(Z\\) axis, around -10 m/s\\(^2\\) (the Nicla has the camera facing up).\nAs discussed before, we should capture data from all four Transportation Classes. So, imagine that you have a container with a built-in accelerometer facing the following situations:\nMaritime (pallets in boats into an angry ocean). The movement is captured on all three axes:\n \nLift (Palettes being handled vertically by a Forklift). Movement captured only in the \\(Z\\)-axis:\n \nIdle (Palettes in a warehouse). No movement detected by the accelerometer:\n \nYou can capture, for example, 2 minutes (twelve samples of 10 seconds) for each of the four classes (a total of 8 minutes of data). Using the three dots menu after each one of the samples, select 2 of them, reserving them for the Test set. Alternatively, you can use the automatic Train/Test Split tool on the Danger Zone of Dashboard tab. Below, you can see the resulting dataset:\n \nOnce you have captured your dataset, you can explore it in more detail using the Data Explorer, a visual tool to find outliers or mislabeled data (helping to correct them). The data explorer first tries to extract meaningful features from your data (by applying signal processing and neural network embeddings) and then uses a dimensionality reduction algorithm such as PCA or t-SNE to map these features to a 2D space. This gives you a one-look overview of your complete dataset.\n \nIn our case, the dataset seems OK (good separation). But the PCA shows we can have issues between maritime (green) and lift (orange). This is expected, once on a boat, sometimes the movement can be only ‚Äúvertical‚Äù.",
    "crumbs": [
      "Arduino Nicla Vision",
      "Motion Classification and Anomaly Detection"
    ]
  },
  {
    "objectID": "contents/arduino/nicla_vision/motion_classification/motion_classification.html#sec-motion-classification-anomaly-detection-impulse-design-076e",
    "href": "contents/arduino/nicla_vision/motion_classification/motion_classification.html#sec-motion-classification-anomaly-detection-impulse-design-076e",
    "title": "Motion Classification and Anomaly Detection",
    "section": "",
    "text": "The next step is the definition of our Impulse, which takes the raw data and uses signal processing to extract features, passing them as the input tensor of a learning block to classify new data. Go to Impulse Design and Create Impulse. The Studio will suggest the basic design. Let‚Äôs also add a second Learning Block for Anomaly Detection.\n \nThis second model uses a K-means model. If we imagine that we could have our known classes as clusters, any sample that could not fit on that could be an outlier, an anomaly such as a container rolling out of a ship on the ocean or falling from a Forklift.\n \nThe sampling frequency should be automatically captured, if not, enter it: [50]Hz. The Studio suggests a Window Size of 2 seconds ([2000] ms) with a sliding window of [20]ms. What we are defining in this step is that we will pre-process the captured data (Time-Seres data), creating a tabular dataset features) that will be the input for a Neural Networks Classifier (DNN) and an Anomaly Detection model (K-Means), as shown below:\n \nLet‚Äôs dig into those steps and parameters to understand better what we are doing here.\n\n\nData pre-processing is extracting features from the dataset captured with the accelerometer, which involves processing and analyzing the raw data. Accelerometers measure the acceleration of an object along one or more axes (typically three, denoted as \\(X\\), \\(Y\\), and \\(Z\\)). These measurements can be used to understand various aspects of the object‚Äôs motion, such as movement patterns and vibrations.\nRaw accelerometer data can be noisy and contain errors or irrelevant information. Preprocessing steps, such as filtering and normalization, can clean and standardize the data, making it more suitable for feature extraction. In our case, we should divide the data into smaller segments or windows. This can help focus on specific events or activities within the dataset, making feature extraction more manageable and meaningful. The window size and overlap (window increase) choice depend on the application and the frequency of the events of interest. As a thumb rule, we should try to capture a couple of ‚Äúcycles of data‚Äù.\n\nWith a sampling rate (SR) of 50 Hz and a window size of 2 seconds, we will get 100 samples per axis, or 300 in total (3 axis \\(\\times\\) 2 seconds \\(\\times\\) 50 samples). We will slide this window every 200 ms, creating a larger dataset where each instance has 300 raw features.\n\n \nOnce the data is preprocessed and segmented, you can extract features that describe the motion‚Äôs characteristics. Some typical features extracted from accelerometer data include:\n\nTime-domain features describe the data‚Äôs statistical properties within each segment, such as mean, median, standard deviation, skewness, kurtosis, and zero-crossing rate.\nFrequency-domain features are obtained by transforming the data into the frequency domain using techniques like the Fast Fourier Transform (FFT). Some typical frequency-domain features include the power spectrum, spectral energy, dominant frequencies (amplitude and frequency), and spectral entropy.\nTime-frequency domain features combine the time and frequency domain information, such as the Short-Time Fourier Transform (STFT) or the Discrete Wavelet Transform (DWT). They can provide a more detailed understanding of how the signal‚Äôs frequency content changes over time.\n\nIn many cases, the number of extracted features can be large, which may lead to overfitting or increased computational complexity. Feature selection techniques, such as mutual information, correlation-based methods, or principal component analysis (PCA), can help identify the most relevant features for a given application and reduce the dimensionality of the dataset. The Studio can help with such feature importance calculations.\n\n\n\nData preprocessing is a challenging area for embedded machine learning, still, Edge Impulse helps overcome this with its digital signal processing (DSP) preprocessing step and, more specifically, the Spectral Features Block.\nOn the Studio, the collected raw dataset will be the input of a Spectral Analysis block, which is excellent for analyzing repetitive motion, such as data from accelerometers. This block will perform a DSP (Digital Signal Processing), extracting features such as FFT or Wavelets.\nFor our project, once the time signal is continuous, we should use FFT with, for example, a length of [32].\nThe per axis/channel Time Domain Statistical features are:\n\nRMS: 1 feature\nSkewness: 1 feature\nKurtosis: 1 feature\n\nThe per axis/channel Frequency Domain Spectral features are:\n\nSpectral Power: 16 features (FFT Length/2)\nSkewness: 1 feature\nKurtosis: 1 feature\n\nSo, for an FFT length of 32 points, the resulting output of the Spectral Analysis Block will be 21 features per axis (a total of 63 features).\n\nYou can learn more about how each feature is calculated by downloading the notebook Edge Impulse - Spectral Features Block Analysis TinyML under the hood: Spectral Analysis or opening it directly on Google CoLab.\n\n\n\n\nOnce we understand what the pre-processing does, it is time to finish the job. So, let‚Äôs take the raw data (time-series type) and convert it to tabular data. For that, go to the Spectral Features section on the Parameters tab, define the main parameters as discussed in the previous section ([FFT] with [32] points), and select[Save Parameters]:\n \nAt the top menu, select the Generate Features option and the Generate Features button. Each 2-second window data will be converted into one data point of 63 features.\n\nThe Feature Explorer will show those data in 2D using UMAP. Uniform Manifold Approximation and Projection (UMAP) is a dimension reduction technique that can be used for visualization similarly to t-SNE but is also applicable for general non-linear dimension reduction.\n\nThe visualization makes it possible to verify that after the feature generation, the classes present keep their excellent separation, which indicates that the classifier should work well. Optionally, you can analyze how important each one of the features is for one class compared with others.",
    "crumbs": [
      "Arduino Nicla Vision",
      "Motion Classification and Anomaly Detection"
    ]
  },
  {
    "objectID": "contents/arduino/nicla_vision/motion_classification/motion_classification.html#sec-motion-classification-anomaly-detection-models-training-3795",
    "href": "contents/arduino/nicla_vision/motion_classification/motion_classification.html#sec-motion-classification-anomaly-detection-models-training-3795",
    "title": "Motion Classification and Anomaly Detection",
    "section": "",
    "text": "Our classifier will be a Dense Neural Network (DNN) that will have 63 neurons on its input layer, two hidden layers with 20 and 10 neurons, and an output layer with four neurons (one per each class), as shown here:\n \nAs hyperparameters, we will use a Learning Rate of [0.005], a Batch size of [32], and [20]% of data for validation for [30] epochs. After training, we can see that the accuracy is 98.5%. The cost of memory and latency is meager.\n \nFor Anomaly Detection, we will choose the suggested features that are precisely the most important ones in the Feature Extraction, plus the accZ RMS. The number of clusters will be [32], as suggested by the Studio:",
    "crumbs": [
      "Arduino Nicla Vision",
      "Motion Classification and Anomaly Detection"
    ]
  },
  {
    "objectID": "contents/arduino/nicla_vision/motion_classification/motion_classification.html#sec-motion-classification-anomaly-detection-testing-5598",
    "href": "contents/arduino/nicla_vision/motion_classification/motion_classification.html#sec-motion-classification-anomaly-detection-testing-5598",
    "title": "Motion Classification and Anomaly Detection",
    "section": "",
    "text": "We can verify how our model will behave with unknown data using 20% of the data left behind during the data capture phase. The result was almost 95%, which is good. You can always work to improve the results, for example, to understand what went wrong with one of the wrong results. If it is a unique situation, you can add it to the training dataset and then repeat it.\nThe default minimum threshold for a considered uncertain result is [0.6] for classification and [0.3] for anomaly. Once we have four classes (their output sum should be 1.0), you can also set up a lower threshold for a class to be considered valid (for example, 0.4). You can Set confidence thresholds on the three dots menu, besides the Classify all button.\n \nYou can also perform Live Classification with your device (which should still be connected to the Studio).\n\nBe aware that here, you will capture real data with your device and upload it to the Studio, where an inference will be taken using the trained model (But the model is NOT in your device).",
    "crumbs": [
      "Arduino Nicla Vision",
      "Motion Classification and Anomaly Detection"
    ]
  },
  {
    "objectID": "contents/arduino/nicla_vision/motion_classification/motion_classification.html#sec-motion-classification-anomaly-detection-deploy-1e77",
    "href": "contents/arduino/nicla_vision/motion_classification/motion_classification.html#sec-motion-classification-anomaly-detection-deploy-1e77",
    "title": "Motion Classification and Anomaly Detection",
    "section": "",
    "text": "It is time to deploy the preprocessing block and the trained model to the Nicla. The Studio will package all the needed libraries, preprocessing functions, and trained models, downloading them to your computer. You should select the option Arduino Library, and at the bottom, you can choose Quantized (Int8) or Unoptimized (float32) and [Build]. A Zip file will be created and downloaded to your computer.\n \nOn your Arduino IDE, go to the Sketch tab, select Add.ZIP Library, and Choose the.zip file downloaded by the Studio. A message will appear in the IDE Terminal: Library installed.\n\n\nNow, it is time for a real test. We will make inferences wholly disconnected from the Studio. Let‚Äôs change one of the code examples created when you deploy the Arduino Library.\nIn your Arduino IDE, go to the File/Examples tab and look for your project, and on examples, select Nicla_vision_fusion:\n \nNote that the code created by Edge Impulse considers a sensor fusion approach where the IMU (Accelerometer and Gyroscope) and the ToF are used. At the beginning of the code, you have the libraries related to our project, IMU and ToF:\n/* Includes ---------------------------------------------- */\n#include &lt;NICLA_Vision_Movement_Classification_inferencing.h&gt;\n#include &lt;Arduino_LSM6DSOX.h&gt; //IMU\n#include \"VL53L1X.h\" // ToF\n\nYou can keep the code this way for testing because the trained model will use only features pre-processed from the accelerometer. But consider that you will write your code only with the needed libraries for a real project.\n\nAnd that is it!\nYou can now upload the code to your device and proceed with the inferences. Press the Nicla [RESET] button twice to put it on boot mode (disconnect from the Studio if it is still connected), and upload the sketch to your board.\nNow you should try different movements with your board (similar to those done during data capture), observing the inference result of each class on the Serial Monitor:\n\nIdle and lift classes:\n\n \n\nMaritime and terrestrial:\n\n \nNote that in all situations above, the value of the anomaly score was smaller than 0.0. Try a new movement that was not part of the original dataset, for example, ‚Äúrolling‚Äù the Nicla, facing the camera upside-down, as a container falling from a boat or even a boat accident:\n\nAnomaly detection:\n\n \nIn this case, the anomaly is much bigger, over 1.00\n\n\n\nNow that we know the model is working since it detects the movements, we suggest that you modify the code to see the result with the NiclaV completely offline (disconnected from the PC and powered by a battery, a power bank, or an independent 5 V power supply).\nThe idea is to do the same as with the KWS project: if one specific movement is detected, a specific LED could be lit. For example, if terrestrial is detected, the Green LED will light; if maritime, the Red LED will light, if it is a lift, the Blue LED will light; and if no movement is detected (idle), the LEDs will be OFF. You can also add a condition when an anomaly is detected, in this case, for example, a white color can be used (all e LEDs light simultaneously).",
    "crumbs": [
      "Arduino Nicla Vision",
      "Motion Classification and Anomaly Detection"
    ]
  },
  {
    "objectID": "contents/arduino/nicla_vision/motion_classification/motion_classification.html#sec-motion-classification-anomaly-detection-summary-b36e",
    "href": "contents/arduino/nicla_vision/motion_classification/motion_classification.html#sec-motion-classification-anomaly-detection-summary-b36e",
    "title": "Motion Classification and Anomaly Detection",
    "section": "",
    "text": "The notebooks and codeused in this hands-on tutorial will be found on the GitHub repository.\n\nBefore we finish, consider that Movement Classification and Object Detection can be utilized in many applications across various domains. Here are some of the potential applications:\n\n\n\n\n\nPredictive Maintenance: Detecting anomalies in machinery motion to predict failures before they occur.\nQuality Control: Monitoring the motion of assembly lines or robotic arms for precision assessment and deviation detection from the standard motion pattern.\nWarehouse Logistics: Managing and tracking the movement of goods with automated systems that classify different types of motion and detect anomalies in handling.\n\n\n\n\n\nPatient Monitoring: Detecting falls or abnormal movements in the elderly or those with mobility issues.\nRehabilitation: Monitoring the progress of patients recovering from injuries by classifying motion patterns during physical therapy sessions.\nActivity Recognition: Classifying types of physical activity for fitness applications or patient monitoring.\n\n\n\n\n\nGesture Control: Interpreting specific motions to control devices, such as turning on lights with a hand wave.\nGaming: Enhancing gaming experiences with motion-controlled inputs.\n\n\n\n\n\nVehicle Telematics: Monitoring vehicle motion for unusual behavior such as hard braking, sharp turns, or accidents.\nCargo Monitoring: Ensuring the integrity of goods during transport by detecting unusual movements that could indicate tampering or mishandling.\n\n\n\n\n\nStructural Health Monitoring: Detecting vibrations or movements within structures that could indicate potential failures or maintenance needs.\nTraffic Management: Analyzing the flow of pedestrians or vehicles to improve urban mobility and safety.\n\n\n\n\n\nIntruder Detection: Detecting motion patterns typical of unauthorized access or other security breaches.\nWildlife Monitoring: Detecting poachers or abnormal animal movements in protected areas.\n\n\n\n\n\nEquipment Monitoring: Tracking the performance and usage of agricultural machinery.\nAnimal Behavior Analysis: Monitoring livestock movements to detect behaviors indicating health issues or stress.\n\n\n\n\n\nSeismic Activity: Detecting irregular motion patterns that precede earthquakes or other geologically relevant events.\nOceanography: Studying wave patterns or marine movements for research and safety purposes.\n\n\n\n\n\nFor real applications, as some described before, we can add a case to our device, and Eoin Jordan, from Edge Impulse, developed a great wearable and machine health case for the Nicla range of boards. It works with a 10mm magnet, 2M screws, and a 16mm strap for human and machine health use case scenarios. Here is the link: Arduino Nicla Voice and Vision Wearable Case.\n \nThe applications for motion classification and anomaly detection are extensive, and the Arduino Nicla Vision is well-suited for scenarios where low power consumption and edge processing are advantageous. Its small form factor and efficiency in processing make it an ideal choice for deploying portable and remote applications where real-time processing is crucial and connectivity may be limited.",
    "crumbs": [
      "Arduino Nicla Vision",
      "Motion Classification and Anomaly Detection"
    ]
  },
  {
    "objectID": "contents/arduino/nicla_vision/motion_classification/motion_classification.html#sec-motion-classification-anomaly-detection-resources-a2eb",
    "href": "contents/arduino/nicla_vision/motion_classification/motion_classification.html#sec-motion-classification-anomaly-detection-resources-a2eb",
    "title": "Motion Classification and Anomaly Detection",
    "section": "",
    "text": "Arduino Code\nEdge Impulse Spectral Features Block Colab Notebook\nEdge Impulse Project",
    "crumbs": [
      "Arduino Nicla Vision",
      "Motion Classification and Anomaly Detection"
    ]
  },
  {
    "objectID": "contents/ide-setup.html",
    "href": "contents/ide-setup.html",
    "title": "IDE Setup",
    "section": "",
    "text": "Setting up your development environment is a critical first step that determines your success throughout the laboratory sequence. Unlike cloud-based ML development where infrastructure is abstracted away, embedded systems require understanding the complete toolchain from code compilation to hardware deployment.\nEnvironment setup typically takes 30-60 minutes, depending on platform choice and internet speed. These procedures are designed for students with no prior embedded systems experience.\n\n\nBefore beginning installation, verify your development computer meets these requirements:\nDevelopment Computer:\n\nOperating System: Windows 10/11, macOS 10.15+, or Linux (Ubuntu 18.04+)\nMemory: 8GB RAM minimum (16GB recommended for Raspberry Pi development)\nStorage: 10GB free space for development tools and libraries\nUSB Ports: At least one USB 2.0/3.0 port for device connection\nInternet Connection: Required for software installation and library downloads\n\nSoftware Prerequisites:\n\nArduino IDE 2.0+ for Arduino-based platforms (XIAO, Nicla Vision)\nPython 3.8+ for Raspberry Pi development\nGit for version control and example code access\nText Editor/IDE such as VS Code or PyCharm\n\nHardware Accessories:\n\nUSB cables: USB-C or Micro-USB (must support data transfer, not power-only)\nSD Card: 32GB+ Class 10 for Raspberry Pi\nPower adapters: Appropriate for each platform\nCamera modules: Included with most kits or available separately\n\n\n\n\nEach hardware platform demands different development approaches that mirror real-world embedded engineering practices. Arduino-based systems focus on resource efficiency and real-time constraints, Raspberry Pi demonstrates comprehensive edge computing capabilities, while specialized AI hardware highlights dedicated acceleration techniques.\nSelect the installation procedures appropriate for your chosen hardware platform.\n\n\nArduino-based embedded systems provide direct hardware control with minimal abstraction layers, making them ideal for understanding resource constraints and optimization techniques. The development environment emphasizes immediate feedback between code changes and system behavior.\nArduino IDE Installation:\n\nDownload Arduino IDE 2.0 from arduino.cc/software\nInstall following the platform-specific setup wizard\nLaunch Arduino IDE and navigate to File ‚Üí Preferences\nAdd board support URLs:\n\nFor XIAOML Kit: https://raw.githubusercontent.com/espressif/arduino-esp32/gh-pages/package_esp32_index.json\nFor Nicla Vision: URL provided in Arduino IDE Board Manager\n\n\nBoard Package Installation:\n\nOpen Tools ‚Üí Board ‚Üí Boards Manager\nSearch for your platform:\n\nXIAOML Kit: Search ‚ÄúESP32‚Äù and install ‚Äúesp32 by Espressif Systems‚Äù\nNicla Vision: Search ‚ÄúArduino Mbed OS Nicla Boards‚Äù and install\n\nSelect your board from Tools ‚Üí Board menu\nInstall required libraries via Library Manager\n\nEssential Libraries:\n\nTensorFlow Lite Micro\nPlatform-specific camera drivers\nSensor interface libraries (I2C, SPI)\n\n\n\n\nThis platform introduces hardware-accelerated AI through dedicated neural processing units, demonstrating how specialized silicon achieves performance improvements impossible with general-purpose processors. The visual programming interface showcases rapid prototyping capabilities, while traditional development environments offer more extensive customization options.\nSenseCraft AI Setup:\n\nCreate an account at sensecraft.seeed.cc\nConnect Grove Vision AI V2 via USB\nAccess the device through the SenseCraft AI web interface\nNo local software installation required for the visual programming workflow\n\nArduino IDE Setup (for custom development):\nFollow Arduino-based platform instructions above, using the Seeed Studio board package URL in the board manager.\n\n\n\nThe Raspberry Pi environment bridges embedded constraints with full computing capabilities, enabling students to experience both resource optimization and advanced ML frameworks. This dual perspective illustrates how computational resources impact algorithmic choices and system architecture decisions.\nOperating System Installation:\n\nDownload Raspberry Pi Imager\nFlash Raspberry Pi OS (64-bit recommended) to a microSD card (32GB minimum)\nConfigure SSH access and WiFi credentials during the imaging process\nInsert the flashed SD card and boot the Raspberry Pi\n\nSoftware Environment Setup:\nThe following commands establish a complete Python-based ML development environment with proper dependency management:\n# Update system packages\nsudo apt update && sudo apt upgrade -y\n\n# Install Python development tools\n# python3-pip: Python package installer\n# python3-venv: Virtual environment creation\n# python3-dev: Python development headers\nsudo apt install python3-pip \\\n                 python3-venv \\\n                 python3-dev -y\n\n# Install ML framework dependencies\n# libatlas-base-dev: Linear algebra library (BLAS/LAPACK)\n# libhdf5-dev: HDF5 data format library\n# libhdf5-serial-dev: HDF5 serial version\nsudo apt install libatlas-base-dev \\\n                 libhdf5-dev \\\n                 libhdf5-serial-dev -y\n\n# Install computer vision dependencies\n# libcamera-dev: Camera interface library\n# python3-libcamera: Python bindings for libcamera\n# python3-kms++: Kernel mode setting library\nsudo apt install libcamera-dev \\\n                 python3-libcamera \\\n                 python3-kms++ -y\n\n# Create virtual environment for projects\npython3 -m venv ~/ml_projects\nsource ~/ml_projects/bin/activate\n\n# Install core ML packages\n# tensorflow: Main ML framework\n# tensorflow-lite: Optimized for edge/mobile devices\n# opencv-python: Computer vision library\n# numpy: Numerical computing foundation\npip install tensorflow \\\n            tensorflow-lite \\\n            opencv-python \\\n            numpy\n\n\n\n\nProper tool configuration ensures reliable communication between your development workstation and embedded hardware. These settings establish the foundation for code deployment, debugging, and performance monitoring throughout the laboratory exercises.\n\n\nSerial communication provides the primary interface for debugging and data monitoring in embedded systems, offering insights into system behavior that are essential for understanding performance constraints and optimization opportunities.\nWindows:\n\nInstall appropriate USB-to-serial drivers (CH340, FTDI, or platform-specific)\nConfigure Device Manager to recognize the development board\n\nmacOS/Linux:\n\nMost USB-to-serial adapters work without additional drivers\nVerify device detection: ls /dev/tty* (macOS/Linux)\nAdd user to dialout group: sudo usermod -a -G dialout $USER (Linux)\n\n\n\n\nDevelopment environment settings directly impact the efficiency of the code-test-deploy cycle that characterizes embedded development. Proper configuration reduces debugging time and provides clear feedback about system performance.\nArduino IDE Settings:\n\nConfigure the appropriate COM port under Tools ‚Üí Port\nSet the correct board and processor selection\nVerify upload speed (typically 115200 baud)\nEnable verbose output during compilation for debugging\n\nRaspberry Pi Development:\n\nConfigure SSH keys for remote development\nInstall VS Code with Python and Remote SSH extensions\nSet up Jupyter notebook access for interactive development\n\n\n\n\n\nVerification procedures confirm that your development environment can successfully communicate with hardware and execute basic operations. These tests establish baseline functionality before proceeding to more complex laboratory exercises.\n\n\nThe following verification procedures test core functionality required for laboratory exercises, ensuring that both hardware communication and software libraries operate correctly.\nArduino Platforms:\nvoid setup() {\n  Serial.begin(115200);\n  Serial.println(\"Development environment test\");\n  Serial.print(\"Board: \");\n  Serial.println(ARDUINO_BOARD);\n}\n\nvoid loop() {\n  Serial.println(\"Environment operational\");\n  delay(1000);\n}\nRaspberry Pi:\n# Test camera interface\nlibcamera-hello --timeout 5000\n\n# Test Python ML environment\npython3 -c \\\n  \"import tensorflow as tf; print('TensorFlow:', tf.__version__)\"\npython3 -c \\\n  \"import cv2; print('OpenCV:', cv2.__version__)\"\nGrove Vision AI V2:\n\nVerify device detection in the SenseCraft AI web interface\nTest basic model deployment through visual programming interface\n\n\n\n\n\nSetup challenges are common and offer valuable learning opportunities regarding embedded system constraints and debugging techniques. The following solutions address the most frequently encountered issues during environment configuration.\nDevice Connection Problems:\n\nVerify the USB cable supports data transfer (not power-only)\nInstall platform-specific USB drivers if the device is not recognized\nTry different USB ports or USB hubs if the connection is unstable\n\nCompilation Errors:\n\nConfirm the correct board and processor selection in the IDE\nVerify all required libraries are installed with compatible versions\nCheck for sufficient disk space for the compilation process\n\nRuntime Issues:\n\nEnsure adequate power supply (especially for camera operations)\nVerify SD card compatibility and formatting (Raspberry Pi)\nCheck memory allocation for ML models within platform constraints\n\nNetwork Connectivity (WiFi-enabled platforms):\n\nConfirm network credentials and security protocols\nCheck firewall settings for development tool access\nVerify that the network allows device-to-development machine communication\n\n\n\n\nCommon Hardware Issues:\n\nDevice not recognized: Ensure the USB cable supports data transfer, try different ports\nUpload failures: Check board selection and port configuration in the IDE\nPower issues: Verify adequate power supply, especially for camera operations\nMemory errors: Confirm model size fits within platform constraints\n\nSoftware Setup Issues:\n\nLibrary conflicts: Use compatible versions specified in the setup guides\nCompilation errors: Verify all dependencies are installed correctly\nNetwork connectivity: Check firewall settings and network permissions\n\nPlatform-Specific Resources:\n\nXIAOML Kit: Seeed Studio Documentation\n\nXIAO ESP32S3 Series documentation\n\nArduino Nicla Vision: Arduino Documentation\nGrove Vision AI V2: SenseCraft AI Platform\nRaspberry Pi: Official Documentation\n\nCommunity Support:\n\nGitHub Issues: Report bugs and request features through the project repository\nDiscussion Forums: Platform-specific communities on Arduino, Raspberry Pi, and Seeed Studio websites\nStack Overflow: Tag questions with appropriate platform tags for community assistance\n\n\n\n\nWith your development environment configured and verified, you have established the foundational tools needed for embedded ML programming. The skills developed during environment setup‚Äîunderstanding toolchains, managing dependencies, and verifying system functionality‚Äîapply throughout all subsequent laboratory work.\nYour configured environment now supports the entire development workflow, from algorithm implementation to hardware deployment and performance optimization. The Laboratory Overview offers exercise categories organized by complexity and learning objectives, designed to systematically build on these foundational capabilities.\nRecommended starting sequence:\n\nBegin with basic sensor exercises to verify hardware functionality\nProgress to single-modality ML applications (image or audio)\nAdvance to multi-modal and optimization exercises\n\nEach laboratory exercise includes detailed implementation procedures, expected performance benchmarks, and troubleshooting guidance specific to the project requirements. The development environment you have established provides the foundation for exploring the complete spectrum of embedded ML applications and optimization techniques.",
    "crumbs": [
      "IDE Setup"
    ]
  },
  {
    "objectID": "contents/ide-setup.html#sec-ide-setup-system-requirements",
    "href": "contents/ide-setup.html#sec-ide-setup-system-requirements",
    "title": "IDE Setup",
    "section": "",
    "text": "Before beginning installation, verify your development computer meets these requirements:\nDevelopment Computer:\n\nOperating System: Windows 10/11, macOS 10.15+, or Linux (Ubuntu 18.04+)\nMemory: 8GB RAM minimum (16GB recommended for Raspberry Pi development)\nStorage: 10GB free space for development tools and libraries\nUSB Ports: At least one USB 2.0/3.0 port for device connection\nInternet Connection: Required for software installation and library downloads\n\nSoftware Prerequisites:\n\nArduino IDE 2.0+ for Arduino-based platforms (XIAO, Nicla Vision)\nPython 3.8+ for Raspberry Pi development\nGit for version control and example code access\nText Editor/IDE such as VS Code or PyCharm\n\nHardware Accessories:\n\nUSB cables: USB-C or Micro-USB (must support data transfer, not power-only)\nSD Card: 32GB+ Class 10 for Raspberry Pi\nPower adapters: Appropriate for each platform\nCamera modules: Included with most kits or available separately",
    "crumbs": [
      "IDE Setup"
    ]
  },
  {
    "objectID": "contents/ide-setup.html#sec-ide-setup-platformspecific-software-installation-f432",
    "href": "contents/ide-setup.html#sec-ide-setup-platformspecific-software-installation-f432",
    "title": "IDE Setup",
    "section": "",
    "text": "Each hardware platform demands different development approaches that mirror real-world embedded engineering practices. Arduino-based systems focus on resource efficiency and real-time constraints, Raspberry Pi demonstrates comprehensive edge computing capabilities, while specialized AI hardware highlights dedicated acceleration techniques.\nSelect the installation procedures appropriate for your chosen hardware platform.\n\n\nArduino-based embedded systems provide direct hardware control with minimal abstraction layers, making them ideal for understanding resource constraints and optimization techniques. The development environment emphasizes immediate feedback between code changes and system behavior.\nArduino IDE Installation:\n\nDownload Arduino IDE 2.0 from arduino.cc/software\nInstall following the platform-specific setup wizard\nLaunch Arduino IDE and navigate to File ‚Üí Preferences\nAdd board support URLs:\n\nFor XIAOML Kit: https://raw.githubusercontent.com/espressif/arduino-esp32/gh-pages/package_esp32_index.json\nFor Nicla Vision: URL provided in Arduino IDE Board Manager\n\n\nBoard Package Installation:\n\nOpen Tools ‚Üí Board ‚Üí Boards Manager\nSearch for your platform:\n\nXIAOML Kit: Search ‚ÄúESP32‚Äù and install ‚Äúesp32 by Espressif Systems‚Äù\nNicla Vision: Search ‚ÄúArduino Mbed OS Nicla Boards‚Äù and install\n\nSelect your board from Tools ‚Üí Board menu\nInstall required libraries via Library Manager\n\nEssential Libraries:\n\nTensorFlow Lite Micro\nPlatform-specific camera drivers\nSensor interface libraries (I2C, SPI)\n\n\n\n\nThis platform introduces hardware-accelerated AI through dedicated neural processing units, demonstrating how specialized silicon achieves performance improvements impossible with general-purpose processors. The visual programming interface showcases rapid prototyping capabilities, while traditional development environments offer more extensive customization options.\nSenseCraft AI Setup:\n\nCreate an account at sensecraft.seeed.cc\nConnect Grove Vision AI V2 via USB\nAccess the device through the SenseCraft AI web interface\nNo local software installation required for the visual programming workflow\n\nArduino IDE Setup (for custom development):\nFollow Arduino-based platform instructions above, using the Seeed Studio board package URL in the board manager.\n\n\n\nThe Raspberry Pi environment bridges embedded constraints with full computing capabilities, enabling students to experience both resource optimization and advanced ML frameworks. This dual perspective illustrates how computational resources impact algorithmic choices and system architecture decisions.\nOperating System Installation:\n\nDownload Raspberry Pi Imager\nFlash Raspberry Pi OS (64-bit recommended) to a microSD card (32GB minimum)\nConfigure SSH access and WiFi credentials during the imaging process\nInsert the flashed SD card and boot the Raspberry Pi\n\nSoftware Environment Setup:\nThe following commands establish a complete Python-based ML development environment with proper dependency management:\n# Update system packages\nsudo apt update && sudo apt upgrade -y\n\n# Install Python development tools\n# python3-pip: Python package installer\n# python3-venv: Virtual environment creation\n# python3-dev: Python development headers\nsudo apt install python3-pip \\\n                 python3-venv \\\n                 python3-dev -y\n\n# Install ML framework dependencies\n# libatlas-base-dev: Linear algebra library (BLAS/LAPACK)\n# libhdf5-dev: HDF5 data format library\n# libhdf5-serial-dev: HDF5 serial version\nsudo apt install libatlas-base-dev \\\n                 libhdf5-dev \\\n                 libhdf5-serial-dev -y\n\n# Install computer vision dependencies\n# libcamera-dev: Camera interface library\n# python3-libcamera: Python bindings for libcamera\n# python3-kms++: Kernel mode setting library\nsudo apt install libcamera-dev \\\n                 python3-libcamera \\\n                 python3-kms++ -y\n\n# Create virtual environment for projects\npython3 -m venv ~/ml_projects\nsource ~/ml_projects/bin/activate\n\n# Install core ML packages\n# tensorflow: Main ML framework\n# tensorflow-lite: Optimized for edge/mobile devices\n# opencv-python: Computer vision library\n# numpy: Numerical computing foundation\npip install tensorflow \\\n            tensorflow-lite \\\n            opencv-python \\\n            numpy",
    "crumbs": [
      "IDE Setup"
    ]
  },
  {
    "objectID": "contents/ide-setup.html#sec-ide-setup-development-tool-configuration-4ab1",
    "href": "contents/ide-setup.html#sec-ide-setup-development-tool-configuration-4ab1",
    "title": "IDE Setup",
    "section": "",
    "text": "Proper tool configuration ensures reliable communication between your development workstation and embedded hardware. These settings establish the foundation for code deployment, debugging, and performance monitoring throughout the laboratory exercises.\n\n\nSerial communication provides the primary interface for debugging and data monitoring in embedded systems, offering insights into system behavior that are essential for understanding performance constraints and optimization opportunities.\nWindows:\n\nInstall appropriate USB-to-serial drivers (CH340, FTDI, or platform-specific)\nConfigure Device Manager to recognize the development board\n\nmacOS/Linux:\n\nMost USB-to-serial adapters work without additional drivers\nVerify device detection: ls /dev/tty* (macOS/Linux)\nAdd user to dialout group: sudo usermod -a -G dialout $USER (Linux)\n\n\n\n\nDevelopment environment settings directly impact the efficiency of the code-test-deploy cycle that characterizes embedded development. Proper configuration reduces debugging time and provides clear feedback about system performance.\nArduino IDE Settings:\n\nConfigure the appropriate COM port under Tools ‚Üí Port\nSet the correct board and processor selection\nVerify upload speed (typically 115200 baud)\nEnable verbose output during compilation for debugging\n\nRaspberry Pi Development:\n\nConfigure SSH keys for remote development\nInstall VS Code with Python and Remote SSH extensions\nSet up Jupyter notebook access for interactive development",
    "crumbs": [
      "IDE Setup"
    ]
  },
  {
    "objectID": "contents/ide-setup.html#sec-ide-setup-environment-verification-11ed",
    "href": "contents/ide-setup.html#sec-ide-setup-environment-verification-11ed",
    "title": "IDE Setup",
    "section": "",
    "text": "Verification procedures confirm that your development environment can successfully communicate with hardware and execute basic operations. These tests establish baseline functionality before proceeding to more complex laboratory exercises.\n\n\nThe following verification procedures test core functionality required for laboratory exercises, ensuring that both hardware communication and software libraries operate correctly.\nArduino Platforms:\nvoid setup() {\n  Serial.begin(115200);\n  Serial.println(\"Development environment test\");\n  Serial.print(\"Board: \");\n  Serial.println(ARDUINO_BOARD);\n}\n\nvoid loop() {\n  Serial.println(\"Environment operational\");\n  delay(1000);\n}\nRaspberry Pi:\n# Test camera interface\nlibcamera-hello --timeout 5000\n\n# Test Python ML environment\npython3 -c \\\n  \"import tensorflow as tf; print('TensorFlow:', tf.__version__)\"\npython3 -c \\\n  \"import cv2; print('OpenCV:', cv2.__version__)\"\nGrove Vision AI V2:\n\nVerify device detection in the SenseCraft AI web interface\nTest basic model deployment through visual programming interface",
    "crumbs": [
      "IDE Setup"
    ]
  },
  {
    "objectID": "contents/ide-setup.html#sec-ide-setup-common-setup-issues-solutions-27a8",
    "href": "contents/ide-setup.html#sec-ide-setup-common-setup-issues-solutions-27a8",
    "title": "IDE Setup",
    "section": "",
    "text": "Setup challenges are common and offer valuable learning opportunities regarding embedded system constraints and debugging techniques. The following solutions address the most frequently encountered issues during environment configuration.\nDevice Connection Problems:\n\nVerify the USB cable supports data transfer (not power-only)\nInstall platform-specific USB drivers if the device is not recognized\nTry different USB ports or USB hubs if the connection is unstable\n\nCompilation Errors:\n\nConfirm the correct board and processor selection in the IDE\nVerify all required libraries are installed with compatible versions\nCheck for sufficient disk space for the compilation process\n\nRuntime Issues:\n\nEnsure adequate power supply (especially for camera operations)\nVerify SD card compatibility and formatting (Raspberry Pi)\nCheck memory allocation for ML models within platform constraints\n\nNetwork Connectivity (WiFi-enabled platforms):\n\nConfirm network credentials and security protocols\nCheck firewall settings for development tool access\nVerify that the network allows device-to-development machine communication",
    "crumbs": [
      "IDE Setup"
    ]
  },
  {
    "objectID": "contents/ide-setup.html#sec-ide-setup-troubleshooting-support-2f28",
    "href": "contents/ide-setup.html#sec-ide-setup-troubleshooting-support-2f28",
    "title": "IDE Setup",
    "section": "",
    "text": "Common Hardware Issues:\n\nDevice not recognized: Ensure the USB cable supports data transfer, try different ports\nUpload failures: Check board selection and port configuration in the IDE\nPower issues: Verify adequate power supply, especially for camera operations\nMemory errors: Confirm model size fits within platform constraints\n\nSoftware Setup Issues:\n\nLibrary conflicts: Use compatible versions specified in the setup guides\nCompilation errors: Verify all dependencies are installed correctly\nNetwork connectivity: Check firewall settings and network permissions\n\nPlatform-Specific Resources:\n\nXIAOML Kit: Seeed Studio Documentation\n\nXIAO ESP32S3 Series documentation\n\nArduino Nicla Vision: Arduino Documentation\nGrove Vision AI V2: SenseCraft AI Platform\nRaspberry Pi: Official Documentation\n\nCommunity Support:\n\nGitHub Issues: Report bugs and request features through the project repository\nDiscussion Forums: Platform-specific communities on Arduino, Raspberry Pi, and Seeed Studio websites\nStack Overflow: Tag questions with appropriate platform tags for community assistance",
    "crumbs": [
      "IDE Setup"
    ]
  },
  {
    "objectID": "contents/ide-setup.html#sec-ide-setup-ready-laboratory-exercises-7950",
    "href": "contents/ide-setup.html#sec-ide-setup-ready-laboratory-exercises-7950",
    "title": "IDE Setup",
    "section": "",
    "text": "With your development environment configured and verified, you have established the foundational tools needed for embedded ML programming. The skills developed during environment setup‚Äîunderstanding toolchains, managing dependencies, and verifying system functionality‚Äîapply throughout all subsequent laboratory work.\nYour configured environment now supports the entire development workflow, from algorithm implementation to hardware deployment and performance optimization. The Laboratory Overview offers exercise categories organized by complexity and learning objectives, designed to systematically build on these foundational capabilities.\nRecommended starting sequence:\n\nBegin with basic sensor exercises to verify hardware functionality\nProgress to single-modality ML applications (image or audio)\nAdvance to multi-modal and optimization exercises\n\nEach laboratory exercise includes detailed implementation procedures, expected performance benchmarks, and troubleshooting guidance specific to the project requirements. The development environment you have established provides the foundation for exploring the complete spectrum of embedded ML applications and optimization techniques.",
    "crumbs": [
      "IDE Setup"
    ]
  },
  {
    "objectID": "contents/seeed/grove_vision_ai_v2/object_detection/object_detection.html",
    "href": "contents/seeed/grove_vision_ai_v2/object_detection/object_detection.html",
    "title": "Object Detection",
    "section": "",
    "text": "Object Detection\nThis Lab is under Development\n\n\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "Grove Vision AI V2",
      "Object Detection"
    ]
  },
  {
    "objectID": "contents/seeed/grove_vision_ai_v2/image_classification/image_classification.html",
    "href": "contents/seeed/grove_vision_ai_v2/image_classification/image_classification.html",
    "title": "Image Classification",
    "section": "",
    "text": "Using Seeed Studio Grove Vision AI Module V2 (Himax WiseEye2)\n\nIn this Lab, we will explore Image Classification using the Seeed Studio Grove Vision AI Module V2, a powerful yet compact device specifically designed for embedded machine learning applications. Based on the Himax WiseEye2 chip, this module is designed to enable AI capabilities on edge devices, making it an ideal tool for Edge Machine Learning (ML) applications.\n\n\nSo far, we have explored several computer vision models previously uploaded by Seeed Studio or used the SenseCraft AI Studio for Image Classification, without choosing a specific model. Let‚Äôs now develop our Image Classification project from scratch, where we will select our data and model.\nBelow, we can see the project‚Äôs main steps and where we will work with them:\n\n\n\n\n\n\n\nThe first step in any machine learning (ML) project is defining the goal. In this case, the goal is to detect and classify two specific objects present in a single image. For this project, we will use two small toys: a robot and a small Brazilian parrot (named Periquito). Also, we will collect images of a background where those two objects are absent.\n\n\n\n\n\n\n\n\nWith the Machine Learning project goal defined, dataset collection is the next and most crucial step. Suppose your project utilizes images that are publicly available on datasets, for example, to be used on a Person Detection project. In that case, you can download the Wake Vision dataset for use in the project.\nBut, in our case, we define a project where the images do not exist publicly, so we need to generate them. We can use a phone, computer camera, or other devices to capture the photos, offline or connected to the Edge Impulse Studio.\nIf you want to use the Grove Vision AI V2 to capture your dataset, you can use the SenseCraft AI Studio as we did in the previous Lab, or the camera_web_server sketch as we will describe later in the Postprocessing / Getting the Video Stream section of this Lab.\n\n\n\n\n\nIn this Lab, we will use the SenseCraft AI Studio to collect the dataset.\n\n\n\nOn SenseCraft AI Studio: Let‚Äôs open the tab Training.\nThe default is to train a Classification model with a WebCam if it is available. Let‚Äôs select the Grove Vision AI V2 instead. Pressing the green button[Connect] (1), a Pop-Up window will appear. Select the corresponding Port (2) and press the blue button [Connect] (3).\n\n\n\n\n\nThe image streamed from the Grove Vision AI V2 will be displayed.\n\n\nLet‚Äôs create the classes, following, for example, an alphabetical order:\n\nClass1: background\nClass 2: periquito\nClass 3: robot\n\n\n\n\n\n\nSelect one of the classes (note that a green line will be around the window) and keep pressing the green button under the preview area. The collected images will appear on the Image Samples Screen.\n\n\n\n\n\nAfter collecting the images, review them and, if necessary, delete any incorrect ones.\n\n\n\n\n\nCollect around 50 images from each class. After you collect the three classes, open the menu on each of them and select Export Data.\n\n\n\n\n\nIn the Download area of the Computer, we will get three zip files, each one with its corresponding class name. Each Zip file contains a folder with the images.\n\n\n\n\nWe will use the Edge Impulse Studio to train our model. Edge Impulse is a leading development platform for machine learning on edge devices.\n\nEnter your account credentials (or create a free account) at Edge Impulse.\nNext, create a new project:\n\n\n\n\n\n\n\nThe dataset comprises approximately 50 images per label, with 40 for training and 10 for testing.\n\n\n\n\nImpulse Design\nAn impulse takes raw data (in this case, images), extracts features (resizes pictures), and then uses a learning block to classify new data.\nClassifying images is the most common application of deep learning, but a substantial amount of data is required to accomplish this task. We have around 50 images for each category. Is this number enough? Not at all! We will need thousands of images to ‚Äúteach‚Äù or ‚Äúmodel‚Äù each class, allowing us to differentiate them. However, we can resolve this issue by retraining a previously trained model using thousands of images. We refer to this technique as ‚ÄúTransfer Learning‚Äù (TL). With TL, we can fine-tune a pre-trained image classification model on our data, achieving good performance even with relatively small image datasets, as in our case.\n\n\n\n\n\nSo, starting from the raw images, we will resize them (96x96) pixels and feed them to our Transfer Learning block:\n\n\n\n\n\n\nFor comparison, we will keep the image size as 96 x 96. However, keep in mind that with the Grove Vision AI Module V2 and its internal SRAM of 2.4 MB, larger images can be utilized (for example, 160 x 160).\n\nAlso select the Target device (Himax WiseEye2 (M55 400 MHz + U55)) on the up-right corner.\n\n\n\nBesides resizing the images, we can convert them to grayscale or retain their original RGB color depth. Let‚Äôs select [RGB] in the Image section. Doing that, each data sample will have a dimension of 27,648 features (96x96x3). Pressing [Save Parameters] will open a new tab, Generate Features. Press the button [Generate Features]to generate the features.\n\n\n\nIn 2007, Google introduced MobileNetV1. In 2018, MobileNetV2: Inverted Residuals and Linear Bottlenecks, was launched, and, in 2019, the V3. The Mobilinet is a family of general-purpose computer vision neural networks explicitly designed for mobile devices to support classification, detection, and other applications. MobileNets are small, low-latency, low-power models parameterized to meet the resource constraints of various use cases.\nAlthough the base MobileNet architecture is already compact and has low latency, a specific use case or application may often require the model to be even smaller and faster. MobileNets introduce a straightforward parameter, Œ± (alpha), called the width multiplier to construct these smaller, less computationally expensive models. The role of the width multiplier Œ± is to thin a network uniformly at each layer.\nEdge Impulse Studio has available MobileNet V1 (96x96 images) and V2 (96x96 and 160x160 images), with several different Œ± values (from 0.05 to 1.0). For example, you will get the highest accuracy with V2, 160x160 images, and Œ±=1.0. Of course, there is a trade-off. The higher the accuracy, the more memory (around 1.3M RAM and 2.6M ROM) will be needed to run the model, implying more latency. The smaller footprint will be obtained at another extreme with MobileNet V1 and Œ±=0.10 (around 53.2K RAM and 101K ROM).\n\nFor comparison, we will use the MobileNet V2 0.1 as our base model (but a model with a greater alpha can be used here). The final layer of our model, preceding the output layer, will have 8 neurons with a 10% dropout rate for preventing overfitting.\n\nAnother necessary technique to use with deep learning is data augmentation. Data augmentation is a method that can help improve the accuracy of machine learning models by creating additional artificial data. A data augmentation system makes small, random changes to your training data during the training process (such as flipping, cropping, or rotating the images).\nSet the Hyperparameters:\n\nEpochs: 20,\nBach Size: 32\nLearning Rate: 0.0005\nValidation size: 20%\n\nTraining result:\n\n\n\n\n\nThe model profile predicts 146 KB of RAM and 187 KB of Flash, indicating no problem with the Grove AI Vision (V2), which has almost 2.5 MB of internal SRAM. Additionally, the Studio indicates a latency of around 4 ms.\n\nDespite this, with a 100% accuracy on the Validation set when using the spare data for testing, we confirmed an Accuracy of 81%, using the Quantized (Int8) trained model. However, it is sufficient for our purposes in this lab.\n\n\n\n\nOn the Deployment tab, we should select: Seeed Grove Vision AI Module V2 (Himax WiseEye2) and press [Build]. A ZIP file will be downloaded to our computer.\nThe Zip file contains the model_vela.tflite, which is a TensorFlow Lite (TFLite) model optimized for neural processing units (NPUs) using the Vela compiler, a tool developed by Arm to adapt TFLite models for Ethos-U NPUs.\n\n\n\n\n\nWe can flash the model following the instructions in the README.txt or use the SenseCraft AI Studio. We will use the latter.\n\n\n\nOn SenseCraft AI Studio, go to the Vision Workspace tab, and connect the device:\n\n\n\n\n\nYou should see the last model that was uploaded to the device. Select the green button [Upload Model]. A pop-up window will ask for the model name, the model file, and to enter the class names (objects). We should use labels following alphabetical order: 0: background, 1: periquito, and 2: robot, and then press [Send].\n\n\n\n\n\nAfter a few seconds, the model will be uploaded (‚Äúflashed‚Äù) to our device, and the camera image will appear in real-time on the Preview Sector. The Classification result will be displayed under the image preview. It is also possible to select the Confidence Threshold of your inference using the cursor on Settings.\nOn the Device Logger, we can view the Serial Monitor, where we can observe the latency, which is approximately 1 to 2 ms for pre-processing and 4 to 5 ms for inference, aligning with the estimates made in Edge Impulse Studio.\n\n\n\n\n\nHere are other screenshots:\n\n\n\n\n\nThe power consumption of this model is approximately 70 mA, equivalent to 0.4 W.\n\n\n\nSeveral development boards can be used for embedded machine learning (tinyML), and the most common ones (so far) for Computer Vision applications (with low energy) are the ESP32 CAM, the Seeed XIAO ESP32S3 Sense, and the Arduino Nicla Vision.\nTaking advantage of this opportunity, a similarly trained model, MobilenetV2 96x96, with an alpha of 0.1, was also deployed on the ESP-CAM, the XIAO, and a Raspberry Pi Zero W2. Here is the result:\n\n\n\n\n\n\nThe Grove Vision AI V2 with an ARM Ethos-U55 was approximately 14 times faster than devices with an ARM-M7, and more than 100 times faster than an Xtensa LX6 (ESP-CAM). Even when compared to a Raspberry Pi, with a much more powerful CPU, the U55 reduces latency by almost half. Additionally, the power consumption is lower than that of other devices (see the full article here for power consumption comparison).\n\n\n\n\nNow that we have the model uploaded to the board and working correctly, classifying our images, let‚Äôs connect a Master Device to export the inference result to it and see the result completely offline (disconnected from the PC and, for example, powered by a battery).\n\nNote that we can use any microcontroller as a Master Controller, such as the XIAO, Arduino, or Raspberry Pi.\n\n\n\nThe image processing and model inference are processed locally in Grove Vision AI (V2), and we want the result to be output to the XIAO (Master Controller) via IIC. For that, we will use the Arduino SSMA library. This library‚Äôs primary purpose is to process Grove Vision AI‚Äôs data stream, which does not involve model inference.\n\nThe Grove Vision AI (V2) communicates (Inference result) with the XIAO via the IIC; the device‚Äôs IIC address is 0x62. Image information transfer is via the USB serial port.\n\nStep 1: Download the Arduino SSMA library as a zip file from its GitHub:\n\n\n\n\n\nStep 2: Install it in the Arduino IDE (sketch &gt; Include Library &gt; Add .Zip Library).\nStep 3: Install the ArduinoJSON library.\n\n\n\n\n\nStep 4: Install the Eigen Library\n\n\n\n\n\nStep 3: Now, connect the XIAO and Grove Vision AI (V2) via the socket (a row of pins) located at the back of the device.\n\n\n\n\n\n\nCAUTION: Please note the direction of the connection, Grove Vision AI‚Äôs Type-C connector should be in the same direction as XIAO‚Äôs Type-C connector.\n\nStep 5: Connect the XIAO USB-C port to your computer\n\n\n\n\n\nStep 6: In the Arduino IDE, select the Xiao board and the corresponding USB port.\nOnce we want to stream the video to a webpage, we will use the XIAO ESP32S3, which has wifi and enough memory to handle images. Select XIAO_ESP32S3 and the appropriate USB Port:\n\n\n\n\n\nBy default, the PSRAM is disabled. Open the Tools menu and on PSRAM: \"OPI PSRAM\"select OPI PSRAM.\n\n\n\n\n\nStep 7: Open the example in Arduino IDE:\nFile -&gt; Examples -&gt; Seeed_Arduino_SSCMA -&gt; camera_web_server.\nAnd edit the ssid and password in the camera_web_server.ino sketch to match the Wi-Fi network.\nStep 8: Upload the sketch to the board and open the Serial Monitor. When connected to the Wi-Fi network, the board‚Äôs IP address will be displayed.\n\n\n\n\n\nOpen the address using a web browser. A Video App will be available. To see only the video stream from the Grove Vision AI V2, press [Sample Only] and [Start Stream].\n\n\n\n\n\nIf you want to create an image dataset, you can use this app, saving frames of the video generated by the device. Pressing [Save Frame], the image will be saved in the download area of our desktop.\n\n\n\n\n\nOpening the App without selecting [Sample Only], the inference result should appear on the video screen, but this does not happen for Image Classification. For Object Detection or Pose Estimation, the result is embedded with the video stream.\nFor example, if the model is a Person Detection using YoloV8:\n\n\n\n\n\n\n\n\n\nGo to File -&gt; Examples -&gt; Seeed_Arduino_SSCMA -&gt; inference_class.\nUpload the sketch to the board, and open the Serial Monitor.\nPointing the camera at one of our objects, we can see the inference result on the Serial Terminal.\n\n\n\n\n\n\n\nThe inference running on the Arduino IDE had an average consumption of 160 mA or 800 mW and a peak of 330 mA 1.65 W when transmitting the image to the App.\n\n\n\n\nThe idea behind our postprocessing is that whenever a specific image is detected (for example, the Periquito - Label:1), the User LED is turned on. If the Robot or a background is detected, the LED will be off.\nCopy the below code and past it to your IDE:\n#include &lt;Seeed_Arduino_SSCMA.h&gt;\nSSCMA AI;\n\nvoid setup()\n{\n    AI.begin();\n\n    Serial.begin(115200);\n    while (!Serial);\n    Serial.println(\"Inferencing - Grove AI V2 / XIAO ESP32S3\");\n\n    // Pins for the built-in LED\n    pinMode(LED_BUILTIN, OUTPUT);\n    // Ensure the LED is OFF by default.\n    // Note: The LED is ON when the pin is LOW, OFF when HIGH.\n    digitalWrite(LED_BUILTIN, HIGH);\n}\n\nvoid loop()\n{\n    if (!AI.invoke()){\n        Serial.println(\"\\nInvoke Success\");\n        Serial.print(\"Latency [ms]: prepocess=\");\n        Serial.print(AI.perf().prepocess);\n        Serial.print(\", inference=\");\n        Serial.print(AI.perf().inference);\n        Serial.print(\", postpocess=\");\n        Serial.println(AI.perf().postprocess);\n        int pred_index = AI.classes()[0].target;\n        Serial.print(\"Result= Label: \");\n        Serial.print(pred_index);\n        Serial.print(\", score=\");\n        Serial.println(AI.classes()[0].score);\n        turn_on_led(pred_index);\n    }\n}\n\n/**\n* @brief      turn_off_led function - turn-off the User LED\n*/\nvoid turn_off_led(){\n    digitalWrite(LED_BUILTIN, HIGH);\n}\n\n/**\n* @brief      turn_on_led function used to turn on the User LED\n* @param[in]  pred_index\n*             label 0: [0] ==&gt; ALL OFF\n*             label 1: [1] ==&gt; LED ON\n*             label 2: [2] ==&gt; ALL OFF\n*             label 3: [3] ==&gt; ALL OFF\n*/\nvoid turn_on_led(int pred_index) {\n    switch (pred_index)\n    {\n        case 0:\n            turn_off_led();\n            break;\n        case 1:\n            turn_off_led();\n            digitalWrite(LED_BUILTIN, LOW);\n            break;\n        case 2:\n            turn_off_led();\n            break;\n        case 3:\n            turn_off_led();\n            break;\n    }\n}\nThis sketch uses the Seeed_Arduino_SSCMA.h library to interface with the Grove Vision AI Module V2. The AI module and the LED are initialized in the setup() function, and serial communication is started.\nThe loop() function repeatedly calls the invoke() method to perform inference using the built-in algorithms of the Grove Vision AI Module V2. Upon a successful inference, the sketch prints out performance metrics to the serial monitor, including preprocessing, inference, and postprocessing times.\nThe sketch processes and prints out detailed information about the results of the inference:\n\n(AI.classes()[0]) that identifies the class of image (.target) and its confidence score (.score).\nThe inference result (class) is stored in the integer variable pred_index, which will be used as an input to the function turn_on_led(). As a result, the LED will turn ON, depending on the classification result.\n\nHere is the result:\nIf the Periquito is detected (Label:1), the LED is ON:\n\n\n\n\n\nIf the Robot is detected (Label:2) the LED is OFF (Same for Background (Label:0):\n\n\n\n\n\nTherefore, we can now power the Grove Vision AI V2 + Xiao ESP32S3 with an external battery, and the inference result will be displayed by the LED completely offline. The consumption is approximately 165 mA or 825 mW.\n\nIt is also possible to send the result using Wifi, BLE, or other communication protocols available on the used Master Device.\n\n\n\n\n\nOf course, one of the significant advantages of working with EdgeAI is that devices can run entirely disconnected from the cloud, allowing for seamless interactions with the real world. We did it in the last section, but using the internal Xiao LED. Now, we will connect external LEDs (which could be any actuator).\n\n\n\n\n\n\nThe LEDS should be connected to the XIAO ground via a 220-ohm resistor.\n\n\n\n\n\n\nThe idea is to modify the previous sketch to handle the three external LEDs.\nGOAL: Whenever the image of a Periquito is detected, the LED Green will be ON; if it is a Robot, the LED Yellow will be ON; if it is a Background, the LED Red will be ON.\nThe image processing and model inference are processed locally in Grove Vision AI (V2), and we want the result to be output to the XIAO via IIC. For that, we will use the Arduino SSMA library again.\nHere the sketch to be used:\n#include &lt;Seeed_Arduino_SSCMA.h&gt;\nSSCMA AI;\n\n// Define the LED pin according to the pin diagram\n// The LEDS negative lead should be connected to the XIAO ground\n// via a 220-ohm resistor.\nint LEDR = D1; # XIAO ESP32S3 Pin 1\nint LEDY = D2; # XIAO ESP32S3 Pin 2\nint LEDG = D3; # XIAO ESP32S3 Pin 3\n\n  void setup()\n{\n    AI.begin();\n\n    Serial.begin(115200);\n    while (!Serial);\n    Serial.println(\"Inferencing - Grove AI V2 / XIAO ESP32S3\");\n\n// Initialize the external LEDs\n    pinMode(LEDR, OUTPUT);\n    pinMode(LEDY, OUTPUT);\n    pinMode(LEDG, OUTPUT);\n    // Ensure the LEDs are OFF by default.\n    // Note: The LEDs are ON when the pin is HIGH, OFF when LOW.\n    digitalWrite(LEDR, LOW);\n    digitalWrite(LEDY, LOW);\n    digitalWrite(LEDG, LOW);\n}\n\nvoid loop()\n{\n    if (!AI.invoke()){\n        Serial.println(\"\\nInvoke Success\");\n        Serial.print(\"Latency [ms]: prepocess=\");\n        Serial.print(AI.perf().prepocess);\n        Serial.print(\", inference=\");\n        Serial.print(AI.perf().inference);\n        Serial.print(\", postpocess=\");\n        Serial.println(AI.perf().postprocess);\n        int pred_index = AI.classes()[0].target;\n        Serial.print(\"Result= Label: \");\n        Serial.print(pred_index);\n        Serial.print(\", score=\");\n        Serial.println(AI.classes()[0].score);\n        turn_on_leds(pred_index);\n    }\n}\n\n/**\n* @brief turn_off_leds function - turn-off all LEDs\n*/\nvoid turn_off_leds(){\n    digitalWrite(LEDR, LOW);\n    digitalWrite(LEDY, LOW);\n    digitalWrite(LEDG, LOW);\n}\n\n/**\n* @brief turn_on_leds function used to turn on a specific LED\n* @param[in]  pred_index\n*             label 0: [0] ==&gt; Red ON\n*             label 1: [1] ==&gt; Green ON\n*             label 2: [2] ==&gt; Yellow ON\n*/\nvoid turn_on_leds(int pred_index) {\n    switch (pred_index)\n    {\n        case 0:\n            turn_off_leds();\n            digitalWrite(LEDR, HIGH);\n            break;\n        case 1:\n            turn_off_leds();\n            digitalWrite(LEDG, HIGH);\n            break;\n        case 2:\n            turn_off_leds();\n            digitalWrite(LEDY, HIGH);\n            break;\n        case 3:\n            turn_off_leds();\n            break;\n    }\n}\nWe should connect the Grove Vision AI V2 with the XIAO using its I2C Grove connector. For the XIAO, we will use an Expansion Board for the facility (although it is possible to connect the I2C directly to the XIAO‚Äôs pins). We will power the boards using the USB-C connector, but a battery can also be used.\n\n\n\n\n\nHere is the result:\n\n\n\n\n\n\nThe power consumption reached a peak of 240 mA (Green LED), equivalent to 1.2 W. Driving the Yellow and Red LEDs consumes 14 mA, equivalent to 0.7 W. Sending information to the terminal via serial has no impact on power consumption.\n\n\n\n\n\nIn this lab, we‚Äôve explored the complete process of developing an image classification system using the Seeed Studio Grove Vision AI Module V2 powered by the Himax WiseEye2 chip. We‚Äôve walked through every stage of the machine learning workflow, from defining our project goals to deploying a working model with real-world interactions.\nThe Grove Vision AI V2 has demonstrated impressive performance, with inference times of just 4-5ms, dramatically outperforming other common tinyML platforms. Our benchmark comparison showed it to be approximately 14 times faster than ARM-M7 devices and over 100 times faster than an Xtensa LX6 (ESP-CAM). Even when compared to a Raspberry Pi Zero W2, the Edge TPU architecture delivered nearly twice the speed while consuming less power.\nThrough this project, we‚Äôve seen how transfer learning enables us to achieve good classification results with a relatively small dataset of custom images. The MobileNetV2 model with an alpha of 0.1 provided an excellent balance of accuracy and efficiency for our three-class problem, requiring only 146 KB of RAM and 187 KB of Flash memory, well within the capabilities of the Grove Vision AI Module V2‚Äôs 2.4 MB internal SRAM.\nWe also explored several deployment options, from viewing inference results through the SenseCraft AI Studio to creating a standalone system with visual feedback using LEDs. The ability to stream video to a web browser and process inference results locally demonstrates the versatility of edge AI systems for real-world applications.\nThe power consumption of our final system remained impressively low, ranging from approximately 70mA (0.4W) for basic inference to 240mA (1.2W) when driving external components. This efficiency makes the Grove Vision AI Module V2 an excellent choice for battery-powered applications where power consumption is critical.\nThis lab has demonstrated that sophisticated computer vision tasks can now be performed entirely at the edge, without reliance on cloud services or powerful computers. With tools like Edge Impulse Studio and SenseCraft AI Studio, the development process has become accessible even to those without extensive machine learning expertise.\nAs edge AI technology continues to evolve, we can expect even more powerful capabilities from compact, energy-efficient devices like the Grove Vision AI Module V2, opening up new possibilities for smart sensors, IoT applications, and embedded intelligence in everyday objects.\n\n\n\nCollecting Images with SenseCraft AI Studio.\nEdge Impulse Studio Project\nSenseCraft AI Studio - Vision Workplace (Deploy Models)\nOther Himax examples\nArduino Sketches",
    "crumbs": [
      "Grove Vision AI V2",
      "Image Classification"
    ]
  },
  {
    "objectID": "contents/seeed/grove_vision_ai_v2/image_classification/image_classification.html#sec-image-classification-introduction-59d5",
    "href": "contents/seeed/grove_vision_ai_v2/image_classification/image_classification.html#sec-image-classification-introduction-59d5",
    "title": "Image Classification",
    "section": "",
    "text": "So far, we have explored several computer vision models previously uploaded by Seeed Studio or used the SenseCraft AI Studio for Image Classification, without choosing a specific model. Let‚Äôs now develop our Image Classification project from scratch, where we will select our data and model.\nBelow, we can see the project‚Äôs main steps and where we will work with them:\n\n\n\n\n\n\n\nThe first step in any machine learning (ML) project is defining the goal. In this case, the goal is to detect and classify two specific objects present in a single image. For this project, we will use two small toys: a robot and a small Brazilian parrot (named Periquito). Also, we will collect images of a background where those two objects are absent.\n\n\n\n\n\n\n\n\nWith the Machine Learning project goal defined, dataset collection is the next and most crucial step. Suppose your project utilizes images that are publicly available on datasets, for example, to be used on a Person Detection project. In that case, you can download the Wake Vision dataset for use in the project.\nBut, in our case, we define a project where the images do not exist publicly, so we need to generate them. We can use a phone, computer camera, or other devices to capture the photos, offline or connected to the Edge Impulse Studio.\nIf you want to use the Grove Vision AI V2 to capture your dataset, you can use the SenseCraft AI Studio as we did in the previous Lab, or the camera_web_server sketch as we will describe later in the Postprocessing / Getting the Video Stream section of this Lab.\n\n\n\n\n\nIn this Lab, we will use the SenseCraft AI Studio to collect the dataset.\n\n\n\nOn SenseCraft AI Studio: Let‚Äôs open the tab Training.\nThe default is to train a Classification model with a WebCam if it is available. Let‚Äôs select the Grove Vision AI V2 instead. Pressing the green button[Connect] (1), a Pop-Up window will appear. Select the corresponding Port (2) and press the blue button [Connect] (3).\n\n\n\n\n\nThe image streamed from the Grove Vision AI V2 will be displayed.\n\n\nLet‚Äôs create the classes, following, for example, an alphabetical order:\n\nClass1: background\nClass 2: periquito\nClass 3: robot\n\n\n\n\n\n\nSelect one of the classes (note that a green line will be around the window) and keep pressing the green button under the preview area. The collected images will appear on the Image Samples Screen.\n\n\n\n\n\nAfter collecting the images, review them and, if necessary, delete any incorrect ones.\n\n\n\n\n\nCollect around 50 images from each class. After you collect the three classes, open the menu on each of them and select Export Data.\n\n\n\n\n\nIn the Download area of the Computer, we will get three zip files, each one with its corresponding class name. Each Zip file contains a folder with the images.\n\n\n\n\nWe will use the Edge Impulse Studio to train our model. Edge Impulse is a leading development platform for machine learning on edge devices.\n\nEnter your account credentials (or create a free account) at Edge Impulse.\nNext, create a new project:\n\n\n\n\n\n\n\nThe dataset comprises approximately 50 images per label, with 40 for training and 10 for testing.\n\n\n\n\nImpulse Design\nAn impulse takes raw data (in this case, images), extracts features (resizes pictures), and then uses a learning block to classify new data.\nClassifying images is the most common application of deep learning, but a substantial amount of data is required to accomplish this task. We have around 50 images for each category. Is this number enough? Not at all! We will need thousands of images to ‚Äúteach‚Äù or ‚Äúmodel‚Äù each class, allowing us to differentiate them. However, we can resolve this issue by retraining a previously trained model using thousands of images. We refer to this technique as ‚ÄúTransfer Learning‚Äù (TL). With TL, we can fine-tune a pre-trained image classification model on our data, achieving good performance even with relatively small image datasets, as in our case.\n\n\n\n\n\nSo, starting from the raw images, we will resize them (96x96) pixels and feed them to our Transfer Learning block:\n\n\n\n\n\n\nFor comparison, we will keep the image size as 96 x 96. However, keep in mind that with the Grove Vision AI Module V2 and its internal SRAM of 2.4 MB, larger images can be utilized (for example, 160 x 160).\n\nAlso select the Target device (Himax WiseEye2 (M55 400 MHz + U55)) on the up-right corner.\n\n\n\nBesides resizing the images, we can convert them to grayscale or retain their original RGB color depth. Let‚Äôs select [RGB] in the Image section. Doing that, each data sample will have a dimension of 27,648 features (96x96x3). Pressing [Save Parameters] will open a new tab, Generate Features. Press the button [Generate Features]to generate the features.\n\n\n\nIn 2007, Google introduced MobileNetV1. In 2018, MobileNetV2: Inverted Residuals and Linear Bottlenecks, was launched, and, in 2019, the V3. The Mobilinet is a family of general-purpose computer vision neural networks explicitly designed for mobile devices to support classification, detection, and other applications. MobileNets are small, low-latency, low-power models parameterized to meet the resource constraints of various use cases.\nAlthough the base MobileNet architecture is already compact and has low latency, a specific use case or application may often require the model to be even smaller and faster. MobileNets introduce a straightforward parameter, Œ± (alpha), called the width multiplier to construct these smaller, less computationally expensive models. The role of the width multiplier Œ± is to thin a network uniformly at each layer.\nEdge Impulse Studio has available MobileNet V1 (96x96 images) and V2 (96x96 and 160x160 images), with several different Œ± values (from 0.05 to 1.0). For example, you will get the highest accuracy with V2, 160x160 images, and Œ±=1.0. Of course, there is a trade-off. The higher the accuracy, the more memory (around 1.3M RAM and 2.6M ROM) will be needed to run the model, implying more latency. The smaller footprint will be obtained at another extreme with MobileNet V1 and Œ±=0.10 (around 53.2K RAM and 101K ROM).\n\nFor comparison, we will use the MobileNet V2 0.1 as our base model (but a model with a greater alpha can be used here). The final layer of our model, preceding the output layer, will have 8 neurons with a 10% dropout rate for preventing overfitting.\n\nAnother necessary technique to use with deep learning is data augmentation. Data augmentation is a method that can help improve the accuracy of machine learning models by creating additional artificial data. A data augmentation system makes small, random changes to your training data during the training process (such as flipping, cropping, or rotating the images).\nSet the Hyperparameters:\n\nEpochs: 20,\nBach Size: 32\nLearning Rate: 0.0005\nValidation size: 20%\n\nTraining result:\n\n\n\n\n\nThe model profile predicts 146 KB of RAM and 187 KB of Flash, indicating no problem with the Grove AI Vision (V2), which has almost 2.5 MB of internal SRAM. Additionally, the Studio indicates a latency of around 4 ms.\n\nDespite this, with a 100% accuracy on the Validation set when using the spare data for testing, we confirmed an Accuracy of 81%, using the Quantized (Int8) trained model. However, it is sufficient for our purposes in this lab.\n\n\n\n\nOn the Deployment tab, we should select: Seeed Grove Vision AI Module V2 (Himax WiseEye2) and press [Build]. A ZIP file will be downloaded to our computer.\nThe Zip file contains the model_vela.tflite, which is a TensorFlow Lite (TFLite) model optimized for neural processing units (NPUs) using the Vela compiler, a tool developed by Arm to adapt TFLite models for Ethos-U NPUs.\n\n\n\n\n\nWe can flash the model following the instructions in the README.txt or use the SenseCraft AI Studio. We will use the latter.\n\n\n\nOn SenseCraft AI Studio, go to the Vision Workspace tab, and connect the device:\n\n\n\n\n\nYou should see the last model that was uploaded to the device. Select the green button [Upload Model]. A pop-up window will ask for the model name, the model file, and to enter the class names (objects). We should use labels following alphabetical order: 0: background, 1: periquito, and 2: robot, and then press [Send].\n\n\n\n\n\nAfter a few seconds, the model will be uploaded (‚Äúflashed‚Äù) to our device, and the camera image will appear in real-time on the Preview Sector. The Classification result will be displayed under the image preview. It is also possible to select the Confidence Threshold of your inference using the cursor on Settings.\nOn the Device Logger, we can view the Serial Monitor, where we can observe the latency, which is approximately 1 to 2 ms for pre-processing and 4 to 5 ms for inference, aligning with the estimates made in Edge Impulse Studio.\n\n\n\n\n\nHere are other screenshots:\n\n\n\n\n\nThe power consumption of this model is approximately 70 mA, equivalent to 0.4 W.\n\n\n\nSeveral development boards can be used for embedded machine learning (tinyML), and the most common ones (so far) for Computer Vision applications (with low energy) are the ESP32 CAM, the Seeed XIAO ESP32S3 Sense, and the Arduino Nicla Vision.\nTaking advantage of this opportunity, a similarly trained model, MobilenetV2 96x96, with an alpha of 0.1, was also deployed on the ESP-CAM, the XIAO, and a Raspberry Pi Zero W2. Here is the result:\n\n\n\n\n\n\nThe Grove Vision AI V2 with an ARM Ethos-U55 was approximately 14 times faster than devices with an ARM-M7, and more than 100 times faster than an Xtensa LX6 (ESP-CAM). Even when compared to a Raspberry Pi, with a much more powerful CPU, the U55 reduces latency by almost half. Additionally, the power consumption is lower than that of other devices (see the full article here for power consumption comparison).\n\n\n\n\nNow that we have the model uploaded to the board and working correctly, classifying our images, let‚Äôs connect a Master Device to export the inference result to it and see the result completely offline (disconnected from the PC and, for example, powered by a battery).\n\nNote that we can use any microcontroller as a Master Controller, such as the XIAO, Arduino, or Raspberry Pi.\n\n\n\nThe image processing and model inference are processed locally in Grove Vision AI (V2), and we want the result to be output to the XIAO (Master Controller) via IIC. For that, we will use the Arduino SSMA library. This library‚Äôs primary purpose is to process Grove Vision AI‚Äôs data stream, which does not involve model inference.\n\nThe Grove Vision AI (V2) communicates (Inference result) with the XIAO via the IIC; the device‚Äôs IIC address is 0x62. Image information transfer is via the USB serial port.\n\nStep 1: Download the Arduino SSMA library as a zip file from its GitHub:\n\n\n\n\n\nStep 2: Install it in the Arduino IDE (sketch &gt; Include Library &gt; Add .Zip Library).\nStep 3: Install the ArduinoJSON library.\n\n\n\n\n\nStep 4: Install the Eigen Library\n\n\n\n\n\nStep 3: Now, connect the XIAO and Grove Vision AI (V2) via the socket (a row of pins) located at the back of the device.\n\n\n\n\n\n\nCAUTION: Please note the direction of the connection, Grove Vision AI‚Äôs Type-C connector should be in the same direction as XIAO‚Äôs Type-C connector.\n\nStep 5: Connect the XIAO USB-C port to your computer\n\n\n\n\n\nStep 6: In the Arduino IDE, select the Xiao board and the corresponding USB port.\nOnce we want to stream the video to a webpage, we will use the XIAO ESP32S3, which has wifi and enough memory to handle images. Select XIAO_ESP32S3 and the appropriate USB Port:\n\n\n\n\n\nBy default, the PSRAM is disabled. Open the Tools menu and on PSRAM: \"OPI PSRAM\"select OPI PSRAM.\n\n\n\n\n\nStep 7: Open the example in Arduino IDE:\nFile -&gt; Examples -&gt; Seeed_Arduino_SSCMA -&gt; camera_web_server.\nAnd edit the ssid and password in the camera_web_server.ino sketch to match the Wi-Fi network.\nStep 8: Upload the sketch to the board and open the Serial Monitor. When connected to the Wi-Fi network, the board‚Äôs IP address will be displayed.\n\n\n\n\n\nOpen the address using a web browser. A Video App will be available. To see only the video stream from the Grove Vision AI V2, press [Sample Only] and [Start Stream].\n\n\n\n\n\nIf you want to create an image dataset, you can use this app, saving frames of the video generated by the device. Pressing [Save Frame], the image will be saved in the download area of our desktop.\n\n\n\n\n\nOpening the App without selecting [Sample Only], the inference result should appear on the video screen, but this does not happen for Image Classification. For Object Detection or Pose Estimation, the result is embedded with the video stream.\nFor example, if the model is a Person Detection using YoloV8:\n\n\n\n\n\n\n\n\n\nGo to File -&gt; Examples -&gt; Seeed_Arduino_SSCMA -&gt; inference_class.\nUpload the sketch to the board, and open the Serial Monitor.\nPointing the camera at one of our objects, we can see the inference result on the Serial Terminal.\n\n\n\n\n\n\n\nThe inference running on the Arduino IDE had an average consumption of 160 mA or 800 mW and a peak of 330 mA 1.65 W when transmitting the image to the App.\n\n\n\n\nThe idea behind our postprocessing is that whenever a specific image is detected (for example, the Periquito - Label:1), the User LED is turned on. If the Robot or a background is detected, the LED will be off.\nCopy the below code and past it to your IDE:\n#include &lt;Seeed_Arduino_SSCMA.h&gt;\nSSCMA AI;\n\nvoid setup()\n{\n    AI.begin();\n\n    Serial.begin(115200);\n    while (!Serial);\n    Serial.println(\"Inferencing - Grove AI V2 / XIAO ESP32S3\");\n\n    // Pins for the built-in LED\n    pinMode(LED_BUILTIN, OUTPUT);\n    // Ensure the LED is OFF by default.\n    // Note: The LED is ON when the pin is LOW, OFF when HIGH.\n    digitalWrite(LED_BUILTIN, HIGH);\n}\n\nvoid loop()\n{\n    if (!AI.invoke()){\n        Serial.println(\"\\nInvoke Success\");\n        Serial.print(\"Latency [ms]: prepocess=\");\n        Serial.print(AI.perf().prepocess);\n        Serial.print(\", inference=\");\n        Serial.print(AI.perf().inference);\n        Serial.print(\", postpocess=\");\n        Serial.println(AI.perf().postprocess);\n        int pred_index = AI.classes()[0].target;\n        Serial.print(\"Result= Label: \");\n        Serial.print(pred_index);\n        Serial.print(\", score=\");\n        Serial.println(AI.classes()[0].score);\n        turn_on_led(pred_index);\n    }\n}\n\n/**\n* @brief      turn_off_led function - turn-off the User LED\n*/\nvoid turn_off_led(){\n    digitalWrite(LED_BUILTIN, HIGH);\n}\n\n/**\n* @brief      turn_on_led function used to turn on the User LED\n* @param[in]  pred_index\n*             label 0: [0] ==&gt; ALL OFF\n*             label 1: [1] ==&gt; LED ON\n*             label 2: [2] ==&gt; ALL OFF\n*             label 3: [3] ==&gt; ALL OFF\n*/\nvoid turn_on_led(int pred_index) {\n    switch (pred_index)\n    {\n        case 0:\n            turn_off_led();\n            break;\n        case 1:\n            turn_off_led();\n            digitalWrite(LED_BUILTIN, LOW);\n            break;\n        case 2:\n            turn_off_led();\n            break;\n        case 3:\n            turn_off_led();\n            break;\n    }\n}\nThis sketch uses the Seeed_Arduino_SSCMA.h library to interface with the Grove Vision AI Module V2. The AI module and the LED are initialized in the setup() function, and serial communication is started.\nThe loop() function repeatedly calls the invoke() method to perform inference using the built-in algorithms of the Grove Vision AI Module V2. Upon a successful inference, the sketch prints out performance metrics to the serial monitor, including preprocessing, inference, and postprocessing times.\nThe sketch processes and prints out detailed information about the results of the inference:\n\n(AI.classes()[0]) that identifies the class of image (.target) and its confidence score (.score).\nThe inference result (class) is stored in the integer variable pred_index, which will be used as an input to the function turn_on_led(). As a result, the LED will turn ON, depending on the classification result.\n\nHere is the result:\nIf the Periquito is detected (Label:1), the LED is ON:\n\n\n\n\n\nIf the Robot is detected (Label:2) the LED is OFF (Same for Background (Label:0):\n\n\n\n\n\nTherefore, we can now power the Grove Vision AI V2 + Xiao ESP32S3 with an external battery, and the inference result will be displayed by the LED completely offline. The consumption is approximately 165 mA or 825 mW.\n\nIt is also possible to send the result using Wifi, BLE, or other communication protocols available on the used Master Device.\n\n\n\n\n\nOf course, one of the significant advantages of working with EdgeAI is that devices can run entirely disconnected from the cloud, allowing for seamless interactions with the real world. We did it in the last section, but using the internal Xiao LED. Now, we will connect external LEDs (which could be any actuator).\n\n\n\n\n\n\nThe LEDS should be connected to the XIAO ground via a 220-ohm resistor.\n\n\n\n\n\n\nThe idea is to modify the previous sketch to handle the three external LEDs.\nGOAL: Whenever the image of a Periquito is detected, the LED Green will be ON; if it is a Robot, the LED Yellow will be ON; if it is a Background, the LED Red will be ON.\nThe image processing and model inference are processed locally in Grove Vision AI (V2), and we want the result to be output to the XIAO via IIC. For that, we will use the Arduino SSMA library again.\nHere the sketch to be used:\n#include &lt;Seeed_Arduino_SSCMA.h&gt;\nSSCMA AI;\n\n// Define the LED pin according to the pin diagram\n// The LEDS negative lead should be connected to the XIAO ground\n// via a 220-ohm resistor.\nint LEDR = D1; # XIAO ESP32S3 Pin 1\nint LEDY = D2; # XIAO ESP32S3 Pin 2\nint LEDG = D3; # XIAO ESP32S3 Pin 3\n\n  void setup()\n{\n    AI.begin();\n\n    Serial.begin(115200);\n    while (!Serial);\n    Serial.println(\"Inferencing - Grove AI V2 / XIAO ESP32S3\");\n\n// Initialize the external LEDs\n    pinMode(LEDR, OUTPUT);\n    pinMode(LEDY, OUTPUT);\n    pinMode(LEDG, OUTPUT);\n    // Ensure the LEDs are OFF by default.\n    // Note: The LEDs are ON when the pin is HIGH, OFF when LOW.\n    digitalWrite(LEDR, LOW);\n    digitalWrite(LEDY, LOW);\n    digitalWrite(LEDG, LOW);\n}\n\nvoid loop()\n{\n    if (!AI.invoke()){\n        Serial.println(\"\\nInvoke Success\");\n        Serial.print(\"Latency [ms]: prepocess=\");\n        Serial.print(AI.perf().prepocess);\n        Serial.print(\", inference=\");\n        Serial.print(AI.perf().inference);\n        Serial.print(\", postpocess=\");\n        Serial.println(AI.perf().postprocess);\n        int pred_index = AI.classes()[0].target;\n        Serial.print(\"Result= Label: \");\n        Serial.print(pred_index);\n        Serial.print(\", score=\");\n        Serial.println(AI.classes()[0].score);\n        turn_on_leds(pred_index);\n    }\n}\n\n/**\n* @brief turn_off_leds function - turn-off all LEDs\n*/\nvoid turn_off_leds(){\n    digitalWrite(LEDR, LOW);\n    digitalWrite(LEDY, LOW);\n    digitalWrite(LEDG, LOW);\n}\n\n/**\n* @brief turn_on_leds function used to turn on a specific LED\n* @param[in]  pred_index\n*             label 0: [0] ==&gt; Red ON\n*             label 1: [1] ==&gt; Green ON\n*             label 2: [2] ==&gt; Yellow ON\n*/\nvoid turn_on_leds(int pred_index) {\n    switch (pred_index)\n    {\n        case 0:\n            turn_off_leds();\n            digitalWrite(LEDR, HIGH);\n            break;\n        case 1:\n            turn_off_leds();\n            digitalWrite(LEDG, HIGH);\n            break;\n        case 2:\n            turn_off_leds();\n            digitalWrite(LEDY, HIGH);\n            break;\n        case 3:\n            turn_off_leds();\n            break;\n    }\n}\nWe should connect the Grove Vision AI V2 with the XIAO using its I2C Grove connector. For the XIAO, we will use an Expansion Board for the facility (although it is possible to connect the I2C directly to the XIAO‚Äôs pins). We will power the boards using the USB-C connector, but a battery can also be used.\n\n\n\n\n\nHere is the result:\n\n\n\n\n\n\nThe power consumption reached a peak of 240 mA (Green LED), equivalent to 1.2 W. Driving the Yellow and Red LEDs consumes 14 mA, equivalent to 0.7 W. Sending information to the terminal via serial has no impact on power consumption.",
    "crumbs": [
      "Grove Vision AI V2",
      "Image Classification"
    ]
  },
  {
    "objectID": "contents/seeed/grove_vision_ai_v2/image_classification/image_classification.html#sec-image-classification-summary-dba5",
    "href": "contents/seeed/grove_vision_ai_v2/image_classification/image_classification.html#sec-image-classification-summary-dba5",
    "title": "Image Classification",
    "section": "",
    "text": "In this lab, we‚Äôve explored the complete process of developing an image classification system using the Seeed Studio Grove Vision AI Module V2 powered by the Himax WiseEye2 chip. We‚Äôve walked through every stage of the machine learning workflow, from defining our project goals to deploying a working model with real-world interactions.\nThe Grove Vision AI V2 has demonstrated impressive performance, with inference times of just 4-5ms, dramatically outperforming other common tinyML platforms. Our benchmark comparison showed it to be approximately 14 times faster than ARM-M7 devices and over 100 times faster than an Xtensa LX6 (ESP-CAM). Even when compared to a Raspberry Pi Zero W2, the Edge TPU architecture delivered nearly twice the speed while consuming less power.\nThrough this project, we‚Äôve seen how transfer learning enables us to achieve good classification results with a relatively small dataset of custom images. The MobileNetV2 model with an alpha of 0.1 provided an excellent balance of accuracy and efficiency for our three-class problem, requiring only 146 KB of RAM and 187 KB of Flash memory, well within the capabilities of the Grove Vision AI Module V2‚Äôs 2.4 MB internal SRAM.\nWe also explored several deployment options, from viewing inference results through the SenseCraft AI Studio to creating a standalone system with visual feedback using LEDs. The ability to stream video to a web browser and process inference results locally demonstrates the versatility of edge AI systems for real-world applications.\nThe power consumption of our final system remained impressively low, ranging from approximately 70mA (0.4W) for basic inference to 240mA (1.2W) when driving external components. This efficiency makes the Grove Vision AI Module V2 an excellent choice for battery-powered applications where power consumption is critical.\nThis lab has demonstrated that sophisticated computer vision tasks can now be performed entirely at the edge, without reliance on cloud services or powerful computers. With tools like Edge Impulse Studio and SenseCraft AI Studio, the development process has become accessible even to those without extensive machine learning expertise.\nAs edge AI technology continues to evolve, we can expect even more powerful capabilities from compact, energy-efficient devices like the Grove Vision AI Module V2, opening up new possibilities for smart sensors, IoT applications, and embedded intelligence in everyday objects.",
    "crumbs": [
      "Grove Vision AI V2",
      "Image Classification"
    ]
  },
  {
    "objectID": "contents/seeed/grove_vision_ai_v2/image_classification/image_classification.html#sec-image-classification-resources-db82",
    "href": "contents/seeed/grove_vision_ai_v2/image_classification/image_classification.html#sec-image-classification-resources-db82",
    "title": "Image Classification",
    "section": "",
    "text": "Collecting Images with SenseCraft AI Studio.\nEdge Impulse Studio Project\nSenseCraft AI Studio - Vision Workplace (Deploy Models)\nOther Himax examples\nArduino Sketches",
    "crumbs": [
      "Grove Vision AI V2",
      "Image Classification"
    ]
  },
  {
    "objectID": "contents/seeed/xiao_esp32s3/setup/setup.html",
    "href": "contents/seeed/xiao_esp32s3/setup/setup.html",
    "title": "Setup",
    "section": "",
    "text": "DALL¬∑E prompt - 1950s cartoon-style drawing of a XIAO ESP32S3 board with a distinctive camera module, as shown in the image provided. The board is placed on a classic lab table with various sensors, including a microphone. Behind the board, a vintage computer screen displays the Arduino IDE in muted colors, with code focusing on LED pin setups and machine learning inference for voice commands. The Serial Monitor on the IDE showcases outputs detecting voice commands like ‚Äòyes‚Äô and ‚Äòno‚Äô. The scene merges the retro charm of mid-century labs with modern electronics.\n\n\n\n\nThe XIAOML Kit is designed to provides hands-on experience with TinyML applications. The kit includes the powerful XIAO ESP32S3 Sense development board and an expansion board that adds essential sensors for machine learning projects.\nComplete XIAOML Kit Components:\n\nXIAO ESP32S3 Sense: Main development board with integrated camera sensor, digital microphone, and SD card support\nExpansion Board: Features a 6-axis IMU (LSM6DS3TR-C) and 0.42‚Äù OLED display for motion sensing and data visualization\nSD Card Toolkit: Includes SD card and USB adapter for data storage and model deployment\nUSB-C Cable: For connecting the board to your computer\nAntenna and Heat Sinks\n\n\n‚ö†Ô∏è Attention\nDo not install the heat sinks (or carefully, remove them) on/from the XIAO ESP32S3 if you want to use the XIAO ML Kit Expansion Board. See Appendix for more information.\n\n \n\n\nThe XIAO ESP32S3 Sense serves as the heart of the XIAOML Kit, integrating embedded ML computing power with photography and audio capabilities, making it an ideal platform for TinyML applications in intelligent voice and vision AI.\n \nKey Features\n\nPowerful MCU: ESP32S3 32-bit, dual-core, Xtensa processor operating up to 240 MHz, with Arduino / MicroPython support\nAdvanced Functionality: Detachable OV2640 camera sensor for 1600 √ó 1200 resolution, compatible with OV5640 camera sensor, plus integrated digital microphone\nElaborate Power Design: Lithium battery charge management with four power consumption models, deep sleep mode with power consumption as low as 14 ŒºA\nGreat Memory: 8 MB PSRAM and 8 MB FLASH, supporting SD card slot for external 32 GB FAT memory\nOutstanding RF Performance: 2.4 GHz Wi-Fi and BLE dual wireless communication, supports 100m+ remote communication with U.FL antenna\nCompact Design: 21 √ó 17.5 mm, adopting the classic XIAO form factor, suitable for space-limited projects\n\n \nBelow is the general board pinout:\n \n\nFor more details, please refer to the Seeed Studio Wiki page\n\n\n\n\nThe expansion board extends the XIAOML Kit‚Äôs capabilities for motion-based machine learning applications:\n \nComponents:\n\n6-axis IMU (LSM6DS3TR-C):\n\n3-axis accelerometer and 3-axis gyroscope for motion detection and classification\n\nAccelerometer range: ¬±2/¬±4/¬±8/¬±16 g\nGyroscope range: ¬±125/¬±250/¬±500/¬±1000/¬±2000 dps\nI2C interface (address: 0x6A)\n\n\n0.42‚Äù OLED Display\n\nMonochrome display (72√ó40 resolution) for real-time data visualization\n\nController: SSD1306\nI2C interface (address: 0x3C)\n\n\nRestart Button (EN)\nBattery Connector (BAT+, BAT- )\n\n\n\n\nThe expansion board connects seamlessly to the XIAO ESP32S3 Sense, creating a comprehensive platform for multimodal machine learning experiments covering vision, audio, and motion sensing.\n \nPlease pay attention to the mounting orientation of the module:\n \nNote that\n\nThe EN connection, shown at the bottom of the ESP32S3 Sense, is available on the expansion board via the RST button.\nThe BAT+ and BAT- connections are also available through the BAT3.7V white connector.\n\nXIAOML Kit Applications:\n\nVision: Image classification and object detection using the integrated camera\nAudio: Keyword spotting and voice recognition with the built-in microphone\nMotion: Activity recognition and anomaly detection using the IMU sensors\nMulti-modal: Combined sensor fusion for complex ML applications\n\n\n\n\n\n\nConnect the XIAOML Kit to your computer via the USB-C port. \nDownload and Install the stable version of Arduino IDE according to your operating system.\n[Download Arduino IDE]\nOpen the Arduino IDE and select the Boards Manager (represented by the UNO Icon).\nEnter ‚ÄúESP32‚Äù, and select‚Äùesp32 by Espressif Systems.‚Äù You can install or update the board support packages.\n\n\nDo not select ‚ÄúArduino ESP32 Boards by Arduino‚Äù, which are the support package for the Arduino Nano ESP32 and not our board.\n\n \n\n‚ö†Ô∏è Attention\nVersions 3.x may experience issues when using the XIAO ESP32S3 Sense with Edge Impulse deploy codes. If this is the case, use the last 2.0.x stable version (for example, 2.0.17) instead.\n\n\nClick Select Board, enter with xiao or esp32s3, and select the XIAO_ESP32S3 in the boards manager and the corresponding PORT where the ESP32S3 is connected.\n\n \nThat is it! The device should be OK. Let‚Äôs do some tests.\n\n\n\nThe XIAO ESP32S3 Sense features a built-in LED connected to GPIO21. So, you can run the blink sketch (which can be found under Files/Examples/Basics/Blink. The sketch uses the LED_BUILTIN Arduino constant, which internally corresponds to the LED connected to pin 21. Alternatively, you can change the Blink sketch accordingly.\n#define LED_BUILT_IN 21 // This line is optional\n\nvoid setup() {\n  pinMode(LED_BUILT_IN, OUTPUT); // Set the pin as output\n}\n\n// Remember that the pins work with inverted logic\n// LOW to turn on and HIGH to turn off\nvoid loop() {\n  digitalWrite(LED_BUILT_IN, LOW); //Turn on\n  delay (1000); //Wait 1 sec\n  digitalWrite(LED_BUILT_IN, HIGH); //Turn off\n  delay (1000); //Wait 1 sec\n}\n\nNote that the pins operate with inverted logic: LOW turns on and HIGH turns off.\n\n \n\n\n\nLet‚Äôs start with sound detection. Enter with the code below or go to the GitHub project and download the sketch: XIAOML_Kit_Mic_Test and run it on the Arduino IDE:\n/*\n  XIAO ESP32S3 Simple Mic Test\n  (for ESP32 Library version 3.0.x and later)\n*/\n\n#include &lt;ESP_I2S.h&gt;\nI2SClass I2S;\n\nvoid setup() {\n  Serial.begin(115200);\n  while (!Serial) {\n    }\n\n  // setup 42 PDM clock and 41 PDM data pins\n  I2S.setPinsPdmRx(42, 41);\n\n  // start I2S at 16 kHz with 16-bits per sample\n  if (!I2S.begin(I2S_MODE_PDM_RX,\n                 16000,\n                 I2S_DATA_BIT_WIDTH_16BIT,\n                 I2S_SLOT_MODE_MONO)) {\n    Serial.println(\"Failed to initialize I2S!\");\n    while (1); // do nothing\n  }\n}\n\nvoid loop() {\n  // read a sample\n  int sample = I2S.read();\n\n  if (sample && sample != -1 && sample != 1) {\n    Serial.println(sample);\n  }\n}\nOpen the Serial Plotter, and you will see the loudness change curve of the sound.\n \nWhen producing sound, you can verify it on the Serial Plotter.\nSave recorded sound (.wav audio files) to a microSD card.\nNow, using the onboard SD Card reader, we can save .wav audio files. To do that, we need first to enable the XIAO PSRAM.\n\nESP32-S3 has only a few hundred kilobytes of internal RAM on the MCU chip. This can be insufficient for some purposes, so up to 16 MB of external PSRAM (pseudo-static RAM) can be connected with the SPI flash chip (The XIAO has 8 MB of PSRAM). The external memory is incorporated in the memory map and, with certain restrictions, is usable in the same way as internal data RAM.\n\n\nTo turn it on, go to Tools-&gt;PSRAM:\"OPI PSRAM\"-&gt;OPI PSRAM\n\n \n\nXIAO ESP32S3 Sense supports microSD cards up to 32GB. If you are ready to purchase a microSD card for XIAO, please refer to the specifications below. Format the microSD card to FAT32 format before using it.\n\nNow, insert the FAT32 formatted SD card into the XIAO as shown in the photo below\n \n/*\n * WAV Recorder for Seeed XIAO ESP32S3 Sense\n * (for ESP32 Library version 3.0.x and later)\n*/\n\n#include \"ESP_I2S.h\"\n#include \"FS.h\"\n#include \"SD.h\"\n\nvoid setup() {\n  // Create an instance of the I2SClass\n  I2SClass i2s;\n\n  // Create variables to store the audio data\n  uint8_t *wav_buffer;\n  size_t wav_size;\n\n  // Initialize the serial port\n  Serial.begin(115200);\n  while (!Serial) {\n    delay(10);\n  }\n\n  Serial.println(\"Initializing I2S bus...\");\n\n  // Set up the pins used for audio input\n  i2s.setPinsPdmRx(42, 41);\n\n  // start I2S at 16 kHz with 16-bits per sample\n  if (!i2s.begin(I2S_MODE_PDM_RX,\n                 16000,\n                 I2S_DATA_BIT_WIDTH_16BIT,\n                 I2S_SLOT_MODE_MONO)) {\n    Serial.println(\"Failed to initialize I2S!\");\n    while (1); // do nothing\n  }\n\n  Serial.println(\"I2S bus initialized.\");\n  Serial.println(\"Initializing SD card...\");\n\n  // Set up the pins used for SD card access\n  if(!SD.begin(21)){\n    Serial.println(\"Failed to mount SD Card!\");\n    while (1) ;\n  }\n  Serial.println(\"SD card initialized.\");\n  Serial.println(\"Recording 20 seconds of audio data...\");\n\n  // Record 20 seconds of audio data\n  wav_buffer = i2s.recordWAV(20, &wav_size);\n\n  // Create a file on the SD card\n  File file = SD.open(\"/arduinor_rec.wav\", FILE_WRITE);\n  if (!file) {\n    Serial.println(\"Failed to open file for writing!\");\n    return;\n  }\n\n  Serial.println(\"Writing audio data to file...\");\n\n  // Write the audio data to the file\n  if (file.write(wav_buffer, wav_size) != wav_size) {\n    Serial.println(\"Failed to write audio data to file!\");\n    return;\n  }\n\n  // Close the file\n  file.close();\n\n  Serial.println(\"Application complete.\");\n}\n\nvoid loop() {\n  delay(1000);\n  Serial.printf(\".\");\n}\n\nSave the code, for example, as Wav_Record.ino, and run it in the Arduino IDE.\nThis program is executed only once after the user turns on the serial monitor (or when the RESET button is pressed). It records for 20 seconds and saves the recording file to a microSD card as ‚Äúarduino_rec.wav.‚Äù\nWhen the ‚Äú.‚Äù is output every second in the serial monitor, the program execution is complete, and you can play the recorded sound file using a card reader.\n\n \nThe sound quality is excellent!\n\nThe explanation of how the code works is beyond the scope of this lab, but you can find an excellent description on the wiki page.\n\nTo know more about the File System on the XIAO ESP32S3 Sense, please refer to this link.\n\n\n\nFor testing (and using the camera, we can use several methods:\n\nThe SenseCraft AI Studio\nThe CameraWebServer app on Arduino IDE (See the next section)\nCapturing images and saving them on an SD card (similar to what we did with audio)\n\n\n\nThe easiest way to see the camera working is to use the SenseCraft AI Studio, a robust platform that offers a wide range of AI models compatible with various devices, including the XIAO ESP32S3 Sense and the Grove Vision AI V2.\n\nWe can also use the SenseCraft Web Toolkit, a simplified version of the SenseCraft AI Studio.\n\nLet‚Äôs follow the steps below to start the SenseCraft AI:\n\nOpen the SenseCraft AI Vision Workspace in a web browser, such as Chrome, and sign in (or create an account).\n\n \n\nHaving the XIAOML Kit physically connected to the notebook, select it as below:\n\n \n\nNote: The WebUSB tool may not function correctly in certain browsers, such as Safari. Use Chrome instead. Also, confirm that the Arduino IDE or any other serial device is not connected to the XIAO.\n\nTo see the camera working, we should upload a model. We can try several Computer Vision models previously uploaded by Seeed Studio. Use the button [Select Model] and choose among the available models.\n \nPassing the cursor over the AI models, we can have some information about them, such as name, description, category or task (Image Classification, Object Detection, or Pose/Keypoint Detection), the algorithm (like YOLO V5 or V8, FOMO, MobileNet V2, etc.) and in some cases, metrics (Accuracy or mAP).\n \nWe can choose one of the ready-to-use AI models, such as ‚ÄúPerson Classification‚Äù, by clicking on it and pressing the [Confirm] button, or upload our own model.\n \nIn the Preview Area, we can see the streaming generated by the camera.\n \n\nWe will return to the SenseCraft AI Studio in more detail during the Vision AI labs.\n\n\n\n\n\n\n\nThe XIAOML Kit arrived fully assembled. First, remove the Sense Expansion Board (which contains the Camera, Mic, and SD Card Reader) from the XIAO.\nOn the bottom left of the front of XIAO ESP32S3, there is a separate ‚ÄúWiFi/BT Antenna Connector‚Äù. To improve your WiFi/Bluetooth signal, remove the antenna from the package and attach it to the connector.\nThere is a small trick to installing the antenna. If you press down hard on it directly, you will find it very difficult to press and your fingers will hurt! The correct way to install the antenna is to insert one side of the antenna connector into the connector block first, then gently press down on the other side to ensure the antenna is securely installed.\nRemoving the antenna is also the case. Do not use brute force to pull the antenna directly; instead, apply force to one side to lift, making the antenna easy to remove.\n \nReinstalling the expansion board is very simple; you just need to align the connector on the expansion board with the B2B connector on the XIAO ESP32S3, press it hard, and hear a ‚Äúclick.‚Äù The installation is complete.\nOne of the XIAO ESP32S3‚Äôs differentiators is its WiFi capability. So, let‚Äôs test its radio by scanning the Wi-Fi networks around it. You can do this by running one of the code examples on the board.\nOpen the Arduino IDE and select our board and port. Go to Examples and look for WiFI ==&gt; WiFIScan under the ‚ÄúExamples for the XIAO ESP32S3‚Äù. Upload the sketch to the board.\nYou should see the Wi-Fi networks (SSIDs and RSSIs) within your device‚Äôs range on the serial monitor. Here is what I got in the lab:\n \n\n\n\nLet‚Äôs test the device‚Äôs capability to behave as a Wi-Fi server. We will host a simple page on the device that sends commands to turn the XIAO built-in LED ON and OFF.\nGo to Examples and look for WiFI ==&gt; SimpleWiFIServer under the ‚ÄúExamples for the XIAO ESP32S3‚Äù.\nBefore running the sketch, you should enter your network credentials:\nconst char* ssid     = \"Your credentials here\";\nconst char* password = \"Your credentials here\";\nAnd modify pin 5 to pin 21, where the built-in LED is installed. Also, let‚Äôs modify the webpage (lines 85 and 86) to reflect the correct LED Pin and that it is active with LOW:\nclient.print(\"Click &lt;a href=\\\"/H\\\"&gt;here&lt;/a&gt; to turn the LED on pin 21 OFF.&lt;br&gt;\");\nclient.print(\"Click &lt;a href=\\\"/L\\\"&gt;here&lt;/a&gt; to turn the LED on pin 21 ON.&lt;br&gt;\");\nYou can monitor your server‚Äôs performance using the Serial Monitor.\n \nTake the IP address shown in the Serial Monitor and enter it in your browser. You will see a page with links that can turn the built-in LED of your XIAO ON and OFF.\n \n\n\n\nIn the Arduino IDE, go to File &gt; Examples &gt; ESP32 &gt; Camera, and select CameraWebServer\nOn the board_config.h tab, comment on all cameras‚Äô models, except the XIAO model pins:\n#define CAMERA_MODEL_XIAO_ESP32S3 // Has PSRAM\n\nDo not forget to check the Tools to see if PSRAM is enabled.\n\n \nAs done before, in the CameraWebServer.ino tab, enter your wifi credentials and upload the code to the device.\nIf the code is executed correctly, you should see the address on the Serial Monitor:\nWiFi connecting....\nWiFi connected\nCamera Ready! Use 'http://192.168.5.60' to connect\nCopy the address into your browser and wait for the page to load. Select the camera resolution (for example, QVGA) and select [START STREAM]. Wait for a few seconds, depending on your connection. Using the [Save] button, you can save an image to your computer‚Äôs download area.\n \nThat‚Äôs it! You can save the images directly on your computer for use on projects.\n\n\n\n\nAn Inertial Measurement Unit (IMU) is a sensor that measures motion and orientation. The LSM6DS3TR-C on your XIAOML kit is a 6-axis IMU, meaning it combines:\n\n3-axis Accelerometer: Measures linear acceleration (including gravity) along X, Y, and Z axes\n3-axis Gyroscope: Measures angular velocity (rotation rate) around X, Y, and Z axes\n\n\n\n\nCommunication: I2C interface at address 0x6A\nAccelerometer Range: ¬±2/¬±4/¬±8/¬±16 g (we use ¬±2g by default)\nGyroscope Range: ¬±125/¬±250/¬±500/¬±1000/¬±2000 dps (we use ¬±250 dps by default)\nResolution: 16-bit ADC\nPower Consumption: Ultra-low power design\n\n\n\n\nThe sensor follows a right-hand coordinate system. When looking at the IMU sensor with the point mark visible (Expansion Board bottom view):\n \n\nX-axis: Points to the right\nY-axis: Points forward (away from you)\nZ-axis: Points upward (out of the board)\n\n \n\n\n\nBefore uploading the code, install the required library:\n\nOpen the Arduino IDE and select Manage Libraries (represented by the Books Icon).\nFor the IMU library, enter ‚ÄúLSM6DS3‚Äù, and select‚ÄùSeeed Arduino LSM6DS3 by Seeed‚Äù. You can INSTALL or UPDATE the board support packages.\n\n \n‚ö†Ô∏è Important: Do NOT install ‚ÄúArduino_LSM6DS3 by Arduino‚Äù - that‚Äôs for different boards!\n\n\n\nEnter with the code below at the Arduino IDE and uploaded it to Kit:\n#include &lt;LSM6DS3.h&gt;\n#include &lt;Wire.h&gt;\n\n// Create IMU object using I2C interface\n// LSM6DS3TR-C sensor is located at I2C address 0x6A\nLSM6DS3 myIMU(I2C_MODE, 0x6A);\n\n// Variables to store sensor readings\nfloat accelX, accelY, accelZ;  // Accelerometer values (g-force)\nfloat gyroX, gyroY, gyroZ;     // Gyroscope values (degrees per second)\n\nvoid setup() {\n  // Initialize serial communication at 115200 baud rate\n  Serial.begin(115200);\n\n  // Wait for serial port to connect (useful for debugging)\n  while (!Serial) {\n    delay(10);\n  }\n\n  Serial.println(\"XIAOML Kit IMU Test\");\n  Serial.println(\"LSM6DS3TR-C 6-Axis IMU Sensor\");\n  Serial.println(\"=============================\");\n\n  // Initialize the IMU sensor\n  if (myIMU.begin() != 0) {\n    Serial.println(\"ERROR: IMU initialization failed!\");\n    Serial.println(\"Check connections and I2C address\");\n    while(1) {\n      delay(1000); // Halt execution if IMU fails to initialize\n    }\n  } else {\n    Serial.println(\"‚úì IMU initialized successfully\");\n    Serial.println();\n\n    // Print sensor information\n    Serial.println(\"Sensor Information:\");\n    Serial.println(\"- Accelerometer range: ¬±2g\");\n    Serial.println(\"- Gyroscope range: ¬±250 dps\");\n    Serial.println(\"- Communication: I2C at address 0x6A\");\n    Serial.println();\n\n    // Print data format explanation\n    Serial.println(\"Data Format:\");\n    Serial.println(\"AccelX,AccelY,AccelZ,GyroX,GyroY,GyroZ\");\n    Serial.println(\"Units: g-force (m/s¬≤), degrees/second\");\n    Serial.println();\n\n    delay(2000); // Brief pause before starting measurements\n  }\n}\n\nvoid loop() {\n  // Read accelerometer data (in g-force units)\n  accelX = myIMU.readFloatAccelX();\n  accelY = myIMU.readFloatAccelY();\n  accelZ = myIMU.readFloatAccelZ();\n\n  // Read gyroscope data (in degrees per second)\n  gyroX = myIMU.readFloatGyroX();\n  gyroY = myIMU.readFloatGyroY();\n  gyroZ = myIMU.readFloatGyroZ();\n\n  // Print readable format to Serial Monitor\n  Serial.print(\"Accelerometer (g): \");\n  Serial.print(\"X=\"); Serial.print(accelX, 3);\n  Serial.print(\" Y=\"); Serial.print(accelY, 3);\n  Serial.print(\" Z=\"); Serial.print(accelZ, 3);\n\n  Serial.print(\" | Gyroscope (¬∞/s): \");\n  Serial.print(\"X=\"); Serial.print(gyroX, 2);\n  Serial.print(\" Y=\"); Serial.print(gyroY, 2);\n  Serial.print(\" Z=\"); Serial.print(gyroZ, 2);\n  Serial.println();\n\n  // Print CSV format for Serial Plotter\n  Serial.println(String(accelX) + \",\" + String(accelY) + \",\" +\n                 String(accelZ) + \",\" + String(gyroX) + \",\" +\n                 String(gyroY) + \",\" + String(gyroZ));\n\n  // Update rate: 10 Hz (100ms delay)\n  delay(100);\n}\nThe Serial monitor will show the values, and the plotter will show their variation over time. For example, by moving the Kit over the y-axis, we will see that value 2 (red line) changes accordingly. Note that z-axis is represented by value 3 (green line), which is near 1.0g. The blue line (value 1) is related to the x-axis.\n \nYou can select the values 4 to 6 to see the Gyroscope behavior.\n\n\n\n\nOLED (Organic Light-Emitting Diode) displays are self-illuminating screens where each pixel produces its own light. The XIAO ML kit features a compact 0.42-inch monochrome OLED display, ideal for displaying sensor data, status information, and simple graphics.\n\n\n\nSize: 0.42 inches diagonal\nResolution: 72 √ó 40 pixels\nController: SSD1306\nInterface: I2C at address 0x3C\nColors: Monochrome (black pixels on white background, or vice versa)\nViewing: High contrast, visible in bright light\nPower: Low power consumption, no backlight needed\n\n\n\n\n\nPixel-perfect: Each of the 2,880 pixels (72√ó40) can be individually controlled\nFast refresh: Suitable for animations and real-time data\nNo ghosting: Instant pixel response\nWide viewing angle: Clear from multiple viewing positions\n\n\n\n\nBefore uploading the code, install the required library:\n\nOpen the Arduino IDE and select the ‚ÄúManage Libraries‚Äù (represented by the Books Icon).\nEnter u8g2 and select U8g2 by oliver. You can install or update the board support packages.\n‚ÑπÔ∏è Note: U8g2 is a powerful graphics library supporting many display types\n\n \nThe U8g2 library is a monochrome graphics library with these features:\n\nSupport for many display controllers (including SSD1306)\nText rendering with various fonts\nDrawing primitives (lines, rectangles, circles)\nMemory-efficient page-based rendering\nHardware and software I2C support\n\n\n\n\nEnter with the code below at the Arduino IDE and uploaded it to Kit:\n#include &lt;U8g2lib.h&gt;\n#include &lt;Wire.h&gt;\n\n// Initialize the OLED display\n// SSD1306 controller, 72x40 resolution, I2C interface\nU8G2_SSD1306_72X40_ER_1_HW_I2C u8g2(U8G2_R2, U8X8_PIN_NONE);\n\nvoid setup() {\n  Serial.begin(115200);\n\n  Serial.println(\"XIAOML Kit - Hello World\");\n  Serial.println(\"==========================\");\n\n  // Initialize the display\n  u8g2.begin();\n\n  Serial.println(\"‚úì Display initialized\");\n  Serial.println(\"Showing Hello World message...\");\n\n  // Clear the display\n  u8g2.clearDisplay();\n}\n\nvoid loop() {\n  // Start drawing sequence\n  u8g2.firstPage();\n  do {\n    // Set font\n    u8g2.setFont(u8g2_font_ncenB08_tr);\n\n    // Display \"Hello World\" centered\n    u8g2.setCursor(8, 15);\n    u8g2.print(\"Hello\");\n\n    u8g2.setCursor(12, 30);\n    u8g2.print(\"World!\");\n\n    // Add a simple decoration - draw a frame around the text\n    u8g2.drawFrame(2, 2, 68, 36);\n\n  } while (u8g2.nextPage());\n\n  // No delay needed - the display will show continuously\n}\nIf everything works fine, you should see at the display, ‚ÄúHello World‚Äù inside a rectangle.\n \n\n\n\n\nNote that the text is positioned with setCursor(x, y), in this case centered:\nu8g2.setCursor(8, 15);\nThe font used in the code was medium.\nu8g2.setFont(u8g2_font_ncenB08_tr);\nBut other font sizes are available:\n\nu8g2_font_4x6_tr: Tiny font (4√ó6 pixels)\nu8g2_font_6x10_tr: Small font (6√ó10 pixels)\nu8g2_font_ncenB08_tr: Medium bold font\nu8g2_font_ncenB14_tr: Large bold font\n\n\n\n\n\nThe code added a simple decoration, drawing a frame around the text\nu8g2.drawFrame(2, 2, 68, 36);\nBut other shapes are available:\n\nRectangle outline: drawFrame(x, y, width, height)\nFilled rectangle: drawBox(x, y, width, height)\nCircle: drawCircle(x, y, radius)\nLine: drawLine(x1, y1, x2, y2)\nIndividual pixels: drawPixel(x, y)\n\n\n\n\nThe display uses a coordinate system where:\n\nOrigin (0,0): Top-left corner\nX-axis: Increases from left to right (0 to 71)\nY-axis: Increases from top to bottom (0 to 39)\nText positioning: setCursor(x, y) where y is the baseline of text\n\n\n\n\n\nYou can change the rotation parameter by using:\n\nU8G2_R0: Normal orientation\nU8G2_R1: 90¬∞ clockwise\nU8G2_R2: 180¬∞ (upside down)\nU8G2_R3: 270¬∞ clockwise\n\n\n\n\n\n// Draw custom bitmap\nstatic const unsigned char myBitmap[] = {0x00, 0x3c, 0x42, 0x42, 0x3c, 0x00};\nu8g2.drawBitmap(x, y, 1, 6, myBitmap);\n\n\n\nint width = u8g2.getStrWidth(\"Hello\");  // Get text width\nint height = u8g2.getAscent();         // Get font height\nThe OLED display is now ready to show your sensor data, system status, or any custom graphics you design for your ML projects!\n\n\n\n\nThe XIAOML Kit with ESP32S3 Sense represents a powerful, yet accessible entry point into the world of TinyML and embedded machine learning. Through this setup process, we have systematically tested every component of the XIAOML Kit, confirming that all sensors and peripherals are functioning correctly. The ESP32S3‚Äôs dual-core processor and 8MB of PSRAM provide sufficient computational power for real-time ML inference, while the OV2640 camera, digital microphone, LSM6DS3TR-C IMU, and 0.42‚Äù OLED display create a complete multimodal sensing platform. WiFi connectivity opens possibilities for edge-to-cloud ML workflows, and our Arduino IDE development environment is now properly configured with all necessary libraries.\nBeyond mere functionality tests, we‚Äôve gained practical insights into coordinate systems, data formats, and operational characteristics of each sensor‚Äîknowledge that will prove invaluable when designing ML data collection and preprocessing pipelines for the upcoming projects.\nThis setup process demonstrates key principles that extend far beyond this specific kit. Working with the ESP32S3‚Äôs memory limitations and processing capabilities provides an authentic experience with the resource constraints inherent in edge AI‚Äîthe same considerations that apply when deploying models on smartphones, IoT devices, or autonomous systems. Having multiple modalities (vision, audio, motion) on a single platform enables exploration of multimodal ML approaches, which are increasingly important in real-world AI applications.\nMost importantly, from raw sensor data to model inference to user feedback via the OLED display, the kit provides a complete ML deployment cycle in miniature, mirroring the challenges faced in production AI systems.\nWith this foundation in place, you‚Äôre now equipped to tackle the core TinyML applications in the following chapters:\n\nVision Projects: Leveraging the camera for image classification and object detection\nAudio Projects: Processing audio streams for keyword spotting and voice recognition\nMotion Projects: Using IMU data for activity recognition and anomaly detection\n\nEach application will build upon the hardware understanding and software infrastructure we‚Äôve established, demonstrating how artificial intelligence can be deployed not just in data centers, but in resource-constrained devices that directly interact with the physical world.\nThe principles encountered with this kit‚Äîreal-time processing, sensor fusion, and edge inference‚Äîare the same ones driving the future of AI deployment in autonomous vehicles, smart cities, medical devices, and industrial automation. By completing this setup successfully, you‚Äôre now prepared to explore this exciting frontier of embedded machine learning.\n\n\n\n\nXIAOML Kit Code\nXIAO ESP32S3 Sense manual & example code\nUsage of Seeed Studio XIAO ESP32S3 microphone\nFile System and XIAO ESP32S3 Sense\nCamera Usage in Seeed Studio XIAO ESP32S3 Sense",
    "crumbs": [
      "Seeed XIAO ESP32S3",
      "Setup"
    ]
  },
  {
    "objectID": "contents/seeed/xiao_esp32s3/setup/setup.html#sec-setup-overview-d638",
    "href": "contents/seeed/xiao_esp32s3/setup/setup.html#sec-setup-overview-d638",
    "title": "Setup",
    "section": "",
    "text": "The XIAOML Kit is designed to provides hands-on experience with TinyML applications. The kit includes the powerful XIAO ESP32S3 Sense development board and an expansion board that adds essential sensors for machine learning projects.\nComplete XIAOML Kit Components:\n\nXIAO ESP32S3 Sense: Main development board with integrated camera sensor, digital microphone, and SD card support\nExpansion Board: Features a 6-axis IMU (LSM6DS3TR-C) and 0.42‚Äù OLED display for motion sensing and data visualization\nSD Card Toolkit: Includes SD card and USB adapter for data storage and model deployment\nUSB-C Cable: For connecting the board to your computer\nAntenna and Heat Sinks\n\n\n‚ö†Ô∏è Attention\nDo not install the heat sinks (or carefully, remove them) on/from the XIAO ESP32S3 if you want to use the XIAO ML Kit Expansion Board. See Appendix for more information.\n\n \n\n\nThe XIAO ESP32S3 Sense serves as the heart of the XIAOML Kit, integrating embedded ML computing power with photography and audio capabilities, making it an ideal platform for TinyML applications in intelligent voice and vision AI.\n \nKey Features\n\nPowerful MCU: ESP32S3 32-bit, dual-core, Xtensa processor operating up to 240 MHz, with Arduino / MicroPython support\nAdvanced Functionality: Detachable OV2640 camera sensor for 1600 √ó 1200 resolution, compatible with OV5640 camera sensor, plus integrated digital microphone\nElaborate Power Design: Lithium battery charge management with four power consumption models, deep sleep mode with power consumption as low as 14 ŒºA\nGreat Memory: 8 MB PSRAM and 8 MB FLASH, supporting SD card slot for external 32 GB FAT memory\nOutstanding RF Performance: 2.4 GHz Wi-Fi and BLE dual wireless communication, supports 100m+ remote communication with U.FL antenna\nCompact Design: 21 √ó 17.5 mm, adopting the classic XIAO form factor, suitable for space-limited projects\n\n \nBelow is the general board pinout:\n \n\nFor more details, please refer to the Seeed Studio Wiki page\n\n\n\n\nThe expansion board extends the XIAOML Kit‚Äôs capabilities for motion-based machine learning applications:\n \nComponents:\n\n6-axis IMU (LSM6DS3TR-C):\n\n3-axis accelerometer and 3-axis gyroscope for motion detection and classification\n\nAccelerometer range: ¬±2/¬±4/¬±8/¬±16 g\nGyroscope range: ¬±125/¬±250/¬±500/¬±1000/¬±2000 dps\nI2C interface (address: 0x6A)\n\n\n0.42‚Äù OLED Display\n\nMonochrome display (72√ó40 resolution) for real-time data visualization\n\nController: SSD1306\nI2C interface (address: 0x3C)\n\n\nRestart Button (EN)\nBattery Connector (BAT+, BAT- )\n\n\n\n\nThe expansion board connects seamlessly to the XIAO ESP32S3 Sense, creating a comprehensive platform for multimodal machine learning experiments covering vision, audio, and motion sensing.\n \nPlease pay attention to the mounting orientation of the module:\n \nNote that\n\nThe EN connection, shown at the bottom of the ESP32S3 Sense, is available on the expansion board via the RST button.\nThe BAT+ and BAT- connections are also available through the BAT3.7V white connector.\n\nXIAOML Kit Applications:\n\nVision: Image classification and object detection using the integrated camera\nAudio: Keyword spotting and voice recognition with the built-in microphone\nMotion: Activity recognition and anomaly detection using the IMU sensors\nMulti-modal: Combined sensor fusion for complex ML applications",
    "crumbs": [
      "Seeed XIAO ESP32S3",
      "Setup"
    ]
  },
  {
    "objectID": "contents/seeed/xiao_esp32s3/setup/setup.html#sec-setup-installing-xiao-esp32s3-sense-arduino-ide-197b",
    "href": "contents/seeed/xiao_esp32s3/setup/setup.html#sec-setup-installing-xiao-esp32s3-sense-arduino-ide-197b",
    "title": "Setup",
    "section": "",
    "text": "Connect the XIAOML Kit to your computer via the USB-C port. \nDownload and Install the stable version of Arduino IDE according to your operating system.\n[Download Arduino IDE]\nOpen the Arduino IDE and select the Boards Manager (represented by the UNO Icon).\nEnter ‚ÄúESP32‚Äù, and select‚Äùesp32 by Espressif Systems.‚Äù You can install or update the board support packages.\n\n\nDo not select ‚ÄúArduino ESP32 Boards by Arduino‚Äù, which are the support package for the Arduino Nano ESP32 and not our board.\n\n \n\n‚ö†Ô∏è Attention\nVersions 3.x may experience issues when using the XIAO ESP32S3 Sense with Edge Impulse deploy codes. If this is the case, use the last 2.0.x stable version (for example, 2.0.17) instead.\n\n\nClick Select Board, enter with xiao or esp32s3, and select the XIAO_ESP32S3 in the boards manager and the corresponding PORT where the ESP32S3 is connected.\n\n \nThat is it! The device should be OK. Let‚Äôs do some tests.",
    "crumbs": [
      "Seeed XIAO ESP32S3",
      "Setup"
    ]
  },
  {
    "objectID": "contents/seeed/xiao_esp32s3/setup/setup.html#sec-setup-testing-board-blink-005d",
    "href": "contents/seeed/xiao_esp32s3/setup/setup.html#sec-setup-testing-board-blink-005d",
    "title": "Setup",
    "section": "",
    "text": "The XIAO ESP32S3 Sense features a built-in LED connected to GPIO21. So, you can run the blink sketch (which can be found under Files/Examples/Basics/Blink. The sketch uses the LED_BUILTIN Arduino constant, which internally corresponds to the LED connected to pin 21. Alternatively, you can change the Blink sketch accordingly.\n#define LED_BUILT_IN 21 // This line is optional\n\nvoid setup() {\n  pinMode(LED_BUILT_IN, OUTPUT); // Set the pin as output\n}\n\n// Remember that the pins work with inverted logic\n// LOW to turn on and HIGH to turn off\nvoid loop() {\n  digitalWrite(LED_BUILT_IN, LOW); //Turn on\n  delay (1000); //Wait 1 sec\n  digitalWrite(LED_BUILT_IN, HIGH); //Turn off\n  delay (1000); //Wait 1 sec\n}\n\nNote that the pins operate with inverted logic: LOW turns on and HIGH turns off.",
    "crumbs": [
      "Seeed XIAO ESP32S3",
      "Setup"
    ]
  },
  {
    "objectID": "contents/seeed/xiao_esp32s3/setup/setup.html#sec-setup-microphone-test-7d6c",
    "href": "contents/seeed/xiao_esp32s3/setup/setup.html#sec-setup-microphone-test-7d6c",
    "title": "Setup",
    "section": "",
    "text": "Let‚Äôs start with sound detection. Enter with the code below or go to the GitHub project and download the sketch: XIAOML_Kit_Mic_Test and run it on the Arduino IDE:\n/*\n  XIAO ESP32S3 Simple Mic Test\n  (for ESP32 Library version 3.0.x and later)\n*/\n\n#include &lt;ESP_I2S.h&gt;\nI2SClass I2S;\n\nvoid setup() {\n  Serial.begin(115200);\n  while (!Serial) {\n    }\n\n  // setup 42 PDM clock and 41 PDM data pins\n  I2S.setPinsPdmRx(42, 41);\n\n  // start I2S at 16 kHz with 16-bits per sample\n  if (!I2S.begin(I2S_MODE_PDM_RX,\n                 16000,\n                 I2S_DATA_BIT_WIDTH_16BIT,\n                 I2S_SLOT_MODE_MONO)) {\n    Serial.println(\"Failed to initialize I2S!\");\n    while (1); // do nothing\n  }\n}\n\nvoid loop() {\n  // read a sample\n  int sample = I2S.read();\n\n  if (sample && sample != -1 && sample != 1) {\n    Serial.println(sample);\n  }\n}\nOpen the Serial Plotter, and you will see the loudness change curve of the sound.\n \nWhen producing sound, you can verify it on the Serial Plotter.\nSave recorded sound (.wav audio files) to a microSD card.\nNow, using the onboard SD Card reader, we can save .wav audio files. To do that, we need first to enable the XIAO PSRAM.\n\nESP32-S3 has only a few hundred kilobytes of internal RAM on the MCU chip. This can be insufficient for some purposes, so up to 16 MB of external PSRAM (pseudo-static RAM) can be connected with the SPI flash chip (The XIAO has 8 MB of PSRAM). The external memory is incorporated in the memory map and, with certain restrictions, is usable in the same way as internal data RAM.\n\n\nTo turn it on, go to Tools-&gt;PSRAM:\"OPI PSRAM\"-&gt;OPI PSRAM\n\n \n\nXIAO ESP32S3 Sense supports microSD cards up to 32GB. If you are ready to purchase a microSD card for XIAO, please refer to the specifications below. Format the microSD card to FAT32 format before using it.\n\nNow, insert the FAT32 formatted SD card into the XIAO as shown in the photo below\n \n/*\n * WAV Recorder for Seeed XIAO ESP32S3 Sense\n * (for ESP32 Library version 3.0.x and later)\n*/\n\n#include \"ESP_I2S.h\"\n#include \"FS.h\"\n#include \"SD.h\"\n\nvoid setup() {\n  // Create an instance of the I2SClass\n  I2SClass i2s;\n\n  // Create variables to store the audio data\n  uint8_t *wav_buffer;\n  size_t wav_size;\n\n  // Initialize the serial port\n  Serial.begin(115200);\n  while (!Serial) {\n    delay(10);\n  }\n\n  Serial.println(\"Initializing I2S bus...\");\n\n  // Set up the pins used for audio input\n  i2s.setPinsPdmRx(42, 41);\n\n  // start I2S at 16 kHz with 16-bits per sample\n  if (!i2s.begin(I2S_MODE_PDM_RX,\n                 16000,\n                 I2S_DATA_BIT_WIDTH_16BIT,\n                 I2S_SLOT_MODE_MONO)) {\n    Serial.println(\"Failed to initialize I2S!\");\n    while (1); // do nothing\n  }\n\n  Serial.println(\"I2S bus initialized.\");\n  Serial.println(\"Initializing SD card...\");\n\n  // Set up the pins used for SD card access\n  if(!SD.begin(21)){\n    Serial.println(\"Failed to mount SD Card!\");\n    while (1) ;\n  }\n  Serial.println(\"SD card initialized.\");\n  Serial.println(\"Recording 20 seconds of audio data...\");\n\n  // Record 20 seconds of audio data\n  wav_buffer = i2s.recordWAV(20, &wav_size);\n\n  // Create a file on the SD card\n  File file = SD.open(\"/arduinor_rec.wav\", FILE_WRITE);\n  if (!file) {\n    Serial.println(\"Failed to open file for writing!\");\n    return;\n  }\n\n  Serial.println(\"Writing audio data to file...\");\n\n  // Write the audio data to the file\n  if (file.write(wav_buffer, wav_size) != wav_size) {\n    Serial.println(\"Failed to write audio data to file!\");\n    return;\n  }\n\n  // Close the file\n  file.close();\n\n  Serial.println(\"Application complete.\");\n}\n\nvoid loop() {\n  delay(1000);\n  Serial.printf(\".\");\n}\n\nSave the code, for example, as Wav_Record.ino, and run it in the Arduino IDE.\nThis program is executed only once after the user turns on the serial monitor (or when the RESET button is pressed). It records for 20 seconds and saves the recording file to a microSD card as ‚Äúarduino_rec.wav.‚Äù\nWhen the ‚Äú.‚Äù is output every second in the serial monitor, the program execution is complete, and you can play the recorded sound file using a card reader.\n\n \nThe sound quality is excellent!\n\nThe explanation of how the code works is beyond the scope of this lab, but you can find an excellent description on the wiki page.\n\nTo know more about the File System on the XIAO ESP32S3 Sense, please refer to this link.",
    "crumbs": [
      "Seeed XIAO ESP32S3",
      "Setup"
    ]
  },
  {
    "objectID": "contents/seeed/xiao_esp32s3/setup/setup.html#sec-setup-testing-camera-bbd0",
    "href": "contents/seeed/xiao_esp32s3/setup/setup.html#sec-setup-testing-camera-bbd0",
    "title": "Setup",
    "section": "",
    "text": "For testing (and using the camera, we can use several methods:\n\nThe SenseCraft AI Studio\nThe CameraWebServer app on Arduino IDE (See the next section)\nCapturing images and saving them on an SD card (similar to what we did with audio)\n\n\n\nThe easiest way to see the camera working is to use the SenseCraft AI Studio, a robust platform that offers a wide range of AI models compatible with various devices, including the XIAO ESP32S3 Sense and the Grove Vision AI V2.\n\nWe can also use the SenseCraft Web Toolkit, a simplified version of the SenseCraft AI Studio.\n\nLet‚Äôs follow the steps below to start the SenseCraft AI:\n\nOpen the SenseCraft AI Vision Workspace in a web browser, such as Chrome, and sign in (or create an account).\n\n \n\nHaving the XIAOML Kit physically connected to the notebook, select it as below:\n\n \n\nNote: The WebUSB tool may not function correctly in certain browsers, such as Safari. Use Chrome instead. Also, confirm that the Arduino IDE or any other serial device is not connected to the XIAO.\n\nTo see the camera working, we should upload a model. We can try several Computer Vision models previously uploaded by Seeed Studio. Use the button [Select Model] and choose among the available models.\n \nPassing the cursor over the AI models, we can have some information about them, such as name, description, category or task (Image Classification, Object Detection, or Pose/Keypoint Detection), the algorithm (like YOLO V5 or V8, FOMO, MobileNet V2, etc.) and in some cases, metrics (Accuracy or mAP).\n \nWe can choose one of the ready-to-use AI models, such as ‚ÄúPerson Classification‚Äù, by clicking on it and pressing the [Confirm] button, or upload our own model.\n \nIn the Preview Area, we can see the streaming generated by the camera.\n \n\nWe will return to the SenseCraft AI Studio in more detail during the Vision AI labs.",
    "crumbs": [
      "Seeed XIAO ESP32S3",
      "Setup"
    ]
  },
  {
    "objectID": "contents/seeed/xiao_esp32s3/setup/setup.html#sec-setup-testing-wifi-311a",
    "href": "contents/seeed/xiao_esp32s3/setup/setup.html#sec-setup-testing-wifi-311a",
    "title": "Setup",
    "section": "",
    "text": "The XIAOML Kit arrived fully assembled. First, remove the Sense Expansion Board (which contains the Camera, Mic, and SD Card Reader) from the XIAO.\nOn the bottom left of the front of XIAO ESP32S3, there is a separate ‚ÄúWiFi/BT Antenna Connector‚Äù. To improve your WiFi/Bluetooth signal, remove the antenna from the package and attach it to the connector.\nThere is a small trick to installing the antenna. If you press down hard on it directly, you will find it very difficult to press and your fingers will hurt! The correct way to install the antenna is to insert one side of the antenna connector into the connector block first, then gently press down on the other side to ensure the antenna is securely installed.\nRemoving the antenna is also the case. Do not use brute force to pull the antenna directly; instead, apply force to one side to lift, making the antenna easy to remove.\n \nReinstalling the expansion board is very simple; you just need to align the connector on the expansion board with the B2B connector on the XIAO ESP32S3, press it hard, and hear a ‚Äúclick.‚Äù The installation is complete.\nOne of the XIAO ESP32S3‚Äôs differentiators is its WiFi capability. So, let‚Äôs test its radio by scanning the Wi-Fi networks around it. You can do this by running one of the code examples on the board.\nOpen the Arduino IDE and select our board and port. Go to Examples and look for WiFI ==&gt; WiFIScan under the ‚ÄúExamples for the XIAO ESP32S3‚Äù. Upload the sketch to the board.\nYou should see the Wi-Fi networks (SSIDs and RSSIs) within your device‚Äôs range on the serial monitor. Here is what I got in the lab:\n \n\n\n\nLet‚Äôs test the device‚Äôs capability to behave as a Wi-Fi server. We will host a simple page on the device that sends commands to turn the XIAO built-in LED ON and OFF.\nGo to Examples and look for WiFI ==&gt; SimpleWiFIServer under the ‚ÄúExamples for the XIAO ESP32S3‚Äù.\nBefore running the sketch, you should enter your network credentials:\nconst char* ssid     = \"Your credentials here\";\nconst char* password = \"Your credentials here\";\nAnd modify pin 5 to pin 21, where the built-in LED is installed. Also, let‚Äôs modify the webpage (lines 85 and 86) to reflect the correct LED Pin and that it is active with LOW:\nclient.print(\"Click &lt;a href=\\\"/H\\\"&gt;here&lt;/a&gt; to turn the LED on pin 21 OFF.&lt;br&gt;\");\nclient.print(\"Click &lt;a href=\\\"/L\\\"&gt;here&lt;/a&gt; to turn the LED on pin 21 ON.&lt;br&gt;\");\nYou can monitor your server‚Äôs performance using the Serial Monitor.\n \nTake the IP address shown in the Serial Monitor and enter it in your browser. You will see a page with links that can turn the built-in LED of your XIAO ON and OFF.\n \n\n\n\nIn the Arduino IDE, go to File &gt; Examples &gt; ESP32 &gt; Camera, and select CameraWebServer\nOn the board_config.h tab, comment on all cameras‚Äô models, except the XIAO model pins:\n#define CAMERA_MODEL_XIAO_ESP32S3 // Has PSRAM\n\nDo not forget to check the Tools to see if PSRAM is enabled.\n\n \nAs done before, in the CameraWebServer.ino tab, enter your wifi credentials and upload the code to the device.\nIf the code is executed correctly, you should see the address on the Serial Monitor:\nWiFi connecting....\nWiFi connected\nCamera Ready! Use 'http://192.168.5.60' to connect\nCopy the address into your browser and wait for the page to load. Select the camera resolution (for example, QVGA) and select [START STREAM]. Wait for a few seconds, depending on your connection. Using the [Save] button, you can save an image to your computer‚Äôs download area.\n \nThat‚Äôs it! You can save the images directly on your computer for use on projects.",
    "crumbs": [
      "Seeed XIAO ESP32S3",
      "Setup"
    ]
  },
  {
    "objectID": "contents/seeed/xiao_esp32s3/setup/setup.html#sec-setup-testing-imu-sensor-lsm6ds3trc-fd9d",
    "href": "contents/seeed/xiao_esp32s3/setup/setup.html#sec-setup-testing-imu-sensor-lsm6ds3trc-fd9d",
    "title": "Setup",
    "section": "",
    "text": "An Inertial Measurement Unit (IMU) is a sensor that measures motion and orientation. The LSM6DS3TR-C on your XIAOML kit is a 6-axis IMU, meaning it combines:\n\n3-axis Accelerometer: Measures linear acceleration (including gravity) along X, Y, and Z axes\n3-axis Gyroscope: Measures angular velocity (rotation rate) around X, Y, and Z axes\n\n\n\n\nCommunication: I2C interface at address 0x6A\nAccelerometer Range: ¬±2/¬±4/¬±8/¬±16 g (we use ¬±2g by default)\nGyroscope Range: ¬±125/¬±250/¬±500/¬±1000/¬±2000 dps (we use ¬±250 dps by default)\nResolution: 16-bit ADC\nPower Consumption: Ultra-low power design\n\n\n\n\nThe sensor follows a right-hand coordinate system. When looking at the IMU sensor with the point mark visible (Expansion Board bottom view):\n \n\nX-axis: Points to the right\nY-axis: Points forward (away from you)\nZ-axis: Points upward (out of the board)\n\n \n\n\n\nBefore uploading the code, install the required library:\n\nOpen the Arduino IDE and select Manage Libraries (represented by the Books Icon).\nFor the IMU library, enter ‚ÄúLSM6DS3‚Äù, and select‚ÄùSeeed Arduino LSM6DS3 by Seeed‚Äù. You can INSTALL or UPDATE the board support packages.\n\n \n‚ö†Ô∏è Important: Do NOT install ‚ÄúArduino_LSM6DS3 by Arduino‚Äù - that‚Äôs for different boards!\n\n\n\nEnter with the code below at the Arduino IDE and uploaded it to Kit:\n#include &lt;LSM6DS3.h&gt;\n#include &lt;Wire.h&gt;\n\n// Create IMU object using I2C interface\n// LSM6DS3TR-C sensor is located at I2C address 0x6A\nLSM6DS3 myIMU(I2C_MODE, 0x6A);\n\n// Variables to store sensor readings\nfloat accelX, accelY, accelZ;  // Accelerometer values (g-force)\nfloat gyroX, gyroY, gyroZ;     // Gyroscope values (degrees per second)\n\nvoid setup() {\n  // Initialize serial communication at 115200 baud rate\n  Serial.begin(115200);\n\n  // Wait for serial port to connect (useful for debugging)\n  while (!Serial) {\n    delay(10);\n  }\n\n  Serial.println(\"XIAOML Kit IMU Test\");\n  Serial.println(\"LSM6DS3TR-C 6-Axis IMU Sensor\");\n  Serial.println(\"=============================\");\n\n  // Initialize the IMU sensor\n  if (myIMU.begin() != 0) {\n    Serial.println(\"ERROR: IMU initialization failed!\");\n    Serial.println(\"Check connections and I2C address\");\n    while(1) {\n      delay(1000); // Halt execution if IMU fails to initialize\n    }\n  } else {\n    Serial.println(\"‚úì IMU initialized successfully\");\n    Serial.println();\n\n    // Print sensor information\n    Serial.println(\"Sensor Information:\");\n    Serial.println(\"- Accelerometer range: ¬±2g\");\n    Serial.println(\"- Gyroscope range: ¬±250 dps\");\n    Serial.println(\"- Communication: I2C at address 0x6A\");\n    Serial.println();\n\n    // Print data format explanation\n    Serial.println(\"Data Format:\");\n    Serial.println(\"AccelX,AccelY,AccelZ,GyroX,GyroY,GyroZ\");\n    Serial.println(\"Units: g-force (m/s¬≤), degrees/second\");\n    Serial.println();\n\n    delay(2000); // Brief pause before starting measurements\n  }\n}\n\nvoid loop() {\n  // Read accelerometer data (in g-force units)\n  accelX = myIMU.readFloatAccelX();\n  accelY = myIMU.readFloatAccelY();\n  accelZ = myIMU.readFloatAccelZ();\n\n  // Read gyroscope data (in degrees per second)\n  gyroX = myIMU.readFloatGyroX();\n  gyroY = myIMU.readFloatGyroY();\n  gyroZ = myIMU.readFloatGyroZ();\n\n  // Print readable format to Serial Monitor\n  Serial.print(\"Accelerometer (g): \");\n  Serial.print(\"X=\"); Serial.print(accelX, 3);\n  Serial.print(\" Y=\"); Serial.print(accelY, 3);\n  Serial.print(\" Z=\"); Serial.print(accelZ, 3);\n\n  Serial.print(\" | Gyroscope (¬∞/s): \");\n  Serial.print(\"X=\"); Serial.print(gyroX, 2);\n  Serial.print(\" Y=\"); Serial.print(gyroY, 2);\n  Serial.print(\" Z=\"); Serial.print(gyroZ, 2);\n  Serial.println();\n\n  // Print CSV format for Serial Plotter\n  Serial.println(String(accelX) + \",\" + String(accelY) + \",\" +\n                 String(accelZ) + \",\" + String(gyroX) + \",\" +\n                 String(gyroY) + \",\" + String(gyroZ));\n\n  // Update rate: 10 Hz (100ms delay)\n  delay(100);\n}\nThe Serial monitor will show the values, and the plotter will show their variation over time. For example, by moving the Kit over the y-axis, we will see that value 2 (red line) changes accordingly. Note that z-axis is represented by value 3 (green line), which is near 1.0g. The blue line (value 1) is related to the x-axis.\n \nYou can select the values 4 to 6 to see the Gyroscope behavior.",
    "crumbs": [
      "Seeed XIAO ESP32S3",
      "Setup"
    ]
  },
  {
    "objectID": "contents/seeed/xiao_esp32s3/setup/setup.html#sec-setup-testing-oled-display-ssd1306-0337",
    "href": "contents/seeed/xiao_esp32s3/setup/setup.html#sec-setup-testing-oled-display-ssd1306-0337",
    "title": "Setup",
    "section": "",
    "text": "OLED (Organic Light-Emitting Diode) displays are self-illuminating screens where each pixel produces its own light. The XIAO ML kit features a compact 0.42-inch monochrome OLED display, ideal for displaying sensor data, status information, and simple graphics.\n\n\n\nSize: 0.42 inches diagonal\nResolution: 72 √ó 40 pixels\nController: SSD1306\nInterface: I2C at address 0x3C\nColors: Monochrome (black pixels on white background, or vice versa)\nViewing: High contrast, visible in bright light\nPower: Low power consumption, no backlight needed\n\n\n\n\n\nPixel-perfect: Each of the 2,880 pixels (72√ó40) can be individually controlled\nFast refresh: Suitable for animations and real-time data\nNo ghosting: Instant pixel response\nWide viewing angle: Clear from multiple viewing positions\n\n\n\n\nBefore uploading the code, install the required library:\n\nOpen the Arduino IDE and select the ‚ÄúManage Libraries‚Äù (represented by the Books Icon).\nEnter u8g2 and select U8g2 by oliver. You can install or update the board support packages.\n‚ÑπÔ∏è Note: U8g2 is a powerful graphics library supporting many display types\n\n \nThe U8g2 library is a monochrome graphics library with these features:\n\nSupport for many display controllers (including SSD1306)\nText rendering with various fonts\nDrawing primitives (lines, rectangles, circles)\nMemory-efficient page-based rendering\nHardware and software I2C support\n\n\n\n\nEnter with the code below at the Arduino IDE and uploaded it to Kit:\n#include &lt;U8g2lib.h&gt;\n#include &lt;Wire.h&gt;\n\n// Initialize the OLED display\n// SSD1306 controller, 72x40 resolution, I2C interface\nU8G2_SSD1306_72X40_ER_1_HW_I2C u8g2(U8G2_R2, U8X8_PIN_NONE);\n\nvoid setup() {\n  Serial.begin(115200);\n\n  Serial.println(\"XIAOML Kit - Hello World\");\n  Serial.println(\"==========================\");\n\n  // Initialize the display\n  u8g2.begin();\n\n  Serial.println(\"‚úì Display initialized\");\n  Serial.println(\"Showing Hello World message...\");\n\n  // Clear the display\n  u8g2.clearDisplay();\n}\n\nvoid loop() {\n  // Start drawing sequence\n  u8g2.firstPage();\n  do {\n    // Set font\n    u8g2.setFont(u8g2_font_ncenB08_tr);\n\n    // Display \"Hello World\" centered\n    u8g2.setCursor(8, 15);\n    u8g2.print(\"Hello\");\n\n    u8g2.setCursor(12, 30);\n    u8g2.print(\"World!\");\n\n    // Add a simple decoration - draw a frame around the text\n    u8g2.drawFrame(2, 2, 68, 36);\n\n  } while (u8g2.nextPage());\n\n  // No delay needed - the display will show continuously\n}\nIf everything works fine, you should see at the display, ‚ÄúHello World‚Äù inside a rectangle.\n \n\n\n\n\nNote that the text is positioned with setCursor(x, y), in this case centered:\nu8g2.setCursor(8, 15);\nThe font used in the code was medium.\nu8g2.setFont(u8g2_font_ncenB08_tr);\nBut other font sizes are available:\n\nu8g2_font_4x6_tr: Tiny font (4√ó6 pixels)\nu8g2_font_6x10_tr: Small font (6√ó10 pixels)\nu8g2_font_ncenB08_tr: Medium bold font\nu8g2_font_ncenB14_tr: Large bold font\n\n\n\n\n\nThe code added a simple decoration, drawing a frame around the text\nu8g2.drawFrame(2, 2, 68, 36);\nBut other shapes are available:\n\nRectangle outline: drawFrame(x, y, width, height)\nFilled rectangle: drawBox(x, y, width, height)\nCircle: drawCircle(x, y, radius)\nLine: drawLine(x1, y1, x2, y2)\nIndividual pixels: drawPixel(x, y)\n\n\n\n\nThe display uses a coordinate system where:\n\nOrigin (0,0): Top-left corner\nX-axis: Increases from left to right (0 to 71)\nY-axis: Increases from top to bottom (0 to 39)\nText positioning: setCursor(x, y) where y is the baseline of text\n\n\n\n\n\nYou can change the rotation parameter by using:\n\nU8G2_R0: Normal orientation\nU8G2_R1: 90¬∞ clockwise\nU8G2_R2: 180¬∞ (upside down)\nU8G2_R3: 270¬∞ clockwise\n\n\n\n\n\n// Draw custom bitmap\nstatic const unsigned char myBitmap[] = {0x00, 0x3c, 0x42, 0x42, 0x3c, 0x00};\nu8g2.drawBitmap(x, y, 1, 6, myBitmap);\n\n\n\nint width = u8g2.getStrWidth(\"Hello\");  // Get text width\nint height = u8g2.getAscent();         // Get font height\nThe OLED display is now ready to show your sensor data, system status, or any custom graphics you design for your ML projects!",
    "crumbs": [
      "Seeed XIAO ESP32S3",
      "Setup"
    ]
  },
  {
    "objectID": "contents/seeed/xiao_esp32s3/setup/setup.html#sec-setup-summary-7446",
    "href": "contents/seeed/xiao_esp32s3/setup/setup.html#sec-setup-summary-7446",
    "title": "Setup",
    "section": "",
    "text": "The XIAOML Kit with ESP32S3 Sense represents a powerful, yet accessible entry point into the world of TinyML and embedded machine learning. Through this setup process, we have systematically tested every component of the XIAOML Kit, confirming that all sensors and peripherals are functioning correctly. The ESP32S3‚Äôs dual-core processor and 8MB of PSRAM provide sufficient computational power for real-time ML inference, while the OV2640 camera, digital microphone, LSM6DS3TR-C IMU, and 0.42‚Äù OLED display create a complete multimodal sensing platform. WiFi connectivity opens possibilities for edge-to-cloud ML workflows, and our Arduino IDE development environment is now properly configured with all necessary libraries.\nBeyond mere functionality tests, we‚Äôve gained practical insights into coordinate systems, data formats, and operational characteristics of each sensor‚Äîknowledge that will prove invaluable when designing ML data collection and preprocessing pipelines for the upcoming projects.\nThis setup process demonstrates key principles that extend far beyond this specific kit. Working with the ESP32S3‚Äôs memory limitations and processing capabilities provides an authentic experience with the resource constraints inherent in edge AI‚Äîthe same considerations that apply when deploying models on smartphones, IoT devices, or autonomous systems. Having multiple modalities (vision, audio, motion) on a single platform enables exploration of multimodal ML approaches, which are increasingly important in real-world AI applications.\nMost importantly, from raw sensor data to model inference to user feedback via the OLED display, the kit provides a complete ML deployment cycle in miniature, mirroring the challenges faced in production AI systems.\nWith this foundation in place, you‚Äôre now equipped to tackle the core TinyML applications in the following chapters:\n\nVision Projects: Leveraging the camera for image classification and object detection\nAudio Projects: Processing audio streams for keyword spotting and voice recognition\nMotion Projects: Using IMU data for activity recognition and anomaly detection\n\nEach application will build upon the hardware understanding and software infrastructure we‚Äôve established, demonstrating how artificial intelligence can be deployed not just in data centers, but in resource-constrained devices that directly interact with the physical world.\nThe principles encountered with this kit‚Äîreal-time processing, sensor fusion, and edge inference‚Äîare the same ones driving the future of AI deployment in autonomous vehicles, smart cities, medical devices, and industrial automation. By completing this setup successfully, you‚Äôre now prepared to explore this exciting frontier of embedded machine learning.",
    "crumbs": [
      "Seeed XIAO ESP32S3",
      "Setup"
    ]
  },
  {
    "objectID": "contents/seeed/xiao_esp32s3/setup/setup.html#sec-setup-resources-3946",
    "href": "contents/seeed/xiao_esp32s3/setup/setup.html#sec-setup-resources-3946",
    "title": "Setup",
    "section": "",
    "text": "XIAOML Kit Code\nXIAO ESP32S3 Sense manual & example code\nUsage of Seeed Studio XIAO ESP32S3 microphone\nFile System and XIAO ESP32S3 Sense\nCamera Usage in Seeed Studio XIAO ESP32S3 Sense",
    "crumbs": [
      "Seeed XIAO ESP32S3",
      "Setup"
    ]
  },
  {
    "objectID": "contents/seeed/xiao_esp32s3/setup/setup.html#sec-setup-heat-sink-considerations-38ea",
    "href": "contents/seeed/xiao_esp32s3/setup/setup.html#sec-setup-heat-sink-considerations-38ea",
    "title": "Setup",
    "section": "Heat Sink Considerations",
    "text": "Heat Sink Considerations\nIf you need to use the XIAO ESP32S3 Sense for camera applications WITHOUT the Expansion Board, you may install the heat sink.\nNote that having the heat sink installed, it is not possible to connect the XIAO ESP32S3 Sense with the Expansion Board.",
    "crumbs": [
      "Seeed XIAO ESP32S3",
      "Setup"
    ]
  },
  {
    "objectID": "contents/seeed/xiao_esp32s3/setup/setup.html#sec-setup-installing-heat-sink-3bde",
    "href": "contents/seeed/xiao_esp32s3/setup/setup.html#sec-setup-installing-heat-sink-3bde",
    "title": "Setup",
    "section": "Installing the Heat Sink",
    "text": "Installing the Heat Sink\nTo ensure optimal cooling for your XIAO ESP32S3 Sense, you should install the provided heat sink during camera applications. Its design is specifically tailored to address cooling needs, particularly during intensive operations such as camera usage.\n\nTwo heat sinks are included in the kit, but you can use only one to guarantee access to the Battery pins.\n\nInstallation:\n\nEnsure your device is powered off and unplugged from any power source before you start.\nPrioritize covering the Thermal PAD with the heat sink, as it is directly above the ESP32S3 chip, the primary source of heat. Proper alignment ensures optimal heat dissipation, and it is essential to keep the BAT pins as unobstructed as possible.\n\nNow, let‚Äôs begin the installation process:\nStep 1. Prepare the Heat Sink: Start by removing the protective cover from the heat sink to expose the thermal adhesive. This will prepare the heat sink for a secure attachment to the ESP32S3 chip.\nStep 2. Assemble the Heat Sink:\n \n\nAfter installation, ensure everything is properly secured with no risk of short circuits. Verify that the heat sink is properly aligned and securely attached.\n\nIf one heat synk is not enough, a second one can be installed, sharing both the thermal pad, but in this situation, be aware that all pins became unavailable.\n\n‚ö†Ô∏è Attention\nRemove carefully the heat sinks before using the IMU expansion board again",
    "crumbs": [
      "Seeed XIAO ESP32S3",
      "Setup"
    ]
  },
  {
    "objectID": "contents/seeed/xiao_esp32s3/kws/kws.html",
    "href": "contents/seeed/xiao_esp32s3/kws/kws.html",
    "title": "Keyword Spotting (KWS)",
    "section": "",
    "text": "DALL¬∑E prompt - 1950s style cartoon illustration based on a real image by Marcelo Rovai\n\n\n\n\nKeyword Spotting (KWS) is integral to many voice recognition systems, enabling devices to respond to specific words or phrases. While this technology underpins popular devices like Google Assistant or Amazon Alexa, it‚Äôs equally applicable and achievable on smaller, low-power devices. This lab will guide you through implementing a KWS system using TinyML on the XIAO ESP32S3 microcontroller board.\nThe XIAO ESP32S3, equipped with Espressif‚Äôs ESP32-S3 chip, is a compact and potent microcontroller offering a dual-core Xtensa LX7 processor, integrated Wi-Fi, and Bluetooth. Its balance of computational power, energy efficiency, and versatile connectivity makes it a fantastic platform for TinyML applications. Also, with its expansion board, we will have access to the ‚Äúsense‚Äù part of the device, which has a camera, an SD card slot, and a digital microphone. The integrated microphone and the SD card will be essential in this project.\nWe will use the Edge Impulse Studio, a powerful, user-friendly platform that simplifies creating and deploying machine learning models onto edge devices. We‚Äôll train a KWS model step-by-step, optimizing and deploying it onto the XIAO ESP32S3 Sense.\nOur model will be designed to recognize keywords that can trigger device wake-up or specific actions (in the case of ‚ÄúYES‚Äù), bringing your projects to life with voice-activated commands.\nLeveraging our experience with TensorFlow Lite for Microcontrollers (the engine ‚Äúunder the hood‚Äù on the EI Studio), we‚Äôll create a KWS system capable of real-time machine learning on the device.\nAs we progress through the lab, we‚Äôll break down each process stage ‚Äì from data collection and preparation to model training and deployment ‚Äì to provide a comprehensive understanding of implementing a KWS system on a microcontroller.\n\n\n\n\n\n\nTipLearning Objectives\n\n\n\n\nUnderstand Voice Assistant Architecture including cascaded detection systems and the role of edge-based keyword spotting as the first stage of voice processing pipelines\nMaster Audio Data Collection Techniques using both offline methods (XIAO ESP32S3 microphone with SD card storage) and online methods (smartphone integration with Edge Impulse Studio)\nImplement Digital Signal Processing for Audio including I2S protocol fundamentals, audio sampling at 16kHz/16-bit, and conversion between time-domain audio signals and frequency-domain features using MFCC\nTrain Convolutional Neural Networks for Audio Classification using transfer learning techniques, data augmentation strategies, and model optimization for four-class classification (YES, NO, NOISE, UNKNOWN)\nDeploy Optimized Models on Microcontrollers through quantization (INT8), memory management with PSRAM, and real-time inference optimization for embedded systems\nDevelop Complete Post-Processing Pipelines including confidence thresholding, GPIO control for external devices, OLED display integration, and creating standalone AI sensor systems\nCompare Development Workflows between no-code platforms (Edge Impulse Studio) and traditional embedded programming (Arduino IDE) for TinyML applications\n\n\n\n\n\n\n\n\nKeyword Spotting (KWS) is critical to many voice assistants, enabling devices to respond to specific words or phrases. To start, it is essential to realize that Voice Assistants on the market, like Google Home or Amazon Echo-Dot, only react to humans when they are ‚Äúwaked up‚Äù by particular keywords such as ‚Äú Hey Google‚Äù on the first one and ‚ÄúAlexa‚Äù on the second.\n \nIn other words, recognizing voice commands is based on a multi-stage model or Cascade Detection.\n \nStage 1: A smaller microprocessor inside the Echo Dot or Google Home continuously listens to the sound, waiting for the keyword to be spotted. For such detection, a TinyML model at the edge is used (KWS application).\nStage 2: Only when triggered by the KWS application on Stage 1 is the data sent to the cloud and processed on a larger model.\nThe video below shows an example where I emulate a Google Assistant on a Raspberry Pi (Stage 2), having an Arduino Nano 33 BLE as the tinyML device (Stage 1).\n\n\n\nIf you want to go deeper on the full project, please see my tutorial: Building an Intelligent Voice Assistant From Scratch.\n\nIn this lab, we will focus on Stage 1 (KWS or Keyword Spotting), where we will use the XIAO ESP2S3 Sense, which has a digital microphone for spotting the keyword.\n\n\n\nThe diagram below will give an idea of how the final KWS application should work (during inference):\n \nOur KWS application will recognize four classes of sound:\n\nYES (Keyword 1)\nNO (Keyword 2)\nNOISE (no keywords spoken, only background noise is present)\nUNKNOWN (a mix of different words than YES and NO)\n\n\nOptionally for real-world projects, it is always advised to include different words than keywords, such as ‚ÄúNoise‚Äù (or Background) and ‚ÄúUnknown.‚Äù\n\n\n\n\nThe main component of the KWS application is its model. So, we must train such a model with our specific keywords, noise, and other words (the ‚Äúunknown‚Äù):\n \n\n\n\n\nThe critical component of Machine Learning Workflow is the dataset. Once we have decided on specific keywords (YES and NO), we can take advantage of the dataset developed by Pete Warden, ‚ÄúSpeech Commands: A Dataset for Limited-Vocabulary Speech Recognition.‚Äù This dataset has 35 keywords (with +1,000 samples each), such as yes, no, stop, and go. In other words, we can get 1,500 samples of yes and no.\nYou can download a small portion of the dataset from Edge Studio (Keyword spotting pre-built dataset), which includes samples from the four classes we will use in this project: yes, no, noise, and background. For this, follow the steps below:\n\nDownload the keywords dataset.\nUnzip the file in a location of your choice.\n\nAlthough we have a lot of data from Pete‚Äôs dataset, collecting some words spoken by us is advised. When working with accelerometers, creating a dataset with data captured by the same type of sensor was essential. In the case of sound, the classification differs because it involves, in reality, audio data.\n\nThe key difference between sound and audio is their form of energy. Sound is mechanical wave energy (longitudinal sound waves) that propagate through a medium causing variations in pressure within the medium. Audio is made of electrical energy (analog or digital signals) that represent sound electrically.\n\nThe sound waves should be converted to audio data when we speak a keyword. The conversion should be done by sampling the signal generated by the microphone in 16 kHz with a 16-bit depth.\nSo, any device that can generate audio data with this basic specification (16 kHz/16 bits) will work fine. As a device, we can use the proper XIAO ESP32S3 Sense, a computer, or even your mobile phone.\n \nCapturing online Audio Data with Edge Impulse and a smartphone\nIn the lab Motion Classification and Anomaly Detection, we connect our device directly to Edge Impulse Studio for data capturing (having a sampling frequency of 50 Hz to 100 Hz). For such low frequency, we could use the EI CLI function Data Forwarder, but according to Jan Jongboom, Edge Impulse CTO, audio (16 kHz) goes too fast for the data forwarder to be captured. So, once we have the digital data captured by the microphone, we can turn it into a WAV file to be sent to the Studio via Data Uploader (same as we will do with Pete‚Äôs dataset).\n\nIf we want to collect audio data directly on the Studio, we can use any smartphone connected online with it. We will not explore this option here, but you can easily follow EI documentation.\n\n\n\nThe built-in microphone is the MSM261D3526H1CPM, a PDM digital output MEMS microphone with Multi-modes. Internally, it is connected to the ESP32S3 via an I2S bus using pins IO41 (Clock) and IO41 (Data).\n \nWhat is I2S?\nI2S, or Inter-IC Sound, is a standard protocol for transmitting digital audio from one device to another. It was initially developed by Philips Semiconductor (now NXP Semiconductors). It is commonly used in audio devices such as digital signal processors, digital audio processors, and, more recently, microcontrollers with digital audio capabilities (our case here).\nThe I2S protocol consists of at least three lines:\n \n1. Bit (or Serial) clock line (BCLK or CLK): This line toggles to indicate the start of a new bit of data (pin IO42).\n2. Word select line (WS): This line toggles to indicate the start of a new word (left channel or right channel). The Word select clock (WS) frequency defines the sample rate. In our case, L/R on the microphone is set to ground, meaning that we will use only the left channel (mono).\n3. Data line (SD): This line carries the audio data (pin IO41)\nIn an I2S data stream, the data is sent as a sequence of frames, each containing a left-channel word and a right-channel word. This makes I2S particularly suited for transmitting stereo audio data. However, it can also be used for mono or multichannel audio with additional data lines.\nLet‚Äôs start understanding how to capture raw data using the microphone. Go to the GitHub project and download the sketch: XIAOEsp2s3_Mic_Test:\n\n‚ö†Ô∏è Attention\n\nThe Xiao ESP32S3 MUST have the PSRAM enabled. You can check it on the Arduino IDE upper menu: Tools‚Äì&gt; PSRAM:OPI PSRAM\nThe Arduino Library (esp32 by Espressif Systems should be version 2.017. Please do not update it.\n\n\n/*\n  XIAO ESP32S3 Simple Mic Test\n*/\n\n#include &lt;I2S.h&gt;\n\nvoid setup() {\n  Serial.begin(115200);\n  while (!Serial) {\n  }\n\n  // start I2S at 16 kHz with 16-bits per sample\n  I2S.setAllPins(-1, 42, 41, -1, -1);\n  if (!I2S.begin(PDM_MONO_MODE, 16000, 16)) {\n    Serial.println(\"Failed to initialize I2S!\");\n    while (1); // do nothing\n  }\n}\n\nvoid loop() {\n  // read a sample\n  int sample = I2S.read();\n\n  if (sample && sample != -1 && sample != 1) {\n    Serial.println(sample);\n  }\n}\nThis code is a simple microphone test for the XIAO ESP32S3 using the I2S (Inter-IC Sound) interface. It sets up the I2S interface to capture audio data at a sample rate of 16 kHz with 16 bits per sample and then continuously reads samples from the microphone and prints them to the serial monitor.\nLet‚Äôs dig into the code‚Äôs main parts:\n\nInclude the I2S library: This library provides functions to configure and use the I2S interface, which is a standard for connecting digital audio devices.\nI2S.setAllPins(‚Äì1, 42, 41, ‚Äì1, ‚Äì1): This sets up the I2S pins. The parameters are (‚Äì1, 42, 41, ‚Äì1, ‚Äì1), where the second parameter (42) is the PIN for the I2S clock (CLK), and the third parameter (41) is the PIN for the I2S data (DATA) line. The other parameters are set to ‚Äì1, meaning those pins are not used.\nI2S.begin(PDM_MONO_MODE, 16000, 16): This initializes the I2S interface in Pulse Density Modulation (PDM) mono mode, with a sample rate of 16 kHz and 16 bits per sample. If the initialization fails, an error message is printed, and the program halts.\nint sample = I2S.read(): This reads an audio sample from the I2S interface.\n\nIf the sample is valid, it is printed on the Serial Monitor and Plotter.\nBelow is a test ‚Äúwhispering‚Äù in two different tones.\n \n\n\n\nLet‚Äôs use the onboard SD Card reader to save .wav audio files; we must habilitate the XIAO PSRAM first.\n\nESP32-S3 has only a few hundred kilobytes of internal RAM on the MCU chip. It can be insufficient for some purposes so that ESP32-S3 can use up to 16 MB of external PSRAM (Pseudo-static RAM) connected in parallel with the SPI flash chip. The external memory is incorporated in the memory map and, with certain restrictions, is usable in the same way as internal data RAM.\n\nFor a start, Insert the SD Card on the XIAO as shown in the photo below (the SD Card should be formatted to FAT32).\n \nTurn the PSRAM function of the ESP-32 chip on (Arduino IDE): Tools&gt;PSRAM: ‚ÄúOPI PSRAM‚Äù&gt;OPI PSRAM\n \n\nDownload the sketch Wav_Record_dataset, which you can find on the project‚Äôs GitHub.\n\nThis code records audio using the I2S interface of the Seeed XIAO ESP32S3 Sense board, saves the recording as a.wav file on an SD card, and allows for control of the recording process through commands sent from the serial monitor. The name of the audio file is customizable (it should be the class labels to be used with the training), and multiple recordings can be made, each saved in a new file. The code also includes functionality to increase the volume of the recordings.\nLet‚Äôs break down the most essential parts of it:\n#include &lt;I2S.h&gt;\n#include \"FS.h\"\n#include \"SD.h\"\n#include \"SPI.h\"\nThose are the necessary libraries for the program. I2S.h allows for audio input, FS.h provides file system handling capabilities, SD.h enables the program to interact with an SD card, and SPI.h handles the SPI communication with the SD card.\n#define RECORD_TIME   10\n#define SAMPLE_RATE 16000U\n#define SAMPLE_BITS 16\n#define WAV_HEADER_SIZE 44\n#define VOLUME_GAIN 2\nHere, various constants are defined for the program.\n\nRECORD_TIME specifies the length of the audio recording in seconds.\nSAMPLE_RATE and SAMPLE_BITS define the audio quality of the recording.\nWAV_HEADER_SIZE specifies the size of the .wav file header.\nVOLUME_GAIN is used to increase the volume of the recording.\n\nint fileNumber = 1;\nString baseFileName;\nbool isRecording = false;\nThese variables keep track of the current file number (to create unique file names), the base file name, and whether the system is currently recording.\nvoid setup() {\n  Serial.begin(115200);\n  while (!Serial);\n\n  I2S.setAllPins(-1, 42, 41, -1, -1);\n  if (!I2S.begin(PDM_MONO_MODE, SAMPLE_RATE, SAMPLE_BITS)) {\n    Serial.println(\"Failed to initialize I2S!\");\n    while (1);\n  }\n\n  if(!SD.begin(21)){\n    Serial.println(\"Failed to mount SD Card!\");\n    while (1);\n  }\n  Serial.printf(\"Enter with the label name\\n\");\n}\nThe setup function initializes the serial communication, I2S interface for audio input, and SD card interface. If the I2S did not initialize or the SD card fails to mount, it will print an error message and halt execution.\nvoid loop() {\n  if (Serial.available() &gt; 0) {\n    String command = Serial.readStringUntil('\\n');\n    command.trim();\n    if (command == \"rec\") {\n      isRecording = true;\n    } else {\n      baseFileName = command;\n      fileNumber = 1; //reset file number each time a new\n                        basefile name is set\n      Serial.printf(\"Send rec for starting recording label \\n\");\n    }\n  }\n  if (isRecording && baseFileName != \"\") {\n    String fileName = \"/\" + baseFileName + \".\"\n                      + String(fileNumber) + \".wav\";\n    fileNumber++;\n    record_wav(fileName);\n    delay(1000); // delay to avoid recording multiple files\n                    at once\n    isRecording = false;\n  }\n}\nIn the main loop, the program waits for a command from the serial monitor. If the command is rec, the program starts recording. Otherwise, the command is assumed to be the base name for the .wav files. If it‚Äôs currently recording and a base file name is set, it records the audio and saves it as a.wav file. The file names are generated by appending the file number to the base file name.\nvoid record_wav(String fileName)\n{\n  ...\n\n  File file = SD.open(fileName.c_str(), FILE_WRITE);\n  ...\n  rec_buffer = (uint8_t *)ps_malloc(record_size);\n  ...\n\n  esp_i2s::i2s_read(esp_i2s::I2S_NUM_0,\n                    rec_buffer,\n                    record_size,\n                    &sample_size,\n                    portMAX_DELAY);\n  ...\n}\nThis function records audio and saves it as a.wav file with the given name. It starts by initializing the sample_size and record_size variables. record_size is calculated based on the sample rate, size, and desired recording time. Let‚Äôs dig into the essential sections;\nFile file = SD.open(fileName.c_str(), FILE_WRITE);\n// Write the header to the WAV file\nuint8_t wav_header[WAV_HEADER_SIZE];\ngenerate_wav_header(wav_header, record_size, SAMPLE_RATE);\nfile.write(wav_header, WAV_HEADER_SIZE);\nThis section of the code opens the file on the SD card for writing and then generates the .wav file header using the generate_wav_header function. It then writes the header to the file.\n// PSRAM malloc for recording\nrec_buffer = (uint8_t *)ps_malloc(record_size);\nif (rec_buffer == NULL) {\n  Serial.printf(\"malloc failed!\\n\");\n  while(1) ;\n}\nSerial.printf(\"Buffer: %d bytes\\n\", ESP.getPsramSize()\n               - ESP.getFreePsram());\nThe ps_malloc function allocates memory in the PSRAM for the recording. If the allocation fails (i.e., rec_buffer is NULL), it prints an error message and halts execution.\n// Start recording\nesp_i2s::i2s_read(esp_i2s::I2S_NUM_0,\n         rec_buffer,\n         record_size,\n         &sample_size,\n         portMAX_DELAY);\nif (sample_size == 0) {\n  Serial.printf(\"Record Failed!\\n\");\n} else {\n    Serial.printf(\"Record %d bytes\\n\", sample_size);\n  }\nThe i2s_read function reads audio data from the microphone into rec_buffer. It prints an error message if no data is read (sample_size is 0).\n// Increase volume\nfor (uint32_t i = 0; i &lt; sample_size; i += SAMPLE_BITS/8) {\n  (*(uint16_t *)(rec_buffer+i)) &lt;&lt;= VOLUME_GAIN;\n}\nThis section of the code increases the recording volume by shifting the sample values by VOLUME_GAIN.\n// Write data to the WAV file\nSerial.printf(\"Writing to the file ...\\n\");\nif (file.write(rec_buffer, record_size) != record_size)\n  Serial.printf(\"Write file Failed!\\n\");\n\nfree(rec_buffer);\nfile.close();\nSerial.printf(\"Recording complete: \\n\");\nSerial.printf(\"Send rec for a new sample or enter\n                a new label\\n\\n\");\nFinally, the audio data is written to the .wav file. If the write operation fails, it prints an error message. After writing, the memory allocated for rec_buffer is freed, and the file is closed. The function finishes by printing a completion message and prompting the user to send a new command.\nvoid generate_wav_header(uint8_t *wav_header,\n             uint32_t wav_size,\n             uint32_t sample_rate)\n{\n  ...\n  memcpy(wav_header, set_wav_header, sizeof(set_wav_header));\n}\nThe generate_wav_header function creates a.wav file header based on the parameters (wav_size and sample_rate). It generates an array of bytes according to the .wav file format, which includes fields for the file size, audio format, number of channels, sample rate, byte rate, block alignment, bits per sample, and data size. The generated header is then copied into the wav_header array passed to the function.\nNow, upload the code to the XIAO and get samples from the keywords (yes and no). You can also capture noise and other words.\nThe Serial monitor will prompt you to receive the label to be recorded.\n \nSend the label (for example, yes). The program will wait for another command: rec\n \nAnd the program will start recording new samples every time a command rec is sent. The files will be saved as yes.1.wav, yes.2.wav, yes.3.wav, etc., until a new label (for example, no) is sent. In this case, you should send the command rec for each new sample, which will be saved as no.1.wav, no.2.wav, no.3.wav, etc.\n \nUltimately, we will get the saved files on the SD card.\n \nThe files are ready to be uploaded to Edge Impulse Studio\n\n\n\nThere are many ways to capture audio data; the simplest one is to use a mobile phone or a PC as a connected device on the Edge Impulse Studio.\n\nThe PC or smartphone should capture audio data with a sampling frequency of 16 kHz and a bit depth of 16 Bits.\n\nAnother alternative is to use dedicated apps. A good app for that is Voice Recorder Pro (IOS). You should save your records as .wav files and send them to your computer.\n \n\n\n\n\n\n\nWhen the raw dataset is defined and collected (Pete‚Äôs dataset + recorded keywords), we should initiate a new project at Edge Impulse Studio:\n \nOnce the project is created, select the Upload Existing Data tool in the Data Acquisition section. Choose the files to be uploaded:\n \nAnd upload them to the Studio (You can automatically split data in train/test). Repeat to all classes and all raw data.\n \nThe samples will now appear in the Data acquisition section.\n \nAll data on Pete‚Äôs dataset have a 1 s length, but the samples recorded in the previous section have 10 s and must be split into 1s samples to be compatible.\nClick on three dots after the sample name and select Split sample.\n \nOnce inside the tool, split the data into 1-second records. If necessary, add or remove segments:\n \nThis procedure should be repeated for all samples.\n\nNote: For longer audio files (minutes), first, split into 10-second segments and after that, use the tool again to get the final 1-second splits.\n\nSuppose we do not split data automatically in train/test during upload. In that case, we can do it manually (using the three dots menu, moving samples individually) or using Perform Train / Test Split on Dashboard ‚Äì Danger Zone.\n\nWe can optionally check all datasets using the tab Data Explorer.\n\n\n\n\nAn impulse takes raw data, uses signal processing to extract features, and then uses a learning block to classify new data.\n \nFirst, we will take the data points with a 1-second window, augmenting the data, sliding that window each 500 ms. Note that the option zero-pad data is set. It is essential to fill with zeros samples smaller than 1 second (in some cases, I reduced the 1000 ms window on the split tool to avoid noises and spikes).\nEach 1-second audio sample should be pre-processed and converted to an image (for example, \\(13\\times 49\\times 1\\)). We will use MFCC, which extracts features from audio signals using Mel Frequency Cepstral Coefficients, which are great for the human voice.\n \nNext, we select KERAS for classification and build our model from scratch by doing Image Classification using Convolution Neural Network).\n\n\n\nThe next step is to create the images to be trained in the next phase:\nWe can keep the default parameter values or take advantage of the DSP Autotuneparameters option, which we will do.\n \nThe result will not spend much memory to pre-process data (only 16KB). Still, the estimated processing time is high, 675 ms for an Espressif ESP-EYE (the closest reference available), with a 240 kHz clock (same as our device), but with a smaller CPU (XTensa LX6, versus the LX7 on the ESP32S). The real inference time should be smaller.\nSuppose we need to reduce the inference time later. In that case, we should return to the pre-processing stage and, for example, reduce the FFT length to 256, change the Number of coefficients, or another parameter.\nFor now, let‚Äôs keep the parameters defined by the Autotuning tool. Save parameters and generate the features.\n \n\nIf you want to go further with converting temporal serial data into images using FFT, Spectrogram, etc., you can play with this CoLab: Audio Raw Data Analysis.\n\n\n\n\nWe will use a Convolution Neural Network (CNN) model. The basic architecture is defined with two blocks of Conv1D + MaxPooling (with 8 and 16 neurons, respectively) and a 0.25 Dropout. And on the last layer, after Flattening four neurons, one for each class:\n \nAs hyper-parameters, we will have a Learning Rate of 0.005 and a model that will be trained by 100 epochs. We will also include data augmentation, as some noise. The result seems OK:\n \nIf you want to understand what is happening ‚Äúunder the hood,‚Äù you can download the dataset and run a Jupyter Notebook playing with the code. For example, you can analyze the accuracy by each epoch:\n \nThis CoLab Notebook can explain how you can go further: KWS Classifier Project - Looking ‚ÄúUnder the hood Training/xiao_esp32s3_keyword_spotting_project_nn_classifier.ipynb).‚Äù\n\n\n\n\nTesting the model with the data put apart before training (Test Data), we got an accuracy of approximately 87%.\n \nInspecting the F1 score, we can see that for YES, we got 0.95, an excellent result once we used this keyword to ‚Äútrigger‚Äù our postprocessing stage (turn on the built-in LED). Even for NO, we got 0.90. The worst result is for unknown, what is OK.\nWe can proceed with the project, but it is possible to perform Live Classification using a smartphone before deployment on our device. Go to the Live Classification section and click on Connect a Development board:\n \nPoint your phone to the barcode and select the link.\n \nYour phone will be connected to the Studio. Select the option Classification on the app, and when it is running, start testing your keywords, confirming that the model is working with live and real data:\n \n\n\n\nThe Studio will package all the needed libraries, preprocessing functions, and trained models, downloading them to your computer. Select the Arduino Library option, then choose Quantized (Int8) from the bottom menu and press Build.\n \nNow it is time for a real test. We will make inferences wholly disconnected from the Studio. Let‚Äôs change one of the ESP32 code examples created when you deploy the Arduino Library.\nIn your Arduino IDE, go to the File/Examples tab look for your project, and select esp32/esp32_microphone:\n \nThis code was created for the ESP-EYE built-in microphone, which should be adapted for our device.\nStart changing the libraries to handle the I2S bus:\n \nBy:\n#include &lt;I2S.h&gt;\n#define SAMPLE_RATE 16000U\n#define SAMPLE_BITS 16\nInitialize the IS2 microphone at setup(), including the lines:\nvoid setup()\n{\n...\n    I2S.setAllPins(-1, 42, 41, -1, -1);\n    if (!I2S.begin(PDM_MONO_MODE, SAMPLE_RATE, SAMPLE_BITS)) {\n      Serial.println(\"Failed to initialize I2S!\");\n    while (1) ;\n...\n}\nOn the static void capture_samples(void* arg) function, replace the line 153 that reads data from I2S mic:\n \nBy:\n/* read data at once from i2s */\nesp_i2s::i2s_read(esp_i2s::I2S_NUM_0,\n                 (void*)sampleBuffer,\n                 i2s_bytes_to_read,\n                 &bytes_read, 100);\nOn function static bool microphone_inference_start(uint32_t n_samples), we should comment or delete lines 198 to 200, where the microphone initialization function is called. This is unnecessary because the I2S microphone was already initialized during the setup().\n \nFinally, on static void microphone_inference_end(void) function, replace line 243:\n \nBy:\nstatic void microphone_inference_end(void)\n{\n    free(sampleBuffer);\n    ei_free(inference.buffer);\n}\nYou can find the complete code on the project‚Äôs GitHub. Upload the sketch to your board and test some real inferences:\n\n‚ö†Ô∏è Attention\n\nThe Xiao ESP32S3 MUST have the PSRAM enabled. You can check it on the Arduino IDE upper menu: Tools‚Äì&gt; PSRAM:OPI PSRAM\nThe Arduino Library (esp32 by Espressif Systems should be version 2.017. Please do not update it.\n\n\n \n\n\n\nIn edge AI applications, the inference result is only as valuable as our ability to act upon it. While serial output provides detailed information for debugging and development, real-world deployments require immediate, human-readable feedback that doesn‚Äôt depend on external monitors or connections.\nLet‚Äôs explore two post-processing approaches. Using the internal XIAO‚Äôs LED and the OLED on the XIAOML Kit.\n\n\nNow that we know the model is working by detecting our keywords, let‚Äôs modify the code to see the internal LED go on every time a YES is detected.\nYou should initialize the LED:\n#define LED_BUILT_IN 21\n...\nvoid setup()\n{\n...\n  pinMode(LED_BUILT_IN, OUTPUT); // Set the pin as output\n  digitalWrite(LED_BUILT_IN, HIGH); //Turn off\n...\n}\nAnd change the // print the predictions portion of the previous code (on loop():\nint pred_index = 0;     // Initialize pred_index\nfloat pred_value = 0;   // Initialize pred_value\n\n// print the predictions\nei_printf(\"Predictions \");\nei_printf(\"(DSP: %d ms., Classification: %d ms., Anomaly: %d ms.)\",\n     result.timing.dsp, result.timing.classification,\n     result.timing.anomaly);\nei_printf(\": \\n\");\nfor (size_t ix = 0; ix &lt; EI_CLASSIFIER_LABEL_COUNT; ix++) {\n      ei_printf(\"    %s: \", result.classification[ix].label);\n      ei_printf_float(result.classification[ix].value);\n      ei_printf(\"\\n\");\n\n      if (result.classification[ix].value &gt; pred_value){\n         pred_index = ix;\n         pred_value = result.classification[ix].value;\n      }\n}\n\n// show the inference result on LED\nif (pred_index == 3){\n    digitalWrite(LED_BUILT_IN, LOW); //Turn on\n}\nelse{\n   digitalWrite(LED_BUILT_IN, HIGH); //Turn off\n}\nYou can find the complete code on the project‚Äôs GitHub. Upload the sketch to your board and test some real inferences:\n \nThe idea is that the LED will be ON whenever the keyword YES is detected. In the same way, instead of turning on an LED, this could be a ‚Äútrigger‚Äù for an external device, as we saw in the introduction.\n\n\n\n\n\nThe XIAOML Kit tiny 0.42‚Äù OLED display (72√ó40 pixels) serves as a crucial post-processing component that transforms raw ML inference results into immediate, human-readable feedback‚Äîdisplaying detected class names and confidence levels directly on the device, eliminating the need for external monitors and enabling truly standalone edge AI deployment in industrial, agricultural, or retail environments where instant visual confirmation of AI predictions is essential.\nSo, let‚Äôs modify the sketch to automatically adapt to the model trained on Edge Impulse by reading the class names and count directly from the model. Download the code from GitHub: xiaoml-kit_kws_oled.\nRunning the code, we can see the result:\n \n\n\n\n\nThis lab demonstrated the complete development cycle of a keyword spotting system using the XIAOML Kit, showcasing how modern TinyML platforms make sophisticated audio AI accessible on resource-constrained devices. Through hands-on implementation, we‚Äôve bridged the gap between theoretical machine learning concepts and practical embedded AI deployment.\nTechnical Achievements:\nThe project successfully implemented a complete audio processing pipeline from raw sound capture through real-time inference. Using the XIAO ESP32S3‚Äôs integrated digital microphone, we captured audio data at professional quality (16kHz/16-bit) and processed it using Mel Frequency Cepstral Coefficients (MFCC) for feature extraction. The deployed CNN model achieved excellent accuracy in distinguishing between our target keywords (‚ÄúYES‚Äù, ‚ÄúNO‚Äù) and background conditions (‚ÄúNOISE‚Äù, ‚ÄúUNKNOWN‚Äù), with inference times suitable for real-time applications.\nPlatform Integration:\nEdge Impulse Studio proved invaluable as a comprehensive MLOps platform for embedded systems, handling everything from data collection and labeling through model training, optimization, and deployment. The seamless integration between cloud-based training and edge deployment exemplifies modern TinyML workflows, while the Arduino IDE provided the flexibility needed for custom post-processing implementations.\nReal-World Applications:\nThe techniques learned extend far beyond simple keyword detection. Voice-activated control systems, industrial safety monitoring through sound classification, medical applications for respiratory analysis, and environmental monitoring for wildlife or equipment sounds all leverage similar audio processing approaches. The cascaded detection architecture demonstrated here‚Äîusing edge-based KWS to trigger more complex cloud processing‚Äîis fundamental to modern voice assistant systems.\nEmbedded AI Principles:\nThis project highlighted crucial TinyML considerations, including power management, memory optimization through PSRAM utilization, and the trade-offs between model complexity and inference speed. The successful deployment of a neural network performing real-time audio analysis on a microcontroller demonstrates how AI capabilities, once requiring powerful desktop computers, can now operate on battery-powered devices.\nDevelopment Methodology:\nWe explored multiple development pathways, from data collection strategies (offline SD card storage versus online streaming) to deployment options (Edge Impulse‚Äôs automated library generation versus custom Arduino implementation). This flexibility is crucial for adapting to various project requirements and constraints.\nFuture Directions:\nThe foundation established here enables the exploration of more advanced audio AI applications. Multi-keyword recognition, speaker identification, emotion detection from voice, and environmental sound classification all build upon the same core techniques. The integration capabilities demonstrated with OLED displays and GPIO control illustrate how KWS can serve as the intelligent interface for broader IoT systems.\nConsider that Sound Classification encompasses much more than just voice recognition. This project‚Äôs techniques apply across numerous domains:\n\nSecurity Applications: Broken glass detection, intrusion monitoring, gunshot detection\nIndustrial IoT: Machinery health monitoring, anomaly detection in manufacturing equipment\nHealthcare: Sleep disorder monitoring, respiratory condition assessment, elderly care systems\nEnvironmental Monitoring: Wildlife tracking, urban noise analysis, smart building acoustic management\nSmart Home Integration: Multi-room voice control, appliance status monitoring through sound signatures\n\nKey Takeaways:\nThe XIAOML Kit proves that professional-grade AI development is achievable with accessible tools and modest budgets. The combination of capable hardware (ESP32S3 with PSRAM and integrated sensors), mature development platforms (Edge Impulse Studio), and comprehensive software libraries creates an environment where complex AI concepts become tangible, working systems.\nThis lab demonstrates that the future of AI isn‚Äôt just in massive data centers, but in intelligent edge devices that can process, understand, and respond to their environment in real-time‚Äîopening possibilities for ubiquitous, privacy-preserving, and responsive artificial intelligence systems.\n\n\n\n\nXIAO ESP32S3 Codes\nXIAOML Kit Code\nSubset of Google Speech Commands Dataset\nKWS MFCC Analysis Colab Notebook\nKWS CNN training Colab Notebook\nXIAO ESP32S3 Post-processing Code\nEdge Impulse Project",
    "crumbs": [
      "Seeed XIAO ESP32S3",
      "Keyword Spotting (KWS)"
    ]
  },
  {
    "objectID": "contents/seeed/xiao_esp32s3/kws/kws.html#sec-keyword-spotting-kws-overview-4373",
    "href": "contents/seeed/xiao_esp32s3/kws/kws.html#sec-keyword-spotting-kws-overview-4373",
    "title": "Keyword Spotting (KWS)",
    "section": "",
    "text": "Keyword Spotting (KWS) is integral to many voice recognition systems, enabling devices to respond to specific words or phrases. While this technology underpins popular devices like Google Assistant or Amazon Alexa, it‚Äôs equally applicable and achievable on smaller, low-power devices. This lab will guide you through implementing a KWS system using TinyML on the XIAO ESP32S3 microcontroller board.\nThe XIAO ESP32S3, equipped with Espressif‚Äôs ESP32-S3 chip, is a compact and potent microcontroller offering a dual-core Xtensa LX7 processor, integrated Wi-Fi, and Bluetooth. Its balance of computational power, energy efficiency, and versatile connectivity makes it a fantastic platform for TinyML applications. Also, with its expansion board, we will have access to the ‚Äúsense‚Äù part of the device, which has a camera, an SD card slot, and a digital microphone. The integrated microphone and the SD card will be essential in this project.\nWe will use the Edge Impulse Studio, a powerful, user-friendly platform that simplifies creating and deploying machine learning models onto edge devices. We‚Äôll train a KWS model step-by-step, optimizing and deploying it onto the XIAO ESP32S3 Sense.\nOur model will be designed to recognize keywords that can trigger device wake-up or specific actions (in the case of ‚ÄúYES‚Äù), bringing your projects to life with voice-activated commands.\nLeveraging our experience with TensorFlow Lite for Microcontrollers (the engine ‚Äúunder the hood‚Äù on the EI Studio), we‚Äôll create a KWS system capable of real-time machine learning on the device.\nAs we progress through the lab, we‚Äôll break down each process stage ‚Äì from data collection and preparation to model training and deployment ‚Äì to provide a comprehensive understanding of implementing a KWS system on a microcontroller.\n\n\n\n\n\n\nTipLearning Objectives\n\n\n\n\nUnderstand Voice Assistant Architecture including cascaded detection systems and the role of edge-based keyword spotting as the first stage of voice processing pipelines\nMaster Audio Data Collection Techniques using both offline methods (XIAO ESP32S3 microphone with SD card storage) and online methods (smartphone integration with Edge Impulse Studio)\nImplement Digital Signal Processing for Audio including I2S protocol fundamentals, audio sampling at 16kHz/16-bit, and conversion between time-domain audio signals and frequency-domain features using MFCC\nTrain Convolutional Neural Networks for Audio Classification using transfer learning techniques, data augmentation strategies, and model optimization for four-class classification (YES, NO, NOISE, UNKNOWN)\nDeploy Optimized Models on Microcontrollers through quantization (INT8), memory management with PSRAM, and real-time inference optimization for embedded systems\nDevelop Complete Post-Processing Pipelines including confidence thresholding, GPIO control for external devices, OLED display integration, and creating standalone AI sensor systems\nCompare Development Workflows between no-code platforms (Edge Impulse Studio) and traditional embedded programming (Arduino IDE) for TinyML applications",
    "crumbs": [
      "Seeed XIAO ESP32S3",
      "Keyword Spotting (KWS)"
    ]
  },
  {
    "objectID": "contents/seeed/xiao_esp32s3/kws/kws.html#sec-keyword-spotting-kws-kws-project-639f",
    "href": "contents/seeed/xiao_esp32s3/kws/kws.html#sec-keyword-spotting-kws-kws-project-639f",
    "title": "Keyword Spotting (KWS)",
    "section": "",
    "text": "Keyword Spotting (KWS) is critical to many voice assistants, enabling devices to respond to specific words or phrases. To start, it is essential to realize that Voice Assistants on the market, like Google Home or Amazon Echo-Dot, only react to humans when they are ‚Äúwaked up‚Äù by particular keywords such as ‚Äú Hey Google‚Äù on the first one and ‚ÄúAlexa‚Äù on the second.\n \nIn other words, recognizing voice commands is based on a multi-stage model or Cascade Detection.\n \nStage 1: A smaller microprocessor inside the Echo Dot or Google Home continuously listens to the sound, waiting for the keyword to be spotted. For such detection, a TinyML model at the edge is used (KWS application).\nStage 2: Only when triggered by the KWS application on Stage 1 is the data sent to the cloud and processed on a larger model.\nThe video below shows an example where I emulate a Google Assistant on a Raspberry Pi (Stage 2), having an Arduino Nano 33 BLE as the tinyML device (Stage 1).\n\n\n\nIf you want to go deeper on the full project, please see my tutorial: Building an Intelligent Voice Assistant From Scratch.\n\nIn this lab, we will focus on Stage 1 (KWS or Keyword Spotting), where we will use the XIAO ESP2S3 Sense, which has a digital microphone for spotting the keyword.\n\n\n\nThe diagram below will give an idea of how the final KWS application should work (during inference):\n \nOur KWS application will recognize four classes of sound:\n\nYES (Keyword 1)\nNO (Keyword 2)\nNOISE (no keywords spoken, only background noise is present)\nUNKNOWN (a mix of different words than YES and NO)\n\n\nOptionally for real-world projects, it is always advised to include different words than keywords, such as ‚ÄúNoise‚Äù (or Background) and ‚ÄúUnknown.‚Äù\n\n\n\n\nThe main component of the KWS application is its model. So, we must train such a model with our specific keywords, noise, and other words (the ‚Äúunknown‚Äù):",
    "crumbs": [
      "Seeed XIAO ESP32S3",
      "Keyword Spotting (KWS)"
    ]
  },
  {
    "objectID": "contents/seeed/xiao_esp32s3/kws/kws.html#sec-keyword-spotting-kws-dataset-6ea2",
    "href": "contents/seeed/xiao_esp32s3/kws/kws.html#sec-keyword-spotting-kws-dataset-6ea2",
    "title": "Keyword Spotting (KWS)",
    "section": "",
    "text": "The critical component of Machine Learning Workflow is the dataset. Once we have decided on specific keywords (YES and NO), we can take advantage of the dataset developed by Pete Warden, ‚ÄúSpeech Commands: A Dataset for Limited-Vocabulary Speech Recognition.‚Äù This dataset has 35 keywords (with +1,000 samples each), such as yes, no, stop, and go. In other words, we can get 1,500 samples of yes and no.\nYou can download a small portion of the dataset from Edge Studio (Keyword spotting pre-built dataset), which includes samples from the four classes we will use in this project: yes, no, noise, and background. For this, follow the steps below:\n\nDownload the keywords dataset.\nUnzip the file in a location of your choice.\n\nAlthough we have a lot of data from Pete‚Äôs dataset, collecting some words spoken by us is advised. When working with accelerometers, creating a dataset with data captured by the same type of sensor was essential. In the case of sound, the classification differs because it involves, in reality, audio data.\n\nThe key difference between sound and audio is their form of energy. Sound is mechanical wave energy (longitudinal sound waves) that propagate through a medium causing variations in pressure within the medium. Audio is made of electrical energy (analog or digital signals) that represent sound electrically.\n\nThe sound waves should be converted to audio data when we speak a keyword. The conversion should be done by sampling the signal generated by the microphone in 16 kHz with a 16-bit depth.\nSo, any device that can generate audio data with this basic specification (16 kHz/16 bits) will work fine. As a device, we can use the proper XIAO ESP32S3 Sense, a computer, or even your mobile phone.\n \nCapturing online Audio Data with Edge Impulse and a smartphone\nIn the lab Motion Classification and Anomaly Detection, we connect our device directly to Edge Impulse Studio for data capturing (having a sampling frequency of 50 Hz to 100 Hz). For such low frequency, we could use the EI CLI function Data Forwarder, but according to Jan Jongboom, Edge Impulse CTO, audio (16 kHz) goes too fast for the data forwarder to be captured. So, once we have the digital data captured by the microphone, we can turn it into a WAV file to be sent to the Studio via Data Uploader (same as we will do with Pete‚Äôs dataset).\n\nIf we want to collect audio data directly on the Studio, we can use any smartphone connected online with it. We will not explore this option here, but you can easily follow EI documentation.\n\n\n\nThe built-in microphone is the MSM261D3526H1CPM, a PDM digital output MEMS microphone with Multi-modes. Internally, it is connected to the ESP32S3 via an I2S bus using pins IO41 (Clock) and IO41 (Data).\n \nWhat is I2S?\nI2S, or Inter-IC Sound, is a standard protocol for transmitting digital audio from one device to another. It was initially developed by Philips Semiconductor (now NXP Semiconductors). It is commonly used in audio devices such as digital signal processors, digital audio processors, and, more recently, microcontrollers with digital audio capabilities (our case here).\nThe I2S protocol consists of at least three lines:\n \n1. Bit (or Serial) clock line (BCLK or CLK): This line toggles to indicate the start of a new bit of data (pin IO42).\n2. Word select line (WS): This line toggles to indicate the start of a new word (left channel or right channel). The Word select clock (WS) frequency defines the sample rate. In our case, L/R on the microphone is set to ground, meaning that we will use only the left channel (mono).\n3. Data line (SD): This line carries the audio data (pin IO41)\nIn an I2S data stream, the data is sent as a sequence of frames, each containing a left-channel word and a right-channel word. This makes I2S particularly suited for transmitting stereo audio data. However, it can also be used for mono or multichannel audio with additional data lines.\nLet‚Äôs start understanding how to capture raw data using the microphone. Go to the GitHub project and download the sketch: XIAOEsp2s3_Mic_Test:\n\n‚ö†Ô∏è Attention\n\nThe Xiao ESP32S3 MUST have the PSRAM enabled. You can check it on the Arduino IDE upper menu: Tools‚Äì&gt; PSRAM:OPI PSRAM\nThe Arduino Library (esp32 by Espressif Systems should be version 2.017. Please do not update it.\n\n\n/*\n  XIAO ESP32S3 Simple Mic Test\n*/\n\n#include &lt;I2S.h&gt;\n\nvoid setup() {\n  Serial.begin(115200);\n  while (!Serial) {\n  }\n\n  // start I2S at 16 kHz with 16-bits per sample\n  I2S.setAllPins(-1, 42, 41, -1, -1);\n  if (!I2S.begin(PDM_MONO_MODE, 16000, 16)) {\n    Serial.println(\"Failed to initialize I2S!\");\n    while (1); // do nothing\n  }\n}\n\nvoid loop() {\n  // read a sample\n  int sample = I2S.read();\n\n  if (sample && sample != -1 && sample != 1) {\n    Serial.println(sample);\n  }\n}\nThis code is a simple microphone test for the XIAO ESP32S3 using the I2S (Inter-IC Sound) interface. It sets up the I2S interface to capture audio data at a sample rate of 16 kHz with 16 bits per sample and then continuously reads samples from the microphone and prints them to the serial monitor.\nLet‚Äôs dig into the code‚Äôs main parts:\n\nInclude the I2S library: This library provides functions to configure and use the I2S interface, which is a standard for connecting digital audio devices.\nI2S.setAllPins(‚Äì1, 42, 41, ‚Äì1, ‚Äì1): This sets up the I2S pins. The parameters are (‚Äì1, 42, 41, ‚Äì1, ‚Äì1), where the second parameter (42) is the PIN for the I2S clock (CLK), and the third parameter (41) is the PIN for the I2S data (DATA) line. The other parameters are set to ‚Äì1, meaning those pins are not used.\nI2S.begin(PDM_MONO_MODE, 16000, 16): This initializes the I2S interface in Pulse Density Modulation (PDM) mono mode, with a sample rate of 16 kHz and 16 bits per sample. If the initialization fails, an error message is printed, and the program halts.\nint sample = I2S.read(): This reads an audio sample from the I2S interface.\n\nIf the sample is valid, it is printed on the Serial Monitor and Plotter.\nBelow is a test ‚Äúwhispering‚Äù in two different tones.\n \n\n\n\nLet‚Äôs use the onboard SD Card reader to save .wav audio files; we must habilitate the XIAO PSRAM first.\n\nESP32-S3 has only a few hundred kilobytes of internal RAM on the MCU chip. It can be insufficient for some purposes so that ESP32-S3 can use up to 16 MB of external PSRAM (Pseudo-static RAM) connected in parallel with the SPI flash chip. The external memory is incorporated in the memory map and, with certain restrictions, is usable in the same way as internal data RAM.\n\nFor a start, Insert the SD Card on the XIAO as shown in the photo below (the SD Card should be formatted to FAT32).\n \nTurn the PSRAM function of the ESP-32 chip on (Arduino IDE): Tools&gt;PSRAM: ‚ÄúOPI PSRAM‚Äù&gt;OPI PSRAM\n \n\nDownload the sketch Wav_Record_dataset, which you can find on the project‚Äôs GitHub.\n\nThis code records audio using the I2S interface of the Seeed XIAO ESP32S3 Sense board, saves the recording as a.wav file on an SD card, and allows for control of the recording process through commands sent from the serial monitor. The name of the audio file is customizable (it should be the class labels to be used with the training), and multiple recordings can be made, each saved in a new file. The code also includes functionality to increase the volume of the recordings.\nLet‚Äôs break down the most essential parts of it:\n#include &lt;I2S.h&gt;\n#include \"FS.h\"\n#include \"SD.h\"\n#include \"SPI.h\"\nThose are the necessary libraries for the program. I2S.h allows for audio input, FS.h provides file system handling capabilities, SD.h enables the program to interact with an SD card, and SPI.h handles the SPI communication with the SD card.\n#define RECORD_TIME   10\n#define SAMPLE_RATE 16000U\n#define SAMPLE_BITS 16\n#define WAV_HEADER_SIZE 44\n#define VOLUME_GAIN 2\nHere, various constants are defined for the program.\n\nRECORD_TIME specifies the length of the audio recording in seconds.\nSAMPLE_RATE and SAMPLE_BITS define the audio quality of the recording.\nWAV_HEADER_SIZE specifies the size of the .wav file header.\nVOLUME_GAIN is used to increase the volume of the recording.\n\nint fileNumber = 1;\nString baseFileName;\nbool isRecording = false;\nThese variables keep track of the current file number (to create unique file names), the base file name, and whether the system is currently recording.\nvoid setup() {\n  Serial.begin(115200);\n  while (!Serial);\n\n  I2S.setAllPins(-1, 42, 41, -1, -1);\n  if (!I2S.begin(PDM_MONO_MODE, SAMPLE_RATE, SAMPLE_BITS)) {\n    Serial.println(\"Failed to initialize I2S!\");\n    while (1);\n  }\n\n  if(!SD.begin(21)){\n    Serial.println(\"Failed to mount SD Card!\");\n    while (1);\n  }\n  Serial.printf(\"Enter with the label name\\n\");\n}\nThe setup function initializes the serial communication, I2S interface for audio input, and SD card interface. If the I2S did not initialize or the SD card fails to mount, it will print an error message and halt execution.\nvoid loop() {\n  if (Serial.available() &gt; 0) {\n    String command = Serial.readStringUntil('\\n');\n    command.trim();\n    if (command == \"rec\") {\n      isRecording = true;\n    } else {\n      baseFileName = command;\n      fileNumber = 1; //reset file number each time a new\n                        basefile name is set\n      Serial.printf(\"Send rec for starting recording label \\n\");\n    }\n  }\n  if (isRecording && baseFileName != \"\") {\n    String fileName = \"/\" + baseFileName + \".\"\n                      + String(fileNumber) + \".wav\";\n    fileNumber++;\n    record_wav(fileName);\n    delay(1000); // delay to avoid recording multiple files\n                    at once\n    isRecording = false;\n  }\n}\nIn the main loop, the program waits for a command from the serial monitor. If the command is rec, the program starts recording. Otherwise, the command is assumed to be the base name for the .wav files. If it‚Äôs currently recording and a base file name is set, it records the audio and saves it as a.wav file. The file names are generated by appending the file number to the base file name.\nvoid record_wav(String fileName)\n{\n  ...\n\n  File file = SD.open(fileName.c_str(), FILE_WRITE);\n  ...\n  rec_buffer = (uint8_t *)ps_malloc(record_size);\n  ...\n\n  esp_i2s::i2s_read(esp_i2s::I2S_NUM_0,\n                    rec_buffer,\n                    record_size,\n                    &sample_size,\n                    portMAX_DELAY);\n  ...\n}\nThis function records audio and saves it as a.wav file with the given name. It starts by initializing the sample_size and record_size variables. record_size is calculated based on the sample rate, size, and desired recording time. Let‚Äôs dig into the essential sections;\nFile file = SD.open(fileName.c_str(), FILE_WRITE);\n// Write the header to the WAV file\nuint8_t wav_header[WAV_HEADER_SIZE];\ngenerate_wav_header(wav_header, record_size, SAMPLE_RATE);\nfile.write(wav_header, WAV_HEADER_SIZE);\nThis section of the code opens the file on the SD card for writing and then generates the .wav file header using the generate_wav_header function. It then writes the header to the file.\n// PSRAM malloc for recording\nrec_buffer = (uint8_t *)ps_malloc(record_size);\nif (rec_buffer == NULL) {\n  Serial.printf(\"malloc failed!\\n\");\n  while(1) ;\n}\nSerial.printf(\"Buffer: %d bytes\\n\", ESP.getPsramSize()\n               - ESP.getFreePsram());\nThe ps_malloc function allocates memory in the PSRAM for the recording. If the allocation fails (i.e., rec_buffer is NULL), it prints an error message and halts execution.\n// Start recording\nesp_i2s::i2s_read(esp_i2s::I2S_NUM_0,\n         rec_buffer,\n         record_size,\n         &sample_size,\n         portMAX_DELAY);\nif (sample_size == 0) {\n  Serial.printf(\"Record Failed!\\n\");\n} else {\n    Serial.printf(\"Record %d bytes\\n\", sample_size);\n  }\nThe i2s_read function reads audio data from the microphone into rec_buffer. It prints an error message if no data is read (sample_size is 0).\n// Increase volume\nfor (uint32_t i = 0; i &lt; sample_size; i += SAMPLE_BITS/8) {\n  (*(uint16_t *)(rec_buffer+i)) &lt;&lt;= VOLUME_GAIN;\n}\nThis section of the code increases the recording volume by shifting the sample values by VOLUME_GAIN.\n// Write data to the WAV file\nSerial.printf(\"Writing to the file ...\\n\");\nif (file.write(rec_buffer, record_size) != record_size)\n  Serial.printf(\"Write file Failed!\\n\");\n\nfree(rec_buffer);\nfile.close();\nSerial.printf(\"Recording complete: \\n\");\nSerial.printf(\"Send rec for a new sample or enter\n                a new label\\n\\n\");\nFinally, the audio data is written to the .wav file. If the write operation fails, it prints an error message. After writing, the memory allocated for rec_buffer is freed, and the file is closed. The function finishes by printing a completion message and prompting the user to send a new command.\nvoid generate_wav_header(uint8_t *wav_header,\n             uint32_t wav_size,\n             uint32_t sample_rate)\n{\n  ...\n  memcpy(wav_header, set_wav_header, sizeof(set_wav_header));\n}\nThe generate_wav_header function creates a.wav file header based on the parameters (wav_size and sample_rate). It generates an array of bytes according to the .wav file format, which includes fields for the file size, audio format, number of channels, sample rate, byte rate, block alignment, bits per sample, and data size. The generated header is then copied into the wav_header array passed to the function.\nNow, upload the code to the XIAO and get samples from the keywords (yes and no). You can also capture noise and other words.\nThe Serial monitor will prompt you to receive the label to be recorded.\n \nSend the label (for example, yes). The program will wait for another command: rec\n \nAnd the program will start recording new samples every time a command rec is sent. The files will be saved as yes.1.wav, yes.2.wav, yes.3.wav, etc., until a new label (for example, no) is sent. In this case, you should send the command rec for each new sample, which will be saved as no.1.wav, no.2.wav, no.3.wav, etc.\n \nUltimately, we will get the saved files on the SD card.\n \nThe files are ready to be uploaded to Edge Impulse Studio\n\n\n\nThere are many ways to capture audio data; the simplest one is to use a mobile phone or a PC as a connected device on the Edge Impulse Studio.\n\nThe PC or smartphone should capture audio data with a sampling frequency of 16 kHz and a bit depth of 16 Bits.\n\nAnother alternative is to use dedicated apps. A good app for that is Voice Recorder Pro (IOS). You should save your records as .wav files and send them to your computer.",
    "crumbs": [
      "Seeed XIAO ESP32S3",
      "Keyword Spotting (KWS)"
    ]
  },
  {
    "objectID": "contents/seeed/xiao_esp32s3/kws/kws.html#sec-keyword-spotting-kws-training-model-edge-impulse-studio-804e",
    "href": "contents/seeed/xiao_esp32s3/kws/kws.html#sec-keyword-spotting-kws-training-model-edge-impulse-studio-804e",
    "title": "Keyword Spotting (KWS)",
    "section": "",
    "text": "When the raw dataset is defined and collected (Pete‚Äôs dataset + recorded keywords), we should initiate a new project at Edge Impulse Studio:\n \nOnce the project is created, select the Upload Existing Data tool in the Data Acquisition section. Choose the files to be uploaded:\n \nAnd upload them to the Studio (You can automatically split data in train/test). Repeat to all classes and all raw data.\n \nThe samples will now appear in the Data acquisition section.\n \nAll data on Pete‚Äôs dataset have a 1 s length, but the samples recorded in the previous section have 10 s and must be split into 1s samples to be compatible.\nClick on three dots after the sample name and select Split sample.\n \nOnce inside the tool, split the data into 1-second records. If necessary, add or remove segments:\n \nThis procedure should be repeated for all samples.\n\nNote: For longer audio files (minutes), first, split into 10-second segments and after that, use the tool again to get the final 1-second splits.\n\nSuppose we do not split data automatically in train/test during upload. In that case, we can do it manually (using the three dots menu, moving samples individually) or using Perform Train / Test Split on Dashboard ‚Äì Danger Zone.\n\nWe can optionally check all datasets using the tab Data Explorer.\n\n\n\n\nAn impulse takes raw data, uses signal processing to extract features, and then uses a learning block to classify new data.\n \nFirst, we will take the data points with a 1-second window, augmenting the data, sliding that window each 500 ms. Note that the option zero-pad data is set. It is essential to fill with zeros samples smaller than 1 second (in some cases, I reduced the 1000 ms window on the split tool to avoid noises and spikes).\nEach 1-second audio sample should be pre-processed and converted to an image (for example, \\(13\\times 49\\times 1\\)). We will use MFCC, which extracts features from audio signals using Mel Frequency Cepstral Coefficients, which are great for the human voice.\n \nNext, we select KERAS for classification and build our model from scratch by doing Image Classification using Convolution Neural Network).\n\n\n\nThe next step is to create the images to be trained in the next phase:\nWe can keep the default parameter values or take advantage of the DSP Autotuneparameters option, which we will do.\n \nThe result will not spend much memory to pre-process data (only 16KB). Still, the estimated processing time is high, 675 ms for an Espressif ESP-EYE (the closest reference available), with a 240 kHz clock (same as our device), but with a smaller CPU (XTensa LX6, versus the LX7 on the ESP32S). The real inference time should be smaller.\nSuppose we need to reduce the inference time later. In that case, we should return to the pre-processing stage and, for example, reduce the FFT length to 256, change the Number of coefficients, or another parameter.\nFor now, let‚Äôs keep the parameters defined by the Autotuning tool. Save parameters and generate the features.\n \n\nIf you want to go further with converting temporal serial data into images using FFT, Spectrogram, etc., you can play with this CoLab: Audio Raw Data Analysis.\n\n\n\n\nWe will use a Convolution Neural Network (CNN) model. The basic architecture is defined with two blocks of Conv1D + MaxPooling (with 8 and 16 neurons, respectively) and a 0.25 Dropout. And on the last layer, after Flattening four neurons, one for each class:\n \nAs hyper-parameters, we will have a Learning Rate of 0.005 and a model that will be trained by 100 epochs. We will also include data augmentation, as some noise. The result seems OK:\n \nIf you want to understand what is happening ‚Äúunder the hood,‚Äù you can download the dataset and run a Jupyter Notebook playing with the code. For example, you can analyze the accuracy by each epoch:\n \nThis CoLab Notebook can explain how you can go further: KWS Classifier Project - Looking ‚ÄúUnder the hood Training/xiao_esp32s3_keyword_spotting_project_nn_classifier.ipynb).‚Äù",
    "crumbs": [
      "Seeed XIAO ESP32S3",
      "Keyword Spotting (KWS)"
    ]
  },
  {
    "objectID": "contents/seeed/xiao_esp32s3/kws/kws.html#sec-keyword-spotting-kws-testing-8a1c",
    "href": "contents/seeed/xiao_esp32s3/kws/kws.html#sec-keyword-spotting-kws-testing-8a1c",
    "title": "Keyword Spotting (KWS)",
    "section": "",
    "text": "Testing the model with the data put apart before training (Test Data), we got an accuracy of approximately 87%.\n \nInspecting the F1 score, we can see that for YES, we got 0.95, an excellent result once we used this keyword to ‚Äútrigger‚Äù our postprocessing stage (turn on the built-in LED). Even for NO, we got 0.90. The worst result is for unknown, what is OK.\nWe can proceed with the project, but it is possible to perform Live Classification using a smartphone before deployment on our device. Go to the Live Classification section and click on Connect a Development board:\n \nPoint your phone to the barcode and select the link.\n \nYour phone will be connected to the Studio. Select the option Classification on the app, and when it is running, start testing your keywords, confirming that the model is working with live and real data:",
    "crumbs": [
      "Seeed XIAO ESP32S3",
      "Keyword Spotting (KWS)"
    ]
  },
  {
    "objectID": "contents/seeed/xiao_esp32s3/kws/kws.html#sec-keyword-spotting-kws-deploy-inference-6b44",
    "href": "contents/seeed/xiao_esp32s3/kws/kws.html#sec-keyword-spotting-kws-deploy-inference-6b44",
    "title": "Keyword Spotting (KWS)",
    "section": "",
    "text": "The Studio will package all the needed libraries, preprocessing functions, and trained models, downloading them to your computer. Select the Arduino Library option, then choose Quantized (Int8) from the bottom menu and press Build.\n \nNow it is time for a real test. We will make inferences wholly disconnected from the Studio. Let‚Äôs change one of the ESP32 code examples created when you deploy the Arduino Library.\nIn your Arduino IDE, go to the File/Examples tab look for your project, and select esp32/esp32_microphone:\n \nThis code was created for the ESP-EYE built-in microphone, which should be adapted for our device.\nStart changing the libraries to handle the I2S bus:\n \nBy:\n#include &lt;I2S.h&gt;\n#define SAMPLE_RATE 16000U\n#define SAMPLE_BITS 16\nInitialize the IS2 microphone at setup(), including the lines:\nvoid setup()\n{\n...\n    I2S.setAllPins(-1, 42, 41, -1, -1);\n    if (!I2S.begin(PDM_MONO_MODE, SAMPLE_RATE, SAMPLE_BITS)) {\n      Serial.println(\"Failed to initialize I2S!\");\n    while (1) ;\n...\n}\nOn the static void capture_samples(void* arg) function, replace the line 153 that reads data from I2S mic:\n \nBy:\n/* read data at once from i2s */\nesp_i2s::i2s_read(esp_i2s::I2S_NUM_0,\n                 (void*)sampleBuffer,\n                 i2s_bytes_to_read,\n                 &bytes_read, 100);\nOn function static bool microphone_inference_start(uint32_t n_samples), we should comment or delete lines 198 to 200, where the microphone initialization function is called. This is unnecessary because the I2S microphone was already initialized during the setup().\n \nFinally, on static void microphone_inference_end(void) function, replace line 243:\n \nBy:\nstatic void microphone_inference_end(void)\n{\n    free(sampleBuffer);\n    ei_free(inference.buffer);\n}\nYou can find the complete code on the project‚Äôs GitHub. Upload the sketch to your board and test some real inferences:\n\n‚ö†Ô∏è Attention\n\nThe Xiao ESP32S3 MUST have the PSRAM enabled. You can check it on the Arduino IDE upper menu: Tools‚Äì&gt; PSRAM:OPI PSRAM\nThe Arduino Library (esp32 by Espressif Systems should be version 2.017. Please do not update it.",
    "crumbs": [
      "Seeed XIAO ESP32S3",
      "Keyword Spotting (KWS)"
    ]
  },
  {
    "objectID": "contents/seeed/xiao_esp32s3/kws/kws.html#sec-keyword-spotting-kws-postprocessing-d5e4",
    "href": "contents/seeed/xiao_esp32s3/kws/kws.html#sec-keyword-spotting-kws-postprocessing-d5e4",
    "title": "Keyword Spotting (KWS)",
    "section": "",
    "text": "In edge AI applications, the inference result is only as valuable as our ability to act upon it. While serial output provides detailed information for debugging and development, real-world deployments require immediate, human-readable feedback that doesn‚Äôt depend on external monitors or connections.\nLet‚Äôs explore two post-processing approaches. Using the internal XIAO‚Äôs LED and the OLED on the XIAOML Kit.\n\n\nNow that we know the model is working by detecting our keywords, let‚Äôs modify the code to see the internal LED go on every time a YES is detected.\nYou should initialize the LED:\n#define LED_BUILT_IN 21\n...\nvoid setup()\n{\n...\n  pinMode(LED_BUILT_IN, OUTPUT); // Set the pin as output\n  digitalWrite(LED_BUILT_IN, HIGH); //Turn off\n...\n}\nAnd change the // print the predictions portion of the previous code (on loop():\nint pred_index = 0;     // Initialize pred_index\nfloat pred_value = 0;   // Initialize pred_value\n\n// print the predictions\nei_printf(\"Predictions \");\nei_printf(\"(DSP: %d ms., Classification: %d ms., Anomaly: %d ms.)\",\n     result.timing.dsp, result.timing.classification,\n     result.timing.anomaly);\nei_printf(\": \\n\");\nfor (size_t ix = 0; ix &lt; EI_CLASSIFIER_LABEL_COUNT; ix++) {\n      ei_printf(\"    %s: \", result.classification[ix].label);\n      ei_printf_float(result.classification[ix].value);\n      ei_printf(\"\\n\");\n\n      if (result.classification[ix].value &gt; pred_value){\n         pred_index = ix;\n         pred_value = result.classification[ix].value;\n      }\n}\n\n// show the inference result on LED\nif (pred_index == 3){\n    digitalWrite(LED_BUILT_IN, LOW); //Turn on\n}\nelse{\n   digitalWrite(LED_BUILT_IN, HIGH); //Turn off\n}\nYou can find the complete code on the project‚Äôs GitHub. Upload the sketch to your board and test some real inferences:\n \nThe idea is that the LED will be ON whenever the keyword YES is detected. In the same way, instead of turning on an LED, this could be a ‚Äútrigger‚Äù for an external device, as we saw in the introduction.\n\n\n\n\n\nThe XIAOML Kit tiny 0.42‚Äù OLED display (72√ó40 pixels) serves as a crucial post-processing component that transforms raw ML inference results into immediate, human-readable feedback‚Äîdisplaying detected class names and confidence levels directly on the device, eliminating the need for external monitors and enabling truly standalone edge AI deployment in industrial, agricultural, or retail environments where instant visual confirmation of AI predictions is essential.\nSo, let‚Äôs modify the sketch to automatically adapt to the model trained on Edge Impulse by reading the class names and count directly from the model. Download the code from GitHub: xiaoml-kit_kws_oled.\nRunning the code, we can see the result:",
    "crumbs": [
      "Seeed XIAO ESP32S3",
      "Keyword Spotting (KWS)"
    ]
  },
  {
    "objectID": "contents/seeed/xiao_esp32s3/kws/kws.html#sec-keyword-spotting-kws-summary-b181",
    "href": "contents/seeed/xiao_esp32s3/kws/kws.html#sec-keyword-spotting-kws-summary-b181",
    "title": "Keyword Spotting (KWS)",
    "section": "",
    "text": "This lab demonstrated the complete development cycle of a keyword spotting system using the XIAOML Kit, showcasing how modern TinyML platforms make sophisticated audio AI accessible on resource-constrained devices. Through hands-on implementation, we‚Äôve bridged the gap between theoretical machine learning concepts and practical embedded AI deployment.\nTechnical Achievements:\nThe project successfully implemented a complete audio processing pipeline from raw sound capture through real-time inference. Using the XIAO ESP32S3‚Äôs integrated digital microphone, we captured audio data at professional quality (16kHz/16-bit) and processed it using Mel Frequency Cepstral Coefficients (MFCC) for feature extraction. The deployed CNN model achieved excellent accuracy in distinguishing between our target keywords (‚ÄúYES‚Äù, ‚ÄúNO‚Äù) and background conditions (‚ÄúNOISE‚Äù, ‚ÄúUNKNOWN‚Äù), with inference times suitable for real-time applications.\nPlatform Integration:\nEdge Impulse Studio proved invaluable as a comprehensive MLOps platform for embedded systems, handling everything from data collection and labeling through model training, optimization, and deployment. The seamless integration between cloud-based training and edge deployment exemplifies modern TinyML workflows, while the Arduino IDE provided the flexibility needed for custom post-processing implementations.\nReal-World Applications:\nThe techniques learned extend far beyond simple keyword detection. Voice-activated control systems, industrial safety monitoring through sound classification, medical applications for respiratory analysis, and environmental monitoring for wildlife or equipment sounds all leverage similar audio processing approaches. The cascaded detection architecture demonstrated here‚Äîusing edge-based KWS to trigger more complex cloud processing‚Äîis fundamental to modern voice assistant systems.\nEmbedded AI Principles:\nThis project highlighted crucial TinyML considerations, including power management, memory optimization through PSRAM utilization, and the trade-offs between model complexity and inference speed. The successful deployment of a neural network performing real-time audio analysis on a microcontroller demonstrates how AI capabilities, once requiring powerful desktop computers, can now operate on battery-powered devices.\nDevelopment Methodology:\nWe explored multiple development pathways, from data collection strategies (offline SD card storage versus online streaming) to deployment options (Edge Impulse‚Äôs automated library generation versus custom Arduino implementation). This flexibility is crucial for adapting to various project requirements and constraints.\nFuture Directions:\nThe foundation established here enables the exploration of more advanced audio AI applications. Multi-keyword recognition, speaker identification, emotion detection from voice, and environmental sound classification all build upon the same core techniques. The integration capabilities demonstrated with OLED displays and GPIO control illustrate how KWS can serve as the intelligent interface for broader IoT systems.\nConsider that Sound Classification encompasses much more than just voice recognition. This project‚Äôs techniques apply across numerous domains:\n\nSecurity Applications: Broken glass detection, intrusion monitoring, gunshot detection\nIndustrial IoT: Machinery health monitoring, anomaly detection in manufacturing equipment\nHealthcare: Sleep disorder monitoring, respiratory condition assessment, elderly care systems\nEnvironmental Monitoring: Wildlife tracking, urban noise analysis, smart building acoustic management\nSmart Home Integration: Multi-room voice control, appliance status monitoring through sound signatures\n\nKey Takeaways:\nThe XIAOML Kit proves that professional-grade AI development is achievable with accessible tools and modest budgets. The combination of capable hardware (ESP32S3 with PSRAM and integrated sensors), mature development platforms (Edge Impulse Studio), and comprehensive software libraries creates an environment where complex AI concepts become tangible, working systems.\nThis lab demonstrates that the future of AI isn‚Äôt just in massive data centers, but in intelligent edge devices that can process, understand, and respond to their environment in real-time‚Äîopening possibilities for ubiquitous, privacy-preserving, and responsive artificial intelligence systems.",
    "crumbs": [
      "Seeed XIAO ESP32S3",
      "Keyword Spotting (KWS)"
    ]
  },
  {
    "objectID": "contents/seeed/xiao_esp32s3/kws/kws.html#sec-keyword-spotting-kws-resources-c867",
    "href": "contents/seeed/xiao_esp32s3/kws/kws.html#sec-keyword-spotting-kws-resources-c867",
    "title": "Keyword Spotting (KWS)",
    "section": "",
    "text": "XIAO ESP32S3 Codes\nXIAOML Kit Code\nSubset of Google Speech Commands Dataset\nKWS MFCC Analysis Colab Notebook\nKWS CNN training Colab Notebook\nXIAO ESP32S3 Post-processing Code\nEdge Impulse Project",
    "crumbs": [
      "Seeed XIAO ESP32S3",
      "Keyword Spotting (KWS)"
    ]
  },
  {
    "objectID": "contents/seeed/xiao_esp32s3/image_classification/image_classification.html",
    "href": "contents/seeed/xiao_esp32s3/image_classification/image_classification.html",
    "title": "Image Classification",
    "section": "",
    "text": "DALL¬∑E prompt - 1950s style cartoon illustration based on a real image by Marcelo Rovai\n\n\n\n\nWe are increasingly facing an artificial intelligence (AI) revolution, where, as Gartner states, Edge AI and Computer Vision have a very high impact potential, and it is for now!\nWhen we look into Machine Learning (ML) applied to vision, the first concept that greets us is Image Classification, a kind of ML‚Äôs Hello World that is both simple and profound!\nThe Seeed Studio XIAOML Kit provides a comprehensive hardware solution centered around theXIAO ESP32-S3 Sense, featuring an integrated OV3660 camera and SD card support. Those features make the XIAO ESP32S3 Sense an excellent starting point for exploring TinyML vision AI.\nIn this Lab, we will explore Image Classification using the non-code tool SenseCraft AI and explore a more detailed development with Edge Impulse Studio and Arduino IDE.\n\n\n\n\n\n\nTipLearning Objectives\n\n\n\n\nDeploy Pre-trained Models using SenseCraft AI Studio for immediate computer vision applications\nCollect and Manage Image Datasets for custom classification tasks with proper data organization\nTrain Custom Image Classification Models using transfer learning with MobileNet V2 architecture\nOptimize Models for Edge Deployment through quantization and memory-efficient preprocessing\nImplement Post-processing Pipelines, including GPIO control and real-time inference integration\nCompare Development Approaches between no-code and advanced ML platforms for embedded applications\n\n\n\n\n\n\nImage classification is a fundamental task in computer vision that involves categorizing entire images into one of several predefined classes. This process entails analyzing the visual content of an image and assigning it a label from a fixed set of categories based on the dominant object or scene it depicts.\nImage classification is crucial in various applications, ranging from organizing and searching through large databases of images in digital libraries and social media platforms to enabling autonomous systems to comprehend their surroundings. Common architectures that have significantly advanced the field of image classification include Convolutional Neural Networks (CNNs), such as AlexNet, VGGNet, and ResNet. These models have demonstrated remarkable accuracy on challenging datasets, such as ImageNet, by learning hierarchical representations of visual data.\nAs the cornerstone of many computer vision systems, image classification drives innovation, laying the groundwork for more complex tasks like object detection and image segmentation, and facilitating a deeper understanding of visual data across various industries. So, let‚Äôs start exploring the Person Classification model (‚ÄúPerson - No Person‚Äù), a ready-to-use computer vision application on the SenseCraft AI.\n \n\n\nStart by connecting the XIAOML Kit (or just the XIAO ESP32S3 Sense, disconnected from the Expansion Board) to the computer via USB-C, and then open the SenseCraft AI Workspace to connect it.\n \nOnce connected, select the option [Select Model...] and enter in the search window: ‚ÄúPerson Classification‚Äù. From the options available, select the one trained over the MobileNet V2 (passing the mouse over the models will open a pop-up window with its main characteristics).\n \nClick on the chosen model and confirm the deployment. A new firmware for the model should start uploading to our device.\n\nNote that the percentage of models downloaded and firmware uploaded will be displayed. If not, try disconnecting the device, then reconnect it and press the boot button.\n\nAfter the model is uploaded successfully, we can view the live feed from the XIAO camera and the classification result (Person or Not a Person) in the Preview area, along with the inference details displayed in the Device Logger.\n\nNote that we can also select our Inference Frame Interval, from ‚ÄúReal-Time‚Äù (Default) to 10 seconds, and the Mode (UART, I2C, etc) as the data is shared by the device (the default is UART via USB).\n\n \nAt the Device Logger, we can see that the latency of the model is from 52 to 78 ms for pre-processing and around 532ms for inference, which will give us a total time of a little less than 600ms, or about 1.7 Frames per second (FPS).\n\nTo run the Mobilenet V2 0.35, the XIAO had a peak current of 160mA at 5.23V, resulting in a power consumption of 830mW.\n\n\n\n\nAn essential step in an Image Classification project pipeline is to define what we want to do with the inference result. So, imagine that we will use the XIAO to automatically turn on the room lights if a person is detected.\n \nWith the SebseCraft AI, we can do it on the Output -&gt; GPIO section. Click on the Add icon to trigger the action when event conditions are met. A pop-up window will open, where you can define the action to be taken. For example, if a person is detected with a confidence of more than 60% the internal LED should be ON. In a real scenario, a GPIO, for example, D0, D1, D2, D11, or D12, would be used to trigger a relay to turn on a light.\n \nOnce confirmed, the created Trigger Action will be shown. Press Send to upload the command to the XIAO.\n \nNow, pointing the XIAO at a person will make the internal LED go ON.\n \n\nWe will explore more trigger actions and post-processing techniques further in this lab.\n\n\n\n\n\nLet‚Äôs create a simple Image Classification project using SenseCraft AI Studio. Below, we can see a typical machine learning pipeline that will be used in our project.\n \nOn SenseCraft AI Studio: Let‚Äôs open the tab Training:\n \nThe default is to train a Classification model with a WebCam if it is available. Let‚Äôs select the XIAOESP32S3 Sense instead. Pressing the green button [Connect] will cause a Pop-Up window to appear. Select the corresponding Port and press the blue button [Connect].\n \nThe image streamed from the Grove Vision AI V2 will be displayed.\n\n\nThe first step, as we can see in the ML pipeline, is to define a goal. Let‚Äôs imagine that we have an industrial installation that should automatically sort wheels and boxes.\n \nSo, let‚Äôs simulate it, classifying, for example, a toy box and a toy wheel. We should also include a 3rd class of images, background, where there are no objects in the scene.\n \n\n\n\nLet‚Äôs create the classes, following, for example, an alphabetical order:\n\nClass1: background\nClass 2: box\nClass 3: wheel\n\n \nSelect one of the classes and keep pressing the green button (Hold to Record) under the preview area. The collected images (and their counting) will appear on the Image Samples Screen. Carefully and slowly, move the camera to capture different angles of the object. To modify the position or interfere with the image, release the green button, rearrange the object, and then hold it again to resume the capture.\n \nAfter collecting the images, review them and delete any incorrect ones.\n \nCollect around 50 images from each class and go to Training Step.\n\nNote that it is possible to download the collected images to be used in another application, for example, with the Edge Impulse Studio.\n\n\n\n\nConfirm if the correct device is selected (XIAO ESP32S3 Sense) and press [Start Training]\n \n\n\n\nAfter training, the inference result can be previewed.\n\nNote that the model is not running on the device. We are, in fact, only capturing the images with the device and performing a live preview using the training model, which is running in the Studio.\n\n \nNow is the time to really deploy the model in the device.\n\n\n\nSelect the trained model and XIAO ESP32S3 Sense at the Supported Devices window. And press [Deploy to device].\n \nThe SeneCrafit AI will redirect us to the Vision Workplace tab. Confirm the deployment, select the Port, and Connect it.\n \nThe model will be flashed into the device. After an automatic reset, the model will start running on the device. On the Device Logger, we can see that the inference has a latency of approximately 426 ms, plus a pre-processing of around 110ms, corresponding to a frame rate of 1.8 frames per second (FPS).\nAlso, note that in Settings, it is possible to adjust the model‚Äôs confidence.\n \n\nTo run the Image Classification Model, the XIAO ESP32S3 had a peak current of 14mA at 5.23V, resulting in a power consumption of 730mW.\n\nAs before, in the Output ‚Äì&gt; GPIO, we can turn the GPIOs or the Internal LED ON based on the detected class. For example, the LED will be turned ON when the wheel is detected.\n \n\n\n\nIt is possible to save the model in the SenseCraft AI Studio. The Studio will retain all our models for later deployment. For that, return to the Training tab and select the button [Save to SenseCraft]:\n \nFollow the instructions to enter the model‚Äôs name, description, image, and other details.\n \nNote that the trained model (an Int8 MobileNet V2 with a size of 320KB) can be downloaded for further use or even analysis, for example, using Netron. Note that the model uses images of size 224x224x3 as its Input Tensor. In the next step, we will use different hyperparameters on the Edge Impulse Studio.\n \nAlso, the model can be deployed again to the device at any time. Automatically, the Workspace will be open on the SenseCraft AI.\n\n\n\n\nThe primary objective of our project is to train a model and perform inference on the XIAO ESP32S3 Sense. For training, we should find some data (in fact, tons of data!).\nBut as we already know, first of all, we need a goal! What do we want to classify?\nWith TinyML, a set of techniques associated with machine learning inference on embedded devices, we should limit the classification to three or four categories due to limitations (mainly memory). We can, for example, train the images captured for the Box versus Wheel, which can be downloaded from the SenseCraft AI Studio.\n\nAlternatively, we can use a completely new dataset, such as one that differentiates apples from bananas and potatoes, or other categories. If possible, try finding a specific dataset that includes images from those categories. Kaggle fruit-and-vegetable-image-recognition is a good start.\n\nLet‚Äôs download the dataset captured in the previous section. Open the menu (3 dots) on each of the captured classes and select Export Data.\n \nThe dataset will be downloaded to the computer as a .ZIP file, with one file for each class. Save them in your working folder and unzip them. You should have three folders, one for each class.\n \n\nOptionally, you can add some fresh images, using, for example, the code discussed in the setup lab.\n\n\n\n\nWe will use the Edge Impulse Studio to train our model. Edge Impulse is a leading development platform for machine learning on edge devices.\nEnter your account credentials (or create a free account) at Edge Impulse. Next, create a new project:\n\n\nNext, go to the Data acquisition section and there, select + Add data. A pop-up window will appear. Select UPLOAD DATA.\n \nAfter selection, a new Pop-Up window will appear, asking to update the data.\n\nIn Upload mode: select a folder and press [Choose Files].\nGo to the folder that contains one of the classes and press [Upload]\n\n \n\nYou will return automatically to the Upload data window.\nSelect Automatically split between training and testing\nAnd enter the label of the images that are in the folder.\nSelect [Upload data]\nAt this point, the files will start to be uploaded, and after that, another Pop-Up window will appear asking if you are building an object detection project. Select [no]\n\n \nRepeat the procedure for all classes. Do not forget to change the label‚Äôs name. If you forget and the images are uploaded, please note that they will be mixed in the Studio. Do not worry, you can manually move the data between classes further.\nClose the Upload Data window and return to the Data acquisition page. We can see that all dataset was uploaded. Note that on the upper panel, we can see that we have 158 items, all of which are balanced. Also, 19% of the images were left for testing.\n \n\n\n\n\nAn impulse takes raw data (in this case, images), extracts features (resizes pictures), and then uses a learning block to classify new data.\n\nClassifying images is the most common application of deep learning, but a substantial amount of data is required to accomplish this task. We have around 50 images for each category. Is this number enough? Not at all! We will need thousands of images to ‚Äúteach‚Äù or ‚Äúmodel‚Äù each class, allowing us to differentiate them. However, we can resolve this issue by retraining a previously trained model using thousands of images. We refer to this technique as ‚ÄúTransfer Learning‚Äù (TL). With TL, we can fine-tune a pre-trained image classification model on our data, achieving good performance even with relatively small image datasets, as in our case.\n \nWith TL, we can fine-tune a pre-trained image classification model on our data, performing well even with relatively small image datasets (our case).\nSo, starting from the raw images, we will resize them \\((96\\times 96)\\) Pixels are fed to our Transfer Learning block. Let‚Äôs create an Impulse.\n\nAt this point, we can also define our target device to monitor our ‚Äúbudget‚Äù (memory and latency). The XIAO ESP32S3 is not officially supported by Edge Impulse, so let‚Äôs consider the Espressif ESP-EYE, which is similar but slower.\n\n \nSave the Impulse, as shown above, and go to the Image section.\n\n\n\nBesides resizing the images, we can convert them to grayscale or retain their original RGB color depth. Let‚Äôs select [RGB] in the Image section. Doing that, each data sample will have a dimension of 27,648 features (96x96x3). Pressing [Save Parameters] will open a new tab, Generate Features. Press the button [Generate Features]to generate the features.\n\n\n\nIn 2007, Google introduced MobileNetV1. In 2018, MobileNetV2: Inverted Residuals and Linear Bottlenecks, was launched, and, in 2019, the V3. The Mobilinet is a family of general-purpose computer vision neural networks explicitly designed for mobile devices to support classification, detection, and other applications. MobileNets are small, low-latency, low-power models parameterized to meet the resource constraints of various use cases.\nAlthough the base MobileNet architecture is already compact and has low latency, a specific use case or application may often require the model to be even smaller and faster. MobileNets introduce a straightforward parameter, Œ± (alpha), called the width multiplier to construct these smaller, less computationally expensive models. The role of the width multiplier Œ± is to thin a network uniformly at each layer.\nEdge Impulse Studio has available MobileNet V1 (96x96 images) and V2 (96x96 and 160x160 images), with several different Œ± values (from 0.05 to 1.0). For example, you will get the highest accuracy with V2, 160x160 images, and Œ±=1.0. Of course, there is a trade-off. The higher the accuracy, the more memory (around 1.3M RAM and 2.6M ROM) will be needed to run the model, implying more latency. The smaller footprint will be obtained at another extreme with MobileNet V1 and Œ±=0.10 (around 53.2K RAM and 101K ROM).\n\nWe will use the MobileNet V2 0.35 as our base model (but a model with a greater alpha can be used here). The final layer of our model, preceding the output layer, will have 16 neurons with a 10% dropout rate for preventing overfitting.\n\nAnother necessary technique to use with deep learning is data augmentation. Data augmentation is a method that can help improve the accuracy of machine learning models by creating additional artificial data. A data augmentation system makes small, random changes to your training data during the training process (such as flipping, cropping, or rotating the images).\nUnder the hood, here you can see how Edge Impulse implements a data Augmentation policy on your data:\n# Implements the data augmentation policy\ndef augment_image(image, label):\n    # Flips the image randomly\n    image = tf.image.random_flip_left_right(image)\n\n    # Increase the image size, then randomly crop it down to\n    # the original dimensions\n    resize_factor = random.uniform(1, 1.2)\n    new_height = math.floor(resize_factor * INPUT_SHAPE[0])\n    new_width = math.floor(resize_factor * INPUT_SHAPE[1])\n    image = tf.image.resize_with_crop_or_pad(image, new_height,\n                                             new_width)\n    image = tf.image.random_crop(image, size=INPUT_SHAPE)\n\n    # Vary the brightness of the image\n    image = tf.image.random_brightness(image, max_delta=0.2)\n\n    return image, label\nNow, let‚Äôs us define the hyperparameters:\n\nEpochs: 20,\nBach Size: 32\nLearning Rate: 0.0005\nValidation size: 20%\n\nAnd, so, we have as a training result:\n \nThe model profile predicts 233 KB of RAM and 546 KB of Flash, indicating no problem with the Xiao ESP32S3, which has 8 MB of PSRAM. Additionally, the Studio indicates a latency of around 1160 ms, which is very high. However, this is to be expected, given that we are using the ESP-EYE, whose CPU is an Extensa LX6, and the ESP32S3 uses a newer and more powerful Xtensa LX7.\n\nWith the test data, we also achieved 100% accuracy, even with a quantized INT8 model. This result is not typical in real projects, but our project here is relatively simple, with two objects that are very distinctive from each other.\n\n\n\n\n\nWe can deploy the trained model:\n\nAs .TFLITE to be used on the SenseCraft AI \nAs an Arduino Library in the Edge Impulse Studio.\n\nLet‚Äôs start with the SenseCraft, which is more straightforward and more intuitive.\n\n\nOn the Dashboard, it is possible to download the trained model in several different formats. Let‚Äôs download TensorFlow Lite (int8 quantized), which has a size of 623KB.\n \nOn SenseCraft AI Studio, go to the Workspace tab, select XIAO ESP32S3, the corresponding Port, and connect the device.\nYou should see the last model that was uploaded to the device. Select the green button [Upload Model]. A pop-up window will prompt you to enter the model name, the model file, and the class names (objects). We should use labels in alphabetical order: 0: background, 1: box, and 2: wheel, and then press [Send].\n \nAfter a few seconds, the model will be uploaded (‚Äúflashed‚Äù) to our device, and the camera image will appear in real-time on the Preview Sector. The Classification result will be displayed under the image preview. It is also possible to select the Confidence Threshold of your inference using the cursor on Settings.\nOn the Device Logger, we can view the Serial Monitor, where we can observe the latency, which is approximately 81 ms for pre-processing and 205 ms for inference, corresponding to a frame rate of 3.4 frames per second (FPS), what is double of we got, training the model on SenseCraft, because we are working with smaller images (96x96 versus 224x224).\n\nThe total latency is around 4 times faster than the estimation made in Edge Impulse Studio on an Xtensa LX6 CPU; now we are performing the inference on an Xtensa LX7 CPU.\n\n \n\n\nIt is possible to obtain the output of a model inference, including Latency, Class ID, and Confidence, as shown on the Device Logger in SenseCraft AI. This allows us to utilize the XIAO ESP32S3 Sense as an AI sensor. In other words, we can retrieve the model data using different communication protocols such as MQTT, UART, I2C, or SPI, depending on our project requirements.\n\nThe idea is similar to what we have done on the Seeed Grove Vision AI V2 Image Classification Post-Processing Lab.\n\nBelow is an example of a connection using the I2C bus.\n\nPlease refer to the Seeed Studio Wiki for more information.\n\n\n\n\nOn the Deploy section at Edge Impulse Studio, Select Arduino library, TensorFlow Lite, Quantized(int8), and press [Build]. The trained model will be downloaded as a .zip Arduino library:\n \nOpen your Arduino IDE, and under Sketch, go to Include Library and add .ZIP Library. Next, select the file downloaded from Edge Impulse Studio and press [Open].\n \nGo to the Arduino IDE Examples and look for the project by its name (in this case: ‚ÄúBox_versus_Whell_‚Ä¶Interfering‚Äù. Open esp32 -&gt; esp32_camera. The sketch esp32_camera.ino will be downloaded to the IDE.\nThis sketch was developed for the standard ESP32 and will not work with the XIAO ESP32S3 Sense. It should be modified. Let‚Äôs download the modified one from the project GitHub: Image_class_XIAOML-Kit.ino.\n\n\nThe code captures images from the onboard camera, processes them, and classifies them (in this case, ‚ÄúBox‚Äù, ‚ÄúWheel‚Äù, or ‚ÄúBackground‚Äù) using the trained model on EI Studio. It runs continuously, performing real-time inference on the edge device.\nIn short,:\nCamera ‚Üí JPEG Image ‚Üí RGB888 Conversion ‚Üí Resize to 96x96 ‚Üí Neural Network ‚Üí Classification Results ‚Üí Serial Output\n\n\n\nLibrary Includes and Dependencies\n\n#include &lt;Box_versus_Wheel_-_XIAO_ESP32S3_inferencing.h&gt;\n#include \"edge-impulse-sdk/dsp/image/image.hpp\"\n#include \"esp_camera.h\"\n\nEdge Impulse Inference Library: Contains our trained model and inference engine\nImage Processing: Provides functions for image manipulation\nESP Camera: Hardware interface for the camera module\n\n\nCamera Pin Configurations\n\nThe XIAO ESP32S3 Sense can work with different camera sensors (OV2640 or OV3660), which may have different pin configurations. The code defines three possible configurations:\n// Configuration 1: Most common OV2640 configuration\n#define CONFIG_1_XCLK_GPIO_NUM    10\n#define CONFIG_1_SIOD_GPIO_NUM    40\n#define CONFIG_1_SIOC_GPIO_NUM    39\n// ... more pins\nThis flexibility allows the code to automatically try different pin mappings if the first one doesn‚Äôt work, making it more robust across different hardware revisions.\n\nMemory Management Settings\n\n#define EI_CAMERA_RAW_FRAME_BUFFER_COLS   320\n#define EI_CAMERA_RAW_FRAME_BUFFER_ROWS   240\n#define EI_CLASSIFIER_ALLOCATION_HEAP      1\n\nFrame Buffer Size: Defines the raw image size (320x240 pixels)\nHeap Allocation: Uses dynamic memory allocation for flexibility\nPSRAM Support: The ESP32S3 has 8MB of PSRAM for storing large data like images\n\n\n\n\nvoid setup() {\n    Serial.begin(115200);\n    while (!Serial);\n\n    if (ei_camera_init() == false) {\n        ei_printf(\"Failed to initialize Camera!\\r\\n\");\n    } else {\n        ei_printf(\"Camera initialized\\r\\n\");\n    }\n\n    ei_sleep(2000);  // Wait 2 seconds before starting\n}\nThis function:\n\nInitializes serial communication for debugging output\nInitializes the camera with automatic configuration detection\nWaits 2 seconds before starting continuous inference\n\n\n\n\nThe loop performs these steps continuously:\nStep 1: Memory Allocation\nsnapshot_buf = (uint8_t*)ps_malloc(EI_CAMERA_RAW_FRAME_BUFFER_COLS *\n                                   EI_CAMERA_RAW_FRAME_BUFFER_ROWS *\n                                   EI_CAMERA_FRAME_BYTE_SIZE);\nAllocates memory for the image buffer, preferring PSRAM (faster external RAM) but falling back to regular heap if needed.\nStep 2: Image Capture\nif (ei_camera_capture((size_t)EI_CLASSIFIER_INPUT_WIDTH,\n                     (size_t)EI_CLASSIFIER_INPUT_HEIGHT,\n                     snapshot_buf) == false) {\n    ei_printf(\"Failed to capture image\\r\\n\");\n    free(snapshot_buf);\n    return;\n}\nCaptures an image from the camera and stores it in the buffer.\nStep 3: Run Inference\nei_impulse_result_t result = { 0 };\nEI_IMPULSE_ERROR err = run_classifier(&signal, &result, false);\nRuns the machine learning model on the captured image.\nStep 4: Output Results\nfor (uint16_t i = 0; i &lt; EI_CLASSIFIER_LABEL_COUNT; i++) {\n    ei_printf(\"  %s: %.5f\\r\\n\",\n              ei_classifier_inferencing_categories[i],\n              result.classification[i].value);\n}\nPrints the classification results showing confidence scores for each category.\n\n\n\nThis function implements an intelligent initialization sequence:\nbool ei_camera_init(void) {\n    // Try Configuration 1 (OV2640 common)\n    update_camera_config(1);\n    esp_err_t err = esp_camera_init(&camera_config);\n    if (err == ESP_OK) goto camera_init_success;\n\n    // Try Configuration 2 (OV3660)\n    esp_camera_deinit();\n    update_camera_config(2);\n    err = esp_camera_init(&camera_config);\n    if (err == ESP_OK) goto camera_init_success;\n\n    // Continue trying other configurations...\n}\nThe function:\n\nTries multiple pin configurations\nTests different clock frequencies (10MHz or 16MHz)\nAttempts PSRAM first, then falls back to DRAM\nApplies sensor-specific settings based on detected hardware\n\n\n\n\nbool ei_camera_capture(uint32_t img_width, uint32_t img_height, uint8_t *out_buf) {\n    // 1. Get frame from camera\n    camera_fb_t *fb = esp_camera_fb_get();\n\n    // 2. Convert JPEG to RGB888 format\n    bool converted = fmt2rgb888(fb-&gt;buf, fb-&gt;len, PIXFORMAT_JPEG, snapshot_buf);\n\n    // 3. Return frame buffer to camera driver\n    esp_camera_fb_return(fb);\n\n    // 4. Resize if needed\n    if (do_resize) {\n        ei::image::processing::crop_and_interpolate_rgb888(...);\n    }\n}\nThis function:\n\nCaptures a JPEG image from the camera\nConverts it to RGB888 format (required by the ML model)\nResizes the image to match the model‚Äôs input size (96x96 pixels)\n\n\n\n\n\n\n\nUpload the code to the XIAO ESP32S3 Sense.\n\n\n‚ö†Ô∏è Attention\n\nThe Xiao ESP32S3 MUST have the PSRAM enabled. You can check it on the Arduino IDE upper menu: Tools‚Äì&gt; PSRAM:OPI PSRAM\nThe Arduino Boards package (esp32 by Espressif Systems) should be version 2.017. Do not update it\n\n\n \n\nOpen the Serial Monitor\nPoint the camera at the objects, and check the result on the Serial Monitor.\n\n \n\n\n\nIn edge AI applications, the inference result is only as valuable as our ability to act upon it. While serial output provides detailed information for debugging and development, real-world deployments require immediate, human-readable feedback that doesn‚Äôt depend on external monitors or connections.\nThe XIAOML Kit tiny 0.42‚Äù OLED display (72√ó40 pixels) serves as a crucial post-processing component that transforms raw ML inference results into immediate, human-readable feedback‚Äîdisplaying detected class names and confidence levels directly on the device, eliminating the need for external monitors and enabling truly standalone edge AI deployment in industrial, agricultural, or retail environments where instant visual confirmation of AI predictions is essential.\nSo, let‚Äôs modify the sketch to automatically adapt to the model trained on Edge Impulse by reading the class names and count directly from the model. The display will show abbreviated class names (3 letters) with larger fonts for better visibility on the tiny 72x40 pixel display. Download the code from the GitHub: XIAOML-Kit-Img_Class_OLED_Gen.\nRunning the code, we can see the result:\n \n\n\n\n\nThe XIAO ESP32S3 Sense is a remarkably capable and flexible platform for image classification applications. Through this lab, we‚Äôve explored two distinct development approaches that cater to different skill levels and project requirements.\n\nThe SenseCraft AI Studio provides an accessible entry point with its no-code interface, enabling rapid prototyping and deployment of pre-trained models like person detection. With real-time inference and integrated post-processing capabilities, it demonstrates how AI can be deployed without extensive programming or ML knowledge.\nFor more advanced applications, Edge Impulse Studio offers comprehensive machine learning pipeline tools, including custom dataset management, transfer learning with several pre-trained models, such as MobileNet, and model optimization.\n\nKey insights from this lab include the importance of image resolution trade-offs, the effectiveness of transfer learning for small datasets, and the practical considerations of edge AI deployment, such as power consumption and memory constraints.\nThe Lab demonstrates fundamental TinyML principles that extend beyond this specific hardware: resource-constrained inference, real-time processing requirements, and the complete pipeline from data collection through model deployment to practical applications. With built-in post-processing capabilities including GPIO control and communication protocols, the XIAO serves as more than just an inference engine‚Äîit becomes a complete AI sensor platform.\nThis foundation in image classification prepares you for more complex computer vision tasks while showcasing how modern edge AI makes sophisticated computer vision accessible, cost-effective, and deployable in real-world embedded applications ranging from industrial automation to smart home systems.\n\n\n\n\nGetting Started with the XIAO ESP32S3\nSenseCraft AI Studio Home\nSenseCraft Vision Workspace\nDataset example\nEdge Impulse Project\nXIAO as an AI Sensor\nSeeed Arduino SSCMA Library\nXIAOML Kit Code",
    "crumbs": [
      "Seeed XIAO ESP32S3",
      "Image Classification"
    ]
  },
  {
    "objectID": "contents/seeed/xiao_esp32s3/image_classification/image_classification.html#sec-image-classification-overview-9a37",
    "href": "contents/seeed/xiao_esp32s3/image_classification/image_classification.html#sec-image-classification-overview-9a37",
    "title": "Image Classification",
    "section": "",
    "text": "We are increasingly facing an artificial intelligence (AI) revolution, where, as Gartner states, Edge AI and Computer Vision have a very high impact potential, and it is for now!\nWhen we look into Machine Learning (ML) applied to vision, the first concept that greets us is Image Classification, a kind of ML‚Äôs Hello World that is both simple and profound!\nThe Seeed Studio XIAOML Kit provides a comprehensive hardware solution centered around theXIAO ESP32-S3 Sense, featuring an integrated OV3660 camera and SD card support. Those features make the XIAO ESP32S3 Sense an excellent starting point for exploring TinyML vision AI.\nIn this Lab, we will explore Image Classification using the non-code tool SenseCraft AI and explore a more detailed development with Edge Impulse Studio and Arduino IDE.\n\n\n\n\n\n\nTipLearning Objectives\n\n\n\n\nDeploy Pre-trained Models using SenseCraft AI Studio for immediate computer vision applications\nCollect and Manage Image Datasets for custom classification tasks with proper data organization\nTrain Custom Image Classification Models using transfer learning with MobileNet V2 architecture\nOptimize Models for Edge Deployment through quantization and memory-efficient preprocessing\nImplement Post-processing Pipelines, including GPIO control and real-time inference integration\nCompare Development Approaches between no-code and advanced ML platforms for embedded applications",
    "crumbs": [
      "Seeed XIAO ESP32S3",
      "Image Classification"
    ]
  },
  {
    "objectID": "contents/seeed/xiao_esp32s3/image_classification/image_classification.html#sec-image-classification-image-classification-d726",
    "href": "contents/seeed/xiao_esp32s3/image_classification/image_classification.html#sec-image-classification-image-classification-d726",
    "title": "Image Classification",
    "section": "",
    "text": "Image classification is a fundamental task in computer vision that involves categorizing entire images into one of several predefined classes. This process entails analyzing the visual content of an image and assigning it a label from a fixed set of categories based on the dominant object or scene it depicts.\nImage classification is crucial in various applications, ranging from organizing and searching through large databases of images in digital libraries and social media platforms to enabling autonomous systems to comprehend their surroundings. Common architectures that have significantly advanced the field of image classification include Convolutional Neural Networks (CNNs), such as AlexNet, VGGNet, and ResNet. These models have demonstrated remarkable accuracy on challenging datasets, such as ImageNet, by learning hierarchical representations of visual data.\nAs the cornerstone of many computer vision systems, image classification drives innovation, laying the groundwork for more complex tasks like object detection and image segmentation, and facilitating a deeper understanding of visual data across various industries. So, let‚Äôs start exploring the Person Classification model (‚ÄúPerson - No Person‚Äù), a ready-to-use computer vision application on the SenseCraft AI.\n \n\n\nStart by connecting the XIAOML Kit (or just the XIAO ESP32S3 Sense, disconnected from the Expansion Board) to the computer via USB-C, and then open the SenseCraft AI Workspace to connect it.\n \nOnce connected, select the option [Select Model...] and enter in the search window: ‚ÄúPerson Classification‚Äù. From the options available, select the one trained over the MobileNet V2 (passing the mouse over the models will open a pop-up window with its main characteristics).\n \nClick on the chosen model and confirm the deployment. A new firmware for the model should start uploading to our device.\n\nNote that the percentage of models downloaded and firmware uploaded will be displayed. If not, try disconnecting the device, then reconnect it and press the boot button.\n\nAfter the model is uploaded successfully, we can view the live feed from the XIAO camera and the classification result (Person or Not a Person) in the Preview area, along with the inference details displayed in the Device Logger.\n\nNote that we can also select our Inference Frame Interval, from ‚ÄúReal-Time‚Äù (Default) to 10 seconds, and the Mode (UART, I2C, etc) as the data is shared by the device (the default is UART via USB).\n\n \nAt the Device Logger, we can see that the latency of the model is from 52 to 78 ms for pre-processing and around 532ms for inference, which will give us a total time of a little less than 600ms, or about 1.7 Frames per second (FPS).\n\nTo run the Mobilenet V2 0.35, the XIAO had a peak current of 160mA at 5.23V, resulting in a power consumption of 830mW.\n\n\n\n\nAn essential step in an Image Classification project pipeline is to define what we want to do with the inference result. So, imagine that we will use the XIAO to automatically turn on the room lights if a person is detected.\n \nWith the SebseCraft AI, we can do it on the Output -&gt; GPIO section. Click on the Add icon to trigger the action when event conditions are met. A pop-up window will open, where you can define the action to be taken. For example, if a person is detected with a confidence of more than 60% the internal LED should be ON. In a real scenario, a GPIO, for example, D0, D1, D2, D11, or D12, would be used to trigger a relay to turn on a light.\n \nOnce confirmed, the created Trigger Action will be shown. Press Send to upload the command to the XIAO.\n \nNow, pointing the XIAO at a person will make the internal LED go ON.\n \n\nWe will explore more trigger actions and post-processing techniques further in this lab.",
    "crumbs": [
      "Seeed XIAO ESP32S3",
      "Image Classification"
    ]
  },
  {
    "objectID": "contents/seeed/xiao_esp32s3/image_classification/image_classification.html#sec-image-classification-image-classification-project-a2cf",
    "href": "contents/seeed/xiao_esp32s3/image_classification/image_classification.html#sec-image-classification-image-classification-project-a2cf",
    "title": "Image Classification",
    "section": "",
    "text": "Let‚Äôs create a simple Image Classification project using SenseCraft AI Studio. Below, we can see a typical machine learning pipeline that will be used in our project.\n \nOn SenseCraft AI Studio: Let‚Äôs open the tab Training:\n \nThe default is to train a Classification model with a WebCam if it is available. Let‚Äôs select the XIAOESP32S3 Sense instead. Pressing the green button [Connect] will cause a Pop-Up window to appear. Select the corresponding Port and press the blue button [Connect].\n \nThe image streamed from the Grove Vision AI V2 will be displayed.\n\n\nThe first step, as we can see in the ML pipeline, is to define a goal. Let‚Äôs imagine that we have an industrial installation that should automatically sort wheels and boxes.\n \nSo, let‚Äôs simulate it, classifying, for example, a toy box and a toy wheel. We should also include a 3rd class of images, background, where there are no objects in the scene.\n \n\n\n\nLet‚Äôs create the classes, following, for example, an alphabetical order:\n\nClass1: background\nClass 2: box\nClass 3: wheel\n\n \nSelect one of the classes and keep pressing the green button (Hold to Record) under the preview area. The collected images (and their counting) will appear on the Image Samples Screen. Carefully and slowly, move the camera to capture different angles of the object. To modify the position or interfere with the image, release the green button, rearrange the object, and then hold it again to resume the capture.\n \nAfter collecting the images, review them and delete any incorrect ones.\n \nCollect around 50 images from each class and go to Training Step.\n\nNote that it is possible to download the collected images to be used in another application, for example, with the Edge Impulse Studio.\n\n\n\n\nConfirm if the correct device is selected (XIAO ESP32S3 Sense) and press [Start Training]\n \n\n\n\nAfter training, the inference result can be previewed.\n\nNote that the model is not running on the device. We are, in fact, only capturing the images with the device and performing a live preview using the training model, which is running in the Studio.\n\n \nNow is the time to really deploy the model in the device.\n\n\n\nSelect the trained model and XIAO ESP32S3 Sense at the Supported Devices window. And press [Deploy to device].\n \nThe SeneCrafit AI will redirect us to the Vision Workplace tab. Confirm the deployment, select the Port, and Connect it.\n \nThe model will be flashed into the device. After an automatic reset, the model will start running on the device. On the Device Logger, we can see that the inference has a latency of approximately 426 ms, plus a pre-processing of around 110ms, corresponding to a frame rate of 1.8 frames per second (FPS).\nAlso, note that in Settings, it is possible to adjust the model‚Äôs confidence.\n \n\nTo run the Image Classification Model, the XIAO ESP32S3 had a peak current of 14mA at 5.23V, resulting in a power consumption of 730mW.\n\nAs before, in the Output ‚Äì&gt; GPIO, we can turn the GPIOs or the Internal LED ON based on the detected class. For example, the LED will be turned ON when the wheel is detected.\n \n\n\n\nIt is possible to save the model in the SenseCraft AI Studio. The Studio will retain all our models for later deployment. For that, return to the Training tab and select the button [Save to SenseCraft]:\n \nFollow the instructions to enter the model‚Äôs name, description, image, and other details.\n \nNote that the trained model (an Int8 MobileNet V2 with a size of 320KB) can be downloaded for further use or even analysis, for example, using Netron. Note that the model uses images of size 224x224x3 as its Input Tensor. In the next step, we will use different hyperparameters on the Edge Impulse Studio.\n \nAlso, the model can be deployed again to the device at any time. Automatically, the Workspace will be open on the SenseCraft AI.",
    "crumbs": [
      "Seeed XIAO ESP32S3",
      "Image Classification"
    ]
  },
  {
    "objectID": "contents/seeed/xiao_esp32s3/image_classification/image_classification.html#sec-image-classification-image-classification-project-dataset-8079",
    "href": "contents/seeed/xiao_esp32s3/image_classification/image_classification.html#sec-image-classification-image-classification-project-dataset-8079",
    "title": "Image Classification",
    "section": "",
    "text": "The primary objective of our project is to train a model and perform inference on the XIAO ESP32S3 Sense. For training, we should find some data (in fact, tons of data!).\nBut as we already know, first of all, we need a goal! What do we want to classify?\nWith TinyML, a set of techniques associated with machine learning inference on embedded devices, we should limit the classification to three or four categories due to limitations (mainly memory). We can, for example, train the images captured for the Box versus Wheel, which can be downloaded from the SenseCraft AI Studio.\n\nAlternatively, we can use a completely new dataset, such as one that differentiates apples from bananas and potatoes, or other categories. If possible, try finding a specific dataset that includes images from those categories. Kaggle fruit-and-vegetable-image-recognition is a good start.\n\nLet‚Äôs download the dataset captured in the previous section. Open the menu (3 dots) on each of the captured classes and select Export Data.\n \nThe dataset will be downloaded to the computer as a .ZIP file, with one file for each class. Save them in your working folder and unzip them. You should have three folders, one for each class.\n \n\nOptionally, you can add some fresh images, using, for example, the code discussed in the setup lab.",
    "crumbs": [
      "Seeed XIAO ESP32S3",
      "Image Classification"
    ]
  },
  {
    "objectID": "contents/seeed/xiao_esp32s3/image_classification/image_classification.html#sec-image-classification-training-model-edge-impulse-studio-f530",
    "href": "contents/seeed/xiao_esp32s3/image_classification/image_classification.html#sec-image-classification-training-model-edge-impulse-studio-f530",
    "title": "Image Classification",
    "section": "",
    "text": "We will use the Edge Impulse Studio to train our model. Edge Impulse is a leading development platform for machine learning on edge devices.\nEnter your account credentials (or create a free account) at Edge Impulse. Next, create a new project:\n\n\nNext, go to the Data acquisition section and there, select + Add data. A pop-up window will appear. Select UPLOAD DATA.\n \nAfter selection, a new Pop-Up window will appear, asking to update the data.\n\nIn Upload mode: select a folder and press [Choose Files].\nGo to the folder that contains one of the classes and press [Upload]\n\n \n\nYou will return automatically to the Upload data window.\nSelect Automatically split between training and testing\nAnd enter the label of the images that are in the folder.\nSelect [Upload data]\nAt this point, the files will start to be uploaded, and after that, another Pop-Up window will appear asking if you are building an object detection project. Select [no]\n\n \nRepeat the procedure for all classes. Do not forget to change the label‚Äôs name. If you forget and the images are uploaded, please note that they will be mixed in the Studio. Do not worry, you can manually move the data between classes further.\nClose the Upload Data window and return to the Data acquisition page. We can see that all dataset was uploaded. Note that on the upper panel, we can see that we have 158 items, all of which are balanced. Also, 19% of the images were left for testing.\n \n\n\n\n\nAn impulse takes raw data (in this case, images), extracts features (resizes pictures), and then uses a learning block to classify new data.\n\nClassifying images is the most common application of deep learning, but a substantial amount of data is required to accomplish this task. We have around 50 images for each category. Is this number enough? Not at all! We will need thousands of images to ‚Äúteach‚Äù or ‚Äúmodel‚Äù each class, allowing us to differentiate them. However, we can resolve this issue by retraining a previously trained model using thousands of images. We refer to this technique as ‚ÄúTransfer Learning‚Äù (TL). With TL, we can fine-tune a pre-trained image classification model on our data, achieving good performance even with relatively small image datasets, as in our case.\n \nWith TL, we can fine-tune a pre-trained image classification model on our data, performing well even with relatively small image datasets (our case).\nSo, starting from the raw images, we will resize them \\((96\\times 96)\\) Pixels are fed to our Transfer Learning block. Let‚Äôs create an Impulse.\n\nAt this point, we can also define our target device to monitor our ‚Äúbudget‚Äù (memory and latency). The XIAO ESP32S3 is not officially supported by Edge Impulse, so let‚Äôs consider the Espressif ESP-EYE, which is similar but slower.\n\n \nSave the Impulse, as shown above, and go to the Image section.\n\n\n\nBesides resizing the images, we can convert them to grayscale or retain their original RGB color depth. Let‚Äôs select [RGB] in the Image section. Doing that, each data sample will have a dimension of 27,648 features (96x96x3). Pressing [Save Parameters] will open a new tab, Generate Features. Press the button [Generate Features]to generate the features.\n\n\n\nIn 2007, Google introduced MobileNetV1. In 2018, MobileNetV2: Inverted Residuals and Linear Bottlenecks, was launched, and, in 2019, the V3. The Mobilinet is a family of general-purpose computer vision neural networks explicitly designed for mobile devices to support classification, detection, and other applications. MobileNets are small, low-latency, low-power models parameterized to meet the resource constraints of various use cases.\nAlthough the base MobileNet architecture is already compact and has low latency, a specific use case or application may often require the model to be even smaller and faster. MobileNets introduce a straightforward parameter, Œ± (alpha), called the width multiplier to construct these smaller, less computationally expensive models. The role of the width multiplier Œ± is to thin a network uniformly at each layer.\nEdge Impulse Studio has available MobileNet V1 (96x96 images) and V2 (96x96 and 160x160 images), with several different Œ± values (from 0.05 to 1.0). For example, you will get the highest accuracy with V2, 160x160 images, and Œ±=1.0. Of course, there is a trade-off. The higher the accuracy, the more memory (around 1.3M RAM and 2.6M ROM) will be needed to run the model, implying more latency. The smaller footprint will be obtained at another extreme with MobileNet V1 and Œ±=0.10 (around 53.2K RAM and 101K ROM).\n\nWe will use the MobileNet V2 0.35 as our base model (but a model with a greater alpha can be used here). The final layer of our model, preceding the output layer, will have 16 neurons with a 10% dropout rate for preventing overfitting.\n\nAnother necessary technique to use with deep learning is data augmentation. Data augmentation is a method that can help improve the accuracy of machine learning models by creating additional artificial data. A data augmentation system makes small, random changes to your training data during the training process (such as flipping, cropping, or rotating the images).\nUnder the hood, here you can see how Edge Impulse implements a data Augmentation policy on your data:\n# Implements the data augmentation policy\ndef augment_image(image, label):\n    # Flips the image randomly\n    image = tf.image.random_flip_left_right(image)\n\n    # Increase the image size, then randomly crop it down to\n    # the original dimensions\n    resize_factor = random.uniform(1, 1.2)\n    new_height = math.floor(resize_factor * INPUT_SHAPE[0])\n    new_width = math.floor(resize_factor * INPUT_SHAPE[1])\n    image = tf.image.resize_with_crop_or_pad(image, new_height,\n                                             new_width)\n    image = tf.image.random_crop(image, size=INPUT_SHAPE)\n\n    # Vary the brightness of the image\n    image = tf.image.random_brightness(image, max_delta=0.2)\n\n    return image, label\nNow, let‚Äôs us define the hyperparameters:\n\nEpochs: 20,\nBach Size: 32\nLearning Rate: 0.0005\nValidation size: 20%\n\nAnd, so, we have as a training result:\n \nThe model profile predicts 233 KB of RAM and 546 KB of Flash, indicating no problem with the Xiao ESP32S3, which has 8 MB of PSRAM. Additionally, the Studio indicates a latency of around 1160 ms, which is very high. However, this is to be expected, given that we are using the ESP-EYE, whose CPU is an Extensa LX6, and the ESP32S3 uses a newer and more powerful Xtensa LX7.\n\nWith the test data, we also achieved 100% accuracy, even with a quantized INT8 model. This result is not typical in real projects, but our project here is relatively simple, with two objects that are very distinctive from each other.",
    "crumbs": [
      "Seeed XIAO ESP32S3",
      "Image Classification"
    ]
  },
  {
    "objectID": "contents/seeed/xiao_esp32s3/image_classification/image_classification.html#sec-image-classification-model-deployment-068b",
    "href": "contents/seeed/xiao_esp32s3/image_classification/image_classification.html#sec-image-classification-model-deployment-068b",
    "title": "Image Classification",
    "section": "",
    "text": "We can deploy the trained model:\n\nAs .TFLITE to be used on the SenseCraft AI \nAs an Arduino Library in the Edge Impulse Studio.\n\nLet‚Äôs start with the SenseCraft, which is more straightforward and more intuitive.\n\n\nOn the Dashboard, it is possible to download the trained model in several different formats. Let‚Äôs download TensorFlow Lite (int8 quantized), which has a size of 623KB.\n \nOn SenseCraft AI Studio, go to the Workspace tab, select XIAO ESP32S3, the corresponding Port, and connect the device.\nYou should see the last model that was uploaded to the device. Select the green button [Upload Model]. A pop-up window will prompt you to enter the model name, the model file, and the class names (objects). We should use labels in alphabetical order: 0: background, 1: box, and 2: wheel, and then press [Send].\n \nAfter a few seconds, the model will be uploaded (‚Äúflashed‚Äù) to our device, and the camera image will appear in real-time on the Preview Sector. The Classification result will be displayed under the image preview. It is also possible to select the Confidence Threshold of your inference using the cursor on Settings.\nOn the Device Logger, we can view the Serial Monitor, where we can observe the latency, which is approximately 81 ms for pre-processing and 205 ms for inference, corresponding to a frame rate of 3.4 frames per second (FPS), what is double of we got, training the model on SenseCraft, because we are working with smaller images (96x96 versus 224x224).\n\nThe total latency is around 4 times faster than the estimation made in Edge Impulse Studio on an Xtensa LX6 CPU; now we are performing the inference on an Xtensa LX7 CPU.\n\n \n\n\nIt is possible to obtain the output of a model inference, including Latency, Class ID, and Confidence, as shown on the Device Logger in SenseCraft AI. This allows us to utilize the XIAO ESP32S3 Sense as an AI sensor. In other words, we can retrieve the model data using different communication protocols such as MQTT, UART, I2C, or SPI, depending on our project requirements.\n\nThe idea is similar to what we have done on the Seeed Grove Vision AI V2 Image Classification Post-Processing Lab.\n\nBelow is an example of a connection using the I2C bus.\n\nPlease refer to the Seeed Studio Wiki for more information.\n\n\n\n\nOn the Deploy section at Edge Impulse Studio, Select Arduino library, TensorFlow Lite, Quantized(int8), and press [Build]. The trained model will be downloaded as a .zip Arduino library:\n \nOpen your Arduino IDE, and under Sketch, go to Include Library and add .ZIP Library. Next, select the file downloaded from Edge Impulse Studio and press [Open].\n \nGo to the Arduino IDE Examples and look for the project by its name (in this case: ‚ÄúBox_versus_Whell_‚Ä¶Interfering‚Äù. Open esp32 -&gt; esp32_camera. The sketch esp32_camera.ino will be downloaded to the IDE.\nThis sketch was developed for the standard ESP32 and will not work with the XIAO ESP32S3 Sense. It should be modified. Let‚Äôs download the modified one from the project GitHub: Image_class_XIAOML-Kit.ino.\n\n\nThe code captures images from the onboard camera, processes them, and classifies them (in this case, ‚ÄúBox‚Äù, ‚ÄúWheel‚Äù, or ‚ÄúBackground‚Äù) using the trained model on EI Studio. It runs continuously, performing real-time inference on the edge device.\nIn short,:\nCamera ‚Üí JPEG Image ‚Üí RGB888 Conversion ‚Üí Resize to 96x96 ‚Üí Neural Network ‚Üí Classification Results ‚Üí Serial Output\n\n\n\nLibrary Includes and Dependencies\n\n#include &lt;Box_versus_Wheel_-_XIAO_ESP32S3_inferencing.h&gt;\n#include \"edge-impulse-sdk/dsp/image/image.hpp\"\n#include \"esp_camera.h\"\n\nEdge Impulse Inference Library: Contains our trained model and inference engine\nImage Processing: Provides functions for image manipulation\nESP Camera: Hardware interface for the camera module\n\n\nCamera Pin Configurations\n\nThe XIAO ESP32S3 Sense can work with different camera sensors (OV2640 or OV3660), which may have different pin configurations. The code defines three possible configurations:\n// Configuration 1: Most common OV2640 configuration\n#define CONFIG_1_XCLK_GPIO_NUM    10\n#define CONFIG_1_SIOD_GPIO_NUM    40\n#define CONFIG_1_SIOC_GPIO_NUM    39\n// ... more pins\nThis flexibility allows the code to automatically try different pin mappings if the first one doesn‚Äôt work, making it more robust across different hardware revisions.\n\nMemory Management Settings\n\n#define EI_CAMERA_RAW_FRAME_BUFFER_COLS   320\n#define EI_CAMERA_RAW_FRAME_BUFFER_ROWS   240\n#define EI_CLASSIFIER_ALLOCATION_HEAP      1\n\nFrame Buffer Size: Defines the raw image size (320x240 pixels)\nHeap Allocation: Uses dynamic memory allocation for flexibility\nPSRAM Support: The ESP32S3 has 8MB of PSRAM for storing large data like images\n\n\n\n\nvoid setup() {\n    Serial.begin(115200);\n    while (!Serial);\n\n    if (ei_camera_init() == false) {\n        ei_printf(\"Failed to initialize Camera!\\r\\n\");\n    } else {\n        ei_printf(\"Camera initialized\\r\\n\");\n    }\n\n    ei_sleep(2000);  // Wait 2 seconds before starting\n}\nThis function:\n\nInitializes serial communication for debugging output\nInitializes the camera with automatic configuration detection\nWaits 2 seconds before starting continuous inference\n\n\n\n\nThe loop performs these steps continuously:\nStep 1: Memory Allocation\nsnapshot_buf = (uint8_t*)ps_malloc(EI_CAMERA_RAW_FRAME_BUFFER_COLS *\n                                   EI_CAMERA_RAW_FRAME_BUFFER_ROWS *\n                                   EI_CAMERA_FRAME_BYTE_SIZE);\nAllocates memory for the image buffer, preferring PSRAM (faster external RAM) but falling back to regular heap if needed.\nStep 2: Image Capture\nif (ei_camera_capture((size_t)EI_CLASSIFIER_INPUT_WIDTH,\n                     (size_t)EI_CLASSIFIER_INPUT_HEIGHT,\n                     snapshot_buf) == false) {\n    ei_printf(\"Failed to capture image\\r\\n\");\n    free(snapshot_buf);\n    return;\n}\nCaptures an image from the camera and stores it in the buffer.\nStep 3: Run Inference\nei_impulse_result_t result = { 0 };\nEI_IMPULSE_ERROR err = run_classifier(&signal, &result, false);\nRuns the machine learning model on the captured image.\nStep 4: Output Results\nfor (uint16_t i = 0; i &lt; EI_CLASSIFIER_LABEL_COUNT; i++) {\n    ei_printf(\"  %s: %.5f\\r\\n\",\n              ei_classifier_inferencing_categories[i],\n              result.classification[i].value);\n}\nPrints the classification results showing confidence scores for each category.\n\n\n\nThis function implements an intelligent initialization sequence:\nbool ei_camera_init(void) {\n    // Try Configuration 1 (OV2640 common)\n    update_camera_config(1);\n    esp_err_t err = esp_camera_init(&camera_config);\n    if (err == ESP_OK) goto camera_init_success;\n\n    // Try Configuration 2 (OV3660)\n    esp_camera_deinit();\n    update_camera_config(2);\n    err = esp_camera_init(&camera_config);\n    if (err == ESP_OK) goto camera_init_success;\n\n    // Continue trying other configurations...\n}\nThe function:\n\nTries multiple pin configurations\nTests different clock frequencies (10MHz or 16MHz)\nAttempts PSRAM first, then falls back to DRAM\nApplies sensor-specific settings based on detected hardware\n\n\n\n\nbool ei_camera_capture(uint32_t img_width, uint32_t img_height, uint8_t *out_buf) {\n    // 1. Get frame from camera\n    camera_fb_t *fb = esp_camera_fb_get();\n\n    // 2. Convert JPEG to RGB888 format\n    bool converted = fmt2rgb888(fb-&gt;buf, fb-&gt;len, PIXFORMAT_JPEG, snapshot_buf);\n\n    // 3. Return frame buffer to camera driver\n    esp_camera_fb_return(fb);\n\n    // 4. Resize if needed\n    if (do_resize) {\n        ei::image::processing::crop_and_interpolate_rgb888(...);\n    }\n}\nThis function:\n\nCaptures a JPEG image from the camera\nConverts it to RGB888 format (required by the ML model)\nResizes the image to match the model‚Äôs input size (96x96 pixels)\n\n\n\n\n\n\n\nUpload the code to the XIAO ESP32S3 Sense.\n\n\n‚ö†Ô∏è Attention\n\nThe Xiao ESP32S3 MUST have the PSRAM enabled. You can check it on the Arduino IDE upper menu: Tools‚Äì&gt; PSRAM:OPI PSRAM\nThe Arduino Boards package (esp32 by Espressif Systems) should be version 2.017. Do not update it\n\n\n \n\nOpen the Serial Monitor\nPoint the camera at the objects, and check the result on the Serial Monitor.\n\n \n\n\n\nIn edge AI applications, the inference result is only as valuable as our ability to act upon it. While serial output provides detailed information for debugging and development, real-world deployments require immediate, human-readable feedback that doesn‚Äôt depend on external monitors or connections.\nThe XIAOML Kit tiny 0.42‚Äù OLED display (72√ó40 pixels) serves as a crucial post-processing component that transforms raw ML inference results into immediate, human-readable feedback‚Äîdisplaying detected class names and confidence levels directly on the device, eliminating the need for external monitors and enabling truly standalone edge AI deployment in industrial, agricultural, or retail environments where instant visual confirmation of AI predictions is essential.\nSo, let‚Äôs modify the sketch to automatically adapt to the model trained on Edge Impulse by reading the class names and count directly from the model. The display will show abbreviated class names (3 letters) with larger fonts for better visibility on the tiny 72x40 pixel display. Download the code from the GitHub: XIAOML-Kit-Img_Class_OLED_Gen.\nRunning the code, we can see the result:",
    "crumbs": [
      "Seeed XIAO ESP32S3",
      "Image Classification"
    ]
  },
  {
    "objectID": "contents/seeed/xiao_esp32s3/image_classification/image_classification.html#sec-image-classification-summary-f24b",
    "href": "contents/seeed/xiao_esp32s3/image_classification/image_classification.html#sec-image-classification-summary-f24b",
    "title": "Image Classification",
    "section": "",
    "text": "The XIAO ESP32S3 Sense is a remarkably capable and flexible platform for image classification applications. Through this lab, we‚Äôve explored two distinct development approaches that cater to different skill levels and project requirements.\n\nThe SenseCraft AI Studio provides an accessible entry point with its no-code interface, enabling rapid prototyping and deployment of pre-trained models like person detection. With real-time inference and integrated post-processing capabilities, it demonstrates how AI can be deployed without extensive programming or ML knowledge.\nFor more advanced applications, Edge Impulse Studio offers comprehensive machine learning pipeline tools, including custom dataset management, transfer learning with several pre-trained models, such as MobileNet, and model optimization.\n\nKey insights from this lab include the importance of image resolution trade-offs, the effectiveness of transfer learning for small datasets, and the practical considerations of edge AI deployment, such as power consumption and memory constraints.\nThe Lab demonstrates fundamental TinyML principles that extend beyond this specific hardware: resource-constrained inference, real-time processing requirements, and the complete pipeline from data collection through model deployment to practical applications. With built-in post-processing capabilities including GPIO control and communication protocols, the XIAO serves as more than just an inference engine‚Äîit becomes a complete AI sensor platform.\nThis foundation in image classification prepares you for more complex computer vision tasks while showcasing how modern edge AI makes sophisticated computer vision accessible, cost-effective, and deployable in real-world embedded applications ranging from industrial automation to smart home systems.",
    "crumbs": [
      "Seeed XIAO ESP32S3",
      "Image Classification"
    ]
  },
  {
    "objectID": "contents/seeed/xiao_esp32s3/image_classification/image_classification.html#sec-image-classification-resources-1cf2",
    "href": "contents/seeed/xiao_esp32s3/image_classification/image_classification.html#sec-image-classification-resources-1cf2",
    "title": "Image Classification",
    "section": "",
    "text": "Getting Started with the XIAO ESP32S3\nSenseCraft AI Studio Home\nSenseCraft Vision Workspace\nDataset example\nEdge Impulse Project\nXIAO as an AI Sensor\nSeeed Arduino SSCMA Library\nXIAOML Kit Code",
    "crumbs": [
      "Seeed XIAO ESP32S3",
      "Image Classification"
    ]
  }
]