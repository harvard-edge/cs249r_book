<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.8.25">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>setup_and_no_code_apps – Hardware Kits</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="../../../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../../../">
<link href="../../../../contents/seeed/grove_vision_ai_v2/image_classification/image_classification.html" rel="next">
<link href="../../../../contents/seeed/grove_vision_ai_v2/grove_vision_ai_v2.html" rel="prev">
<script src="../../../../site_libs/quarto-html/quarto.js" type="module"></script>
<script src="../../../../site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="../../../../site_libs/quarto-html/axe/axe-check.js" type="module"></script>
<script src="../../../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../../../site_libs/quarto-html/quarto-syntax-highlighting-7967bf6c3ffb24f4d1c78c3e5f15981c.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="../../../../site_libs/quarto-html/quarto-syntax-highlighting-dark-b248dc20f024d102a7d82dc1fc891d4b.css" rel="stylesheet" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<link href="../../../../site_libs/quarto-html/quarto-syntax-highlighting-7967bf6c3ffb24f4d1c78c3e5f15981c.css" rel="stylesheet" class="quarto-color-scheme-extra" id="quarto-text-highlighting-styles">
<script src="../../../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../../../site_libs/bootstrap/bootstrap-da191edd467e11f2f44f5b3c6902b616.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="../../../../site_libs/bootstrap/bootstrap-dark-d5fd3d3512f39c4bb1c0c98c22df562f.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="light">
<link href="../../../../site_libs/bootstrap/bootstrap-da191edd467e11f2f44f5b3c6902b616.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme-extra" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "/"
  ],
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>


</head>

<body class="nav-sidebar floating nav-fixed quarto-light"><script id="quarto-html-before-body" type="application/javascript">
    const toggleBodyColorMode = (bsSheetEl) => {
      const mode = bsSheetEl.getAttribute("data-mode");
      const bodyEl = window.document.querySelector("body");
      if (mode === "dark") {
        bodyEl.classList.add("quarto-dark");
        bodyEl.classList.remove("quarto-light");
      } else {
        bodyEl.classList.add("quarto-light");
        bodyEl.classList.remove("quarto-dark");
      }
    }
    const toggleBodyColorPrimary = () => {
      const bsSheetEl = window.document.querySelector("link#quarto-bootstrap:not([rel=disabled-stylesheet])");
      if (bsSheetEl) {
        toggleBodyColorMode(bsSheetEl);
      }
    }
    const setColorSchemeToggle = (alternate) => {
      const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
      for (let i=0; i < toggles.length; i++) {
        const toggle = toggles[i];
        if (toggle) {
          if (alternate) {
            toggle.classList.add("alternate");
          } else {
            toggle.classList.remove("alternate");
          }
        }
      }
    };
    const toggleColorMode = (alternate) => {
      // Switch the stylesheets
      const primaryStylesheets = window.document.querySelectorAll('link.quarto-color-scheme:not(.quarto-color-alternate)');
      const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
      manageTransitions('#quarto-margin-sidebar .nav-link', false);
      if (alternate) {
        // note: dark is layered on light, we don't disable primary!
        enableStylesheet(alternateStylesheets);
        for (const sheetNode of alternateStylesheets) {
          if (sheetNode.id === "quarto-bootstrap") {
            toggleBodyColorMode(sheetNode);
          }
        }
      } else {
        disableStylesheet(alternateStylesheets);
        enableStylesheet(primaryStylesheets)
        toggleBodyColorPrimary();
      }
      manageTransitions('#quarto-margin-sidebar .nav-link', true);
      // Switch the toggles
      setColorSchemeToggle(alternate)
      // Hack to workaround the fact that safari doesn't
      // properly recolor the scrollbar when toggling (#1455)
      if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
        manageTransitions("body", false);
        window.scrollTo(0, 1);
        setTimeout(() => {
          window.scrollTo(0, 0);
          manageTransitions("body", true);
        }, 40);
      }
    }
    const disableStylesheet = (stylesheets) => {
      for (let i=0; i < stylesheets.length; i++) {
        const stylesheet = stylesheets[i];
        stylesheet.rel = 'disabled-stylesheet';
      }
    }
    const enableStylesheet = (stylesheets) => {
      for (let i=0; i < stylesheets.length; i++) {
        const stylesheet = stylesheets[i];
        if(stylesheet.rel !== 'stylesheet') { // for Chrome, which will still FOUC without this check
          stylesheet.rel = 'stylesheet';
        }
      }
    }
    const manageTransitions = (selector, allowTransitions) => {
      const els = window.document.querySelectorAll(selector);
      for (let i=0; i < els.length; i++) {
        const el = els[i];
        if (allowTransitions) {
          el.classList.remove('notransition');
        } else {
          el.classList.add('notransition');
        }
      }
    }
    const isFileUrl = () => {
      return window.location.protocol === 'file:';
    }
    const hasAlternateSentinel = () => {
      let styleSentinel = getColorSchemeSentinel();
      if (styleSentinel !== null) {
        return styleSentinel === "alternate";
      } else {
        return false;
      }
    }
    const setStyleSentinel = (alternate) => {
      const value = alternate ? "alternate" : "default";
      if (!isFileUrl()) {
        window.localStorage.setItem("quarto-color-scheme", value);
      } else {
        localAlternateSentinel = value;
      }
    }
    const getColorSchemeSentinel = () => {
      if (!isFileUrl()) {
        const storageValue = window.localStorage.getItem("quarto-color-scheme");
        return storageValue != null ? storageValue : localAlternateSentinel;
      } else {
        return localAlternateSentinel;
      }
    }
    const toggleGiscusIfUsed = (isAlternate, darkModeDefault) => {
      const baseTheme = document.querySelector('#giscus-base-theme')?.value ?? 'light';
      const alternateTheme = document.querySelector('#giscus-alt-theme')?.value ?? 'dark';
      let newTheme = '';
      if(authorPrefersDark) {
        newTheme = isAlternate ? baseTheme : alternateTheme;
      } else {
        newTheme = isAlternate ? alternateTheme : baseTheme;
      }
      const changeGiscusTheme = () => {
        // From: https://github.com/giscus/giscus/issues/336
        const sendMessage = (message) => {
          const iframe = document.querySelector('iframe.giscus-frame');
          if (!iframe) return;
          iframe.contentWindow.postMessage({ giscus: message }, 'https://giscus.app');
        }
        sendMessage({
          setConfig: {
            theme: newTheme
          }
        });
      }
      const isGiscussLoaded = window.document.querySelector('iframe.giscus-frame') !== null;
      if (isGiscussLoaded) {
        changeGiscusTheme();
      }
    };
    const authorPrefersDark = false;
    const queryPrefersDark = window.matchMedia('(prefers-color-scheme: dark)');
    const darkModeDefault = queryPrefersDark.matches;
      document.querySelector('link#quarto-text-highlighting-styles.quarto-color-scheme-extra').rel = 'disabled-stylesheet';
      document.querySelector('link#quarto-bootstrap.quarto-color-scheme-extra').rel = 'disabled-stylesheet';
    let localAlternateSentinel = darkModeDefault ? 'alternate' : 'default';
    // Dark / light mode switch
    window.quartoToggleColorScheme = () => {
      // Read the current dark / light value
      let toAlternate = !hasAlternateSentinel();
      toggleColorMode(toAlternate);
      setStyleSentinel(toAlternate);
      toggleGiscusIfUsed(toAlternate, darkModeDefault);
      window.dispatchEvent(new Event('resize'));
    };
    queryPrefersDark.addEventListener("change", e => {
      if(window.localStorage.getItem("quarto-color-scheme") !== null)
        return;
      const alternate = e.matches
      toggleColorMode(alternate);
      localAlternateSentinel = e.matches ? 'alternate' : 'default'; // this is used alongside local storage!
      toggleGiscusIfUsed(alternate, darkModeDefault);
    });
    // Switch to dark mode if need be
    if (hasAlternateSentinel()) {
      toggleColorMode(true);
    } else {
      toggleColorMode(false);
    }
  </script>

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-md " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a href="../../../../index.html" class="navbar-brand navbar-brand-logo">
    <img src="../../../../assets/images/favicon.png" alt="" class="navbar-logo light-content">
    <img src="../../../../assets/images/favicon.png" alt="" class="navbar-logo dark-content">
    </a>
    <a class="navbar-brand" href="../../../../index.html">
    <span class="navbar-title">Machine Learning Systems</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-hardware-kits" role="link" data-bs-toggle="dropdown" aria-expanded="false">
 <span class="menu-text">Hardware Kits</span>
    </a>
    <ul class="dropdown-menu" aria-labelledby="nav-menu-hardware-kits">    
        <li>
    <a class="dropdown-item" href="../../../../../book/"><i class="bi bi-book-half" role="img">
</i> 
 <span class="dropdown-text">Textbook</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../../../../tinytorch/"><i class="bi bi-fire" role="img">
</i> 
 <span class="dropdown-text">TinyTorch</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../../.././"><i class="bi bi-cpu" role="img">
</i> 
 <span class="dropdown-text">Hardware Kits</span></a>
  </li>  
        <li><hr class="dropdown-divider"></li>
        <li>
    <a class="dropdown-item" href="../../../../../collabs/"><i class="bi bi-lightbulb" role="img">
</i> 
 <span class="dropdown-text">Co-Labs (Coming 2026)</span></a>
  </li>  
    </ul>
  </li>
</ul>
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-downloads" role="link" data-bs-toggle="dropdown" aria-expanded="false">
      <i class="bi bi-download" role="img">
</i> 
 <span class="menu-text">Downloads</span>
    </a>
    <ul class="dropdown-menu dropdown-menu-end" aria-labelledby="nav-menu-downloads">    
        <li>
    <a class="dropdown-item" href="../../../../assets/downloads/Hardware-Kits.pdf" target="_blank"><i class="bi bi-file-pdf" role="img">
</i> 
 <span class="dropdown-text">Hardware Kits PDF</span></a>
  </li>  
    </ul>
  </li>
  <li class="nav-item">
    <a class="nav-link" href="https://github.com/harvard-edge/cs249r_book#support-this-work" target="_blank"> <i class="bi bi-star" role="img">
</i> 
<span class="menu-text">Star</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="https://opencollective.com/mlsysbook" target="_blank"> <i class="bi bi-heart" role="img">
</i> 
<span class="menu-text">Support</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../../../#subscribe"> <i class="bi bi-envelope" role="img">
</i> 
<span class="menu-text">Subscribe</span></a>
  </li>  
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-github" role="link" data-bs-toggle="dropdown" aria-expanded="false">
      <i class="bi bi-github" role="img">
</i> 
 <span class="menu-text">GitHub</span>
    </a>
    <ul class="dropdown-menu dropdown-menu-end" aria-labelledby="nav-menu-github">    
        <li>
    <a class="dropdown-item" href="https://github.com/harvard-edge/cs249r_book" target="_blank"><i class="bi bi-pencil" role="img">
</i> 
 <span class="dropdown-text">Edit this page</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="https://github.com/harvard-edge/cs249r_book/issues/new" target="_blank"><i class="bi bi-bug" role="img">
</i> 
 <span class="dropdown-text">Report an issue</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="https://github.com/harvard-edge/cs249r_book/discussions" target="_blank"><i class="bi bi-chat" role="img">
</i> 
 <span class="dropdown-text">Discussions</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="https://github.com/harvard-edge/cs249r_book" target="_blank"><i class="bi bi-code" role="img">
</i> 
 <span class="dropdown-text">View source</span></a>
  </li>  
    </ul>
  </li>
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
  <a href="" class="quarto-color-scheme-toggle quarto-navigation-tool  px-1" onclick="window.quartoToggleColorScheme(); return false;" title="Toggle dark mode"><i class="bi"></i></a>
</div>
      </div> <!-- /container-fluid -->
    </nav>
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../../../../contents/seeed/grove_vision_ai_v2/grove_vision_ai_v2.html">Grove Vision AI V2</a></li><li class="breadcrumb-item"><a href="../../../../contents/seeed/grove_vision_ai_v2/setup_and_no_code_apps/setup_and_no_code_apps.html">Setup and No-Code Applications</a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
      <a href="../../../../index.html" class="sidebar-logo-link">
      </a>
      </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Overview</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../contents/getting-started.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Getting Started</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../contents/platforms.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Platforms</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../contents/ide-setup.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">IDE Setup</span></a>
  </div>
</li>
        <li class="px-0"><hr class="sidebar-divider hi "></li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true">
 <span class="menu-text">Arduino Nicla Vision</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../contents/arduino/nicla_vision/nicla_vision.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Overview</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../contents/arduino/nicla_vision/setup/setup.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Setup</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../contents/arduino/nicla_vision/image_classification/image_classification.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Image Classification</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../contents/arduino/nicla_vision/object_detection/object_detection.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Object Detection</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../contents/arduino/nicla_vision/kws/kws.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Keyword Spotting (KWS)</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../contents/arduino/nicla_vision/motion_classification/motion_classification.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Motion Classification and Anomaly Detection</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true">
 <span class="menu-text">Seeed XIAO ESP32S3</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../contents/seeed/xiao_esp32s3/xiao_esp32s3.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Overview</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../contents/seeed/xiao_esp32s3/setup/setup.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Setup</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../contents/seeed/xiao_esp32s3/image_classification/image_classification.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Image Classification</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../contents/seeed/xiao_esp32s3/object_detection/object_detection.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Object Detection</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../contents/seeed/xiao_esp32s3/kws/kws.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Keyword Spotting (KWS)</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../contents/seeed/xiao_esp32s3/motion_classification/motion_classification.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Motion Classification and Anomaly Detection</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="true">
 <span class="menu-text">Grove Vision AI V2</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../contents/seeed/grove_vision_ai_v2/grove_vision_ai_v2.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Overview</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../contents/seeed/grove_vision_ai_v2/setup_and_no_code_apps/setup_and_no_code_apps.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text">Setup and No-Code Applications</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../contents/seeed/grove_vision_ai_v2/image_classification/image_classification.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Image Classification</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../contents/seeed/grove_vision_ai_v2/object_detection/object_detection.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Object Detection</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" role="navigation" aria-expanded="true">
 <span class="menu-text">Raspberry Pi</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-4" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../contents/raspi/raspi.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Overview</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../contents/raspi/setup/setup.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Setup</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../contents/raspi/image_classification/image_classification.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Image Classification</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../contents/raspi/object_detection/object_detection.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Object Detection</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../contents/raspi/llm/llm.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Small Language Models (SLM)</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../contents/raspi/vlm/vlm.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Vision-Language Models (VLM)</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" role="navigation" aria-expanded="true">
 <span class="menu-text">Shared Resources</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-5" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../contents/shared/shared.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Overview</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../contents/shared/kws_feature_eng/kws_feature_eng.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">KWS Feature Engineering</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../contents/shared/dsp_spectral_features_block/dsp_spectral_features_block.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">DSP Spectral Features</span></a>
  </div>
</li>
      </ul>
  </li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#setup-and-no-code-applications" id="toc-setup-and-no-code-applications" class="nav-link active" data-scroll-target="#setup-and-no-code-applications">Setup and No-Code Applications</a>
  <ul class="collapse">
  <li><a href="#sec-setup-nocode-applications-introduction-b740" id="toc-sec-setup-nocode-applications-introduction-b740" class="nav-link" data-scroll-target="#sec-setup-nocode-applications-introduction-b740">Introduction</a>
  <ul class="collapse">
  <li><a href="#sec-setup-nocode-applications-grove-vision-ai-module-v2-overview-f313" id="toc-sec-setup-nocode-applications-grove-vision-ai-module-v2-overview-f313" class="nav-link" data-scroll-target="#sec-setup-nocode-applications-grove-vision-ai-module-v2-overview-f313">Grove Vision AI Module (V2) Overview</a></li>
  <li><a href="#sec-setup-nocode-applications-camera-installation-46cb" id="toc-sec-setup-nocode-applications-camera-installation-46cb" class="nav-link" data-scroll-target="#sec-setup-nocode-applications-camera-installation-46cb">Camera Installation</a></li>
  </ul></li>
  <li><a href="#sec-setup-nocode-applications-sensecraft-ai-studio-04c9" id="toc-sec-setup-nocode-applications-sensecraft-ai-studio-04c9" class="nav-link" data-scroll-target="#sec-setup-nocode-applications-sensecraft-ai-studio-04c9">The SenseCraft AI Studio</a>
  <ul class="collapse">
  <li><a href="#sec-setup-nocode-applications-sensecraft-webtoolkit-939f" id="toc-sec-setup-nocode-applications-sensecraft-webtoolkit-939f" class="nav-link" data-scroll-target="#sec-setup-nocode-applications-sensecraft-webtoolkit-939f">The SenseCraft Web-Toolkit</a></li>
  </ul></li>
  <li><a href="#sec-setup-nocode-applications-exploring-cv-ai-models-90ce" id="toc-sec-setup-nocode-applications-exploring-cv-ai-models-90ce" class="nav-link" data-scroll-target="#sec-setup-nocode-applications-exploring-cv-ai-models-90ce">Exploring CV AI models</a>
  <ul class="collapse">
  <li><a href="#sec-setup-nocode-applications-object-detection-8a88" id="toc-sec-setup-nocode-applications-object-detection-8a88" class="nav-link" data-scroll-target="#sec-setup-nocode-applications-object-detection-8a88">Object Detection</a></li>
  <li><a href="#sec-setup-nocode-applications-posekeypoint-detection-bc09" id="toc-sec-setup-nocode-applications-posekeypoint-detection-bc09" class="nav-link" data-scroll-target="#sec-setup-nocode-applications-posekeypoint-detection-bc09">Pose/Keypoint Detection</a></li>
  <li><a href="#sec-setup-nocode-applications-image-classification-e810" id="toc-sec-setup-nocode-applications-image-classification-e810" class="nav-link" data-scroll-target="#sec-setup-nocode-applications-image-classification-e810">Image Classification</a></li>
  <li><a href="#sec-setup-nocode-applications-exploring-models-sensecraft-ai-studio-2fa8" id="toc-sec-setup-nocode-applications-exploring-models-sensecraft-ai-studio-2fa8" class="nav-link" data-scroll-target="#sec-setup-nocode-applications-exploring-models-sensecraft-ai-studio-2fa8">Exploring Other Models on SenseCraft AI Studio</a></li>
  </ul></li>
  <li><a href="#sec-setup-nocode-applications-image-classification-project-0e45" id="toc-sec-setup-nocode-applications-image-classification-project-0e45" class="nav-link" data-scroll-target="#sec-setup-nocode-applications-image-classification-project-0e45">An Image Classification Project</a>
  <ul class="collapse">
  <li><a href="#sec-setup-nocode-applications-goal-bd92" id="toc-sec-setup-nocode-applications-goal-bd92" class="nav-link" data-scroll-target="#sec-setup-nocode-applications-goal-bd92">The Goal</a></li>
  <li><a href="#sec-setup-nocode-applications-data-collection-2e78" id="toc-sec-setup-nocode-applications-data-collection-2e78" class="nav-link" data-scroll-target="#sec-setup-nocode-applications-data-collection-2e78">Data Collection</a></li>
  <li><a href="#sec-setup-nocode-applications-training-db50" id="toc-sec-setup-nocode-applications-training-db50" class="nav-link" data-scroll-target="#sec-setup-nocode-applications-training-db50">Training</a></li>
  <li><a href="#sec-setup-nocode-applications-test-2855" id="toc-sec-setup-nocode-applications-test-2855" class="nav-link" data-scroll-target="#sec-setup-nocode-applications-test-2855">Test</a></li>
  <li><a href="#sec-setup-nocode-applications-deployment-b828" id="toc-sec-setup-nocode-applications-deployment-b828" class="nav-link" data-scroll-target="#sec-setup-nocode-applications-deployment-b828">Deployment</a></li>
  <li><a href="#sec-setup-nocode-applications-saving-model-2f46" id="toc-sec-setup-nocode-applications-saving-model-2f46" class="nav-link" data-scroll-target="#sec-setup-nocode-applications-saving-model-2f46">Saving the Model</a></li>
  </ul></li>
  <li><a href="#sec-setup-nocode-applications-summary-d118" id="toc-sec-setup-nocode-applications-summary-d118" class="nav-link" data-scroll-target="#sec-setup-nocode-applications-summary-d118">Summary</a></li>
  <li><a href="#sec-setup-nocode-applications-resources-85aa" id="toc-sec-setup-nocode-applications-resources-85aa" class="nav-link" data-scroll-target="#sec-setup-nocode-applications-resources-85aa">Resources</a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content"><header id="title-block-header" class="quarto-title-block"><nav class="quarto-page-breadcrumbs quarto-title-breadcrumbs d-none d-lg-block" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../../../../contents/seeed/grove_vision_ai_v2/grove_vision_ai_v2.html">Grove Vision AI V2</a></li><li class="breadcrumb-item"><a href="../../../../contents/seeed/grove_vision_ai_v2/setup_and_no_code_apps/setup_and_no_code_apps.html">Setup and No-Code Applications</a></li></ol></nav></header>





<section id="setup-and-no-code-applications" class="level1 unnumbered">
<h1 class="unnumbered">Setup and No-Code Applications</h1>
<p><img src="./images/jpeg/cover-part1.jpg" class="img-fluid"></p>
<p>In this Lab, we will explore computer vision (CV) applications using the Seeed Studio <a href="https://wiki.seeedstudio.com/grove_vision_ai_v2/"><em>Grove Vision AI Module V2</em></a>, a powerful yet compact device specifically designed for embedded machine learning applications. Based on the <strong>Himax WiseEye2</strong> chip, this module is designed to enable AI capabilities on edge devices, making it an ideal tool for Edge Machine Learning (ML) applications.</p>
<section id="sec-setup-nocode-applications-introduction-b740" class="level2">
<h2 class="anchored" data-anchor-id="sec-setup-nocode-applications-introduction-b740">Introduction</h2>
<section id="sec-setup-nocode-applications-grove-vision-ai-module-v2-overview-f313" class="level3">
<h3 class="anchored" data-anchor-id="sec-setup-nocode-applications-grove-vision-ai-module-v2-overview-f313">Grove Vision AI Module (V2) Overview</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="./images/jpeg/grove.jpg" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:60.0%"></p>
</figure>
</div>
<p>The Grove Vision AI (V2) is an MCU-based vision AI module that utilizes a <a href="https://www.himax.com.tw/products/intelligent-sensing/always-on-smart-sensing/wiseeye2-ai-processor/">Himax WiseEye2 HX6538</a> processor featuring a <strong>dual-core Arm Cortex-M55 and an integrated ARM Ethos-U55 neural network unit</strong>. The <a href="https://www.arm.com/products/silicon-ip-cpu/ethos/ethos-u55">Arm Ethos-U55</a> is a machine learning (ML) processor class, specifically designed as a microNPU, to accelerate ML inference in area-constrained embedded and IoT devices. The Ethos-U55, combined with the AI-capable Cortex-M55 processor, provides a 480x uplift in ML performance over existing Cortex-M-based systems. Its clock frequency is 400 MHz, and its internal system memory (SRAM) is configurable, with a maximum capacity of 2.4 MB.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="./images/jpeg/himax-wiseeye2.jpg" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:80.0%"></p>
</figure>
</div>
<blockquote class="blockquote">
<p>Note: Based on Seeed Studio documentation, besides the Himax internal memory of 2.5MB (2.4MB SRAM + 64KB ROM), the Grove Vision AI (V2) is also equipped with a 16MB/133 MHz external flash.</p>
</blockquote>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="./images/jpeg/device.jpeg" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:80.0%"></p>
</figure>
</div>
<p>Below is a block Diagram of the Grove Vision AI (V2) system, including a camera and a master controller.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="./images/png/system.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:80.0%"></p>
</figure>
</div>
<p>With interfaces like <strong>IIC, UART, SPI, and Type-C,</strong> the Grove Vision AI (V2) can be easily connected to devices such as <strong>XIAO, Raspberry Pi, BeagleBoard</strong>, <strong>and ESP-based products</strong> for further development. For instance, integrating Grove Vision AI V2 with one of the devices from the XIAO family makes it easy to access the data resulting from inference on the device through the Arduino IDE or MicroPython, and conveniently connect to the cloud or dedicated servers, such as Home Assistance.</p>
<blockquote class="blockquote">
<p>Using the <strong>I2C Grove connector</strong>, the Grove Vision AI V2 can be easily connected with any Master Device.</p>
</blockquote>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="./images/png/montage.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:60.0%"></p>
</figure>
</div>
<p>Besides performance, another area to comment on is <strong>Power Consumption</strong>. For example, in a comparative test against the XIAO ESP32S3 Sense, running Swift-YOLO Tiny 96x96, despite achieving higher performance (30 FPS vs.&nbsp;5.5 FPS), the Grove Vision AI V2 exhibited lower power consumption (0.35 W vs.&nbsp;0.45 W) when compared with the XIAO ESP32S3 Sense.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="./images/png/bench-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:80.0%"></p>
</figure>
</div>
<blockquote class="blockquote">
<p>The above comparison (and with other devices) can be found in the article <a href="https://www.hackster.io/limengdu0117/2024-mcu-ai-vision-boards-performance-comparison-998505">2024 MCU AI Vision Boards: Performance Comparison</a>, which confirms the power of Grove Vision AI (V2).</p>
</blockquote>
</section>
<section id="sec-setup-nocode-applications-camera-installation-46cb" class="level3">
<h3 class="anchored" data-anchor-id="sec-setup-nocode-applications-camera-installation-46cb">Camera Installation</h3>
<p>Having the Grove Vision AI (V2) and camera ready, you can connect, for example, a <strong>Raspberry Pi OV5647 Camera Module</strong> via the CSI cable.</p>
<blockquote class="blockquote">
<p>When connecting, please pay attention to the direction of the row of pins and ensure they are plugged in correctly, not in the opposite direction.</p>
</blockquote>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="./images/jpeg/cable_connection_csgwmpmy5i.jpg" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:60.0%"></p>
</figure>
</div>
</section>
</section>
<section id="sec-setup-nocode-applications-sensecraft-ai-studio-04c9" class="level2">
<h2 class="anchored" data-anchor-id="sec-setup-nocode-applications-sensecraft-ai-studio-04c9">The SenseCraft AI Studio</h2>
<p>The <a href="https://sensecraft.seeed.cc/ai/home">SenseCraft AI</a> Studio is a robust platform that offers a wide range of AI models compatible with various devices, including the XIAO ESP32S3 Sense and the <strong>Grove Vision AI V2</strong>. In this lab, we will walk through the process of using an AI model with the Grove Vision AI V2 and preview the model’s output. We will also explore some key concepts, settings, and how to optimize the model’s performance.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="./images/png/studio.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:80.0%"></p>
</figure>
</div>
<p>Models can also be deployed using the <a href="https://seeed-studio.github.io/SenseCraft-Web-Toolkit/#/setup/process"><strong>SenseCraft Web Toolkit</strong></a>, a simplified version of the SenseCraft AI Studio.</p>
<blockquote class="blockquote">
<p>We can start using the SenseCraft Web Toolkit for simplicity, or go directly to the <a href="https://sensecraft.seeed.cc/ai/model">SenseCraft AI Studio</a>, which has more resources.</p>
</blockquote>
<section id="sec-setup-nocode-applications-sensecraft-webtoolkit-939f" class="level3">
<h3 class="anchored" data-anchor-id="sec-setup-nocode-applications-sensecraft-webtoolkit-939f">The SenseCraft Web-Toolkit</h3>
<p>The SenseCraft Web Toolkit is a visual model deployment tool included in the <a href="https://sensecraftma.seeed.cc/">SSCMA</a>(Seeed SenseCraft Model Assistant). This tool enables us to deploy models to various platforms with ease through simple operations. The tool offers a user-friendly interface and does not require any coding.</p>
<p>The SenseCraft Web Toolkit is based on the Himax AI Web Toolkit, which can (<strong>optionally</strong>) be downloaded from <a href="https://github.com/HimaxWiseEyePlus/Seeed_Grove_Vision_AI_Module_V2/releases/download/v1.1/Himax_AI_web_toolkit.zip">here</a>. Once downloaded and unzipped to the local PC, double-click <code>index.html</code> to run it locally.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="./images/png/himax-toolkit.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:60.0%"></p>
</figure>
</div>
<p>But in our case, let’s follow the steps below to start the <strong>SenseCraft-Web-Toolkit</strong>:</p>
<ul>
<li>Open the <a href="https://seeed-studio.github.io/SenseCraft-Web-Toolkit/#/setup/process">SenseCraft-Web-Toolkit website</a> on a web browser as <strong>Chrome</strong>.</li>
<li>Connect Grove Vision AI (V2) to your computer using a Type-C cable.</li>
<li>Having the XIAO connected, select it as below:</li>
</ul>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="./images/png/sensecraft1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:80.0%"></p>
</figure>
</div>
<ul>
<li>Select the device/Port and press <code>[Connect]</code>:</li>
</ul>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="./images/png/sensecraft2.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:80.0%"></p>
</figure>
</div>
<blockquote class="blockquote">
<p>Note: The <strong>WebUSB tool</strong> may not function correctly in certain browsers, such as Safari. Use Chrome instead.</p>
</blockquote>
<p>We can try several Basic Computer Vision models previously uploaded by Seeed Studio. Passing the cursor over the AI models, we can have some information about them, such as name, description, <strong>category</strong> (Image Classification, Object Detection, or Pose/Keypoint Detection), the <strong>algorithm</strong> (like YOLO V5 or V8, FOMO, MobileNet V2, etc.) and <strong>metrics</strong> (Accuracy or mAP).</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="./images/png/apps.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:80.0%"></p>
</figure>
</div>
<p>We can choose one of those ready-to-use AI models by clicking on it and pressing the <code>[Send]</code> button, or upload our model.</p>
<p>For the <strong>SenseCraft AI</strong> platform, follow the instructions <a href="https://wiki.seeedstudio.com/sensecraft_ai_pretrained_models_for_grove_visionai_v2/">here</a>.</p>
</section>
</section>
<section id="sec-setup-nocode-applications-exploring-cv-ai-models-90ce" class="level2">
<h2 class="anchored" data-anchor-id="sec-setup-nocode-applications-exploring-cv-ai-models-90ce">Exploring CV AI models</h2>
<section id="sec-setup-nocode-applications-object-detection-8a88" class="level3">
<h3 class="anchored" data-anchor-id="sec-setup-nocode-applications-object-detection-8a88">Object Detection</h3>
<p>Object detection is a pivotal technology in computer vision that focuses on identifying and locating objects within digital images or video frames. Unlike image classification, which categorizes an entire image into a single label, object detection recognizes multiple objects within the image and determines their precise locations, typically represented by bounding boxes. This capability is crucial for a wide range of applications, including autonomous vehicles, security, surveillance systems, and augmented reality, where understanding the context and content of the visual environment is essential.</p>
<p>Common architectures that have set the benchmark in object detection include the YOLO (You Only Look Once), SSD (Single Shot MultiBox Detector), FOMO (Faster Objects, More Objects), and Faster R-CNN (Region-based Convolutional Neural Networks) models.</p>
<p>Let’s choose one of the ready-to-use AI models, such as <strong>Person Detection</strong>, which was trained using the Swift-YOLO algorithm.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="./images/png/obj-det.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:80.0%"></p>
</figure>
</div>
<p>Once the model is uploaded successfully, you can see the live feed from the Grove Vision AI (V2) camera in the Preview area on the right. Also, the inference details can be shown on the Serial Monitor by clicking on the <code>[Device Log</code>] button at the top.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="./images/png/img-class-yolo.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:80.0%"></p>
</figure>
</div>
<blockquote class="blockquote">
<p>In the SenseCraft AI Studio, the Device Logger is always on the screen.</p>
</blockquote>
<p>Pointing the camera at me, only one person was detected, so that the model output will be a single “box”. Looking in detail, the module sends continuously two lines of information:</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="./images/png/img-class-yolo-result.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:80.0%"></p>
</figure>
</div>
<p><strong>perf</strong> (Performance), displays latency in milliseconds.</p>
<ul>
<li>Preprocess time (image capture and Crop): <strong>7ms</strong>;</li>
<li>Inference time (model latency): <strong>76ms (13 fps)</strong></li>
<li>Postprocess time (display of the image and inclusion of data): less than 0ms.</li>
</ul>
<p><strong>boxes</strong>: Show the objects detected in the image. In this case, only one.</p>
<ul>
<li>The box has the x, y, w, and h coordinates of (<strong>245</strong>, <strong>292</strong>,<strong>449</strong>,<strong>392</strong>), and the object (person, label <strong>0</strong>) was captured with a value of .<strong>89</strong>.</li>
</ul>
<p>If we point the camera at an image with several people, we will get one box for each person (object):</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="./images/png/beatles.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:80.0%"></p>
</figure>
</div>
<blockquote class="blockquote">
<p>On the SenseCraft AI Studio, the inference latency (48ms) is lower than on the SenseCraft ToolKit (76ms), due to a distinct deployment implementation.</p>
</blockquote>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="./images/png/img-class-studio.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:80.0%"></p>
</figure>
</div>
<p><strong>Power Consumption</strong></p>
<p>The peak power consumption running this Swift-YOLO model was 410 milliwatts.</p>
<p><strong>Preview Settings</strong></p>
<p>We can see that in the Settings, two settings options can be adjusted to optimize the model’s recognition accuracy.</p>
<ul>
<li><p><strong>Confidence:</strong> Refers to the level of certainty or probability assigned to its predictions by a model. This value determines the minimum confidence level required for the model to consider a detection as valid. A higher confidence threshold will result in fewer detections but with higher certainty, while a lower threshold will allow more detections but may include some false positives.</p></li>
<li><p><strong>IoU:</strong> Used to assess the accuracy of predicted bounding boxes compared to truth bounding boxes. IoU is a metric that measures the overlap between the predicted bounding box and the ground truth bounding box. It is used to determine the accuracy of the object detection. The IoU threshold sets the minimum IoU value required for a detection to be considered a true positive. Adjusting this threshold can help in fine-tuning the model’s precision and recall.</p></li>
</ul>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="./images/png/settings.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:80.0%"></p>
</figure>
</div>
<blockquote class="blockquote">
<p>Experiment with different values for the Confidence Threshold and IoU Threshold to find the optimal balance between detecting persons accurately and minimizing false positives. The best settings may vary depending on our specific application and the characteristics of the images or video feed.</p>
</blockquote>
</section>
<section id="sec-setup-nocode-applications-posekeypoint-detection-bc09" class="level3">
<h3 class="anchored" data-anchor-id="sec-setup-nocode-applications-posekeypoint-detection-bc09">Pose/Keypoint Detection</h3>
<p>Pose or keypoint detection is a sophisticated area within computer vision that focuses on identifying specific points of interest within an image or video frame, often related to human bodies, faces, or other objects of interest. This technology can detect and map out the various keypoints of a subject, such as the <strong>joints on a human body</strong> or the features of a face, enabling the analysis of postures, movements, and gestures. This has profound implications for various applications, including augmented reality, human-computer interaction, sports analytics, and healthcare monitoring, where understanding human motion and activity is crucial.</p>
<p>Unlike general object detection, which identifies and locates objects, pose detection drills down to a finer level of detail, capturing the nuanced positions and orientations of specific parts. Leading architectures in this field include OpenPose, AlphaPose, and PoseNet, each designed to tackle the challenges of pose estimation with varying degrees of complexity and precision. Through advancements in deep learning and neural networks, pose detection has become increasingly accurate and efficient, offering real-time insights into the intricate dynamics of subjects captured in visual data.</p>
<p>So, let’s explore this popular CV application, <em>Pose/Keypoint Detection</em>.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="./images/png/pose.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:80.0%"></p>
</figure>
</div>
<p>Stop the current model inference by pressing <code>[Stop]</code> in the Preview area. Select the model and press <code>[Send]</code>. Once the model is uploaded successfully, you can view the live feed from the Grove Vision AI (V2) camera in the Preview area on the right, along with the inference details displayed in the Serial Monitor (accessible by clicking the <code>[Device Log]</code> button at the top).</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="./images/png/people-dancing.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:80.0%"></p>
</figure>
</div>
<p>The YOLOV8 Pose model was trained using the <a href="https://docs.ultralytics.com/datasets/pose/coco/">COCO-Pose Dataset</a>, which contains 200K images labeled with <strong>17</strong> keypoints for pose estimation tasks.</p>
<p>Let’s look at a single screenshot of the inference (to simplify, let’s analyse an image with a single person in it). We can note that we have two lines, one with the inference <strong>performance</strong> in milliseconds (121 ms) and a second line with the <strong>keypoints</strong> as below:</p>
<ul>
<li>1 box of info, the same as we got with the object detection example (box coordinates (113, 119, 67, 208), inference result (90), label (0).</li>
<li>17 groups of 4 numbers represent the 17 “joints” of the body, where ‘0’ is the nose, ‘1’ and ‘2’ are the eyes, ‘15’ and’ 16’ are the feet, and so on.</li>
</ul>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="./images/png/pose-girl.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:80.0%"></p>
</figure>
</div>
<blockquote class="blockquote">
<p>To understand a pose estimation project more deeply, please refer to the tutorial: <a href="https://www.hackster.io/mjrobot/exploring-ai-at-the-edge-97588d#toc-pose-estimation-10">Exploring AI at the Edge! - Pose Estimation</a>.</p>
</blockquote>
</section>
<section id="sec-setup-nocode-applications-image-classification-e810" class="level3">
<h3 class="anchored" data-anchor-id="sec-setup-nocode-applications-image-classification-e810">Image Classification</h3>
<p>Image classification is a foundational task within computer vision aimed at categorizing <strong>entire images</strong> into one of several predefined classes. This process involves analyzing the visual content of an image and assigning it a label from a fixed set of categories based on the predominant object or scene it contains.</p>
<p>Image classification is crucial in various applications, ranging from organizing and searching through large databases of images in digital libraries and social media platforms to enabling autonomous systems to comprehend their surroundings. Common architectures that have significantly advanced the field of image classification include Convolutional Neural Networks (CNNs), such as AlexNet, VGGNet, and ResNet. These models have demonstrated remarkable accuracy on challenging datasets, such as <strong>ImageNet,</strong> by learning hierarchical representations of visual data.</p>
<p>As the cornerstone of many computer vision systems, image classification drives innovation, laying the groundwork for more complex tasks like object detection and image segmentation, and facilitating a deeper understanding of visual data across various industries. So, let’s also explore this computer vision application.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="./images/png/person-class.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:80.0%"></p>
</figure>
</div>
<blockquote class="blockquote">
<p>This example is available on the SenseCraft ToolKit, but not in the SenseCraft AI Studio. In the last one, it is possible to find other examples of Image Classification.</p>
</blockquote>
<p>After the model is uploaded successfully, we can view the live feed from the Grove Vision AI (V2) camera in the Preview area on the right, along with the inference details displayed in the Serial Monitor (by clicking the <code>[Device Log]</code> button at the top).</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="./images/png/person.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:80.0%"></p>
</figure>
</div>
<p>As a result, we will receive a score and the class as output.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="./images/png/img-clas-result.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:80.0%"></p>
</figure>
</div>
<p>For example, <strong>[99, 1]</strong> means class: 1 (Person) with a score of 0.99. Once this model is a binary classification, class 0 will be “No Person” (or Background). The Inference latency is <strong>15ms</strong> or around 70fps.</p>
<section id="sec-setup-nocode-applications-power-consumption-20f9" class="level4">
<h4 class="anchored" data-anchor-id="sec-setup-nocode-applications-power-consumption-20f9">Power Consumption</h4>
<p>To run the Mobilenet V2 0.35, the Grove Vision AI V2 had a peak current of 80mA at 5.24V, resulting in a <strong>power consumption of 420mW</strong>.</p>
<p>Running the same model on XIAO ESP32S3 Sense, the <strong>power consumption was 523mW</strong> with a latency of 291ms.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="./images/png/inf-xiao.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:80.0%"></p>
</figure>
</div>
</section>
</section>
<section id="sec-setup-nocode-applications-exploring-models-sensecraft-ai-studio-2fa8" class="level3">
<h3 class="anchored" data-anchor-id="sec-setup-nocode-applications-exploring-models-sensecraft-ai-studio-2fa8">Exploring Other Models on SenseCraft AI Studio</h3>
<p>Several public AI models can also be downloaded from the <a href="https://sensecraft.seeed.cc/ai/model">SenseCraft AI WebPage</a>. For example, you can run a Swift-YOLO model, <a href="https://sensecraft.seeed.cc/ai/view-model/60281-traffic-light-detection?tab=public">detecting traffic lights</a> as shown here:</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="./images/png/new-webpage.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:80.0%"></p>
</figure>
</div>
<p>The latency of this model is approximately 86 ms, with an average power consumption of 420 mW.</p>
</section>
</section>
<section id="sec-setup-nocode-applications-image-classification-project-0e45" class="level2">
<h2 class="anchored" data-anchor-id="sec-setup-nocode-applications-image-classification-project-0e45">An Image Classification Project</h2>
<p>Let’s create a complete Image Classification project, using the SenseCraft AI Studio.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="./images/png/project-block-diagram.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:80.0%"></p>
</figure>
</div>
<p>On SenseCraft AI Studio: Let’s open the tab <a href="https://sensecraft.seeed.cc/ai/training">Training</a>:</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="./images/png/img-class-project.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:80.0%"></p>
</figure>
</div>
<p>The default is to train a <code>Classification</code> model with a WebCam if it is available. Let’s select the Grove Vision AI V2 instead. Pressing the green button<code>[Connect]</code>, a Pop-Up window will appear. Select the corresponding Port and press the blue button <code>[Connect]</code>.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="./images/png/connect-sensecraft-train.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:80.0%"></p>
</figure>
</div>
<p>The image streamed from the Grove Vision AI V2 will be displayed.</p>
<section id="sec-setup-nocode-applications-goal-bd92" class="level3">
<h3 class="anchored" data-anchor-id="sec-setup-nocode-applications-goal-bd92">The Goal</h3>
<p>The first step is always to define a goal. Let’s classify, for example, two simple objects—for instance, a toy <code>box</code> and a toy <code>wheel</code>. We should also include a 3rd class of images, <code>background</code>, where no object is in the scene.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="./images/png/classes_img_class.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:80.0%"></p>
</figure>
</div>
</section>
<section id="sec-setup-nocode-applications-data-collection-2e78" class="level3">
<h3 class="anchored" data-anchor-id="sec-setup-nocode-applications-data-collection-2e78">Data Collection</h3>
<p>Let’s create the classes, following, for example, an alphabetical order:</p>
<ul>
<li>Class1: background</li>
<li>Class 2: box</li>
<li>Class 3: wheel</li>
</ul>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="./images/png/classes.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:80.0%"></p>
</figure>
</div>
<p>Select one of the classes and keep pressing the green button under the preview area. The collected images will appear on the Image Samples Screen.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="./images/png/collect-imaages.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:80.0%"></p>
</figure>
</div>
<p>After collecting the images, review them and delete any incorrect ones.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="./images/png/clean_dataset.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:80.0%"></p>
</figure>
</div>
<p>Collect around 50 images from each class and go to Training Step:</p>
</section>
<section id="sec-setup-nocode-applications-training-db50" class="level3">
<h3 class="anchored" data-anchor-id="sec-setup-nocode-applications-training-db50">Training</h3>
<p>Confirm if the correct device is selected (<code>Grove Vision AI V2</code>) and press <code>[Start Training]</code></p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="./images/png/train-img-class.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:80.0%"></p>
</figure>
</div>
</section>
<section id="sec-setup-nocode-applications-test-2855" class="level3">
<h3 class="anchored" data-anchor-id="sec-setup-nocode-applications-test-2855">Test</h3>
<p>After training, the inference result can be previewed.</p>
<blockquote class="blockquote">
<p>Note that the model is not running on the device. We are, in fact, only capturing the images with the device and performing a live preview using the training model, which is running in the Studio.</p>
</blockquote>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="./images/png/img-class-infer.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:80.0%"></p>
</figure>
</div>
<p>Now is time to really deploy the model in the device:</p>
</section>
<section id="sec-setup-nocode-applications-deployment-b828" class="level3">
<h3 class="anchored" data-anchor-id="sec-setup-nocode-applications-deployment-b828">Deployment</h3>
<p>Select the trained model on <code>[Deploy to device]</code>, select the Grove Vision AI V2:</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="./images/png/img_class-model-deploy.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:80.0%"></p>
</figure>
</div>
<p>The Studio will redirect us to the <code>Vision Workplace</code> tab. Confirm the deployment, select the appropriate Port, and connect it:</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="./images/png/deploy-2.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:80.0%"></p>
</figure>
</div>
<p>The model will be flashed into the device. After an automatic reset, the model will start running on the device. On the Device Logger, we can see that the inference has a <strong>latency of approximately 8 ms</strong>, corresponding to a <strong>frame rate of 125 frames per second (FPS)</strong>.</p>
<p>Also, note that it is possible to adjust the model’s confidence.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="./images/png/inf-result-deploy.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:80.0%"></p>
</figure>
</div>
<blockquote class="blockquote">
<p>To run the Image Classification Model, the Grove Vision AI V2 had a peak current of 80mA at 5.24V, resulting in a <strong>power consumption of 420mW</strong>.</p>
</blockquote>
</section>
<section id="sec-setup-nocode-applications-saving-model-2f46" class="level3">
<h3 class="anchored" data-anchor-id="sec-setup-nocode-applications-saving-model-2f46">Saving the Model</h3>
<p>It is possible to save the model in the SenseCraft AI Studio. The Studio will keep all our models, which can be deployed later. For that, return to the <code>Training</code> tab and select the button <code>[Save to SenseCraft</code>]:</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="./images/png/saving-model.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:80.0%"></p>
</figure>
</div>
</section>
</section>
<section id="sec-setup-nocode-applications-summary-d118" class="level2">
<h2 class="anchored" data-anchor-id="sec-setup-nocode-applications-summary-d118">Summary</h2>
<p>In this lab, we explored several computer vision (CV) applications using the <a href="https://wiki.seeedstudio.com/grove_vision_ai_v2/">Seeed Studio Grove Vision AI Module V2</a>, demonstrating its exceptional capabilities as a powerful yet compact device specifically designed for embedded machine learning applications.</p>
<p><strong>Performance Excellence</strong>: The Grove Vision AI V2 demonstrated remarkable performance across multiple computer vision tasks. With its <strong>Himax WiseEye2 chip</strong> featuring a <strong>dual-core Arm Cortex-M55 and integrated ARM Ethos-U55 neural network unit</strong>, the device delivered:</p>
<ul>
<li><strong>Image Classification</strong>: <strong>15 ms</strong> inference time (67 FPS)</li>
<li><strong>Object Detection (Person)</strong>: <strong>48 ms to 76 ms</strong> inference time (21 FPS to 13 FPS)</li>
<li><strong>Pose Detection</strong>: <strong>121 ms</strong> real-time keypoint detection with 17-joint tracking (8 FPS)</li>
</ul>
<p><strong>Power Efficiency Leadership</strong>: One of the most compelling advantages of the Grove Vision AI V2 is its superior power efficiency. Comparative testing revealed significant improvements over traditional embedded platforms:</p>
<ul>
<li><strong>Grove Vision AI V2</strong>: 80 mA (<strong>410 mW</strong>) peak consumption (60+ FPS)</li>
<li><strong>XIAO ESP32S3</strong>: Performing similar CV tasks (Image Classification) <strong>523 mW</strong> (3+ FPS)</li>
</ul>
<p><strong>Practical Implementation</strong>: The device’s versatility was demonstrated through a comprehensive end-to-end project, encompassing dataset creation, model training, deployment, and offline inference.</p>
<p><strong>Developer-Friendly Ecosystem</strong>: The SenseCraft AI Studio, with its no-code deployment and integration capabilities for custom applications, makes the Grove Vision AI V2 accessible to both beginners and advanced developers. The extensive library of pre-trained models and support for custom model deployment provide flexibility for diverse applications.</p>
<p>The Grove Vision AI V2 represents a significant advancement in edge AI hardware, offering professional-grade computer vision capabilities in a compact, energy-efficient package that democratizes AI deployment for embedded applications across industrial, IoT, and educational domains.</p>
<p><strong>Key Takeaways</strong></p>
<p>This Lab demonstrates that sophisticated computer vision applications are not limited to cloud-based solutions or power-hungry hardware, as the Raspberry Pi or Jetson Nanos – they can now be deployed effectively at the edge with remarkable efficiency and performance.</p>
<p>Optionally, we can have the <a href="https://www.seeedstudio.com/XIAO-Vision-AI-Camera-p-6450.html">XIAO Vision AI Camera</a>. This innovative vision solution seamlessly combines the Grove Vision AI V2 module, XIAO ESP32-C3 controller, and an OV5647 camera, all housed in a custom 3D-printed enclosure:</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="./images/png/xiao-vision-ai-cam-2.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:80.0%"></p>
</figure>
</div>
</section>
<section id="sec-setup-nocode-applications-resources-85aa" class="level2">
<h2 class="anchored" data-anchor-id="sec-setup-nocode-applications-resources-85aa">Resources</h2>
<p><a href="https://wiki.seeedstudio.com/sensecraft_ai_pretrained_models_for_grove_visionai_v2/">SenseCraft AI Studio Instructions</a>.</p>
<p><a href="https://seeed-studio.github.io/SenseCraft-Web-Toolkit/#/setup/process">SenseCraft-Web-Toolkit website.</a></p>
<p><a href="https://sensecraft.seeed.cc/ai/model">SenseCraft AI Studio</a></p>
<p><a href="https://github.com/HimaxWiseEyePlus/Seeed_Grove_Vision_AI_Module_V2/releases/download/v1.1/Himax_AI_web_toolkit.zip">Himax AI Web Toolkit</a></p>
<p><a href="https://github.com/Seeed-Studio/wiki-documents/blob/docusaurus-version/docs/Sensor/Grove/Grove_Sensors/AI-powered/Grove-vision-ai-v2/Development/grove-vision-ai-v2-himax-sdk.md">Himax examples</a></p>


</section>
</section>

<a onclick="window.scrollTo(0, 0); return false;" role="button" id="quarto-back-to-top"><i class="bi bi-arrow-up"></i> Back to top</a></main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    // Ensure there is a toggle, if there isn't float one in the top right
    if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
      const a = window.document.createElement('a');
      a.classList.add('top-right');
      a.classList.add('quarto-color-scheme-toggle');
      a.href = "";
      a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
      const i = window.document.createElement("i");
      i.classList.add('bi');
      a.appendChild(i);
      window.document.body.appendChild(a);
    }
    setColorSchemeToggle(hasAlternateSentinel())
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
      const outerScaffold = trigger.parentElement.cloneNode(true);
      const codeEl = outerScaffold.querySelector('code');
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp('/' + window.location.host + '/');
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
            // target, if specified
            link.setAttribute("target", "_blank");
            if (link.getAttribute("rel") === null) {
              link.setAttribute("rel", "noopener");
            }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="../../../../contents/seeed/grove_vision_ai_v2/grove_vision_ai_v2.html" class="pagination-link" aria-label="Overview">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text">Overview</span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="../../../../contents/seeed/grove_vision_ai_v2/image_classification/image_classification.html" class="pagination-link" aria-label="Image Classification">
        <span class="nav-page-text">Image Classification</span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
<p>© 2024-2025 Harvard University. Licensed under <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC-BY-NC-SA 4.0</a></p>
</div>   
    <div class="nav-footer-center">
<p>Part of the <a href="../book/">Machine Learning Systems</a> textbook</p>
</div>
    <div class="nav-footer-right">
      <ul class="footer-items list-unstyled">
    <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/harvard-edge/cs249r_book">
      <i class="bi bi-github" role="img" aria-label="View source on GitHub">
</i> 
    </a>
  </li>  
    <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/harvard-edge/cs249r_book">
      <i class="bi bi-star" role="img" aria-label="Star this repository">
</i> 
    </a>
  </li>  
</ul>
    </div>
  </div>
</footer>
<script src="assets/scripts/subscribe-modal.js" defer=""></script>




</body></html>