# Getting Started {.unnumbered}

This guide walks you through selecting hardware, configuring your development environment, and running your first embedded ML application. Most students complete setup in under an hour.

## Step 1: Select Your Platform

Your choice depends on budget, learning objectives, and the types of applications you want to build.

**For beginners or budget-conscious learners:**

| Platform | Cost | Why Choose It |
|----------|------|---------------|
| [Grove Vision AI V2](seeed/grove_vision_ai_v2/grove_vision_ai_v2.qmd) | ~$25 | No-code interface, fastest path to running models |
| [XIAOML Kit](seeed/xiao_esp32s3/xiao_esp32s3.qmd) | ~$40 | Best value, supports vision, audio, and motion |

**For advanced applications:**

| Platform | Cost | Why Choose It |
|----------|------|---------------|
| [Raspberry Pi](raspi/raspi.qmd) | ~$60-80 | Full Linux environment, LLMs and VLMs |
| [Nicla Vision](arduino/nicla_vision/nicla_vision.qmd) | ~$95 | Professional-grade, ultra-low power design |

For detailed specifications and technical comparisons, see [Platforms](platforms.qmd).

## Step 2: Set Up Your Environment

Development environment configuration is platform-dependent but follows a common pattern: install software tools, configure communication with hardware, and verify the setup works.

**Time estimate:** 30-60 minutes depending on platform and internet speed.

Follow the [IDE Setup Guide](ide-setup.qmd) for complete procedures covering:

- System requirements for your development computer
- Arduino IDE installation for microcontroller platforms
- Python environment configuration for Raspberry Pi
- SenseCraft AI web interface for Grove Vision AI V2
- Serial communication and hardware verification

## Step 3: Choose Your First Lab

Each platform supports different exercise categories. Select labs that match both your hardware and learning goals.

| Lab Category | Grove Vision | XIAOML Kit | Nicla | Raspberry Pi |
|--------------|:------------:|:----------:|:-----:|:------------:|
| Image Classification | ✓ | ✓ | ✓ | ✓ |
| Object Detection | ✓ | ✓ | ✓ | ✓ |
| Keyword Spotting | | ✓ | ✓ | |
| Motion Classification | | ✓ | ✓ | |
| Large Language Models | | | | ✓ |
| Vision Language Models | | | | ✓ |

: Exercise availability by platform {.striped .hover}

## Step 4: Start Your First Lab

**Grove Vision AI V2:** Begin with [Setup and No-Code Apps](seeed/grove_vision_ai_v2/setup_and_no_code_apps/setup_and_no_code_apps.qmd). You'll deploy a pre-trained model in minutes using the visual interface.

**XIAOML Kit:** Start with [Setup](seeed/xiao_esp32s3/setup/setup.qmd), then proceed to [Image Classification](seeed/xiao_esp32s3/image_classification/image_classification.qmd) to train and deploy your first custom model.

**Nicla Vision:** Complete [Setup](arduino/nicla_vision/setup/setup.qmd) to configure your board, then try [Image Classification](arduino/nicla_vision/image_classification/image_classification.qmd).

**Raspberry Pi:** Follow [Setup](raspi/setup/setup.qmd), then choose your path:
- [Image Classification](raspi/image_classification/image_classification.qmd) for computer vision fundamentals
- [LLM Deployment](raspi/llm/llm.qmd) to run language models on edge hardware

## Prerequisites

These labs assume:

- **Programming:** Proficiency in Python. Familiarity with C/C++ is helpful for microcontroller platforms but not required.
- **Mathematics:** Working knowledge of linear algebra and basic probability at the undergraduate level.
- **Hardware:** No prior embedded systems experience. Each lab includes complete setup and troubleshooting procedures.

## Connection to ML Systems Textbook

These laboratories complement specific chapters in the ML Systems textbook:

- **Image Classification labs** reinforce concepts from the Computer Vision and Model Optimization chapters
- **Keyword Spotting labs** connect to Audio Processing and Real-time Inference
- **Motion Classification labs** demonstrate Sensor Fusion and Time-series Analysis
- **LLM/VLM labs** extend Large Model Deployment to resource-constrained environments

Each lab identifies relevant textbook sections for deeper theoretical understanding.
