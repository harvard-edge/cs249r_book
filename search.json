[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Embedded AI: Principles, Algorithms, and Applications",
    "section": "",
    "text": "Preface\nIn “Embedded AI: Principles, Algorithms, and Applications”, we will embark on a critical exploration of the rapidly evolving field of artificial intelligence in the context of embedded systems, originally nurtured from the foundational course, tinyML from CS249r.\nThe goal of this book is to bring about a collaborative endeavor with insights and contributions from students, practitioners and the wider community, blossoming into a comprehensive guide that delves into the principles governing embedded AI and its myriad applications.\n\n“If you want to go fast, go alone, if you want to go far, go together.” – African Proverb\n\nAs a living document, this open-source textbook aims to bridge gaps and foster innovation by being globally accessible and continually updated, addressing the pressing need for a centralized resource in this dynamic field. With a rich tapestry of knowledge woven from various expert perspectives, readers can anticipate a guided journey that unveils the intricate dance between cutting-edge algorithms and the principles that ground them, paving the way for the next wave of technological transformation.\n\n\nThe Philosophy Behind the Book\nWe live in a world where technology perpetually reshapes itself, fostering an ecosystem of open collaboration and knowledge sharing stands as the cornerstone of innovation. This philosophy fuels the creation of “Embedded AI: Principles, Algorithms, and Applications.” This is a venture that transcends conventional textbook paradigms to foster a living repository of knowledge. Anchoring its content on principles, algorithms, and applications, the book aims to cultivate a deep-rooted understanding that empowers individuals to navigate the fluid landscape of embedded AI with agility and foresight. By embracing an open approach, we not only democratize learning but also pave avenues for fresh perspectives and iterative enhancements, thus fostering a community where knowledge is not confined but is nurtured to grow, adapt, and illuminate the path of progress in embedded AI technologies globally.\n\n\nPrerequisites\nVenturing into “Embedded AI: Principles, Algorithms, and Applications” does not mandate you to be a maestro in machine learning from the outset. At its core, this resource seeks to nurture learners who bear a fundamental understanding of systems and harbor a curiosity to explore the confluence of disparate, yet interconnected domains: embedded hardware, artificial intelligence, and software. This confluence forms a vibrant nexus where innovations and new knowledge streams emerge, making a basic grounding in system operations a pivotal tool in navigating this dynamic space.\nMoreover, the goal of this book is to delve into the synergies created at the intersection of these fields, fostering a learning environment where the boundaries of traditional disciplines blur to give way to a holistic, integrative approach to modern technological innovations. Your interest in unraveling embedded AI technologies and low-level software mechanics would be guiding you through a rich learning experience.\n\n\nConventions Used in This Book\nPlease follow the conventions listed in Conventions\n\n\nHow to Contact Us\nPlease contact vj@eecs.harvard.edu\n\n\nHow to Contribute\nPlease see instructions at here.\n\n\nContributors\nPlease see Credits."
  },
  {
    "objectID": "dedication.html",
    "href": "dedication.html",
    "title": "Dedication",
    "section": "",
    "text": "This book is a testament to the idea that, in the vast expanse of technology and innovation, it’s not always the largest systems, but the smallest ones, that can change the world."
  },
  {
    "objectID": "acknowledgements.html",
    "href": "acknowledgements.html",
    "title": "Acknowledgements",
    "section": "",
    "text": "To every endeavor, there lies a tapestry of effort, woven with threads of inspiration, dedication, and collaboration. This book, born out of a collective spirit, is no exception.\nFirst and foremost, gratitude must be extended to our ever-expanding community on GitHub. Each contributor, whether through a paragraph, a sentence, or a mere punctuation correction, has imbued this work with a wealth of knowledge and perspective. To each of you, your gift of time and expertise has not gone unnoticed or unappreciated. This book is as much yours as it is any single individual’s.\nSpecial thanks to Professor Vijay Janapa Reddi, whose vision planted the seed for this collaboration. Your unwavering faith in the power of open-source communities and your dedication to guiding this project to fruition have been the guiding star throughout.\nWe are also deeply indebted to the developers and staff at GitHub. Your platform has redefined what is possible in the world of collaboration, allowing disparate voices from across the globe to unite in a harmonious undertaking. This book stands as a testament to what can be achieved when barriers to entry are lowered, and voices are amplified.\nTo every reader who embarks on this journey with us, we hope this work enriches your understanding and inspires your curiosity. Without readers, words would hold no weight. We wrote for you, with the belief that shared knowledge is the keystone to progress.\nLastly, but by no means least, our gratitude extends to friends, families, mentors, and everyone who offered words of encouragement, late-night discussions, and unwavering support as this book transitioned from idea to reality.\nMay this collaborative effort serve as a beacon for what is possible when hearts and minds come together in the name of knowledge and progress."
  },
  {
    "objectID": "copyright.html",
    "href": "copyright.html",
    "title": "Copyright",
    "section": "",
    "text": "This book is open-source and developed collaboratively through GitHub. Unless otherwise stated, this work is licensed under the Creative Commons Attribution-ShareAlike 4.0 International License (CC BY-SA 4.0). You can find the full text of the license here.\nContributors to this project have dedicated their contributions to the public domain or under the same open license as the original project. While the contributions are collaborative, each contributor retains copyright in their respective contributions.\nFor details on authorship, contributions, and how to contribute, please see the project repository on GitHub.\nAll trademarks and registered trademarks mentioned in this book are the property of their respective owners.\nThe information provided in this book is believed to be accurate and reliable. However, the authors, editors, and publishers cannot be held liable for any damages caused or alleged to be caused either directly or indirectly by the information contained in this book."
  },
  {
    "objectID": "about.html#overview",
    "href": "about.html#overview",
    "title": "About This Book",
    "section": "Overview",
    "text": "Overview\nThis book is a collaborative effort started by the CS249r Tiny Machine Learning class at Harvard University. We intend for this book to become a community-driven effort to help educators and learners get started with TinyML. This living document will be continually updated as we continue to learn more about TinyML and how to teach it."
  },
  {
    "objectID": "about.html#topics-covered",
    "href": "about.html#topics-covered",
    "title": "About This Book",
    "section": "Topics Covered",
    "text": "Topics Covered\nThe book covers a wide range of topics related to embedded machine learning, providing a comprehensive understanding of the field. The topics covered include:\n\nOverview and Introduction to Embedded Machine Learning\nData Engineering\nEmbedded Machine Learning Frameworks\nEfficient Model Representation and Compression\nPerformance Metrics and Benchmarking of ML Systems\nLearning on the Edge\nHardware Acceleration for Edge ML: GPUs, TPUs, and FPGAs\nEmbedded MLOps\nSecure and Privacy-Preserving On-Device ML\nResponsible AI\nSustainability at the Edge\nGenerative AI at the Edge\n\nBy the end of this book, you will gain a brief introduction to machine learning and IoT. You will learn about real-world deployments of embedded machine learning systems. We hope you will also gain practical experience through hands-on project assignments."
  },
  {
    "objectID": "about.html#intended-audience",
    "href": "about.html#intended-audience",
    "title": "About This Book",
    "section": "Intended Audience",
    "text": "Intended Audience\nThis book is designed specifically for newcomers who wish to explore the fascinating and nascent world of tiny machine learning (tinyML). It provides the basic underpinnings of ML and embedded systems, and moves into more complex and broader topics relevant to both the tinyML and broader research community. More specifically, we believe the book will confer the following benefits to these groups of people:\n\nEmbedded Systems Engineers: This book is a valuable resource for engineers working in the field of embedded systems. It provides a solid foundation in TinyML, allowing them to design and implement intelligent applications on microcontrollers and other embedded platforms with limited resources.\nComputer Science and Electrical Engineering Students: Students pursuing degrees in computer science and electrical engineering can benefit from this book. It offers an introduction to the concepts, algorithms, and techniques used in TinyML, preparing students to tackle real-world challenges in the emerging field of embedded machine learning.\nResearchers and Academics: Researchers and academics in the field of machine learning, computer vision, and signal processing will find this book useful. It offers insights into the unique challenges of deploying machine learning algorithms on low-power, low-memory devices, enabling them to develop new approaches and advance the field of TinyML.\nIndustry Professionals: Professionals working in industries like IoT, robotics, wearable technology, and smart devices will find this book relevant. It equips them with the knowledge required to integrate machine learning capabilities into their products, enabling intelligent and autonomous behavior."
  },
  {
    "objectID": "about.html#key-takeaways",
    "href": "about.html#key-takeaways",
    "title": "About This Book",
    "section": "Key Takeaways",
    "text": "Key Takeaways\nUsers of this book will learn how to train and deploy deep neural network models on resource-constrained microcontrollers and the broader challenges associated with their design, development, deployment, and use.\n\nIntroduction to Machine Learning: A fundamental understanding of machine learning concepts, including supervised, unsupervised, and reinforcement learning.\nTinyML Fundamentals: Exploring the challenges and constraints associated with deploying machine learning on small, low-power devices.\nHardware Platforms: Coverage of popular microcontrollers and development boards specifically designed for TinyML applications, along with their architecture and specifications.\nTraining Models: Techniques and tools for training machine learning models suitable for embedded systems, including considerations for model size, accuracy, and resource utilization.\nOptimization Techniques: Strategies for model compression, quantization, and algorithmic optimization to ensure efficient execution on resource-constrained devices.\nReal-world Applications: Practical use cases and examples demonstrating the deployment of TinyML in various domains, such as industrial automation, healthcare, and environmental monitoring.\nChallenges and Future Trends: Discussion on the current challenges in TinyML, potential solutions, and emerging trends in the field.\n\nBy encompassing these aspects, our aim is to make this book a go-to resource for anyone interested in developing intelligent applications on embedded systems.\nAfter completing the chapters, readers will be empowered with the capabilities to design and implement their own ML-enabled projects, starting from defining a problem to gathering data and training the neural network model and finally deploying it to the device to display inference results or control other hardware appliances based on inference data."
  },
  {
    "objectID": "about.html#prerequisites",
    "href": "about.html#prerequisites",
    "title": "About This Book",
    "section": "Prerequisites",
    "text": "Prerequisites\n\nBasic Programming Knowledge: It is recommended that readers have some prior experience with programming, preferably in Python. Understanding variables, data types, control structures, and basic programming concepts will facilitate comprehension and engagement with the book.\nFamiliarity with Machine Learning Concepts: While not essential, a basic understanding of machine learning concepts, such as supervised and unsupervised learning, will help readers grasp the material more easily. However, the book provides sufficient explanations to bring readers up to speed if they are new to the field.\nPython Programming Skills (Optional): Readers with some Python programming experience will have an advantage when engaging with the coding portions of the book. Familiarity with libraries such as NumPy, scikit-learn, and TensorFlow will greatly facilitate the implementation and experimentation with machine learning models.\nLearning Mindset: The book has been structured to be accessible to a wide audience, including readers with varying levels of technical expertise. It provides a gradual learning curve, allowing readers to start with general knowledge about the field, progress to coding exercises, and potentially advance to deploying models on embedded devices. However, to fully benefit from the book, readers should be willing to challenge themselves and engage in practical exercises and projects.\nAvailability of Resources: To fully explore the practical aspects of TinyML, readers should have access to the necessary resources. These include a computer with Python and relevant libraries installed, as well as optional access to an embedded development board or microcontroller for experimenting with deploying machine learning models.\n\nBy ensuring that these general requirements are met, readers will have the opportunity to broaden their understanding of TinyML, gain hands-on experience with coding exercises, and even venture into practical implementation on embedded devices, enabling them to push the boundaries of their knowledge and skills."
  },
  {
    "objectID": "introduction.html",
    "href": "introduction.html",
    "title": "1  Introduction",
    "section": "",
    "text": "Welcome to our comprehensive guide to Tiny Machine Learning (TinyML), where we endeavor to bring a fresh perspective to the rapidly emerging field that straddles the domains of electrical engineering, computer science, and applied data science. This book aims to close the gap between complex machine learning abstractions and real-world applications on small devices, providing both theory enthusiasts and practitioners an end-to-end understanding of TinyML.\nWe begin with an overall introduction to the field of embedd systems and machine learning. We start by elaborating on the key principles of embedded systems, setting the groundwork for embedded machine learning. Then we pivot our attention to deep learning, focusing specifically on deep learning methods given their representation capacity and overall performance in a variety of tasks, especially when applied to small devices.\nThe book goes on to discuss step-by-step workflows in machine learning, data engineering, pre-processing, and advanced model training techniques. It provides comprehensive analyses of several in-use machine learning frameworks, and how they can be employed effectively to develop efficient AI models.\nIn a world where efficiency is key, we also discuss TinyML model optimization and deployment strategies. Special focus is given to on-device learning. How do we train a machine learning model on a tiny device while achieving admirable efficiency? What are the current hardware acceleration techniques? And how can we manage the lifecycle of these models? The reader can expect exhaustive answers to these and many more questions in our dedicated chapters.\nImportantly, we adopt a forward-looking stance, discussing the sustainability and ecological footprint of AI. We explore the location of TinyML within such debates, and how TinyML may contribute to more sustainable and responsible practices.\nFinally, the book ends with a speculative leap into the world of generative AI, outlining its potentials in the TinyML context.\nWhether you are an absolute beginner, a professional in the field, or an academic pursuing rigorous research, this book aims to offer a seamless blend of essential theory and practical insight, triggering stimulating conversations around TinyML. Let’s embark on this thrilling journey to explore the incredible world of TinyML!"
  },
  {
    "objectID": "embedded_sys.html#introduction-to-embedded-systems",
    "href": "embedded_sys.html#introduction-to-embedded-systems",
    "title": "2  Embedded Systems",
    "section": "2.1 Introduction to Embedded Systems",
    "text": "2.1 Introduction to Embedded Systems\n\n2.1.1 Definition and Characteristics\nEmbedded systems are specialized computing systems that do not look like computers. They are dedicated to specific tasks and “embed” as part of a larger device. Unlike general-purpose computers that can run a wide variety of applications, embedded systems perform pre-defined tasks, often with very specific requirements. Since they are task-specific, their design ensures optimized performance and reliability. The characteristics that define these systems are as follows:\n\nDedicated Functionality: They are designed to execute a specific function or a set of closely related functions. This focus on specific tasks allows them to be optimized, offering faster performance and reliability.\nReal-Time Operation: Many embedded systems operate in real-time, which means they are required to respond to inputs or changes in the environment immediately or within a predetermined time frame.\nIntegration with Physical Hardware: Embedded systems are closely integrated with physical hardware, making them more mechanically inclined compared to general-purpose computing systems.\nLong Lifecycle: These systems typically have a long lifecycle and can continue to function for many years after their initial deployment.\nResource Constraints: Embedded systems are often resource-constrained, operating with limited computational power and memory. This necessitates the development of efficient algorithms and software.\n\n\n\n2.1.2 Historical Background\nEmbedded systems have a rich history, with their roots tracing back to the 1960s when the first microprocessor, the Figure 2.1, made its debut. This paved the way for the creation of the first embedded system which was used in the Apollo Guidance Computer, the primary navigation system of the Apollo spacecraft. Over the years, the field has evolved dramatically, finding applications in various domains including automotive electronics, consumer electronics, telecommunications, and healthcare, among others.\n\n\n\nFigure 2.1: Intel 4004\n\n\n\n\n2.1.3 Importance in tinyML\nIn the context of tinyML, embedded systems represent a significant frontier. The incorporation of machine learning models directly onto these systems facilitates intelligent decision-making at the edge, reducing latency and enhancing security. Here are several reasons why embedded systems are critical in the tinyML landscape:\n\nEdge Computing: By bringing computation closer to the data source, embedded systems enhance efficiency and reduce the necessity for constant communication with centralized data centers.\nLow Power Consumption: Embedded systems in tinyML are designed to consume minimal power, a critical requirement for battery-operated devices and IoT applications.\nReal-Time Analysis and Decision Making: Embedded systems can facilitate real-time data analysis, allowing for immediate decision-making based on the insights generated.\nSecurity and Privacy: Processing data locally on embedded systems ensures better security and privacy, as it reduces the chances of data interception during transmission.\nCost-Effective: Implementing ML models on embedded systems can be cost-effective, especially in scenarios where data transmission and storage in cloud servers might incur significant costs.\n\nAs we venture deeper into this chapter, we will unveil the intricacies that govern the functioning of embedded systems and explore how they form the bedrock upon which tinyML stands, promising a future of integrated, intelligent, and efficient devices and systems."
  },
  {
    "objectID": "embedded_sys.html#architecture-of-embedded-systems",
    "href": "embedded_sys.html#architecture-of-embedded-systems",
    "title": "2  Embedded Systems",
    "section": "2.2 Architecture of Embedded Systems",
    "text": "2.2 Architecture of Embedded Systems\nThe architecture of embedded systems forms the blueprint that delineates the structure and functioning of these specialized systems. It provides insights into how different components within an embedded system interact and collaborate to achieve specific functionalities. This section dissects the integral components of the architecture - microcontrollers, microprocessors, different memory types and their management, and the intricacies of System on Chip (SoC).\n\n2.2.1 Microcontrollers vs Microprocessors\nUnderstanding the difference between microcontrollers and microprocessors is pivotal to grasping the fundamentals of embedded system architecture. Here, we delve into the characteristics of both:\n\nMicrocontrollers\nMicrocontrollers are compact, integrated circuits designed to govern specific operations in an embedded system. They house a processor, memory, and input/output peripherals in a single unit as shown in Figure 2.2, facilitating simplicity and ease of operation. Microcontrollers are typically used in products where the computational requirements are not highly demanding, and cost-effectiveness is a priority.\nCharacteristics:\n\nSingle-chip solution\nOn-chip memory and peripherals\nLow power consumption\nIdeal for control-oriented applications\n\n\n\n\n\nFigure 2.2: Microcontrollers\n\n\n\nMicroprocessors\nOn the other hand, microprocessors are more complex, forming the central processing unit within a system, lacking the integrated memory and I/O peripherals found in microcontrollers. They are usually found in systems that demand higher computational power and flexibility. These are used in devices where substantial processing power is required, and the tasks are more data-intensive.\nCharacteristics:\n\nRequires external components such as memory and I/O peripherals\nHigher processing power compared to microcontrollers\nMore flexible in terms of connectivity with various components\nIdeal for data-intensive applications\n\n\n\n\n2.2.2 Memory Types and Management\nEmbedded systems leverage various types of memory, each serving distinct purposes. Effective memory management is crucial to optimize performance and resource utilization. Below we discuss different memory types and how they are managed in an embedded system environment:\n\nROM (Read-Only Memory): This is non-volatile memory where data is written during manufacturing and remains unchanged throughout the device’s life. It stores the firmware and boot-up instructions.\nRAM (Random Access Memory): A volatile memory used to store temporary data generated during the system’s operation. It is faster and allows read-write operations, but data is lost once power is turned off.\nFlash Memory: A non-volatile memory type that can be electrically erased and reprogrammed. It finds applications in storing firmware or data that needs to persist between reboots.\n\nMemory Management:\n\nStatic Memory Allocation: Memory is allocated before runtime, and the allocation does not change during the system’s operation.\nDynamic Memory Allocation: Memory is allocated at runtime, allowing flexibility but at the cost of increased complexity and potential memory leaks.\n\n\n\n2.2.3 System on Chip (SoC)\nMost embedded systems are SoCs. A System on Chip (SoC) represents an advanced integration technology where most of the components required to build a complete system are integrated onto a single chip. It usually contains a microprocessor or microcontroller, memory blocks, peripheral interfaces, and other components required for a fully functioning system. Here’s a deeper look at its characteristics and applications:\n\nIntegration of Multiple Components: SoCs house multiple components, including CPUs, memory, and peripherals, in a single chip, promoting higher integration levels and minimizing the need for external components.\nPower Efficiency: Due to the high level of integration, SoCs are often more power-efficient compared to systems built using separate chips.\nCost-Effectiveness: The integration leads to reduced manufacturing costs, as fewer separate components are required.\nApplications: SoCs find applications in a variety of domains, including mobile computing, automotive electronics, and IoT devices, where compact size and power efficiency are prized.\n\nHere are some examples of widely used SoCs that you may recognize given that they have found substantial applications across various domains:\n\nQualcomm Snapdragon: Predominantly found in smartphones and tablets, they offer a combination of processing power, graphics, and connectivity solutions.\nApple A-series: Custom SoCs developed by Apple, utilized in their range of iPhones, iPads, and even in some versions of Apple TV and HomePod. Notable examples include the A14 Bionic and A15 Bionic chips.\nSamsung Exynos: Developed by Samsung, these are utilized extensively in their range of smartphones, tablets, and other electronic devices.\nNVIDIA Tegra: Initially designed for mobile devices, they have found substantial applications in automotive and gaming consoles, like the Nintendo Switch. You can see a picture of it below in Figure 2.3.\nIntel Atom: These are used in a wide variety of systems including netbooks, smartphones, and even embedded systems owing to their power efficiency.\nMediaTek Helio: Popular in budget to mid-range smartphones, these chips offer a good balance of power efficiency and performance.\nBroadcom SoCs: Used extensively in networking equipment, Broadcom offers a range of SoCs with different functionalities including those optimized for wireless communications and data processing.\nTexas Instruments (TI) OMAP: These were popular in smartphones and tablets, offering a range of functionalities including multimedia processing and connectivity.\nXilinx Zynq: Predominantly used in embedded systems for industrial automation and for applications demanding high levels of data processing, such as advanced driver-assistance systems (ADAS).\nAltera SoC FPGA: Now under Intel, these SoCs integrate FPGA technology with ARM cores, offering flexibility and performance for various applications including automotive and industrial systems.\n\n\n\n\nFigure 2.3: NVIDIA’s Tegra 2 combines two ARM Cortex-A9 cores with an ARM7 for SoC management tasks.\n\n\nEach of these SoCs presents a unique set of features and capabilities, catering to the diverse needs of the rapidly evolving technology landscape. They integrate multiple components into a single chip, offering power efficiency, cost-effectiveness, and compact solutions for modern electronic devices."
  },
  {
    "objectID": "embedded_sys.html#embedded-systems-programming",
    "href": "embedded_sys.html#embedded-systems-programming",
    "title": "2  Embedded Systems",
    "section": "2.3 Embedded Systems Programming",
    "text": "2.3 Embedded Systems Programming\nEmbedded systems programming diverges considerably from traditional software development, specifically honed to navigate the limited resources and the real-time requirements frequently associated with embedded hardware. This section will illuminate the nuances of the different programming languages utilized, delve into the intricacies of firmware development, and explore the critical role of Real-time Operating Systems (RTOS) in this specialized field.\n\n2.3.1 Programming Languages: C, C++, Python, etc.\nThe selection of appropriate programming languages is crucial in embedded systems, often prioritizing direct hardware interaction and optimization of memory usage. Let us explore the specifics of these languages and how they stand apart from those typically utilized in more conventional systems:\n\nC: Traditionally the cornerstone of embedded systems programming, the C language facilitates direct interaction with hardware components, offering capabilities for bit-wise operations and manipulating memory addresses. Its procedural approach and low-level capabilities make it the favored choice for constrained environments, focusing on firmware development.\nC++: Building on the foundation laid by C, C++ integrates object-oriented principles, fostering organized and modular code development. Despite its inherent complexity, it is embraced in scenarios where higher-level abstractions do not compromise the granular control provided by C.\nPython: While not a classic choice for embedded systems due to its relative memory consumption and runtime delays, Python is finding its place in the embedded domain, especially in systems where resource constraints are less stringent. In recent times, a variant known as MicroPython has emerged, specifically tailored for microcontrollers. MicroPython retains the simplicity and ease of use of Python while being optimized for embedded environments, offering a flexible programming paradigm that facilitates rapid prototyping and development. For instance, the code snipped below shows how we can use MicroPython to interface with the pins on a PyBoard.\n\nimport pyb # Package from PyBoard\n\n# turn on an LED\npyb.LED(1).on()\n\n# print some text to the serial console\nprint('Hello MicroPython!')\nComparison with Traditional Systems: In stark contrast to conventional systems, where languages like Java, Python, or JavaScript are celebrated for their development ease and comprehensive libraries, embedded systems are geared towards languages that offer refined control over hardware components and potential optimization opportunities, carefully navigating the limited resources at their disposal.\n\n\n2.3.2 Firmware Development\nThe realm of firmware development within embedded systems encompasses crafting programs permanently stored in the non-volatile memory of hardware, thus ensuring persistent operation. Here, we delineate how it distinguishes itself from software development for traditional systems:\n\nResource Optimization: The necessity for constant optimization is paramount, enabling the code to function within the confines of restricted memory and processing capacities.\nHardware Interaction: Firmware typically exhibits a close-knit relationship with hardware, necessitating a profound comprehension of the hardware components and their functionalities.\nLifecycle Management: Firmware updates are less frequent compared to software updates in traditional systems, mandating rigorous testing procedures to avert failures that could culminate in hardware malfunctions.\nSecurity Concerns: Given its integral role, firmware is a potential target for security breaches, warranting meticulous scrutiny towards security elements, including secure coding practices and encryption protocols.\n\n\n\n2.3.3 Real-time Operating Systems (RTOS)\nRTOS serve as the backbone for real-time systems, orchestrating task execution in a predictable, deterministic manner. This is a sharp deviation from the operating systems in mainstream computing environments, as delineated below:\n\nDeterministic Timing: RTOS are structured to respond to inputs or events within a well-defined timeframe, meeting the critical time-sensitive requisites of many embedded systems.\nTask Prioritization: They facilitate task prioritization, where critical tasks are accorded precedence in processing time allocation over less vital tasks.\nMicrokernel Architecture: A substantial number of RTOS leverage a microkernel architecture, epitomizing minimalism and efficiency by focusing only on the necessary functionalities to facilitate their operations.\nMemory Management: Memory management in RTOS is often more streamlined compared to their counterparts in traditional operating systems, aiding in swift response times and operational efficacy.\n\nExamples of RTOS: Notable examples in this category include FreeRTOS, RTEMS, and VxWorks, each offering unique features tailored to meet the diverse requirements of various embedded systems applications."
  },
  {
    "objectID": "embedded_sys.html#interfaces-and-peripherals",
    "href": "embedded_sys.html#interfaces-and-peripherals",
    "title": "2  Embedded Systems",
    "section": "2.4 Interfaces and Peripherals",
    "text": "2.4 Interfaces and Peripherals\nEmbedded systems interact with the external world through various interfaces and peripherals, which are distinctly streamlined and specialized compared to general-purpose systems. Let’s delve into the specifics:\n\n2.4.1 Digital I/O\nDigital Input/Output (I/O) interfaces are foundational in embedded systems, allowing them to interact with other devices and components. For example, a digital I/O pin can be used to read a binary signal (0 or 1) from sensors or to control actuators.\nIn embedded systems, these I/O ports often need to function under strict timing constraints, something which is less prevalent in general-purpose computing systems. Furthermore, these systems are usually programmed to perform specific, optimized operations on digital signals, sometimes needing to work in real time or near-real-time environments.\n\n\n2.4.2 Analog Interfaces\nAnalog interfaces in embedded systems are crucial for interacting with a world that largely communicates in analog signals. These interfaces can include components like Analog-to-Digital Converters (ADCs) and Digital-to-Analog Converters (DACs). ADCs, for instance, can be used to read sensor data from environmental sensors like temperature or humidity sensors, translating real-world analog data into a digital format that can be processed by the microcontroller.\nCompared to general-purpose systems, embedded systems might employ analog interfaces in a more direct and frequent manner, especially in applications involving sensor integrations, which necessitate the conversion of a wide variety of analog signals to digital data for processing and analysis.\nIf you look closely enough in Figure 2.4, you will see there are indications of I/O pinouts for analog, digital, as well as communication layouts.\n\n\n\nFigure 2.4: Nicla Vision pinout\n\n\n\n\n2.4.3 Communication Protocols (SPI, I2C, UART, etc.)\nCommunication protocols serve as the conduits for facilitating communication between various components within or connected to an embedded system. Let’s explore a few widely adopted ones:\n\nSPI (Serial Peripheral Interface): This is a synchronous serial communication protocol, which is used for short-distance communication primarily in embedded systems. For example, it is often utilized in SD card and TFT display communications.\nI2C (Inter-Integrated Circuit): This is a multi-master, multi-slave, packet switched, single-ended, serial communication bus, which is used widely in embedded systems to connect low-speed peripherals to a motherboard, embedded system, or cellphone. It’s known for its simplicity and low pin count.\nUART (Universal Asynchronous Receiver-Transmitter): This communication protocol allows for asynchronous serial communication between devices. It’s widely used in embedded systems to transmit data between devices over a serial port, facilitating the transmission of data logs from a sensor node to a computer, for instance.\n\nCompared to general-purpose systems, communication protocols in embedded systems are often more optimized for speed and reliability, as they may be used in critical applications where data transmission integrity is paramount. Moreover, they might be directly integrated into the microcontroller, emphasizing a more harmonized and seamless interaction between components, which is typically not observed in general-purpose systems."
  },
  {
    "objectID": "embedded_sys.html#power-management-in-embedded-systems",
    "href": "embedded_sys.html#power-management-in-embedded-systems",
    "title": "2  Embedded Systems",
    "section": "2.5 Power Management in Embedded Systems",
    "text": "2.5 Power Management in Embedded Systems\nWhen engineering embedded systems, power management emerges as a pivotal focus area, shaping not only the system’s efficiency but also its viability in real-world applications. The sheer diversity in the deployment of embedded systems, ranging from handheld devices to industrial machinery, underscores the imperative to optimize power management meticulously. Let’s delve into this critical facet of embedded systems:\n\n2.5.1 Power Consumption Considerations\nIn embedded systems, power consumption is a vital parameter that governs the system’s performance and lifespan. Typically, microcontrollers in these systems operate in the range of 1.8V to 5V, with current consumption being in the microampere (μA) to milliampere (mA) range during active modes. In sleep or standby modes, the current consumption can plummet to nanoamperes (nA), ensuring battery longevity.\nComparatively, general-purpose computing systems, like personal computers, consume power in the order of tens to hundreds of watts, which is several orders of magnitude higher. This stark contrast delineates the necessity for meticulous power management in embedded systems, where the available power budget is often significantly restrained.\nThe intricacies of managing power consumption hinge on a variety of factors including the operating voltage, clock frequency, and the specific tasks being performed by the system. Often, engineers find themselves navigating a complex trade-off landscape, balancing power consumption against system performance and responsiveness.\n\n\n2.5.2 Energy-Efficient Design\nEmbedding energy efficiency into the design phase is integral to the successful deployment of embedded systems. Engineers often employ techniques such as dynamic voltage and frequency scaling (DVFS), which allows the system to adjust the voltage and frequency dynamically based on the processing requirements, thereby optimizing power consumption.\nAdditionally, leveraging low-power modes where non-essential peripherals are turned off or clock frequencies are reduced can significantly conserve power. For instance, utilizing deep sleep modes where the system consumes as little as 100 nA can dramatically enhance battery life, especially in battery-powered embedded systems.\nIn embedded systems, energy-efficient design isn’t confined to just power-saving modes and techniques like Dynamic Voltage and Frequency Scaling (DVFS); it extends fundamentally to the architecture of the microcontroller itself, particularly in its instruction set architecture (ISA).\nThe microcontroller instruction set architecture (ISA) in embedded systems is often highly specialized, stripped of any unnecessary complexities that might add to power consumption. This specialization facilitates executing operations using a smaller number of cycles compared to general-purpose processors, which, in turn, reduces the power consumption per operation. Moreover, these specialized ISAs are crafted to efficiently execute the specific types of tasks that the embedded system is designed to perform, optimizing the execution path and thereby conserving energy.\nFurthermore, it’s not uncommon to find RISC (Reduced Instruction Set Computer) architectures in embedded systems. These architectures utilize a smaller set of simple instructions compared to Complex Instruction Set Computer (CISC) architectures found in traditional general-purpose systems. This design choice significantly reduces the power consumed per instruction, making these microcontrollers inherently more energy-efficient.\nApart from ISAs, embedded microcontrollers are often integrated with peripherals and components that are tailored to exhibit minimal energy expenditure, further reinforcing the emphasis on energy efficiency. Through careful design, engineers can craft systems that harmoniously integrate performance requirements with power management strategies, crafting solutions that stand as testimony to innovation and sustainability in the field of embedded systems. This meticulous approach to design, focusing on both macro and micro-level optimizations, forms the bedrock of energy efficiency in embedded systems, differentiating them from their general-purpose counterparts which are often characterized by higher power consumption and a broader range of functionalities.\nBy focusing on these elements, engineers can forge pathways to create systems that not only fulfill their functional roles but do so with an acumen that reflects a deep understanding of the broader impacts of technology on society and the environment.\n\n\n2.5.3 Battery Management\nBattery management constitutes a vital part of power management strategies in embedded systems. The objective here is to maximize battery life without compromising system performance. Battery-powered embedded systems often employ lithium-ion or lithium-polymer batteries due to their high energy density and rechargeable nature. These batteries usually have a voltage range of 3.7V to 4.2V per cell. For instance, the Nicla Vision uses 3.7V battery as shown in Figure 2.5.\n\n\n\nFigure 2.5: Nicla Vision battery.\n\n\nEngineers need to incorporate strategies like efficient charge management, overvoltage protection, and temperature monitoring to safeguard the battery’s health and prolong its lifespan. Moreover, developing systems that can harvest energy from the environment, like solar or vibrational energy, can supplement battery power and create sustainable, long-lasting solutions.\nThe focus on power management stems from the necessity to optimize resource utilization, extend battery life, and reduce operational costs. In deployments where systems are remote or inaccessible, efficient power management can significantly reduce the need for maintenance interventions, thereby ensuring sustained, uninterrupted operation.\nOne could say that power management in embedded systems is not just a technical requirement but a critical enabler that can dictate the success or failure of a deployment. Engineers invest significantly in optimizing power management strategies to craft systems that are not only efficient but also sustainable, showcasing a deep-seated commitment to innovation and excellence in the embedded systems domain."
  },
  {
    "objectID": "embedded_sys.html#real-time-characteristics",
    "href": "embedded_sys.html#real-time-characteristics",
    "title": "2  Embedded Systems",
    "section": "2.6 Real-Time Characteristics",
    "text": "2.6 Real-Time Characteristics\nIn the intricate fabric of embedded systems, the real-time characteristics stand as defining threads, weaving together components and tasks into a coherent, responsive entity. This facet, which is often unique to embedded systems, holds a critical place in the architecture and operation of these systems, providing them with the agility and precision to interact with their environment in a timely manner. Let’s explore the intricacies that underline the real-time characteristics of embedded systems:\n\n2.6.1 Real-time Clocks\nReal-time clocks (RTCs) play a pivotal role in embedded systems, providing a precise time reference that governs the operations of the system. These clocks often have battery backups to ensure consistent timekeeping even when the main power source is unavailable. The utilization of RTCs is far more prevalent and critical in embedded systems compared to general-purpose computing systems, where timekeeping, although necessary, often doesn’t dictate the system’s core functionality.\nFor instance, in industrial automation systems, RTCs help in coordinating tasks with high precision, ensuring that processes occur in sync and without delay. They find significant applications in systems where time-stamped data logging is necessary, such as in environmental monitoring systems where data accuracy and time correlation are vital.\n\n\n2.6.2 Timing and Synchronization\nTiming and synchronization are hallmarks of embedded systems, where multiple components and processes need to work in harmony. The very essence of a real-time embedded system is dictated by its ability to perform tasks within a defined time frame. These systems usually have stringent timing requirements, demanding synchronization mechanisms that are both robust and precise.\nFor example, in automotive control systems, the timely and synchronized functioning of various sensors and actuators is non-negotiable to ensure safety and optimal performance. This is a stark contrast to general-purpose systems, where timing, although managed, doesn’t often have immediate and critical repercussions.\n\n\n2.6.3 Task Management and Scheduling\nIn embedded systems, task management and scheduling are critical to ensuring that the system can respond to real-time events effectively. Task schedulers in these systems might employ strategies such as priority scheduling, where tasks are assigned priority levels, and higher-priority tasks are allowed to pre-empt lower-priority tasks. This is particularly vital in systems where certain operations have a higher criticality.\nFor instance, in medical devices like pacemakers, the timely delivery of electrical pulses is a critical task, and the scheduling mechanism must prioritize this above all other tasks to ensure the patient’s safety. This finely tuned scheduling and task management is quite unique to embedded systems, distinguishing them markedly from the more flexible and less deterministic scheduling observed in general-purpose systems.\n\n\n2.6.4 Error Handling and Fault Tolerance\nTo further bolster their real-time characteristics, embedded systems often feature mechanisms for error handling and fault tolerance. These are designed to quickly identify and correct errors, or to maintain system operation even in the face of faults. In aviation control systems, for example, real-time fault tolerance is crucial to maintain flight stability and safety in drones. This level of meticulous error handling is somewhat distinctive to embedded systems compared to general-purpose systems, highlighting the critical nature of many embedded system applications.\nThe real-time characteristics of embedded systems set them apart, crafting a landscape where precision, synchrony, and timely responses are not just desired but mandatory. These characteristics find resonance in myriad applications, from automotive control systems to industrial automation and healthcare devices, underscoring the embedded systems’ role as silent, yet powerful, orchestrators of a technologically harmonized world. Through their real-time attributes, embedded systems are able to deliver solutions that not only meet the functional requirements but do so with a level of precision and reliability that is both remarkable and indispensable in the contemporary world. ## Security and Reliability\nIn a world that is ever-increasingly connected and reliant on technology, the topics of security and reliability have vaulted to the forefront of concerns in system design. Particularly in the realm of embedded systems, where these units are often integral parts in critical infrastructures and applications, the stakes are exponentially higher. Let’s delve into the vital aspects that uphold the fortress of security and reliability in embedded systems:"
  },
  {
    "objectID": "embedded_sys.html#security-and-reliability",
    "href": "embedded_sys.html#security-and-reliability",
    "title": "2  Embedded Systems",
    "section": "2.7 Security and Reliability",
    "text": "2.7 Security and Reliability\n\n2.7.1 Secure Boot and Root of Trust\nAs embedded systems find themselves at the heart of numerous critical applications, ensuring the authenticity and integrity of the system right from the moment of booting is paramount. The secure boot process is a cornerstone in this security paradigm, allowing the system to only execute code that is verified and trusted. This mechanism is often complemented by a “Root of Trust,” an immutable and trusted environment, usually hardware-based, that validates the initial firmware and subsequent software layers during the boot process.\nFor instance, in financial transactions using Point-of-Sale (POS) terminals, a secure boot process ensures that the firmware is unaltered and secure, thwarting attempts of malicious firmware alterations which can potentially lead to significant data breaches. Similarly, in home automation systems, a robust secure boot process prevents unauthorized access, safeguarding user privacy and data.\n\n\n2.7.2 Fault Tolerance\nFault tolerance is an indispensable attribute in embedded systems, bestowing the system with the resilience to continue operations even in the presence of faults or failures. This is achieved through various mechanisms like redundancy, where critical components are duplicated to take over in case of a failure, or through advanced error detection and correction techniques.\nIn applications such as aerospace and aviation, fault tolerance is not just a desirable feature but a mandatory requirement. Aircraft control systems, for instance, employ multiple redundant systems operating in parallel, ensuring continuous operation even in the case of a component failure. This level of fault tolerance ensures a high degree of reliability, making sure that the system can withstand failures without catastrophic consequences, a characteristic quite unique and elevated compared to traditional computing systems.\n\n\n2.7.3 Safety-Critical Systems\nSafety-critical systems are those where a failure could result in loss of life, significant property damage, or environmental harm. These systems require meticulous design to ensure the utmost reliability and safety. Embedded systems in this category often adhere to strict development standards and undergo rigorous testing to validate their reliability and safety characteristics.\nFor example, in automotive safety systems like Anti-lock Braking Systems (ABS) and Electronic Stability Control (ESC), embedded controllers play a pivotal role. These controllers are developed following stringent standards such as ISO 26262, ensuring that they meet the high reliability and safety requirements necessary to protect lives. In healthcare, devices like pacemakers and infusion pumps fall under this category, where the reliability of embedded systems can literally be a matter of life and death.\nThe emphasis on security and reliability in embedded systems cannot be overstated and I would state that these are often overlooked topics by most. As these systems intertwine deeper into the fabric of our daily lives and critical infrastructures, the doctrines of security and reliability stand as the beacon guiding the development and deployment of embedded systems. Through mechanisms like secure boot processes and fault tolerance techniques, these systems promise not only functional efficacy but also a shield of trust and safety, offering a robust and secure harbor in a sea of technological advancements and innovations. It’s these foundational principles that shape the embedded systems of today, sculpting them into reliable guardians and efficient executors in various critical spheres of modern society."
  },
  {
    "objectID": "embedded_sys.html#future-trends-and-challenges",
    "href": "embedded_sys.html#future-trends-and-challenges",
    "title": "2  Embedded Systems",
    "section": "2.8 Future Trends and Challenges",
    "text": "2.8 Future Trends and Challenges\nArm, the largest manufacturer of microcontrollers, has shipped (either directly or indirectly through partners) a record 8.0 billion chips, taking total shipped to date to more than a quarter of a trillion or 250 billion (ARM.com)!\nWe stand on the cusp of an era of unprecedented growth in the field of embedded systems, it is both exciting and crucial to cast a discerning eye on the possible future trends and challenges that await us. From the burgeoning realms of edge computing to the demands of scalability, the landscape is set to evolve, bringing forth new vistas of opportunities and trials. Let’s venture into the dynamic frontier that the future holds for embedded systems:\n\n2.8.1 Edge Computing and IoT\nWith the proliferation of the Internet of Things (IoT), the role of edge computing is becoming increasingly vital. Edge computing essentially allows data processing at the source, reducing latency and the load on central data centers. This paradigm shift is expected to redefine embedded systems, imbuing them with greater processing capabilities and intelligence to execute complex tasks locally.\nMoreover, with the IoT expected to encompass billions of devices globally, embedded systems will play a central role in facilitating seamless connectivity and interoperability among a diverse array of devices. This ecosystem would foster real-time analytics and decision-making, paving the way for smarter cities, industries, and homes. The challenge here lies in developing systems that are secure, energy-efficient, and capable of handling the surge in data volumes efficiently.\n\n\n2.8.2 Scalability and Upgradation\nAs embedded systems continue to evolve, the need for scalability and easy upgradation will become a focal point. Systems will be expected to adapt to changing technologies and user requirements without substantial overhauls. This calls for modular designs and open standards that allow for seamless integration of new features and functionalities.\nFurthermore, with the rapid advancements in technology, embedded systems will need to be equipped with mechanisms for remote upgrades and maintenance, ensuring their longevity and relevance in a fast-paced technological landscape. The onus will be on developers and manufacturers to create systems that not only meet the current demands but are also primed for future expansions, thereby ensuring a sustainable and progressive development trajectory.\n\n\n2.8.3 Market Opportunities\nThe market dynamics surrounding embedded systems are poised for exciting shifts. As industries increasingly adopt automation and digital transformation, the demand for sophisticated embedded systems is expected to soar. AI and ML are set to integrate deeper into embedded systems, offering unprecedented levels of intelligence and automation.\nSimultaneously, there is a burgeoning market for embedded systems in consumer electronics, automotive, healthcare, and industrial applications, presenting vast opportunities for innovation and growth. However, this expansion also brings forth challenges, including increased competition and the need for compliance with evolving regulatory standards. Companies venturing into this space will need to be agile, innovative, and responsive to the changing market dynamics to carve a niche for themselves.\n\n\n2.8.4 Conclusion\nAs we look into the horizon, it’s evident that the world of embedded systems is on the brink of a transformative phase, marked by innovations, opportunities, and challenges. The future beckons with promises of greater connectivity, intelligence, and efficiency, forging a path where embedded systems will be at the helm, steering the technological advancements of society. The journey ahead is one of exploration and adaptation, where the marriage of technology and ingenuity will craft a future that is not only technologically enriched but also responsive to the complex and ever-evolving demands of a dynamic world. It is a landscape ripe with potential, beckoning pioneers to venture forth and shape the contours of a promising and vibrant future.\n\n\n\n\nARM.com. “The Future Is Being Built on Arm: Market Diversification Continues to Drive Strong Royalty and Licensing Growth as Ecosystem Reaches Quarter of a Trillion Chips Milestone – Arm®.” https://www.arm.com/company/news/2023/02/arm-announces-q3-fy22-results."
  },
  {
    "objectID": "dl_primer.html#overview",
    "href": "dl_primer.html#overview",
    "title": "3  Deep Learning Primer",
    "section": "3.1 Overview",
    "text": "3.1 Overview\n\n3.1.1 Definition and Importance\nDeep learning, a subset of machine learning and artificial intelligence (AI), involves algorithms inspired by the structure and function of the human brain, called artificial neural networks. It stands as a cornerstone in the field of AI, spearheading advancements in various domains including computer vision, natural language processing, and autonomous vehicles. Its relevance in embedded AI systems is underscored by its ability to facilitate complex computations and predictions, leveraging the limited resources available in embedded environments.\n\n\n\n3.1.2 Brief History of Deep Learning\nThe concept of deep learning has its roots in the early artificial neural networks. It has witnessed several waves of popularity, starting with the introduction of the Perceptron in the 1950s (Rosenblatt 1957), followed by the development of backpropagation algorithms in the 1980s (Rumelhart, Hinton, and Williams 1986).\nThe term “deep learning” emerged in the 2000s, marked by breakthroughs in computational power and data availability. Key milestones include the successful training of deep networks by Geoffrey Hinton, one of the god fathers of AI, and the resurgence of neural networks as a potent tool for data analysis and modeling.\nIn recent years, deep learning has witnessed exponential growth, becoming a transformative force across various industries. Figure 3.1 shows that we are currently in the third era of deep learning. From 1952 to 2010, computational growth followed an 18-month doubling pattern. This dramatically accelerated to a 6-month cycle from 2010 to 2022. At the same time, we witnessed the advent of major-scale models between 2015 and 2022; these appeared 2 to 3 orders of magnitude faster and followed a 10-month doubling cycle.\n\n\n\nFigure 3.1: Growth of deep learning models.\n\n\nA confluence of factors has fueled this surge, including advancements in computational power, the proliferation of big data, and improvements in algorithmic designs. Firstly, the expansion of computational capabilities, particularly the advent of Graphics Processing Units (GPUs) and Tensor Processing Units (TPUs) (Jouppi et al. 2017), has significantly accelerated the training and inference times of deep learning models. These hardware advancements have made it feasible to construct and train more complex, deeper networks than were possible in the earlier years.\nSecondly, the digital revolution has brought forth an abundance of “big” data, providing rich material for deep learning models to learn from and excel in tasks such as image and speech recognition, language translation, and game playing. The availability of large, labeled datasets has been instrumental in the refinement and successful deployment of deep learning applications in real-world scenarios.\nAdditionally, collaborations and open-source initiatives have fostered a vibrant community of researchers and practitioners, propelling rapid advancements in deep learning techniques. Innovations such as deep reinforcement learning, transfer learning, and generative adversarial networks have expanded the boundaries of what is achievable with deep learning, opening new avenues and opportunities in various fields including healthcare, finance, transportation, and entertainment.\nCompanies and organizations worldwide are recognizing the transformative potential of deep learning, investing heavily in research and development to harness its power in offering innovative solutions, optimizing operations, and creating new business opportunities. As deep learning continues its upward trajectory, it is poised to revolutionize how we interact with technology, making our lives more convenient, safe, and connected.\n\n\n3.1.3 Applications of Deep Learning\nDeep learning is widely used in many industries today. It is used in finance for things such as stock market prediction, risk assessment, and fraud detection. It is also used in marketing for things such as customer segmentation, personalization, and content optimization. In healthcare, machine learning is used for tasks such as diagnosis, treatment planning, and patient monitoring. It has had a transformational impact on our society.\nAn example of the transformative impact that machine learning has had on society is how it has saved money and lives. For example, as mentioned earlier, deep learning algorithms can make predictions about stocks, like predicting whether they will go up or down. These predictions guide investment strategies and improve financial decisions. Similarly, deep learning can also make medical predictions to improve patient diagnosis and save lives. The possibilities are endless and the benefits are clear. Machine learning is not only able to make predictions with greater accuracy than humans but it is also able to do so at a much faster pace.\nDeep learning has been applied to manufacturing to great effect. By using software to constantly learn from the vast amounts of data collected throughout the manufacturing process, companies are able to increase productivity while reducing wastage through improved efficiency. Companies are benefiting financially from these effects while customers are receiving better quality products at lower prices. Machine learning enables manufacturers to constantly improve their processes to create higher quality goods faster and more efficiently than ever before.\nDeep learning has also improved products that we use daily like Netflix recommendations or Google Translate’s text translations, but it also allows companies such as Amazon and Uber to save money on customer service costs by quickly identifying unhappy customers.\n\n\n3.1.4 Relevance to Embedded AI\nEmbedded AI, which involves integrating AI algorithms directly into hardware devices, naturally benefits from the capabilities of deep learning. The synergy of deep learning algorithms with embedded systems has paved the way for intelligent, autonomous devices capable of sophisticated on-device data processing and analysis. Deep learning facilitates the extraction of intricate patterns and information from input data, making it a vital tool in the development of smart embedded systems, ranging from household appliances to industrial machines. This union aims to foster a new era of smart, interconnected devices that can learn and adapt to user behaviors and environmental conditions, optimizing performance and offering unprecedented levels of convenience and efficiency."
  },
  {
    "objectID": "dl_primer.html#neural-networks",
    "href": "dl_primer.html#neural-networks",
    "title": "3  Deep Learning Primer",
    "section": "3.2 Neural Networks",
    "text": "3.2 Neural Networks\nDeep learning takes inspiration from the human brain’s neural networks to create patterns utilized in decision-making. This section explores the foundational concepts that comprise deep learning, offering insights into the underpinnings of more complex topics explored later in this primer.\nNeural networks form the basis of deep learning, drawing inspiration from the biological neural networks of the human brain to process and analyze data in a hierarchical manner. Below, we dissect the primary components and structures commonly found in neural networks.\n\n3.2.1 Perceptrons\nAt the foundation of neural networks is the perceptron, a basic unit or node that forms the basis of more complex structures. A perceptron receives various inputs, applies weights and a bias to these inputs, and then employs an activation function to produce an output as shown below in Figure 3.2.\n\n\n\nFigure 3.2: Perceptron\n\n\nInitially conceptualized in the 1950s, perceptrons paved the way for the development of more intricate neural networks, serving as a fundamental building block in the field of deep learning.\n\n\n3.2.2 Multi-layer Perceptrons\nMulti-layer perceptrons (MLPs) evolve from the single-layer perceptron model, incorporating multiple layers of nodes connected in a feedforward manner. These layers include an input layer to receive data, several hidden layers to process this data, and an output layer to generate the final results. MLPs excel in identifying non-linear relationships, utilizing a backpropagation technique for training, wherein the weights are optimized through a gradient descent algorithm.\n\n\n\nMultilayer Perceptron\n\n\n\n\n3.2.3 Activation Functions\nActivation functions stand as vital components in neural networks, providing the mathematical equations that determine a network’s output. These functions introduce non-linearity to the network, facilitating the learning of complex patterns by allowing the network to adjust weights based on the error during the learning process. Popular activation functions encompass the sigmoid, tanh, and ReLU (Rectified Linear Unit) functions.\n\n\n\nActivation Function\n\n\n\n\n3.2.4 Computational Graphs\nDeep learning employs computational graphs to illustrate the various operations and their interactions within a neural network. This subsection explores the essential phases of computational graph processing.\n\n\n\nTensorFlow Computational Graph\n\n\n\n3.2.4.1 Forward Pass\nThe forward pass denotes the initial phase where data progresses through the network from the input to the output layer. During this phase, each layer conducts specific computations on the input data, utilizing weights and biases before passing the resulting values onto subsequent layers. The ultimate output of this phase is employed to compute the loss, representing the disparity between the predicted output and actual target values.\n\n\n3.2.4.2 Backward Pass (Backpropagation)\nBackpropagation signifies a pivotal algorithm in the training of deep neural networks. This phase involves computing the gradient of the loss function with respect to each weight using the chain rule, effectively maneuvering backwards through the network. The gradients calculated in this step guide the adjustment of weights with the objective of minimizing the loss function, thereby enhancing the network’s performance with each iteration of training.\nGrasping these foundational concepts paves the way to understanding more intricate deep learning architectures and techniques, fostering the development of more sophisticated and efficacious applications, especially within the realm of embedded AI systems.\n\n\n\n\n\n3.2.5 Training Concepts\nIn the realm of deep learning, it’s crucial to comprehend various key concepts and terms that set the foundation for creating, training, and optimizing deep neural networks. This section clarifies these essential concepts, providing a straightforward path to delve deeper into the intricate dynamics of deep learning. Overall, ML training is an iterative process. An untrained neural network model takes some features as input and makes a forward prediction pass. Given some ground truth about the prediction, which is known during the training process, we can compute a loss using a loss function and update the neural network parameters during the backward pass. We repeat this process until the network converges towards correct predictions with satisfactory accuracy.\n\n\n\nAn iterative approach to training a model.\n\n\n\n3.2.5.1 Loss Functions\nLoss functions, also known as cost functions, quantify how well a neural network is performing by calculating the difference between the actual and predicted outputs. The objective during the training process is to minimize this loss function to improve the model’s accuracy. As Figure 3.3 shows, models can either have high loss or low loss depending on where in the training phase the network is in.\n\n\n\nFigure 3.3: High loss in the left model; low loss in the right model.\n\n\nVarious loss functions are employed depending on the specific task, such as mean squared error, log loss and cross-entropy loss for regression tasks and categorical crossentropy for classification tasks.\n\n\n3.2.5.2 Optimization Algorithms\nOptimization algorithms play a crucial role in the training process, aiming to minimize the loss function by adjusting the model’s weights. These algorithms navigate through the model’s parameter space to find the optimal set of parameters that yield the minimum loss. Some commonly used optimization algorithms are:\n\nGradient Descent: A first-order optimization algorithm that uses the gradient of the loss function to move the weights in the direction that minimizes the loss.\nStochastic Gradient Descent (SGD): A variant of gradient descent that updates the weights using a subset of the data, thus accelerating the training process.\nAdam: A popular optimization algorithm that combines the benefits of other extensions of gradient descent, often providing faster convergence.\n\n\n\n\nMinimizing loss during the training process.\n\n\n\n\n3.2.5.3 Regularization Techniques\nTo prevent overfitting and help the model generalize better to unseen data, regularization techniques are employed. These techniques penalize the complexity of the model, encouraging simpler models that can perform better on new data.\n\nCommon regularization techniques include:\n\nL1 and L2 Regularization: These techniques add a penalty term to the loss function, discouraging large weights and promoting simpler models.\nDropout: A technique where randomly selected neurons are ignored during training, forcing the network to learn more robust features.\nBatch Normalization: This technique normalizes the activations of the neurons in a given layer, improving the stability and performance of the network.\n\nUnderstanding these fundamental concepts and terms forms the backbone of deep learning, setting the stage for a more in-depth exploration into the intricacies of various deep learning architectures and their applications, particularly in embedded AI systems.\n\n\n\n3.2.6 Model Architectures\nDeep learning architectures refer to the various structured approaches that dictate how neurons and layers are organized and interact in neural networks. These architectures have evolved to address different problems and data types efficiently. This section provides an overview of some prominent deep learning architectures and their characteristics.\n\n3.2.6.1 Multi-Layer Perceptrons (MLPs)\nMLPs are fundamental deep learning architectures, consisting of three or more layers: an input layer, one or more hidden layers, and an output layer. These layers are fully connected, meaning every neuron in a layer is connected to every neuron in the preceding and succeeding layers. MLPs can model complex functions and find applications in a wide range of tasks, including regression, classification, and pattern recognition. Their ability to learn non-linear relationships through backpropagation makes them a versatile tool in the deep learning arsenal.\nIn embedded AI systems, MLPs can serve as compact models for simpler tasks, such as sensor data analysis or basic pattern recognition, where computational resources are constrained. Their capability to learn non-linear relationships with relatively less complexity makes them a viable option for embedded systems.\n\n\n3.2.6.2 Convolutional Neural Networks (CNNs)\nCNNs are primarily used in image and video recognition tasks. This architecture uses convolutional layers that apply a series of filters to the input data to identify various features such as edges, corners, and textures. A typical CNN also includes pooling layers that reduce the spatial dimensions of the data, and fully connected layers for classification. CNNs have proven highly effective in tasks like image recognition, object detection, and computer vision applications.\nIn the realm of embedded AI, CNNs are pivotal for image and video recognition applications, where real-time processing is often required. They can be optimized for embedded systems by employing techniques such as quantization and pruning to reduce memory usage and computational demands, enabling efficient object detection and facial recognition functionalities in devices with limited computational resources.\n\n\n3.2.6.3 Recurrent Neural Networks (RNNs)\nRNNs are suited for sequential data analysis, such as time series forecasting and natural language processing. In this architecture, connections between nodes form a directed graph along a temporal sequence, allowing information to be carried across sequences through hidden state vectors. Variations of RNNs include Long Short-Term Memory (LSTM) and Gated Recurrent Units (GRU), which are designed to capture longer dependencies in sequence data.\nIn embedded systems, these networks can be implemented in voice recognition systems, predictive maintenance, or in IoT devices where sequential data patterns are prevalent. Optimizations specific to embedded platforms can help in managing their typically high computational and memory requirements.\n\n\n3.2.6.4 Generative Adversarial Networks (GANs)\nGANs consist of two networks, a generator and a discriminator, that are trained simultaneously through adversarial training. The generator produces data that tries to mimic the real data distribution, while the discriminator aims to distinguish between real and generated data. GANs are widely used in image generation, style transfer, and data augmentation.\nIn embedded contexts, GANs could be used for on-device data augmentation to enhance the training of models directly on the embedded device, facilitating continual learning and adaptation to new data without the need for cloud computing resources.\n\n\n3.2.6.5 Autoencoders\nAutoencoders are neural networks used for data compression and noise reduction. They are structured to encode input data into a lower-dimensional representation and then decode it back to the original form. Variations like Variational Autoencoders (VAEs) introduce probabilistic layers that allow for generative properties, finding applications in image generation and anomaly detection.\nImplementing autoencoders can assist in efficient data transmission and storage, enhancing the overall performance of embedded systems with limited computational and memory resources.\n\n\n3.2.6.6 Transformer Networks\nTransformer networks have emerged as a powerful architecture, especially in the field of natural language processing. These networks use self-attention mechanisms to weigh the influence of different input words on each output word, facilitating parallel computation and capturing complex patterns in data. Transformer networks have led to state-of-the-art results in tasks such as language translation, summarization, and text generation.\nThese networks can be optimized to perform language-related tasks directly on-device. For instance, transformers can be utilized in embedded systems for real-time translation services or voice-assisted interfaces, where latency and computational efficiency are critical factors. Techniques such as model distillation which we will discss later on can be employed to deploy these networks on embedded devices with constrained resources.\nEach of these architectures serves specific purposes and excel in different domains, offering a rich toolkit for tackling diverse problems in the realm of embedded AI systems. Understanding the nuances of these architectures is vital in designing effective and efficient deep learning models for various applications."
  },
  {
    "objectID": "dl_primer.html#libraries-and-frameworks",
    "href": "dl_primer.html#libraries-and-frameworks",
    "title": "3  Deep Learning Primer",
    "section": "3.3 Libraries and Frameworks",
    "text": "3.3 Libraries and Frameworks\nIn the world of deep learning, the availability of robust libraries and frameworks has been a cornerstone in facilitating the development, training, and deployment of models, particularly in embedded AI systems where efficiency and optimization are key. These libraries and frameworks are often equipped with pre-defined functions and tools that allow for rapid prototyping and deployment. This section sheds light on popular libraries and frameworks, emphasizing their utility in embedded AI scenarios.\n\n3.3.1 TensorFlow\nTensorFlow, developed by Google, stands as one of the premier frameworks for developing deep learning models. Its ability to work seamlessly with embedded systems comes from TensorFlow Lite, a lightweight solution designed to run on mobile and embedded devices. TensorFlow Lite enables the execution of optimized models on a variety of platforms, making it easier to integrate AI functionalities in embedded systems. For TinyML we will be dealing with TensorFlow Lite for Microcontrollers.\n\n\n3.3.2 PyTorch\nPyTorch, an open-source library developed by Facebook, is praised for its dynamic computation graph and ease of use. For embedded AI, PyTorch can be a suitable choice for research and prototyping, offering a seamless transition from research to production with the use of the TorchScript scripting language. PyTorch Mobile further facilitates the deployment of models on mobile and embedded devices, offering tools and workflows to optimize performance.\n\n\n3.3.3 ONNX Runtime\nThe Open Neural Network Exchange (ONNX) Runtime is a cross-platform, high-performance engine for running machine learning models. It is not particularly developed for embedded AI systems, though it supports a wide range of hardware accelerators and is capable of optimizing computations to improve performance in resource-constrained environments.\n\n\n3.3.4 Keras\nKeras serves as a high-level neural networks API, capable of running on top of TensorFlow, and other frameworks like Theano, or CNTK. For developers venturing into embedded AI, Keras offers a simplified interface for building and training models. Its ease of use and modularity can be especially beneficial in the rapid development and deployment of models in embedded systems, facilitating the integration of AI capabilities with minimal complexity.\n\n\n3.3.5 TVM\nTVM is an open-source machine learning compiler stack that aims to enable efficient deployment of deep learning models on a variety of platforms. Particularly in embedded AI, TVM and µTVM (Micro TVM) can be crucial in optimizing and streamlining models to suit the restricted computational and memory resources, thus making deep learning more accessible and feasible on embedded devices.\nThese libraries and frameworks are pivotal in leveraging the capabilities of deep learning in embedded AI systems, offering a range of tools and functionalities that enable the development of intelligent and optimized solutions. Selecting the appropriate library or framework, however, is a crucial step in the development pipeline, aligning with the specific requirements and constraints of embedded systems."
  },
  {
    "objectID": "dl_primer.html#embedded-ai-challenges",
    "href": "dl_primer.html#embedded-ai-challenges",
    "title": "3  Deep Learning Primer",
    "section": "3.4 Embedded AI Challenges",
    "text": "3.4 Embedded AI Challenges\nEmbedded AI systems often operate within environments with constrained resources, posing unique challenges in implementing the deep learning algorithms we discussed above efficiently. In this section, we explore various challenges encountered in the deployment of deep learning in embedded systems and potential solutions to navigate these complexities.\n\n3.4.1 Memory Constraints\n\nChallenge: Embedded systems usually have limited memory, which can be a bottleneck when deploying large deep learning models.\nSolution: Employing model compression techniques such as pruning and quantization to reduce the memory footprint without significantly affecting performance.\n\n\n\n3.4.2 Computational Limitations\n\nChallenge: The computational capacity in embedded systems can be limited, hindering the deployment of complex deep learning models.\nSolution: Utilizing hardware acceleration through GPUs or dedicated AI chips to boost computational power, and optimizing models for inference through techniques like layer fusion.\n\n\n\n3.4.3 Energy Efficiency\n\nChallenge: Embedded systems, particularly battery-powered devices, require energy-efficient operations to prolong battery life.\nSolution: Implementing energy-efficient neural networks that are designed to minimize energy consumption during operation, and employing dynamic voltage and frequency scaling to adjust the power consumption dynamically.\n\n\n\n3.4.4 Data Privacy and Security\n\nChallenge: Embedded AI systems often process sensitive data, raising concerns regarding data privacy and security.\nSolution: Employing on-device processing to keep sensitive data on the device itself, and incorporating encryption and secure channels for any necessary data transmission.\n\n\n\n3.4.5 Real-Time Processing Requirements\n\nChallenge: Many embedded AI applications demand real-time processing to provide instantaneous responses, which can be challenging to achieve with deep learning models.\nSolution: Streamlining the model through methods such as model distillation to reduce complexity and employing real-time operating systems to ensure timely processing.\n\n\n\n3.4.6 Model Robustness and Generalization\n\nChallenge: Ensuring that deep learning models are robust and capable of generalizing well to unseen data in embedded AI settings.\nSolution: Incorporating techniques like data augmentation and adversarial training to enhance model robustness and improve generalization capabilities.\n\n\n\n3.4.7 Integration with Existing Systems\n\nChallenge: Integrating deep learning capabilities into existing embedded systems can pose compatibility and interoperability issues.\nSolution: Adopting modular design approaches and leveraging APIs and middleware solutions to facilitate smooth integration with existing systems and infrastructures.\n\n\n\n3.4.8 Scalability\n\nChallenge: Scaling deep learning solutions to cater to a growing number of devices and users in embedded AI ecosystems.\nSolution: Utilizing cloud-edge computing paradigms to distribute computational loads effectively and ensuring that the models can be updated seamlessly to adapt to changing requirements.\n\nUnderstanding and addressing these challenges are vital in the successful deployment of deep learning solutions in embedded AI systems. By adopting appropriate strategies and solutions, developers can navigate these hurdles effectively, fostering the creation of reliable, efficient, and intelligent embedded AI systems.\n\n\n\n\nJouppi, Norman P, Cliff Young, Nishant Patil, David Patterson, Gaurav Agrawal, Raminder Bajwa, Sarah Bates, et al. 2017. “In-Datacenter Performance Analysis of a Tensor Processing Unit.” In Proceedings of the 44th Annual International Symposium on Computer Architecture, 1–12.\n\n\nRosenblatt, Frank. 1957. The Perceptron, a Perceiving and Recognizing Automaton Project Para. Cornell Aeronautical Laboratory.\n\n\nRumelhart, David E, Geoffrey E Hinton, and Ronald J Williams. 1986. “Learning Representations by Back-Propagating Errors.” Nature 323 (6088): 533–36."
  },
  {
    "objectID": "embedded_ml.html#cloud-ml",
    "href": "embedded_ml.html#cloud-ml",
    "title": "4  Embedded ML",
    "section": "4.1 Cloud ML",
    "text": "4.1 Cloud ML\n\n4.1.1 Characteristics\nCloud ML is a facet of the broader machine learning discipline that operates on cloud computing infrastructure. It essentially facilitates the development, training, and deployment of machine learning models on a virtual platform, offering both flexibility and scalability.\nAt its core, Cloud ML relies on a potent combination of high-capacity servers, vast storage solutions, and robust network architectures that are housed in data centers globally. This infrastructure permits the centralization of computational resources, making it easier to manage and scale machine learning projects seamlessly.\nThe cloud serves as a prolific environment for data processing and model training, equipped to handle extensive data loads and complex computations. Models developed under Cloud ML can be trained using a wealth of data, which is processed and analyzed in a centralized location, thereby optimizing the model’s learning and predictive capabilities.\n\n\n4.1.2 Benefits\nCloud ML is synonymous with high computational power, capable of managing intricate algorithms and large datasets with finesse. This aspect is particularly beneficial in advancing machine learning models that require substantial computational resources, effectively bypassing the limitations of local setups.\nOne of the standout benefits of Cloud ML is the ability to scale resources dynamically based on project demands. This scalability ensures that as the data volume or computational needs escalate, the infrastructure can adapt accordingly without compromising performance.\nCloud ML platforms often provide access to a plethora of advanced tools and algorithms. Developers can leverage these resources to build, train, and deploy sophisticated models, accelerating the development cycle and fostering innovation.\n\n\n4.1.3 Challenges\nDespite its prowess, Cloud ML is often hampered by latency issues, particularly in applications necessitating real-time responses. The process of transmitting data to centralized servers and back can introduce delays, a critical drawback in time-sensitive applications.\nCentralizing data processing and storage sometimes open up vulnerabilities in data privacy and security. The data centers become prime targets for cyber-attacks, necessitating significant investments in security infrastructure to safeguard sensitive information.\nAs the data processing needs grow, so do the costs associated with using cloud services. Organizations working with large data volumes may find escalating costs, which could potentially limit the scalability and feasibility of their operations over time.\n\n\n4.1.4 Example Use Cases\nCloud ML has been instrumental in powering virtual assistants like Siri and Alexa. These systems leverage the cloud’s computational capabilities to process and analyze voice inputs, providing users with intelligent and personalized responses.\nCloud ML also forms the backbone of sophisticated recommendation systems seen in platforms like Netflix and Amazon. These systems analyze extensive datasets to discern patterns and preferences, delivering personalized content or product recommendations to enhance user engagement.\nIn the financial sector, Cloud ML has significantly contributed to the development of robust fraud detection systems. These systems analyze large swathes of transactional data to identify potential fraudulent activities, allowing for timely interventions and minimizing risks associated with financial frauds.\nThere are countless other examples of Cloud ML, but briefly, nowadays, it is impossible to get on the internet without interacting with some form of it, either directly or indirectly. From personalized advertisements that pop up on your social media feed to the predictive text functionality in email services, Cloud ML intricately weaves itself into the fabric of online experiences. It fuels the smart algorithms that suggest products on e-commerce websites, power search engines to deliver precise results, and even facilitate automatic tagging and categorization of photos on platforms like Facebook.\nMoreover, Cloud ML enhances user security through anomaly detection systems that monitor for suspicious activities, potentially protecting users from cyber threats. In essence, it functions as the silent powerhouse, continually working behind the scenes to streamline, secure, and personalize our digital interactions, making the modern internet a more intuitive and user-friendly space."
  },
  {
    "objectID": "embedded_ml.html#edge-ml",
    "href": "embedded_ml.html#edge-ml",
    "title": "4  Embedded ML",
    "section": "4.2 Edge ML",
    "text": "4.2 Edge ML\n\n4.2.1 Characteristics\nDefinition of Edge ML\nEdge Machine Learning (Edge ML) refers to the deployment of machine learning algorithms directly on endpoint devices or closer to where the data is generated, instead of relying on centralized cloud servers. This approach aims to bring computation near the data source, thereby reducing the need to transmit large volumes of data over networks, which often leads to lower latency and enhanced data privacy.\nDecentralized Data Processing\nIn an Edge ML setup, data processing occurs in a decentralized manner. Instead of sending data all the way to distant servers, the data is processed locally on devices like smartphones, tablets, or IoT devices. This localized processing means that devices can make swift decisions based on the data they collect, without relying heavily on a central server’s resources. This decentralization is critical in real-time applications where even a slight delay can have significant repercussions.\nLocal Data Storage and Computation\nLocal data storage and computation are hallmarks of Edge ML. This approach ensures that data can be stored and analyzed directly on the devices, thereby retaining the data’s privacy and reducing the reliance on continuous internet connectivity. Moreover, it can often lead to more efficient computation as data doesn’t have to travel over long distances, and the computations are performed with a more intimate understanding of the local context, which can sometimes lead to more insightful analyses.\n\n\n4.2.2 Benefits\nReduced Latency\nBy virtue of processing data locally, Edge ML significantly reduces latency compared to Cloud ML. In scenarios where milliseconds matter, like in autonomous vehicles where rapid decision-making can mean the difference between safety and accident, the reduced latency can be a critical advantage.\nEnhanced Data Privacy\nEdge ML also offers enhanced data privacy as data is primarily stored and processed locally, minimizing the risk of data breaches that are more common in centralized data storage solutions. This means sensitive information can be kept more secure, as it’s not transmitted over networks where it might potentially be intercepted.\nLess Bandwidth Usage\nOperating on the edge means that less data needs to be transmitted over networks, thereby reducing bandwidth usage. This can lead to cost savings and efficiency improvements, particularly in environments where bandwidth is limited or expensive.\n\n\n4.2.3 Challenges\nLimited Computational Resources Compared to Cloud ML\nHowever, Edge ML comes with its set of challenges. One of the primary concerns is the limited computational resources compared to cloud-based solutions. Endpoint devices may not have the same processing power or storage capacity as cloud servers, which can restrict the complexity of the machine learning models that can be deployed.\nComplexity in Managing Edge Nodes\nManaging a network of edge nodes can introduce complexity, particularly when it comes to coordination, updates, and maintenance. Ensuring that all nodes operate seamlessly and are up-to-date with the latest algorithms and security protocols can be a logistical challenge.\nSecurity Concerns at the Edge Nodes\nDespite the enhanced data privacy, edge nodes can sometimes be more vulnerable to physical and cyber-attacks. Developing robust security protocols that protect data at each node, without compromising the system’s efficiency, remains a significant challenge in the deployment of Edge ML solutions.\n\n\n4.2.4 Example Use Cases\nIn highlighting use cases such as autonomous vehicles, smart homes and buildings, and industrial IoT, we aim to underscore the diverse range of environments where Edge ML can be a potent force. The below examples were chosen as they encapsulate scenarios where real-time data processing, reduced latency, and enhanced privacy are not just beneficial but often critical to the operation and success of these technologies. They serve to demonstrate the pivotal role that Edge ML can play in spearheading advancements in different sectors, fostering innovation, and paving the way for more intelligent, responsive, and adaptive systems.\nAutonomous Vehicles\nAutonomous vehicles stand as a beacon of Edge ML’s potential. These vehicles rely heavily on real-time data processing to navigate and make decisions. The localized machine learning models help in swiftly analyzing data from various sensors to make immediate driving decisions, essentially ensuring safety and smooth operation.\nSmart Homes and Buildings\nIn the context of smart homes and buildings, Edge ML plays a vital role in managing various systems efficiently, from lighting and heating to security systems. By processing data locally, these systems can operate more responsively and in tune with the occupants’ habits and preferences, creating a more harmonized living environment.\nIndustrial IoT\nIndustrial Internet of Things (IoT) leverages Edge ML to monitor and control complex industrial processes. Here, machine learning models can analyze data from a plethora of sensors in real-time, facilitating predictive maintenance, optimizing operations, and improving safety measures, thereby bringing about a revolution in industrial automation and efficiency.\nWhile the aforementioned cases offer a glimpse into the versatility and potential of Edge ML, it is crucial to note that its applicability is vast and not limited to these scenarios. Various other domains, including healthcare, agriculture, and urban planning, to name a few, are exploring and integrating Edge ML to develop solutions that are both innovative and responsive to real-world needs and challenges, heralding a new era of smart, interconnected systems."
  },
  {
    "objectID": "embedded_ml.html#tiny-ml",
    "href": "embedded_ml.html#tiny-ml",
    "title": "4  Embedded ML",
    "section": "4.3 Tiny ML",
    "text": "4.3 Tiny ML\n\n4.3.1 Characteristics\nDefinition of TinyML\nTinyML stands at the intersection of embedded systems and machine learning, representing an emerging field that brings intelligent algorithms directly onto minuscule microcontrollers and sensors. These microcontrollers operate in environments characterized by severe resource constraints, particularly with respect to memory, storage, and computational power.\nOn-Device Machine Learning\nIn the realm of TinyML, the crux of operations lies in on-device machine learning. This means the machine learning models are not just deployed but also trained directly on the device itself, without relying on external servers or cloud infrastructures. By doing this, TinyML can facilitate intelligent decision-making right where the data is being generated, thereby making real-time insights and responses a possibility, even in environments where connectivity is limited or non-existent.\nLow Power and Resource-Constrained Environments\nTinyML thrives in low power and resource-constrained environments. These environments demand solutions that are highly optimized to function within the limited available resources. TinyML achieves this through specialized algorithms and models that are designed to deliver reasonable performance while consuming minimal energy, hence ensuring prolonged operational periods even in battery-powered devices.\n\n\n4.3.2 Benefits\nExtremely Low Latency\nA defining benefit of TinyML is its capacity to offer extremely low latency. Given that the computation happens directly on the device, the time taken to transmit data to external servers and receive a response is eliminated. This becomes a critical advantage in applications where instantaneous decision-making is required, thereby facilitating rapid responses to changing conditions.\nHigh Data Security\nWith TinyML, data security is inherently enhanced. Since data processing and analysis are confined to the device itself, the risk of data interception during transmission is practically nullified. This localized approach to data handling ensures that sensitive information remains confined to the device, thereby bolstering the security of user data.\nEnergy Efficiency\nTinyML operates under a paradigm of energy efficiency, a necessity given the resource-constrained environments in which it functions. Through the implementation of lean algorithms and optimized computational methods, TinyML ensures that devices can perform complex tasks without draining battery life rapidly, making it a sustainable choice for long-term deployments.\n\n\n4.3.3 Challenges\nLimited Computational Capabilities\nHowever, the transition to TinyML is not without challenges. The primary constraint lies in the limited computational capabilities of the devices. The need to function within constrained environments means that the models deployed must be simplified, potentially impacting the accuracy and sophistication of the solutions developed.\nComplex Development Cycle\nTinyML also introduces a complex development cycle. Creating models that are both lightweight and effective requires a deep understanding of machine learning principles, coupled with expertise in embedded systems. This complexity necessitates a collaborative approach to development, where expertise in multiple domains is a prerequisite for success.\nModel Optimization and Compression\nOne of the central challenges in TinyML is model optimization and compression. Developing machine learning models that can function effectively within the limited memory and computational power of microcontrollers demands innovative approaches to model design and implementation. Developers are often tasked with striking a delicate balance, optimizing models to a point where they retain efficacy while fitting within the stringent resource constraints.\n\n\n4.3.4 Example Use Cases\nWearable Devices\nIn the context of wearable devices, TinyML opens up possibilities for smarter, more responsive gadgets. From fitness trackers that can provide real-time feedback on your workouts to smart glasses that can process visual data on the go, TinyML is revolutionizing the way we interact with wearable technology, offering personalized experiences and insights directly from the device itself.\nPredictive Maintenance\nTinyML finds a significant role in predictive maintenance, particularly in industrial settings. By deploying TinyML algorithms on sensors and devices that monitor equipment health, companies can identify potential issues before they escalate, thereby reducing downtime and averting costly breakdowns. The ability to analyze data on-site ensures swift responses, potentially preventing minor issues from escalating into significant problems.\nEnvironmental Monitoring\nIn environmental monitoring, TinyML can facilitate the real-time analysis of data collected from various sensors deployed in the field. These could range from monitoring air quality in urban areas to tracking wildlife movements in protected areas. Through TinyML, data can be processed locally, enabling swift responses to changing conditions and providing a nuanced understanding of environmental patterns and trends, which is crucial for informed decision-making and policy formulation.\nIn essence, TinyML stands as a vanguard in the evolution of machine learning, fostering innovation in a myriad of fields by bringing intelligence directly to the edge, where data meets the real world. Its potential to revolutionize how we interact with technology and the world around us is immense, promising a future where devices are not just connected but also intelligent, capable of making decisions and responding to the environment in real-time.\nTinyML stands as a vanguard in the evolution of machine learning, fostering innovation in a myriad of fields by bringing intelligence directly to the edge, where data meets the real world. Its potential to revolutionize how we interact with technology and the world around us is immense, promising a future where devices are not just connected but also intelligent, capable of making decisions and responding to the environment in real-time."
  },
  {
    "objectID": "embedded_ml.html#comparison",
    "href": "embedded_ml.html#comparison",
    "title": "4  Embedded ML",
    "section": "4.4 Comparison",
    "text": "4.4 Comparison\nThus far, we have discussed each of the different ML variants in isolation. Now, let’s put them all together. Here’s a table that offers a comparative analysis of Cloud ML, Edge ML, and TinyML based on various features and aspects. It helps to provide a clear perspective on the distinguishing factors and unique advantages of each, helping in making informed decisions based on specific requirements and constraints for a given application or project.\n\n\n\n\n\n\n\n\n\nFeature/Aspect\nCloud ML\nEdge ML\nTinyML\n\n\n\n\nProcessing Location\nCentralized servers (Data Centers)\nLocal devices (closer to data sources)\nOn-device (microcontrollers, embedded systems)\n\n\nLatency\nHigh (Depends on internet connectivity)\nModerate (Reduced latency compared to Cloud ML)\nLow (Immediate processing without network delay)\n\n\nData Privacy\nModerate (Data transmitted over networks)\nHigh (Data remains on local networks)\nVery High (Data processed on-device, not transmitted)\n\n\nComputational Power\nHigh (Utilizes powerful data center infrastructure)\nModerate (Utilizes local device capabilities)\nLow (Limited to the power of the embedded system)\n\n\nEnergy Consumption\nHigh (Data centers consume significant energy)\nModerate (Less than data centers, more than TinyML)\nLow (Highly energy-efficient, designed for low power)\n\n\nScalability\nHigh (Easy to scale with additional server resources)\nModerate (Depends on local device capabilities)\nLow (Limited by the hardware resources of the device)\n\n\nCost\nHigh (Recurring costs for server usage, maintenance)\nVariable (Depends on the complexity of local setup)\nLow (Primarily upfront costs for hardware components)\n\n\nConnectivity Dependence\nHigh (Requires stable internet connectivity)\nLow (Can operate with intermittent connectivity)\nVery Low (Can operate without any network connectivity)\n\n\nReal-time Processing\nModerate (Can be affected by network latency)\nHigh (Capable of real-time processing locally)\nVery High (Immediate processing with minimal latency)\n\n\nApplication Examples\nBig Data Analysis, Virtual Assistants\nAutonomous Vehicles, Smart Homes\nWearables, Sensor Networks\n\n\nDevelopment Complexity\nModerate to High (Requires knowledge in cloud computing)\nModerate (Requires knowledge in local network setup)\nModerate to High (Requires expertise in embedded systems)\n\n\n\nThe table delineates a comparative analysis of these three paradigms, highlighting the distinctions in processing location, latency, data privacy, computational power, energy consumption, scalability, cost, connectivity dependence, real-time processing capabilities, and application examples. Cloud ML, centralized and power-abundant, stands as a giant in computational prowess but may face hurdles in latency and energy consumption. Edge ML emerges as a bridge, offering moderate computational power with reduced latency, situated closer to the data sources, thus ensuring heightened data privacy. TinyML, the newest entrant, operates on constrained devices, excelling in low-energy consumption and on-device processing, promising significant strides in applications like wearables and sensor networks. This table serves as a roadmap for enthusiasts and professionals alike, offering a panoramic view of the attributes and capabilities of each paradigm, aiding in the selection of the most suitable approach for specific project requirements."
  },
  {
    "objectID": "mlworkflow.html#data-collection",
    "href": "mlworkflow.html#data-collection",
    "title": "5  ML Workflow",
    "section": "5.1 Data Collection",
    "text": "5.1 Data Collection"
  },
  {
    "objectID": "mlworkflow.html#pre-processing",
    "href": "mlworkflow.html#pre-processing",
    "title": "5  ML Workflow",
    "section": "5.2 Pre-Processing",
    "text": "5.2 Pre-Processing"
  },
  {
    "objectID": "mlworkflow.html#training",
    "href": "mlworkflow.html#training",
    "title": "5  ML Workflow",
    "section": "5.3 Training",
    "text": "5.3 Training"
  },
  {
    "objectID": "mlworkflow.html#optimization",
    "href": "mlworkflow.html#optimization",
    "title": "5  ML Workflow",
    "section": "5.4 Optimization",
    "text": "5.4 Optimization"
  },
  {
    "objectID": "mlworkflow.html#deployment",
    "href": "mlworkflow.html#deployment",
    "title": "5  ML Workflow",
    "section": "5.5 Deployment",
    "text": "5.5 Deployment"
  },
  {
    "objectID": "mlworkflow.html#evaluation",
    "href": "mlworkflow.html#evaluation",
    "title": "5  ML Workflow",
    "section": "5.6 Evaluation",
    "text": "5.6 Evaluation"
  },
  {
    "objectID": "mlworkflow.html#quiz",
    "href": "mlworkflow.html#quiz",
    "title": "5  ML Workflow",
    "section": "5.7 Quiz",
    "text": "5.7 Quiz"
  },
  {
    "objectID": "data_engineering.html#data-sources",
    "href": "data_engineering.html#data-sources",
    "title": "6  Data Engineering",
    "section": "6.1 Data Sources",
    "text": "6.1 Data Sources"
  },
  {
    "objectID": "data_engineering.html#training-data",
    "href": "data_engineering.html#training-data",
    "title": "6  Data Engineering",
    "section": "6.2 Training Data",
    "text": "6.2 Training Data"
  },
  {
    "objectID": "data_engineering.html#training-data-splits",
    "href": "data_engineering.html#training-data-splits",
    "title": "6  Data Engineering",
    "section": "6.3 Training Data Splits",
    "text": "6.3 Training Data Splits"
  },
  {
    "objectID": "data_engineering.html#data-labeling",
    "href": "data_engineering.html#data-labeling",
    "title": "6  Data Engineering",
    "section": "6.4 Data Labeling",
    "text": "6.4 Data Labeling"
  },
  {
    "objectID": "data_engineering.html#types-of-data",
    "href": "data_engineering.html#types-of-data",
    "title": "6  Data Engineering",
    "section": "6.5 Types of Data",
    "text": "6.5 Types of Data"
  },
  {
    "objectID": "preprocessing.html#what-is-data-pre-processing",
    "href": "preprocessing.html#what-is-data-pre-processing",
    "title": "7  Pre-processing",
    "section": "7.1 What is Data Pre-processing?",
    "text": "7.1 What is Data Pre-processing?"
  },
  {
    "objectID": "preprocessing.html#whats-involved-with-data-pre-processing",
    "href": "preprocessing.html#whats-involved-with-data-pre-processing",
    "title": "7  Pre-processing",
    "section": "7.2 What’s Involved with Data Pre-processing?",
    "text": "7.2 What’s Involved with Data Pre-processing?"
  },
  {
    "objectID": "preprocessing.html#whats-the-importance-of-data-pre-processing",
    "href": "preprocessing.html#whats-the-importance-of-data-pre-processing",
    "title": "7  Pre-processing",
    "section": "7.3 What’s The Importance Of Data Pre-Processing?",
    "text": "7.3 What’s The Importance Of Data Pre-Processing?"
  },
  {
    "objectID": "frameworks.html",
    "href": "frameworks.html",
    "title": "8  ML Frameworks",
    "section": "",
    "text": "coming soon."
  },
  {
    "objectID": "training.html#selecting-a-training-dataset",
    "href": "training.html#selecting-a-training-dataset",
    "title": "9  Model Training",
    "section": "9.1 Selecting a Training Dataset",
    "text": "9.1 Selecting a Training Dataset"
  },
  {
    "objectID": "training.html#neural-network-architectures",
    "href": "training.html#neural-network-architectures",
    "title": "9  Model Training",
    "section": "9.2 Neural Network Architectures",
    "text": "9.2 Neural Network Architectures\n\n9.2.1 Multilayer Perceptron (MLP)\n\n\n9.2.2 Convolutional Neural Networks\n\n\n9.2.3 Recurrent Neural Networks\n\n\n9.2.4 Transformers"
  },
  {
    "objectID": "training.html#back-propagation",
    "href": "training.html#back-propagation",
    "title": "9  Model Training",
    "section": "9.3 Back Propagation",
    "text": "9.3 Back Propagation"
  },
  {
    "objectID": "training.html#convergence",
    "href": "training.html#convergence",
    "title": "9  Model Training",
    "section": "9.4 Convergence",
    "text": "9.4 Convergence"
  },
  {
    "objectID": "training.html#overfitting-and-underfitting",
    "href": "training.html#overfitting-and-underfitting",
    "title": "9  Model Training",
    "section": "9.5 Overfitting and Underfitting",
    "text": "9.5 Overfitting and Underfitting"
  },
  {
    "objectID": "training.html#hyperparameters",
    "href": "training.html#hyperparameters",
    "title": "9  Model Training",
    "section": "9.6 Hyperparameters",
    "text": "9.6 Hyperparameters\n\n9.6.1 Epochs\n\n\n9.6.2 Learning Rate"
  },
  {
    "objectID": "training.html#transfer-learning",
    "href": "training.html#transfer-learning",
    "title": "9  Model Training",
    "section": "9.7 Transfer Learning",
    "text": "9.7 Transfer Learning\n\n9.7.1 Optimizer"
  },
  {
    "objectID": "training.html#summary",
    "href": "training.html#summary",
    "title": "9  Model Training",
    "section": "9.8 Summary",
    "text": "9.8 Summary"
  },
  {
    "objectID": "training.html#quiz",
    "href": "training.html#quiz",
    "title": "9  Model Training",
    "section": "9.9 Quiz",
    "text": "9.9 Quiz"
  },
  {
    "objectID": "efficient_ai.html",
    "href": "efficient_ai.html",
    "title": "10  Efficient AI",
    "section": "",
    "text": "This is an efficient test of a forked repo."
  },
  {
    "objectID": "optimizations.html#software-optimizations",
    "href": "optimizations.html#software-optimizations",
    "title": "11  Optimizations",
    "section": "11.1 Software Optimizations",
    "text": "11.1 Software Optimizations\n\n11.1.1 Compression\n\n\n11.1.2 Quantization\n\n\n11.1.3 Weight Pruning\n\n\n11.1.4 Knowledge Distillation"
  },
  {
    "objectID": "optimizations.html#hardware-optimizations",
    "href": "optimizations.html#hardware-optimizations",
    "title": "11  Optimizations",
    "section": "11.2 Hardware Optimizations",
    "text": "11.2 Hardware Optimizations\n\n11.2.1 GPUs\n\n\n11.2.2 TPUs\n\n\n11.2.3 NPUs"
  },
  {
    "objectID": "ondevice_learning.html#federated-learning",
    "href": "ondevice_learning.html#federated-learning",
    "title": "13  On-Device Learning",
    "section": "13.1 Federated Learning",
    "text": "13.1 Federated Learning"
  },
  {
    "objectID": "ondevice_learning.html#on-device-training",
    "href": "ondevice_learning.html#on-device-training",
    "title": "13  On-Device Learning",
    "section": "13.2 On-Device Training",
    "text": "13.2 On-Device Training\ncoming soon."
  },
  {
    "objectID": "hw_acceleration.html",
    "href": "hw_acceleration.html",
    "title": "14  Hardware Acceleration",
    "section": "",
    "text": "coming soon."
  },
  {
    "objectID": "privacy_security.html",
    "href": "privacy_security.html",
    "title": "16  Privacy and Security",
    "section": "",
    "text": "coming soon."
  },
  {
    "objectID": "sustainable_ai.html",
    "href": "sustainable_ai.html",
    "title": "17  AI Sustainability",
    "section": "",
    "text": "coming soon."
  },
  {
    "objectID": "responsible_ai.html",
    "href": "responsible_ai.html",
    "title": "18  Responsible AI",
    "section": "",
    "text": "coming soon."
  },
  {
    "objectID": "generative_ai.html",
    "href": "generative_ai.html",
    "title": "19  Generative AI",
    "section": "",
    "text": "coming soon."
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "ARM.com. “The Future Is Being Built on Arm: Market Diversification\nContinues to Drive Strong Royalty and Licensing Growth as Ecosystem\nReaches Quarter of a Trillion Chips Milestone – Arm®.” https://www.arm.com/company/news/2023/02/arm-announces-q3-fy22-results.\n\n\nJouppi, Norman P, Cliff Young, Nishant Patil, David Patterson, Gaurav\nAgrawal, Raminder Bajwa, Sarah Bates, et al. 2017. “In-Datacenter\nPerformance Analysis of a Tensor Processing Unit.” In\nProceedings of the 44th Annual International Symposium on Computer\nArchitecture, 1–12.\n\n\nRosenblatt, Frank. 1957. The Perceptron, a Perceiving and\nRecognizing Automaton Project Para. Cornell Aeronautical\nLaboratory.\n\n\nRumelhart, David E, Geoffrey E Hinton, and Ronald J Williams. 1986.\n“Learning Representations by Back-Propagating Errors.”\nNature 323 (6088): 533–36."
  },
  {
    "objectID": "resources.html#coding",
    "href": "resources.html#coding",
    "title": "Appendix B: Resources",
    "section": "B.1 Coding",
    "text": "B.1 Coding\n\nGitHub Description: There are various GitHub repositories dedicated to TinyML where you can contribute or learn from existing projects. Some popular organizations/repos to check out are:\n\nTensorFlow Lite Micro: GitHub Repository\nTinyML4D: GitHub Repository\n\nStack Overflow Tags: tinyml Description: Use the “tinyml” tag on Stack Overflow to ask technical questions and find answers from the community."
  },
  {
    "objectID": "resources.html#courses-and-learning-platforms",
    "href": "resources.html#courses-and-learning-platforms",
    "title": "Appendix B: Resources",
    "section": "B.2 Courses and Learning Platforms",
    "text": "B.2 Courses and Learning Platforms\n\nCoursera Course: Introduction to Embedded Machine Learning Description: A dedicated course on Coursera to learn the basics and advances of TinyML.\nEdX Course: Intro to TinyML Description: Learn about TinyML with this HarvardX course."
  },
  {
    "objectID": "community.html#online-forums",
    "href": "community.html#online-forums",
    "title": "Appendix C: Communities",
    "section": "C.1 Online Forums",
    "text": "C.1 Online Forums\n\nTinyML Forum Website: TinyML Forum Description: A dedicated forum for discussions, news, and updates on TinyML.\nReddit Subreddits: r/TinyML Description: Reddit community discussing various topics related to TinyML."
  },
  {
    "objectID": "community.html#blogs-and-websites",
    "href": "community.html#blogs-and-websites",
    "title": "Appendix C: Communities",
    "section": "C.2 Blogs and Websites",
    "text": "C.2 Blogs and Websites\n\nTinyML Foundation Website: TinyML Foundation Description: The official website offers a wealth of information including research, news, and events.\nEdge Impulse Blog Website: Blog Description: Contains several articles, tutorials, and resources on TinyML."
  },
  {
    "objectID": "community.html#social-media-groups",
    "href": "community.html#social-media-groups",
    "title": "Appendix C: Communities",
    "section": "C.3 Social Media Groups",
    "text": "C.3 Social Media Groups\n\nLinkedIn Groups Description: Join TinyML groups on LinkedIn to connect with professionals and enthusiasts in the field.\nTwitter Description: Follow TinyML enthusiasts, organizations, and experts on Twitter for the latest news and updates. Example handles to follow:\n\nTwitter\nEdgeImpulse"
  },
  {
    "objectID": "community.html#conferences-and-meetups",
    "href": "community.html#conferences-and-meetups",
    "title": "Appendix C: Communities",
    "section": "C.4 Conferences and Meetups",
    "text": "C.4 Conferences and Meetups\n\nTinyML Summit Website: TinyML Summit Description: Annual event where professionals and enthusiasts gather to discuss the latest developments in TinyML.\nMeetup Website: Meetup Description: Search for TinyML groups on Meetup to find local or virtual gatherings.\n\nRemember to always check the credibility and activity level of the platforms and groups before diving in to ensure a productive experience."
  },
  {
    "objectID": "case_studies.html",
    "href": "case_studies.html",
    "title": "Appendix D: Case Studies",
    "section": "",
    "text": "coming soon."
  }
]