[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Embedded AI: Principles, Algorithms, and Applications",
    "section": "",
    "text": "Preface\nIn “Embedded AI: Principles, Algorithms, and Applications”, we will embark on a critical exploration of the rapidly evolving field of artificial intelligence in the context of embedded systems, originally nurtured from the foundational course, tinyML from CS249r.\nThe goal of this book is to bring about a collaborative endeavor with insights and contributions from students, practitioners and the wider community, blossoming into a comprehensive guide that delves into the principles governing embedded AI and its myriad applications.\n\n“If you want to go fast, go alone, if you want to go far, go together.” – African Proverb\n\nAs a living document, this open-source textbook aims to bridge gaps and foster innovation by being globally accessible and continually updated, addressing the pressing need for a centralized resource in this dynamic field. With a rich tapestry of knowledge woven from various expert perspectives, readers can anticipate a guided journey that unveils the intricate dance between cutting-edge algorithms and the principles that ground them, paving the way for the next wave of technological transformation.\n\n\nThe Philosophy Behind the Book\nWe live in a world where technology perpetually reshapes itself, fostering an ecosystem of open collaboration and knowledge sharing stands as the cornerstone of innovation. This philosophy fuels the creation of “Embedded AI: Principles, Algorithms, and Applications.” This is a venture that transcends conventional textbook paradigms to foster a living repository of knowledge. Anchoring its content on principles, algorithms, and applications, the book aims to cultivate a deep-rooted understanding that empowers individuals to navigate the fluid landscape of embedded AI with agility and foresight. By embracing an open approach, we not only democratize learning but also pave avenues for fresh perspectives and iterative enhancements, thus fostering a community where knowledge is not confined but is nurtured to grow, adapt, and illuminate the path of progress in embedded AI technologies globally.\n\n\nPrerequisites\nVenturing into “Embedded AI: Principles, Algorithms, and Applications” does not mandate you to be a maestro in machine learning from the outset. At its core, this resource seeks to nurture learners who bear a fundamental understanding of systems and harbor a curiosity to explore the confluence of disparate, yet interconnected domains: embedded hardware, artificial intelligence, and software. This confluence forms a vibrant nexus where innovations and new knowledge streams emerge, making a basic grounding in system operations a pivotal tool in navigating this dynamic space.\nMoreover, the goal of this book is to delve into the synergies created at the intersection of these fields, fostering a learning environment where the boundaries of traditional disciplines blur to give way to a holistic, integrative approach to modern technological innovations. Your interest in unraveling embedded AI technologies and low-level software mechanics would be guiding you through a rich learning experience.\n\n\nConventions Used in This Book\nPlease follow the conventions listed in Conventions\n\n\nHow to Contact Us\nPlease contact vj@eecs.harvard.edu\n\n\nHow to Contribute\nPlease see instructions at here.\n\n\nContributors\nPlease see Credits."
  },
  {
    "objectID": "dedication.html",
    "href": "dedication.html",
    "title": "Dedication",
    "section": "",
    "text": "This book is a testament to the idea that, in the vast expanse of technology and innovation, it’s not always the largest systems, but the smallest ones, that can change the world."
  },
  {
    "objectID": "acknowledgements.html",
    "href": "acknowledgements.html",
    "title": "Acknowledgements",
    "section": "",
    "text": "To every endeavor, there lies a tapestry of effort, woven with threads of inspiration, dedication, and collaboration. This book, born out of a collective spirit, is no exception.\nFirst and foremost, gratitude must be extended to our ever-expanding community on GitHub. Each contributor, whether through a paragraph, a sentence, or a mere punctuation correction, has imbued this work with a wealth of knowledge and perspective. To each of you, your gift of time and expertise has not gone unnoticed or unappreciated. This book is as much yours as it is any single individual’s.\nSpecial thanks to Professor Vijay Janapa Reddi, whose vision planted the seed for this collaboration. Your unwavering faith in the power of open-source communities and your dedication to guiding this project to fruition have been the guiding star throughout.\nWe are also deeply indebted to the developers and staff at GitHub. Your platform has redefined what is possible in the world of collaboration, allowing disparate voices from across the globe to unite in a harmonious undertaking. This book stands as a testament to what can be achieved when barriers to entry are lowered, and voices are amplified.\nTo every reader who embarks on this journey with us, we hope this work enriches your understanding and inspires your curiosity. Without readers, words would hold no weight. We wrote for you, with the belief that shared knowledge is the keystone to progress.\nLastly, but by no means least, our gratitude extends to friends, families, mentors, and everyone who offered words of encouragement, late-night discussions, and unwavering support as this book transitioned from idea to reality.\nMay this collaborative effort serve as a beacon for what is possible when hearts and minds come together in the name of knowledge and progress."
  },
  {
    "objectID": "copyright.html",
    "href": "copyright.html",
    "title": "Copyright",
    "section": "",
    "text": "This book is open-source and developed collaboratively through GitHub. Unless otherwise stated, this work is licensed under the Creative Commons Attribution-ShareAlike 4.0 International License (CC BY-SA 4.0). You can find the full text of the license here.\nContributors to this project have dedicated their contributions to the public domain or under the same open license as the original project. While the contributions are collaborative, each contributor retains copyright in their respective contributions.\nFor details on authorship, contributions, and how to contribute, please see the project repository on GitHub.\nAll trademarks and registered trademarks mentioned in this book are the property of their respective owners.\nThe information provided in this book is believed to be accurate and reliable. However, the authors, editors, and publishers cannot be held liable for any damages caused or alleged to be caused either directly or indirectly by the information contained in this book."
  },
  {
    "objectID": "about.html#overview",
    "href": "about.html#overview",
    "title": "About This Book",
    "section": "Overview",
    "text": "Overview\nThis book is a collaborative effort started by the CS249r Tiny Machine Learning class at Harvard University. We intend for this book to become a community-driven effort to help educators and learners get started with TinyML. This living document will be continually updated as we continue to learn more about TinyML and how to teach it."
  },
  {
    "objectID": "about.html#topics-covered",
    "href": "about.html#topics-covered",
    "title": "About This Book",
    "section": "Topics Covered",
    "text": "Topics Covered\nThe book covers a wide range of topics related to embedded machine learning, providing a comprehensive understanding of the field. The topics covered include:\n\nOverview and Introduction to Embedded Machine Learning\nData Engineering\nEmbedded Machine Learning Frameworks\nEfficient Model Representation and Compression\nPerformance Metrics and Benchmarking of ML Systems\nLearning on the Edge\nHardware Acceleration for Edge ML: GPUs, TPUs, and FPGAs\nEmbedded MLOps\nSecure and Privacy-Preserving On-Device ML\nResponsible AI\nSustainability at the Edge\nGenerative AI at the Edge\n\nBy the end of this book, you will gain a brief introduction to machine learning and IoT. You will learn about real-world deployments of embedded machine learning systems. We hope you will also gain practical experience through hands-on project assignments."
  },
  {
    "objectID": "about.html#intended-audience",
    "href": "about.html#intended-audience",
    "title": "About This Book",
    "section": "Intended Audience",
    "text": "Intended Audience\nThis book is designed specifically for newcomers who wish to explore the fascinating and nascent world of tiny machine learning (tinyML). It provides the basic underpinnings of ML and embedded systems, and moves into more complex and broader topics relevant to both the tinyML and broader research community. More specifically, we believe the book will confer the following benefits to these groups of people:\n\nEmbedded Systems Engineers: This book is a valuable resource for engineers working in the field of embedded systems. It provides a solid foundation in TinyML, allowing them to design and implement intelligent applications on microcontrollers and other embedded platforms with limited resources.\nComputer Science and Electrical Engineering Students: Students pursuing degrees in computer science and electrical engineering can benefit from this book. It offers an introduction to the concepts, algorithms, and techniques used in TinyML, preparing students to tackle real-world challenges in the emerging field of embedded machine learning.\nResearchers and Academics: Researchers and academics in the field of machine learning, computer vision, and signal processing will find this book useful. It offers insights into the unique challenges of deploying machine learning algorithms on low-power, low-memory devices, enabling them to develop new approaches and advance the field of TinyML.\nIndustry Professionals: Professionals working in industries like IoT, robotics, wearable technology, and smart devices will find this book relevant. It equips them with the knowledge required to integrate machine learning capabilities into their products, enabling intelligent and autonomous behavior."
  },
  {
    "objectID": "about.html#key-takeaways",
    "href": "about.html#key-takeaways",
    "title": "About This Book",
    "section": "Key Takeaways",
    "text": "Key Takeaways\nUsers of this book will learn how to train and deploy deep neural network models on resource-constrained microcontrollers and the broader challenges associated with their design, development, deployment, and use.\n\nIntroduction to Machine Learning: A fundamental understanding of machine learning concepts, including supervised, unsupervised, and reinforcement learning.\nTinyML Fundamentals: Exploring the challenges and constraints associated with deploying machine learning on small, low-power devices.\nHardware Platforms: Coverage of popular microcontrollers and development boards specifically designed for TinyML applications, along with their architecture and specifications.\nTraining Models: Techniques and tools for training machine learning models suitable for embedded systems, including considerations for model size, accuracy, and resource utilization.\nOptimization Techniques: Strategies for model compression, quantization, and algorithmic optimization to ensure efficient execution on resource-constrained devices.\nReal-world Applications: Practical use cases and examples demonstrating the deployment of TinyML in various domains, such as industrial automation, healthcare, and environmental monitoring.\nChallenges and Future Trends: Discussion on the current challenges in TinyML, potential solutions, and emerging trends in the field.\n\nBy encompassing these aspects, our aim is to make this book a go-to resource for anyone interested in developing intelligent applications on embedded systems.\nAfter completing the chapters, readers will be empowered with the capabilities to design and implement their own ML-enabled projects, starting from defining a problem to gathering data and training the neural network model and finally deploying it to the device to display inference results or control other hardware appliances based on inference data."
  },
  {
    "objectID": "about.html#prerequisites",
    "href": "about.html#prerequisites",
    "title": "About This Book",
    "section": "Prerequisites",
    "text": "Prerequisites\n\nBasic Programming Knowledge: It is recommended that readers have some prior experience with programming, preferably in Python. Understanding variables, data types, control structures, and basic programming concepts will facilitate comprehension and engagement with the book.\nFamiliarity with Machine Learning Concepts: While not essential, a basic understanding of machine learning concepts, such as supervised and unsupervised learning, will help readers grasp the material more easily. However, the book provides sufficient explanations to bring readers up to speed if they are new to the field.\nPython Programming Skills (Optional): Readers with some Python programming experience will have an advantage when engaging with the coding portions of the book. Familiarity with libraries such as NumPy, scikit-learn, and TensorFlow will greatly facilitate the implementation and experimentation with machine learning models.\nLearning Mindset: The book has been structured to be accessible to a wide audience, including readers with varying levels of technical expertise. It provides a gradual learning curve, allowing readers to start with general knowledge about the field, progress to coding exercises, and potentially advance to deploying models on embedded devices. However, to fully benefit from the book, readers should be willing to challenge themselves and engage in practical exercises and projects.\nAvailability of Resources: To fully explore the practical aspects of TinyML, readers should have access to the necessary resources. These include a computer with Python and relevant libraries installed, as well as optional access to an embedded development board or microcontroller for experimenting with deploying machine learning models.\n\nBy ensuring that these general requirements are met, readers will have the opportunity to broaden their understanding of TinyML, gain hands-on experience with coding exercises, and even venture into practical implementation on embedded devices, enabling them to push the boundaries of their knowledge and skills."
  },
  {
    "objectID": "introduction.html#overview",
    "href": "introduction.html#overview",
    "title": "1  Introduction",
    "section": "1.1 Overview",
    "text": "1.1 Overview\nWe begin with an overall introduction to the field of embedd systems and machine learning. We start by elaborating on the key principles of embedded systems, setting the groundwork for embedded machine learning. Then we pivot our attention to deep learning, focusing specifically on deep learning methods given their representation capacity and overall performance in a variety of tasks, especially when applied to small devices.\nThe book goes on to discuss step-by-step workflows in machine learning, data engineering, pre-processing, and advanced model training techniques. It provides comprehensive analyses of several in-use machine learning frameworks, and how they can be employed effectively to develop efficient AI models.\nIn a world where efficiency is key, we also discuss TinyML model optimization and deployment strategies. Special focus is given to on-device learning. How do we train a machine learning model on a tiny device while achieving admirable efficiency? What are the current hardware acceleration techniques? And how can we manage the lifecycle of these models? The reader can expect exhaustive answers to these and many more questions in our dedicated chapters.\nImportantly, we adopt a forward-looking stance, discussing the sustainability and ecological footprint of AI. We explore the location of TinyML within such debates, and how TinyML may contribute to more sustainable and responsible practices.\nFinally, the book ends with a speculative leap into the world of generative AI, outlining its potentials in the TinyML context.\nWhether you are an absolute beginner, a professional in the field, or an academic pursuing rigorous research, this book aims to offer a seamless blend of essential theory and practical insight, triggering stimulating conversations around TinyML. Let’s embark on this thrilling journey to explore the incredible world of TinyML!"
  },
  {
    "objectID": "introduction.html#chapter-details",
    "href": "introduction.html#chapter-details",
    "title": "1  Introduction",
    "section": "1.2 Chapter Details",
    "text": "1.2 Chapter Details\nHere are additional details about each chapter that follows:\nChapter 1: Introduction\nWe are here! We begin our journey with a bird’s eye view of the embedded AI landscape and what is to come in the next chapters. We set the stage here by providing the readers with the background, contextual understanding, and the terminologies that will be recurrent throughout the book.\nChapter 2: Embedded Systems\nBefore delving deeper into the intricacies of AI, we acquaint ourselves with the basic framework of embedded systems, the platform where AI algorithms find their wide-ranging applications.\nChapter 3: Deep Learning Primer\nThis chapter serves as a primer on deep learning, providing a thorough understanding of the algorithms and principles that form the bedrock of AI applications in embedded systems.\nChapter 4: Embedded ML\nEmbedded Machine Learning (ML) stands as a cornerstone in our exploration. Here, we venture into the integration of ML techniques into embedded systems, opening avenues for intelligent and autonomous functionalities.\nChapter 5: ML Workflow\nWe dissect the workflow of machine learning, offering insights into the various stages that culminate in the development of proficient AI applications.\nChapter 6: Data Engineering\nData stands at the core of AI systems. This chapter elucidates the processes involved in harnessing, organizing, and managing data effectively to facilitate optimized AI functionalities.\nChapter 7: ML Frameworks\nHere, we explore the different frameworks available for developing machine learning models, providing a guide to selecting the most suitable one for your embedded AI projects.\nChapter 8: Model Training\nIn this chapter, we delve into the critical phase of model training, unraveling the techniques to develop models that are both efficient and reliable.\nChapter 9: Efficient AI\nEfficiency is the hallmark of successful AI integration. Here, we address the techniques and strategies to foster efficiency in AI applications, from optimizing computational resources to enhancing performance.\nChapter 10: Optimizations\nThis chapter presents the avenues available for optimizing AI models, ensuring they are streamlined for seamless integration into embedded systems.\nChapter 11: Deployment\nDeployment marks the fruition of the embedded AI development cycle. This section sheds light on the processes and considerations vital for successful deployment of AI models into embedded systems.\nChapter 12: On-Device Learning\nHere, we explore the frontiers of on-device learning, focusing on the techniques that facilitate localized learning, enhancing both efficiency and privacy.\nChapter 13: Hardware Acceleration\nThis chapter presents an insightful exposition on hardware acceleration, unraveling the role of specialized hardware in enhancing the performance and capabilities of embedded AI systems.\nChapter 14: MLOps\nMLOps stands as the backbone ensuring the smooth operation of AI systems. Here, we explore the processes involved in the seamless integration, monitoring, and maintenance of AI functionalities in embedded systems.\nChapter 15: Privacy and Security\nAs we move towards an era of ubiquitous AI, concerns about privacy and security take center stage. This chapter addresses the imperative measures and strategies to ensure the privacy and security of embedded AI systems.\nChapter 16: AI Sustainability\nSustainability is a critical aspect in the lifecycle of AI systems. In this chapter, we delve into the practices and strategies to foster sustainability, ensuring long-term viability and reduced environmental impact.\nChapter 17: Responsible AI\nResponsible AI advocates for the ethical development and deployment of AI systems. This chapter discusses the principles guiding the responsible use of AI, focusing on fairness, accountability, and transparency.\nChapter 18: Generative AI\nAs we conclude our journey, we venture into the captivating world of generative AI. Here, we explore the algorithms and techniques that drive the creation of new, synthetic data, opening avenues for innovation and creativity.\nAbsolutely, here is a guideline that readers might find beneficial to maximize the value they get from your book:"
  },
  {
    "objectID": "introduction.html#navigating-this-book",
    "href": "introduction.html#navigating-this-book",
    "title": "1  Introduction",
    "section": "1.3 Navigating this Book",
    "text": "1.3 Navigating this Book\nEmbarking on the journey through the riveting world of Embedded AI demands a strategic approach to fully grasp the intricate layers of knowledge encapsulated in this textbook. Here’s a structured pathway designed to help you glean the maximum value as you navigate through the chapters:\n\nFoundational Knowledge (Chapters 1-4): Begin your journey by building a strong foundation with a firm understanding of the basics outlined in the initial chapters. These chapters are crafted to provide you with the context and groundwork necessary for tackling the more advanced concepts in later sections.\nDeep Dive – Practical Insights (Chapters 5-14): Armed with foundational knowledge, steer your journey towards the deep-dive section. Here, focus on acquiring practical insights into the workflow of machine learning, data engineering, and optimizations that are pivotal in real-world applications. It’s advisable to immerse yourself in hands-on exercises, case studies, and projects present in these chapters to cement your understanding.\nCritical Reflections – Ethics and Sustainability (Chapters 15-17): As you approach the end of the deep dive, gear up for an engaging discourse on the ethical and sustainable practices in AI. These chapters provide a critical lens through which to view the implications of AI technologies, fostering a responsible approach to AI deployment.\nInnovation and Future Trends (Chapter 18): Round off your exploration with a foray into the enthralling domain of generative AI. This final chapter offers a glimpse into the future, encouraging you to think innovatively and explore the burgeoning opportunities in the field.\nInterconnected Learning: While the chapters are arranged to facilitate a progressive learning curve, seasoned professionals might choose to navigate through the chapters non-linearly, focusing on areas most pertinent to their field or interest. The book is designed to allow for such flexibility, catering to both novices and experts alike.\nPractical Applications: Throughout your reading journey, consistently try to relate the theoretical knowledge acquired to real-world applications. Engage with the practical exercises, simulations, and case studies that pepper the textbook, which are designed to provide a hands-on experience, bridging the gap between theory and practice.\nDiscussion and Networking: Foster a collaborative learning experience by engaging in discussions, forums, or study groups. Sharing insights and debating concepts with peers can often unveil new perspectives and deepen your understanding.\nRevisit and Reflect: The dynamic field of AI means that there is always room for further exploration. Don’t hesitate to revisit chapters, as a second reading can often offer new insights, fostering a cycle of continuous learning.\n\nBy adopting a structured yet flexible approach to navigating this book, you are setting the stage for a fulfilling and enriching learning experience. Harness your curiosity and eagerness to explore as you traverse the captivating landscape of Embedded AI, ready to contribute meaningfully to this ever-evolving field.\nCertainly! Here’s a guide on how readers can best leverage the diverse array of resources included in your book for an engaging and interactive learning experience:"
  },
  {
    "objectID": "introduction.html#learning-experience",
    "href": "introduction.html#learning-experience",
    "title": "1  Introduction",
    "section": "1.4 Learning Experience",
    "text": "1.4 Learning Experience\nWe live in the contemporary era of learning with a rich and varied tapestry of resources that can significantly enhance the learning trajectory. Therefore, this book is thoughtfully crafted to be a dynamic amalgamation of various learning mediums – videos, notes, coding exercises, collaborative notebooks (Colabs), and more.\nHere’s how you can make the most of these resources:\n\nIntegrative Learning: Alternate between reading the notes and watching the accompanying videos to get a well-rounded understanding of the concepts. The videos provide visual and auditory insights that complement the detailed explanations in the notes, offering an integrative learning experience.\nHands-On Coding Exercises: Engage with the coding exercises that offer a practical approach to understanding the algorithms and methodologies discussed in the chapters. Through these exercises, you can apply the theoretical knowledge garnered in real-time, fostering a deeper understanding of the subject matter. Utilize the Colabs to collaborate with peers or instructors. These platforms serve as interactive environments where you can run code, share your findings, and engage in fruitful discussions, fostering a community learning experience.\nResourceful Links: Don’t skip the external links provided in the chapters. These links are curated to offer additional insights, deeper explorations, and supplementary knowledge that can widen your understanding of the topics at hand.\nProject-Based Learning: Engage with the projects and case studies woven into the textbook. These allow you to work on real-world challenges, offering a glimpse into the practical applications of embedded AI, and fostering problem-solving skills.\nFeedback and Reflection: Utilize the interactive elements of the book to constantly assess your understanding. Engage in quizzes, feedback sessions, and reflective activities that encourage you to think critically and consolidate your learning.\nCommunity Engagement: Participate in forums and discussion groups linked within the book. These platforms offer a space to network with like-minded individuals, share your projects, and receive feedback, fostering a vibrant learning community.\nSequential & Selective Approach: While a sequential approach to the chapters would offer a progressive learning curve, feel free to navigate selectively to topics of your interest. The multifaceted resources cater to learners with varied preferences and needs, allowing for a personalized learning pathway.\nConsistent Practice: The field of AI demands consistent practice. Revisit the resources, undertake various coding challenges, and continuously engage with the interactive elements to hone your skills over time.\n\nRemember, the goal is to foster a deep and lasting understanding, and the diverse resources at your disposal are designed to facilitate just that. Approach this book not just as a learning resource, but as an engaging tool that nurtures curiosity, encourages practical application, and fosters a spirit of innovation and collaboration.\n\nI hope this guide provides a pathway for readers to have an enriching and interactive learning experience!"
  },
  {
    "objectID": "introduction.html#the-road-ahead",
    "href": "introduction.html#the-road-ahead",
    "title": "1  Introduction",
    "section": "1.5 The Road Ahead",
    "text": "1.5 The Road Ahead\nAs we embark on this intellectual expedition, it will become evident that there is a substantial ground to cover - a testament to the sheer scope encompassed in the field of embedded AI. Yet, it is precisely this multidisciplinary convergence that infuses the subject with an exhilarating richness and depth that keeps us up at night (in a good way)! This textbook aims to be a resource, showing the intricate pathways that lead to the successful integration of AI into embedded systems.\nWe are standing at the cusp of an era where boundaries are continuously redefined, fostering a synergistic marriage of computational theories, engineering principles, ethical considerations, and innovative applications. Each chapter in this textbook unveils a fragment of this expansive mosaic, inviting readers to forge new connections in their minds, ignite discussions in classrooms, and fuel a perpetual curiosity about embedded AI. Together, we will navigate this fascinating intertwining of domains, witnessing firsthand the remarkable innovations that are not only reshaping embedded systems but also redrawing the contours of our technological future.\nJoin us as we traverse through the journey of embedded AI, setting the stage for a future where machines complement human endeavors with unprecedented intelligence and efficiency in a seamless and ubiquitous manner."
  },
  {
    "objectID": "introduction.html#contribute-back",
    "href": "introduction.html#contribute-back",
    "title": "1  Introduction",
    "section": "1.6 Contribute Back",
    "text": "1.6 Contribute Back\nIn the rapidly evolving landscape of embedded AI, learning is not a solitary endeavor but a collaborative journey. This book is designed not only as a repository of knowledge but also as a catalyst to nurture a vibrant community of learners, innovators, and contributors. We firmly believe that the collective wisdom and collaboration of a community can significantly enhance the depth and breadth of understanding in this field.\nAs you navigate through the concepts, case studies, and interactive exercises, we encourage you to actively share your insights, discoveries, and experiences. Whether it is a novel approach to a problem, an interesting application of a concept, or a question that provokes deeper reflection, your contribution can be a valuable asset to the learning ecosystem. Harness the power of community through discussion forums, collaborative platforms, and social networks integrated within this book. Engage in meaningful dialogues, offer and seek guidance, and collaborate on projects, fostering a culture of mutual growth and learning. Through sharing, you not only consolidate your understanding but potentially ignite sparks of inspiration in others.\nFurthermore, we encourage you to extend the collaborative spirit beyond the confines of this book. Share what you have learned with your local and global communities. Engage in mentorship, organize workshops, or contribute to online discussions. By disseminating knowledge, you play a pivotal role in fostering a globally connected, informed, and empowered community.\nAs the adage goes, “knowledge increases by sharing, not by saving.” In this spirit, let us embark on this learning journey with open minds and generous hearts, ready to learn from one another and contribute towards the collective wisdom of the community. If you spot any mistakes, please feel free to issue pull requests to the GitHub repository. Together, we can push the boundaries of what is possible in the realm of embedded AI, nurturing a future where technology is shaped by a diverse set of voices and perspectives, each adding value to the rich narrative of innovation and progress."
  },
  {
    "objectID": "embedded_sys.html#basics-and-components",
    "href": "embedded_sys.html#basics-and-components",
    "title": "2  Embedded Systems",
    "section": "2.1 Basics and Components",
    "text": "2.1 Basics and Components\n\n2.1.1 Definition and Characteristics\nEmbedded systems are specialized computing systems that do not look like computers. They are dedicated to specific tasks and “embed” as part of a larger device. Unlike general-purpose computers that can run a wide variety of applications, embedded systems perform pre-defined tasks, often with very specific requirements. Since they are task-specific, their design ensures optimized performance and reliability. The characteristics that define these systems are as follows:\n\nDedicated Functionality: They are designed to execute a specific function or a set of closely related functions. This focus on specific tasks allows them to be optimized, offering faster performance and reliability.\nReal-Time Operation: Many embedded systems operate in real-time, which means they are required to respond to inputs or changes in the environment immediately or within a predetermined time frame.\nIntegration with Physical Hardware: Embedded systems are closely integrated with physical hardware, making them more mechanically inclined compared to general-purpose computing systems.\nLong Lifecycle: These systems typically have a long lifecycle and can continue to function for many years after their initial deployment.\nResource Constraints: Embedded systems are often resource-constrained, operating with limited computational power and memory. This necessitates the development of efficient algorithms and software.\n\n\n\n2.1.2 Historical Background\nEmbedded systems have a rich history, with their roots tracing back to the 1960s when the first microprocessor, the Figure 2.1, made its debut. This paved the way for the creation of the first embedded system which was used in the Apollo Guidance Computer, the primary navigation system of the Apollo spacecraft. Over the years, the field has evolved dramatically, finding applications in various domains including automotive electronics, consumer electronics, telecommunications, and healthcare, among others.\n\n\n\nFigure 2.1: Intel 4004\n\n\n\n\n2.1.3 Importance in tinyML\nIn the context of tinyML, embedded systems represent a significant frontier. The incorporation of machine learning models directly onto these systems facilitates intelligent decision-making at the edge, reducing latency and enhancing security. Here are several reasons why embedded systems are critical in the tinyML landscape:\n\nEdge Computing: By bringing computation closer to the data source, embedded systems enhance efficiency and reduce the necessity for constant communication with centralized data centers.\nLow Power Consumption: Embedded systems in tinyML are designed to consume minimal power, a critical requirement for battery-operated devices and IoT applications.\nReal-Time Analysis and Decision Making: Embedded systems can facilitate real-time data analysis, allowing for immediate decision-making based on the insights generated.\nSecurity and Privacy: Processing data locally on embedded systems ensures better security and privacy, as it reduces the chances of data interception during transmission.\nCost-Effective: Implementing ML models on embedded systems can be cost-effective, especially in scenarios where data transmission and storage in cloud servers might incur significant costs.\n\nAs we venture deeper into this chapter, we will unveil the intricacies that govern the functioning of embedded systems and explore how they form the bedrock upon which tinyML stands, promising a future of integrated, intelligent, and efficient devices and systems."
  },
  {
    "objectID": "embedded_sys.html#embedded-system-architecture",
    "href": "embedded_sys.html#embedded-system-architecture",
    "title": "2  Embedded Systems",
    "section": "2.2 Embedded System Architecture",
    "text": "2.2 Embedded System Architecture\nThe architecture of embedded systems forms the blueprint that delineates the structure and functioning of these specialized systems. It provides insights into how different components within an embedded system interact and collaborate to achieve specific functionalities. This section dissects the integral components of the architecture - microcontrollers, microprocessors, different memory types and their management, and the intricacies of System on Chip (SoC).\n\n2.2.1 Microcontrollers vs Microprocessors\nUnderstanding the difference between microcontrollers and microprocessors is pivotal to grasping the fundamentals of embedded system architecture. Here, we delve into the characteristics of both:\n\nMicrocontrollers\nMicrocontrollers are compact, integrated circuits designed to govern specific operations in an embedded system. They house a processor, memory, and input/output peripherals in a single unit as shown in Figure 2.2, facilitating simplicity and ease of operation. Microcontrollers are typically used in products where the computational requirements are not highly demanding, and cost-effectiveness is a priority.\nCharacteristics:\n\nSingle-chip solution\nOn-chip memory and peripherals\nLow power consumption\nIdeal for control-oriented applications\n\n\n\n\n\nFigure 2.2: Microcontrollers\n\n\n\nMicroprocessors\nOn the other hand, microprocessors are more complex, forming the central processing unit within a system, lacking the integrated memory and I/O peripherals found in microcontrollers. They are usually found in systems that demand higher computational power and flexibility. These are used in devices where substantial processing power is required, and the tasks are more data-intensive.\nCharacteristics:\n\nRequires external components such as memory and I/O peripherals\nHigher processing power compared to microcontrollers\nMore flexible in terms of connectivity with various components\nIdeal for data-intensive applications\n\n\n\n\n2.2.2 Memory Types and Management\nEmbedded systems leverage various types of memory, each serving distinct purposes. Effective memory management is crucial to optimize performance and resource utilization. Below we discuss different memory types and how they are managed in an embedded system environment:\n\nROM (Read-Only Memory): This is non-volatile memory where data is written during manufacturing and remains unchanged throughout the device’s life. It stores the firmware and boot-up instructions.\nRAM (Random Access Memory): A volatile memory used to store temporary data generated during the system’s operation. It is faster and allows read-write operations, but data is lost once power is turned off.\nFlash Memory: A non-volatile memory type that can be electrically erased and reprogrammed. It finds applications in storing firmware or data that needs to persist between reboots.\n\nMemory Management:\n\nStatic Memory Allocation: Memory is allocated before runtime, and the allocation does not change during the system’s operation.\nDynamic Memory Allocation: Memory is allocated at runtime, allowing flexibility but at the cost of increased complexity and potential memory leaks.\n\n\n\n2.2.3 System on Chip (SoC)\nMost embedded systems are SoCs. A System on Chip (SoC) represents an advanced integration technology where most of the components required to build a complete system are integrated onto a single chip. It usually contains a microprocessor or microcontroller, memory blocks, peripheral interfaces, and other components required for a fully functioning system. Here’s a deeper look at its characteristics and applications:\n\nIntegration of Multiple Components: SoCs house multiple components, including CPUs, memory, and peripherals, in a single chip, promoting higher integration levels and minimizing the need for external components.\nPower Efficiency: Due to the high level of integration, SoCs are often more power-efficient compared to systems built using separate chips.\nCost-Effectiveness: The integration leads to reduced manufacturing costs, as fewer separate components are required.\nApplications: SoCs find applications in a variety of domains, including mobile computing, automotive electronics, and IoT devices, where compact size and power efficiency are prized.\n\nHere are some examples of widely used SoCs that you may recognize given that they have found substantial applications across various domains:\n\nQualcomm Snapdragon: Predominantly found in smartphones and tablets, they offer a combination of processing power, graphics, and connectivity solutions.\nApple A-series: Custom SoCs developed by Apple, utilized in their range of iPhones, iPads, and even in some versions of Apple TV and HomePod. Notable examples include the A14 Bionic and A15 Bionic chips.\nSamsung Exynos: Developed by Samsung, these are utilized extensively in their range of smartphones, tablets, and other electronic devices.\nNVIDIA Tegra: Initially designed for mobile devices, they have found substantial applications in automotive and gaming consoles, like the Nintendo Switch. You can see a picture of it below in Figure 2.3.\nIntel Atom: These are used in a wide variety of systems including netbooks, smartphones, and even embedded systems owing to their power efficiency.\nMediaTek Helio: Popular in budget to mid-range smartphones, these chips offer a good balance of power efficiency and performance.\nBroadcom SoCs: Used extensively in networking equipment, Broadcom offers a range of SoCs with different functionalities including those optimized for wireless communications and data processing.\nTexas Instruments (TI) OMAP: These were popular in smartphones and tablets, offering a range of functionalities including multimedia processing and connectivity.\nXilinx Zynq: Predominantly used in embedded systems for industrial automation and for applications demanding high levels of data processing, such as advanced driver-assistance systems (ADAS).\nAltera SoC FPGA: Now under Intel, these SoCs integrate FPGA technology with ARM cores, offering flexibility and performance for various applications including automotive and industrial systems.\n\n\n\n\nFigure 2.3: NVIDIA’s Tegra 2 combines two ARM Cortex-A9 cores with an ARM7 for SoC management tasks.\n\n\nEach of these SoCs presents a unique set of features and capabilities, catering to the diverse needs of the rapidly evolving technology landscape. They integrate multiple components into a single chip, offering power efficiency, cost-effectiveness, and compact solutions for modern electronic devices."
  },
  {
    "objectID": "embedded_sys.html#embedded-system-programming",
    "href": "embedded_sys.html#embedded-system-programming",
    "title": "2  Embedded Systems",
    "section": "2.3 Embedded System Programming",
    "text": "2.3 Embedded System Programming\nEmbedded systems programming diverges considerably from traditional software development, specifically honed to navigate the limited resources and the real-time requirements frequently associated with embedded hardware. This section will illuminate the nuances of the different programming languages utilized, delve into the intricacies of firmware development, and explore the critical role of Real-time Operating Systems (RTOS) in this specialized field.\n\n2.3.1 Programming Languages: C, C++, Python, etc.\nThe selection of appropriate programming languages is crucial in embedded systems, often prioritizing direct hardware interaction and optimization of memory usage. Let us explore the specifics of these languages and how they stand apart from those typically utilized in more conventional systems:\n\nC: Traditionally the cornerstone of embedded systems programming, the C language facilitates direct interaction with hardware components, offering capabilities for bit-wise operations and manipulating memory addresses. Its procedural approach and low-level capabilities make it the favored choice for constrained environments, focusing on firmware development.\nC++: Building on the foundation laid by C, C++ integrates object-oriented principles, fostering organized and modular code development. Despite its inherent complexity, it is embraced in scenarios where higher-level abstractions do not compromise the granular control provided by C.\nPython: While not a classic choice for embedded systems due to its relative memory consumption and runtime delays, Python is finding its place in the embedded domain, especially in systems where resource constraints are less stringent. In recent times, a variant known as MicroPython has emerged, specifically tailored for microcontrollers. MicroPython retains the simplicity and ease of use of Python while being optimized for embedded environments, offering a flexible programming paradigm that facilitates rapid prototyping and development. For instance, the code snipped below shows how we can use MicroPython to interface with the pins on a PyBoard.\n\nimport pyb # Package from PyBoard\n\n# turn on an LED\npyb.LED(1).on()\n\n# print some text to the serial console\nprint('Hello MicroPython!')\nComparison with Traditional Systems: In stark contrast to conventional systems, where languages like Java, Python, or JavaScript are celebrated for their development ease and comprehensive libraries, embedded systems are geared towards languages that offer refined control over hardware components and potential optimization opportunities, carefully navigating the limited resources at their disposal.\n\n\n2.3.2 Firmware Development\nThe realm of firmware development within embedded systems encompasses crafting programs permanently stored in the non-volatile memory of hardware, thus ensuring persistent operation. Here, we delineate how it distinguishes itself from software development for traditional systems:\n\nResource Optimization: The necessity for constant optimization is paramount, enabling the code to function within the confines of restricted memory and processing capacities.\nHardware Interaction: Firmware typically exhibits a close-knit relationship with hardware, necessitating a profound comprehension of the hardware components and their functionalities.\nLifecycle Management: Firmware updates are less frequent compared to software updates in traditional systems, mandating rigorous testing procedures to avert failures that could culminate in hardware malfunctions.\nSecurity Concerns: Given its integral role, firmware is a potential target for security breaches, warranting meticulous scrutiny towards security elements, including secure coding practices and encryption protocols.\n\n\n\n2.3.3 Real-time Operating Systems (RTOS)\nRTOS serve as the backbone for real-time systems, orchestrating task execution in a predictable, deterministic manner. This is a sharp deviation from the operating systems in mainstream computing environments, as delineated below:\n\nDeterministic Timing: RTOS are structured to respond to inputs or events within a well-defined timeframe, meeting the critical time-sensitive requisites of many embedded systems.\nTask Prioritization: They facilitate task prioritization, where critical tasks are accorded precedence in processing time allocation over less vital tasks.\nMicrokernel Architecture: A substantial number of RTOS leverage a microkernel architecture, epitomizing minimalism and efficiency by focusing only on the necessary functionalities to facilitate their operations.\nMemory Management: Memory management in RTOS is often more streamlined compared to their counterparts in traditional operating systems, aiding in swift response times and operational efficacy.\n\nExamples of RTOS: Notable examples in this category include FreeRTOS, RTEMS, and VxWorks, each offering unique features tailored to meet the diverse requirements of various embedded systems applications."
  },
  {
    "objectID": "embedded_sys.html#interfaces-and-peripherals",
    "href": "embedded_sys.html#interfaces-and-peripherals",
    "title": "2  Embedded Systems",
    "section": "2.4 Interfaces and Peripherals",
    "text": "2.4 Interfaces and Peripherals\nEmbedded systems interact with the external world through various interfaces and peripherals, which are distinctly streamlined and specialized compared to general-purpose systems. Let’s delve into the specifics:\n\n2.4.1 Digital I/O\nDigital Input/Output (I/O) interfaces are foundational in embedded systems, allowing them to interact with other devices and components. For example, a digital I/O pin can be used to read a binary signal (0 or 1) from sensors or to control actuators.\nIn embedded systems, these I/O ports often need to function under strict timing constraints, something which is less prevalent in general-purpose computing systems. Furthermore, these systems are usually programmed to perform specific, optimized operations on digital signals, sometimes needing to work in real time or near-real-time environments.\n\n\n2.4.2 Analog Interfaces\nAnalog interfaces in embedded systems are crucial for interacting with a world that largely communicates in analog signals. These interfaces can include components like Analog-to-Digital Converters (ADCs) and Digital-to-Analog Converters (DACs). ADCs, for instance, can be used to read sensor data from environmental sensors like temperature or humidity sensors, translating real-world analog data into a digital format that can be processed by the microcontroller.\nCompared to general-purpose systems, embedded systems might employ analog interfaces in a more direct and frequent manner, especially in applications involving sensor integrations, which necessitate the conversion of a wide variety of analog signals to digital data for processing and analysis.\nIf you look closely enough in Figure 2.4, you will see there are indications of I/O pinouts for analog, digital, as well as communication layouts.\n\n\n\nFigure 2.4: Nicla Vision pinout\n\n\n\n\n2.4.3 Communication Protocols (SPI, I2C, UART, etc.)\nCommunication protocols serve as the conduits for facilitating communication between various components within or connected to an embedded system. Let’s explore a few widely adopted ones:\n\nSPI (Serial Peripheral Interface): This is a synchronous serial communication protocol, which is used for short-distance communication primarily in embedded systems. For example, it is often utilized in SD card and TFT display communications.\nI2C (Inter-Integrated Circuit): This is a multi-master, multi-slave, packet switched, single-ended, serial communication bus, which is used widely in embedded systems to connect low-speed peripherals to a motherboard, embedded system, or cellphone. It’s known for its simplicity and low pin count.\nUART (Universal Asynchronous Receiver-Transmitter): This communication protocol allows for asynchronous serial communication between devices. It’s widely used in embedded systems to transmit data between devices over a serial port, facilitating the transmission of data logs from a sensor node to a computer, for instance.\n\nCompared to general-purpose systems, communication protocols in embedded systems are often more optimized for speed and reliability, as they may be used in critical applications where data transmission integrity is paramount. Moreover, they might be directly integrated into the microcontroller, emphasizing a more harmonized and seamless interaction between components, which is typically not observed in general-purpose systems."
  },
  {
    "objectID": "embedded_sys.html#power-management-in-embedded-systems",
    "href": "embedded_sys.html#power-management-in-embedded-systems",
    "title": "2  Embedded Systems",
    "section": "2.5 Power Management in Embedded Systems",
    "text": "2.5 Power Management in Embedded Systems\nWhen engineering embedded systems, power management emerges as a pivotal focus area, shaping not only the system’s efficiency but also its viability in real-world applications. The sheer diversity in the deployment of embedded systems, ranging from handheld devices to industrial machinery, underscores the imperative to optimize power management meticulously. Let’s delve into this critical facet of embedded systems:\n\n2.5.1 Power Consumption Considerations\nIn embedded systems, power consumption is a vital parameter that governs the system’s performance and lifespan. Typically, microcontrollers in these systems operate in the range of 1.8V to 5V, with current consumption being in the microampere (μA) to milliampere (mA) range during active modes. In sleep or standby modes, the current consumption can plummet to nanoamperes (nA), ensuring battery longevity.\nComparatively, general-purpose computing systems, like personal computers, consume power in the order of tens to hundreds of watts, which is several orders of magnitude higher. This stark contrast delineates the necessity for meticulous power management in embedded systems, where the available power budget is often significantly restrained.\nThe intricacies of managing power consumption hinge on a variety of factors including the operating voltage, clock frequency, and the specific tasks being performed by the system. Often, engineers find themselves navigating a complex trade-off landscape, balancing power consumption against system performance and responsiveness.\n\n\n2.5.2 Energy-Efficient Design\nEmbedding energy efficiency into the design phase is integral to the successful deployment of embedded systems. Engineers often employ techniques such as dynamic voltage and frequency scaling (DVFS), which allows the system to adjust the voltage and frequency dynamically based on the processing requirements, thereby optimizing power consumption.\nAdditionally, leveraging low-power modes where non-essential peripherals are turned off or clock frequencies are reduced can significantly conserve power. For instance, utilizing deep sleep modes where the system consumes as little as 100 nA can dramatically enhance battery life, especially in battery-powered embedded systems.\nIn embedded systems, energy-efficient design isn’t confined to just power-saving modes and techniques like Dynamic Voltage and Frequency Scaling (DVFS); it extends fundamentally to the architecture of the microcontroller itself, particularly in its instruction set architecture (ISA).\nThe microcontroller instruction set architecture (ISA) in embedded systems is often highly specialized, stripped of any unnecessary complexities that might add to power consumption. This specialization facilitates executing operations using a smaller number of cycles compared to general-purpose processors, which, in turn, reduces the power consumption per operation. Moreover, these specialized ISAs are crafted to efficiently execute the specific types of tasks that the embedded system is designed to perform, optimizing the execution path and thereby conserving energy.\nFurthermore, it’s not uncommon to find RISC (Reduced Instruction Set Computer) architectures in embedded systems. These architectures utilize a smaller set of simple instructions compared to Complex Instruction Set Computer (CISC) architectures found in traditional general-purpose systems. This design choice significantly reduces the power consumed per instruction, making these microcontrollers inherently more energy-efficient.\nApart from ISAs, embedded microcontrollers are often integrated with peripherals and components that are tailored to exhibit minimal energy expenditure, further reinforcing the emphasis on energy efficiency. Through careful design, engineers can craft systems that harmoniously integrate performance requirements with power management strategies, crafting solutions that stand as testimony to innovation and sustainability in the field of embedded systems. This meticulous approach to design, focusing on both macro and micro-level optimizations, forms the bedrock of energy efficiency in embedded systems, differentiating them from their general-purpose counterparts which are often characterized by higher power consumption and a broader range of functionalities.\nBy focusing on these elements, engineers can forge pathways to create systems that not only fulfill their functional roles but do so with an acumen that reflects a deep understanding of the broader impacts of technology on society and the environment.\n\n\n2.5.3 Battery Management\nBattery management constitutes a vital part of power management strategies in embedded systems. The objective here is to maximize battery life without compromising system performance. Battery-powered embedded systems often employ lithium-ion or lithium-polymer batteries due to their high energy density and rechargeable nature. These batteries usually have a voltage range of 3.7V to 4.2V per cell. For instance, the Nicla Vision uses 3.7V battery as shown in Figure 2.5.\n\n\n\nFigure 2.5: Nicla Vision battery.\n\n\nEngineers need to incorporate strategies like efficient charge management, overvoltage protection, and temperature monitoring to safeguard the battery’s health and prolong its lifespan. Moreover, developing systems that can harvest energy from the environment, like solar or vibrational energy, can supplement battery power and create sustainable, long-lasting solutions.\nThe focus on power management stems from the necessity to optimize resource utilization, extend battery life, and reduce operational costs. In deployments where systems are remote or inaccessible, efficient power management can significantly reduce the need for maintenance interventions, thereby ensuring sustained, uninterrupted operation.\nOne could say that power management in embedded systems is not just a technical requirement but a critical enabler that can dictate the success or failure of a deployment. Engineers invest significantly in optimizing power management strategies to craft systems that are not only efficient but also sustainable, showcasing a deep-seated commitment to innovation and excellence in the embedded systems domain."
  },
  {
    "objectID": "embedded_sys.html#real-time-characteristics",
    "href": "embedded_sys.html#real-time-characteristics",
    "title": "2  Embedded Systems",
    "section": "2.6 Real-Time Characteristics",
    "text": "2.6 Real-Time Characteristics\nIn the intricate fabric of embedded systems, the real-time characteristics stand as defining threads, weaving together components and tasks into a coherent, responsive entity. This facet, which is often unique to embedded systems, holds a critical place in the architecture and operation of these systems, providing them with the agility and precision to interact with their environment in a timely manner. Let’s explore the intricacies that underline the real-time characteristics of embedded systems:\n\n2.6.1 Real-time Clocks\nReal-time clocks (RTCs) play a pivotal role in embedded systems, providing a precise time reference that governs the operations of the system. These clocks often have battery backups to ensure consistent timekeeping even when the main power source is unavailable. The utilization of RTCs is far more prevalent and critical in embedded systems compared to general-purpose computing systems, where timekeeping, although necessary, often doesn’t dictate the system’s core functionality.\nFor instance, in industrial automation systems, RTCs help in coordinating tasks with high precision, ensuring that processes occur in sync and without delay. They find significant applications in systems where time-stamped data logging is necessary, such as in environmental monitoring systems where data accuracy and time correlation are vital.\n\n\n2.6.2 Timing and Synchronization\nTiming and synchronization are hallmarks of embedded systems, where multiple components and processes need to work in harmony. The very essence of a real-time embedded system is dictated by its ability to perform tasks within a defined time frame. These systems usually have stringent timing requirements, demanding synchronization mechanisms that are both robust and precise.\nFor example, in automotive control systems, the timely and synchronized functioning of various sensors and actuators is non-negotiable to ensure safety and optimal performance. This is a stark contrast to general-purpose systems, where timing, although managed, doesn’t often have immediate and critical repercussions.\n\n\n2.6.3 Task Management and Scheduling\nIn embedded systems, task management and scheduling are critical to ensuring that the system can respond to real-time events effectively. Task schedulers in these systems might employ strategies such as priority scheduling, where tasks are assigned priority levels, and higher-priority tasks are allowed to pre-empt lower-priority tasks. This is particularly vital in systems where certain operations have a higher criticality.\nFor instance, in medical devices like pacemakers, the timely delivery of electrical pulses is a critical task, and the scheduling mechanism must prioritize this above all other tasks to ensure the patient’s safety. This finely tuned scheduling and task management is quite unique to embedded systems, distinguishing them markedly from the more flexible and less deterministic scheduling observed in general-purpose systems.\n\n\n2.6.4 Error Handling and Fault Tolerance\nTo further bolster their real-time characteristics, embedded systems often feature mechanisms for error handling and fault tolerance. These are designed to quickly identify and correct errors, or to maintain system operation even in the face of faults. In aviation control systems, for example, real-time fault tolerance is crucial to maintain flight stability and safety in drones. This level of meticulous error handling is somewhat distinctive to embedded systems compared to general-purpose systems, highlighting the critical nature of many embedded system applications.\nThe real-time characteristics of embedded systems set them apart, crafting a landscape where precision, synchrony, and timely responses are not just desired but mandatory. These characteristics find resonance in myriad applications, from automotive control systems to industrial automation and healthcare devices, underscoring the embedded systems’ role as silent, yet powerful, orchestrators of a technologically harmonized world. Through their real-time attributes, embedded systems are able to deliver solutions that not only meet the functional requirements but do so with a level of precision and reliability that is both remarkable and indispensable in the contemporary world. ## Security and Reliability\nIn a world that is ever-increasingly connected and reliant on technology, the topics of security and reliability have vaulted to the forefront of concerns in system design. Particularly in the realm of embedded systems, where these units are often integral parts in critical infrastructures and applications, the stakes are exponentially higher. Let’s delve into the vital aspects that uphold the fortress of security and reliability in embedded systems:"
  },
  {
    "objectID": "embedded_sys.html#security-and-reliability",
    "href": "embedded_sys.html#security-and-reliability",
    "title": "2  Embedded Systems",
    "section": "2.7 Security and Reliability",
    "text": "2.7 Security and Reliability\n\n2.7.1 Secure Boot and Root of Trust\nAs embedded systems find themselves at the heart of numerous critical applications, ensuring the authenticity and integrity of the system right from the moment of booting is paramount. The secure boot process is a cornerstone in this security paradigm, allowing the system to only execute code that is verified and trusted. This mechanism is often complemented by a “Root of Trust,” an immutable and trusted environment, usually hardware-based, that validates the initial firmware and subsequent software layers during the boot process.\nFor instance, in financial transactions using Point-of-Sale (POS) terminals, a secure boot process ensures that the firmware is unaltered and secure, thwarting attempts of malicious firmware alterations which can potentially lead to significant data breaches. Similarly, in home automation systems, a robust secure boot process prevents unauthorized access, safeguarding user privacy and data.\n\n\n2.7.2 Fault Tolerance\nFault tolerance is an indispensable attribute in embedded systems, bestowing the system with the resilience to continue operations even in the presence of faults or failures. This is achieved through various mechanisms like redundancy, where critical components are duplicated to take over in case of a failure, or through advanced error detection and correction techniques.\nIn applications such as aerospace and aviation, fault tolerance is not just a desirable feature but a mandatory requirement. Aircraft control systems, for instance, employ multiple redundant systems operating in parallel, ensuring continuous operation even in the case of a component failure. This level of fault tolerance ensures a high degree of reliability, making sure that the system can withstand failures without catastrophic consequences, a characteristic quite unique and elevated compared to traditional computing systems.\n\n\n2.7.3 Safety-Critical Systems\nSafety-critical systems are those where a failure could result in loss of life, significant property damage, or environmental harm. These systems require meticulous design to ensure the utmost reliability and safety. Embedded systems in this category often adhere to strict development standards and undergo rigorous testing to validate their reliability and safety characteristics.\nFor example, in automotive safety systems like Anti-lock Braking Systems (ABS) and Electronic Stability Control (ESC), embedded controllers play a pivotal role. These controllers are developed following stringent standards such as ISO 26262, ensuring that they meet the high reliability and safety requirements necessary to protect lives. In healthcare, devices like pacemakers and infusion pumps fall under this category, where the reliability of embedded systems can literally be a matter of life and death.\nThe emphasis on security and reliability in embedded systems cannot be overstated and I would state that these are often overlooked topics by most. As these systems intertwine deeper into the fabric of our daily lives and critical infrastructures, the doctrines of security and reliability stand as the beacon guiding the development and deployment of embedded systems. Through mechanisms like secure boot processes and fault tolerance techniques, these systems promise not only functional efficacy but also a shield of trust and safety, offering a robust and secure harbor in a sea of technological advancements and innovations. It’s these foundational principles that shape the embedded systems of today, sculpting them into reliable guardians and efficient executors in various critical spheres of modern society."
  },
  {
    "objectID": "embedded_sys.html#future-trends-and-challenges",
    "href": "embedded_sys.html#future-trends-and-challenges",
    "title": "2  Embedded Systems",
    "section": "2.8 Future Trends and Challenges",
    "text": "2.8 Future Trends and Challenges\nArm, the largest manufacturer of microcontrollers, has shipped (either directly or indirectly through partners) a record 8.0 billion chips, taking total shipped to date to more than a quarter of a trillion or 250 billion (ARM.com)!\nWe stand on the cusp of an era of unprecedented growth in the field of embedded systems, it is both exciting and crucial to cast a discerning eye on the possible future trends and challenges that await us. From the burgeoning realms of edge computing to the demands of scalability, the landscape is set to evolve, bringing forth new vistas of opportunities and trials. Let’s venture into the dynamic frontier that the future holds for embedded systems:\n\n2.8.1 Edge Computing and IoT\nWith the proliferation of the Internet of Things (IoT), the role of edge computing is becoming increasingly vital. Edge computing essentially allows data processing at the source, reducing latency and the load on central data centers. This paradigm shift is expected to redefine embedded systems, imbuing them with greater processing capabilities and intelligence to execute complex tasks locally.\nMoreover, with the IoT expected to encompass billions of devices globally, embedded systems will play a central role in facilitating seamless connectivity and interoperability among a diverse array of devices. This ecosystem would foster real-time analytics and decision-making, paving the way for smarter cities, industries, and homes. The challenge here lies in developing systems that are secure, energy-efficient, and capable of handling the surge in data volumes efficiently.\n\n\n2.8.2 Scalability and Upgradation\nAs embedded systems continue to evolve, the need for scalability and easy upgradation will become a focal point. Systems will be expected to adapt to changing technologies and user requirements without substantial overhauls. This calls for modular designs and open standards that allow for seamless integration of new features and functionalities.\nFurthermore, with the rapid advancements in technology, embedded systems will need to be equipped with mechanisms for remote upgrades and maintenance, ensuring their longevity and relevance in a fast-paced technological landscape. The onus will be on developers and manufacturers to create systems that not only meet the current demands but are also primed for future expansions, thereby ensuring a sustainable and progressive development trajectory.\n\n\n2.8.3 Market Opportunities\nThe market dynamics surrounding embedded systems are poised for exciting shifts. As industries increasingly adopt automation and digital transformation, the demand for sophisticated embedded systems is expected to soar. AI and ML are set to integrate deeper into embedded systems, offering unprecedented levels of intelligence and automation.\nSimultaneously, there is a burgeoning market for embedded systems in consumer electronics, automotive, healthcare, and industrial applications, presenting vast opportunities for innovation and growth. However, this expansion also brings forth challenges, including increased competition and the need for compliance with evolving regulatory standards. Companies venturing into this space will need to be agile, innovative, and responsive to the changing market dynamics to carve a niche for themselves."
  },
  {
    "objectID": "embedded_sys.html#conclusion",
    "href": "embedded_sys.html#conclusion",
    "title": "2  Embedded Systems",
    "section": "2.9 Conclusion",
    "text": "2.9 Conclusion\nThe table delineates a comparative analysis between these two types of computing systems, encompassing diverse categories such as processing power, memory capabilities, user interface, and real-time functionalities, among others. This comparison aims to provide readers with a succinct yet comprehensive insight into the inherent characteristics and specificities of both traditional and embedded computing frameworks, thus facilitating a deeper comprehension and discernment of their individual roles in the modern computing sphere.\n\n\n\n\n\n\n\n\nCategory\nTraditional Computing System\nEmbedded System Architecture\n\n\n\n\nHardware Characteristics\n\n\n\n\nProcessing Power\nHigh (Multi-core processors)\nModerate to Low (Single/Multi-core, optimized for specific tasks)\n\n\nMemory\nHigh (Upgradable)\nLimited (Fixed)\n\n\nStorage\nHigh (Upgradable)\nLimited (Fixed or expandable to a certain extent)\n\n\nHardware Scalability\nHigh (Can upgrade various components)\nLow (Hardware is often fixed and focused)\n\n\nSoftware Characteristics\n\n\n\n\nOperating System\nGeneral Purpose (Windows, Linux, macOS)\nReal-Time Operating System (RTOS) or No OS\n\n\nDevelopment Flexibility\nHigh (Supports multiple programming languages and frameworks)\nModerate (Focused on specific programming languages and tools)\n\n\nPerformance & Efficiency\n\n\n\n\nPower Consumption\nHigh\nLow (Optimized for energy efficiency)\n\n\nReal-Time Capabilities\nLimited (Not optimized for real-time tasks)\nHigh (Designed for real-time tasks)\n\n\nUser Interaction\n\n\n\n\nUser Interface\nComplex (GUI-Based)\nSimple or None (Can be GUI, command-line, or none)\n\n\nConnectivity\nExtensive (Multiple ports and connectivity options)\nLimited (Focused on necessary connectivity options)\n\n\nLifecycle & Maintenance\n\n\n\n\nMaintenance\nRegular Maintenance Required\nLow Maintenance (Set up to run specific tasks consistently)\n\n\nLifecycle\nShorter (Due to rapid technological advancements)\nLonger (Designed to perform specific tasks over a long period)\n\n\nCost and Use Cases\n\n\n\n\nCost\nVariable (Can be high depending on specifications)\nGenerally Lower (Due to focused functionalities)\n\n\nUse Cases\nGeneral (Various applications across sectors)\nSpecific (Dedicated to particular tasks or applications)\n\n\n\nAs we look into the horizon, it’s evident that the world of embedded systems is on the brink of a transformative phase, marked by innovations, opportunities, and challenges. The future beckons with promises of greater connectivity, intelligence, and efficiency, forging a path where embedded systems will be at the helm, steering the technological advancements of society. The journey ahead is one of exploration and adaptation, where the marriage of technology and ingenuity will craft a future that is not only technologically enriched but also responsive to the complex and ever-evolving demands of a dynamic world. It is a landscape ripe with potential, beckoning pioneers to venture forth and shape the contours of a promising and vibrant future.\n\n\n\n\nARM.com. “The Future Is Being Built on Arm: Market Diversification Continues to Drive Strong Royalty and Licensing Growth as Ecosystem Reaches Quarter of a Trillion Chips Milestone – Arm®.” https://www.arm.com/company/news/2023/02/arm-announces-q3-fy22-results."
  },
  {
    "objectID": "dl_primer.html#overview",
    "href": "dl_primer.html#overview",
    "title": "3  Deep Learning Primer",
    "section": "3.1 Overview",
    "text": "3.1 Overview\n\n3.1.1 Definition and Importance\nDeep learning, a subset of machine learning and artificial intelligence (AI), involves algorithms inspired by the structure and function of the human brain, called artificial neural networks. It stands as a cornerstone in the field of AI, spearheading advancements in various domains including computer vision, natural language processing, and autonomous vehicles. Its relevance in embedded AI systems is underscored by its ability to facilitate complex computations and predictions, leveraging the limited resources available in embedded environments.\n\n\n\n3.1.2 Brief History of Deep Learning\nThe concept of deep learning has its roots in the early artificial neural networks. It has witnessed several waves of popularity, starting with the introduction of the Perceptron in the 1950s (Rosenblatt 1957), followed by the development of backpropagation algorithms in the 1980s (Rumelhart, Hinton, and Williams 1986).\nThe term deep learning emerged in the 2000s, marked by breakthroughs in computational power and data availability. Key milestones include the successful training of deep networks such as AlexNet (Krizhevsky, Sutskever, and Hinton 2012) by Geoffrey Hinton, one of the god fathers of AI, and the resurgence of neural networks as a potent tool for data analysis and modeling.\nIn recent years, deep learning has witnessed exponential growth, becoming a transformative force across various industries. Figure 3.1 shows that we are currently in the third era of deep learning. From 1952 to 2010, computational growth followed an 18-month doubling pattern. This dramatically accelerated to a 6-month cycle from 2010 to 2022. At the same time, we witnessed the advent of major-scale models between 2015 and 2022; these appeared 2 to 3 orders of magnitude faster and followed a 10-month doubling cycle.\n\n\n\nFigure 3.1: Growth of deep learning models.\n\n\nA confluence of factors has fueled this surge, including advancements in computational power, the proliferation of big data, and improvements in algorithmic designs. Firstly, the expansion of computational capabilities, particularly the advent of Graphics Processing Units (GPUs) and Tensor Processing Units (TPUs) (Jouppi et al. 2017), has significantly accelerated the training and inference times of deep learning models. These hardware advancements have made it feasible to construct and train more complex, deeper networks than were possible in the earlier years.\nSecondly, the digital revolution has brought forth an abundance of “big” data, providing rich material for deep learning models to learn from and excel in tasks such as image and speech recognition, language translation, and game playing. The availability of large, labeled datasets has been instrumental in the refinement and successful deployment of deep learning applications in real-world scenarios.\nAdditionally, collaborations and open-source initiatives have fostered a vibrant community of researchers and practitioners, propelling rapid advancements in deep learning techniques. Innovations such as deep reinforcement learning, transfer learning, and generative adversarial networks have expanded the boundaries of what is achievable with deep learning, opening new avenues and opportunities in various fields including healthcare, finance, transportation, and entertainment.\nCompanies and organizations worldwide are recognizing the transformative potential of deep learning, investing heavily in research and development to harness its power in offering innovative solutions, optimizing operations, and creating new business opportunities. As deep learning continues its upward trajectory, it is poised to revolutionize how we interact with technology, making our lives more convenient, safe, and connected.\n\n\n3.1.3 Applications of Deep Learning\nDeep learning is widely used in many industries today. It is used in finance for things such as stock market prediction, risk assessment, and fraud detection. It is also used in marketing for things such as customer segmentation, personalization, and content optimization. In healthcare, machine learning is used for tasks such as diagnosis, treatment planning, and patient monitoring. It has had a transformational impact on our society.\nAn example of the transformative impact that machine learning has had on society is how it has saved money and lives. For example, as mentioned earlier, deep learning algorithms can make predictions about stocks, like predicting whether they will go up or down. These predictions guide investment strategies and improve financial decisions. Similarly, deep learning can also make medical predictions to improve patient diagnosis and save lives. The possibilities are endless and the benefits are clear. Machine learning is not only able to make predictions with greater accuracy than humans but it is also able to do so at a much faster pace.\nDeep learning has been applied to manufacturing to great effect. By using software to constantly learn from the vast amounts of data collected throughout the manufacturing process, companies are able to increase productivity while reducing wastage through improved efficiency. Companies are benefiting financially from these effects while customers are receiving better quality products at lower prices. Machine learning enables manufacturers to constantly improve their processes to create higher quality goods faster and more efficiently than ever before.\nDeep learning has also improved products that we use daily like Netflix recommendations or Google Translate’s text translations, but it also allows companies such as Amazon and Uber to save money on customer service costs by quickly identifying unhappy customers.\n\n\n3.1.4 Relevance to Embedded AI\nEmbedded AI, which involves integrating AI algorithms directly into hardware devices, naturally benefits from the capabilities of deep learning. The synergy of deep learning algorithms with embedded systems has paved the way for intelligent, autonomous devices capable of sophisticated on-device data processing and analysis. Deep learning facilitates the extraction of intricate patterns and information from input data, making it a vital tool in the development of smart embedded systems, ranging from household appliances to industrial machines. This union aims to foster a new era of smart, interconnected devices that can learn and adapt to user behaviors and environmental conditions, optimizing performance and offering unprecedented levels of convenience and efficiency."
  },
  {
    "objectID": "dl_primer.html#neural-networks",
    "href": "dl_primer.html#neural-networks",
    "title": "3  Deep Learning Primer",
    "section": "3.2 Neural Networks",
    "text": "3.2 Neural Networks\nDeep learning takes inspiration from the human brain’s neural networks to create patterns utilized in decision-making. This section explores the foundational concepts that comprise deep learning, offering insights into the underpinnings of more complex topics explored later in this primer.\nNeural networks form the basis of deep learning, drawing inspiration from the biological neural networks of the human brain to process and analyze data in a hierarchical manner. Below, we dissect the primary components and structures commonly found in neural networks.\n\n3.2.1 Perceptrons\nAt the foundation of neural networks is the perceptron, a basic unit or node that forms the basis of more complex structures. A perceptron receives various inputs, applies weights and a bias to these inputs, and then employs an activation function to produce an output as shown below in Figure 3.2.\n\n\n\nFigure 3.2: Perceptron\n\n\nInitially conceptualized in the 1950s, perceptrons paved the way for the development of more intricate neural networks, serving as a fundamental building block in the field of deep learning.\n\n\n3.2.2 Multi-layer Perceptrons\nMulti-layer perceptrons (MLPs) evolve from the single-layer perceptron model, incorporating multiple layers of nodes connected in a feedforward manner. These layers include an input layer to receive data, several hidden layers to process this data, and an output layer to generate the final results. MLPs excel in identifying non-linear relationships, utilizing a backpropagation technique for training, wherein the weights are optimized through a gradient descent algorithm.\n\n\n\nMultilayer Perceptron\n\n\n\n\n3.2.3 Activation Functions\nActivation functions stand as vital components in neural networks, providing the mathematical equations that determine a network’s output. These functions introduce non-linearity to the network, facilitating the learning of complex patterns by allowing the network to adjust weights based on the error during the learning process. Popular activation functions encompass the sigmoid, tanh, and ReLU (Rectified Linear Unit) functions.\n\n\n\nActivation Function\n\n\n\n\n3.2.4 Computational Graphs\nDeep learning employs computational graphs to illustrate the various operations and their interactions within a neural network. This subsection explores the essential phases of computational graph processing.\n\n\n\nTensorFlow Computational Graph\n\n\n\n3.2.4.1 Forward Pass\nThe forward pass denotes the initial phase where data progresses through the network from the input to the output layer. During this phase, each layer conducts specific computations on the input data, utilizing weights and biases before passing the resulting values onto subsequent layers. The ultimate output of this phase is employed to compute the loss, representing the disparity between the predicted output and actual target values.\n\n\n3.2.4.2 Backward Pass (Backpropagation)\nBackpropagation signifies a pivotal algorithm in the training of deep neural networks. This phase involves computing the gradient of the loss function with respect to each weight using the chain rule, effectively maneuvering backwards through the network. The gradients calculated in this step guide the adjustment of weights with the objective of minimizing the loss function, thereby enhancing the network’s performance with each iteration of training.\nGrasping these foundational concepts paves the way to understanding more intricate deep learning architectures and techniques, fostering the development of more sophisticated and efficacious applications, especially within the realm of embedded AI systems.\n\n\n\n\n\n3.2.5 Training Concepts\nIn the realm of deep learning, it’s crucial to comprehend various key concepts and terms that set the foundation for creating, training, and optimizing deep neural networks. This section clarifies these essential concepts, providing a straightforward path to delve deeper into the intricate dynamics of deep learning. Overall, ML training is an iterative process. An untrained neural network model takes some features as input and makes a forward prediction pass. Given some ground truth about the prediction, which is known during the training process, we can compute a loss using a loss function and update the neural network parameters during the backward pass. We repeat this process until the network converges towards correct predictions with satisfactory accuracy.\n\n\n\nAn iterative approach to training a model.\n\n\n\n3.2.5.1 Loss Functions\nLoss functions, also known as cost functions, quantify how well a neural network is performing by calculating the difference between the actual and predicted outputs. The objective during the training process is to minimize this loss function to improve the model’s accuracy. As Figure 3.3 shows, models can either have high loss or low loss depending on where in the training phase the network is in.\n\n\n\nFigure 3.3: High loss in the left model; low loss in the right model.\n\n\nVarious loss functions are employed depending on the specific task, such as mean squared error, log loss and cross-entropy loss for regression tasks and categorical crossentropy for classification tasks.\n\n\n3.2.5.2 Optimization Algorithms\nOptimization algorithms play a crucial role in the training process, aiming to minimize the loss function by adjusting the model’s weights. These algorithms navigate through the model’s parameter space to find the optimal set of parameters that yield the minimum loss. Some commonly used optimization algorithms are:\n\nGradient Descent: A first-order optimization algorithm that uses the gradient of the loss function to move the weights in the direction that minimizes the loss.\nStochastic Gradient Descent (SGD): A variant of gradient descent that updates the weights using a subset of the data, thus accelerating the training process.\nAdam: A popular optimization algorithm that combines the benefits of other extensions of gradient descent, often providing faster convergence.\n\n\n\n\nMinimizing loss during the training process.\n\n\n\n\n3.2.5.3 Regularization Techniques\nTo prevent overfitting and help the model generalize better to unseen data, regularization techniques are employed. These techniques penalize the complexity of the model, encouraging simpler models that can perform better on new data.\n\nCommon regularization techniques include:\n\nL1 and L2 Regularization: These techniques add a penalty term to the loss function, discouraging large weights and promoting simpler models.\nDropout: A technique where randomly selected neurons are ignored during training, forcing the network to learn more robust features.\nBatch Normalization: This technique normalizes the activations of the neurons in a given layer, improving the stability and performance of the network.\n\nUnderstanding these fundamental concepts and terms forms the backbone of deep learning, setting the stage for a more in-depth exploration into the intricacies of various deep learning architectures and their applications, particularly in embedded AI systems.\n\n\n\n3.2.6 Model Architectures\nDeep learning architectures refer to the various structured approaches that dictate how neurons and layers are organized and interact in neural networks. These architectures have evolved to address different problems and data types efficiently. This section provides an overview of some prominent deep learning architectures and their characteristics.\n\n3.2.6.1 Multi-Layer Perceptrons (MLPs)\nMLPs are fundamental deep learning architectures, consisting of three or more layers: an input layer, one or more hidden layers, and an output layer. These layers are fully connected, meaning every neuron in a layer is connected to every neuron in the preceding and succeeding layers. MLPs can model complex functions and find applications in a wide range of tasks, including regression, classification, and pattern recognition. Their ability to learn non-linear relationships through backpropagation makes them a versatile tool in the deep learning arsenal.\nIn embedded AI systems, MLPs can serve as compact models for simpler tasks, such as sensor data analysis or basic pattern recognition, where computational resources are constrained. Their capability to learn non-linear relationships with relatively less complexity makes them a viable option for embedded systems.\n\n\n3.2.6.2 Convolutional Neural Networks (CNNs)\nCNNs are primarily used in image and video recognition tasks. This architecture uses convolutional layers that apply a series of filters to the input data to identify various features such as edges, corners, and textures. A typical CNN also includes pooling layers that reduce the spatial dimensions of the data, and fully connected layers for classification. CNNs have proven highly effective in tasks like image recognition, object detection, and computer vision applications.\nIn the realm of embedded AI, CNNs are pivotal for image and video recognition applications, where real-time processing is often required. They can be optimized for embedded systems by employing techniques such as quantization and pruning to reduce memory usage and computational demands, enabling efficient object detection and facial recognition functionalities in devices with limited computational resources.\n\n\n3.2.6.3 Recurrent Neural Networks (RNNs)\nRNNs are suited for sequential data analysis, such as time series forecasting and natural language processing. In this architecture, connections between nodes form a directed graph along a temporal sequence, allowing information to be carried across sequences through hidden state vectors. Variations of RNNs include Long Short-Term Memory (LSTM) and Gated Recurrent Units (GRU), which are designed to capture longer dependencies in sequence data.\nIn embedded systems, these networks can be implemented in voice recognition systems, predictive maintenance, or in IoT devices where sequential data patterns are prevalent. Optimizations specific to embedded platforms can help in managing their typically high computational and memory requirements.\n\n\n3.2.6.4 Generative Adversarial Networks (GANs)\nGANs consist of two networks, a generator and a discriminator, that are trained simultaneously through adversarial training (Goodfellow et al. 2020). The generator produces data that tries to mimic the real data distribution, while the discriminator aims to distinguish between real and generated data. GANs are widely used in image generation, style transfer, and data augmentation.\nIn embedded contexts, GANs could be used for on-device data augmentation to enhance the training of models directly on the embedded device, facilitating continual learning and adaptation to new data without the need for cloud computing resources.\n\n\n3.2.6.5 Autoencoders\nAutoencoders are neural networks used for data compression and noise reduction (Bank, Koenigstein, and Giryes 2023). They are structured to encode input data into a lower-dimensional representation and then decode it back to the original form. Variations like Variational Autoencoders (VAEs) introduce probabilistic layers that allow for generative properties, finding applications in image generation and anomaly detection.\nImplementing autoencoders can assist in efficient data transmission and storage, enhancing the overall performance of embedded systems with limited computational and memory resources.\n\n\n3.2.6.6 Transformer Networks\nTransformer networks have emerged as a powerful architecture, especially in the field of natural language processing (Vaswani et al. 2017). These networks use self-attention mechanisms to weigh the influence of different input words on each output word, facilitating parallel computation and capturing complex patterns in data. Transformer networks have led to state-of-the-art results in tasks such as language translation, summarization, and text generation.\nThese networks can be optimized to perform language-related tasks directly on-device. For instance, transformers can be utilized in embedded systems for real-time translation services or voice-assisted interfaces, where latency and computational efficiency are critical factors. Techniques such as model distillation which we will discss later on can be employed to deploy these networks on embedded devices with constrained resources.\nEach of these architectures serves specific purposes and excel in different domains, offering a rich toolkit for tackling diverse problems in the realm of embedded AI systems. Understanding the nuances of these architectures is vital in designing effective and efficient deep learning models for various applications."
  },
  {
    "objectID": "dl_primer.html#libraries-and-frameworks",
    "href": "dl_primer.html#libraries-and-frameworks",
    "title": "3  Deep Learning Primer",
    "section": "3.3 Libraries and Frameworks",
    "text": "3.3 Libraries and Frameworks\nIn the world of deep learning, the availability of robust libraries and frameworks has been a cornerstone in facilitating the development, training, and deployment of models, particularly in embedded AI systems where efficiency and optimization are key. These libraries and frameworks are often equipped with pre-defined functions and tools that allow for rapid prototyping and deployment. This section sheds light on popular libraries and frameworks, emphasizing their utility in embedded AI scenarios.\n\n3.3.1 TensorFlow\nTensorFlow, developed by Google (Abadi et al. 2016), stands as one of the premier frameworks for developing deep learning models. Its ability to work seamlessly with embedded systems comes from TensorFlow Lite, a lightweight solution designed to run on mobile and embedded devices. TensorFlow Lite enables the execution of optimized models on a variety of platforms, making it easier to integrate AI functionalities in embedded systems. For TinyML we will be dealing with TensorFlow Lite for Microcontrollers.\n\n\n3.3.2 PyTorch\nPyTorch, an open-source library developed by Facebook (Paszke et al. 2019), is praised for its dynamic computation graph and ease of use. For embedded AI, PyTorch can be a suitable choice for research and prototyping, offering a seamless transition from research to production with the use of the TorchScript scripting language. PyTorch Mobile further facilitates the deployment of models on mobile and embedded devices, offering tools and workflows to optimize performance.\n\n\n3.3.3 ONNX Runtime\nThe Open Neural Network Exchange (ONNX) Runtime is a cross-platform, high-performance engine for running machine learning models. It is not particularly developed for embedded AI systems, though it supports a wide range of hardware accelerators and is capable of optimizing computations to improve performance in resource-constrained environments.\n\n\n3.3.4 Keras\nKeras (Chollet 2015) serves as a high-level neural networks API, capable of running on top of TensorFlow, and other frameworks like Theano, or CNTK. For developers venturing into embedded AI, Keras offers a simplified interface for building and training models. Its ease of use and modularity can be especially beneficial in the rapid development and deployment of models in embedded systems, facilitating the integration of AI capabilities with minimal complexity.\n\n\n3.3.5 TVM\nTVM is an open-source machine learning compiler stack that aims to enable efficient deployment of deep learning models on a variety of platforms (Chen et al. 2018). Particularly in embedded AI, TVM and µTVM (Micro TVM) can be crucial in optimizing and streamlining models to suit the restricted computational and memory resources, thus making deep learning more accessible and feasible on embedded devices.\nThese libraries and frameworks are pivotal in leveraging the capabilities of deep learning in embedded AI systems, offering a range of tools and functionalities that enable the development of intelligent and optimized solutions. Selecting the appropriate library or framework, however, is a crucial step in the development pipeline, aligning with the specific requirements and constraints of embedded systems."
  },
  {
    "objectID": "dl_primer.html#embedded-ai-challenges",
    "href": "dl_primer.html#embedded-ai-challenges",
    "title": "3  Deep Learning Primer",
    "section": "3.4 Embedded AI Challenges",
    "text": "3.4 Embedded AI Challenges\nEmbedded AI systems often operate within environments with constrained resources, posing unique challenges in implementing the deep learning algorithms we discussed above efficiently. In this section, we explore various challenges encountered in the deployment of deep learning in embedded systems and potential solutions to navigate these complexities.\n\n3.4.1 Memory Constraints\n\nChallenge: Embedded systems usually have limited memory, which can be a bottleneck when deploying large deep learning models.\nSolution: Employing model compression techniques such as pruning and quantization to reduce the memory footprint without significantly affecting performance.\n\n\n\n3.4.2 Computational Limitations\n\nChallenge: The computational capacity in embedded systems can be limited, hindering the deployment of complex deep learning models.\nSolution: Utilizing hardware acceleration through GPUs or dedicated AI chips to boost computational power, and optimizing models for inference through techniques like layer fusion.\n\n\n\n3.4.3 Energy Efficiency\n\nChallenge: Embedded systems, particularly battery-powered devices, require energy-efficient operations to prolong battery life.\nSolution: Implementing energy-efficient neural networks that are designed to minimize energy consumption during operation, and employing dynamic voltage and frequency scaling to adjust the power consumption dynamically.\n\n\n\n3.4.4 Data Privacy and Security\n\nChallenge: Embedded AI systems often process sensitive data, raising concerns regarding data privacy and security.\nSolution: Employing on-device processing to keep sensitive data on the device itself, and incorporating encryption and secure channels for any necessary data transmission.\n\n\n\n3.4.5 Real-Time Processing Requirements\n\nChallenge: Many embedded AI applications demand real-time processing to provide instantaneous responses, which can be challenging to achieve with deep learning models.\nSolution: Streamlining the model through methods such as model distillation to reduce complexity and employing real-time operating systems to ensure timely processing.\n\n\n\n3.4.6 Model Robustness and Generalization\n\nChallenge: Ensuring that deep learning models are robust and capable of generalizing well to unseen data in embedded AI settings.\nSolution: Incorporating techniques like data augmentation and adversarial training to enhance model robustness and improve generalization capabilities.\n\n\n\n3.4.7 Integration with Existing Systems\n\nChallenge: Integrating deep learning capabilities into existing embedded systems can pose compatibility and interoperability issues.\nSolution: Adopting modular design approaches and leveraging APIs and middleware solutions to facilitate smooth integration with existing systems and infrastructures.\n\n\n\n3.4.8 Scalability\n\nChallenge: Scaling deep learning solutions to cater to a growing number of devices and users in embedded AI ecosystems.\nSolution: Utilizing cloud-edge computing paradigms to distribute computational loads effectively and ensuring that the models can be updated seamlessly to adapt to changing requirements.\n\nUnderstanding and addressing these challenges are vital in the successful deployment of deep learning solutions in embedded AI systems. By adopting appropriate strategies and solutions, developers can navigate these hurdles effectively, fostering the creation of reliable, efficient, and intelligent embedded AI systems."
  },
  {
    "objectID": "dl_primer.html#limitations-of-deep-learning",
    "href": "dl_primer.html#limitations-of-deep-learning",
    "title": "3  Deep Learning Primer",
    "section": "3.5 Limitations of Deep Learning",
    "text": "3.5 Limitations of Deep Learning\nChoosing the right approach between traditional machine learning and deep learning hinges on various factors that delineate the complexity and nature of the problem at hand. This section aims to provide a balanced perspective that assists practitioners, researchers, and enthusiasts in making well-informed decisions while selecting between traditional machine learning and deep learning approaches for their specific tasks and projects.\n\n3.5.1 The Predicament\nDeep learning, although powerful, is not a universal solution for every problem. Its effectiveness often comes at the cost of requiring substantial data and computational resources. Moreover, deep learning models, especially deep neural networks, are often perceived as “black boxes”, offering no clear insight into their decision-making process, which can be a significant downside in sectors where interpretability is critical.\nTraditional machine learning, encompassing various algorithms and methodologies like decision trees, support vector machines, and logistic regression, retains significant value in data analytics and predictive modeling. It often presents several advantages including better interpretability, efficiency, and suitability for smaller datasets.\n\n\n3.5.2 Traditional ML vs Deep Learning\nTo succinctly highlight the differences, a comparative table illustrates the contrasting characteristics between traditional ML and deep learning:\n\n\n\n\n\n\n\n\nAspect\nTraditional ML\nDeep Learning\n\n\n\n\nData Requirements\nLow to Moderate (efficient with smaller datasets)\nHigh (requires large datasets for nuanced learning)\n\n\nModel Complexity\nModerate (suitable for well-defined problems)\nHigh (detects intricate patterns, suited for complex tasks)\n\n\nComputational Resources\nLow to Moderate (cost-effective, less resource-intensive)\nHigh (demands substantial computational power and resources)\n\n\nDeployment Speed\nFast (quicker training and deployment cycles)\nSlow (prolonged training times, especially with larger datasets)\n\n\nInterpretability\nHigh (clear insights into decision pathways)\nLow (complex layered structures, “black box” nature)\n\n\nMaintenance\nEasier (simple to update and maintain)\nComplex (requires more efforts in maintenance and updates)\n\n\n\n\n\n3.5.3 Choosing Traditional ML vs. DL\n\n3.5.3.1 Data Availability and Volume\n\nQuantity of Data: Traditional machine learning models, like decision trees or Naive Bayes, are often favorable when data availability is limited, providing robust predictions even with smaller datasets. This is evident in scenarios such as disease prediction in medical diagnostics and customer segmentation in marketing.\nData Diversity and Quality: Traditional machine learning models are versatile in handling various data types, often requiring less preprocessing compared to deep learning models. They may also offer more robust predictions in scenarios with noisy data.\n\n\n\n3.5.3.2 Complexity of the Problem\n\nProblem Granularity: Simple to moderately complex problems, which can have linear or polynomial relationships between variables, often find a better match with traditional machine learning techniques.\nHierarchical Feature Representation: Deep learning models excel in tasks requiring hierarchical feature representation like image and speech recognition. However, not all problems demand this level of complexity, and traditional machine learning algorithms might sometimes provide more straightforward and equally accurate solutions.\n\n\n\n3.5.3.3 Hardware and Computational Resources\n\nResource Constraints: The availability of computational resources often dictates the choice between traditional ML and deep learning, with the former being less resource-intensive and thereby preferable in environments with hardware limitations or budget constraints.\nScalability and Speed: Traditional machine learning models, like support vector machines (SVM), often allow for quicker training times and graceful scalability, especially advantageous in projects with tight timelines and increasing data volumes.\n\n\n\n3.5.3.4 Regulatory Compliance\nRegulatory compliance is paramount in several industries, necessitating adherence to guidelines and best practices such as GDPR in the EU. Traditional ML models, due to their inherent interpretability, often align better with these regulations, especially in sectors like finance and healthcare.\n\n\n3.5.3.5 Interpretability\nUnderstanding the decision-making process is simpler with traditional machine learning techniques, as opposed to deep learning models that function as a “black box”, making the tracing of decision pathways challenging.\n\n\n\n3.5.4 Making an Informed Choice\nGiven the constraints of embedded AI systems, understanding the nuances between traditional ML techniques and deep learning becomes critical. Both avenues offer unique advantages, and their distinct characteristics often dictate the choice of one over the other in different scenarios.\nWith all that said, deep learning has been steadily outpacing traditional machine learning methods in several critical areas due to a combination of data abundance, computational advancements, and proven efficacy in complex tasks.\nHere are a few concrete reasons why we are centering our attention on deep learning in this text:\n\nSuperior Performance in Complex Tasks: Deep learning models, especially deep neural networks, excel in tasks where the relationships between data points are incredibly complex. Tasks such as image and speech recognition, language translation, and playing intricate games like Go and Chess have seen breakthroughs primarily through deep learning algorithms.\nEfficient Handling of Unstructured Data: Unlike traditional machine learning methods, deep learning can process unstructured data more effectively. This is a vital asset in the modern data landscape where a vast majority of data is unstructured, including text, images, and videos.\nLeveraging Big Data: With the availability of big data, deep learning models have the ability to continually learn and improve. These models are adept at utilizing large datasets to enhance their predictive accuracy, something that can be limiting in traditional machine learning approaches.\nHardware Advancements and Parallel Computing: The development of powerful GPUs and the availability of cloud computing platforms have enabled the rapid training of deep learning models. These advancements have addressed one of the significant hurdles of deep learning – the requirement of substantial computational resources.\nDynamic Adaptability and Continuous Learning: Deep learning models are capable of adapting to new information or data dynamically. They can be trained to generalize their learning to new, unseen data, which is essential in rapidly evolving fields such as autonomous driving or real-time language translation.\n\nDespite the traction gained by deep learning, it’s imperative to understand that traditional machine learning is not obsolete. While we delve deeper into the nuances of deep learning, we will also highlight situations where traditional machine learning methods may be more appropriate due to their simplicity, efficiency, and ease of interpretation. By focusing on deep learning in this text, we aim to equip readers with the knowledge and tools necessary to tackle modern, complex problems in various domains, while also offering insights into the comparative advantages and suitable application scenarios for both deep learning and traditional machine learning methods."
  },
  {
    "objectID": "dl_primer.html#conclusion",
    "href": "dl_primer.html#conclusion",
    "title": "3  Deep Learning Primer",
    "section": "3.6 Conclusion",
    "text": "3.6 Conclusion\nDeep learning has emerged as a powerful set of techniques for tackling complex pattern recognition and prediction problems. Beginning with an overview, we delineated the fundamental concepts and principles that govern deep learning, establishing a foundational knowledge that would pave the way for more advanced studies.\nAt the heart of deep learning, we delved into the basic concepts of neural networks, the powerful computational models inspired by the human brain’s interconnected neuron structure. This discussion allowed us to appreciate the capabilities and potential of neural networks in crafting sophisticated algorithms that can learn and adapt from data.\nUnderstanding the role of libraries and frameworks was a crucial part of our discussion, offering insights into the tools that can help streamline the development and deployment of deep learning models. These resources not only facilitate the implementation of neural networks but also provide avenues for innovation and optimization.\nSubsequently, we addressed the challenges that one might encounter in embedding deep learning algorithms within embedded systems, offering a critical perspective on the complexities and considerations that come with bringing AI to edge devices.\nMoreover, we ventured into an examination of the limitations of deep learning. Through a series of discussions, we unraveled the issues faced in deep learning applications and outlined the scenarios where traditional machine learning might supersede deep learning. These sections are instrumental in fostering a balanced view of the capabilities and restrictions of deep learning.\nIn this primer, we equipped you with the knowledge to make informed choices between deploying traditional machine learning or deep learning techniques, depending on the unique demands and constraints of a particular problem.\nAs we conclude this chapter, it is our hope that you are now well-versed with the basic “language” of deep learning, prepared to delve deeper into the subsequent chapters with a solid understanding and critical perspective. The journey ahead is lined with exciting opportunities and challenges that embed AI within systems.\n\n\n\n\nAbadi, Martı́n, Paul Barham, Jianmin Chen, Zhifeng Chen, Andy Davis, Jeffrey Dean, Matthieu Devin, et al. 2016. “\\(\\{\\)TensorFlow\\(\\}\\): A System for \\(\\{\\)Large-Scale\\(\\}\\) Machine Learning.” In 12th USENIX Symposium on Operating Systems Design and Implementation (OSDI 16), 265–83.\n\n\nBank, Dor, Noam Koenigstein, and Raja Giryes. 2023. “Autoencoders.” Machine Learning for Data Science Handbook: Data Mining and Knowledge Discovery Handbook, 353–74.\n\n\nChen, Tianqi, Thierry Moreau, Ziheng Jiang, Lianmin Zheng, Eddie Yan, Haichen Shen, Meghan Cowan, et al. 2018. “\\(\\{\\)TVM\\(\\}\\): An Automated \\(\\{\\)End-to-End\\(\\}\\) Optimizing Compiler for Deep Learning.” In 13th USENIX Symposium on Operating Systems Design and Implementation (OSDI 18), 578–94.\n\n\nChollet, François. 2015. “Keras.” GitHub Repository. https://github.com/fchollet/keras; GitHub.\n\n\nGoodfellow, Ian, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. 2020. “Generative Adversarial Networks.” Communications of the ACM 63 (11): 139–44.\n\n\nJouppi, Norman P, Cliff Young, Nishant Patil, David Patterson, Gaurav Agrawal, Raminder Bajwa, Sarah Bates, et al. 2017. “In-Datacenter Performance Analysis of a Tensor Processing Unit.” In Proceedings of the 44th Annual International Symposium on Computer Architecture, 1–12.\n\n\nKrizhevsky, Alex, Ilya Sutskever, and Geoffrey E Hinton. 2012. “Imagenet Classification with Deep Convolutional Neural Networks.” Advances in Neural Information Processing Systems 25.\n\n\nPaszke, Adam, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, et al. 2019. “Pytorch: An Imperative Style, High-Performance Deep Learning Library.” Advances in Neural Information Processing Systems 32.\n\n\nRosenblatt, Frank. 1957. The Perceptron, a Perceiving and Recognizing Automaton Project Para. Cornell Aeronautical Laboratory.\n\n\nRumelhart, David E, Geoffrey E Hinton, and Ronald J Williams. 1986. “Learning Representations by Back-Propagating Errors.” Nature 323 (6088): 533–36.\n\n\nVaswani, Ashish, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. 2017. “Attention Is All You Need.” Advances in Neural Information Processing Systems 30."
  },
  {
    "objectID": "embedded_ml.html#cloud-ml",
    "href": "embedded_ml.html#cloud-ml",
    "title": "4  Embedded ML",
    "section": "4.1 Cloud ML",
    "text": "4.1 Cloud ML\n\n4.1.1 Characteristics\nCloud ML is a facet of the broader machine learning discipline that operates on cloud computing infrastructure. It essentially facilitates the development, training, and deployment of machine learning models on a virtual platform, offering both flexibility and scalability.\nAt its core, Cloud ML relies on a potent combination of high-capacity servers, vast storage solutions, and robust network architectures that are housed in data centers globally. This infrastructure permits the centralization of computational resources, making it easier to manage and scale machine learning projects seamlessly.\nThe cloud serves as a prolific environment for data processing and model training, equipped to handle extensive data loads and complex computations. Models developed under Cloud ML can be trained using a wealth of data, which is processed and analyzed in a centralized location, thereby optimizing the model’s learning and predictive capabilities.\n\n\n4.1.2 Benefits\nCloud ML is synonymous with high computational power, capable of managing intricate algorithms and large datasets with finesse. This aspect is particularly beneficial in advancing machine learning models that require substantial computational resources, effectively bypassing the limitations of local setups.\nOne of the standout benefits of Cloud ML is the ability to scale resources dynamically based on project demands. This scalability ensures that as the data volume or computational needs escalate, the infrastructure can adapt accordingly without compromising performance.\nCloud ML platforms often provide access to a plethora of advanced tools and algorithms. Developers can leverage these resources to build, train, and deploy sophisticated models, accelerating the development cycle and fostering innovation.\n\n\n4.1.3 Challenges\nDespite its prowess, Cloud ML is often hampered by latency issues, particularly in applications necessitating real-time responses. The process of transmitting data to centralized servers and back can introduce delays, a critical drawback in time-sensitive applications.\nCentralizing data processing and storage sometimes open up vulnerabilities in data privacy and security. The data centers become prime targets for cyber-attacks, necessitating significant investments in security infrastructure to safeguard sensitive information.\nAs the data processing needs grow, so do the costs associated with using cloud services. Organizations working with large data volumes may find escalating costs, which could potentially limit the scalability and feasibility of their operations over time.\n\n\n4.1.4 Example Use Cases\nCloud ML has been instrumental in powering virtual assistants like Siri and Alexa. These systems leverage the cloud’s computational capabilities to process and analyze voice inputs, providing users with intelligent and personalized responses.\nCloud ML also forms the backbone of sophisticated recommendation systems seen in platforms like Netflix and Amazon. These systems analyze extensive datasets to discern patterns and preferences, delivering personalized content or product recommendations to enhance user engagement.\nIn the financial sector, Cloud ML has significantly contributed to the development of robust fraud detection systems. These systems analyze large swathes of transactional data to identify potential fraudulent activities, allowing for timely interventions and minimizing risks associated with financial frauds.\nThere are countless other examples of Cloud ML, but briefly, nowadays, it is impossible to get on the internet without interacting with some form of it, either directly or indirectly. From personalized advertisements that pop up on your social media feed to the predictive text functionality in email services, Cloud ML intricately weaves itself into the fabric of online experiences. It fuels the smart algorithms that suggest products on e-commerce websites, power search engines to deliver precise results, and even facilitate automatic tagging and categorization of photos on platforms like Facebook.\nMoreover, Cloud ML enhances user security through anomaly detection systems that monitor for suspicious activities, potentially protecting users from cyber threats. In essence, it functions as the silent powerhouse, continually working behind the scenes to streamline, secure, and personalize our digital interactions, making the modern internet a more intuitive and user-friendly space."
  },
  {
    "objectID": "embedded_ml.html#edge-ml",
    "href": "embedded_ml.html#edge-ml",
    "title": "4  Embedded ML",
    "section": "4.2 Edge ML",
    "text": "4.2 Edge ML\n\n4.2.1 Characteristics\nDefinition of Edge ML\nEdge Machine Learning (Edge ML) refers to the deployment of machine learning algorithms directly on endpoint devices or closer to where the data is generated, instead of relying on centralized cloud servers. This approach aims to bring computation near the data source, thereby reducing the need to transmit large volumes of data over networks, which often leads to lower latency and enhanced data privacy.\nDecentralized Data Processing\nIn an Edge ML setup, data processing occurs in a decentralized manner. Instead of sending data all the way to distant servers, the data is processed locally on devices like smartphones, tablets, or IoT devices. This localized processing means that devices can make swift decisions based on the data they collect, without relying heavily on a central server’s resources. This decentralization is critical in real-time applications where even a slight delay can have significant repercussions.\nLocal Data Storage and Computation\nLocal data storage and computation are hallmarks of Edge ML. This approach ensures that data can be stored and analyzed directly on the devices, thereby retaining the data’s privacy and reducing the reliance on continuous internet connectivity. Moreover, it can often lead to more efficient computation as data doesn’t have to travel over long distances, and the computations are performed with a more intimate understanding of the local context, which can sometimes lead to more insightful analyses.\n\n\n4.2.2 Benefits\nReduced Latency\nBy virtue of processing data locally, Edge ML significantly reduces latency compared to Cloud ML. In scenarios where milliseconds matter, like in autonomous vehicles where rapid decision-making can mean the difference between safety and accident, the reduced latency can be a critical advantage.\nEnhanced Data Privacy\nEdge ML also offers enhanced data privacy as data is primarily stored and processed locally, minimizing the risk of data breaches that are more common in centralized data storage solutions. This means sensitive information can be kept more secure, as it’s not transmitted over networks where it might potentially be intercepted.\nLess Bandwidth Usage\nOperating on the edge means that less data needs to be transmitted over networks, thereby reducing bandwidth usage. This can lead to cost savings and efficiency improvements, particularly in environments where bandwidth is limited or expensive.\n\n\n4.2.3 Challenges\nLimited Computational Resources Compared to Cloud ML\nHowever, Edge ML comes with its set of challenges. One of the primary concerns is the limited computational resources compared to cloud-based solutions. Endpoint devices may not have the same processing power or storage capacity as cloud servers, which can restrict the complexity of the machine learning models that can be deployed.\nComplexity in Managing Edge Nodes\nManaging a network of edge nodes can introduce complexity, particularly when it comes to coordination, updates, and maintenance. Ensuring that all nodes operate seamlessly and are up-to-date with the latest algorithms and security protocols can be a logistical challenge.\nSecurity Concerns at the Edge Nodes\nDespite the enhanced data privacy, edge nodes can sometimes be more vulnerable to physical and cyber-attacks. Developing robust security protocols that protect data at each node, without compromising the system’s efficiency, remains a significant challenge in the deployment of Edge ML solutions.\n\n\n4.2.4 Example Use Cases\nIn highlighting use cases such as autonomous vehicles, smart homes and buildings, and industrial IoT, we aim to underscore the diverse range of environments where Edge ML can be a potent force. The below examples were chosen as they encapsulate scenarios where real-time data processing, reduced latency, and enhanced privacy are not just beneficial but often critical to the operation and success of these technologies. They serve to demonstrate the pivotal role that Edge ML can play in spearheading advancements in different sectors, fostering innovation, and paving the way for more intelligent, responsive, and adaptive systems.\nAutonomous Vehicles\nAutonomous vehicles stand as a beacon of Edge ML’s potential. These vehicles rely heavily on real-time data processing to navigate and make decisions. The localized machine learning models help in swiftly analyzing data from various sensors to make immediate driving decisions, essentially ensuring safety and smooth operation.\nSmart Homes and Buildings\nIn the context of smart homes and buildings, Edge ML plays a vital role in managing various systems efficiently, from lighting and heating to security systems. By processing data locally, these systems can operate more responsively and in tune with the occupants’ habits and preferences, creating a more harmonized living environment.\nIndustrial IoT\nIndustrial Internet of Things (IoT) leverages Edge ML to monitor and control complex industrial processes. Here, machine learning models can analyze data from a plethora of sensors in real-time, facilitating predictive maintenance, optimizing operations, and improving safety measures, thereby bringing about a revolution in industrial automation and efficiency.\nWhile the aforementioned cases offer a glimpse into the versatility and potential of Edge ML, it is crucial to note that its applicability is vast and not limited to these scenarios. Various other domains, including healthcare, agriculture, and urban planning, to name a few, are exploring and integrating Edge ML to develop solutions that are both innovative and responsive to real-world needs and challenges, heralding a new era of smart, interconnected systems."
  },
  {
    "objectID": "embedded_ml.html#tiny-ml",
    "href": "embedded_ml.html#tiny-ml",
    "title": "4  Embedded ML",
    "section": "4.3 Tiny ML",
    "text": "4.3 Tiny ML\n\n4.3.1 Characteristics\nDefinition of TinyML\nTinyML stands at the intersection of embedded systems and machine learning, representing an emerging field that brings intelligent algorithms directly onto minuscule microcontrollers and sensors. These microcontrollers operate in environments characterized by severe resource constraints, particularly with respect to memory, storage, and computational power.\nOn-Device Machine Learning\nIn the realm of TinyML, the crux of operations lies in on-device machine learning. This means the machine learning models are not just deployed but also trained directly on the device itself, without relying on external servers or cloud infrastructures. By doing this, TinyML can facilitate intelligent decision-making right where the data is being generated, thereby making real-time insights and responses a possibility, even in environments where connectivity is limited or non-existent.\nLow Power and Resource-Constrained Environments\nTinyML thrives in low power and resource-constrained environments. These environments demand solutions that are highly optimized to function within the limited available resources. TinyML achieves this through specialized algorithms and models that are designed to deliver reasonable performance while consuming minimal energy, hence ensuring prolonged operational periods even in battery-powered devices.\n\n\n4.3.2 Benefits\nExtremely Low Latency\nA defining benefit of TinyML is its capacity to offer extremely low latency. Given that the computation happens directly on the device, the time taken to transmit data to external servers and receive a response is eliminated. This becomes a critical advantage in applications where instantaneous decision-making is required, thereby facilitating rapid responses to changing conditions.\nHigh Data Security\nWith TinyML, data security is inherently enhanced. Since data processing and analysis are confined to the device itself, the risk of data interception during transmission is practically nullified. This localized approach to data handling ensures that sensitive information remains confined to the device, thereby bolstering the security of user data.\nEnergy Efficiency\nTinyML operates under a paradigm of energy efficiency, a necessity given the resource-constrained environments in which it functions. Through the implementation of lean algorithms and optimized computational methods, TinyML ensures that devices can perform complex tasks without draining battery life rapidly, making it a sustainable choice for long-term deployments.\n\n\n4.3.3 Challenges\nLimited Computational Capabilities\nHowever, the transition to TinyML is not without challenges. The primary constraint lies in the limited computational capabilities of the devices. The need to function within constrained environments means that the models deployed must be simplified, potentially impacting the accuracy and sophistication of the solutions developed.\nComplex Development Cycle\nTinyML also introduces a complex development cycle. Creating models that are both lightweight and effective requires a deep understanding of machine learning principles, coupled with expertise in embedded systems. This complexity necessitates a collaborative approach to development, where expertise in multiple domains is a prerequisite for success.\nModel Optimization and Compression\nOne of the central challenges in TinyML is model optimization and compression. Developing machine learning models that can function effectively within the limited memory and computational power of microcontrollers demands innovative approaches to model design and implementation. Developers are often tasked with striking a delicate balance, optimizing models to a point where they retain efficacy while fitting within the stringent resource constraints.\n\n\n4.3.4 Example Use Cases\nWearable Devices\nIn the context of wearable devices, TinyML opens up possibilities for smarter, more responsive gadgets. From fitness trackers that can provide real-time feedback on your workouts to smart glasses that can process visual data on the go, TinyML is revolutionizing the way we interact with wearable technology, offering personalized experiences and insights directly from the device itself.\nPredictive Maintenance\nTinyML finds a significant role in predictive maintenance, particularly in industrial settings. By deploying TinyML algorithms on sensors and devices that monitor equipment health, companies can identify potential issues before they escalate, thereby reducing downtime and averting costly breakdowns. The ability to analyze data on-site ensures swift responses, potentially preventing minor issues from escalating into significant problems.\nAnomaly detection\nTinyML can be used to develop anomaly detection models that can identify unusual patterns in data. The best anomaly detection algorithm for a particular application will depend on the type of data and the desired outcome, but here are some examples of anomaly detection with TinyML. A smart factory could use TinyML to develop an anomaly detection model that can monitor industrial processes and identify unusual patterns in the data. This could help to prevent accidents and improve the quality of products. A security company could use TinyML to develop an anomaly detection model that can monitor network traffic and identify unusual patterns in the data. This could help to detect and prevent cyber attacks. A healthcare company could use TinyML to develop an anomaly detection model that can monitor patient data and identify unusual patterns in the data. This could help to detect diseases early and provide better treatment to patients.\nEnvironmental Monitoring\nIn environmental monitoring, TinyML can facilitate the real-time analysis of data collected from various sensors deployed in the field. These could range from monitoring air quality in urban areas to tracking wildlife movements in protected areas. Through TinyML, data can be processed locally, enabling swift responses to changing conditions and providing a nuanced understanding of environmental patterns and trends, which is crucial for informed decision-making and policy formulation.\nIn essence, TinyML stands as a vanguard in the evolution of machine learning, fostering innovation in a myriad of fields by bringing intelligence directly to the edge, where data meets the real world. Its potential to revolutionize how we interact with technology and the world around us is immense, promising a future where devices are not just connected but also intelligent, capable of making decisions and responding to the environment in real-time.\nTinyML stands as a vanguard in the evolution of machine learning, fostering innovation in a myriad of fields by bringing intelligence directly to the edge, where data meets the real world. Its potential to revolutionize how we interact with technology and the world around us is immense, promising a future where devices are not just connected but also intelligent, capable of making decisions and responding to the environment in real-time."
  },
  {
    "objectID": "embedded_ml.html#comparison",
    "href": "embedded_ml.html#comparison",
    "title": "4  Embedded ML",
    "section": "4.4 Comparison",
    "text": "4.4 Comparison\nThus far, we have discussed each of the different ML variants in isolation. Now, let’s put them all together. Here’s a table that offers a comparative analysis of Cloud ML, Edge ML, and TinyML based on various features and aspects. It helps to provide a clear perspective on the distinguishing factors and unique advantages of each, helping in making informed decisions based on specific requirements and constraints for a given application or project.\n\n\n\n\n\n\n\n\n\nFeature/Aspect\nCloud ML\nEdge ML\nTinyML\n\n\n\n\nProcessing Location\nCentralized servers (Data Centers)\nLocal devices (closer to data sources)\nOn-device (microcontrollers, embedded systems)\n\n\nLatency\nHigh (Depends on internet connectivity)\nModerate (Reduced latency compared to Cloud ML)\nLow (Immediate processing without network delay)\n\n\nData Privacy\nModerate (Data transmitted over networks)\nHigh (Data remains on local networks)\nVery High (Data processed on-device, not transmitted)\n\n\nComputational Power\nHigh (Utilizes powerful data center infrastructure)\nModerate (Utilizes local device capabilities)\nLow (Limited to the power of the embedded system)\n\n\nEnergy Consumption\nHigh (Data centers consume significant energy)\nModerate (Less than data centers, more than TinyML)\nLow (Highly energy-efficient, designed for low power)\n\n\nScalability\nHigh (Easy to scale with additional server resources)\nModerate (Depends on local device capabilities)\nLow (Limited by the hardware resources of the device)\n\n\nCost\nHigh (Recurring costs for server usage, maintenance)\nVariable (Depends on the complexity of local setup)\nLow (Primarily upfront costs for hardware components)\n\n\nConnectivity Dependence\nHigh (Requires stable internet connectivity)\nLow (Can operate with intermittent connectivity)\nVery Low (Can operate without any network connectivity)\n\n\nReal-time Processing\nModerate (Can be affected by network latency)\nHigh (Capable of real-time processing locally)\nVery High (Immediate processing with minimal latency)\n\n\nApplication Examples\nBig Data Analysis, Virtual Assistants\nAutonomous Vehicles, Smart Homes\nWearables, Sensor Networks\n\n\nDevelopment Complexity\nModerate to High (Requires knowledge in cloud computing)\nModerate (Requires knowledge in local network setup)\nModerate to High (Requires expertise in embedded systems)\n\n\n\nThe table delineates a comparative analysis of these three paradigms, highlighting the distinctions in processing location, latency, data privacy, computational power, energy consumption, scalability, cost, connectivity dependence, real-time processing capabilities, and application examples. Cloud ML, centralized and power-abundant, stands as a giant in computational prowess but may face hurdles in latency and energy consumption. Edge ML emerges as a bridge, offering moderate computational power with reduced latency, situated closer to the data sources, thus ensuring heightened data privacy. TinyML, the newest entrant, operates on constrained devices, excelling in low-energy consumption and on-device processing, promising significant strides in applications like wearables and sensor networks. This table serves as a roadmap for enthusiasts and professionals alike, offering a panoramic view of the attributes and capabilities of each paradigm, aiding in the selection of the most suitable approach for specific project requirements."
  },
  {
    "objectID": "embedded_ml.html#evolution-timeline",
    "href": "embedded_ml.html#evolution-timeline",
    "title": "4  Embedded ML",
    "section": "4.5 Evolution Timeline",
    "text": "4.5 Evolution Timeline\n\n4.5.1 Late 1990s - Early 2000s: Birth of Wireless Sensor Networks\nDuring the late 1990s and early 2000s, wireless sensor networks (WSNs) emerged as a significant milestone in the field of information technology. These networks comprised sensor nodes capable of collecting and transmitting data wirelessly. With the ability to monitor a variety of environmental conditions such as temperature, humidity, and light, WSNs found applications in various fields including industrial automation, healthcare, and environmental monitoring. This period saw the development of initial standardized protocols like Zigbee, facilitating secure and reliable data transmission in WSNs.\n\n\n4.5.2 Mid-2000s: Internet of Things (IoT) Emergence\nAs we moved into the mid-2000s, the concept of the Internet of Things (IoT) started to take shape. IoT extended the principles of WSNs, interconnecting various devices and allowing them to communicate and share data over the internet. The integration of embedded systems in IoT devices enabled smarter operations, with devices now capable of not just collecting data but also processing it to make intelligent decisions. This period saw the proliferation of smart homes and industrial IoT, revolutionizing the way we interact with devices and systems in our surroundings.\n\n\n4.5.3 Late 2000s - Early 2010s: Smartphone Revolution & Mobile Computing\nThe late 2000s saw the advent of the smartphone revolution, which significantly influenced the evolution of embedded systems. Smartphones became powerful computing devices equipped with various sensors and embedded systems capable of performing complex tasks. The integration of embedded systems into smartphones laid the groundwork for mobile computing, with applications spanning from gaming to navigation and health monitoring.\n\n\n4.5.4 Mid-2010s: Big Data & Edge Computing\nBy the mid-2010s, the sheer volume of data generated by interconnected devices necessitated new approaches to data processing. Big Data technologies emerged to handle this influx of data, and concurrently, the concept of edge computing began gaining traction. Edge computing brought data processing capabilities closer to the source of data, minimizing latency and bandwidth use. Embedded systems evolved to support edge computing, with devices now capable of performing substantial data processing locally, reducing the dependence on centralized data centers.\n\n\n4.5.5 Late 2010s - Early 2020s: Machine Learning & AI Integration\nTowards the end of the 2010s and into the early 2020s, machine learning and AI started becoming integral components of embedded systems. The integration of AI capabilities into embedded systems facilitated the development of smart devices with improved decision-making and predictive capabilities. This period saw advancements in natural language processing, computer vision, and predictive analytics, with embedded systems now capable of supporting complex AI algorithms and applications.\n\n\n4.5.6 Early 2020s: Advent of tinyML\nAs we entered the 2020s, the field witnessed the advent of tiny Machine Learning (tinyML), a development that brought machine learning capabilities to ultra-low power microcontrollers. TinyML enabled the deployment of ML models directly onto small embedded devices, allowing for intelligent data processing at the edge, even on devices with limited computational resources. This has opened up new possibilities for IoT devices, making them smarter and more autonomous than ever before.\n\n\n4.5.7 2023 and Beyond: Towards a Ubiquitous Embedded AI Era\nAs we progress further into this decade, we anticipate a transformative phase in the landscape of technology where embedded AI and TinyML evolves from being a noteworthy innovation to a pervasive force integral to our technological infrastructures, thus ushering in an era of ubiquitous embedded AI. The horizon of embedded AI is massive and expansive, potentially bringing us a future where the boundaries between artificial intelligence and daily functionalities become increasingly blurred, fostering a new era of innovation and efficiency."
  },
  {
    "objectID": "embedded_ml.html#conclusion",
    "href": "embedded_ml.html#conclusion",
    "title": "4  Embedded ML",
    "section": "4.6 Conclusion",
    "text": "4.6 Conclusion\nIn this chapter, we provided an overview of the emerging landscape of embedded machine learning, spanning cloud, edge, and tiny ML approaches. Cloud-based machine learning enables powerful and accurate models by leveraging the vast compute resources of cloud platforms. However, cloud ML incurs latency, connectivity, privacy, and cost limitations for many embedded use cases. Edge ML addresses these constraints by deploying ML inference directly on edge devices, providing lower latency and reduced connectivity needs. TinyML miniaturizes ML models to run directly on microcontrollers and other highly resource-constrained devices, enabling a new class of intelligent applications.\nEach approach has tradeoffs in terms of model complexity, latency, privacy, connectivity requirements, and hardware costs. Cloud ML enables the most sophisticated models, while edge and tiny ML simplify models to meet real-time and hardware constraints. Over time, we expect embedded ML approaches to converge, with cloud pre-training enabling sophisticated edge and tiny ML execution. Federated learning and on-device learning will also allow embedded devices to improve their models by learning from real-world data over time.\nThe landscape of embedded ML is rapidly evolving to enable intelligent applications across a spectrum of devices and use cases. This chapter provided a snapshot of current embedded ML approaches and their capabilities. As algorithms, hardware, and connectivity continue improving, embedded devices of all scales will become even more capable, unlocking transformative new applications of artificial intelligence."
  },
  {
    "objectID": "mlworkflow.html#overview",
    "href": "mlworkflow.html#overview",
    "title": "5  ML Workflow",
    "section": "5.1 Overview",
    "text": "5.1 Overview\nA machine learning (ML) workflow is the process of developing, deploying, and maintaining ML models. It typically consists of the following steps:\n\nDefine the problem. What are you trying to achieve with your ML model? Do you want to classify images, predict customer churn, or generate text? Once you have a clear understanding of the problem, you can start to collect data and choose a suitable ML algorithm.\nCollect and prepare data. ML models are trained on data, so it’s important to collect a high-quality dataset that is representative of the real-world problem you’re trying to solve. Once you have your data, you need to clean it and prepare it for training. This may involve tasks such as removing outliers, imputing missing values, and scaling features.\nChoose an ML algorithm. There are many different ML algorithms available, each with its own strengths and weaknesses. The best algorithm for your project will depend on the type of data you have and the problem you’re trying to solve.\nTrain the model. Once you have chosen an ML algorithm, you need to train the model on your prepared data. This process can take some time, depending on the size and complexity of your dataset.\nEvaluate the model. Once the model is trained, you need to evaluate its performance on a held-out test set. This will give you an idea of how well the model will generalize to new data.\nDeploy the model. Once you’re satisfied with the performance of the model, you can deploy it to production. This may involve integrating the model into a software application or making it available as a web service.\nMonitor and maintain the model. Once the model is deployed, you need to monitor its performance and make updates as needed. This is because the real world is constantly changing, and your model may need to be updated to reflect these changes.\n\nThe ML workflow is an iterative process. Once you have deployed a model, you may find that it needs to be retrained on new data or that the algorithm needs to be adjusted. It’s important to monitor the performance of your model closely and make changes as needed to ensure that it is still meeting your needs. In addition to the above steps, there are a number of other important considerations for ML workflows, such as:\n\nVersion control: It’s important to track changes to your code and data so that you can easily reproduce your results and revert to previous versions if necessary.\nDocumentation: It’s important to document your ML workflow so that others can understand and reproduce your work.\nTesting: It’s important to test your ML workflow thoroughly to ensure that it is working as expected.\nSecurity: It’s important to consider the security of your ML workflow and data, especially if you are deploying your model to production."
  },
  {
    "objectID": "mlworkflow.html#general-vs.-embedded-ai",
    "href": "mlworkflow.html#general-vs.-embedded-ai",
    "title": "5  ML Workflow",
    "section": "5.2 General vs. Embedded AI",
    "text": "5.2 General vs. Embedded AI\nThe ML workflow delineated above serves as a comprehensive guide applicable broadly across various platforms and ecosystems, encompassing cloud-based solutions, edge computing, and tinyML. However, when we delineate the nuances of the general ML workflow and contrast it with the workflow in Embedded AI environments, we encounter a series of intricate differences and complexities. These nuances not only elevate the embedded AI workflow to a challenging and captivating domain but also open avenues for remarkable innovations and advancements.\nNow, let’s explore these differences in detail:\n\nResource Optimization:\n\nGeneral ML Workflow: Generally has the luxury of substantial computational resources available in cloud or data center environments. It focuses more on model accuracy and performance.\nEmbedded AI Workflow: Needs meticulous planning and execution to optimize the model’s size and computational demands, as they have to operate within the limited resources available in embedded systems. Techniques like model quantization and pruning become essential.\n\nReal-time Processing:\n\nGeneral ML Workflow: The emphasis on real-time processing is usually less, and batch processing of data is quite common.\nEmbedded AI Workflow: Focuses heavily on real-time data processing, necessitating a workflow where low latency and rapid execution are a priority, especially in applications like autonomous driving and industrial automation.\n\nData Management and Privacy:\n\nGeneral ML Workflow: Data is typically processed in centralized locations, sometimes requiring extensive data transfer, with a focus on securing data during transit and storage.\nEmbedded AI Workflow: Promotes edge computing, which facilitates data processing closer to the source, reducing data transmission needs and enhancing privacy by keeping sensitive data localized.\n\nHardware-Software Integration:\n\nGeneral ML Workflow: Often operates on general-purpose hardware platforms with software development happening somewhat independently.\nEmbedded AI Workflow: Involves a tighter hardware-software co-design where both are developed in tandem to achieve optimal performance and efficiency, integrating custom chips or utilizing hardware accelerators."
  },
  {
    "objectID": "mlworkflow.html#roles-responsibilities",
    "href": "mlworkflow.html#roles-responsibilities",
    "title": "5  ML Workflow",
    "section": "5.3 Roles & Responsibilities",
    "text": "5.3 Roles & Responsibilities\nAs we work through the various tasks at hand, you will realize that there is a lot of complexity. Creating a machine learning solution, particularly for embedded AI systems, is a multidisciplinary endeavor involving various experts and specialists. Here is a list of personnel that are typically involved in the process, along with brief descriptions of their roles:\nProject Manager:\n\nCoordinates and manages the overall project.\nEnsures all team members are working synergistically.\nResponsible for project timelines and milestones.\n\nDomain Experts:\n\nProvide insights into the specific domain where the AI system will be implemented.\nHelp in defining project requirements and constraints based on domain-specific knowledge.\n\nData Scientists:\n\nSpecialize in analyzing data to develop machine learning models.\nResponsible for data cleaning, exploration, and feature engineering.\n\nMachine Learning Engineers:\n\nFocus on the development and deployment of machine learning models.\nCollaborate with data scientists to optimize models for embedded systems.\n\nData Engineers:\n\nResponsible for managing and optimizing data pipelines.\nWork on the storage and retrieval of data used for machine learning model training.\n\nEmbedded Systems Engineers:\n\nFocus on integrating machine learning models into embedded systems.\nOptimize system resources for running AI applications.\n\nSoftware Developers:\n\nDevelop software components that interface with the machine learning models.\nResponsible for implementing APIs and other integration points for the AI system.\n\nHardware Engineers:\n\nInvolved in designing and optimizing the hardware that hosts the embedded AI system.\nCollaborate with embedded systems engineers to ensure compatibility.\n\nUI/UX Designers:\n\nDesign the user interface and experience for interacting with the AI system.\nFocus on user-centric design and ensuring usability.\n\nQuality Assurance (QA) Engineers:\n\nResponsible for testing the overall system to ensure it meets quality standards.\nWork on identifying bugs and issues before the system is deployed.\n\nEthicists and Legal Advisors:\n\nConsult on the ethical implications of the AI system.\nEnsure compliance with legal and regulatory requirements related to AI.\n\nOperations and Maintenance Personnel:\n\nResponsible for monitoring the system after deployment.\nWork on maintaining and upgrading the system as needed.\n\nSecurity Specialists:\n\nFocus on ensuring the security of the AI system.\nWork on identifying and mitigating potential security vulnerabilities.\n\nDon’t worry! You don’t have to be a one-stop ninja.\nUnderstanding the diversified roles and responsibilities is paramount in the journey to building a successful machine learning project. As we traverse the upcoming chapters, we will wear the different hats, embracing the essence and expertise of each role described herein. This immersive method nurtures a deep-seated appreciation for the inherent complexities, thereby facilitating an encompassing grasp of the multifaceted dynamics of embedded AI projects.\nMoreover, this well-rounded insight promotes not only seamless collaboration and unified efforts but also fosters an environment ripe for innovation. It enables us to identify areas where cross-disciplinary insights might foster novel thoughts, nurturing ideas and ushering in breakthroughs in the field. Additionally, being aware of the intricacies of each role allows us to anticipate potential obstacles and strategize effectively, guiding the project towards triumph with foresight and detailed understanding.\nAs we advance, we encourage you to hold a deep appreciation for the amalgamation of expertise that contributes to the fruition of a successful machine learning initiative. In later discussions, particularly when we delve into MLOps, we will examine these different facets or personas in greater detail. It’s worth noting at this point that the range of topics touched upon might seem overwhelming. This endeavor aims to provide you with a comprehensive view of the intricacies involved in constructing an embedded AI system, without the expectation of mastering every detail personally."
  },
  {
    "objectID": "data_engineering.html#introduction",
    "href": "data_engineering.html#introduction",
    "title": "6  Data Engineering",
    "section": "6.1 Introduction",
    "text": "6.1 Introduction\nExplanation: This section establishes the groundwork, defining data engineering and explaining its importance and role in Embedded AI. A well-rounded introduction will help in establishing the foundation for the readers.\n\nDefinition and Importance of Data Engineering in AI\nRole of Data Engineering in Embedded AI\nSynergy with Machine Learning and Deep Learning"
  },
  {
    "objectID": "data_engineering.html#problem",
    "href": "data_engineering.html#problem",
    "title": "6  Data Engineering",
    "section": "6.2 Problem",
    "text": "6.2 Problem\nExplanation: This section is a crucial starting point in any data engineering project, as it lays the groundwork for the project’s trajectory and ultimate success. Here’s a brief explanation of why each subsection within the “Problem Definition” is important:\n\nIdentifying the Problem\nSetting Clear Objectives\nBenchmarks for Success\nStakeholder Engagement and Understanding"
  },
  {
    "objectID": "data_engineering.html#data-sourcing",
    "href": "data_engineering.html#data-sourcing",
    "title": "6  Data Engineering",
    "section": "6.3 Data Sourcing",
    "text": "6.3 Data Sourcing\nExplanation: This section delves into the first step in data engineering - gathering data. Understanding various data types and sources is vital for developing robust AI systems, especially in the context of embedded systems where resources might be limited.\n\nData Sources\nData Types: Structured, Semi-Structured, and Unstructured\nReal-time Data Processing in Embedded Systems"
  },
  {
    "objectID": "data_engineering.html#data-storage-and-management",
    "href": "data_engineering.html#data-storage-and-management",
    "title": "6  Data Engineering",
    "section": "6.4 Data Storage and Management",
    "text": "6.4 Data Storage and Management\nExplanation: Data must be stored and managed efficiently to facilitate easy access and processing. This section would provide insights into different data storage options and their respective advantages and challenges in embedded systems.\n\nDatabase Selection: SQL vs NoSQL\nData Warehousing\nData Lakes\nMetadata Management"
  },
  {
    "objectID": "data_engineering.html#data-processing",
    "href": "data_engineering.html#data-processing",
    "title": "6  Data Engineering",
    "section": "6.5 Data Processing",
    "text": "6.5 Data Processing\nExplanation: Data processing is a pivotal step in transforming raw data into a usable format. This section provides a deep dive into the necessary processes, including cleaning, integration, and establishing data pipelines, all crucial for streamlining operations in embedded AI systems.\n\nData Cleaning and Transformation\nData Integration\nData Pipelines\nStream Processing"
  },
  {
    "objectID": "data_engineering.html#data-quality",
    "href": "data_engineering.html#data-quality",
    "title": "6  Data Engineering",
    "section": "6.6 Data Quality",
    "text": "6.6 Data Quality\nExplanation: Ensuring data quality is critical to developing reliable AI models. This section outlines various strategies to maintain and assess data quality.\n\nData Validation\nHandling Missing Values\nOutlier Detection"
  },
  {
    "objectID": "data_engineering.html#feature-engineering",
    "href": "data_engineering.html#feature-engineering",
    "title": "6  Data Engineering",
    "section": "6.7 Feature Engineering",
    "text": "6.7 Feature Engineering\nExplanation: Feature engineering involves selecting and transforming variables to improve the performance of AI models. It’s vital in embedded AI systems where computational resources are limited, and optimized feature sets can significantly improve performance.\n\nImportance of Feature Engineering\nTechniques of Feature Selection\nFeature Transformation for Embedded Systems\nReal-time Feature Engineering in Embedded Systems"
  },
  {
    "objectID": "data_engineering.html#data-labeling",
    "href": "data_engineering.html#data-labeling",
    "title": "6  Data Engineering",
    "section": "6.8 Data Labeling",
    "text": "6.8 Data Labeling\nExplanation: Labeling is an essential part of preparing data for supervised learning. This section focuses on various strategies and tools available for data labeling, a vital process in the data preparation phase.\n\nManual Data Labeling\nAutomated Data Labeling\nLabeling Tools"
  },
  {
    "objectID": "data_engineering.html#data-version-control",
    "href": "data_engineering.html#data-version-control",
    "title": "6  Data Engineering",
    "section": "6.9 Data Version Control",
    "text": "6.9 Data Version Control\nExplanation: Version control is critical for managing changes and tracking versions of datasets during the development of AI models, facilitating reproducibility and collaboration.\n\nVersion Control Systems\nData Versioning in ML Projects"
  },
  {
    "objectID": "data_engineering.html#optimizing-data-for-embedded-ai",
    "href": "data_engineering.html#optimizing-data-for-embedded-ai",
    "title": "6  Data Engineering",
    "section": "6.10 Optimizing Data for Embedded AI",
    "text": "6.10 Optimizing Data for Embedded AI\nExplanation: This section concentrates on optimization techniques specifically suited for embedded systems, focusing on strategies to reduce data volume and enhance storage and retrieval efficiency, crucial for resource-constrained embedded environments.\n\nData Reduction Techniques\nOptimizing Data Storage and Retrieval"
  },
  {
    "objectID": "data_engineering.html#challenges-in-data-engineering",
    "href": "data_engineering.html#challenges-in-data-engineering",
    "title": "6  Data Engineering",
    "section": "6.11 Challenges in Data Engineering",
    "text": "6.11 Challenges in Data Engineering\nExplanation: Understanding potential challenges can help in devising strategies to mitigate them. This section discusses common challenges encountered in data engineering, particularly focusing on embedded systems.\n\nScalability\nData Security and Privacy\nData Bias and Representativity"
  },
  {
    "objectID": "data_engineering.html#conclusion",
    "href": "data_engineering.html#conclusion",
    "title": "6  Data Engineering",
    "section": "6.12 Conclusion",
    "text": "6.12 Conclusion\n\nThe Future of Data Engineering in Embedded AI\nKey Takeaways"
  },
  {
    "objectID": "training.html#introduction",
    "href": "training.html#introduction",
    "title": "7  ML Training",
    "section": "7.1 Introduction",
    "text": "7.1 Introduction\n\nImportance of ML Training\nOverview of ML Training Process"
  },
  {
    "objectID": "training.html#data-preparation",
    "href": "training.html#data-preparation",
    "title": "7  ML Training",
    "section": "7.2 Data Preparation",
    "text": "7.2 Data Preparation\n\nData Collection\nData Cleaning\nData Augmentation\nFeature Engineering\nSplitting the Data (Training, Validation, and Test Sets)"
  },
  {
    "objectID": "training.html#model-selection",
    "href": "training.html#model-selection",
    "title": "7  ML Training",
    "section": "7.3 Model Selection",
    "text": "7.3 Model Selection\n\nOverview of Model Types\nCriteria for Model Selection\nModel Complexity and Capacity"
  },
  {
    "objectID": "training.html#training-algorithms",
    "href": "training.html#training-algorithms",
    "title": "7  ML Training",
    "section": "7.4 Training Algorithms",
    "text": "7.4 Training Algorithms\n\nGradient Descent\n\nBatch Gradient Descent\nStochastic Gradient Descent\nMini-Batch Gradient Descent\n\nOptimization Algorithms\n\nAdam\nRMSprop\n\nMomentum"
  },
  {
    "objectID": "training.html#loss-functions",
    "href": "training.html#loss-functions",
    "title": "7  ML Training",
    "section": "7.5 Loss Functions",
    "text": "7.5 Loss Functions\n\nMean Squared Error (MSE)\nCross-Entropy Loss\nHuber Loss\nCustom Loss Functions"
  },
  {
    "objectID": "training.html#regularization-techniques",
    "href": "training.html#regularization-techniques",
    "title": "7  ML Training",
    "section": "7.6 Regularization Techniques",
    "text": "7.6 Regularization Techniques\n\nL1 and L2 Regularization\nDropout\nBatch Normalization\nEarly Stopping"
  },
  {
    "objectID": "training.html#model-evaluation",
    "href": "training.html#model-evaluation",
    "title": "7  ML Training",
    "section": "7.7 Model Evaluation",
    "text": "7.7 Model Evaluation\n\nEvaluation Metrics\n\n.1 Accuracy\n.2 Precision and Recall\n.3 F1-Score\n\nConfusion Matrix\nROC and AUC"
  },
  {
    "objectID": "training.html#hyperparameter-tuning",
    "href": "training.html#hyperparameter-tuning",
    "title": "7  ML Training",
    "section": "7.8 Hyperparameter Tuning",
    "text": "7.8 Hyperparameter Tuning\n\nGrid Search\nRandom Search\nBayesian Optimization"
  },
  {
    "objectID": "training.html#scaling-up-training",
    "href": "training.html#scaling-up-training",
    "title": "7  ML Training",
    "section": "7.9 Scaling Up Training",
    "text": "7.9 Scaling Up Training\n\nParallel Training\nDistributed Training\nTraining with GPUs"
  },
  {
    "objectID": "training.html#conclusion",
    "href": "training.html#conclusion",
    "title": "7  ML Training",
    "section": "7.10 Conclusion",
    "text": "7.10 Conclusion"
  },
  {
    "objectID": "efficient_ai.html#introduction",
    "href": "efficient_ai.html#introduction",
    "title": "8  Efficient AI",
    "section": "8.1 Introduction",
    "text": "8.1 Introduction\nExplanation: The introduction sets the stage for the entire chapter, offering readers an insight into the critical role efficiency plays in the sphere of AI. It outlines the core objectives of the chapter, providing context and framing the ensuing discussion.\n\nBackground and Importance of Efficiency in AI\nDiscuss how Cloud, Edge and TinyML differ (again)"
  },
  {
    "objectID": "efficient_ai.html#the-need-for-efficient-ai",
    "href": "efficient_ai.html#the-need-for-efficient-ai",
    "title": "8  Efficient AI",
    "section": "8.2 The Need for Efficient AI",
    "text": "8.2 The Need for Efficient AI\nExplanation: This section articulates the pressing necessity for efficiency in AI systems, particularly in resource-constrained environments. Discussing these aspects will underline the crucial role of efficient AI in modern technology deployments, facilitating a smooth transition to discussing potential approaches in the next section.\n\nResource Constraints in Embedded Systems\nEnergy Efficiency\nComputational Efficiency\nLatency Reduction\nReal-time Processing Requirements"
  },
  {
    "objectID": "efficient_ai.html#approaches-to-efficient-ai",
    "href": "efficient_ai.html#approaches-to-efficient-ai",
    "title": "8  Efficient AI",
    "section": "8.3 Approaches to Efficient AI",
    "text": "8.3 Approaches to Efficient AI\nExplanation: After establishing the necessity for efficient AI, this section delves into various strategies and methodologies to achieve it. It explores the technical avenues available to optimize AI models and algorithms, thus serving as a bridge between the identified needs and the practical solutions presented in the following sections on specific efficient AI models.\n\nAlgorithm Optimization\nModel Compression\nHardware-Aware Neural Architecture Search (NAS)\nCompiler Optimizations for AI\nML for ML Systems"
  },
  {
    "objectID": "efficient_ai.html#efficient-ai-models",
    "href": "efficient_ai.html#efficient-ai-models",
    "title": "8  Efficient AI",
    "section": "8.4 Efficient AI Models",
    "text": "8.4 Efficient AI Models\nExplanation: This section offers an in-depth exploration of different AI models that are designed to be efficient in terms of computational resources and energy. It not only discusses the models but also offers insights into how they are optimized, thus preparing the ground for the benchmarking and evaluation section where these models are assessed and compared.\n\nModel compression techniques\n\nPruning\nQuantization\nKnowledge distillation\n\nEfficient model architectures\n\nMobileNet\nSqueezeNet\nResNet variants"
  },
  {
    "objectID": "efficient_ai.html#efficient-inference",
    "href": "efficient_ai.html#efficient-inference",
    "title": "8  Efficient AI",
    "section": "8.5 Efficient Inference",
    "text": "8.5 Efficient Inference\n\nOptimized inference engines\n\nTPUs\nEdge TPU\nNN accelerators\n\nModel optimizations\n\nQuantization\nPruning\nNeural architecture search\n\nFramework optimizations\n\nTensorFlow Lite\nPyTorch Mobile"
  },
  {
    "objectID": "efficient_ai.html#efficient-training",
    "href": "efficient_ai.html#efficient-training",
    "title": "8  Efficient AI",
    "section": "8.6 Efficient Training",
    "text": "8.6 Efficient Training\n\nTechniques\n\nPruning\nQuantization-aware training\nKnowledge distillation\n\nLow precision training\n\nFP16\nINT8\nLower bit widths"
  },
  {
    "objectID": "efficient_ai.html#benchmarking-and-evaluation-of-ai-models",
    "href": "efficient_ai.html#benchmarking-and-evaluation-of-ai-models",
    "title": "8  Efficient AI",
    "section": "8.7 Benchmarking and Evaluation of AI Models",
    "text": "8.7 Benchmarking and Evaluation of AI Models\nExplanation: This part of the chapter emphasizes the importance of evaluating the efficiency of AI models using appropriate metrics and benchmarks. This process is vital to ensuring the effectiveness of the approaches discussed earlier and seamlessly connects with case studies where these benchmarks can be seen in a real-world context.\n\nMetrics for Efficiency\n\nFLOPs (Floating Point Operations)\nMemory Usage\nPower Consumption\nInference Time\n\nBenchmark Datasets and Tools\nComparative Analysis of AI Models\nEEMBC, MLPerf Tiny, Edge"
  },
  {
    "objectID": "efficient_ai.html#caveat-on-efficiency-metrics",
    "href": "efficient_ai.html#caveat-on-efficiency-metrics",
    "title": "8  Efficient AI",
    "section": "8.8 Caveat on Efficiency Metrics",
    "text": "8.8 Caveat on Efficiency Metrics\nExplanation: This section emphasizes the diverse aspects that constitute “efficiency” in machine learning systems. It aims to guide readers in identifying the crucial metrics that matter, depending on the specific use case, and underscores the importance of considering these metrics early in the ML workflow.\n\nMulti-faceted nature of efficiency in ML systems\n\nBeyond accuracy: various critical metrics\nLatency as a pivotal component\n\nImportance of low latency in real-time applications\n\nSpecific application dictates acceptable latency\n\nPower efficiency in embedded systems\n\nStrategies for extending battery life\nRole of specialized hardware\n\nConsiderations for cost-efficient deployments\n\nHardware costs vs. model accuracy\nBalancing accuracy, latency, and costs\n\nTailoring efficiency to the product\n\nComparison: automotive, mobile, smart home applications\nDistinct constraints necessitate diverse efficiency approaches\n\nEarly integration of efficiency metrics in ML workflow\n\nInfluence on architecture, hardware, and algorithm selection\nProactive consideration of efficiency metrics"
  },
  {
    "objectID": "efficient_ai.html#emerging-directions",
    "href": "efficient_ai.html#emerging-directions",
    "title": "8  Efficient AI",
    "section": "8.9 Emerging Directions",
    "text": "8.9 Emerging Directions\n\nAutomated model search\nMulti-task learning\n\nMeta learning\nLottery ticket hypothesis\nHardware-algorithm co-design"
  },
  {
    "objectID": "efficient_ai.html#conclusion",
    "href": "efficient_ai.html#conclusion",
    "title": "8  Efficient AI",
    "section": "8.10 Conclusion",
    "text": "8.10 Conclusion\nExplanation: This section synthesizes the information presented throughout the chapter, offering a coherent summary and emphasizing the critical takeaways. It helps in consolidating the knowledge acquired, setting the stage for the subsequent chapters on optimization and deployment."
  },
  {
    "objectID": "frameworks.html#introduction",
    "href": "frameworks.html#introduction",
    "title": "10  ML Frameworks",
    "section": "10.1 Introduction",
    "text": "10.1 Introduction\nExplanation: Discuss what ML frameworks are and why they are important. Also, elaborate on the aspects involved in understanding how an ML framework is developed and deployed.\n\nDefinition of ML Frameworks\nWhat is an embedded ML framework?\nWhy are embedded ML frameworks important?\nChallenges of embedded ML\nBenefits of using embedded ML frameworks, trade-offs, and differences."
  },
  {
    "objectID": "frameworks.html#typical-ml-frameworks",
    "href": "frameworks.html#typical-ml-frameworks",
    "title": "10  ML Frameworks",
    "section": "10.2 Typical ML Frameworks",
    "text": "10.2 Typical ML Frameworks\nExplanation: Discuss the most common types of ML frameworks that are around and provide a high-level overview, so that we can set into motion what makes embedded ML frameworks unique.\n\nTensorFlow, PyTorch, Keras, ONNX Runtime, Scikit-learn\nKey Features and Advantages\nAPI and Programming Paradigms"
  },
  {
    "objectID": "frameworks.html#constraints-for-embedded-ai",
    "href": "frameworks.html#constraints-for-embedded-ai",
    "title": "10  ML Frameworks",
    "section": "10.3 Constraints for Embedded AI",
    "text": "10.3 Constraints for Embedded AI\nExplanation: Describe the constraints of embedded systems, referring to the previous chapters, and remind readers about the challenges and why we need to consider creating lean and efficient solutions.\n\n10.3.1 Hardware\n\nMemory Usage\nProcessing Power\nEnergy Efficiency\nStorage Limitations\nHardware Diversity\n\n\n\n10.3.2 Software\n\nLibrary Dependency\nLack of OS"
  },
  {
    "objectID": "frameworks.html#embedded-ai-frameworks",
    "href": "frameworks.html#embedded-ai-frameworks",
    "title": "10  ML Frameworks",
    "section": "10.4 Embedded AI Frameworks",
    "text": "10.4 Embedded AI Frameworks\nExplanation: Now, discuss specifically about the unique embedded AI frameworks that are available and why they are special, etc.\n\nTensorFlow Lite\nONNX Runtime\nMicroPython\nCMSIS-NN\nEdge Impulse\nOthers (mentioning briefly some less common but significant frameworks)"
  },
  {
    "objectID": "frameworks.html#framework-comparison",
    "href": "frameworks.html#framework-comparison",
    "title": "10  ML Frameworks",
    "section": "10.5 Framework Comparison",
    "text": "10.5 Framework Comparison\nExplanation: Provide a high-level comparison of the different frameworks based on class slides, etc.\n\nTable of differences and similarities"
  },
  {
    "objectID": "frameworks.html#toolchain-integration",
    "href": "frameworks.html#toolchain-integration",
    "title": "10  ML Frameworks",
    "section": "10.6 Toolchain Integration",
    "text": "10.6 Toolchain Integration\nExplanation: Help people understand that it’s more than just the framework, and that elements need to fit into the ecosystem of different things that are around in an embedded system.\n\nCompatibility with Embedded Development Environments\nIntegration with Firmware and Hardware"
  },
  {
    "objectID": "frameworks.html#trends-in-ml-frameworks",
    "href": "frameworks.html#trends-in-ml-frameworks",
    "title": "10  ML Frameworks",
    "section": "10.7 Trends in ML Frameworks",
    "text": "10.7 Trends in ML Frameworks\nExplanation: Discuss where these ML frameworks are heading in the future. Perhaps consider discussing ML for ML frameworks?\n\nFramework Developments on the Horizon\nAnticipated Innovations in the Field"
  },
  {
    "objectID": "frameworks.html#conclusion",
    "href": "frameworks.html#conclusion",
    "title": "10  ML Frameworks",
    "section": "10.8 Conclusion",
    "text": "10.8 Conclusion\n\nSummary of Key Takeaways\nRecommendations for Further Learning"
  },
  {
    "objectID": "hw_acceleration.html#introduction",
    "href": "hw_acceleration.html#introduction",
    "title": "11  AI Hardware",
    "section": "11.1 Introduction",
    "text": "11.1 Introduction\nExplanation: This section lays the groundwork for the chapter, introducing readers to the fundamental concepts of hardware acceleration and its role in enhancing the performance of AI systems, particularly embedded AI. Setting the stage for deeper discussions that follow, this section contextualizes why hardware acceleration is a pivotal topic in the domain of embedded AI."
  },
  {
    "objectID": "hw_acceleration.html#background-and-basics",
    "href": "hw_acceleration.html#background-and-basics",
    "title": "11  AI Hardware",
    "section": "11.2 Background and Basics",
    "text": "11.2 Background and Basics\nExplanation: Here, the readers are provided a foundational understanding of the historical and theoretical aspects of hardware acceleration technologies. This section is essential to give readers a historical perspective and a base that aids in understanding the current state of hardware acceleration technologies.\n\nHistorical Background\nThe Need for Hardware Acceleration\nGeneral Principles of Hardware Acceleration"
  },
  {
    "objectID": "hw_acceleration.html#types-of-hardware-accelerators",
    "href": "hw_acceleration.html#types-of-hardware-accelerators",
    "title": "11  AI Hardware",
    "section": "11.3 Types of Hardware Accelerators",
    "text": "11.3 Types of Hardware Accelerators\nExplanation: This section gives an overview of the hardware options available for accelerating AI tasks, discussing each type in detail and comparing their advantages and disadvantages. It is key for readers to comprehend the different hardware solutions available for specific AI tasks and to make informed decisions when selecting hardware solutions.\n\nGraphics Processing Units (GPUs)\nDigital Signal Processors (DSPs)\nCentral Processing Units (CPUs) with AI Capabilities\nField-Programmable Gate Arrays (FPGAs)\nApplication-Specific Integrated Circuits (ASICs)\nTensor Processing Units (TPUs)\nVision Processing Units (VPUs)\nComparative Analysis of Different Hardware Accelerators"
  },
  {
    "objectID": "hw_acceleration.html#hardware-software-co-design",
    "href": "hw_acceleration.html#hardware-software-co-design",
    "title": "11  AI Hardware",
    "section": "11.4 Hardware-Software Co-Design",
    "text": "11.4 Hardware-Software Co-Design\nExplanation: Focusing on the synergies between hardware and software components, this section discusses the principles and techniques of hardware-software co-design to achieve optimized performance in AI systems. It is crucial to understanding how to design powerful and efficient AI systems that leverage both hardware and software components effectively.\n\nPrinciples of Hardware-Software Co-Design\nOptimization Techniques\nIntegration with Embedded Systems"
  },
  {
    "objectID": "hw_acceleration.html#acceleration-techniques",
    "href": "hw_acceleration.html#acceleration-techniques",
    "title": "11  AI Hardware",
    "section": "11.5 Acceleration Techniques",
    "text": "11.5 Acceleration Techniques\nExplanation: Here, various techniques to enhance computational efficiency and reduce latency through hardware acceleration are discussed. This section is fundamental for readers to understand how to maximize the benefits of hardware acceleration in AI systems, focusing on achieving superior computational performance.\n\nParallel Computing\nPipeline Computing\nMemory Hierarchy Optimization\nInstruction Set Optimization\n\n\n11.5.1 Tools and Frameworks\nExplanation: This section introduces the readers to the array of tools and frameworks available for facilitating work with hardware accelerators. Essential for practical applications, it helps readers understand the resources they have at their disposal for implementing and optimizing hardware-accelerated AI systems.\n\nSoftware Tools for Hardware Acceleration\nDevelopment Environments\nLibraries and APIs\n\n\n\n11.5.2 Case Studies\nExplanation: Providing real-world case studies offers practical insights and lessons from actual hardware-accelerated AI implementations. This section helps readers bridge theory with practice, demonstrating potential benefits and challenges in real-world scenarios and offering a practical perspective on the topics discussed.\n\nReal-world Applications\nCase Study 1: Implementing Neural Networks on FPGAs\nCase Study 2: Optimizing Performance with GPUs\nLessons Learned from Case Studies"
  },
  {
    "objectID": "hw_acceleration.html#challenges-and-solutions",
    "href": "hw_acceleration.html#challenges-and-solutions",
    "title": "11  AI Hardware",
    "section": "11.6 Challenges and Solutions",
    "text": "11.6 Challenges and Solutions\nExplanation: This segment discusses the prevalent challenges encountered in implementing hardware acceleration in AI systems and proposes potential solutions. It equips readers with a realistic view of the complexities involved and guides them in overcoming common hurdles.\n\nPortability/Compatibility Issues\nPower Consumption Concerns\nLatency Reduction\nOvercoming Resource Constraints"
  },
  {
    "objectID": "hw_acceleration.html#future-trends",
    "href": "hw_acceleration.html#future-trends",
    "title": "11  AI Hardware",
    "section": "11.7 Future Trends",
    "text": "11.7 Future Trends\nExplanation: Discussing emerging technologies and trends, this section offers readers a glimpse into the future developments in the field of hardware acceleration. It is vital to help readers stay abreast of the evolving landscape and possibly guide research and development efforts in the sector.\n\nEmerging Hardware Technologies\nEdge AI and Hardware Acceleration"
  },
  {
    "objectID": "hw_acceleration.html#conclusion",
    "href": "hw_acceleration.html#conclusion",
    "title": "11  AI Hardware",
    "section": "11.8 Conclusion",
    "text": "11.8 Conclusion\nExplanation: This section consolidates the key learnings from the chapter, providing a summary and a future outlook on hardware acceleration in embedded AI systems. It helps readers to syn\n\nSummary of Key Points\nThe Future Outlook for Hardware Acceleration in Embedded AI Systems"
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Abadi, Martı́n, Paul Barham, Jianmin Chen, Zhifeng Chen, Andy Davis,\nJeffrey Dean, Matthieu Devin, et al. 2016. “{TensorFlow}: A System for {Large-Scale} Machine Learning.” In 12th\nUSENIX Symposium on Operating Systems Design and Implementation (OSDI\n16), 265–83.\n\n\nARM.com. “The Future Is Being Built on Arm: Market Diversification\nContinues to Drive Strong Royalty and Licensing Growth as Ecosystem\nReaches Quarter of a Trillion Chips Milestone – Arm®.” https://www.arm.com/company/news/2023/02/arm-announces-q3-fy22-results.\n\n\nBank, Dor, Noam Koenigstein, and Raja Giryes. 2023.\n“Autoencoders.” Machine Learning for Data Science\nHandbook: Data Mining and Knowledge Discovery Handbook, 353–74.\n\n\nChen, Tianqi, Thierry Moreau, Ziheng Jiang, Lianmin Zheng, Eddie Yan,\nHaichen Shen, Meghan Cowan, et al. 2018. “{TVM}: An\nAutomated {End-to-End} Optimizing Compiler for Deep\nLearning.” In 13th USENIX Symposium on Operating Systems\nDesign and Implementation (OSDI 18), 578–94.\n\n\nChollet, François. 2015. “Keras.” GitHub\nRepository. https://github.com/fchollet/keras; GitHub.\n\n\nGoodfellow, Ian, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David\nWarde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. 2020.\n“Generative Adversarial Networks.” Communications of\nthe ACM 63 (11): 139–44.\n\n\nJouppi, Norman P, Cliff Young, Nishant Patil, David Patterson, Gaurav\nAgrawal, Raminder Bajwa, Sarah Bates, et al. 2017. “In-Datacenter\nPerformance Analysis of a Tensor Processing Unit.” In\nProceedings of the 44th Annual International Symposium on Computer\nArchitecture, 1–12.\n\n\nKrizhevsky, Alex, Ilya Sutskever, and Geoffrey E Hinton. 2012.\n“Imagenet Classification with Deep Convolutional Neural\nNetworks.” Advances in Neural Information Processing\nSystems 25.\n\n\nPaszke, Adam, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury,\nGregory Chanan, Trevor Killeen, et al. 2019. “Pytorch: An\nImperative Style, High-Performance Deep Learning Library.”\nAdvances in Neural Information Processing Systems 32.\n\n\nRosenblatt, Frank. 1957. The Perceptron, a Perceiving and\nRecognizing Automaton Project Para. Cornell Aeronautical\nLaboratory.\n\n\nRumelhart, David E, Geoffrey E Hinton, and Ronald J Williams. 1986.\n“Learning Representations by Back-Propagating Errors.”\nNature 323 (6088): 533–36.\n\n\nVaswani, Ashish, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion\nJones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. 2017.\n“Attention Is All You Need.” Advances in Neural\nInformation Processing Systems 30."
  },
  {
    "objectID": "tools.html#hardware-kits",
    "href": "tools.html#hardware-kits",
    "title": "Appendix A: Tools",
    "section": "A.1 Hardware Kits",
    "text": "A.1 Hardware Kits\n\nA.1.1 Microcontrollers and Development Boards\n\n\n\n\n\n\n\n\n\n\nNo\nHardware\nProcessor\nFeatures\ntinyML Compatibility\n\n\n\n\n1\nArduino Nano 33 BLE Sense\nARM Cortex-M4\nOnboard sensors, Bluetooth connectivity\nTensorFlow Lite Micro\n\n\n2\nRaspberry Pi Pico\nDual-core Arm Cortex-M0+\nLow-cost, large community support\nTensorFlow Lite Micro\n\n\n3\nSparkFun Edge\nAmbiq Apollo3 Blue\nUltra-low power consumption, onboard microphone\nTensorFlow Lite Micro\n\n\n4\nAdafruit EdgeBadge\nATSAMD51 32-bit Cortex M4\nCompact size, integrated display and microphone\nTensorFlow Lite Micro\n\n\n5\nGoogle Coral Development Board\nNXP i.MX 8M SOC (quad Cortex-A53, Cortex-M4F)\nEdge TPU, Wi-Fi, Bluetooth\nTensorFlow Lite for Coral\n\n\n6\nSTM32 Discovery Kits\nVarious (e.g., STM32F7, STM32H7)\nDifferent configurations, Cube.AI software support\nSTM32Cube.AI\n\n\n7\nArduino Nicla Vision\nSTM32H747AII6 Dual Arm® Cortex® M7/M4\nIntegrated camera, low power, compact design\nTensorFlow Lite Micro\n\n\n8\nArduino Nicla Sense ME\n64 MHz Arm® Cortex M4 (nRF52832)\nMulti-sensor platform, environment sensing, BLE, Wi-Fi\nTensorFlow Lite Micro"
  },
  {
    "objectID": "tools.html#software-tools",
    "href": "tools.html#software-tools",
    "title": "Appendix A: Tools",
    "section": "A.2 Software Tools",
    "text": "A.2 Software Tools\n\nA.2.1 Machine Learning Frameworks\n\n\n\n\n\n\n\n\n\nNo\nMachine Learning Framework\nDescription\nUse Cases\n\n\n\n\n1\nTensorFlow Lite\nLightweight library for running machine learning models on constrained devices\nImage recognition, voice commands, anomaly detection\n\n\n2\nEdge Impulse\nA platform providing tools for creating machine learning models optimized for edge devices\nData collection, model training, deployment on tiny devices\n\n\n3\nONNX Runtime\nA performance-optimized engine for running ONNX models, fine-tuned for edge devices\nCross-platform deployment of machine learning models\n\n\n\n\n\nA.2.2 Libraries and APIs\n\n\n\n\n\n\n\n\n\nNo\nLibrary/API\nDescription\nUse Cases\n\n\n\n\n1\nCMSIS-NN\nA collection of efficient neural network kernels optimized for Cortex-M processors\nEmbedded vision and AI applications\n\n\n2\nARM NN\nAn inference engine for CPUs, GPUs, and NPUs, enabling the translation of neural network frameworks\nAccelerating machine learning model inference on ARM-based devices"
  },
  {
    "objectID": "tools.html#ides-and-development-environments",
    "href": "tools.html#ides-and-development-environments",
    "title": "Appendix A: Tools",
    "section": "A.3 IDEs and Development Environments",
    "text": "A.3 IDEs and Development Environments\n\n\n\n\n\n\n\n\n\nNo\nIDE/Development Environment\nDescription\nFeatures\n\n\n\n\n1\nPlatformIO\nAn open-source ecosystem for IoT development catering to various boards & platforms\nCross-platform build system, continuous testing, firmware updates\n\n\n2\nEclipse Embedded CDT\nA plugin for Eclipse facilitating embedded systems development\nSupports various compilers and debuggers, integrates with popular build tools\n\n\n3\nArduino IDE\nOfficial development environment for Arduino supporting various boards & languages\nUser-friendly interface, large community support, extensive library collection\n\n\n4\nMbed Studio\nARM’s IDE for developing robust embedded software with Mbed OS\nIntegrated debugger, Mbed OS integration, version control support\n\n\n5\nSegger Embedded Studio\nA powerful IDE for ARM microcontrollers supporting a wide range of development boards\nAdvanced code editor, project management, debugging capabilities"
  },
  {
    "objectID": "zoo_datasets.html",
    "href": "zoo_datasets.html",
    "title": "Appendix B: Datasets",
    "section": "",
    "text": "Google Speech Commands Dataset\n\nDescription: A set of one-second .wav audio files, each containing a single spoken English word.\nLink to the Dataset\n\nVisualWakeWords Dataset\n\nDescription: A dataset tailored for tinyML vision applications, consisting of binary labeled images indicating whether a person is in the image or not.\nLink to the Dataset\n\nEMNIST Dataset\n\nDescription: A dataset containing 28x28 pixel images of handwritten characters and digits, which is an extension of the MNIST dataset but includes letters.\nLink to the Dataset\n\nUCI Machine Learning Repository: Human Activity Recognition Using Smartphones\n\nDescription: A dataset with the recordings of 30 study participants performing activities of daily living (ADL) while carrying a waist-mounted smartphone with embedded inertial sensors.\nLink to the Dataset\n\nPlantVillage Dataset\n\nDescription: A dataset comprising of images of healthy and diseased crop leaves categorized based on the crop type and disease type, which could be used in a tinyML agricultural project.\nLink to the Dataset\n\nGesture Recognition using 3D Motion Sensing (3D Gesture Database)\n\nDescription: This dataset contains 3D gesture data recorded using a Leap Motion Controller, which might be useful for gesture recognition projects.\nLink to the Dataset\n\nMultilingual Spoken Words Corpus\n\nDescription: A dataset containing recordings of common spoken words in various languages, useful for speech recognition projects targeting multiple languages.\nLink to the Dataset\n\n\nRemember to verify the dataset’s license or terms of use to ensure it can be used for your intended purpose."
  },
  {
    "objectID": "learning_resources.html#books",
    "href": "learning_resources.html#books",
    "title": "Appendix D: Resources",
    "section": "D.1 Books",
    "text": "D.1 Books\nHere is a list of recommended books for learning about TinyML or embedded AI:\n\nTinyML: Machine Learning with TensorFlow Lite on Arduino and Ultra-Low-Power Microcontrollers by Pete Warden and Daniel Situnayake\nAI at the Edge: Solving Real-World Problems with Embedded Machine Learning by Daniel Situnayake\nTinyML Cookbook: Combine artificial intelligence and ultra-low-power embedded devices to make the world smarter by Gian Marco Iodice\nDeep Learning on Microcontrollers: Learn how to develop embedded AI applications using TinyML by Ashish Vaswani\nIntroduction to TinyML by Rohit Sharma\n\nThese books cover a range of topics related to TinyML and embedded AI, including:\n\nThe fundamentals of machine learning and TinyML\nHow to choose the right hardware and software for your project\nHow to train and deploy TinyML models on embedded devices\nReal-world examples of TinyML applications\n\nIn addition to the above books, there are a number of other resources available for learning about TinyML and embedded AI, including online courses, tutorials, and blog posts. Some of these are listed below. Another great way to learn is join the community of embedded AI developers."
  },
  {
    "objectID": "learning_resources.html#tutorials",
    "href": "learning_resources.html#tutorials",
    "title": "Appendix D: Resources",
    "section": "D.2 Tutorials",
    "text": "D.2 Tutorials"
  },
  {
    "objectID": "learning_resources.html#frameworks",
    "href": "learning_resources.html#frameworks",
    "title": "Appendix D: Resources",
    "section": "D.3 Frameworks",
    "text": "D.3 Frameworks\n\nGitHub Description: There are various GitHub repositories dedicated to TinyML where you can contribute or learn from existing projects. Some popular organizations/repos to check out are:\n\nTensorFlow Lite Micro: GitHub Repository\nTinyML4D: GitHub Repository\n\nStack Overflow Tags: tinyml Description: Use the “tinyml” tag on Stack Overflow to ask technical questions and find answers from the community."
  },
  {
    "objectID": "learning_resources.html#courses-and-learning-platforms",
    "href": "learning_resources.html#courses-and-learning-platforms",
    "title": "Appendix D: Resources",
    "section": "D.4 Courses and Learning Platforms",
    "text": "D.4 Courses and Learning Platforms\n\nCoursera Course: Introduction to Embedded Machine Learning Description: A dedicated course on Coursera to learn the basics and advances of TinyML.\nEdX Course: Intro to TinyML Description: Learn about TinyML with this HarvardX course."
  },
  {
    "objectID": "community.html#online-forums",
    "href": "community.html#online-forums",
    "title": "Appendix E: Communities",
    "section": "E.1 Online Forums",
    "text": "E.1 Online Forums\n\nTinyML Forum Website: TinyML Forum Description: A dedicated forum for discussions, news, and updates on TinyML.\nReddit Subreddits: r/TinyML Description: Reddit community discussing various topics related to TinyML."
  },
  {
    "objectID": "community.html#blogs-and-websites",
    "href": "community.html#blogs-and-websites",
    "title": "Appendix E: Communities",
    "section": "E.2 Blogs and Websites",
    "text": "E.2 Blogs and Websites\n\nTinyML Foundation Website: TinyML Foundation Description: The official website offers a wealth of information including research, news, and events.\nEdge Impulse Blog Website: Blog Description: Contains several articles, tutorials, and resources on TinyML."
  },
  {
    "objectID": "community.html#social-media-groups",
    "href": "community.html#social-media-groups",
    "title": "Appendix E: Communities",
    "section": "E.3 Social Media Groups",
    "text": "E.3 Social Media Groups\n\nLinkedIn Groups Description: Join TinyML groups on LinkedIn to connect with professionals and enthusiasts in the field.\nTwitter Description: Follow TinyML enthusiasts, organizations, and experts on Twitter for the latest news and updates. Example handles to follow:\n\nTwitter\nEdgeImpulse"
  },
  {
    "objectID": "community.html#conferences-and-meetups",
    "href": "community.html#conferences-and-meetups",
    "title": "Appendix E: Communities",
    "section": "E.4 Conferences and Meetups",
    "text": "E.4 Conferences and Meetups\n\nTinyML Summit Website: TinyML Summit Description: Annual event where professionals and enthusiasts gather to discuss the latest developments in TinyML.\nMeetup Website: Meetup Description: Search for TinyML groups on Meetup to find local or virtual gatherings.\n\nRemember to always check the credibility and activity level of the platforms and groups before diving in to ensure a productive experience."
  },
  {
    "objectID": "case_studies.html",
    "href": "case_studies.html",
    "title": "Appendix F: Case Studies",
    "section": "",
    "text": "coming soon."
  }
]