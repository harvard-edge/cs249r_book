[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Machine Learning Systems",
    "section": "",
    "text": "Preface\nWelcome to Machine Learning Systems, your gateway to the fast-paced world of machine learning (ML) systems. This book is an extension of the CS249r course at Harvard University, taught by Prof. Vijay Janapa Reddi, and is the result of a collaborative effort involving students, professionals, and the broader community of AI practitioners.\nWe’ve created this open-source book to demystify the process of building efficient and scalable ML systems. Our goal is to provide a comprehensive guide that covers the principles, practices, and challenges of developing robust ML pipelines for deployment. This isn’t a static textbook—it’s a living, evolving resource designed to keep pace with advancements in the field.\nAs a living and breathing resource, this book is a continual work in progress, reflecting the ever-evolving nature of machine learning systems. Advancements in the ML landscape drive our commitment to keeping this resource updated with the latest insights, techniques, and best practices. We warmly invite you to join us on this journey by contributing your expertise, feedback, and ideas.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#global-outreach",
    "href": "index.html#global-outreach",
    "title": "Machine Learning Systems",
    "section": "Global Outreach",
    "text": "Global Outreach\nThank you to all our readers and visitors. Your engagement with the material keeps us motivated.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#why-we-wrote-this-book",
    "href": "index.html#why-we-wrote-this-book",
    "title": "Machine Learning Systems",
    "section": "Why We Wrote This Book",
    "text": "Why We Wrote This Book\nWhile there are plenty of resources that focus on the algorithmic side of machine learning, resources on the systems side of things are few and far between. This gap inspired us to create this book—a resource dedicated to the principles and practices of building efficient and scalable ML systems.\nOur vision for this book and its broader mission is deeply rooted in the transformative potential of AI and the need to make AI education globally accessible to all. To learn more about the inspiration behind this project and the values driving its creation, we encourage you to read the Author’s Note.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#want-to-help-out",
    "href": "index.html#want-to-help-out",
    "title": "Machine Learning Systems",
    "section": "Want to Help Out?",
    "text": "Want to Help Out?\nThis is a collaborative project, and your input matters! If you’d like to contribute, check out our contribution guidelines. Feedback, corrections, and new ideas are welcome—simply file a GitHub issue.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#whats-next",
    "href": "index.html#whats-next",
    "title": "Machine Learning Systems",
    "section": "What’s Next?",
    "text": "What’s Next?\nIf you’re ready to dive deeper into the book’s structure, learning objectives, and practical use, visit the About the Book section for more details.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "contents/frontmatter/foreword.html",
    "href": "contents/frontmatter/foreword.html",
    "title": "Author’s Note",
    "section": "",
    "text": "AI is bound to transform the world in profound ways, much like computers and the Internet revolutionized every aspect of society in the 20th century. From systems that generate creative content to those driving breakthroughs in drug discovery, AI is ushering in a new era—one that promises to be even more transformative in its scope and impact. But how do we make it accessible to everyone?\nWith its transformative power comes an equally great responsibility for those who access it or work with it. Just as we expect companies to wield their influence ethically, those of us in academia bear a parallel responsibility: to share our knowledge openly, so it benefits everyone—not just a select few. This conviction inspired the creation of this book—an open-source resource aimed at making AI education, particularly in AI engineering and systems, inclusive and accessible to everyone from all walks of life.\nMy passion for creating, curating, and editing this content has been deeply influenced by landmark textbooks that have profoundly shaped both my academic and personal journey. Whether I studied them cover to cover or drew insights from key passages, these resources fundamentally shaped the way I think. I reflect on the books that guided my path: works by Turing Award winners such as David Patterson and John Hennessy—pioneers in computer architecture and system design—and foundational research papers by luminaries like Yann LeCun, Geoffrey Hinton, and Yoshua Bengio. In some small part, my hope is that this book will inspire students to chart their own unique paths.\nI am optimistic about what lies ahead for AI. It has the potential to solve global challenges and unlock creativity in ways we have yet to imagine. To achieve this, however, we must train the next generation of AI engineers and practitioners—those who can transform novel AI algorithms into working systems that enable real-world application. This book is a step toward curating the material needed to build the next generation of AI engineers who will transform today’s visions into tomorrow’s reality.\nThis book is a work in progress, but knowing that even one learner benefits from its content motivates me to continually refine and expand it. To that end, if there’s one thing I ask of readers, it’s this: please show your support by starring the GitHub repository here. Your star ⭐ reflects your belief in this mission—not just to me, but to the growing global community of learners, educators, and practitioners. This small act is more than symbolic—it amplifies the importance of making AI education accessible.\nI am a student of my own writing, and every chapter of this book has taught me something new—thanks to the numerous people who have played, and continue to play, an important role in shaping this work. Professors, students, practitioners, and researchers contributed by offering suggestions, sharing expertise, identifying errors, and proposing improvements. Every interaction, whether a detailed critique or a simple correction from a GitHub contributor, has been a lesson in itself. These contributions have not only refined the material but also deepened my understanding of how knowledge grows through collaboration. This book is, therefore, not solely my work; it is a shared endeavor, reflecting the collective spirit of those dedicated to sharing their knowledge and effort.\nThis book is dedicated to the loving memory of my father. His passion for education, endless curiosity, generosity in sharing knowledge, and unwavering commitment to quality challenge me daily to strive for excellence in all I do. In his honor, I extend this dedication to teachers and mentors everywhere, whose efforts and guidance transform lives every day. Your selfless contributions remind me to persevere.\nLast but certainly not least, this work would not be possible without the unwavering support of my wonderful wife and children. Their love, patience, and encouragement form the foundation that enables me to pursue my passion and bring this work to life. For this, and so much more, I am deeply grateful.\n— Prof. Vijay Janapa Reddi",
    "crumbs": [
      "Author's Note"
    ]
  },
  {
    "objectID": "contents/frontmatter/about/about.html",
    "href": "contents/frontmatter/about/about.html",
    "title": "About the Book",
    "section": "",
    "text": "Overview",
    "crumbs": [
      "About the Book"
    ]
  },
  {
    "objectID": "contents/frontmatter/about/about.html#overview",
    "href": "contents/frontmatter/about/about.html#overview",
    "title": "About the Book",
    "section": "",
    "text": "Purpose of the Book\nWelcome to this collaborative textbook. It originated as part of the CS249r: Tiny Machine Learning course that Prof. Vijay Janapa Reddi teaches at Harvard University.\nThe goal of this book is to provide a resource for educators and learners seeking to understand the principles and practices of machine learning systems. This book is continually updated to incorporate the latest insights and effective teaching strategies with the intent that it remains a valuable resource in this fast-evolving field. So please check back often!\n\n\nContext and Development\nThe book reflects a blend of pedagogical expertise and cutting-edge research. Developed collaboratively with contributions from students, researchers, and practitioners, it bridges academic rigor and real-world application.\nWe’ve designed the book to evolve alongside advancements in the field, fostering a collaborative environment where knowledge can grow and adapt.\n\n\nWhat to Expect\nThis textbook explores the foundational principles, practical workflows, and critical challenges of building and deploying machine learning systems. Starting with foundational concepts, it progresses through engineering principles, examines operational considerations for deploying AI systems, and concludes by reflecting on the societal and technological implications of machine learning.",
    "crumbs": [
      "About the Book"
    ]
  },
  {
    "objectID": "contents/frontmatter/about/about.html#learning-goals",
    "href": "contents/frontmatter/about/about.html#learning-goals",
    "title": "About the Book",
    "section": "Learning Goals",
    "text": "Learning Goals\n\nKey Learning Outcomes\nThis book is structured with Bloom’s Taxonomy in mind, which defines six levels of learning, ranging from foundational knowledge to advanced creative thinking:\n\n\n\n\n\n\nFigure 1: Bloom’s Taxonomy (2021 edition).\n\n\n\n\nRemembering: Recalling basic facts and concepts.\nUnderstanding: Explaining ideas or processes.\nApplying: Using knowledge in new situations.\nAnalyzing: Breaking down information into components.\nEvaluating: Making judgments based on criteria and standards.\nCreating: Producing original work or solutions.\n\n\n\nLearning Objectives\nThis book supports readers in:\n\nUnderstanding Fundamentals: Explain the foundational principles of machine learning, including theoretical underpinnings and practical applications.\nAnalyzing System Components: Evaluate the critical components of AI systems and their roles within various architectures.\nDesigning Workflows: Outline workflows for developing machine learning systems, from data collection to deployment.\nOptimizing Models: Apply methods to enhance performance, such as hyperparameter tuning and regularization.\nEvaluating Ethical Implications: Analyze societal impacts and address potential biases in AI systems.\nExploring Applications: Investigate real-world use cases across diverse domains.\nConsidering Deployment Challenges: Address security, scalability, and maintainability in real-world systems.\nEnvisioning Future Trends: Reflect on emerging challenges and technologies in machine learning.\n\n\n\nAI Learning Companion\nThroughout this resource, you’ll find SocratiQ—an AI learning assistant designed to enhance your learning experience. Inspired by the Socratic method of teaching, SocratiQ combines interactive quizzes, personalized assistance, and real-time feedback to help you reinforce your understanding and create new connections. As part of our experiment with Generative AI technologies, SocratiQ encourages critical thinking and active engagement with the material.\nSocratiQ is still a work in progress, and we welcome your feedback to make it better. For more details about how SocratiQ works and how to get the most out of it, visit the AI Learning Companion page.",
    "crumbs": [
      "About the Book"
    ]
  },
  {
    "objectID": "contents/frontmatter/about/about.html#how-to-navigate-this-book",
    "href": "contents/frontmatter/about/about.html#how-to-navigate-this-book",
    "title": "About the Book",
    "section": "How to Navigate This Book",
    "text": "How to Navigate This Book\n\nBook Structure\nThe book is organized into four main parts, each building on the previous one:\n\nThe Essentials (Chapters 1-4)\nCore principles, components, and architectures that underpin machine learning systems.\nEngineering Principles (Chapters 5-13)\nCovers workflows, data engineering, optimization strategies, and operational challenges in system design.\nAI Best Practice (Chapters 14-18)\nFocuses on key considerations for deploying AI systems in real-world environments, including security, privacy, robustness, and sustainability.\nClosing Perspectives (Chapter 19-20)\nSynthesizes key lessons and explores emerging trends shaping the future of ML systems.\n\n\n\nSuggested Reading Paths\n\nBeginners: Start with The Essentials to build a strong conceptual base before progressing to other parts.\nPractitioners: Focus on Engineering Principles and AI in Practice for hands-on, real-world insights.\nResearchers: Dive into AI in Practice and Closing Perspectives to explore advanced topics and societal implications.\n\n\n\nModular Design\nThe book is modular, allowing readers to explore chapters independently or sequentially. Each chapter includes supplementary resources:\n\nSlides summarizing key concepts.\nVideos providing in-depth explanations.\nExercises reinforcing understanding.\nLabs offering practical, hands-on experience.\n\nWhile several of these resources are still a work in progress, we believe it’s better to share valuable insights and tools as they become available rather than wait for everything to be perfect. After all, progress is far more important than perfection, and your feedback will help us improve and refine this resource over time.\nAdditionally, we try to reuse and build upon the incredible work created by amazing experts in the field, rather than reinventing everything from scratch. This philosophy reflects the fundamental essence of community-driven learning: collaboration, sharing knowledge, and collectively advancing our understanding.",
    "crumbs": [
      "About the Book"
    ]
  },
  {
    "objectID": "contents/frontmatter/about/about.html#transparency-and-collaboration",
    "href": "contents/frontmatter/about/about.html#transparency-and-collaboration",
    "title": "About the Book",
    "section": "Transparency and Collaboration",
    "text": "Transparency and Collaboration\nThis book is a community-driven project, with content generated collaboratively by numerous contributors over time. The content creation process may have involved various editing tools, including generative AI technology. As the main author, editor, and curator, Prof. Vijay Janapa Reddi maintains human oversight to ensure the content is accurate and relevant.\nHowever, no one is perfect, and inaccuracies may still exist. Your feedback is highly valued, and we encourage you to provide corrections or suggestions. This collaborative approach is crucial for maintaining high-quality information and making it globally accessible.",
    "crumbs": [
      "About the Book"
    ]
  },
  {
    "objectID": "contents/frontmatter/about/about.html#copyright-and-licensing",
    "href": "contents/frontmatter/about/about.html#copyright-and-licensing",
    "title": "About the Book",
    "section": "Copyright and Licensing",
    "text": "Copyright and Licensing\nThis book is open-source and developed collaboratively through GitHub. Unless otherwise stated, this work is licensed under the Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International (CC BY-NC-SA 4.0).\nContributors retain copyright over their individual contributions, dedicated to the public domain or released under the same open license as the original project. For more information on authorship and contributions, visit the GitHub repository.",
    "crumbs": [
      "About the Book"
    ]
  },
  {
    "objectID": "contents/frontmatter/about/about.html#join-the-community",
    "href": "contents/frontmatter/about/about.html#join-the-community",
    "title": "About the Book",
    "section": "Join the Community",
    "text": "Join the Community\nThis textbook is more than just a resource—it’s an invitation to collaborate and learn together. Engage in community discussions to share insights, tackle challenges, and learn alongside fellow students, researchers, and practitioners.\nWhether you’re a student starting your journey, a practitioner solving real-world challenges, or a researcher exploring advanced concepts, your contributions will enrich this learning community. Introduce yourself, share your goals, and let’s collectively build a deeper understanding of machine learning systems.",
    "crumbs": [
      "About the Book"
    ]
  },
  {
    "objectID": "contents/frontmatter/acknowledgements/acknowledgements.html",
    "href": "contents/frontmatter/acknowledgements/acknowledgements.html",
    "title": "Acknowledgements",
    "section": "",
    "text": "Funding Agencies and Companies\nThis book, inspired by the TinyML edX course and CS294r at Harvard University, is the result of years of hard work and collaboration with many students, researchers and practioners. We are deeply indebted to the folks whose groundbreaking work laid its foundation.\nAs our understanding of machine learning systems deepened, we realized that fundamental principles apply across scales, from tiny embedded systems to large-scale deployments. This realization shaped the book’s expansion into an exploration of machine learning systems with the aim of providing a foundation applicable across the spectrum of implementations.",
    "crumbs": [
      "Acknowledgements"
    ]
  },
  {
    "objectID": "contents/frontmatter/acknowledgements/acknowledgements.html#funding-agencies-and-companies",
    "href": "contents/frontmatter/acknowledgements/acknowledgements.html#funding-agencies-and-companies",
    "title": "Acknowledgements",
    "section": "",
    "text": "Academic Support\nWe are grateful for the academic support that has made it possible to hire teaching assistants to help improve instructional material and quality:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNon-Profit and Institutional Support\nWe gratefully acknowledge the support of the following non-profit organizations and institutions that have contributed to educational outreach efforts, provided scholarship funds to students in developing countries, and organized workshops to teach using the material:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCorporate Support\nThe following companies contributed hardware kits used for the labs in this book and/or supported the development of hands-on educational materials:",
    "crumbs": [
      "Acknowledgements"
    ]
  },
  {
    "objectID": "contents/frontmatter/acknowledgements/acknowledgements.html#contributors",
    "href": "contents/frontmatter/acknowledgements/acknowledgements.html#contributors",
    "title": "Acknowledgements",
    "section": "Contributors",
    "text": "Contributors\nWe express our sincere gratitude to the open-source community of learners, educators, and contributors. Each contribution, whether a chapter section or a single-word correction, has significantly enhanced the quality of this resource. We also acknowledge those who have shared insights, identified issues, and provided valuable feedback behind the scenes.\nA comprehensive list of all GitHub contributors, automatically updated with each new contribution, is available below. For those interested in contributing further, please consult our GitHub page for more information.\n\n\n\n\n\n\n\n\nVijay Janapa Reddi\n\n\njasonjabbour\n\n\nIkechukwu Uchendu\n\n\nNaeem Khoshnevis\n\n\nMarcelo Rovai\n\n\n\n\nKai Kleinbard\n\n\nSara Khosravi\n\n\nDouwe den Blanken\n\n\nshanzehbatool\n\n\nZeljko Hrcek\n\n\n\n\nElias\n\n\nJared Ping\n\n\nMatthew Stewart\n\n\nItai Shapira\n\n\nMaximilian Lam\n\n\n\n\nJayson Lin\n\n\nJeffrey Ma\n\n\nAndrea\n\n\nSophia Cho\n\n\nAlex Rodriguez\n\n\n\n\nKorneel Van den Berghe\n\n\nZishen Wan\n\n\nColby Banbury\n\n\nMark Mazumder\n\n\nAbdulrahman Mahmoud\n\n\n\n\nSrivatsan Krishnan\n\n\nDivya Amirtharaj\n\n\nHaoran Qiu\n\n\nmarin-llobet\n\n\nAghyad Deeb\n\n\n\n\nAditi Raju\n\n\nELSuitorHarvard\n\n\nJared Ni\n\n\noishib\n\n\nMichael Schnebly\n\n\n\n\nEmil Njor\n\n\nYu-Shun Hsiao\n\n\nHenry Bae\n\n\nJae-Won Chung\n\n\nShvetank Prakash\n\n\n\n\nEmeka Ezike\n\n\nPong Trairatvorakul\n\n\nMarco Zennaro\n\n\nAndrew Bass\n\n\nEura Nofshin\n\n\n\n\nJennifer Zhou\n\n\nAllen-Kuang\n\n\nGauri Jain\n\n\ngnodipac886\n\n\nFin Amin\n\n\n\n\nBruno Scaglione\n\n\nAlex Oesterling\n\n\nFatima Shah\n\n\nArya Tschand\n\n\nSercan Aygün\n\n\n\n\nThe Random DIY\n\n\nAbenezer\n\n\nJessica Quaye\n\n\nhappyappledog\n\n\nEmmanuel Rassou\n\n\n\n\nabigailswallow\n\n\nYang Zhou\n\n\nAritra Ghosh\n\n\nBilge Acun\n\n\nJason Yik\n\n\n\n\nSonia Murthy\n\n\nShreya Johri\n\n\nCostin-Andrei Oncescu\n\n\nBaldassarre Cesarano\n\n\nAnnie Laurie Cook\n\n\n\n\nVijay Edupuganti\n\n\nJothi Ramaswamy\n\n\nBatur Arslan\n\n\nCurren Iyer\n\n\nFatima Shah\n\n\n\n\nEdward Jin\n\n\nyanjingl\n\n\na-saraf\n\n\nsonghan\n\n\nZishen",
    "crumbs": [
      "Acknowledgements"
    ]
  },
  {
    "objectID": "contents/frontmatter/ai/socratiq.html",
    "href": "contents/frontmatter/ai/socratiq.html",
    "title": "SocratiQ AI",
    "section": "",
    "text": "AI Learning Companion\nWelcome to SocratiQ (pronounced ``Socratic’’), an AI learning assistant seamlessly integrated throughout this resource. Inspired by the Socratic method of teaching—emphasizing thoughtful questions and answers to stimulate critical thinking—SocratiQ is part of our experiment with what we call as Generative Learning. By combining interactive quizzes, personalized assistance, and real-time feedback, SocratiQ is meant to reinforce your understanding and help you create new connections. SocratiQ is still a work in progress, and we welcome your feedback.\nYou can enable SocratiQ by clicking the button below:\nSocratiQ’s goal is to adapt to your needs while generating targeted questions and engaging in meaningful dialogue about the material. Unlike traditional textbook study, SocratiQ offers an interactive, personalized learning experience that can help you better understand and retain complex concepts. It is only available as an online feature.",
    "crumbs": [
      "SocratiQ AI"
    ]
  },
  {
    "objectID": "contents/frontmatter/ai/socratiq.html#ai-learning-companion",
    "href": "contents/frontmatter/ai/socratiq.html#ai-learning-companion",
    "title": "SocratiQ AI",
    "section": "",
    "text": "Listen to this AI-generated podcast about SocratiQ. \n\n    \n    \n    \n    \n\n\n\nSocratiQ: OFF\n\n\n\n\n\n\n\n\n\nDirect URL Access\n\n\n\n\n\nYou can directly control SocratiQ by adding ?socratiq= parameters to your URL:\n\nTo activate: mlsysbook.ai/?socratiq=true\nTo deactivate: mlsysbook.ai/?socratiq=false\n\nThis gives you with quick access to toggle SocratiQ’s functionality directly from your browser’s address bar if you are on a page and do not want to return here to toggle functionality.",
    "crumbs": [
      "SocratiQ AI"
    ]
  },
  {
    "objectID": "contents/frontmatter/ai/socratiq.html#quick-start-guide",
    "href": "contents/frontmatter/ai/socratiq.html#quick-start-guide",
    "title": "SocratiQ AI",
    "section": "Quick Start Guide",
    "text": "Quick Start Guide\n\nEnable SocratiQ using the button below or URL parameters\nUse keyboard shortcut (Cmd/Ctrl + /) to open SocratiQ anytime\nSet your academic level in Settings\nStart learning! Look for quiz buttons at the end of sections\n\nPlease note that this is an experimental feature. We are experimenting with the idea of creating a dynamic and personalized learning experience by harnessing the power of generative AI. We hope that this approach will transform how you interact with and absorb the complex concepts.\n\n\n\n\n\n\nWarning\n\n\n\nAbout AI Responses: While SocratiQ uses advanced AI to generate quizzes and provide assistance, like all AI systems, it may occasionally provide imperfect or incomplete answers. However, we’ve designed and tested it to ensure it’s effective for supporting your learning journey. If you’re unsure about any response, refer to the textbook content or consult your instructor.\n\n\nOnce you’ve enabled SocratiQ it will always be available when you visit this site.\nYou can access SocratiQ at any time using a keyboard shortcut shown in Figure 1, which brings up the interface shown in Figure 2.\n\n\n\n\n\n\nFigure 1: Keyboard shortcut for SocratiQ.\n\n\n\n\n\n\n\n\n\nFigure 2: The main SocratiQ interface, showing the key components of your AI learning assistant.",
    "crumbs": [
      "SocratiQ AI"
    ]
  },
  {
    "objectID": "contents/frontmatter/ai/socratiq.html#button-overview",
    "href": "contents/frontmatter/ai/socratiq.html#button-overview",
    "title": "SocratiQ AI",
    "section": "Button Overview",
    "text": "Button Overview\nThe top nav bar provices quick access to the following features:\n\nAdjust your settings at any time.\nTrack your progress by viewing the dashboard.\nStart new or save your conversations with SocratiQ.\n\n\n\n\n\n\n\nFigure 3: View of the top nav menu.",
    "crumbs": [
      "SocratiQ AI"
    ]
  },
  {
    "objectID": "contents/frontmatter/ai/socratiq.html#sec-socratiq-settings",
    "href": "contents/frontmatter/ai/socratiq.html#sec-socratiq-settings",
    "title": "SocratiQ AI",
    "section": "Personalize Your Learning",
    "text": "Personalize Your Learning\nBefore diving into your studies, take a moment to configure SocratiQ for your academic level. This initial setup ensures that all interactions, from quiz questions to explanations, are tailored to your background knowledge. Figure 4 shows where you can adjust these preferences.\nYou can augment any AI SocratiQ response using the dropdown menu at the top of each message.\n\n\n\n\n\n\nFigure 4: The settings panel where you can customize SocratiQ to match your academic level.",
    "crumbs": [
      "SocratiQ AI"
    ]
  },
  {
    "objectID": "contents/frontmatter/ai/socratiq.html#sec-socratiq-learning",
    "href": "contents/frontmatter/ai/socratiq.html#sec-socratiq-learning",
    "title": "SocratiQ AI",
    "section": "Learning with SocratiQ",
    "text": "Learning with SocratiQ\n\nQuizzes\nAs you progress through each section of the textbook, you have the option to ask SocratiQ to automatically generate quizzes tailored to reinforce key concepts. These quizzes are conveniently inserted at the end of every major subsection (e.g., 1.1, 1.2, 1.3, and so on), as illustrated in Figure 6.\n\n\n\n\n\n\nFigure 5: Redo an AI message by choosing a new experience level.\n\n\n\n\n\n\n\n\n\nFigure 6: Quizzes are generated at the end of every section.\n\n\n\nEach quiz typically consists of 3-5 multiple-choice questions and takes only 1-2 minutes to complete. These questions are designed to assess your understanding of the material covered in the preceding section, as shown in Figure 7 (a).\nUpon submitting your answers, SocratiQ provides immediate feedback along with detailed explanations for each question, as demonstrated in Figure 7 (b).\n\n\n\n\n\n\n\n\n\n\n\n(a) Example of AI-generated quiz questions.\n\n\n\n\n\n \n\n\n\n\n\n\n\n\n(b) Example of AI-generated feedback and explanations for quizzes.\n\n\n\n\n\n\n\nFigure 7: SocratiQ uses a Large Language Model (LLM) to automatically generate and grade quizzes.\n\n\n\n\n\nExample Learning Flow\n\nRead a section\nSelect challenging text → Ask SocratiQ for explanation\nTake the section quiz\nReview related content suggestions\nTrack progress in dashboard\n\n\n\nGetting Help with Concepts\nWhen you encounter challenging concepts, SocratiQ offers two powerful ways to get help. First, you can select any text from the textbook and ask for a detailed explanation, as demonstrated in Figure 8.\n\n\n\n\n\n\nFigure 8: Selecting specific text to ask for clarification.\n\n\n\nOnce you’ve selected the text, you can ask questions about it, and SocratiQ will provide detailed explanations based on that context, as illustrated in Figure 9.\n\n\n\n\n\n\nFigure 9: Example of how SocratiQ provides explanations based on selected text.\n\n\n\nFigure 11 shows the response for the ask in Figure 9.\nAdditionally, you can also reference Sections, as shown in Figure 10, Sub-sections and keywords directly as you converse with SocratiQ. Use the @ symbol to reference a section, sub-section or keyword. You can also click the + Context button right above the input.\n\n\n\n\n\n\nFigure 10: Referencing different sections from the textbook.\n\n\n\n\n\n\n\n\n\nFigure 11: An interactive chat session with SocratiQ, demonstrating how to get clarification on concepts.\n\n\n\nTo enhance your learning experience, SocratiQ doesn’t just answer your questions—it also suggests related content from the textbook that might be helpful for deeper understanding, as shown in Figure 12.\n\n\n\n\n\n\nFigure 12: SocratiQ suggests related content based on your questions to help deepen your understanding.",
    "crumbs": [
      "SocratiQ AI"
    ]
  },
  {
    "objectID": "contents/frontmatter/ai/socratiq.html#sec-socratiq-dashboard",
    "href": "contents/frontmatter/ai/socratiq.html#sec-socratiq-dashboard",
    "title": "SocratiQ AI",
    "section": "Tracking Your Progress",
    "text": "Tracking Your Progress\n\nPerformance Dashboard\nSocratiQ maintains a comprehensive record of your learning journey. The progress dashboard (Figure 13) displays your quiz performance statistics, learning streaks, and achievement badges. This dashboard updates real-time.\n\n\n\n\n\n\nFigure 13: The progress dashboard showing your learning statistics and achievements.\n\n\n\nAs you continue to engage with the material and complete quizzes, you’ll earn various badges that recognize your progress, as shown in Figure 14.\n\n\n Achievement Badges\nAs you progress through the quizzes, you’ll earn special badges to mark your achievements! Here’s what you can earn:\n\n\n\n\n\n\n\n\nBadge\nName\nHow to Earn\n\n\n\n\n\nFirst Steps\nComplete your first quiz\n\n\n\nOn a Streak\nMaintain a streak of perfect scores\n\n\n\nQuiz Medalist\nComplete 10 quizzes\n\n\n\nQuiz Champion\nComplete 20 quizzes\n\n\n\nQuiz Legend\nComplete 30 quizzes\n\n\n x n\nQuiz AGI Super Human\nComplete 40 or more quizzes\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nKeep taking quizzes to collect all badges and improve your learning journey! Your current badges will appear in the quiz statistics dashboard.\n\n\n\n\n\n\n\n\nFigure 14: Examples of achievement badges you can earn through consistent engagement.\n\n\n\nIf you’d like a record of your progress you can generate a PDF report. It will show your progress, average performance and all the questions you’ve attempted. The PDF is a generated with a unique hash and can be uniquely validated.\n\n\n\n\n\n\nFigure 15: You can click the Download Report button to view your report. You can verify that your PDF has been created by SocratiQ by clicking the verify button and uploading your generated PDF.",
    "crumbs": [
      "SocratiQ AI"
    ]
  },
  {
    "objectID": "contents/frontmatter/ai/socratiq.html#data-storage",
    "href": "contents/frontmatter/ai/socratiq.html#data-storage",
    "title": "SocratiQ AI",
    "section": "Data Storage",
    "text": "Data Storage\n\n\n\n\n\n\nImportant\n\n\n\nImportant Note: All progress data is stored locally in your browser. Clearing your browser history or cache will erase your entire learning history, including quiz scores, streaks, and achievement badges.\n\n\nYou can also delete all of your saved conversations by clicking the New Chat button in the nav bar.\n\n\n\n\n\n\nFigure 16: Load or delete previous chats or start a new chat.",
    "crumbs": [
      "SocratiQ AI"
    ]
  },
  {
    "objectID": "contents/frontmatter/ai/socratiq.html#technical-requirements",
    "href": "contents/frontmatter/ai/socratiq.html#technical-requirements",
    "title": "SocratiQ AI",
    "section": "Technical Requirements",
    "text": "Technical Requirements\nTo use SocratiQ effectively, you’ll need:\n\nChrome or Safari browser\nJavaScript enabled\nStable internet connection",
    "crumbs": [
      "SocratiQ AI"
    ]
  },
  {
    "objectID": "contents/frontmatter/ai/socratiq.html#common-issues-and-troubleshooting",
    "href": "contents/frontmatter/ai/socratiq.html#common-issues-and-troubleshooting",
    "title": "SocratiQ AI",
    "section": "Common Issues and Troubleshooting",
    "text": "Common Issues and Troubleshooting\n\nIf SocratiQ isn’t responding: Refresh the page\nIf quizzes don’t load: Check your internet connection\nIf progress isn’t saving: Ensure cookies are enabled\n\nFor persistent issues, please contact us at vj[@]eecs.harvard.edu.",
    "crumbs": [
      "SocratiQ AI"
    ]
  },
  {
    "objectID": "contents/frontmatter/ai/socratiq.html#providing-feedback",
    "href": "contents/frontmatter/ai/socratiq.html#providing-feedback",
    "title": "SocratiQ AI",
    "section": "Providing Feedback",
    "text": "Providing Feedback\nYour feedback helps us improve SocratiQ.\nYou can report technical issues, suggest improvements to quiz questions, or share thoughts about AI responses using the feedback buttons located throughout the interface. You can submit a GitHub issue.\nIf you prefer leaving feedback via Google Form, you are welcome to do so via this link:\n\nShare Your Feedback",
    "crumbs": [
      "SocratiQ AI"
    ]
  },
  {
    "objectID": "contents/core/introduction/introduction.html",
    "href": "contents/core/introduction/introduction.html",
    "title": "1  Introduction",
    "section": "",
    "text": "1.1 AI is Everywhere\nArtificial Intelligence (AI) has emerged as one of the most transformative forces in human history. From the moment we wake up to when we go to sleep, AI systems invisibly shape our world. They manage traffic flows in our cities, optimize power distribution across electrical grids, and enable billions of wireless devices to communicate seamlessly. In hospitals, AI analyzes medical images and helps doctors diagnose diseases. In research laboratories, it accelerates scientific discovery by simulating molecular interactions and processing vast datasets from particle accelerators. In space exploration, it helps rovers navigate distant planets and telescopes detect new celestial phenomena.\nThroughout history, certain technologies have fundamentally transformed human civilization, defining their eras. The 18th and 19th centuries were shaped by the Industrial Revolution, where steam power and mechanization transformed how humans could harness physical energy. The 20th century was defined by the Digital Revolution, where the computer and internet transformed how we process and share information. Now, the 21st century appears to be the era of Artificial Intelligence, a shift noted by leading thinkers in technological evolution (Brynjolfsson and McAfee 2014; Domingos 2016).\nThe vision driving AI development extends far beyond the practical applications we see today. We aspire to create systems that can work alongside humanity, enhancing our problem-solving capabilities and accelerating scientific progress. Imagine AI systems that could help us understand consciousness, decode the complexities of biological systems, or unravel the mysteries of dark matter. Consider the potential of AI to help address global challenges like climate change, disease, or sustainable energy production. This is not just about automation or efficiency—it’s about expanding the boundaries of human knowledge and capability.\nThe impact of this revolution operates at multiple scales, each with profound implications. At the individual level, AI personalizes our experiences and augments our daily decision-making capabilities. At the organizational level, it transforms how businesses operate and how research institutions make discoveries. At the societal level, it reshapes everything from transportation systems to healthcare delivery. At the global level, it offers new approaches to addressing humanity’s greatest challenges, from climate change to drug discovery.\nWhat makes this transformation unique is its unprecedented pace. While the Industrial Revolution unfolded over centuries and the Digital Revolution over decades, AI capabilities are advancing at an extraordinary rate. Technologies that seemed impossible just years ago—systems that can understand human speech, generate novel ideas, or make complex decisions—are now commonplace. This acceleration suggests we are only beginning to understand how profoundly AI will reshape our world.\nWe stand at a historic inflection point. Just as the Industrial Revolution required us to master mechanical engineering to harness the power of steam and machinery, and the Digital Revolution demanded expertise in electrical and computer engineering to build the internet age, the AI Revolution presents us with a new engineering challenge. We must learn to build systems that can learn, reason, and potentially achieve superhuman capabilities in specific domains.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "contents/core/introduction/introduction.html#ai-is-everywhere",
    "href": "contents/core/introduction/introduction.html#ai-is-everywhere",
    "title": "1  Introduction",
    "section": "",
    "text": "Brynjolfsson, Erik, and Andrew McAfee. 2014. The Second Machine Age: Work, Progress, and Prosperity in a Time of Brilliant Technologies, 1st Edition. W. W. Norton Company.\n\nDomingos, Pedro. 2016. “The Master Algorithm: How the Quest for the Ultimate Learning Machine Will Remake Our World.” Choice Reviews Online 53 (07): 53–3100. https://doi.org/10.5860/choice.194685.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "contents/core/introduction/introduction.html#understanding-ai-and-ml",
    "href": "contents/core/introduction/introduction.html#understanding-ai-and-ml",
    "title": "1  Introduction",
    "section": "1.2 Understanding AI and ML",
    "text": "1.2 Understanding AI and ML\nThe exploration of artificial intelligence’s transformative impact across society presents a fundamental question: How can we create these intelligent capabilities? Understanding the relationship between AI and ML provides the theoretical and practical framework necessary to address this question.\nArtificial Intelligence represents the systematic pursuit of understanding and replicating intelligent behavior—specifically, the capacity to learn, reason, and adapt to new situations. It encompasses fundamental questions about the nature of intelligence, knowledge, and learning. How do we recognize patterns? How do we learn from experience? How do we adapt our behavior based on new information? AI as a field explores these questions, drawing insights from cognitive science, psychology, neuroscience, and computer science.\nMachine Learning, in contrast, constitutes the methodological approach to creating systems that demonstrate intelligent behavior. Instead of implementing intelligence through predetermined rules, machine learning systems utilize gradient descent1 and other optimization techniques to identify patterns and relationships. This methodology reflects fundamental learning processes observed in biological systems. For instance, object recognition in machine learning systems parallels human visual learning processes, requiring exposure to numerous examples to develop robust recognition capabilities. Similarly, natural language processing systems acquire linguistic capabilities through extensive analysis of textual data.\n1 Gradient Descent: An optimization algorithm that iteratively adjusts model parameters to minimize prediction errors by following the gradient (slope) of the error surface, similar to finding the bottom of a valley by always walking downhill.\n\n\n\n\n\nAI and ML: Key Definitions\n\n\n\n\nArtificial Intelligence (AI): The goal of creating machines that can match or exceed human intelligence—representing humanity’s quest to build systems that can think, reason, and adapt.\nMachine Learning (ML): The scientific discipline of understanding how systems can learn and improve from experience—providing the theoretical foundation for building intelligent systems.\n\n\n\nThe relationship between AI and ML exemplifies the connection between theoretical understanding and practical engineering implementation observed in other scientific fields. For instance, physics provides the theoretical foundation for mechanical engineering’s practical applications in structural design and machinery, while AI’s theoretical frameworks inform machine learning’s practical development of intelligent systems. Similarly, electrical engineering’s transformation of electromagnetic theory into functional power systems parallels machine learning’s implementation of intelligence theories into operational ML systems.\nThe emergence of machine learning as a viable scientific discipline approach to artificial intelligence resulted from extensive research and fundamental paradigm shifts in the field. The progression of artificial intelligence encompasses both theoretical advances in understanding intelligence and practical developments in implementation methodologies. This development mirrors the evolution of other scientific and engineering disciplines—from mechanical engineering’s advancement from basic force principles to contemporary robotics, to electrical engineering’s progression from fundamental electromagnetic theory to modern power and communication networks. Analysis of this historical trajectory reveals both the technological innovations leading to current machine learning approaches and the emergence of deep reinforcement learning2 that inform contemporary AI system development.\n2 Deep Reinforcement Learning: A machine learning approach that combines deep neural networks with reinforcement learning principles, allowing agents to learn optimal actions through trial and error interaction with an environment while receiving rewards or penalties.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "contents/core/introduction/introduction.html#evolution-of-ai",
    "href": "contents/core/introduction/introduction.html#evolution-of-ai",
    "title": "1  Introduction",
    "section": "1.3 Evolution of AI",
    "text": "1.3 Evolution of AI\nThe evolution of AI, depicted in the timeline shown in Figure 1.1, highlights key milestones such as the development of the perceptron3 in 1957 by Frank Rosenblatt, a foundational element for modern neural networks. Imagine walking into a computer lab in 1965. You’d find room-sized mainframes running programs that could prove basic mathematical theorems or play simple games like tic-tac-toe. These early artificial intelligence systems, while groundbreaking for their time, were a far cry from today’s machine learning systems that can detect cancer in medical images or understand human speech. The timeline shows the progression from early innovations like the ELIZA chatbot in 1966, to significant breakthroughs such as IBM’s Deep Blue defeating chess champion Garry Kasparov in 1997. More recent advancements include the introduction of OpenAI’s GPT-3 in 2020 and GPT-4 in 2023, demonstrating the dramatic evolution and increasing complexity of AI systems over the decades.\n3 Perceptron: The first artificial neural network—a simple model that could learn to classify visual patterns, similar to a single neuron making a yes/no decision based on its inputs.\n\n\n\n\n\nFigure 1.1: Milestones in AI from 1950 to 2020. Source: IEEE Spectrum\n\n\n\nLet’s explore how we got here.\n\n1.3.1 Symbolic AI (1956-1974)\nThe story of machine learning begins at the historic Dartmouth Conference in 1956, where pioneers like John McCarthy, Marvin Minsky, and Claude Shannon first coined the term “artificial intelligence.” Their approach was based on a compelling idea: intelligence could be reduced to symbol manipulation. Consider Daniel Bobrow’s STUDENT system from 1964, one of the first AI programs that could solve algebra word problems. It was one of the first AI programs to demonstrate natural language understanding by converting English text into algebraic equations, marking an important milestone in symbolic AI.\n\n\n\n\n\n\nExample: STUDENT (1964)\n\n\n\nProblem: \"If the number of customers Tom gets is twice the \nsquare of 20% of the number of advertisements he runs, and \nthe number of advertisements is 45, what is the number of \ncustomers Tom gets?\"\n \nSTUDENT would:\n\n1. Parse the English text\n2. Convert it to algebraic equations\n3. Solve the equation: n = 2(0.2 × 45)²\n4. Provide the answer: 162 customers\n\n\nEarly AI like STUDENT suffered from a fundamental limitation: they could only handle inputs that exactly matched their pre-programmed patterns and rules. Imagine a language translator that only works when sentences follow perfect grammatical structure—even slight variations like changing word order, using synonyms, or natural speech patterns would cause the STUDENT to fail. This “brittleness” meant that while these solutions could appear intelligent when handling very specific cases they were designed for, they would break down completely when faced with even minor variations or real-world complexity. This limitation wasn’t just a technical inconvenience—it revealed a deeper problem with rule-based approaches to AI: they couldn’t genuinely understand or generalize from their programming, they could only match and manipulate patterns exactly as specified.\n\n\n1.3.2 Expert Systems (1970s-1980s)\nBy the mid-1970s, researchers realized that general AI was too ambitious. Instead, they focused on capturing human expert knowledge in specific domains. MYCIN, developed at Stanford, was one of the first large-scale expert systems designed to diagnose blood infections.\n\n\n\n\n\n\nExample: MYCIN (1976)\n\n\n\nRule Example from MYCIN:\nIF\n    The infection is primary-bacteremia\n    The site of the culture is one of the sterile sites\n    The suspected portal of entry is the gastrointestinal tract\nTHEN\n    There is suggestive evidence (0.7) that infection is bacteroid \n\n\nWhile MYCIN represented a major advance in medical AI with its 600 expert rules for diagnosing blood infections, it revealed fundamental challenges that still plague ML today. Getting domain knowledge from human experts and converting it into precise rules proved incredibly time-consuming and difficult—doctors often couldn’t explain exactly how they made decisions. MYCIN struggled with uncertain or incomplete information, unlike human doctors who could make educated guesses. Perhaps most importantly, maintaining and updating the rule base became exponentially more complex as MYCIN grew—adding new rules often conflicted with existing ones, and medical knowledge itself kept evolving. These same challenges of knowledge capture, uncertainty handling, and maintenance remain central concerns in modern machine learning, even though we now use different technical approaches to address them.\n\n\n1.3.3 Statistical Learning: A Paradigm Shift (1990s)\nThe 1990s marked a radical transformation in artificial intelligence as the field moved away from hand-coded rules toward statistical learning approaches. This wasn’t a simple choice—it was driven by three converging factors that made statistical methods both possible and powerful. The digital revolution meant massive amounts of data were suddenly available to train the algorithms. Moore’s Law4 delivered the computational power needed to process this data effectively. And researchers developed new algorithms like Support Vector Machines and improved neural networks that could actually learn patterns from this data rather than following pre-programmed rules. This combination fundamentally changed how we built AI: instead of trying to encode human knowledge directly, we could now let machines discover patterns automatically from examples, leading to more robust and adaptable AI.\n4 Moore’s Law: The observation made by Intel co-founder Gordon Moore in 1965 that the number of transistors on a microchip doubles approximately every two years, while the cost halves. This exponential growth in computing power has been a key driver of advances in machine learning, though the pace has begun to slow in recent years.Consider how email spam filtering evolved:\n\n\n\n\n\n\nExample: Early Spam Detection Systems\n\n\n\nRule-based (1980s):\nIF contains(\"viagra\") OR contains(\"winner\") THEN spam\n\nStatistical (1990s):\nP(spam|word) = (frequency in spam emails) / (total frequency)\n\nCombined using Naive Bayes:\nP(spam|email) ∝ P(spam) × ∏ P(word|spam)\n\n\nThe move to statistical approaches fundamentally changed how we think about building AI by introducing three core concepts that remain important today. First, the quality and quantity of training data became as important as the algorithms themselves—AI could only learn patterns that were present in its training examples. Second, we needed rigorous ways to evaluate how well AI actually performed, leading to metrics that could measure success and compare different approaches. Third, we discovered an inherent tension between precision (being right when we make a prediction) and recall (catching all the cases we should find), forcing designers to make explicit trade-offs based on their application’s needs. For example, a spam filter might tolerate some spam to avoid blocking important emails, while medical diagnosis might need to catch every potential case even if it means more false alarms.\nTable 1.1 encapsulates the evolutionary journey of AI approaches we have discussed so far, highlighting the key strengths and capabilities that emerged with each new paradigm. As we move from left to right across the table, we can observe several important trends. We will talk about shallow and deep learning next, but it is useful to understand the trade-offs between the approaches we have covered so far.\n\n\n\nTable 1.1: Evolution of AI – Key Positive Aspects\n\n\n\n\n\n\n\n\n\n\n\n\nAspect\nSymbolic AI\nExpert Systems\nStatistical Learning\nShallow / Deep Learning\n\n\n\n\nKey Strength\nLogical reasoning\nDomain expertise\nVersatility\nPattern recognition\n\n\nBest Use Case\nWell-defined, rule-based problems\nSpecific domain problems\nVarious structured data problems\nComplex, unstructured data problems\n\n\nData Handling\nMinimal data needed\nDomain knowledge-based\nModerate data required\nLarge-scale data processing\n\n\nAdaptability\nFixed rules\nDomain-specific adaptability\nAdaptable to various domains\nHighly adaptable to diverse tasks\n\n\nProblem Complexity\nSimple, logic-based\nComplicated, domain- specific\nComplex, structured\nHighly complex, unstructured\n\n\n\n\n\n\nThe table serves as a bridge between the early approaches we’ve discussed and the more recent developments in shallow and deep learning that we’ll explore next. It sets the stage for understanding why certain approaches gained prominence in different eras and how each new paradigm built upon and addressed the limitations of its predecessors. Moreover, it illustrates how the strengths of earlier approaches continue to influence and enhance modern AI techniques, particularly in the era of foundation models.\n\n\n1.3.4 Shallow Learning (2000s)\nThe 2000s marked a fascinating period in machine learning history that we now call the ``shallow learning’’ era. To understand why it’s “shallow,” imagine building a house: deep learning (which came later) is like having multiple construction crews working at different levels simultaneously, each crew learning from the work of crews below them. In contrast, shallow learning typically had just one or two levels of processing – like having just a foundation crew and a framing crew.\nDuring this time, several powerful algorithms dominated the machine learning landscape. Each brought unique strengths to different problems: Decision trees provided interpretable results by making choices much like a flowchart. K-nearest neighbors made predictions by finding similar examples in past data, like asking your most experienced neighbors for advice. Linear and logistic regression offered straightforward, interpretable models that worked well for many real-world problems. Support Vector Machines (SVMs) excelled at finding complex boundaries between categories using the “kernel trick”—imagine being able to untangle a bowl of spaghetti into straight lines by lifting it into a higher dimension. These algorithms formed the foundation of practical machine.\nConsider a typical computer vision solution from 2005:\n\n\n\n\n\n\nExample: Traditional Computer Vision Pipeline\n\n\n\n1. Manual Feature Extraction\n   - SIFT (Scale-Invariant Feature Transform)\n   - HOG (Histogram of Oriented Gradients)\n   - Gabor filters\n2. Feature Selection/Engineering\n3. \"Shallow\" Learning Model (e.g., SVM)\n4. Post-processing\n\n\nWhat made this era distinct was its hybrid approach: human-engineered features combined with statistical learning. They had strong mathematical foundations (researchers could prove why they worked). They performed well even with limited data. They were computationally efficient. They produced reliable, reproducible results.\nTake the example of face detection, where the Viola-Jones algorithm (2001) achieved real-time performance using simple rectangular features and a cascade of classifiers. This algorithm powered digital camera face detection for nearly a decade.\n\n\n1.3.5 Deep Learning (2012-Present)\nWhile Support Vector Machines excelled at finding complex boundaries between categories using mathematical transformations, deep learning took a radically different approach inspired by the human brain’s architecture. Deep learning is built from layers of artificial neurons5, where each layer learns to transform its input data into increasingly abstract representations. Imagine processing an image of a cat: the first layer might learn to detect simple edges and contrasts, the next layer combines these into basic shapes and textures, another layer might recognize whiskers and pointy ears, and the final layers assemble these features into the concept of “cat.”\n5 Artificial Neurons: Basic computational units in neural networks that mimic biological neurons, taking multiple inputs, applying weights and biases, and producing an output signal through an activation function.Unlike shallow learning methods that required humans to carefully engineer features, deep learning networks can automatically discover useful features directly from raw data. This ability to learn hierarchical representations—from simple to complex, concrete to abstract—is what makes deep learning “deep,” and it turned out to be a remarkably powerful approach for handling complex, real-world data like images, speech, and text.\nIn 2012, a deep neural network called AlexNet, shown in Figure 1.2, achieved a breakthrough in the ImageNet competition that would transform the field of machine learning. The challenge was formidable: correctly classify 1.2 million high-resolution images into 1,000 different categories. While previous approaches struggled with error rates above 25%, AlexNet6 achieved a 15.3% error rate, dramatically outperforming all existing methods.\n6 A breakthrough deep neural network from 2012 that won the ImageNet competition by a large margin and helped spark the deep learning revolution.The success of AlexNet wasn’t just a technical achievement—it was a watershed moment that demonstrated the practical viability of deep learning. It showed that with sufficient data, computational power, and architectural innovations, neural networks could outperform hand-engineered features and shallow learning methods that had dominated the field for decades. This single result triggered an explosion of research and applications in deep learning that continues to this day.\n\n\n\n\n\n\nFigure 1.2: Deep neural network architecture for Alexnet. Source: Krizhevsky, Sutskever, and Hinton (2017)\n\n\nKrizhevsky, Alex, Ilya Sutskever, and Geoffrey E. Hinton. 2017. “ImageNet Classification with Deep Convolutional Neural Networks.” Communications of the ACM 60 (6): 84–90. https://doi.org/10.1145/3065386.\n\n\nFrom this foundation, deep learning entered an era of unprecedented scale. By the late 2010s, companies like Google, Facebook, and OpenAI were training neural networks thousands of times larger than AlexNet. These massive models, often called “foundation models,” took deep learning to new heights. GPT-3, released in 2020, contained 175 billion parameters7—imagine a student that could read through all of Wikipedia multiple times and learn patterns from every article. These models showed remarkable abilities: writing human-like text, engaging in conversation, generating images from descriptions, and even writing computer code. The key insight was simple but powerful: as we made neural networks bigger and fed them more data, they became capable of solving increasingly complex tasks. However, this scale brought unprecedented systems challenges: how do you efficiently train models that require thousands of GPUs working in parallel? How do you store and serve models that are hundreds of gigabytes in size? How do you handle the massive datasets needed for training?\n7 Parameters: The adjustable values within a neural network that are modified during training, similar to how the brain’s neural connections grow stronger as you learn a new skill. Having more parameters generally means that the model can learn more complex patterns.8 Convolutional Neural Network (CNN): A type of neural network specially designed for processing images, inspired by how the human visual system works. The “convolutional” part refers to how it scans images in small chunks, similar to how our eyes focus on different parts of a scene.The deep learning revolution of 2012 didn’t emerge from nowhere—it was built on neural network research dating back to the 1950s. The story begins with Frank Rosenblatt’s Perceptron in 1957, which captured the imagination of researchers by showing how a simple artificial neuron could learn to classify patterns. While it could only handle linearly separable problems—a limitation dramatically highlighted by Minsky and Papert’s 1969 book “Perceptrons”—it introduced the fundamental concept of trainable neural networks. The 1980s brought more important breakthroughs: Rumelhart, Hinton, and Williams introduced backpropagation in 1986, providing a systematic way to train multi-layer networks, while Yann LeCun demonstrated its practical application in recognizing handwritten digits using convolutional neural networks (CNNs)8.\n\n\n\n\n\n\nImportant 1.1: Convolutional Network Demo from 1989\n\n\n\n\n\n\nYet these networks largely languished through the 1990s and 2000s, not because the ideas were wrong, but because they were ahead of their time—the field lacked three important ingredients: sufficient data to train complex networks, enough computational power to process this data, and the technical innovations needed to train very deep networks effectively.\nThe field had to wait for the convergence of big data, better computing hardware, and algorithmic breakthroughs before deep learning’s potential could be unlocked. This long gestation period helps explain why the 2012 ImageNet moment was less a sudden revolution and more the culmination of decades of accumulated research finally finding its moment. As we’ll explore in the following sections, this evolution has led to two significant developments in the field. First, it has given rise to define the field of machine learning systems engineering, a discipline that teaches how to bridge the gap between theoretical advancements and practical implementation. Second, it has necessitated a more comprehensive definition of machine learning systems, one that encompasses not just algorithms, but also data and computing infrastructure. Today’s challenges of scale echo many of the same fundamental questions about computation, data, and learning methods that researchers have grappled with since the field’s inception, but now within a more complex and interconnected framework.\nAs AI progressed from symbolic reasoning to statistical learning and deep learning, its applications became increasingly ambitious and complex. This growth introduced challenges that extended beyond algorithms, necessitating a new focus: engineering entire systems capable of deploying and sustaining AI at scale. This gave rise to the discipline of Machine Learning Systems Engineering.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "contents/core/introduction/introduction.html#rise-of-ml-systems-engineering",
    "href": "contents/core/introduction/introduction.html#rise-of-ml-systems-engineering",
    "title": "1  Introduction",
    "section": "1.4 Rise of ML Systems Engineering",
    "text": "1.4 Rise of ML Systems Engineering\nThe story we’ve traced—from the early days of the Perceptron through the deep learning revolution—has largely been one of algorithmic breakthroughs. Each era brought new mathematical insights and modeling approaches that pushed the boundaries of what AI could achieve. But something important changed over the past decade: the success of AI systems became increasingly dependent not just on algorithmic innovations, but on sophisticated engineering.\nThis shift mirrors the evolution of computer science and engineering in the late 1960s and early 1970s. During that period, as computing systems grew more complex, a new discipline emerged: Computer Engineering. This field bridged the gap between Electrical Engineering’s hardware expertise and Computer Science’s focus on algorithms and software. Computer Engineering arose because the challenges of designing and building complex computing systems required an integrated approach that neither discipline could fully address on its own.\nToday, we’re witnessing a similar transition in the field of AI. While Computer Science continues to push the boundaries of ML algorithms and Electrical Engineering advances specialized AI hardware, neither discipline fully addresses the engineering principles needed to deploy, optimize, and sustain ML systems at scale. This gap highlights the need for a new discipline: Machine Learning Systems Engineering.\nThere is no explicit definition of what this field is as such today, but it can be broadly defined as such:\n\n\n\n\n\n\nDefinition of Machine Learning Systems Engineering\n\n\n\nMachine Learning Systems Engineering (MLSysEng) is the discipline of designing, implementing, and operating artificially intelligent systems across computing scales—from resource-constrained embedded devices to warehouse-scale computers. This field integrates principles from engineering disciplines spanning hardware to software to create systems that are reliable, efficient, and optimized for their deployment context. It encompasses the complete lifecycle of AI applications: from requirements engineering and data collection through model development, system integration, deployment, monitoring, and maintenance. The field emphasizes engineering principles of systematic design, resource constraints, performance requirements, and operational reliability.\n\n\nLet’s consider space exploration. While astronauts venture into new frontiers and explore the vast unknowns of the universe, their discoveries are only possible because of the complex engineering systems supporting them—the rockets that lift them into space, the life support systems that keep them alive, and the communication networks that keep them connected to Earth. Similarly, while AI researchers push the boundaries of what’s possible with learning algorithms, their breakthroughs only become practical reality through careful systems engineering. Modern AI systems need robust infrastructure to collect and manage data, powerful computing systems to train models, and reliable deployment platforms to serve millions of users.\nThis emergence of machine learning systems engineering as a important discipline reflects a broader reality: turning AI algorithms into real-world systems requires bridging the gap between theoretical possibilities and practical implementation. It’s not enough to have a brilliant algorithm if you can’t efficiently collect and process the data it needs, distribute its computation across hundreds of machines, serve it reliably to millions of users, or monitor its performance in production.\nUnderstanding this interplay between algorithms and engineering has become fundamental for modern AI practitioners. While researchers continue to push the boundaries of what’s algorithmically possible, engineers are tackling the complex challenge of making these algorithms work reliably and efficiently in the real world. This brings us to a fundamental question: what exactly is a machine learning system, and what makes it different from traditional software systems?",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "contents/core/introduction/introduction.html#definition-of-a-ml-system",
    "href": "contents/core/introduction/introduction.html#definition-of-a-ml-system",
    "title": "1  Introduction",
    "section": "1.5 Definition of a ML System",
    "text": "1.5 Definition of a ML System\nThere’s no universally accepted, clear-cut textbook definition of a machine learning system. This ambiguity stems from the fact that different practitioners, researchers, and industries often refer to machine learning systems in varying contexts and with different scopes. Some might focus solely on the algorithmic aspects, while others might include the entire pipeline from data collection to model deployment. This loose usage of the term reflects the rapidly evolving and multidisciplinary nature of the field.\nGiven this diversity of perspectives, it is important to establish a clear and comprehensive definition that encompasses all these aspects. In this textbook, we take a holistic approach to machine learning systems, considering not just the algorithms but also the entire ecosystem in which they operate. Therefore, we define a machine learning system as follows:\n\n\n\n\n\n\nDefinition of a Machine Learning System\n\n\n\nA machine learning system is an integrated computing system comprising three core components: (1) data that guides algorithmic behavior, (2) learning algorithms that extract patterns from this data, and (3) computing infrastructure that enables both the learning process (i.e., training) and the application of learned knowledge (i.e., inference/serving). Together, these components create a computing system capable of making predictions, generating content, or taking actions based on learned patterns.\n\n\nThe core of any machine learning system consists of three interrelated components, as illustrated in Figure 1.3: Models/Algorithms, Data, and Computing Infrastructure. These components form a triangular dependency where each element fundamentally shapes the possibilities of the others. The model architecture dictates both the computational demands for training and inference, as well as the volume and structure of data required for effective learning. The data’s scale and complexity influence what infrastructure is needed for storage and processing, while simultaneously determining which model architectures are feasible. The infrastructure capabilities establish practical limits on both model scale and data processing capacity, creating a framework within which the other components must operate.\n\n\n\n\n\n\nFigure 1.3: Machine learning systems involve algorithms, data, and computation, all intertwined together.\n\n\n\nEach of these components serves a distinct but interconnected purpose:\n\nAlgorithms: Mathematical models and methods that learn patterns from data to make predictions or decisions\nData: Processes and infrastructure for collecting, storing, processing, managing, and serving data for both training and inference.\nComputing: Hardware and software infrastructure that enables efficient training, serving, and operation of models at scale.\n\nThe interdependency of these components means no single element can function in isolation. The most sophisticated algorithm cannot learn without data or computing resources to run on. The largest datasets are useless without algorithms to extract patterns or infrastructure to process them. And the most powerful computing infrastructure serves no purpose without algorithms to execute or data to process.\nTo illustrate these relationships, we can draw an analogy to space exploration. Algorithm developers are like astronauts—exploring new frontiers and making discoveries. Data science teams function like mission control specialists—ensuring the constant flow of critical information and resources needed to keep the mission running. Computing infrastructure engineers are like rocket engineers—designing and building the systems that make the mission possible. Just as a space mission requires the seamless integration of astronauts, mission control, and rocket systems, a machine learning system demands the careful orchestration of algorithms, data, and computing infrastructure.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "contents/core/introduction/introduction.html#ml-systems-lifecycle",
    "href": "contents/core/introduction/introduction.html#ml-systems-lifecycle",
    "title": "1  Introduction",
    "section": "1.6 ML Systems Lifecycle",
    "text": "1.6 ML Systems Lifecycle\nTraditional software systems follow a predictable lifecycle where developers write explicit instructions for computers to execute. These systems are built on decades of established software engineering practices. Version control systems maintain precise histories of code changes. Continuous integration and deployment pipelines automate testing and release processes. Static analysis tools measure code quality and identify potential issues. This infrastructure enables reliable development, testing, and deployment of software systems, following well-defined principles of software engineering.\nMachine learning systems represent a fundamental departure from this traditional paradigm. While traditional systems execute explicit programming logic, machine learning systems derive their behavior from patterns in data. This shift from code to data as the primary driver of system behavior introduces new complexities.\nAs illustrated in Figure 1.4, the ML lifecycle consists of interconnected stages from data collection through model monitoring, with feedback loops for continuous improvement when performance degrades or models need enhancement.\n\n\n\n\n\n\nFigure 1.4: The typical lifecycle of a machine learning system.\n\n\n\nUnlike source code, which changes only when developers modify it, data reflects the dynamic nature of the real world. Changes in data distributions can silently alter system behavior. Traditional software engineering tools, designed for deterministic code-based systems, prove insufficient for managing these data-dependent systems. For example, version control systems that excel at tracking discrete code changes struggle to manage large, evolving datasets. Testing frameworks designed for deterministic outputs must be adapted for probabilistic predictions. This data-dependent nature creates a more dynamic lifecycle, requiring continuous monitoring and adaptation to maintain system relevance as real-world data patterns evolve.\nUnderstanding the machine learning system lifecycle requires examining its distinct stages. Each stage presents unique requirements from both learning and infrastructure perspectives. This dual consideration—of learning needs and systems support—is wildly important for building effective machine learning systems.\nHowever, the various stages of the ML lifecycle in production are not isolated; they are, in fact, deeply interconnected. This interconnectedness can create either virtuous or vicious cycles. In a virtuous cycle, high-quality data enables effective learning, robust infrastructure supports efficient processing, and well-engineered systems facilitate the collection of even better data. However, in a vicious cycle, poor data quality undermines learning, inadequate infrastructure hampers processing, and system limitations prevent the improvement of data collection—each problem compounds the others.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "contents/core/introduction/introduction.html#the-spectrum-of-ml-systems",
    "href": "contents/core/introduction/introduction.html#the-spectrum-of-ml-systems",
    "title": "1  Introduction",
    "section": "1.7 The Spectrum of ML Systems",
    "text": "1.7 The Spectrum of ML Systems\nThe complexity of managing machine learning systems becomes even more apparent when we consider the broad spectrum across which ML is deployed today. ML systems exist at vastly different scales and in diverse environments, each presenting unique challenges and constraints.\nAt one end of the spectrum, we have cloud-based ML systems running in massive data centers. These systems, like large language models or recommendation engines, process petabytes of data and serve millions of users simultaneously. They can leverage virtually unlimited computing resources but must manage enormous operational complexity and costs.\nAt the other end, we find TinyML systems running on microcontrollers and embedded devices. These systems must perform ML tasks with severe constraints on memory, computing power, and energy consumption. Imagine a smart home device, such as Alexa or Google Assistant, that must recognize voice commands using less power than a LED bulb, or a sensor that must detect anomalies while running on a battery for months or even years.\nBetween these extremes, we find a rich variety of ML systems adapted for different contexts. Edge ML systems bring computation closer to data sources, reducing latency and bandwidth requirements while managing local computing resources. Mobile ML systems must balance sophisticated capabilities with battery life and processor limitations on smartphones and tablets. Enterprise ML systems often operate within specific business constraints, focusing on particular tasks while integrating with existing infrastructure. Some organizations employ hybrid approaches, distributing ML capabilities across multiple tiers to balance various requirements.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "contents/core/introduction/introduction.html#ml-system-implications-on-the-ml-lifecycle",
    "href": "contents/core/introduction/introduction.html#ml-system-implications-on-the-ml-lifecycle",
    "title": "1  Introduction",
    "section": "1.8 ML System Implications on the ML Lifecycle",
    "text": "1.8 ML System Implications on the ML Lifecycle\nThe diversity of ML systems across the spectrum represents a complex interplay of requirements, constraints, and trade-offs. These decisions fundamentally impact every stage of the ML lifecycle we discussed earlier, from data collection to continuous operation.\nPerformance requirements often drive initial architectural decisions. Latency-sensitive applications, like autonomous vehicles or real-time fraud detection, might require edge or embedded architectures despite their resource constraints. Conversely, applications requiring massive computational power for training, such as large language models, naturally gravitate toward centralized cloud architectures. However, raw performance is just one consideration in a complex decision space.\nResource management varies dramatically across architectures. Cloud systems must optimize for cost efficiency at scale—balancing expensive GPU clusters, storage systems, and network bandwidth. Edge systems face fixed resource limits and must carefully manage local compute and storage. Mobile and embedded systems operate under the strictest constraints, where every byte of memory and milliwatt of power matters. These resource considerations directly influence both model design and system architecture.\nOperational complexity increases with system distribution. While centralized cloud architectures benefit from mature deployment tools and managed services, edge and hybrid systems must handle the complexity of distributed system management. This complexity manifests throughout the ML lifecycle—from data collection and version control to model deployment and monitoring. This operational complexity can compound over time if not carefully managed.\nData considerations often introduce competing pressures. Privacy requirements or data sovereignty regulations might push toward edge or embedded architectures, while the need for large-scale training data might favor cloud approaches. The velocity and volume of data also influence architectural choices—real-time sensor data might require edge processing to manage bandwidth, while batch analytics might be better suited to cloud processing.\nEvolution and maintenance requirements must be considered from the start. Cloud architectures offer flexibility for system evolution but can incur significant ongoing costs. Edge and embedded systems might be harder to update but could offer lower operational overhead. The continuous cycle of ML systems we discussed earlier becomes particularly challenging in distributed architectures, where updating models and maintaining system health requires careful orchestration across multiple tiers.\nThese trade-offs are rarely simple binary choices. Modern ML systems often adopt hybrid approaches, carefully balancing these considerations based on specific use cases and constraints. The key is understanding how these decisions will impact the system throughout its lifecycle, from initial development through continuous operation and evolution.\n\n1.8.1 Emerging Trends\nThe landscape of machine learning systems is evolving rapidly, with innovations happening from user-facing applications down to core infrastructure. These changes are reshaping how we design and deploy ML systems.\n\nApplication-Level Innovation\nThe rise of agentic systems marks a profound shift from traditional reactive ML systems that simply made predictions based on input data. Modern applications can now take actions, learn from outcomes, and adapt their behavior accordingly through multi-agent systems9 and advanced planning algorithms. These autonomous agents can plan, reason, and execute complex tasks, introducing new requirements for decision-making frameworks and safety constraints.\n9 Multi-Agent System: A computational system where multiple intelligent agents interact within an environment, each pursuing their own objectives while potentially cooperating or competing with other agents.This increased sophistication extends to operational intelligence. Applications will likely incorporate sophisticated self-monitoring, automated resource management, and adaptive deployment strategies. They can automatically handle data distribution shifts, model updates, and system optimization, marking a significant advance in autonomous operation.\n\n\nSystem Architecture Evolution\nSupporting these advanced applications requires fundamental changes in the underlying system architecture. Integration frameworks are evolving to handle increasingly complex interactions between ML systems and broader technology ecosystems. Modern ML systems must seamlessly connect with existing software, process diverse data sources, and operate across organizational boundaries, driving new approaches to system design.\nResource efficiency has become a central architectural concern as ML systems scale. Innovation in model compression and efficient training techniques is being driven by both environmental and economic factors. Future architectures must carefully balance the pursuit of more powerful models against growing sustainability concerns.\nAt the infrastructure level, new hardware is reshaping deployment possibilities. Specialized AI accelerators are emerging across the spectrum—from powerful data center chips to efficient edge processors10 to tiny neural processing units in mobile devices. This heterogeneous computing landscape enables dynamic model distribution across tiers based on computing capabilities and conditions, blurring traditional boundaries between cloud, edge, and embedded systems.\n10 Edge Processor: A specialized computing device designed to perform AI computations close to where data is generated, optimized for low latency and energy efficiency rather than raw computing power.These trends are creating ML systems that are more capable and efficient while managing increasing complexity. Success in this evolving landscape requires understanding how application requirements flow down to infrastructure decisions, ensuring systems can grow sustainably while delivering increasingly sophisticated capabilities.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "contents/core/introduction/introduction.html#real-world-applications",
    "href": "contents/core/introduction/introduction.html#real-world-applications",
    "title": "1  Introduction",
    "section": "1.9 Real-world Applications",
    "text": "1.9 Real-world Applications\nThe diverse architectures and scales of ML systems demonstrate their potential to revolutionize industries. By examining real-world applications, we can see how these systems address practical challenges and drive innovation. Their ability to operate effectively across varying scales and environments has already led to significant changes in numerous sectors. This section highlights examples where theoretical concepts and practical considerations converge to produce tangible, impactful results.\n\n1.9.1 FarmBeats: Edge and Embedded ML for Agriculture\nFarmBeats, a project developed by Microsoft Research, shown in Figure 1.5 is a significant advancement in the application of machine learning to agriculture. This system aims to increase farm productivity and reduce costs by leveraging AI and IoT technologies. FarmBeats exemplifies how edge and embedded ML systems can be deployed in challenging, real-world environments to solve practical problems. By bringing ML capabilities directly to the farm, FarmBeats demonstrates the potential of distributed AI systems in transforming traditional industries.\n\n\n\n\n\n\nFigure 1.5: Microsoft FarmBeats: AI, Edge & IoT for Agriculture.\n\n\n\n\nData Aspects\nThe data ecosystem in FarmBeats is diverse and distributed. Sensors deployed across fields collect real-time data on soil moisture, temperature, and nutrient levels. Drones equipped with multispectral cameras capture high-resolution imagery of crops, providing insights into plant health and growth patterns. Weather stations contribute local climate data, while historical farming records offer context for long-term trends. The challenge lies not just in collecting this heterogeneous data, but in managing its flow from dispersed, often remote locations with limited connectivity. FarmBeats employs innovative data transmission techniques, such as using TV white spaces (unused broadcasting frequencies) to extend internet connectivity to far-flung sensors. This approach to data collection and transmission embodies the principles of edge computing we discussed earlier, where data processing begins at the source to reduce bandwidth requirements and enable real-time decision making.\n\n\nAlgorithm/Model Aspects\nFarmBeats uses a variety of ML algorithms tailored to agricultural applications. For soil moisture prediction, it uses temporal neural networks that can capture the complex dynamics of water movement in soil. Computer vision algorithms process drone imagery to detect crop stress, pest infestations, and yield estimates. These models must be robust to noisy data and capable of operating with limited computational resources. Machine learning methods such as “transfer learning” allow models to learn on data-rich farms to be adapted for use in areas with limited historical data. The system also incorporates a mixture of methods that combine outputs from multiple algorithms to improve prediction accuracy and reliability. A key challenge FarmBeats addresses is model personalization—adapting general models to the specific conditions of individual farms, which may have unique soil compositions, microclimates, and farming practices.\n\n\nComputing Infrastructure Aspects\nFarmBeats exemplifies the edge computing paradigm we explored in our discussion of the ML system spectrum. At the lowest level, embedded ML models run directly on IoT devices and sensors, performing basic data filtering and anomaly detection. Edge devices, such as ruggedized field gateways, aggregate data from multiple sensors and run more complex models for local decision-making. These edge devices operate in challenging conditions, requiring robust hardware designs and efficient power management to function reliably in remote agricultural settings. The system employs a hierarchical architecture, with more computationally intensive tasks offloaded to on-premises servers or the cloud. This tiered approach allows FarmBeats to balance the need for real-time processing with the benefits of centralized data analysis and model training. The infrastructure also includes mechanisms for over-the-air model updates, ensuring that edge devices can receive improved models as more data becomes available and algorithms are refined.\n\n\nImpact and Future Implications\nFarmBeats shows how ML systems can be deployed in resource-constrained, real-world environments to drive significant improvements in traditional industries. By providing farmers with AI-driven insights, the system has shown potential to increase crop yields, reduce water usage, and optimize resource allocation. Looking forward, the FarmBeats approach could be extended to address global challenges in food security and sustainable agriculture. The success of this system also highlights the growing importance of edge and embedded ML in IoT applications, where bringing intelligence closer to the data source can lead to more responsive, efficient, and scalable solutions. As edge computing capabilities continue to advance, we can expect to see similar distributed ML architectures applied to other domains, from smart cities to environmental monitoring.\n\n\n\n1.9.2 AlphaFold: Large-Scale Scientific ML\nAlphaFold, developed by DeepMind, is a landmark achievement in the application of machine learning to complex scientific problems. This AI system is designed to predict the three-dimensional structure of proteins, as shown in Figure 1.6, from their amino acid sequences, a challenge known as the “protein folding problem” that has puzzled scientists for decades. AlphaFold’s success demonstrates how large-scale ML systems can accelerate scientific discovery and potentially revolutionize fields like structural biology and drug design. This case study exemplifies the use of advanced ML techniques and massive computational resources to tackle problems at the frontiers of science.\n\n\n\n\n\n\nFigure 1.6: Examples of protein targets within the free modeling category. Source: Google DeepMind\n\n\n\n\nData Aspects\nThe data underpinning AlphaFold’s success is vast and multifaceted. The primary dataset is the Protein Data Bank (PDB), which contains the experimentally determined structures of over 180,000 proteins. This is complemented by databases of protein sequences, which number in the hundreds of millions. AlphaFold also utilizes evolutionary data in the form of multiple sequence alignments (MSAs), which provide insights into the conservation patterns of amino acids across related proteins. The challenge lies not just in the volume of data, but in its quality and representation. Experimental protein structures can contain errors or be incomplete, requiring sophisticated data cleaning and validation processes. Moreover, the representation of protein structures and sequences in a form amenable to machine learning is a significant challenge in itself. AlphaFold’s data pipeline involves complex preprocessing steps to convert raw sequence and structural data into meaningful features that capture the physical and chemical properties relevant to protein folding.\n\n\nAlgorithm/Model Aspects\nAlphaFold’s algorithmic approach represents a tour de force in the application of deep learning to scientific problems. At its core, AlphaFold uses a novel neural network architecture that combines with techniques from computational biology. The model learns to predict inter-residue distances and torsion angles, which are then used to construct a full 3D protein structure. A key innovation is the use of “equivariant attention” layers that respect the symmetries inherent in protein structures. The learning process involves multiple stages, including initial “pretraining” on a large corpus of protein sequences, followed by fine-tuning on known structures. AlphaFold also incorporates domain knowledge in the form of physics-based constraints and scoring functions, creating a hybrid system that leverages both data-driven learning and scientific prior knowledge. The model’s ability to generate accurate confidence estimates for its predictions is crucial, allowing researchers to assess the reliability of the predicted structures.\n\n\nComputing Infrastructure Aspects\nThe computational demands of AlphaFold epitomize the challenges of large-scale scientific ML systems. Training the model requires massive parallel computing resources, leveraging clusters of GPUs or TPUs (Tensor Processing Units) in a distributed computing environment. DeepMind utilized Google’s cloud infrastructure, with the final version of AlphaFold trained on 128 TPUv3 cores for several weeks. The inference process, while less computationally intensive than training, still requires significant resources, especially when predicting structures for large proteins or processing many proteins in parallel. To make AlphaFold more accessible to the scientific community, DeepMind has collaborated with the European Bioinformatics Institute to create a public database of predicted protein structures, which itself represents a substantial computing and data management challenge. This infrastructure allows researchers worldwide to access AlphaFold’s predictions without needing to run the model themselves, demonstrating how centralized, high-performance computing resources can be leveraged to democratize access to advanced ML capabilities.\n\n\nImpact and Future Implications\nAlphaFold’s impact on structural biology has been profound, with the potential to accelerate research in areas ranging from fundamental biology to drug discovery. By providing accurate structural predictions for proteins that have resisted experimental methods, AlphaFold opens new avenues for understanding disease mechanisms and designing targeted therapies. The success of AlphaFold also serves as a powerful demonstration of how ML can be applied to other complex scientific problems, potentially leading to breakthroughs in fields like materials science or climate modeling. However, it also raises important questions about the role of AI in scientific discovery and the changing nature of scientific inquiry in the age of large-scale ML systems. As we look to the future, the AlphaFold approach suggests a new paradigm for scientific ML, where massive computational resources are combined with domain-specific knowledge to push the boundaries of human understanding.\n\n\n\n1.9.3 Autonomous Vehicles: Spanning the ML Spectrum\nWaymo, a subsidiary of Alphabet Inc., stands at the forefront of autonomous vehicle technology, representing one of the most ambitious applications of machine learning systems to date. Evolving from the Google Self-Driving Car Project initiated in 2009, Waymo’s approach to autonomous driving exemplifies how ML systems can span the entire spectrum from embedded systems to cloud infrastructure. This case study demonstrates the practical implementation of complex ML systems in a safety-critical, real-world environment, integrating real-time decision-making with long-term learning and adaptation.\n\nData Aspects\nThe data ecosystem underpinning Waymo’s technology is vast and dynamic. Each vehicle serves as a roving data center, its sensor suite—comprising LiDAR11, radar, and high-resolution cameras—generating approximately one terabyte of data per hour of driving. This real-world data is complemented by an even more extensive simulated dataset, with Waymo’s vehicles having traversed over 20 billion miles in simulation and more than 20 million miles on public roads. The challenge lies not just in the volume of data, but in its heterogeneity and the need for real-time processing. Waymo must handle both structured (e.g., GPS coordinates) and unstructured data (e.g., camera images) simultaneously. The data pipeline spans from edge processing on the vehicle itself to massive cloud-based storage and processing systems. Sophisticated data cleaning and validation processes are necessary, given the safety-critical nature of the application. Moreover, the representation of the vehicle’s environment in a form amenable to machine learning presents significant challenges, requiring complex preprocessing to convert raw sensor data into meaningful features that capture the dynamics of traffic scenarios.\n11 LiDAR (Light Detection and Ranging): A remote sensing technology that uses pulsed laser light to measure distances to objects, creating detailed 3D maps of the environment essential for autonomous vehicle navigation.\n\nAlgorithm/Model Aspects\nWaymo’s ML stack represents a sophisticated ensemble of algorithms tailored to the multifaceted challenge of autonomous driving. The perception system employs deep learning techniques, including convolutional neural networks, to process visual data for object detection and tracking. Prediction models, needed for anticipating the behavior of other road users, leverage recurrent neural networks (RNNs)12 to understand temporal sequences. Waymo has developed custom ML models like VectorNet for predicting vehicle trajectories. The planning and decision-making systems may incorporate reinforcement learning or imitation learning techniques to navigate complex traffic scenarios. A key innovation in Waymo’s approach is the integration of these diverse models into a coherent system capable of real-time operation. The ML models must also be interpretable to some degree, as understanding the reasoning behind a vehicle’s decisions is vital for safety and regulatory compliance. Waymo’s learning process involves continuous refinement based on real-world driving experiences and extensive simulation, creating a feedback loop that constantly improves the system’s performance.\n12 Recurrent Neural Network (RNN): A type of neural network specifically designed to handle sequential data by maintaining an internal memory state that allows it to learn patterns across time, making it particularly useful for tasks like language processing and time series prediction.\n\nComputing Infrastructure Aspects\nThe computing infrastructure supporting Waymo’s autonomous vehicles epitomizes the challenges of deploying ML systems across the full spectrum from edge to cloud. Each vehicle is equipped with a custom-designed compute platform capable of processing sensor data and making decisions in real-time, often leveraging specialized hardware like GPUs or tensor processing units (TPUs)13. This edge computing is complemented by extensive use of cloud infrastructure, leveraging the power of Google’s data centers for training models, running large-scale simulations, and performing fleet-wide learning. The connectivity between these tiers is critical, with vehicles requiring reliable, high-bandwidth communication for real-time updates and data uploading. Waymo’s infrastructure must be designed for robustness and fault tolerance, ensuring safe operation even in the face of hardware failures or network disruptions. The scale of Waymo’s operation presents significant challenges in data management, model deployment, and system monitoring across a geographically distributed fleet of vehicles.\n13 Tensor Processing Unit (TPU): A specialized AI accelerator chip designed by Google specifically for neural network machine learning, particularly efficient at matrix operations common in deep learning workloads.\n\nImpact and Future Implications\nWaymo’s impact extends beyond technological advancement, potentially revolutionizing transportation, urban planning, and numerous aspects of daily life. The launch of Waymo One, a commercial ride-hailing service using autonomous vehicles in Phoenix, Arizona, represents a significant milestone in the practical deployment of AI systems in safety-critical applications. Waymo’s progress has broader implications for the development of robust, real-world AI systems, driving innovations in sensor technology, edge computing, and AI safety that have applications far beyond the automotive industry. However, it also raises important questions about liability, ethics, and the interaction between AI systems and human society. As Waymo continues to expand its operations and explore applications in trucking and last-mile delivery, it serves as an important test bed for advanced ML systems, driving progress in areas such as continual learning, robust perception, and human-AI interaction. The Waymo case study underscores both the tremendous potential of ML systems to transform industries and the complex challenges involved in deploying AI in the real world.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "contents/core/introduction/introduction.html#challenges-and-considerations",
    "href": "contents/core/introduction/introduction.html#challenges-and-considerations",
    "title": "1  Introduction",
    "section": "1.10 Challenges and Considerations",
    "text": "1.10 Challenges and Considerations\nBuilding and deploying machine learning systems presents unique challenges that go beyond traditional software development. These challenges help explain why creating effective ML systems is about more than just choosing the right algorithm or collecting enough data. Let’s explore the key areas where ML practitioners face significant hurdles.\n\n1.10.1 Data Challenges\nThe foundation of any ML system is its data, and managing this data introduces several fundamental challenges. First, there’s the basic question of data quality—real-world data is often messy and inconsistent. Imagine a healthcare application that needs to process patient records from different hospitals. Each hospital might record information differently, use different units of measurement, or have different standards for what data to collect. Some records might have missing information, while others might contain errors or inconsistencies that need to be cleaned up before the data can be useful.\nAs ML systems grow, they often need to handle increasingly large amounts of data. A video streaming service like Netflix, for example, needs to process billions of viewer interactions to power its recommendation system. This scale introduces new challenges in how to store, process, and manage such large datasets efficiently.\nAnother critical challenge is how data changes over time. This phenomenon, known as “data drift”14, occurs when the patterns in new data begin to differ from the patterns the system originally learned from. For example, many predictive models struggled during the COVID-19 pandemic because consumer behavior changed so dramatically that historical patterns became less relevant. ML systems need ways to detect when this happens and adapt accordingly.\n14 Data Drift: The gradual change in the statistical properties of the target variable (what the model is trying to predict) over time, which can degrade model performance if not properly monitored and addressed.\n\n1.10.2 Model Challenges\nCreating and maintaining the ML models themselves presents another set of challenges. Modern ML models, particularly in deep learning, can be extremely complex. Consider a language model like GPT-3, which has hundreds of billions of parameters that need to be optimized through backpropagation15. This complexity creates practical challenges: these models require enormous computing power to train and run, making it difficult to deploy them in situations with limited resources, like on mobile phones or IoT devices.\n15 Backpropagation: The primary algorithm used to train neural networks, which calculates how each parameter in the network should be adjusted to minimize prediction errors by propagating error gradients backward through the network layers.16 Transfer Learning: A machine learning method where a model developed for one task is reused as the starting point for a model on a second task, significantly reducing the amount of training data and computation required.Training these models effectively is itself a significant challenge. Unlike traditional programming where we write explicit instructions, ML models learn from examples through techniques like transfer learning16. This learning process involves many choices: How should we structure the model? How long should we train it? How can we tell if it’s learning the right things? Making these decisions often requires both technical expertise and considerable trial and error.\nA particularly important challenge is ensuring that models work well in real-world conditions. A model might perform excellently on its training data but fail when faced with slightly different situations in the real world. This gap between training performance and real-world performance is a central challenge in machine learning, especially for critical applications like autonomous vehicles or medical diagnosis systems.\n\n\n1.10.3 System Challenges\nGetting ML systems to work reliably in the real world introduces its own set of challenges. Unlike traditional software that follows fixed rules, ML systems need to handle uncertainty and variability in their inputs and outputs. They also typically need both training systems (for learning from data) and serving systems (for making predictions), each with different requirements and constraints.\nConsider a company building a speech recognition system. They need infrastructure to collect and store audio data, systems to train models on this data, and then separate systems to actually process users’ speech in real-time. Each part of this pipeline needs to work reliably and efficiently, and all the parts need to work together seamlessly.\nThese systems also need constant monitoring and updating. How do we know if the system is working correctly? How do we update models without interrupting service? How do we handle errors or unexpected inputs? These operational challenges become particularly complex when ML systems are serving millions of users.\n\n\n1.10.4 Ethical and Social Considerations\nAs ML systems become more prevalent in our daily lives, their broader impacts on society become increasingly important to consider. One major concern is fairness – ML systems can sometimes learn to make decisions that discriminate against certain groups of people. This often happens unintentionally, as the systems pick up biases present in their training data. For example, a job application screening system might inadvertently learn to favor certain demographics if those groups were historically more likely to be hired.\nAnother important consideration is transparency. Many modern ML models, particularly deep learning models, work as “black boxes” – while they can make predictions, it’s often difficult to understand how they arrived at their decisions. This becomes particularly problematic when ML systems are making important decisions about people’s lives, such as in healthcare or financial services.\nPrivacy is also a major concern. ML systems often need large amounts of data to work effectively, but this data might contain sensitive personal information. How do we balance the need for data with the need to protect individual privacy? How do we ensure that models don’t inadvertently memorize and reveal private information through inference attacks17? These challenges aren’t merely technical problems to be solved, but ongoing considerations that shape how we approach ML system design and deployment.\n17 Inference Attack: A technique where an adversary attempts to extract sensitive information about the training data by making careful queries to a trained model, exploiting patterns the model may have inadvertently memorized during training.These challenges aren’t merely technical problems to be solved, but ongoing considerations that shape how we approach ML system design and deployment. Throughout this book, we’ll explore these challenges in detail and examine strategies for addressing them effectively.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "contents/core/introduction/introduction.html#future-directions",
    "href": "contents/core/introduction/introduction.html#future-directions",
    "title": "1  Introduction",
    "section": "1.11 Future Directions",
    "text": "1.11 Future Directions\nAs we look to the future of machine learning systems, several exciting trends are shaping the field. These developments promise to both solve existing challenges and open new possibilities for what ML systems can achieve.\nOne of the most significant trends is the democratization of AI technology. Just as personal computers transformed computing from specialized mainframes to everyday tools, ML systems are becoming more accessible to developers and organizations of all sizes. Cloud providers now offer pre-trained models and automated ML platforms that reduce the expertise needed to deploy AI solutions. This democratization is enabling new applications across industries, from small businesses using AI for customer service to researchers applying ML to previously intractable problems.\nAs concerns about computational costs and environmental impact grow, there’s an increasing focus on making ML systems more efficient. Researchers are developing new techniques for training models with less data and computing power. Innovation in specialized hardware, from improved GPUs to custom AI chips, is making ML systems faster and more energy-efficient. These advances could make sophisticated AI capabilities available on more devices, from smartphones to IoT sensors.\nPerhaps the most transformative trend is the development of more autonomous ML systems that can adapt and improve themselves. These systems are beginning to handle their own maintenance tasks – detecting when they need retraining, automatically finding and correcting errors, and optimizing their own performance. This automation could dramatically reduce the operational overhead of running ML systems while improving their reliability.\nWhile these trends are promising, it’s important to recognize the field’s limitations. Creating truly artificial general intelligence remains a distant goal. Current ML systems excel at specific tasks but lack the flexibility and understanding that humans take for granted. Challenges around bias, transparency, and privacy continue to require careful consideration. As ML systems become more prevalent, addressing these limitations while leveraging new capabilities will be crucial.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "contents/core/introduction/introduction.html#learning-path-and-book-structure",
    "href": "contents/core/introduction/introduction.html#learning-path-and-book-structure",
    "title": "1  Introduction",
    "section": "1.12 Learning Path and Book Structure",
    "text": "1.12 Learning Path and Book Structure\nThis book is designed to guide you from understanding the fundamentals of ML systems to effectively designing and implementing them. To address the complexities and challenges of Machine Learning Systems engineering, we’ve organized the content around five fundamental pillars that encompass the lifecycle of ML systems. These pillars provide a framework for understanding, developing, and maintaining robust ML systems.\n\n\n\n\n\n\nFigure 1.7: Overview of the five fundamental system pillars of Machine Learning Systems engineering.\n\n\n\nAs illustrated in Figure 1.7, the five pillars central to the framework are:\n\nData: Emphasizing data engineering and foundational principles critical to how AI operates in relation to data.\nTraining: Exploring the methodologies for AI training, focusing on efficiency, optimization, and acceleration techniques to enhance model performance.\nDeployment: Encompassing benchmarks, on-device learning strategies, and machine learning operations to ensure effective model application.\nOperations: Highlighting the maintenance challenges unique to machine learning systems, which require specialized approaches distinct from traditional engineering systems.\nEthics & Governance: Addressing concerns such as security, privacy, responsible AI practices, and the broader societal implications of AI technologies.\n\nEach pillar represents a critical phase in the lifecycle of ML systems and is composed of foundational elements that build upon each other. This structure ensures a comprehensive understanding of MLSE, from basic principles to advanced applications and ethical considerations.\nFor more detailed information about the book’s overview, contents, learning outcomes, target audience, prerequisites, and navigation guide, please refer to the About the Book section. There, you’ll also find valuable details about our learning community and how to maximize your experience with this resource.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "contents/core/ml_systems/ml_systems.html",
    "href": "contents/core/ml_systems/ml_systems.html",
    "title": "2  ML Systems",
    "section": "",
    "text": "Purpose\nResources: Slides, Videos, Exercises\nHow do the diverse environments where machine learning operates shape the fundamental nature of these systems, and what drives their widespread deployment across computing platforms?\nThe deployment of machine learning systems across varied computing environments reveals essential insights into the relationship between theoretical principles and practical implementation. Each computing environment – from large-scale distributed systems to resource-constrained devices – introduces distinct requirements that influence both system architecture and algorithmic approaches. Understanding these relationships reveals core engineering principles that govern the design of machine learning systems. This understanding provides a foundation for examining how theoretical concepts translate into practical implementations, and how system designs adapt to meet diverse computational, memory, and energy constraints.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>ML Systems</span>"
    ]
  },
  {
    "objectID": "contents/core/ml_systems/ml_systems.html#purpose",
    "href": "contents/core/ml_systems/ml_systems.html#purpose",
    "title": "2  ML Systems",
    "section": "",
    "text": "Learning Objectives\n\n\n\n\nUnderstand the key characteristics and differences between Cloud ML, Edge ML, Mobile ML, and Tiny ML systems.\nAnalyze the benefits and challenges associated with each ML paradigm.\nExplore real-world applications and use cases for Cloud ML, Edge ML, Mobile ML, and Tiny ML.\nCompare the performance aspects of each ML approach, including latency, privacy, and resource utilization.\nExamine the evolving landscape of ML systems and potential future developments.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>ML Systems</span>"
    ]
  },
  {
    "objectID": "contents/core/ml_systems/ml_systems.html#overview",
    "href": "contents/core/ml_systems/ml_systems.html#overview",
    "title": "2  ML Systems",
    "section": "2.1 Overview",
    "text": "2.1 Overview\nModern machine learning systems span a spectrum of deployment options, each with its own set of characteristics and use cases. At one end, we have cloud-based ML, which leverages powerful centralized computing resources for complex, data-intensive tasks. Moving along the spectrum, we encounter edge ML, which brings computation closer to the data source for reduced latency and improved privacy. Mobile ML further extends these capabilities to smartphones and tablets, while at the far end, we find Tiny ML, which enables machine learning on extremely low-power devices with severe memory and processing constraints.\nThis spectrum of deployment can be visualized like Earth’s geological features, each operating at different scales in our computational landscape. Cloud ML systems operate like continents, processing vast amounts of data across interconnected centers; Edge ML exists where these continental powers meet the sea, creating dynamic coastlines where computation flows into local waters; Mobile ML moves through these waters like ocean currents, carrying computing power across the digital seas; and where these currents meet the physical world, TinyML systems rise like islands, each a precise point of intelligence in the vast computational ocean.\nFigure 2.1 illustrates the spectrum of distributed intelligence across these approaches, providing a visual comparison of their characteristics. We will examine the unique characteristics, advantages, and challenges of each approach, as depicted in the figure. Additionally, we will discuss the emerging trends and technologies that are shaping the future of machine learning deployment, considering how they might influence the balance between these three paradigms.\n\n\n\n\n\n\nFigure 2.1: Cloud vs. Edge vs. Mobile vs. Tiny ML: The Spectrum of Distributed Intelligence. Source: ABI Research – Tiny ML.\n\n\n\nTo better understand the dramatic differences between these ML deployment options, Table 2.1 provides examples of representative hardware platforms for each category. These examples illustrate the vast range of computational resources, power requirements, and cost considerations across the ML systems spectrum. As we explore each paradigm in detail, you can refer back to these concrete examples to better understand the practical implications of each approach.\n\n\n\nTable 2.1: Representative hardware platforms across the ML systems spectrum, showing typical specifications and capabilities for each category.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCategory\nExample Device\nProcessor\nMemory\nStorage\nPower\nPrice Range\nExample Models/Tasks\n\n\n\n\nCloud ML\nNVIDIA DGX A100\n8x NVIDIA A100 GPUs (40GB/80GB)\n1TB System RAM\n15TB NVMe SSD\n6.5kW\n$200K+\nLarge language models (GPT-3), real-time video processing\n\n\n\nGoogle TPU v4 Pod\n4096 TPU v4 chips\n128TB+\nNetworked storage\n~MW\nPay-per-use\nTraining foundation models, large-scale ML research\n\n\nEdge ML\nNVIDIA Jetson AGX Orin\n12-core Arm® Cortex®-A78AE, NVIDIA Ampere GPU\n32GB LPDDR5\n64GB eMMC\n15-60W\n$899\nComputer vision, robotics, autonomous systems\n\n\n\nIntel NUC 12 Pro\nIntel Core i7-1260P, Intel Iris Xe\n32GB DDR4\n1TB SSD\n28W\n$750\nEdge AI servers, industrial automation\n\n\nMobile ML\niPhone 15 Pro\nA17 Pro (6-core CPU, 6-core GPU)\n8GB RAM\n128GB-1TB\n3-5W\n$999+\nFace ID, computational photography, voice recognition\n\n\nTiny ML\nArduino Nano 33 BLE Sense\nArm Cortex-M4 @ 64MHz\n256KB RAM\n1MB Flash\n0.02-0.04W\n$35\nGesture recognition, voice detection\n\n\n\nESP32-CAM\nDual-core @ 240MHz\n520KB RAM\n4MB Flash\n0.05-0.25W\n$10\nImage classification, motion detection\n\n\n\n\n\n\nThe evolution of machine learning systems can be seen as a progression from centralized to increasingly distributed and specialized computing paradigms:\nCloud ML: Initially, ML was predominantly cloud-based. Powerful, scalable servers in data centers are used to train and run large ML models. This approach leverages vast computational resources and storage capacities, enabling the development of complex models trained on massive datasets. Cloud ML excels at tasks requiring extensive processing power, distributed training of large models, and is ideal for applications where real-time responsiveness isn’t critical. Popular platforms like AWS SageMaker, Google Cloud AI, and Azure ML offer flexible, scalable solutions for model development, training, and deployment. Cloud ML can handle models with billions of parameters, training on petabytes of data, but may incur latencies of 100-500 ms for online inference due to network delays.\nEdge ML: As the need for real-time, low-latency processing grew, Edge ML emerged. This paradigm brings inference capabilities closer to the data source, typically on edge devices such as industrial gateways, smart cameras, autonomous vehicles, or IoT hubs. Edge ML reduces latency (often to less than 50ms), enhances privacy by keeping data local, and can operate with intermittent cloud connectivity. It’s particularly useful for applications requiring quick responses or handling sensitive data in industrial or enterprise settings. Frameworks like NVIDIA Jetson or Google’s Edge TPU enable powerful ML capabilities on edge devices. Edge ML plays a crucial role in IoT ecosystems, enabling real-time decision making and reducing bandwidth usage by processing data locally.\nMobile ML: Building on edge computing concepts, Mobile ML focuses on leveraging the computational capabilities of smartphones and tablets. This approach enables personalized, responsive applications while reducing reliance on constant network connectivity. Mobile ML offers a balance between the power of edge computing and the ubiquity of personal devices. It utilizes on-device sensors (e.g., cameras, GPS, accelerometers) for unique ML applications. Frameworks like TensorFlow Lite and Core ML allow developers to deploy optimized models on mobile devices, with inference times often under 30ms for common tasks. Mobile ML enhances privacy by keeping personal data on the device and can operate offline, but must balance model performance with device resource constraints (typically 4-8 GB RAM, 100-200 GB storage).\nTiny ML: The latest development in this progression is Tiny ML, which enables ML models to run on extremely resource-constrained microcontrollers and small embedded systems. Tiny ML allows for on-device inference without relying on connectivity to the cloud, edge, or even the processing power of mobile devices. This approach is crucial for applications where size, power consumption, and cost are critical factors. Tiny ML devices typically operate with less than 1 MB of RAM and flash memory, consuming only milliwatts of power, enabling battery life of months or years. Applications include wake word detection, gesture recognition, and predictive maintenance in industrial settings. Platforms like Arduino Nano 33 BLE Sense and STM32 microcontrollers, coupled with frameworks like TensorFlow Lite for Microcontrollers, enable ML on these tiny devices. However, Tiny ML requires significant model optimization and quantization to fit within these constraints.\nEach of these paradigms has its own strengths and is suited to different use cases:\n\nCloud ML remains essential for tasks requiring massive computational power or large-scale data analysis.\nEdge ML is ideal for applications needing low-latency responses or local data processing in industrial or enterprise environments.\nMobile ML is suited for personalized, responsive applications on smartphones and tablets.\nTiny ML enables AI capabilities in small, power-efficient devices, expanding the reach of ML to new domains.\n\nThis progression reflects a broader trend in computing towards more distributed, localized, and specialized processing. The evolution is driven by the need for faster response times, improved privacy, reduced bandwidth usage, and the ability to operate in environments with limited or no connectivity, while also catering to the specific capabilities and constraints of different types of devices.\nFigure 2.2 illustrates the key differences between Cloud ML, Edge ML, Mobile ML, and Tiny ML in terms of hardware, latency, connectivity, power requirements, and model complexity. As we move from Cloud to Edge to Tiny ML, we see a dramatic reduction in available resources, which presents significant challenges for deploying sophisticated machine learning models. This resource disparity becomes particularly apparent when attempting to deploy deep learning models on microcontrollers, the primary hardware platform for Tiny ML. These tiny devices have severely constrained memory and storage capacities, which are often insufficient for conventional deep learning models. We will learn to put these things into perspective in this chapter.\n\n\n\n\n\n\nFigure 2.2: From cloud GPUs to microcontrollers: Navigating the memory and storage landscape across computing devices. Source: (Lin et al. 2023)\n\n\nLin, Ji, Ligeng Zhu, Wei-Ming Chen, Wei-Chen Wang, and Song Han. 2023. “Tiny Machine Learning: Progress and Futures [Feature].” IEEE Circuits and Systems Magazine 23 (3): 8–34. https://doi.org/10.1109/mcas.2023.3302182.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>ML Systems</span>"
    ]
  },
  {
    "objectID": "contents/core/ml_systems/ml_systems.html#cloud-ml",
    "href": "contents/core/ml_systems/ml_systems.html#cloud-ml",
    "title": "2  ML Systems",
    "section": "2.2 Cloud ML",
    "text": "2.2 Cloud ML\nThe vast computational demands of modern machine learning often require the scalability and power of centralized cloud infrastructures. Cloud Machine Learning (Cloud ML) handles tasks such as large-scale data processing, collaborative model development, and advanced analytics. Cloud data centers leverage distributed architectures, offering specialized resources to train complex models and support diverse applications, from recommendation systems to natural language processing.\n\n\n\n\n\n\nDefinition of Cloud ML\n\n\n\nCloud Machine Learning (Cloud ML) refers to the deployment of machine learning models on centralized computing infrastructures, such as data centers. These systems operate in the kilowatt to megawatt power range and utilize specialized computing systems to handle large-scale datasets and train complex models. Cloud ML offers scalability and computational capacity, making it well-suited for tasks requiring extensive resources and collaboration. However, it depends on consistent connectivity and may introduce latency for real-time applications.\n\n\nFigure 2.3 provides an overview of Cloud ML’s capabilities, which we will discuss in greater detail throughout this section.\n\n\n\n\n\n\nFigure 2.3: Section overview for Cloud ML.\n\n\n\n\n2.2.1 Characteristics\n\nCentralized Infrastructure\nOne of the key characteristics of Cloud ML is its centralized infrastructure. Figure 2.4 illustrates this concept with an example from Google’s Cloud TPU data center. Cloud service providers offer a virtual platform that consists of high-capacity servers, expansive storage solutions, and robust networking architectures, all housed in data centers distributed across the globe. As shown in the figure, these centralized facilities can be massive in scale, housing rows upon rows of specialized hardware. This centralized setup allows for the pooling and efficient management of computational resources, making it easier to scale machine learning projects as needed.\n\n\n\n\n\n\nFigure 2.4: Cloud TPU data center at Google. Source: Google.\n\n\n\n\n\nScalable Data Processing and Model Training\nCloud ML excels in its ability to process and analyze massive volumes of data. The centralized infrastructure is designed to handle complex computations and model training tasks that require significant computational power. By leveraging the scalability of the cloud, machine learning models can be trained on vast amounts of data, leading to improved learning capabilities and predictive performance.\n\n\nFlexible Deployment and Accessibility\nAnother advantage of Cloud ML is the flexibility it offers in terms of deployment and accessibility. Once a machine learning model is trained and validated, it can be easily deployed and made accessible to users through cloud-based services. This allows for seamless integration of machine learning capabilities into various applications and services, regardless of the user’s location or device.\n\n\nCollaboration and Resource Sharing\nCloud ML promotes collaboration and resource sharing among teams and organizations. The centralized nature of the cloud infrastructure enables multiple users to access and work on the same machine learning projects simultaneously. This collaborative approach facilitates knowledge sharing, accelerates the development process, and optimizes resource utilization.\n\n\nCost-Effectiveness and Scalability\nBy leveraging the pay-as-you-go pricing model offered by cloud service providers, Cloud ML allows organizations to avoid the upfront costs associated with building and maintaining their own machine learning infrastructure. The ability to scale resources up or down based on demand ensures cost-effectiveness and flexibility in managing machine learning projects.\nCloud ML has revolutionized the way machine learning is approached, making it more accessible, scalable, and efficient. It has opened up new possibilities for organizations to harness the power of machine learning without the need for significant investments in hardware and infrastructure.\n\n\n\n2.2.2 Benefits\nCloud ML offers several significant benefits that make it a powerful choice for machine learning projects:\n\nImmense Computational Power\nOne of the key advantages of Cloud ML is its ability to provide vast computational resources. The cloud infrastructure is designed to handle complex algorithms and process large datasets efficiently. This is particularly beneficial for machine learning models that require significant computational power, such as deep learning networks or models trained on massive datasets. By leveraging the cloud’s computational capabilities, organizations can overcome the limitations of local hardware setups and scale their machine learning projects to meet demanding requirements.\n\n\nDynamic Scalability\nCloud ML offers dynamic scalability, allowing organizations to easily adapt to changing computational needs. As the volume of data grows or the complexity of machine learning models increases, the cloud infrastructure can seamlessly scale up or down to accommodate these changes. This flexibility ensures consistent performance and enables organizations to handle varying workloads without the need for extensive hardware investments. With Cloud ML, resources can be allocated on-demand, providing a cost-effective and efficient solution for managing machine learning projects.\n\n\nAccess to Advanced Tools and Algorithms\nCloud ML platforms provide access to a wide range of advanced tools and algorithms specifically designed for machine learning. These tools often include pre-built libraries, frameworks, and APIs that simplify the development and deployment of machine learning models. Developers can leverage these resources to accelerate the building, training, and optimization of sophisticated models. By utilizing the latest advancements in machine learning algorithms and techniques, organizations can stay at the forefront of innovation and achieve better results in their machine learning projects.\n\n\nCollaborative Environment\nCloud ML fosters a collaborative environment that enables teams to work together seamlessly. The centralized nature of the cloud infrastructure allows multiple users to access and contribute to the same machine learning projects simultaneously. This collaborative approach facilitates knowledge sharing, promotes cross-functional collaboration, and accelerates the development and iteration of machine learning models. Teams can easily share code, datasets, and results, enabling efficient collaboration and driving innovation across the organization.\n\n\nCost-Effectiveness\nAdopting Cloud ML can be a cost-effective solution for organizations, especially compared to building and maintaining an on-premises machine learning infrastructure. Cloud service providers offer flexible pricing models, such as pay-as-you-go or subscription-based plans, allowing organizations to pay only for the resources they consume. This eliminates the need for upfront capital investments in hardware and infrastructure, reducing the overall cost of implementing machine learning projects. Additionally, the scalability of Cloud ML ensures that organizations can optimize their resource usage and avoid over provisioning, further enhancing cost-efficiency.\nThe benefits of Cloud ML, including its immense computational power, dynamic scalability, access to advanced tools and algorithms, collaborative environment, and cost-effectiveness, make it a compelling choice for organizations looking to harness the potential of machine learning. By leveraging the capabilities of the cloud, organizations can accelerate their machine learning initiatives, drive innovation, and gain a competitive edge in today’s data-driven landscape.\n\n\n\n2.2.3 Challenges\nWhile Cloud ML offers numerous benefits, it also comes with certain challenges that organizations need to consider:\n\nLatency Issues\nOne of the main challenges of Cloud ML is the potential for latency issues, especially in applications that require real-time responses. Since data needs to be sent from the data source to centralized cloud servers for processing and then back to the application, there can be delays introduced by network transmission. This latency can be a significant drawback in time-sensitive scenarios, such as autonomous vehicles, real-time fraud detection, or industrial control systems, where immediate decision-making is critical. Developers need to carefully design their systems to minimize latency and ensure acceptable response times.\n\n\nData Privacy and Security Concerns\nCentralizing data processing and storage in the cloud can raise concerns about data privacy and security. When sensitive data is transmitted and stored in remote data centers, it becomes vulnerable to potential cyber-attacks and unauthorized access. Cloud data centers can become attractive targets for hackers seeking to exploit vulnerabilities and gain access to valuable information. Organizations need to invest in robust security measures, such as encryption, access controls, and continuous monitoring, to protect their data in the cloud. Compliance with data privacy regulations, such as GDPR or HIPAA, also becomes a critical consideration when handling sensitive data in the cloud.\n\n\nCost Considerations\nAs data processing needs grow, the costs associated with using cloud services can escalate. While Cloud ML offers scalability and flexibility, organizations dealing with large data volumes may face increasing costs as they consume more cloud resources. The pay-as-you-go pricing model of cloud services means that costs can quickly add up, especially for compute-intensive tasks like model training and inference. Organizations need to carefully monitor and optimize their cloud usage to ensure cost-effectiveness. They may need to consider strategies such as data compression, efficient algorithm design, and resource allocation optimization to minimize costs while still achieving desired performance.\n\n\nDependency on Internet Connectivity\nCloud ML relies on stable and reliable internet connectivity to function effectively. Since data needs to be transmitted to and from the cloud, any disruptions or limitations in network connectivity can impact the performance and availability of the machine learning system. This dependency on internet connectivity can be a challenge in scenarios where network access is limited, unreliable, or expensive. Organizations need to ensure robust network infrastructure and consider failover mechanisms or offline capabilities to mitigate the impact of connectivity issues.\n\n\nVendor Lock-In\nWhen adopting Cloud ML, organizations often become dependent on the specific tools, APIs, and services provided by their chosen cloud vendor. This vendor lock-in can make it difficult to switch providers or migrate to different platforms in the future. Organizations may face challenges in terms of portability, interoperability, and cost when considering a change in their cloud ML provider. It is important to carefully evaluate vendor offerings, consider long-term strategic goals, and plan for potential migration scenarios to minimize the risks associated with vendor lock-in.\nAddressing these challenges requires careful planning, architectural design, and risk mitigation strategies. Organizations need to weigh the benefits of Cloud ML against the potential challenges and make informed decisions based on their specific requirements, data sensitivity, and business objectives. By proactively addressing these challenges, organizations can effectively leverage the power of Cloud ML while ensuring data privacy, security, cost-effectiveness, and overall system reliability.\n\n\n\n2.2.4 Example Use Cases\nCloud ML has found widespread adoption across various domains, revolutionizing the way businesses operate and users interact with technology. Let’s explore some notable examples of Cloud ML in action:\n\nVirtual Assistants\nCloud ML plays a crucial role in powering virtual assistants like Siri and Alexa. These systems leverage the immense computational capabilities of the cloud to process and analyze voice inputs in real-time. By harnessing the power of natural language processing and machine learning algorithms, virtual assistants can understand user queries, extract relevant information, and generate intelligent and personalized responses. The cloud’s scalability and processing power enable these assistants to handle a vast number of user interactions simultaneously, providing a seamless and responsive user experience.\n\n\nRecommendation Systems\nCloud ML forms the backbone of advanced recommendation systems used by platforms like Netflix and Amazon. These systems use the cloud’s ability to process and analyze massive datasets to uncover patterns, preferences, and user behavior. By leveraging collaborative filtering and other machine learning techniques, recommendation systems can offer personalized content or product suggestions tailored to each user’s interests. The cloud’s scalability allows these systems to continuously update and refine their recommendations based on the ever-growing amount of user data, enhancing user engagement and satisfaction.\n\n\nFraud Detection\nIn the financial industry, Cloud ML has revolutionized fraud detection systems. By leveraging the cloud’s computational power, these systems can analyze vast amounts of transactional data in real-time to identify potential fraudulent activities. Machine learning algorithms trained on historical fraud patterns can detect anomalies and suspicious behavior, enabling financial institutions to take proactive measures to prevent fraud and minimize financial losses. The cloud’s ability to process and store large volumes of data makes it an ideal platform for implementing robust and scalable fraud detection systems.\n\n\nPersonalized User Experiences\nCloud ML is deeply integrated into our online experiences, shaping the way we interact with digital platforms. From personalized ads on social media feeds to predictive text features in email services, Cloud ML powers smart algorithms that enhance user engagement and convenience. It enables e-commerce sites to recommend products based on a user’s browsing and purchase history, fine-tunes search engines to deliver accurate and relevant results, and automates the tagging and categorization of photos on platforms like Facebook. By leveraging the cloud’s computational resources, these systems can continuously learn and adapt to user preferences, providing a more intuitive and personalized user experience.\n\n\nSecurity and Anomaly Detection\nCloud ML plays a role in bolstering user security by powering anomaly detection systems. These systems continuously monitor user activities and system logs to identify unusual patterns or suspicious behavior. By analyzing vast amounts of data in real-time, Cloud ML algorithms can detect potential cyber threats, such as unauthorized access attempts, malware infections, or data breaches. The cloud’s scalability and processing power enable these systems to handle the increasing complexity and volume of security data, providing a proactive approach to protecting users and systems from potential threats.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>ML Systems</span>"
    ]
  },
  {
    "objectID": "contents/core/ml_systems/ml_systems.html#edge-ml",
    "href": "contents/core/ml_systems/ml_systems.html#edge-ml",
    "title": "2  ML Systems",
    "section": "2.3 Edge ML",
    "text": "2.3 Edge ML\nAs machine learning applications grow, so does the need for faster, localized decision-making. Edge Machine Learning (Edge ML) shifts computation away from centralized servers, processing data closer to its source. This paradigm is critical for time-sensitive applications, such as autonomous systems, industrial IoT, and smart infrastructure, where minimizing latency and preserving data privacy are paramount. Edge devices, like gateways and IoT hubs, enable these systems to function efficiently while reducing dependence on cloud infrastructures.\n\n\n\n\n\n\nDefinition of Edge ML\n\n\n\nEdge Machine Learning (Edge ML) describes the deployment of machine learning models at or near the edge of the network1. These systems operate in the tens to hundreds of watts range and rely on localized hardware optimized for real-time processing. Edge ML minimizes latency and enhances privacy by processing data locally, but its primary limitation lies in restricted computational resources.\n\n\n1 The “edge of the network” refers to devices or systems positioned between centralized cloud infrastructures and end-user devices, such as gateways, IoT hubs, or industrial sensors.Figure 2.5 provides an overview of this section.\n\n\n\n\n\n\nFigure 2.5: Section overview for Edge ML.\n\n\n\n\n2.3.1 Characteristics\n\nDecentralized Data Processing\nIn Edge ML, data processing happens in a decentralized fashion, as illustrated in Figure 2.6. Instead of sending data to remote servers, the data is processed locally on devices like smartphones, tablets, or Internet of Things (IoT) devices. The figure showcases various examples of these edge devices, including wearables, industrial sensors, and smart home appliances. This local processing allows devices to make quick decisions based on the data they collect without relying heavily on a central server’s resources.\n\n\n\n\n\n\nFigure 2.6: Edge ML Examples. Source: Edge Impulse.\n\n\n\n\n\nLocal Data Storage and Computation\nLocal data storage and computation are key features of Edge ML. This setup ensures that data can be stored and analyzed directly on the devices, thereby maintaining the privacy of the data and reducing the need for constant internet connectivity. Moreover, this often leads to more efficient computation, as data doesn’t have to travel long distances, and computations are performed with a more nuanced understanding of the local context, which can sometimes result in more insightful analyses.\n\n\n\n2.3.2 Benefits\n\nReduced Latency\nOne of Edge ML’s main advantages is the significant latency reduction compared to Cloud ML. This reduced latency can be a critical benefit in situations where milliseconds count, such as in autonomous vehicles, where quick decision-making can mean the difference between safety and an accident.\n\n\nEnhanced Data Privacy\nEdge ML also offers improved data privacy, as data is primarily stored and processed locally. This minimizes the risk of data breaches that are more common in centralized data storage solutions. Sensitive information can be kept more secure, as it’s not sent over networks that could be intercepted.\n\n\nLower Bandwidth Usage\nOperating closer to the data source means less data must be sent over networks, reducing bandwidth usage. This can result in cost savings and efficiency gains, especially in environments where bandwidth is limited or costly.\n\n\n\n2.3.3 Challenges\n\nLimited Computational Resources Compared to Cloud ML\nHowever, Edge ML has its challenges. One of the main concerns is the limited computational resources compared to cloud-based solutions. Endpoint devices may have a different processing power or storage capacity than cloud servers, limiting the complexity of the machine learning models that can be deployed.\n\n\nComplexity in Managing Edge Nodes\nManaging a network of edge nodes can introduce complexity, especially regarding coordination, updates, and maintenance. Ensuring all nodes operate seamlessly and are up-to-date with the latest algorithms and security protocols can be a logistical challenge.\n\n\nSecurity Concerns at the Edge Nodes\nWhile Edge ML offers enhanced data privacy, edge nodes can sometimes be more vulnerable to physical and cyber-attacks. Developing robust security protocols that protect data at each node without compromising the system’s efficiency remains a significant challenge in deploying Edge ML solutions.\n\n\n\n2.3.4 Example Use Cases\nEdge ML has many applications, from autonomous vehicles and smart homes to industrial Internet of Things (IoT). These examples were chosen to highlight scenarios where real-time data processing, reduced latency, and enhanced privacy are not just beneficial but often critical to the operation and success of these technologies. They demonstrate the role that Edge ML can play in driving advancements in various sectors, fostering innovation, and paving the way for more intelligent, responsive, and adaptive systems.\n\nAutonomous Vehicles\nAutonomous vehicles stand as a prime example of Edge ML’s potential. These vehicles rely heavily on real-time data processing to navigate and make decisions. Localized machine learning models assist in quickly analyzing data from various sensors to make immediate driving decisions, ensuring safety and smooth operation.\n\n\nSmart Homes and Buildings\nEdge ML plays a crucial role in efficiently managing various systems in smart homes and buildings, from lighting and heating to security. By processing data locally, these systems can operate more responsively and harmoniously with the occupants’ habits and preferences, creating a more comfortable living environment.\n\n\nIndustrial IoT\nThe Industrial IoT leverages Edge ML to monitor and control complex industrial processes. Here, machine learning models can analyze data from numerous sensors in real-time, enabling predictive maintenance, optimizing operations, and enhancing safety measures. This revolution in industrial automation and efficiency is transforming manufacturing and production across various sectors.\nThe applicability of Edge ML is vast and not limited to these examples. Various other sectors, including healthcare, agriculture, and urban planning, are exploring and integrating Edge ML to develop innovative solutions responsive to real-world needs and challenges, heralding a new era of smart, interconnected systems.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>ML Systems</span>"
    ]
  },
  {
    "objectID": "contents/core/ml_systems/ml_systems.html#mobile-ml",
    "href": "contents/core/ml_systems/ml_systems.html#mobile-ml",
    "title": "2  ML Systems",
    "section": "2.4 Mobile ML",
    "text": "2.4 Mobile ML\nMachine learning is increasingly being integrated into portable devices like smartphones and tablets, empowering users with real-time, personalized capabilities. Mobile Machine Learning (Mobile ML) supports applications like voice recognition, computational photography, and health monitoring, all while maintaining data privacy through on-device computation. These battery-powered devices are optimized for responsiveness and can operate offline, making them indispensable in everyday consumer technologies.\n\n\n\n\n\n\nDefinition of Mobile ML\n\n\n\nMobile Machine Learning (Mobile ML) enables machine learning models to run directly on portable, battery-powered devices like smartphones and tablets. Operating within the single-digit to tens of watts range, Mobile ML leverages on-device computation to provide personalized and responsive applications. This paradigm preserves privacy and ensures offline functionality, though it must balance performance with battery and storage limitations.\n\n\n\n2.4.1 Characteristics\n\nOn-Device Processing\nMobile ML utilizes the processing power of mobile devices’ System-on-Chip (SoC) architectures, including specialized Neural Processing Units (NPUs) and AI accelerators. This enables efficient execution of ML models directly on the device, allowing for real-time processing of data from device sensors like cameras, microphones, and motion sensors without constant cloud connectivity.\n\n\nOptimized Frameworks\nMobile ML is supported by specialized frameworks and tools designed specifically for mobile deployment, such as TensorFlow Lite for Android devices and Core ML for iOS devices. These frameworks are optimized for mobile hardware and provide efficient model compression and quantization techniques to ensure smooth performance within mobile resource constraints.\n\n\n\n2.4.2 Benefits\n\nReal-Time Processing\nMobile ML enables real-time processing of data directly on mobile devices, eliminating the need for constant server communication. This results in faster response times for applications requiring immediate feedback, such as real-time translation, face detection, or gesture recognition.\n\n\nPrivacy Preservation\nBy processing data locally on the device, Mobile ML helps maintain user privacy. Sensitive information doesn’t need to leave the device, reducing the risk of data breaches and addressing privacy concerns, particularly important for applications handling personal data.\n\n\nOffline Functionality\nMobile ML applications can function without constant internet connectivity, making them reliable in areas with poor network coverage or when users are offline. This ensures consistent performance and user experience regardless of network conditions.\n\n\n\n2.4.3 Challenges\n\nResource Constraints\nDespite modern mobile devices being powerful, they still face resource constraints compared to cloud servers. Mobile ML must operate within limited RAM, storage, and processing power, requiring careful optimization of models and efficient resource management.\n\n\nBattery Life Impact\nML operations can be computationally intensive, potentially impacting device battery life. Developers must balance model complexity and performance with power consumption to ensure reasonable battery life for users.\n\n\nModel Size Limitations\nMobile devices have limited storage space, necessitating careful consideration of model size. This often requires model compression and quantization techniques, which can affect model accuracy and performance.\n\n\n\n2.4.4 Example Use Cases\n\nComputer Vision Applications\nMobile ML has revolutionized how we use cameras on mobile devices, enabling sophisticated computer vision applications that process visual data in real-time. Modern smartphone cameras now incorporate ML models that can detect faces, analyze scenes, and apply complex filters instantaneously. These models work directly on the camera feed to enable features like portrait mode photography, where ML algorithms separate foreground subjects from backgrounds. Document scanning applications use ML to detect paper edges, correct perspective, and enhance text readability, while augmented reality applications use ML-powered object detection to accurately place virtual objects in the real world.\n\n\nNatural Language Processing\nNatural language processing on mobile devices has transformed how we interact with our phones and communicate with others. Speech recognition models run directly on device, enabling voice assistants to respond quickly to commands even without internet connectivity. Real-time translation applications can now translate conversations and text without sending data to the cloud, preserving privacy and working reliably regardless of network conditions. Mobile keyboards have become increasingly intelligent, using ML to predict not just the next word but entire phrases based on the user’s writing style and context, while maintaining all learning and personalization locally on the device.\n\n\nHealth and Fitness Monitoring\nMobile ML has enabled smartphones and tablets to become sophisticated health monitoring devices. Through clever use of existing sensors combined with ML models, mobile devices can now track physical activity, analyze sleep patterns, and monitor vital signs. For example, cameras can measure heart rate by detecting subtle color changes in the user’s skin, while accelerometers and ML models work together to recognize specific exercises and analyze workout form. These applications process sensitive health data directly on the device, ensuring privacy while providing users with real-time feedback and personalized health insights.\n\n\nPersonalization and User Experience\nPerhaps the most pervasive but least visible application of Mobile ML lies in how it personalizes and enhances the overall user experience. ML models continuously analyze how users interact with their devices to optimize everything from battery usage to interface layouts. These models learn individual usage patterns to predict which apps users are likely to open next, preload content they might want to see, and adjust system settings like screen brightness and audio levels based on environmental conditions and user preferences. This creates a deeply personalized experience that adapts to each user’s needs while maintaining privacy by keeping all learning and adaptation on the device itself.\nThese applications demonstrate how Mobile ML bridges the gap between cloud-based solutions and edge computing, providing efficient, privacy-conscious, and user-friendly machine learning capabilities on personal mobile devices. The continuous advancement in mobile hardware capabilities and optimization techniques continues to expand the possibilities for Mobile ML applications.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>ML Systems</span>"
    ]
  },
  {
    "objectID": "contents/core/ml_systems/ml_systems.html#tiny-ml",
    "href": "contents/core/ml_systems/ml_systems.html#tiny-ml",
    "title": "2  ML Systems",
    "section": "2.5 Tiny ML",
    "text": "2.5 Tiny ML\nTiny Machine Learning (Tiny ML) brings intelligence to the smallest devices, from microcontrollers to embedded sensors, enabling real-time computation in resource-constrained environments. These systems power applications such as predictive maintenance, environmental monitoring, and simple gesture recognition. Tiny ML devices are optimized for energy efficiency, often running for months or years on limited power sources, such as coin-cell batteries, while delivering actionable insights in remote or disconnected environments.\n\n\n\n\n\n\nDefinition of Tiny ML\n\n\n\nTiny Machine Learning (Tiny ML) refers to the execution of machine learning models on ultra-constrained devices, such as microcontrollers and sensors. These devices operate in the milliwatt to sub-watt power range, prioritizing energy efficiency and compactness. Tiny ML enables localized decision-making in resource-constrained environments, excelling in applications where extended operation on limited power sources is required. However, it is limited by severely restricted computational resources.\n\n\nFigure 2.7 encapsulates the key aspects of Tiny ML discussed in this section.\n\n\n\n\n\n\nFigure 2.7: Section overview for Tiny ML.\n\n\n\n\n2.5.1 Characteristics\n\nOn-Device Machine Learning\nIn Tiny ML, the focus, much like in Mobile ML, is on on-device machine learning. This means that machine learning models are deployed and trained on the device, eliminating the need for external servers or cloud infrastructures. This allows Tiny ML to enable intelligent decision-making right where the data is generated, making real-time insights and actions possible, even in settings where connectivity is limited or unavailable.\n\n\nLow Power and Resource-Constrained Environments\nTiny ML excels in low-power and resource-constrained settings. These environments require highly optimized solutions that function within the available resources. Figure 2.8 showcases an example Tiny ML device kit, illustrating the compact nature of these systems. These devices can typically fit in the palm of your hand or, in some cases, are even as small as a fingernail. Tiny ML meets the need for efficiency through specialized algorithms and models designed to deliver decent performance while consuming minimal energy, thus ensuring extended operational periods, even in battery-powered devices like those shown.\n\n\n\n\n\n\nFigure 2.8: Examples of Tiny ML device kits. Source: Widening Access to Applied Machine Learning with Tiny ML.\n\n\n\n\n\n\n\n\n\nExercise 2.1: Tiny ML with Arduino\n\n\n\n\n\nGet ready to bring machine learning to the smallest of devices! In the embedded machine learning world, Tiny ML is where resource constraints meet ingenuity. This Colab notebook will walk you through building a gesture recognition model designed on an Arduino board. You’ll learn how to train a small but effective neural network, optimize it for minimal memory usage, and deploy it to your microcontroller. If you’re excited about making everyday objects smarter, this is where it begins!\n\n\n\n\n\n\n\n2.5.2 Benefits\n\nExtremely Low Latency\nOne of the standout benefits of Tiny ML is its ability to offer ultra-low latency. Since computation occurs directly on the device, the time required to send data to external servers and receive a response is eliminated. This is crucial in applications requiring immediate decision-making, enabling quick responses to changing conditions.\n\n\nHigh Data Security\nTiny ML inherently enhances data security. Because data processing and analysis happen on the device, the risk of data interception during transmission is virtually eliminated. This localized approach to data management ensures that sensitive information stays on the device, strengthening user data security.\n\n\nEnergy Efficiency\nTiny ML operates within an energy-efficient framework, a necessity given its resource-constrained environments. By employing lean algorithms and optimized computational methods, Tiny ML ensures that devices can execute complex tasks without rapidly depleting battery life, making it a sustainable option for long-term deployments.\n\n\n\n2.5.3 Challenges\n\nLimited Computational Capabilities\nHowever, the shift to Tiny ML comes with its set of hurdles. The primary limitation is the devices’ constrained computational capabilities. The need to operate within such limits means that deployed models must be simplified, which could affect the accuracy and sophistication of the solutions.\n\n\nComplex Development Cycle\nTiny ML also introduces a complicated development cycle. Crafting light­weight and effective models demands a deep understanding of machine learning principles and expertise in embedded systems. This complexity calls for a collaborative development approach, where multi-domain expertise is essential for success.\n\n\nModel Optimization and Compression\nA central challenge in Tiny ML is model optimization and compression. Creating machine learning models that can operate effectively within the limited memory and computational power of microcontrollers requires innovative approaches to model design. Developers often face the challenge of striking a delicate balance and optimizing models to maintain effectiveness while fitting within stringent resource constraints.\n\n\n\n2.5.4 Example Use Cases\n\nWearable Devices\nIn wearables, Tiny ML opens the door to smarter, more responsive gadgets. From fitness trackers offering real-time workout feedback to smart glasses processing visual data on the fly, Tiny ML transforms how we engage with wearable tech, delivering personalized experiences directly from the device.\n\n\nPredictive Maintenance\nIn industrial settings, Tiny ML plays a significant role in predictive maintenance. By deploying Tiny ML algorithms on sensors that monitor equipment health, companies can preemptively identify potential issues, reducing downtime and preventing costly breakdowns. On-site data analysis ensures quick responses, potentially stopping minor issues from becoming major problems.\n\n\nAnomaly Detection\nTiny ML can be employed to create anomaly detection models that identify unusual data patterns. For instance, a smart factory could use Tiny ML to monitor industrial processes and spot anomalies, helping prevent accidents and improve product quality. Similarly, a security company could use Tiny ML to monitor network traffic for unusual patterns, aiding in detecting and preventing cyber-attacks. Tiny ML could monitor patient data for anomalies in healthcare, aiding early disease detection and better patient treatment.\n\n\nEnvironmental Monitoring\nIn environmental monitoring, Tiny ML enables real-time data analysis from various field-deployed sensors. These could range from city air quality monitoring to wildlife tracking in protected areas. Through Tiny ML, data can be processed locally, allowing for quick responses to changing conditions and providing a nuanced understanding of environmental patterns, crucial for informed decision-making.\nIn summary, Tiny ML serves as a trailblazer in the evolution of machine learning, fostering innovation across various fields by bringing intelligence directly to the edge. Its potential to transform our interaction with technology and the world is immense, promising a future where devices are connected, intelligent, and capable of making real-time decisions and responses.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>ML Systems</span>"
    ]
  },
  {
    "objectID": "contents/core/ml_systems/ml_systems.html#hybrid-ml",
    "href": "contents/core/ml_systems/ml_systems.html#hybrid-ml",
    "title": "2  ML Systems",
    "section": "2.6 Hybrid ML",
    "text": "2.6 Hybrid ML\nThe increasingly complex demands of modern applications often require a blend of machine learning approaches. Hybrid Machine Learning (Hybrid ML) combines the computational power of the cloud, the efficiency of edge and mobile devices, and the compact capabilities of Tiny ML. This approach enables architects to create systems that balance performance, privacy, and resource efficiency, addressing real-world challenges with innovative, distributed solutions.\n\n\n\n\n\n\nDefinition of Hybrid ML\n\n\n\nHybrid Machine Learning (Hybrid ML) refers to the integration of multiple ML paradigms—such as Cloud, Edge, Mobile, and Tiny ML—to form a unified, distributed system. These systems leverage the complementary strengths of each paradigm while addressing their individual limitations. Hybrid ML supports scalability, adaptability, and privacy-preserving capabilities, enabling sophisticated ML applications for diverse scenarios. By combining centralized and decentralized computing, Hybrid ML facilitates efficient resource utilization while meeting the demands of complex real-world requirements.\n\n\n\n2.6.1 Design Patterns\nDesign patterns in Hybrid ML represent reusable solutions to common challenges faced when integrating multiple ML paradigms (cloud, edge, mobile, and tiny). These patterns guide system architects in combining the strengths of different approaches—such as the computational power of the cloud and the efficiency of edge devices—while mitigating their individual limitations. By following these patterns, architects can address key trade-offs in performance, latency, privacy, and resource efficiency.\nHybrid ML design patterns serve as blueprints, enabling the creation of scalable, efficient, and adaptive systems tailored to diverse real-world applications. Each pattern reflects a specific strategy for organizing and deploying ML workloads across different tiers of a distributed system, ensuring optimal use of available resources while meeting application-specific requirements.\n\nTrain-Serve Split\nOne of the most common hybrid patterns is the train-serve split, where model training occurs in the cloud but inference happens on edge, mobile, or tiny devices. This pattern takes advantage of the cloud’s vast computational resources for the training phase while benefiting from the low latency and privacy advantages of on-device inference. For example, smart home devices often use models trained on large datasets in the cloud but run inference locally to ensure quick response times and protect user privacy. In practice, this might involve training models on powerful systems like the NVIDIA DGX A100, leveraging its 8 A100 GPUs and terabyte-scale memory, before deploying optimized versions to edge devices like the NVIDIA Jetson AGX Orin for efficient inference. Similarly, mobile vision models for computational photography are typically trained on powerful cloud infrastructure but deployed to run efficiently on phone hardware.\n\n\nHierarchical Processing\nHierarchical processing creates a multi-tier system where data and intelligence flow between different levels of the ML stack. In industrial IoT applications, tiny sensors might perform basic anomaly detection, edge devices aggregate and analyze data from multiple sensors, and cloud systems handle complex analytics and model updates. For instance, we might see ESP32-CAM devices performing basic image classification at the sensor level with their minimal 520 KB RAM, feeding data up to Jetson AGX Orin devices for more sophisticated computer vision tasks, and ultimately connecting to cloud infrastructure for complex analytics and model updates.\nThis hierarchy allows each tier to handle tasks appropriate to its ca­pa­bil­ities—Tiny ML devices handle immediate, simple decisions; edge devices manage local coordination; and cloud systems tackle complex analytics and learning tasks. Smart city installations often use this pattern, with street-level sensors feeding data to neighborhood-level edge processors, which in turn connect to city-wide cloud analytics.\n\n\nProgressive Deployment\nProgressive deployment strategies adapt models for different computational tiers, creating a cascade of increasingly lightweight versions. A model might start as a large, complex version in the cloud, then be progressively compressed and optimized for edge servers, mobile devices, and finally tiny sensors. Voice assistant systems often employ this pattern—full natural language processing runs in the cloud, while simplified wake-word detection runs on-device. This allows the system to balance capability and resource constraints across the ML stack.\n\n\nFederated Learning\nFederated learning represents a sophisticated hybrid approach where model training is distributed across many edge or mobile devices while maintaining privacy. Devices learn from local data and share model updates, rather than raw data, with cloud servers that aggregate these updates into an improved global model. This pattern is particularly powerful for applications like keyboard prediction on mobile devices or healthcare analytics, where privacy is paramount but benefits from collective learning are valuable. The cloud coordinates the learning process without directly accessing sensitive data, while devices benefit from the collective intelligence of the network.\n\n\nCollaborative Learning\nCollaborative learning enables peer-to-peer learning between devices at the same tier, often complementing hierarchical structures. Autonomous vehicle fleets, for example, might share learning about road conditions or traffic patterns directly between vehicles while also communicating with cloud infrastructure. This horizontal collaboration allows systems to share time-sensitive information and learn from each other’s experiences without always routing through central servers.\n\n\n\n2.6.2 Real-world Integration\nDesign patterns establish a foundation for organizing and optimizing ML workloads across distributed systems. However, the practical application of these patterns often requires combining multiple paradigms into integrated workflows. Thus, in practice, ML systems rarely operate in isolation. Instead, they form interconnected networks where each paradigm—Cloud, Edge, Mobile, and Tiny ML—plays a specific role while communicating with other parts of the system. These interconnected networks follow integration patterns that assign specific roles to Cloud, Edge, Mobile, and Tiny ML systems based on their unique strengths and limitations. Recall that cloud systems excel at training and analytics but require significant infrastructure. Edge systems provide local processing power and reduced latency. Mobile devices offer personal computing capabilities and user interaction. Tiny ML enables intelligence in the smallest devices and sensors.\nFigure 2.9 illustrates these key interactions through specific connection types: “Deploy” paths show how models flow from cloud training to various devices, “Data” and “Results” show information flow from sensors through processing stages, “Analyze” shows how processed information reaches cloud analytics, and “Sync” demonstrates device coordination. Notice how data generally flows upward from sensors through processing layers to cloud analytics, while model deployments flow downward from cloud training to various inference points. The interactions aren’t strictly hierarchical—mobile devices might communicate directly with both cloud services and tiny sensors, while edge systems can assist mobile devices with complex processing tasks.\n\n\n\n\n\n\nFigure 2.9: Example interaction patterns between ML paradigms, showing data flows, model deployment, and processing relationships across Cloud, Edge, Mobile, and Tiny ML systems.\n\n\n\nTo understand how these labeled interactions manifest in real applications, let’s explore several common scenarios using Figure 2.9:\n\nModel Deployment Scenario: A company develops a computer vision model for defect detection. Following the “Deploy” paths shown in Figure 2.9, the cloud-trained model is distributed to edge servers in factories, quality control tablets on the production floor, and tiny cameras embedded in the production line. This showcases how a single ML solution can be distributed across different computational tiers for optimal performance.\nData Flow and Analysis Scenario: In a smart agriculture system, soil sensors (Tiny ML) collect moisture and nutrient data, following the “Data” path to Tiny ML inference. The “Results” flow to edge processors in local stations, which process this information and use the “Analyze” path to send insights to the cloud for farm-wide analytics, while also sharing results with farmers’ mobile apps. This demonstrates the hierarchical flow shown in Figure 2.9 from sensors through processing to cloud analytics.\nEdge-Mobile Assistance Scenario: When a mobile app needs to perform complex image processing that exceeds the phone’s capabilities, it utilizes the “Assist” connection shown in Figure 2.9. The edge system helps process the heavier computational tasks, sending back results to enhance the mobile app’s performance. This shows how different ML tiers can cooperate to handle demanding tasks.\nTiny ML-Mobile Integration Scenario: A fitness tracker uses Tiny ML to continuously monitor activity patterns and vital signs. Using the “Sync” pathway shown in Figure 2.9, it synchronizes this processed data with the user’s smartphone, which combines it with other health data before sending consolidated updates via the “Analyze” path to the cloud for long-term health analysis. This illustrates the common pattern of tiny devices using mobile devices as gateways to larger networks.\nMulti-Layer Processing Scenario: In a smart retail environment, tiny sensors monitor inventory levels, using “Data” and “Results” paths to send inference results to both edge systems for immediate stock management and mobile devices for staff notifications. Following the “Analyze” path, the edge systems process this data alongside other store metrics, while the cloud analyzes trends across all store locations. This demonstrates how the interactions shown in Figure 2.9 enable ML tiers to work together in a complete solution.\n\nThese real-world patterns demonstrate how different ML paradigms naturally complement each other in practice. While each approach has its own strengths, their true power emerges when they work together as an integrated system. By understanding these patterns, system architects can better design solutions that effectively leverage the capabilities of each ML tier while managing their respective constraints.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>ML Systems</span>"
    ]
  },
  {
    "objectID": "contents/core/ml_systems/ml_systems.html#shared-principles",
    "href": "contents/core/ml_systems/ml_systems.html#shared-principles",
    "title": "2  ML Systems",
    "section": "2.7 Shared Principles",
    "text": "2.7 Shared Principles\nThe design and integration patterns illustrate how ML paradigms—Cloud, Edge, Mobile, and Tiny–interact to address real-world challenges. While each paradigm is tailored to specific roles, their interactions reveal recurring principles that guide effective system design. These shared principles provide a unifying framework for understanding both individual ML paradigms and their hybrid combinations. As we explore these principles, a deeper system design perspective emerges, showing how different ML implementations—optimized for distinct contexts—converge around core concepts. This convergence forms the foundation for systematically understanding ML systems, despite their diversity and breadth.\nFigure 2.10 illustrates this convergence, highlighting the relationships that underpin practical system design and implementation. Grasping these principles is invaluable not only for working with individual ML systems but also for developing hybrid solutions that leverage their strengths, mitigate their limitations, and create cohesive, efficient ML workflows.\n\n\n\n\n\n\nFigure 2.10: Core principles converge across different ML system implementations, from cloud to tiny deployments, sharing common foundations in data pipelines, resource management, and system architecture.\n\n\n\nThe figure shows three key layers that help us understand how ML systems relate to each other. At the top, we see the diverse implementations that we have explored throughout this chapter. Cloud ML operates in data centers, focusing on training at scale with vast computational resources. Edge ML emphasizes local processing with inference capabilities closer to data sources. Mobile ML leverages personal devices for user-centric applications. Tiny ML brings intelligence to highly constrained embedded systems and sensors.\nDespite their distinct characteristics, the arrows in the figure show how all these implementations connect to the same core system principles. This reflects an important reality in ML systems—while they may operate at dramatically different scales, from cloud systems processing petabytes to tiny devices handling kilobytes, they all must solve similar fundamental challenges in terms of:\n\nManaging data pipelines from collection through processing to deployment\nBalancing resource utilization across compute, memory, energy, and network\nImplementing system architectures that effectively integrate models, hardware, and software\n\nThese core principles then lead to shared system considerations around optimization, operations, and trustworthiness. This progression helps explain why techniques developed for one scale of ML system often transfer effectively to others. The underlying problems—efficiently processing data, managing resources, and ensuring reliable operation—remain consistent even as the specific solutions vary based on scale and context.\nUnderstanding this convergence becomes particularly valuable as we move towards hybrid ML systems. When we recognize that different ML implementations share fundamental principles, combining them effectively becomes more intuitive. We can better appreciate why, for example, a cloud-trained model can be effectively deployed to edge devices, or why mobile and tiny ML systems can complement each other in IoT applications.\n\n2.7.1 Implementations Layer\nThe top layer of Figure 2.10 represents the diverse landscape of ML systems we’ve explored throughout this chapter. Each implementation addresses specific needs and operational contexts, yet all contribute to the broader ecosystem of ML deployment options.\nCloud ML, centered in data centers, provides the foundation for large-scale training and complex model serving. With access to vast computational resources like the NVIDIA DGX A100 systems we saw in Table 2.1, cloud implementations excel at handling massive datasets and training sophisticated models. This makes them particularly suited for tasks requiring extensive computational power, such as training foundation models or processing large-scale analytics.\nEdge ML shifts the focus to local processing, prioritizing inference capabilities closer to data sources. Using devices like the NVIDIA Jetson AGX Orin, edge implementations balance computational power with reduced latency and improved privacy. This approach proves especially valuable in scenarios requiring quick decisions based on local data, such as industrial automation or real-time video analytics.\nMobile ML leverages the capabilities of personal devices, particularly smartphones and tablets. With specialized hardware like Apple’s A17 Pro chip, mobile implementations enable sophisticated ML capabilities while maintaining user privacy and providing offline functionality. This paradigm has revolutionized applications from computational photography to on-device speech recognition.\nTiny ML represents the frontier of embedded ML, bringing intelligence to highly constrained devices. Operating on microcontrollers like the Arduino Nano 33 BLE Sense2, tiny implementations must carefully balance functionality with severe resource constraints. Despite these limitations, Tiny ML enables ML capabilities in scenarios where power efficiency and size constraints are paramount.\n2 The Arduino Nano 33 BLE Sense, introduced in 2019, is a microcontroller specifically designed for Tiny ML applications, featuring sensors and Bluetooth connectivity to facilitate on-device intelligence.\n\n2.7.2 System Principles Layer\nThe middle layer reveals the fundamental principles that unite all ML systems, regardless of their implementation scale. These core principles remain consistent even as their specific manifestations vary dramatically across different deployments.\nData Pipeline principles govern how systems handle information flow, from initial collection through processing to final deployment. In cloud systems, this might mean processing petabytes of data through distributed pipelines. For tiny systems, it could involve carefully managing sensor data streams within limited memory. Despite these scale differences, all systems must address the same fundamental challenges of data ingestion, transformation, and utilization.\nResource Management emerges as a universal challenge across all implementations. Whether managing thousands of GPUs in a data center or optimizing battery life on a microcontroller, all systems must balance competing demands for computation, memory, energy, and network resources. The quantities involved may differ by orders of magnitude, but the core principles of resource allocation and optimization remain remarkably consistent.\nSystem Architecture principles guide how ML systems integrate models, hardware, and software components. Cloud architectures might focus on distributed computing and scalability, while tiny systems emphasize efficient memory mapping and interrupt handling. Yet all must solve fundamental problems of component integration, data flow optimization, and processing coordination.\n\n\n2.7.3 System Considerations Layer\nThe bottom layer of Figure 2.10 illustrates how fundamental principles manifest in practical system-wide considerations. These considerations span all ML implementations, though their specific challenges and solutions vary based on scale and context.\nOptimization and Efficiency shape how ML systems balance performance with resource utilization. In cloud environments, this often means optimizing model training across GPU clusters while managing energy consumption in data centers. Edge systems focus on reducing model size and accelerating inference without compromising accuracy. Mobile implementations must balance model performance with battery life and thermal constraints. Tiny ML pushes optimization to its limits, requiring extensive model compression and quantization to fit within severely constrained environments. Despite these different emphases, all implementations grapple with the core challenge of maximizing performance within their available resources.\nOperational Aspects affect how ML systems are deployed, monitored, and maintained in production environments. Cloud systems must handle continuous deployment across distributed infrastructure while monitoring model performance at scale. Edge implementations need robust update mechanisms and health monitoring across potentially thousands of devices. Mobile systems require seamless app updates and performance monitoring without disrupting user experience. Tiny ML faces unique challenges in deploying updates to embedded devices while ensuring continuous operation. Across all scales, the fundamental problems of deployment, monitoring, and maintenance remain consistent, even as solutions vary.\nTrustworthy AI considerations ensure ML systems operate reliably, securely, and with appropriate privacy protections. Cloud implementations must secure massive amounts of data while ensuring model predictions remain reliable at scale. Edge systems need to protect local data processing while maintaining model accuracy in diverse environments. Mobile ML must preserve user privacy while delivering consistent performance. Tiny ML systems, despite their size, must still ensure secure operation and reliable inference. These trustworthiness considerations cut across all implementations, reflecting the critical importance of building ML systems that users can depend on.\nThe progression through these layers—from diverse implementations through core principles to shared considerations—reveals why ML systems can be studied as a unified field despite their apparent differences. While specific solutions may vary dramatically based on scale and context, the fundamental challenges remain remarkably consistent. This understanding becomes particularly valuable as we move toward increasingly sophisticated hybrid systems that combine multiple implementation approaches.\nThe convergence of fundamental principles across ML implementations helps explain why hybrid approaches work so effectively in practice. As we saw in our discussion of hybrid ML, different implementations naturally complement each other precisely because they share these core foundations. Whether we’re looking at train-serve splits that leverage cloud resources for training and edge devices for inference, or hierarchical processing that combines Tiny ML sensors with edge aggregation and cloud analytics, the shared principles enable seamless integration across scales.\n\n\n2.7.4 From Principles to Practice\nThis convergence also suggests why techniques and insights often transfer well between different scales of ML systems. A deep understanding of data pipelines in cloud environments can inform how we structure data flow in embedded systems. Resource management strategies developed for mobile devices might inspire new approaches to cloud optimization. System architecture patterns that prove effective at one scale often adapt surprisingly well to others.\nUnderstanding these fundamental principles and shared considerations provides a foundation for comparing different ML implementations more effectively. While each approach has its distinct characteristics and optimal use cases, they all build upon the same core elements. As we move into our detailed comparison in the next section, keeping these shared foundations in mind will help us better appreciate both the differences and similarities between various ML system implementations.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>ML Systems</span>"
    ]
  },
  {
    "objectID": "contents/core/ml_systems/ml_systems.html#ml-system-comparison",
    "href": "contents/core/ml_systems/ml_systems.html#ml-system-comparison",
    "title": "2  ML Systems",
    "section": "2.8 ML System Comparison",
    "text": "2.8 ML System Comparison\nBuilding on the shared principles explored earlier, we can synthesize our understanding by examining how the various ML system approaches compare across different dimensions. This synthesis highlights the trade-offs system designers often face when choosing deployment options and how these decisions align with core principles like resource management, data pipelines, and system architecture.\nThe relationship between computational resources and deployment location forms one of the most fundamental comparisons across ML systems. As we move from cloud deployments to tiny devices, we observe a dramatic reduction in available computing power, storage, and energy consumption. Cloud ML systems, with their data center infrastructure, can leverage virtually unlimited resources, processing data at the scale of petabytes and training models with billions of parameters. Edge ML systems, while more constrained, still offer significant computational capability through specialized hardware like edge GPUs and neural processing units. Mobile ML represents a middle ground, balancing computational power with energy efficiency on devices like smartphones and tablets. At the far end of the spectrum, TinyML operates under severe resource constraints, often limited to kilobytes of memory and milliwatts of power consumption.\n\n\n\nTable 2.2: Comparison of feature aspects across Cloud ML, Edge ML, and Tiny ML.\n\n\n\n\n\n\n\n\n\n\n\n\nAspect\nCloud ML\nEdge ML\nMobile ML\nTiny ML\n\n\n\n\nPerformance\n\n\n\n\n\n\nProcessing Location\nCentralized cloud servers (Data Centers)\nLocal edge devices (gateways, servers)\nSmartphones and tablets\nUltra-low-power microcontrollers and embedded systems\n\n\nLatency\nHigh (100ms-1000ms+)\nModerate (10-100ms)\nLow-Moderate (5-50ms)\nVery Low (1-10ms)\n\n\nCompute Power\nVery High (Multiple GPUs/TPUs)\nHigh (Edge GPUs)\nModerate (Mobile NPUs/GPUs)\nVery Low (MCU/tiny processors)\n\n\nStorage Capacity\nUnlimited (petabytes+)\nLarge (terabytes)\nModerate (gigabytes)\nVery Limited (kilobytes-megabytes)\n\n\nEnergy Consumption\nVery High (kW-MW range)\nHigh (100s W)\nModerate (1-10W)\nVery Low (mW range)\n\n\nScalability\nExcellent (virtually unlimited)\nGood (limited by edge hardware)\nModerate (per-device scaling)\nLimited (fixed hardware)\n\n\nOperational\n\n\n\n\n\n\nData Privacy\nBasic-Moderate (Data leaves device)\nHigh (Data stays in local network)\nHigh (Data stays on phone)\nVery High (Data never leaves sensor)\n\n\nConnectivity Required\nConstant high-bandwidth\nIntermittent\nOptional\nNone\n\n\nOffline Capability\nNone\nGood\nExcellent\nComplete\n\n\nReal-time Processing\nDependent on network\nGood\nVery Good\nExcellent\n\n\nDeployment\n\n\n\n\n\n\nCost\nHigh ($1000s+/month)\nModerate ($100s-1000s)\nLow ($0-10s)\nVery Low ($1-10s)\n\n\nHardware Requirements\nCloud infrastructure\nEdge servers/gateways\nModern smartphones\nMCUs/embedded systems\n\n\nDevelopment Complexity\nHigh (cloud expertise needed)\nModerate-High (edge+networking)\nModerate (mobile SDKs)\nHigh (embedded expertise)\n\n\nDeployment Speed\nFast\nModerate\nFast\nSlow\n\n\n\n\n\n\nThe operational characteristics of these systems reveal another important dimension of comparison. Table 2.2 organizes these characteristics into logical groupings, highlighting performance, operational considerations, costs, and development aspects. For instance, latency shows a clear gradient: cloud systems typically incur delays of 100-1000ms due to network communication, while edge systems reduce this to 10-100 ms by processing data locally. Mobile ML achieves even lower latencies of 5-50 ms for many tasks, and TinyML systems can respond in 1-10 ms for simple inferences. Similarly, privacy and data handling improve progressively as computation shifts closer to the data source, with TinyML offering the strongest guarantees by keeping data entirely local to the device.\nThe table is designed to provide a high-level view of how these paradigms differ across key dimensions, making it easier to understand the trade-offs and select the most appropriate approach for specific deployment needs.\nTo complement the details presented in Table 2.2, radar plots are presented below. These visualizations highlight two critical dimensions: performance characteristics and operational characteristics. The performance characteristics plot in Figure 2.11 focuses on latency, compute power, energy consumption, and scalability. As discussed earlier, Cloud ML demands exceptional compute power and demonstrates good scalability, making it ideal for large-scale tasks requiring extensive resources. Tiny ML, in contrast, excels in latency and energy efficiency due to its lightweight and localized processing, suitable for low-power, real-time scenarios. Edge ML and Mobile ML strike a balance, offering moderate scalability and efficiency for a variety of applications.\nThe operational characteristics plot in Figure 2.12 emphasizes data privacy, connectivity independence, offline capability, and real-time processing. Tiny ML emerges as a highly independent and private paradigm, excelling in offline functionality and real-time responsiveness. In contrast, Cloud ML relies on centralized infrastructure and constant connectivity, which can be a limitation in scenarios demanding autonomy or low-latency decision-making.\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 2.11: Performance characteristics.\n\n\n\n\n\n\n\n\n\n\n\nFigure 2.12: Operational characteristics.\n\n\n\n\n\n\nDevelopment complexity and deployment considerations also vary significantly across these paradigms. Cloud ML benefits from mature development tools and frameworks but requires expertise in cloud infrastructure. Edge ML demands knowledge of both ML and networking protocols, while Mobile ML developers must understand mobile-specific optimizations and platform constraints. TinyML development, though targeting simpler devices, often requires specialized knowledge of embedded systems and careful optimization to work within severe resource constraints.\nCost structures differ markedly as well. Cloud ML typically involves ongoing operational costs for computation and storage, often running into thousands of dollars monthly for large-scale deployments. Edge ML requires significant upfront investment in edge devices but may reduce ongoing costs. Mobile ML leverages existing consumer devices, minimizing additional hardware costs, while TinyML solutions can be deployed for just a few dollars per device, though development costs may be higher.\nThese comparisons reveal that each paradigm has distinct advantages and limitations. Cloud ML excels at complex, data-intensive tasks but requires constant connectivity. Edge ML offers a balance of computational power and local processing. Mobile ML provides personalized intelligence on ubiquitous devices. TinyML enables ML in previously inaccessible contexts but requires careful optimization. Understanding these trade-offs is crucial for selecting the appropriate deployment strategy for specific applications and constraints.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>ML Systems</span>"
    ]
  },
  {
    "objectID": "contents/core/ml_systems/ml_systems.html#ml-deployment-decision-framework",
    "href": "contents/core/ml_systems/ml_systems.html#ml-deployment-decision-framework",
    "title": "2  ML Systems",
    "section": "2.9 ML Deployment Decision Framework",
    "text": "2.9 ML Deployment Decision Framework\nWe have examined the diverse paradigms of machine learning systems—Cloud ML, Edge ML, Mobile ML, and Tiny ML—each with its own characteristics, trade-offs, and use cases. Selecting an optimal deployment strategy requires careful consideration of multiple factors.\n\n\n\n\n\n\nFigure 2.13: A decision flowchart for selecting the most suitable ML deployment paradigm.\n\n\n\nTo facilitate this decision-making process, we present a structured framework in Figure 2.13. This framework distills the chapter’s key insights into a systematic approach for determining the most suitable deployment paradigm based on specific requirements and constraints.\nThe framework is organized into five fundamental layers of consideration:\n\nPrivacy: Determines whether processing can occur in the cloud or must remain local to safeguard sensitive data.\nLatency: Evaluates the required decision-making speed, particularly for real-time or near-real-time processing needs.\nReliability: Assesses network stability and its impact on deployment feasibility.\nCompute Needs: Identifies whether high-performance infrastructure is required or if lightweight processing suffices.\nCost and Energy Efficiency: Balances resource availability with financial and energy constraints, particularly crucial for low-power or budget-sensitive applications.\n\nAs designers progress through these layers, each decision point narrows the viable options, ultimately guiding them toward one of the four deployment paradigms. This systematic approach proves valuable across various scenarios. For instance, privacy-sensitive healthcare applications might prioritize local processing over cloud solutions, while high-performance recommendation engines typically favor cloud infrastructure. Similarly, applications requiring real-time responses often gravitate toward edge or mobile-based deployment.\nWhile not exhaustive, this framework provides a practical roadmap for navigating deployment decisions. By following this structured approach, system designers can evaluate trade-offs and align their deployment choices with technical, financial, and operational priorities, even as they address the unique challenges of each application.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>ML Systems</span>"
    ]
  },
  {
    "objectID": "contents/core/ml_systems/ml_systems.html#conclusion",
    "href": "contents/core/ml_systems/ml_systems.html#conclusion",
    "title": "2  ML Systems",
    "section": "2.10 Conclusion",
    "text": "2.10 Conclusion\nThis chapter has explored the diverse landscape of machine learning systems, highlighting their unique characteristics, benefits, challenges, and applications. Cloud ML leverages immense computational resources, excelling in large-scale data processing and model training but facing limitations such as latency and privacy concerns. Edge ML bridges this gap by enabling localized processing, reducing latency, and enhancing privacy. Mobile ML builds on these strengths, harnessing the ubiquity of smartphones to provide responsive, user-centric applications. At the smallest scale, Tiny ML extends the reach of machine learning to resource-constrained devices, opening new domains of application.\nTogether, these paradigms reflect an ongoing progression in machine learning, moving from centralized systems in the cloud to increasingly distributed and specialized deployments across edge, mobile, and tiny devices. This evolution marks a shift toward systems that are finely tuned to specific deployment contexts, balancing computational power, energy efficiency, and real-time responsiveness. As these paradigms mature, hybrid approaches are emerging, blending their strengths to unlock new possibilities—from cloud-based training paired with edge inference to federated learning and hierarchical processing.\nDespite their variety, ML systems can be distilled into a core set of unifying principles that span resource management, data pipelines, and system architecture. These principles provide a structured framework for understanding and designing ML systems at any scale. By focusing on these shared fundamentals and mastering their design and optimization, we can navigate the complexity of the ML landscape with clarity and confidence. As we continue to advance, these principles will act as a compass, guiding our exploration and innovation within the ever-evolving field of machine learning systems. Regardless of how diverse or complex these systems become, a strong grasp of these foundational concepts will remain essential to unlocking their full potential.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>ML Systems</span>"
    ]
  },
  {
    "objectID": "contents/core/ml_systems/ml_systems.html#sec-ml-systems-resource",
    "href": "contents/core/ml_systems/ml_systems.html#sec-ml-systems-resource",
    "title": "2  ML Systems",
    "section": "2.11 Resources",
    "text": "2.11 Resources\nHere is a curated list of resources to support students and instructors in their learning and teaching journeys. We are continuously working on expanding this collection and will be adding new exercises soon.\n\n\n\n\n\n\nSlides\n\n\n\n\n\nThese slides are a valuable tool for instructors to deliver lectures and for students to review the material at their own pace. We encourage students and instructors to leverage these slides to improve their understanding and facilitate effective knowledge transfer.\n\nEmbedded Systems Overview.\nEmbedded Computer Hardware.\nEmbedded I/O.\nEmbedded systems software.\nEmbedded ML software.\nEmbedded Inference.\nTiny ML on Microcontrollers.\nTiny ML as a Service (Tiny MLaaS):\n\n—Tiny MLaaS: Introduction.\n—Tiny MLaaS: Design Overview.\n\n\n\n\n\n\n\n\n\nVideos\n\n\n\n\n\n\nComing soon.\n\n\n\n\n\n\n\n\n\n\nExercises\n\n\n\n\n\nTo reinforce the concepts covered in this chapter, we have curated a set of exercises that challenge students to apply their knowledge and deepen their understanding.\n\nComing soon.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>ML Systems</span>"
    ]
  },
  {
    "objectID": "contents/core/dl_primer/dl_primer.html",
    "href": "contents/core/dl_primer/dl_primer.html",
    "title": "3  DL Primer",
    "section": "",
    "text": "Purpose\nResources: Slides, Videos, Exercises\nWhat inspiration from nature drives the development of machine learning systems, and how do biological neural processes inform their fundamental design?\nThe neural systems of nature offer profound insights into information processing and adaptation, inspiring the core principles of modern machine learning. Translating biological mechanisms into computational frameworks illuminates fundamental patterns that shape artificial neural networks. These patterns reveal essential relationships between biological principles and their digital counterparts, establishing building blocks for understanding more complex architectures. Analyzing these mappings from natural to artificial provides critical insights into system design, laying the foundation for exploring advanced neural architectures and their practical implementations.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>DL Primer</span>"
    ]
  },
  {
    "objectID": "contents/core/dl_primer/dl_primer.html#purpose",
    "href": "contents/core/dl_primer/dl_primer.html#purpose",
    "title": "3  DL Primer",
    "section": "",
    "text": "Learning Objectives\n\n\n\n\nUnderstand the biological inspiration for artificial neural networks and how this foundation informs their design and function.\nExplore the fundamental structure of neural networks, including neurons, layers, and connections.\nExamine the processes of forward propagation, backward propagation, and optimization as the core mechanisms of learning.\nUnderstand the complete machine learning pipeline, from pre-processing through neural computation to post-processing.\nCompare and contrast training and inference phases, understanding their distinct computational requirements and optimizations.\nLearn how neural networks process data to extract patterns and make predictions, bridging theoretical concepts with computational implementations.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>DL Primer</span>"
    ]
  },
  {
    "objectID": "contents/core/dl_primer/dl_primer.html#overview",
    "href": "contents/core/dl_primer/dl_primer.html#overview",
    "title": "3  DL Primer",
    "section": "3.1 Overview",
    "text": "3.1 Overview\nNeural networks, a foundational concept within machine learning and artificial intelligence, are computational models inspired by the structure and function of biological neural systems. These networks represent a critical intersection of algorithms, mathematical frameworks, and computing infrastructure, making them integral to solving complex problems in AI.\nWhen studying neural networks, it is helpful to place them within the broader hierarchy of AI and machine learning. Figure 3.1 provides a visual representation of this context. AI, as the overarching field, encompasses all computational methods that aim to mimic human cognitive functions. Within AI, machine learning includes techniques that enable systems to learn patterns from data. Neural networks, a key subset of ML, form the backbone of more advanced learning systems, including deep learning, by modeling complex relationships in data through interconnected computational units.\n\n\n\n\n\n\nFigure 3.1: The diagram illustrates artificial intelligence as the overarching field encompassing all computational methods that mimic human cognitive functions. Machine learning is a subset of AI that includes algorithms capable of learning from data. Deep learning, a further subset of ML, specifically involves neural networks that are able to learn more complex patterns in large volumes of data. Source: NVIDIA.\n\n\n\nThe emergence of neural networks reflects key shifts in how AI systems process information across three fundamental dimensions:\n\nData: From manually structured and rule-based datasets to raw, high-dimensional data. Neural networks are particularly adept at learning from complex and unstructured data, making them essential for tasks involving images, speech, and text.\nAlgorithms: From explicitly programmed rules to adaptive systems capable of learning patterns directly from data. Neural networks eliminate the need for manual feature engineering by discovering representations automatically through layers of interconnected units.\nComputation: From simple, sequential operations to massively parallel computations. The scalability of neural networks has driven demand for advanced hardware, such as GPUs, that can efficiently process large models and datasets.\n\nThese shifts underscore the importance of understanding neural networks, not only as mathematical constructs but also as practical components of real-world AI systems. The development and deployment of neural networks require careful consideration of computational efficiency, data processing workflows, and hardware optimization.\nTo build a strong foundation, this chapter focuses on the core principles of neural networks, exploring their structure, functionality, and learning mechanisms. By understanding these basics, readers will be well-prepared to delve into more advanced architectures and their systems-level implications in later chapters.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>DL Primer</span>"
    ]
  },
  {
    "objectID": "contents/core/dl_primer/dl_primer.html#what-makes-deep-learning-different",
    "href": "contents/core/dl_primer/dl_primer.html#what-makes-deep-learning-different",
    "title": "3  DL Primer",
    "section": "3.2 What Makes Deep Learning Different",
    "text": "3.2 What Makes Deep Learning Different\nDeep learning represents a fundamental shift in how we approach problem solving with computers. To understand this shift, let’s consider the classic example of computer vision—specifically, the task of identifying objects in images.\n\n3.2.1 Traditional Programming: The Era of Explicit Rules\nTraditional programming requires developers to explicitly define rules that tell computers how to process inputs and produce outputs. Consider a simple game like Breakout, shown in Figure 3.2. The program needs explicit rules for every interaction: when the ball hits a brick, the code must specify that the brick should be removed and the ball’s direction should be reversed. While this approach works well for games with clear physics and limited states, it demonstrates an inherent limitation of rule-based systems.\n\n\n\n\n\n\nFigure 3.2: Rule-based programming.\n\n\n\nThis rules-based paradigm extends to all traditional programming, as illustrated in Figure 3.3. The program takes both rules for processing and input data to produce outputs. Early artificial intelligence research explored whether this approach could scale to solve complex problems by encoding sufficient rules to capture intelligent behavior.\n\n\n\n\n\n\nFigure 3.3: Traditional programming.\n\n\n\nHowever, the limitations of rule-based approaches become evident when addressing complex real-world tasks. Consider the problem of recognizing human activities, shown in Figure 3.4. Initial rules might appear straightforward: classify movement below 4 mph as walking and faster movement as running. Yet real-world complexity quickly emerges. The classification must account for variations in speed, transitions between activities, and numerous edge cases. Each new consideration requires additional rules, leading to increasingly complex decision trees.\n\n\n\n\n\n\nFigure 3.4: Activity rules.\n\n\n\nThis challenge extends to computer vision tasks. Detecting objects like cats in images would require rules about System Implications: pointed ears, whiskers, typical body shapes. Such rules would need to account for variations in viewing angle, lighting conditions, partial occlusions, and natural variations among instances. Early computer vision systems attempted this approach through geometric rules but achieved success only in controlled environments with well-defined objects.\nThis knowledge engineering approach characterized artificial intelligence research in the 1970s and 1980s. Expert systems encoded domain knowledge as explicit rules, showing promise in specific domains with well-defined parameters but struggling with tasks humans perform naturally—such as object recognition, speech understanding, or natural language interpretation. These limitations highlighted a fundamental challenge: many aspects of intelligent behavior rely on implicit knowledge that resists explicit rule-based representation.\n\n\n3.2.2 Machine Learning: Learning from Engineered Patterns\nThe limitations of pure rule-based systems led researchers to explore approaches that could learn from data. Machine learning offered a promising direction: instead of writing rules for every situation, we could write programs that found patterns in examples. However, the success of these methods still depended heavily on human insight to define what patterns might be important—a process known as feature engineering.\nFeature engineering involves transforming raw data into representations that make patterns more apparent to learning algorithms. In computer vision, researchers developed sophisticated methods to extract meaningful patterns from images. The Histogram of Oriented Gradients (HOG) method, shown in Figure 3.5, exemplifies this approach. HOG works by first identifying edges in an image—places where brightness changes sharply, often indicating object boundaries. It then divides the image into small cells and measures how edges are oriented within each cell, summarizing these orientations in a histogram. This transformation converts raw pixel values into a representation that captures important shape information while being robust to variations in lighting and small changes in position.\n\n\n\n\n\n\nFigure 3.5: Histogram of Oriented Gradients (HOG) requires explicit feature engineering.\n\n\n\nOther feature extraction methods like SIFT (Scale-Invariant Feature Transform) and Gabor filters provided different ways to capture patterns in images. SIFT found distinctive points that could be recognized even when an object’s size or orientation changed. Gabor filters helped identify textures and repeated patterns. Each method encoded different types of human insight about what makes visual patterns recognizable.\nThese engineered features enabled significant advances in computer vision during the 2000s. Systems could now recognize objects with some robustness to real-world variations, leading to applications in face detection, pedestrian detection, and object recognition. However, the approach had fundamental limitations. Experts needed to carefully design feature extractors for each new problem, and the resulting features might miss important patterns that weren’t anticipated in their design.\n\n\n3.2.3 Deep Learning Paradigm\nDeep learning fundamentally differs by learning directly from raw data. Traditional programming, as we saw earlier in Figure 3.3, required both rules and data as inputs to produce answers. Machine learning inverts this relationship, as shown in Figure 3.6. Instead of writing rules, we provide examples (data) and their correct answers to discover the underlying rules automatically. This shift eliminates the need for humans to specify what patterns are important.\n\n\n\n\n\n\nFigure 3.6: Deep learning.\n\n\n\nThe system discovers these patterns automatically from examples. When shown millions of images of cats, the system learns to identify increasingly complex visual patterns—from simple edges to more sophisticated combinations that make up cat-like features. This mirrors how our own visual system works, building up understanding from basic visual elements to complex objects.\nUnlike traditional approaches where performance often plateaus with more data and computation, deep learning systems continue to improve as we provide more resources. More training examples help the system recognize more variations and nuances. More computational power enables the system to discover more subtle patterns. This scalability has led to dramatic improvements in performance—for example, the accuracy of image recognition systems has improved from 74% in 2012 to over 95% today.\nThis different approach has profound implications for how we build AI systems. Deep learning’s ability to learn directly from raw data eliminates the need for manual feature engineering, but it comes with new demands. We need sophisticated infrastructure to handle massive datasets, powerful computers to process this data, and specialized hardware to perform the complex mathematical calculations efficiently. The computational requirements of deep learning have even driven the development of new types of computer chips optimized for these calculations.\nThe success of deep learning in computer vision exemplifies how this approach, when given sufficient data and computation, can surpass traditional methods. This pattern has repeated across many domains, from speech recognition to game playing, establishing deep learning as a transformative approach to artificial intelligence.\n\n\n3.2.4 Systems Implications of Each Approach\nThe progression from traditional programming to deep learning represents not just a shift in how we solve problems, but a fundamental transformation in computing system requirements. This transformation becomes particularly critical when we consider the full spectrum of ML systems—from massive cloud deployments to resource-constrained tiny ML devices.\nTraditional programs follow predictable patterns. They execute sequential instructions, access memory in regular patterns, and utilize computing resources in well-understood ways. A typical rule-based image processing system might scan through pixels methodically, applying fixed operations with modest and predictable computational and memory requirements. These characteristics made traditional programs relatively straightforward to deploy across different computing platforms.\nMachine learning with engineered features introduced new complexities. Feature extraction algorithms required more intensive computation and structured data movement. The HOG feature extractor discussed earlier, for instance, requires multiple passes over image data, computing gradients and constructing histograms. While this increased both computational demands and memory complexity, the resource requirements remained relatively predictable and scalable across platforms.\nDeep learning, however, fundamentally reshapes system requirements across multiple dimensions. Table 3.1 shows the evolution of system requirements across programming paradigms:\n\n\n\nTable 3.1: Evolution of system requirements across programming paradigms.\n\n\n\n\n\n\n\n\n\n\n\nSystem Aspect\nTraditional Programming\nML with Features\nDeep Learning\n\n\n\n\nComputation\nSequential, predictable paths\nStructured parallel operations\nMassive matrix parallelism\n\n\nMemory Access\nSmall, predictable patterns\nMedium, batch-oriented\nLarge, complex hierarchical patterns\n\n\nData Movement\nSimple input/output flows\nStructured batch processing\nIntensive cross-system movement\n\n\nHardware Needs\nCPU-centric\nCPU with vector units\nSpecialized accelerators\n\n\nResource Scaling\nFixed requirements\nLinear with data size\nExponential with complexity\n\n\n\n\n\n\nThese differences manifest in several critical ways, with implications across the entire ML systems spectrum.\n\nComputation Patterns\nWhile traditional programs follow sequential logic flows, deep learning requires massive parallel operations on matrices. This shift explains why conventional CPUs, designed for sequential processing, prove inefficient for neural network computations. The need for parallel processing has driven the adoption of specialized hardware architectures—from powerful cloud GPUs to specialized mobile processors to tiny ML accelerators.\n\n\nMemory Systems\nTraditional programs typically maintain small, fixed memory footprints. Deep learning models, however, must manage parameters across complex memory hierarchies. Memory bandwidth often becomes the primary performance bottleneck, creating particular challenges for resource-constrained systems. This drives different optimization strategies across the ML systems spectrum—from memory-rich cloud deployments to heavily optimized tiny ML implementations.\n\n\nSystem Scale\nPerhaps most importantly, deep learning fundamentally changes how systems scale and the critical importance of efficiency. Traditional programs have relatively fixed resource requirements with predictable performance characteristics. Deep learning systems, however, can consume exponentially more resources as models grow in complexity. This relationship between model capability and resource consumption makes system efficiency a central concern.\nThe need to bridge algorithmic concepts with hardware realities becomes crucial. While traditional programs map relatively straightforwardly to standard computer architectures, deep learning requires us to think carefully about:\n\nHow to efficiently map matrix operations to physical hardware\nWays to minimize data movement across memory hierarchies\nMethods to balance computational capability with resource constraints\nTechniques to optimize both algorithm and system-level efficiency\n\nThese fundamental shifts explain why deep learning has spurred innovations across the entire computing stack. From specialized hardware accelerators to new memory architectures to sophisticated software frameworks, the demands of deep learning continue to reshape computer system design. Interestingly, many of these challenges—efficiency, scaling, and adaptability—are ones that biological systems have already solved. This brings us to a critical question: what can we learn from nature’s own information processing system and strive to mimic them as artificially intelligent systems.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>DL Primer</span>"
    ]
  },
  {
    "objectID": "contents/core/dl_primer/dl_primer.html#from-brain-to-artificial-neurons",
    "href": "contents/core/dl_primer/dl_primer.html#from-brain-to-artificial-neurons",
    "title": "3  DL Primer",
    "section": "3.3 From Brain to Artificial Neurons",
    "text": "3.3 From Brain to Artificial Neurons\nThe quest to create artificial intelligence has been profoundly influenced by our understanding of biological intelligence, particularly the human brain. This isn’t surprising—the brain represents the most sophisticated information processing system we know of, capable of learning, adapting, and solving complex problems while maintaining remarkable energy efficiency. The way our brains function has provided fundamental insights that continue to shape how we approach artificial intelligence.\n\n3.3.1 Biological Intelligence\nWhen we observe biological intelligence, several key principles emerge. The brain demonstrates an extraordinary ability to learn from experience, constantly modifying its neural connections based on new information and interactions with the environment. This adaptability is fundamental—every experience potentially alters the brain’s structure, refining its responses for future situations. This biological capability directly inspired one of the core principles of machine learning: the ability to learn and improve from data rather than following fixed, pre-programmed rules.\nAnother striking feature of biological intelligence is its parallel processing capability. The brain processes vast amounts of information simultaneously, with different regions specializing in specific functions while working in concert. This distributed, parallel architecture stands in stark contrast to traditional sequential computing and has significantly influenced modern AI system design. The brain’s ability to efficiently coordinate these parallel processes while maintaining coherent function represents a level of sophistication we’re still working to fully understand and replicate.\nThe brain’s pattern recognition capabilities are particularly noteworthy. Biological systems excel at identifying patterns in complex, noisy data—whether recognizing faces in a crowd, understanding speech in a noisy environment, or identifying objects from partial information. This remarkable ability has inspired numerous AI applications, particularly in computer vision and speech recognition systems. The brain accomplishes these tasks with an efficiency that artificial systems are still striving to match.\nPerhaps most remarkably, biological systems achieve all this with incredible energy efficiency. The human brain operates on approximately 20 watts of power—about the same as a low-power light bulb—while performing complex cognitive tasks that would require orders of magnitude more power in current artificial systems. This efficiency hasn’t just impressed researchers; it has become a crucial goal in the development of AI hardware and algorithms.\nThese biological principles have led to two distinct but complementary approaches in artificial intelligence. The first attempts to directly mimic neural structure and function, leading to artificial neural networks and deep learning architectures that structurally resemble biological neural networks. The second takes a more abstract approach, adapting biological principles to work efficiently within the constraints of computer hardware without necessarily copying biological structures exactly. In the following sections, we will explore how these approaches manifest in practice, beginning with the fundamental building block of neural networks: the neuron itself.\n\n\n3.3.2 Biological to Artificial Neurons\nTo understand how biological principles translate into artificial systems, we must first examine the basic unit of biological information processing: the neuron. This cellular building block provides the blueprint for its artificial counterpart and helps us understand how complex neural networks emerge from simple components working in concert.\nIn biological systems, the neuron (or cell) is the basic functional unit of the nervous system. Understanding its structure is crucial before we draw parallels to artificial systems. Figure 3.7 illustrates the structure of a biological neuron.\n\n\n\n\n\n\nFigure 3.7: Bilogical structure of a neuron and its mapping to an artificial neuron. Source: Geeksforgeeks\n\n\n\nA biological neuron consists of several key components. The central part is the cell body, or soma, which contains the nucleus and performs the cell’s basic life processes. Extending from the soma are branch-like structures called dendrites, which receive signals from other neurons. At the junctions where signals are passed between neurons are synapses. Finally, a long, slender projection called the axon conducts electrical impulses away from the cell body to other neurons.\nThe neuron functions as follows: Dendrites receive inputs from other neurons, with synapses determining the strength of the connections. The soma integrates these signals and decides whether to trigger an output signal. If triggered, the axon transmits this signal to other neurons.\nEach element of a biological neuron has a computational analog in artificial systems, reflecting the principles of learning, adaptability, and efficiency found in nature. To better understand how biological intelligence informs artificial systems, Table 3.2 captures the mapping between the components of biological and artificial neurons. This should be viewed alongside Figure 3.7 for a complete picture. Together, they paint a picture of the biological-to-artificial neuron mapping.\n\n\n\nTable 3.2: Mapping the biological neuron structure to an artificial neuron.\n\n\n\n\n\n\n\n\n\nBiological Neuron\nArtificial Neuron\n\n\n\n\nCell\nNeuron / Node\n\n\nDendrites / Synapse\nWeights\n\n\nSoma\nNet Input\n\n\nAxon\nOutput\n\n\n\n\n\n\nEach component serves a similar function, albeit through vastly different mechanisms. Here, we explain these mappings and their implications for artificial neural networks.\n\nCell \\(\\longleftrightarrow\\) Neuron/Node: The artificial neuron or node serves as the fundamental computational unit, mirroring the cell’s role in biological systems.\nDendrites/Synapse \\(\\longleftrightarrow\\) Weights: Weights in artificial neurons represent connection strengths, analogous to synapses in biological neurons. These weights are adjustable, enabling learning and optimization over time.\nSoma \\(\\longleftrightarrow\\) Net Input: The net input in artificial neurons sums weighted inputs to determine activation, similar to how the soma integrates signals in biological neurons.\nAxon \\(\\longleftrightarrow\\) Output: The output of an artificial neuron passes processed information to subsequent network layers, much like an axon transmits signals to other neurons.\n\nThis mapping illustrates how artificial neural networks simplify and abstract biological processes while preserving their essential computational principles. However, understanding individual neurons is just the beginning—the true power of neural networks emerges from how these basic units work together in larger systems.\n\n\n3.3.3 Artificial Intelligence\nThe translation from biological principles to artificial computation requires a deep appreciation of what makes biological neural networks so effective at both the cellular and network levels. The brain processes information through distributed computation across billions of neurons, each operating relatively slowly compared to silicon transistors. A biological neuron fires at approximately 200Hz, while modern processors operate at gigahertz frequencies. Despite this speed limitation, the brain’s parallel architecture enables sophisticated real-time processing of complex sensory input, decision making, and control of behavior.\nThis computational efficiency emerges from the brain’s basic organizational principles. Each neuron acts as a simple processing unit, integrating inputs from thousands of other neurons and producing a binary output signal based on whether this integrated input exceeds a threshold. The connection strengths between neurons, mediated by synapses, are continuously modified through experience. This synaptic plasticity forms the basis for learning and adaptation in biological neural networks. These biological principles suggest key computational elements needed in artificial neural systems:\n\nSimple processing units that integrate multiple inputs\nAdjustable connection strengths between units\nNonlinear activation based on input thresholds\nParallel processing architecture\nLearning through modification of connection strengths\n\n\n\n3.3.4 Computational Translation\nWe face the challenge of capturing the essence of neural computation within the rigid framework of digital systems. The implementation of biological principles in artificial neural systems represents a nuanced balance between biological fidelity and computational efficiency. At its core, an artificial neuron captures the essential computational properties of its biological counterpart through mathematical operations that can be efficiently executed on digital hardware.\nTable 3.3 provides a systematic view of how key biological features map to their computational counterparts. Each biological feature has an analog in computational systems, revealing both the possibilities and limitations of digital neural implementation, which we will learn more about later.\n\n\n\nTable 3.3: Translating biological features to the computing domain.\n\n\n\n\n\n\n\n\n\nBiological Feature\nComputational Translation\n\n\n\n\nNeuron firing\nActivation function\n\n\nSynaptic strength\nWeighted connections\n\n\nSignal integration\nSummation operation\n\n\nDistributed memory\nWeight matrices\n\n\nParallel processing\nConcurrent computation\n\n\n\n\n\n\nThe basic computational unit in artificial neural networks, the artificial neuron, simplifies the complex electrochemical processes of biological neurons into three fundamental operations. First, input signals are weighted, mimicking how biological synapses modulate incoming signals with different strengths. Second, these weighted inputs are summed together, analogous to how a biological neuron integrates incoming signals in its cell body. Finally, the summed input passes through an activation function that determines the neuron’s output, similar to how a biological neuron fires based on whether its membrane potential exceeds a threshold.\nThis mathematical abstraction preserves key computational principles while enabling efficient digital implementation. The weighting of inputs allows the network to learn which connections are important, just as biological neural networks strengthen or weaken synaptic connections through experience. The summation operation captures how biological neurons integrate multiple inputs into a single decision. The activation function introduces nonlinearity essential for learning complex patterns, much like the threshold-based firing of biological neurons.\nMemory in artificial neural networks takes a markedly different form from biological systems. While biological memories are distributed across synaptic connections and neural patterns, artificial networks store information in discrete weights and parameters. This architectural difference reflects the constraints of current computing hardware, where memory and processing are physically separated rather than integrated as in biological systems. Despite these implementation differences, artificial neural networks achieve similar functional capabilities in pattern recognition and learning.\nThe brain’s massive parallelism represents a fundamental challenge in artificial implementation. While biological neural networks process information through billions of neurons operating simultaneously, artificial systems approximate this parallelism through specialized hardware like GPUs and tensor processing units. These devices efficiently compute the matrix operations that form the mathematical foundation of artificial neural networks, achieving parallel processing at a different scale and granularity than biological systems.\n\n\n3.3.5 System Requirements\nThe computational translation of neural principles creates specific demands on the underlying computing infrastructure. These requirements emerge from the fundamental differences between biological and artificial implementations of neural processing, shaping how we design and build systems capable of supporting artificial neural networks.\nTable 3.4 shows how each computational element drives particular system requirements. From this mapping, we can see how the choices made in computational translation directly influence the hardware and system architecture needed for implementation.\n\n\n\nTable 3.4: From computation to system requirements.\n\n\n\n\n\n\n\n\n\nComputational Element\nSystem Requirements\n\n\n\n\nActivation functions\nFast nonlinear operation units\n\n\nWeight operations\nHigh-bandwidth memory access\n\n\nParallel computation\nSpecialized parallel processors\n\n\nWeight storage\nLarge-scale memory systems\n\n\nLearning algorithms\nGradient computation hardware\n\n\n\n\n\n\nStorage architecture represents a critical requirement, driven by the fundamental difference in how biological and artificial systems handle memory. In biological systems, memory and processing are intrinsically integrated—synapses both store connection strengths and process signals. Artificial systems, however, must maintain a clear separation between processing units and memory. This creates a need for both high-capacity storage to hold millions or billions of connection weights and high-bandwidth pathways to move this data quickly between storage and processing units. The efficiency of this data movement often becomes a critical bottleneck that biological systems do not face.\nThe learning process itself imposes distinct requirements on artificial systems. While biological networks modify synaptic strengths through local chemical processes, artificial networks must coordinate weight updates across the entire network. This creates substantial computational and memory demands during training—systems must not only store current weights but also maintain space for gradients and intermediate calculations. The requirement to backpropagate error signals, with no real biological analog, further complicates the system architecture.\nEnergy efficiency emerges as a final critical requirement, highlighting perhaps the starkest contrast between biological and artificial implementations. The human brain’s remarkable energy efficiency—operating on roughly 20 watts—stands in sharp contrast to the substantial power demands of artificial neural networks. Current systems often require orders of magnitude more energy to implement similar capabilities. This gap drives ongoing research in more efficient hardware architectures and has profound implications for the practical deployment of neural networks, particularly in resource-constrained environments like mobile devices or edge computing systems.\n\n\n3.3.6 Evolution and Impact\nWe can now better appreciate how the field of deep learning evolved to meet these challenges through advances in hardware and algorithms. This journey began with early artificial neural networks in the 1950s, marked by the introduction of the Perceptron. While groundbreaking in concept, these early systems were severely limited by the computational capabilities of their era—primarily mainframe computers that lacked both the processing power and memory capacity needed for complex networks.\nThe development of backpropagation algorithms in the 1980s (Rumelhart, Hinton, and Williams 1986), which we will learn about later, represented a theoretical breakthrough and povided a systematic way to train multi-layer networks. However, the computational demands of this algorithm far exceeded available hardware capabilities. Training even modest networks could take weeks, making experimentation and practical applications challenging. This mismatch between algorithmic requirements and hardware capabilities contributed to a period of reduced interest in neural networks.\n\nRumelhart, David E., Geoffrey E. Hinton, and Ronald J. Williams. 1986. “Learning Representations by Back-Propagating Errors.” Nature 323 (6088): 533–36. https://doi.org/10.1038/323533a0.\n\n\n\n\n\n\nFigure 3.8: Growth of deep learning models. Source: EOOCHS AI\n\n\n\nThe term “deep learning” gained prominence in the 2010s, coinciding with significant advances in computational power and data accessibility. The field has since experienced exponential growth, as illustrated in Figure 3.8. The graph reveals two remarkable trends: computational capabilities measured in the number of Floating Point Operations per Second (FLOPS) initially followed a 1.4x improvement pattern from 1952 to 2010, then accelerated to a 3.4-month doubling cycle from 2012 to 2022. Perhaps more striking is the emergence of large-scale models between 2015 and 2022 (not explicitly shown or easily seen in the figure), which scaled 2 to 3 orders of magnitude faster than the general trend, following an aggressive 10-month doubling cycle.\nThe evolutionary trends were driven by parallel advances across three fundamental dimensions: data availability, algorithmic innovations, and computing infrastructure. These three factors—data, algorithms, and infrastructure—reinforced each other in a virtuous cycle that continues to drive progress in the field today. As Figure 9.5 shows, more powerful computing infrastructure enabled processing larger datasets. Larger datasets drove algorithmic innovations. Better algorithms demanded more sophisticated computing systems. This virtuous cycle continues to drive progress in the field today.\n\n\n\n\n\n\nFigure 3.9: The virtuous cycle enabled by key breakthroughs in each layer.\n\n\n\nThe data revolution transformed what was possible with neural networks. The rise of the internet and digital devices created unprecedented access to training data. Image sharing platforms provided millions of labeled images. Digital text collections enabled language processing at scale. Sensor networks and IoT devices generated continuous streams of real-world data. This abundance of data provided the raw material needed for neural networks to learn complex patterns effectively.\nAlgorithmic innovations made it possible to harness this data effectively. New methods for initializing networks and controlling learning rates made training more stable. Techniques for preventing overfitting allowed models to generalize better to new data. Most importantly, researchers discovered that neural network performance scaled predictably with model size, computation, and data quantity, leading to increasingly ambitious architectures.\nComputing infrastructure evolved to meet these growing demands. On the hardware side, graphics processing units (GPUs) provided the parallel processing capabilities needed for efficient neural network computation. Specialized AI accelerators like TPUs (Jouppi et al. 2017) pushed performance further. High-bandwidth memory systems and fast interconnects addressed data movement challenges. Equally important were software advances—frameworks and libraries that made it easier to build and train networks, distributed computing systems that enabled training at scale, and tools for optimizing model deployment.\n\nJouppi, Norman P., Cliff Young, Nishant Patil, David Patterson, Gaurav Agrawal, Raminder Bajwa, Sarah Bates, et al. 2017. “In-Datacenter Performance Analysis of a Tensor Processing Unit.” In Proceedings of the 44th Annual International Symposium on Computer Architecture, 1–12. ISCA ’17. New York, NY, USA: ACM. https://doi.org/10.1145/3079856.3080246.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>DL Primer</span>"
    ]
  },
  {
    "objectID": "contents/core/dl_primer/dl_primer.html#neural-network-foundations",
    "href": "contents/core/dl_primer/dl_primer.html#neural-network-foundations",
    "title": "3  DL Primer",
    "section": "3.4 Neural Network Foundations",
    "text": "3.4 Neural Network Foundations\nWe can now examine the fundamental building blocks that make machine learning systems work. While the field has grown tremendously in sophistication, all modern neural networks—from simple classifiers to large language models—share a common architectural foundation built upon basic computational units and principles.\nThis foundation begins with understanding how individual artificial neurons process information, how they are organized into layers, and how these layers are connected to form complete networks. By starting with these fundamental concepts, we can progressively build up to understanding more complex architectures and their applications.\nNeural networks have come a long way since their inception in the 1950s, when the perceptron was first introduced. After a period of decline in popularity due to computational and theoretical limitations, the field saw a resurgence in the 2000s, driven by advancements in hardware (e.g., GPUs) and innovations like deep learning. These breakthroughs have made it possible to train networks with millions of parameters, enabling applications once considered impossible.\n\n3.4.1 Basic Architecture\nThe architecture of a neural network determines how information flows through the system, from input to output. While modern networks can be tremendously complex, they all build upon a few key organizational principles that we will explore in the following sections. Understanding these principles is essential for both implementing neural networks and appreciating how they achieve their remarkable capabilities.\n\nNeurons and Activations\nThe Perceptron is the basic unit or node that forms the foundation for more complex structures. It functions by taking multiple inputs, each representing a feature of the object under analysis, such as the characteristics of a home for predicting its price or the attributes of a song to forecast its popularity in music streaming services. These inputs are denoted as \\(x_1, x_2, ..., x_n\\). A perceptron can be configured to perform either regression or classification tasks. For regression, the actual numerical output \\(\\hat{y}\\) is used. For classification, the output depends on whether \\(\\hat{y}\\) crosses a certain threshold. If \\(\\hat{y}\\) exceeds this threshold, the perceptron might output one class (e.g., ‘yes’), and if it does not, another class (e.g., ‘no’).\nFigure 3.10 illustrates the fundamental building blocks of a perceptron, which serves as the foundation for more complex neural networks. A perceptron can be thought of as a miniature decision-maker, utilizing its weights, bias, and activation function to process inputs and generate outputs based on learned parameters. This concept forms the basis for understanding more intricate neural network architectures, such as multilayer perceptrons.\nIn these advanced structures, layers of perceptrons work in concert, with each layer’s output serving as the input for the subsequent layer. This hierarchical arrangement creates a deep learning model capable of comprehending and modeling complex, abstract patterns within data. By stacking these simple units, neural networks gain the ability to tackle increasingly sophisticated tasks, from image recognition to natural language processing.\n\n\n\n\n\n\nFigure 3.10: Perceptron. Conceived in the 1950s, perceptrons paved the way for developing more intricate neural networks and have been a fundamental building block in deep learning.\n\n\n\nEach input \\(x_i\\) has a corresponding weight \\(w_{ij}\\), and the perceptron simply multiplies each input by its matching weight. This operation is similar to linear regression, where the intermediate output, \\(z\\), is computed as the sum of the products of inputs and their weights: \\[\nz = \\sum (x_i \\cdot w_{ij})\n\\]\nTo this intermediate calculation, a bias term \\(b\\) is added, allowing the model to better fit the data by shifting the linear output function up or down. Thus, the intermediate linear combination computed by the perceptron including the bias becomes: \\[\nz = \\sum (x_i \\cdot w_{ij}) + b\n\\]\nCommon activation functions include:\n\nReLU (Rectified Linear Unit): Defined as \\(f(x) = \\max(0,x)\\), it introduces sparsity and accelerates convergence in deep networks. Its simplicity and effectiveness have made it the default choice in many modern architectures.\nSigmoid: Historically popular, the sigmoid function maps inputs to a range between 0 and 1 but is prone to vanishing gradients in deeper architectures. It’s particularly useful in binary classification problems where probabilities are needed.\nTanh: Similar to sigmoid but maps inputs to a range of \\(-1\\) to 1, centering the data. This centered output often leads to faster convergence in practice compared to sigmoid.\n\n\n\n\n\n\n\nFigure 3.11: Activation functions enable the modeling of complex non-linear relationships. Source: Medium—Sachin Kaushik.\n\n\n\nThese activation functions transform the linear input sum into a non-linear output: \\[\n\\hat{y} = \\sigma(z)\n\\]\nThus, the final output of the perceptron, including the activation function, can be expressed as:\nFigure 3.11 shows an example where data exhibit a nonlinear pattern that could not be adequately modeled with a linear approach. The activation function enables the network to learn and represent complex relationships in the data, making it possible to solve sophisticated tasks like image recognition or speech processing.\nThus, the final output of the perceptron, including the activation function, can be expressed as: \\[\nz = \\sigma\\left(\\sum (x_i \\cdot w_{ij}) + b\\right)\n\\]\n\n\nLayers and Connections\nWhile a single perceptron can model simple decisions, the power of neural networks comes from combining multiple neurons into layers. A layer is a collection of neurons that process information in parallel. Each neuron in a layer operates independently on the same input but with its own set of weights and bias, allowing the layer to learn different features or patterns from the same input data.\nIn a typical neural network, we organize these layers hierarchically:\n\nInput Layer: Receives the raw data features\nHidden Layers: Process and transform the data through multiple stages\nOutput Layer: Produces the final prediction or decision\n\nFigure 3.12 illustrates this layered architecture. When data flows through these layers, each successive layer transforms the representation of the data, gradually building more complex and abstract features. This hierarchical processing is what gives deep neural networks their remarkable ability to learn complex patterns.\n\n\n\n\n\n\nFigure 3.12: Neural network layers. Source: BrunelloN\n\n\n\n\n\nData Flow and Layer Transformations\nAs data flows through the network, it is transformed at each layer (l) to extract meaningful patterns. Each layer combines the input data using learned weights and biases, then applies an activation function to introduce non-linearity. This process can be written mathematically as: \\[\n\\mathbf{z}^{(l)} = \\mathbf{W}^{(l)}\\mathbf{x}^{(l-1)} + \\mathbf{b}^{(l)}\n\\] Where:\n\n\\(\\mathbf{x}^{(l-1)}\\) is the input vector from the previous layer\n\\(\\mathbf{W}^{(l)}\\) is the weight matrix for the current layer\n\\(\\mathbf{b}^{(l)}\\) is the bias vector\n\\(\\mathbf{z}^{(l)}\\) is the pre-activation output\n\nNow that we have covered the basics, Video 3.1 provides a great overview of how neural networks work using handwritten digit recognition. It introduces some new concepts that we will explore in more depth soon, but it serves as an excellent introduction.\n\n\n\n\n\n\nImportant 3.1: Neural Network\n\n\n\n\n\n\n\n\n\n3.4.2 Weights and Biases\n\nWeight Matrices\nWeights in neural networks determine how strongly inputs influence the output of a neuron. While we first discussed weights for a single perceptron, in larger networks, weights are organized into matrices for efficient computation across entire layers. For example, in a layer with \\(n\\) input features and \\(m\\) neurons, the weights form a matrix \\(\\mathbf{W} \\in \\mathbb{R}^{n \\times m}\\). Each column in this matrix represents the weights for a single neuron in the layer. This organization allows the network to process multiple inputs simultaneously, an essential feature for handling real-world data efficiently.\nLet’s consider how this extends our previous perceptron equations to handle multiple neurons simultaneously. For a layer of \\(m\\) neurons, instead of computing each neuron’s output separately: \\[\nz_j = \\sum_{i=1}^n (x_i \\cdot w_{ij}) + b_j\n\\]\nWe can compute all outputs at once using matrix multiplication: \\[\n\\mathbf{z} = \\mathbf{x}^T\\mathbf{W} + \\mathbf{b}\n\\]\nThis matrix organization is more than just mathematical convenience—it reflects how modern neural networks are implemented for efficiency. Each weight \\(w_{ij}\\) represents the strength of the connection between input feature \\(i\\) and neuron \\(j\\) in the layer.\n\n\nConnection Patterns\nIn the simplest and most common case, each neuron in a layer is connected to every neuron in the previous layer, forming what we call a “dense” or “fully-connected” layer. This pattern means that each neuron has the opportunity to learn from all available features from the previous layer.\nFigure 3.13 illustrates these dense connections between layers. For a network with layers of sizes \\((n_1, n_2, n_3)\\), the weight matrices would have dimensions:\n\nBetween first and second layer: \\(\\mathbf{W}^{(1)} \\in \\mathbb{R}^{n_1 \\times n_2}\\)\nBetween second and third layer: \\(\\mathbf{W}^{(2)} \\in \\mathbb{R}^{n_2 \\times n_3}\\)\n\n\n\n\n\n\n\nFigure 3.13: Dense connections between layers in a MLP. Source: J. McCaffrey\n\n\n\n\n\nBias Terms\nEach neuron in a layer also has an associated bias term. While weights determine the relative importance of inputs, biases allow neurons to shift their activation functions. This shifting is crucial for learning, as it gives the network flexibility to fit more complex patterns.\nFor a layer with \\(m\\) neurons, the bias terms form a vector \\(\\mathbf{b} \\in \\mathbb{R}^m\\). When we compute the layer’s output, this bias vector is added to the weighted sum of inputs: \\[\n\\mathbf{z} = \\mathbf{x}^T\\mathbf{W} + \\mathbf{b}\n\\]\nThe bias terms effectively allow each neuron to have a different “threshold” for activation, making the network more expressive.\n\n\nParameter Organization\nThe organization of weights and biases across a neural network follows a systematic pattern. For a network with \\(L\\) layers, we maintain:\n\nA weight matrix \\(\\mathbf{W}^{(l)}\\) for each layer \\(l\\)\nA bias vector \\(\\mathbf{b}^{(l)}\\) for each layer \\(l\\)\nActivation functions \\(f^{(l)}\\) for each layer \\(l\\)\n\nThis gives us the complete layer computation: \\[\n\\mathbf{h}^{(l)} = f^{(l)}(\\mathbf{z}^{(l)}) = f^{(l)}(\\mathbf{h}^{(l-1)T}\\mathbf{W}^{(l)} + \\mathbf{b}^{(l)})\n\\] Where \\(\\mathbf{h}^{(l)}\\) represents the layer’s output after applying the activation function.\n\n\n\n3.4.3 Network Topology\nNetwork topology describes how the basic building blocks we’ve discussed—neurons, layers, and connections—come together to form a complete neural network. We can best understand network topology through a concrete example. Consider the task of recognizing handwritten digits, a classic problem in deep learning using the MNIST1 dataset.\n1 MNIST (Modified National Institute of Standards and Technology) is a large database of handwritten digits that has been widely used to train and test machine learning systems since its creation in 1998. The dataset consists of 60,000 training images and 10,000 testing images, each being a 28×28 pixel grayscale image of a single handwritten digit from 0 to 9.\nBasic Structure\nThe fundamental structure of a neural network consists of three main components: input layer, hidden layers, and output layer. As shown in Figure 3.14, a \\(28\\times 28\\) pixel grayscale image of a handwritten digit must be processed through these layers to produce a classification output.\nThe input layer’s width is directly determined by our data format. As shown in Figure 3.15, for a \\(28\\times 28\\) pixel image, each pixel becomes an input feature, requiring 784 input neurons \\((28\\times 28 = 784)\\). We can think of this either as a 2D grid of pixels or as a flattened vector of 784 values, where each value represents the intensity of one pixel.\nThe output layer’s structure is determined by our task requirements. For digit classification, we use 10 output neurons, one for each possible digit (0-9). When presented with an image, the network produces a value for each output neuron, where higher values indicate greater confidence that the image represents that particular digit.\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 3.14: A neural network topology for classifying MNIST digits, showing how a \\(28\\times 28\\) pixel image is processed. The image on the left shows the original digit, with dimensions labeled. The network on the right shows how each pixel connects to the hidden layers, ultimately producing 10 outputs for digit classification.\n\n\n\n\n\n\n\n\n\n\n\nFigure 3.15: Alternative visualization of the MNIST network topology, showing how the 2D image is flattened into a 784-dimensional vector before being processed by the network. This representation emphasizes how spatial data is transformed into a format suitable for neural network processing.\n\n\n\n\n\n\nBetween these fixed input and output layers, we have flexibility in designing the hidden layer topology. The choice of hidden layer structure—how many layers to use and how wide to make them—represents one of the fundamental design decisions in neural networks. Additional layers increase the network’s depth, allowing it to learn more abstract features through successive transformations. The width of each layer provides capacity for learning different features at each level of abstraction.\nThese basic topological choices have significant implications for both the network’s capabilities and its computational requirements. Each additional layer or neuron increases the number of parameters that must be stored and computed during both training and inference. However, without sufficient depth or width, the network may lack the capacity to learn complex patterns in the data.\n\n\nDesign Trade-offs\nThe design of neural network topology centers on three fundamental decisions: the number of layers (depth), the size of each layer (width), and how these layers connect. Each choice affects both the network’s learning capability and its computational requirements.\nNetwork depth determines the level of abstraction the network can achieve. Each layer transforms its input into a new representation, and stacking multiple layers allows the network to build increasingly complex features. In our MNIST example, a deeper network might first learn to detect edges, then combine these edges into strokes, and finally assemble strokes into complete digit patterns. However, adding layers isn’t always beneficial—deeper networks increase computational cost substantially, can be harder to train due to vanishing gradients, and may require more sophisticated training techniques.\nThe width of each layer—the number of neurons it contains—controls how much information the network can process in parallel at each stage. Wider layers can learn more features simultaneously but require proportionally more parameters and computation. For instance, if a hidden layer is processing edge features in our digit recognition task, its width determines how many different edge patterns it can detect simultaneously.\nA very important consideration in topology design is the total parameter count. For a network with layers of size \\((n_1, n_2, \\ldots, n_L)\\), each pair of adjacent layers \\(l\\) and \\(l+1\\) requires \\(n_l \\times n_{l+1}\\) weight parameters, plus \\(n_{l+1}\\) bias parameters. These parameters must be stored in memory and updated during training, making the parameter count a key constraint in practical applications.\nWhen designing networks, we need to balance learning capacity, computational efficiency, and ease of training. While the basic approach connects every neuron to every neuron in the next layer (fully connected), this isn’t always the most effective strategy. Sometimes, using fewer but more strategic connections—like in convolutional networks—can achieve better results with less computation. Consider our MNIST example—when humans recognize digits, we don’t analyze every pixel independently but look for meaningful patterns like lines and curves. Similarly, we can design our network to focus on local patterns in the image rather than treating each pixel as completely independent.\nAnother important consideration is how information flows through the network. While the basic flow is from input to output, some network designs include additional paths for information to flow, such as skip connections or residual connections. These alternative paths can make the network easier to train and more effective at learning complex patterns. Think of these as shortcuts that help information flow more directly when needed, similar to how our brain can combine both detailed and general impressions when recognizing objects.\nThese design decisions have significant practical implications for memory usage for storing network parameters, computational costs during both training and inference, training behavior and convergence, and the network’s ability to generalize to new examples. The optimal balance of these trade-offs depends heavily on your specific problem, available computational resources, and dataset characteristics. Successful network design requires carefully weighing these factors against practical constraints.\n\n\nConnection Patterns\nNeural networks can be structured with different connection patterns between layers, each offering distinct advantages for learning and computation. Understanding these fundamental patterns provides insight into how networks process information and learn representations from data.\nDense connectivity represents the standard pattern where each neuron connects to every neuron in the subsequent layer. In our MNIST example, connecting our 784-dimensional input layer to a hidden layer of 100 neurons requires 78,400 weight parameters. This full connectivity enables the network to learn arbitrary relationships between inputs and outputs, but the number of parameters scales quadratically with layer width.\nSparse connectivity patterns introduce purposeful restrictions in how neurons connect between layers. Rather than maintaining all possible connections, neurons connect to only a subset of neurons in the adjacent layer. This approach draws inspiration from biological neural systems, where neurons typically form connections with a limited number of other neurons. In visual processing tasks like our MNIST example, neurons might connect only to inputs representing nearby pixels, reflecting the local nature of visual features.\nAs networks grow deeper, the path from input to output becomes longer, potentially complicating the learning process. Skip connections address this by adding direct paths between non-adjacent layers. These connections provide alternative routes for information flow, supplementing the standard layer-by-layer progression. In our digit recognition example, skip connections might allow later layers to reference both high-level patterns and the original pixel values directly.\nThese connection patterns have significant implications for both the theoretical capabilities and practical implementation of neural networks. Dense connections maximize learning flexibility at the cost of computational efficiency. Sparse connections can reduce computational requirements while potentially improving the network’s ability to learn structured patterns. Skip connections help maintain effective information flow in deeper networks.\n\n\nParameters Considerations\nThe arrangement of parameters (weights and biases) in a neural network determines both its learning capacity and computational requirements. While topology defines the network’s structure, the initialization and organization of parameters plays a crucial role in learning and performance.\nParameter count grows with network width and depth. For our MNIST example, consider a network with a 784-dimensional input layer, two hidden layers of 100 neurons each, and a 10-neuron output layer. The first layer requires 78,400 weights and 100 biases, the second layer 10,000 weights and 100 biases, and the output layer 1,000 weights and 10 biases, totaling 89,610 parameters. Each must be stored in memory and updated during learning.\nParameter initialization is fundamental to network behavior. Setting all parameters to zero would cause neurons in a layer to behave identically, preventing diverse feature learning. Instead, weights are typically initialized randomly, while biases often start at small constant values or even zeros. The scale of these initial values matters significantly – too large or too small can lead to poor learning dynamics.\nThe distribution of parameters affects information flow through layers. In digit recognition, if weights are too small, important input details might not propagate to later layers. If too large, the network might amplify noise. Biases help adjust the activation threshold of each neuron, enabling the network to learn optimal decision boundaries.\nDifferent architectures may impose specific constraints on parameter organization. Some share weights across network regions to encode position-invariant pattern recognition. Others might restrict certain weights to zero, implementing sparse connectivity patterns.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>DL Primer</span>"
    ]
  },
  {
    "objectID": "contents/core/dl_primer/dl_primer.html#learning-process",
    "href": "contents/core/dl_primer/dl_primer.html#learning-process",
    "title": "3  DL Primer",
    "section": "3.5 Learning Process",
    "text": "3.5 Learning Process\nNeural networks learn to perform tasks through a process of training on examples. This process transforms the network from its initial state, where its weights are randomly initialized, to a trained state where the weights encode meaningful patterns from the training data. Understanding this process is fundamental to both the theoretical foundations and practical implementations of deep learning systems.\n\n3.5.1 Training Overview\nThe core principle of neural network training is supervised learning from labeled examples. Consider our MNIST digit recognition task: we have a dataset of 60,000 training images, each a 28×28 pixel grayscale image paired with its correct digit label. The network must learn the relationship between these images and their corresponding digits through an iterative process of prediction and weight adjustment.\nTraining operates as a loop, where each iteration involves processing a subset of training examples called a batch. For each batch, the network performs several key operations:\n\nForward computation through the network layers to generate predictions\nEvaluation of prediction accuracy using a loss function\nComputation of weight adjustments based on prediction errors\nUpdate of network weights to improve future predictions\n\nThis process can be expressed mathematically. Given an input image \\(x\\) and its true label \\(y\\), the network computes its prediction: \\[\n\\hat{y} = f(x; \\theta)\n\\]\nwhere \\(f\\) represents the neural network function and \\(\\theta\\) represents all trainable parameters (weights and biases, which we discussed earlier). The network’s error is measured by a loss function \\(L\\): \\[\n\\text{loss} = L(\\hat{y}, y)\n\\]\nThis error measurement drives the adjustment of network parameters through a process called “backpropagation,” which we will examine in detail later.\nIn practice, training operates on batches of examples rather than individual inputs. For the MNIST dataset, each training iteration might process, for example, 32, 64, or 128 images simultaneously. This batch processing serves two purposes: it enables efficient use of modern computing hardware through parallel processing, and it provides more stable parameter updates by averaging errors across multiple examples.\nThe training cycle continues until the network achieves sufficient accuracy or reaches a predetermined number of iterations. Throughout this process, the loss function serves as a guide, with its minimization indicating improved network performance.\n\n\n3.5.2 Forward Propagation\nForward propagation, as illustrated in Figure 3.16, is the core computational process in a neural network, where input data flows through the network’s layers to generate predictions. Understanding this process is essential as it forms the foundation for both network inference and training. Let’s examine how forward propagation works using our MNIST digit recognition example.\n\n\n\n\n\n\nFigure 3.16: Neural networks—forward and backward propagation.\n\n\n\nWhen an image of a handwritten digit enters our network, it undergoes a series of transformations through the layers. Each transformation combines the weighted inputs with learned patterns to progressively extract relevant features. In our MNIST example, a \\(28\\times 28\\) pixel image is processed through multiple layers to ultimately produce probabilities for each possible digit (0-9).\nThe process begins with the input layer, where each pixel’s grayscale value becomes an input feature. For MNIST, this means 784 input values \\((28\\times 28 = 784)\\), each normalized between 0 and 1. These values then propagate forward through the hidden layers, where each neuron combines its inputs according to its learned weights and applies a nonlinear activation function.\n\nLayer-by-Layer Computation\nThe forward computation through a neural network proceeds systematically, with each layer transforming its inputs into increasingly abstract representations. In our MNIST network, this transformation process occurs in distinct stages.\nAt each layer, the computation involves two key steps: a linear transformation of inputs followed by a nonlinear activation. The linear transformation combines all inputs to a neuron using learned weights and a bias term. For a single neuron receiving inputs from the previous layer, this computation takes the form: \\[\nz = \\sum_{i=1}^n w_ix_i + b\n\\] where \\(w_i\\) represents the weights, \\(x_i\\) the inputs, and \\(b\\) the bias term. For an entire layer of neurons, we can express this more efficiently using matrix operations: \\[\n\\mathbf{Z}^{(l)} = \\mathbf{W}^{(l)}\\mathbf{A}^{(l-1)} + \\mathbf{b}^{(l)}\n\\]\nHere, \\(\\mathbf{W}^{(l)}\\) represents the weight matrix for layer \\(l\\), \\(\\mathbf{A}^{(l-1)}\\) contains the activations from the previous layer, and \\(\\mathbf{b}^{(l)}\\) is the bias vector.\nFollowing this linear transformation, each layer applies a nonlinear activation function \\(f\\): \\[\n\\mathbf{A}^{(l)} = f(\\mathbf{Z}^{(l)})\n\\]\nThis process repeats at each layer, creating a chain of transformations:\nInput → Linear Transform → Activation → Linear Transform → Activation → … → Output\nIn our MNIST example, the pixel values first undergo a transformation by the first hidden layer’s weights, converting the 784-dimensional input into an intermediate representation. Each subsequent layer further transforms this representation, ultimately producing a 10-dimensional output vector representing the network’s confidence in each possible digit.\n\n\nMathematical Representation\nThe complete forward propagation process can be expressed as a composition of functions, each representing a layer’s transformation. Let us formalize this mathematically, building on our MNIST example.\nFor a network with \\(L\\) layers, we can express the full forward computation as: \\[\n\\mathbf{A}^{(L)} = f^{(L)}\\Big(\\mathbf{W}^{(L)}f^{(L-1)}\\Big(\\mathbf{W}^{(L-1)}\\cdots\\big(f^{(1)}(\\mathbf{W}^{(1)}\\mathbf{X} + \\mathbf{b}^{(1)})\\big)\\cdots + \\mathbf{b}^{(L-1)}\\Big) + \\mathbf{b}^{(L)}\\Big)\n\\]\nWhile this nested expression captures the complete process, we typically compute it step by step:\n\nFirst layer: \\[\n\\mathbf{Z}^{(1)} = \\mathbf{W}^{(1)}\\mathbf{X} + \\mathbf{b}^{(1)}\n\\] \\[\n\\mathbf{A}^{(1)} = f^{(1)}(\\mathbf{Z}^{(1)})\n\\]\nHidden layers \\((l = 2,\\ldots, L-1)\\): \\[\n\\mathbf{Z}^{(l)} = \\mathbf{W}^{(l)}\\mathbf{A}^{(l-1)} + \\mathbf{b}^{(l)}\n\\] \\[\n\\mathbf{A}^{(l)} = f^{(l)}(\\mathbf{Z}^{(l)})\n\\]\nOutput layer: \\[\n\\mathbf{Z}^{(L)} = \\mathbf{W}^{(L)}\\mathbf{A}^{(L-1)} + \\mathbf{b}^{(L)}\n\\] \\[\n\\mathbf{A}^{(L)} = f^{(L)}(\\mathbf{Z}^{(L)})\n\\]\n\nIn our MNIST example, if we have a batch of B images, the dimensions of these operations are:\n\nInput \\(\\mathbf{X}\\): B × 784\nFirst layer weights \\(\\mathbf{W}^{(1)}\\): n₁ × 784\nHidden layer weights \\(\\mathbf{W}^{(l)}\\): nₗ × n_{l-1}\nOutput layer weights \\(\\mathbf{W}^{(L)}\\): 10 × n_{L-1}\n\n\n\nComputational Process\nTo understand how these mathematical operations translate into actual computation, let’s walk through the forward propagation process for a batch of MNIST images. This process illustrates how data is transformed from raw pixel values to digit predictions.\nConsider a batch of 32 images entering our network. Each image starts as a \\(28\\times 28\\) grid of pixel values, which we flatten into a 784-dimensional vector. For the entire batch, this gives us an input matrix \\(\\mathbf{X}\\) of size \\(32\\times 784\\), where each row represents one image. The values are typically normalized to lie between 0 and 1.\nThe transformation at each layer proceeds as follows:\n\nInput Layer Processing: The network takes our input matrix \\(\\mathbf{X}\\) \\((32\\times 784)\\) and transforms it using the first layer’s weights. If our first hidden layer has 128 neurons, \\(\\mathbf{W}^{(1)}\\) is a \\(784\\times 128\\) matrix. The resulting computation \\(\\mathbf{X}\\mathbf{W}^{(1)}\\) produces a \\(32\\times 128\\) matrix.\nHidden Layer Transformations: Each element in this matrix then has its corresponding bias added and passes through an activation function. For example, with a ReLU activation, any negative values become zero while positive values remain unchanged. This nonlinear transformation enables the network to learn complex patterns in the data.\nOutput Generation: The final layer transforms its inputs into a \\(32\\times 10\\) matrix, where each row contains 10 values corresponding to the network’s confidence scores for each possible digit. Often, these scores are converted to probabilities using a softmax function: \\[\nP(\\text{digit } j) = \\frac{e^{z_j}}{\\sum_{k=1}^{10} e^{z_k}}\n\\]\n\nFor each image in our batch, this gives us a probability distribution over the possible digits. The digit with the highest probability becomes the network’s prediction.\n\n\nPractical Considerations\nThe implementation of forward propagation requires careful attention to several practical aspects that affect both computational efficiency and memory usage. These considerations become particularly important when processing large batches of data or working with deep networks.\nMemory management plays an important role during forward propagation. Each layer’s activations must be stored for potential use in the backward pass during training. For our MNIST example with a batch size of 32, if we have three hidden layers of sizes 128, 256, and 128, the activation storage requirements are:\n\nFirst hidden layer: \\(32\\times 128 = 4,096\\) values\nSecond hidden layer: \\(32\\times 256 = 8,192\\) values\nThird hidden layer: \\(32\\times 128 = 4,096\\) values\nOutput layer: \\(32\\times 10 = 320\\) values\n\nThis gives us a total of 16,704 values that must be maintained in memory for each batch during training. The memory requirements scale linearly with batch size and can become substantial for larger networks.\nBatch processing introduces important trade-offs. Larger batches enable more efficient matrix operations and better hardware utilization but require more memory. For example, doubling the batch size to 64 would double our memory requirements for activations. This relationship between batch size, memory usage, and computational efficiency often guides the choice of batch size in practice.\nThe organization of computations also affects performance. Matrix operations can be optimized through careful memory layout and the use of specialized libraries. The choice of activation functions impacts not only the network’s learning capabilities but also its computational efficiency, as some functions (like ReLU) are less expensive to compute than others (like tanh or sigmoid).\nThese considerations form the foundation for understanding the system requirements of neural networks, which we will explore in more detail in later chapters.\n\n\n\n3.5.3 Loss Functions\nNeural networks learn by measuring and minimizing their prediction errors. Loss functions provide the Algorithmic Structure for quantifying these errors, serving as the essential feedback mechanism that guides the learning process. Through loss functions, we can convert the abstract goal of “making good predictions” into a concrete optimization problem.\nTo understand the role of loss functions, let’s continue with our MNIST digit recognition example. When the network processes a handwritten digit image, it outputs ten numbers representing its confidence in each possible digit (0-9). The loss function measures how far these predictions deviate from the true answer. For instance, if an image shows a “7”, we want high confidence for digit “7” and low confidence for all other digits. The loss function penalizes the network when its prediction differs from this ideal.\nConsider a concrete example: if the network sees an image of “7” and outputs confidences:\n[0.1, 0.1, 0.1, 0.0, 0.0, 0.0, 0.2, 0.3, 0.1, 0.1]\nThe highest confidence (0.3) is assigned to digit “7”, but this confidence is quite low, indicating uncertainty in the prediction. A good loss function would produce a high loss value here, signaling that the network needs significant improvement. Conversely, if the network outputs:\n[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9, 0.0, 0.1]\nThe loss function should produce a lower value, as this prediction is much closer to ideal.\n\nBasic Concepts\nA loss function measures how far the network’s predictions are from the correct answers. This difference is expressed as a single number: a lower loss means the predictions are more accurate, while a higher loss indicates the network needs improvement. During training, the loss function guides the network by helping it adjust its weights to make better predictions. For example, in recognizing handwritten digits, the loss will penalize predictions that assign low confidence to the correct digit.\nMathematically, a loss function \\(L\\) takes two inputs: the network’s predictions \\(\\hat{y}\\) and the true values \\(y\\). For a single training example in our MNIST task: \\[\nL(\\hat{y}, y) = \\text{measure of discrepancy between prediction and truth}\n\\]\nWhen training with batches of data, we typically compute the average loss across all examples in the batch: \\[\nL_{\\text{batch}} = \\frac{1}{B}\\sum_{i=1}^B L(\\hat{y}_i, y_i)\n\\] where \\(B\\) is the batch size and \\((\\hat{y}_i, y_i)\\) represents the prediction and truth for the \\(i\\)-th example.\nThe choice of loss function depends on the type of task. For our MNIST classification problem, we need a loss function that can:\n\nHandle probability distributions over multiple classes\nProvide meaningful gradients for learning\nPenalize wrong predictions effectively\nScale well with batch processing\n\n\n\nCommon Classification Losses\nFor classification tasks like MNIST digit recognition, “cross-entropy” loss has emerged as the standard choice. This loss function is particularly well-suited for comparing predicted probability distributions with true class labels.\nFor a single digit image, our network outputs a probability distribution over the ten possible digits. We represent the true label as a one-hot vector where all entries are 0 except for a 1 at the correct digit’s position. For instance, if the true digit is “7”, the label would be: \\[\ny = \\big[0, 0, 0, 0, 0, 0, 0, 1, 0, 0\\big]\n\\]\nThe cross-entropy loss for this example is: \\[\nL(\\hat{y}, y) = -\\sum_{j=1}^{10} y_j \\log(\\hat{y}_j)\n\\] where \\(\\hat{y}_j\\) represents the network’s predicted probability for digit j. Given our one-hot encoding, this simplifies to: \\[\nL(\\hat{y}, y) = -\\log(\\hat{y}_c)\n\\] where \\(c\\) is the index of the correct class. This means the loss depends only on the predicted probability for the correct digit—the network is penalized based on how confident it is in the right answer.\nFor example, if our network predicts the following probabilities for an image of “7”:\nPredicted: [0.1, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.8, 0.0, 0.1]\nTrue: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0]\nThe loss would be \\(-\\log(0.8)\\), which is approximately 0.223. If the network were more confident and predicted 0.9 for the correct digit, the loss would decrease to approximately 0.105.\n\n\nLoss Computation\nThe practical computation of loss involves considerations for both numerical stability and batch processing. When working with batches of data, we compute the average loss across all examples in the batch.\nFor a batch of B examples, the cross-entropy loss becomes: \\[\nL_{\\text{batch}} = -\\frac{1}{B}\\sum_{i=1}^B \\sum_{j=1}^{10} y_{ij} \\log(\\hat{y}_{ij})\n\\]\nComputing this loss efficiently requires careful consideration of numerical precision. Taking the logarithm of very small probabilities can lead to numerical instability. Consider a case where our network predicts a probability of 0.0001 for the correct class. Computing \\(\\log(0.0001)\\) directly might cause underflow or result in imprecise values.\nTo address this, we typically implement the loss computation with two key modifications:\n\nAdd a small epsilon to prevent taking log of zero: \\[\nL = -\\log(\\hat{y} + \\epsilon)\n\\]\nApply the log-sum-exp trick for numerical stability: \\[\n\\text{softmax}(z_i) = \\frac{\\exp\\big(z_i - \\max(z)\\big)}{\\sum_j \\exp\\big(z_j - \\max(z)\\big)}\n\\]\n\nFor our MNIST example with a batch size of 32, this means:\n\nProcessing 32 sets of 10 probabilities\nComputing 32 individual loss values\nAveraging these values to produce the final batch loss\n\n\n\nTraining Implications\nUnderstanding how loss functions influence training helps explain key implementation decisions in deep learning systems.\nDuring each training iteration, the loss value serves multiple purposes:\n\nPerformance Metric: It quantifies current network accuracy\nOptimization Target: Its gradients guide weight updates\nConvergence Signal: Its trend indicates training progress\n\nFor our MNIST classifier, monitoring the loss during training reveals the network’s learning trajectory. A typical pattern might show:\n\nInitial high loss (\\(\\sim 2.3\\), equivalent to random guessing among 10 classes)\nRapid decrease in early training iterations\nGradual improvement as the network fine-tunes its predictions\nEventually stabilizing at a lower loss (\\(\\sim 0.1\\), indicating confident correct predictions)\n\nThe loss function’s gradients with respect to the network’s outputs provide the initial error signal that drives backpropagation. For cross-entropy loss, these gradients have a particularly simple form: the difference between predicted and true probabilities. This mathematical property makes cross-entropy loss especially suitable for classification tasks, as it provides strong gradients even when predictions are very wrong.\nThe choice of loss function also influences other training decisions:\n\nLearning rate selection (larger loss gradients might require smaller learning rates)\nBatch size (loss averaging across batches affects gradient stability)\nOptimization algorithm behavior\nConvergence criteria\n\n\n\n\n3.5.4 Backward Propagation\nBackward propagation, often called backpropagation, is the algorithmic cornerstone of neural network training. While forward propagation computes predictions, backward propagation determines how to adjust the network’s weights to improve these predictions. This process enables neural networks to learn from their mistakes.\nTo understand backward propagation, let’s continue with our MNIST example. When the network predicts a “3” for an image of “7”, we need a systematic way to adjust weights throughout the network to make this mistake less likely in the future. Backward propagation provides this by calculating how each weight contributed to the error.\nThe process begins at the network’s output, where we compare the predicted digit probabilities with the true label. This error then flows backward through the network, with each layer’s weights receiving an update signal based on their contribution to the final prediction. The computation follows the chain rule of calculus, breaking down the complex relationship between weights and final error into manageable steps.\nVideo 3.2 and Video 3.3 give a good high level overview of cost functions help neural networks learn\n\n\n\n\n\n\nImportant 3.2: Gradient descent – Part 1\n\n\n\n\n\n\n\n\n\n\n\n\nImportant 3.3: Gradient descent – Part 2\n\n\n\n\n\n\n\nGradient Flow\nThe flow of gradients through a neural network follows a path opposite to the forward propagation. Starting from the loss at the output layer, gradients propagate backwards, computing how each layer, and ultimately each weight, influenced the final prediction error.\nIn our MNIST example, consider what happens when the network misclassifies a “7” as a “3”. The loss function generates an initial error signal at the output layer—essentially indicating that the probability for “7” should increase while the probability for “3” should decrease. This error signal then propagates backward through the network layers.\nFor a network with L layers, the gradient flow can be expressed mathematically. At each layer l, we compute how the layer’s output affected the final loss: \\[\n\\frac{\\partial L}{\\partial \\mathbf{A}^{(l)}} = \\frac{\\partial L}{\\partial \\mathbf{A}^{(l+1)}} \\frac{\\partial \\mathbf{A}^{(l+1)}}{\\partial \\mathbf{A}^{(l)}}\n\\]\nThis computation cascades backward through the network, with each layer’s gradients depending on the gradients computed in the layer previous to it. The process reveals how each layer’s transformation contributed to the final prediction error. For instance, if certain weights in an early layer strongly influenced a misclassification, they will receive larger gradient values, indicating a need for more substantial adjustment.\nHowever, this process faces important challenges in deep networks. As gradients flow backward through many layers, they can either vanish or explode. When gradients are repeatedly multiplied through many layers, they can become exponentially small, particularly with sigmoid or tanh activation functions. This causes early layers to learn very slowly or not at all, as they receive negligible (vanishing) updates. Conversely, if gradient values are consistently greater than 1, they can grow exponentially, leading to unstable training and destructive weight updates.\n\n\nComputing Gradients\nThe actual computation of gradients involves calculating several partial derivatives at each layer. For each layer, we need to determine how changes in weights, biases, and activations affect the final loss. These computations follow directly from the chain rule of calculus but must be implemented efficiently for practical neural network training.\nAt each layer \\(l\\), we compute three main gradient components:\n\nWeight Gradients: \\[\n\\frac{\\partial L}{\\partial \\mathbf{W}^{(l)}} = \\frac{\\partial L}{\\partial \\mathbf{Z}^{(l)}} {\\mathbf{A}^{(l-1)}}^T\n\\]\nBias Gradients: \\[\n\\frac{\\partial L}{\\partial \\mathbf{b}^{(l)}} = \\frac{\\partial L}{\\partial \\mathbf{Z}^{(l)}}\n\\]\nInput Gradients (for propagating to previous layer): \\[\n\\frac{\\partial L}{\\partial \\mathbf{A}^{(l-1)}} = {\\mathbf{W}^{(l)}}^T \\frac{\\partial L}{\\partial \\mathbf{Z}^{(l)}}\n\\]\n\nIn our MNIST example, consider the final layer where the network outputs digit probabilities. If the network predicted \\([0.1, 0.2, 0.5,\\ldots, 0.05]\\) for an image of “7”, the gradient computation would:\n\nStart with the error in these probabilities\nCompute how weight adjustments would affect this error\nPropagate these gradients backward to help adjust earlier layer weights\n\n\n\nImplementation Aspects\nThe practical implementation of backward propagation requires careful consideration of computational resources and memory management. These implementation details significantly impact training efficiency and scalability.\nMemory requirements during backward propagation stem from two main sources. First, we need to store the intermediate activations from the forward pass, as these are required for computing gradients. For our MNIST network with a batch size of 32, each layer’s activations must be maintained:\n\nInput layer: \\(32\\times 784\\) values\nHidden layers: \\(32\\times h\\) values (where \\(h\\) is the layer width)\nOutput layer: \\(32\\times 10\\) values\n\nSecond, we need storage for the gradients themselves. For each layer, we must maintain gradients of similar dimensions to the weights and biases. Taking our previous example of a network with hidden layers of size 128, 256, and 128, this means storing:\n\nFirst layer gradients: \\(784\\times 128\\) values\nSecond layer gradients: \\(128\\times 256\\) values\nThird layer gradients: \\(256\\times 128\\) values\nOutput layer gradients: \\(128\\times 10\\) values\n\nThe computational pattern of backward propagation follows a specific sequence:\n\nCompute gradients at current layer\nUpdate stored gradients\nPropagate error signal to previous layer\nRepeat until input layer is reached\n\nFor batch processing, these computations are performed simultaneously across all examples in the batch, enabling efficient use of matrix operations and parallel processing capabilities.\n\n\n\n3.5.5 Optimization Process\n\nGradient Descent Basics\nThe optimization process adjusts the network’s weights to improve its predictions. Using a method called gradient descent, the network calculates how much each weight contributes to the error and updates it to reduce the loss. This process is repeated over many iterations, gradually refining the network’s ability to make accurate predictions.\nThe fundamental update rule for gradient descent is: \\[\n\\theta_{\\text{new}} = \\theta_{\\text{old}} - \\alpha \\nabla_{\\theta}L\n\\] where \\(\\theta\\) represents any network parameter (weights or biases), \\(\\alpha\\) is the learning rate, and \\(\\nabla_{\\theta}L\\) is the gradient of the loss with respect to that parameter.\nFor our MNIST example, this means adjusting weights to improve digit classification accuracy. If the network frequently confuses “7”s with “1”s, gradient descent will modify the weights to better distinguish between these digits. The learning rate \\(\\alpha\\) controls how large these adjustments are—too large, and the network might overshoot optimal values; too small, and training will progress very slowly.\nVideo 3.4 demonstrates how the backpropagation math works in neural networks for those inclined towards a more theoretical foundation.\n\n\n\n\n\n\nImportant 3.4: Backpropagation\n\n\n\n\n\n\n\n\nBatch Processing\nNeural networks typically process multiple examples simultaneously during training, an approach known as mini-batch gradient descent. Rather than updating weights after each individual image, we compute the average gradient over a batch of examples before performing the update.\nFor a batch of size \\(B\\), the loss gradient becomes: \\[\n\\nabla_{\\theta}L_{\\text{batch}} = \\frac{1}{B}\\sum_{i=1}^B \\nabla_{\\theta}L_i\n\\]\nIn our MNIST training, with a typical batch size of 32, this means:\n\nProcess 32 images through forward propagation\nCompute loss for all 32 predictions\nAverage the gradients across all 32 examples\nUpdate weights using this averaged gradient\n\n\n\nTraining Loop\nThe complete training process combines forward propagation, backward propagation, and weight updates into a systematic training loop. This loop repeats until the network achieves satisfactory performance or reaches a predetermined number of iterations.\nA single pass through the entire training dataset is called an epoch. For MNIST, with 60,000 training images and a batch size of 32, each epoch consists of 1,875 batch iterations. The training loop structure is:\n\nFor each epoch:\n\nShuffle training data to prevent learning order-dependent patterns\nFor each batch:\n\nPerform forward propagation\nCompute loss\nExecute backward propagation\nUpdate weights using gradient descent\n\nEvaluate network performance\n\n\nDuring training, we monitor several key metrics:\n\nTraining loss: average loss over recent batches\nValidation accuracy: performance on held-out test data\nLearning progress: how quickly the network improves\n\nFor our digit recognition task, we might observe the network’s accuracy improve from 10% (random guessing) to over 95% through multiple epochs of training.\n\n\nPractical Considerations\nThe successful implementation of neural network training requires attention to several key practical aspects that significantly impact learning effectiveness. These considerations bridge the gap between theoretical understanding and practical implementation.\nLearning rate selection is perhaps the most critical parameter affecting training. For our MNIST network, the choice of learning rate dramatically influences the training dynamics. A large learning rate of 0.1 might cause unstable training where the loss oscillates or explodes as weight updates overshoot optimal values. Conversely, a very small learning rate of 0.0001 might result in extremely slow convergence, requiring many more epochs to achieve good performance. A moderate learning rate of 0.01 often provides a good balance between training speed and stability, allowing the network to make steady progress while maintaining stable learning.\nConvergence monitoring provides crucial feedback during the training process. As training progresses, we typically observe the loss value stabilizing around a particular value, indicating the network is approaching a local optimum. The validation accuracy often plateaus as well, suggesting the network has extracted most of the learnable patterns from the data. The gap between training and validation performance offers insights into whether the network is overfitting or generalizing well to new examples.\nResource requirements become increasingly important as we scale neural network training. The memory footprint must accommodate both model parameters and the intermediate computations needed for backpropagation. Computation scales linearly with batch size, affecting training speed and hardware utilization. Modern training often leverages GPU acceleration, making efficient use of parallel computing capabilities crucial for practical implementation.\nTraining neural networks also presents several fundamental challenges. Overfitting occurs when the network becomes too specialized to the training data, performing well on seen examples but poorly on new ones. Gradient instability can manifest as either vanishing or exploding gradients, making learning difficult. The interplay between batch size, available memory, and computational resources often requires careful balancing to achieve efficient training while working within hardware constraints.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>DL Primer</span>"
    ]
  },
  {
    "objectID": "contents/core/dl_primer/dl_primer.html#prediction-phase",
    "href": "contents/core/dl_primer/dl_primer.html#prediction-phase",
    "title": "3  DL Primer",
    "section": "3.6 Prediction Phase",
    "text": "3.6 Prediction Phase\nNeural networks serve two distinct purposes: learning from data during training and making predictions during inference. While we’ve explored how networks learn through forward propagation, backward propagation, and weight updates, the prediction phase operates differently. During inference, networks use their learned parameters to transform inputs into outputs without the need for learning mechanisms. This simpler computational process still requires careful consideration of how data flows through the network and how system resources are utilized. Understanding the prediction phase is crucial as it represents how neural networks are actually deployed to solve real-world problems, from classifying images to generating text predictions.\n\n3.6.1 Inference Fundamentals\n\nTraining vs Inference\nThe computation flow fundamentally changes when moving from training to inference. While training requires both forward and backward passes through the network to compute gradients and update weights, inference involves only the forward pass computation. This simpler flow means that each layer needs to perform only one set of operations—transforming inputs to outputs using the learned weights—rather than tracking intermediate values for gradient computation.\nParameter freezing is another another major distinction between training and inference phases. During training, weights and biases continuously update to minimize the loss function. In inference, these parameters remain fixed, acting as static transformations learned from the training data. This freezing of parameters not only simplifies computation but also enables optimizations impossible during training, such as weight quantization or pruning.\nThe structural difference between training loops and inference passes significantly impacts system design. Training operates in an iterative loop, processing multiple batches of data repeatedly across many epochs to refine the network’s parameters. Inference, in contrast, typically processes each input just once, generating predictions in a single forward pass. This fundamental shift from iterative refinement to single-pass prediction influences how we architect systems for deployment.\nMemory and computation requirements differ substantially between training and inference. Training demands considerable memory to store intermediate activations for backpropagation, gradients for weight updates, and optimization states. Inference eliminates these memory-intensive requirements, needing only enough memory to store the model parameters and compute a single forward pass. This reduction in memory footprint, coupled with simpler computation patterns, enables inference to run efficiently on a broader range of devices, from powerful servers to resource-constrained edge devices.\nIn general, the training phase requires more computational resources and memory for learning, while inference is streamlined for efficient prediction. Table 3.5 summarizes the key differences between training and inference.\n\n\n\nTable 3.5: Key differences between training and inference phases in neural networks.\n\n\n\n\n\n\n\n\n\n\nAspect\nTraining\nInference\n\n\n\n\nComputation Flow\nForward and backward passes, gradient computation\nForward pass only, direct input to output\n\n\nParameters\nContinuously updated weights and biases\nFixed/frozen weights and biases\n\n\nProcessing Pattern\nIterative loops over multiple epochs\nSingle pass through the network\n\n\nMemory Requirements\nHigh – stores activations, gradients, optimizer state\nLower– stores only model | parameters and current input\n\n\nComputational Needs\nHeavy – gradient updates, backpropagation\nLighter – matrix multiplication only\n\n\nHardware Requirements\nGPUs/specialized hardware for efficient training\nCan run on simpler devices, including mobile/edge\n\n\n\n\n\n\nThis stark contrast between training and inference phases highlights why system architectures often differ significantly between development and deployment environments. While training requires substantial computational resources and specialized hardware, inference can be optimized for efficiency and deployed across a broader range of devices.\n\n\nBasic Pipeline\nThe implementation of neural networks in practical applications requires a complete processing pipeline that extends beyond the network itself. This pipeline, which is illustrated in Figure 3.17 transforms raw inputs into meaningful outputs through a series of distinct stages, each essential for the system’s operation. Understanding this complete pipeline provides critical insights into the design and deployment of machine learning systems.\n\n\n\n\n\n\nFigure 3.17: End-to-end workflow for the inference prediction phase.\n\n\n\nThe key thing to notice from the figure is that machine learning systems operate as hybrid architectures that combine conventional computing operations with neural network computations. The neural network component, focused on learned transformations through matrix operations, represents just one element within a broader computational framework. This framework encompasses both the preparation of input data and the interpretation of network outputs, processes that rely primarily on traditional computing methods.\nConsider how data flows through the pipeline in Figure 3.17:\n\nRaw inputs arrive in their original form, which might be images, text, sensor readings, or other data types\nPre-processing transforms these inputs into a format suitable for neural network consumption\nThe neural network performs its learned transformations\nRaw outputs emerge from the network, often in numerical form\nPost-processing converts these outputs into meaningful, actionable results\n\nThis pipeline structure reveals several fundamental characteristics of machine learning systems. The neural network, despite its computational sophistication, functions as a component within a larger system. Performance bottlenecks may arise at any stage of the pipeline, not exclusively within the neural network computation. System optimization must therefore consider the entire pipeline rather than focusing solely on the neural network’s operation.\nThe hybrid nature of this architecture has significant implications for system implementation. While neural network computations may benefit from specialized hardware accelerators, pre- and post-processing operations typically execute on conventional processors. This distribution of computation across heterogeneous hardware resources represents a fundamental consideration in system design.\n\n\n\n3.6.2 Pre-processing\nThe pre-processing stage transforms raw inputs into a format suitable for neural network computation. While often overlooked in theoretical discussions, this stage forms a critical bridge between real-world data and neural network operations. Consider our MNIST digit recognition example: before a handwritten digit image can be processed by the neural network we designed earlier, it must undergo several transformations. Raw images of handwritten digits arrive in various formats, sizes, and pixel value ranges. For instance, in Figure 3.18, we see that the digits are all of different sizes, and even the number 6 is written differently by the same person.\n\n\n\n\n\n\nFigure 3.18: Images of handwritten digits. Source: O. Augereau\n\n\n\nThe pre-processing stage standardizes these inputs through conventional computing operations:\n\nImage scaling to the required \\(28\\times 28\\) pixel dimensions, camera images are usually large(r).\nPixel value normalization from \\([0,255]\\) to \\([0,1]\\), most cameras generate colored images.\nFlattening the 2D image array into a 784-dimensional vector, preparing it for the neural network.\nBasic validation to ensure data integrity, making sure the network predicted correctly.\n\nWhat distinguishes pre-processing from neural network computation is its reliance on traditional computing operations rather than learned transformations. While the neural network learns to recognize digits through training, pre-processing operations remain fixed, deterministic transformations. This distinction has important system implications: pre-processing operates on conventional CPUs rather than specialized neural network hardware, and its performance characteristics follow traditional computing patterns.\nThe effectiveness of pre-processing directly impacts system performance. Poor normalization can lead to reduced accuracy, inconsistent scaling can introduce artifacts, and inefficient implementation can create bottlenecks. Understanding these implications helps in designing robust machine learning systems that perform well in real-world conditions.\n\n\n3.6.3 Inference\nThe inference phase represents the operational state of a neural network, where learned parameters are used to transform inputs into predictions. Unlike the training phase we discussed earlier, inference focuses solely on forward computation with fixed parameters.\n\nNetwork Initialization\nBefore processing any inputs, the neural network must be properly initialized for inference. This initialization phase involves loading the model parameters learned during training into memory. For our MNIST digit recognition network, this means loading specific weight matrices and bias vectors for each layer. Let’s examine the exact memory requirements for our architecture:\n\nInput to first hidden layer:\n\nWeight matrix: \\(784\\times 100 = 78,400\\) parameters\nBias vector: 100 parameters\n\nFirst to second hidden layer:\n\nWeight matrix: \\(100\\times 100 = 10,000\\) parameters\nBias vector: 100 parameters\n\nSecond hidden layer to output:\n\nWeight matrix: \\(100\\times 10 = 1,000\\) parameters\nBias vector: 10 parameters\n\n\nIn total, the network requires storage for 89,610 learned parameters (89,400 weights plus 210 biases). Beyond these fixed parameters, memory must also be allocated for intermediate activations during forward computation. For processing a single image, this means allocating space for:\n\nFirst hidden layer activations: 100 values\nSecond hidden layer activations: 100 values\nOutput layer activations: 10 values\n\nThis memory allocation pattern differs significantly from training, where additional memory was needed for gradients, optimizer states, and backpropagation computations.\n\n\nForward Pass Computation\nDuring inference, data propagates through the network’s layers using the initialized parameters. This forward propagation process, while similar in structure to its training counterpart, operates with different computational constraints and optimizations. The computation follows a deterministic path from input to output, transforming the data at each layer using learned parameters.\nFor our MNIST digit recognition network, consider the precise computations at each layer. The network processes a pre-processed image represented as a 784-dimensional vector through successive transformations:\n\nFirst Hidden Layer Computation:\n\nInput transformation: 784 inputs combine with 78,400 weights through matrix multiplication\nLinear computation: \\(\\mathbf{z}^{(1)} = \\mathbf{x}\\mathbf{W}^{(1)} + \\mathbf{b}^{(1)}\\)\nActivation: \\(\\mathbf{a}^{(1)} = \\text{ReLU}(\\mathbf{z}^{(1)})\\)\nOutput: 100-dimensional activation vector\n\nSecond Hidden Layer Computation:\n\nInput transformation: 100 values combine with 10,000 weights\nLinear computation: \\(\\mathbf{z}^{(2)} = \\mathbf{a}^{(1)}\\mathbf{W}^{(2)} + \\mathbf{b}^{(2)}\\)\nActivation: \\(\\mathbf{a}^{(2)} = \\text{ReLU}(\\mathbf{z}^{(2)})\\)\nOutput: 100-dimensional activation vector\n\nOutput Layer Computation:\n\nFinal transformation: 100 values combine with 1,000 weights\nLinear computation: \\(\\mathbf{z}^{(3)} = \\mathbf{a}^{(2)}\\mathbf{W}^{(3)} + \\mathbf{b}^{(3)}\\)\nActivation: \\(\\mathbf{a}^{(3)} = \\text{softmax}(\\mathbf{z}^{(3)})\\)\nOutput: 10 probability values\n\n\nTable 3.6 shows how these computations, while mathematically identical to training-time forward propagation, show important operational differences:\n\n\n\nTable 3.6: Operational characteristics of forward pass computation during training versus inference\n\n\n\n\n\n\n\n\n\n\nCharacteristic\nTraining Forward Pass\nInference Forward Pass\n\n\n\n\nActivation Storage\nMaintains complete activation history for backpropagation\nRetains only current layer activations\n\n\nMemory Pattern\nPreserves intermediate states throughout forward pass\nReleases memory after layer computation completes\n\n\nComputational Flow\nStructured for gradient computation preparation\nOptimized for direct output generation\n\n\nResource Profile\nHigher memory requirements for training operations\nMinimized memory footprint for efficient execution\n\n\n\n\n\n\nThis streamlined computation pattern enables efficient inference while maintaining the network’s learned capabilities. The reduction in memory requirements and simplified computational flow make inference particularly suitable for deployment in resource-constrained environments, such as Mobile ML and Tiny ML.\n\n\nResource Requirements\nNeural networks consume computational resources differently during inference compared to training. During inference, resource utilization focuses primarily on efficient forward pass computation and minimal memory overhead. Let’s examine the specific requirements for our MNIST digit recognition network.\nMemory requirements during inference can be precisely quantified:\n\nStatic Memory (Model Parameters):\n\nLayer 1: 78,400 weights + 100 biases\nLayer 2: 10,000 weights + 100 biases\nLayer 3: 1,000 weights + 10 biases\nTotal: 89,610 parameters (\\(\\approx 358.44\\) KB at 32-bit floating point precision)\n\nDynamic Memory (Activations):\n\nLayer 1 output: 100 values\nLayer 2 output: 100 values\nLayer 3 output: 10 values\nTotal: 210 values (\\(\\approx 0.84\\) KB at 32-bit floating point precision)\n\n\nComputational requirements follow a fixed pattern for each input:\n\nFirst layer: 78,400 multiply-adds\nSecond layer: 10,000 multiply-adds\nOutput layer: 1,000 multiply-adds\nTotal: 89,400 multiply-add operations per inference\n\nThis resource profile stands in stark contrast to training requirements, where additional memory for gradients and computational overhead for backpropagation significantly increase resource demands. The predictable, streamlined nature of inference computations enables various optimization opportunities and efficient hardware utilization.\n\n\nOptimization Opportunities\nThe fixed nature of inference computation presents several opportunities for optimization that are not available during training. Once a neural network’s parameters are frozen, the predictable pattern of computation allows for systematic improvements in both memory usage and computational efficiency.\nBatch size selection represents a fundamental trade-off in inference optimization. During training, large batches were necessary for stable gradient computation, but inference offers more flexibility. Processing single inputs minimizes latency, making it ideal for real-time applications where immediate responses are crucial. However, batch processing can significantly improve throughput by better utilizing parallel computing capabilities, particularly on GPUs. For our MNIST network, consider the memory implications: processing a single image requires storing 210 activation values, while a batch of 32 images requires 6,720 activation values but can process images up to 32 times faster on parallel hardware.\nMemory management during inference can be significantly more efficient than during training. Since intermediate values are only needed for forward computation, memory buffers can be carefully managed and reused. The activation values from each layer need only exist until the next layer’s computation is complete. This enables in-place operations where possible, reducing the total memory footprint. Furthermore, the fixed nature of inference allows for precise memory alignment and access patterns optimized for the underlying hardware architecture.\nHardware-specific optimizations become particularly important during inference. On CPUs, computations can be organized to maximize cache utilization and take advantage of SIMD (Single Instruction, Multiple Data) capabilities. GPU deployments benefit from optimized matrix multiplication routines and efficient memory transfer patterns. These optimizations extend beyond pure computational efficiency—they can significantly impact power consumption and hardware utilization, critical factors in real-world deployments.\nThe predictable nature of inference also enables more aggressive optimizations like reduced numerical precision. While training typically requires 32-bit floating-point precision to maintain stable gradient computation, inference can often operate with 16-bit or even 8-bit precision while maintaining acceptable accuracy. For our MNIST network, this could reduce the memory footprint from 358.44 KB to 179.22 KB or even 89.61 KB, with corresponding improvements in computational efficiency.\nThese optimization principles, while illustrated through our simple MNIST feedforward network, represent only the foundation of neural network optimization. More sophisticated architectures introduce additional considerations and opportunities. Convolutional Neural Networks (CNNs), for instance, present unique optimization opportunities in handling spatial data and filter operations. Recurrent Neural Networks (RNNs) require careful consideration of sequential computation and state management. Transformer architectures introduce distinct patterns of attention computation and memory access. These architectural variations and their optimizations will be explored in detail in subsequent chapters, particularly when we discuss deep learning architectures, model optimizations, and efficient AI implementations.\n\n\n\n3.6.4 Post-processing\nThe transformation of neural network outputs into actionable predictions requires a return to traditional computing paradigms. Just as pre-processing bridges real-world data to neural computation, post-processing bridges neural outputs back to conventional computing systems. This completes the hybrid computing pipeline we examined earlier, where neural and traditional computing operations work in concert to solve real-world problems.\nThe complexity of post-processing extends beyond simple mathematical transformations. Real-world systems must handle uncertainty, validate outputs, and integrate with larger computing systems. In our MNIST example, a digit recognition system might require not just the most likely digit, but also confidence measures to determine when human intervention is needed. This introduces additional computational steps: confidence thresholds, secondary prediction checks, and error handling logic—all of which are implemented in traditional computing frameworks.\nThe computational requirements of post-processing differ significantly from neural network inference. While inference benefits from parallel processing and specialized hardware, post-processing typically runs on conventional CPUs and follows sequential logic patterns. This return to traditional computing brings both advantages and constraints. Operations are more flexible and easier to modify than neural computations, but they may become bottlenecks if not carefully implemented. For instance, computing softmax probabilities for a batch of predictions requires different optimization strategies than the matrix multiplications of neural network layers.\nSystem integration considerations often dominate post-processing design. Output formats must match downstream system requirements, error handling must align with broader system protocols, and performance must meet system-level constraints. In a complete mail sorting system, the post-processing stage must not only identify digits but also format these predictions for the sorting machinery, handle uncertainty cases appropriately, and maintain processing speeds that match physical mail flow rates.\nThis return to traditional computing paradigms completes the hybrid nature of machine learning systems. Just as pre-processing prepared real-world data for neural computation, post-processing adapts neural outputs for real-world use. Understanding this hybrid nature—the interplay between neural and traditional computing— is essential for designing and implementing effective machine learning systems.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>DL Primer</span>"
    ]
  },
  {
    "objectID": "contents/core/dl_primer/dl_primer.html#case-study-usps-postal-service",
    "href": "contents/core/dl_primer/dl_primer.html#case-study-usps-postal-service",
    "title": "3  DL Primer",
    "section": "3.7 Case study: USPS Postal Service",
    "text": "3.7 Case study: USPS Postal Service\n\n3.7.1 Real-world Problem\nThe United States Postal Service (USPS) processes over 100 million pieces of mail daily, each requiring accurate routing based on handwritten ZIP codes. In the early 1990s, this task was primarily performed by human operators, making it one of the largest manual data entry operations in the world. The automation of this process through neural networks represents one of the earliest and most successful large-scale deployments of artificial intelligence, embodying many of the principles we’ve explored in this chapter.\nConsider the complexity of this task: a ZIP code recognition system must process images of handwritten digits captured under varying conditions—different writing styles, pen types, paper colors, and environmental factors. It must make accurate predictions within milliseconds to maintain mail processing speeds. Furthermore, errors in recognition can lead to significant delays and costs from misrouted mail. This real-world constraint meant the system needed not just high accuracy, but also reliable measures of prediction confidence to identify when human intervention was necessary.\nThis challenging environment presented requirements spanning every aspect of neural network implementation we’ve discussed—from biological inspiration to practical deployment considerations. The success or failure of the system would depend not just on the neural network’s accuracy, but on the entire pipeline from image capture through to final sorting decisions.\n\n\n3.7.2 System Development\nThe development of the USPS digit recognition system required careful consideration at every stage, from data collection to deployment. This process illustrates how theoretical principles of neural networks translate into practical engineering decisions.\nData collection presented the first major challenge. Unlike controlled laboratory environments, postal facilities needed to process mail pieces with tremendous variety. The training dataset had to capture this diversity. Digits written by people of different ages, educational backgrounds, and writing styles formed just part of the challenge. Envelopes came in varying colors and textures, and images were captured under different lighting conditions and orientations. This extensive data collection effort later contributed to the creation of the MNIST database we’ve used in our examples.\nThe network architecture design required balancing multiple constraints. While deeper networks might achieve higher accuracy, they would also increase processing time and computational requirements. Processing 28×28 pixel images of individual digits needed to complete within strict time constraints while running reliably on available hardware. The network had to maintain consistent accuracy across varying conditions, from well-written digits to hurried scrawls.\nTraining the network introduced additional complexity. The system needed to achieve high accuracy not just on a test dataset, but on the endless variety of real-world handwriting styles. Careful preprocessing normalized input images to account for variations in size and orientation. Data augmentation techniques increased the variety of training samples. The team validated performance across different demographic groups and tested under actual operating conditions to ensure robust performance.\nThe engineering team faced a critical decision regarding confidence thresholds. Setting these thresholds too high would route too many pieces to human operators, defeating the purpose of automation. Setting them too low would risk delivery errors. The solution emerged from analyzing the confidence distributions of correct versus incorrect predictions. This analysis established thresholds that optimized the tradeoff between automation rate and error rate, ensuring efficient operation while maintaining acceptable accuracy.\n\n\n3.7.3 Complete Pipeline\nFollowing a single piece of mail through the USPS recognition system illustrates how the concepts we’ve discussed integrate into a complete solution. The journey from physical mail piece to sorted letter demonstrates the interplay between traditional computing, neural network inference, and physical machinery.\nThe process begins when an envelope reaches the imaging station. High-speed cameras capture the ZIP code region at rates exceeding several pieces of mail (e.g. 10) pieces per second. This image acquisition process must adapt to varying envelope colors, handwriting styles, and environmental conditions. The system must maintain consistent image quality despite the speed of operation—motion blur and proper illumination present significant engineering challenges.\nPre-processing transforms these raw camera images into a format suitable for neural network analysis. The system must locate the ZIP code region, segment individual digits, and normalize each digit image. This stage employs traditional computer vision techniques: image thresholding adapts to envelope background color, connected component analysis identifies individual digits, and size normalization produces standard 28×28 pixel images. Speed remains critical—these operations must complete within milliseconds to maintain throughput.\nThe neural network then processes each normalized digit image. The trained network, with its 89,610 parameters (as we detailed earlier), performs forward propagation to generate predictions. Each digit passes through two hidden layers of 100 neurons each, ultimately producing ten output values representing digit probabilities. This inference process, while computationally intensive, benefits from the optimizations we discussed in the previous section.\nPost-processing converts these neural network outputs into sorting decisions. The system applies confidence thresholds to each digit prediction. A complete ZIP code requires high confidence in all five digits—a single uncertain digit flags the entire piece for human review. When confidence meets thresholds, the system transmits sorting instructions to mechanical systems that physically direct the mail piece to its appropriate bin.\nThe entire pipeline operates under strict timing constraints. From image capture to sorting decision, processing must complete before the mail piece reaches its sorting point. The system maintains multiple pieces in various pipeline stages simultaneously, requiring careful synchronization between computing and mechanical systems. This real-time operation illustrates why the optimizations we discussed in inference and post-processing become crucial in practical applications.\n\n\n3.7.4 Results and Impact\nThe implementation of neural network-based ZIP code recognition transformed USPS mail processing operations. By 2000, several facilities across the country utilized this technology, processing millions of mail pieces daily. This real-world deployment demonstrated both the potential and limitations of neural network systems in mission-critical applications.\nPerformance metrics revealed interesting patterns that validate many of the principles discussed earlier in this chapter. The system achieved its highest accuracy on clearly written digits, similar to those in the training data. However, performance varied significantly with real-world factors. Lighting conditions affected pre-processing effectiveness. Unusual writing styles occasionally confused the neural network. Environmental vibrations could also impact image quality. These challenges led to continuous refinements in both the physical system and the neural network pipeline.\nThe economic impact proved substantial. Prior to automation, manual sorting required operators to read and key in ZIP codes at an average rate of one piece per second. The neural network system processed pieces at ten times this rate while reducing labor costs and error rates. However, the system didn’t eliminate human operators entirely—their role shifted to handling uncertain cases and maintaining system performance. This hybrid approach, combining artificial and human intelligence, became a model for other automation projects.\nThe system also revealed important lessons about deploying neural networks in production environments. Training data quality proved crucial—the network performed best on digit styles well-represented in its training set. Regular retraining helped adapt to evolving handwriting styles. Maintenance required both hardware specialists and machine learning experts, introducing new operational considerations. These insights influenced subsequent deployments of neural networks in other industrial applications.\nPerhaps most importantly, this implementation demonstrated how theoretical principles translate into practical constraints. The biological inspiration of neural networks provided the foundation for digit recognition, but successful deployment required careful consideration of system-level factors: processing speed, error handling, maintenance requirements, and integration with existing infrastructure. These lessons continue to inform modern machine learning deployments, where similar challenges of scale, reliability, and integration persist.\n\n\n3.7.5 Takeaway\nThe USPS ZIP code recognition system is an excellent example of the journey from biological inspiration to practical neural network deployment that we’ve explored throughout this chapter. It demonstrates how the basic principles of neural computation—from pre-processing through inference to post-processing—come together in solving real-world problems.\nThe system’s development shows why understanding both the theoretical foundations and practical considerations is crucial. While the biological visual system processes handwritten digits effortlessly, translating this capability into an artificial system required careful consideration of network architecture, training procedures, and system integration.\nThe success of this early large-scale neural network deployment helped establish many practices we now consider standard: the importance of comprehensive training data, the need for confidence metrics, the role of pre- and post-processing, and the critical nature of system-level optimization.\nAs we move forward to explore more complex architectures and applications in subsequent chapters, this case study reminds us that successful deployment requires mastery of both fundamental principles and practical engineering considerations.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>DL Primer</span>"
    ]
  },
  {
    "objectID": "contents/core/dl_primer/dl_primer.html#conclusion",
    "href": "contents/core/dl_primer/dl_primer.html#conclusion",
    "title": "3  DL Primer",
    "section": "3.8 Conclusion",
    "text": "3.8 Conclusion\nIn this chapter, we explored the foundational concepts of neural networks, bridging the gap between biological inspiration and artificial implementation. We began by examining the remarkable efficiency and adaptability of the human brain, uncovering how its principles influence the design of artificial neurons. From there, we delved into the behavior of a single artificial neuron, breaking down its components and operations. This understanding laid the groundwork for constructing neural networks, where layers of interconnected neurons collaborate to tackle increasingly complex tasks.\nThe progression from single neurons to network-wide behavior underscored the power of hierarchical learning, where each layer extracts and transforms patterns from raw data into meaningful abstractions. We examined both the learning process and the prediction phase, showing how neural networks first refine their performance through training and then deploy that knowledge through inference. The distinction between these phases revealed important system-level considerations for practical implementations.\nOur exploration of the complete processing pipeline—from pre-processing through inference to post-processing—highlighted the hybrid nature of machine learning systems, where traditional computing and neural computation work together. The USPS case study demonstrated how these theoretical principles translate into practical applications, revealing both the power and complexity of deployed neural networks. These real-world considerations, from data collection to system integration, form an essential part of understanding machine learning systems.\nIn the next chapter, we will expand on these ideas, exploring sophisticated deep learning architectures such as convolutional and recurrent neural networks. These architectures are tailored to process diverse data types, from images and text to time series, enabling breakthroughs across a wide range of applications. By building on the concepts introduced here, we will gain a deeper appreciation for the design, capabilities, and versatility of modern deep learning systems.\n\n\n\n\n\n\nSlides\n\n\n\n\n\nThese slides are a valuable tool for instructors to deliver lectures and for students to review the material at their own pace. We encourage students and instructors to leverage these slides to improve their understanding and facilitate effective knowledge transfer.\n\nPast, Present, and Future of ML.\nThinking About Loss.\nMinimizing Loss.\nFirst Neural Network.\nUnderstanding Neurons.\nIntro to CLassification.\nTraining, Validation, and Test Data.\nIntro to Convolutions.\n\n\n\n\n\n\n\n\n\n\nVideos\n\n\n\n\n\n\nVideo 3.1\nVideo 3.2\nVideo 3.3\nVideo 3.4\n\n\n\n\n\n\n\n\n\n\nExercises\n\n\n\n\n\nTo reinforce the concepts covered in this chapter, we have curated a set of exercises that challenge students to apply their knowledge and deepen their understanding.\nComing soon.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>DL Primer</span>"
    ]
  },
  {
    "objectID": "contents/core/dnn_architectures/dnn_architectures.html",
    "href": "contents/core/dnn_architectures/dnn_architectures.html",
    "title": "4  DNN Architectures",
    "section": "",
    "text": "Purpose\nResources: Slides, Videos, Exercises\nWhat recurring patterns emerge across modern deep learning architectures, and how do these patterns enable systematic approaches to AI system design?\nDeep learning architectures represent a convergence of computational patterns that form the building blocks of modern AI systems. These foundational patterns — from convolutional structures to attention mechanisms — reveal how complex models arise from simple, repeatable components. The examination of these architectural elements provides insights into the systematic construction of flexible, efficient AI systems, establishing core principles that influence every aspect of system design and deployment. These structural insights illuminate the path toward creating scalable, adaptable solutions across diverse application domains.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>DNN Architectures</span>"
    ]
  },
  {
    "objectID": "contents/core/dnn_architectures/dnn_architectures.html#purpose",
    "href": "contents/core/dnn_architectures/dnn_architectures.html#purpose",
    "title": "4  DNN Architectures",
    "section": "",
    "text": "Learning Objectives\n\n\n\n\nMap fundamental neural network concepts to deep learning architectures (dense, spatial, temporal, attention-based).\nAnalyze how architectural patterns shape computational and memory demands.\nEvaluate system-level impacts of architectural choices on system attributes.\nCompare architectures’ hardware mapping and identify optimization strategies.\nAssess trade-offs between complexity and system needs for specific applications.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>DNN Architectures</span>"
    ]
  },
  {
    "objectID": "contents/core/dnn_architectures/dnn_architectures.html#overview",
    "href": "contents/core/dnn_architectures/dnn_architectures.html#overview",
    "title": "4  DNN Architectures",
    "section": "4.1 Overview",
    "text": "4.1 Overview\nDeep learning architecture stands for specific representation or organizations of neural network components—the neurons, weights, and connections (as introduced in Chapter 3)—arranged to efficiently process different types of patterns in data. While the previous chapter established the fundamental building blocks of neural networks, in this chapter we examine how these components are structured into architectures that map efficiently to computer systems.\nNeural network architectures have evolved to address specific pattern processing challenges. Whether processing arbitrary feature relationships, exploiting spatial patterns, managing temporal dependencies, or handling dynamic information flow, each architectural pattern emerged from particular computational needs. These architectures, from a computer systems perspective, require an examination of how their computational patterns map to system resources.\nMost often the architectures are discussed in terms of their algorithmic structures (MLPs, CNNs, RNNs, Transformers). However, in this chapter we take a more fundamental approach by examining how their computational patterns map to hardware resources. Each section analyzes how specific pattern processing needs influence algorithmic structure and how these structures map to computer system resources. The implications for computer system design require examining how their computational patterns map to hardware resources. The mapping from algorithmic requirements to computer system design involves several key considerations:\n\nMemory access patterns: How data moves through the memory hierarchy\nComputation characteristics: The nature and organization of arithmetic operations\nData movement: Requirements for on-chip and off-chip data transfer\nResource utilization: How computational and memory resources are allocated\n\nFor example, dense connectivity patterns generate different memory bandwidth demands than localized processing structures. Similarly, stateful processing creates distinct requirements for on-chip memory organization compared to stateless operations. Getting a firm grasp on these mappings is important for modern computer architects and system designers who must implement these algorithms efficiently in hardware.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>DNN Architectures</span>"
    ]
  },
  {
    "objectID": "contents/core/dnn_architectures/dnn_architectures.html#multi-layer-perceptrons-dense-pattern-processing",
    "href": "contents/core/dnn_architectures/dnn_architectures.html#multi-layer-perceptrons-dense-pattern-processing",
    "title": "4  DNN Architectures",
    "section": "4.2 Multi-Layer Perceptrons: Dense Pattern Processing",
    "text": "4.2 Multi-Layer Perceptrons: Dense Pattern Processing\nMulti-Layer Perceptrons (MLPs) represent the most direct extension of neural networks into deep architectures. Unlike more specialized networks, MLPs process each input element with equal importance, making them versatile but computationally intensive. Their architecture, while simple, establishes fundamental computational patterns that appear throughout deep learning systems. These patterns were initially formalized by the introduction of the Universal Approximation Theorem (UAT) (Cybenko 1992; Hornik, Stinchcombe, and White 1989), which states that a sufficiently large MLP with non-linear activation functions can approximate any continuous function on a compact domain, given suitable weights and biases.\n\nCybenko, G. 1992. “Approximation by Superpositions of a Sigmoidal Function.” Mathematics of Control, Signals, and Systems 5 (4): 455–55. https://doi.org/10.1007/bf02134016.\n\nHornik, Kurt, Maxwell Stinchcombe, and Halbert White. 1989. “Multilayer Feedforward Networks Are Universal Approximators.” Neural Networks 2 (5): 359–66. https://doi.org/10.1016/0893-6080(89)90020-8.\nWhen applied to the MNIST handwritten digit recognition challenge, an MLP reveals its computational power by transforming a complex \\(28\\times 28\\) pixel image into a precise digit classification. By treating each of the 784 pixels as an equally weighted input, the network learns to decompose visual information through a systematic progression of layers, converting raw pixel intensities into increasingly abstract representations that capture the essential characteristics of handwritten digits.\n\n4.2.1 Pattern Processing Needs\nDeep learning systems frequently encounter problems where any input feature could potentially influence any output—there are no inherent constraints on these relationships. Consider analyzing financial market data: any economic indicator might affect any market outcome or in natural language processing, where the meaning of a word could depend on any other word in the sentence. These scenarios demand an architectural pattern capable of learning arbitrary relationships across all input features.\nDense pattern processing addresses this fundamental need by enabling several key capabilities. First, it allows unrestricted feature interactions where each output can depend on any combination of inputs. Second, it facilitates learned feature importance, allowing the system to determine which connections matter rather than having them prescribed. Finally, it provides adaptive representation, enabling the network to reshape its internal representations based on the data.\nFor example, in the MNIST digit recognition task, while humans might focus on specific parts of digits (like loops in ‘6’ or crossings in ‘8’), we cannot definitively say which pixel combinations are important for classification. A ‘7’ written with a serif could share pixel patterns with a ‘2’, while variations in handwriting mean discriminative features might appear anywhere in the image. This uncertainty about feature relationships necessitates a dense processing approach where every pixel can potentially influence the classification decision.\n\n\n4.2.2 Algorithmic Structure\nTo enable unrestricted feature interactions, MLPs implement a direct algorithmic solution: connect everything to everything. This is realized through a series of fully-connected layers, where each neuron connects to every neuron in adjacent layers. The dense connectivity pattern translates mathematically into matrix multiplication operations. As shown in Figure 4.1, each layer transforms its input through matrix multiplication followed by element-wise activation: \\[\n\\mathbf{h}^{(l)} = f\\big(\\mathbf{W}^{(l)}\\mathbf{h}^{(l-1)} + \\mathbf{b}^{(l)}\\big)\n\\]\n\n\n\n\n\n\nFigure 4.1: MLP layers and its associated matrix representation. Source: Reagen et al. (2017)\n\n\nReagen, Brandon, Robert Adolf, Paul Whatmough, Gu-Yeon Wei, and David Brooks. 2017. Deep Learning for Computer Architects. Springer International Publishing. https://doi.org/10.1007/978-3-031-01756-8.\n\n\nThe dimensions of these operations reveal the computational scale of dense pattern processing:\n\nInput vector: \\(\\mathbf{h}^{(0)} \\in \\mathbb{R}^{d_{\\text{in}}}\\) represents all potential input features\nWeight matrices: \\(\\mathbf{W}^{(l)} \\in \\mathbb{R}^{d_{\\text{out}} \\times d_{\\text{in}}}\\) capture all possible input-output relationships\nOutput vector: \\(\\mathbf{h}^{(l)} \\in \\mathbb{R}^{d_{\\text{out}}}\\) produces transformed representations\n\nIn the MNIST example, this means:\n\nEach 784-dimensional input (\\(28\\times 28\\) pixels) connects to every neuron in the first hidden layer\nA hidden layer with 100 neurons requires a \\(784\\times 100\\) weight matrix\nEach weight in this matrix is a learnable relationship between an input pixel and a hidden feature\n\nThis algorithmic structure directly addresses our need for arbitrary feature relationships but creates specific computational patterns that must be handled efficiently by computer systems.\n\n\n4.2.3 Computational Mapping\nThe elegant mathematical representation of dense matrix multiplication maps to specific computational patterns that systems must handle. Let’s examine how this mapping progresses from mathematical abstraction to computational reality.\nThe first implementation, mlp_layer_matrix, directly mirrors our mathematical equation. It uses high-level matrix operations (matmul) to express the computation in a single line, hiding the underlying complexity. This is the style commonly used in deep learning frameworks, where optimized libraries handle the actual computation.\n# Mathematical abstraction in code\ndef mlp_layer_matrix(X, W, b):\n    # X: input matrix (batch_size × num_inputs)\n    # W: weight matrix (num_inputs × num_outputs)\n    # b: bias vector (num_outputs)\n    H = activation(matmul(X, W) + b)    # One clean line of math\n    return H\nThe second implementation, mlp_layer_compute, exposes the actual computational pattern through nested loops. This version shows us what really happens when we compute a layer’s output: we process each sample in the batch, computing each output neuron by accumulating weighted contributions from all inputs.\n# Core computational pattern\ndef mlp_layer_compute(X, W, b):\n    # Process each sample in the batch\n    for batch in range(batch_size):\n        # Compute each output neuron\n        for out in range(num_outputs):\n            # Initialize with bias\n            Z[batch,out] = b[out]\n            # Accumulate weighted inputs\n            for in_ in range(num_inputs):\n                Z[batch,out] += X[batch,in_] * W[in_,out]\n    \n    H = activation(Z)\n    return H\nThis translation from mathematical abstraction to concrete computation exposes how dense matrix multiplication decomposes into nested loops of simpler operations. The outer loop processes each sample in the batch, while the middle loop computes values for each output neuron. Within the innermost loop, the system performs repeated multiply-accumulate operations, combining each input with its corresponding weight.\nIn the MNIST example, each output neuron requires 784 multiply-accumulate operations and at least 1,568 memory accesses (784 for inputs, 784 for weights). While actual implementations use sophisticated optimizations through libraries like BLAS or cuBLAS, these fundamental patterns drive key system design decisions.\n\n\n4.2.4 System Implications\nWhen analyzing how computational patterns impact computer systems, we typically examine three fundamental dimensions: memory requirements, computation needs, and data movement. This framework enables a systematic analysis of how algorithmic patterns influence system design decisions. We will use this framework for analyzing other network architectures, allowing us to compare and contrast their different characteristics.\n\nMemory Requirements\nFor dense pattern processing, the memory requirements stem from storing and accessing weights, inputs, and intermediate results. In our MNIST example, connecting our 784-dimensional input layer to a hidden layer of 100 neurons requires 78,400 weight parameters. Each forward pass must access all these weights, along with input data and intermediate results. The all-to-all connectivity pattern means there’s no inherent locality in these accesses—every output needs every input and its corresponding weights.\nThese memory access patterns suggest opportunities for optimization through careful data organization and reuse. Modern processors handle these patterns differently—CPUs leverage their cache hierarchy for data reuse, while GPUs employ specialized memory hierarchies designed for high-bandwidth access. Deep learning frameworks abstract these hardware-specific details through optimized matrix multiplication implementations.\n\n\nComputation Needs\nThe core computation revolves around multiply-accumulate operations arranged in nested loops. Each output value requires as many multiply-accumulates as there are inputs. For MNIST, this means 784 multiply-ac­cu­mu­lates per output neuron. With 100 neurons in our hidden layer, we’re performing 78,400 multiply-ac­cu­mu­lates for a single input image. While these operations are simple, their volume and arrangement create specific demands on processing resources.\nThis computational structure lends itself to particular optimization strategies in modern hardware. The dense matrix multiplication pattern can be efficiently parallelized across multiple processing units, with each handling different subsets of neurons. Modern hardware accelerators take advantage of this through specialized matrix multiplication units, while deep learning frameworks automatically convert these operations into optimized BLAS (Basic Linear Algebra Subprograms) calls. CPUs and GPUs can both exploit cache locality by carefully tiling the computation to maximize data reuse, though their specific approaches differ based on their architectural strengths.\n\n\nData Movement\nThe all-to-all connectivity pattern in MLPs creates significant data movement requirements. Each multiply-accumulate operation needs three pieces of data: an input value, a weight value, and the running sum. For our MNIST example layer, computing a single output value requires moving 784 inputs and 784 weights to wherever the computation occurs. This movement pattern repeats for each of the 100 output neurons, creating substantial data transfer demands between memory and compute units.\nThe predictable nature of these data movement patterns enables strategic data staging and transfer optimizations. Different architectures address this challenge through various mechanisms—CPUs use sophisticated prefetching and multi-level caches, while GPUs employ high-bandwidth memory systems and latency hiding through massive threading. Deep learning frameworks orchestrate these data movements through optimized memory management systems.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>DNN Architectures</span>"
    ]
  },
  {
    "objectID": "contents/core/dnn_architectures/dnn_architectures.html#convolutional-neural-networks-spatial-pattern-processing",
    "href": "contents/core/dnn_architectures/dnn_architectures.html#convolutional-neural-networks-spatial-pattern-processing",
    "title": "4  DNN Architectures",
    "section": "4.3 Convolutional Neural Networks: Spatial Pattern Processing",
    "text": "4.3 Convolutional Neural Networks: Spatial Pattern Processing\nWhile MLPs treat each input element independently, many real-world data types exhibit strong spatial relationships. Images, for example, derive their meaning from the spatial arrangement of pixels—a pattern of edges and textures that form recognizable objects. Audio signals show temporal patterns of frequency components, and sensor data often contains spatial or temporal correlations. These spatial relationships suggest that treating every input-output connection with equal importance, as MLPs do, might not be the most effective approach.\n\n4.3.1 Pattern Processing Needs\nSpatial pattern processing addresses scenarios where the relationship between data points depends on their relative positions or proximity. Consider processing a natural image: a pixel’s relationship with its neighbors is important for detecting edges, textures, and shapes. These local patterns then combine hierarchically to form more complex features—edges form shapes, shapes form objects, and objects form scenes.\nThis hierarchical spatial pattern processing appears across many domains. In computer vision, local pixel patterns form edges and textures that combine into recognizable objects. Speech processing relies on patterns across nearby time segments to identify phonemes and words. Sensor networks analyze correlations between physically proximate sensors to understand environmental patterns. Medical imaging depends on recognizing tissue patterns that indicate biological structures.\nTaking image processing as an example, if we want to detect a cat in an image, certain spatial patterns must be recognized: the triangular shape of ears, the round contours of the face, the texture of fur. Importantly, these patterns maintain their meaning regardless of where they appear in the image—a cat is still a cat whether it’s in the top-left or bottom-right corner. This suggests two key requirements for spatial pattern processing: the ability to detect local patterns and the ability to recognize these patterns regardless of their position.\nThis leads us to the convolutional neural network architecture (CNN), introduced by Y. LeCun et al. (1989). CNNs address spatial pattern processing through a fundamentally different connection pattern than MLPs. Instead of connecting every input to every output, CNNs use a local connection pattern where each output connects only to a small, spatially contiguous region of the input. This local receptive field moves across the input space, applying the same set of weights at each position—a process known as convolution.\n\nLeCun, Y., B. Boser, J. S. Denker, D. Henderson, R. E. Howard, W. Hubbard, and L. D. Jackel. 1989. “Backpropagation Applied to Handwritten Zip Code Recognition.” Neural Computation 1 (4): 541–51. https://doi.org/10.1162/neco.1989.1.4.541.\n\n\n4.3.2 Algorithmic Structure\nThe core operation in a CNN can be expressed mathematically as: \\[\n\\mathbf{H}^{(l)}_{i,j,k} = f\\left(\\sum_{di}\\sum_{dj}\\sum_{c} \\mathbf{W}^{(l)}_{di,dj,c,k}\\mathbf{H}^{(l-1)}_{i+di,j+dj,c} + \\mathbf{b}^{(l)}_k\\right)\n\\]\nHere, \\((i,j)\\) corresponds to spatial positions, \\(k\\) indexes output channels, \\(c\\) indexes input channels, and \\((di,dj)\\) spans the local receptive field. Unlike the dense matrix multiplication of MLPs, this operation:\n\nProcesses local neighborhoods (typically \\(3\\times 3\\) or \\(5\\times 5\\))\nReuses the same weights at each spatial position\nMaintains spatial structure in its output\n\nFor a concrete example, consider our MNIST digit classification task with \\(28\\times 28\\) grayscale images. Each convolutional layer applies a set of filters (say \\(3\\times 3\\)) that slide across the image, computing local weighted sums. If we use 32 filters, the layer produces a \\(28\\times 28\\times 32\\) output, where each spatial position contains 32 different feature measurements of its local neighborhood. This is in stark contrast to our MLP approach where we flattened the entire image into a 784-dimensional vector.\nThis algorithmic structure directly implements the requirements we identified for spatial pattern processing, creating distinct computational patterns that influence system design. For a detailed visual exploration of these network structures, the CNN Explainer project provides an interactive visualization that illuminates how different convolutional networks are constructed.\n\n\n\n\n\n\nFigure 4.2: Convolution operation, image data (blue) and 3x3 filter (green). Source: V. Dumoulin, F. Visin, MIT\n\n\n\n\n\n4.3.3 Computational Mapping\nThe elegant spatial structure of convolution operations maps to computational patterns quite different from the dense matrix multiplication of MLPs. Let’s examine how this mapping progresses from mathematical abstraction to computational reality.\nThe first implementation, conv_layer_spatial, uses high-level convolution operations to express the computation concisely. This is typical in deep learning frameworks, where optimized libraries handle the underlying complexity.\n# Mathematical abstraction - simple and clean\ndef conv_layer_spatial(input, kernel, bias):\n    output = convolution(input, kernel) + bias\n    return activation(output)\nThe second implementation, conv_layer_compute, reveals the actual computational pattern: nested loops that process each spatial position, applying the same filter weights to local regions of the input. The nested loops in conv_layer_compute reveal the true nature of convolution’s computational pattern.\n# System reality - nested loops of computation\ndef conv_layer_compute(input, kernel, bias):\n  # Loop 1: Process each image in batch\n  for image in range(batch_size):\n    \n    # Loop 2&3: Move across image spatially\n    for y in range(height):\n      for x in range(width):\n        \n       # Loop 4: Compute each output feature\n       for out_channel in range(num_output_channels):\n         result = bias[out_channel]\n         \n         # Loop 5&6: Move across kernel window\n         for ky in range(kernel_height):\n           for kx in range(kernel_width):\n             \n             # Loop 7: Process each input feature\n             for in_channel in range(num_input_channels):\n               # Get input value from correct window position\n               in_y = y + ky  \n               in_x = x + kx\n               # Perform multiply-accumulate operation\n               result += input[image, in_y, in_x, in_channel] * \\\n                  kernel[ky, kx, in_channel, out_channel]\n         \n         # Store result for this output position\n         output[image, y, x, out_channel] = result\n                  \nThe seven nested loops reveal different aspects of the computation:\n\nOuter loops (1-3) manage position: which image and where in the image\nMiddle loop (4) handles output features: computing different learned patterns\nInner loops (5-7) perform the actual convolution: sliding the kernel window\n\nLet’s take a closer look. The outer two loops (for y and for x) traverse each spatial position in the output feature map—for our MNIST example, this means moving across all \\(28\\times 28\\) positions. At each position, we compute values for each output channel (for k loop), which represents different learned features or patterns—our 32 different feature detectors.\nThe inner three loops implement the actual convolution operation at each position. For each output value, we process a local \\(3\\times 3\\) region of the input (the dy and dx loops) across all input channels (for c loop). This creates a sliding window effect, where the same \\(3\\times 3\\) filter moves across the image, performing multiply-accumulates between the filter weights and the local input values. Unlike the MLP’s global connectivity, this local processing pattern means each output value depends only on a small neighborhood of the input.\nFor our MNIST example with \\(3\\times 3\\) filters and 32 output channels, each output position requires only 9 multiply-accumulate operations per input channel, compared to the 784 operations needed in our MLP layer. However, this operation must be repeated for every spatial position \\((28\\times 28)\\) and every output channel (32).\nWhile using fewer operations per output, the spatial structure creates different patterns of memory access and computation that systems must handle efficiently. These patterns fundamentally influence system design, creating both challenges and opportunities for optimization, which we’ll examine next.\n\n\n4.3.4 System Implications\nWhen analyzing how computational patterns impact computer systems, we examine three fundamental dimensions: memory requirements, computation needs, and data movement. For CNNs, the spatial nature of processing creates distinctive patterns in each dimension that differ significantly from the dense connectivity of MLPs.\n\nMemory Requirements\nFor convolutional layers, memory requirements center around two key components: filter weights and feature maps. Unlike MLPs that require storing full connection matrices, CNNs use small, reusable filters. In our MNIST example, a convolutional layer with 32 filters of size \\(3\\times 3\\) requires storing only 288 weight parameters \\((3\\times 3\\times 32)\\), in contrast to the 78,400 weights needed for our MLP’s fully-connected layer. However, the system must store feature maps for all spatial positions, creating a different memory demand—a \\(28\\times 28\\) input with 32 output channels requires storing 25,088 activation values \\((28\\times 28\\times 32)\\).\nThese memory access patterns suggest opportunities for optimization through weight reuse and careful feature map management. Modern processors handle these patterns by caching filter weights, which are reused across spatial positions, while streaming through feature map data. Deep learning frameworks typically implement this through specialized memory layouts that optimize for both filter reuse and spatial locality in feature map access. CPUs and GPUs approach this differently—CPUs leverage their cache hierarchy to keep frequently used filters resident, while GPUs use specialized memory architectures designed for the spatial access patterns of image processing.\n\n\nComputation Needs\nThe core computation in CNNs involves repeatedly applying small filters across spatial positions. Each output value requires a local multiply-accumulate operation over the filter region. For our MNIST example with \\(3\\times 3\\) filters and 32 output channels, computing one spatial position involves 288 multiply-accumulates \\((3\\times 3\\times 32)\\), and this must be repeated for all 784 spatial positions \\((28\\times 8)\\). While each individual computation involves fewer operations than an MLP layer, the total computational load remains substantial due to spatial repetition.\nThis computational pattern presents different optimization opportunities than MLPs. The regular, repeated nature of convolution operations enables efficient hardware utilization through structured parallelism. Modern processors exploit this pattern in various ways. CPUs leverage SIMD instructions to process multiple filter positions simultaneously, while GPUs parallelize computation across spatial positions and channels. Deep learning frameworks further optimize this through specialized convolution algorithms that transform the computation to better match hardware capabilities.\n\n\nData Movement\nThe sliding window pattern of convolutions creates a distinctive data movement profile. Unlike MLPs where each weight is used once per forward pass, CNN filter weights are reused many times as the filter slides across spatial positions. For our MNIST example, each \\(3\\times 3\\) filter weight is reused 784 times (once for each position in the \\(28\\times 28\\) feature map). However, this creates a different challenge: the system must stream input features through the computation unit while keeping filter weights stable.\nThe predictable spatial access pattern enables strategic data movement optimizations. Different architectures handle this movement pattern through specialized mechanisms. CPUs maintain frequently used filter weights in cache while streaming through input features. GPUs employ memory architectures optimized for spatial locality and provide hardware support for efficient sliding window operations. Deep learning frameworks orchestrate these movements by organizing computations to maximize filter weight reuse and minimize redundant feature map accesses.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>DNN Architectures</span>"
    ]
  },
  {
    "objectID": "contents/core/dnn_architectures/dnn_architectures.html#recurrent-neural-networks-sequential-pattern-processing",
    "href": "contents/core/dnn_architectures/dnn_architectures.html#recurrent-neural-networks-sequential-pattern-processing",
    "title": "4  DNN Architectures",
    "section": "4.4 Recurrent Neural Networks: Sequential Pattern Processing",
    "text": "4.4 Recurrent Neural Networks: Sequential Pattern Processing\nWhile MLPs handle arbitrary relationships and CNNs process spatial patterns, many real-world problems involve sequential data where the order and relationship between elements over time matters. Text processing requires understanding how words relate to previous context, speech recognition needs to track how sounds form coherent patterns, and time-series analysis must capture how values evolve over time. These sequential relationships suggest that treating each time step independently misses crucial temporal patterns.\n\n4.4.1 Pattern Processing Needs\nSequential pattern processing addresses scenarios where the meaning of current input depends on what came before it. Consider natural language processing: the meaning of a word often depends heavily on previous words in the sentence. The word “bank” means something different in “river bank” versus “bank account.” Similarly, in speech recognition, a phoneme’s interpretation often depends on surrounding sounds, and in financial forecasting, future predictions require understanding patterns in historical data.\nThe key challenge in sequential processing is maintaining and updating relevant context over time. When reading text, humans don’t start fresh with each word—we maintain a running understanding that evolves as we process new information. Similarly, when processing time-series data, patterns might span different timescales, from immediate dependencies to long-term trends. This suggests we need an architecture that can both maintain state over time and update it based on new inputs.\nThese requirements demand specific capabilities from our processing architecture. The system must maintain internal state to capture temporal context, update this state based on new inputs, and learn which historical information is relevant for current predictions. Unlike MLPs and CNNs, which process fixed-size inputs, sequential processing must handle variable-length sequences while maintaining computational efficiency. This leads us to the recurrent neural network (RNN) architecture.\n\n\n4.4.2 Algorithmic Structure\nRNNs address sequential processing through a fundamentally different approach than MLPs or CNNs by introducing recurrent connections. Instead of just mapping inputs to outputs, RNNs maintain an internal state that is updated at each time step. This creates a memory mechanism that allows the network to carry information forward in time. This unique ability to model temporal dependencies was first explored by Elman (2002), who demonstrated how RNNs could find structure in time-dependent data.\nThe core operation in a basic RNN can be expressed mathematically as: \\[\n\\mathbf{h}_t = f(\\mathbf{W}_{hh}\\mathbf{h}_{t-1} + \\mathbf{W}_{xh}\\mathbf{x}_t + \\mathbf{b}_h)\n\\] where \\(\\mathbf{h}_t\\) corresponds to the hidden state at time \\(t\\), \\(\\mathbf{x}_t\\) is the input at time \\(t\\), \\(\\mathbf{W}_{hh}\\) contains the recurrent weights, and \\(\\mathbf{W}_{xh}\\) contains the input weights, as shown in the unfolded network structure in Figure 4.3.\nFor example, in processing a sequence of words, each word might be represented as a 100-dimensional vector (\\(\\mathbf{x}_t\\)), and we might maintain a hidden state of 128 dimensions (\\(\\mathbf{h}_t\\)). At each time step, the network combines the current input with its previous state to update its understanding of the sequence. This creates a form of memory that can capture patterns across time steps.\nThis recurrent structure directly implements our requirements for sequential processing through the introduction of recurrent connections, which maintain internal state and allow the network to carry information forward in time. Instead of processing all inputs independently, RNNs process sequences of data by iteratively updating a hidden state based on the current input and the previous hidden state, as depicted in Figure 4.3. This makes RNNs well-suited for tasks such as language modeling, speech recognition, and time-series forecasting.\n\n\n\n\n\n\nFigure 4.3: RNN architecture. Source: A. Amidi, S. Amidi, Stanford\n\n\n\n\n\n4.4.3 Computational Mapping\nThe sequential structure of RNNs maps to computational patterns quite different from both MLPs and CNNs. Let’s examine how this mapping progresses from mathematical abstraction to computational reality.\nThe rnn_layer_step function shows how the operation looks when using high-level matrix operations found in deep learning frameworks. It handles a single time step, taking the current input x_t and previous hidden state h_prev, along with two weight matrices: W_hh for hidden-to-hidden connections and W_xh for input-to-hidden connections. Through matrix multiplication operations (matmul), it merges the previous state and current input to generate the next hidden state.\n# Mathematical abstraction in code\ndef rnn_layer_step(x_t, h_prev, W_hh, W_xh, b):\n  # x_t: input at time t (batch_size × input_dim)\n  # h_prev: previous hidden state (batch_size × hidden_dim)\n  # W_hh: recurrent weights (hidden_dim × hidden_dim)\n  # W_xh: input weights (input_dim × hidden_dim)\n  h_t = activation(matmul(h_prev, W_hh) + matmul(x_t, W_xh) + b)\n  return h_t\nThis simplified view masks the underlying complexity of the nested loops and individual computations shown in the detailed implementation. Its actual implementation reveals a more detailed computational reality:\n# Core computational pattern\ndef rnn_layer_compute(x_t, h_prev, W_hh, W_xh, b):\n    # Initialize next hidden state\n    h_t = np.zeros_like(h_prev)\n    \n    # Loop 1: Process each sequence in the batch\n    for batch in range(batch_size):\n        # Loop 2: Compute recurrent contribution (h_prev × W_hh)\n        for i in range(hidden_dim):\n            for j in range(hidden_dim):\n                h_t[batch,i] += h_prev[batch,j] * W_hh[j,i]\n                \n        # Loop 3: Compute input contribution (x_t × W_xh)\n        for i in range(hidden_dim):\n            for j in range(input_dim):\n                h_t[batch,i] += x_t[batch,j] * W_xh[j,i]\n                \n        # Loop 4: Add bias and apply activation\n        for i in range(hidden_dim):\n            h_t[batch,i] = activation(h_t[batch,i] + b[i])\n    \n    return h_t\nThe nested loops in rnn_layer_compute expose the core computational pattern of RNNs. Loop 1 processes each sequence in the batch independently, allowing for batch-level parallelism. Within each batch item, Loop 2 computes how the previous hidden state influences the next state through the recurrent weights W_hh. Loop 3 then incorporates new information from the current input through the input weights W_xh. Finally, Loop 4 adds biases and applies the activation function to produce the new hidden state.\nFor a sequence processing task with input dimension 100 and hidden state dimension 128, each time step requires two matrix multiplications: one \\(128\\times 128\\) for the recurrent connection and one \\(100\\times 128\\) for the input projection. While individual time steps can process in parallel across batch elements, the time steps themselves must process sequentially. This creates a unique computational pattern that systems must handle efficiently.\n\n\n4.4.4 System Implications\nFor RNNs, the sequential nature of processing creates distinctive patterns in each dimension (memory requirements, computation needs, and data movement) that differ significantly from both MLPs and CNNs.\n\nMemory Requirements\nRNNs require storing two sets of weights (input-to-hidden and hidden-to-hidden) along with the hidden state. For our example with input dimension 100 and hidden state dimension 128, this means storing 12,800 weights for input projection \\((100\\times 128)\\) and 16,384 weights for recurrent connections \\((128\\times 128)\\). Unlike CNNs where weights are reused across spatial positions, RNN weights are reused across time steps. Additionally, the system must maintain the hidden state, which becomes a critical factor in memory usage and access patterns.\nThese memory access patterns create a different profile from MLPs and CNNs. Modern processors handle these patterns by keeping the weight matrices in cache while streaming through sequence elements. Deep learning frameworks optimize memory access by batching sequences together and carefully managing hidden state storage between time steps. CPUs and GPUs approach this through different strategies—CPUs leverage their cache hierarchy for weight reuse, while GPUs use specialized memory architectures designed for maintaining state across sequential operations.\n\n\nComputation Needs\nThe core computation in RNNs involves repeatedly applying weight matrices across time steps. For each time step, we perform two matrix multiplications: one with the input weights and one with the recurrent weights. In our example, processing a single time step requires 12,800 multiply-accumulates for the input projection \\((100\\times 128)\\) and 16,384 multiply-accumulates for the recurrent connection \\((128\\times 128)\\).\nThis computational pattern differs from both MLPs and CNNs in a key way: while we can parallelize across batch elements, we cannot parallelize across time steps due to the sequential dependency. Each time step must wait for the previous step’s hidden state before it can begin computation. This creates a tension between the inherent sequential nature of the algorithm and the desire for parallel execution in modern hardware.\nModern processors handle these patterns through different approaches. CPUs pipeline operations within each time step while maintaining the sequential order across steps. GPUs batch multiple sequences together to maintain high throughput despite sequential dependencies. Deep learning frameworks optimize this further by techniques like sequence packing and unrolling computations across multiple time steps when possible.\n\n\nData Movement\nThe sequential processing in RNNs creates a distinctive data movement pattern that differs from both MLPs and CNNs. While MLPs need each weight only once per forward pass and CNNs reuse weights across spatial positions, RNNs reuse their weights across time steps while requiring careful management of the hidden state data flow.\nFor our example with a 128-dimensional hidden state, each time step must: load the previous hidden state (128 values), access both weight matrices (29,184 total weights from both input and recurrent connections), and store the new hidden state (128 values). This pattern repeats for every element in the sequence. Unlike CNNs where we can predict and prefetch data based on spatial patterns, RNN data movement is driven by temporal dependencies.\nDifferent architectures handle this sequential data movement through specialized mechanisms. CPUs maintain weight matrices in cache while streaming through sequence elements and managing hidden state updates. GPUs employ memory architectures optimized for maintaining state information across sequential operations while processing multiple sequences in parallel. Deep learning frameworks orchestrate these movements by managing data transfers between time steps and optimizing batch operations.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>DNN Architectures</span>"
    ]
  },
  {
    "objectID": "contents/core/dnn_architectures/dnn_architectures.html#attention-mechanisms-dynamic-pattern-processing",
    "href": "contents/core/dnn_architectures/dnn_architectures.html#attention-mechanisms-dynamic-pattern-processing",
    "title": "4  DNN Architectures",
    "section": "4.5 Attention Mechanisms: Dynamic Pattern Processing",
    "text": "4.5 Attention Mechanisms: Dynamic Pattern Processing\nWhile previous architectures process patterns in fixed ways—MLPs with dense connectivity, CNNs with spatial operations, and RNNs with sequential updates—many tasks require dynamic relationships between elements that change based on content. Language understanding, for instance, needs to capture relationships between words that depend on meaning rather than just position. Graph analysis requires understanding connections that vary by node. These dynamic relationships suggest we need an architecture that can learn and adapt its processing patterns based on the data itself.\n\n4.5.1 Pattern Processing Needs\nDynamic pattern processing addresses scenarios where relationships between elements aren’t fixed by architecture but instead emerge from content. Consider language translation: when translating “the bank by the river,” understanding “bank” requires attending to “river,” but in “the bank approved the loan,” the important relationship is with “approved” and “loan.” Unlike RNNs that process information sequentially or CNNs that use fixed spatial patterns, we need an architecture that can dynamically determine which relationships matter.\nThis requirement for dynamic processing appears across many domains. In protein structure prediction, interactions between amino acids depend on their chemical properties and spatial arrangements. In graph analysis, node relationships vary based on graph structure and node features. In document analysis, connections between different sections depend on semantic content rather than just proximity.\nThese scenarios demand specific capabilities from our processing architecture. The system must compute relationships between all pairs of elements, weigh these relationships based on content, and use these weights to selectively combine information. Unlike previous architectures with fixed connectivity patterns, dynamic processing requires the flexibility to modify its computation graph based on the input itself. This leads us to the Transformer architecture, which implements these capabilities through attention mechanisms.\n\n\n4.5.2 Basic Attention Mechanism\n\nAlgorithmic Structure\nAttention mechanisms form the foundation of dynamic pattern processing by computing weighted connections between elements based on their content (Bahdanau, Cho, and Bengio 2014). This approach allows for the processing of relationships that aren’t fixed by architecture but instead emerge from the data itself. At the core of an attention mechanism is a fundamental operation that can be expressed mathematically as: \\[\n\\text{Attention}(\\mathbf{Q}, \\mathbf{K}, \\mathbf{V}) = \\text{softmax}\n\\left(\\frac{\\mathbf{Q}\\mathbf{K}^T}{\\sqrt{d_k}}\\right)\\mathbf{V}\n\\]\nIn this equation, \\(\\mathbf{Q}\\) (queries), \\(\\mathbf{K}\\) (keys), and \\(\\mathbf{V}\\) (values) represent learned projections of the input. For a sequence of length \\(N\\) with dimension \\(d\\), this operation creates an \\(N\\times N\\) attention matrix, determining how each position should attend to all others.\nThe attention operation involves several key steps. First, it computes query, key, and value projections for each position in the sequence. Next, it generates an \\(N\\times N\\) attention matrix through query-key interactions. These steps are illustrated in Figure 4.4. Finally, it uses these attention weights to combine value vectors, producing the output.\n\n\n\n\n\n\nFigure 4.4: The interaction between Query, Key, and Value components. Source: Transformer Explainer.\n\n\n\nThe key is that, unlike the fixed weight matrices found in previous architectures, as shown in Figure 4.5, these attention weights are computed dynamically for each input. This allows the model to adapt its processing based on the dynamic content at hand.\n\n\n\n\n\n\nFigure 4.5: Dynamic weight calculation. Source: Transformer Explainer.\n\n\n\n\n\n\nComputational Mapping\nThe dynamic structure of attention operations maps to computational patterns that differ significantly from those of previous architectures. To understand this mapping, let’s examine how it progresses from mathematical abstraction to computational reality:\n# Mathematical abstraction in code\ndef attention_layer_matrix(Q, K, V):\n    # Q, K, V: (batch_size × seq_len × d_model)\n    scores = matmul(Q, K.transpose(-2, -1)) / \\\n             sqrt(d_k)                 # Compute attention scores\n    weights = softmax(scores)          # Normalize scores\n    output = matmul(weights, V)        # Combine values\n    return output\n\n# Core computational pattern\ndef attention_layer_compute(Q, K, V):\n    # Initialize outputs\n    scores = np.zeros((batch_size, seq_len, seq_len))\n    outputs = np.zeros_like(V)\n    \n    # Loop 1: Process each sequence in batch\n    for b in range(batch_size):\n        # Loop 2: Compute attention for each query position\n        for i in range(seq_len):\n            # Loop 3: Compare with each key position\n            for j in range(seq_len):\n                # Compute attention score\n                for d in range(d_model):\n                    scores[b,i,j] += Q[b,i,d] * K[b,j,d]\n                scores[b,i,j] /= sqrt(d_k)\n        \n        # Apply softmax to scores\n        for i in range(seq_len):\n            scores[b,i] = softmax(scores[b,i])\n            \n        # Loop 4: Combine values using attention weights\n        for i in range(seq_len):\n            for j in range(seq_len):\n                for d in range(d_model):\n                    outputs[b,i,d] += scores[b,i,j] * V[b,j,d]\n    \n    return outputs\nThe nested loops in attention_layer_compute reveal the true nature of attention’s computational pattern. The first loop processes each sequence in the batch independently. The second and third loops compute attention scores between all pairs of positions, creating a quadratic computation pattern with respect to sequence length. The fourth loop uses these attention weights to combine values from all positions, producing the final output.\n\n\nSystem Implications\nThe attention mechanism creates distinctive patterns in memory requirements, computation needs, and data movement that set it apart from previous architectures.\n\nMemory Requirements\nIn terms of memory requirements, attention mechanisms necessitate storage for attention weights, key-query-value projections, and intermediate feature representations. For a sequence length \\(N\\) and dimension d, each attention layer must store an \\(N\\times N\\) attention weight matrix for each sequence in the batch, three sets of projection matrices for queries, keys, and values (each sized \\(d\\times d\\)), and input and output feature maps of size \\(N\\times d\\). The dynamic generation of attention weights for every input creates a memory access pattern where intermediate attention weights become a significant factor in memory usage.\n\n\nComputation Needs\nComputation needs in attention mechanisms center around two main phases: generating attention weights and applying them to values. For each attention layer, the system performs substantial multiply-accumulate operations across multiple computational stages. The query-key interactions alone require \\(N\\times N\\times d\\) multiply-accumulates, with an equal number needed for applying attention weights to values. Additional computations are required for the projection matrices and softmax operations. This computational pattern differs from previous architectures due to its quadratic scaling with sequence length and the need to perform fresh computations for each input.\n\n\nData Movement\nData movement in attention mechanisms presents unique challenges. Each attention operation involves projecting and moving query, key, and value vectors for each position, storing and accessing the full attention weight matrix, and coordinating the movement of value vectors during the weighted combination phase. This creates a data movement pattern where intermediate attention weights become a major factor in system bandwidth requirements. Unlike the more predictable access patterns of CNNs or the sequential access of RNNs, attention operations require frequent movement of dynamically computed weights across the memory hierarchy.\nThese distinctive characteristics of attention mechanisms in terms of memory, computation, and data movement have significant implications for system design and optimization, setting the stage for the development of more advanced architectures like Transformers.\n\n\n\n\n4.5.3 Transformers and Self-Attention\nTransformers, first introduced by Chen et al. (2018), represent a significant evolution in the application of attention mechanisms, introducing the concept of self-attention to create a powerful architecture for dynamic pattern processing. While the basic attention mechanism allows for content-based weighting of information from a source sequence, Transformers extend this idea by applying attention within a single sequence, enabling each element to attend to all other elements including itself.\n\nChen, Mia Xu, Orhan Firat, Ankur Bapna, Melvin Johnson, Wolfgang Macherey, George Foster, Llion Jones, et al. 2018. “The Best of Both Worlds: Combining Recent Advances in Neural Machine Translation.” In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), 30:5998–6008. Association for Computational Linguistics. https://doi.org/10.18653/v1/p18-1008.\n\nAlgorithmic Structure\nThe key innovation in Transformers lies in their use of self-attention layers. In a self-attention layer, the queries, keys, and values are all derived from the same input sequence. This allows the model to weigh the importance of different positions within the same sequence when encoding each position. For instance, in processing the sentence “The animal didn’t cross the street because it was too wide,” self-attention allows the model to link “it” with “street,” capturing long-range dependencies that are challenging for traditional sequential models.\nTransformers typically employ multi-head attention, which involves multiple sets of query/key/value projections. Each set, or “head,” can focus on different aspects of the input, allowing the model to jointly attend to information from different representation subspaces. This multi-head structure provides the model with a richer representational capability, enabling it to capture various types of relationships within the data simultaneously.\nThe self-attention mechanism in Transformers can be expressed mathematically in a form similar to the basic attention mechanism: \\[\n\\text{SelfAttention}(\\mathbf{X}) = \\text{softmax}\n\\left(\\frac{\\mathbf{XW_Q}(\\mathbf{XW_K})^T}{\\sqrt{d_k}}\\right)\\mathbf{XW_V}\n\\]\nHere, \\(\\mathbf{X}\\) is the input sequence, and \\(\\mathbf{W_Q}\\), \\(\\mathbf{W_K}\\), and \\(\\mathbf{W_V}\\) are learned weight matrices for queries, keys, and values respectively. This formulation highlights how self-attention derives all its components from the same input, creating a dynamic, content-dependent processing pattern.\nThe Transformer architecture leverages this self-attention mechanism within a broader structure that typically includes feed-forward layers, layer normalization, and residual connections (see Figure 4.6). This combination allows Transformers to process input sequences in parallel, capturing complex dependencies without the need for sequential computation. As a result, Transformers have demonstrated remarkable effectiveness across a wide range of tasks, from natural language processing to computer vision, revolutionizing the landscape of deep learning architectures.\n\n\n\n\n\n\nFigure 4.6: The Transformer model architecture. Source: Attention Is All You Need\n\n\n\n\n\nComputational Mapping\nWhile Transformer self-attention builds upon the basic attention mechanism, it introduces distinct computational patterns that set it apart. To understand these patterns, we must examine the typical implementation of self-attention in Transformers:\ndef self_attention_layer(X, W_Q, W_K, W_V, d_k):\n    # X: input tensor (batch_size × seq_len × d_model)\n    # W_Q, W_K, W_V: weight matrices (d_model × d_k)\n    \n    Q = matmul(X, W_Q)\n    K = matmul(X, W_K)\n    V = matmul(X, W_V)\n    \n    scores = matmul(Q, K.transpose(-2, -1)) / sqrt(d_k)\n    attention_weights = softmax(scores, dim=-1)\n    output = matmul(attention_weights, V)\n    \n    return output\n\ndef multi_head_attention(X, W_Q, W_K, W_V, W_O, num_heads, d_k):\n    outputs = []\n    for i in range(num_heads):\n        head_output = self_attention_layer(X, W_Q[i], W_K[i], \\\n                                           W_V[i], d_k)\n        outputs.append(head_output)\n    \n    concat_output = torch.cat(outputs, dim=-1)\n    final_output = matmul(concat_output, W_O)\n    \n    return final_output\n\n\nSystem Implications\nThis implementation reveals several key computational characteristics of Transformer self-attention. First, self-attention enables parallel processing across all positions in the sequence. This is evident in the matrix multiplications that compute Q, K, and V simultaneously for all positions. Unlike recurrent architectures that process inputs sequentially, this parallel nature allows for more efficient computation, especially on modern hardware designed for parallel operations.\nSecond, the attention score computation results in a matrix of size (seq_len × seq_len), leading to quadratic complexity with respect to sequence length. This quadratic relationship becomes a significant computational bottleneck when processing long sequences, a challenge that has spurred research into more efficient attention mechanisms.\nThird, the multi-head attention mechanism effectively runs multiple self-attention operations in parallel, each with its own set of learned projections. While this increases the computational load linearly with the number of heads, it allows the model to capture different types of relationships within the same input, enhancing the model’s representational power.\nFourth, the core computations in self-attention are dominated by large matrix multiplications. For a sequence of length \\(N\\) and embedding dimension \\(d\\), the main operations involve matrices of sizes \\((N\\times d)\\), \\((d\\times d)\\), and \\((N\\times N)\\). These intensive matrix operations are well-suited for acceleration on specialized hardware like GPUs, but they also contribute significantly to the overall computational cost of the model.\nFinally, self-attention generates memory-intensive intermediate results. The attention weights matrix \\((N\\times N)\\) and the intermediate results for each attention head create substantial memory requirements, especially for long sequences. This can pose challenges for deployment on memory-constrained devices and necessitates careful memory management in implementations.\nThese computational patterns create a unique profile for Transformer self-attention, distinct from previous architectures. The parallel nature of the computations makes Transformers well-suited for modern parallel processing hardware, but the quadratic complexity with sequence length poses challenges for processing long sequences. As a result, much research has focused on developing optimization techniques, such as sparse attention patterns or low-rank approximations, to address these challenges. Each of these optimizations presents its own trade-offs between computational efficiency and model expressiveness, a balance that must be carefully considered in practical applications.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>DNN Architectures</span>"
    ]
  },
  {
    "objectID": "contents/core/dnn_architectures/dnn_architectures.html#architectural-building-blocks",
    "href": "contents/core/dnn_architectures/dnn_architectures.html#architectural-building-blocks",
    "title": "4  DNN Architectures",
    "section": "4.6 Architectural Building Blocks",
    "text": "4.6 Architectural Building Blocks\nDeep learning architectures, while we presented them as distinct approaches in the previous sections, are better understood as compositions of fundamental building blocks that evolved over time. Much like how complex LEGO structures are built from basic bricks, modern neural networks combine and iterate on core computational patterns that emerged through decades of research (Yann LeCun, Bengio, and Hinton 2015). Each architectural innovation introduced new building blocks while finding novel ways to use existing ones.\n\nLeCun, Yann, Yoshua Bengio, and Geoffrey Hinton. 2015. “Deep Learning.” Nature 521 (7553): 436–44. https://doi.org/10.1038/nature14539.\n\nRosenblatt, F. 1958. “The Perceptron: A Probabilistic Model for Information Storage and Organization in the Brain.” Psychological Review 65 (6): 386–408. https://doi.org/10.1037/h0042519.\n\nRumelhart, David E., Geoffrey E. Hinton, and Ronald J. Williams. 1986. “Learning Representations by Back-Propagating Errors.” Nature 323 (6088): 533–36. https://doi.org/10.1038/323533a0.\nThese building blocks and their evolution provide insight into modern architectures. What began with the simple perceptron (Rosenblatt 1958) evolved into multi-layer networks (Rumelhart, Hinton, and Williams 1986), which then spawned specialized patterns for spatial and sequential processing. Each advancement maintained useful elements from its predecessors while introducing new computational primitives. Today’s sophisticated architectures, like Transformers, can be seen as carefully engineered combinations of these fundamental building blocks.\nThis progression reveals not just the evolution of neural networks, but also the discovery and refinement of core computational patterns that remain relevant. As we have seen through our exploration of different neural network architectures, deep learning has evolved significantly, with each new architecture bringing its own set of computational demands and system-level challenges.\nTable 4.1 summarizes this evolution, highlighting the key primitives and system focus for each era of deep learning development. This table encapsulates the major shifts in deep learning architecture design and the corresponding changes in system-level considerations. From the early focus on dense matrix operations optimized for CPUs, we see a progression through convolutions leveraging GPU acceleration, to sequential operations necessitating sophisticated memory hierarchies, and finally to the current era of attention mechanisms requiring flexible accelerators and high-bandwidth memory.\n\n\n\nTable 4.1: Evolution of deep learning architectures and their system implications\n\n\n\n\n\n\n\n\n\n\n\nEra\nDominant Architecture\nKey Primitives\nSystem Focus\n\n\n\n\nEarly NN\nMLP\nDense Matrix Ops\nCPU optimization\n\n\nCNN Revolution\nCNN\nConvolutions\nGPU acceleration\n\n\nSequence Modeling\nRNN\nSequential Ops\nMemory hierarchies\n\n\nAttention Era\nTransformer\nAttention, Dynamic Compute\nFlexible accelerators, High-bandwidth memory\n\n\n\n\n\n\nAs we dive deeper into each of these building blocks, we see how these primitives evolved and combined to create increasingly powerful and complex neural network architectures.\n\n4.6.1 From Perceptron to Multi-Layer Networks\nWhile we examined MLPs earlier as a mechanism for dense pattern processing, here we focus on how they established fundamental building blocks that appear throughout deep learning. The evolution from perceptron to MLP introduced several key concepts: the power of layer stacking, the importance of non-linear transformations, and the basic feedforward computation pattern.\nThe introduction of hidden layers between input and output created a template for feature transformation that appears in virtually every modern architecture. Even in sophisticated networks like Transformers, we find MLP-style feedforward layers performing feature processing. The concept of transforming data through successive non-linear layers has become a fundamental paradigm that transcends the specific architecture types.\nPerhaps most importantly, the development of MLPs established the backpropagation algorithm, which to this day remains the cornerstone of neural network training. This key contribution has enabled the training of deep architectures and influenced how later architectures would be designed to maintain gradient flow.\nThese building blocks—layered feature transformation, non-linear activation, and gradient-based learning—set the foundation for more specialized architectures. Subsequent innovations often focused on structuring these basic components in new ways rather than replacing them entirely.\n\n\n4.6.2 From Dense to Spatial Processing\nThe development of CNNs marked a significant architectural innovation—the realization that we could specialize the dense connectivity of MLPs for spatial patterns. While retaining the core concept of layer-wise processing, CNNs introduced several fundamental building blocks that would influence all future architectures.\nThe first key innovation was the concept of parameter sharing. Unlike MLPs where each connection had its own weight, CNNs showed how the same parameters could be reused across different parts of the input. This not only made the networks more efficient but introduced the powerful idea that architectural structure could encode useful priors about the data (Lecun et al. 1998).\n\nLecun, Y., L. Bottou, Y. Bengio, and P. Haffner. 1998. “Gradient-Based Learning Applied to Document Recognition.” Proceedings of the IEEE 86 (11): 2278–2324. https://doi.org/10.1109/5.726791.\n\nHe, Kaiming, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016. “Deep Residual Learning for Image Recognition.” In 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 770–78. IEEE. https://doi.org/10.1109/cvpr.2016.90.\nPerhaps even more influential was the introduction of skip connections through ResNets (He et al. 2016). Originally they were designed to help train very deep CNNs, skip connections have become a fundamental building block that appears in virtually every modern architecture. They showed how direct paths through the network could help gradient flow and information propagation, a concept now central to Transformer designs.\nCNNs also introduced batch normalization, a technique for stabilizing neural network training by normalizing intermediate features (Ioffe and Szegedy 2015); we will learn more about this in the AI Training chapter. This concept of feature normalization, while originating in CNNs, evolved into layer normalization and is now a key component in modern architectures.\n\nIoffe, Sergey, and Christian Szegedy. 2015. “Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift.” International Conference on Machine Learning, 448–56.\nThese innovations—parameter sharing, skip connections, and nor­mal­iza­tion—transcended their origins in spatial processing to become essential building blocks in the deep learning toolkit.\n\n\n4.6.3 The Evolution of Sequence Processing\nWhile CNNs specialized MLPs for spatial patterns, sequence models adapted neural networks for temporal dependencies. RNNs introduced the fundamental concept of maintaining and updating state—a building block that influenced how networks could process sequential information (Elman 2002).\n\nElman, Jeffrey L. 2002. “Finding Structure in Time.” In Cognitive Modeling, 14:257–88. 2. The MIT Press. https://doi.org/10.7551/mitpress/1888.003.0015.\n\nHochreiter, Sepp, and Jürgen Schmidhuber. 1997. “Long Short-Term Memory.” Neural Computation 9 (8): 1735–80. https://doi.org/10.1162/neco.1997.9.8.1735.\n\nCho, Kyunghyun, Bart van Merrienboer, Dzmitry Bahdanau, and Yoshua Bengio. 2014. “On the Properties of Neural Machine Translation: Encoder-Decoder Approaches.” In Eighth Workshop on Syntax, Semantics and Structure in Statistical Translation (SSST-8), 103–11. Association for Computational Linguistics.\nThe development of LSTMs and GRUs brought sophisticated gating mechanisms to neural networks (Hochreiter and Schmidhuber 1997; Cho et al. 2014). These gates, themselves small MLPs, showed how simple feedforward computations could be composed to control information flow. This concept of using neural networks to modulate other neural networks became a recurring pattern in architecture design.\nPerhaps most significantly, sequence models demonstrated the power of adaptive computation paths. Unlike the fixed patterns of MLPs and CNNs, RNNs showed how networks could process variable-length inputs by reusing weights over time. This insight—that architectural patterns could adapt to input structure—laid groundwork for more flexible architectures.\nSequence models also popularized the concept of attention through encoder-decoder architectures (Bahdanau, Cho, and Bengio 2014). Initially introduced as an improvement to machine translation, attention mechanisms showed how networks could learn to dynamically focus on relevant information. This building block would later become the foundation of Transformer architectures.\n\nBahdanau, Dzmitry, Kyunghyun Cho, and Yoshua Bengio. 2014. “Neural Machine Translation by Jointly Learning to Align and Translate.” arXiv Preprint arXiv:1409.0473, September. http://arxiv.org/abs/1409.0473v7.\n\n\n4.6.4 Modern Architectures: Synthesis and Innovation\nModern architectures, particularly Transformers, represent a sophisticated synthesis of these fundamental building blocks. Rather than introducing entirely new patterns, they innovate through clever combination and refinement of existing components. Consider the Transformer architecture: at its core, we find MLP-style feedforward networks processing features between attention layers. The attention mechanism itself builds on ideas from sequence models but removes the recurrent connection, instead using position embeddings inspired by CNN intuitions. Skip connections, inherited from ResNets, appear throughout the architecture, while layer normalization, evolved from CNN’s batch normalization, stabilizes training (Ba, Kiros, and Hinton 2016).\n\nBa, Jimmy Lei, Jamie Ryan Kiros, and Geoffrey E. Hinton. 2016. “Layer Normalization.” arXiv Preprint arXiv:1607.06450, July. http://arxiv.org/abs/1607.06450v1.\nThis composition of building blocks creates something greater than the sum of its parts. The self-attention mechanism, while building on previous attention concepts, enables a new form of dynamic pattern processing. The arrangement of these components—attention followed by feedforward layers, with skip connections and normalization—has proven so effective it’s become a template for new architectures.\nEven recent innovations in vision and language models follow this pattern of recombining fundamental building blocks. Vision Transformers adapt the Transformer architecture to images while maintaining its essential components (Dosovitskiy et al. 2021). Large language models scale up these patterns while introducing refinements like grouped-query attention or sliding window attention, yet still rely on the core building blocks established through this architectural evolution (Brown et al. 2020).\n\nDosovitskiy, Alexey, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, et al. 2021. “An Image Is Worth 16x16 Words: Transformers for Image Recognition at Scale.” International Conference on Learning Representations.\n\nBrown, Tom B, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, et al. 2020. “Language Models Are Few-Shot Learners.” Advances in Neural Information Processing Systems 33: 1877–1901.\nTo illustrate how these modern architectures synthesize and innovate upon previous approaches, consider the following comparison of primitive utilization across different neural network architectures:\n\n\n\nTable 4.2: Comparison of primitive utilization across neural network architectures.\n\n\n\n\n\n\n\n\n\n\n\n\nPrimitive Type\nMLP\nCNN\nRNN\nTransformer\n\n\n\n\nComputational\nMatrix Multiplication\nConvolution (Matrix Mult.)\nMatrix Mult. + State Update\nMatrix Mult. + Attention\n\n\nMemory Access\nSequential\nStrided\nSequential + Random\nRandom (Attention)\n\n\nData Movement\nBroadcast\nSliding Window\nSequential\nBroadcast + Gather\n\n\n\n\n\n\nAs shown in Table 4.2, Transformers combine elements from previous architectures while introducing new patterns. They retain the core matrix multiplication operations common to all architectures but introduce a more complex memory access pattern with their attention mechanism. Their data movement patterns blend the broadcast operations of MLPs with the gather operations reminiscent of more dynamic architectures.\nThis synthesis of primitives in Transformers exemplifies how modern architectures innovate by recombining and refining existing building blocks, rather than inventing entirely new computational paradigms. Also, this evolutionary process provides insight into the development of future architectures and helps to guide the design of efficient systems to support them.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>DNN Architectures</span>"
    ]
  },
  {
    "objectID": "contents/core/dnn_architectures/dnn_architectures.html#system-level-building-blocks",
    "href": "contents/core/dnn_architectures/dnn_architectures.html#system-level-building-blocks",
    "title": "4  DNN Architectures",
    "section": "4.7 System-Level Building Blocks",
    "text": "4.7 System-Level Building Blocks\nAfter having examined different deep learning architectures, we can distill their system requirements into fundamental primitives that underpin both hardware and software implementations. These primitives represent operations that cannot be broken down further while maintaining their essential characteristics. Just as complex molecules are built from basic atoms, sophisticated neural networks are constructed from these fundamental operations.\n\n4.7.1 Core Computational Primitives\nThree fundamental operations serve as the building blocks for all deep learning computations: matrix multiplication, sliding window operations, and dynamic computation. What makes these operations primitive is that they cannot be further decomposed without losing their essential computational properties and efficiency characteristics.\nMatrix multiplication represents the most basic form of transforming sets of features. When we multiply a matrix of inputs by a matrix of weights, we’re computing weighted combinations—the fundamental operation of neural networks. For example, in our MNIST network, each 784-dimensional input vector multiplies with a \\(784\\times 100\\) weight matrix. This pattern appears everywhere: MLPs use it directly for layer computations, CNNs reshape convolutions into matrix multiplications through im2col (turning a \\(3\\times 3\\) convolution into a matrix operation), and Transformers use it extensively in their attention mechanisms.\nIn modern systems, matrix multiplication maps to specific hardware and software implementations. Hardware accelerators provide specialized tensor cores that can perform thousands of multiply-accumulates in parallel—NVIDIA’s A100 tensor cores can achieve up to 312 TFLOPS (32-bit) through massive parallelization of these operations. Software frameworks like PyTorch and TensorFlow automatically map these high-level operations to optimized matrix libraries (NVIDIA cuBLAS, Intel MKL) that exploit these hardware capabilities.\nSliding window operations compute local relationships by applying the same operation to chunks of data. In CNNs processing MNIST images, a \\(3\\times 3\\) convolution filter slides across the \\(28\\times 28\\) input, requiring \\(26\\times 26\\) windows of computation,1 assuming a stride size of 1. Modern hardware accelerators implement this through specialized memory access patterns and data buffering schemes that optimize data reuse. For example, Google’s TPU uses a \\(128\\times 128\\) systolic array where data flows systematically through processing elements, allowing each input value to be reused across multiple computations without accessing memory. Software frameworks optimize these operations by transforming them into efficient matrix multiplications (a \\(3\\times 3\\) convolution becomes a \\(9\\times N\\) matrix multiplication) and carefully managing data layout in memory to maximize spatial locality.\n1 The \\(26\\times 26\\) output dimension comes from the formula \\((N-F+1)\\) where \\(N\\) is the input dimension (28) and \\(F\\) is the filter size (3), calculated as: \\(28-3+1=26\\) for both dimensions.Dynamic computation, where the operation itself depends on the input data, emerged prominently with attention mechanisms but represents a fundamental capability needed for adaptive processing. In Transformer attention, each query dynamically determines its interaction weights with all keys—for a sequence of length 512, this means 512 different weight patterns must be computed on the fly. Unlike fixed patterns where we know the computation graph in advance, dynamic computation requires runtime decisions. This creates specific implementation challenges—hardware must provide flexible routing of data (modern GPUs use dynamic scheduling) and support variable computation patterns, while software frameworks need efficient mechanisms for handling data-dependent execution paths (PyTorch’s dynamic computation graphs, TensorFlow’s dynamic control flow).\nThese primitives combine in sophisticated ways in modern architectures. A Transformer layer processing a sequence of 512 tokens demonstrates this clearly: it uses matrix multiplications for feature projections (\\(512\\times 512\\) operations implemented through tensor cores), may employ sliding windows for efficient attention over long sequences (using specialized memory access patterns for local regions), and requires dynamic computation for attention weights (computing \\(512\\times 512\\) attention patterns at runtime). The way these primitives interact creates specific demands on system design—from memory hierarchy organization to computation scheduling.\nThe building blocks we’ve discussed help explain why certain hardware features exist (like tensor cores for matrix multiplication) and why software frameworks organize computations in particular ways (like batching similar operations together). As we move from computational primitives to consider memory access and data movement patterns, it’s important to recognize how these fundamental operations shape the demands placed on memory systems and data transfer mechanisms. The way computational primitives are implemented and combined has direct implications for how data needs to be stored, accessed, and moved within the system.\n\n\n4.7.2 Memory Access Primitives\nThe efficiency of deep learning systems heavily depends on how they access and manage memory. In fact, memory access often becomes the primary bottleneck in modern ML systems—while a matrix multiplication unit might be capable of performing thousands of operations per cycle, it will sit idle if data isn’t available at the right time. For example, accessing data from DRAM typically takes hundreds of cycles, while on-chip computation takes only a few cycles.\nThree fundamental memory access patterns dominate in deep learning architectures: sequential access, strided access, and random access. Each pattern creates different demands on the memory system and offers different opportunities for optimization.\nSequential access is the simplest and most efficient pattern. Consider an MLP performing matrix multiplication with a batch of MNIST images: it needs to access both the \\(784\\times 100\\) weight matrix and the input vectors sequentially. This pattern maps well to modern memory systems—DRAM can operate in burst mode for sequential reads (achieving up to 400 GB/s in modern GPUs), and hardware prefetchers can effectively predict and fetch upcoming data. Software frameworks optimize for this by ensuring data is laid out contiguously in memory and aligning data to cache line boundaries.\nStrided access appears prominently in CNNs, where each output position needs to access a window of input values at regular intervals. For a CNN processing MNIST images with \\(3\\times 3\\) filters, each output position requires accessing 9 input values with a stride matching the input width. While less efficient than sequential access, hardware supports this through pattern-aware caching strategies and specialized memory controllers. Software frameworks often transform these strided patterns into sequential access through data layout reorganization—the im2col transformation in deep learning frameworks converts convolution’s strided access into efficient matrix multiplications.\nRandom access poses the greatest challenge for system efficiency. In a Transformer processing a sequence of 512 tokens, each attention operation potentially needs to access any position in the sequence, creating unpredictable memory access patterns. Random access can severely impact performance through cache misses (potentially causing 100+ cycle stalls per access) and unpredictable memory latencies. Systems address this through large cache hierarchies (modern GPUs have several MB of L2 cache) and sophisticated prefetching strategies, while software frameworks employ techniques like attention pattern pruning to reduce random access requirements.\nThese different memory access patterns contribute significantly to the overall memory requirements of each architecture. To illustrate this, Table 4.3 compares the memory complexity of MLPs, CNNs, RNNs, and Transformers.\n\n\n\nTable 4.3: DNN architecture complexity. Note that for RNNs, parameter storage is bounded by \\(O(N \\times h)\\) when \\(N &gt; h\\).\n\n\n\n\n\n\n\n\n\n\n\n\nArchitecture\nInput Dependency\nParameter Storage\nActivation Storage\nScaling Behavior\n\n\n\n\nMLP\nLinear\n\\(O(N \\times W)\\)\n\\(O(B \\times W)\\)\nPredictable\n\n\nCNN\nConstant\n\\(O(K \\times C)\\)\n\\(O(B \\times H_{\\text{img}} \\times W_{\\text{img}})\\)\nEfficient\n\n\nRNN\nLinear\n\\(O(h^2)\\)\n\\(O(B \\times T \\times h)\\)\nChallenging\n\n\nTransformer\nQuadratic\n\\(O(N \\times d)\\)\n\\(O(B \\times N^2)\\)\nProblematic\n\n\n\n\n\n\nWhere:\n\n\\(N\\): Input or sequence size\n\\(W\\): Layer width\n\\(B\\): Batch size\n\\(K\\): Kernel size\n\\(C\\): Number of channels\n\\(H_{\\text{img}}\\): Height of input feature map (CNN)\n\\(W_{\\text{img}}\\): Width of input feature map (CNN)\n\\(h\\): Hidden state size (RNN)\n\\(T\\): Sequence length\n\\(d\\): Model dimensionality\n\nTable 4.3 reveals how memory requirements scale with different architectural choices. The quadratic scaling of activation storage in Transformers, for instance, highlights the need for large memory capacities and efficient memory management in systems designed for Transformer-based workloads. In contrast, CNNs exhibit more favorable memory scaling due to their parameter sharing and localized processing. These memory complexity considerations are crucial when making system-level design decisions, such as choosing memory hierarchy configurations and developing memory optimization strategies.\nThe impact of these patterns becomes clearer when we consider data reuse opportunities. In CNNs, each input pixel participates in multiple convolution windows (typically 9 times for a \\(3\\times 3\\) filter), making effective data reuse fundamental for performance. Modern GPUs provide multi-level cache hierarchies (L1, L2, shared memory) to capture this reuse, while software techniques like loop tiling ensure data remains in cache once loaded.\nWorking set size—the amount of data needed simultaneously for com­pu­ta­tion—varies dramatically across architectures. An MLP layer processing MNIST images might need only a few hundred KB (weights plus activations), while a Transformer processing long sequences can require several MB just for storing attention patterns. These differences directly influence hardware design choices, like the balance between compute units and on-chip memory, and software optimizations like activation checkpointing or attention approximation techniques.\nHaving a good grasp of these memory access patterns is essential as architectures evolve. The shift from CNNs to Transformers, for instance, has driven the development of hardware with larger on-chip memories and more sophisticated caching strategies to handle increased working sets and more dynamic access patterns. Future architectures will likely continue to be shaped by their memory access characteristics as much as their computational requirements.\n\n\n4.7.3 Data Movement Primitives\nWhile computational and memory access patterns define what operations occur where, data movement primitives characterize how information flows through the system. These patterns are key because data movement often consumes more time and energy than computation itself—moving data from off-chip memory typically requires 100-1000x more energy than performing a floating-point operation.\nFour fundamental data movement patterns are prevalent in deep learning architectures: broadcast, scatter, gather, and reduction. These patterns determine how data is distributed and collected across computational units.\nBroadcast operations send the same data to multiple destinations simultaneously. In matrix multiplication with batch size 32, each weight must be broadcast to process different inputs in parallel. Modern hardware supports this through specialized interconnects—NVIDIA GPUs provide hardware multicast capabilities achieving up to 600GB/s broadcast bandwidth, while TPUs use dedicated broadcast buses. Software frameworks optimize broadcasts by restructuring computations (like matrix tiling) to maximize data reuse.\nScatter operations distribute different elements to different destinations. When parallelizing a \\(512\\times 512\\) matrix multiplication across GPU cores, each core receives a subset of the computation. This parallelization is important for performance but challenging—memory conflicts and load imbalance can reduce efficiency by 50% or more. Hardware provides flexible interconnects (like NVIDIA’s NVLink offering 600 GB/s bi-directional bandwidth), while software frameworks employ sophisticated work distribution algorithms to maintain high utilization.\nGather operations collect data from multiple sources. In Transformer attention with sequence length 512, each query must gather information from 512 different key-value pairs. These irregular access patterns are challenging—random gathering can be \\(10\\times\\) slower than sequential access. Hardware supports this through high-bandwidth interconnects and large caches, while software frameworks employ techniques like attention pattern pruning to reduce gathering overhead.\nReduction operations combine multiple values into a single result through operations like summation. When computing attention scores in Transformers or layer outputs in MLPs, efficient reduction is essential. Hardware implements tree-structured reduction networks (reducing latency from \\(O(n)\\) to \\(O(\\log n)\\)), while software frameworks use optimized parallel reduction algorithms that can achieve near-theoretical peak performance.\nThese patterns combine in sophisticated ways. A Transformer attention operation with sequence length 512 and batch size 32 involves:\n\nBroadcasting query vectors (\\(512\\times 64\\) elements)\nGathering relevant keys and values (\\(512\\times 512\\times 64\\) elements)\nReducing attention scores (\\(512\\times 512\\) elements per sequence)\n\nThe evolution from CNNs to Transformers has increased reliance on gather and reduction operations, driving hardware innovations like more flexible interconnects and larger on-chip memories. As models grow (some now exceeding 100 billion parameters), efficient data movement becomes increasingly critical, leading to innovations like near-memory processing and sophisticated data flow optimizations.\n\n\n4.7.4 System Design Impact\nThe computational, memory access, and data movement primitives we’ve explored form the foundational requirements that shape the design of systems for deep learning. The way these primitives influence hardware design, create common bottlenecks, and drive trade-offs is important for developing efficient and effective deep learning systems.\nOne of the most significant impacts of these primitives on system design is the push towards specialized hardware. The prevalence of matrix multiplications and convolutions in deep learning has led to the development of tensor processing units (TPUs) and tensor cores in GPUs, which are specifically designed to perform these operations efficiently. These specialized units can perform many multiply-accumulate operations in parallel, dramatically accelerating the core computations of neural networks.\nMemory systems have also been profoundly influenced by the demands of deep learning primitives. The need to support both sequential and random access patterns efficiently has driven the development of sophisticated memory hierarchies. High-bandwidth memory (HBM) has become common in AI accelerators to support the massive data movement requirements, especially for operations like attention mechanisms in Transformers. On-chip memory hierarchies have grown in complexity, with multiple levels of caching and scratchpad memories to support the diverse working set sizes of different neural network layers.\nThe data movement primitives have particularly influenced the design of interconnects and on-chip networks. The need to support efficient broadcasts, gathers, and reductions has led to the development of more flexible and higher-bandwidth interconnects. Some AI chips now feature specialized networks-on-chip designed to accelerate common data movement patterns in neural networks.\nTable 4.4 summarizes the system implications of these primitives:\n\n\n\nTable 4.4: System implications of primitives.\n\n\n\n\n\n\n\n\n\n\n\nPrimitive\nHardware Impact\nSoftware Optimization\nKey Challenges\n\n\n\n\nMatrix Multiplication\nTensor Cores\nBatching, GEMM libraries\nParallelization, precision\n\n\nSliding Window\nSpecialized datapaths\nData layout optimization\nStride handling\n\n\nDynamic Computation\nFlexible routing\nDynamic graph execution\nLoad balancing\n\n\nSequential Access\nBurst mode DRAM\nContiguous allocation\nAccess latency\n\n\nRandom Access\nLarge caches\nMemory-aware scheduling\nCache misses\n\n\nBroadcast\nSpecialized interconnects\nOperation fusion\nBandwidth\n\n\nGather/Scatter\nHigh-bandwidth memory\nWork distribution\nLoad balancing\n\n\n\n\n\n\nDespite these advancements, several common bottlenecks persist in deep learning systems. Memory bandwidth often remains a key limitation, particularly for models with large working sets or those that require frequent random access. The energy cost of data movement, especially between off-chip memory and processing units, continues to be a significant concern. For large-scale models, the communication overhead in distributed training can become a bottleneck, limiting scaling efficiency.\nSystem designers must navigate complex trade-offs in supporting different primitives, each with unique characteristics that influence system design and performance. For example, optimizing for the dense matrix operations common in MLPs and CNNs might come at the cost of flexibility needed for the more dynamic computations in attention mechanisms. Supporting large working sets for Transformers might require sacrificing energy efficiency.\nBalancing these trade-offs requires careful consideration of the target workloads and deployment scenarios. Having a good grip on the nature of each primitive guides the development of both hardware and software optimizations in deep learning systems, allowing designers to make informed decisions about system architecture and resource allocation.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>DNN Architectures</span>"
    ]
  },
  {
    "objectID": "contents/core/dnn_architectures/dnn_architectures.html#conclusion",
    "href": "contents/core/dnn_architectures/dnn_architectures.html#conclusion",
    "title": "4  DNN Architectures",
    "section": "4.8 Conclusion",
    "text": "4.8 Conclusion\nDeep learning architectures, despite their diversity, exhibit common patterns in their algorithmic structures that significantly influence computational requirements and system design. In this chapter, we explored the intricate relationship between high-level architectural concepts and their practical implementation in computing systems.\nFrom the straightforward dense connections of MLPs to the complex, dynamic patterns of Transformers, each architecture builds upon a set of fundamental building blocks. These core computational primitives—such as matrix multiplication, sliding windows, and dynamic computation—recur across various architectures, forming a universal language of deep learning computation.\nThe identification of these shared elements provides a valuable framework for understanding and designing deep learning systems. Each primitive brings its own set of requirements in terms of memory access patterns and data movement, which in turn shape both hardware and software design decisions. This relationship between algorithmic intent and system implementation is crucial for optimizing performance and efficiency.\nAs the field of deep learning continues to evolve, the ability to efficiently support and optimize these fundamental building blocks will be key to the development of more powerful and scalable systems. Future advancements in deep learning are likely to stem not only from novel architectural designs but also from innovative approaches to implementing and optimizing these essential computational patterns.\nIn conclusion, understanding the mapping between neural architectures and their computational requirements is vital for pushing the boundaries of what’s possible in artificial intelligence. As we look to the future, the interplay between algorithmic innovation and systems optimization will continue to drive progress in this rapidly advancing field.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>DNN Architectures</span>"
    ]
  },
  {
    "objectID": "contents/core/workflow/workflow.html",
    "href": "contents/core/workflow/workflow.html",
    "title": "5  AI Workflow",
    "section": "",
    "text": "Purpose\nResources: Slides, Videos, Exercises\nWhat are the diverse elements of AI systems and how do we combine to create effective machine learning system solutions?\nThe creation of practical AI solutions requires the orchestration of multiple components into coherent workflows. Workflow design highlights the connections and interactions that animate these components. This systematic perspective reveals how data flow, model training, and deployment considerations are intertwined to form robust AI systems. Analyzing these interconnections offers important insights into system-level design choices, establishing a framework for understanding how theoretical concepts can be translated into deployable solutions that meet real-world needs.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>AI Workflow</span>"
    ]
  },
  {
    "objectID": "contents/core/workflow/workflow.html#purpose",
    "href": "contents/core/workflow/workflow.html#purpose",
    "title": "5  AI Workflow",
    "section": "",
    "text": "Learning Objectives\n\n\n\n\nUnderstand the ML lifecycle and gain insights into the structured approach and stages of developing, deploying, and maintaining machine learning models.\nIdentify the unique challenges and distinctions between lifecycles for traditional machine learning and specialized applications.\nExplore the various people and roles involved in ML projects.\nExamine the importance of system-level considerations, including resource constraints, infrastructure, and deployment environments.\nAppreciate the iterative nature of ML lifecycles and how feedback loops drive continuous improvement in real-world applications.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>AI Workflow</span>"
    ]
  },
  {
    "objectID": "contents/core/workflow/workflow.html#overview",
    "href": "contents/core/workflow/workflow.html#overview",
    "title": "5  AI Workflow",
    "section": "5.1 Overview",
    "text": "5.1 Overview\nThe machine learning lifecycle is a systematic, interconnected process that guides the transformation of raw data into actionable models deployed in real-world applications. Each stage builds upon the outcomes of the previous one, creating an iterative cycle of refinement and improvement that supports robust, scalable, and reliable systems.\nFigure 5.1 illustrates the lifecycle as a series of stages connected through continuous feedback loops. The process begins with data collection, which ensures a steady input of raw data from various sources. The collected data progresses to data ingestion, where it is prepared for downstream machine learning applications. Subsequently, data analysis and curation involve inspecting and selecting the most appropriate data for the task at hand. Following this, data labeling and data validation, which nowadays involves both humans and AI itself, ensure that the data is properly annotated and verified for usability before advancing further.\n\n\n\n\n\n\nFigure 5.1: The ML lifecycle.\n\n\n\nThe data then enters the preparation stage, where it is transformed into machine learning-ready datasets through processes such as splitting and versioning. These datasets are used in the model training stage, where machine learning algorithms are applied to create predictive models. The resulting models are rigorously tested in the model evaluation stage, where performance metrics, such as key performance indicators (KPIs), are computed to assess reliability and effectiveness. The validated models move to the ML system validation phase, where they are verified for deployment readiness. Once validated, these models are integrated into production systems during the ML system deployment stage, ensuring alignment with operational requirements. The final stage tracks the performance of deployed systems in real time, enabling continuous adaptation to new data and evolving conditions.\nThis general lifecycle forms the backbone of machine learning systems, with each stage contributing to the creation, validation, and maintenance of scalable and efficient solutions. While the lifecycle provides a detailed view of the interconnected processes in machine learning systems, it can be distilled into a simplified framework for practical implementation.\nEach stage aligns with one of the following overarching categories:\n\nData Collection and Preparation ensures the availability of high-quality, representative datasets.\nModel Development and Training focuses on creating accurate and efficient models tailored to the problem at hand.\nEvaluation and Validation rigorously tests models to ensure reliability and robustness in real-world conditions.\nDeployment and Integration translates models into production-ready systems that align with operational realities.\nMonitoring and Maintenance ensures ongoing system performance and adaptability in dynamic environments.\n\nA defining feature of this framework is its iterative and dynamic nature. Feedback loops, such as those derived from monitoring that guide data collection improvements or deployment adjustments, ensure that machine learning systems maintain effectiveness and relevance over time. This adaptability is critical for addressing challenges such as shifting data distributions, operational constraints, and evolving user requirements.\nBy studying this framework, we establish a solid foundation for exploring specialized topics such as data engineering, model optimization, and deployment strategies in subsequent chapters. Viewing the ML lifecycle as an integrated and iterative process promotes a deeper understanding of how systems are designed, implemented, and maintained over time. To that end, this chapter focuses on the machine learning lifecycle as a systems-level framework, providing a high-level overview that bridges theoretical concepts with practical implementation. Through an examination of the lifecycle in its entirety, we gain insight into the interdependencies among its stages and the iterative processes that ensure long-term system scalability and relevance.\n\n5.1.1 Definition\nThe ML lifecycle is a structured, iterative process that guides the development, evaluation, and refinement of machine learning systems. Integrating machine learning into broader software engineering practices introduces unique challenges that require systematic approaches to experimentation and adapting systems over time (Amershi et al. 2019).\n\nAmershi, Saleema, Andrew Begel, Christian Bird, Rob DeLine, Harald Gall, Ece Kamar, Nachiappan Nagappan, Besmira Nushi, and Thomas Zimmermann. 2019. “Software Engineering for Machine Learning: A Case Study.” Proceedings of the 41st International Conference on Software Engineering: Software Engineering in Practice, 291–300.\n\n\n\n\n\n\nDefinition of the ML Lifecycle\n\n\n\nThe Machine Learning (ML) lifecycle is a structured, iterative process that defines the key stages required to develop, evaluate, and refine ML systems.\n\n\nThe ML lifecycle emphasizes achieving specific objectives at each stage rather than prescribing rigid methodologies. This flexibility allows practitioners to adapt their approaches to the distinct challenges of individual projects. Typically, the lifecycle includes stages such as problem formulation, data acquisition and preprocessing, model development and training, evaluation, deployment, and continuous optimization.\nWhile these stages generally progress sequentially, they are often revisited, creating a dynamic and interconnected process. This iterative approach fosters feedback loops, where lessons from later stages inform earlier ones. For example, insights gained during deployment might highlight deficiencies in data preparation or model design. Such adaptability requires a development approach that embraces flexibility and frequent iteration.\nFrom a pedagogical perspective, the ML lifecycle provides a framework for breaking down the complexities of machine learning into manageable, interconnected components. Each stage, from problem formulation to iterative refinement, can be examined in depth, giving students a clear roadmap to understand and master the distinct subcomponents of machine learning systems. This structure mirrors real-world development while enabling a systematic exploration of the field’s core concepts.\nIt is important to distinguish between the ML lifecycle and MLOps (Machine Learning Operations), as these terms are often conflated. The ML lifecycle, as discussed in this chapter, focuses on the stages and evolution of ML systems—the “what” and “why” of development. MLOps, which will be explored in the MLOps Chapter, addresses the “how,” including tools, practices, and automation that enable the efficient implementation of lifecycle stages. Beginning with the lifecycle establishes a conceptual foundation for later discussions of operational considerations.\n\n\n5.1.2 Comparison with Traditional Lifecycles\nSoftware development lifecycles have evolved through decades of engineering practice, establishing well-defined patterns for system development. Traditional lifecycles consist of sequential phases: requirements gathering, system design, implementation, testing, and deployment. Each phase produces specific artifacts that serve as inputs to subsequent phases. In financial software development, for instance, the requirements phase produces detailed specifications for transaction processing, security protocols, and regulatory compliance—specifications that directly translate into system behavior through explicit programming.\nMachine learning systems require a fundamentally different approach to this traditional lifecycle model. The deterministic nature of conventional software, where behavior is explicitly programmed, contrasts sharply with the probabilistic nature of ML systems. Consider financial transaction processing: traditional systems follow predetermined rules (if account balance &gt; transaction amount, then allow transaction), while ML-based fraud detection systems learn to recognize suspicious patterns from historical transaction data. This shift from explicit programming to learned behavior fundamentally reshapes the development lifecycle.\nThe unique characteristics of machine learning systems—data dependency, probabilistic outputs, and evolving performance—introduce new dynamics that alter how lifecycle stages interact. These systems require ongoing refinement, with insights from later stages frequently feeding back into earlier ones. Unlike traditional systems, where lifecycle stages aim to produce stable outputs, machine learning systems are inherently dynamic and must adapt to changing data distributions and objectives.\nThe key distinctions are summarized in Table 5.1 below:\n\n\n\nTable 5.1: Differences between traditional and ML lifecycles.\n\n\n\n\n\n\n\n\n\n\nAspect\nTraditional Software Lifecycles\nMachine Learning Lifecycles\n\n\n\n\nProblem Definition\nPrecise functional specifications are defined upfront.\nPerformance-driven objectives evolve as the problem space is explored.\n\n\nDevelopment Process\nLinear progression of feature implementation.\nIterative experimentation with data, features and models.\n\n\nTesting and Validation\nDeterministic, binary pass/fail testing criteria.\nStatistical validation and metrics that involve uncertainty.\n\n\nDeployment\nBehavior remains static until explicitly updated.\nPerformance may change over time due to shifts in data distributions.\n\n\nMaintenance\nMaintenance involves modifying code to address bugs or add features.\nContinuous monitoring, updating data pipelines, retraining models, and adapting to new data distributions.\n\n\nFeedback Loops\nMinimal; later stages rarely impact earlier phases.\nFrequent; insights from deployment and monitoring often refine earlier stages like data preparation and model design.\n\n\n\n\n\n\nThese differences underline the need for a robust ML lifecycle framework that can accommodate iterative development, dynamic behavior, and data-driven decision-making. This lifecycle ensures that machine learning systems remain effective not only at launch but throughout their operational lifespan, even as environments evolve.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>AI Workflow</span>"
    ]
  },
  {
    "objectID": "contents/core/workflow/workflow.html#stages-of-the-lifecycle",
    "href": "contents/core/workflow/workflow.html#stages-of-the-lifecycle",
    "title": "5  AI Workflow",
    "section": "5.2 Stages of the Lifecycle",
    "text": "5.2 Stages of the Lifecycle\nThe AI lifecycle consists of several interconnected stages, each essential to the development and maintenance of effective machine learning systems. While the specific implementation details may vary across projects and organizations, Figure 5.2 provides a high-level illustration of the ML system development lifecycle. This chapter focuses on the overview, with subsequent chapters diving into the implementation aspects of each stage.\n\n\n\n\n\n\nFigure 5.2: ML lifecycle overview.\n\n\n\nProblem Definition and Requirements: The first stage involves clearly defining the problem to be solved, establishing measurable performance objectives, and identifying key constraints. Precise problem definition ensures alignment between the system’s goals and the desired outcomes.\nData Collection and Preparation: This stage includes gathering relevant data, cleaning it, and preparing it for model training. This process often involves curating diverse datasets, ensuring high-quality labeling, and developing preprocessing pipelines to address variations in the data.\nModel Development and Training: In this stage, researchers select appropriate algorithms, design model architectures, and train models using the prepared data. Success depends on choosing techniques suited to the problem and iterating on the model design for optimal performance.\nEvaluation and Validation: Evaluation involves rigorously testing the model’s performance against predefined metrics and validating its behavior in different scenarios. This stage ensures the model is not only accurate but also reliable and robust in real-world conditions.\nDeployment and Integration: Once validated, the trained model is integrated into production systems and workflows. This stage requires addressing practical challenges such as system compatibility, scalability, and operational constraints.\nMonitoring and Maintenance: The final stage focuses on continuously monitoring the system’s performance in real-world environments and maintaining or updating it as necessary. Effective monitoring ensures the system remains relevant and accurate over time, adapting to changes in data, requirements, or external conditions.\nA Case Study in Medical AI: To further ground our discussion on these stages, we will explore Google’s Diabetic Retinopathy (DR) screening project as a case study. This project exemplifies the transformative potential of machine learning in medical imaging analysis, an area where the synergy between algorithmic innovation and robust systems engineering plays a pivotal role. Building upon the foundational work by Gulshan et al. (2016), which demonstrated the effectiveness of deep learning algorithms in detecting diabetic retinopathy from retinal fundus photographs, the project progressed from research to real-world deployment, revealing the complex challenges that characterize modern ML systems.\n\nGulshan, Varun, Lily Peng, Marc Coram, Markus C Stumpe, Derek Wu, Arunachalam Narayanaswamy, Subhashini Venugopalan, et al. 2016. “Development and Validation of a Deep Learning Algorithm for Detection of Diabetic Retinopathy in Retinal Fundus Photographs.” JAMA 316 (22): 2402–10. https://doi.org/10.1001/jama.2016.17216.\nDiabetic retinopathy, a leading cause of preventable blindness worldwide, can be detected through regular screening of retinal photographs. Figure 5.3 illustrates examples of such images: (A) a healthy retina and (B) a retina with diabetic retinopathy, marked by hemorrhages (red spots). The goal is to train a model to detect the hemorrhages.\n\n\n\n\n\n\nFigure 5.3: Retinal fundus photos: (A) healthy retina and (B) retina with diabetic retinopathy showing hemorrhages (red spots). Source: Google\n\n\n\nOn the surface, the goal appears straightforward: develop an AI system that could analyze retinal images and identify signs of DR with accuracy comparable to expert ophthalmologists. However, as the project progressed from research to real-world deployment, it revealed the complex challenges that characterize modern ML systems.\nThe initial results in controlled settings were promising. The system achieved performance comparable to expert ophthalmologists in detecting DR from high-quality retinal photographs. Yet, when the team attempted to deploy the system in rural clinics across Thailand and India, they encountered a series of challenges that spanned the entire ML lifecycle, from data collection through deployment and maintenance.\nThis case study will serve as a recurring thread throughout this chapter to illustrate how success in machine learning systems depends on more than just model accuracy. It requires careful orchestration of data pipelines, training infrastructure, deployment systems, and monitoring frameworks. Furthermore, the project highlights the iterative nature of ML system development, where real-world deployment often necessitates revisiting and refining earlier stages.\nWhile this narrative is inspired by Google’s documented experiences in Thailand and India, certain aspects have been embellished to emphasize specific challenges frequently encountered in real-world healthcare ML deployments. These enhancements are to provide a richer understanding of the complexities involved while maintaining credibility and relevance to practical applications.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>AI Workflow</span>"
    ]
  },
  {
    "objectID": "contents/core/workflow/workflow.html#problem-definition-and-requirements",
    "href": "contents/core/workflow/workflow.html#problem-definition-and-requirements",
    "title": "5  AI Workflow",
    "section": "5.3 Problem Definition and Requirements",
    "text": "5.3 Problem Definition and Requirements\nThe development of machine learning systems begins with a critical challenge that fundamentally differs from traditional software development: defining not just what the system should do, but how it should learn to do it. Unlike conventional software, where requirements directly translate into implementation rules, ML systems require teams to consider how the system will learn from data while operating within real-world constraints. This stage lays the foundation for all subsequent phases in the ML lifecycle.\nIn our case study, diabetic retinopathy (DR) represents a problem that blends technical complexity with global healthcare implications. With 415 million diabetic patients at risk of blindness worldwide and limited access to specialists in underserved regions, defining the problem required balancing technical goals—like expert-level diagnostic accuracy—with practical constraints. The system needed to prioritize cases for early intervention while operating effectively in resource-limited settings. These constraints showcased how problem definition must integrate learning capabilities with operational needs to deliver actionable and sustainable solutions.\n\n5.3.1 Problem Requirements and System Impact\nDefining an ML problem involves more than specifying desired performance metrics. It requires a deep understanding of the broader context in which the system will operate. For instance, developing a system to detect DR with expert-level accuracy might initially appear to be a straightforward classification task. After all, one might assume that training a model on a sufficiently large dataset of labeled retinal images and evaluating its performance against standard metrics would suffice.\nHowever, real-world challenges complicate this picture. ML systems must function effectively in diverse environments, where factors like computational constraints, data variability, and integration requirements play significant roles. For example, the DR system needed to detect subtle features like microaneurysms, hemorrhages, and hard exudates across retinal images of varying quality while operating within the limitations of hardware in rural clinics. A model that performs well in isolation may falter if it cannot handle operational realities, such as inconsistent imaging conditions or time-sensitive clinical workflows. Addressing these factors requires aligning learning objectives with system constraints, ensuring the system’s long-term viability in its intended context.\n\n\n5.3.2 Problem Definition Workflow\nEstablishing clear and actionable problem definitions involves a multi-step workflow that bridges technical, operational, and user considerations. The process begins with identifying the core objective of the system—what tasks it must perform and what constraints it must satisfy. Teams collaborate with stakeholders to gather domain knowledge, outline requirements, and anticipate challenges that may arise in real-world deployment.\nIn the DR project, this phase involved close collaboration with clinicians to determine the diagnostic needs of rural clinics. Key decisions, such as balancing model complexity with hardware limitations and ensuring interpretability for healthcare providers, were made during this phase. The team’s iterative approach also accounted for regulatory considerations, such as patient privacy and compliance with healthcare standards. This collaborative process ensured that the problem definition aligned with both technical feasibility and clinical relevance.\n\n\n5.3.3 Scale and Distribution Challenges\nAs ML systems scale, their problem definitions must adapt to new operational challenges. For example, the DR project initially focused on a limited number of clinics with consistent imaging setups. However, as the system expanded to include clinics with varying equipment, staff expertise, and patient demographics, the original problem definition required adjustments to accommodate these variations.\nScaling also introduces data challenges. Larger datasets may include more diverse edge cases, which can expose weaknesses in the initial model design. In the DR project, for instance, expanding the deployment to new regions introduced variations in imaging equipment and patient populations that required further tuning of the system. Defining a problem that accommodates such diversity from the outset ensures the system can handle future expansion without requiring a complete redesign.\n\n\n5.3.4 Systems Thinking\nProblem definition, viewed through a systems lens, connects deeply with every stage of the ML lifecycle. Choices made during this phase shape how data is collected, how models are developed, and how systems are deployed and maintained. A poorly defined problem can lead to inefficiencies or failures in later stages, emphasizing the need for a holistic perspective.\nFeedback loops are central to effective problem definition. As the system evolves, real-world feedback from deployment and monitoring often reveals new constraints or requirements that necessitate revisiting the problem definition. For example, feedback from clinicians about system usability or patient outcomes may guide refinements in the original goals. In the DR project, the need for interpretable outputs that clinicians could trust and act upon influenced both model development and deployment strategies.\nEmergent behaviors also play a role. A system that was initially designed to detect retinopathy might reveal additional use cases, such as identifying other conditions like diabetic macular edema, which can reshape the problem’s scope and requirements. In the DR project, insights from deployment highlighted potential extensions to other imaging modalities, such as 3D Optical Coherence Tomography (OCT).\nResource dependencies further highlight the interconnectedness of problem definition. Decisions about model complexity, for instance, directly affect infrastructure needs, data collection strategies, and deployment feasibility. Balancing these dependencies requires careful planning during the problem definition phase, ensuring that early decisions do not create bottlenecks in later stages.\n\n\n5.3.5 Lifecycle Implications\nThe problem definition phase is foundational, influencing every subsequent stage of the lifecycle. A well-defined problem ensures that data collection focuses on the most relevant features, that models are developed with the right constraints in mind, and that deployment strategies align with operational realities.\nIn the DR project, defining the problem with scalability and adaptability in mind enabled the team to anticipate future challenges, such as accommodating new imaging devices or expanding to additional clinics. For instance, early considerations of diverse imaging conditions and patient demographics reduced the need for costly redesigns later in the lifecycle. This forward-thinking approach ensured the system’s long-term success and adaptability in dynamic healthcare environments.\nBy embedding lifecycle thinking into problem definition, teams can create systems that not only meet initial requirements but also adapt and evolve in response to changing conditions. This ensures that ML systems remain effective, scalable, and impactful over time.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>AI Workflow</span>"
    ]
  },
  {
    "objectID": "contents/core/workflow/workflow.html#data-collection-and-preparation",
    "href": "contents/core/workflow/workflow.html#data-collection-and-preparation",
    "title": "5  AI Workflow",
    "section": "5.4 Data Collection and Preparation",
    "text": "5.4 Data Collection and Preparation\nData is the foundation of machine learning systems, yet collecting and preparing data for ML applications introduces challenges that extend far beyond gathering enough training examples. Modern ML systems often need to handle terabytes of data—ranging from raw, unstructured inputs to carefully annotated datasets—while maintaining quality, diversity, and relevance for model training. For medical systems like DR screening, data preparation must meet the highest standards to ensure diagnostic accuracy.\nIn the DR project, data collection involved a development dataset of 128,000 retinal fundus photographs evaluated by a panel of 54 ophthalmologists, with each image reviewed by 3-7 experts. This collaborative effort ensured high-quality labels that captured clinically relevant features like microaneurysms, hemorrhages, and hard exudates. Additionally, clinical validation datasets comprising 12,000 images provided an independent benchmark to test the model’s robustness against real-world variability, illustrating the importance of rigorous and representative data collection. The scale and complexity of this effort highlight how domain expertise and interdisciplinary collaboration are critical to building datasets for high-stakes ML systems.\n\n5.4.1 Data Requirements and System Impact\nThe requirements for data collection and preparation emerge from the dual perspectives of machine learning and operational constraints. In the DR project, high-quality retinal images annotated by experts were a foundational need to train accurate models. However, real-world conditions quickly revealed additional complexities. Images were collected from rural clinics using different camera equipment, operated by staff with varying levels of expertise, and often under conditions of limited network connectivity.\nThese operational realities shaped the system architecture in significant ways. The volume and size of high-resolution images necessitated local storage and preprocessing capabilities at clinics, as centralizing all data collection was impractical due to unreliable internet access. Furthermore, patient privacy regulations required secure data handling at every stage, from image capture to model training. Coordinating expert annotations also introduced logistical challenges, necessitating systems that could bridge the physical distance between clinics and ophthalmologists while maintaining workflow efficiency.\nThese considerations demonstrate how data collection requirements influence the entire ML lifecycle. Infrastructure design, annotation pipelines, and privacy protocols all play critical roles in ensuring that collected data aligns with both technical and operational goals.\n\n\n5.4.2 Data Flow and Infrastructure\nThe flow of data through the system highlights critical infrastructure requirements at every stage. In the DR project, the journey of a single retinal image offers a glimpse into these complexities. From its capture on a retinal camera, where image quality is paramount, the data moves through local clinic systems for initial storage and preprocessing. Eventually, it must reach central systems where it is aggregated with data from other clinics for model training and validation.\nAt each step, the system must balance local needs with centralized aggregation requirements. Clinics with reliable high-speed internet could transmit data in real-time, but many rural locations relied on store-and-forward systems, where data was queued locally and transmitted in bulk when connectivity permitted. These differences necessitated flexible infrastructure that could adapt to varying conditions while maintaining data consistency and integrity across the lifecycle. This adaptability ensured that the system could function reliably despite the diverse operational environments of the clinics.\n\n\n5.4.3 Scale and Distribution Challenges\nAs ML systems scale, the challenges of data collection grow exponentially. In the DR project, scaling from an initial few clinics to a broader network introduced significant variability in equipment, workflows, and operating conditions. Each clinic effectively became an independent data node, yet the system needed to ensure consistent performance and reliability across all locations.\nThis scaling effort also brought increasing data volumes, as higher-resolution imaging devices became standard, generating larger and more detailed images. These advances amplified the demands on storage and processing infrastructure, requiring optimizations to maintain efficiency without compromising quality. Differences in patient demographics, clinic workflows, and connectivity patterns further underscored the need for robust design to handle these variations gracefully.\nScaling challenges highlight how decisions made during the data collection phase ripple through the lifecycle, impacting subsequent stages like model development, deployment, and monitoring. For instance, accommodating higher-resolution data during collection directly influences computational requirements for training and inference, emphasizing the need for lifecycle thinking even at this early stage.\n\n\n5.4.4 Quality and Validation Systems\nQuality assurance is an integral part of the data collection process, ensuring that data meets the requirements for downstream stages. In the DR project, automated checks at the point of collection flagged issues like poor focus or incorrect framing, allowing clinic staff to address problems immediately. These proactive measures ensured that low-quality data was not propagated through the pipeline.\nValidation systems extended these efforts by verifying not just image quality but also proper labeling, patient association, and compliance with privacy regulations. Operating at both local and centralized levels, these systems ensured data reliability and robustness, safeguarding the integrity of the entire ML pipeline.\n\n\n5.4.5 Systems Thinking\nViewing data collection and preparation through a lifecycle lens reveals the interconnected nature of these processes. Each decision made during this phase influences subsequent stages of the ML system. For instance, choices about camera equipment and image preprocessing affect not only the quality of the training dataset but also the computational requirements for model development and the accuracy of predictions during deployment.\nFigure 5.4 illustrates the key feedback loops that characterize the ML lifecycle, with particular relevance to data collection and preparation. Looking at the left side of the diagram, we see how monitoring and maintenance activities feed back to both data collection and preparation stages. For example, when monitoring reveals data quality issues in production (shown by the “Data Quality Issues” feedback arrow), this triggers refinements in our data preparation pipelines. Similarly, performance insights from deployment might highlight gaps in our training data distribution (indicated by the “Performance Insights” loop back to data collection), prompting the collection of additional data to cover underrepresented cases. In the DR project, this manifested when monitoring revealed that certain demographic groups were underrepresented in the training data, leading to targeted data collection efforts to improve model fairness and accuracy across all populations.\n\n\n\n\n\n\nFigure 5.4: Feedback loops and dependencies between stages in the ML lifecycle.\n\n\n\nFeedback loops are another critical aspect of this lifecycle perspective. Insights from model performance often lead to adjustments in data collection strategies, creating an iterative improvement process. For example, in the DR project, patterns observed during model evaluation influenced updates to preprocessing pipelines, ensuring that new data aligned with the system’s evolving requirements.\nThe scaling of data collection introduces emergent behaviors that must be managed holistically. While individual clinics may function well in isolation, the simultaneous operation of multiple clinics can lead to system-wide patterns like network congestion or storage bottlenecks. These behaviors reinforce the importance of considering data collection as a system-level challenge rather than a discrete, isolated task.\nIn the following chapters, we will step through each of the major stages of the lifecycle shown in Figure 5.4. We will consider several key questions like what influences data source selection, how feedback loops can be systematically incorporated, and how emergent behaviors can be anticipated and managed holistically.\nIn addition, by adopting a systems thinking approach, we emphasize the iterative and interconnected nature of the ML lifecycle. How do choices in data collection and preparation ripple through the entire pipeline? What mechanisms ensure that monitoring insights and performance evaluations effectively inform improvements at earlier stages? And how can governance frameworks and infrastructure design evolve to meet the challenges of scaling while maintaining fairness and efficiency? These questions will guide our exploration of the lifecycle, offering a foundation for designing robust and adaptive ML systems.\n\n\n5.4.6 Lifecycle Implications\nThe success of ML systems depends on how effectively data collection integrates with the entire lifecycle. Decisions made in this stage affect not only the quality of the initial model but also the system’s ability to evolve and adapt. For instance, data distribution shifts or changes in imaging equipment over time require the system to handle new inputs without compromising performance.\nIn the DR project, embedding lifecycle thinking into data management strategies ensured the system remained robust and scalable as it expanded to new clinics and regions. By proactively addressing variability and quality during data collection, the team minimized the need for costly downstream adjustments, aligning the system with long-term goals and operational realities.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>AI Workflow</span>"
    ]
  },
  {
    "objectID": "contents/core/workflow/workflow.html#model-development-and-training",
    "href": "contents/core/workflow/workflow.html#model-development-and-training",
    "title": "5  AI Workflow",
    "section": "5.5 Model Development and Training",
    "text": "5.5 Model Development and Training\nModel development and training form the core of machine learning systems, yet this stage presents unique challenges that extend far beyond selecting algorithms and tuning hyperparameters. It involves designing architectures suited to the problem, optimizing for computational efficiency, and iterating on models to balance performance with deployability. In high-stakes domains like healthcare, the stakes are particularly high, as every design decision impacts clinical outcomes.\nFor DR detection, the model needed to achieve expert-level accuracy while handling the high resolution and variability of retinal images. Using a deep neural network trained on their meticulously labeled dataset, the team achieved an F-score of 0.95, slightly exceeding the median score of the consulted ophthalmologists (0.91). This outcome highlights the effectiveness of state-of-the-art methods, such as transfer learning, and the importance of interdisciplinary collaboration between data scientists and medical experts to refine features and interpret model outputs.\n\n5.5.1 Model Requirements and System Impact\nThe requirements for model development emerge not only from the specific learning task but also from broader system constraints. In the DR project, the model needed high sensitivity and specificity to detect different stages of retinopathy. However, achieving this purely from an ML perspective was not sufficient. The system had to meet operational constraints, including running on limited hardware in rural clinics, producing results quickly enough to fit into clinical workflows, and being interpretable enough for healthcare providers to trust its outputs.\nThese requirements shaped decisions during model development. While state-of-the-art accuracy might favor the largest and most complex models, such approaches were infeasible given hardware and workflow constraints. The team focused on designing architectures that balanced accuracy with efficiency, exploring lightweight models that could perform well on constrained devices. For example, techniques like pruning and quantization were employed to optimize the models for resource-limited environments, ensuring compatibility with rural clinic infrastructure.\nThis balancing act influenced every part of the system lifecycle. Decisions about model architecture affected data preprocessing, shaped the training infrastructure, and determined deployment strategies. For example, choosing to use an ensemble of smaller models instead of a single large model altered data batching during training, required changes to inference pipelines, and introduced complexities in how model updates were managed in production.\n\n\n5.5.2 Model Development Workflow\nThe model development workflow reflects the complex interplay between data, compute resources, and human expertise. In the DR project, this process began with data exploration and feature engineering, where data scientists collaborated with ophthalmologists to identify image characteristics indicative of retinopathy.\nThis initial stage required tools capable of handling large medical images and facilitating experimentation with preprocessing techniques. The team needed an environment that supported collaboration, visualization, and rapid iteration while managing the sheer scale of high-resolution data.\nAs the project advanced to model design and training, computational demands escalated. Training deep learning models on high-resolution images required extensive GPU resources and sophisticated infrastructure. The team implemented distributed training systems that could scale across multiple machines while managing large datasets, tracking experiments, and ensuring reproducibility. These systems also supported experiment comparison, enabling rapid evaluation of different architectures, hyperparameters, and preprocessing pipelines.\nModel development was inherently iterative, with each cycle—adjusting DNN architectures, refining hyperparameters, or incorporating new data—producing extensive metadata, including checkpoints, validation results, and performance metrics. Managing this information across the team required robust tools for experiment tracking and version control to ensure that progress remained organized and reproducible.\n\n\n5.5.3 Scale and Distribution Challenges\nAs ML systems scale in both data volume and model complexity, the challenges of model development grow exponentially. The DR project’s evolution from prototype models to production-ready systems highlights these hurdles. Expanding datasets, more sophisticated models, and concurrent experiments demanded increasingly powerful computational resources and meticulous organization.\nDistributed training became essential to meet these demands. While it significantly reduced training time, it introduced complexities in data synchronization, gradient aggregation, and fault tolerance. The team relied on advanced frameworks to optimize GPU clusters, manage network latency, and address hardware failures, ensuring training processes remained efficient and reliable. These frameworks included automated failure recovery mechanisms, which helped maintain progress even in the event of hardware interruptions.\nThe need for continuous experimentation and improvement compounded these challenges. Over time, the team managed an expanding repository of model versions, training datasets, and experimental results. This growth required scalable systems for tracking experiments, versioning models, and analyzing results to maintain consistency and focus across the project.\n\n\n5.5.4 Systems Thinking\nApproaching model development through a systems perspective reveals its connections to every other stage of the ML lifecycle. Decisions about model architecture ripple through the system, influencing preprocessing requirements, deployment strategies, and clinical workflows. For instance, adopting a complex model might improve accuracy but increase memory usage, complicating deployment in resource-constrained environments.\nFeedback loops are inherent to this stage. Insights from deployment inform adjustments to models, while performance on test sets guides future data collection and annotation. Understanding these cycles is critical for iterative improvement and long-term success.\nScaling model development introduces emergent behaviors, such as bottlenecks in shared resources or unexpected interactions between multiple training experiments. Addressing these behaviors requires robust planning and the ability to anticipate system-wide patterns that might arise from local changes.\nThe boundaries between model development and other lifecycle stages often blur. Feature engineering overlaps with data preparation, while optimization for inference spans both development and deployment. Navigating these overlaps effectively requires careful coordination and clear interface definitions.\n\n\n5.5.5 Lifecycle Implications\nModel development is not an isolated task; it exists within the broader ML lifecycle. Decisions made here influence data preparation strategies, training infrastructure, and deployment feasibility. The iterative nature of this stage ensures that insights gained feed back into data collection and system optimization, reinforcing the interconnectedness of the lifecycle.\nIn subsequent chapters, we will explore key questions that arise during model development:\n\nHow can scalable training infrastructures be designed for large-scale ML models?\nWhat frameworks and tools help manage the complexity of distributed training?\nHow can model reproducibility and version control be ensured in evolving projects?\nWhat trade-offs must be made to balance accuracy with operational constraints?\nHow can continual learning and updates be handled in production systems?\n\nThese questions highlight how model development sits at the core of ML systems, with decisions in this stage resonating throughout the entire lifecycle.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>AI Workflow</span>"
    ]
  },
  {
    "objectID": "contents/core/workflow/workflow.html#deployment-and-integration",
    "href": "contents/core/workflow/workflow.html#deployment-and-integration",
    "title": "5  AI Workflow",
    "section": "5.6 Deployment and Integration",
    "text": "5.6 Deployment and Integration\nOnce validated, the trained model is integrated into production systems and workflows. Deployment requires addressing practical challenges such as system compatibility, scalability, and operational constraints. Successful integration hinges on ensuring that the model’s predictions are not only accurate but also actionable in real-world settings, where resource limitations and workflow disruptions can pose significant barriers.\nIn the DR project, deployment strategies were shaped by the diverse environments in which the system would operate. Edge deployment enabled local processing of retinal images in rural clinics with intermittent connectivity, while automated quality checks flagged poor-quality images for recapture, ensuring reliable predictions. These measures demonstrate how deployment must bridge technological sophistication with usability and scalability across varied clinical settings.\n\n5.6.1 Deployment Requirements and System Impact\nThe requirements for deployment stem from both the technical specifications of the model and the operational constraints of its intended environment. In the DR project, the model needed to operate in rural clinics with limited computational resources and intermittent internet connectivity. Additionally, it had to fit seamlessly into the existing clinical workflow, which required rapid, interpretable results that could assist healthcare providers without causing disruption.\nThese requirements influenced deployment strategies significantly. A cloud-based deployment, while technically simpler, was not feasible due to unreliable connectivity in many clinics. Instead, the team opted for edge deployment, where models ran locally on clinic hardware. This approach required optimizing the model for smaller, less powerful devices while maintaining high accuracy. Optimization techniques such as model quantization and pruning were employed to reduce resource demands without sacrificing performance.\nIntegration with existing systems posed additional challenges. The ML system had to interface with hospital information systems (HIS) for accessing patient records and storing results. Privacy regulations mandated secure data handling at every step, further shaping deployment decisions. These considerations ensured that the system adhered to clinical and legal standards while remaining practical for daily use.\n\n\n5.6.2 Deployment and Integration Workflow\nThe deployment and integration workflow in the DR project highlighted the interplay between model functionality, infrastructure, and user experience. The process began with thorough testing in simulated environments that replicated the technical constraints and workflows of the target clinics. These simulations helped identify potential bottlenecks and incompatibilities early, allowing the team to refine the deployment strategy before full-scale rollout.\nOnce the deployment strategy was finalized, the team implemented a phased rollout. Initial deployments were limited to a few pilot sites, allowing for controlled testing in real-world conditions. This approach provided valuable feedback from clinicians and technical staff, helping to identify issues that hadn’t surfaced during simulations.\nIntegration efforts focused on ensuring seamless interaction between the ML system and existing tools. For example, the DR system had to pull patient information from the HIS, process retinal images from connected cameras, and return results in a format that clinicians could easily interpret. These tasks required the development of robust APIs, real-time data processing pipelines, and user-friendly interfaces tailored to the needs of healthcare providers.\n\n\n5.6.3 Scale and Distribution Challenges\nScaling deployment across multiple locations introduced new complexities. Each clinic had unique infrastructure, ranging from differences in imaging equipment to variations in network reliability. These differences necessitated flexible deployment strategies that could adapt to diverse environments while ensuring consistent performance.\nDespite achieving high performance metrics during development, the DR system faced unexpected challenges in real-world deployment. For example, in rural clinics, variations in imaging equipment and operator expertise led to inconsistencies in image quality that the model struggled to handle. These issues underscored the gap between laboratory success and operational reliability, prompting iterative refinements in both the model and the deployment strategy. Feedback from clinicians further revealed that initial system interfaces were not intuitive enough for widespread adoption, leading to additional redesigns.\nDistribution challenges extended beyond infrastructure variability. The team needed to maintain synchronized updates across all deployment sites to ensure that improvements in model performance or system features were universally applied. This required implementing centralized version control systems and automated update pipelines that minimized disruption to clinical operations.\nDespite achieving high performance metrics during development, the DR system faced unexpected challenges in real-world deployment. As illustrated in Figure 5.4, these challenges create multiple feedback paths—“Deployment Constraints” flowing back to model training to trigger optimizations, while “Performance Insights” from monitoring could necessitate new data collection. For example, when the system struggled with images from older camera models, this triggered both model optimizations and targeted data collection to improve performance under these conditions.\nAnother critical scaling challenge was training and supporting end-users. Clinicians and staff needed to understand how to operate the system, interpret its outputs, and provide feedback. The team developed comprehensive training programs and support channels to facilitate this transition, recognizing that user trust and proficiency were essential for system adoption.\n\n\n5.6.4 Ensuring Robustness and Reliability\nIn a clinical context, reliability is paramount. The DR system needed to function seamlessly under a wide range of conditions, from high patient volumes to suboptimal imaging setups. To ensure robustness, the team implemented fail-safes that could detect and handle common issues, such as incomplete or poor-quality data. These mechanisms included automated image quality checks and fallback workflows for cases where the system encountered errors.\nTesting played a central role in ensuring reliability. The team conducted extensive stress testing to simulate peak usage scenarios, validating that the system could handle high throughput without degradation in performance. Redundancy was built into critical components to minimize the risk of downtime, and all interactions with external systems, such as the HIS, were rigorously tested for compatibility and security.\n\n\n5.6.5 Systems Thinking\nDeployment and integration, viewed through a systems lens, reveal deep connections to every other stage of the ML lifecycle. Decisions made during model development influence deployment architecture, while choices about data handling affect integration strategies. Monitoring requirements often dictate how deployment pipelines are structured, ensuring compatibility with real-time feedback loops.\nFeedback loops are integral to deployment and integration. Real-world usage generates valuable insights that inform future iterations of model development and evaluation. For example, clinician feedback on system usability during the DR project highlighted the need for clearer interfaces and more interpretable outputs, prompting targeted refinements in design and functionality.\nEmergent behaviors frequently arise during deployment. In the DR project, early adoption revealed unexpected patterns, such as clinicians using the system for edge cases or non-critical diagnostics. These behaviors, which were not predicted during development, necessitated adjustments to both the system’s operational focus and its training programs.\nDeployment introduces significant resource dependencies. Running ML models on edge devices required balancing computational efficiency with accuracy, while ensuring other clinic operations were not disrupted. These trade-offs extended to the broader system, influencing everything from hardware requirements to scheduling updates without affecting clinical workflows.\nThe boundaries between deployment and other lifecycle stages are fluid. Optimization efforts for edge devices often overlapped with model development, while training programs for clinicians fed directly into monitoring and maintenance. Navigating these overlaps required clear communication and collaboration between teams, ensuring seamless integration and ongoing system adaptability.\nBy applying a systems perspective to deployment and integration, we can better anticipate challenges, design robust solutions, and maintain the flexibility needed to adapt to evolving operational and technical demands. This approach ensures that ML systems not only achieve initial success but remain effective and reliable in real-world applications.\n\n\n5.6.6 Lifecycle Implications\nDeployment and integration are not terminal stages; they are the point at which an ML system becomes operationally active and starts generating real-world feedback. This feedback loops back into earlier stages, informing data collection strategies, model improvements, and evaluation protocols. By embedding lifecycle thinking into deployment, teams can design systems that are not only operationally effective but also adaptable and resilient to evolving needs.\nIn subsequent chapters, we will explore key questions related to deployment and integration:\n\nHow can deployment strategies balance computational constraints with performance needs?\nWhat frameworks support scalable, synchronized deployments across diverse environments?\nHow can systems be designed for seamless integration with existing workflows and tools?\nWhat are best practices for ensuring user trust and proficiency in operating ML systems?\nHow do deployment insights feed back into the ML lifecycle to drive continuous improvement?\n\nThese questions emphasize the interconnected nature of deployment and integration within the lifecycle, highlighting the importance of aligning technical and operational priorities to create systems that deliver meaningful, lasting impact.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>AI Workflow</span>"
    ]
  },
  {
    "objectID": "contents/core/workflow/workflow.html#monitoring-and-maintenance",
    "href": "contents/core/workflow/workflow.html#monitoring-and-maintenance",
    "title": "5  AI Workflow",
    "section": "5.7 Monitoring and Maintenance",
    "text": "5.7 Monitoring and Maintenance\nMonitoring and maintenance represent the ongoing, critical processes that ensure the continued effectiveness and reliability of deployed machine learning systems. Unlike traditional software, ML systems must account for shifts in data distributions, changing usage patterns, and evolving operational requirements. Monitoring provides the feedback necessary to adapt to these challenges, while maintenance ensures the system evolves to meet new needs.\nAs shown in Figure 5.4, monitoring serves as a central hub for system improvement, generating three critical feedback loops: “Performance Insights” flowing back to data collection to address gaps, “Data Quality Issues” triggering refinements in data preparation, and “Model Updates” initiating retraining when performance drifts. In the DR project, these feedback loops enabled continuous system improvement—from identifying underrepresented patient demographics (triggering new data collection) to detecting image quality issues (improving preprocessing) and addressing model drift (initiating retraining).\nFor DR screening, continuous monitoring tracked system performance across diverse clinics, detecting issues such as changing patient demographics or new imaging technologies that could impact accuracy. Proactive maintenance included plans to incorporate 3D imaging modalities like Optical Coherence Tomography (OCT), expanding the system’s capabilities to diagnose a wider range of conditions. This highlights the importance of designing systems that can adapt to future challenges while maintaining compliance with rigorous healthcare regulations.\n\n5.7.1 Monitoring Requirements and System Impact\nThe requirements for monitoring and maintenance emerged from both technical needs and operational realities. In the DR project, the technical perspective required continuous tracking of model performance, data quality, and system resource usage. However, operational constraints added layers of complexity: monitoring systems had to align with clinical workflows, detect shifts in patient demographics, and provide actionable insights to both technical teams and healthcare providers.\nInitial deployment highlighted several areas where the system failed to meet real-world needs, such as decreased accuracy in clinics with outdated equipment or lower-quality images. Monitoring systems detected performance drops in specific subgroups, such as patients with less common retinal conditions, demonstrating that even a well-trained model could face blind spots in practice. These insights informed maintenance strategies, including targeted updates to address specific challenges and expanded training datasets to cover edge cases.\nThese requirements influenced system design significantly. The critical nature of the DR system’s function demanded real-time monitoring capabilities rather than periodic offline evaluations. To support this, the team implemented advanced logging and analytics pipelines to process large amounts of operational data from clinics without disrupting diagnostic workflows. Secure and efficient data handling was essential to transmit data across multiple clinics while preserving patient confidentiality.\nMonitoring requirements also affected model design, as the team incorporated mechanisms for granular performance tracking and anomaly detection. Even the system’s user interface was influenced, needing to present monitoring data in a clear, actionable manner for clinical and technical staff alike.\n\n\n5.7.2 Monitoring and Maintenance Workflow\nThe monitoring and maintenance workflow in the DR project revealed the intricate interplay between automated systems, human expertise, and evolving healthcare practices. The process began with defining a comprehensive monitoring framework, establishing key performance indicators (KPIs), and implementing dashboards and alert systems. This framework had to balance depth of monitoring with system performance and privacy considerations, collecting sufficient data to detect issues without overburdening the system or violating patient confidentiality.\nAs the system matured, maintenance became an increasingly dynamic process. Model updates driven by new medical knowledge or performance improvements required careful validation and controlled rollouts. The team employed A/B testing frameworks to evaluate updates in real-world conditions and implemented rollback mechanisms to address issues quickly when they arose.\nMonitoring and maintenance formed an iterative cycle rather than discrete phases. Insights from monitoring informed maintenance activities, while maintenance efforts often necessitated updates to monitoring strategies. The team developed workflows to transition seamlessly from issue detection to resolution, involving collaboration across technical and clinical domains.\n\n\n5.7.3 Scale and Distribution Challenges\nAs the DR project scaled from pilot sites to widespread deployment, monitoring and maintenance complexities grew exponentially. Each additional clinic added to the volume of operational data and introduced new environmental variables, such as differing hardware configurations or demographic patterns.\nThe need to monitor both global performance metrics and site-specific behaviors required sophisticated infrastructure. While global metrics provided an overview of system health, localized issues—such as a hardware malfunction at a specific clinic or unexpected patterns in patient data—needed targeted monitoring. Advanced analytics systems processed data from all clinics to identify these localized anomalies while maintaining a system-wide perspective.\nContinuous adaptation added further complexity. Real-world usage exposed the system to an ever-expanding range of scenarios. Capturing insights from these scenarios and using them to drive system updates required efficient mechanisms for integrating new data into training pipelines and deploying improved models without disrupting clinical workflows.\n\n\n5.7.4 Proactive Maintenance and Continuous Learning\nReactive maintenance alone was insufficient for the DR project’s dynamic operating environment. Proactive strategies became essential to anticipate and prevent issues before they affected clinical operations.\nThe team implemented predictive maintenance models to identify potential problems based on patterns in operational data. Continuous learning pipelines allowed the system to retrain and adapt based on new data, ensuring its relevance as clinical practices or patient demographics evolved. These capabilities required careful balancing to ensure safety and reliability while maintaining system performance.\nMetrics assessing adaptability and resilience became as important as accuracy, reflecting the system’s ability to evolve alongside its operating environment. Proactive maintenance ensured the system could handle future challenges without sacrificing reliability.\n\n\n5.7.5 Systems Thinking\nMonitoring and maintenance, viewed through a systems lens, reveal their deep integration with every other stage of the ML lifecycle. Changes in data collection affect model behavior, which influences monitoring thresholds. Maintenance actions can alter system availability or performance, impacting users and clinical workflows.\nFeedback loops are central to these processes. Monitoring insights drive updates to models and workflows, while user feedback informs maintenance priorities. These loops ensure the system remains responsive to both technical and clinical needs.\nEmergent behaviors often arise in distributed deployments. The DR team identified subtle system-wide shifts in diagnostic patterns that were invisible in individual clinics but evident in aggregated data. Managing these behaviors required sophisticated analytics and a holistic view of the system.\nResource dependencies also presented challenges. Real-time monitoring competed with diagnostic functions for computational resources, while maintenance activities required skilled personnel and occasional downtime. Effective resource planning was critical to balancing these demands.\n\n\n5.7.6 Lifecycle Implications\nMonitoring and maintenance are not isolated stages but integral parts of the ML lifecycle. Insights gained from these activities feed back into data collection, model development, and evaluation, ensuring the system evolves in response to real-world challenges. This lifecycle perspective emphasizes the need for strategies that not only address immediate concerns but also support long-term adaptability and improvement.\nIn subsequent chapters, we will explore critical questions related to monitoring and maintenance:\n\nHow can monitoring systems detect subtle degradations in ML performance across diverse environments?\nWhat strategies support efficient maintenance of ML systems deployed at scale?\nHow can continuous learning pipelines ensure relevance without compromising safety?\nWhat tools facilitate proactive maintenance and minimize disruption in production systems?\nHow do monitoring and maintenance processes influence the design of future ML models?\n\nThese questions highlight the interconnected nature of monitoring and maintenance, where success depends on creating a framework that ensures both immediate reliability and long-term viability in complex, dynamic environments.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>AI Workflow</span>"
    ]
  },
  {
    "objectID": "contents/core/workflow/workflow.html#roles-in-ai-lifecycle",
    "href": "contents/core/workflow/workflow.html#roles-in-ai-lifecycle",
    "title": "5  AI Workflow",
    "section": "5.8 Roles in AI Lifecycle",
    "text": "5.8 Roles in AI Lifecycle\nBuilding effective and resilient machine learning systems is far more than a solo pursuit; it’s a collaborative endeavor that thrives on the diverse expertise of a multidisciplinary team. Each role in this intricate dance brings unique skills and insights, supporting different phases of the AI development process. Understanding who these players are, what they contribute, and how they interconnect is crucial to navigating the complexities of modern AI systems.\n\n5.8.1 A Collaborative Ensemble\nAt the heart of any AI project is a team of data scientists. These innovative thinkers focus on model creation, experiment with architectures, and refine the algorithms that will become the neural networks driving insights from data. In our Diabetic Retinopathy (DR) project, data scientists were instrumental in architecting neural networks capable of identifying retinal anomalies, advancing through iterations to fine-tune a balance between accuracy and computational efficiency.\nBehind the scenes, data engineers work tirelessly to design robust data pipelines, ensuring that vast amounts of data are ingested, transformed, and stored effectively. They play a crucial role in the DR project, handling data from various clinics and automating quality checks to guarantee that the training inputs were standardized and reliable.\nMeanwhile, machine learning engineers take the baton to integrate these models into production settings. They guarantee that models are nimble, scalable, and fit the constraints of the deployment environment. In rural clinics where computational resources can be scarce, their work in optimizing models was pivotal to enabling on-the-spot diagnosis.\nDomain experts, such as ophthalmologists in the DR project, infuse technical progress with practical relevance. Their insights shape early problem definitions and ensure that AI tools align closely with real-world needs, offering a measure of validation that keeps the outcome aligned with clinical and operational realities.\nMLOps engineers are the guardians of workflow automation, orchestrating the continuous integration and monitoring systems that keep AI models up and running. They crafted centralized monitoring frameworks in the DR project, ensuring that updates were streamlined and model performance remained optimal across different deployment sites.\nEthicists and compliance officers remind us of the larger responsibility that accompanies AI deployment, ensuring adherence to ethical standards and legal requirements. Their oversight in the DR initiative safeguarded patient privacy amidst strict healthcare regulations.\nProject managers weave together these diverse strands, orchestrating timelines, resources, and communication streams to maintain project momentum and alignment with objectives. They acted as linchpins within the project, harmonizing efforts between tech teams, clinical practitioners, and policy makers.\n\n\n5.8.2 The Interplay of Roles\nThe synergy between these roles fuels the AI machinery toward successful outcomes. Data engineers establish a solid foundation for data scientists’ creative model-building endeavors. As models transition into real-world applications, ML engineers ensure compatibility and efficiency. Meanwhile, feedback loops between MLOps engineers and data scientists foster continuous improvement, enabling quick adaptation to data-driven discoveries.\nUltimately, the success of the DR project underscores the irreplaceable value of interdisciplinary collaboration. From bridging clinical insights with technical prowess to ensuring ethical deployment, this collective effort exemplifies how AI initiatives can be both technically successful and socially impactful.\nThis interconnected approach underlines why our exploration in later chapters will delve into various aspects of AI development, including those that may be seen as outside an individual’s primary expertise. Understanding these diverse roles will equip us to build more robust, well-rounded AI solutions. By comprehending the broader context and the interplay of roles, you’ll be better prepared to address challenges and collaborate effectively, paving the way for innovative and responsible AI systems.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>AI Workflow</span>"
    ]
  },
  {
    "objectID": "contents/core/workflow/workflow.html#conclusion",
    "href": "contents/core/workflow/workflow.html#conclusion",
    "title": "5  AI Workflow",
    "section": "5.9 Conclusion",
    "text": "5.9 Conclusion\nThe AI workflow we’ve explored, while illustrated through the Diabetic Retinopathy project, represents a framework applicable across diverse domains of AI application. From finance and manufacturing to environmental monitoring and autonomous vehicles, the core stages of the workflow remain consistent, even as their specific implementations vary widely.\nThe interconnected nature of the AI lifecycle, illustrated in Figure 5.4, is a universal constant. The feedback loops—from “Performance Insights” driving data collection to “Validation Issues” triggering model updates—demonstrate how decisions in one stage invariably impact others. Data quality affects model performance, deployment constraints influence architecture choices, and real-world usage patterns drive ongoing refinement through these well-defined feedback paths.\nRegardless of the application, the interconnected nature of the AI lifecycle is a universal constant. Whether developing fraud detection systems for banks or predictive maintenance models for industrial equipment, decisions made in one stage invariably impact others. Data quality affects model performance, deployment constraints influence architecture choices, and real-world usage patterns drive ongoing refinement.\nThis interconnectedness underscores the importance of systems thinking in AI development across all sectors. Success in AI projects, regardless of domain, comes from understanding and managing the complex interactions between stages, always considering the broader context in which the system will operate.\nAs AI continues to evolve and expand into new areas, this holistic approach becomes increasingly crucial. Future challenges in AI development—be they in healthcare, finance, environmental science, or any other field—will likely center around managing increased complexity, ensuring adaptability, and balancing performance with ethical considerations. By approaching AI development with a systems-oriented mindset, we can create solutions that are not only technically proficient but also robust, adaptable, and aligned with real-world needs across a wide spectrum of applications.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>AI Workflow</span>"
    ]
  },
  {
    "objectID": "contents/core/data_engineering/data_engineering.html",
    "href": "contents/core/data_engineering/data_engineering.html",
    "title": "6  Data Engineering",
    "section": "",
    "text": "Purpose\nResources: Slides, Videos, Exercises\nHow does data shape ML systems engineering?\nIn the field of machine learning, data engineering is often overshadowed by the allure of sophisticated algorithms, when in fact data plays a foundational role in determining an AI system’s capabilities and limitations. We need to understand the core principles of data in ML systems, exploring how the acquisition, processing, storage, and governance of data directly impact the performance, reliability, and ethical considerations of AI systems. By understanding these fundamental concepts, we can unlock the true potential of AI and build a solid foundation of high-quality ML solutions.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Data Engineering</span>"
    ]
  },
  {
    "objectID": "contents/core/data_engineering/data_engineering.html#purpose",
    "href": "contents/core/data_engineering/data_engineering.html#purpose",
    "title": "6  Data Engineering",
    "section": "",
    "text": "Learning Objectives\n\n\n\n\nAnalyze different data sourcing methods (datasets, web scraping, crowdsourcing, synthetic data).\nExplain the importance of data labeling and ensure label quality.\nEvaluate data storage systems for ML workloads (databases, data warehouses, data lakes).\nDescribe the role of data pipelines in ML systems.\nExplain the importance of data governance in ML (security, privacy, ethics).\nIdentify key challenges in data engineering for ML.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Data Engineering</span>"
    ]
  },
  {
    "objectID": "contents/core/data_engineering/data_engineering.html#overview",
    "href": "contents/core/data_engineering/data_engineering.html#overview",
    "title": "6  Data Engineering",
    "section": "6.1 Overview",
    "text": "6.1 Overview\nData is the foundation of modern machine learning systems, as success is governed by the quality and accessibility of training and evaluation data. Despite its pivotal role, data engineering is often overlooked compared to algorithm design and model development. However, the effectiveness of any machine learning system hinges on the robustness of its data pipeline. As machine learning applications become more sophisticated, the challenges associated with curating, cleaning, organizing, and storing data have grown significantly. These activities have emerged as some of the most resource-intensive aspects of the data engineering process, requiring sustained effort and attention.\nThe concept of “Data Cascades,” introduced by Sambasivan et al. (2021), highlights the systemic failures that can arise when data quality issues are left unaddressed. Errors originating during data collection or processing stages can compound over time, creating cascading effects that lead to model failures, costly retraining, or even project termination. The failures of IBM Watson Health in 2019, where flawed training data resulted in unsafe and incorrect cancer treatment recommendations (Strickland 2019), show the real-world consequences of neglecting data quality and its associated engineering requirements.\n\nStrickland, Eliza. 2019. “IBM Watson, Heal Thyself: How IBM Overpromised and Underdelivered on AI Health Care.” IEEE Spectrum 56 (4): 24–31. https://doi.org/10.1109/mspec.2019.8678513.\nIt is therefore unsurprising that data scientists spend the majority of their time—up to 60%, as shown in Figure 6.1—is spent on cleaning and organizing data. This statistic highlights the critical need to prioritize data-related challenges early in the pipeline to avoid downstream issues and ensure the effectiveness of machine learning systems.\n\n\n\n\n\n\nFigure 6.1: What do data scientists spend most of their time on?\n\n\n\nThis chapter examines the lifecycle of data engineering in machine learning systems, presenting an overview of the stages involved and the unique challenges at each step. The discussion begins with the identification and sourcing of data, exploring diverse origins such as pre-existing datasets, web scraping, crowdsourcing, and synthetic data generation. Special attention is given to the complexities of integrating heterogeneous sources, validating incoming data, and handling errors during ingestion.\nNext, the chapter explores the transformation of raw data into machine learning-ready formats. This process involves cleaning, normalizing, and extracting features, tasks that are critical to optimizing model learning and ensuring robust performance. The challenges of scale and computational efficiency are also discussed, as they are particularly important for systems that operate on vast and complex datasets.\nBeyond data processing, the chapter addresses the intricacies of data labeling, a crucial step for supervised learning systems. Effective labeling requires sound annotation methodologies and advanced techniques such as AI-assisted annotation to ensure the accuracy and consistency of labeled data. Challenges such as bias and ambiguity in labeling are explored, with examples illustrating their potential impact on downstream tasks.\nThe chapter also examines the storage and organization of data, a vital aspect of supporting machine learning pipelines across their lifecycle. Topics such as storage system design, feature stores, caching strategies, and access patterns are discussed, with a focus on ensuring scalability and efficiency. Governance is highlighted as a key component of data storage and management, emphasizing the importance of compliance with privacy regulations, ethical considerations, and the use of documentation frameworks to maintain transparency and accountability.\nThis chapter provides an exploration of data engineering practices necessary for building and maintaining effective machine learning systems. The end goal is to emphasize the often-overlooked importance of data in enabling the success of machine learning applications.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Data Engineering</span>"
    ]
  },
  {
    "objectID": "contents/core/data_engineering/data_engineering.html#problem-definition",
    "href": "contents/core/data_engineering/data_engineering.html#problem-definition",
    "title": "6  Data Engineering",
    "section": "6.2 Problem Definition",
    "text": "6.2 Problem Definition\nAs discussed in the overview, Sambasivan et al. (2021) observes that neglecting the fundamental importance of data quality gives rise to “Data Cascades” — events where lapses in data quality compound, leading to negative downstream consequences such as flawed predictions, project terminations, and even potential harm to communities. Despite many ML professionals recognizing the importance of data, numerous practitioners report facing these cascades.\nFigure 6.2 illustrates these potential data pitfalls at every stage and how they influence the entire process down the line. The influence of data collection errors is especially pronounced. As illustrated in the figure, any lapses in this initial stage will become apparent at later stages (in model evaluation and deployment) and might lead to costly consequences, such as abandoning the entire model and restarting anew. Therefore, investing in data engineering techniques from the onset will help us detect errors early, mitigating these cascading effects.\n\n\n\n\n\n\nFigure 6.2: Data cascades: compounded costs. Source: Sambasivan et al. (2021).\n\n\nSambasivan, Nithya, Shivani Kapania, Hannah Highfill, Diana Akrong, Praveen Paritosh, and Lora M Aroyo. 2021. “‘Everyone Wants to Do the Model Work, Not the Data Work’: Data Cascades in High-Stakes AI.” In Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems, 1–15. ACM. https://doi.org/10.1145/3411764.3445518.\n\n\nThis emphasis on data quality and proper problem definition is fundamental across all types of ML systems. As Sculley et al. (2021) emphasize, it is important to distinguish ML-specific problem framing from the broader context of general software development. Whether developing recommendation engines processing millions of user interactions, computer vision systems analyzing medical images, or natural language models handling diverse text data, each system brings unique challenges that must be carefully considered from the outset. Production ML systems are particularly sensitive to data quality issues, as they must handle continuous data streams, maintain consistent processing pipelines, and adapt to evolving patterns while maintaining performance standards.\n\nSculley, David, Gary Holt, Daniel Golovin, Eugene Davydov, Todd Phillips, Dietmar Ebner, Vinay Chaudhary, Michael Young, Jean-Francois Crespo, and Dan Dennison. 2021. “Technical Debt in Machine Learning Systems.” In Technical Debt in Practice, 177–92. The MIT Press. https://doi.org/10.7551/mitpress/12440.003.0011.\nA solid project foundation is essential for setting the trajectory and ensuring the eventual success of any initiative. At the heart of this foundation lies the crucial first step: identifying a clear problem to solve. This could involve challenges like developing a recommendation system that effectively handles cold-start scenarios, or creating a classification model that maintains consistent accuracy across diverse population segments.\nAs we will explore later in this chapter, establishing clear objectives provides a unified direction that guides the entire project. These objectives might include creating representative datasets that account for various real-world scenarios. Equally important is defining specific benchmarks, such as prediction accuracy and system latency, which offer measurable outcomes to gauge progress and success.\nThroughout this process, engaging with stakeholders—from end-users to business leaders—provides invaluable insights that ensure the project remains aligned with real-world needs and expectations.\nIn particular, a cardinal sin in ML is to begin collecting data (or augmenting an existing dataset) without clearly specifying the underlying problem definition to guide the data collection. We identify the key steps that should precede any data collection effort here:\n\nIdentify and clearly state the problem definition\nSet clear objectives to meet\nEstablish success benchmarks\nUnderstand end-user engagement/use\nUnderstand the constraints and limitations of deployment\nPerform data collection.\nIterate and refine.\n\n\n6.2.1 Keyword Spotting Example\nKeyword Spotting (KWS) is an excellent example to illustrate all of the general steps in action. This technology is critical for voice-enabled interfaces on endpoint devices such as smartphones. Typically functioning as lightweight wake-word engines, KWS systems are constantly active, listening for a specific phrase to trigger further actions.\nAs shown in Figure 6.3, when we say “OK, Google” or “Alexa,” this initiates a process on a microcontroller embedded within the device.\n\n\n\n\n\n\nFigure 6.3: Keyword Spotting example: interacting with Alexa. Source: Amazon.\n\n\n\nBuilding a reliable KWS model is a complex task. It demands a deep understanding of the deployment scenario, encompassing where and how these devices will operate. For instance, a KWS model’s effectiveness is not just about recognizing a word; it’s about discerning it among various accents and background noises, whether in a bustling cafe or amid the blaring sound of a television in a living room or a kitchen where these devices are commonly found. It’s about ensuring that a whispered “Alexa” in the dead of night or a shouted “OK Google” in a noisy marketplace are recognized with equal precision.\nMoreover, many current KWS voice assistants support a limited number of languages, leaving a substantial portion of the world’s linguistic diversity unrepresented. This limitation is partly due to the difficulty in gathering and monetizing data for languages spoken by smaller populations. In the long-tail distribution of languages, most languages have limited or zero speech training data available, making the development of voice assistants challenging.\nKeyword spotting models can run on low-power, low-price microcontrollers, so theoretically voice interfaces could be expanded to a huge gamut of devices worldwide, beyond smartphones and home assistants. But the level of accuracy and robustness that end-users expect hinges on the availability and quality of speech data, and the ability to label the data correctly. Developing a keyword-spotting model for an arbitrary word or phrase in an arbitrary language begins with clearly understanding the problem statement or definition. Using KWS as an example, we can break down each of the steps as follows:\n\nIdentifying the Problem: KWS detects specific keywords amidst ambient sounds and other spoken words. The primary problem is to design a system that can recognize these keywords with high accuracy, low latency, and minimal false positives or negatives, especially when deployed on devices with limited computational resources. A well-specified problem definition for developing a new KWS model should identify the desired keywords along with the envisioned application and deployment scenario.\nSetting Clear Objectives: The objectives for a KWS system might include:\n\nAchieving a specific accuracy rate (e.g., 98% accuracy in keyword detection).\nEnsuring low latency (e.g., keyword detection and response within 200 milliseconds).\nMinimizing power consumption to extend battery life on embedded devices.\nEnsuring the model’s size is optimized for the available memory on the device.\n\nBenchmarks for Success: Establish clear metrics to measure the success of the KWS system. This could include:\n\nTrue Positive Rate: The percentage of correctly identified keywords relative to all spoken keywords.\nFalse Positive Rate: The percentage of non-keywords (including silence, background noise, and out-of-vocabulary words) incorrectly identified as keywords.\nDetection/Error Tradeoff These curves evaluate KWS on streaming audio representative of a real-world deployment scenario, by comparing the number of false accepts per hour (the number of false positives over the total duration of the evaluation audio) against the false rejection rate (the number of missed keywords relative to the number of spoken keywords in the evaluation audio). Nayak et al. (2022) provides one example of this.\nResponse Time: The time taken from keyword utterance to system response.\nPower Consumption: Average power used during keyword detection.\n\nStakeholder Engagement and Understanding: Engage with stakeholders, which include device manufacturers, hardware and software developers, and end-users. Understand their needs, capabilities, and constraints. For instance:\n\nDevice manufacturers might prioritize low power consumption.\nSoftware developers might emphasize ease of integration.\nEnd-users would prioritize accuracy and responsiveness.\n\nUnderstanding the Constraints and Limitations of Embedded Systems: Embedded devices come with their own set of challenges:\n\nMemory Limitations: KWS models must be lightweight to fit within the memory constraints of embedded devices. Typically, KWS models need to be as small as 16KB to fit in the always-on island of the SoC. Moreover, this is just the model size. Additional application code for preprocessing may also need to fit within the memory constraints.\nProcessing Power: The computational capabilities of embedded devices are limited (a few hundred MHz of clock speed), so the KWS model must be optimized for efficiency.\nPower Consumption: Since many embedded devices are battery-powered, the KWS system must be power-efficient.\nEnvironmental Challenges: Devices might be deployed in various environments, from quiet bedrooms to noisy industrial settings. The KWS system must be robust enough to function effectively across these scenarios.\n\nData Collection and Analysis: For a KWS system, the quality and diversity of data are paramount. Considerations might include:\n\nDemographics: Collect data from speakers with various accents across age and gender to ensure wide-ranging recognition support.\nKeyword Variations: People might pronounce keywords differently or express slight variations in the wake word itself. Ensure the dataset captures these nuances.\nBackground Noises: Include or augment data samples with different ambient noises to train the model for real-world scenarios.\n\nIterative Feedback and Refinement: Once a prototype KWS system is developed, it is important to do the following to ensure that the system remains aligned with the defined problem and objectives as the deployment scenarios change over time and as use-cases evolve.\n\nTest it in real-world scenarios\nGather feedback - are some users or deployment scenarios encountering underperformance relative to others?\nIteratively refine the dataset and model\n\n\n\nNayak, Prateeth, Takuya Higuchi, Anmol Gupta, Shivesh Ranjan, Stephen Shum, Siddharth Sigtia, Erik Marchi, et al. 2022. “Improving Voice Trigger Detection with Metric Learning.” arXiv Preprint arXiv:2204.02455, April. http://arxiv.org/abs/2204.02455v2.\nThe KWS example illustrates the broader principles of problem definition, showing how initial decisions about data requirements ripple throughout a project’s lifecycle. By carefully considering each aspect—from core problem identification through performance benchmarks to deployment constraints—teams can build a strong foundation for their ML systems. The methodical problem definition process provides a framework applicable across the ML spectrum. Whether developing computer vision systems for medical diagnostics, recommendation engines processing millions of user interactions, or natural language models analyzing diverse text corpora, this structured approach helps teams anticipate and plan for their data needs.\nThis brings us to data pipelines—the foundational infrastructure that transforms raw data into ML—ready formats while maintaining quality and reliability throughout the process. These pipelines implement our carefully defined requirements in production systems, handling everything from initial data ingestion to final feature generation.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Data Engineering</span>"
    ]
  },
  {
    "objectID": "contents/core/data_engineering/data_engineering.html#pipeline-fundamentals",
    "href": "contents/core/data_engineering/data_engineering.html#pipeline-fundamentals",
    "title": "6  Data Engineering",
    "section": "6.3 Pipeline Fundamentals",
    "text": "6.3 Pipeline Fundamentals\nModern machine learning systems depend on data pipelines to process massive amounts of data efficiently and reliably. For instance, recommendation systems at companies like Netflix process billions of user interactions daily, while autonomous vehicle systems must handle terabytes of sensor data in real-time. These pipelines serve as the backbone of ML systems, acting as the infrastructure through which raw data transforms into ML-ready training data.\nThese data pipelines are not simple linear paths but rather complex systems. They must manage data acquisition, transformation, storage, and delivery while ensuring data quality and system reliability. The design of these pipelines fundamentally shapes what is possible with an ML system.\n\n\n\n\n\n\nFigure 6.4: Overview of the data pipeline.\n\n\n\nML data pipelines consist of several distinct layers: data sources, ingestion, processing, labeling, storage, and eventually ML training (Figure 6.4). Each layer plays a specific role in the data preparation workflow. The interactions between these layers are crucial to the system’s overall effectiveness. The flow from raw data sources to ML training demonstrates the importance of maintaining data quality and meeting system requirements throughout the pipeline.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Data Engineering</span>"
    ]
  },
  {
    "objectID": "contents/core/data_engineering/data_engineering.html#data-sources",
    "href": "contents/core/data_engineering/data_engineering.html#data-sources",
    "title": "6  Data Engineering",
    "section": "6.4 Data Sources",
    "text": "6.4 Data Sources\nThe first stage of the pipeline architecture sourcing appropriate data to meet the training needs. The quality and diversity of this data will fundamentally determine our ML system’s learning and prediction capabilities and limitations. ML systems can obtain their training data through several different approaches, each with their own advantages and challenges. Let’s examine each of these approaches in detail.\n\n6.4.1 Pre-existing datasets\nPlatforms like Kaggle and UCI Machine Learning Repository provide ML practitioners with ready-to-use datasets that can jumpstart system development. These pre-existing datasets are particularly valuable when building ML systems as they offer immediate access to cleaned, formatted data with established benchmarks. One of their primary advantages is cost efficiency – creating datasets from scratch requires significant time and resources, especially when building production ML systems that need large amounts of high-quality training data.\nMany of these datasets, such as ImageNet, have become standard benchmarks in the machine learning community, enabling consistent performance comparisons across different models and architectures. For ML system developers, this standardization provides clear metrics for evaluating model improvements and system performance. The immediate availability of these datasets allows teams to begin experimentation and prototyping without delays in data collection and preprocessing.\nHowever, ML practitioners must carefully consider the quality assurance aspects of pre-existing datasets. For instance, the ImageNet dataset was found to have label errors on 6.4% of the validation set (Northcutt, Athalye, and Mueller 2021). While popular datasets benefit from community scrutiny that helps identify and correct errors and biases, most datasets remain “untended gardens” where quality issues can significantly impact downstream system performance if not properly addressed. Moreover, as (Gebru et al. 2021) highlighted in her paper, simply providing a dataset without documentation can lead to misuse and misinterpretation, potentially amplifying biases present in the data.\n\nGebru, Timnit, Jamie Morgenstern, Briana Vecchione, Jennifer Wortman Vaughan, Hanna Wallach, Hal Daumé III, and Kate Crawford. 2021. “Datasheets for Datasets.” Communications of the ACM 64 (12): 86–92. https://doi.org/10.1145/3458723.\n\nPineau, Joelle, Philippe Vincent-Lamarre, Koustuv Sinha, Vincent Larivière, Alina Beygelzimer, Florence d’Alché-Buc, Emily Fox, and Hugo Larochelle. 2021. “Improving Reproducibility in Machine Learning Research (a Report from the Neurips 2019 Reproducibility Program).” Journal of Machine Learning Research 22 (164): 1–20.\nSupporting documentation accompanying existing datasets is invaluable, yet is often only present in widely-used datasets. Good documentation provides insights into the data collection process and variable definitions and sometimes even offers baseline model performances. This information not only aids understanding but also promotes reproducibility in research, a cornerstone of scientific integrity; currently, there is a crisis around improving reproducibility in machine learning systems (Pineau et al. 2021). When other researchers have access to the same data, they can validate findings, test new hypotheses, or apply different methodologies, thus allowing us to build on each other’s work more rapidly.\nWhile existing datasets are invaluable resources, it’s essential to understand the context in which the data was collected. Researchers should be wary of potential overfitting when using popular datasets such as ImageNet (Beyer et al. 2020), leading to inflated performance metrics. Sometimes, these datasets do not reflect the real-world data.\n\nBeyer, Lucas, Olivier J. Hénaff, Alexander Kolesnikov, Xiaohua Zhai, and Aäron van den Oord. 2020. “Are We Done with ImageNet?” arXiv Preprint arXiv:2006.07159, June. http://arxiv.org/abs/2006.07159v1.\nA key consideration for ML systems is how well pre-existing datasets reflect real-world deployment conditions. Relying on standard datasets can create a concerning disconnect between training and production environments. This misalignment becomes particularly problematic when multiple ML systems are trained on the same datasets (Figure 6.5), potentially propagating biases and limitations throughout an entire ecosystem of deployed models.\n\n\n\n\n\n\nFigure 6.5: Training different models on the same dataset.\n\n\n\n\n\n6.4.2 Web Scraping\nWhen building ML systems, particularly in domains where pre-existing datasets are insufficient, web scraping offers a powerful approach to gathering training data at scale. This automated technique for extracting data from websites has become a powerful tool in modern ML system development. It enables teams to build custom datasets tailored to their specific needs.\nWeb scraping has proven particularly valuable for building large-scale ML systems when human-labeled data is scarce. Consider computer vision systems: major datasets like ImageNet and OpenImages were built through systematic web scraping, fundamentally advancing the field of computer vision. In production environments, companies regularly scrape e-commerce sites to gather product images for recognition systems or social media platforms for computer vision applications. Stanford’s LabelMe project demonstrated this approach’s potential early on, scraping Flickr to create a diverse dataset of over 63,000 annotated images.\nThe impact of web scraping extends well beyond computer vision systems. In natural language processing, web-scraped data has enabled the development of increasingly sophisticated ML systems. Large language models, such as ChatGPT and Claude, rely on vast amounts of text scraped from the public internet and media to learn language patterns and generate responses (Groeneveld et al. 2024). Similarly, specialized ML systems like GitHub’s Copilot demonstrate how targeted web scraping—in this case, of code repositories—can create powerful domain-specific assistants (Chen et al. 2021).\n\nGroeneveld, Dirk, Iz Beltagy, Pete Walsh, Akshita Bhagia, Rodney Kinney, Oyvind Tafjord, Ananya Harsh Jha, et al. 2024. “OLMo: Accelerating the Science of Language Models.” arXiv Preprint arXiv:2402.00838, February. http://arxiv.org/abs/2402.00838v4.\n\nChen, Mark, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, et al. 2021. “Evaluating Large Language Models Trained on Code.” arXiv Preprint arXiv:2107.03374, July. http://arxiv.org/abs/2107.03374v2.\nProduction ML systems often require continuous data collection to maintain relevance and performance. Web scraping facilitates this by gathering structured data like stock prices, weather patterns, or product information for analytical applications. However, this continuous collection introduces unique challenges for ML systems. Data consistency becomes crucial—variations in website structure or content formatting can disrupt the data pipeline and affect model performance. Proper data management through databases or warehouses becomes essential not just for storage, but for maintaining data quality and enabling model updates.\nDespite its utility, web scraping presents several challenges that ML system developers must carefully consider. Legal and ethical constraints can limit data collection—not all websites permit scraping, and violating these restrictions can have serious consequences. When building ML systems with scraped data, teams must carefully document data sources and ensure compliance with terms of service and copyright laws. Privacy considerations become particularly critical when dealing with user-generated content, often requiring robust anonymization procedures.\nTechnical limitations also affect the reliability of web-scraped training data. Rate limiting by websites can slow data collection, while the dynamic nature of web content can introduce inconsistencies that impact model training. As shown in Figure 6.6, web scraping can yield unexpected or irrelevant data—such as historical images appearing in contemporary image searches—that can pollute training datasets and degrade model performance. These issues highlight the importance of thorough data validation and cleaning processes in ML pipelines built on web-scraped data.\n\n\n\n\n\n\nFigure 6.6: A picture of old traffic lights (1914). Source: Vox.\n\n\n\n\n\n\n\n\n\nExercise 6.1: Web Scraping\n\n\n\n\n\nDiscover the power of web scraping with Python using libraries like Beautiful Soup and Pandas. This exercise will scrape Python documentation for function names and descriptions and explore NBA player stats. By the end, you’ll have the skills to extract and analyze data from real-world websites. Ready to dive in? Access the Google Colab notebook below and start practicing!\n\n\n\n\n\n\n\n6.4.3 Crowdsourcing\nCrowdsourcing is a collaborative approach to data collection, leveraging the collective efforts of distributed individuals via the internet to tackle tasks requiring human judgment. By engaging a global pool of contributors, this method accelerates the creation of high-quality, labeled datasets for machine learning systems, especially in scenarios where pre-existing data is scarce or domain-specific. Platforms like Amazon Mechanical Turk exemplify how crowdsourcing facilitates this process by distributing annotation tasks to a global workforce. This enables the rapid collection of labels for complex tasks such as sentiment analysis, image recognition, and speech transcription, significantly expediting the data preparation phase.\nOne of the most impactful examples of crowdsourcing in machine learning is the creation of the ImageNet dataset. ImageNet, which revolutionized computer vision, was built by distributing image labeling tasks to contributors via Amazon Mechanical Turk. The contributors categorized millions of images into thousands of classes, enabling researchers to train and benchmark models for a wide variety of visual recognition tasks.\nThe dataset’s availability spurred advancements in deep learning, including the breakthrough AlexNet model in 2012, which demonstrated how large-scale, crowdsourced datasets could drive innovation. ImageNet’s success highlights how leveraging a diverse group of contributors for annotation can enable machine learning systems to achieve unprecedented performance.\nAnother example of crowdsourcing’s potential is Google’s Crowdsource, a platform where volunteers contribute labeled data to improve AI systems in applications like language translation, handwriting recognition, and image understanding. By gamifying the process and engaging global participants, Google harnesses diverse datasets, particularly for underrepresented languages. This approach not only enhances the quality of AI systems but also empowers communities by enabling their contributions to influence technological development.\nCrowdsourcing has also been instrumental in applications beyond traditional dataset annotation. For instance, the navigation app Waze uses crowdsourced data from its users to provide real-time traffic updates, route suggestions, and incident reporting. While this involves dynamic data collection rather than static dataset labeling, it demonstrates how crowdsourcing can generate continuously updated datasets essential for applications like mobile or edge ML systems. These systems often require real-time input to maintain relevance and accuracy in changing environments.\nOne of the primary advantages of crowdsourcing is its scalability. By distributing microtasks to a large audience, projects can process enormous volumes of data quickly and cost-effectively. This scalability is particularly beneficial for machine learning systems that require extensive datasets to achieve high performance. Additionally, the diversity of contributors introduces a wide range of perspectives, cultural insights, and linguistic variations, enriching datasets and improving models’ ability to generalize across populations.\nFlexibility is a key benefit of crowdsourcing. Tasks can be adjusted dynamically based on initial results, allowing for iterative improvements in data collection. For example, Google’s reCAPTCHA system uses crowdsourcing to verify human users while simultaneously labeling datasets for training machine learning models. Users identify objects in images—such as street signs or cars—contributing to the training of autonomous systems. This clever integration demonstrates how crowdsourcing can scale seamlessly when embedded into everyday workflows.\nDespite its advantages, crowdsourcing presents challenges that require careful management. Quality control is a major concern, as the variability in contributors’ expertise and attention can lead to inconsistent or inaccurate annotations. Providing clear instructions and training materials helps ensure participants understand the task requirements. Techniques such as embedding known test cases, leveraging consensus algorithms, or using redundant annotations can mitigate quality issues and align the process with the problem definition discussed earlier.\nEthical considerations are paramount in crowdsourcing, especially when datasets are built at scale using global contributors. It is essential to ensure that participants are fairly compensated for their work and that they are informed about how their contributions will be used. Additionally, privacy concerns must be addressed, particularly when dealing with sensitive or personal information. Transparent sourcing practices, clear communication with contributors, and robust auditing mechanisms are crucial for building trust and maintaining ethical standards.\nThe issue of fair compensation and ethical data sourcing was brought into sharp focus during the development of large-scale AI systems like OpenAI’s ChatGPT. Reports revealed that OpenAI outsourced data annotation tasks to workers in Kenya, employing them to moderate content and identify harmful or inappropriate material that the model might generate. This involved reviewing and labeling distressing content, such as graphic violence and explicit material, to train the AI in recognizing and avoiding such outputs. While this approach enabled OpenAI to improve the safety and utility of ChatGPT, significant ethical concerns arose around the working conditions, the nature of the tasks, and the compensation provided to Kenyan workers.\nMany of the contributors were reportedly paid as little as $1.32 per hour for reviewing and labeling highly traumatic material. The emotional toll of such work, coupled with low wages, raised serious questions about the fairness and transparency of the crowdsourcing process. This controversy highlights a critical gap in ethical crowdsourcing practices. The workers, often from economically disadvantaged regions, were not adequately supported to cope with the psychological impact of their tasks. The lack of mental health resources and insufficient compensation underscored the power imbalances that can emerge when outsourcing data annotation tasks to lower-income regions.\nThe challenges highlighted by the ChatGPT—Kenya controversy are not unique to OpenAI. Many organizations that rely on crowdsourcing for data annotation face similar issues. As machine learning systems grow more complex and require larger datasets, the demand for annotated data will continue to increase. This shows the need for industry-wide standards and best practices to ensure ethical data sourcing. This case emphasizes the importance of considering the human labor behind AI systems. While crowdsourcing offers scalability and diversity, it also brings ethical responsibilities that cannot be overlooked. Organizations must prioritize the well-being and fair treatment of contributors as they build the datasets that drive AI innovation.\nMoreover, when dealing with specialized applications like mobile ML, edge ML, or cloud ML, additional challenges may arise. These applications often require data collected from specific environments or devices, which can be difficult to gather through general crowdsourcing platforms. For example, data for mobile applications utilizing smartphone sensors may necessitate participants with specific hardware features or software versions. Similarly, edge ML systems deployed in industrial settings may require data involving proprietary processes or secure environments, introducing privacy and accessibility challenges.\nHybrid approaches that combine crowdsourcing with other data collection methods can address these challenges. Organizations may engage specialized communities, partner with relevant stakeholders, or create targeted initiatives to collect domain-specific data. Additionally, synthetic data generation, as discussed in the next section, can augment real-world data when crowdsourcing falls short.\n\n\n6.4.4 Data Anonymization\nProtecting the privacy of individuals while still enabling data-driven insights is a central challenge in the modern data landscape. As organizations collect and analyze vast quantities of information, the risk of exposing sensitive details—either accidentally or through malicious attacks—heightens. To mitigate these risks, practitioners have developed a commonly used range of anonymization techniques. These methods transform datasets such that individual identities and sensitive attributes become difficult or nearly impossible to re-identify, while preserving, to varying extents, the overall utility of the data for analysis.\nMasking involves altering or obfuscating sensitive values so that they cannot be directly traced back to the original data subject. For instance, digits in financial account numbers or credit card numbers can be replaced with asterisks, a fixed set of dummy characters, or hashed values to protect sensitive information during display or logging. This anonymization technique is straightforward to implement and understand while clearly protecting identifiable values from being viewed, but may struggle with protecting broader context (e.g. relationships between data points).\nGeneralization reduces the precision or granularity of data to decrease the likelihood of re-identification. Instead of revealing an exact date of birth or address, the data is aggregated into broader categories (e.g., age ranges, zip code prefixes). For example, a user’s exact age of 37 might be generalized to an age range of 30-39, while their exact address might be bucketed into a city level granularity. This technique clearly reduces the risk of identifying an individual by sharing data in aggregated form; however, we might consequently lose analytical prediction. Furthermore, if granularity is not chosen correctly, individuals may still be able to be identified under certain conditions.\nPseudonymization is the process of replacing direct identifiers (like names, Social Security numbers, or email addresses) with artificial identifiers, or “pseudonyms.” These pseudonyms must not reveal, or be easily traceable to, the original data subject. This is commonly used in health records or in any situation where datasets need personal identities removed, but maintain unique entries. This approach allow maintaining individual-level data for analysis (since records can be traced through pseudonyms), while reducing the risk of direct identification. However, if the “key” linking the pseudonym to the real identifier is compromised, re-identification becomes possible.\n\\(k\\)-anonymity ensures that each record in a dataset is indistinguishable from at least \\(𝑘−1\\) other records. This is achieved by suppressing or generalizing quasi-identifiers, or attributes that, in combination, could be used to re-identify an individual (e.g., zip code, age, gender). For example, if \\(k=5\\), every record in the dataset must share the same combination of quasi-identifiers with at least four other records. Thus, an attacker cannot pinpoint a single individual simply by looking at these attributes. This approach provides a formal privacy guaruntee that helps reduce chances of individual re-identification. However, it is extremely high touch and may require a significant level of data distortion and does not protect against things like homogeneity or background knowledge attacks.\nDifferential privacy (DP) adds carefully calibrated “noise” or randomized data perturbations to query results or datasets. The goal is to ensure that the inclusion or exclusion of any single individual’s data does not significantly affect the output, thereby concealing their presence. Introduced noise is controled by the \\(\\epsilon\\) parameter in \\(\\epsilon\\)-Differential Privacy, balancing data utility and privacy guarantees. The clear advantages this approach provides are strong mathematical guarantees of privacy, and DP is widely used in academic and industrial settings (e.g., large-scale data analysis). However, the added noise can affect data accuracy and subsequent model performance; proper parameter tuning is crucial to ensure both privacy and usefulness.\nIn summary, effective data anonymization is a balancing act between privacy and utility. Techniques such as masking, generalization, pseudonymization, k-anonymity, and differential privacy each target different aspects of re-identification risk. By carefully selecting and combining these methods, organizations can responsibly derive value from sensitive datasets while respecting the privacy rights and expectations of the individuals represented within them.\n\n\n6.4.5 Synthetic Data\nSynthetic data generation has emerged as a powerful tool for addressing limitations in data collection, particularly in machine learning applications where real-world data is scarce, expensive, or ethically challenging to obtain. This approach involves creating artificial data using algorithms, simulations, or generative models to mimic real-world datasets. The generated data can be used to supplement or replace real-world data, expanding the possibilities for training robust and accurate machine learning systems. Figure 6.7 illustrates the process of combining synthetic data with historical datasets to create larger, more diverse training sets.\n\n\n\n\n\n\nFigure 6.7: Increasing training data size with synthetic data generation. Source: AnyLogic.\n\n\n\nAdvancements in generative modeling techniques, such as diffusion models and flow-matching algorithms1, Generative Adversarial Networks (GANs)2, and Variational Autoencoders (VAEs)3, have greatly enhanced the quality of synthetic data. These techniques can produce data that closely resembles real-world distributions, making it suitable for applications ranging from computer vision to natural language processing. For example, GANs have been used to generate synthetic images for object recognition tasks, creating diverse datasets that are almost indistinguishable from real-world images. Similarly, synthetic data has been leveraged to simulate speech patterns, enhancing the robustness of voice recognition systems.\n1 Diffusion models use noise prediction across time to simulate generation, while flow-matching algorithms minimize the displacement between source and target distributions.2 Generative Adversarial Networks (GANs): Machine learning models with a generator creating data and a discriminator assessing its realism.3 Variational Autoencoders (VAEs): Generative models that encode data into a latent space and decode it to generate new samples.Synthetic data has become particularly valuable in domains where obtaining real-world data is either impractical or costly. The automotive industry has embraced synthetic data to train autonomous vehicle systems; there are only so many cars you can physically crash to get crash-test data that might help an ML system know how to avoid crashes in the first place. Capturing real-world scenarios, especially rare edge cases such as near-accidents or unusual road conditions, is inherently difficult. Synthetic data allows researchers to simulate these scenarios in a controlled virtual environment, ensuring that models are trained to handle a wide range of conditions. This approach has proven invaluable for advancing the capabilities of self-driving cars.\nAnother important application of synthetic data lies in augmenting existing datasets. Introducing variations into datasets enhances model robustness by exposing the model to diverse conditions. For instance, in speech recognition, data augmentation techniques like SpecAugment (Park et al. 2019) introduce noise, shifts, or pitch variations, enabling models to generalize better across different environments and speaker styles. This principle extends to other domains as well, where synthetic data can fill gaps in underrepresented scenarios or edge cases.\n\nPark, Daniel S., William Chan, Yu Zhang, Chung-Cheng Chiu, Barret Zoph, Ekin D. Cubuk, and Quoc V. Le. 2019. “SpecAugment: A Simple Data Augmentation Method for Automatic Speech Recognition.” arXiv Preprint arXiv:1904.08779, April. http://arxiv.org/abs/1904.08779v3.\n4 General Data Protection Regulation (GDPR): A regulation in EU law on data protection and privacy in the European Union and the European Economic Area.5 Health Insurance Portability and Accountability Act (HIPAA): A US law designed to provide privacy standards to protect patients’ medical records and other health information.In addition to expanding datasets, synthetic data addresses critical ethical and privacy concerns. Unlike real-world data, synthetic data attempts to not tie back to specific individuals or entities. This makes it especially useful in sensitive domains such as finance, healthcare, or human resources, where data confidentiality is paramount. The ability to preserve statistical properties while removing identifying information allows researchers to maintain high ethical standards without compromising the quality of their models. In healthcare, privacy regulations such as GDPR4 and HIPAA5 limit the sharing of sensitive patient information. Synthetic data generation enables the creation of realistic yet anonymized datasets that can be used for training diagnostic models without compromising patient privacy.\nPoorly generated data can misrepresent underlying real-world distributions, introducing biases or inaccuracies that degrade model performance. Validating synthetic data against real-world benchmarks is essential to ensure its reliability. Additionally, models trained primarily on synthetic data must be rigorously tested in real-world scenarios to confirm their ability to generalize effectively. Another challenge is the potential amplification of biases present in the original datasets used to inform synthetic data generation. If these biases are not carefully addressed, they may be inadvertently reinforced in the resulting models.\nSynthetic data has revolutionized the way machine learning systems are trained, providing flexibility, diversity, and scalability in data preparation. However, as its adoption grows, practitioners must remain vigilant about its limitations and ethical implications. By combining synthetic data with rigorous validation and thoughtful application, machine learning researchers and engineers can unlock its full potential while ensuring reliability and fairness in their systems.\n\n\n6.4.6 Case Study: KWS\nKWS is an excellent case study of how different data collection approaches can be combined effectively. Each method we’ve discussed plays a role in building robust wake word detection systems, albeit with different trade-offs:\nPre-existing datasets like Google’s Speech Commands (Warden 2018) provide a foundation for initial development, offering carefully curated voice samples for common wake words. However, these datasets often lack diversity in accents, environments, and languages, necessitating additional data collection strategies.\n\nWarden, Pete. 2018. “Speech Commands: A Dataset for Limited-Vocabulary Speech Recognition.” arXiv Preprint arXiv:1804.03209, April. http://arxiv.org/abs/1804.03209v1.\nWeb scraping can supplement these baseline datasets by gathering diverse voice samples from video platforms, podcast repositories, and speech databases. This helps capture natural speech patterns and wake word variations, though careful attention must be paid to audio quality and privacy considerations when scraping voice data.\nCrowdsourcing becomes valuable for collecting specific wake word samples across different demographics and environments. Platforms like Amazon Mechanical Turk can engage contributors to record wake words in various accents, speaking styles, and background conditions. This approach is particularly useful for gathering data for underrepresented languages or specific acoustic environments.\nSynthetic data generation helps fill remaining gaps by creating unlimited variations of wake word utterances. Using speech synthesis (Werchniak et al. 2021) and audio augmentation techniques, developers can generate training data that captures different acoustic environments (busy streets, quiet rooms, moving vehicles), speaker characteristics (age, accent, gender), and background noise conditions.\n\nWerchniak, Andrew, Roberto Barra Chicote, Yuriy Mishchenko, Jasha Droppo, Jeff Condal, Peng Liu, and Anish Shah. 2021. “Exploring the Application of Synthetic Audio in Training Keyword Spotters.” In ICASSP 2021 - 2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 7993–96. IEEE; IEEE. https://doi.org/10.1109/icassp39728.2021.9413448.\nThis multi-faceted approach to data collection enables the development of KWS systems that perform robustly across diverse real-world conditions. The combination of methods helps address the unique challenges of wake word detection, from handling various accents and background noise to maintaining consistent performance across different devices and environments.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Data Engineering</span>"
    ]
  },
  {
    "objectID": "contents/core/data_engineering/data_engineering.html#data-ingestion",
    "href": "contents/core/data_engineering/data_engineering.html#data-ingestion",
    "title": "6  Data Engineering",
    "section": "6.5 Data Ingestion",
    "text": "6.5 Data Ingestion\nThe collected data must be reliably and efficiently ingested into our ML systems through well-designed data pipelines. This transformation presents several challenges that ML engineers must address.\n\n6.5.1 Ingestion Patterns\nIn ML systems, data ingestion typically follows two primary patterns: batch ingestion and stream ingestion. Each pattern has distinct characteristics and use cases that students should understand to design effective ML systems.\nBatch ingestion involves collecting data in groups or batches over a specified period before processing. This method is appropriate when real-time data processing is not critical and data can be processed at scheduled intervals. It’s also useful for loading large volumes of historical data. For example, a retail company might use batch ingestion to process daily sales data overnight, updating their ML models for inventory prediction each morning (Akidau et al. 2015).\n\nAkidau, Tyler, Robert Bradshaw, Craig Chambers, Slava Chernyak, Rafael J. Fernández-Moctezuma, Reuven Lax, Sam McVeety, et al. 2015. “The Dataflow Model: A Practical Approach to Balancing Correctness, Latency, and Cost in Massive-Scale, Unbounded, Out-of-Order Data Processing.” Proceedings of the VLDB Endowment 8 (12): 1792–1803. https://doi.org/10.14778/2824032.2824076.\nIn contrast, stream ingestion processes data in real-time as it arrives. This pattern is crucial for applications requiring immediate data processing, scenarios where data loses value quickly, and systems that need to respond to events as they occur. A financial institution, for instance, might use stream ingestion for real-time fraud detection, processing each transaction as it occurs to flag suspicious activity immediately (Kleppmann 2016).\nMany modern ML systems employ a hybrid approach, combining both batch and stream ingestion to handle different data velocities and use cases. This flexibility allows systems to process both historical data in batches and real-time data streams, providing a comprehensive view of the data landscape.\n\n\n6.5.2 ETL vs. ELT\nWhen designing data ingestion pipelines for ML systems, it’s necessary to understand the differences between Extract, Transform, Load (ETL) and Extract, Load, Transform (ELT) approaches. These paradigms determine when data transformations occur relative to the loading phase, significantly impacting the flexibility and efficiency of your ML pipeline.\nETL is a well-established paradigm in which data is first gathered from a source, then transformed to match the target schema or model, and finally loaded into a data warehouse or other repository. This approach typically results in data being stored in a ready-to-query format, which can be advantageous for ML systems that require consistent, pre-processed data. For instance, an ML system predicting customer churn might use ETL to standardize and aggregate customer interaction data from multiple sources before loading it into a format suitable for model training (Inmon 2005).\n\nInmon, W. H. 2005. Building the Data Warehouse. John Wiley & Sons.\n6 Schema-on-read: A flexible approach where data structure is defined at access time, not during ingestion, enabling versatile use of raw data.However, ETL can be less flexible when schemas or requirements change frequently, a common occurrence in evolving ML projects. This is where the ELT approach comes into play. ELT reverses the order by first loading raw data and then applying transformations as needed. This method is often seen in modern data lake or schema-on-read6 environments, allowing for a more agile approach when addressing evolving analytical needs in ML systems.\nBy deferring transformations, ELT can accommodate varying uses of the same dataset, which is particularly useful in exploratory data analysis phases of ML projects or when multiple models with different data requirements are being developed simultaneously. However, it’s important to note that ELT places greater demands on storage systems and query engines, which must handle large amounts of unprocessed information.\nIn practice, many ML systems employ a hybrid approach, selecting ETL or ELT on a case-by-case basis depending on the specific requirements of each data source or ML model. For example, a system might use ETL for structured data from relational databases where schemas are well-defined and stable, while employing ELT for unstructured data like text or images where transformation requirements may evolve as the ML models are refined.\n\n\n6.5.3 Source Integration\nIntegrating diverse data sources is a key challenge in data ingestion for ML systems. Data may come from various origins, including databases, APIs, file systems, and IoT devices. Each source may have its own data format, access protocol, and update frequency.\nTo effectively integrate these sources, ML engineers must develop robust connectors or adapters for each data source. These connectors handle the specifics of data extraction, including authentication, rate limiting, and error handling. For example, when integrating with a REST API, the connector would manage API keys, respect rate limits, and handle HTTP status codes appropriately.\nFurthermore, source integration often involves data transformation at the ingestion point. This might include parsing JSON or XML responses, converting timestamps to a standard format, or performing basic data cleaning operations. The goal is to standardize the data format as it enters the ML pipeline, simplifying downstream processing.\nIt’s also essential to consider the reliability and availability of data sources. Some sources may experience downtime or have inconsistent data quality. Implementing retry mechanisms, data quality checks, and fallback procedures can help ensure a steady flow of reliable data into the ML system.\n\n\n6.5.4 Data Validation\nData validation is an important step in the ingestion process, ensuring that incoming data meets quality standards and conforms to expected schemas. This step helps prevent downstream issues in ML pipelines caused by data anomalies or inconsistencies.\nAt the ingestion stage, validation typically encompasses several key aspects. First, it checks for schema conformity, ensuring that incoming data adheres to the expected structure, including data types and field names. Next, it verifies data ranges and constraints, confirming that numeric fields fall within expected ranges and that categorical fields contain valid values. Completeness checks are also performed, looking for missing or null values in required fields. Additionally, consistency checks ensure that related data points are logically coherent (Gudivada, Rao, et al. 2017).\n\nGudivada, Venkat N., Dhana Rao Rao, et al. 2017. “Data Quality Considerations for Big Data and Machine Learning: Going Beyond Data Cleaning and Transformations.” IEEE Transactions on Knowledge and Data Engineering.\nFor example, in a healthcare ML system ingesting patient data, validation might include checking that age values are positive integers, diagnosis codes are from a predefined set, and admission dates are not in the future. By implementing robust validation at the ingestion stage, ML engineers can detect and handle data quality issues early, significantly reducing the risk of training models on flawed or inconsistent data.\n\n\n6.5.5 Error Handling\nError handling in data ingestion is essential for building resilient ML systems. Errors can occur at various points in the ingestion process, from source connection issues to data validation failures. Effective error handling strategies ensure that the ML pipeline can continue to operate even when faced with data ingestion challenges.\nA key concept in error handling is graceful degradation. This involves designing systems to continue functioning, possibly with reduced capabilities, when faced with partial data loss or temporary source unavailability. Implementing intelligent retry logic for transient errors, such as network interruptions or temporary service outages, is another important aspect of robust error handling. Many ML systems employ the concept of dead letter queues7, using separate storage for data that fails processing. This allows for later analysis and potential reprocessing of problematic data (Kleppmann 2016).\n7 Dead Letter Queues: Queues that store unprocessed messages for analysis or reprocessing.\nKleppmann, Martin. 2016. Designing Data-Intensive Applications: The Big Ideas Behind Reliable, Scalable, and Maintainable Systems. O’Reilly Media. http://shop.oreilly.com/product/0636920032175.do.\nFor instance, in a financial ML system ingesting market data, error handling might involve falling back to slightly delayed data sources if real-time feeds fail, while simultaneously alerting the operations team to the issue. This approach ensures that the system continues to function and that responsible parties are aware of and can address the problem.\nThis ensures that downstream processes have access to reliable, high-quality data for training and inference tasks, even in the face of ingestion challenges. Understanding these concepts of data validation and error handling is essential for students and practitioners aiming to build robust, production-ready ML systems.\nOnce ingestion is complete and data is validated, it is typically loaded into a storage environment suited to the organization’s analytical or machine learning needs. Some datasets flow into data warehouses for structured queries, whereas others are retained in data lakes for exploratory or large-scale analyses. Advanced systems may also employ feature stores to provide standardized features for machine learning.\n\n\n6.5.6 Case Study: KWS\nA production KWS system typically employs both streaming and batch ingestion patterns. The streaming pattern handles real-time audio data from active devices, where wake words must be detected with minimal latency. This requires careful implementation of pub/sub mechanisms—for example, using Apache Kafka-like streams to buffer incoming audio data and enable parallel processing across multiple inference servers.\nSimultaneously, the system processes batch data for model training and updates. This includes ingesting new wake word recordings from crowdsourcing efforts, synthetic data from voice generation systems, and validated user interactions. The batch processing typically follows an ETL pattern, where audio data is preprocessed (normalized, filtered, segmented) before being stored in a format optimized for model training.\nKWS systems must integrate data from diverse sources, such as real-time audio streams from deployed devices, crowdsourced recordings from data collection platforms etc. Each source presents unique challenges. Real-time audio streams require rate limiting to prevent system overload during usage spikes. Crowdsourced data needs robust validation to ensure recording quality and correct labeling. Synthetic data must be verified for realistic representation of wake word variations.\nKWS systems employ sophisticated error handling mechanisms due to the nature of voice interaction. When processing real-time audio, dead letter queues store failed recognition attempts for analysis, helping identify patterns in false negatives or system failures. Data validation becomes particularly important for maintaining wake word detection accuracy—incoming audio must be checked for quality issues like clipping, noise levels, and appropriate sampling rates.\nFor example, consider a smart home device processing the wake word “Alexa.” The ingestion pipeline must validate:\n\nAudio quality metrics (signal-to-noise ratio, sample rate, bit depth)\nRecording duration (typically 1-2 seconds for wake words)\nBackground noise levels\nSpeaker proximity indicators\n\nInvalid samples are routed to dead letter queues for analysis, while valid samples are processed in real-time for wake word detection.\nThis case study illustrates how real-world ML systems must carefully balance different ingestion patterns, handle multiple data sources, and maintain robust error handling—all while meeting strict latency and reliability requirements. The lessons from KWS systems apply broadly to other ML applications requiring real-time processing capabilities alongside continuous model improvement.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Data Engineering</span>"
    ]
  },
  {
    "objectID": "contents/core/data_engineering/data_engineering.html#data-processing",
    "href": "contents/core/data_engineering/data_engineering.html#data-processing",
    "title": "6  Data Engineering",
    "section": "6.6 Data Processing",
    "text": "6.6 Data Processing\nData processing is a stage in the machine learning pipeline that transforms raw data into a format suitable for model training and inference. This stage encompasses several key activities, each playing a role in preparing data for effective use in ML systems. The approach to data processing is closely tied to the ETL (Extract, Transform, Load) or ELT (Extract, Load, Transform) paradigms discussed earlier.\nIn traditional ETL workflows, much of the data processing occurs before the data is loaded into the target system. This approach front-loads the cleaning, transformation, and feature engineering steps, ensuring that data is in a ready-to-use state when it reaches the data warehouse or ML pipeline. ETL is often preferred when dealing with structured data or when there’s a need for significant data cleansing before analysis.\nConversely, in ELT workflows, raw data is first loaded into the target system, and transformations are applied afterwards. This approach, often used with data lakes, allows for more flexibility in data processing. It’s particularly useful when dealing with unstructured or semi-structured data, or when the exact transformations needed are not known in advance. In ELT, many of the data processing steps we’ll discuss might be performed on-demand or as part of the ML pipeline itself.\nThe choice between ETL and ELT can impact how and when data processing occurs in an ML system. For instance, in an ETL-based system, data cleaning and initial transformations might happen before the data even reaches the ML team. In contrast, an ELT-based system might require ML engineers to handle more of the data processing tasks as part of their workflow.\nRegardless of whether an organization follows an ETL or ELT approach, understanding the following data processing steps is crucial for ML practitioners. These processes ensure that data is clean, relevant, and optimally formatted for machine learning algorithms.\n\n6.6.1 Data Cleaning\nData cleaning involves identifying and correcting errors, inconsistencies, and inaccuracies in datasets. Raw data frequently contains issues such as missing values, duplicates, or outliers that can significantly impact model performance if left unaddressed.\nIn practice, data cleaning might involve removing duplicate records, handling missing values through imputation or deletion, and correcting formatting inconsistencies. For instance, in a customer database, names might be inconsistently capitalized or formatted. A data cleaning process would standardize these entries, ensuring that “John Doe,” “john doe,” and “DOE, John” are all treated as the same entity.\nOutlier detection and treatment is another important aspect of data cleaning. Outliers can sometimes represent valuable information about rare events, but they can also be the result of measurement errors or data corruption. ML practitioners must carefully consider the nature of their data and the requirements of their models when deciding how to handle outliers.\n\n\n6.6.2 Quality Assessment\nQuality assessment goes hand in hand with data cleaning, providing a systematic approach to evaluating the reliability and usefulness of data. This process involves examining various aspects of data quality, including accuracy, completeness, consistency, and timeliness.\nTools and techniques for quality assessment range from simple statistical measures to more complex machine learning-based approaches. For example, data profiling tools can provide summary statistics and visualizations that help identify potential quality issues. More advanced techniques might involve using unsupervised learning algorithms to detect anomalies or inconsistencies in large datasets.\nEstablishing clear quality metrics and thresholds is essential for maintaining data quality over time. These metrics might include the percentage of missing values, the frequency of outliers, or measures of data freshness. Regular quality assessments help ensure that data entering the ML pipeline meets the necessary standards for reliable model training and inference.\n\n\n6.6.3 Data Transformation\nData transformation converts the data from its raw form into a format more suitable for analysis and modeling. This process can include a wide range of operations, from simple conversions to complex mathematical transformations.\nCommon transformation tasks include normalization and standardization, which scale numerical features to a common range or distribution. For example, in a housing price prediction model, features like square footage and number of rooms might be on vastly different scales. Normalizing these features ensures that they contribute more equally to the model’s predictions (Bishop 2006).\n\nBishop, Christopher M. 2006. Pattern Recognition and Machine Learning. Springer.\n8 One-Hot Encoding: Converts categorical variables into binary vectors, where each category is represented by a unique vector with one element set to 1 and the rest to 0. This allows categorical data to be used in ML models requiring numerical input.Other transformations might involve encoding categorical variables, handling date and time data, or creating derived features. For instance, one-hot encoding8 is often used to convert categorical variables into a format that can be readily understood by many machine learning algorithms.\n\n\n6.6.4 Feature Engineering\nFeature engineering is the process of using domain knowledge to create new features that make machine learning algorithms work more effectively. This step is often considered more of an art than a science, requiring creativity and deep understanding of both the data and the problem at hand.\nFeature engineering might involve combining existing features, extracting information from complex data types, or creating entirely new features based on domain insights. For example, in a retail recommendation system, engineers might create features that capture the recency, frequency, and monetary value of customer purchases, known as RFM analysis (Kuhn and Johnson 2013).\n\nKuhn, Max, and Kjell Johnson. 2013. Applied Predictive Modeling. Springer New York. https://doi.org/10.1007/978-1-4614-6849-3.\nThe importance of feature engineering cannot be overstated. Well-en­gi­neered features can often lead to significant improvements in model performance, sometimes outweighing the impact of algorithm selection or hyperparameter tuning.\n\n\n6.6.5 Processing Pipelines\nProcessing pipelines bring together the various data processing steps into a coherent, reproducible workflow. These pipelines ensure that data is consistently prepared across training and inference stages, reducing the risk of data leakage and improving the reliability of ML systems.\nModern ML frameworks and tools often provide capabilities for building and managing data processing pipelines. For instance, Apache Beam and TensorFlow Transform allow developers to define data processing steps that can be applied consistently during both model training and serving.\nEffective pipeline design involves considerations such as modularity, scalability, and version control. Modular pipelines allow for easy updates and maintenance of individual processing steps. Version control for pipelines is crucial, ensuring that changes in data processing can be tracked and correlated with changes in model performance.\n\n\n6.6.6 Scale Considerations\nAs datasets grow larger and ML systems become more complex, the scalability of data processing becomes increasingly important. Processing large volumes of data efficiently often requires distributed computing approaches and careful consideration of computational resources.\nTechniques for scaling data processing include parallel processing, where data is divided across multiple machines or processors for simultaneous processing. Distributed frameworks like Apache Spark are commonly used for this purpose, allowing data processing tasks to be scaled across large clusters of computers.\nAnother important consideration is the balance between preprocessing and on-the-fly computation. While extensive preprocessing can speed up model training and inference, it can also lead to increased storage requirements and potential data staleness. Some ML systems opt for a hybrid approach, preprocessing certain features while computing others on-the-fly during model training or inference.\nEffective data processing is fundamental to the success of ML systems. By carefully cleaning, transforming, and engineering data, practitioners can significantly improve the performance and reliability of their models. As the field of machine learning continues to evolve, so too do the techniques and tools for data processing, making this an exciting and dynamic area of study and practice.\n\n\n6.6.7 Case Study: KWS\nA KWS system requires careful cleaning of audio recordings to ensure reliable wake word detection. Raw audio data often contains various imperfections—background noise, clipped signals, varying volumes, and inconsistent sampling rates. For example, when processing the wake word “Alexa,” the system must clean recordings to standardize volume levels, remove ambient noise, and ensure consistent audio quality across different recording environments, all while preserving the essential characteristics that make the wake word recognizable.\nBuilding on clean data, quality assessment becomes important for KWS systems. Quality metrics for KWS data are uniquely focused on audio characteristics, including signal-to-noise ratio (SNR), audio clarity scores, and speaking rate consistency. For instance, a KWS quality assessment pipeline might automatically flag recordings where background noise exceeds acceptable thresholds or where the wake word is spoken too quickly or unclearly, ensuring only high-quality samples are used for model training.\nThese quality metrics must be carefully calibrated to reflect real-world operating conditions. A robust training dataset incorporates both pristine recordings and samples containing controlled levels of environmental variations. For instance, while recordings with signal-masking interference are excluded, the dataset should include samples with measured background acoustics, variable speaker distances, and concurrent speech or other forms of audio signals. This approach to data diversity ensures the model maintains wake word detection reliability across the full spectrum of deployment environments and acoustic conditions.\nOnce quality is assured, transforming audio data for KWS involves converting raw waveforms into formats suitable for ML models. The typical transformation pipeline converts audio signals into spectrograms9 or mel-frequency cepstral coefficients (MFCCs)10, standardizing the representation across different recording conditions. This transformation must be consistently applied across both training and inference, often with additional considerations for real-time processing on edge devices.\n9 Spectrogram: A visual representation of the spectrum of frequencies in a signal as it varies over time, commonly used in audio processing.10 Mel-Frequency Cepstral Coefficients (MFCCs): Features extracted from audio signals that represent the short-term power spectrum, widely used in speech and audio analysis.Figure 6.8 illustrates this transformation process. The top panel is a raw waveform of a simulated audio signal, which consists of a sine wave mixed with noise. This time-domain representation highlights the challenges posed by real-world recordings, where noise and variability must be addressed. The middle panel shows the spectrogram of the signal, which maps its frequency content over time. The spectrogram provides a detailed view of how energy is distributed across frequencies, making it easier to analyze patterns that could influence wake word recognition, such as the presence of background noise or signal distortions The bottom panel shows the MFCCs, derived from the spectrogram. These coefficients compress the audio information into a format that emphasizes speech-related characteristics, making them well-suited for KWS tasks.\n\n\n\n\n\n\nFigure 6.8: KWS data processing of an audio signal (top panel) represented in a spectrogram (middle panel) showing the energy distribution across time and frequency, along with the corresponding MFCCs (bottom panel) that capture perceptually relevant features.\n\n\n\nWith transformed data in hand, feature engineering for KWS focuses on extracting characteristics that help distinguish wake words from background speech. Engineers might create features capturing tonal variations, speech energy patterns, or temporal characteristics. For the wake word “Alexa,” features might include energy distribution across frequency bands, pitch contours, and duration patterns that characterize typical pronunciations. While hand-engineered speech features have seen much success, learned features (Zeghidour et al. 2021) are increasingly common.\n\nZeghidour, Neil, Olivier Teboul, Félix de Chaumont Quitry, and Marco Tagliasacchi. 2021. “LEAF: A Learnable Frontend for Audio Classification.” arXiv Preprint arXiv:2101.08596, January. http://arxiv.org/abs/2101.08596v1.\nIn practice, bringing all these elements together, KWS processing pipelines must handle both batch processing for training and real-time processing for inference. The pipeline typically includes stages for audio preprocessing, feature extraction, and quality filtering. Importantly, these pipelines must be designed to operate efficiently on edge devices while maintaining consistent processing steps between training and deployment.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Data Engineering</span>"
    ]
  },
  {
    "objectID": "contents/core/data_engineering/data_engineering.html#data-labeling",
    "href": "contents/core/data_engineering/data_engineering.html#data-labeling",
    "title": "6  Data Engineering",
    "section": "6.7 Data Labeling",
    "text": "6.7 Data Labeling\nWhile data engineering encompasses many aspects of preparing data for machine learning systems, data labeling represents a particularly complex systems challenge. As training datasets grow to millions or billions of examples, the infrastructure supporting labeling operations becomes increasingly critical to system performance.\nModern machine learning systems must efficiently handle the creation, storage, and management of labels across their data pipeline. The systems architecture must support various labeling workflows while maintaining data consistency, ensuring quality, and managing computational resources effectively. These requirements compound when dealing with large-scale datasets or real-time labeling needs.\nThe systematic challenges extend beyond just storing and managing labels. Production ML systems need robust pipelines that integrate labeling workflows with data ingestion, preprocessing, and training components. These pipelines must maintain high throughput while ensuring label quality and adapting to changing requirements. For instance, a speech recognition system might need to continuously update its training data with new audio samples and corresponding transcription labels, requiring careful coordination between data collection, labeling, and training subsystems.\nInfrastructure requirements vary significantly based on labeling approach and scale. Manual expert labeling may require specialized interfaces and security controls, while automated labeling systems need substantial compute resources for inference. Organizations must carefully balance these requirements against performance needs and resource constraints.\nWe explore how data labeling fundamentally shapes machine learning system design. From storage architectures to quality control pipelines, each aspect of the labeling process introduces unique technical challenges that ripple throughout the ML infrastructure. Understanding these systems-level implications is essential for building robust, scalable labeling solutions which are an integral part of data negineering.\n\n6.7.1 Label Types\nTo build effective machine learning systems, we must first understand how different types of labels affect our system architecture and resource requirements. Let’s explore this through a practical example: imagine building a smart city system that needs to detect and track various objects like vehicles, pedestrians, and traffic signs from video feeds. Labels capture information about key tasks or concepts.\n\nClassification labels are the simplest form, categorizing images with a specific tag or (in multi-label classification) tags (e.g., labeling an image as “car” or “pedestrian”). While conceptually straightforward, a production system processing millions of video frames must efficiently store and retrieve these labels.\nBounding boxes go further by identifying object locations, drawing a box around each object of interest. Our system now needs to track not just what objects exist, but where they are in each frame. This spatial information introduces new storage and processing challenges, especially when tracking moving objects across video frames.\nSegmentation maps provide the most detailed information by classifying objects at the pixel level, highlighting each object in a distinct color. For our traffic monitoring system, this might mean precisely outlining each vehicle, pedestrian, and road sign. These detailed annotations significantly increase our storage and processing requirements.\n\nFigure 6.9 illustrates the common label types:\n\n\n\n\n\n\nFigure 6.9: An overview of common label types.\n\n\n\nThe choice of label format depends heavily on our system requirements and resource constraints (Johnson-Roberson et al. 2017). While classification labels might suffice for simple traffic counting, autonomous vehicles need detailed segmentation maps to make precise navigation decisions. Leading autonomous vehicle companies often maintain hybrid systems that store multiple label types for the same data, allowing flexible use across different applications.\n\nJohnson-Roberson, Matthew, Charles Barto, Rounak Mehta, Sharath Nittur Sridhar, Karl Rosaen, and Ram Vasudevan. 2017. “Driving in the Matrix: Can Virtual Worlds Replace Human-Generated Annotations for Real World Tasks?” In 2017 IEEE International Conference on Robotics and Automation (ICRA), 746–53. Singapore, Singapore: IEEE. https://doi.org/10.1109/icra.2017.7989092.\n\nArdila, Rosana, Megan Branson, Kelly Davis, Michael Kohler, Josh Meyer, Michael Henretty, Reuben Morais, Lindsay Saunders, Francis Tyers, and Gregor Weber. 2020. “Common Voice: A Massively-Multilingual Speech Corpus.” In Proceedings of the Twelfth Language Resources and Evaluation Conference, 4218–22. Marseille, France: European Language Resources Association. https://aclanthology.org/2020.lrec-1.520.\nBeyond the core labels, production systems must also handle rich metadata. The Common Voice dataset (Ardila et al. 2020), for instance, exemplifies this in its management of audio data for speech recognition. The system tracks speaker demographics for model fairness, recording quality metrics for data filtering, validation status for label reliability, and language information for multilingual support.\nModern labeling platforms have built sophisticated metadata management systems to handle these complex relationships. This metadata becomes important for maintaining and managing data quality and debugging model behavior. If our traffic monitoring system performs poorly in rainy conditions, having metadata about weather conditions during data collection helps identify and address the issue. The infrastructure must efficiently index and query this metadata alongside the primary labels.\nThe choice of label type cascades through our entire system design. A system built for simple classification labels would need significant modifications to handle segmentation maps efficiently. The infrastructure must optimize storage systems for the chosen label format, implement efficient data retrieval patterns for training, maintain quality control pipelines for validation, and manage version control for label updates. Resource allocation becomes particularly critical as data volume grows, requiring careful capacity planning across storage, compute, and networking components.\n\n\n6.7.2 Annotation Methods\nManual labeling by experts is the primary approach in many annotation pipelines. This method produces high-quality results but also raises considerable system design challenges. For instance, in medical imaging systems, experienced radiologists offer essential annotations. Such systems necessitate specialized interfaces for accurate labeling, secure data access controls to protect patient privacy, and reliable version control mechanisms to monitor annotation revisions. Despite the dependable outcomes of expert labeling, the scarcity and high expenses of specialists render it challenging to implement on a large scale for extensive datasets.\nAs we discussed earlier, crowdsourcing offers a path to greater scalability by distributing annotation tasks across many annotators (Sheng and Zhang 2019). Crowdsourcing enables non-experts to distribute annotation tasks, often through dedicated platforms (Sheng and Zhang 2019). Several companies have emerged as leaders in this space, building sophisticated platforms for large-scale annotation. For instance, companies such as Scale AI specialize in managing thousands of concurrent annotators through their platform. Appen focuses on linguistic annotation and text data, while Labelbox has developed specialized tools for computer vision tasks. These platforms allow dataset creators to access a large pool of annotators, making it possible to label vast amounts of data relatively quickly.\n\nSheng, Victor S., and Jing Zhang. 2019. “Machine Learning with Crowdsourcing: A Brief Summary of the Past Research and Future Directions.” Proceedings of the AAAI Conference on Artificial Intelligence 33 (01): 9837–43. https://doi.org/10.1609/aaai.v33i01.33019837.\n\nRatner, Alex, Braden Hancock, Jared Dunnmon, Roger Goldman, and Christopher Ré. 2018. “Snorkel MeTaL: Weak Supervision for Multi-Task Learning.” In Proceedings of the Second Workshop on Data Management for End-to-End Machine Learning. ACM. https://doi.org/10.1145/3209889.3209898.\nWeakly supervised and programmatic methods represent a third approach, using automation to reduce manual effort (Ratner et al. 2018). These systems leverage existing knowledge bases and heuristics to automatically generate labels. For example, distant supervision techniques might use a knowledge base to label mentions of companies in text data. While these methods can rapidly label large datasets, they require substantial compute resources for inference, sophisticated caching systems to avoid redundant computation, and careful monitoring to manage potential noise and bias.\nMost production systems combine multiple annotation approaches to balance speed, cost, and quality. A common pattern employs programmatic labeling for initial coverage, followed by crowdsourced verification and expert review of uncertain cases. This hybrid approach requires careful system design to manage the flow of data between different annotation stages. The infrastructure must track label provenance, manage quality control at each stage, and ensure consistent data access patterns across different annotator types.\nThe choice of annotation method significantly impacts system architecture. Expert-only systems might employ centralized architectures with high-speed access to a single data store. Crowdsourcing demands distributed architectures to handle concurrent annotators. Automated systems need substantial compute resources and caching infrastructure. Many organizations implement tiered architectures where different annotation methods operate on different subsets of data based on complexity and criticality.\nClear guidelines and thorough training remain essential regardless of the chosen architecture. The system must provide consistent interfaces, documentation, and quality metrics across all annotation methods. This becomes particularly challenging when managing diverse annotator pools with varying levels of expertise. Some platforms address this by offering access to specialized annotators. For instance, providing medical professionals for healthcare datasets or domain experts for technical content.\n\n\n6.7.3 Label Quality\nLabel quality is extremely important for machine learning system performance. A model can only be as good as its training data. However, ensuring quality at scale presents significant systems challenges. The fundamental challenge stems from label uncertainty.\nFigure 6.10 illustrates common failure modes in labeling systems: some errors arise from data quality issues (like the blurred frog image), while others require deep domain expertise (as with the black stork identification). Even with clear instructions and careful system design, some fraction of labels will inevitably be incorrect Thyagarajan et al. (2022).\n\nThyagarajan, Aditya, Elías Snorrason, Curtis G. Northcutt, and Jonas Mueller 0001. 2022. “Identifying Incorrect Annotations in Multi-Label Classification Data.” CoRR. https://doi.org/10.48550/ARXIV.2211.13895.\n\n\n\n\n\n\nFigure 6.10: Some examples of hard labeling cases. Source: Northcutt, Athalye, and Mueller (2021)\n\n\nNorthcutt, Curtis G, Anish Athalye, and Jonas Mueller. 2021. “Pervasive Label Errors in Test Sets Destabilize Machine Learning Benchmarks.” arXiv. https://doi.org/https://doi.org/10.48550/arXiv.2103.14749 arXiv-issued DOI via DataCite.\n\n\nProduction ML systems implement multiple layers of quality control to address these challenges. Typically, systematic quality checks continuously monitor the labeling pipeline. These systems randomly sample labeled data for expert review and employ statistical methods to flag potential errors. The infrastructure must efficiently process these checks across millions of examples without creating bottlenecks in the labeling pipeline.\nCollecting multiple labels per data point, often referred to as “consensus labeling,” can help identify controversial or ambiguous cases. Professional labeling companies have developed sophisticated infrastructure for this process. For example, Labelbox has consensus tools that track inter-annotator agreement rates and automatically route controversial cases for expert review. Scale AI implements tiered quality control, where experienced annotators verify the work of newer team members.\nBeyond technical infrastructure, successful labeling systems must consider human factors. When working with annotators, organizations need robust systems for training and guidance. This includes good documentation, clear examples of correct labeling, and regular feedback mechanisms. For complex or domain-specific tasks, the system might implement tiered access levels, routing challenging cases to annotators with appropriate expertise.\nEthical considerations also significantly impact system design. For datasets containing potentially disturbing content, systems should implement protective features like grayscale viewing options (Blackwood et al. 2019). This requires additional image processing pipelines and careful interface design. We need to develop workload management systems that track annotator exposure to sensitive content and enforce appropriate limits.\n\nBlackwood, Jayden, Frances C. Wright, Nicole J. Look Hong, and Anna R. Gagliardi. 2019. “Quality of DCIS Information on the Internet: A Content Analysis.” Breast Cancer Research and Treatment 177 (2): 295–305. https://doi.org/10.1007/s10549-019-05315-8.\nThe quality control system itself generates substantial data that must be efficiently processed and monitored. Organizations typically track inter-annotator agreement rates, label confidence scores, time spent per annotation, error patterns and types, annotator performance metrics, and bias indicators. These metrics must be computed and updated efficiently across millions of examples, often requiring dedicated analytics pipelines.\nRegular bias audits are another critical component of quality control. Systems must monitor for cultural, personal, or professional biases that could skew the labeled dataset. This requires infrastructure for collecting and analyzing demographic information, measuring label distributions across different annotator groups, identifying systematic biases in the labeling process, and implementing corrective measures when biases are detected.\nPerhaps the most important aspect is that the process must remain iterative. As new challenges emerge, quality control systems must adapt and evolve. Through careful system design and implementation of these quality control mechanisms, organizations can maintain high label quality even at a massive scale.\n\n\n6.7.4 AI-Assisted Annotation\nAs machine learning systems grow in scale and complexity, organizations increasingly leverage AI to accelerate and enhance their labeling pipelines. This approach introduces new system design considerations around model deployment, resource management, and human-AI collaboration. The fundamental challenge stems from data volume. Manual annotation alone cannot keep pace with modern ML systems’ data needs. As illustrated in Figure 6.11, AI assistance offers several paths to scale labeling operations, each requiring careful system design to balance speed, quality, and resource usage.\n\n\n\n\n\n\nFigure 6.11: Strategies for acquiring additional labeled training data. Source: Stanford AI Lab.\n\n\n\nModern AI-assisted labeling typically employs a combination of approaches. Pre-annotation involves using AI models to generate preliminary labels for a dataset, which humans can then review and correct. Major labeling platforms have made significant investments in this technology. Snorkel AI uses programmatic labeling to automatically generate initial labels at scale. Scale AI deploys pre-trained models to accelerate annotation in specific domains like autonomous driving, while manycompanies like SuperAnnotate provide automated pre-labeling tools that can reduce manual effort drastically. This method, which often employs semi-supervised learning techniques (Chapelle, Scholkopf, and Zien 2009), can save a significant amount of time, especially for extremely large datasets.\n\nChapelle, O., B. Scholkopf, and A. Zien Eds. 2009. “Semi-Supervised Learning (Chapelle, o. Et Al., Eds.; 2006) [Book Reviews].” IEEE Transactions on Neural Networks 20 (3): 542–42. https://doi.org/10.1109/tnn.2009.2015974.\nThe emergence of Large Language Models (LLMs) like ChatGPT has further transformed labeling pipelines. Beyond simple classification, LLMs can generate rich text descriptions, create labeling guidelines, and even explain their reasoning. For instance, content moderation systems use LLMs to perform initial content classification and generate explanations for policy violations. However, integrating LLMs introduces new system challenges around inference costs, rate limiting, and output validation. Many organizations adopt a tiered approach, using smaller specialized models for routine cases while reserving larger LLMs for complex scenarios.\nMethods such as active learning11 complement these approaches by intelligently prioritizing which examples need human attention (Coleman et al. 2022). These systems continuously analyze model uncertainty to identify valuable labeling candidates for humans to label. The infrastructure must efficiently compute uncertainty metrics, maintain task queues, and adapt prioritization strategies based on incoming labels. Consider a medical imaging system: active learning might identify unusual pathologies for expert review while handling routine cases automatically.\n11 A machine learning approach where the model selects the most informative data points for labeling to improve learning efficiency.\nColeman, Cody, Edward Chou, Julian Katz-Samuels, Sean Culatana, Peter Bailis, Alexander C. Berg, Robert Nowak, Roshan Sumbaly, Matei Zaharia, and I. Zeki Yalniz. 2022. “Similarity Search for Efficient Active Learning and Search of Rare Concepts.” Proceedings of the AAAI Conference on Artificial Intelligence 36 (6): 6402–10. https://doi.org/10.1609/aaai.v36i6.20591.\nQuality control becomes increasingly crucial as these AI components interact. The system must monitor both AI and human performance, detect potential errors, and maintain clear label provenance. This requires dedicated infrastructure tracking metrics like model confidence and human-AI agreement rates. In safety-critical domains like self-driving cars, these systems must maintain particularly rigorous standards while processing massive streams of sensor data.\nReal-world deployments demonstrate these principles at scale. Medical imaging systems (Krishnan, Rajpurkar, and Topol 2022) combine pre-annotation for common conditions with active learning for unusual cases, all while maintaining strict patient privacy.\n\nKrishnan, Rayan, Pranav Rajpurkar, and Eric J. Topol. 2022. “Self-Supervised Learning in Medicine and Healthcare.” Nature Biomedical Engineering 6 (12): 1346–52. https://doi.org/10.1038/s41551-022-00914-1.\nSelf-driving vehicle systems coordinate multiple AI models to label diverse sensor data in real-time. Social media platforms process millions of items hourly, using tiered approaches where simpler models handle clear cases while complex content routes to more sophisticated models or human reviewers.\nWhile AI assistance offers clear benefits, it also introduces new failure modes. Systems must guard against bias amplification, where AI models trained on biased data perpetuate those biases in new labels. The infrastructure needs robust monitoring to detect such issues and mechanisms to break problematic feedback loops. Human oversight remains essential, requiring careful interface design to help annotators effectively supervise and correct AI output.\n\n\n6.7.5 Challenges and Limitations\nWhile data labeling is essential for the development of supervised machine learning models, it comes with its own set of challenges and limitations that practitioners must be aware of and address. One of the primary challenges in data labeling is the inherent subjectivity in many labeling tasks. Even with clear guidelines, human annotators may interpret data differently, leading to inconsistencies in labeling. This is particularly evident in tasks involving sentiment analysis, image classification of ambiguous objects, or labeling of complex medical conditions. For instance, in a study of medical image annotation, Oakden-Rayner et al. (2020) found significant variability in labels assigned by different radiologists, highlighting the challenge of obtaining “ground truth” in inherently subjective tasks.\n\nOakden-Rayner, Luke, Jared Dunnmon, Gustavo Carneiro, and Christopher Re. 2020. “Hidden Stratification Causes Clinically Meaningful Failures in Machine Learning for Medical Imaging.” In Proceedings of the ACM Conference on Health, Inference, and Learning, 151–59. ACM. https://doi.org/10.1145/3368555.3384468.\nScalability presents another significant challenge, especially as datasets grow larger and more complex. Manual labeling is time-consuming and expensive, often becoming a bottleneck in the machine learning pipeline. While crowdsourcing and AI-assisted methods can help address this issue to some extent, they introduce their own complications in terms of quality control and potential biases.\nThe issue of bias in data labeling is particularly concerning. Annotators bring their own cultural, personal, and professional biases to the labeling process, which can be reflected in the resulting dataset. For example, Wang et al. (2019) found that image datasets labeled predominantly by annotators from one geographic region showed biases in object recognition tasks, performing poorly on images from other regions. This highlights the need for diverse annotator pools and careful consideration of potential biases in the labeling process.\n\nWang, Tianlu, Jieyu Zhao, Mark Yatskar, Kai-Wei Chang, and Vicente Ordonez. 2019. “Balanced Datasets Are Not Enough: Estimating and Mitigating Gender Bias in Deep Image Representations.” In 2019 IEEE/CVF International Conference on Computer Vision (ICCV), 5309–18. IEEE. https://doi.org/10.1109/iccv.2019.00541.\nData privacy and ethical considerations also pose challenges in data labeling. Leading data labeling companies have developed specialized solutions for these challenges. Scale AI, for instance, maintains dedicated teams and secure infrastructure for handling sensitive data in healthcare and finance. Appen implements strict data access controls and anonymization protocols, while Labelbox offers private cloud deployments for organizations with strict security requirements. When dealing with sensitive data, such as medical records or personal communications, ensuring annotator access while maintaining data privacy can be complex.\nThe dynamic nature of real-world data presents another limitation. Labels that are accurate at the time of annotation may become outdated or irrelevant as the underlying distribution of data changes over time. This concept, known as concept drift, necessitates ongoing labeling efforts and periodic re-evaluation of existing labels.\nLastly, the limitations of current labeling approaches become apparent when dealing with edge cases or rare events. In many real-world applications, it’s the unusual or rare instances that are often most critical (e.g., rare diseases in medical diagnosis, or unusual road conditions in autonomous driving). However, these cases are, by definition, underrepresented in most datasets and may be overlooked or mislabeled in large-scale annotation efforts.\n\n\n6.7.6 Case Study: KWS\nThe complex requirements of KWS reveal the role of automated data labeling in modern machine learning. The Multilingual Spoken Words Corpus (MSWC) (Mazumder et al. 2021) illustrates this through its innovative approach to generating labeled wake word data at scale. MSWC is large, containing over 23.4 million one-second spoken examples across 340,000 keywords in 50 different languages.\n\nMazumder, Mark, Sharad Chitlangia, Colby Banbury, Yiping Kang, Juan Manuel Ciro, Keith Achorn, Daniel Galvez, et al. 2021. “Multilingual Spoken Words Corpus.” In Thirty-Fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 2).\n12 Forced Alignment: A technique in audio processing that synchronizes spoken words in an audio file with their corresponding text transcription by analyzing phoneme-level timing.The core of this system, as illustrated in Figure 6.12, begins with paired sentence audio recordings and corresponding transcriptions, which can be sourced from projects like Common Voice or multilingual captioned content platforms such as YouTube. The system processes paired audio-text inputs through forced alignment12 to identify word boundaries, extracts individual keywords as one-second segments, and generates a large-scale multilingual dataset suitable for training keyword spotting models. For example, when a speaker says, “He gazed up the steep bank,” their voice generates a complex acoustic signal that conveys more than just the words themselves. This signal encapsulates subtle transitions between words, variations in pronunciation, and the natural rhythm of speech. The primary challenge lies in accurately pinpointing the exact location of each word within this continuous audio stream.\n\n\n\n\n\n\nFigure 6.12: MSWC’s automated data labeling pipeline.\n\n\n\nThis is where automated forced alignment proves useful. Tools such as the Montreal Forced Aligner (McAuliffe et al. 2017) analyze both the audio and its transcription, mapping the timing relationship between written words and spoken sounds, and attempts to mark the boundaries of when each word begins and ends in a speech recording at millisecond-level precision. For high-resource languages such as English, high-quality automated alignments are available “out-of-box” while alignments for low-resource languages must be bootstrapped on the speech data and transcriptions themselves, which can negatively impact timing quality.\n\nMcAuliffe, Michael, Michaela Socolof, Sarah Mihuc, Michael Wagner, and Morgan Sonderegger. 2017. “Montreal Forced Aligner: Trainable Text-Speech Alignment Using Kaldi.” In Interspeech 2017, 498–502. ISCA. https://doi.org/10.21437/interspeech.2017-1386.\nWith these precise timestamps, the extraction system can generate clean, one-second samples of individual keywords. However, this process requires careful engineering decisions. Background noise might interfere with detecting word boundaries. Speakers may stretch, compress, or mispronounce words in unexpected ways. Longer words may not fit within the default 1-second boundary. In order to aid ML practitioners in filtering out lower-quality samples in an automated fashion, MSWC provides a self-supervised anomaly detection algorithm, using acoustic embeddings to identify potential issues based on embedding distances to k-means clusters. This automated validation becomes particularly crucial given the scale of the dataset—over 23 million samples across more than 340,000 words in 50+ languages. Traditional manual review could not maintain consistent standards across such volume without significant expense.\nModern voice assistant developers often build upon this type of labeling foundation. An automated corpus like MSWC may not contain the specific keywords an application developer wishes to use for their envisioned KWS system, but the corpus can provide a starting point for KWS prototyping in many underserved languages spoken around the world. While MSWC provides automated labeling at scale, production systems may add targeted human recording and verification for challenging cases, rare words, or difficult acoustic environments. The infrastructure must gracefully coordinate between automated processing and human expertise.\nThe impact of this careful engineering extends far beyond the dataset itself. Automated labeling pipelines open new avenues to how we approach wake word detection and other ML tasks across languages or other demographic boundaries. Where manual collection and annotation might yield thousands of examples, automated dataset generation can yield millions while maintaining consistent quality. This enables voice interfaces to understand an ever-expanding vocabulary across the world’s languages.\nThrough this approach to data labeling, MSWC demonstrates how thoughtful data engineering directly impacts production machine learning systems. The careful orchestration of forced alignment, extraction, and quality control creates a foundation for reliable voice interaction across languages. When a voice assistant responds to its wake word, it draws upon this sophisticated labeling infrastructure—a testament to the power of automated data processing in modern machine learning systems.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Data Engineering</span>"
    ]
  },
  {
    "objectID": "contents/core/data_engineering/data_engineering.html#data-storage",
    "href": "contents/core/data_engineering/data_engineering.html#data-storage",
    "title": "6  Data Engineering",
    "section": "6.8 Data Storage",
    "text": "6.8 Data Storage\nMachine learning workloads have data access patterns that differ markedly from those of traditional transactional systems or routine analytics. Whereas transactional databases optimize for frequent writes and row-level updates, most ML pipelines rely on high-throughput reads, large-scale data scans, and evolving schemas. This difference reflects the iterative nature of model development: data scientists repeatedly load and transform vast datasets to engineer features, test new hypotheses, and refine models.\nAdditionally, ML pipelines must accommodate real-world considerations such as evolving business requirements, new data sources, and changes in data availability. These realities push storage solutions to be both scalable and flexible, ensuring that organizations can manage data collected from diverse channels—from sensor feeds to social media text—without constantly retooling the entire infrastructure. In this section, we will compare the practical use of databases, data warehouses, and data lakes for ML projects, then delve into how specialized services, metadata, and governance practices unify these varied systems into a coherent strategy.\n\n6.8.1 Storage Systems\nAll raw and labeled data needs to be stored and accessed efficiently. When considering storage systems for ML, it is essential to understand the differences among different storage systems: databases, data warehouses, and data lakes. Each system has its strengths and is suited to different aspects of ML workflows.\nTable Table 6.1 provides an overview of these storage systems. Databases usually support operational and transactional purposes. They work well for smaller, well-structured datasets, but can become cumbersome and expensive when applied to large-scale ML contexts involving unstructured data (such as images, audio, or free-form text).\n\n\n\nTable 6.1: Comparative overview of the database, data warehouse, and data lake.\n\n\n\n\n\n\n\n\n\n\n\nAttribute\nConventional Database\nData Warehouse\nData Lake\n\n\n\n\nPurpose\nOperational and transactional\nAnalytical and reporting\nStorage for raw and diverse data for future processing\n\n\nData type\nStructured\nStructured\nStructured, semi-structured, and unstructured\n\n\nScale\nSmall to medium volumes\nMedium to large volumes\nLarge volumes of diverse data\n\n\nPerformance Optimization\nOptimized for transactional queries (OLTP)\nOptimized for analytical queries (OLAP)\nOptimized for scalable storage and retrieval\n\n\nExamples\nMySQL, PostgreSQL, Oracle DB\nGoogle BigQuery, Amazon Redshift, Microsoft Azure Synapse\nGoogle Cloud Storage, AWS S3, Azure Data Lake Storage\n\n\n\n\n\n\nData warehouses, by contrast, are optimized for analytical queries across integrated datasets that have been transformed into a standardized schema. As indicated in the table, they handle large volumes of integrated data. Many ML systems successfully draw on data warehouses to power model training because the structured environment simplifies data exploration and feature engineering. Yet one limitation remains: a data warehouse may not accommodate truly unstructured data or rapidly changing data formats, particularly if the data originates from web scraping or Internet of Things (IoT) sensors.\nData lakes address this gap by storing structured, semi-structured, and unstructured data in its native format, deferring schema definitions until the point of reading or analysis (sometimes called schema-on-read)13. As Table Table 6.1 shows, data lakes can handle large volumes of diverse data types. This approach grants data scientists tremendous latitude when dealing with experimental use cases or novel data types. However, data lakes also demand careful cataloging and metadata management. Without sufficient governance, these expansive repositories risk devolving into unsearchable, disorganized silos.\n13 Schema-on-read: A data management approach where data schema definitions are applied at the time of query or analysis rather than during initial data storage.The examples provided in Table Table 6.1 illustrate the range of technologies available for each storage system type. For instance, MySQL represents a traditional database system, while solutions like Google BigQuery and Amazon Redshift are examples of modern, cloud-based data warehouses. For data lakes, cloud storage solutions such as Google Cloud Storage, AWS S3, and Azure Data Lake Storage are commonly used due to their scalability and flexibility.\n\n\n6.8.2 Storage Considerations\nWhile traditional storage systems provide a foundation for ML workflows, the unique characteristics of machine learning workloads necessitate additional considerations. These ML-specific storage needs stem from the nature of ML development, training, and deployment processes, and addressing them is necessary for building efficient and scalable ML systems.\nOne of the primary challenges in ML storage is handling large model weights. Modern ML models, especially deep learning models, can have millions or even billions of parameters. For instance, GPT-3, a large language model, has 175 billion parameters, requiring approximately 350 GB of storage just for the model weights (Brown et al. 2020). Storage systems need to be capable of handling these large, often dense, numerical arrays efficiently, both in terms of storage capacity and access speed. This requirement goes beyond traditional data storage and enters the realm of high-performance computing storage solutions.\n\nBrown, Tom B., Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, et al. 2020. “Language Models Are Few-Shot Learners.” Advances in Neural Information Processing Systems 33: 1877–1901.\nThe iterative nature of ML development introduces another critical storage consideration: versioning for both datasets and models. Unlike traditional software version control, ML versioning needs to track large binary files efficiently. As data scientists experiment with different model architectures and hyperparameters, they generate numerous versions of models and datasets. Effective storage systems for ML must provide mechanisms to track these changes, revert to previous versions, and maintain reproducibility throughout the ML lifecycle. This capability is essential not only for development efficiency but also for regulatory compliance and model auditing in production environments.\nDistributed training, often necessary for large models or datasets, generates substantial intermediate data, including partial model updates, gradients, and checkpoints. Storage systems for ML need to handle frequent, possibly concurrent, read and write operations of these intermediate results. Moreover, they should provide low-latency access to support efficient synchronization between distributed workers. This requirement pushes storage systems to balance between high throughput for large data transfers and low latency for quick synchronization operations.\nThe diversity of data types in ML workflows presents another unique challenge. ML systems often work with a wide variety of data – from structured tabular data to unstructured images, audio, and text. Storage systems need to efficiently handle this diversity, often requiring a combination of different storage technologies optimized for specific data types. For instance, a single ML project might need to store and process tabular data in a columnar format for efficient feature extraction, while also managing large volumes of image data for computer vision tasks.\nAs organizations collect more data and create more sophisticated models, storage systems need to scale seamlessly. This scalability should support not just growing data volumes, but also increasing concurrent access from multiple data scientists and ML models. Cloud-based object storage systems have emerged as a popular solution due to their virtually unlimited scalability, but they introduce their own challenges in terms of data access latency and cost management.\nThe tension between sequential read performance for training and random access for inference is another key consideration. While training on large datasets benefits from high-throughput sequential reads, many ML serving scenarios require fast random access to individual data points or features. Storage systems for ML need to balance these potentially conflicting requirements, often leading to tiered storage architectures where frequently accessed data is kept in high-performance storage while less frequently used data is moved to cheaper, higher-latency storage.\nThe choice and configuration of storage systems can significantly impact the performance, cost-effectiveness, and overall success of ML initiatives. As the field of machine learning continues to evolve, storage solutions will need to adapt to meet the changing demands of increasingly sophisticated ML workflows.\n\n\n6.8.3 Performance Considerations\nThe performance of storage systems is critical in ML workflows, directly impacting the efficiency of model training, the responsiveness of inference, and the overall productivity of data science teams. Understanding and optimizing storage performance requires a focus on several key metrics and strategies tailored to ML workloads.\nOne of the primary performance metrics for ML storage is throughput, particularly for large-scale data processing and model training. High throughput is essential when ingesting and preprocessing vast datasets or when reading large batches of data during model training. For instance, distributed training of deep learning models on large datasets may require sustained read throughput of several gigabytes per second to keep GPU accelerators fully utilized.\nLatency is another metric, especially for online inference and interactive data exploration. Low latency access to individual data points or small batches of data is vital for maintaining responsive ML services. In recommendation systems or real-time fraud detection, for example, storage systems must be able to retrieve relevant features or model parameters within milliseconds to meet strict service level agreements (SLAs).\nThe choice of file format can significantly impact both throughput and latency. Columnar storage formats such as Parquet or ORC14 are particularly well-suited for ML workloads. These formats allow for efficient retrieval of specific features without reading entire records, substantially reducing I/O operations and speeding up data loading for model training and inference. For example, when training a model that only requires a subset of features from a large dataset, columnar formats can reduce data read times by an order of magnitude compared to row-based formats.\n14 Parquet and ORC: Columnar storage formats optimized for analytical workloads and machine learning pipelines. They store data by columns rather than rows, enabling selective retrieval of specific features and reducing I/O overhead for large datasets.Compression is another key factor in storage performance optimization. While compression reduces storage costs and can improve read performance by reducing the amount of data transferred from disk, it also introduces computational overhead for decompression. The choice of compression algorithm often involves a trade-off between compression ratio and decompression speed. For ML workloads, fast decompression is usually prioritized over maximum compression, with algorithms like Snappy or LZ4 being popular choices.\nData partitioning strategies play a role in optimizing query performance for ML workloads. By intelligently partitioning data based on frequently used query parameters (such as date ranges or categorical variables), systems can dramatically improve the efficiency of data retrieval operations. For instance, in a recommendation system processing user interactions, partitioning data by user demographic attributes and time periods can significantly speed up the retrieval of relevant training data for personalized models.\nTo handle the scale of data in modern ML systems, distributed storage architectures are often employed. These systems, such as HDFS (Hadoop Distributed File System) or cloud-based object stores like Amazon S3, distribute data across multiple machines or data centers. This approach not only provides scalability but also enables parallel data access, which can substantially improve read performance for large-scale data processing tasks common in ML workflows.\nCaching strategies are also vital for optimizing storage performance in ML systems. In-memory caching of frequently accessed data or computed features can significantly reduce latency and computational overhead. Distributed caching systems like Redis or Memcached are often used to scale caching capabilities across clusters of machines, providing low-latency access to hot data for distributed training or serving systems.\nAs ML workflows increasingly span from cloud to edge devices, storage performance considerations must extend to these distributed environments. Edge caching and intelligent data synchronization strategies become needed for maintaining performance in scenarios where network connectivity may be limited or unreliable. In the end, the goal is to create a storage infrastructure that can handle the volume and velocity of data in ML workflows while providing the low-latency access needed for responsive model training and inference.\n\n\n6.8.4 Storage Across ML Lifecycle Phases\nThe storage needs of machine learning systems evolve significantly across different phases of the ML lifecycle. Understanding these changing requirements is important for designing effective and efficient ML data infrastructures.\n\nDevelopment Phase\nIn the development phase, storage systems play a critical role in supporting exploratory data analysis and iterative model development. This stage demands flexibility and collaboration, as data scientists often work with various datasets, experiment with feature engineering techniques, and rapidly iterate on model designs to refine their approaches.\nOne of the key challenges at this stage is managing the versions of datasets used in experiments. While traditional version control systems like Git excel at tracking code changes, they fall short when dealing with large datasets. This gap has led to the emergence of specialized tools like DVC (Data Version Control), which enable data scientists to efficiently track dataset changes, revert to previous versions, and share large files without duplication. These tools ensure that teams can maintain reproducibility and transparency throughout the iterative development process.\nBalancing data accessibility and security further complicates the storage requirements in this phase. Data scientists require seamless access to datasets for experimentation, but organizations must simultaneously safeguard sensitive data. This tension often results in the implementation of sophisticated access control mechanisms, ensuring that datasets remain both accessible and protected. Secure data sharing systems enhance collaboration while adhering to strict organizational and regulatory requirements, enabling teams to work productively without compromising data integrity.\n\n\nTraining Phase\nThe training phase presents unique storage challenges due to the sheer volume of data processed and the computational intensity of model training. At this stage, the interplay between storage performance and computational efficiency becomes critical, as modern ML algorithms demand seamless integration between data access and processing.\nTo meet these demands, high-performance storage systems must provide the throughput required to feed data to multiple GPU or TPU accelerators simultaneously. Distributed training scenarios amplify this need, often requiring data transfer rates in the gigabytes per second range to ensure that accelerators remain fully utilized. This highlights the importance of optimizing storage for both capacity and speed.\nBeyond data ingestion, managing intermediate results and checkpoints is another critical challenge in the training phase. Long-running training jobs frequently save intermediate model states to allow for resumption in case of interruptions. These checkpoints can grow significantly in size, especially for large-scale models, necessitating storage solutions that enable efficient saving and retrieval without impacting overall performance.\nComplementing these systems is the concept of burst buffers15, borrowed from high-performance computing. These high-speed, temporary storage layers are particularly valuable during training, as they can absorb large, bursty I/O operations. By buffering these spikes in demand, burst buffers help smooth out performance fluctuations and reduce the load on primary storage systems, ensuring that training pipelines remain efficient and reliable.\n15 Burst Buffers: High-speed storage layers used to absorb large, temporary I/O demands in high-performance computing, smoothing performance during data-intensive operations.\n\nDeployment and Serving Phase\nIn the deployment and serving phase, the focus shifts from high-throughput batch operations during training to low-latency, often real-time, data access. This transition highlights the need to balance conflicting requirements, where storage systems must simultaneously support responsive model serving and enable continued learning in dynamic environments.\nReal-time inference demands storage solutions capable of extremely fast access to model parameters and relevant features. To achieve this, systems often rely on in-memory databases or sophisticated caching strategies, ensuring that predictions can be made within milliseconds. These requirements become even more challenging in edge deployment scenarios, where devices operate with limited storage resources and intermittent connectivity to central data stores.\nAdding to this complexity is the need to manage model updates in production environments. Storage systems must facilitate smooth transitions between model versions, ensuring minimal disruption to ongoing services. Techniques like shadow deployment, where new models run alongside existing ones for validation, allow organizations to iteratively roll out updates while monitoring their performance in real-world conditions.\n\n\nMonitoring and Maintenance Phase\nThe monitoring and maintenance phase brings its own set of storage challenges, centered on ensuring the long-term reliability and performance of ML systems. At this stage, the focus shifts to capturing and analyzing data to monitor model behavior, detect issues, and maintain compliance with regulatory requirements.\nA critical aspect of this phase is managing data drift, where the characteristics of incoming data change over time. Storage systems must efficiently capture and store incoming data along with prediction results, enabling ongoing analysis to detect and address shifts in data distributions. This ensures that models remain accurate and aligned with their intended use cases.\nThe sheer volume of logging and monitoring data generated by high-traffic ML services introduces questions of data retention and accessibility. Organizations must balance the need to retain historical data for analysis against the cost and complexity of storing it. Strategies such as tiered storage and compression can help manage costs while ensuring that critical data remains accessible when needed.\nRegulated industries often require immutable storage to support auditing and compliance efforts. Storage systems designed for this purpose guarantee data integrity and non-repudiability, ensuring that stored data cannot be altered or deleted. Blockchain-inspired solutions and write-once-read-many (WORM) technologies are commonly employed to meet these stringent requirements.\n\n\n\n6.8.5 Feature Stores\nFeature stores are a centralized repository that stores and serves pre-computed features for machine learning models, ensuring consistency between training and inference workflows. They have emerged as a critical component in the ML infrastructure stack, addressing the unique challenges of managing and serving features for machine learning models. They act as a central repository for storing, managing, and serving machine learning features, bridging the gap between data engineering and machine learning operations.\nWhat makes feature stores particularly interesting is their role in solving several key challenges in ML pipelines. First, they address the problem of feature consistency between training and serving environments. In traditional ML workflows, features are often computed differently in offline (training) and online (serving) environments, leading to discrepancies that can degrade model performance. Feature stores provide a single source of truth for feature definitions, ensuring consistency across all stages of the ML lifecycle.\nAnother fascinating aspect of feature stores is their ability to promote feature reuse across different models and teams within an organization. By centralizing feature computation and storage, feature stores can significantly reduce redundant work. For instance, if multiple teams are working on different models that require similar features (e.g., customer lifetime value in a retail context), these features can be computed once and reused across projects, improving efficiency and consistency.\nFeature stores also play a role in managing the temporal aspects of features. Many ML use cases require correct point-in-time feature values, especially in scenarios involving time-series data or where historical context is important. Feature stores typically offer time-travel capabilities, allowing data scientists to retrieve feature values as they were at any point in the past. This is crucial for training models on historical data and for ensuring consistency between training and serving environments.\nThe performance characteristics of feature stores are particularly intriguing from a storage perspective. They need to support both high-throughput batch retrieval for model training and low-latency lookups for online inference. This often leads to hybrid architectures where feature stores maintain both an offline store (optimized for batch operations) and an online store (optimized for real-time serving). Synchronization between these stores becomes a critical consideration.\nFeature stores also introduce interesting challenges in terms of data freshness and update strategies. Some features may need to be updated in real-time (e.g., current user session information), while others might be updated on a daily or weekly basis (e.g., aggregated customer behavior metrics). Managing these different update frequencies and ensuring that the most up-to-date features are always available for inference can be complex.\nFrom a storage perspective, feature stores often leverage a combination of different storage technologies to meet their diverse requirements. This might include columnar storage formats like Parquet for the offline store, in-memory databases or key-value stores for the online store, and streaming platforms like Apache Kafka for real-time feature updates.\n\n\n6.8.6 Caching Strategies\nCaching plays a role in optimizing the performance of ML systems, particularly in scenarios involving frequent data access or computation-intensive operations. In the context of machine learning, caching strategies extend beyond traditional web or database caching, addressing unique challenges posed by ML workflows.\nOne of the primary applications of caching in ML systems is in feature computation and serving. Many features used in ML models are computationally expensive to calculate, especially those involving complex aggregations or time-window operations. By caching these computed features, systems can significantly reduce latency in both training and inference scenarios. For instance, in a recommendation system, caching user embedding vectors can dramatically speed up the generation of personalized recommendations.\nCaching strategies in ML systems often need to balance between memory usage and computation time. This trade-off is particularly evident in large-scale distributed training scenarios. Caching frequently accessed data shards or mini-batches in memory can significantly reduce I/O overhead, but it requires careful memory management to avoid out-of-memory errors, especially when working with large datasets or models.\nAnother interesting application of caching in ML systems is model caching. In scenarios where multiple versions of a model are deployed (e.g., for A/B testing or gradual rollout), caching the most frequently used model versions in memory can significantly reduce inference latency. This becomes especially important in edge computing scenarios, where storage and computation resources are limited.\nCaching also plays a vital role in managing intermediate results in ML pipelines. For instance, in feature engineering pipelines that involve multiple transformation steps, caching intermediate results can prevent redundant computations when rerunning pipelines with minor changes. This is particularly useful during the iterative process of model development and experimentation.\nOne of the challenges in implementing effective caching strategies for ML is managing cache invalidation and updates. ML systems often deal with dynamic data where feature values or model parameters may change over time. Implementing efficient cache update mechanisms that balance between data freshness and system performance is an ongoing area of research and development.\nDistributed caching becomes particularly important in large-scale ML systems. Technologies like Redis or Memcached are often employed to create distributed caching layers that can serve multiple training or inference nodes. These distributed caches need to handle challenges like maintaining consistency across nodes and managing failover scenarios.\nEdge caching is another fascinating area in ML systems, especially with the growing trend of edge AI. In these scenarios, caching strategies need to account for limited storage and computational resources on edge devices, as well as potentially intermittent network connectivity. Intelligent caching strategies that prioritize the most relevant data or model components for each edge device can significantly improve the performance and reliability of edge ML systems.\nLastly, the concept of semantic caching16 is gaining traction in ML systems. Unlike traditional caching that operates on exact matches, semantic caching attempts to reuse cached results for semantically similar queries. This can be particularly useful in ML systems where slight variations in input may not significantly change the output, potentially leading to substantial performance improvements.\n16 Semantic Caching: A caching technique that reuses results of previous computations for semantically similar queries, reducing redundancy in data processing.\n\n6.8.7 Access Patterns\nUnderstanding the access patterns in ML systems is useful for designing efficient storage solutions and optimizing the overall system performance. ML workloads exhibit distinct data access patterns that often differ significantly from traditional database or analytics workloads.\nOne of the most prominent access patterns in ML systems is sequential reading of large datasets during model training. Unlike transactional systems that typically access small amounts of data randomly, ML training often involves reading entire datasets multiple times (epochs) in a sequential manner. This pattern is particularly evident in deep learning tasks, where large volumes of data are fed through neural networks repeatedly. Storage systems optimized for high-throughput sequential reads, such as distributed file systems or object stores, are well-suited for this access pattern.\nHowever, the sequential read pattern is often combined with random shuffling between epochs to prevent overfitting and improve model generalization. This introduces an interesting challenge for storage systems, as they need to efficiently support both sequential and random access patterns, often within the same training job.\nIn contrast to the bulk sequential reads common in training, inference workloads often require fast random access to specific data points or features. For example, a recommendation system might need to quickly retrieve user and item features for real-time personalization. This necessitates storage solutions with low-latency random read capabilities, often leading to the use of in-memory databases or caching layers.\nFeature stores, which we discussed earlier, introduce their own unique access patterns. They typically need to support both high-throughput batch reads for offline training and low-latency point lookups for online inference. This dual-nature access pattern often leads to the implementation of separate offline and online storage layers, each optimized for its specific access pattern.\nTime-series data, common in many ML applications such as financial forecasting or IoT analytics, presents another interesting access pattern. These workloads often involve reading contiguous blocks of time-ordered data, but may also require efficient retrieval of specific time ranges or periodic patterns. Specialized time-series databases or carefully designed partitioning schemes in general-purpose databases are often employed to optimize these access patterns.\nAnother important consideration is the write access pattern in ML systems. While training workloads are often read-heavy, there are scenarios that involve significant write operations. For instance, continual learning systems may frequently update model parameters, and online learning systems may need to efficiently append new training examples to existing datasets.\nUnderstanding these diverse access patterns is helpful in designing and optimizing storage systems for ML workloads. It often leads to hybrid storage architectures that combine different technologies to address various access patterns efficiently. For example, a system might use object storage for large-scale sequential reads during training, in-memory databases for low-latency random access during inference, and specialized time-series storage for temporal data analysis.\nAs ML systems continue to evolve, new access patterns are likely to emerge, driving further innovation in storage technologies and architectures. The challenge lies in creating flexible, scalable storage solutions that can efficiently support the diverse and often unpredictable access patterns of modern ML workloads.\n\n\n6.8.8 Case Study: KWS\nDuring development and training, KWS systems must efficiently store and manage large collections of audio data. This includes raw audio recordings from various sources (crowd-sourced, synthetic, and real-world captures), processed features (like spectrograms or MFCCs), and model checkpoints. A typical architecture might use a data lake for raw audio files, allowing flexible storage of diverse audio formats, while processed features are stored in a more structured data warehouse for efficient access during training.\nKWS systems benefit significantly from feature stores, particularly for managing pre-computed audio features. For example, commonly used spectrogram representations or audio embeddings can be computed once and stored for reuse across different experiments or model versions. The feature store must handle both batch access for training and real-time access for inference, often implementing a dual storage architecture – an offline store for training data and an online store for low-latency inference.\nIn production, KWS systems require careful consideration of edge storage requirements. The models must be compact enough to fit on resource-constrained devices while maintaining quick access to necessary parameters for real-time wake word detection. This often involves optimized storage formats and careful caching strategies to balance between memory usage and inference speed.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Data Engineering</span>"
    ]
  },
  {
    "objectID": "contents/core/data_engineering/data_engineering.html#data-governance",
    "href": "contents/core/data_engineering/data_engineering.html#data-governance",
    "title": "6  Data Engineering",
    "section": "6.9 Data Governance",
    "text": "6.9 Data Governance\nData governance is a significant component in the development and deployment of ML systems. It encompasses a set of practices and policies that ensure data is accurate, secure, compliant, and ethically used throughout the ML lifecycle. As ML systems become increasingly integral to decision-making processes across various domains, the importance of robust data governance has grown significantly.\nOne of the central challenges of data governance is addressing the unique complexities posed by ML workflows. These workflows often involve opaque processes, such as feature engineering and model training, which can obscure how data is being used. Governance practices aim to tackle these issues by focusing on maintaining data privacy, ensuring fairness, and providing transparency in decision-making processes. These practices go beyond traditional data management to address the evolving needs of ML systems.\nSecurity and access control form an essential aspect of data governance. Implementing measures to protect data from unauthorized access or breaches is critical in ML systems, which often deal with sensitive or proprietary information. For instance, a healthcare application may require granular access controls to ensure that only authorized personnel can view patient data. Encrypting data both at rest and in transit is another common approach to safeguarding information while enabling secure collaboration among ML teams.\nPrivacy protection is another key pillar of data governance. As ML models often rely on large-scale datasets, there is a risk of infringing on individual privacy rights. Techniques such as differential privacy17 can address this concern by adding carefully calibrated noise to the data. This ensures that individual identities are protected while preserving the statistical patterns necessary for model training. These techniques allow ML systems to benefit from data-driven insights without compromising ethical considerations (Dwork, n.d.), which we will learn more about in the Responsible AI chapter.\n17 Differential Privacy: A technique that preserves privacy by adding random noise to outputs, ensuring individual data points remain unidentifiable.\nDwork, Cynthia. n.d. “Differential Privacy: A Survey of Results.” In Theory and Applications of Models of Computation, 1–19. Springer Berlin Heidelberg. https://doi.org/10.1007/978-3-540-79228-4\\_1.\n\nWachter, Sandra, Brent Mittelstadt, and Chris Russell. 2017. “Counterfactual Explanations Without Opening the Black Box: Automated Decisions and the GDPR.” SSRN Electronic Journal 31: 841. https://doi.org/10.2139/ssrn.3063289.\nRegulatory compliance is a critical area where data governance plays a central role. Laws such as the GDPR in Europe and the HIPAA in the United States impose strict requirements on data handling. Compliance with these regulations often involves implementing features like the ability to delete data upon request or providing individuals with copies of their data, and a “right to explanation” on decisions made by algorithms (Wachter, Mittelstadt, and Russell 2017). These measures not only protect individuals but also ensure organizations avoid legal and reputational risks.\nDocumentation and metadata management, which are often less discussed, are just as important for transparency and reproducibility in ML systems. Clear records of data lineage, including how data flows and transforms throughout the ML pipeline, are essential for accountability. Standardized documentation frameworks, such as Data Cards proposed by Pushkarna, Zaldivar, and Kjartansson (2022), offer a structured way to document the characteristics, limitations, and potential biases of datasets. For example, the Open Images Extended – More Inclusively Annotated People (MIAP) dataset uses a data card to provide detailed information about its motivations, intended use cases, and known risks. This type of documentation enables developers to evaluate datasets effectively and promotes responsible use.\n\nPushkarna, Mahima, Andrew Zaldivar, and Oddur Kjartansson. 2022. “Data Cards: Purposeful and Transparent Dataset Documentation for Responsible AI.” In 2022 ACM Conference on Fairness, Accountability, and Transparency, 1776–826. ACM. https://doi.org/10.1145/3531146.3533231.\n\n\n\n\n\n\nFigure 6.13: Data card example for the Open Images Extended dataset.\n\n\n\nAudit trails are another important component of data governance. These detailed logs track data access and usage throughout the lifecycle of ML models, from collection to deployment. Comprehensive audit trails are invaluable for troubleshooting and accountability, especially in cases of data breaches or unexpected model behavior. They help organizations understand what actions were taken and why, providing a clear path for resolving issues and ensuring compliance.\nConsider a hypothetical ML system designed to predict patient outcomes in a hospital. Such a system would need to address several governance challenges. It would need to ensure that patient data is securely stored and accessed only by authorized personnel, with privacy-preserving techniques in place to protect individual identities. The system would also need to comply with healthcare regulations governing the use of patient data, including detailed documentation of how data is processed and transformed. Comprehensive audit logs would be necessary to track data usage and ensure accountability.\nAs ML systems grow more complex and influential, the challenges of data governance will continue to evolve. Emerging trends, such as blockchain-inspired technologies for tamper-evident logs and automated governance tools, offer promising solutions for real-time monitoring and issue detection. By adopting robust data governance practices, including tools like Data Cards, organizations can build ML systems that are transparent, ethical, and trustworthy.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Data Engineering</span>"
    ]
  },
  {
    "objectID": "contents/core/data_engineering/data_engineering.html#conclusion",
    "href": "contents/core/data_engineering/data_engineering.html#conclusion",
    "title": "6  Data Engineering",
    "section": "6.10 Conclusion",
    "text": "6.10 Conclusion\nData engineering is the backbone of any successful ML system. By thoughtfully defining problems, designing robust pipelines, and practicing rigorous data governance, teams establish a foundation that directly influences model performance, reliability, and ethical standing. Effective data acquisition strategies—whether through existing datasets, web scraping, or crowdsourcing—must balance the realities of domain constraints, privacy obligations, and labeling complexities. Likewise, decisions around data ingestion (batch or streaming) and transformation (ETL or ELT) affect both cost and throughput, with monitoring and observability essential to detect shifting data quality.\nThroughout this chapter, we saw how critical it is to prepare data well in advance of modeling. Data labeling emerges as a particularly delicate phase: it involves human effort, requires strong quality control practices, and has ethical ramifications. Storage choices—relational databases, data warehouses, data lakes, or specialized systems—must align with both the volume and velocity of ML workloads. Feature stores and caching strategies support efficient retrieval across training and serving pipelines, while good data governance ensures adherence to legal regulations, protects privacy, and maintains stakeholder trust.\nAll these elements interlock to create an ecosystem that reliably supplies ML models with the high-quality data they need. When done well, data engineering empowers teams to iterate faster, confidently deploy new features, and build systems capable of adapting to real-world complexity. The next chapters will build on these foundations, exploring how optimized training, robust model operations, and security considerations together form a holistic approach to delivering AI solutions that perform reliably and responsibly at scale.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Data Engineering</span>"
    ]
  },
  {
    "objectID": "contents/core/data_engineering/data_engineering.html#sec-data-engineering-resource",
    "href": "contents/core/data_engineering/data_engineering.html#sec-data-engineering-resource",
    "title": "6  Data Engineering",
    "section": "6.11 Resources",
    "text": "6.11 Resources\nHere is a curated list of resources to support students and instructors in their learning and teaching journeys. We are continuously working on expanding this collection and will add new exercises soon.\n\n\n\n\n\n\nSlides\n\n\n\n\n\nThese slides are a valuable tool for instructors to deliver lectures and for students to review the material at their own pace. We encourage students and instructors to leverage these slides to improve their understanding and facilitate effective knowledge transfer.\n\nData Engineering: Overview.\nFeature engineering.\nData Standards: Speech Commands.\nCrowdsourcing Data for the Long Tail.\nReusing and Adapting Existing Datasets.\nResponsible Data Collection.\nData Anomaly Detection:\n\nAnomaly Detection: Overview.\nAnomaly Detection: Challenges.\nAnomaly Detection: Datasets.\nAnomaly Detection: using Autoencoders.\n\n\n\n\n\n\n\n\n\n\n\nVideos\n\n\n\n\n\n\nComing soon.\n\n\n\n\n\n\n\n\n\n\nExercises\n\n\n\n\n\nTo reinforce the concepts covered in this chapter, we have curated a set of exercises that challenge students to apply their knowledge and deepen their understanding.\n\nExercise 6.1",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Data Engineering</span>"
    ]
  },
  {
    "objectID": "contents/core/frameworks/frameworks.html",
    "href": "contents/core/frameworks/frameworks.html",
    "title": "7  AI Frameworks",
    "section": "",
    "text": "Purpose\nResources: Slides, Videos, Exercises\nHow do AI frameworks bridge the gap between theoretical design and practical implementation, and what role do they play in enabling scalable and effiicent machine learning systems?\nAI frameworks are the middleware software layer that transforms abstract model specifications into executable implementations. The evolution of these frameworks reveals fundamental patterns for translating high-level designs into efficient computational workflows and system execution. Their architecture shines light on the essential trade-offs between abstraction, performance, and portability, providing systematic approaches to managing complexity in machine learning systems. Understanding framework capabilities and constraints offers insights into the engineering decisions that shape system scalability, enabling the development of robust, deployable solutions across diverse computing environments.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>AI Frameworks</span>"
    ]
  },
  {
    "objectID": "contents/core/frameworks/frameworks.html#purpose",
    "href": "contents/core/frameworks/frameworks.html#purpose",
    "title": "7  AI Frameworks",
    "section": "",
    "text": "Learning Objectives\n\n\n\n\nTrace the evolution of machine learning frameworks from early numerical libraries to modern deep learning systems\nAnalyze framework fundamentals including tensor data structures, computational graphs, execution models, and memory management\nDifferentiate between machine learning frameworks architectures, execution strategies, and development tools\nCompare framework specializations across cloud, edge, mobile, and TinyML applications",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>AI Frameworks</span>"
    ]
  },
  {
    "objectID": "contents/core/frameworks/frameworks.html#overview",
    "href": "contents/core/frameworks/frameworks.html#overview",
    "title": "7  AI Frameworks",
    "section": "7.1 Overview",
    "text": "7.1 Overview\nModern machine learning development relies fundamentally on machine learning frameworks, which are comprehensive software libraries or platforms designed to simplify the development, training, and deployment of machine learning models. These frameworks play multiple roles in ML systems, much like operating systems are the foundation of computing systems. Just as operating systems abstract away the complexity of hardware resources and provide standardized interfaces for applications, ML frameworks abstract the intricacies of mathematical operations and hardware acceleration, providing standardized APIs for ML development.\nThe capabilities of ML frameworks are diverse and continuously evolving. They provide efficient implementations of mathematical operations, automatic differentiation capabilities, and tools for managing model development, hardware acceleration, and memory utilization. For production systems, they offer standardized approaches to model deployment, versioning, and optimization. However, due to their diversity, there is no universally agreed-upon definition of an ML framework. To establish clarity for this chapter, we adopt the following definition:\n\n\n\n\n\n\nDefinition of Machine Learning Framework\n\n\n\nA Machine Learning Framework (ML Framework) is a software platform that provides tools and abstractions for designing, training, and deploying machine learning models. It bridges user applications with infrastructure, enabling algorithmic expressiveness through computational graphs and operators, workflow orchestration across the machine learning lifecycle, hardware optimization with schedulers and compilers, scalability for distributed and edge systems, and extensibility to support diverse use cases. ML frameworks form the foundation of modern machine learning systems by simplifying development and deployment processes.\n\n\nThe landscape of ML frameworks continues to evolve with the field itself. Today’s frameworks must address diverse requirements: from training large language models on distributed systems to deploying compact neural networks on tiny IoT devices. Popular frameworks like PyTorch and TensorFlow have developed rich ecosystems that extend far beyond basic model implementation, encompassing tools for data preprocessing, model optimization, and deployment.\nAs we progress into examining training, optimization, and deployment, understanding ML frameworks becomes necessary as they orchestrate the entire machine learning lifecycle. These frameworks provide the architecture that connects all aspects of ML systems, from data ingestion to model deployment. Just as understanding a blueprint is important before studying construction techniques, grasping framework architecture is vital before diving into training methodologies and deployment strategies. Modern frameworks encapsulate the complete ML workflow, and their design choices influence how we approach training, optimization, and inference.\nThis chapter helps us learn how these complex frameworks function, their architectural principles, and their role in modern ML systems. Understanding these concepts will provide the necessary context as we explore specific aspects of the ML lifecycle in subsequent chapters.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>AI Frameworks</span>"
    ]
  },
  {
    "objectID": "contents/core/frameworks/frameworks.html#historical-evolution",
    "href": "contents/core/frameworks/frameworks.html#historical-evolution",
    "title": "7  AI Frameworks",
    "section": "7.2 Historical Evolution",
    "text": "7.2 Historical Evolution\nThe evolution of machine learning frameworks mirrors the broader development of artificial intelligence and computational capabilities. This section explores the distinct phases that reflect both technological advances and changing requirements of the AI community, from early numerical computing libraries to modern deep learning frameworks.\n\n7.2.1 Timeline\nThe development of machine learning frameworks has been built upon decades of foundational work in computational libraries. From the early building blocks of BLAS and LAPACK to today’s cutting-edge frameworks like TensorFlow, PyTorch, and JAX, this journey represents a steady progression toward higher-level abstractions that make machine learning more accessible and powerful.\nLooking at Figure 7.1, we can trace how these fundamental numerical computing libraries laid the groundwork for modern ML development. The mathematical foundations established by BLAS and LAPACK enabled the creation of more user-friendly tools like NumPy and SciPy, which in turn set the stage for today’s sophisticated deep learning frameworks.\n\n\n\n\n\n\nFigure 7.1: Timeline of major developments in computational libraries and machine learning frameworks.\n\n\n\nThis evolution reflects a clear trend: each new layer of abstraction has made complex computational tasks more approachable while building upon the robust foundations of its predecessors. Let us examine how these systems built on top of one another.\n\n\n7.2.2 Early Numerical Libraries\nThe foundation for modern ML frameworks begins at the most fundamental level of computation: matrix operations. Machine learning computations are primarily matrix-matrix and matrix-vector multiplications. The Basic Linear Algebra Subprograms (BLAS), developed in 1979, provided these essential matrix operations that would become the computational backbone of machine learning (Kung and Leiserson 1979). These low-level operations, when combined and executed efficiently, enable the complex calculations required for training neural networks and other ML models.\n\nKung, Hsiang Tsung, and Charles E Leiserson. 1979. “Systolic Arrays (for VLSI).” In Sparse Matrix Proceedings 1978, 1:256–82. Society for industrial; applied mathematics Philadelphia, PA, USA.\nBuilding upon BLAS, the Linear Algebra Package (LAPACK) emerged in 1992, extending these capabilities with more sophisticated linear algebra operations such as matrix decompositions, eigenvalue problems, and linear system solutions. This layered approach of building increasingly complex operations from fundamental matrix computations became a defining characteristic of ML frameworks.\nThe development of NumPy in 2006 marked a crucial milestone in this evolution, building upon its predecessors Numeric and Numarray to become the fundamental package for numerical computation in Python. NumPy introduced n-dimensional array objects and essential mathematical functions, but more importantly, it provided an efficient interface to these underlying BLAS and LAPACK operations. This abstraction allowed developers to work with high-level array operations while maintaining the performance of optimized low-level matrix computations.\nIn 2001, SciPy emerged as a powerful extension built on top of NumPy, adding specialized functions for optimization, linear algebra, and signal processing. This further exemplified the pattern of progressive abstraction in ML frameworks: from basic matrix operations to sophisticated numerical computations, and eventually to high-level machine learning algorithms. This layered architecture, starting from fundamental matrix operations and building upward, would become a blueprint for future ML frameworks, as we will see in this chapter.\n\n\n7.2.3 First-Generation ML Frameworks\nThe transition from numerical libraries to dedicated machine learning frameworks marked a crucial evolution in abstraction. While the underlying computations remained rooted in matrix operations, frameworks began to encapsulate these operations into higher-level machine learning primitives. The University of Waikato introduced Weka in 1993 (Witten and Frank 2002), one of the earliest ML frameworks, which abstracted matrix operations into data mining tasks, though it was limited by its Java implementation and focus on smaller-scale computations.\n\nWitten, Ian H., and Eibe Frank. 2002. “Data Mining: Practical Machine Learning Tools and Techniques with Java Implementations.” ACM SIGMOD Record 31 (1): 76–77. https://doi.org/10.1145/507338.507355.\nScikit-learn, emerging in 2007, was a significant advancement in this abstraction. Building upon the NumPy and SciPy foundation, it transformed basic matrix operations into intuitive ML algorithms. For example, what was fundamentally a series of matrix multiplications and gradient computations became a simple fit() method call in a logistic regression model. This abstraction pattern - hiding complex matrix operations behind clean APIs - would become a defining characteristic of modern ML frameworks.\nTheano, which appeared in 2007, was a major advancement—developed at the Montreal Institute for Learning Algorithms (MILA)—Theano introduced two revolutionary concepts: computational graphs and GPU acceleration (Team et al. 2016). Computational graphs represented mathematical operations as directed graphs, with matrix operations as nodes and data flowing between them. This graph-based approach allowed for automatic differentiation and optimization of the underlying matrix operations. More importantly, it enabled the framework to automatically route these operations to GPU hardware, dramatically accelerating matrix computations.\n\nTeam, The Theano Development, Rami Al-Rfou, Guillaume Alain, Amjad Almahairi, Christof Angermueller, Dzmitry Bahdanau, Nicolas Ballas, et al. 2016. “Theano: A Python Framework for Fast Computation of Mathematical Expressions,” May. http://arxiv.org/abs/1605.02688v1.\nMeanwhile, Torch, created at NYU in 2002, took a different approach to handling matrix operations. It emphasized immediate execution of operations (eager execution) and provided a flexible interface for neural network implementations. Torch’s design philosophy of prioritizing developer experience while maintaining high performance influenced many subsequent frameworks. Its architecture demonstrated how to balance high-level abstractions with efficient low-level matrix operations, establishing design patterns that would later influence frameworks like PyTorch.\n\n\n7.2.4 Rise of Deep Learning Frameworks\nThe deep learning revolution demanded a fundamental shift in how frameworks handled matrix operations, primarily due to three factors: the massive scale of computations, the complexity of gradient calculations through deep networks, and the need for distributed processing. Traditional frameworks, designed for classical machine learning algorithms, could not efficiently handle the billions of matrix operations required for training deep neural networks.\nThe foundations for modern deep learning frameworks emerged from academic research. The University of Montreal’s Theano, released in 2007, established the concepts that would shape future frameworks (Bergstra et al. 2010). It introduced key concepts such as computational graphs1 for automatic differentiation and GPU acceleration, which we will explore in more detail later in this chapter, demonstrating how to efficiently organize and optimize complex neural network computations.\n\nBergstra, James, Olivier Breuleux, Frédéric Bastien, Pascal Lamblin, Razvan Pascanu, Guillaume Desjardins, Joseph Turian, David Warde-Farley, and Yoshua Bengio. 2010. “Theano: A CPU and GPU Math Compiler in Python.” In Proceedings of the 9th Python in Science Conference, 4:18–24. 1. SciPy. https://doi.org/10.25080/majora-92bf1922-003.\n1 Computational Graph: A representation of mathematical computations as a directed graph, where nodes represent operations and edges represent data dependencies, used to enable automatic differentiation.\nJia, Yangqing, Evan Shelhamer, Jeff Donahue, Sergey Karayev, Jonathan Long, Ross Girshick, Sergio Guadarrama, and Trevor Darrell. 2014. “Caffe: Convolutional Architecture for Fast Feature Embedding.” In Proceedings of the 22nd ACM International Conference on Multimedia, 675–78. ACM. https://doi.org/10.1145/2647868.2654889.\nCaffe, released by UC Berkeley in 2013, advanced this evolution by introducing specialized implementations of convolutional operations (Jia et al. 2014). While convolutions are mathematically equivalent to specific patterns of matrix multiplication, Caffe optimized these patterns specifically for computer vision tasks, demonstrating how specialized matrix operation implementations could dramatically improve performance for specific network architectures.\nGoogle’s TensorFlow, introduced in 2015, revolutionized the field by treating matrix operations as part of a distributed computing problem (Dean et al. 2012). It represented all computations, from individual matrix multiplications to entire neural networks, as a static computational graph that could be split across multiple devices. This approach enabled training of unprecedented model sizes by distributing matrix operations across clusters of computers and specialized hardware. TensorFlow’s static graph approach, while initially constraining, allowed for aggressive optimization of matrix operations through techniques like kernel fusion (combining multiple operations into a single kernel for efficiency) and memory planning (pre-allocating memory for operations).\n\nDean, Jeffrey, Greg Corrado, Rajat Monga, Kai Chen 0010, Matthieu Devin, Quoc V. Le, Mark Z. Mao, et al. 2012. “Large Scale Distributed Deep Networks.” In Advances in Neural Information Processing Systems 25: 26th Annual Conference on Neural Information Processing Systems 2012. Proceedings of a Meeting Held December 3-6, 2012, Lake Tahoe, Nevada, United States, edited by Peter L. Bartlett, Fernando C. N. Pereira, Christopher J. C. Burges, Léon Bottou, and Kilian Q. Weinberger, 1232–40. https://proceedings.neurips.cc/paper/2012/hash/6aca97005c68f1206823815f66102863-Abstract.html.\n\nSeide, Frank, and Amit Agarwal. 2016. “CNTK: Microsoft’s Open-Source Deep-Learning Toolkit.” In Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, 2135–35. ACM. https://doi.org/10.1145/2939672.2945397.\nMicrosoft’s CNTK entered the landscape in 2016, bringing robust implementations for speech recognition and natural language processing tasks (Seide and Agarwal 2016). Its architecture emphasized scalability across distributed systems while maintaining efficient computation for sequence-based models.\nFacebook’s PyTorch, also launched in 2016, took a radically different approach to handling matrix computations. Instead of static graphs, PyTorch introduced dynamic computational graphs that could be modified on the fly (Ansel et al. 2024). This dynamic approach, while potentially sacrificing some optimization opportunities, made it much easier for researchers to debug and understand the flow of matrix operations in their models. PyTorch’s success demonstrated that the ability to introspect and modify computations dynamically was as important as raw performance for many applications.\nAmazon’s MXNet approached the challenge of large-scale matrix operations by focusing on memory efficiency and scalability across different hardware configurations. It introduced a hybrid approach that combined aspects of both static and dynamic graphs, allowing for flexible model development while still enabling aggressive optimization of the underlying matrix operations.\nAs deep learning applications grew more diverse, the need for specialized and higher-level abstractions became apparent. Keras emerged in 2015 to address this need, providing a unified interface that could run on top of multiple lower-level frameworks (Chollet et al. 2015).\n\nChollet, François et al. 2015. “Keras.” GitHub Repository. https://github.com/fchollet/keras.\n\nBradbury, James, Roy Frostig, Peter Hawkins, Matthew James Johnson, Chris Leary, Dougal Maclaurin, George Necula, et al. 2018. “JAX: Composable Transformations of Python+NumPy Programs.” http://github.com/google/jax.\n\nHoward, Jeremy, and Sylvain Gugger. 2020. “Fastai: A Layered API for Deep Learning.” Information 11 (2): 108. https://doi.org/10.3390/info11020108.\nGoogle’s JAX, introduced in 2018, brought functional programming principles to deep learning computations, enabling new patterns of model development (Bradbury et al. 2018). FastAI built upon PyTorch to package common deep learning patterns into reusable components, making advanced techniques more accessible to practitioners (Howard and Gugger 2020). These higher-level frameworks demonstrated how abstraction could simplify development while maintaining the performance benefits of their underlying implementations.\n\n\n7.2.5 Hardware Influence on Design\nHardware developments have fundamentally reshaped how frameworks implement and optimize matrix operations. The introduction of NVIDIA’s CUDA platform in 2007 marked a pivotal moment in framework design by enabling general-purpose computing on GPUs. This was transformative because GPUs excel at parallel matrix operations, offering orders of magnitude speedup for the computations in deep learning. While a CPU might process matrix elements sequentially, a GPU can process thousands of elements simultaneously, fundamentally changing how frameworks approach computation scheduling.\nThe development of hardware-specific accelerators further revolutionized framework design. Google’s Tensor Processing Units (TPUs), first deployed in 2016, were purpose-built for tensor operations, the fundamental building blocks of deep learning computations. TPUs introduced systolic array architectures2, which are particularly efficient for matrix multiplication and convolution operations. This hardware architecture prompted frameworks like TensorFlow to develop specialized compilation strategies that could map high-level operations directly to TPU instructions, bypassing traditional CPU-oriented optimizations.\n2 Systolic Array: A hardware architecture designed to perform a series of parallel computations in a time-synchronized manner, optimizing the flow of data through a grid of processors for tasks like matrix multiplication.3 Operation fusion: A technique that combines multiple consecutive operations into a single kernel to reduce memory bandwidth usage and improve computational efficiency, particularly for element-wise operations.Mobile hardware accelerators, such as Apple’s Neural Engine (2017) and Qualcomm’s Neural Processing Units, brought new constraints and opportunities to framework design. These devices emphasized power efficiency over raw computational speed, requiring frameworks to develop new strategies for quantization and operator fusion3. Mobile frameworks like TensorFlow Lite (more recently rebraneded to LiteRT) and PyTorch Mobile needed to balance model accuracy with energy consumption, leading to innovations in how matrix operations are scheduled and executed.\nThe emergence of custom ASIC (Application-Specific Integrated Circuit)4 solutions has further diversified the hardware landscape. Companies like Graphcore, Cerebras, and SambaNova have developed unique architectures for matrix computation, each with different strengths and optimization opportunities. This proliferation of specialized hardware has pushed frameworks to adopt more flexible intermediate representations of matrix operations, allowing for target-specific optimization while maintaining a common high-level interface.\n4 Application-Specific Integrated Circuit (ASIC): is a custom-built hardware chip optimized for specific tasks, such as matrix computations in deep learning, offering superior performance and energy efficiency compared to general-purpose processors.Field Programmable Gate Arrays (FPGAs) introduced yet another dimension to framework optimization. Unlike fixed-function ASICs, FPGAs allow for reconfigurable circuits that can be optimized for specific matrix operation patterns. Frameworks responding to this capability developed just-in-time compilation strategies that could generate optimized hardware configurations based on the specific needs of a model.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>AI Frameworks</span>"
    ]
  },
  {
    "objectID": "contents/core/frameworks/frameworks.html#framework-fundamentals",
    "href": "contents/core/frameworks/frameworks.html#framework-fundamentals",
    "title": "7  AI Frameworks",
    "section": "7.3 Framework Fundamentals",
    "text": "7.3 Framework Fundamentals\nModern machine learning frameworks operate through the integration of four key layers: Fundamentals, Data Handling, Developer Interface, and Execution and Abstraction. These layers function together to provide a structured and efficient foundation for model development and deployment, as illustrated in Figure 7.2.\n\n\n\n\n\n\n\n\ngraph TD\n    %% Fundamentals\n    subgraph Layer_1[\"&lt;span style=margin-right:5em;&gt;Fundamentals&lt;/span&gt;\"]\n        A[Computational Graphs]\n    end\n\n    %% Data Handling\n    subgraph Layer_2[\"&lt;span style=margin-left:5em;&gt;Data Handling&lt;/span&gt;\"]\n        B[Specialized Data Structures]\n        C[Memory Management and Device Placement]\n    end\n    A --&gt;|Provides Structure For| B\n    A --&gt;|Optimizes Execution| C\n\n    %% Developer Interface\n    subgraph Layer_3[Developer Interface]\n        D[Programming Models]\n        E[Execution Models]\n    end\n    D --&gt;|Defines| E\n    E --&gt;|Generates| A\n    \n    %% Execution and Abstraction\n    subgraph Layer_4[\"&lt;span style=margin-left:1em;&gt;Execution and Abstraction&lt;/span&gt;\"]\n        F[Core Operations]\n    end\n    C --&gt;|Coordinates with| F\n    B --&gt;|Feeds Data Into| F\n\n    %% Cross-Layer Dependencies\n    D --&gt;|Influences Data Flow| B\n    D --&gt;|Shapes Execution Behavior| C\n\n    %% Styling\n    style A fill:#d8b9f2,stroke:#333,stroke-width:2px\n    style B fill:#b3e5fc,stroke:#333,stroke-width:2px\n    style C fill:#b3e5fc,stroke:#333,stroke-width:2px\n    style D fill:#fff9c4,stroke:#333,stroke-width:2px\n    style E fill:#ffe0b2,stroke:#333,stroke-width:2px\n    style F fill:#c8e6c9,stroke:#333,stroke-width:2px\n\n    style Layer_1 stroke:#8e44ad,stroke-width:2px\n    style Layer_2 stroke:#0288d1,stroke-width:2px\n    style Layer_3 stroke:#fbc02d,stroke-width:2px\n    style Layer_4 stroke:#388e3c,stroke-width:2px\n\n\n\n\n\n\n\n\n\nFigure 7.2: Framework component interaction.\n\n\n\nThe Fundamentals layer establishes the structural basis of these frameworks through computational graphs. These graphs represent the operations within a model as directed acyclic graphs (DAGs), enabling automatic differentiation and optimization. By organizing operations and data dependencies, computational graphs provide the framework with the ability to distribute workloads and execute computations efficiently across a variety of hardware platforms.\nThe Data Handling layer manages numerical data and parameters essential for machine learning workflows. Central to this layer are specialized data structures, such as tensors, which handle high-dimensional arrays while optimizing memory usage and device placement. Additionally, memory management and data movement strategies ensure that computational workloads are executed efficiently, particularly in environments with diverse or limited hardware resources.\nThe Developer Interface layer provides the tools and abstractions through which users interact with the framework. Programming models allow developers to define machine learning algorithms in a manner suited to their specific needs. These are categorized as either imperative or symbolic. Imperative models offer flexibility and ease of debugging, while symbolic models prioritize performance and deployment efficiency. Execution models further shape this interaction by defining whether computations are carried out eagerly (immediately) or as pre-optimized static graphs.\nThe Execution and Abstraction layer transforms these high-level representations into efficient hardware-executable operations. Core operations, encompassing everything from basic linear algebra to complex neural network layers, are highly optimized for diverse hardware platforms. This layer also includes mechanisms for allocating resources and managing memory dynamically, ensuring robust and scalable performance in both training and inference settings.\nUnderstanding these interconnected layers is essential for leveraging machine learning frameworks effectively. Each layer plays a distinct yet interdependent role in facilitating experimentation, optimization, and deployment. By mastering these concepts, practitioners can make informed decisions about resource utilization, scaling strategies, and the suitability of specific frameworks for various tasks.\n\n7.3.1 Computational Graphs\nMachine learning frameworks must efficiently translate high-level model descriptions into executable computations across diverse hardware platforms. At the center of this translation lies the computational graph—a powerful abstraction that represents mathematical operations and their dependencies. We begin by examining the fundamental structure of computational graphs, then investigate their implementation in modern frameworks, and analyze their implications for system design and performance.\n\nGraph Basics\nComputational graphs emerged as a fundamental abstraction in machine learning frameworks to address the growing complexity of deep learning models. As models grew larger and more sophisticated, the need for efficient execution across diverse hardware platforms became crucial. The computational graph bridges the gap between high-level model descriptions and low-level hardware execution (Baydin et al. 2017a), representing a machine learning model as a directed acyclic graph (DAG) where nodes represent operations and edges represent data flow.\n\nBaydin, Atilim Gunes, Barak A. Pearlmutter, Alexey Andreyevich Radul, and Jeffrey Mark Siskind. 2017a. “Automatic Differentiation in Machine Learning: A Survey.” J. Mach. Learn. Res. 18: 153:1–43. https://jmlr.org/papers/v18/17-468.html.\nFor example, a node might represent a matrix multiplication operation, taking two input matrices (or tensors) and producing an output matrix (or tensor). To visualize this, consider the simple example in Figure 7.3. The directed acyclic graph computes \\(z = x \\times y\\), where each variable is just numbers.\n\n\n\n\n\n\n\n\ngraph LR\n    x((x)) --&gt; star((\"f(x, y)\"))\n    y((y)) --&gt; star((\"f(x, y)\"))\n    star((\"f(x, y)\")) --&gt; z((z))\n\n\n\n\n\n\n\n\nFigure 7.3: Basic example of a computational graph.\n\n\n\nAs shown in Figure 7.4, the structure of the computation graph involves defining interconnected layers, such as convolution, activation, pooling, and normalization, which are optimized before execution. The figure also demonstrates key system-level interactions, including memory management and device placement, showing how the static graph approach enables comprehensive pre-execution analysis and resource allocation.\n\n\n\n\n\n\n\n\nflowchart LR\n subgraph ComputationalGraph[\"Computational Graph\"]\n        B([\"Operation Node 2\"])\n        A([\"Operation Node 1\"])\n        C([\"Operation Node 3\"])\n        D([\"Operation Node 4\"])\n        E([\"Operation Node 5\"])\n        F([\"Operation Node 6\"])\n  end\n subgraph SystemLevelComponents[\"System Components\"]\n        Memory(\"Memory Management\")\n        Device[\"Device Placement\"]\n  end\n    A -- Data Flow --&gt; B\n    B -- Data Flow --&gt; C & D\n    C -- Data Flow --&gt; E\n    D -- Data Flow --&gt; F\n    ComputationalGraph -- Interacts with --&gt; Memory & Device\n\n\n\n\n\n\n\n\n\nFigure 7.4: Example of a computational graph.\n\n\n\n\nLayers and Tensors\nModern machine learning frameworks implement neural network computations through two key abstractions: layers and tensors. Layers represent computational units that perform operations like convolution, pooling, or dense transformations. Each layer maintains internal states, including weights and biases, that evolve during model training. When data flows through these layers, it takes the form of tensors—immutable mathematical objects that hold and transmit numerical values.\nThe relationship between layers and tensors mirrors the distinction between operations and data in traditional programming. A layer defines how to transform input tensors into output tensors, much like a function defines how to transform its inputs into outputs. However, layers add an extra dimension: they maintain and update internal parameters during training. For example, a convolutional layer not only specifies how to perform convolution operations but also learns and stores the optimal convolution filters for a given task.\nFrameworks like TensorFlow and PyTorch leverage this abstraction to simplify model implementation. When a developer writes tf.keras.layers. Conv2D, the framework constructs the necessary graph nodes for convolution operations, parameter management, and data flow. This high-level interface shields developers from the complexities of implementing convolution operations, managing memory, or handling parameter updates during training.\n\n\nBuilding Neural Networks\nThe power of computational graphs extends beyond basic layer operations. Activation functions, essential for introducing non-linearity in neural networks, become nodes in the graph. Functions like ReLU, sigmoid, and tanh transform the output tensors of layers, enabling networks to approximate complex mathematical functions. Frameworks provide optimized implementations of these activation functions, allowing developers to experiment with different non-linearities without worrying about implementation details.\nModern frameworks further extend this abstraction by providing complete model architectures as pre-configured computational graphs. Models like ResNet and MobileNet, which have proven effective across many tasks, come ready to use. Developers can start with these architectures, customize specific layers for their needs, and leverage transfer learning from pre-trained weights. This approach accelerates development while maintaining the benefits of carefully optimized implementations.\n\n\nSystem-Level Implications\nThe computational graph abstraction fundamentally shapes how machine learning frameworks operate. By representing computations as a directed acyclic graph, frameworks gain the ability to analyze and optimize the entire computation before execution begins. The explicit representation of data dependencies enables automatic differentiation—a crucial capability for training neural networks through gradient-based optimization.\nThis graph structure also provides flexibility in execution. The same model definition can run efficiently across different hardware platforms, from CPUs to GPUs to specialized accelerators. The framework handles the complexity of mapping operations to specific hardware capabilities, optimizing memory usage, and coordinating parallel execution. Moreover, the graph structure enables model serialization, allowing trained models to be saved, shared, and deployed across different environments.\nWhile neural network diagrams help visualize model architecture, computational graphs serve a deeper purpose. They provide the precise mathematical representation needed to bridge the gap between intuitive model design and efficient execution. Understanding this representation reveals how frameworks transform high-level model descriptions into optimized, hardware-specific implementations, making modern deep learning practical at scale.\nIt is important to differentiate computational graphs from neural network diagrams, such as those for multilayer perceptrons (MLPs), which depict nodes and layers. Neural network diagrams visualize the architecture and flow of data through nodes and layers, providing an intuitive understanding of the model’s structure. In contrast, computational graphs provide a low-level representation of the underlying mathematical operations and data dependencies required to implement and train these networks.\nFrom a systems perspective, computational graphs provide several key capabilities that influence the entire machine learning pipeline. They enable automatic differentiation5, which we will discuss later, provide clear structure for analyzing data dependencies and potential parallelism, and serve as an intermediate representation that can be optimized and transformed for different hardware targets. Understanding this architecture is essential for comprehending how frameworks translate high-level model descriptions into efficient executable code.\n5 A computational technique that systematically computes derivatives of functions using the chain rule, crucial for training machine learning models through gradient-based optimization.\n\n\nStatic Graphs\nStatic computation graphs, pioneered by early versions of TensorFlow, implement a “define-then-run” execution model. In this approach, developers must specify the entire computation graph before execution begins. This architectural choice has significant implications for both system performance and development workflow, as we will examine later.\nA static computation graph implements a clear separation between the definition of operations and their execution. During the definition phase, each mathematical operation, variable, and data flow connection is explicitly declared and added to the graph structure. This graph is a complete specification of the computation but does not perform any actual calculations. Instead, the framework constructs an internal representation of all operations and their dependencies, which will be executed in a subsequent phase.\nThis upfront definition enables powerful system-level optimizations. The framework can analyze the complete structure to identify opportunities for operation fusion, eliminating unnecessary intermediate results. Memory requirements can be precisely calculated and optimized in advance, leading to efficient allocation strategies. Furthermore, static graphs can be compiled into highly optimized executable code for specific hardware targets, taking full advantage of platform-specific features. Once validated, the same computation can be run repeatedly with high confidence in its behavior and performance characteristics.\nFigure 7.5 illustrates this fundamental two-phase approach: first, the complete computational graph is constructed and optimized; then, during the execution phase, actual data flows through the graph to produce results. This separation enables the framework to perform comprehensive analysis and optimization of the entire computation before any execution begins.\n\n\n\n\n\n\n\n\ngraph LR\n    subgraph \"Definition Phase\"\n        A[Define&lt;br&gt; Operations] --&gt; B[Declare&lt;br&gt; Variables]\n        B --&gt; C[Build&lt;br&gt; Graph]\n    end\n    \n    subgraph \"Execution Phase\"\n        D[Load&lt;br&gt; Data] --&gt; E[Run&lt;br&gt; Graph]\n        E --&gt; F[Get&lt;br&gt; Results]\n    end\n    \n    C --&gt; D\n    \n    classDef phase fill:#d8b9f2,stroke:#333,stroke-width:2px\n    class A,B,C,D,E,F phase\n\n\n\n\n\n\n\n\nFigure 7.5: The two-phase execution model of static computation graphs.\n\n\n\n\n\nDynamic Graphs\nDynamic computation graphs, popularized by PyTorch, implement a “define-by-run” execution model. This approach constructs the graph during execution, offering greater flexibility in model definition and debugging. Unlike static graphs, which rely on predefined memory allocation, dynamic graphs allocate memory as operations execute, making them susceptible to memory fragmentation6 in long-running tasks.\n6 Memory Fragmentation: The inefficient use of memory caused by small, unused gaps between allocated memory blocks, often resulting in wasted memory or reduced performance.As shown in Figure 7.6, each operation is defined, executed, and completed before moving on to define the next operation. This contrasts sharply with static graphs, where all operations must be defined upfront. When an operation is defined, it is immediately executed, and its results become available for subsequent operations or for inspection during debugging. This cycle continues until all operations are complete.\n\n\n\n\n\n\nFigure 7.6: Dynamic graph execution model, illustrating runtime graph construction and immediate execution.\n\n\n\nDynamic graphs excel in scenarios that require conditional execution or dynamic control flow, such as when processing variable-length sequences or implementing complex branching logic. They provide immediate feedback during development, making it easier to identify and fix issues in the computational pipeline. This flexibility aligns naturally with imperative programming patterns familiar to most developers, allowing them to inspect and modify computations at runtime. These characteristics make dynamic graphs particularly valuable during the research and development phase of ML projects.\n\n\nSystem Implications\nThe architectural differences between static and dynamic computational graphs have multiple implications for how machine learning systems are designed and executed. These implications touch on various aspects of memory usage, device utilization, execution optimization, and debugging, all of which play crucial roles in determining the efficiency and scalability of a system. Here, we start with a focus on memory management and device placement as foundational concepts, leaving more detailed discussions for later chapters. This allows us to build a clear understanding before exploring more complex topics like optimization and fault tolerance.\n\nMemory Management\nMemory management occurs when executing computational graphs. Static graphs benefit from their predefined structure, allowing for precise memory planning before execution. Frameworks can calculate memory requirements in advance, optimize allocation, and minimize overhead through techniques like memory reuse. This structured approach helps ensure consistent performance, particularly in resource-constrained environments, such as Mobile and Tiny ML systems.\nDynamic graphs, by contrast, allocate memory dynamically as operations are executed. While this flexibility is invaluable for handling dynamic control flows or variable input sizes, it can result in higher memory overhead and fragmentation. These trade-offs are often most apparent during development, where dynamic graphs enable rapid iteration and debugging but may require additional optimization for production deployment.\n\n\nDevice Placement\nDevice placement, the process of assigning operations to hardware resources such as CPUs, GPUs, or specialized ASICS like TPUs, is another system-level consideration. Static graphs allow for detailed pre-execution analysis, enabling the framework to map computationally intensive operations efficiently to devices while minimizing communication overhead. This capability makes static graphs well-suited for optimizing execution on specialized hardware, where performance gains can be significant.\nDynamic graphs, in contrast, handle device placement at runtime. This allows them to adapt to changing conditions, such as hardware availability or workload demands. However, the lack of a complete graph structure before execution can make it challenging to optimize device utilization fully, potentially leading to inefficiencies in large-scale or distributed setups.\n\n\nA Broader Perspective\nThe trade-offs between static and dynamic graphs extend well beyond memory and device considerations. As shown in Table 7.1, these architectures influence optimization potential, debugging capabilities, scalability, and deployment complexity. While these broader implications are not the focus of this section, they will be explored in detail in later chapters, particularly in the context of training workflows and system-level optimizations.\nThese hybrid solutions aim to provide the flexibility of dynamic graphs during development while enabling the performance optimizations of static graphs in production environments. The choice between static and dynamic graphs often depends on specific project requirements, balancing factors like development speed, production performance, and system complexity.\n\n\n\nTable 7.1: Comparison of static and dynamic computational graphs.\n\n\n\n\n\n\n\n\n\n\nAspect\nStatic Graphs\nDynamic Graphs\n\n\n\n\nMemory Management\nPrecise allocation planning, optimized memory usage\nFlexible but potentially less efficient allocation\n\n\nOptimization Potential\nComprehensive graph-level optimizations possible\nLimited to local optimizations due to runtime construction\n\n\nHardware Utilization\nCan generate highly optimized hardware-specific code\nMay sacrifice some hardware-specific optimizations\n\n\nDevelopment Experience\nRequires more upfront planning, harder to debug\nBetter debugging, faster iteration cycles\n\n\nRuntime Flexibility\nFixed computation structure\nCan adapt to runtime conditions\n\n\nProduction Performance\nGenerally better performance at scale\nMay have overhead from runtime graph construction\n\n\nIntegration with Traditional Code\nMore separation between definition and execution\nNatural integration with imperative code\n\n\nMemory Overhead\nLower memory overhead due to planned allocations\nHigher memory overhead due to dynamic allocations\n\n\nDebugging Capability\nLimited to pre-execution analysis\nRuntime inspection and modification possible\n\n\nDeployment Complexity\nSimpler deployment due to fixed structure\nMay require additional runtime support\n\n\n\n\n\n\n\n\n\n\n7.3.2 Automatic Differentiation\nMachine learning frameworks must solve a fundamental computational challenge: calculating derivatives through complex chains of mathematical operations efficiently and accurately. This capability enables the training of neural networks by computing how millions of parameters should be adjusted to improve the model’s performance (Baydin et al. 2017b).\n\n———. 2017b. “Automatic Differentiation in Machine Learning: A Survey.” J. Mach. Learn. Res. 18 (153): 153:1–43. https://jmlr.org/papers/v18/17-468.html.\nConsider a simple computation that illustrates this challenge:\ndef f(x):\n    a = x * x      # Square\n    b = sin(x)     # Sine\n    return a * b   # Product\nEven in this basic example, computing derivatives manually would require careful application of calculus rules - the product rule, the chain rule, and derivatives of trigonometric functions. Now imagine scaling this to a neural network with millions of operations. This is where automatic differentiation (AD) becomes essential.\nAutomatic differentiation calculates derivatives of functions implemented as computer programs by decomposing them into elementary operations. In our example, AD breaks down f(x) into three basic steps:\n\nComputing a = x * x (squaring)\nComputing b = sin(x) (sine function)\nComputing the final product a * b\n\nFor each step, AD knows the basic derivative rules:\n\nFor squaring: d(x²)/dx = 2x\nFor sine: d(sin(x))/dx = cos(x)\nFor products: d(uv)/dx = u(dv/dx) + v(du/dx)\n\nBy tracking how these operations combine and systematically applying the chain rule, AD computes exact derivatives through the entire computation. When implemented in frameworks like PyTorch or TensorFlow, this enables automatic computation of gradients through arbitrary neural network architectures.7 This fundamental understanding of how AD decomposes and tracks computations sets the foundation for examining its implementation in machine learning frameworks. We will explore its mathematical principles, system architecture implications, and performance considerations that make modern machine learning possible.\n7 Automatic differentiation (AD) benefits diverse fields beyond machine learning, including physics simulations, design optimization, and financial risk analysis, by efficiently and accurately computing derivatives for complex processes (Paszke et al. 2019).\nPaszke, Adam, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, et al. 2019. “PyTorch: An Imperative Style, High-Performance Deep Learning Library.” In Advances in Neural Information Processing Systems, 8026–37.\n\nComputational Approaches\n\nForward Mode\nForward mode automatic differentiation computes derivatives alongside the original computation, tracking how changes propagate from input to output. This approach mirrors how we might manually compute derivatives, making it intuitive to understand and implement in machine learning frameworks.\nConsider our previous example with a slight modification to show how forward mode works:\ndef f(x):    # Computing both value and derivative\n  # Step 1: x -&gt; x²\n  a = x * x           # Value: x²\n  da = 2 * x          # Derivative: 2x\n  \n  # Step 2: x -&gt; sin(x)\n  b = sin(x)          # Value: sin(x)\n  db = cos(x)         # Derivative: cos(x)\n  \n  # Step 3: Combine using product rule\n  result = a * b      # Value: x² * sin(x)\n  dresult = a * db + b * da  # Derivative: x²*cos(x) + sin(x)*2x\n  \n  return result, dresult\nForward mode achieves this systematic derivative computation by augmenting each number with its derivative value, creating what mathematicians call a “dual number.” When x = 2.0, the computation tracks both values and derivatives:\nx = 2.0    # Initial value\ndx = 1.0   # We're tracking derivative with respect to x\n\n# Step 1: x²\na = 4.0    # (2.0)²\nda = 4.0   # 2 * 2.0\n\n# Step 2: sin(x)\nb = 0.909   # sin(2.0)\ndb = -0.416 # cos(2.0)\n\n# Final result\nresult = 3.637   # 4.0 * 0.909\ndresult = 2.805  # 4.0 * (-0.416) + 0.909 * 4.0\n\nImplementation Structure\nForward mode AD structures computations to track both values and derivatives simultaneously through programs. Consider again our simple example:\ndef f(x):\n    a = x * x     \n    b = sin(x)    \n    return a * b   \nWhen a framework executes this function in forward mode, it augments each computation to carry two pieces of information: the value itself and how that value changes with respect to the input. This paired movement of value and derivative mirrors how we think about rates of change:\n# Conceptually, each computation tracks (value, derivative)\nx = (2.0, 1.0)           # Input value and its derivative\na = (4.0, 4.0)           # x² and its derivative 2x\nb = (0.909, -0.416)      # sin(x) and its derivative cos(x)\nresult = (3.637, 2.805)  # Final value and derivative\nThis forward propagation of derivative information happens automatically within the framework’s computational machinery. The framework: 1. Enriches each value with derivative information 2. Transforms each basic operation to handle both value and derivative 3. Propagates this information forward through the computation\nThe beauty of this approach is that it follows the natural flow of computation - as values move forward through the program, their derivatives move with them. This makes forward mode particularly well-suited for functions with single inputs and multiple outputs, as the derivative information follows the same path as the regular computation.\n\n\nPerformance Characteristics\nForward mode AD exhibits distinct performance patterns that influence when and how frameworks employ it. Understanding these characteristics helps explain why frameworks choose different AD approaches for different scenarios.\nForward mode performs one derivative computation alongside each original operation. For a function with one input variable, this means roughly doubling the computational work - once for the value, once for the derivative. The cost scales linearly with the number of operations in the program, making it predictable and manageable for simple computations.\nHowever, consider a neural network layer computing derivatives for matrix multiplication between weights and inputs. To compute derivatives with respect to all weights, forward mode would need to perform the computation once for each weight parameter - potentially thousands of times. This reveals a crucial characteristic: forward mode’s efficiency depends on the number of input variables we need derivatives for.\nForward mode’s memory requirements are relatively modest. It needs to store the original value, a single derivative value, and temporary results during computation. The memory usage stays constant regardless of how complex the computation becomes. This predictable memory pattern makes forward mode particularly suitable for embedded systems with limited memory, real-time applications requiring consistent memory use, and systems where memory bandwidth is a bottleneck.\nThis combination of computational scaling with input variables but constant memory usage creates specific trade-offs that influence framework design decisions. Forward mode shines in scenarios with few inputs but many outputs, where its straightforward implementation and predictable resource usage outweigh the computational cost of multiple passes.\n\n\nUse Cases\nWhile forward mode automatic differentiation isn’t the primary choice for training full neural networks, it plays several important roles in modern machine learning frameworks. Its strength lies in scenarios where we need to understand how small changes in inputs affect a network’s behavior. Consider a data scientist trying to understand why their model makes certain predictions. They might want to analyze how changing a single pixel in an image or a specific feature in their data affects the model’s output:\ndef analyze_image_sensitivity(model, image):\n    # Forward mode tracks how changing one pixel\n    # affects the final classification\n    layer1 = relu(W1 @ image + b1)\n    layer2 = relu(W2 @ layer1 + b2)\n    predictions = softmax(W3 @ layer2 + b3)\n    return predictions\nAs the computation moves through each layer, forward mode carries both values and derivatives, making it straightforward to see how input perturbations ripple through to the final prediction. For each operation, we can track exactly how small changes propagate forward.\nNeural network interpretation presents another compelling application. When researchers want to generate saliency maps or attribution scores, they often need to compute how each input element influences the output:\ndef compute_feature_importance(model, input_features):\n    # Track influence of each input feature\n    # through the network's computation\n    hidden = tanh(W1 @ input_features + b1)\n    logits = W2 @ hidden + b2\n    # Forward mode efficiently computes d(logits)/d(input)\n    return logits\nIn specialized training scenarios, particularly those involving online learning where models update on individual examples, forward mode offers advantages. The framework can track derivatives for a single example through the network efficiently, though this approach becomes less practical when dealing with batch training or updating multiple model parameters simultaneously.\nUnderstanding these use cases helps explain why machine learning frameworks maintain forward mode capabilities alongside other differentiation strategies. While reverse mode handles the heavy lifting of full model training, forward mode provides an elegant solution for specific analytical tasks where its computational pattern matches the problem structure.\n\n\n\nReverse Mode\nReverse mode automatic differentiation forms the computational backbone of modern neural network training. This isn’t by accident - reverse mode’s structure perfectly matches what we need for training neural networks. During training, we have one scalar output (the loss function) and need derivatives with respect to millions of parameters (the network weights). Reverse mode is exceptionally efficient at computing exactly this pattern of derivatives.\nLet’s examine a simple computation in detail:\ndef f(x):\n    a = x * x        # First operation: square x\n    b = sin(x)       # Second operation: sine of x\n    c = a * b        # Third operation: multiply results\n    return c\nIn this function, we have three operations that create a computational chain. Notice how ‘x’ influences the final result ‘c’ through two different paths: once through squaring (a = x²) and once through sine (b = sin(x)). We’ll need to account for both paths when computing derivatives.\nFirst, the forward pass computes and stores values:\n# Forward pass - computing and storing each intermediate value\nx = 2.0             # Our input value\na = 4.0             # x * x = 2.0 * 2.0 = 4.0\nb = 0.909           # sin(2.0) ≈ 0.909\nc = 3.637           # a * b = 4.0 * 0.909 ≈ 3.637\nThen comes the backward pass. This is where reverse mode shows its elegance. We start at the output and work backwards:\n# Backward pass - computing derivatives in reverse\ndc/dc = 1.0    # Derivative of output with respect to itself is 1\n\n# Moving backward through multiplication c = a * b\ndc/da = b      # ∂(a*b)/∂a = b = 0.909\ndc/db = a      # ∂(a*b)/∂b = a = 4.0\n\n# Finally, combining derivatives for x through both paths\n# Path 1: x -&gt; x² -&gt; c    contribution: 2x * dc/da\n# Path 2: x -&gt; sin(x) -&gt; c contribution: cos(x) * dc/db\ndc/dx = (2x * dc/da) + (cos(x) * dc/db)\n      = (2 * 2.0 * 0.909) + (cos(2.0) * 4.0)\n      = 3.636 + (-0.416 * 4.0)\n      = 2.805\nThe power of reverse mode becomes clear when we consider what would happen if we added more operations that depend on x. Forward mode would need to track derivatives through each new path, but reverse mode efficiently handles all paths in a single backward pass. This is exactly the scenario in neural networks, where each weight can affect the final loss through multiple paths in the network.\n\nImplementation Structure\nThe implementation of reverse mode in machine learning frameworks requires careful orchestration of computation and memory. While forward mode simply augments each computation, reverse mode needs to maintain a record of the forward computation to enable the backward pass. Modern frameworks accomplish this through computational graphs and automatic gradient accumulation.\nLet’s extend our previous example to a small neural network computation to see how this works:\ndef simple_network(x, w1, w2):\n    # Forward pass\n    hidden = x * w1             # First layer multiplication\n    activated = max(0, hidden)  # ReLU activation\n    output = activated * w2     # Second layer multiplication\n    return output               # Final output (before loss)\nDuring the forward pass, the framework doesn’t just compute values - it builds a graph of operations while tracking intermediate results:\n# Forward pass with value tracking\nx = 1.0\nw1 = 2.0\nw2 = 3.0\n\nhidden = 2.0        # x * w1 = 1.0 * 2.0\nactivated = 2.0     # max(0, 2.0) = 2.0\noutput = 6.0        # activated * w2 = 2.0 * 3.0\nThe backward pass then uses this saved information to compute gradients for each parameter:\n# Backward pass through computation\nd_output = 1.0          # Start with derivative of output\n\nd_w2 = activated        # d_output * d(output)/d_w2 \n                        # = 1.0 * 2.0 = 2.0\nd_activated = w2        # d_output * d(output)/d_activated \n                        # = 1.0 * 3.0 = 3.0\n\n# ReLU gradient: 1 if input was &gt; 0, 0 otherwise\nd_hidden = d_activated * (1 if hidden &gt; 0 else 0) # 3.0 * 1 = 3.0\n\nd_w1 = x * d_hidden    # 1.0 * 3.0 = 3.0\nd_x = w1 * d_hidden    # 2.0 * 3.0 = 6.0\nThis example illustrates several key implementation considerations: 1. The framework must track dependencies between operations 2. Intermediate values must be stored for the backward pass 3. Gradient computations follow the reverse topological order of the forward computation 4. Each operation needs both forward and backward implementations\n\n\nMemory Management Strategies\nMemory management represents one of the key challenges in implementing reverse mode differentiation in machine learning frameworks. Unlike forward mode where we can discard intermediate values as we go, reverse mode requires storing results from the forward pass to compute gradients during the backward pass.\nConsider our neural network example extended to show memory usage patterns:\ndef deep_network(x, w1, w2, w3):\n    # Forward pass - must store intermediates\n    hidden1 = x * w1              \n    activated1 = max(0, hidden1)   # Store for backward\n    hidden2 = activated1 * w2      \n    activated2 = max(0, hidden2)   # Store for backward\n    output = activated2 * w3\n    return output\nEach intermediate value needed for gradient computation must be kept in memory until its backward pass completes. As networks grow deeper, this memory requirement grows linearly with network depth. For a typical deep neural network processing a batch of images, this can mean gigabytes of stored activations.\nFrameworks employ several strategies to manage this memory burden:\n# Conceptual example of memory management\ndef training_step(model, input_batch):\n    # Strategy 1: Checkpointing\n    with checkpoint_scope():\n        hidden1 = activation(layer1(input_batch))\n        # Framework might free some memory here\n        hidden2 = activation(layer2(hidden1))\n        # More selective memory management\n        output = layer3(hidden2)\n\n    # Strategy 2: Gradient accumulation\n    loss = compute_loss(output)\n    # Backward pass with managed memory\n    loss.backward()\nModern frameworks automatically balance memory usage and computation speed. They might recompute some intermediate values during the backward pass rather than storing everything, particularly for memory-intensive operations. This trade-off between memory and computation becomes especially important in large-scale training scenarios.\n\n\nOptimization Techniques\nReverse mode automatic differentiation in machine learning frameworks employs several key optimization techniques to enhance training efficiency. These optimizations become crucial when training large neural networks where computational and memory resources are pushed to their limits.\nModern frameworks implement gradient checkpointing, a technique that strategically balances computation and memory. Consider a deep neural network:\ndef deep_network(input_tensor):\n    # A typical deep network computation\n    layer1 = large_dense_layer(input_tensor)\n    activation1 = relu(layer1)\n    layer2 = large_dense_layer(activation1)\n    activation2 = relu(layer2)\n    # ... many more layers\n    output = final_layer(activation_n)\n    return output\nInstead of storing all intermediate activations, frameworks can strategically recompute certain values during the backward pass. This trades additional computation for reduced memory usage. The framework might save activations only every few layers:\n# Conceptual representation of checkpointing\ncheckpoint1 = save_for_backward(activation1)\n# Intermediate activations can be recomputed\ncheckpoint2 = save_for_backward(activation4)\n# Framework balances storage vs recomputation\nAnother crucial optimization involves operation fusion. Rather than treating each mathematical operation separately, frameworks combine operations that commonly occur together. Matrix multiplication followed by bias addition, for instance, can be fused into a single operation, reducing memory transfers and improving hardware utilization.\nThe backward pass itself can be optimized by reordering computations to maximize hardware efficiency. Consider the gradient computation for a convolution layer - rather than directly translating the mathematical definition into code, frameworks implement specialized backward operations that take advantage of modern hardware capabilities.\nThese optimizations work together to make the training of large neural networks practical. Without them, many modern architectures would be prohibitively expensive to train, both in terms of memory usage and computation time.\n\n\n\n\nFramework Integration\nThe integration of automatic differentiation into machine learning frameworks requires careful system design to balance flexibility, performance, and usability. Modern frameworks like PyTorch and TensorFlow expose AD capabilities through high-level APIs while maintaining the sophisticated underlying machinery.\nLet’s examine how frameworks present AD to users:\n# PyTorch-style automatic differentiation\ndef neural_network(x):\n    # Framework transparently tracks operations\n    layer1 = nn.Linear(784, 256)\n    layer2 = nn.Linear(256, 10)\n    \n    # Each operation is automatically tracked\n    hidden = torch.relu(layer1(x))\n    output = layer2(hidden)\n    return output\n\n# Training loop showing AD integration\nfor batch_x, batch_y in data_loader:\n    optimizer.zero_grad()    # Clear previous gradients\n    output = neural_network(batch_x)\n    loss = loss_function(output, batch_y)\n    \n    # Framework handles all AD machinery\n    loss.backward()         # Automatic backward pass\n    optimizer.step()        # Parameter updates\nWhile this code appears straightforward, it masks considerable complexity. The framework must: 1. Track all operations during the forward pass 2. Build and maintain the computational graph 3. Manage memory for intermediate values 4. Schedule gradient computations efficiently 5. Interface with hardware accelerators\nThis integration extends beyond basic training. Frameworks must handle complex scenarios like higher-order gradients, where we compute derivatives of derivatives, and mixed-precision training, where different parts of the computation use different numerical precisions:\n# Computing higher-order gradients\nwith torch.set_grad_enabled(True):\n  # First-order gradient computation\n  output = model(input)\n  grad_output = torch.autograd.grad(\n       output, \n       model.parameters())\n  \n  # Second-order gradient computation\n  grad2_output = torch.autograd.grad(\n       grad_output, \n       model.parameters())\n\n\nMemory Implications\nThe memory demands of automatic differentiation stem from a fundamental requirement: to compute gradients during the backward pass, we must remember what happened during the forward pass. This seemingly simple requirement creates interesting challenges for machine learning frameworks. Unlike traditional programs that can discard intermediate results as soon as they’re used, AD systems must carefully preserve computational history.\nConsider what happens in a neural network’s forward pass:\ndef neural_network(x):\n    # Each operation creates values we need to remember\n    a = layer1(x)      # Must store for backward pass\n    b = relu(a)        # Must store input to relu\n    c = layer2(b)      # Must store for backward pass\n    return c\nWhen this network processes data, each operation creates not just its output, but also a memory obligation. The multiplication in layer1 needs to remember its inputs because computing its gradient later will require them. Even the seemingly simple relu function must track which inputs were negative to correctly propagate gradients. As networks grow deeper, these memory requirements accumulate.\nThis memory challenge becomes particularly interesting with deep neural networks:\n# A deeper network shows the accumulating memory needs\nhidden1 = large_matrix_multiply(input, weights1)\nactivated1 = relu(hidden1)\nhidden2 = large_matrix_multiply(activated1, weights2)\nactivated2 = relu(hidden2)\noutput = large_matrix_multiply(activated2, weights3)\nEach layer’s computation adds to our memory burden. The framework must keep hidden1 in memory until we’ve computed gradients through hidden2, but after that, we can safely discard it. This creates a wave of memory usage that peaks when we start the backward pass and gradually recedes as we compute gradients.\nModern frameworks handle this memory choreography automatically. They track the lifetime of each intermediate value - how long it must remain in memory for gradient computation. When training large models, this careful memory management becomes as crucial as the numerical computations themselves. The framework frees memory as soon as it’s no longer needed for gradient computation, ensuring that our memory usage, while necessarily large, remains as efficient as possible.\n\n\nSystem Considerations\nAutomatic differentiation’s integration into machine learning frameworks raises important system-level considerations that affect both framework design and training performance. These considerations become particularly apparent when training large neural networks where efficiency at every level matters.\nConsider a typical training loop that highlights these system-level interactions:\ndef train_epoch(model, data_loader):\n    for batch_x, batch_y in data_loader:\n        # Moving data between CPU and accelerator\n        batch_x = batch_x.to(device)\n        batch_y = batch_y.to(device)\n        \n        # Forward pass builds computational graph\n        outputs = model(batch_x)\n        loss = criterion(outputs, batch_y)\n        \n        # Backward pass computes gradients\n        loss.backward()\n        optimizer.step()\n        optimizer.zero_grad()\nThis simple loop masks complex system interactions. The AD system must coordinate with multiple framework components: the memory allocator, the device manager, the operation scheduler, and the optimizer. Each gradient computation potentially triggers data movement between devices, memory allocation, and kernel launches on accelerators.\nThe scheduling of AD operations becomes particularly intricate with modern hardware accelerators:\n# Complex model with parallel computations\ndef parallel_network(x):\n    # These operations could run concurrently\n    branch1 = conv_layer1(x)\n    branch2 = conv_layer2(x)\n    \n    # Must synchronize for combination\n    combined = branch1 + branch2\n    return final_layer(combined)\nThe AD system must track dependencies not just for correct gradient computation, but also for efficient hardware utilization. It needs to determine which gradient computations can run in parallel and which must wait for others to complete. This dependency tracking extends across both forward and backward passes, creating a complex scheduling problem.\nModern frameworks handle these system-level concerns while maintaining a simple interface for users. Behind the scenes, they make sophisticated decisions about operation scheduling, memory allocation, and data movement, all while ensuring correct gradient computation through the computational graph.\n\n\nSummary\nAutomatic differentiation systems represent an important computational abstraction in machine learning frameworks, transforming the mathematical concept of derivatives into efficient implementations. Through our examination of both forward and reverse modes, we’ve seen how frameworks balance mathematical precision with computational efficiency to enable training of modern neural networks.\nThe implementation of AD systems reveals key design patterns in machine learning frameworks:\n# Simple computation showing AD machinery\ndef computation(x, w):\n    # Framework tracks operations\n    hidden = x * w     # Stored for backward pass\n    output = relu(hidden)  # Tracks activation pattern\n    return output\nThis simple computation embodies several fundamental concepts:\n\nOperation tracking for derivative computation\nMemory management for intermediate values\nSystem coordination for efficient execution\n\nModern frameworks abstract these complexities behind clean interfaces while maintaining high performance:\n# Framework hides AD complexity\nloss = model(input)  # Forward pass tracks computation\nloss.backward()      # Triggers efficient reverse mode AD\noptimizer.step()     # Uses computed gradients\nThe effectiveness of automatic differentiation systems stems from their careful balance of competing demands. They must maintain sufficient computational history for accurate gradients while managing memory constraints, schedule operations efficiently while preserving correctness, and provide flexibility while optimizing performance.\nUnderstanding these systems proves essential for both framework developers and practitioners. Framework developers must implement efficient AD to enable modern deep learning, while practitioners benefit from understanding AD’s capabilities and constraints when designing and training models.\nWhile automatic differentiation provides the computational foundation for gradient-based learning, its practical implementation depends heavily on how frameworks organize and manipulate data. This brings us to our next topic: the data structures that enable efficient computation and memory management in machine learning frameworks. These structures must not only support AD operations but also provide efficient access patterns for the diverse hardware platforms that power modern machine learning.\n\nLooking Forward\nThe automatic differentiation systems we’ve explored provide the computational foundation for neural network training, but they don’t operate in isolation. These systems need efficient ways to represent and manipulate the data flowing through them. This brings us to our next topic: the data structures that machine learning frameworks use to organize and process information.\nConsider how our earlier examples handled numerical values:\ndef neural_network(x):\n    hidden = w1 * x     # What exactly is x?\n    activated = relu(hidden)  # How is hidden stored?\n    output = w2 * activated  # What type of multiplication?\n    return output\nThese operations appear straightforward, but they raise important questions. How do frameworks represent these values? How do they organize data to enable efficient computation and automatic differentiation? Most importantly, how do they structure data to take advantage of modern hardware?\nThe next section examines how frameworks answer these questions through specialized data structures, particularly tensors, that form the basic building blocks of machine learning computations.\n\n\n\n\n7.3.3 Data Structures\nMachine learning frameworks extend computational graphs with specialized data structures, bridging high-level computations with practical implementations. These data structures have two essential purposes: they provide containers for the numerical data that powers machine learning models, and they manage how this data is stored and moved across different memory spaces and devices.\nWhile computational graphs specify the logical flow of operations, data structures determine how these operations actually access and manipulate data in memory. This dual role of organizing numerical data for model computations while handling the complexities of memory management and device placement shapes how frameworks translate mathematical operations into efficient executions across diverse computing platforms.\nThe effectiveness of machine learning frameworks depends heavily on their underlying data organization. While machine learning theory can be expressed through mathematical equations, turning these equations into practical implementations demands thoughtful consideration of data organization, storage, and manipulation. Modern machine learning models must process enormous amounts of data during training and inference, making efficient data access and memory usage critical across diverse hardware platforms.\nA framework’s data structures must excel in three key areas. First, they need to deliver high performance, supporting rapid data access and efficient memory use across different hardware. This includes optimizing memory layouts for cache efficiency and enabling smooth data transfer between memory hierarchies and devices. Second, they must offer flexibility, accommodating various model architectures and training approaches while supporting different data types and precision requirements. Third, they should provide clear and intuitive interfaces to developers while handling complex memory management and device placement behind the scenes.\nThese data structures bridge mathematical concepts and practical computing systems. The operations in machine learning—matrix multiplication, convolution, activation functions—set basic requirements for how data must be organized. These structures must maintain numerical precision and stability while enabling efficient implementation of common operations and automatic gradient computation. However, they must also work within real-world computing constraints, dealing with limited memory bandwidth, varying hardware capabilities, and the needs of distributed computing.\nThe design choices made in implementing these data structures significantly influence what machine learning frameworks can achieve. Poor decisions in data structure design can result in excessive memory use, limiting model size and batch capabilities. They might create performance bottlenecks that slow down training and inference, or produce interfaces that make programming error-prone. On the other hand, thoughtful design enables automatic optimization of memory usage and computation, efficient scaling across hardware configurations, and intuitive programming interfaces that support rapid implementation of new techniques.\nAs we explore specific data structures in the following sections, we’ll examine how frameworks address these challenges through careful design decisions and optimization approaches. This understanding proves essential for anyone working with machine learning systems, whether developing new models, optimizing existing ones, or creating new framework capabilities. We begin with tensor abstractions, the fundamental building blocks of modern machine learning frameworks, before exploring more specialized structures for parameter management, dataset handling, and execution control.\n\nTensors\nMachine learning frameworks process and store numerical data as tensors. Every computation in a neural network, from processing input data to updating model weights, operates on tensors. Training batches of images, activation maps in convolutional networks, and parameter gradients during backpropagation all take the form of tensors. This unified representation allows frameworks to implement consistent interfaces for data manipulation and optimize operations across different hardware architectures.\n\nStructure and Dimensionality\nA tensor is a mathematical object that generalizes scalars, vectors, and matrices to higher dimensions. The dimensionality forms a natural hierarchy: a scalar is a zero-dimensional tensor containing a single value, a vector is a one-dimensional tensor containing a sequence of values, and a matrix is a two-dimensional tensor containing values arranged in rows and columns. Higher-dimensional tensors extend this pattern through nested structures; for instance, as illustrated in Figure 7.7, a three-dimensional tensor can be visualized as a stack of matrices. Therefore, vectors and matrices can be considered special cases of tensors with 1D and 2D dimensions, respectively.\n\n\n\n\n\n\nFigure 7.7: Visualization of a tensor data structure.\n\n\n\nIn practical applications, tensors naturally arise when dealing with complex data structures. As illustrated in Figure 7.8, image data exemplifies this concept particularly well. Color images comprise three channels, where each channel represents the intensity values of red, green, or blue as a distinct matrix. These channels combine to create the full colored image, forming a natural 3D tensor structure. When processing multiple images simultaneously, such as in batch operations, a fourth dimension can be added to create a 4D tensor, where each slice represents a complete three-channel image. This hierarchical organization demonstrates how tensors efficiently handle multidimensional data while maintaining clear structural relationships.\n\n\n\n\n\n\nFigure 7.8: Visualization of colored image structure that can be easily stored as a 3D Tensor. Credit: Niklas Lang\n\n\n\nIn machine learning frameworks, tensors take on additional properties beyond their mathematical definition to meet the demands of modern ML systems. While mathematical tensors provide a foundation as multi-dimensional arrays with transformation properties, machine learning introduces requirements for practical computation. These requirements shape how frameworks balance mathematical precision with computational performance.\nFramework tensors combine numerical data arrays with computational metadata. The dimensional structure, or shape, ranges from simple vectors and matrices to higher-dimensional arrays that represent complex data like image batches or sequence models. This dimensional information plays a critical role in operation validation and optimization. Matrix multiplication operations, for example, depend on shape metadata to verify dimensional compatibility and determine optimal computation paths.\nMemory layout implementation introduces distinct challenges in tensor design. While tensors provide an abstraction of multi-dimensional data, physical computer memory remains linear. Stride patterns address this disparity by creating mappings between multi-dimensional tensor indices and linear memory addresses. These patterns significantly impact computational performance by determining memory access patterns during tensor operations. Careful alignment of stride patterns with hardware memory hierarchies maximizes cache efficiency and memory throughput.\n\n\nType Systems and Precision\nTensor implementations use type systems to control numerical precision and memory consumption. The standard choice in machine learning has been 32-bit floating-point numbers (float32), offering a balance of precision and efficiency. Modern frameworks extend this with multiple numeric types for different needs. Integer types support indexing and embedding operations. Reduced-precision types like 16-bit floating-point numbers enable efficient mobile deployment. 8-bit integers allow fast inference on specialized hardware.\nThe choice of numeric type affects both model behavior and computational efficiency. Neural network training typically requires float32 precision to maintain stable gradient computations. Inference tasks can often use lower precision (int8 or even int4), reducing memory usage and increasing processing speed. Mixed-precision training8 approaches combine these benefits by using float32 for critical accumulations while performing most computations at lower precision.\n8 Mixed-precision training: A training approach that uses lower-precision arithmetic for most calculations while retaining higher-precision for critical operations, balancing performance and numerical stability.Type conversions between different numeric representations require careful management. Operating on tensors with different types demands explicit conversion rules to preserve numerical correctness. These conversions introduce computational costs and risk precision loss. Frameworks provide type casting capabilities but rely on developers to maintain numerical precision across operations.\n\n\nDevice Placement and Memory Management\nThe rise of heterogeneous computing has transformed how machine learning frameworks manage tensor operations. Modern frameworks must seamlessly operate across CPUs, GPUs, TPUs, and various other accelerators, each offering different computational advantages and memory characteristics. This diversity creates a fundamental challenge: tensors must move efficiently between devices while maintaining computational coherency throughout the execution of machine learning workloads.\nDevice placement decisions significantly influence both computational performance and memory utilization. Moving tensors between devices introduces latency costs and consumes precious bandwidth on system interconnects. Keeping multiple copies of tensors across different devices can accelerate computation by reducing data movement, but this strategy increases overall memory consumption and requires careful management of consistency between copies. Frameworks must therefore implement sophisticated memory management systems that track tensor locations and orchestrate data movement while considering these tradeoffs.\nThese memory management systems maintain a dynamic view of available device memory and implement strategies for efficient data transfer. When operations require tensors that reside on different devices, the framework must either move data or redistribute computation. This decision process integrates deeply with the framework’s computational graph execution and operation scheduling. Memory pressure on individual devices, data transfer costs, and computational load all factor into placement decisions.\nThe interplay between device placement and memory management extends beyond simple data movement. Frameworks must anticipate future computational needs to prefetch data efficiently, manage memory fragmentation across devices, and handle cases where memory demands exceed device capabilities. This requires close coordination between the memory management system and the operation scheduler, especially in scenarios involving parallel computation across multiple devices or distributed training across machine boundaries.\n\n\n\nSpecialized Structures\nWhile tensors are the building blocks of machine learning frameworks, they are not the only structures required for effective system operation. Frameworks rely on a suite of specialized data structures tailored to address the distinct needs of data processing, model parameter management, and execution coordination. These structures ensure that the entire workflow—from raw data ingestion to optimized execution on hardware—proceeds seamlessly and efficiently.\n\nDataset Structures\nDataset structures handle the critical task of transforming raw input data into a format suitable for machine learning computations. These structures bridge the gap between diverse data sources and the tensor abstractions required by models, automating the process of reading, parsing, and preprocessing data.\nDataset structures must support efficient memory usage while dealing with input data far larger than what can fit into memory at once. For example, when training on large image datasets, these structures load images from disk, decode them into tensor-compatible formats, and apply transformations like normalization or augmentation in real time. Frameworks implement mechanisms such as data streaming, caching, and shuffling to ensure a steady supply of preprocessed batches without bottlenecks.\nThe design of dataset structures directly impacts training performance. Poorly designed structures can create significant overhead, limiting data throughput to GPUs or other accelerators. In contrast, well-optimized dataset handling can leverage parallelism across CPU cores, disk I/O, and memory transfers to feed accelerators at full capacity.\nIn large, multi-system distributed training scenarios, dataset structures also handle coordination between nodes, ensuring that each worker processes a distinct subset of data while maintaining consistency in operations like shuffling. This coordination prevents redundant computation and supports scalability across multiple devices and machines.\n\n\nParameter Structures\nParameter structures store the numerical values that define a machine learning model. These include the weights and biases of neural network layers, along with auxiliary data such as batch normalization statistics and optimizer state. Unlike datasets, which are transient, parameters persist throughout the lifecycle of model training and inference.\nThe design of parameter structures must balance efficient storage with rapid access during computation. For example, convolutional neural networks require parameters for filters, fully connected layers, and normalization layers, each with unique shapes and memory alignment requirements. Frameworks organize these parameters into compact representations that minimize memory consumption while enabling fast read and write operations.\nA key challenge for parameter structures is managing memory efficiently across multiple devices (0003 et al. 2014). During distributed training, frameworks may replicate parameters across GPUs for parallel computation while keeping a synchronized master copy on the CPU. This strategy ensures consistency while reducing the latency of gradient updates. Additionally, parameter structures often leverage memory sharing techniques to minimize duplication, such as storing gradients and optimizer states in place to conserve memory.\n\n0003, Mu Li, David G. Andersen, Alexander J. Smola, and Kai Yu. 2014. “Communication Efficient Distributed Machine Learning with the Parameter Server.” In Advances in Neural Information Processing Systems 27: Annual Conference on Neural Information Processing Systems 2014, December 8-13 2014, Montreal, Quebec, Canada, edited by Zoubin Ghahramani, Max Welling, Corinna Cortes, Neil D. Lawrence, and Kilian Q. Weinberger, 19–27. https://proceedings.neurips.cc/paper/2014/hash/1ff1de774005f8da13f42943881c655f-Abstract.html.\nParameter structures must also adapt to various precision requirements. While training typically uses 32-bit floating-point precision for stability, reduced precision such as 16-bit floating-point or even 8-bit integers is increasingly used for inference and large-scale training. Frameworks implement type casting and mixed-precision management to enable these optimizations without compromising numerical accuracy.\n\n\nExecution Structures\nExecution structures coordinate how computations are performed on hardware, ensuring that operations execute efficiently while respecting device constraints. These structures work closely with computational graphs, determining how data flows through the system and how memory is allocated for intermediate results.\nOne of the primary roles of execution structures is memory management. During training or inference, intermediate computations such as activation maps or gradients can consume significant memory. Execution structures dynamically allocate and deallocate memory buffers to avoid fragmentation and maximize hardware utilization. For example, a deep neural network might reuse memory allocated for activation maps across layers, reducing the overall memory footprint.\nThese structures also handle operation scheduling, ensuring that computations are performed in the correct order and with optimal hardware utilization. On GPUs, for instance, execution structures can overlap computation and data transfer operations, hiding latency and improving throughput. When running on multiple devices, they synchronize dependent computations to maintain consistency without unnecessary delays.\nDistributed training introduces additional complexity, as execution structures must manage data and computation across multiple nodes. This includes partitioning computational graphs, synchronizing gradients, and redistributing data as needed. Efficient execution structures minimize communication overhead, allowing distributed systems to scale linearly with additional hardware (McMahan et al. 2017).\n\nMcMahan, Brendan, Eider Moore, Daniel Ramage, Seth Hampson, and Blaise Agüera y Arcas. 2017. “Communication-Efficient Learning of Deep Networks from Decentralized Data.” In Proceedings of the 20th International Conference on Artificial Intelligence and Statistics, AISTATS 2017, 20-22 April 2017, Fort Lauderdale, FL, USA, edited by Aarti Singh and Xiaojin (Jerry) Zhu, 54:1273–82. Proceedings of Machine Learning Research. PMLR. http://proceedings.mlr.press/v54/mcmahan17a.html.\n\n\n\n\n7.3.4 Programming Models\nProgramming models define how developers express computations in code. In previous sections, we explored computational graphs and specialized data structures, which together define the computational processes of machine learning frameworks. Computational graphs outline the sequence of operations, such as matrix multiplication or convolution, while data structures like tensors store the numerical values that these operations manipulate. These models fall into two categories: symbolic programming and imperative programming.\n\nSymbolic Programming\nSymbolic programming involves constructing abstract representations of computations first and executing them later. This approach aligns naturally with static computational graphs, where the entire structure is defined before any computation occurs.\nFor instance, in symbolic programming, variables and operations are represented as symbols. These symbolic expressions are not evaluated until explicitly executed, allowing the framework to analyze and optimize the computation graph before running it.\nConsider the following symbolic programming example:\n# Expressions are constructed but not evaluated\nweights = tf.Variable(tf.random.normal([784, 10]))\ninput = tf.placeholder(tf.float32, [None, 784])\noutput = tf.matmul(input, weights)\n\n# Separate evaluation phase\nwith tf.Session() as sess:\n    sess.run(tf.global_variables_initializer())\n    result = sess.run(output, feed_dict={input: data})\nThis approach enables frameworks to apply global optimizations across the entire computation, making it efficient for deployment scenarios. Additionally, static graphs can be serialized and executed across different environments, enhancing portability. Predefined graphs also facilitate efficient parallel execution strategies. However, debugging can be challenging because errors often surface during execution rather than graph construction, and modifying a static graph dynamically is cumbersome.\n\n\nImperative Programming\nImperative programming takes a more traditional approach, executing operations immediately as they are encountered. This method corresponds to dynamic computational graphs, where the structure evolves dynamically during execution.\nIn this programming paradigm, computations are performed directly as the code executes, closely resembling the procedural style of most general-purpose programming languages. For example:\n# Imperative Programming Example\n# Each expression evaluates immediately\nweights = torch.randn(784, 10)\ninput = torch.randn(32, 784)\noutput = input @ weights  # Computation occurs now\nThe immediate execution model is intuitive and aligns with common programming practices, making it easier to use. Errors can be detected and resolved immediately during execution, simplifying debugging. Dynamic graphs allow for adjustments on-the-fly, making them ideal for tasks requiring variable graph structures, such as reinforcement learning or sequence modeling. However, the creation of dynamic graphs at runtime can introduce computational overhead, and the framework’s ability to optimize the entire computation graph is limited due to the step-by-step execution process.\n\n\nSystem Implementation Considerations\nThe choice between symbolic and imperative programming models fundamentally influences how ML frameworks manage system-level features such as memory management and optimization strategies.\n\nPerformance Trade-offs\nIn symbolic programming, frameworks can analyze the entire computation graph upfront. This allows for efficient memory allocation strategies. For example, memory can be reused for intermediate results that are no longer needed during later stages of computation. This global view also enables advanced optimization techniques such as operation fusion, automatic differentiation, and hardware-specific kernel selection. These optimizations make symbolic programming highly effective for production environments where performance is critical.\nIn contrast, imperative programming makes memory management and optimization more challenging since decisions must be made at runtime. Each operation executes immediately, which prevents the framework from globally analyzing the computation. This trade-off, however, provides developers with greater flexibility and immediate feedback during development. Beyond system-level features, the choice of programming model also impacts the developer experience, particularly during model development and debugging.\n\n\nDevelopment and Debugging\nSymbolic programming requires developers to conceptualize their models as complete computational graphs. This often involves extra steps to inspect intermediate values, as symbolic execution defers computation until explicitly invoked. For example, in TensorFlow 1.x, developers need to use sessions and feed dictionaries to debug intermediate results, which can slow down the development process.\nImperative programming offers a more straightforward debugging experience. Operations execute immediately, allowing developers to inspect tensor values and shapes as the code runs. This immediate feedback simplifies experimentation and makes it easier to identify and fix issues in the model. As a result, imperative programming is well-suited for rapid prototyping and iterative model development.\n\n\nNavigating the Trade-offs\nThe choice between symbolic and imperative programming models often depends on the specific needs of a project. Symbolic programming excels in scenarios where performance and optimization are critical, such as production deployments. In contrast, imperative programming provides the flexibility and ease of use necessary for research and development.\nModern frameworks have introduced hybrid approaches that combine the strengths of both paradigms. For instance, TensorFlow 2.x allows developers to write code in an imperative style while converting computations into optimized graph representations for deployment. Similarly, PyTorch provides tools like TorchScript to convert dynamic models into static graphs for production use. These hybrid approaches help bridge the gap between the flexibility of imperative programming and the efficiency of symbolic programming, enabling developers to navigate the trade-offs effectively.\n\n\n\n\n7.3.5 Execution Models\nMachine learning frameworks employ various execution paradigms to determine how computations are performed. These paradigms significantly influence the development experience, performance characteristics, and deployment options of ML systems. Understanding the trade-offs between execution models is essential for selecting the right approach for a given application. Let’s explore three key execution paradigms: eager execution, graph execution, and just-in-time (JIT) compilation.\n\nEager Execution\nEager execution is the most straightforward and intuitive execution paradigm. In this model, operations are executed immediately as they are called in the code. This approach closely mirrors the way traditional imperative programming languages work, making it familiar to many developers.\nConsider the following example using TensorFlow 2.x, which employs eager execution by default:\nimport tensorflow as tf\n\nx = tf.constant([[1., 2.], [3., 4.]])\ny = tf.constant([[1, 2], [3, 4]])\nz = tf.matmul(x, y)\nprint(z)\nIn this code snippet, each line is executed sequentially. When we create the tensors x and y, they are immediately instantiated in memory. The matrix multiplication tf.matmul(x, y) is computed right away, and the result is stored in z. When we print z, we see the output of the computation immediately.\nEager execution offers several advantages. It provides immediate feedback, allowing developers to inspect intermediate values easily. This makes debugging more straightforward and intuitive. It also allows for more dynamic and flexible code structures, as the computation graph can change with each execution.\nHowever, eager execution has its trade-offs. Since operations are executed immediately, the framework has less opportunity to optimize the overall computation graph. This can lead to lower performance compared to more optimized execution paradigms, especially for complex models or when dealing with large datasets.\nEager execution is particularly well-suited for research, interactive development, and rapid prototyping. It allows data scientists and researchers to quickly iterate on their ideas and see results immediately. Many modern ML frameworks, including TensorFlow 2.x and PyTorch, use eager execution as their default mode due to its developer-friendly nature.\n\n\nGraph Execution\nGraph execution, also known as static graph execution, takes a different approach to computing operations in ML frameworks. In this paradigm, developers first define the entire computational graph, and then execute it as a separate step.\nConsider the following example using TensorFlow 1.x style, which employs graph execution:\nimport tensorflow.compat.v1 as tf\ntf.disable_eager_execution()\n\n# Define the graph\nx = tf.placeholder(tf.float32, shape=(2, 2))\ny = tf.placeholder(tf.float32, shape=(2, 2))\nz = tf.matmul(x, y)\n\n# Execute the graph\nwith tf.Session() as sess:\n    result = sess.run(z, feed_dict={\n        x: [[1., 2.], [3., 4.]],\n        y: [[1, 2], [3, 4]]\n    })\n    print(result)\nIn this code snippet, we first define the structure of our computation. The placeholder operations create nodes in the graph for input data, while tf.matmul creates a node representing matrix multiplication. Importantly, no actual computation occurs during this definition phase.\nThe execution of the graph happens when we create a session and call sess.run(). At this point, we provide the actual input data through the feed_dict parameter. The framework then has the complete graph and can perform optimizations before running the computation.\nGraph execution offers several advantages. It allows the framework to see the entire computation ahead of time, enabling global optimizations that can improve performance, especially for complex models. Once defined, the graph can be easily saved and deployed across different environments, enhancing portability. It’s particularly efficient for scenarios where the same computation is repeated many times with different data inputs.\nHowever, graph execution also has its trade-offs. It requires developers to think in terms of building a graph rather than writing sequential operations, which can be less intuitive. Debugging can be more challenging because errors often don’t appear until the graph is executed. Additionally, implementing dynamic computations can be more difficult with a static graph.\nGraph execution is well-suited for production environments where performance and deployment consistency are crucial. It is commonly used in scenarios involving large-scale distributed training and when deploying models for predictions in high-throughput applications.\n\n\nJust-In-Time Compilation\nJust-In-Time (JIT) compilation is a middle ground between eager execution and graph execution. This paradigm aims to combine the flexibility of eager execution with the performance benefits of graph optimization.\nLet’s examine an example using PyTorch’s JIT compilation:\nimport torch\n\n@torch.jit.script\ndef compute(x, y):\n    return torch.matmul(x, y)\n\nx = torch.randn(2, 2)\ny = torch.randn(2, 2)\n\n# First call compiles the function\nresult = compute(x, y)\nprint(result)\n\n# Subsequent calls use the optimized version\nresult = compute(x, y)\nprint(result)\nIn this code snippet, we define a function compute and decorate it with @torch.jit.script. This decorator tells PyTorch to compile the function using its JIT compiler. The first time compute is called, PyTorch analyzes the function, optimizes it, and generates efficient machine code. This compilation process occurs just before the function is executed, hence the term “Just-In-Time”.\nSubsequent calls to compute use the optimized version, potentially offering significant performance improvements, especially for complex operations or when called repeatedly.\nJIT compilation provides a balance between development flexibility and runtime performance. It allows developers to write code in a natural, eager-style manner while still benefiting from many of the optimizations typically associated with graph execution.\nThis approach offers several advantages. It maintains the immediate feedback and intuitive debugging of eager execution, as most of the code still executes eagerly. At the same time, it can deliver performance improvements for critical parts of the computation. JIT compilation can also adapt to the specific data types and shapes being used, potentially resulting in more efficient code than static graph compilation.\nHowever, JIT compilation also has some considerations. The first execution of a compiled function may be slower due to the overhead of the compilation process. Additionally, some complex Python constructs may not be easily JIT-compiled, requiring developers to be aware of what can be optimized effectively.\nJIT compilation is particularly useful in scenarios where you need both the flexibility of eager execution for development and prototyping, and the performance benefits of compilation for production or large-scale training. It’s commonly used in research settings where rapid iteration is necessary but performance is still a concern.\nMany modern ML frameworks incorporate JIT compilation to provide developers with a balance of ease-of-use and performance optimization, as shown in Table 7.2. This balance manifests across multiple dimensions, from the learning curve that gradually introduces optimization concepts to the runtime behavior that combines immediate feedback with performance enhancements. The table highlights how JIT compilation bridges the gap between eager execution’s programming simplicity and graph execution’s performance benefits, particularly in areas like memory usage and optimization scope.\n\n\n\nTable 7.2: Comparison of execution models in machine learning frameworks.\n\n\n\n\n\n\n\n\n\n\n\nAspect\nEager Execution\nGraph Execution\nJIT Compilation\n\n\n\n\nApproach\nComputes each operation immediately when encountered\nBuilds entire computation plan first, then executes\nAnalyzes code at runtime, creates optimized version\n\n\nMemory Usage\nHolds intermediate results throughout computation\nOptimizes memory by planning complete data flow\nAdapts memory usage based on actual execution patterns\n\n\nOptimization Scope\nLimited to local operation patterns\nGlobal optimization across entire computation chain\nCombines runtime analysis with targeted optimizations\n\n\nDebugging Approach\nExamine values at any point during computation\nMust set up specific monitoring points in graph\nInitial runs show original behavior, then optimizes\n\n\nSpeed vs Flexibility\nPrioritizes flexibility over speed\nPrioritizes performance over flexibility\nBalances flexibility and performance\n\n\n\n\n\n\n\n\n\n7.3.6 Core Operations\nMachine learning frameworks employ multiple layers of operations that translate high-level model descriptions into efficient computations on hardware. These operations form a hierarchy: hardware abstraction operations manage the complexity of diverse computing platforms, basic numerical operations implement fundamental mathematical computations, and system-level operations coordinate resources and execution. This operational hierarchy is key to understanding how frameworks transform mathematical models into practical implementations. Figure 7.9 illustrates this hierarchy, showing the relationship between the three layers and their respective subcomponents.\n\n\n\n\n\n\n\n\ngraph TD\n    %% Layers\n    subgraph hardware_abstraction_operations [Hardware Operations]\n        direction TB\n        J[Compute Kernel Management]\n        K[Memory Abstraction]\n        L[Execution Control]\n    end\n\n    subgraph basic_numerical_operations [Basic Numerical Operations]\n        direction TB\n        G[GEMM Operations]\n        H[BLAS Operations]\n        I[Element-wise Operations]\n    end\n\n    subgraph system_level_operations [System-Level Operations]\n        direction TB\n        D[Scheduling]\n        E[Memory Management]\n        F[Resource Optimization]\n    end\n\n    %% Connections between layers (top of subgraphs)\n    system_level_operations --&gt; basic_numerical_operations --&gt; hardware_abstraction_operations\n\n\n\n\n\n\n\n\n\nFigure 7.9: Hierarchical structure of operations in machine learning frameworks.\n\n\n\n\nHardware Abstraction Operations\nAt the lowest level, hardware abstraction operations provide the foundation for executing computations across diverse computing platforms. These operations isolate higher layers from hardware-specific details while maintaining computational efficiency. The abstraction layer must handle three fundamental aspects: compute kernel management, memory system abstraction, and execution control.\n\nCompute Kernel Management\nCompute kernel management involves selecting and dispatching optimal implementations of mathematical operations for different hardware architectures. This requires maintaining multiple implementations of core operations and sophisticated dispatch logic. For example, a matrix multiplication operation might be implemented using AVX-5129 vector instructions on modern CPUs, cuBLAS on NVIDIA GPUs, or specialized tensor processing instructions on AI accelerators. The kernel manager must consider input sizes, data layout, and hardware capabilities when selecting implementations. It must also handle fallback paths for when specialized implementations are unavailable or unsuitable.\n9 A set of 512-bit single-instruction, multiple-data (SIMD) extensions to the x86 instruction set architecture.\n\nMemory System Abstraction\nMemory system abstractions manage data movement through complex memory hierarchies. These abstractions must handle various memory types (registered, pinned, unified) and their specific access patterns. Data layouts often require transformation between hardware-preferred formats - for instance, between row-major and column-major matrix layouts, or between interleaved and planar image formats. The memory system must also manage alignment requirements, which can vary from 4-byte alignment on CPUs to 128-byte alignment on some accelerators. Additionally, it handles cache coherency issues when multiple execution units access the same data.\n\n\nExecution Control\nExecution control operations coordinate computation across multiple execution units and memory spaces. This includes managing execution queues, handling event dependencies, and controlling asynchronous operations. Modern hardware often supports multiple execution streams that can operate concurrently. For example, independent GPU streams or CPU thread pools. The execution controller must manage these streams, handle synchronization points, and ensure correct ordering of dependent operations. It must also provide error handling and recovery mechanisms for hardware-specific failures.\n\n\n\nBasic Numerical Operations\nBuilding upon hardware abstractions, frameworks implement fundamental numerical operations that form the building blocks of machine learning computations. These operations must balance mathematical precision with computational efficiency. General Matrix Multiply (GEMM) operations, which dominate the computational cost of most machine learning workloads. GEMM operations follow the pattern C = αAB + βC, where A, B, and C are matrices, and α and β are scaling factors.\nThe implementation of GEMM operations requires sophisticated optimization techniques. These include blocking10 for cache efficiency, where matrices are divided into smaller tiles that fit in cache memory; loop unrolling11 to increase instruction-level parallelism; and specialized implementations for different matrix shapes and sparsity patterns. For example, fully-connected neural network layers typically use regular dense GEMM operations, while convolutional layers often employ specialized GEMM variants that exploit input locality patterns.\n10 An optimization technique where computations are performed on submatrices (tiles) that fit into cache memory, reducing memory access overhead and improving computational efficiency.11 A method of increasing instruction-level parallelism by manually replicating loop iterations in the code, reducing branching overhead and enabling better utilization of CPU pipelines.Beyond GEMM, frameworks must efficiently implement BLAS operations such as vector addition (AXPY), matrix-vector multiplication (GEMV), and various reduction operations. These operations require different optimization strategies. AXPY operations are typically memory-bandwidth limited, while GEMV operations must balance memory access patterns with computational efficiency.\nElement-wise operations form another critical category, including both basic arithmetic operations (addition, multiplication) and transcendental functions (exponential, logarithm, trigonometric functions). While conceptually simpler than GEMM, these operations present significant optimization opportunities through vectorization and operation fusion. For example, multiple element-wise operations can often be fused into a single kernel to reduce memory bandwidth requirements. The efficiency of these operations becomes particularly important in neural network activation functions and normalization layers, where they process large volumes of data.\nModern frameworks must also handle operations with varying numerical precision requirements. For example, training often requires 32-bit floating-point precision for numerical stability, while inference can often use reduced precision formats like 16-bit floating-point or even 8-bit integers. Frameworks must therefore provide efficient implementations across multiple numerical formats while maintaining acceptable accuracy.\n\n\nSystem-Level Operations\nSystem-level operations build upon the previously discussed computational graph abstractions, hardware abstractions, and numerical operations to manage overall computation flow and resource utilization. These operations handle three critical aspects: operation scheduling, memory management, and resource optimization.\nOperation scheduling leverages the computational graph structure discussed earlier to determine execution ordering. Building on the static or dynamic graph representation, the scheduler must identify parallelization opportunities while respecting dependencies. The implementation challenges differ between static graphs, where the entire dependency structure is known in advance, and dynamic graphs, where dependencies emerge during execution. The scheduler must also handle advanced execution patterns like conditional operations and loops that create dynamic control flow within the graph structure.\nMemory management implements sophisticated strategies for allocating and deallocating memory resources across the computational graph. Different data types require different management strategies. Model parameters typically persist throughout execution and may require specific memory types for efficient access. Intermediate results have bounded lifetimes defined by the operation graph. For example, activation values are needed only during the backward pass. The memory manager employs techniques like reference counting for automatic cleanup, memory pooling to reduce allocation overhead, and workspace management for temporary buffers. It must also handle memory fragmentation, particularly in long-running training sessions where allocation patterns can change over time.\nResource optimization integrates scheduling and memory decisions to maximize performance within system constraints. A key optimization is gradient checkpointing12, where some intermediate results are discarded and recomputed rather than stored, trading computation time for memory savings. The optimizer must also manage concurrent execution streams, balancing load across available compute units while respecting dependencies. For operations with multiple possible implementations, it selects between alternatives based on runtime conditions - for instance, choosing between matrix multiplication algorithms based on matrix shapes and system load.\n12 Gradient checkpointing: A memory-saving optimization technique that stores a limited set of intermediate activations during the forward pass and recomputes the others during the backward pass to reduce memory usage.Together, these operational layers build upon the computational graph foundation to execute machine learning workloads efficiently while abstracting implementation complexity from model developers. The interaction between these layers determines overall system performance and sets the foundation for advanced optimization techniques discussed in subsequent chapters.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>AI Frameworks</span>"
    ]
  },
  {
    "objectID": "contents/core/frameworks/frameworks.html#framework-components",
    "href": "contents/core/frameworks/frameworks.html#framework-components",
    "title": "7  AI Frameworks",
    "section": "7.4 Framework Components",
    "text": "7.4 Framework Components\nMachine learning frameworks organize their fundamental capabilities into distinct components that work together to provide a complete development and deployment environment. These components create layers of abstraction that make frameworks both usable for high-level model development and efficient for low-level execution. Understanding how these components interact helps developers choose and use frameworks effectively.\n\n7.4.1 APIs and Abstractions\nThe API layer of machine learning frameworks provides the primary interface through which developers interact with the framework’s capabilities. This layer must balance multiple competing demands: it must be intuitive enough for rapid development, flexible enough to support diverse use cases, and efficient enough to enable high-performance implementations.\nModern framework APIs typically implement multiple levels of abstraction. At the lowest level, they provide direct access to tensor operations and computational graph construction. These low-level APIs expose the fundamental operations discussed in the previous section, allowing fine-grained control over computation. For example, frameworks like PyTorch and TensorFlow offer such low-level interfaces, enabling researchers to define custom computations and explore novel algorithms (Ansel et al. 2024; Abadi et al. 2016).\n\nAnsel, Jason, Edward Yang, Horace He, Natalia Gimelshein, Animesh Jain, Michael Voznesensky, Bin Bao, et al. 2024. “PyTorch 2: Faster Machine Learning Through Dynamic Python Bytecode Transformation and Graph Compilation.” In Proceedings of the 29th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 2, edited by Hanna M. Wallach, Hugo Larochelle, Alina Beygelzimer, Florence d’Alché-Buc, Emily B. Fox, and Roman Garnett, 929–47. ACM. https://doi.org/10.1145/3620665.3640366.\n\nAbadi, Martín, Paul Barham, Jianmin Chen, Zhifeng Chen, Andy Davis, Jeffrey Dean, Matthieu Devin, et al. 2016. “TensorFlow: A System for Large-Scale Machine Learning.” In 12th USENIX Symposium on Operating Systems Design and Implementation (OSDI 16), 265–83. USENIX Association. https://www.usenix.org/conference/osdi16/technical-sessions/presentation/abadi.\n# Low-level API example\nimport torch\n\n# Manual tensor operations\nx = torch.randn(2, 3)\nw = torch.randn(3, 4)\nb = torch.randn(4)\ny = torch.matmul(x, w) + b\n\n# Manual gradient computation\ny.backward(torch.ones_like(y))\nBuilding on these primitives, frameworks implement higher-level APIs that package common patterns into reusable components. Neural network layers represent a classic example—while a convolution operation could be implemented manually using basic tensor operations, frameworks provide pre-built layer abstractions that handle the implementation details. This approach is exemplified by libraries such as PyTorch’s torch.nn and TensorFlow’s Keras API, which enable efficient and user-friendly model development (Chollet 2018).\n\nChollet, François. 2018. “Introduction to Keras.” March 9th.\n# Mid-level API example using nn modules\nimport torch.nn as nn\n\nclass SimpleNet(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = nn.Conv2d(3, 64, kernel_size=3)\n        self.fc = nn.Linear(64, 10)\n        \n    def forward(self, x):\n        x = self.conv(x)\n        x = torch.relu(x)\n        x = self.fc(x)\n        return x\nAt the highest level, frameworks often provide model-level abstractions that automate common workflows. For example, the Keras API provides a highly abstract interface that hides most implementation details:\n# High-level API example using Keras\nfrom tensorflow import keras\n\nmodel = keras.Sequential([\n    keras.layers.Conv2D(\n        64, \n        3, \n        activation='relu', \n        input_shape=(32, 32, 3)),\n    keras.layers.Flatten(),\n    keras.layers.Dense(10)\n])\n\n# Automated training workflow\nmodel.compile(\n    optimizer='adam', \n    loss='sparse_categorical_crossentropy')\nmodel.fit(train_data, train_labels, epochs=10)\nThe organization of these API layers reflects fundamental trade-offs in framework design. Lower-level APIs provide maximum flexibility but require more expertise to use effectively. Higher-level APIs improve developer productivity but may constrain implementation choices. Framework APIs must therefore provide clear paths between abstraction levels, allowing developers to mix different levels of abstraction as needed for their specific use cases.## Framework Components\nMachine learning frameworks organize their fundamental capabilities into distinct components that work together to provide a complete development and deployment environment. These components create layers of abstraction that make frameworks both usable for high-level model development and efficient for low-level execution. Understanding how these components interact helps developers choose and use frameworks effectively.\n\n\n7.4.2 Core Libraries\nAt the heart of every machine learning framework lies a set of core libraries, forming the foundation upon which all other components are built. These libraries provide the essential building blocks for machine learning operations, implementing fundamental tensor operations that serve as the backbone of numerical computations. Heavily optimized for performance, these operations often leverage low-level programming languages and hardware-specific optimizations to ensure efficient execution of tasks like matrix multiplication, a cornerstone of neural network computations.\nAlongside these basic operations, core libraries implement automatic differentiation capabilities, enabling the efficient computation of gradients for complex functions. This feature is crucial for the backpropagation algorithm that powers most neural network training. The implementation often involves intricate graph manipulation and symbolic computation techniques, abstracting away the complexities of gradient calculation from the end-user.\nBuilding upon these fundamental operations, core libraries typically provide pre-implemented neural network layers such as convolutional, recurrent, and attention mechanisms. These ready-to-use components save developers from reinventing the wheel for common model architectures, allowing them to focus on higher-level model design rather than low-level implementation details. Similarly, optimization algorithms like various flavors of gradient descent are provided out-of-the-box, further streamlining the model development process.\nHere is a simplified example of how these components might be used in practice:\nimport torch\nimport torch.nn as nn\n\n# Create a simple neural network\nmodel = nn.Sequential(\n    nn.Linear(10, 20),\n    nn.ReLU(),\n    nn.Linear(20, 1)\n)\n\n# Define loss function and optimizer\nloss_fn = nn.MSELoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n\n# Forward pass, compute loss, and backward pass\nx = torch.randn(32, 10)\ny = torch.randn(32, 1)\ny_pred = model(x)\nloss = loss_fn(y_pred, y)\nloss.backward()\noptimizer.step()\nThis example demonstrates how core libraries provide high-level abstractions for model creation, loss computation, and optimization, while handling low-level details internally.\n\n\n7.4.3 Extensions and Plugins\nWhile core libraries offer essential functionality, the true power of modern machine learning frameworks often lies in their extensibility. Extensions and plugins expand the capabilities of frameworks, allowing them to address specialized needs and leverage cutting-edge research. Domain-specific libraries, for instance, cater to particular areas like computer vision or natural language processing, providing pre-trained models, specialized data augmentation techniques, and task-specific layers.\nHardware acceleration plugins play an important role in performance optimization as it enables frameworks to take advantage of specialized hardware like GPUs or TPUs. These plugins dramatically speed up computations and allow seamless switching between different hardware backends, a key feature for scalability and flexibility in modern machine learning workflows.\nAs models and datasets grow in size and complexity, distributed computing extensions also become important. These tools enable training across multiple devices or machines, handling complex tasks like data parallelism, model parallelism, and synchronization between compute nodes. This capability is essential for researchers and companies tackling large-scale machine learning problems.\nComplementing these computational tools are visualization and experiment tracking extensions. Visualization tools provide invaluable insights into the training process and model behavior, displaying real-time metrics and even offering interactive debugging capabilities. Experiment tracking extensions help manage the complexity of machine learning research, allowing systematic logging and comparison of different model configurations and hyperparameters.\n\n\n7.4.4 Development Tools\nThe ecosystem of development tools surrounding a machine learning framework further enhances its effectiveness and adoption. Interactive development environments, such as Jupyter notebooks, have become nearly ubiquitous in machine learning workflows, allowing for rapid prototyping and seamless integration of code, documentation, and outputs. Many frameworks provide custom extensions for these environments to enhance the development experience.\nDebugging and profiling tools address the unique challenges presented by machine learning models. Specialized debuggers allow developers to inspect the internal state of models during training and inference, while profiling tools identify bottlenecks in model execution, guiding optimization efforts. These tools are essential for developing efficient and reliable machine learning systems.\nAs projects grow in complexity, version control integration becomes increasingly important. Tools that allow versioning of not just code, but also model weights, hyperparameters, and training data, help manage the iterative nature of model development. This comprehensive versioning approach ensures reproducibility and facilitates collaboration in large-scale machine learning projects.\nFinally, deployment utilities bridge the gap between development and production environments. These tools handle tasks like model compression, conversion to deployment-friendly formats, and integration with serving infrastructure, streamlining the process of moving models from experimental settings to real-world applications.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>AI Frameworks</span>"
    ]
  },
  {
    "objectID": "contents/core/frameworks/frameworks.html#system-integration",
    "href": "contents/core/frameworks/frameworks.html#system-integration",
    "title": "7  AI Frameworks",
    "section": "7.5 System Integration",
    "text": "7.5 System Integration\nSystem integration is about implementing machine learning frameworks in real-world environments. This section explores how ML frameworks integrate with broader software and hardware ecosystems, addressing the challenges and considerations at each level of the integration process.\n\n7.5.1 Hardware Integration\nEffective hardware integration is crucial for optimizing the performance of machine learning models. Modern ML frameworks must adapt to a diverse range of computing environments, from high-performance GPU clusters to resource-constrained edge devices.\nFor GPU acceleration, frameworks like TensorFlow and PyTorch provide robust support, allowing seamless utilization of NVIDIA’s CUDA platform. This integration enables significant speedups in both training and inference tasks. Similarly, support for Google’s TPUs in TensorFlow allows for even further acceleration of specific workloads.\nIn distributed computing scenarios, frameworks must efficiently manage multi-device and multi-node setups. This involves strategies for data parallelism, where the same model is replicated across devices, and model parallelism, where different parts of the model are distributed across hardware units. Frameworks like Horovod have emerged to simplify distributed training across different backend frameworks.\nFor edge deployment, frameworks are increasingly offering lightweight versions optimized for mobile and IoT devices. TensorFlow Lite and PyTorch Mobile, for instance, provide tools for model compression and optimization, ensuring efficient execution on devices with limited computational resources and power constraints.\n\n\n7.5.2 Software Stack\nIntegrating ML frameworks into existing software stacks presents unique challenges and opportunities. A key consideration is how the ML system interfaces with data processing pipelines. Frameworks often provide connectors to popular big data tools like Apache Spark or Apache Beam, allowing seamless data flow between data processing systems and ML training environments.\nContainerization technologies like Docker have become essential in ML workflows, ensuring consistency between development and production environments. Kubernetes has emerged as a popular choice for orchestrating containerized ML workloads, providing scalability and manageability for complex deployments.\nML frameworks must also interface with other enterprise systems such as databases, message queues, and web services. For instance, TensorFlow Serving provides a flexible, high-performance serving system for machine learning models, which can be easily integrated into existing microservices architectures.\n\n\n7.5.3 Deployment Considerations\nDeploying ML models to production environments involves several critical considerations. Model serving strategies must balance performance, scalability, and resource efficiency. Approaches range from batch prediction for large-scale offline processing to real-time serving for interactive applications.\nScaling ML systems to meet production demands often involves techniques like horizontal scaling of inference servers, caching of frequent predictions, and load balancing across multiple model versions. Frameworks like TensorFlow Serving and TorchServe provide built-in solutions for many of these scaling challenges.\nMonitoring and logging are crucial for maintaining ML systems in production. This includes tracking model performance metrics, detecting concept drift, and logging prediction inputs and outputs for auditing purposes. Tools like Prometheus and Grafana are often integrated with ML serving systems to provide comprehensive monitoring solutions.\n\n\n7.5.4 Workflow Orchestration\nManaging end-to-end ML pipelines requires orchestrating multiple stages, from data preparation and model training to deployment and monitoring. MLOps practices have emerged to address these challenges, bringing DevOps principles to machine learning workflows.\nContinuous Integration and Continuous Deployment (CI/CD) practices are being adapted for ML workflows. This involves automating model testing, validation, and deployment processes. Tools like Jenkins or GitLab CI can be extended with ML-specific stages to create robust CI/CD pipelines for machine learning projects.\nAutomated model retraining and updating is another critical aspect of ML workflow orchestration. This involves setting up systems to automatically retrain models on new data, evaluate their performance, and seamlessly update production models when certain criteria are met. Frameworks like Kubeflow provide end-to-end ML pipelines that can automate many of these processes.\nVersion control for ML assets, including data, model architectures, and hyperparameters, is essential for reproducibility and collaboration. Tools like DVC (Data Version Control) and MLflow have emerged to address these ML-specific version control needs.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>AI Frameworks</span>"
    ]
  },
  {
    "objectID": "contents/core/frameworks/frameworks.html#major-frameworks",
    "href": "contents/core/frameworks/frameworks.html#major-frameworks",
    "title": "7  AI Frameworks",
    "section": "7.6 Major Frameworks",
    "text": "7.6 Major Frameworks\nAs we have seen earlier, machine learning frameworks are complicated. Over the years, several machine learning frameworks have emerged, each with its unique strengths and ecosystem, but few have remained as industry standards. Here we examine the mature and major players in the field, starting with a comprehensive look at TensorFlow, followed by PyTorch, JAX, and other notable frameworks.\n\n7.6.1 TF Ecosystem\nTensorFlow was developed by the Google Brain team and was released as an open-source software library on November 9, 2015. It was designed for numerical computation using data flow graphs and has since become popular for a wide range of machine learning applications.\nTensorFlow is a training and inference framework that provides built-in functionality to handle everything from model creation and training to deployment, as shown in Figure 7.10. Since its initial development, the TensorFlow ecosystem has grown to include many different “varieties” of TensorFlow, each intended to allow users to support ML on different platforms.\n\nTensorFlow Core: primary package that most developers engage with. It provides a comprehensive, flexible platform for defining, training, and deploying machine learning models. It includes tf.keras as its high-level API.\nTensorFlow Lite: designed for deploying lightweight models on mobile, embedded, and edge devices. It offers tools to convert TensorFlow models to a more compact format suitable for limited-resource devices and provides optimized pre-trained models for mobile.\nTensorFlow Lite Micro: designed for running machine learning models on microcontrollers with minimal resources. It operates without the need for operating system support, standard C or C++ libraries, or dynamic memory allocation, using only a few kilobytes of memory.\nTensorFlow.js: JavaScript library that allows training and deployment of machine learning models directly in the browser or on Node.js. It also provides tools for porting pre-trained TensorFlow models to the browser-friendly format.\nTensorFlow on Edge Devices (Coral): platform of hardware components and software tools from Google that allows the execution of TensorFlow models on edge devices, leveraging Edge TPUs for acceleration.\nTensorFlow Federated (TFF): framework for machine learning and other computations on decentralized data. TFF facilitates federated learning, allowing model training across many devices without centralizing the data.\nTensorFlow Graphics: library for using TensorFlow to carry out graphics-related tasks, including 3D shapes and point clouds processing, using deep learning.\nTensorFlow Hub: repository of reusable machine learning model components to allow developers to reuse pre-trained model components, facilitating transfer learning and model composition.\nTensorFlow Serving: framework designed for serving and deploying machine learning models for inference in production environments. It provides tools for versioning and dynamically updating deployed models without service interruption.\nTensorFlow Extended (TFX): end-to-end platform designed to deploy and manage machine learning pipelines in production settings. TFX encompasses data validation, preprocessing, model training, validation, and serving components.\n\n\n\n\n\n\n\nFigure 7.10: Architecture overview of TensorFlow 2.0. Source: Tensorflow.\n\n\n\n\n\n7.6.2 PyTorch\nPyTorch, developed by Facebook’s AI Research lab, has gained significant traction in the machine learning community, particularly among researchers and academics. Its design philosophy emphasizes ease of use, flexibility, and dynamic computation, which aligns well with the iterative nature of research and experimentation.\nPyTorch’s architecture lies its dynamic computational graph system. Unlike the static graphs used in earlier versions of TensorFlow, PyTorch builds the computational graph on-the-fly during execution. This approach, often referred to as “define-by-run,” allows for more intuitive model design and easier debugging that we discussed earlier. Moreover, developers can use standard Python control flow statements within their models, and the graph structure can change from iteration to iteration. This flexibility is particularly advantageous when working with variable-length inputs or complex, dynamic neural network architectures.\nPyTorch’s eager execution mode is tightly coupled with its dynamic graph approach. Operations are executed immediately as they are called, rather than being deferred for later execution in a static graph. This immediate execution facilitates easier debugging and allows for more natural integration with Python’s native debugging tools. The eager execution model aligns closely with PyTorch’s imperative programming style, which many developers find more intuitive and Pythonic.\nPyTorch’s fundamental data structure is the tensor, similar to TensorFlow and other frameworks discussed in earlier sections. PyTorch tensors are conceptually equivalent to multi-dimensional arrays and can be manipulated using a rich set of operations. The framework provides seamless integration with CUDA, much like TensorFlow, enabling efficient GPU acceleration for tensor computations. PyTorch’s autograd system automatically tracks all operations performed on tensors, facilitating automatic differentiation for gradient-based optimization algorithms.\n\n\n7.6.3 JAX\nJAX, developed by Google Research, is a newer entrant in the field of machine learning frameworks. Unlike TensorFlow and PyTorch, which were primarily designed for deep learning, JAX focuses on high-performance numerical computing and advanced machine learning research. Its design philosophy centers around functional programming principles and composition of transformations, offering a fresh perspective on building and optimizing machine learning systems.\nJAX is built as a NumPy-like library with added capabilities for automatic differentiation and just-in-time (JIT) compilation. This foundation makes JAX feel familiar to researchers accustomed to scientific computing in Python, while providing powerful tools for optimization and acceleration. Where TensorFlow uses static computational graphs and PyTorch employs dynamic ones, JAX takes a different approach altogether—a system for transforming numerical functions.\nOne of JAX’s key features is its powerful automatic differentiation system. Unlike TensorFlow’s static graph approach or PyTorch’s dynamic computation, JAX can differentiate native Python and NumPy functions, including those with loops, branches, and recursion. This capability extends beyond simple scalar-to-scalar functions, allowing for complex transformations like vectorization and JIT compilation. This flexibility is particularly valuable for researchers exploring novel machine learning techniques and architectures.\nJAX leverages XLA (Accelerated Linear Algebra) for just-in-time compilation, similar to TensorFlow but with a more central role in its operation. This allows JAX to optimize and compile Python code for various hardware accelerators, including GPUs and TPUs. In contrast to PyTorch’s eager execution and TensorFlow’s graph optimization, JAX’s approach can lead to significant performance improvements, especially for complex computational patterns.\nWhere TensorFlow and PyTorch primarily use object-oriented and imperative programming models, JAX embraces functional programming. This approach encourages the use of pure functions and immutable data, which can lead to more predictable and easier-to-optimize code. It’s a significant departure from the stateful models common in other frameworks and can require a shift in thinking for developers accustomed to TensorFlow or PyTorch.\nJAX introduces a set of composable function transformations that set it apart from both TensorFlow and PyTorch. These include automatic differentiation (grad), just-in-time compilation (jit), automatic vectorization (vmap), and parallel execution across multiple devices (pmap). These transformations can be composed, allowing for powerful and flexible operations that are not as straightforward in other frameworks.\n\n\n7.6.4 Comparison\nTable 7.3 provides a concise comparison of three major machine learning frameworks: TensorFlow, PyTorch, and JAX. These frameworks, while serving similar purposes, exhibit fundamental differences in their design philosophies and technical implementations.\n\n\n\nTable 7.3: Core characteristics of major machine learning frameworks.\n\n\n\n\n\n\n\n\n\n\n\nAspect\nTensorFlow\nPyTorch\nJAX\n\n\n\n\nGraph Type\nStatic (1.x), Dynamic (2.x)\nDynamic\nFunctional transformations\n\n\nProgramming Model\nImperative (2.x), Symbolic (1.x)\nImperative\nFunctional\n\n\nCore Data Structure\nTensor (mutable)\nTensor (mutable)\nArray (immutable)\n\n\nExecution Mode\nEager (2.x default), Graph\nEager\nJust-in-time compilation\n\n\nAutomatic Differentiation\nReverse mode\nReverse mode\nForward and Reverse mode\n\n\nHardware Acceleration\nCPU, GPU, TPU\nCPU, GPU\nCPU, GPU, TPU",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>AI Frameworks</span>"
    ]
  },
  {
    "objectID": "contents/core/frameworks/frameworks.html#framework-specialization",
    "href": "contents/core/frameworks/frameworks.html#framework-specialization",
    "title": "7  AI Frameworks",
    "section": "7.7 Framework Specialization",
    "text": "7.7 Framework Specialization\nMachine learning frameworks have evolved significantly to meet the diverse needs of different computational environments. As ML applications expand beyond traditional data centers to encompass edge devices, mobile platforms, and even tiny microcontrollers, the need for specialized frameworks has become increasingly apparent.\nFramework specialization refers to the process of tailoring ML frameworks to optimize performance, efficiency, and functionality for specific deployment environments. This specialization is crucial because the computational resources, power constraints, and use cases vary dramatically across different platforms.\nThe Open Neural Network Exchange (ONNX) format plays a vital role in framework interoperability across these specialized environments. ONNX provides a standardized representation for ML models, allowing them to move between different frameworks and deployment targets. This standardization helps bridge the gap between framework specializations, enabling models trained in one environment to be optimized and deployed in another.\nMachine learning deployment environments shape how frameworks specialize and evolve. Cloud ML environments leverage high-performance servers that offer abundant computational resources for complex operations. Edge ML operates on devices with moderate computing power, where real-time processing often takes priority. Mobile ML adapts to the varying capabilities and energy constraints of smartphones and tablets. Tiny ML functions within the strict limitations of microcontrollers and other highly constrained devices that possess minimal resources.\nEach of these environments presents unique challenges that influence framework design. Cloud frameworks prioritize scalability and distributed computing. Edge frameworks focus on low-latency inference and adaptability to diverse hardware. Mobile frameworks emphasize energy efficiency and integration with device-specific features. TinyML frameworks specialize in extreme resource optimization for severely constrained environments.\nIn the following sections, we will explore how ML frameworks adapt to each of these environments. We will examine the specific techniques and design choices that enable frameworks to address the unique challenges of each domain, highlighting the trade-offs and optimizations that characterize framework specialization.\n\n7.7.1 Cloud ML Frameworks\nCloud ML frameworks are sophisticated software infrastructures designed to leverage the vast computational resources available in cloud environments. These frameworks specialize in three primary areas: distributed computing architectures, management of large-scale data and models, and integration with cloud-native services.\nDistributed computing is a fundamental specialization of cloud ML frameworks. These frameworks implement advanced strategies for partitioning and coordinating computational tasks across multiple machines or graphics processing units (GPUs). This capability is essential for training large-scale models on massive datasets. Both TensorFlow and PyTorch, two leading cloud ML frameworks, offer robust support for distributed computing. TensorFlow’s graph-based approach (in its 1.x version) was particularly well-suited for distributed execution, while PyTorch’s dynamic computational graph allows for more flexible distributed training strategies.\nThe ability to handle large-scale data and models is another key specialization. Cloud ML frameworks are optimized to work with datasets and models that far exceed the capacity of single machines. This specialization is reflected in the data structures of these frameworks. For instance, both TensorFlow and PyTorch use mutable Tensor objects as their primary data structure, allowing for efficient in-place operations on large datasets. JAX, a more recent framework, uses immutable arrays, which can provide benefits in terms of functional programming paradigms and optimization opportunities in distributed settings.\nIntegration with cloud-native services is the third major specialization area. This integration enables automated resource scaling, seamless access to cloud storage, and incorporation of cloud-based monitoring and logging systems. The execution modes of different frameworks play a role here. TensorFlow 2.x and PyTorch both default to eager execution, which allows for easier integration with cloud services and debugging. JAX’s just-in-time compilation offers potential performance benefits in cloud environments by optimizing computations for specific hardware.\nHardware acceleration is an important aspect of cloud ML frameworks. All major frameworks support CPU and GPU execution, with TensorFlow and JAX also offering native support for Google’s TPU. NVIDIA’s TensorRT is an optimization tool dedicated for GPU-based inference, providing sophisticated optimizations like layer fusion, precision calibration13, and kernel auto-tuning to maximize throughput on NVIDIA GPUs. These hardware acceleration options allow cloud ML frameworks to efficiently utilize the diverse computational resources available in cloud environments.\n13 A process of adjusting computations to use reduced numerical precision, balancing performance improvements with acceptable losses in accuracy.The automatic differentiation capabilities of these frameworks are particularly important in cloud settings where complex models with millions of parameters are common. While TensorFlow and PyTorch primarily use reverse-mode differentiation, JAX’s support for both forward and reverse-mode differentiation can offer advantages in certain large-scale optimization scenarios.\nThese specializations enable cloud ML frameworks to fully utilize the scalability and computational power of cloud infrastructure. However, this capability comes with increased complexity in deployment and management, often requiring specialized knowledge to fully leverage these frameworks. The focus on scalability and integration makes cloud ML frameworks particularly suitable for large-scale research projects, enterprise-level ML applications, and scenarios requiring massive computational resources.\n\n\n7.7.2 Edge ML Frameworks\nEdge ML frameworks are specialized software tools designed to facilitate machine learning operations in edge computing environments, characterized by proximity to data sources, stringent latency requirements, and limited computational resources. Examples of popular edge ML frameworks include TensorFlow Lite and Edge Impulse. The specialization of these frameworks addresses three primary challenges: real-time inference optimization, adaptation to heterogeneous hardware, and resource-constrained operation.\nReal-time inference optimization is a critical feature of edge ML frameworks. This often involves leveraging different execution modes and graph types. For instance, while TensorFlow Lite (the edge-focused version of TensorFlow) uses a static graph approach to optimize inference, frameworks like PyTorch Mobile maintain a dynamic graph capability, allowing for more flexible model structures at the cost of some performance. The choice between static and dynamic graphs in edge frameworks often is a trade-off between optimization potential and model flexibility.\nAdaptation to heterogeneous hardware is crucial for edge deployments. Edge ML frameworks extend the hardware acceleration capabilities of their cloud counterparts but with a focus on edge-specific hardware. For instance, TensorFlow Lite supports acceleration on mobile GPUs and edge TPUs, while frameworks like ARM’s Compute Library optimize for ARM-based processors. This specialization often involves custom operator implementations and low-level optimizations specific to edge hardware.\nOperating within resource constraints is another aspect of edge ML framework specialization. This is reflected in the data structures and execution models of these frameworks. For instance, many edge frameworks use quantized tensors as their primary data structure, representing values with reduced precision (e.g., 8-bit integers instead of 32-bit floats) to decrease memory usage and computational demands. The automatic differentiation capabilities, while crucial for training in cloud environments, are often stripped down or removed entirely in edge frameworks to reduce model size and improve inference speed.\nEdge ML frameworks also often include features for model versioning and updates, allowing for the deployment of new models with minimal system downtime. Some frameworks support limited on-device learning, enabling models to adapt to local data without compromising data privacy.\nThe specializations of edge ML frameworks collectively enable high-performance inference in resource-constrained environments. This capability expands the potential applications of AI in areas with limited cloud connectivity or where real-time processing is crucial. However, effective utilization of these frameworks requires careful consideration of target hardware specifications and application-specific requirements, necessitating a balance between model accuracy and resource utilization.\n\n\n7.7.3 Mobile ML Frameworks\nMobile ML frameworks are specialized software tools designed for deploying and executing machine learning models on smartphones and tablets. Examples include TensorFlow Lite and Apple’s Core ML. These frameworks address the unique challenges of mobile environments, including limited computational resources, constrained power consumption, and diverse hardware configurations. The specialization of mobile ML frameworks primarily focuses on on-device inference optimization, energy efficiency, and integration with mobile-specific hardware and sensors.\nOn-device inference optimization in mobile ML frameworks often involves a careful balance between graph types and execution modes. For instance, TensorFlow Lite, also a popular mobile ML framework, uses a static graph approach to optimize inference performance. This contrasts with the dynamic graph capability of PyTorch Mobile, which offers more flexibility at the cost of some performance. The choice between static and dynamic graphs in mobile frameworks is a trade-off between optimization potential and model adaptability, crucial in the diverse and changing mobile environment.\nThe data structures in mobile ML frameworks are optimized for efficient memory usage and computation. While cloud-based frameworks like TensorFlow and PyTorch use mutable tensors, mobile frameworks often employ more specialized data structures. For example, many mobile frameworks use quantized tensors, representing values with reduced precision (e.g., 8-bit integers instead of 32-bit floats) to decrease memory footprint and computational demands. This specialization is critical given the limited RAM and processing power of mobile devices.\nEnergy efficiency, a paramount concern in mobile environments, influences the design of execution modes in mobile ML frameworks. Unlike cloud frameworks that may use eager execution for ease of development, mobile frameworks often prioritize graph-based execution for its potential energy savings. For instance, Apple’s Core ML uses a compiled model approach, converting ML models into a form that can be efficiently executed by iOS devices, optimizing for both performance and energy consumption.\nIntegration with mobile-specific hardware and sensors is another key specialization area. Mobile ML frameworks extend the hardware acceleration capabilities of their cloud counterparts but with a focus on mobile-specific processors. For example, TensorFlow Lite can leverage mobile GPUs and neural processing units (NPUs) found in many modern smartphones. Qualcomm’s Neural Processing SDK is designed to efficiently utilize the AI accelerators present in Snapdragon SoCs. This hardware-specific optimization often involves custom operator implementations and low-level optimizations tailored for mobile processors.\nAutomatic differentiation, while crucial for training in cloud environments, is often minimized or removed entirely in mobile frameworks to reduce model size and improve inference speed. Instead, mobile ML frameworks focus on efficient inference, with model updates typically performed off-device and then deployed to the mobile application.\nMobile ML frameworks also often include features for model updating and versioning, allowing for the deployment of improved models without requiring full app updates. Some frameworks support limited on-device learning, enabling models to adapt to user behavior or environmental changes without compromising data privacy.\nThe specializations of mobile ML frameworks collectively enable the deployment of sophisticated ML models on resource-constrained mobile devices. This expands the potential applications of AI in mobile environments, ranging from real-time image and speech recognition to personalized user experiences. However, effectively utilizing these frameworks requires careful consideration of the target device capabilities, user experience requirements, and privacy implications, necessitating a balance between model performance and resource utilization.\n\n\n7.7.4 TinyML Frameworks\nTinyML frameworks are specialized software infrastructures designed for deploying machine learning models on extremely resource-constrained devices, typically microcontrollers and low-power embedded systems. These frameworks address the severe limitations in processing power, memory, and energy consumption characteristic of tiny devices. The specialization of TinyML frameworks primarily focuses on extreme model compression, optimizations for severely constrained environments, and integration with microcontroller-specific architectures.\nExtreme model compression in TinyML frameworks takes the quantization techniques mentioned in mobile and edge frameworks to their logical conclusion. While mobile frameworks might use 8-bit quantization, TinyML often employs even more aggressive techniques, such as 4-bit, 2-bit, or even 1-bit (binary) representations of model parameters. Frameworks like TensorFlow Lite Micro exemplify this approach (David et al. 2021), pushing the boundaries of model compression to fit within the kilobytes of memory available on microcontrollers.\n\nDavid, Robert, Jared Duke, Advait Jain, Vijay Janapa Reddi, Nat Jeffries, Jian Li, Nick Kreeger, et al. 2021. “Tensorflow Lite Micro: Embedded Machine Learning for Tinyml Systems.” Proceedings of Machine Learning and Systems 3: 800–811.\nThe execution model in TinyML frameworks is highly specialized. Unlike the dynamic graph capabilities seen in some cloud and mobile frameworks, TinyML frameworks almost exclusively use static, highly optimized graphs. The just-in-time compilation approach seen in frameworks like JAX is typically not feasible in TinyML due to memory constraints. Instead, these frameworks often employ ahead-of-time compilation techniques to generate highly optimized, device-specific code.\nMemory management in TinyML frameworks is far more constrained than in other environments. While edge and mobile frameworks might use dynamic memory allocation, TinyML frameworks like uTensor often rely on static memory allocation to avoid runtime overhead and fragmentation. This approach requires careful planning of the memory layout at compile time, a stark contrast to the more flexible memory management in cloud-based frameworks.\nHardware integration in TinyML frameworks is highly specific to microcontroller architectures. Unlike the general GPU support seen in cloud frameworks or the mobile GPU/NPU support in mobile frameworks, TinyML frameworks often provide optimizations for specific microcontroller instruction sets. For example, ARM’s CMSIS-NN (Lai, Suda, and Chandra 2018) provides optimized neural network kernels for Cortex-M series microcontrollers, which are often integrated into TinyML frameworks.\n\nLai, Liangzhen, Naveen Suda, and Vikas Chandra. 2018. “CMSIS-NN: Efficient Neural Network Kernels for Arm Cortex-m CPUs.” ArXiv Preprint abs/1801.06601 (January). http://arxiv.org/abs/1801.06601v1.\nThe concept of automatic differentiation, central to cloud-based frameworks and present to some degree in edge and mobile frameworks, is typically absent in TinyML frameworks. The focus is almost entirely on inference, with any learning or model updates usually performed off-device due to the severe computational constraints.\nTinyML frameworks also specialize in power management to a degree not seen in other ML environments. Features like duty cycling and ultra-low-power wake-up capabilities are often integrated directly into the ML pipeline, enabling always-on sensing applications that can run for years on small batteries.\nThe extreme specialization of TinyML frameworks enables ML deployments in previously infeasible environments, from smart dust sensors to implantable medical devices. However, this specialization comes with significant trade-offs in model complexity and accuracy, requiring careful consideration of the balance between ML capabilities and the severe resource constraints of target devices.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>AI Frameworks</span>"
    ]
  },
  {
    "objectID": "contents/core/frameworks/frameworks.html#choosing-a-framework",
    "href": "contents/core/frameworks/frameworks.html#choosing-a-framework",
    "title": "7  AI Frameworks",
    "section": "7.8 Choosing a Framework",
    "text": "7.8 Choosing a Framework\nFramework selection builds on our understanding of framework specialization across computing environments. Engineers must evaluate three interdependent factors when choosing a framework: model requirements, hardware constraints, and software dependencies. The TensorFlow ecosystem demonstrates how these factors shape framework design through its variants: TensorFlow, TensorFlow Lite, and TensorFlow Lite Micro.\nTable 7.4 illustrates key differences between TensorFlow variants. Each variant represents specific trade-offs between computational capability and resource requirements. These trade-offs manifest in supported operations, binary size, and integration requirements.\n\n\n\nTable 7.4: TensorFlow framework comparison - General.\n\n\n\n\n\n\n\n\n\n\n\nModel\n\n\n\n\n\n\n\nTraining\nYes\nNo\nNo\n\n\nInference\nYes\nYes\nYes\n\n\n\n(but inefficient on edge)\n(and efficient)\n(and even more efficient)\n\n\nHow Many Ops\n~1400\n~130\n~50\n\n\nNative Quantization Tooling + Support\nNo\nYes\nYes\n\n\n\n\n\n\nEngineers analyze three primary aspects when selecting a framework:\n\nModel requirements determine which operations and architectures the framework must support\nSoftware dependencies define operating system and runtime requirements\nHardware constraints establish memory and processing limitations\n\nThis systematic analysis enables engineers to select frameworks that align with their deployment requirements. As we examine the TensorFlow variants, we will explore how each aspect influences framework selection and shapes the capabilities of deployed machine learning systems.\n\n7.8.1 Model Requirements\nModel architecture capabilities vary significantly across TensorFlow variants, with clear trade-offs between functionality and efficiency. Table 7.4 quantifies these differences across four key dimensions: training capability, inference efficiency, operation support, and quantization features.\nTensorFlow supports approximately 1,400 operations and enables both training and inference. However, as Table 7.4 indicates, its inference capabilities are inefficient for edge deployment. TensorFlow Lite reduces the operation count to roughly 130 operations while improving inference efficiency. It eliminates training support but adds native quantization tooling. TensorFlow Lite Micro further constrains the operation set to approximately 50 operations, achieving even higher inference efficiency through these constraints. Like TensorFlow Lite, it includes native quantization support but removes training capabilities.\nThis progressive reduction in operations enables deployment on increasingly constrained devices. The addition of native quantization in both TensorFlow Lite and TensorFlow Lite Micro provides essential optimization capabilities absent in the full TensorFlow framework. Quantization transforms models to use lower precision operations, reducing computational and memory requirements for resource-constrained deployments.\n\n\n7.8.2 Software Dependencies\nTable 7.5 reveals three key software considerations that differentiate TensorFlow variants: operating system requirements, memory management capabilities, and accelerator support. These differences reflect each variant’s optimization for specific deployment environments.\n\n\n\nTable 7.5: TensorFlow framework comparison - Software.\n\n\n\n\n\n\n\n\n\n\n\nSoftware\n\n\n\n\n\n\n\nNeeds an OS\nYes\nYes\nNo\n\n\nMemory Mapping of Models\nNo\nYes\nYes\n\n\nDelegation to accelerators\nYes\nYes\nNo\n\n\n\n\n\n\nOperating system dependencies mark a fundamental distinction between variants. TensorFlow and TensorFlow Lite require an operating system, while TensorFlow Lite Micro operates without OS support. This enables TensorFlow Lite Micro to reduce memory overhead and startup time, though it can still integrate with real-time operating systems like FreeRTOS, Zephyr, and Mbed OS when needed.\nMemory management capabilities also distinguish the variants. TensorFlow Lite and TensorFlow Lite Micro support model memory mapping, enabling direct model access from flash storage rather than loading into RAM. TensorFlow lacks this capability, reflecting its design for environments with abundant memory resources. Memory mapping becomes increasingly important as deployment moves toward resource-constrained devices.\nAccelerator delegation capabilities further differentiate the variants. Both TensorFlow and TensorFlow Lite support delegation to accelerators, enabling efficient computation distribution. TensorFlow Lite Micro omits this feature, acknowledging the limited availability of specialized accelerators in embedded systems. This design choice maintains the framework’s minimal footprint while matching typical embedded hardware configurations.\n\n\n7.8.3 Hardware Constraints\nTable 7.6 quantifies the hardware requirements across TensorFlow variants through three metrics: base binary size, memory footprint, and processor architecture support. These metrics demonstrate the progressive optimization for constrained computing environments.\n\n\n\nTable 7.6: TensorFlow framework comparison – Hardware.\n\n\n\n\n\n\n\n\n\n\n\nHardware\n\n\n\n\n\n\n\nBase Binary Size\n3 MB+\n100 KB\n~10 KB\n\n\nBase Memory Footprint\n~5 MB\n300 KB\n20 KB\n\n\nOptimized Architectures\nX86, TPUs, GPUs\nArm Cortex A, x86\nArm Cortex M, DSPs, MCUs\n\n\n\n\n\n\nBinary size requirements decrease significantly across variants. TensorFlow requires over 3 MB for its base binary, reflecting its comprehensive feature set. TensorFlow Lite reduces this to 100 KB by eliminating training capabilities and unused operations. TensorFlow Lite Micro achieves a remarkable 10 KB binary size through aggressive optimization and feature reduction.\nMemory footprint follows a similar pattern of reduction. TensorFlow requires approximately 5 MB of base memory, while TensorFlow Lite operates within 300 KB. TensorFlow Lite Micro further reduces memory requirements to 20 KB, enabling deployment on highly constrained devices.\nProcessor architecture support aligns with each variant’s intended deployment environment. TensorFlow supports x86 processors and accelerators including TPUs and GPUs, enabling high-performance computing in data centers. TensorFlow Lite targets mobile and edge processors, supporting Arm Cortex-A and x86 architectures. TensorFlow Lite Micro specializes in microcontroller deployment, supporting Arm Cortex-M cores, digital signal processors (DSPs), and various microcontroller units (MCUs) including STM32, NXP Kinetis, and Microchip AVR.\n\n\n7.8.4 Additional Selection Factors\nFramework selection for embedded systems extends beyond technical specifications of model architecture, hardware requirements, and software dependencies. Additional factors affect development efficiency, maintenance requirements, and deployment success. These factors require systematic evaluation to ensure optimal framework selection.\n\nPerformance Optimization\nPerformance in embedded systems encompasses multiple metrics beyond computational speed. Framework evaluation must consider:\nInference latency determines system responsiveness and real-time processing capabilities. Memory utilization affects both static storage requirements and runtime efficiency. Power consumption impacts battery life and thermal management requirements. Frameworks must provide optimization tools for these metrics, including quantization, operator fusion, and hardware-specific acceleration.\n\n\nDeployment Scalability\nScalability requirements span both technical capabilities and operational considerations. Framework support must extend across deployment scales and scenarios:\nDevice scaling enables consistent deployment from microcontrollers to more powerful embedded processors. Operational scaling supports the transition from development prototypes to production deployments. Version management facilitates model updates and maintenance across deployed devices. The framework must maintain consistent performance characteristics throughout these scaling dimensions.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>AI Frameworks</span>"
    ]
  },
  {
    "objectID": "contents/core/frameworks/frameworks.html#conclusion",
    "href": "contents/core/frameworks/frameworks.html#conclusion",
    "title": "7  AI Frameworks",
    "section": "7.9 Conclusion",
    "text": "7.9 Conclusion\nAI frameworks have evolved from basic numerical libraries into sophisticated software systems that shape how we develop and deploy machine learning applications. The progression from early numerical computing to modern deep learning frameworks demonstrates the field’s rapid technological advancement.\nModern frameworks like TensorFlow, PyTorch, and JAX implement distinct approaches to common challenges in machine learning development. Each framework offers varying tradeoffs between ease of use, performance, and flexibility. TensorFlow emphasizes production deployment, PyTorch focuses on research and experimentation, while JAX prioritizes functional programming patterns.\nThe specialization of frameworks into cloud, edge, mobile, and tiny ML implementations reflects the diverse requirements of machine learning applications. Cloud frameworks optimize for scalability and distributed computing. Edge and mobile frameworks prioritize model efficiency and reduced resource consumption. TinyML frameworks target constrained environments with minimal computing resources.\nUnderstanding framework architecture, from tensor operations to execution models, enables developers to select appropriate tools for specific use cases, optimize application performance, debug complex computational graphs, and deploy models across different computing environments.\nThe continuing evolution of AI frameworks will likely focus on improving developer productivity, hardware acceleration, and deployment flexibility. These advancements will shape how machine learning systems are built and deployed across increasingly diverse computing environments.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>AI Frameworks</span>"
    ]
  },
  {
    "objectID": "contents/core/training/training.html",
    "href": "contents/core/training/training.html",
    "title": "8  AI Training",
    "section": "",
    "text": "Purpose\nResources: Slides, Videos, Exercises\nHow do machine learning training workloads manifest as systems challenges, and what architectural principles guide their efficient implementation?\nMachine learning training is a unique class of computational workload that demands careful orchestration of computation, memory, and data movement. The process of transforming training algorithms into efficient system implementations requires understanding how mathematical operations map to hardware resources, how data flows through memory hierarchies, and how system architectures influence training performance. Investigating these system-level considerations helps establish core principles for designing and optimizing training infrastructure. By understanding and addressing these challenges, we can develop more efficient and scalable solutions to meet the demands of modern machine learning workloads.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>AI Training</span>"
    ]
  },
  {
    "objectID": "contents/core/training/training.html#purpose",
    "href": "contents/core/training/training.html#purpose",
    "title": "8  AI Training",
    "section": "",
    "text": "Learning Objectives\n\n\n\n\nExplain the link between mathematical operations and system trade-offs in AI training.\nIdentify bottlenecks in training systems and their impact on performance.\nOutline the key components of training pipelines and their roles in model training.\nDetermine appropriate optimization techniques to improve training efficiency.\nAnalyze training systems beyond a single machine, including distributed approaches.\nEvaluate and design training processes with a focus on efficiency and scalability.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>AI Training</span>"
    ]
  },
  {
    "objectID": "contents/core/training/training.html#overview",
    "href": "contents/core/training/training.html#overview",
    "title": "8  AI Training",
    "section": "8.1 Overview",
    "text": "8.1 Overview\nMachine learning has revolutionized modern computing by enabling systems to learn patterns from data, with training being its cornerstone. This computationally intensive process involves adjusting millions—or even billions—of parameters to minimize errors on training examples while ensuring the model generalizes effectively to unseen data. The success of machine learning models hinges on this training phase.\nThe training process brings together algorithms, data, and computational resources into an integrated workflow. Models, particularly deep neural networks used in domains such as computer vision and natural language processing, require significant computational effort due to their complexity and scale. Even resource-constrained models, such as those used in Mobile ML or Tiny ML applications, require careful tuning to achieve an optimal balance between accuracy, computational efficiency, and generalization.\nAs models have grown in size and complexity1, the systems that enable efficient training have become increasingly sophisticated. Training systems must coordinate computation across memory hierarchies, manage data movement, and optimize resource utilization—all while maintaining numerical stability and convergence properties. This intersection of mathematical optimization with systems engineering creates unique challenges in maximizing training throughput.\n1 Model sizes have grown exponentially since AlexNet (60M parameters) in 2012, with modern large language models like GPT-4 estimated to have over 1 trillion parameters—an increase of over 16,000x in just over a decade.This chapter examines the key components and architecture of machine learning training systems. We discuss the design of training pipelines, memory and computation systems, data management strategies, and advanced optimization techniques. Additionally, we explore distributed training frameworks and their role in scaling training processes. Real-world examples and case studies are provided to connect theoretical principles to practical implementations, offering insight into the development of efficient, scalable, and effective training systems.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>AI Training</span>"
    ]
  },
  {
    "objectID": "contents/core/training/training.html#ai-training-systems",
    "href": "contents/core/training/training.html#ai-training-systems",
    "title": "8  AI Training",
    "section": "8.2 AI Training Systems",
    "text": "8.2 AI Training Systems\nMachine learning training systems represent a distinct class of computational workload with unique demands on hardware and software infrastructure. These systems must efficiently orchestrate repeated computations over large datasets while managing substantial memory requirements and data movement patterns. Unlike traditional high-performance computing workloads, training systems exhibit specific characteristics that influence their design and implementation.\n\n8.2.1 Evolution of Systems\nComputing system architectures have evolved through distinct generations, with each new era building upon previous advances while introducing specialized optimizations for emerging application requirements (Figure 8.1). This progression demonstrates how hardware adaptation to application needs shapes modern machine learning systems.\n\n\n\n\n\n\ngantt\n    dateFormat YYYY\n    axisFormat %Y\n    section Mainframe\n    IBM System/360      :done, a1, 1964, 10y\n    ENIAC               :done, a2, 1945, 10y\n    section High-Performance Computing\n    CDC 6600            :done, b1, 1964, 20y\n    Cray-1              :done, b2, 1976, 15y\n    section Warehouse Scale Computing\n    Google Data Centers :done, c1, 2000, 10y\n    AWS                 :done, c2, 2006, 10y\n    section AI Hypercomputing Era\n    NVIDIA GPUs         :done, d1, 2015, 10y\n    Google TPUs         :done, d2, 2015, 10d\n\n\n\n\nFigure 8.1: Timeline of major advancements in computing systems for machine learning, showing the evolution from mainframes to AI hypercomputing systems.\n\n\n\n\n\nElectronic computation began with the mainframe era. ENIAC (1945) established the viability of electronic computation at scale, while the IBM System/360 (1964) introduced architectural principles of standardized instruction sets and memory hierarchies. These fundamental concepts laid the groundwork for all subsequent computing systems.\nHigh-performance computing (HPC) systems (Thornton 1965) built upon these foundations while specializing for scientific computation. The CDC 6600 and later systems like the CM-5 (Corporation 1992) optimized for dense matrix operations and floating-point calculations.\n\nThornton, James E. 1965. “Design of a Computer: The Control Data 6600.” Communications of the ACM 8 (6): 330–35.\n\nCorporation, Thinking Machines. 1992. CM-5 Technical Summary. Thinking Machines Corporation.\nHPC These systems implemented specific architectural features for scientific workloads: high-bandwidth memory systems for array operations, vector processing units for mathematical computations, and specialized interconnects for collective communication patterns. Scientific computing demanded emphasis on numerical precision and stability, with processors and memory systems designed for regular, predictable access patterns. The interconnects supported tightly synchronized parallel execution, enabling efficient collective operations across computing nodes.\nWarehouse-scale computing marked the next evolutionary step. Google’s data center implementations (Barroso and Hölzle 2007) introduced new optimizations for internet-scale data processing. Unlike HPC systems focused on tightly coupled scientific calculations, warehouse computing handled loosely coupled tasks with irregular data access patterns.\n\nBarroso, Luiz André, and Urs Hölzle. 2007. “The Case for Energy-Proportional Computing.” Computer 40 (12): 33–37. https://doi.org/10.1109/mc.2007.443.\nWSC systems introduced architectural changes to support high throughput for independent tasks, with robust fault tolerance and recovery mechanisms. The storage and memory systems adapted to handle sparse data structures efficiently, moving away from the dense array optimizations of HPC. Resource management systems evolved to support multiple applications sharing the computing infrastructure, contrasting with HPC’s dedicated application execution model.\nDeep learning computation emerged as the next frontier, building upon this accumulated architectural knowledge. AlexNet’s (Krizhevsky, Sutskever, and Hinton 2017) success in 2012 highlighted the need for further specialization. While previous systems focused on either scientific calculations or independent data processing tasks, neural network training introduced new computational patterns. The training process required continuous updates to large sets of parameters, with complex data dependencies during model optimization. These workloads demanded new approaches to memory management and inter-device communication that neither HPC nor warehouse computing had fully addressed.\n\nKrizhevsky, Alex, Ilya Sutskever, and Geoffrey E. Hinton. 2017. “ImageNet Classification with Deep Convolutional Neural Networks.” Communications of the ACM 60 (6): 84–90. https://doi.org/10.1145/3065386.\nThe AI hypercomputing era, beginning in 2015, represents the latest step in this evolutionary chain. NVIDIA GPUs and Google TPUs introduced hardware designs specifically optimized for neural network computations, moving beyond adaptations of existing architectures. These systems implemented new approaches to parallel processing, memory access, and device communication to handle the distinct patterns of model training. The resulting architectures balanced the numerical precision needs of scientific computing with the scale requirements of warehouse systems, while adding specialized support for the iterative nature of neural network optimization.\nThis architectural progression illuminates why traditional computing systems proved insufficient for neural network training. As shown in Table 8.1, while HPC systems provided the foundation for parallel numerical computation and warehouse-scale systems demonstrated distributed processing at scale, neither fully addressed the computational patterns of model training. Modern neural networks combine intensive parameter updates, complex memory access patterns, and coordinated distributed computation in ways that demanded new architectural approaches.\n\n\n\nTable 8.1: Comparison of computing system characteristics across different eras\n\n\n\n\n\n\n\n\n\n\n\n\nEra\nPrimary Workload\nMemory Patterns\nProcessing Model\nSystem Focus\n\n\n\n\nMainframe\nSequential batch processing\nSimple memory hierarchy\nSingle instruction stream\nGeneral-purpose computation\n\n\nHPC\nScientific simulation\nRegular array access\nSynchronized parallel\nNumerical precision, collective operations\n\n\nWarehouse-scale\nInternet services\nSparse, irregular access\nIndependent parallel tasks\nThroughput, fault tolerance\n\n\nAI Hypercomputing\nNeural network training\nParameter-heavy, mixed access\nHybrid parallel, distributed\nTraining optimization, model scale\n\n\n\n\n\n\nUnderstanding these distinct characteristics and their evolution from previous computing eras explains why modern AI training systems require dedicated hardware features and optimized system designs. This historical context provides the foundation for examining machine learning training system architectures in detail.\n\n\n8.2.2 Role in ML Systems\nThe development of modern machine learning models relies critically on specialized systems for training and optimization. These systems are a complex interplay of hardware and software components that must efficiently handle massive datasets while maintaining numerical precision and computational stability. While there is no universally accepted definition of training systems due to their rapid evolution and diverse implementations, they share common characteristics and requirements that distinguish them from traditional computing infrastructures.\n\n\n\n\n\n\nDefinition of Training Systems\n\n\n\nMachine Learning Training Systems refer to the specialized computational frameworks that manage and execute the iterative optimization of machine learning models. These systems encompass the software and hardware stack responsible for processing training data, computing gradients, updating model parameters, and coordinating distributed computation. Training systems operate at multiple scales, from single hardware accelerators to distributed clusters, and incorporate components for data management, computation scheduling, memory optimization, and performance monitoring. They serve as the foundational infrastructure that enables the systematic development and refinement of machine learning models through empirical training on data.\n\n\nThese training systems constitute the fundamental infrastructure required for developing predictive models. They execute the mathematical optimization of model parameters, converting input data into computational representations for tasks such as pattern recognition, language understanding, and decision automation. The training process involves systematic iteration over datasets to minimize error functions and achieve optimal model performance.\nTraining systems function as integral components within the broader machine learning pipeline. They interface with preprocessing frameworks that standardize and transform raw data, while connecting to deployment architectures that enable model serving. The computational efficiency and reliability of training systems directly influence the development cycle, from initial experimentation through model validation to production deployment.\nThe emergence of transformer architectures and large-scale models has introduced new requirements for training systems. Contemporary implementations must efficiently process petabyte-scale datasets, orchestrate distributed training across multiple accelerators, and optimize memory utilization for models containing billions of parameters. The management of data parallelism, model parallelism, and inter-device communication presents significant technical challenges in modern training architectures.\nTraining systems also significantly impact the operational considerations of machine learning development. System design must address multiple technical constraints: computational throughput, energy consumption, hardware compatibility, and scalability with increasing model complexity. These factors determine both the technical feasibility and operational viability of machine learning implementations across different scales and applications.\n\n\n8.2.3 Systems Thinking\nThe practical execution of training models is deeply tied to system design. Training is not merely a mathematical optimization problem; it is a system-driven process that requires careful orchestration of computing hardware, memory, and data movement.\nTraining workflows consist of interdependent stages: data preprocessing, forward and backward passes, and parameter updates. Each stage imposes specific demands on system resources. For instance, data preprocessing relies on storage and I/O subsystems to provide computing hardware with continuous input. While traditional processors like CPUs handle many training tasks effectively, increasingly complex models have driven the adoption of hardware accelerators—including Graphics Processing Units (GPUs) and specialized machine learning processors—that can process mathematical operations in parallel. These accelerators, alongside CPUs, handle operations like gradient computation and parameter updates. The performance of these stages depends on how well the system manages bottlenecks such as memory bandwidth and communication latency.\nSystem constraints often dictate the performance limits of training workloads. Modern accelerators are frequently bottlenecked by memory bandwidth, as data movement between memory hierarchies can be slower and more energy-intensive than the computations themselves (Hooker 2021). In distributed setups, synchronization across devices introduces additional latency, with the performance of interconnects (e.g., NVLink, InfiniBand) playing a crucial role.\n\nHooker, Sara. 2021. “The Hardware Lottery.” Communications of the ACM 64 (12): 58–65. https://doi.org/10.1145/3467017.\nOptimizing training workflows is essential to overcoming these limitations. Techniques like overlapping computation with data loading, mixed-precision training (Kuchaiev et al. 2018), and efficient memory allocation can significantly enhance performance. These optimizations ensure that accelerators are utilized effectively, minimizing idle time and maximizing throughput.\nBeyond training infrastructure, systems thinking has also informed model architecture decisions. System-level constraints often guide the development of new model architectures and training approaches. For example, memory limitations have motivated research into more efficient neural network architectures (M. X. Chen et al. 2018), while communication overhead in distributed systems has influenced the design of optimization algorithms. These adaptations demonstrate how practical system considerations shape the evolution of machine learning approaches within given computational bounds.\n\nChen, Mia Xu, Orhan Firat, Ankur Bapna, Melvin Johnson, Wolfgang Macherey, George Foster, Llion Jones, et al. 2018. “The Best of Both Worlds: Combining Recent Advances in Neural Machine Translation.” In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), 30:5998–6008. Association for Computational Linguistics. https://doi.org/10.18653/v1/p18-1008.\nFor example, training large Transformer models requires partitioning data and model parameters across multiple devices. This introduces synchronization challenges, particularly during gradient updates. Communication libraries such as NVIDIA’s Collective Communications Library (NCCL) enable efficient gradient sharing, providing the foundation for more advanced techniques we discuss in later sections. These examples illustrate how system-level considerations influence the feasibility and efficiency of modern training workflows.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>AI Training</span>"
    ]
  },
  {
    "objectID": "contents/core/training/training.html#mathematical-foundations",
    "href": "contents/core/training/training.html#mathematical-foundations",
    "title": "8  AI Training",
    "section": "8.3 Mathematical Foundations",
    "text": "8.3 Mathematical Foundations\nNeural networks are grounded in mathematical principles that define their structure and functionality. These principles encompass key operations essential for enabling networks to learn complex patterns from data. A thorough understanding of the mathematical foundations underlying these operations is vital, not only for comprehending the mechanics of neural network computation but also for recognizing their broader implications at the system level.\nTherefore, we need to connect the theoretical underpinnings of these operations to their practical implementation, examining how modern systems optimize these computations to address critical challenges such as memory management, computational efficiency, and scalability in training deep learning models.\n\n8.3.1 Neural Network Computation\nWe have previously introduced the basic operations involved in training a neural network (see Chapter 3 and Chapter 4), such as forward propagation and the use of loss functions to evaluate performance. Here, we build on those foundational concepts to explore how these operations are executed at the system level. Key mathematical operations such as matrix multiplications and activation functions underpin the system requirements for training neural networks. Foundational works by Rumelhart, Hinton, and Williams (1986) via the introduction of backpropagation and the development of efficient matrix computation libraries, e.g., BLAS (Dongarra et al. 1988), laid the groundwork for modern training architectures.\n\nRumelhart, David E., Geoffrey E. Hinton, and Ronald J. Williams. 1986. “Learning Representations by Back-Propagating Errors.” Nature 323 (6088): 533–36. https://doi.org/10.1038/323533a0.\n\nDongarra, Jack J., Jeremy Du Croz, Sven Hammarling, and Richard J. Hanson. 1988. “An Extended Set of FORTRAN Basic Linear Algebra Subprograms.” ACM Transactions on Mathematical Software 14 (1): 1–17. https://doi.org/10.1145/42288.42291.\n\nCore Operations\nAt the heart of a neural network is the process of forward propagation, in its simplest case, involves two primary operations: matrix multiplication and the application of an activation function. Matrix multiplication forms the basis of the linear transformation in each layer of the network. At layer \\(l\\), the computation can be described as:\n\\[\nA^{(l)} = f\\left(W^{(l)} A^{(l-1)} + b^{(l)}\\right)\n\\]\nWhere:\n\n\\(A^{(l-1)}\\) represents the activations from the previous layer (or the input layer for the first layer),\n\\(W^{(l)}\\) is the weight matrix at layer \\(l\\), which contains the parameters learned by the network,\n\\(b^{(l)}\\) is the bias vector for layer \\(l\\),\n\\(f(\\cdot)\\) is the activation function applied element-wise (e.g., ReLU, sigmoid) to introduce non-linearity.\n\n\n\nMatrix Operations in Neural Networks\nThe computational patterns in neural networks revolve around various types of matrix operations. Understanding these operations and their evolution reveals the reasons why specific system designs and optimizations emerged in machine learning training systems.\n\nDense Matrix-Matrix Multiplication\nMatrix-matrix multiplication dominates computation in neural networks, accounting for 60-90% of training time (He et al. 2016). Early neural network implementations relied on standard CPU-based linear algebra libraries. The evolution of matrix multiplication algorithms has closely followed advancements in numerical linear algebra. From Strassen’s algorithm, which reduced the naive \\(O(n^3)\\) complexity to approximately \\(O(n^{2.81})\\) (Strassen 1969), to contemporary hardware-accelerated libraries like cuBLAS, these innovations have continually pushed the limits of computational efficiency.\n\nHe, Kaiming, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016. “Deep Residual Learning for Image Recognition.” In 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 770–78. IEEE. https://doi.org/10.1109/cvpr.2016.90.\n\nStrassen, Volker. 1969. “Gaussian Elimination Is Not Optimal.” Numerische Mathematik 13 (4): 354–56. https://doi.org/10.1007/bf02165411.\nModern systems implement blocked matrix computations for parallel processing across multiple units. As neural architectures grew in scale, these multiplications began to demand significant memory resources—weight matrices and activation matrices must both remain accessible for the backward pass during training. Hardware designs adapted to optimize for these dense multiplication patterns while managing growing memory requirements.\n\n\nMatrix-Vector Operations\nMatrix-vector multiplication became essential with the introduction of normalization techniques in neural architectures. While computationally simpler than matrix-matrix multiplication, these operations present unique system challenges. They exhibit lower hardware utilization due to their limited parallelization potential. This characteristic influences both hardware design and model architecture decisions, particularly in networks processing sequential inputs or computing layer statistics.\n\n\nBatched Matrix Operations\nThe introduction of batching transformed matrix computation in neural networks. By processing multiple inputs simultaneously, training systems convert matrix-vector operations into more efficient matrix-matrix operations. This approach improves hardware utilization but increases memory demands for storing intermediate results. Modern implementations must balance batch sizes against available memory, leading to specific optimizations in memory management and computation scheduling.\nHardware accelerators like Google’s TPU (Jouppi, Young, Patil, Patterson, and al. 2017) reflect this evolution, incorporating specialized matrix units and memory hierarchies for these diverse multiplication patterns. These hardware adaptations enable training of large-scale models like GPT-3 (Brown, Mann, Ryder, Subbiah, Kaplan, and al. 2020) through efficient handling of varied matrix operations.\n\nBrown, Tom B., Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, and et al. 2020. “Language Models Are Few-Shot Learners.” Advances in Neural Information Processing Systems (NeurIPS) 33: 1877–1901.\n\n\n\nActivation Functions\nActivation functions are central to neural network operation. As shown in Figure 8.2, these functions apply different non-linear transformations to input values, which is essential for enabling neural networks to approximate complex mappings between inputs and outputs. Without activation functions, neural networks, regardless of depth, would collapse into linear systems, severely limiting their representational power (Goodfellow, Courville, and Bengio 2013).\n\nGoodfellow, Ian J., Aaron Courville, and Yoshua Bengio. 2013. “Scaling up Spike-and-Slab Models for Unsupervised Feature Learning.” IEEE Transactions on Pattern Analysis and Machine Intelligence 35 (8): 1902–14. https://doi.org/10.1109/tpami.2012.273.\n\n\n\n\n\n\nFigure 8.2: Activation functions. Note that the axes are different across graphs.\n\n\n\nWhile activation functions are applied element-wise to the outputs of each neuron, their computational cost is significantly lower than that of matrix multiplications. Typically, activation functions contribute to about 5-10% of the total computation time. However, their impact on the learning process is profound, influencing not only the network’s ability to learn but also its convergence rate and gradient flow.\nA careful understanding of activation functions and their computational implications is vital for designing efficient machine learning pipelines. Selecting the appropriate activation function can minimize computation time without compromising the network’s ability to learn complex patterns, ensuring both efficiency and accuracy.\n\nSigmoid Function\nThe sigmoid function is one of the original activation functions in neural networks. It maps input values to the range \\((0, 1)\\) through the following mathematical expression:\n\\[\n\\text{sigmoid}(x) = \\frac{1}{1 + e^{-x}}\n\\]\nThis function produces an S-shaped curve, where inputs far less than zero approach an output of \\(0\\), and inputs much greater than zero approach \\(1\\). The smooth transition between these bounds makes sigmoid particularly useful in scenarios where outputs need to be interpreted as probabilities. It is therefore commonly applied in the output layer of networks for binary classification tasks.\nThe sigmoid function is differentiable and has a well-defined gradient, which makes it suitable for use with gradient-based optimization methods. Its bounded output ensures numerical stability, preventing excessively large activations that might destabilize the training process. However, for inputs with very high magnitudes (positive or negative), the gradient becomes negligible, which can lead to the vanishing gradient problem. This issue is particularly detrimental in deep networks, where gradients must propagate through many layers during training (Hochreiter 1998).\n\nHochreiter, Sepp. 1998. “The Vanishing Gradient Problem During Learning Recurrent Neural Nets and Problem Solutions.” International Journal of Uncertainty, Fuzziness and Knowledge-Based Systems 06 (02): 107–16. https://doi.org/10.1142/s0218488598000094.\n2 Batch Normalization: A technique that normalizes the input of each layer by adjusting and scaling the activations, reducing internal covariate shift and enabling faster training.Additionally, sigmoid outputs are not zero-centered, meaning that the function produces only positive values. This lack of symmetry can cause optimization algorithms like stochastic gradient descent (SGD) to exhibit inefficient updates, as gradients may introduce biases that slow convergence. To mitigate these issues, techniques such as batch normalization2 or careful initialization may be employed.\nDespite its limitations, sigmoid remains an effective choice in specific contexts. It is often used in the final layer of binary classification models, where its output can be interpreted directly as the probability of a particular class. For example, in a network designed to classify emails as either spam or not spam, the sigmoid function converts the network’s raw score into a probability, making the output more interpretable.\n\n\nTanh Function\nThe hyperbolic tangent, or tanh, is a commonly used activation function in neural networks. It maps input values through a nonlinear transformation into the range \\((-1, 1)\\). The mathematical definition of the tanh function is:\n\\[\n\\tanh(x) = \\frac{e^x - e^{-x}}{e^x + e^{-x}}\n\\]\nThis function produces an S-shaped curve, similar to the sigmoid function, but with the important distinction that its output is centered around zero. Negative inputs are mapped to values in the range \\([-1, 0)\\), while positive inputs are mapped to values in the range \\((0, 1]\\). This zero-centered property makes tanh advantageous for hidden layers, as it reduces bias in weight updates and facilitates faster convergence during optimization (LeCun et al. 1998).\nThe tanh function is smooth and differentiable, with a gradient that is well-defined for all input values. Its symmetry around zero helps balance the activations of neurons, leading to more stable and efficient learning dynamics. However, for inputs with very large magnitudes (positive or negative), the function saturates, and the gradient approaches zero. This vanishing gradient problem can impede training in deep networks.\nThe tanh function is often used in the hidden layers of neural networks, particularly for tasks where the input data contains both positive and negative values. Its symmetric range \\((-1, 1)\\) ensures balanced activations, making it well-suited for applications such as sequence modeling and time series analysis.\nFor example, tanh is widely used in recurrent neural networks (RNNs), where its bounded and symmetric properties help stabilize learning dynamics over time. While tanh has largely been replaced by ReLU in many modern architectures due to its computational inefficiencies and vanishing gradient issues, it remains a viable choice in scenarios where its range and symmetry are beneficial.\n\n\nReLU Function\nThe Rectified Linear Unit (ReLU) is one of the most widely used activation functions in modern neural networks. Its simplicity and effectiveness have made it the default choice for most machine learning architectures. The ReLU function is defined as:\n\\[\n\\text{ReLU}(x) = \\max(0, x)\n\\]\nThis function outputs the input value if it is positive and zero otherwise. Unlike sigmoid and tanh, which produce smooth, bounded outputs, ReLU introduces sparsity in the network by setting all negative inputs to zero. This sparsity can help reduce overfitting and improve computation efficiency in many scenarios.\nReLU is particularly effective in avoiding the vanishing gradient problem, as it maintains a constant gradient for positive inputs. However, it introduces another issue known as the dying ReLU problem, where neurons can become permanently inactive if they consistently output zero. This occurs when the weights cause the input to remain in the negative range. In such cases, the neuron no longer contributes to learning.\nReLU is commonly used in the hidden layers of neural networks, particularly in convolutional neural networks (CNNs) and machine learning models for image and speech recognition tasks. Its computational simplicity and ability to prevent vanishing gradients make it ideal for training deep architectures.\n\n\nSoftmax Function\nThe softmax function is a widely used activation function, primarily applied in the output layer of classification models. It transforms raw scores into a probability distribution, ensuring that the outputs sum to 1. This makes it particularly suitable for multi-class classification tasks, where each output represents the probability of the input belonging to a specific class.\nThe mathematical definition of the softmax function for a vector of inputs \\(\\mathbf{z}=[z_1,z_2,\\dots,z_K]\\) is:\n\\[\n\\sigma(z_i)=\\frac{e^{z_i}}{\\sum_{j=1}^K e^{z_j}},\\quad i=1,2,\\dots,K\n\\]\nHere, \\(K\\) is the number of classes, \\(z_i\\) represents the raw score (logit) for the \\(i\\)-th class, and \\(\\sigma(z_i)\\) is the probability of the input belonging to that class.\nSoftmax has several desirable properties that make it essential for classification tasks. It converts arbitrary real-valued inputs into probabilities, with each output value in the range \\((0,1)\\) and the sum of all outputs equal to 1. The function is differentiable, which allows it to be used with gradient-based optimization methods. Additionally, the probabilistic interpretation of its output is crucial for tasks where confidence levels are needed, such as object detection or language modeling.\nHowever, softmax is sensitive to the magnitude of the input logits. Large differences in logits can lead to highly peaked distributions, where most of the probability mass is concentrated on a single class, potentially leading to overconfidence in predictions.\nSoftmax finds extensive application in the final layer of neural networks for multi-class classification tasks. For instance, in image classification, models such as AlexNet and ResNet employ softmax in their final layers to assign probabilities to different image categories. Similarly, in natural language processing tasks like language modeling and machine translation, softmax is applied over large vocabularies to predict the next word or token, making it an essential component in understanding and generating human language.\n\n\nSystem Trade-offs\nActivation functions in neural networks significantly impact both mathematical properties and system-level performance. The selection of an activation function directly influences training time, model scalability, and hardware efficiency through three primary factors: computational cost, gradient behavior, and memory usage.\nBenchmarking common activation functions on an Apple M2 single-threaded CPU reveals meaningful performance differences, as illustrated in Figure 8.3. The data demonstrates that Tanh and ReLU execute more efficiently than Sigmoid on CPU architectures, making them particularly suitable for real-time applications and large-scale systems.\n\n\n\n\n\n\nFigure 8.3: Activation function performance.\n\n\n\nWhile these benchmark results provide valuable insights, they represent CPU-only performance without hardware acceleration. In production environments, modern hardware accelerators like GPUs can substantially alter the relative performance characteristics of activation functions. System architects must therefore consider their specific hardware environment and deployment context when evaluating computational efficiency.\nThe selection of activation functions requires careful balancing of computational considerations against mathematical properties. Key factors include the function’s ability to mitigate vanishing gradients and introduce beneficial sparsity in neural activations. Each major activation function presents distinct advantages and challenges:\n\nSigmoid\nThe sigmoid function has smooth gradients and a bounded output in the range \\((0, 1)\\), making it useful in probabilistic settings. However, the computation of the sigmoid involves an exponential function, which becomes a key consideration in both software and hardware implementations. In software, this computation is expensive and inefficient, particularly for deep networks or large datasets. Additionally, sigmoid suffers from vanishing gradients, especially for large input values, which can hinder the learning process in deep architectures. Its non-zero-centered output can also slow optimization, requiring more epochs to converge.\nThese computational challenges are addressed differently in hardware. Modern accelerators like GPUs and TPUs typically avoid direct computation of the exponential function, instead using lookup tables (LUTs) or piece-wise linear approximations to balance accuracy with speed. While these hardware optimizations help, the multiple memory lookups and interpolation calculations still make sigmoid more resource-intensive than simpler functions like ReLU, even on highly parallel architectures.\n\n\nTanh\nThe tanh function outputs values in the range \\((-1, 1)\\), making it zero-centered and helping to stabilize gradient-based optimization algorithms. This zero-centered output helps reduce biases in weight updates, an advantage over sigmoid. Like sigmoid, however, tanh involves exponential computations that impact both software and hardware implementations. In software, this computational overhead can slow training, particularly when working with large datasets or deep models. While tanh helps prevent some of the saturation issues associated with sigmoid, it still suffers from vanishing gradients for large inputs, especially in deep networks.\nIn hardware, tanh leverages its mathematical relationship with sigmoid (being essentially a scaled and shifted version) to optimize implementation. Modern hardware often implement tanh using a hybrid approach: lookup tables for common input ranges combined with piece-wise approximations for edge cases. This approach helps balance accuracy with computational efficiency, though tanh remains more resource-intensive than simpler functions. Despite these challenges, tanh remains common in RNNs and LSTMs where balanced gradients are crucial.\n\n\nReLU\nThe ReLU function stands out for its mathematical simplicity: it passes positive values unchanged and sets negative values to zero. This straightforward behavior has profound implications for both software and hardware implementations. In software, ReLU’s simple thresholding operation results in faster computation compared to sigmoid or tanh. It also helps prevent vanishing gradients and introduces beneficial sparsity in activations, as many neurons output zero. However, ReLU can suffer from the “dying ReLU” problem in deep networks, where neurons become permanently inactive and never update their weights.\nThe hardware implementation of ReLU showcases why it has become the dominant activation function in modern neural networks. Its simple max\\((0,x)\\) operation requires just a single comparison and conditional set, translating to minimal circuit complexity. Modern GPUs and TPUs can implement ReLU using a simple multiplexer that checks the input’s sign bit, allowing for extremely efficient parallel processing. This hardware efficiency, combined with the sparsity it introduces, results in both reduced computation time and lower memory bandwidth requirements.\n\n\nSoftmax\nThe softmax function transforms raw logits into a probability distribution, ensuring outputs sum to 1, making it essential for classification tasks. Its computation involves exponentiating each input value and normalizing by their sum, a process that becomes increasingly complex with larger output spaces. In software, this creates significant computational overhead for tasks like natural language processing, where vocabulary sizes can reach hundreds of thousands of terms. The function also requires keeping all values in memory during computation, as each output probability depends on the entire input.\nAt the hardware level, softmax faces unique challenges because it can’t process each value independently like other activation functions. Unlike ReLU’s simple threshold or even sigmoid’s per-value computation, softmax needs access to all values to perform normalization. This becomes particularly demanding in modern transformer architectures, where softmax computations in attention mechanisms process thousands of values simultaneously. To manage these demands, hardware implementations often use approximation techniques or simplified versions of softmax, especially when dealing with large vocabularies or attention mechanisms.\nTable 8.2 summarizes the trade-offs of these commonly used activation functions and highlights how these choices affect system performance.\n\n\n\nTable 8.2: Comparison of different actiation functions and their advances and distagnets anad system implications.\n\n\n\n\n\n\n\n\n\n\n\nFunction\nKey Advantages\nKey Disadvantages\nSystem Implications\n\n\n\n\nSigmoid\nSmooth gradients; bounded output in \\((0, 1)\\).\nVanishing gradients; non-zero-centered output.\nExponential computation adds overhead; limited scalability for deep networks on modern accelerators.\n\n\nTanh\nZero-centered output in \\((-1, 1)\\); stabilizes gradients.\nVanishing gradients for large inputs.\nMore expensive than ReLU; effective for RNNs/LSTMs but less common in CNNs and Transformers.\n\n\nReLU\nComputationally efficient; avoids vanishing gradients; introduces sparsity.\nDying neurons; unbounded output.\nSimple operations optimize well on GPUs/TPUs; sparse activations reduce memory and computation needs.\n\n\nSoftmax\nConverts logits into probabilities; sums to \\(1\\).\nComputationally expensive for large outputs.\nHigh cost for large vocabularies; hierarchical or sampled softmax needed for scalability in NLP tasks.\n\n\n\n\n\n\nThe choice of activation function should balance computational considerations with their mathematical properties, such as handling vanishing gradients or introducing sparsity in neural activations. This data emphasizes the importance of evaluating both theoretical and practical performance when designing neural networks. For large-scale networks or real-time applications, ReLU is often the best choice due to its efficiency and scalability. However, for tasks requiring probabilistic outputs, such as classification, softmax remains indispensable despite its computational cost. Ultimately, the ideal activation function depends on the specific task, network architecture, and hardware environment.\n\n\n\n\n\n8.3.2 Optimization Algorithms\nOptimization algorithms play an important role in neural network training by guiding the adjustment of model parameters to minimize a loss function. This process is fundamental to enabling neural networks to learn from data, and it involves finding the optimal set of parameters that yield the best model performance on a given task. Broadly, these algorithms can be divided into two categories: classical methods, which provide the theoretical foundation, and advanced methods, which introduce enhancements for improved performance and efficiency.\nThese algorithms are responsible for navigating the complex, high-dimensional landscape of the loss function, identifying regions where the function achieves its lowest values. This task is challenging because the loss function surface is rarely smooth or simple, often characterized by local minima, saddle points, and sharp gradients. Effective optimization algorithms are designed to overcome these challenges, ensuring convergence to a solution that generalizes well to unseen data.\nThe selection and design of optimization algorithms have significant system-level implications, such as computation efficiency, memory requirements, and scalability to large datasets or models. A deeper understanding of these algorithms is essential for addressing the trade-offs between accuracy, speed, and resource usage.\n\nClassical Methods\nModern neural network training relies on variations of gradient descent for parameter optimization. These approaches differ in how they process training data, leading to distinct system-level implications.\n\nGradient Descent\nGradient descent is the mathematical foundation of neural network training, iteratively adjusting parameters to minimize a loss function. The basic gradient descent algorithm computes the gradient of the loss with respect to each parameter, then updates parameters in the opposite direction of the gradient:\n\\[ \\theta_{t+1} = \\theta_t - \\alpha \\nabla L(\\theta_t) \\]\nIn training systems, this mathematical operation translates into specific computational patterns. For each iteration, the system must:\n\nCompute forward pass activations\nCalculate loss value\nCompute gradients through backpropagation\nUpdate parameters using the gradient values\n\nThe computational demands of gradient descent scale with both model size and dataset size. Consider a neural network with \\(M\\) parameters training on \\(N\\) examples. Computing gradients requires storing intermediate activations during the forward pass for use in backpropagation. These activations consume memory proportional to the depth of the network and the number of examples being processed.\nTraditional gradient descent processes the entire dataset in each iteration. For a training set with 1 million examples, computing gradients requires evaluating and storing results for each example before performing a parameter update. This approach poses significant system challenges:\n\\[ \\text{Memory Required} = N \\times \\text{(Activation Memory + Gradient Memory)} \\]\nThe memory requirements often exceed available hardware resources on modern hardware. A ResNet-50 model processing ImageNet-scale datasets would require hundreds of gigabytes of memory using this approach. Additionally, processing the full dataset before each update creates long iteration times, reducing the rate at which the model can learn from the data.\n\nStochastic Gradient Descent (SGD)\nThese system constraints led to the development of variants that better align with hardware capabilities. The key insight was that exact gradient computation, while mathematically appealing, is not necessary for effective learning. This realization opened the door to methods that trade gradient accuracy for improved system efficiency.\nThese system limitations motivated the development of more efficient optimization approaches. Stochastic gradient descent (SGD) represents a fundamental shift in the optimization strategy. Rather than computing gradients over the entire dataset, SGD estimates gradients using individual training examples:\n\\[ \\theta_{t+1} = \\theta_t - \\alpha \\nabla L(\\theta_t; x_i, y_i) \\]\nwhere \\((x_i, y_i)\\) represents a single training example. This approach drastically reduces memory requirements since only one example’s activations and gradients need storage at any time. The stochastic nature of these updates introduces noise into the optimization process, but this noise often helps escape local minima and reach better solutions.\nHowever, processing single examples creates new system challenges. Modern accelerators achieve peak performance through parallel computation, processing multiple data elements simultaneously. Single-example updates leave most computing resources idle, resulting in poor hardware utilization. The frequent parameter updates also increase memory bandwidth requirements, as weights must be read and written for each example rather than amortizing these operations across multiple examples.\n\n\n\nMini-batch Processing\nMini-batch gradient descent emerges as a practical compromise between full-batch and stochastic methods. It computes gradients over small batches of examples, enabling parallel computations that align well with modern GPU architectures (Dean and Ghemawat 2008).\n\nDean, Jeffrey, and Sanjay Ghemawat. 2008. “MapReduce: Simplified Data Processing on Large Clusters.” Communications of the ACM 51 (1): 107–13. https://doi.org/10.1145/1327452.1327492.\n\\[ \\theta_{t+1} = \\theta_t - \\alpha \\frac{1}{B} \\sum_{i=1}^B \\nabla L(\\theta_t; x_i, y_i) \\]\nMini-batch processing aligns well with modern hardware capabilities. Consider a training system using GPU hardware. These devices contain thousands of cores designed for parallel computation. Mini-batch processing allows these cores to simultaneously compute gradients for multiple examples, improving hardware utilization. The batch size B becomes a key system parameter, influencing both computational efficiency and memory requirements.\nThe relationship between batch size and system performance follows clear patterns. Memory requirements scale linearly with batch size:\n\\[ \\text{Memory Required} = B \\times \\text{(Activation Memory + Gradient Memory)} \\]\nHowever, larger batches enable more efficient computation through improved parallelism. This creates a trade-off between memory constraints and computational efficiency. Training systems must select batch sizes that maximize hardware utilization while fitting within available memory.\n\n\n\nAdvanced Optimization Algorithms\nAdvanced optimization algorithms introduce mechanisms like momentum and adaptive learning rates to improve convergence. These methods have been instrumental in addressing the inefficiencies of classical approaches (Kingma and Ba 2014).\n\nKingma, Diederik P., and Jimmy Ba. 2014. “Adam: A Method for Stochastic Optimization.” ICLR, December. http://arxiv.org/abs/1412.6980v9.\n\nMomentum-Based Methods\nMomentum methods enhance gradient descent by accumulating a velocity vector across iterations. The momentum update equations introduce an additional term to track the history of parameter updates:\n\\[ v_{t+1} = \\beta v_t + \\nabla L(\\theta_t) \\] \\[ \\theta_{t+1} = \\theta_t - \\alpha v_{t+1} \\]\nwhere \\(\\beta\\) is the momentum coefficient, typically set between 0.9 and 0.99. From a systems perspective, momentum introduces additional memory requirements. The training system must maintain a velocity vector with the same dimensionality as the parameter vector, effectively doubling the memory needed for optimization state.\n\n\nAdaptive Learning Rate Methods\nRMSprop modifies the basic gradient descent update by maintaining a moving average of squared gradients for each parameter:\n\\[ s_t = \\gamma s_{t-1} + (1-\\gamma)(\\nabla L(\\theta_t))^2 \\] \\[ \\theta_{t+1} = \\theta_t - \\alpha \\frac{\\nabla L(\\theta_t)}{\\sqrt{s_t + \\epsilon}} \\]\nThis per-parameter adaptation requires storing the moving average \\(s_t\\), creating memory overhead similar to momentum methods. The element-wise operations in RMSprop also introduce additional computational steps compared to basic gradient descent.\n\n\nAdam Optimization\nAdam combines concepts from both momentum and RMSprop, maintaining two moving averages for each parameter:\n\\[ m_t = \\beta_1 m_{t-1} + (1-\\beta_1)\\nabla L(\\theta_t) \\] \\[ v_t = \\beta_2 v_{t-1} + (1-\\beta_2)(\\nabla L(\\theta_t))^2 \\] \\[ \\theta_{t+1} = \\theta_t - \\alpha \\frac{m_t}{\\sqrt{v_t + \\epsilon}} \\]\nThe system implications of Adam are more substantial than previous methods. The optimizer must store two additional vectors (\\(m_t\\) and \\(v_t\\)) for each parameter, tripling the memory required for optimization state. For a model with 100 million parameters using 32-bit floating-point numbers, the additional memory requirement is approximately 800MB.\n\n\n\nSystem Implications\nThe practical implementation of both classical and advanced optimization methods requires careful consideration of system resources and hardware capabilities. Understanding these implications helps inform algorithm selection and system design choices.\n\nTrade-offs\nThe choice of optimization algorithm creates specific patterns of computation and memory access that influence training efficiency. Memory requirements increase progressively from basic gradient descent to more sophisticated methods:\n\\[ \\text{Memory}_{\\text{SGD}} = \\text{Size}_{\\text{params}} \\] \\[ \\text{Memory}_{\\text{Momentum}} = 2 \\times \\text{Size}_{\\text{params}} \\] \\[ \\text{Memory}_{\\text{Adam}} = 3 \\times \\text{Size}_{\\text{params}} \\]\nThese memory costs must be balanced against convergence benefits. While Adam often requires fewer iterations to reach convergence, its per-iteration memory and computation overhead may impact training speed on memory-constrained systems.\n\n\nImplementation Considerations\nThe efficient implementation of optimization algorithms in training frameworks hinges on strategic system-level considerations that directly influence performance. Key factors include memory bandwidth management, operation fusion techniques, and numerical precision optimization. These elements collectively determine the computational efficiency, memory utilization, and scalability of optimizers across diverse hardware architectures.\nMemory bandwidth presents the primary bottleneck in optimizer implementation. Modern frameworks address this through operation fusion, which reduces memory access overhead by combining multiple operations into a single kernel. For example, the Adam optimizer’s memory access requirements can grow linearly with parameter size when operations are performed separately:\n\\[ \\text{Bandwidth}_{\\text{separate}} = 5 \\times \\text{Size}_{\\text{params}} \\]\nHowever, fusing these operations into a single computational kernel significantly reduces the bandwidth requirement:\n\\[ \\text{Bandwidth}_{\\text{fused}} = 2 \\times \\text{Size}_{\\text{params}} \\]\nThese techniques have been effectively demonstrated in systems like cuDNN and other GPU-accelerated frameworks that optimize memory bandwidth usage and operation fusion (Chetlur et al. 2014; Jouppi, Young, Patil, Patterson, and al. 2017).\n\nChetlur, Sharan, Cliff Woolley, Philippe Vandermersch, Jonathan Cohen, John Tran, Bryan Catanzaro, and Evan Shelhamer. 2014. “Cudnn: Efficient Primitives for Deep Learning.” arXiv Preprint arXiv:1410.0759.\nMemory access patterns also play an important role in determining the efficiency of cache utilization. Sequential access to parameter and optimizer state vectors maximizes cache hit rates and effective memory bandwidth. This principle is evident in hardware such as GPUs and tensor processing units (TPUs), where optimized memory layouts significantly improve performance (Jouppi, Young, Patil, Patterson, and al. 2017).\nNumerical precision represents another important tradeoff in implementation. Empirical studies have shown that optimizer states remain stable even when reduced precision formats, such as 16-bit floating-point (FP16), are used. Transitioning from 32-bit to 16-bit formats reduces memory requirements, as illustrated for the Adam optimizer:\n\\[ \\text{Memory}_{\\text{Adam-FP16}} = \\frac{3}{2} \\times \\text{Size}_{\\text{params}} \\]\nMixed-precision training has been shown to achieve comparable accuracy while significantly reducing memory consumption and computational overhead (Kuchaiev et al. 2018; Krishnamoorthi 2018).\n\nKuchaiev, Oleksii, Boris Ginsburg, Igor Gitman, Vitaly Lavrukhin, Carl Case, and Paulius Micikevicius. 2018. “OpenSeq2Seq: Extensible Toolkit for Distributed and Mixed Precision Training of Sequence-to-Sequence Models.” In Proceedings of Workshop for NLP Open Source Software (NLP-OSS), 41–46. Association for Computational Linguistics. https://doi.org/10.18653/v1/w18-2507.\n\nKrishnamoorthi, Raghuraman. 2018. “Quantizing Deep Convolutional Networks for Efficient Inference: A Whitepaper.” arXiv Preprint arXiv:1806.08342.\n\nChen, Tianqi, Mu Li, Yutian Li, Min Lin, Naiyan Wang, Minjie Wang, Tianjun Xiao, Bing Xu, Chiyuan Zhang, and Zheng Zhang. 2015. “MXNet: A Flexible and Efficient Machine Learning Library for Heterogeneous Distributed Systems.” arXiv Preprint arXiv:1512.01274.\nThe above implementation factors determine the practical performance of optimization algorithms in deep learning systems, emphasizing the importance of tailoring memory, computational, and numerical strategies to the underlying hardware architecture (T. Chen et al. 2015).\n\n\nOptimizer Trade-offs\nThe evolution of optimization algorithms in neural network training reveals an important intersection between algorithmic efficiency and system performance. While optimizers were primarily developed to improve model convergence, their implementation significantly impacts memory usage, computational requirements, and hardware utilization.\n\n\n\nTable 8.3: Optimizer characteristics and system implications\n\n\n\n\n\n\n\n\n\n\n\n\nProperty\nSGD\nMomentum\nRMSprop\nAdam\n\n\n\n\nMemory Overhead\nNone\nVelocity terms\nSquared gradients\nBoth velocity and squared gradients\n\n\nMemory Cost\n\\(1\\times\\)\n\\(2\\times\\)\n\\(2\\times\\)\n\\(3\\times\\)\n\n\nAccess Pattern\nSequential\nSequential\nRandom\nRandom\n\n\nOperations/Parameter\n2\n3\n4\n5\n\n\nHardware Efficiency\nLow\nMedium\nHigh\nHighest\n\n\nConvergence Speed\nSlowest\nMedium\nFast\nFastest\n\n\n\n\n\n\nA deeper examination of popular optimization algorithms reveals their varying impacts on system resources. As shown in Table 8.3, each optimizer presents distinct trade-offs between memory usage, computational patterns, and convergence behavior. SGD maintains minimal memory overhead, requiring storage only for model parameters and current gradients. This lightweight memory footprint comes at the cost of slower convergence and potentially poor hardware utilization due to its sequential update nature.\nMomentum methods introduce additional memory requirements by storing velocity terms for each parameter, doubling the memory footprint compared to SGD. This increased memory cost brings improved convergence through better gradient estimation, while maintaining relatively efficient memory access patterns. The sequential nature of momentum updates allows for effective hardware prefetching and cache utilization.\nRMSprop adapts learning rates per parameter by tracking squared gradient statistics. Its memory overhead matches momentum methods, but its computation patterns become more irregular. The algorithm requires additional arithmetic operations for maintaining running averages and computing adaptive learning rates, increasing computational intensity from 3 to 4 operations per parameter.\nAdam combines the benefits of momentum and adaptive learning rates, but at the highest system resource cost. Table 8.3 reveals that it maintains both velocity terms and squared gradient statistics, tripling the memory requirements compared to SGD. The algorithm’s computational patterns involve 5 operations per parameter update, though these operations often utilize hardware more effectively due to their regular structure and potential for parallelization.\nTraining system designers must balance these trade-offs when selecting optimization strategies. Modern hardware architectures influence these decisions. GPUs excel at the parallel computations required by adaptive methods, while memory-constrained systems might favor simpler optimizers. The choice of optimizer affects not only training dynamics but also maximum feasible model size, achievable batch size, hardware utilization efficiency, and overall training time to convergence.\nModern training frameworks continue to evolve, developing techniques like optimizer state sharding, mixed-precision storage, and fused operations to better balance these competing demands. Understanding these system implications helps practitioners make informed decisions about optimization strategies based on their specific hardware constraints and training requirements.\n\n\n\n\n8.3.3 Backpropagation Mechanics\nThe backpropagation algorithm computes gradients by systematically moving backward through a neural network’s computational graph. While earlier discussions introduced backpropagation’s mathematical principles, implementing this algorithm in training systems requires careful management of memory, computation, and data flow.\n\nBasic Mechanics\nDuring the forward pass, each layer in a neural network performs computations and produces activations. These activations must be stored for use during the backward pass:\n\\[ z^{(l)} = W^{(l)}a^{(l-1)} + b^{(l)} \\] \\[ a^{(l)} = f(z^{(l)}) \\]\nwhere \\(z^{(l)}\\) represents the pre-activation values and \\(a^{(l)}\\) represents the activations at layer \\(l\\). The storage of these intermediate values creates specific memory requirements that scale with network depth and batch size.\nThe backward pass computes gradients by applying the chain rule, starting from the network’s output and moving toward the input:\n\\[ \\frac{\\partial L}{\\partial z^{(l)}} = \\frac{\\partial L}{\\partial a^{(l)}} \\odot f'(z^{(l)}) \\] \\[ \\frac{\\partial L}{\\partial W^{(l)}} = \\frac{\\partial L}{\\partial z^{(l)}} (a^{(l-1)})^T \\]\nEach gradient computation requires access to stored activations from the forward pass, creating a specific pattern of memory access and computation that training systems must manage efficiently.\n\n\nBackpropagation Mechanics\nNeural networks learn by adjusting their parameters to reduce errors. Backpropagation computes how much each parameter contributed to the error by systematically moving backward through the network’s computational graph. This process forms the computational core of the optimization algorithms discussed earlier.\nFor a network with parameters \\(W_i\\) at each layer, we need to compute \\(\\frac{\\partial L}{\\partial W_i}\\)—how much the loss L changes when we adjust each parameter. The computation builds on the core operations covered earlier: matrix multiplications and activation functions, but in reverse order. The chain rule provides a systematic way to organize these computations:\n\\[ \\frac{\\partial L_{full}}{\\partial L_{i}} = \\frac{\\partial A_{i}}{\\partial L_{i}} \\frac{\\partial L_{i+1}}{\\partial A_{i}} ... \\frac{\\partial A_{n}}{\\partial L_{n}} \\frac{\\partial L_{full}}{\\partial A_{n}} \\]\nThis equation reveals key requirements for training systems. Computing gradients for early layers requires information from all later layers, creating specific patterns in data storage and access. These patterns directly influence the efficiency of optimization algorithms like SGD or Adam discussed earlier. Modern training systems use autodifferentiation to handle these computations automatically, but the underlying system requirements remain the same.\n\n\nMemory Requirements\nTraining systems must maintain intermediate values (activations) from the forward pass to compute gradients during the backward pass. This requirement compounds the memory demands we saw with optimization algorithms. For each layer l, the system must store:\n\nInput activations from the forward pass\nOutput activations after applying layer operations\nLayer parameters being optimized\nComputed gradients for parameter updates\n\nConsider a batch of training examples passing through a network. The forward pass computes and stores:\n\\[ z^{(l)} = W^{(l)}a^{(l-1)} + b^{(l)} \\] \\[ a^{(l)} = f(z^{(l)}) \\]\nBoth \\(z^{(l)}\\) and \\(a^{(l)}\\) must be cached for the backward pass. This creates a multiplicative effect on memory usage: each layer’s memory requirement is multiplied by the batch size, and the optimizer’s memory overhead (discussed in the previous section) applies to each parameter.\nThe total memory needed scales with:\n\nNetwork depth (number of layers)\nLayer widths (number of parameters per layer)\nBatch size (number of examples processed together)\nOptimizer state (additional memory for algorithms like Adam)\n\nThis creates a complex set of trade-offs. Larger batch sizes enable more efficient computation and better gradient estimates for optimization, but require proportionally more memory for storing activations. More sophisticated optimizers like Adam can achieve faster convergence but require additional memory per parameter.\n\n\nMemory-Computation Trade-offs\nTraining systems must balance memory usage against computational efficiency. Each forward pass through the network generates a set of activations that must be stored for the backward pass. For a neural network with L layers, processing a batch of B examples requires storing:\n\\[ \\text{Memory per batch} = B \\times \\sum_{l=1}^L (s_l + a_l) \\]\nwhere \\(s_l\\) represents the size of intermediate computations (like \\(z^{(l)}\\)) and \\(a_l\\) represents the activation outputs at layer l.\nThis memory requirement compounds with the optimizer’s memory needs discussed in the previous section. The total memory consumption of a training system includes both the stored activations and the optimizer state:\n\\[ \\text{Total Memory} = \\text{Memory per batch} + \\text{Memory}_{\\text{optimizer}} \\]\nTo manage these substantial memory requirements, training systems use several sophisticated strategies. Gradient checkpointing is a basic approach, strategically recomputing some intermediate values during the backward pass rather than storing them. While this increases computational work, it can significantly reduce memory usage, enabling training of deeper networks or larger batch sizes on memory-constrained hardware (T. Chen et al. 2016).\n\nChen, Tianqi, Bing Xu, Chiyuan Zhang, and Carlos Guestrin. 2016. “Training Deep Nets with Sublinear Memory Cost.” CoRR abs/1604.06174 (April). http://arxiv.org/abs/1604.06174v2.\n\nJouppi, Norman P., Cliff Young, Nishant Patil, David Patterson, and et al. 2017. “In-Datacenter Performance Analysis of a Tensor Processing Unit.” Proceedings of the 44th Annual International Symposium on Computer Architecture (ISCA), 1–12.\nThe efficiency of these memory management strategies depends heavily on the underlying hardware architecture. GPU systems, with their high computational throughput but limited memory bandwidth, often encounter different bottlenecks than CPU systems. Memory bandwidth limitations on GPUs mean that even when sufficient storage exists, moving data between memory and compute units can become the primary performance constraint (Jouppi, Young, Patil, Patterson, and al. 2017).\nThese hardware considerations guide the implementation of backpropagation in modern training systems. Specialized memory-efficient algorithms for operations like convolutions compute gradients in tiles or chunks, adapting to available memory bandwidth. Dynamic memory management tracks the lifetime of intermediate values throughout the computation graph, deallocating memory as soon as tensors become unnecessary for subsequent computations (Paszke et al. 2019).\n\nPaszke, Adam, Sam Gross, Francisco Massa, and et al. 2019. “PyTorch: An Imperative Style, High-Performance Deep Learning Library.” Advances in Neural Information Processing Systems (NeurIPS) 32: 8026–37.\n\n\n\n8.3.4 System Implications\nEfficiently managing the forward pass, backward pass, and parameter updates requires a holistic understanding of how these operations interact with data loading, preprocessing pipelines, and hardware accelerators. For instance, matrix multiplications shape decisions about batch size, data parallelism, and memory allocation, while activation functions influence convergence rates and require careful trade-offs between computational efficiency and learning dynamics.\nThese operations set the stage for addressing the challenges of training pipeline architecture. From designing workflows for data preprocessing to employing advanced techniques like mixed-precision training, gradient accumulation, and checkpointing, their implications are far-reaching.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>AI Training</span>"
    ]
  },
  {
    "objectID": "contents/core/training/training.html#training-pipeline-architecture",
    "href": "contents/core/training/training.html#training-pipeline-architecture",
    "title": "8  AI Training",
    "section": "8.4 Training Pipeline Architecture",
    "text": "8.4 Training Pipeline Architecture\nA training pipeline is the framework that governs how raw data is transformed into a trained machine learning model. Within the confines of a single system, it orchestrates the steps necessary for data preparation, computational execution, and model evaluation. The design of such pipelines is critical to ensure that training is both efficient and reproducible, allowing machine learning workflows to operate reliably.\nAs shown in Figure 8.4, the training pipeline consists of three main components: the data pipeline for ingestion and preprocessing, the training loop that handles model updates, and the evaluation pipeline for assessing performance. These components work together in a coordinated manner, with processed batches flowing from the data pipeline to the training loop, and evaluation metrics providing feedback to guide the training process.\n\n\n\n\n\n\ngraph LR\n  A[\"📂 Data Pipeline&lt;br&gt;&lt;small&gt;Ingestion, Preprocessing, Batching&lt;/small&gt;\"] \n  --&gt;|Processed Batches| B[\"⚙️ Training Loop&lt;br&gt;&lt;small&gt;Forward Pass, Loss Calculation, Backward Pass&lt;/small&gt;\"]\n  B --&gt;|Evaluation Metrics| C[\"📊 Evaluation Pipeline&lt;br&gt;&lt;small&gt;Validation and Metrics Computation&lt;/small&gt;\"]\n  C --&gt;|Feedback| B\n\n\n\n\n\nFigure 8.4: Training pipeline showing the three main components. The arrows indicate the flow of data and feedback between components.\n\n\n\n\n\n\n8.4.1 Architectural Overview\nThe architecture of a training pipeline is organized around three interconnected components: the data pipeline, the training loop, and the evaluation pipeline. These components collectively process raw data, train the model, and assess its performance, ensuring that the training process is efficient and effective.\nThe data pipeline initiates the process by ingesting raw data and transforming it into a format suitable for the model. This data is passed to the training loop, where the model performs its core computations to learn from the inputs. Periodically, the evaluation pipeline assesses the model’s performance using a separate validation dataset. This modular structure ensures that each stage operates efficiently while contributing to the overall workflow.\n\nData Pipeline\nThe data pipeline manages the ingestion, preprocessing, and batching of data for training. Raw data is typically loaded from local storage and transformed dynamically during training to avoid redundancy and enhance diversity. For instance, image datasets may undergo preprocessing steps like normalization, resizing, and augmentation to improve the robustness of the model. These operations are performed in real time to minimize storage overhead and adapt to the specific requirements of the task (LeCun et al. 1998). Once processed, the data is packaged into batches and handed off to the training loop.\n\nLeCun, Yann, Leon Bottou, Genevieve B. Orr, and Klaus -Robert Müller. 1998. “Efficient BackProp.” In Neural Networks: Tricks of the Trade, 1524:9–50. Springer Berlin Heidelberg. https://doi.org/10.1007/3-540-49430-8\\_2.\n\n\nTraining Loop\nThe training loop is the computational core of the pipeline, where the model learns from the input data. Each iteration of the loop involves several key steps:\n\nDuring the forward pass, the model processes a batch of inputs to produce predictions. This is achieved by passing the data through the layers of the model, where mathematical operations such as matrix multiplications and activations transform the inputs into meaningful outputs.\nThe predictions are compared to the ground truth labels using a loss function, which quantifies the difference between the predicted and actual values. This loss value serves as a measure of the model’s performance.\nIn the backward pass, gradients are calculated for each parameter of the model using backpropagation. These gradients indicate how the parameters should be adjusted to reduce the loss. Finally, an optimization algorithm updates the model parameters, completing the training step.\n\nThis iterative process continues across multiple batches and epochs, gradually improving the model’s ability to make accurate predictions.\n\n\nEvaluation Pipeline\nThe evaluation pipeline provides periodic feedback on the model’s performance during training. Using a separate validation dataset, the model’s predictions are compared against known outcomes to compute metrics such as accuracy or loss. These metrics help to monitor progress and detect issues like overfitting or underfitting. Evaluation is typically performed at regular intervals, such as at the end of each epoch, ensuring that the training process aligns with the desired objectives.\n\n\nIntegration of Components\nThe data pipeline, training loop, and evaluation pipeline are tightly integrated to ensure a smooth and efficient workflow. Data preparation often overlaps with computation, such as when preprocessing the next batch while the current batch is being processed in the training loop. Similarly, the evaluation pipeline operates in tandem with training, providing insights that inform adjustments to the model or training procedure. This integration minimizes idle time for the system’s resources and ensures that training proceeds without interruptions.\n\n\n\n8.4.2 Data Pipeline\nThe data pipeline moves data from storage to computational devices during training. Like a highway system moving vehicles from neighborhoods to city centers, the data pipeline transports training data through multiple stages to reach computational resources.\nThe data pipeline running on the CPU transforms raw data stored on disk into processed batches ready for model training on the GPUs. For an image recognition model, the pipeline reads image files from storage, converts them to the correct format, applies preprocessing operations like resizing and normalization, and delivers them to GPUs for computation. Each stage in this process must work efficiently to maintain a smooth data flow during training, ensuring that the GPUs are consistently fed with data to maximize their utilization and minimize idle time.\n\n\n\n\n\n\nflowchart LR\n  subgraph Storage_Zone[Storage Zone]\n    Raw_Data[fa:fa-database Raw Data]\n  end\n\n  subgraph CPU_Zone[CPU Preprocessing Zone]\n    Format[fa:fa-cogs Format]\n    Process[fa:fa-cogs Process]\n    Batch[fa:fa-layer-group Batch]\n  end\n\n  subgraph Training_Zone[GPU Training Zone]\n    GPU1[fa:fa-microchip GPU 1]\n    GPU2[fa:fa-microchip GPU 2]\n    GPU3[fa:fa-microchip GPU 3]\n  end\n\n  Raw_Data --&gt; Format\n  Format --&gt; Process\n  Process --&gt; Batch\n  Batch --&gt;|Data| GPU1\n  Batch --&gt;|Data| GPU2\n  Batch --&gt;|Data| GPU3\n\n\n\n\n\nFigure 8.5: Data pipeline architecture illustrating the flow of data from raw storage through CPU preprocessing stages to GPU training units.\n\n\n\n\n\n\nCore Components\nThe performance of machine learning systems is fundamentally constrained by storage access speed, which determines the rate at which training data can be retrieved. This access speed is governed by two primary hardware constraints: disk bandwidth and network bandwidth. The maximum theoretical throughput is determined by the following relationship:\n\\[T_{storage} = min(B_{disk}, B_{network})\\]\nwhere \\(B_{disk}\\) is the physical disk bandwidth (the rate at which data can be read from storage devices) and \\(B_{network}\\) represents the network bandwidth (the rate of data transfer across distributed storage systems). Both quantities are measured in bytes per second.\nHowever, the actual throughput achieved during training operations typically falls below this theoretical maximum due to non-sequential data access patterns. The effective throughput can be expressed as:\n\\[T_{effective} = T_{storage} \\times F_{access}\\]\nwhere \\(F_{access}\\) represents the access pattern factor. In typical training scenarios, \\(F_{access}\\) approximates 0.1, indicating that effective throughput achieves only 10% of the theoretical maximum. This significant reduction occurs because storage systems are optimized for sequential access patterns rather than the random access patterns common in training procedures.\nThis relationship between theoretical and effective throughput has important implications for system design and training optimization. Understanding these constraints allows practitioners to make informed decisions about data pipeline architecture and training methodology.\n\n\nPreprocessing\nAs the data becomes available, data preprocessing transforms raw input data into a format suitable for model training. This process, traditionally implemented through Extract-Transform-Load (ETL) or Extract-Load-Transform (ELT) pipelines, is a critical determinant of training system performance. The throughput of preprocessing operations can be expressed mathematically as:\n\\[T_{preprocessing} = \\frac{N_{workers}}{t_{transform}}\\]\nThis equation captures two key factors:\n\n\\(N_{workers}\\) represents the number of parallel processing threads\n\\(t_{transform}\\) represents the time required for each transformation operation\n\nModern training architectures employ multiple processing threads to ensure preprocessing keeps pace with the consumption rates. This parallel processing approach is essential for maintaining efficient high processor utilization.\nThe final stage of preprocessing involves transferring the processed data to computational devices (typically GPUs). The overall training throughput is constrained by three factors, expressed as:\n\\[T_{training} = min(T_{preprocessing}, B_{GPU\\_transfer}, B_{GPU\\_compute})\\]\nwhere:\n\n\\(B_{GPU\\_transfer}\\) represents GPU memory bandwidth\n\\(B_{GPU\\_compute}\\) represents GPU computational throughput\n\nThis relationship illustrates a fundamental principle in training system design: the system’s overall performance is limited by its slowest component. Whether preprocessing speed, data transfer rates, or computational capacity, the bottleneck stage determines the effective training throughput of the entire system. Understanding these relationships enables system architects to design balanced training pipelines where preprocessing capacity aligns with computational resources, ensuring optimal resource utilization.\n\n\nSystem Implications\nThe relationship between data pipeline architecture and computational resources fundamentally determines the performance of machine learning training systems. This relationship can be simply expressed through a basic throughput equation:\n\\[T_{system} = min(T_{pipeline}, T_{compute})\\]\nwhere \\(T_{system}\\) represents the overall system throughput, constrained by both pipeline throughput (\\(T_{pipeline}\\)) and computational speed (\\(T_{compute}\\)).\nTo illustrate these constraints, consider image classification systems. The performance dynamics can be analyzed through two critical metrics. The GPU Processing Rate (\\(R_{GPU}\\)) represents the maximum number of images a GPU can process per second, determined by model architecture complexity and GPU hardware capabilities. The Pipeline Delivery Rate (\\(R_{pipeline}\\)) is the rate at which the data pipeline can deliver preprocessed images to the GPU.\nIn this case, at a high level, the system’s effective training speed is governed by the lower of these two rates. When \\(R_{pipeline}\\) is less than \\(R_{GPU}\\), the system experiences underutilization of GPU resources. The degree of GPU utilization can be expressed as:\n\\[\\text{GPU Utilization} = \\frac{R_{pipeline}}{R_{GPU}} \\times 100\\%\\]\nLet us consider an example. A ResNet-50 model implemented on modern GPU hardware might achieve a processing rate of 1000 images per second. However, if the data pipeline can only deliver 200 images per second, the GPU utilization would be merely 20%, meaning the GPU remains idle 80% of the time. This results in significantly reduced training efficiency. Importantly, this inefficiency persists even with more powerful GPU hardware, as the pipeline throughput becomes the limiting factor in system performance. This demonstrates why balanced system design, where pipeline and computational capabilities are well-matched, is crucial for optimal training performance.\n\n\nData Flows\nMachine learning systems manage complex data flows through multiple memory tiers while coordinating pipeline operations. The interplay between memory bandwidth constraints and pipeline execution directly impacts training performance. The maximum data transfer rate through the memory hierarchy is bounded by:\n\\[T_{memory} = min(B_{storage}, B_{system}, B_{accelerator})\\]\nWhere bandwidth varies significantly across tiers:\n\nStorage (\\(B_{storage}\\)): NVMe storage devices provide 1-2 GB/s\nSystem (\\(B_{system}\\)): Main memory transfers data at 50-100 GB/s\nAccelerator (\\(B_{accelerator}\\)): GPU memory achieves 900 GB/s or higher\n\nThese order-of-magnitude differences create distinct performance characteristics that must be carefully managed. The total time required for each training iteration comprises multiple pipelined operations:\n\\[t_{iteration} = max(t_{fetch}, t_{process}, t_{transfer})\\]\nThis equation captures three components: storage read time (\\(t_{fetch}\\)), preprocessing time (\\(t_{process}\\)), and accelerator transfer time (\\(t_{transfer}\\)).\nModern training architectures optimize performance by overlapping these operations. When one batch undergoes preprocessing, the system simultaneously fetches the next batch from storage while transferring the previously processed batch to accelerator memory.\nThis coordinated movement requires precise management of system resources, particularly memory buffers and processing units. The memory hierarchy must account for bandwidth disparities while maintaining continuous data flow. Effective pipelining minimizes idle time and maximizes resource utilization through careful buffer sizing and memory allocation strategies. The successful orchestration of these components enables efficient training across the memory hierarchy while managing the inherent bandwidth constraints of each tier.\n\n\nPractical Architectures\nThe ImageNet dataset serves as a canonical example for understanding data pipeline requirements in modern machine learning systems. This analysis examines system performance characteristics when training vision models on large-scale image datasets.\nStorage performance in practical systems follows a defined relationship between theoretical and practical throughput:\n\\[T_{practical} = 0.5 \\times B_{theoretical}\\]\nTo illustrate this relationship, consider an NVMe storage device with 3GB/s theoretical bandwidth. Such a device achieves approximately 1.5GB/s sustained read performance. However, the random access patterns required for training data shuffling further reduce this effective bandwidth by 90%. System designers must account for this reduction through careful memory buffer design.\nThe total memory requirements for the system scale with batch size according to the following relationship:\n\\[M_{required} = (B_{prefetch} + B_{processing} + B_{transfer}) \\times S_{batch}\\]\nIn this equation, \\(B_{prefetch}\\) represents memory allocated for data prefetching, \\(B_{processing}\\) represents memory required for active preprocessing operations, \\(B_{transfer}\\) represents memory allocated for accelerator transfers, and \\(S_{batch}\\) represents the training batch size.\nPreprocessing operations introduce additional computational requirements. Common operations such as image resizing, augmentation, and normalization consume CPU resources. These preprocessing operations must satisfy a fundamental time constraint:\n\\[t_{preprocessing} &lt; t_{GPU\\_compute}\\]\nThis inequality plays a crucial role in determining system efficiency. When preprocessing time exceeds GPU computation time, accelerator utilization decreases proportionally. The relationship between preprocessing and computation time thus establishes fundamental efficiency limits in training system design.\n\n\n\n8.4.3 Forward Pass\nThe forward pass is the phase where input data propagates through the model, layer by layer, to generate predictions. Each layer performs mathematical operations such as matrix multiplications and activations, progressively transforming the data into meaningful outputs. While the conceptual flow of the forward pass is straightforward, it poses several system-level challenges that are critical for efficient execution.\n\nCompute Operations\nThe forward pass in deep neural networks orchestrates a diverse set of computational patterns, each optimized for specific neural network operations. Understanding these patterns and their efficient implementation is fundamental to machine learning system design.\nAt their core, neural networks rely heavily on matrix multiplications, particularly in fully connected layers. The basic transformation follows the form:\n\\[\nz^{(l)} = W^{(l)}a^{(l-1)} + b^{(l)}\n\\]\nHere, \\(W^{(l)}\\) represents the weight matrix, \\(a^{(l-1)}\\) contains activations from the previous layer, and \\(b^{(l)}\\) is the bias vector. For a layer with \\(N\\) neurons in the current layer and \\(M\\) neurons in the previous layer, processing a batch of \\(B\\) samples requires \\(N \\times M \\times B\\) floating-point operations. A typical layer with dimensions of 512×1024 processing a batch of 64 samples executes over 33 million operations.\nModern neural architectures extend beyond these basic matrix operations to include specialized computational patterns. Convolutional networks, for instance, perform systematic kernel operations across input tensors. Consider a typical input tensor of dimensions \\(64 \\times 224 \\times 224 \\times 3\\) (batch size × height × width × channels) processed by \\(7 \\times 7\\) kernels. Each position requires 147 multiply-accumulate operations, and with 64 filters operating across \\(218 \\times 218\\) spatial dimensions, the computational demands become substantial.\nTransformer architectures introduce attention mechanisms, which compute similarity scores between sequences. These operations combine matrix multiplications with softmax normalization, requiring efficient broadcasting and reduction operations across varying sequence lengths. The computational pattern here differs significantly from convolutions, demanding flexible execution strategies from hardware accelerators.\nThroughout these networks, element-wise operations play a crucial supporting role. Activation functions like ReLU and sigmoid transform values independently. While conceptually simple, these operations can become bottlenecked by memory bandwidth rather than computational capacity, as they perform relatively few calculations per memory access. Batch normalization presents similar challenges, computing statistics and normalizing values across batch dimensions while creating synchronization points in the computation pipeline.\nModern hardware accelerators, particularly GPUs, optimize these diverse computations through massive parallelization. However, achieving peak performance requires careful attention to hardware architecture. GPUs process data in fixed-size blocks of threads called warps (in NVIDIA architectures) or wavefronts (in AMD architectures). Peak efficiency occurs when matrix dimensions align with these hardware-specific sizes. For instance, NVIDIA GPUs typically achieve optimal performance when processing matrices aligned to 32×32 dimensions.\nLibraries like cuDNN address these challenges by providing optimized implementations for each operation type. These systems dynamically select algorithms based on input dimensions, hardware capabilities, and memory constraints. The selection process balances computational efficiency with memory usage, often requiring empirical measurement to determine optimal configurations for specific hardware setups.\nThe relationship between batch size and hardware utilization illuminates these trade-offs. When batch size decreases from 32 to 16, GPU utilization often drops due to incomplete warp occupation. While larger batch sizes improve hardware utilization, memory constraints in modern architectures may necessitate smaller batches, creating a fundamental tension between computational efficiency and memory usage. This balance exemplifies a central challenge in machine learning systems: maximizing computational throughput within hardware resource constraints.\n\n\nMemory Management\nMemory management is a critical challenge in general, but it is particularly crucial during the forward pass when intermediate activations must be stored for subsequent backward propagation. The total memory footprint grows with both network depth and batch size, following a basic relationship.\n\\[\n\\text{Total Memory} \\sim B \\times \\sum_{l=1}^{L} A_l\n\\]\nwhere \\(B\\) represents the batch size, \\(L\\) is the number of layers, and \\(A_l\\) represents the activation size at layer \\(l\\). This simple equation masks considerable complexity in practice.\nConsider ResNet-50 processing images at 224×224 resolution with a batch size of 32. The initial convolutional layer produces activation maps of dimension 112×112×64. Using single-precision floating-point format (4 bytes per value), this single layer’s activation storage requires approximately 98 MB. As the network progresses through its 50 layers, the dimensions of these activation maps change—typically decreasing spatially while increasing in channel depth—creating a cumulative memory demand that can reach several gigabytes.\nModern GPUs typically provide between 16 and 24 GB of memory, which must accommodate not just these activations but also model parameters, gradients, and optimization states. This constraint has motivated several memory management strategies:\nActivation checkpointing trades computational cost for memory efficiency by strategically discarding and recomputing activations during the backward pass. Rather than storing all intermediate values, the system maintains checkpoints at selected layers. During backpropagation, it regenerates necessary activations from these checkpoints. While this approach can reduce memory usage by 50% or more, it typically increases computation time by 20-30%.\nMixed precision training offers another approach to memory efficiency. By storing activations in half-precision (FP16) format instead of single-precision (FP32), memory requirements are immediately halved. Modern hardware architectures provide specialized support for these reduced-precision operations, often maintaining computational throughput while saving memory.\nThe relationship between batch size and memory usage creates practical trade-offs in training regimes. While larger batch sizes can improve computational efficiency, they proportionally increase memory demands. A machine learning practitioner might start with large batch sizes during initial development on smaller networks, then adjust downward when scaling to deeper architectures or when working with memory-constrained hardware.\nThis memory management challenge becomes particularly acute in state-of-the-art models. Recent transformer architectures can require tens of gigabytes just for activations, necessitating sophisticated memory management strategies or distributed training approaches. Understanding these memory constraints and management strategies proves essential for designing and deploying machine learning systems effectively.\n\n\n\n8.4.4 Backward Pass\n\nCompute Operations\nThe backward pass involves processing parameter gradients in reverse order through the network’s layers. Computing these gradients requires matrix operations that demand significant memory and processing power.\nNeural networks store activation values from each layer during the forward pass. Computing gradients combines these stored activations with gradient signals to generate weight updates. This design requires twice the memory compared to forward computation. Consider the gradient computation for a layer’s weights:\n\\[\n\\frac{\\partial L}{\\partial W^{(l)}} = \\delta^{(l)} \\cdot \\left(a^{(l-1)}\\right)^T\n\\]\nThe gradient signals \\(\\delta^{(l)}\\) at layer \\(l\\) multiply with transposed activations \\(a^{(l-1)}\\) from layer \\(l-1\\). This matrix multiplication forms the primary computational load. For example, in a layer with 1000 input features and 100 output features, computing gradients requires multiplying matrices of size 100 × batch_size and batch_size × 1000, resulting in millions of floating-point operations.\n\n\nMemory Operations\nThe backward pass moves large amounts of data between memory and compute units. Each time a layer computes gradients, it orchestrates a sequence of memory operations. The GPU first loads stored activations from memory, then reads incoming gradient signals, and finally writes the computed gradients back to memory.\nTo understand the scale of these memory transfers, consider a convolutional layer processing a batch of 64 images. Each image measures 224 × 224 pixels with 3 color channels. The activation maps alone occupy 0.38 GB of memory, storing 64 copies of the input images. The gradient signals expand this memory usage significantly - they require 8.1 GB to hold gradients for each of the layer’s 64 filters. Even the weight gradients, which only store updates for the convolutional kernels, need 0.037 GB3.\n\n3 Memory calculations: * Activation maps: 64 × 224 × 224 × 3 × 4 bytes = 0.38 GB * Gradient signals: 64 × 224 × 224 × 64 × 4 bytes = 8.1 GB * Weight gradients: 7 × 7 × 3 × 64 × 4 bytes = 0.037 GBMoreover, the backward pass in neural networks require coordinated data movement through a hierarchical memory system. During backpropagation, each computation requires specific activation values from the forward pass, creating a pattern of data movement between memory levels. This movement pattern shapes the performance characteristics of neural network training.\nThese backward pass computations operate across a memory hierarchy that balances speed and capacity requirements. When computing gradients, the processor must retrieve activation values stored in high-bandwidth memory (HBM) or system memory, transfer them to fast static RAM (SRAM) for computation, and write results back to larger storage. Each gradient calculation triggers this sequence of memory transfers, making memory access patterns a key factor in backward pass performance. The frequent transitions between memory levels introduce latency that accumulates across the backward pass computation chain.\n\n\nReal-World Training Considerations\nConsider training a ResNet-50 model on the ImageNet dataset with a batch of 64 images. The first convolutional layer applies 64 filters of size 7 × 7 to RGB images sized 224 × 224. During the backward pass, this single layer’s computation requires:\n\\[\n\\text{Memory per image} = 224 \\times 224 \\times 64 \\times 4 \\text{ bytes}\n\\]\nThe total memory requirement multiplies by the batch size of 64, reaching approximately 3.2 GB just for storing gradients. When we add memory for activations, weight updates, and intermediate computations, a single layer approaches the memory limits of many GPUs.\nDeeper in the network, layers with more filters demand even greater resources. A mid-network convolutional layer might use 256 filters, quadrupling the memory and computation requirements. The backward pass must manage these resources while maintaining efficient computation. Each layer’s computation can only begin after receiving gradient signals from the subsequent layer, creating a strict sequential dependency in memory usage and computation patterns.\nThis dependency means the GPU must maintain a large working set of memory throughout the backward pass. As gradients flow backward through the network, each layer temporarily requires peak memory usage during its computation phase. The system cannot release this memory until the layer completes its gradient calculations and passes the results to the previous layer.\n\n\n\n8.4.5 Parameter Updates and Optimizers\nThe process of updating model parameters is a fundamental operation in machine learning systems. During training, after gradients are computed in the backward pass, the system must allocate and manage memory for both the parameters and their gradients, then perform the update computations. The choice of optimizer determines not only the mathematical update rule, but also the system resources required for training.\nConsider the parameter update process in a machine learning framework:\nloss.backward()  # Compute gradients\noptimizer.step() # Update parameters\nThese operations initiate a sequence of memory accesses and computations. The system must load parameters from memory, compute updates using the stored gradients, and write the modified parameters back to memory. Different optimizers vary in their memory requirements and computational patterns, directly affecting system performance and resource utilization.\n\nMemory Requirements\nGradient descent, the most basic optimization algorithm that we discussed earlier, illustrates the fundamental memory and computation patterns in parameter updates. From a systems perspective, each parameter update must:\n\nRead the current parameter value from memory\nAccess the computed gradient from memory\nPerform the multiplication and subtraction operations\nWrite the new parameter value back to memory\n\nBecause gradient descent only requires memory for storing parameters and gradients, it has relatively low memory overhead compared to more complex optimizers. However, more advanced optimizers introduce additional memory requirements and computational complexity. For example, as we discussed previously, Adam maintains two extra vectors for each parameter: one for the first moment (the moving average of gradients) and one for the second moment (the moving average of squared gradients). This triples the memory usage but can lead to faster convergence. Consider the situation where there are 100,000 parameters, and each gradient requires 4 bytes (32 bits):\n\nGradient Descent: \\(100,000 \\times 4 \\, \\text{bytes} = 400,000 \\, \\text{bytes} = 0.4 \\, \\text{MB}\\)\nAdam: \\(3 \\times 100,000 \\times 4 \\, \\text{bytes} = 1,200,000 \\, \\text{bytes} = 1.2 \\, \\text{MB}\\)\n\n\n\nComputational Load\nThe computational cost of parameter updates also depends on the optimizer’s complexity. For gradient descent, each update involves simple gradient calculation and application. More sophisticated optimizers like Adam require additional calculations, such as computing running averages of gradients and their squares. This increases the computational load per parameter update.\nThe efficiency of these computations on modern hardware like GPUs and TPUs depends on how well the optimizer’s operations can be parallelized. While matrix operations in Adam may be efficiently handled by these accelerators, some operations in complex optimizers might not parallelize well, potentially leading to hardware underutilization.\nIn summary, the choice of optimizer directly impacts both system memory requirements and computational load. More sophisticated optimizers often trade increased memory usage and computational complexity for potentially faster convergence, presenting important considerations for system design and resource allocation in ML systems.\n\n\nBatch Size and Parameter Updates\nBatch size, a critical hyperparameter in machine learning systems, significantly influences the parameter update process, memory usage, and hardware efficiency. It determines the number of training examples processed in a single iteration before the model parameters are updated.\nLarger batch sizes generally provide more accurate gradient estimates, potentially leading to faster convergence and more stable parameter updates. However, they also increase memory demands proportionally:\n\\[\n\\text{Memory for Batch} = \\text{Batch Size} \\times \\text{Size of One Training Example}\n\\]\nThis increase in memory usage directly affects the parameter update process, as it determines how much data is available for computing gradients in each iteration.\nLarger batches tend to improve hardware utilization, particularly on GPUs and TPUs optimized for parallel processing. This can lead to more efficient parameter updates and faster training times, provided sufficient memory is available.\nHowever, there’s a trade-off to consider. While larger batches can improve computational efficiency by allowing more parallel computations during gradient calculation and parameter updates, they also require more memory. On systems with limited memory, this might necessitate reducing the batch size, potentially slowing down training or leading to less stable parameter updates.\nThe choice of batch size interacts with various aspects of the optimization process. For instance, it affects the frequency of parameter updates: larger batches result in less frequent but potentially more impactful updates. Additionally, batch size influences the behavior of adaptive optimization algorithms, which may need to be tuned differently depending on the batch size. In distributed training, which we discuss later, batch size often determines the degree of data parallelism, impacting how gradient computations and parameter updates are distributed across devices.\nDetermining the optimal batch size involves balancing these factors within hardware constraints. It often requires experimentation to find the sweet spot that maximizes both learning efficiency and hardware utilization while ensuring effective parameter updates.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>AI Training</span>"
    ]
  },
  {
    "objectID": "contents/core/training/training.html#training-pipeline-optimizations",
    "href": "contents/core/training/training.html#training-pipeline-optimizations",
    "title": "8  AI Training",
    "section": "8.5 Training Pipeline Optimizations",
    "text": "8.5 Training Pipeline Optimizations\nEfficient training of machine learning models is constrained by bottlenecks in data transfer, computation, and memory usage. These limitations manifest in specific ways: data transfer delays occur when loading training batches from disk to GPU memory, computational bottlenecks arise during matrix operations in forward and backward passes, and memory constraints emerge when storing large intermediate values like activation maps.\nThese bottlenecks often lead to underutilized hardware, prolonged training times, and restricted model scalability. For machine learning practitioners, understanding and implementing pipeline optimizations enables training of larger models, faster experimentation cycles, and more efficient use of available computing resources.\nHere, we explore three widely adopted optimization strategies that address key performance bottlenecks in training pipelines:\n\nPrefetching and Overlapping: Techniques to minimize data transfer delays and maximize GPU utilization.\nMixed-Precision Training: A method to reduce memory demands and computational load using lower precision formats.\nGradient Accumulation and Checkpointing: Strategies to overcome memory limitations during backpropagation and parameter updates.\n\nEach technique is discussed in detail, covering its mechanics, advantages, and practical considerations.\n\n8.5.1 Prefetching and Overlapping\nTraining machine learning models involves significant data movement between storage, memory, and computational units. The data pipeline consists of sequential transfers: from disk storage to CPU memory, CPU memory to GPU memory, and through the GPU processing units. In standard implementations, each transfer must complete before the next begins, as shown in Figure 8.6, resulting in computational inefficiencies.\n\n\n\n\n\n\n---\ndisplayMode: compact\n---\ngantt\n    dateFormat  HH:mm:ss\n    axisFormat  %M:%S\n\n    section Open\n    Open 1       :crit, done, 00:00:00, 00:00:05\n    Open 2       :crit, done, 00:00:50, 00:00:55\n\n    section Read\n    Read 1       :crit, active, 00:00:05, 00:00:10\n    Read 2       :crit, active, 00:00:15, 00:00:20\n    Read 3       :crit, active, 00:00:25, 00:00:30\n    Read 4       :crit, active, 00:00:55, 00:01:00\n    Read 5       :crit, active, 00:01:05, 00:01:10\n    Read 6       :crit, active, 00:01:15, 00:01:20\n\n    section Train\n    Train 1      :crit, 00:00:10, 00:00:15\n    Train 2      :crit, 00:00:20, 00:00:25\n    Train 3      :crit, 00:00:30, 00:00:35\n    Train 4      :crit, 00:01:00, 00:01:05\n    Train 5      :crit, 00:01:10, 00:01:15\n    Train 6      :crit, 00:01:20, 00:01:25\n\n    section Epoch\n    Epoch 1      :00:00:00, 00:00:40\n    Epoch 2      :00:00:50, 00:01:30\n\n\n\n\nFigure 8.6: Naive data fetching implementation.\n\n\n\n\n\nPrefetching addresses these inefficiencies by loading data into memory before its scheduled computation time. During the processing of the current batch, the system loads and prepares subsequent batches, maintaining a consistent supply of ready data (Abadi et al. 2015).\n\nAbadi, Martín, Ashish Agarwal, Paul Barham, et al. 2015. “TensorFlow: Large-Scale Machine Learning on Heterogeneous Systems.” Google Brain.\nOverlapping builds upon prefetching by coordinating multiple pipeline stages to execute concurrently. The system processes the current batch while simultaneously preparing future batches through data loading and preprocessing operations. This coordination establishes a continuous data flow through the training pipeline, as illustrated in Figure 8.7.\n\n\n\n\n\n\n\n---\ndisplayMode: compact\n---\ngantt\n    dateFormat  HH:mm:ss\n    axisFormat  %M:%S\n\n    section Open\n    Open 1       :crit, done, 00:00:00, 00:00:02\n    Open 2       :crit, done, 00:00:25, 00:00:27\n\n    section Read\n    Read 1       :crit, active, 00:00:02, 00:00:05\n    Read 2       :crit, active, 00:00:05, 00:00:07\n    Read 3       :crit, active, 00:00:07, 00:00:10\n    Read 4       :crit, active, 00:00:27, 00:00:30\n    Read 5       :crit, active, 00:00:30, 00:00:32\n    Read 6       :crit, active, 00:00:32, 00:00:35\n\n    section Train\n    Train 1      :crit, 00:00:05, 00:00:07\n    Train 2      :crit, 00:00:07, 00:00:10\n    Train 3      :crit, 00:00:10, 00:00:12\n    Train 4      :crit, 00:00:30, 00:00:32\n    Train 5      :crit, 00:00:32, 00:00:35\n    Train 6      :crit, 00:00:35, 00:00:37\n\n    section Epoch\n    Epoch 1      :00:00:00, 00:00:15\n    Epoch 2      :00:00:25, 00:00:40\n\n\n\n\nFigure 8.7: Parallel fetching and overlapping implementation. The job finishes at 00:40 seconds, instead of 01:30 seconds as in Figure 8.6.\n\n\n\n\n\nThese optimization techniques demonstrate particular value in scenarios involving large-scale datasets, preprocessing-intensive data, multi-GPU training configurations, or high-latency storage systems. The following section examines the specific mechanics of implementing these techniques in modern training systems.\n\nMechanics\nPrefetching and overlapping optimize the training pipeline by enabling different stages of data processing and computation to operate concurrently rather than sequentially. These techniques maximize resource utilization by addressing bottlenecks in data transfer and preprocessing.\nAs you recall, training data undergoes three main stages: retrieval from storage, transformation into a suitable format, and utilization in model training. An unoptimized pipeline executes these stages sequentially. The GPU remains idle during data fetching and preprocessing, waiting for data preparation to complete. This sequential execution creates significant inefficiencies in the training process.\nPrefetching eliminates waiting time by loading data asynchronously during model computation. Data loaders operate as separate threads or processes, preparing the next batch while the current batch trains. This ensures immediate data availability for the GPU when the current batch completes.\nOverlapping extends this efficiency by coordinating all three pipeline stages simultaneously. As the GPU processes one batch, preprocessing begins on the next batch, while data fetching starts for the subsequent batch. This coordination maintains constant activity across all pipeline stages.\nModern machine learning frameworks implement these techniques through built-in utilities. PyTorch’s DataLoader class demonstrates this implementation:\n\nPyTorch\n\n\nloader = DataLoader(dataset, batch_size=32, num_workers=4, prefetch_factor=2)\n\n\n\nThe parameters num_workers and prefetch_factor control parallel processing and data buffering. Multiple worker processes handle data loading and preprocessing concurrently, while prefetch_factor determines the number of batches prepared in advance.\nBuffer management plays a key role in pipeline efficiency. The prefetch buffer size requires careful tuning to balance resource utilization. A buffer that is too small causes the GPU to wait for data preparation, reintroducing the idle time these techniques aim to eliminate. Conversely, allocating an overly large buffer consumes memory that could otherwise store model parameters or larger batch sizes.\nThe implementation relies on effective CPU-GPU coordination. The CPU manages data preparation tasks while the GPU handles computation. This division of labor, combined with storage I/O operations, creates an efficient pipeline that minimizes idle time across hardware resources.\nThese optimization techniques yield particular benefits in scenarios involving slow storage access, complex data preprocessing, or large datasets. The next section examines the specific advantages these techniques offer in different training contexts.\n\n\nBenefits\nPrefetching and overlapping are powerful techniques that significantly enhance the efficiency of training pipelines by addressing key bottlenecks in data handling and computation. To illustrate the impact of these benefits, Table 8.4 presents the following comparison:\n\n\n\nTable 8.4: Comparison of training pipeline characteristics with and without prefetching and overlapping.\n\n\n\n\n\n\n\n\n\n\nAspect\nTraditional Pipeline\nWith Prefetching & Overlapping\n\n\n\n\nGPU Utilization\nFrequent idle periods\nNear-constant utilization\n\n\nTraining Time\nLonger due to sequential operations\nReduced through parallelism\n\n\nResource Usage\nOften suboptimal\nMaximized across available hardware\n\n\nScalability\nLimited by slowest component\nAdaptable to various bottlenecks\n\n\n\n\n\n\nOne of the most critical advantages of these methods is the improvement in GPU utilization. In traditional, unoptimized pipelines, the GPU often remains idle while waiting for data to be fetched and preprocessed. This idle time creates inefficiencies, especially in workflows where data augmentation or preprocessing involves complex transformations. By introducing asynchronous data loading and overlapping, these techniques ensure that the GPU consistently has data ready to process, eliminating unnecessary delays.\nAnother important benefit is the reduction in overall training time. Prefetching and overlapping allow the computational pipeline to operate continuously, with multiple stages working simultaneously rather than sequentially. For example, while the GPU processes the current batch, the data loader fetches and preprocesses the next batch, ensuring a steady flow of data through the system. This parallelism minimizes latency between training iterations, allowing for faster completion of training cycles, particularly in scenarios involving large-scale datasets.\nAdditionally, these techniques are highly scalable and adaptable to various hardware configurations. Prefetching buffers and overlapping mechanisms can be tuned to match the specific requirements of a system, whether the bottleneck lies in slow storage, limited network bandwidth, or computational constraints. By aligning the data pipeline with the capabilities of the underlying hardware, prefetching and overlapping maximize resource utilization, making them invaluable for large-scale machine learning workflows.\nOverall, prefetching and overlapping directly address some of the most common inefficiencies in training pipelines. By optimizing data flow and computation, these methods not only improve hardware efficiency but also enable the training of more complex models within shorter timeframes.\n\n\nUse Cases\nPrefetching and overlapping are highly versatile techniques that can be applied across various machine learning domains and tasks to enhance pipeline efficiency. Their benefits are most evident in scenarios where data handling and preprocessing are computationally expensive or where large-scale datasets create potential bottlenecks in data transfer and loading.\nOne of the primary use cases is in computer vision, where datasets often consist of high-resolution images requiring extensive preprocessing. Tasks such as image classification, object detection, or semantic segmentation typically involve operations like resizing, normalization, and data augmentation, all of which can significantly increase preprocessing time. By employing prefetching and overlapping, these operations can be carried out concurrently with computation, ensuring that the GPU remains busy during the training process.\nFor example, a typical image classification pipeline might include random cropping (10ms), color jittering (15ms), and normalization (5ms). Without prefetching, these 30ms of preprocessing would delay each training step. Prefetching allows these operations to occur during the previous batch’s computation.\nNatural language processing (NLP) workflows also benefit from these techniques, particularly when working with large corpora of text data. For instance, preprocessing text data involves tokenization (converting words to numbers), padding sequences to equal length, and potentially subword tokenization. In a BERT model training pipeline, these steps might process thousands of sentences per batch. Prefetching allows this text processing to happen concurrently with model training. Prefetching ensures that these transformations occur in parallel with training, while overlapping optimizes data transfer and computation. This is especially useful in transformer-based models like BERT or GPT, which require consistent throughput to maintain efficiency given their high computational demand.\nDistributed training systems, which we will discuss next, involve multiple GPUs or nodes, present another critical application for prefetching and overlapping. In distributed setups, network latency and data transfer rates often become the primary bottleneck. Prefetching mitigates these issues by ensuring that data is ready and available before it is required by any specific GPU. Overlapping further optimizes distributed training pipelines by coordinating the data preprocessing on individual nodes while the central computation continues, thus reducing overall synchronization delays.\nBeyond these domains, prefetching and overlapping are particularly valuable in workflows involving large-scale datasets stored on remote or cloud-based systems. When training on cloud platforms, the data may need to be fetched over a network or from distributed storage, which introduces additional latency. Using prefetching and overlapping in such cases helps minimize the impact of these delays, ensuring that training proceeds smoothly despite slower data access speeds.\nThese use cases illustrate how prefetching and overlapping address inefficiencies in various machine learning pipelines. By optimizing the flow of data and computation, these techniques enable faster, more reliable training workflows across a wide range of applications.\n\n\nChallenges and Trade-offs\nWhile prefetching and overlapping are powerful techniques for optimizing training pipelines, their implementation comes with certain challenges and trade-offs. Understanding these limitations is crucial for effectively applying these methods in real-world machine learning workflows.\nOne of the primary challenges is the increased memory usage that accompanies prefetching and overlapping. By design, these techniques rely on maintaining a buffer of prefetched data batches, which requires additional memory resources. For large datasets or high-resolution inputs, this memory demand can become significant, especially when training on GPUs with limited memory capacity. If the buffer size is not carefully tuned, it may lead to out-of-memory errors, forcing practitioners to reduce batch sizes or adjust other parameters, which can impact overall efficiency.\nFor example, with a prefetch factor of 2 and batch size of 256 high-resolution images (1024x1024 pixels), the buffer might require an additional 2GB of GPU memory. This becomes particularly challenging when training vision models that already require significant memory for their parameters and activations.\nAnother difficulty lies in tuning the parameters that control prefetching and overlapping. Settings such as num_workers and prefetch_factor in PyTorch, or buffer sizes in other frameworks, need to be optimized for the specific hardware and workload. For instance, increasing the number of worker threads can improve throughput up to a point, but beyond that, it may lead to contention for CPU resources or even degrade performance due to excessive context switching. Determining the optimal configuration often requires empirical testing, which can be time-consuming. A common starting point is to set num_workers to the number of CPU cores available. However, on a 16-core system processing large images, using all cores for data loading might leave insufficient CPU resources for other essential operations, potentially slowing down the entire pipeline.\nDebugging also becomes more complex in pipelines that employ prefetching and overlapping. Asynchronous data loading and multithreading or multiprocessing introduce potential race conditions, deadlocks, or synchronization issues. Diagnosing errors in such systems can be challenging because the execution flow is no longer straightforward. Developers may need to invest additional effort into monitoring, logging, and debugging tools to ensure that the pipeline operates reliably.\nMoreover, there are scenarios where prefetching and overlapping may offer minimal benefits. For instance, in systems where storage access or network bandwidth is significantly faster than the computation itself, these techniques might not noticeably improve throughput. In such cases, the additional complexity and memory overhead introduced by prefetching may not justify its use.\nFinally, prefetching and overlapping require careful coordination across different components of the training pipeline, such as storage, CPUs, and GPUs. Poorly designed pipelines can lead to imbalances where one stage becomes a bottleneck, negating the advantages of these techniques. For example, if the data loading process is too slow to keep up with the GPU’s processing speed, the benefits of overlapping will be limited.\nDespite these challenges, prefetching and overlapping remain essential tools for optimizing training pipelines when used appropriately. By understanding and addressing their trade-offs, practitioners can implement these techniques effectively, ensuring smoother and more efficient machine learning workflows.\n\n\n\n8.5.2 Mixed-Precision Training\nMixed-precision training combines different numerical precisions during model training to optimize computational efficiency. This approach uses combinations of 32-bit floating-point (FP32), 16-bit floating-point (FP16), and brain floating-point (bfloat16) formats to reduce memory usage and speed up computation while preserving model accuracy (Micikevicius et al. 2017a; Wang and Kanwar 2019).\n\nMicikevicius, Paulius, Sharan Narang, Jonah Alben, Gregory Diamos, Erich Elsen, David Garcia, Boris Ginsburg, et al. 2017a. “Mixed Precision Training.” arXiv Preprint arXiv:1710.03740, October. http://arxiv.org/abs/1710.03740v3.\n\nWang, Y., and P. Kanwar. 2019. “BFloat16: The Secret to High Performance on Cloud TPUs.” Google Cloud Blog.\nA neural network trained in FP32 requires 4 bytes per parameter, while both FP16 and bfloat16 use 2 bytes. For a model with \\(10^9\\) parameters, this reduction cuts memory usage from 4GB to 2GB. This memory reduction enables larger batch sizes and deeper architectures on the same hardware.\nThe numerical precision differences between these formats shape their use cases. FP32 represents numbers from approximately \\(\\pm1.18 \\times 10^{-38}\\) to \\(\\pm3.4 \\times 10^{38}\\) with 7 decimal digits of precision. FP16 ranges from \\(\\pm6.10 \\times 10^{-5}\\) to \\(\\pm65,504\\) with 3-4 decimal digits of precision. Bfloat16, developed by Google Brain, maintains the same dynamic range as FP32 (\\(\\pm1.18 \\times 10^{-38}\\) to \\(\\pm3.4 \\times 10^{38}\\)) but with reduced precision (3-4 decimal digits). This range preservation makes bfloat16 particularly suited for deep learning training, as it handles large and small gradients more effectively than FP16.\nThe hybrid approach proceeds in three main phases, as illustrated in Figure 8.8. During the forward pass, input data converts to reduced precision (FP16 or bfloat16), and matrix multiplications execute in this format, including activation function computations. In the gradient computation phase, the backward pass calculates gradients in reduced precision, but results are stored in FP32 master weights. Finally, during weight updates, the optimizer updates the main weights in FP32, and these updated weights convert back to reduced precision for the next forward pass.\n\n\n\n\n\n\nflowchart\n\n    Model[Model FP16] --&gt; Grad_FP16[Gradients FP16]\n    Grad_FP16 --&gt; Grad_FP32[Gradients FP32]\n\n    %% Subgraph for the Optimizer\n    subgraph Optimizer[Optimizer]\n        Grad_FP32 --&gt; Optimizer_Core[Optimizer Core FP32] \n        Optimizer_Core --&gt; Weights_FP32[Weights FP32]\n    end\n\n    Weights_FP32 --&gt; Weights_FP16[Weights FP16]\n    Weights_FP16 --&gt; Model\n\n\n\n\n\nFigure 8.8: Mixed preicision training flow.\n\n\n\n\n\nModern hardware architectures are specifically designed to accelerate reduced precision computations. GPUs from NVIDIA include Tensor Cores4 optimized for FP16 and bfloat16 operations (Jia et al. 2018). Google’s TPUs natively support bfloat16, as this format was specifically designed for machine learning workloads. These architectural optimizations typically enable an order of magnitude higher computational throughput for reduced precision operations compared to FP32, making mixed-precision training particularly efficient on modern hardware.\n4 Tensor Cores: NVIDIA GPU units that accelerate matrix operations with reduced precision formats like FP16 and bfloat16, boosting deep learning performance by enabling parallel computations.\nJia, Xianyan, Shutao Song, Wei He, Yangzihao Wang, Haidong Rong, Feihu Zhou, Liqiang Xie, et al. 2018. “Highly Scalable Deep Learning Training System with Mixed-Precision: Training ImageNet in Four Minutes.” arXiv Preprint arXiv:1807.11205, July. http://arxiv.org/abs/1807.11205v1.\n\nFP16 Computation\nThe majority of operations in mixed-precision training, such as matrix multiplications and activation functions, are performed in FP16. The reduced precision allows these calculations to be executed faster and with less memory consumption compared to FP32. FP16 operations are particularly effective on modern GPUs equipped with Tensor Cores, which are designed to accelerate computations involving half-precision values. These cores perform FP16 operations natively, resulting in significant speedups.\n\n\nFP32 Accumulation\nWhile FP16 is efficient, its limited precision can lead to numerical instability, especially in critical operations like gradient updates. To mitigate this, mixed-precision training retains FP32 precision for certain steps, such as weight updates and gradient accumulation. By maintaining higher precision for these calculations, the system avoids the risk of gradient underflow or overflow, ensuring the model converges correctly during training.\n\n\nLoss Scaling\nOne of the key challenges with FP16 is its reduced dynamic range, which increases the likelihood of gradient values becoming too small to be represented accurately. Loss scaling addresses this issue by temporarily amplifying gradient values during backpropagation. Specifically, the loss value is scaled by a large factor (e.g., \\(2^{10}\\)) before gradients are computed, ensuring they remain within the representable range of FP16. Once the gradients are computed, the scaling factor is reversed during the weight update step to restore the original gradient magnitude. This process allows FP16 to be used effectively without sacrificing numerical stability.\nModern machine learning frameworks, such as PyTorch and TensorFlow, provide built-in support for mixed-precision training. These frameworks abstract the complexities of managing different precisions, enabling practitioners to implement mixed-precision workflows with minimal effort. For instance, PyTorch’s torch.cuda.amp (Automatic Mixed Precision) library automates the process of selecting which operations to perform in FP16 or FP32, as well as applying loss scaling when necessary.\nCombining FP16 computation, FP32 accumulation, and loss scaling allows us to achieve mixed-precision training, resulting in a significant reduction in memory usage and computational overhead without compromising the accuracy or stability of the training process. The following sections will explore the practical advantages of this approach and its impact on modern machine learning workflows.\n\n\nBenefits\nMixed-precision training offers several significant advantages that make it an essential optimization technique for modern machine learning workflows. By reducing memory usage and computational load, it enables practitioners to train larger models, process bigger batches, and achieve faster results, all while maintaining model accuracy and convergence.\nOne of the most prominent benefits of mixed-precision training is its substantial reduction in memory consumption. FP16 computations require only half the memory of FP32 computations, which directly reduces the storage required for activations, weights, and gradients during training. For instance, a transformer model with 1 billion parameters requires 4GB of memory for weights in FP32, but only 2GB in FP16. This memory efficiency allows for larger batch sizes, which can lead to more stable gradient estimates and faster convergence. Additionally, with less memory consumed per operation, practitioners can train deeper and more complex models on the same hardware, unlocking capabilities that were previously limited by memory constraints.\nAnother key advantage is the acceleration of computations. Modern GPUs, such as those equipped with Tensor Cores, are specifically optimized for FP16 operations. These cores enable hardware to process more operations per cycle compared to FP32, resulting in faster training times. For matrix multiplication operations, which constitute 80-90% of training computation time in large models, FP16 can achieve 2-3\\(\\times\\) speedup compared to FP32. This computational speedup becomes particularly noticeable in large-scale models, such as transformers and convolutional neural networks, where matrix multiplications dominate the workload.\nMixed-precision training also improves hardware utilization by better matching the capabilities of modern accelerators. In traditional FP32 workflows, the computational throughput of GPUs is often underutilized due to their design for parallel processing. FP16 operations, being less demanding, allow more computations to be performed simultaneously, ensuring that the hardware operates closer to its full capacity.\nFinally, mixed-precision training aligns well with the requirements of distributed and cloud-based systems. In distributed training, where large-scale models are trained across multiple GPUs or nodes, memory and bandwidth become critical constraints. By reducing the size of tensors exchanged between devices, mixed precision not only speeds up inter-device communication but also decreases overall resource demands. This makes it particularly effective in environments where scalability and cost-efficiency are priorities.\nOverall, the benefits of mixed-precision training extend beyond performance improvements. By optimizing memory usage and computation, this technique empowers machine learning practitioners to train cutting-edge models more efficiently, making it a cornerstone of modern machine learning.\n\n\nUse Cases\nMixed-precision training has become a essential in machine learning workflows, particularly in domains and scenarios where computational efficiency and memory optimization are critical. Its ability to enable faster training and larger model capacities makes it highly applicable across a variety of machine learning tasks and architectures.\nOne of the most prominent use cases is in training large-scale machine learning models. In natural language processing (NLP), models such as BERT (345M parameters), GPT-3 (175B parameters), and Transformer-based architectures involve extensive matrix multiplications and large parameter sets. Mixed-precision training allows these models to operate with larger batch sizes or deeper configurations, facilitating faster convergence and improved accuracy on massive datasets.\nIn computer vision, tasks such as image classification, object detection, and segmentation often require handling high-resolution images and applying computationally intensive convolutional operations. By leveraging mixed-precision training, these workloads can be executed more efficiently, enabling the training of advanced architectures like ResNet, EfficientNet, and vision transformers within practical resource limits.\nMixed-precision training is also particularly valuable in reinforcement learning (RL), where models interact with environments to optimize decision-making policies. RL often involves high-dimensional state spaces and requires substantial computational resources for both model training and simulation. Mixed precision reduces the overhead of these processes, allowing researchers to focus on larger environments and more complex policy networks.\nAnother critical application is in distributed training systems. When training models across multiple GPUs or nodes, memory and bandwidth become limiting factors for scalability. Mixed precision addresses these issues by reducing the size of activations, weights, and gradients exchanged between devices. For example, in a distributed training setup with 8 GPUs, reducing tensor sizes from FP32 to FP16 can halve the communication bandwidth requirements from 320GB/s to 160GB/s. This optimization is especially beneficial in cloud-based environments, where resource allocation and cost efficiency are paramount.\nAdditionally, mixed-precision training is increasingly used in areas such as speech processing, generative modeling, and scientific simulations. Models in these fields often have large data and parameter requirements that can push the limits of traditional FP32 workflows. By optimizing memory usage and leveraging the speedups provided by Tensor Cores, practitioners can train state-of-the-art models faster and more cost-effectively.\nThe adaptability of mixed-precision training to diverse tasks and domains underscores its importance in modern machine learning. Whether applied to large-scale natural language models, computationally intensive vision architectures, or distributed training environments, this technique empowers researchers and engineers to push the boundaries of what is computationally feasible.\n\n\nChallenges and Trade-offs\nWhile mixed-precision training offers significant advantages in terms of memory efficiency and computational speed, it also introduces several challenges and trade-offs that must be carefully managed to ensure successful implementation.\nOne of the primary challenges lies in the reduced precision of FP16. While FP16 computations are faster and require less memory, their limited dynamic range (\\(\\pm65,504\\)) can lead to numerical instability, particularly during gradient computations. Small gradient values below \\(6 \\times 10^{-5}\\) become too small to be represented accurately in FP16, resulting in underflow. While loss scaling addresses this by multiplying gradients by factors like \\(2^{8}\\) to \\(2^{14}\\), implementing and tuning this scaling factor adds complexity to the training process.\nAnother trade-off involves the increased risk of convergence issues. While many modern machine learning tasks perform well with mixed-precision training, certain models or datasets may require higher precision to achieve stable and reliable results. For example, recurrent neural networks with long sequences often accumulate numerical errors in FP16, requiring careful gradient clipping and precision management. In such cases, practitioners may need to experiment with selectively enabling or disabling FP16 computations for specific operations, which can complicate the training workflow.\nDebugging and monitoring mixed-precision training also require additional attention. Numerical issues such as NaN (Not a Number) values in gradients or activations are more common in FP16 workflows and may be difficult to trace without proper tools and logging. For instance, gradient explosions in deep networks might manifest differently in mixed precision, appearing as infinities in FP16 before they would in FP32. Frameworks like PyTorch and TensorFlow provide utilities for debugging mixed-precision training, but these tools may not catch every edge case, especially in custom implementations.\nAnother challenge is the dependency on specialized hardware. Mixed-precision training relies heavily on GPU architectures optimized for FP16 operations, such as Tensor Cores in NVIDIA’s GPUs. While these GPUs are becoming increasingly common, not all hardware supports mixed-precision operations, limiting the applicability of this technique in some environments.\nFinally, there are scenarios where mixed-precision training may not provide significant benefits. Models with relatively low computational demand (less than 10M parameters) or small parameter sizes may not fully utilize the speedups offered by FP16 operations. In such cases, the additional complexity of mixed-precision workflows may outweigh their potential advantages.\nDespite these challenges, mixed-precision training remains a highly effective optimization technique for most large-scale machine learning tasks. By understanding and addressing its trade-offs, practitioners can harness its benefits while minimizing potential drawbacks, ensuring efficient and reliable training workflows.\n\n\n\n8.5.3 Gradient Accumulation and Checkpointing\nTraining large machine learning models often requires significant memory resources, particularly for storing three key components: activations (intermediate layer outputs), gradients (parameter updates), and model parameters (weights and biases) during forward and backward passes. However, memory constraints on GPUs can limit the batch size or the complexity of models that can be trained on a given device.\nGradient accumulation and activation checkpointing are two techniques designed to address these limitations by optimizing how memory is utilized during training. Both techniques enable researchers and practitioners to train larger and more complex models, making them indispensable tools for modern deep learning workflows. In the following sections, we will go deeper into the mechanics of gradient accumulation and activation checkpointing, exploring their benefits, use cases, and practical implementation.\n\nMechanics\nGradient accumulation and activation checkpointing operate on distinct principles, but both aim to optimize memory usage during training by modifying how forward and backward computations are handled.\n\nGradient Accumulation\nGradient accumulation simulates larger batch sizes by splitting a single effective batch into smaller “micro-batches.” As illustrated in Figure 8.9, during each forward and backward pass, the gradients for a micro-batch are computed and added to an accumulated gradient buffer. Instead of immediately applying the gradients to update the model parameters, this process repeats for several micro-batches. Once the gradients from all micro-batches in the effective batch are accumulated, the parameters are updated using the combined gradients.\n\n\n\n\n\n\n\n\nG\n\n\ncluster_losses\n\nLOSSES\n\n\ncluster_gradients\n\nGRADIENTS\n\n\ncluster_sum\n\nSUM\n\n\n\nA\n\nBatch 1\n\n\n\nB\n\nL₁\n\n\n\nA-&gt;B\n\n\n\n\n\nG\n\nδ₁\n\n\n\nB-&gt;G\n\n\n∂L₁/∂x\n\n\n\nC\n\nBatch 2\n\n\n\nD\n\nL₂\n\n\n\nC-&gt;D\n\n\n\n\n\nH\n\nδ₂\n\n\n\nD-&gt;H\n\n\n∂L₂/∂x\n\n\n\nE\n\nBatch 3\n\n\n\nF\n\nL₃\n\n\n\nE-&gt;F\n\n\n\n\n\nI\n\nδ₃\n\n\n\nF-&gt;I\n\n\n∂L₃/∂x\n\n\n\n\nJ\n\nδ₁+δ₂+δ₃\n\n\n\nG-&gt;J\n\n\n\n\n\n\nH-&gt;J\n\n\n\n\n\nI-&gt;J\n\n\n\n\n\n\n\n\nFigure 8.9: Gradient accumulation.\n\n\n\n\n\nThis process allows models to achieve the benefits of training with larger batch sizes, such as improved gradient estimates and convergence stability, without requiring the memory to store an entire batch at once. For instance, in PyTorch, this can be implemented by adjusting the learning rate proportionally to the number of accumulated micro-batches and calling optimizer.step() only after processing the entire effective batch.\nThe key steps in gradient accumulation are:\n\nPerform the forward pass for a micro-batch.\nCompute the gradients during the backward pass.\nAccumulate the gradients into a buffer without updating the model parameters.\nRepeat steps 1-3 for all micro-batches in the effective batch.\nUpdate the model parameters using the accumulated gradients after all micro-batches are processed.\n\n\n\nActivation Checkpointing\nActivation checkpointing reduces memory usage during the backward pass by discarding and selectively recomputing activations. In standard training, activations from the forward pass are stored in memory for use in gradient computations during backpropagation. However, these activations can consume significant memory, particularly in deep networks.\nWith checkpointing, only a subset of the activations is retained during the forward pass. When gradients need to be computed during the backward pass, the discarded activations are recomputed on demand by re-executing parts of the forward pass. This approach trades computational efficiency for memory savings, as the recomputation increases training time but allows deeper models to be trained within limited memory constraints.\nThe implementation involves:\n\nSplitting the model into segments.\nRetaining activations only at the boundaries of these segments during the forward pass.\nRecomputing activations for intermediate layers during the backward pass when needed.\n\nFrameworks like PyTorch provide tools such as torch.utils.checkpoint to simplify this process. Checkpointing is particularly effective for very deep architectures, such as transformers or large convolutional networks, where the memory required for storing activations can exceed the GPU’s capacity.\nThe synergy between gradient accumulation and checkpointing enables training of larger, more complex models. Gradient accumulation manages memory constraints related to batch size, while checkpointing optimizes memory usage for intermediate activations. Together, these techniques expand the range of models that can be trained on available hardware.\n\n\n\nBenefits\nGradient accumulation and activation checkpointing provide solutions to the memory limitations often encountered in training large-scale machine learning models. By optimizing how memory is used during training, these techniques enable the development and deployment of complex architectures, even on hardware with constrained resources.\nOne of the primary benefits of gradient accumulation is its ability to simulate larger batch sizes without increasing the memory requirements for storing the full batch. Larger batch sizes are known to improve gradient estimates, leading to more stable convergence and faster training. With gradient accumulation, practitioners can achieve these benefits while working with smaller micro-batches that fit within the GPU’s memory. This flexibility is useful when training models on high-resolution data, such as large images or 3D volumetric data, where even a single batch may exceed available memory.\nActivation checkpointing, on the other hand, significantly reduces the memory footprint of intermediate activations during the forward pass. This allows for the training of deeper models, which would otherwise be infeasible due to memory constraints. By discarding and recomputing activations as needed, checkpointing frees up memory that can be used for larger models, additional layers, or higher resolution data. This is especially important in state-of-the-art architectures, such as transformers or dense convolutional networks, which require substantial memory to store intermediate computations.\nBoth techniques enhance the scalability of machine learning workflows. In resource-constrained environments, such as cloud-based platforms or edge devices, these methods provide a means to train models efficiently without requiring expensive hardware upgrades. Furthermore, they enable researchers to experiment with larger and more complex architectures, pushing the boundaries of what is computationally feasible.\nBeyond memory optimization, these techniques also contribute to cost efficiency. By reducing the hardware requirements for training, gradient accumulation and checkpointing lower the overall cost of development, making them valuable for organizations working within tight budgets. This is particularly relevant for startups, academic institutions, or projects running on shared computing resources.\nGradient accumulation and activation checkpointing provide both technical and practical advantages. These techniques create a more flexible, scalable, and cost-effective approach to training large-scale models, empowering practitioners to tackle increasingly complex machine learning challenges.\n\n\nUse Cases\nGradient accumulation and activation checkpointing are particularly valuable in scenarios where hardware memory limitations present significant challenges during training. These techniques are widely used in training large-scale models, working with high-resolution data, and optimizing workflows in resource-constrained environments.\nA common use case for gradient accumulation is in training models that require large batch sizes to achieve stable convergence. For example, models like GPT, BERT, and other transformer architectures often benefit from larger batch sizes due to their improved gradient estimates. However, these batch sizes can quickly exceed the memory capacity of GPUs, especially when working with high-dimensional inputs or multiple GPUs. By accumulating gradients over multiple smaller micro-batches, gradient accumulation enables the use of effective large batch sizes without exceeding memory limits. This is particularly beneficial for tasks like language modeling, sequence-to-sequence learning, and image classification, where batch size significantly impacts training dynamics.\nActivation checkpointing enables training of deep neural networks with numerous layers or complex computations. In computer vision, architectures like ResNet-152, EfficientNet, and DenseNet require substantial memory to store intermediate activations during training. Checkpointing reduces this memory requirement through strategic recomputation of activations, making it possible to train these deeper architectures within GPU memory constraints.\nIn the domain of natural language processing, models like GPT-3 or T5, with hundreds of layers and billions of parameters, rely heavily on checkpointing to manage memory usage. These models often exceed the memory capacity of a single GPU, making checkpointing a necessity for efficient training. Similarly, in generative adversarial networks (GANs), which involve both generator and discriminator models, checkpointing helps manage the combined memory requirements of both networks during training.\nAnother critical application is in resource-constrained environments, such as edge devices or cloud-based platforms. In these scenarios, memory is often a limiting factor, and upgrading hardware may not always be a viable option. Gradient accumulation and checkpointing provide a cost-effective solution for training models on existing hardware, enabling efficient workflows without requiring additional investment in resources.\nThese techniques are also indispensable in research and experimentation. They allow practitioners to prototype and test larger and more complex models, exploring novel architectures that would otherwise be infeasible due to memory constraints. This is particularly valuable for academic researchers and startups operating within limited budgets.\nGradient accumulation and activation checkpointing solve fundamental challenges in training large-scale models within memory-constrained environments. These techniques have become essential tools for practitioners in natural language processing, computer vision, generative modeling, and edge computing, enabling broader adoption of advanced machine learning architectures.\n\n\nChallenges and Trade-offs\nWhile gradient accumulation and activation checkpointing are powerful tools for optimizing memory usage during training, their implementation introduces several challenges and trade-offs that must be carefully managed to ensure efficient and reliable workflows.\nOne of the primary trade-offs of activation checkpointing is the additional computational overhead it introduces. By design, checkpointing saves memory by discarding and recomputing intermediate activations during the backward pass. This recomputation increases the training time, as portions of the forward pass must be executed multiple times. For example, in a transformer model with 12 layers, if checkpoints are placed every 4 layers, each intermediate activation would need to be recomputed up to three times during the backward pass. The extent of this overhead depends on how the model is segmented for checkpointing and the computational cost of each segment. Practitioners must strike a balance between memory savings and the additional time spent on recomputation, which may affect overall training efficiency.\nGradient accumulation, while effective at simulating larger batch sizes, can lead to slower parameter updates. Since gradients are accumulated over multiple micro-batches, the model parameters are updated less frequently compared to training with full batches. This delay in updates can impact the speed of convergence, particularly in models sensitive to batch size dynamics. Additionally, gradient accumulation requires careful tuning of the learning rate. For instance, if accumulating gradients over 4 micro-batches to simulate a batch size of 128, the learning rate typically needs to be scaled up by a factor of 4 to maintain the same effective learning rate as training with full batches. The effective batch size increases with accumulation, necessitating proportional adjustments to the learning rate to maintain stable training.\nDebugging and monitoring are also more complex when using these techniques. In activation checkpointing, errors may arise during recomputation, making it more difficult to trace issues back to their source. Similarly, gradient accumulation requires ensuring that gradients are correctly accumulated and reset after each effective batch, which can introduce bugs if not handled properly.\nAnother challenge is the increased complexity in implementation. While modern frameworks like PyTorch provide utilities to simplify gradient accumulation and checkpointing, effective use still requires understanding the underlying principles. For instance, activation checkpointing demands segmenting the model appropriately to minimize recomputation overhead while achieving meaningful memory savings. Improper segmentation can lead to suboptimal performance or excessive computational cost.\nThese techniques may also have limited benefits in certain scenarios. For example, if the computational cost of recomputation in activation checkpointing is too high relative to the memory savings, it may negate the advantages of the technique. Similarly, for models or datasets that do not require large batch sizes, the complexity introduced by gradient accumulation may not justify its use.\nDespite these challenges, gradient accumulation and activation checkpointing remain indispensable for training large-scale models under memory constraints. By carefully managing their trade-offs and tailoring their application to specific workloads, practitioners can maximize the efficiency and effectiveness of these techniques.\n\n\n\n8.5.4 Comparison\nAs summarized in Table 8.5, these techniques vary in their implementation complexity, hardware requirements, and impact on computation speed and memory usage. The selection of an appropriate optimization strategy depends on factors such as the specific use case, available hardware resources, and the nature of performance bottlenecks in the training process.\n\n\n\nTable 8.5: High-level comparison of the three optimization strategies, highlighting their key aspects, benefits, and challenges.\n\n\n\n\n\n\n\n\n\n\n\nAspect\nPrefetching and Overlapping\nMixed-Precision Training\nGradient Accumulation and Checkpointing\n\n\n\n\nPrimary Goal\nMinimize data transfer delays and maximize GPU utilization\nReduce memory consumption and computational overhead\nOvercome memory limitations during backpropagation and parameter updates\n\n\nKey Mechanism\nAsynchronous data loading and parallel processing\nCombining FP16 and FP32 computations\nSimulating larger batch sizes and selective activation storage\n\n\nMemory Impact\nIncreases memory usage for prefetch buffer\nReduces memory usage by using FP16\nReduces memory usage for activations and gradients\n\n\nComputation Speed\nImproves by reducing idle time\nAccelerates computations using FP16\nMay slow down due to recomputations in checkpointing\n\n\nScalability\nHighly scalable, especially for large datasets\nEnables training of larger models\nAllows training deeper models on limited hardware\n\n\nHardware Requirements\nBenefits from fast storage and multi-core CPUs\nRequires GPUs with FP16 support (e.g., Tensor Cores)\nWorks on standard hardware\n\n\nImplementation Complexity\nModerate (requires tuning of prefetch parameters)\nLow to moderate (with framework support)\nModerate (requires careful segmentation and accumulation)\n\n\nMain Benefits\nReduces training time, improves hardware utilization\nFaster training, larger models, reduced memory usage\nEnables larger batch sizes and deeper models\n\n\nPrimary Challenges\nTuning buffer sizes, increased memory usage\nPotential numerical instability, loss scaling needed\nIncreased computational overhead, slower parameter updates\n\n\nIdeal Use Cases\nLarge datasets, complex preprocessing\nLarge-scale models, especially in NLP and computer vision\nVery deep networks, memory-constrained environments\n\n\n\n\n\n\nWhile these three techniques represent core optimization strategies in machine learning, they are part of a larger optimization landscape. Other notable techniques include pipeline parallelism for multi-GPU training, dynamic batching for variable-length inputs, and quantization for inference optimization. Practitioners should evaluate their specific requirements—such as model architecture, dataset characteristics, and hardware constraints—to select the most appropriate combination of optimization techniques for their use case.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>AI Training</span>"
    ]
  },
  {
    "objectID": "contents/core/training/training.html#distributed-training-systems",
    "href": "contents/core/training/training.html#distributed-training-systems",
    "title": "8  AI Training",
    "section": "8.6 Distributed Training Systems",
    "text": "8.6 Distributed Training Systems\nThus far, we have focused on ML training pipelines from a single-system perspective. However, training machine learning models often requires scaling beyond a single machine due to increasing model complexity and dataset sizes. The demand for computational power, memory, and storage can exceed the capacity of individual devices, especially in domains like natural language processing, computer vision, and scientific computing. Distributed training addresses this challenge by spreading the workload across multiple machines, which coordinate to train a single model efficiently.\nThis coordination introduces several fundamental challenges that distributed training systems must address. A distributed training system must orchestrate multi-machine computation by splitting up the work, managing communication between machines, and maintaining synchronization throughout the training process. Understanding these basic requirements provides the foundation for examining the main approaches to distributed training: data parallelism, which divides the training data across machines; model parallelism, which splits the model itself; and hybrid approaches that combine both strategies.\n\n8.6.1 Data Parallelism\nData parallelism is a method for distributing the training process across multiple devices by splitting the dataset into smaller subsets. Each device trains a complete copy of the model using its assigned subset of the data. For example, when training an image classification model on 1 million images using 4 GPUs, each GPU would process 250,000 images while maintaining an identical copy of the model architecture.\nData parallelism is particularly effective when the dataset size is large but the model size is manageable, as each device must store a full copy of the model in memory. This method is widely used in scenarios such as image classification and natural language processing, where the dataset can be processed in parallel without dependencies between data samples. For instance, when training a ResNet model on ImageNet, each GPU can independently process its portion of images since the classification of one image doesn’t depend on the results of another.\n\nMathematical Foundations\nData parallelism builds on a key insight from stochastic gradient descent. Gradients computed on different minibatches can be averaged. This property enables parallel computation across devices. Let’s examine why this works mathematically.\nConsider a model with parameters \\(θ\\) training on a dataset \\(D\\). The loss function for a single data point \\(x_i\\) is \\(L(θ, x_i)\\). In standard SGD with batch size \\(B\\), the gradient update for a minibatch is:\n\\[\ng = \\frac{1}{B} \\sum_{i=1}^B \\nabla_θ L(θ, x_i)\n\\]\nIn data parallelism with \\(N\\) devices, each device \\(k\\) computes gradients on its own minibatch \\(B_k\\):\n\\[\ng_k = \\frac{1}{|B_k|} \\sum_{x_i \\in B_k} \\nabla_θ L(θ, x_i)\n\\]\nThe global update averages these local gradients:\n\\[\ng_{global} = \\frac{1}{N} \\sum_{k=1}^N g_k\n\\]\nThis averaging is mathematically equivalent to computing the gradient on the combined batch \\(B_{total} = \\bigcup_{k=1}^N B_k\\):\n\\[\ng_{global} = \\frac{1}{|B_{total}|} \\sum_{x_i \\in B_{total}} \\nabla_θ L(θ, x_i)\n\\]\nThis equivalence shows why data parallelism maintains the statistical properties of SGD training. The approach distributes distinct data subsets across devices, computes local gradients independently, and averages these gradients to approximate the full-batch gradient.\nThe method parallels gradient accumulation, where a single device accumulates gradients over multiple forward passes before updating parameters. Both techniques leverage the additive properties of gradients to process large batches efficiently.\n\n\nMechanics\nThe process of data parallelism can be broken into a series of distinct steps, each with its role in ensuring the system operates efficiently. These steps are illustrated in Figure 8.10.\n\n\n\n\n\n\ngraph TD\n  %% Input Data and Splitting\n  A[fa:fa-database Input Data] --&gt;|Split into &lt;br&gt;Non-Overlapping Subsets| B[fa:fa-box Batch 1]\n  A --&gt;|Split into &lt;br&gt;Non-Overlapping Subsets| C[fa:fa-box Batch 2]\n  A --&gt;|Split into &lt;br&gt;Non-Overlapping Subsets| D[fa:fa-box Batch 3]\n  A --&gt;|Split into &lt;br&gt;Non-Overlapping Subsets| E[fa:fa-box Batch N]\n\n  %% Forward and Backward Pass on GPUs\n  B --&gt;|\"Assigned to&lt;br&gt;GPU 1\"| F[fa:fa-microchip GPU 1&lt;br&gt;Forward & Backward Pass]\n  C --&gt;|\"Assigned to&lt;br&gt;GPU 2\"| G[fa:fa-microchip GPU 2&lt;br&gt;Forward & Backward Pass]\n  D --&gt;|\"Assigned to&lt;br&gt;GPU 3\"| H[fa:fa-microchip GPU 3&lt;br&gt;Forward & Backward Pass]\n  E --&gt;|\"Assigned to&lt;br&gt;GPU N\"| I[fa:fa-microchip GPU N&lt;br&gt;Forward & Backward Pass]\n\n  %% Gradient Synchronization\n  F --&gt;|Compute Gradients| J[fa:fa-chart-line Gradients GPU 1]\n  G --&gt;|Compute Gradients| J[fa:fa-chart-line Gradients GPU 2]\n  H --&gt;|Compute Gradients| J[fa:fa-chart-line Gradients GPU 3]\n  I --&gt;|Compute Gradients| J[fa:fa-chart-line Gradients GPU N]\n\n  J --&gt;|Synchronize Gradients| K[fa:fa-sync Gradient Aggregation]\n\n  %% Parameter Update\n  K --&gt;|Aggregate Gradients and Update Parameters| L[fa:fa-cogs Model Update]\n\n  %% Loop Back for Iteration\n  L --&gt;|Next Mini-Batch| A\n\n\n\n\nFigure 8.10: Data-level parallelism.\n\n\n\n\n\n\nStep 1: Splitting the Dataset\nThe first step in data parallelism involves dividing the dataset into smaller, non-overlapping subsets. This ensures that each device processes a unique portion of the data, avoiding redundancy and enabling efficient utilization of available hardware. For instance, with a dataset of 100,000 training examples and 4 GPUs, each GPU would be assigned 25,000 examples. Modern frameworks like PyTorch’s DistributedSampler handle this distribution automatically, implementing prefetching and caching mechanisms to ensure data is readily available for processing.\n\n\nStep 2: Forward Pass on Each Device\nOnce the data subsets are distributed, each device performs the forward pass independently. During this stage, the model processes its assigned batch of data, generating predictions and calculating the loss. For example, in a ResNet-50 model, each GPU would independently compute the convolutions, activations, and final loss for its batch. The forward pass is computationally intensive and benefits from hardware accelerators like NVIDIA V100 GPUs or Google TPUs, which are optimized for matrix operations.\n\n\nStep 3: Backward Pass and Gradient Calculation\nFollowing the forward pass, each device computes the gradients of the loss with respect to the model’s parameters during the backward pass. Modern frameworks like PyTorch and TensorFlow handle this automatically through their autograd systems. For instance, if a model has 50 million parameters, each device calculates gradients for all parameters but based only on its local data subset.\n\n\nStep 4: Gradient Synchronization Across Devices\nTo maintain consistency across the distributed system, the gradients computed by each device must be synchronized. This step typically uses the ring all-reduce algorithm, where each GPU communicates only with its neighbors, reducing communication overhead. For example, with 8 GPUs, each sharing gradients for a 100MB model, ring all-reduce requires only 7 communication steps instead of the 56 steps needed for naive peer-to-peer synchronization.\n\n\nStep 5: Updating Model Parameters\nAfter the gradients are aggregated, the model parameters are updated using the chosen optimization algorithm (e.g., stochastic gradient descent with momentum). In frameworks like PyTorch DDP (DistributedDataParallel), these updates occur independently on each device after gradient synchronization, eliminating the need for a central parameter server.\nThis process—splitting data, performing computations, synchronizing results, and updating parameters—repeats for each batch of data. Modern frameworks automate this cycle, allowing developers to focus on model architecture and hyperparameter tuning rather than distributed computing logistics.\n\n\n\nBenefits\nData parallelism offers several key benefits that make it the predominant approach for distributed training. By splitting the dataset across multiple devices and allowing each device to train an identical copy of the model, this approach effectively addresses the core challenges in modern AI training systems.\nThe primary advantage of data parallelism is its linear scaling capability with large datasets. As datasets grow into the terabyte range, processing them on a single machine becomes prohibitively time-consuming. For example, training a vision transformer on ImageNet (1.2 million images) might take weeks on a single GPU, but only days when distributed across 8 GPUs. This scalability is particularly valuable in domains like language modeling, where datasets can exceed billions of tokens.\nHardware utilization efficiency represents another crucial benefit. Data parallelism maintains high GPU utilization rates—typically above 85%—by ensuring each device actively processes its data portion. Modern implementations achieve this through asynchronous data loading and gradient computation overlapping with communication. For instance, while one batch computes gradients, the next batch’s data is already being loaded and preprocessed.\nImplementation simplicity sets data parallelism apart from other distribution strategies. Modern frameworks have reduced complex distributed training to just a few lines of code. For example, converting a PyTorch model to use data parallelism often requires only wrapping it in DistributedDataParallel and initializing a distributed environment. This accessibility has contributed significantly to its widespread adoption in both research and industry.\nThe approach also offers remarkable flexibility across model architectures. Whether training a ResNet (vision), BERT (language), or Graph Neural Network (graph data), the same data parallelism principles apply without modification. This universality makes it particularly valuable as a default choice for distributed training.\nTraining time reduction is perhaps the most immediate benefit. Given proper implementation, data parallelism can achieve near-linear speedup with additional devices. Training that takes 100 hours on a single GPU might complete in roughly 13 hours on 8 GPUs, assuming efficient gradient synchronization and minimal communication overhead.\nWhile these benefits make data parallelism compelling, it’s important to note that achieving these advantages requires careful system design. The next section examines the challenges that must be addressed to fully realize these benefits.\n\n\nChallenges\nWhile data parallelism is a powerful approach for distributed training, it introduces several challenges that must be addressed to achieve efficient and scalable training systems. These challenges stem from the inherent trade-offs between computation and communication, as well as the limitations imposed by hardware and network infrastructures.\nCommunication overhead represents the most significant bottleneck in data parallelism. During gradient synchronization, each device must exchange gradient updates—often hundreds of megabytes per step for large models. With 8 GPUs training a 1-billion-parameter model, each synchronization step might require transferring several gigabytes of data across the network. While high-speed interconnects like NVLink (300 GB/s) or InfiniBand (200 Gb/s) help, the overhead remains substantial. NCCL’s ring-allreduce5 algorithm reduces this burden by organizing devices in a ring topology, but communication costs still grow with model size and device count.\n5 A communication strategy that minimizes data transfer overhead by organizing devices in a ring topology, first introduced for distributed machine learning in Horovod.Scalability limitations become apparent as device count increases. While 8 GPUs might achieve 7x speedup (87.5% scaling efficiency), scaling to 64 GPUs typically yields only 45-50x speedup (70-78% efficiency) due to growing synchronization costs. This non-linear scaling means that doubling the number of devices rarely halves the training time, particularly in configurations exceeding 16-32 devices.\nMemory constraints present a hard limit for data parallelism. Consider a transformer model with 175 billion parameters—it requires approximately 350GB just to store model parameters in FP32. When accounting for optimizer states and activation memories, the total requirement often exceeds 1TB per device. Since even high-end GPUs typically offer 80GB or less, such models cannot use pure data parallelism.\nWorkload imbalance affects heterogeneous systems significantly. In a cluster mixing A100 and V100 GPUs, the A100s might process batches 1.7x faster, forcing them to wait for the V100s to catch up. This idle time can reduce cluster utilization by 20-30% without proper load balancing mechanisms.\nFinally, there are challenges related to implementation complexity in distributed systems. While modern frameworks abstract much of the complexity, implementing data parallelism at scale still requires significant engineering effort. Ensuring fault tolerance, debugging synchronization issues, and optimizing data pipelines are non-trivial tasks that demand expertise in both machine learning and distributed systems.\nDespite these challenges, data parallelism remains an important technique for distributed training, with many strategies available to address its limitations. In the next section, we will explore model parallelism, another strategy for scaling training that is particularly well-suited for handling extremely large models that cannot fit on a single device.\n\n\n\n8.6.2 Model Parallelism\nModel parallelism splits neural networks across multiple computing devices when the model’s parameters exceed single-device memory limits. Unlike data parallelism, where each device contains a complete model copy, model parallelism assigns different model components to different devices (Shazeer et al. 2017).\n\nShazeer, Noam, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton, and Jeff Dean. 2017. “Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer.” arXiv Preprint arXiv:1701.06538, January. http://arxiv.org/abs/1701.06538v1.\nSeveral implementations of model parallelism exist. In layer-based splitting, devices process distinct groups of layers sequentially. For instance, the first device might compute layers 1-4 while the second handles layers 5-8. Channel-based splitting divides the channels within each layer across devices, such as the first device processing 512 channels while the second manages the remaining ones. For transformer architectures, attention head splitting distributes different attention heads to separate devices.\nThis distribution method enables training of large-scale models. GPT-3, with 175 billion parameters, relies on model parallelism for training. Vision transformers processing high-resolution 16k x 16k pixel images use model parallelism to manage memory constraints. Mixture-of-Expert architectures leverage this approach to distribute their conditional computation paths across hardware.\nDevice coordination follows a specific pattern during training. In the forward pass, data flows sequentially through model segments on different devices. The backward pass propagates gradients in reverse order through these segments. During parameter updates, each device modifies only its assigned portion of the model. This coordination ensures mathematical equivalence to training on a single device while enabling the handling of models that exceed individual device memory capacities.\n\nMechanics\nModel parallelism divides neural networks across multiple computing devices, with each device computing a distinct portion of the model’s operations. This division allows training of models whose parameter counts exceed single-device memory capacity. The technique encompasses device coordination, data flow management, and gradient computation across distributed model segments. The mechanics of model parallelism are illustrated in Figure 8.11. These steps are described next:\n\n\n\n\n\n\ngraph TD\n  A[\"&lt;i class='fas fa-database'&gt;&lt;/i&gt; Input Data\"] --&gt;|Forward Pass| B[\"&lt;i class='fas fa-network-wired'&gt;&lt;/i&gt; Model Part 1 &lt;br&gt;on Device 1 &lt;i class='fas fa-microchip'&gt;&lt;/i&gt;\"]\n  B --&gt;|Intermediate Data| C[\"&lt;i class='fas fa-network-wired'&gt;&lt;/i&gt; Model Part 2 &lt;br&gt;on Device 2 &lt;i class='fas fa-microchip'&gt;&lt;/i&gt;\"]\n  C --&gt;|Intermediate Data| D[\"&lt;i class='fas fa-network-wired'&gt;&lt;/i&gt; Model Part 3 &lt;br&gt;on Device 3 &lt;i class='fas fa-microchip'&gt;&lt;/i&gt;\"]\n  D --&gt;|Output| E[\"&lt;i class='fas fa-chart-line'&gt;&lt;/i&gt; Predictions\"]\n  \n  E --&gt;|Backward Pass| D\n  D --&gt;|Gradient Updates| C\n  C --&gt;|Gradient Updates| B\n  B --&gt;|Gradient Updates| A\n\n\n\n\nFigure 8.11: Model-level parallelism.\n\n\n\n\n\n\nStep 1: Partitioning the Model\nThe first step in model parallelism is dividing the model into smaller segments. For instance, in a deep neural network, layers are often divided among devices. In a system with two GPUs, the first half of the layers might reside on GPU 1, while the second half resides on GPU 2. Another approach is to split computations within a single layer, such as dividing matrix multiplications in transformer models across devices.\n\n\nStep 2: Forward Pass Through the Model\nDuring the forward pass, data flows sequentially through the partitions. For example, data processed by the first set of layers on GPU 1 is sent to GPU 2 for processing by the next set of layers. This sequential flow ensures that the entire model is used, even though it is distributed across multiple devices. Efficient inter-device communication is crucial to minimize delays during this step (Research 2021).\n\nResearch, Microsoft. 2021. DeepSpeed: Extreme-Scale Model Training for Everyone.\n\n\nStep 3: Backward Pass and Gradient Calculation\nThe backward pass computes gradients through the distributed model segments in reverse order. Each device calculates local gradients for its parameters and propagates necessary gradient information to previous devices. In transformer models, this means backpropagating through attention computations and feed-forward networks across device boundaries.\nFor example, in a two-device setup with attention mechanisms split between devices, the backward computation works as follows: The second device computes gradients for the final feed-forward layers and attention heads. It then sends the gradient tensors for the attention output to the first device. The first device uses these received gradients to compute updates for its attention parameters and earlier layer weights.\n\n\nStep 4: Parameter Updates\nParameter updates occur independently on each device using the computed gradients and an optimization algorithm. A device holding attention layer parameters applies updates using only the gradients computed for those specific parameters. This localized update approach differs from data parallelism, which requires gradient averaging across devices.\nThe optimization step proceeds as follows: Each device applies its chosen optimizer (such as Adam or AdaFactor) to update its portion of the model parameters. A device holding the first six transformer layers updates only those layers’ weights and biases. This local parameter update eliminates the need for cross-device synchronization during the optimization step, reducing communication overhead.\n\n\nIterative Process\nLike other training strategies, model parallelism repeats these steps for every batch of data. As the dataset is processed over multiple iterations, the distributed model converges toward optimal performance.\n\n\nVariations of Model Parallelism\nModel parallelism can be implemented through different strategies for dividing the model across devices. The three primary approaches are layer-wise partitioning, operator-level partitioning, and pipeline parallelism, each suited to different model structures and computational needs.\n\nLayer-wise Partitioning\nLayer-wise partitioning assigns distinct model layers to separate computing devices. In transformer architectures, this translates to specific devices managing defined sets of attention and feed-forward blocks. As illustrated in Figure 8.12, a 24-layer transformer model distributed across four devices assigns six consecutive transformer blocks to each device. Device 1 processes blocks 1-6, device 2 handles blocks 7-12, and so forth.\n\n\n\n\n\n\nflowchart LR\n    subgraph Device 1\n        A1[&lt;i class=\"fas fa-cube\"&gt;&lt;/i&gt; Blocks 1-6]\n        B1[&lt;i class=\"fas fa-microchip\"&gt;&lt;/i&gt; GPU 1]\n    end\n    subgraph Device 2\n        A2[&lt;i class=\"fas fa-cube\"&gt;&lt;/i&gt; Blocks 7-12]\n        B2[&lt;i class=\"fas fa-microchip\"&gt;&lt;/i&gt; GPU 2]\n    end\n    subgraph Device 3\n        A3[&lt;i class=\"fas fa-cube\"&gt;&lt;/i&gt; Blocks 13-18]\n        B3[&lt;i class=\"fas fa-microchip\"&gt;&lt;/i&gt; GPU 3]\n    end\n    subgraph Device 4\n        A4[&lt;i class=\"fas fa-cube\"&gt;&lt;/i&gt; Blocks 19-24]\n        B4[&lt;i class=\"fas fa-microchip\"&gt;&lt;/i&gt; GPU 4]\n    end\n    A1 --&gt; A2\n    A2 --&gt; A3\n    A3 --&gt; A4\n    A4 --&gt; A3\n    A3 --&gt; A2\n    A2 --&gt; A1\n\n    %% Defining the styles\n    classDef Red fill:#FF9999;\n    classDef Amber fill:#FFDEAD;\n    classDef Green fill:#BDFFA4;\n    classDef Blue fill:#99CCFF; \n\n    %% Assigning styles to nodes\n    class A1 Red;\n    class A2 Amber;\n    class A3 Green;\n    class A4 Blue;\n\n\n\n\nFigure 8.12: Example of pipeline parallelism.\n\n\n\n\n\nThis sequential processing introduces device idle time, as each device must wait for the previous device to complete its computation before beginning work. For example, while device 1 processes the initial blocks, devices 2, 3, and 4 remain inactive. Similarly, when device 2 begins its computation, device 1 sits idle. This pattern of waiting and idle time reduces hardware utilization efficiency compared to other parallelization strategies.\nLayer-wise partitioning assigns distinct model layers to separate computing devices. In transformer architectures, this translates to specific devices managing defined sets of attention and feed-forward blocks. A 24-layer transformer model distributed across four devices assigns six consecutive transformer blocks to each device. Device 1 processes blocks 1-6, device 2 handles blocks 7-12, and so forth.\n\n\nPipeline Parallelism\nPipeline parallelism extends layer-wise partitioning by introducing microbatching to minimize device idle time, as illustrated in Figure 8.13. Instead of waiting for an entire batch to sequentially pass through all devices, the computation is divided into smaller segments called microbatches [harlap2018pipedream]. Each device, as represented by the rows in the drawing, processes its assigned model layers for different microbatches simultaneously. For example, the forward pass involves devices passing activations to the next stage (e.g., \\(F_{0,0}\\) to \\(F_{1,0}\\). The backward pass transfers gradients back through the pipeline (e.g., \\(B_{3,3}\\) to \\(B_{2,3}\\)). This overlapping computation reduces idle time and increases throughput while maintaining the logical sequence of operations across devices.\n\n\n\n\n\n\nFigure 8.13: Example of pipeline parallelism.\n\n\n\nIn a transformer model distributed across four devices, device 1 would process blocks 1-6 for microbatch N+1 while device 2 computes blocks 7-12 for microbatch N. Simultaneously, device 3 executes blocks 13-18 for microbatch N-1, and device 4 processes blocks 19-24 for microbatch N-2. Each device maintains its assigned transformer blocks but operates on a different microbatch, creating a continuous flow of computation.\nThe transfer of hidden states between devices occurs continuously rather than in distinct phases. When device 1 completes processing a microbatch, it immediately transfers the output tensor of shape [microbatch_size, sequence_length, hidden_dimension] to device 2 and begins processing the next microbatch. This overlapping computation pattern maintains full hardware utilization while preserving the model’s mathematical properties.\n\n\nOperator-level Parallelism\nOperator-level parallelism divides individual neural network operations across devices. In transformer models, this often means splitting attention computations. Consider a transformer with 64 attention heads and a hidden dimension of 4096. Two devices might split this computation as follows: Device 1 processes attention heads 1-32, computing queries, keys, and values for its assigned heads. Device 2 simultaneously processes heads 33-64. Each device handles attention computations for [batch_size, sequence_length, 2048] dimensional tensors.\nMatrix multiplication operations in feed-forward networks also benefit from operator-level splitting. A feed-forward layer with input dimension 4096 and intermediate dimension 16384 can split across devices along the intermediate dimension. Device 1 computes the first 8192 intermediate features, while device 2 computes the remaining 8192 features. This division reduces peak memory usage while maintaining mathematical equivalence to the original computation.\n\n\nSummary\nEach of these partitioning methods addresses specific challenges in training large models, and their applicability depends on the model architecture and the resources available. By selecting the appropriate strategy, practitioners can train models that exceed the limits of individual devices, enabling the development of cutting-edge machine learning systems.\n\n\n\n\n\n8.6.3 Benefits\nModel parallelism offers several significant benefits, making it an essential strategy for training large-scale models that exceed the capacity of individual devices. These advantages stem from its ability to partition the workload across multiple devices, enabling the training of more complex and resource-intensive architectures.\nMemory scaling represents the primary advantage of model parallelism. Current transformer architectures contain up to hundreds of billions of parameters. A 175 billion parameter model with 32-bit floating point precision requires 700 GB of memory just to store its parameters. When accounting for activations, optimizer states, and gradients during training, the memory requirement multiplies several fold. Model parallelism makes training such architectures feasible by distributing these memory requirements across devices.\nAnother key advantage is the efficient utilization of device memory and compute power. Since each device only needs to store and process a portion of the model, memory usage is distributed across the system. This allows practitioners to work with larger batch sizes or more complex layers without exceeding memory limits, which can also improve training stability and convergence.\nModel parallelism also provides flexibility for different model architectures. Whether the model is sequential, as in many natural language processing tasks, or composed of computationally intensive operations, as in attention-based models or convolutional networks, there is a partitioning strategy that fits the architecture. This adaptability makes model parallelism applicable to a wide variety of tasks and domains.\nFinally, model parallelism is a natural complement to other distributed training strategies, such as data parallelism and pipeline parallelism. By combining these approaches, it becomes possible to train models that are both large in size and require extensive data. This hybrid flexibility is especially valuable in cutting-edge research and production environments, where scaling models and datasets simultaneously is critical for achieving state-of-the-art performance.\nWhile model parallelism introduces these benefits, its effectiveness depends on the careful design and implementation of the partitioning strategy. In the next section, we will discuss the challenges associated with model parallelism and the trade-offs involved in its use.\n\n\n8.6.4 Challenges\nWhile model parallelism provides a powerful approach for training large-scale models, it also introduces unique challenges. These challenges arise from the complexity of partitioning the model and the dependencies between partitions during training. Addressing these issues requires careful system design and optimization.\nOne major challenge in model parallelism is balancing the workload across devices. Not all parts of a model require the same amount of computation. For instance, in layer-wise partitioning, some layers may perform significantly more operations than others, leading to an uneven distribution of work. Devices responsible for the heavier computations may become bottlenecks, leaving others underutilized. This imbalance reduces overall efficiency and slows down training. Identifying optimal partitioning strategies is critical to ensuring all devices contribute evenly.\nAnother challenge is data dependency between devices. During the forward pass, activation tensors of shape [batch_size, sequence_length, hidden_dimension] must transfer between devices. For a typical transformer model with batch size 32, sequence length 2048, and hidden dimension 2048, each transfer moves approximately 512MB of data at float32 precision. With gradient transfers in the backward pass, a single training step can require several gigabytes of inter-device communication. On systems using PCIe interconnects with 64 GB/s theoretical bandwidth, these transfers introduce significant latency.\nModel parallelism also increases the complexity of implementation and debugging. Partitioning the model, ensuring proper data flow, and synchronizing gradients across devices require detailed coordination. Errors in any of these steps can lead to incorrect gradient updates or even model divergence. Debugging such errors is often more difficult in distributed systems, as issues may arise only under specific conditions or workloads.\nA further challenge is pipeline bubbles in pipeline parallelism. With m pipeline stages, the first m-1 steps operate at reduced efficiency as the pipeline fills. For example, in an 8-device pipeline, the first device begins processing immediately, but the eighth device remains idle for 7 steps. This warmup period reduces hardware utilization by approximately (m-1)/b percent, where b is the number of batches in the training step.\nFinally, model parallelism may be less effective for certain architectures, such as models with highly interdependent operations. In these cases, splitting the model may lead to excessive communication overhead, outweighing the benefits of parallel computation. For such models, alternative strategies like data parallelism or hybrid approaches might be more suitable.\nDespite these challenges, model parallelism remains an indispensable tool for training large models. With careful optimization and the use of modern frameworks, many of these issues can be mitigated, enabling efficient distributed training at scale.\n\n\n8.6.5 Hybrid Parallelism\nHybrid parallelism combines model parallelism and data parallelism when training neural networks (Narayanan et al. 2021). A model might be too large to store on one GPU (requiring model parallelism) while simultaneously needing to process large batches of data efficiently (requiring data parallelism).\n\nNarayanan, Deepak, Mohammad Shoeybi, Jared Casper, Patrick LeGresley, Mostofa Patwary, Vijay Korthikanti, Dmitri Vainbrand, et al. 2021. “Efficient Large-Scale Language Model Training on GPU Clusters Using Megatron-LM.” In Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis, 1–15. ACM. https://doi.org/10.1145/3458817.3476209.\nTraining a 175-billion parameter language model on a dataset of 300 billion tokens demonstrates hybrid parallelism in practice. The neural network layers distribute across multiple GPUs through model parallelism, while data parallelism enables different GPU groups to process separate batches. The hybrid approach coordinates these two forms of parallelization.\nThis strategy addresses two fundamental constraints. First, memory constraints arise when model parameters exceed single-device memory capacity. Second, computational demands increase when dataset size necessitates distributed processing.\n\nMechanics\nHybrid parallelism operates by combining the processes of model partitioning and dataset splitting, ensuring efficient utilization of both memory and computation across devices. This integration allows large-scale machine learning systems to overcome the constraints imposed by individual parallelism strategies.\n\nPartitioning Model and Data\nHybrid parallelism divides both model architecture and training data across devices. The model divides through layer-wise or operator-level partitioning, where GPUs process distinct neural network segments. Simultaneously, the dataset splits into subsets, allowing each device group to train on different batches. A transformer model might distribute its attention layers across four GPUs, while each GPU group processes a unique 1,000-example batch. This dual partitioning distributes memory requirements and computational workload.\n\n\nForward Pass\nDuring the forward pass, input data flows through the distributed model. Each device processes its assigned portion of the model using the data subset it holds. For example, in a hybrid system with four devices, two devices might handle different layers of the model (model parallelism) while simultaneously processing distinct data batches (data parallelism). Communication between devices ensures that intermediate outputs from model partitions are passed seamlessly to subsequent partitions.\n\n\nBackward Pass and Gradient Calculation\nDuring the backward pass, gradients are calculated for the model partitions stored on each device. Data-parallel devices that process the same subset of the model but different data batches aggregate their gradients, ensuring that updates reflect contributions from the entire dataset. For model-parallel devices, gradients are computed locally and passed to the next layer in reverse order. In a two-device model-parallel configuration, for example, the first device computes gradients for layers 1-3, then transmits these to the second device for layers 4-6. This combination of gradient synchronization and inter-device communication ensures consistency across the distributed system.\n\n\nParameter Updates\nAfter gradient synchronization, model parameters are updated using the chosen optimization algorithm. Devices working in data parallelism update their shared model partitions consistently, while model-parallel devices apply updates to their local segments. Efficient communication is critical in this step to minimize delays and ensure that updates are correctly propagated across all devices.\n\n\nIterative Process\nHybrid parallelism follows an iterative process similar to other training strategies. The combination of model and data distribution allows the system to process large datasets and complex models efficiently over multiple training epochs. By balancing the computational workload and memory requirements, hybrid parallelism enables the training of advanced machine learning models that would otherwise be infeasible.\n\n\nVariations of Hybrid Parallelism\nHybrid parallelism can be implemented in different configurations, depending on the model architecture, dataset characteristics, and available hardware. These variations allow for tailored solutions that optimize performance and scalability for specific training requirements.\n\nHierarchical Hybrid Parallelism\nHierarchical hybrid parallelism applies model parallelism to divide the model across devices first and then layers data parallelism on top to handle the dataset distribution. For example, in a system with eight devices, four devices may hold different partitions of the model, while each partition is replicated across the other four devices for data parallel processing. This approach is well-suited for large models with billions of parameters, where memory constraints are a primary concern.\nHierarchical hybrid parallelism ensures that the model size is distributed across devices, reducing memory requirements, while data parallelism ensures that multiple data samples are processed simultaneously, improving throughput. This dual-layered approach is particularly effective for models like transformers, where each layer may have a significant memory footprint.\n\n\nIntra-layer Hybrid Parallelism\nIntra-layer hybrid parallelism combines model and data parallelism within individual layers of the model. For instance, in a transformer architecture, the attention mechanism can be split across multiple devices (model parallelism), while each device processes distinct batches of data (data parallelism). This fine-grained integration allows the system to optimize resource usage at the level of individual operations, enabling training for models with extremely large intermediate computations.\nThis variation is particularly useful in scenarios where specific layers, such as attention or feedforward layers, have computationally intensive operations that are difficult to distribute effectively using model or data parallelism alone. Intra-layer hybrid parallelism addresses this challenge by applying both strategies simultaneously.\n\n\nInter-layer Hybrid Parallelism\nInter-layer hybrid parallelism focuses on distributing the workload between model and data parallelism at the level of distinct model layers. For example, early layers of a neural network may be distributed using model parallelism, while later layers leverage data parallelism. This approach aligns with the observation that certain layers in a model may be more memory-intensive, while others benefit from increased data throughput.\nThis configuration allows for dynamic allocation of resources, adapting to the specific demands of different layers in the model. By tailoring the parallelism strategy to the unique characteristics of each layer, inter-layer hybrid parallelism achieves an optimal balance between memory usage and computational efficiency.\n\n\n\n\nBenefits\nThe adoption of hybrid parallelism in machine learning systems addresses some of the most significant challenges posed by the ever-growing scale of models and datasets. By blending the strengths of model parallelism and data parallelism, this approach provides a comprehensive solution to scaling modern machine learning workloads.\nOne of the most prominent benefits of hybrid parallelism is its ability to scale seamlessly across both the model and the dataset. Modern neural networks, particularly transformers used in natural language processing and vision applications, often contain billions of parameters. These models, paired with massive datasets, make training on a single device impractical or even impossible. Hybrid parallelism enables the division of the model across multiple devices to manage memory constraints while simultaneously distributing the dataset to process vast amounts of data efficiently. This dual capability ensures that training systems can handle the computational and memory demands of the largest models and datasets without compromise.\nAnother critical advantage lies in hardware utilization. In many distributed training systems, inefficiencies can arise when devices sit idle during different stages of computation or synchronization. Hybrid parallelism mitigates this issue by ensuring that all devices are actively engaged. Whether a device is computing forward passes through its portion of the model or processing data batches, hybrid strategies maximize resource usage, leading to faster training times and improved throughput.\nFlexibility is another hallmark of hybrid parallelism. Machine learning models vary widely in architecture and computational demands. For instance, convolutional neural networks prioritize spatial data processing, while transformers require intensive operations like matrix multiplications in attention mechanisms. Hybrid parallelism adapts to these diverse needs by allowing practitioners to apply model and data parallelism selectively. This adaptability ensures that hybrid approaches can be tailored to the specific requirements of a given model, making it a versatile solution for diverse training scenarios.\nMoreover, hybrid parallelism reduces communication bottlenecks, a common issue in distributed systems. By striking a balance between distributing model computations and spreading data processing, hybrid strategies minimize the amount of inter-device communication required during training. This efficient coordination not only speeds up the training process but also enables the effective use of large-scale distributed systems where network latency might otherwise limit performance.\nFinally, hybrid parallelism supports the ambitious scale of modern AI research and development. It provides a framework for leveraging cutting-edge hardware infrastructures, including clusters of GPUs or TPUs, to train models that push the boundaries of what’s possible. Without hybrid parallelism, many of the breakthroughs in AI—such as large language models or advanced vision systems—would remain unattainable due to resource limitations.\nBy enabling scalability, maximizing hardware efficiency, and offering flexibility, hybrid parallelism has become an essential strategy for training the most complex machine learning systems. It is not just a solution to today’s challenges but also a foundation for the future of AI, where models and datasets will continue to grow in complexity and size.\n\n\nChallenges\nWhile hybrid parallelism provides a robust framework for scaling machine learning training, it also introduces complexities that require careful consideration. These challenges stem from the intricate coordination needed to integrate both model and data parallelism effectively. Understanding these obstacles is crucial for designing efficient hybrid systems and avoiding potential bottlenecks.\nOne of the primary challenges of hybrid parallelism is communication overhead. Both model and data parallelism involve significant inter-device communication. In model parallelism, devices must exchange intermediate outputs and gradients to maintain the sequential flow of computation. In data parallelism, gradients computed on separate data subsets must be synchronized across devices. Hybrid parallelism compounds these demands, as it requires efficient communication for both processes simultaneously. If not managed properly, the resulting overhead can negate the benefits of parallelization, particularly in large-scale systems with slower interconnects or high network latency.\nAnother critical challenge is the complexity of implementation. Hybrid parallelism demands a nuanced understanding of both model and data parallelism techniques, as well as the underlying hardware and software infrastructure. Designing efficient hybrid strategies involves making decisions about how to partition the model, how to distribute data, and how to synchronize computations across devices. This process often requires extensive experimentation and optimization, particularly for custom architectures or non-standard hardware setups. While modern frameworks like PyTorch and TensorFlow provide tools for distributed training, implementing hybrid parallelism at scale still requires significant engineering expertise.\nWorkload balancing also presents a challenge in hybrid parallelism. In a distributed system, not all devices may have equal computational capacity. Some devices may process data or compute gradients faster than others, leading to inefficiencies as faster devices wait for slower ones to complete their tasks. Additionally, certain model layers or operations may require more resources than others, creating imbalances in computational load. Managing this disparity requires careful tuning of partitioning strategies and the use of dynamic workload distribution techniques.\nMemory constraints remain a concern, even in hybrid setups. While model parallelism addresses the issue of fitting large models into device memory, the additional memory requirements for data parallelism, such as storing multiple data batches and gradient buffers, can still exceed available capacity. This is especially true for models with extremely large intermediate computations, such as transformers with high-dimensional attention mechanisms. Balancing memory usage across devices is essential to prevent resource exhaustion during training.\nLastly, hybrid parallelism poses challenges related to fault tolerance and debugging. Distributed systems are inherently more prone to hardware failures and synchronization errors. Debugging issues in hybrid setups can be significantly more complex than in standalone model or data parallelism systems, as errors may arise from interactions between the two approaches. Ensuring robust fault-tolerance mechanisms and designing tools for monitoring and debugging distributed systems are essential for maintaining reliability.\nDespite these challenges, hybrid parallelism remains an indispensable strategy for training state-of-the-art machine learning models. By addressing these obstacles through optimized communication protocols, intelligent partitioning strategies, and robust fault-tolerance systems, practitioners can unlock the full potential of hybrid parallelism and drive innovation in AI research and applications.\n\n\n\n8.6.6 Comparison\nThe features of data parallelism, model parallelism, and hybrid parallelism are summarized in Table 8.6. This comparison highlights their respective focuses, memory requirements, communication overheads, scalability, implementation complexity, and ideal use cases. By examining these factors, practitioners can determine the most suitable approach for their training needs.\n\n\n\nTable 8.6: Comparison of data parallelism, model parallelism, and hybrid parallelism across key aspects.\n\n\n\n\n\n\n\n\n\n\n\nAspect\nData Parallelism\nModel Parallelism\nHybrid Parallelism\n\n\n\n\nFocus\nDistributes dataset across devices, each with a full model copy\nDistributes the model across devices, each handling a portion of the model\nCombines model and data parallelism for balanced scalability\n\n\nMemory Requirement per Device\nHigh (entire model on each device)\nLow (model split across devices)\nModerate (splits model and dataset across devices)\n\n\nCommunication Overhead\nModerate to High (gradient synchronization across devices)\nHigh (communication for intermediate activations and gradients)\nVery High (requires synchronization for both model and data)\n\n\nScalability\nGood for large datasets with moderate model sizes\nGood for very large models with smaller datasets\nExcellent for extremely large models and datasets\n\n\nImplementation Complexity\nLow to Moderate (relatively straightforward with existing tools)\nModerate to High (requires careful partitioning and coordination)\nHigh (complex integration of model and data parallelism)\n\n\nIdeal Use Case\nLarge datasets where model fits within a single device\nExtremely large models that exceed single-device memory limits\nTraining massive models on vast datasets in large-scale systems\n\n\n\n\n\n\nFigure 8.14 provides a general guideline for selecting parallelism strategies in distributed training systems. While the chart offers a structured decision-making process based on model size, dataset size, and scaling constraints, it is intentionally simplified. Real-world scenarios often involve additional complexities such as hardware heterogeneity, communication bandwidth, and workload imbalance, which may influence the choice of parallelism techniques. The chart is best viewed as a foundational tool for understanding the trade-offs and decision points in parallelism strategy selection. Practitioners should consider this guideline as a starting point and adapt it to the specific requirements and constraints of their systems to achieve optimal performance.\n\n\n\n\n\n\nFigure 8.14: Decision flowchart for selecting parallelism strategies in distributed training.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>AI Training</span>"
    ]
  },
  {
    "objectID": "contents/core/training/training.html#optimization-techniques-for-training-systems",
    "href": "contents/core/training/training.html#optimization-techniques-for-training-systems",
    "title": "8  AI Training",
    "section": "8.7 Optimization Techniques for Training Systems",
    "text": "8.7 Optimization Techniques for Training Systems\nEfficient training of machine learning models relies on identifying and addressing the factors that limit performance and scalability. This section explores a range of optimization techniques designed to improve the efficiency of training systems. By targeting specific bottlenecks, optimizing hardware and software interactions, and employing scalable training strategies, these methods enable practitioners to build systems that effectively utilize resources while minimizing training time.\n\n8.7.1 Identifying Bottlenecks in Training\nEffective optimization of training systems requires a systematic approach to identifying and addressing performance bottlenecks. Bottlenecks can arise at various levels, including computation, memory, and data handling, and they directly impact the efficiency and scalability of the training process.\nComputational bottlenecks can significantly impact training efficiency. One common bottleneck occurs when computational resources, such as GPUs or TPUs, are underutilized. This can happen due to imbalanced workloads or inefficient parallelization strategies. For example, if one device completes its assigned computation faster than others, it remains idle while waiting for the slower devices to catch up. Such inefficiencies reduce the overall training throughput.\nMemory-related bottlenecks are particularly challenging when dealing with large models. Insufficient memory can lead to frequent swapping of data between device memory and slower storage, significantly slowing down the training process. In some cases, the memory required to store intermediate activations during the forward and backward passes can exceed the available capacity, forcing the system to employ techniques such as gradient checkpointing, which trade off computational efficiency for memory savings.\nData handling bottlenecks can severely limit the utilization of computational resources. Training systems often rely on a continuous supply of data to keep computational resources fully utilized. If data loading and preprocessing are not optimized, computational devices may sit idle while waiting for new batches of data to arrive. This issue is particularly prevalent when training on large datasets stored on networked file systems or remote storage solutions.\nIdentifying these bottlenecks typically involves using profiling tools to analyze the performance of the training system. Tools integrated into machine learning frameworks, such as PyTorch’s torch.profiler or TensorFlow’s tf.data analysis utilities, can provide detailed insights into where time and resources are being spent during training. By pinpointing the specific stages or operations that are causing delays, practitioners can design targeted optimizations to address these issues effectively.\n\n\n8.7.2 System-Level Optimizations\nAfter identifying the bottlenecks in a training system, the next step is to implement optimizations at the system level. These optimizations target the underlying hardware, data flow, and resource allocation to improve overall performance and scalability.\nOne essential technique is profiling training workloads. Profiling involves collecting detailed metrics about the system’s performance during training, such as computation times, memory usage, and communication overhead. These metrics help reveal inefficiencies, such as imbalanced resource usage or excessive time spent in specific stages of the training pipeline. Profiling tools such as NVIDIA Nsight Systems or TensorFlow Profiler can provide actionable insights, enabling developers to make informed adjustments to their training configurations.\nLeveraging hardware-specific features is another critical aspect of system-level optimization. Modern accelerators, such as GPUs and TPUs, include specialized capabilities that can significantly enhance performance when utilized effectively. For instance, mixed precision training, which uses lower-precision floating-point formats like FP16 or bfloat166 for computations, can dramatically reduce memory usage and improve throughput without sacrificing model accuracy. Similarly, tensor cores in NVIDIA GPUs are designed to accelerate matrix operations, a common computational workload in deep learning, making them ideal for optimizing forward and backward passes.\n6 Google’s bfloat16 format retains FP32’s dynamic range while reducing precision, making it highly effective for deep learning training on TPUs.Data pipeline optimization is also an important consideration at the system level. Ensuring that data is loaded, preprocessed, and delivered to the training devices efficiently can eliminate potential bottlenecks caused by slow data delivery. Techniques such as caching frequently used data, prefetching batches to overlap computation and data loading, and using efficient data storage formats like TFRecord or RecordIO can help maintain a steady flow of data to computational devices.\n\n\n8.7.3 Software-Level Optimizations\nIn addition to system-level adjustments, software-level optimizations focus on improving the efficiency of training algorithms and their implementation within machine learning frameworks.\nOne effective software-level optimization is the use of fused kernels. In traditional implementations, operations like matrix multiplications, activation functions, and gradient calculations are often executed as separate steps. Fused kernels combine these operations into a single optimized routine, reducing the overhead associated with launching multiple operations and improving cache utilization. Many frameworks, such as PyTorch and TensorFlow, automatically apply kernel fusion where possible, but developers can further optimize custom operations by explicitly using libraries like cuBLAS or cuDNN.\nDynamic graph execution is another powerful technique for software-level optimization. In frameworks that support dynamic computation graphs, such as PyTorch, the graph of operations is constructed on-the-fly during each training iteration. This flexibility allows for fine-grained optimizations based on the specific inputs and outputs of a given iteration. Dynamic graphs also enable more efficient handling of variable-length sequences, such as those encountered in natural language processing tasks.\nGradient accumulation is an additional strategy that can be implemented at the software level to address memory constraints. Instead of updating model parameters after every batch, gradient accumulation allows the system to compute gradients over multiple smaller batches and update parameters only after aggregating them. This approach effectively increases the batch size without requiring additional memory, enabling training on larger datasets or models.\n\n\n8.7.4 Scaling Techniques\nScaling techniques aim to extend the capabilities of training systems to handle larger datasets and models by optimizing the training configuration and resource allocation.\nOne common scaling technique is batch size scaling. Increasing the batch size can reduce the number of synchronization steps required during training, as fewer updates are needed to process the same amount of data. However, larger batch sizes may introduce challenges, such as slower convergence or reduced generalization. Techniques like learning rate scaling and warmup schedules can help mitigate these issues, ensuring stable and effective training even with large batches.\nLayer-freezing strategies provide another method for scaling training systems efficiently. In many scenarios, particularly in transfer learning, the lower layers of a model capture general features and do not need frequent updates. By freezing these layers and allowing only the upper layers to train, memory and computational resources can be conserved, enabling the system to focus its efforts on fine-tuning the most critical parts of the model.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>AI Training</span>"
    ]
  },
  {
    "objectID": "contents/core/training/training.html#training-on-specialized-hardware",
    "href": "contents/core/training/training.html#training-on-specialized-hardware",
    "title": "8  AI Training",
    "section": "8.8 Training on Specialized Hardware",
    "text": "8.8 Training on Specialized Hardware\nThe evolution of specialized machine learning hardware represents a critical development in addressing the computational demands of modern training systems. Each hardware architecture—including GPUs, TPUs, FPGAs, and ASICs—embodies distinct design philosophies and engineering trade-offs that optimize for specific aspects of the training process. These specialized processors have fundamentally altered the scalability and efficiency constraints of machine learning systems, enabling breakthroughs in model complexity and training speed. We briefly examine the architectural principles, performance characteristics, and practical applications of each hardware type, highlighting their indispensable role in shaping the future capabilities of machine learning training systems.\n\n8.8.1 GPUs\n\n\n\n\n\n\nFigure 8.15: GPU design has dramatically accelerated AI training, enabling breakthroughs in large-scale models like GPT-3.\n\n\n\nMachine learning training systems demand immense computational power to process large datasets, perform gradient computations, and update model parameters efficiently. GPUs have emerged as a critical technology to meet these requirements (Figure 8.15), primarily due to their highly parallelized architecture and ability to execute the dense linear algebra operations central to neural network training (Dally, Keckler, and Kirk 2021).\n\nDally, William J, Stephen W Keckler, and David B Kirk. 2021. “Evolution of the Graphics Processing Unit (GPU).” IEEE Micro 41 (6): 42–51.\n\nPatterson, David A., and John L. Hennessy. 2021. Computer Organization and Design RISC-v Edition: The Hardware Software Interface. 2nd ed. San Francisco, CA: Morgan Kaufmann.\nFrom the perspective of training pipeline architecture, GPUs address several key bottlenecks. The large number of cores in GPUs allows for simultaneous processing of thousands of matrix multiplications, accelerating the forward and backward passes of training. In systems where data throughput limits GPU utilization, prefetching and caching mechanisms help maintain a steady flow of data. These optimizations, previously discussed in training pipeline design, are critical to unlocking the full potential of GPUs (Patterson and Hennessy 2021).\nIn distributed training systems, GPUs enable scalable strategies such as data parallelism and model parallelism. NVIDIA’s ecosystem, including tools like NCCL for multi-GPU communication, facilitates efficient parameter synchronization, a frequent challenge in large-scale setups. For example, in training large models like GPT-3, GPUs were used in tandem with distributed frameworks to split computations across thousands of devices while addressing memory and compute scaling issues (Brown, Mann, Ryder, Subbiah, Kaplan, Dhariwal, et al. 2020).\n\n———, et al. 2017b. “Mixed Precision Training.” arXiv Preprint arXiv:1710.03740, October. http://arxiv.org/abs/1710.03740v3.\nHardware-specific features further enhance GPU performance. NVIDIA’s tensor cores, for instance, are optimized for mixed-precision training, which reduces memory usage while maintaining numerical stability (Micikevicius et al. 2017b). This directly addresses memory constraints, a common bottleneck in training massive models. Combined with software-level optimizations like fused kernels, GPUs deliver substantial speedups in both single-device and multi-device configurations.\nA case study that exemplifies the role of GPUs in machine learning training is OpenAI’s use of NVIDIA hardware for large language models. Training GPT-3, with its 175 billion parameters, required distributed processing across thousands of V100 GPUs. The combination of GPU-optimized frameworks, advanced communication protocols, and hardware features enabled OpenAI to achieve this ambitious scale efficiently (Brown, Mann, Ryder, Subbiah, Kaplan, Dhariwal, et al. 2020).\nDespite their advantages, GPUs are not without challenges. Effective utilization of GPUs demands careful attention to workload balancing and inter-device communication. Training systems must also consider the cost implications, as GPUs are resource-intensive and require optimized data centers to operate at scale. However, with innovations like NVLink and CUDA-X libraries, these challenges are continually being addressed.\nIn conclusion, GPUs are indispensable for modern machine learning training systems due to their versatility, scalability, and integration with advanced software frameworks. By addressing key bottlenecks in computation, memory, and distribution, GPUs play a foundational role in enabling the large-scale training pipelines discussed throughout this chapter.\n\n\n8.8.2 TPUs\n\n\n\n\n\n\nFigure 8.16: Tensor Processing Units (TPUs), a single, specific purpose chip desigend for accelerated AI.\n\n\n\nTensor Processing Units (TPUs) and other custom accelerators have been purpose-built to address the unique challenges of large-scale machine learning training. Unlike GPUs, which are versatile and serve a wide range of applications, TPUs are specifically optimized for the computational patterns found in deep learning, such as matrix multiplications and convolutional operations (Jouppi, Young, Patil, Patterson, Agrawal, et al. 2017). These devices address several bottlenecks in training pipelines by offering high throughput, specialized memory handling, and tight integration with specific machine learning frameworks.\n\nJouppi, Norman P., Cliff Young, Nishant Patil, David Patterson, Gaurav Agrawal, Raminder Bajwa, Sarah Bates, et al. 2017. “In-Datacenter Performance Analysis of a Tensor Processing Unit.” In Proceedings of the 44th Annual International Symposium on Computer Architecture, 1–12. ACM. https://doi.org/10.1145/3079856.3080246.\nTPUs were developed by Google with a primary goal: to accelerate machine learning workloads at scale while reducing the energy and infrastructure costs associated with traditional hardware. Their architecture is optimized for tasks that benefit from batch processing, making them particularly effective in distributed training systems where large datasets are split across multiple devices. For example, TPUs leverage systolic array architectures, which perform efficient matrix multiplications by streaming data through a network of processing elements. This design reduces latency and energy consumption, a key advantage when training large-scale models like transformers (Jouppi, Young, Patil, Patterson, Agrawal, et al. 2017).\nFrom the perspective of training pipeline optimization, TPUs simplify integration with data pipelines in the TensorFlow ecosystem. Features such as the TPU runtime and TensorFlow’s tf.data API enable seamless preprocessing, caching, and batching of data to feed the accelerators efficiently (Abadi et al. 2016). Additionally, TPUs are designed to work in pods—clusters of interconnected TPU devices that allow for massive parallelism. In such setups, TPU pods enable hybrid parallelism strategies by combining data parallelism across devices with model parallelism within devices, addressing memory and compute constraints simultaneously.\n\nAbadi, Martín, Ashish Agarwal, Paul Barham, Eugene Brevdo, Zhifeng Chen, Craig Citro, Greg S. Corrado, et al. 2016. “TensorFlow: Large-Scale Machine Learning on Heterogeneous Distributed Systems.” arXiv Preprint arXiv:1603.04467, March. http://arxiv.org/abs/1603.04467v2.\n\nDevlin, Jacob, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018. “BERT: Pre-Training of Deep Bidirectional Transformers for Language Understanding,” October, 4171–86. http://arxiv.org/abs/1810.04805v2.\nOne of the most notable applications of TPUs is in the training of models like BERT and T5. For instance, Google’s use of TPUs to train BERT demonstrates their ability to handle both the memory-intensive requirements of large transformer models and the synchronization challenges of distributed setups (Devlin et al. 2018). By splitting the model across TPU cores and optimizing communication patterns, Google achieved state-of-the-art results with significantly lower training times compared to traditional hardware.\nCustom accelerators such as AWS Trainium and Intel Gaudi chips are also gaining traction in the machine learning ecosystem. These devices are designed to compete with TPUs by offering similar performance benefits while catering to diverse cloud and on-premise environments. For example, AWS Trainium provides deep integration with the AWS ecosystem, allowing users to seamlessly scale their training pipelines with services like Amazon SageMaker.\nWhile TPUs and custom accelerators excel in throughput and energy efficiency, their specialized nature introduces limitations. TPUs, for example, are tightly coupled with Google’s ecosystem, making them less accessible to practitioners using alternative frameworks. Similarly, the high upfront investment required for TPU pods may deter smaller organizations or those with limited budgets. Despite these challenges, the performance gains offered by custom accelerators make them a compelling choice for large-scale training tasks.\nIn summary, TPUs and custom accelerators address many of the key challenges in machine learning training systems, from handling massive datasets to optimizing distributed training. Their unique architectures and deep integration with specific ecosystems make them powerful tools for organizations seeking to scale their training workflows. As machine learning models and datasets continue to grow, these accelerators are likely to play an increasingly central role in shaping the future of AI training.\n\n\n8.8.3 FPGAs\n\n\n\n\n\n\nFigure 8.17: Microsoft’s FPGA advancements through Project Catapult and Project Brainwave highlight their focus on accelerating AI and other cloud workloads with reconfigurable hardware.\n\n\n\nField-Programmable Gate Arrays (FPGAs) are versatile hardware solutions that allow developers to tailor their architecture for specific machine learning workloads. Unlike GPUs or TPUs, which are designed with fixed architectures, FPGAs can be reconfigured dynamically, offering a unique level of flexibility. This adaptability makes them particularly valuable for applications that require customized optimizations, low-latency processing, or experimentation with novel algorithms.\nMicrosoft had been exploring the use of FPGAs for a while, as seen in Figure 8.17, with one prominent example being Project Brainwave. This initiative leverages FPGAs to accelerate machine learning workloads in the Azure cloud. Microsoft chose FPGAs for their ability to provide low-latency inference (not training) while maintaining high throughput. This approach is especially beneficial in scenarios where real-time predictions are critical, such as search engine queries or language translation services. By integrating FPGAs directly into their data center network, Microsoft has achieved significant performance gains while minimizing power consumption.\nFrom a training perspective, FPGAs offer unique advantages in optimizing training pipelines. Their reconfigurability allows them to implement custom dataflow architectures tailored to specific model requirements. For instance, data preprocessing and augmentation steps, which can often become bottlenecks in GPU-based systems, can be offloaded to FPGAs, freeing up GPUs for core training tasks. Additionally, FPGAs can be programmed to perform operations such as sparse matrix multiplications, which are common in recommendation systems and graph-based models but are less efficient on traditional accelerators (Putnam et al. 2014).\n\nPutnam, Andrew, Adrian M. Caulfield, Eric S. Chung, Derek Chiou, Kypros Constantinides, John Demme, Hadi Esmaeilzadeh, et al. 2014. “A Reconfigurable Fabric for Accelerating Large-Scale Datacenter Services.” ACM SIGARCH Computer Architecture News 42 (3): 13–24. https://doi.org/10.1145/2678373.2665678.\nIn distributed training systems, FPGAs provide fine-grained control over communication patterns. This control allows developers to optimize inter-device communication and memory access, addressing challenges such as parameter synchronization overheads. For example, FPGAs can be configured to implement custom all-reduce algorithms for gradient aggregation, reducing latency compared to general-purpose hardware.\nDespite their benefits, FPGAs come with challenges. Programming FPGAs requires expertise in hardware description languages (HDLs) like Verilog or VHDL, which can be a barrier for many machine learning practitioners. To address this, frameworks like Xilinx’s Vitis AI and Intel’s OpenVINO have simplified FPGA programming by providing tools and libraries tailored for AI workloads. However, the learning curve remains steep compared to the well-established ecosystems of GPUs and TPUs.\nMicrosoft’s use of FPGAs highlights their potential to integrate seamlessly into existing machine learning workflows. By incorporating FPGAs into Azure, Microsoft has demonstrated how these devices can complement other accelerators, optimizing end-to-end pipelines for both training and inference. This hybrid approach leverages the strengths of FPGAs for specific tasks while relying on GPUs or CPUs for others, creating a balanced and efficient system.\nIn summary, FPGAs offer a compelling solution for machine learning training systems that require customization, low latency, or novel optimizations. While their adoption may be limited by programming complexity, advancements in tooling and real-world implementations like Microsoft’s Project Brainwave demonstrate their growing relevance in the AI hardware ecosystem.\n\n\n8.8.4 ASICs\n\n\n\n\n\n\nFigure 8.18: Cerebras Wafer Scale Engine (WSE) 2 is the largest AI chip ever built with nearly a cores.\n\n\n\nApplication-Specific Integrated Circuits (ASICs) represent a class of hardware designed for specific tasks, offering unparalleled efficiency and performance by eschewing the general-purpose flexibility of GPUs or FPGAs. Among the most innovative examples of ASICs for machine learning training is the Cerebras Wafer-Scale Engine (WSE), as shown in Figure 8.18, which stands apart for its unique approach to addressing the computational and memory challenges of training massive machine learning models.\nThe Cerebras WSE is unlike traditional chips in that it is a single wafer-scale processor, spanning the entire silicon wafer rather than being cut into smaller chips. This architecture enables Cerebras to pack 2.6 trillion transistors and 850,000 cores onto a single device. These cores are connected via a high-bandwidth, low-latency interconnect, allowing data to move across the chip without the bottlenecks associated with external communication between discrete GPUs or TPUs (Feldman et al. 2020).\n\nFeldman, Andrew, Sean Lie, Michael James, et al. 2020. “The Cerebras Wafer-Scale Engine: Opportunities and Challenges of Building an Accelerator at Wafer Scale.” IEEE Micro 40 (2): 20–29. https://doi.org/10.1109/MM.2020.2975796.\nFrom a machine learning training perspective, the WSE addresses several critical bottlenecks:\n\nData Movement: In traditional distributed systems, significant time is spent transferring data between devices. The WSE eliminates this by keeping all computations and memory on a single wafer, drastically reducing communication overhead.\nMemory Bandwidth: The WSE integrates 40 GB of high-speed on-chip memory directly adjacent to its processing cores. This proximity allows for near-instantaneous access to data, overcoming the latency challenges that GPUs often face when accessing off-chip memory.\nScalability: While traditional distributed systems rely on complex software frameworks to manage multiple devices, the WSE simplifies scaling by consolidating all resources into one massive chip. This design is particularly well-suited for training large language models and other deep learning architectures that require significant parallelism.\n\nA key example of Cerebras’ impact is its application in natural language processing (NLP). Organizations using the WSE have demonstrated substantial speedups in training transformer models, which are notoriously compute-intensive due to their reliance on attention mechanisms. By leveraging the chip’s massive parallelism and memory bandwidth, training times for models like BERT have been significantly reduced compared to GPU-based systems (Brown, Mann, Ryder, Subbiah, Kaplan, Dhariwal, et al. 2020).\n\nBrown, Tom B., Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, et al. 2020. “Language Models Are Few-Shot Learners.” arXiv Preprint arXiv:2005.14165, May. http://arxiv.org/abs/2005.14165v4.\nHowever, the Cerebras WSE also comes with limitations. Its single-chip design is optimized for specific use cases, such as dense matrix computations in deep learning, but may not be as versatile as multi-purpose hardware like GPUs or FPGAs. Additionally, the cost of acquiring and integrating such a specialized device can be prohibitive for smaller organizations or those with diverse workloads.\nCerebras’ strategy of targeting the largest models aligns with the trends discussed earlier in this chapter, such as the growing emphasis on scaling techniques and hybrid parallelism strategies. The WSE’s unique design addresses challenges like memory bottlenecks and inter-device communication overhead, making it a pioneering solution for next-generation AI workloads.\nIn conclusion, the Cerebras Wafer-Scale Engine exemplifies how ASICs can push the boundaries of what is possible in machine learning training. By addressing fundamental bottlenecks in computation and data movement, the WSE offers a glimpse into the future of specialized hardware for AI, where the integration of highly optimized, task-specific architectures unlocks unprecedented performance.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>AI Training</span>"
    ]
  },
  {
    "objectID": "contents/core/training/training.html#conclusion",
    "href": "contents/core/training/training.html#conclusion",
    "title": "8  AI Training",
    "section": "8.9 Conclusion",
    "text": "8.9 Conclusion\nAI training systems are built upon a foundation of mathematical principles, computational strategies, and architectural considerations. The exploration of neural network computation has shown how core operations, activation functions, and optimization algorithms come together to enable efficient model training, while also emphasizing the trade-offs that must be balanced between memory, computation, and performance.\nThe design of training pipelines incorporates key components such as data flows, forward and backward passes, and memory management. Understanding these elements in conjunction with hardware execution patterns is essential for achieving efficient and scalable training processes. Strategies like parameter updates, prefetching, and gradient accumulation further enhance the effectiveness of training by optimizing resource utilization and reducing computational bottlenecks.\nDistributed training systems, including data parallelism, model parallelism, and hybrid approaches, are topics that we examined as solutions for scaling AI training to larger datasets and models. Each approach comes with its own benefits and challenges, highlighting the need for careful consideration of system requirements and resource constraints.\nAltogether, the combination of theoretical foundations and practical implementations forms a cohesive framework for addressing the complexities of AI training. By leveraging this knowledge, it is possible to design robust, efficient systems capable of meeting the demands of modern machine learning applications.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>AI Training</span>"
    ]
  },
  {
    "objectID": "contents/core/efficient_ai/efficient_ai.html",
    "href": "contents/core/efficient_ai/efficient_ai.html",
    "title": "9  Efficient AI",
    "section": "",
    "text": "Purpose\nResources: Slides, Videos, Exercises\nWhat principles guide the efficient design of machine learning systems, and why is understanding the interdependence of key resources essential?\nMachine learning systems are shaped by the complex interplay among data, models, and computing resources. Decisions on efficiency in one dimension often have ripple effects in the others, presenting both opportunities for synergy and inevitable trade-offs. Understanding these individual components and their interdependencies exposes not only how systems can be optimized but also why these optimizations are crucial for achieving scalability, sustainability, and real-world applicability. The relationship between data, model, and computing efficiency forms the basis for designing machine learning systems that maximize capabilities while working within resource limitations. Each efficiency decision represents a balance between performance and practicality, underscoring the significance of a holistic approach to system design. Exploring these relationships equips us with the strategies necessary to navigate the intricacies of developing efficient, impactful AI solutions.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Efficient AI</span>"
    ]
  },
  {
    "objectID": "contents/core/efficient_ai/efficient_ai.html#purpose",
    "href": "contents/core/efficient_ai/efficient_ai.html#purpose",
    "title": "9  Efficient AI",
    "section": "",
    "text": "Learning Objectives\n\n\n\n\nDefine the principles of algorithmic, compute and data efficiency in AI systems.\nIdentify and analyze trade-offs between algorithmic, compute, and data efficiency in system design.\nApply strategies for achieving efficiency across diverse deployment contexts, such as edge, cloud, and Tiny ML applications.\nExamine the historical evolution and emerging trends in machine learning efficiency.\nEvaluate the broader ethical and environmental implications of efficient AI system design.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Efficient AI</span>"
    ]
  },
  {
    "objectID": "contents/core/efficient_ai/efficient_ai.html#overview",
    "href": "contents/core/efficient_ai/efficient_ai.html#overview",
    "title": "9  Efficient AI",
    "section": "9.1 Overview",
    "text": "9.1 Overview\nMachine learning systems have become ubiquitous. As these systems grow in complexity and scale, they must operate effectively across a wide range of deployments and scenarios. This requires careful consideration of factors such as processing speed, memory usage, and power consumption to ensure that models can handle large workloads, operate on energy-constrained devices, and remain cost-effective.\nAchieving this balance involves navigating trade-offs. For instance, in autonomous vehicles, reducing a model’s size to fit the low-power constraints of an edge device in a car might slightly decrease accuracy, but it ensures real-time processing and decision-making. Conversely, a cloud-based system can afford higher model complexity for improved accuracy but at the cost of increased latency and energy consumption.\nIn the medical field, deploying machine learning models on portable devices for diagnostics requires efficient models that can operate with limited computational resources and power, ensuring accessibility in remote or resource-constrained areas. Conversely, hospital-based systems can leverage more powerful hardware to run complex models for detailed analysis, albeit with higher energy demands.\nUnderstanding and managing these trade-offs is crucial for designing machine learning systems that meet diverse application needs within real-world constraints. The implications of these design choices extend beyond performance and cost. Efficient systems can be deployed across diverse environments, from cloud infrastructures to edge devices, enhancing accessibility and adoption. Additionally, they help reduce the environmental impact of machine learning workloads by lowering energy consumption and carbon emissions, aligning technological progress with ethical and ecological responsibilities.\nThis chapter focuses on the “why” and “how” of efficiency in machine learning systems. By establishing the foundational principles and exploring strategies to achieve efficiency, it sets the stage for deeper discussions on topics such as optimization, deployment, and sustainability in later chapters.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Efficient AI</span>"
    ]
  },
  {
    "objectID": "contents/core/efficient_ai/efficient_ai.html#ai-efficiency-evolution",
    "href": "contents/core/efficient_ai/efficient_ai.html#ai-efficiency-evolution",
    "title": "9  Efficient AI",
    "section": "9.2 AI Efficiency Evolution",
    "text": "9.2 AI Efficiency Evolution\nEfficiency in machine learning systems has evolved significantly over time, reflecting shifts in the field’s priorities and constraints. To understand this evolution, it is helpful to focus on three interrelated dimensions: algorithmic efficiency, compute efficiency, and data efficiency. Each dimension represents a critical aspect of machine learning systems and has played a central role during different eras of the field’s development. The timeline in Figure 9.1 illustrates this evolution, which we will discuss in the following sections.\n\nAlgorithmic Efficiency: Model efficiency focuses on designing models that deliver high performance while minimizing resource consumption.\nCompute Efficiency: Compute efficiency addresses the effective utilization of computational resources, including energy and hardware infrastructure.\nData Efficiency: Data efficiency emphasizes optimizing the amount and quality of data required to achieve desired performance.\n\n\n\n\n\n\n\ntimeline\n    dateFormat YYYY\n    section Algorithmic Efficiency\n        Algorithmic Efficiency: 1980: 2010\n        Deep Learning Era: 2010: 2022\n        Modern Efficiency: 2023: Future\n    section Compute Efficiency\n        General-Purpose Computing: 1980: 2010\n        Accelerated Computing: 2010: 2022\n        Sustainable Computing: 2023: Future\n    section Data Efficiency\n        Data Scarcity: 1980: 2010\n        Big Data Era: 2010: 2022\n        Data-Centric AI: 2023: Future\n\n\n\n\nFigure 9.1: Evolution of AI Efficiency over the past few decades.\n\n\n\n\n\n\n9.2.1 Evolution of Algorithmic Efficiency\nModel efficiency addresses the design and optimization of machine learning models to deliver high performance while minimizing computational and memory requirements. It is a critical component of machine learning systems, enabling models to operate effectively across a range of platforms, from cloud servers to resource-constrained edge devices. The evolution of algorithmic efficiency mirrors the broader trajectory of machine learning itself, shaped by algorithmic advances, hardware developments, and the increasing complexity of real-world applications.\n\nThe Era of Algorithmic Efficiency (1980s-2010)\nDuring the early decades of machine learning, algorithmic efficiency was closely tied to algorithmic efficiency. Researchers focused on developing models that were compact, interpretable, and computationally inexpensive, as hardware resources were limited. Algorithms such as decision trees, support vector machines, and logistic regression exemplified this era. These models achieved efficiency through their simplicity and their ability to generalize well without requiring vast amounts of computational power or memory.\nNeural networks also began to emerge during this period, but they were constrained by the limited computational capacity of the time. This led to careful optimizations in their design, such as limiting the number of layers or neurons to keep computations manageable. Efficiency was achieved not only through model simplicity but also through innovations in optimization techniques, such as the adoption of stochastic gradient descent1, which made training more practical for the hardware available.\n1 Stochastic Gradient Descent (SGD): A widely used optimization algorithm that updates model parameters iteratively based on a random subset (batch) of the training data, enabling faster convergence with limited resources.The era of algorithmic efficiency laid the groundwork for machine learning by emphasizing the importance of achieving high performance under strict resource constraints. It was an era of problem-solving through mathematical rigor and computational restraint.\n\n\nThe Shift to Deep Learning (2010-2022)\nThe introduction of deep learning in the early 2010s marked a turning point for algorithmic efficiency. Neural networks, which had previously been constrained by hardware limitations, now benefited from advancements in computational power, particularly the adoption of GPUs (Krizhevsky, Sutskever, and Hinton 2012). This capability allowed researchers to train larger, more complex models, leading to breakthroughs in tasks such as image recognition, natural language processing, and speech synthesis.\n\nKrizhevsky, Alex, Ilya Sutskever, and Geoffrey E. Hinton. 2012. “ImageNet Classification with Deep Convolutional Neural Networks.” Communications of the ACM 60 (6): 84–90. https://doi.org/10.1145/3065386.\n\nLeCun, Yann, John S. Denker, and Sara A. Solla. 1990. “Optimal Brain Damage.” In Advances in Neural Information Processing Systems, 2:598–605. Morgan-Kaufmann.\n\nJacob, Benoit, Skirmantas Kligys, Bo Chen, Menglong Zhu, Matthew Tang, Andrew Howard, Hartwig Adam, and Dmitry Kalenichenko. 2018. “Quantization and Training of Neural Networks for Efficient Integer-Arithmetic-Only Inference.” In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2704–13.\n\nHinton, Geoffrey, Oriol Vinyals, and Jeff Dean. 2015. “Distilling the Knowledge in a Neural Network.” arXiv Preprint arXiv:1503.02531. https://arxiv.org/abs/1503.02531.\nHowever, the growing size and complexity of these models introduced new challenges. Larger models required significant computational resources and memory, making them difficult to deploy in practical applications. To address these challenges, researchers developed techniques to reduce model size and computational requirements without sacrificing accuracy. Pruning, for instance, involved removing redundant or less significant connections within a neural network, reducing both the model’s parameters and its computational overhead (LeCun, Denker, and Solla 1990). Quantization focused on lowering the precision of numerical representations, enabling models to run faster and with less memory (Jacob et al. 2018). Knowledge distillation allowed large, resource-intensive models (referred to as “teachers”) to transfer their knowledge to smaller, more efficient models (referred to as “students”), achieving comparable performance with reduced complexity (Hinton, Vinyals, and Dean 2015).\nAt the same time, new architectures specifically designed for efficiency began to emerge. Models such as MobileNet (Howard et al. 2017), EfficientNet (Tan and Le 2019), and SqueezeNet (Iandola et al. 2016) demonstrated that compact designs could deliver high performance, enabling their deployment on devices with limited computational power, such as smartphones and IoT devices2.\n\nHoward, Andrew G., Menglong Zhu, Bo Chen, Dmitry Kalenichenko, Weijun Wang, Tobias Weyand, Marco Andreetto, and Hartwig Adam. 2017. “MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications.” In arXiv Preprint arXiv:1704.04861. https://arxiv.org/abs/1704.04861.\n\nTan, Mingxing, and Quoc V. Le. 2019. “EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks.” In Proceedings of the International Conference on Machine Learning (ICML), 6105–14.\n\nIandola, Forrest N., Song Han, Matthew W. Moskewicz, Khalid Ashraf, William J. Dally, and Kurt Keutzer. 2016. “SqueezeNet: AlexNet-Level Accuracy with 50x Fewer Parameters and &lt;0.5MB Model Size.” In arXiv Preprint arXiv:1602.07360. https://arxiv.org/abs/1602.07360.\n2 MobileNet/EfficientNet/SqueezeNet: Compact neural network architectures designed for efficiency, balancing high performance with reduced computational demands. MobileNet introduced depthwise separable convolutions (2017), EfficientNet applied compound scaling (2019), and SqueezeNet focused on reducing parameters using 1x1 convolutions (2016).\n\nThe Modern Era of Algorithmic Efficiency (2023-Future)\nAs machine learning systems continue to grow in scale and complexity, the focus on algorithmic efficiency has expanded to address sustainability and scalability. Today’s challenges require balancing performance with resource efficiency, particularly as models like GPT-4 and beyond are applied to increasingly diverse tasks and environments. One emerging approach involves sparsity, where only the most critical parameters of a model are retained, significantly reducing computational and memory demands. Hardware-aware design has also become a priority, as researchers optimize models to take full advantage of specific accelerators, such as GPUs, TPUs, and edge processors. Another important trend is parameter-efficient fine-tuning, where large pre-trained models can be adapted to new tasks by updating only a small subset of parameters. Low-Rank Adaptation (LoRA)3 and prompt-tuning exemplify this approach, allowing systems to achieve task-specific performance while maintaining the efficiency advantages of smaller models.\n3 Low-Rank Adaptation (LoRA): A technique that adapts large pre-trained models to new tasks by updating only a small subset of parameters, significantly reducing computational and memory requirements.These advancements reflect a broader shift in focus: from scaling models indiscriminately to creating architectures that are purpose-built for efficiency. This modern era emphasizes not only technical excellence but also the practicality and sustainability of machine learning systems.\n\n\nThe Role of Algorithmic Efficiency in System Design\nModel efficiency is fundamental to the design of scalable and sustainable machine learning systems. By reducing computational and memory demands, efficient models lower energy consumption and operational costs, making machine learning systems accessible to a wider range of applications and deployment environments. Moreover, algorithmic efficiency complements other dimensions of efficiency, such as compute and data efficiency, by reducing the overall burden on hardware and enabling faster training and inference cycles.\nNotably, as Figure 9.2 shows, by 9, the computational resources needed to train a neural network to achieve AlexNet-level performance on ImageNet classification had decreased by 44x compared to 2012. This improvement—halving every 16 months—outpaced the hardware efficiency gains of Moore’s Law4. Such rapid progress demonstrates the role of algorithmic advancements in driving efficiency alongside hardware innovations (Hernandez, Brown, et al. 2020).\n4 Moore’s Law: An observation made by Gordon Moore in 1965, stating that the number of transistors on a microchip doubles approximately every two years, leading to an exponential increase in computational power and a corresponding decrease in relative cost.\nHernandez, Danny, Tom B. Brown, et al. 2020. “Measuring the Algorithmic Efficiency of Neural Networks.” OpenAI Blog. https://openai.com/research/ai-and-efficiency.\n\n\n\n\n\n\nFigure 9.2: Within just seven years, 44 times less compute was required to achieve AlexNet performance. Source: (“AI and Efficiency | OpenAI”).\n\n\n“AI and Efficiency | OpenAI.” https://openai.com/index/ai-and-efficiency/.\n\n\nThe evolution of algorithmic efficiency, from algorithmic innovations to hardware-aware optimization, is of importance in machine learning. As the field advances, algorithmic efficiency will remain central to the design of systems that are high-performing, scalable, and sustainable.\n\n\n\n9.2.2 Evolution of Compute Efficiency\nCompute efficiency focuses on the effective use of hardware and computational resources to train and deploy machine learning models. It encompasses strategies for reducing energy consumption, optimizing processing speed, and leveraging hardware capabilities to achieve scalable and sustainable system performance. The evolution of compute efficiency is closely tied to advancements in hardware technologies, reflecting the growing demands of machine learning applications over time.\n\nThe Era of General-Purpose Computing (1980s-2010)\nIn the early days of machine learning, compute efficiency was shaped by the limitations of general-purpose CPUs. During this period, machine learning models had to operate within strict computational constraints, as specialized hardware for machine learning did not yet exist. Efficiency was achieved through algorithmic innovations, such as simplifying mathematical operations, reducing model size, and optimizing data handling to minimize computational overhead.\nResearchers worked to maximize the capabilities of CPUs by using parallelism where possible, though options were limited. Training times for models were often measured in days or weeks, as even relatively small datasets and models pushed the boundaries of available hardware. The focus on compute efficiency during this era was less about hardware optimization and more about designing algorithms that could run effectively within these constraints.\n\n\nThe Rise of Accelearted Computing (2010-2022)\nThe introduction of deep learning in the early 2010s brought a seismic shift in the landscape of compute efficiency. Models like AlexNet and ResNet showed the potential of neural networks, but their computational demands quickly surpassed the capabilities of traditional CPUs. As shown in Figure 9.3, this marked the beginning of an era of exponential growth in compute usage. OpenAI’s analysis reveals that the amount of compute used in AI training has increased 300,000 times since 2012, doubling approximately every 3.4 months—a rate far exceeding Moore’s Law (Amodei, Hernandez, et al. 2018).\n\nAmodei, Dario, Danny Hernandez, et al. 2018. “AI and Compute.” OpenAI Blog. https://openai.com/research/ai-and-compute.\n\n\n\n\n\n\nFigure 9.3: From AlexNet to AlphaGo Zero, there has been a 300,000x increase in demand for computing power over seven years. Source: (“AI and Compute | OpenAI”).\n\n\n“AI and Compute | OpenAI.” https://openai.com/index/ai-and-compute/.\n\n\nThis rapid growth was driven not only by the adoption of GPUs, which offered unparalleled parallel processing capabilities, but also by the willingness of researchers to scale up experiments by using large GPU clusters. Specialized hardware accelerators such as Google’s Tensor Processing Units (TPUs) and application-specific integrated circuits (ASICs) further revolutionized compute efficiency. These innovations enabled significant reductions in training times for deep learning models, transforming tasks that once took weeks into operations completed in hours or days.\nThe rise of large-scale compute also highlighted the complementary relationship between algorithmic innovation and hardware efficiency. Advances such as architecture search and massive batch processing leveraged the increasing availability of computational power, demonstrating that more compute could directly lead to better performance in many domains.\n\n\nThe Era of Sustainable Computing (2023-Future)\nAs machine learning systems scale further, compute efficiency has become closely tied to sustainability. Training state-of-the-art models like GPT-4 requires massive computational resources, leading to increased attention on the environmental impact of large-scale computing. The focus today is on optimizing hardware utilization and minimizing energy consumption, both in cloud data centers and at the edge.\nOne key trend is the adoption of energy-aware scheduling and resource allocation techniques, which ensure that computational workloads are distributed efficiently across available hardware (D. Patterson et al. 2021). Researchers are also developing methods to dynamically adjust precision levels during training and inference, using lower precision operations (e.g., mixed-precision training) to reduce power consumption without sacrificing accuracy.\n\nPatterson, David, Joseph Gonzalez, Quoc V. Le, Chen Liang, Lluis-Miquel Munguia, Daniel Rothchild, David So, Morgan Texier, and Jeff Dean. 2021. “Carbon Emissions and Large Neural Network Training.” arXiv Preprint arXiv:2104.10350. https://arxiv.org/abs/2104.10350.\nAnother focus is on distributed systems, where compute efficiency is achieved by splitting workloads across multiple machines. Techniques such as model parallelism and data parallelism allow large-scale models to be trained more efficiently, leveraging clusters of GPUs or TPUs to maximize throughput. These methods reduce training times while minimizing the idle time of hardware resources.\nAt the edge, compute efficiency is evolving to address the growing demand for real-time processing in energy-constrained environments. Innovations such as hardware-aware model optimization, lightweight inference engines, and adaptive computing architectures are paving the way for highly efficient edge systems. These advancements are critical for enabling applications like autonomous vehicles and smart home devices, where latency and energy efficiency are paramount.\n\n\nThe Role of Compute Efficiency in ML Systems\nCompute efficiency is a critical enabler of system-wide performance and scalability. By optimizing hardware utilization and energy consumption, it ensures that machine learning systems remain practical and cost-effective, even as models and datasets grow larger. Moreover, compute efficiency directly complements model and data efficiency. For example, compact models reduce computational requirements, while efficient data pipelines streamline hardware usage.\nThe evolution of compute efficiency highlights its essential role in addressing the growing demands of modern machine learning systems. From early reliance on CPUs to the emergence of specialized accelerators and sustainable computing practices, this dimension remains central to building scalable, accessible, and environmentally responsible machine learning systems.\n\n\n\n9.2.3 Evolution of Data Efficiency\nData efficiency focuses on optimizing the amount and quality of data required to train machine learning models effectively. As datasets have grown in scale and complexity, managing data efficiently has become an increasingly critical challenge for machine learning systems. While historically less emphasized than model or compute efficiency, data efficiency has emerged as a pivotal dimension, driven by the rising costs of data collection, storage, and processing. Its evolution reflects the changing role of data in machine learning, from a scarce resource to a massive but unwieldy asset.\n\nThe Era of Data Scarcity (1980s-2010)\nIn the early days of machine learning, data efficiency was not a significant focus, largely because datasets were relatively small and manageable. The challenge during this period was often acquiring enough labeled data to train models effectively. Researchers relied heavily on curated datasets, such as UCI’s Machine Learning Repository, which provided clean, well-structured data for experimentation. Feature selection and dimensionality reduction techniques, such as principal component analysis (PCA), were common methods for ensuring that models extracted the most valuable information from limited data.\nDuring this era, data efficiency was achieved through careful preprocessing and data cleaning. Algorithms were designed to work well with relatively small datasets, and computational limitations reinforced the need for data parsimony. These constraints shaped the development of techniques that maximized performance with minimal data, ensuring that every data point contributed meaningfully to the learning process.\n\n\nThe Era of Big Data (2010-2022)\nThe advent of deep learning in the 2010s transformed the role of data in machine learning. Models such as AlexNet and GPT-3 demonstrated that larger datasets often led to better performance, particularly for complex tasks like image classification and natural language processing. This marked the beginning of the “big data” era, where the focus shifted from making the most of limited data to scaling data collection and processing to unprecedented levels.\nHowever, this reliance on large datasets introduced significant inefficiencies. Data collection became a costly and time-consuming endeavor, requiring vast amounts of labeled data for supervised learning tasks. To address these challenges, researchers developed techniques to enhance data efficiency, even as datasets continued to grow. Transfer learning allowed pre-trained models to be fine-tuned on smaller datasets, reducing the need for task-specific data (Yosinski et al. 2014). Data augmentation techniques, such as image rotations or text paraphrasing, artificially expanded datasets by creating new variations of existing samples. Additionally, active learning[^fn-active-learning] prioritized labeling only the most informative data points, minimizing the overall labeling effort while maintaining performance (Settles 2009a).\n\nYosinski, Jason, Jeff Clune, Yoshua Bengio, and Hod Lipson. 2014. “How Transferable Are Features in Deep Neural Networks?” 27: 3320–28. https://proceedings.neurips.cc/paper/2014/file/375c71349b295fbe2dcdca9206f20a06-Paper.pdf.\n\n———. 2009a. “Active Learning Literature Survey.” Computer Sciences Technical Report. http://burrsettles.com/pub/settles.activelearning.pdf.\nDespite these advancements, the “more data is better” paradigm dominated this period, with less attention paid to streamlining data usage. As a result, the environmental and economic costs of managing large datasets began to emerge as significant concerns.\n\n\nThe Modern Era of Data Efficiency (2023-Future)\nAs machine learning systems grow in scale, the inefficiencies of big data have become increasingly apparent. The costs of data collection, storage, and processing, combined with the environmental impact of large-scale training, have made data efficiency a central focus of modern research and development.\nEmerging techniques aim to reduce the dependency on massive datasets while maintaining or improving model performance. Self-supervised learning has gained prominence as a way to extract meaningful representations from unlabeled data, significantly reducing the need for human-labeled datasets. Synthetic data generation, which creates artificial data points that mimic real-world distributions, offers another path to increasing data efficiency. These methods enable models to train effectively without relying on exhaustive real-world data collection efforts.\nActive learning and curriculum learning are also gaining traction. Active learning focuses on selectively labeling only the most informative examples, while curriculum learning structures training to start with simpler data and progress to more complex examples, improving learning efficiency. These approaches reduce the amount of data required for training, streamlining the pipeline and lowering computational costs.\nIn addition, there is growing interest in data-centric AI, where the emphasis shifts from optimizing models to improving data quality. Better data preprocessing, de-duplication, and cleaning can lead to significant gains in performance, even without changes to the underlying model. This approach aligns with broader sustainability goals by reducing redundancy and waste in data handling.\n\n\nThe Role of Data Efficiency in Machine Learning Systems\nData efficiency is integral to the design of scalable and sustainable machine learning systems. By reducing the dependency on large datasets, data efficiency directly impacts both model and compute efficiency. For instance, smaller, higher-quality datasets reduce training times and computational demands, while enabling models to generalize more effectively. This dimension of efficiency is particularly critical for edge applications, where bandwidth and storage limitations make it impractical to rely on large datasets.\nAs the field advances, data efficiency will play an increasingly prominent role in addressing the challenges of scalability, accessibility, and sustainability. By rethinking how data is collected, processed, and utilized, machine learning systems can achieve higher levels of efficiency across the entire pipeline.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Efficient AI</span>"
    ]
  },
  {
    "objectID": "contents/core/efficient_ai/efficient_ai.html#system-efficiency",
    "href": "contents/core/efficient_ai/efficient_ai.html#system-efficiency",
    "title": "9  Efficient AI",
    "section": "9.3 System Efficiency",
    "text": "9.3 System Efficiency\nThe efficiency of machine learning systems has become a crucial area of focus. Optimizing these systems helps us ensure that they are not only high-performing but also adaptable, cost-effective, and environmentally sustainable. Understanding the concept of ML system efficiency, its key dimensions, and the interplay between them is essential for uncovering how these principles can drive impactful, scalable, and responsible AI solutions.\n\n9.3.1 Defining ML System Efficiency\nMachine learning is a highly complex field, involving a multitude of components across a vast domain. Despite its complexity, there has not been a synthesis of what it truly means to have an efficient machine learning system. Here, we take a first step towards defining this concept.\n\n\n\n\n\n\nDefinition of Machine Learning System Efficiency\n\n\n\nMachine Learning System Efficiency refers to the optimization of machine learning systems across three interconnected dimensions—algorithmic efficiency, compute efficiency, and data efficiency. Its goal is to minimize computational, memory, and energy demands while maintaining or improving system performance. This efficiency ensures that machine learning systems are scalable, cost-effective, and sustainable, which allows them to adapt to diverse deployment contexts, ranging from cloud data centers to edge devices. Achieving system efficiency, however, often requires navigating trade-offs between dimensions, such as balancing model complexity with hardware constraints or reducing data dependency without compromising generalization.\n\n\nThis definition highlights the holistic nature of efficiency in machine learning systems, emphasizing that the three dimensions—algorithmic efficiency, compute efficiency, and data efficiency—are deeply interconnected. Optimizing one dimension often affects the others, either by creating synergies or necessitating trade-offs. Understanding these interdependencies is essential for designing systems that are not only performant but also scalable, adaptable, and sustainable (Jeffrey and Benares 2022; D. A. Patterson and Hennessy 2021).\n\nJeffrey, Dave, and Lisa Benares. 2022. “AI Efficiency: Balancing Performance with Energy Sustainability.” Journal of AI Research 75: 345–60.\n\nPatterson, David A, and John L Hennessy. 2021. “Carbon Emissions and Large Neural Network Optimization.” Communications of the ACM 64 (7): 54–61.\nTo better understand this interplay, we must examine how these dimensions reinforce one another and the challenges in balancing them. While each dimension contributes uniquely, the true complexity lies in their interdependencies. Historically, optimizations were often approached in isolation. However, recent years have seen a shift towards co-design, where multiple dimensions are optimized concurrently to achieve superior overall efficiency.\n\n\n9.3.2 Interdependencies Between Efficiency Dimensions\nThe efficiency of machine learning systems is inherently a multifaceted challenge that encompasses model design, computational resources, and data utilization. These dimensions—algorithmic efficiency, compute efficiency, and data efficiency—are deeply interdependent, forming a dynamic ecosystem where improvements in one area often ripple across the others. Understanding these interdependencies is crucial for building scalable, cost-effective, and high-performing systems that can adapt to diverse application demands.\nThis interplay is best captured through a conceptual visualization. Figure 9.4 illustrates how these efficiency dimensions overlap and interact with each other in a simple Venn diagram. Each circle represents one of the efficiency dimensions, and their intersections highlight the areas where they influence one another, which we will explore next.\n\n\n\n\n\n\n\n\n\nTransparency\n\n\n\n1\n\nModel\nEfficiency\n\n\n\n2\n\nCompute\nEfficiency\n\n\n\n1--2\n\n\n\n\n3\n\nData\nEfficiency\n\n\n\n2--3\n\n\n\n\n3--1\n\n\n\n\n\n\n\nFigure 9.4: Interdependence of the different efficiency dimensions.\n\n\n\n\n\n\nAlgorithmic Efficiency Reinforces Compute and Data Efficiency\nModel efficiency is essential for efficient machine learning system. By designing compact and streamlined models, we can significantly reduce computational demands, leading to faster and more cost-effective inference. These compact models not only consume fewer resources but are also easier to deploy across diverse environments, such as resource-constrained edge devices or energy-intensive cloud infrastructure.\nMoreover, efficient models often require less data for training, as they avoid over-parameterization and focus on capturing essential patterns within the data. This results in shorter training times and reduced dependency on massive datasets, which can be expensive and time-consuming to curate. Ultimately, optimizing algorithmic efficiency creates a ripple effect, enhancing both compute and data efficiency.\n\n\nCompute Efficiency Supports Model and Data Efficiency\nCompute efficiency is a key factor in optimizing machine learning systems. Through maximizing hardware utilization and employing efficient algorithms, compute efficiency speeds up both model training and inference processes, ultimately cutting down on the time and resources needed, even when working with complex or large-scale models.\nEfficient computation also enables models to handle large datasets more effectively, minimizing bottlenecks associated with memory or processing power. Techniques such as parallel processing, hardware accelerators (e.g., GPUs, TPUs), and energy-aware scheduling contribute to reducing overhead while ensuring peak performance. As a result, compute efficiency not only supports model optimization but also enhances data handling, making it feasible to train models on high-quality datasets without unnecessary computational strain.\n\n\nData Efficiency Strengthens Model and Compute Efficiency\nData efficiency plays an important role in bolstering both model and compute efficiency. By focusing on high-quality, compact datasets, the training process becomes more streamlined, requiring fewer computational resources to achieve comparable or superior model performance. This targeted approach reduces data redundancy and minimizes the overhead associated with handling excessively large datasets.\nFurthermore, data efficiency enables more focused model design. When datasets emphasize relevant features and minimize noise, models can achieve high performance with simpler architectures. This reduces computational requirements during both training and inference, allowing more efficient use of computing resources.\n\n\n\n9.3.3 Examples of ML System Efficiency\nTo understand system efficiency in machine learning, it helps to start with familiar, resource-constrained contexts like mobile devices and then progressively explore environments with increasing complexity and scale, such as Edge ML, Cloud ML, and Tiny ML. This progression mirrors how efficiency is often conceptualized—from optimizing battery usage and resource consumption in smartphones to achieving high-throughput performance in large-scale cloud systems or extreme efficiency in Tiny ML devices. Here, we study practical examples that illustrate how algorithmic efficiency, computing efficiency, and data efficiency intersect to address the unique challenges in different scenarios.\n\nMobile ML Deployment\nMobile devices, such as smartphones, provide an accessible introduction to the interplay of efficiency dimensions. Consider a photo-editing application that uses machine learning to apply real-time filters. Compute efficiency is achieved through hardware accelerators like mobile GPUs or Neural Processing Units (NPUs), ensuring tasks are performed quickly while minimizing battery usage.\nThis compute efficiency, in turn, is supported by algorithmic efficiency. The application relies on a lightweight neural network architecture that reduces the computational load, allowing it to take full advantage of the mobile device’s hardware. Streamlined models also help reduce memory consumption, further enhancing computational performance and enabling real-time responsiveness.\nData efficiency strengthens both compute and algorithmic efficiency by ensuring the model is trained on carefully curated and augmented datasets. These datasets allow the model to generalize effectively, reducing the need for extensive retraining and lowering the demand for computational resources during training. Additionally, by minimizing the complexity of the training data, the model can remain lightweight without sacrificing accuracy, reinforcing both model and compute efficiency.\nBy integrating these dimensions, mobile deployments achieve a seamless balance between performance, energy efficiency, and practicality. The interdependence of model, compute, and data efficiencies ensures that even resource-constrained devices can deliver advanced AI capabilities to users on the go.\n\n\nEdge ML Deployment\nEdge deployments, such as those in autonomous vehicles, highlight the intricate balance required between real-time constraints and energy efficiency. Compute efficiency is central, as vehicles rely on high-performance onboard hardware to process massive streams of sensor data—such as from cameras, LiDAR, and radar—in real time. These computations must be performed with minimal latency to ensure safe navigation and split-second decision-making.\nThis compute efficiency is closely supported by algorithmic efficiency, as the system depends on compact, high-accuracy models designed for low latency. By employing streamlined neural network architectures or hybrid models combining deep learning and traditional algorithms, the computational demands on hardware are reduced. These optimized models not only lower the processing load but also consume less energy, reinforcing the system’s overall energy efficiency.\nData efficiency enhances both compute and algorithmic efficiency by reducing the dependency on vast amounts of training data. Through synthetic and augmented datasets, the model can generalize effectively across diverse scenarios—such as varying lighting, weather, and traffic conditions—without requiring extensive retraining. This targeted approach minimizes computational costs during training and allows the model to remain efficient while adapting to a wide range of real-world environments.\nTogether, the interdependence of these efficiencies ensures that autonomous vehicles can operate safely and reliably while minimizing energy consumption. This balance not only improves real-time performance but also contributes to broader goals, such as reducing fuel consumption and enhancing environmental sustainability.\n\n\nCloud ML Deployment\nCloud deployments exemplify how system efficiency can be achieved across interconnected dimensions. Consider a recommendation system operating in a data center, where high throughput and rapid inference are critical. Compute efficiency is achieved by leveraging parallelized processing on GPUs or TPUs, which optimize the computational workload to ensure timely and resource-efficient performance. This high-performance hardware allows the system to handle millions of simultaneous queries while keeping energy and operational costs in check.\nThis compute efficiency is bolstered by algorithmic efficiency, as the recommendation system employs streamlined architectures, such as pruned or simplified models. By reducing the computational and memory footprint, these models enable the system to scale efficiently, processing large volumes of data without overwhelming the infrastructure. The streamlined design also reduces the burden on accelerators, improving energy usage and maintaining throughput.\nData efficiency strengthens both compute and algorithmic efficiency by enabling the system to learn and adapt without excessive data overhead. By focusing on actively labeled datasets, the system can prioritize high-value training data, ensuring better model performance with fewer computational resources. This targeted approach reduces the size and complexity of training tasks, freeing up resources for inference and scaling while maintaining high recommendation accuracy.\nTogether, the interdependence of these efficiencies enables cloud-based systems to achieve a balance of performance, scalability, and cost-effectiveness. By optimizing model, compute, and data dimensions in harmony, cloud deployments become a cornerstone of modern AI applications, supporting millions of users with efficiency and reliability.\n\n\nTiny ML Deployment\nTiny ML represents the pinnacle of efficiency, where machine learning systems operate on ultra-constrained hardware. Consider an IoT device monitoring temperature in real-time. Model efficiency is essential in this context, as the system relies on an ultra-lightweight architecture designed to run on minimal resources. By focusing on compact, streamlined models, the computational demands are kept low, allowing the system to function reliably on small, power-constrained hardware.\nThis algorithmic efficiency directly supports compute efficiency, as it reduces the processing requirements on low-power microcontrollers or specialized Tiny ML hardware. These devices, optimized for energy-efficient operation, can process data locally while extending battery life. The lightweight model design minimizes energy consumption, enabling sustained operation in remote or inaccessible locations where power is limited.\nData efficiency further reinforces both model and compute efficiency by reducing the need for large-scale data collection and processing. Through the use of synthetic data and targeted datasets, the model can generalize effectively across diverse conditions without requiring massive or redundant training data. This targeted approach allows the training process to be resource-efficient, reducing computational costs and storage needs.\nBy interconnecting these dimensions, Tiny ML systems achieve operation under extreme resource constraints. These systems enable AI applications in settings previously considered impractical, from battery-powered environmental sensors to wearable health monitors, while maintaining reliability and energy efficiency.\n\n\nProgression and Key Takeaways\nStarting with Mobile ML deployments and progressing to Edge ML, Cloud ML, and Tiny ML, these examples illustrate how system efficiency adapts to diverse operational contexts. Mobile ML emphasizes battery life and hardware limitations, edge systems balance real-time demands with energy efficiency, cloud systems prioritize scalability and throughput, and Tiny ML demonstrates how AI can thrive in environments with severe resource constraints.\nDespite these differences, the fundamental principles remain consistent: achieving system efficiency requires optimizing model, compute, and data dimensions. These dimensions are deeply interconnected, with improvements in one often reinforcing the others. For instance, lightweight models enhance computational performance and reduce data requirements, while efficient hardware accelerates model training and inference. Similarly, focused datasets streamline model training and reduce computational overhead.\nBy understanding the interplay between these dimensions, we can design machine learning systems that meet specific deployment requirements while maintaining flexibility across contexts. For instance, a model architected for edge deployment can often be adapted for cloud scaling or simplified for mobile use, provided we carefully consider the relationships between model architecture, computational resources, and data requirements.\n\n\n\n9.3.4 Scalability and Sustainability\nSystem efficiency directly impacts environmental sustainability. Optimized systems can scale widely while minimizing environmental impact, and their sustainable design reinforces the need for continued efficiency improvements, creating a positive feedback loop.\n\nEfficiency Enables Scalability\nEfficient systems are inherently scalable. Reducing resource demands through lightweight models, targeted datasets, and optimized compute utilization allows systems to deploy broadly across diverse environments. For example, a speech recognition model that is efficient enough to run on mobile devices can serve millions of users globally without relying on costly infrastructure upgrades. Similarly, Tiny ML technologies, designed to operate on low-power hardware, make it possible to deploy thousands of devices in remote areas for applications like environmental monitoring or precision agriculture.\nScalability becomes feasible because efficiency reduces barriers to entry. Systems that are compact and energy-efficient require less infrastructure, making them more adaptable to different deployment contexts, from cloud data centers to edge and IoT devices. This adaptability is key to ensuring that advanced AI solutions reach users worldwide, fostering inclusion and innovation.\n\n\nScalability Drives Sustainability\nWhen efficient systems scale, they amplify their contribution to sustainability. Energy-efficient designs deployed at scale reduce overall energy consumption and computational waste, mitigating the environmental impact of machine learning systems. For instance, deploying Tiny ML devices for on-device data processing avoids the energy costs of transmitting raw data to the cloud, while efficient recommendation engines in the cloud reduce the operational footprint of serving millions of users.\nThe wide-scale adoption of efficient systems not only reduces environmental costs but also fosters sustainable development in underserved regions. Efficient AI applications in healthcare, education, and agriculture can provide transformative benefits without imposing significant resource demands, aligning technological growth with ethical and environmental goals.\n\n\nSustainability Reinforces Efficiency\nSustainability itself reinforces the need for efficiency, creating a feedback loop that strengthens the entire system. Practices like minimizing data redundancy, designing energy-efficient hardware, and developing low-power models all emphasize efficient resource utilization. These efforts not only reduce the environmental footprint of AI systems but also set the stage for further scalability by making systems cost-effective and accessible.\n\n\nThe Virtuous Cycle of Machine Learning Systems\nEfficiency, scalability, and sustainability are deeply interconnected, forming a virtuous cycle that propels machine learning systems toward broader impact. Efficient systems enable scalable deployments, which amplify their sustainability benefits. In turn, sustainable practices drive the need for more efficient designs, ensuring the cycle continues. This interplay creates systems that are not only technically impressive but also socially and environmentally responsible, aligning AI innovation with the needs of a global community.\nFigure 9.5 below illustrates the virtuous cycle of machine learning systems. It highlights how efficiency drives scalability, scalability fosters sustainability, and sustainability reinforces efficiency.\n\n\n\n\n\n\n\nFigure 9.5: The virtuous cycle of machine learning system. Efficiency drives scalability and widespread adoption, which in turn drives the need for sustainable solutions, fueling the need for further efficiency.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Efficient AI</span>"
    ]
  },
  {
    "objectID": "contents/core/efficient_ai/efficient_ai.html#trade-offs-and-challenges-in-achieving-efficiency",
    "href": "contents/core/efficient_ai/efficient_ai.html#trade-offs-and-challenges-in-achieving-efficiency",
    "title": "9  Efficient AI",
    "section": "9.4 Trade-offs and Challenges in Achieving Efficiency",
    "text": "9.4 Trade-offs and Challenges in Achieving Efficiency\nIn the previous section, we explored how the dimensions of system efficiency—algorithmic efficiency, compute efficiency, and data efficiency—are deeply interconnected. Ideally, these dimensions reinforce one another, creating a system that is both efficient and high-performing. Compact models reduce computational demands, efficient hardware accelerates processes, and high-quality datasets streamline training and inference. However, achieving this harmony is far from straightforward.\n\n9.4.1 The Source of Trade-offs\nIn practice, balancing these dimensions often uncovers underlying tensions. Improvements in one area can impose constraints on others, highlighting the interconnected nature of machine learning systems. For instance, simplifying a model to reduce computational demands might result in reduced accuracy, while optimizing compute efficiency for real-time responsiveness can conflict with energy efficiency goals. These trade-offs are not limitations but reflections of the intricate design decisions required to build adaptable and efficient systems.\nUnderstanding the root of these trade-offs is essential for navigating the challenges of system design. Each efficiency dimension influences the others, creating a dynamic interplay that shapes system performance. The following sections delve into these interdependencies, beginning with the relationship between algorithmic efficiency and compute requirements.\n\nAlgorithmic Efficiency and Compute Requirements\nModel efficiency focuses on designing compact and streamlined models that minimize computational and memory demands. By reducing the size or complexity of a model, it becomes easier to deploy on devices with limited resources, such as mobile phones or IoT sensors.\nHowever, overly simplifying a model can reduce its accuracy, especially for complex tasks. To make up for this loss, additional computational resources may be required during training to fine-tune the model or during deployment to apply more sophisticated inference algorithms. Thus, while algorithmic efficiency can reduce computational costs, achieving this often places additional strain on compute efficiency.\n\n\nCompute Efficiency and Real-Time Needs\nCompute efficiency aims to minimize the resources required for tasks like training and inference, reducing energy consumption, processing time, and memory use. In many applications, particularly in cloud computing or data centers, this optimization works seamlessly with algorithmic efficiency to improve system performance.\nHowever, in scenarios that require real-time responsiveness—such as autonomous vehicles or augmented reality—compute efficiency is harder to maintain. Real-time systems often require high-performance hardware to process large amounts of data instantly, which can conflict with energy efficiency goals or increase system costs. Balancing compute efficiency with stringent real-time application needs becomes a key challenge in such applications.\n\n\nData Efficiency and Model Generalization\nData efficiency seeks to minimize the amount of data required to train a model without sacrificing performance. By curating smaller, high-quality datasets, the training process becomes faster and less resource-intensive. Ideally, this reinforces both model and compute efficiency, as smaller datasets reduce the computational load and support more compact models.\nHowever, reducing the size of a dataset can also limit its diversity, making it harder for the model to generalize to unseen scenarios. To address this, additional compute resources or model complexity may be required, creating a tension between data efficiency and the broader goals of system efficiency.\n\n\nSummary\nThe interdependencies between model, compute, and data efficiency are the foundation of a well-designed machine learning system. While these dimensions can reinforce one another, building a system that achieves this synergy often requires navigating difficult trade-offs. These trade-offs highlight the complexity of designing machine learning systems that balance performance, scalability, and resource constraints.\n\n\n\n9.4.2 Common Trade-offs\nIn machine learning system design, trade-offs are an inherent reality. As we explored in the previous section, the interdependencies between algorithmic efficiency, compute efficiency, and data efficiency ideally work together to create powerful, resource-conscious systems. However, achieving this harmony is far from straightforward. In practice, improvements in one dimension often come at the expense of another. Designers must carefully weigh these trade-offs to achieve a balance that aligns with the system’Tiny ML’s goals and deployment context.\nThis balancing act is especially challenging because trade-offs are rarely one-dimensional. Decisions made in one area often have cascading effects on the rest of the system. For instance, choosing a larger, more complex model may improve accuracy, but it also increases computational demands and the size of the training dataset required. Similarly, reducing energy consumption may limit the ability to meet real-time performance requirements, particularly in latency-sensitive applications.\nWe explore three of the most common trade-offs encountered in machine learning system design:\n\nModel complexity vs. compute resources,\n\nEnergy efficiency vs. real-time performance, and\n\nData size vs. model generalization.\n\nEach of these trade-offs illustrates the nuanced decisions that system designers must make and the challenges involved in achieving efficient, high-performing systems.\n\nModel Complexity and Compute Resources\nThe relationship between model complexity and compute resources is one of the most fundamental trade-offs in machine learning system design. Complex models, such as deep neural networks with millions or even billions of parameters, are often capable of achieving higher accuracy by capturing intricate patterns in data. However, this complexity comes at a cost. These models require significant computational power and memory to train and deploy, often making them impractical for environments with limited resources.\nFor example, consider a recommendation system deployed in a cloud data center. A highly complex model may deliver better recommendations, but it increases the computational demands on servers, leading to higher energy consumption and operating costs. On the other hand, a simplified model may reduce these demands but might compromise the quality of recommendations, especially when handling diverse or unpredictable user behavior.\nThe trade-off becomes even more pronounced in resource-constrained environments such as mobile or edge devices. A compact, streamlined model designed for a smartphone or an autonomous vehicle may operate efficiently within the device’s hardware limits but might require more sophisticated data preprocessing or training procedures to compensate for its reduced capacity. This balancing act highlights the interconnected nature of efficiency dimensions, where gains in one area often demand sacrifices in another.\n\n\nEnergy Efficiency and Real-Time Performance\nEnergy efficiency and real-time performance often pull machine learning systems in opposite directions, particularly in applications requiring low-latency responses. Real-time systems, such as those in autonomous vehicles or augmented reality applications, rely on high-performance hardware to process large volumes of data quickly. This ensures responsiveness and safety in scenarios where even small delays can lead to significant consequences. However, achieving such performance typically increases energy consumption, creating tension with the goal of minimizing resource use.\nFor instance, an autonomous vehicle must process sensor data from cameras, LiDAR, and radar in real time to make navigation decisions. The computational demands of these tasks often require specialized accelerators, such as GPUs, which can consume significant energy. While optimizing hardware utilization and model architecture can improve energy efficiency to some extent, the demands of real-time responsiveness make it challenging to achieve both goals simultaneously.\nIn edge deployments, where devices rely on battery power or limited energy sources, this trade-off becomes even more critical. Striking a balance between energy efficiency and real-time performance often involves prioritizing one over the other, depending on the application’s requirements. This trade-off underscores the importance of context-specific design, where the constraints and priorities of the deployment environment dictate the balance between competing objectives.\n\n\nData Size and Model Generalization\nThe size and quality of the dataset used to train a machine learning model play a role in its ability to generalize to new, unseen data. Larger datasets generally provide greater diversity and coverage, enabling models to capture subtle patterns and reduce the risk of overfitting. However, the computational and memory demands of training on large datasets can be substantial, leading to trade-offs between data efficiency and computational requirements.\nIn resource-constrained environments such as Tiny ML deployments, the challenge of dataset size is particularly evident. For example, an IoT device monitoring environmental conditions might need a model that generalizes well to varying temperatures, humidity levels, or geographic regions. Collecting and processing extensive datasets to capture these variations may be impractical due to storage, computational, and energy limitations. In such cases, smaller, carefully curated datasets or synthetic data generated to mimic real-world conditions are used to reduce computational strain. However, this reduction often risks missing key edge cases, which could degrade the model’s performance in diverse environments.\nConversely, in cloud-based systems, where compute resources are more abundant, training on massive datasets can still pose challenges. Managing data redundancy, ensuring high-quality labeling, and handling the time and cost associated with large-scale data pipelines often require significant computational infrastructure. This trade-off highlights how the need to balance dataset size and model generalization depends heavily on the deployment context and available resources.\n\n\nSummary\nThe interplay between model complexity, compute resources, energy efficiency, real-time performance, and dataset size illustrates the inherent trade-offs in machine learning system design. These trade-offs are rarely one-dimensional; decisions to optimize one aspect of a system often ripple through the others, requiring careful consideration of the specific goals and constraints of the application.\nDesigners must weigh the advantages and limitations of each trade-off in the context of the deployment environment. For instance, a cloud-based system might prioritize scalability and throughput over energy efficiency, while an edge system must balance real-time performance with strict power constraints. Similarly, resource-limited Tiny ML deployments require exceptional data and algorithmic efficiency to operate within severe hardware restrictions.\nBy understanding these common trade-offs, we can begin to identify strategies for navigating them effectively. The next section will explore practical approaches to managing these tensions, focusing on techniques and design principles that enable system efficiency while addressing the complexities of real-world applications.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Efficient AI</span>"
    ]
  },
  {
    "objectID": "contents/core/efficient_ai/efficient_ai.html#managing-the-trade-offs",
    "href": "contents/core/efficient_ai/efficient_ai.html#managing-the-trade-offs",
    "title": "9  Efficient AI",
    "section": "9.5 Managing the Trade-offs",
    "text": "9.5 Managing the Trade-offs\nThe trade-offs inherent in machine learning system design require thoughtful strategies to navigate effectively. While the interdependencies between algorithmic efficiency, compute efficiency, and data efficiency create opportunities for synergy, achieving this balance often involves difficult decisions. The specific goals and constraints of the deployment environment heavily influence how these trade-offs are addressed. For example, a system designed for cloud deployment may prioritize scalability and throughput, while a Tiny ML system must focus on extreme resource efficiency.\nTo manage these challenges, designers can adopt a range of strategies that address the unique requirements of different contexts. By prioritizing efficiency dimensions based on the application, collaborating across system components, and leveraging automated optimization tools, it is possible to create systems that balance performance, cost, and resource use. This section explores these approaches and provides guidance for designing systems that are both efficient and adaptable.\n\n9.5.1 Prioritization by Context\nEfficiency goals are rarely universal. The specific demands of an application or deployment scenario heavily influence which dimension of efficiency—model, compute, or data—takes precedence. Designing an efficient system requires a deep understanding of the operating environment and the constraints it imposes. Prioritizing the right dimensions based on context is the first step in effectively managing trade-offs.\nFor instance, in Mobile ML deployments, battery life is often the primary constraint. This places a premium on compute efficiency, as energy consumption must be minimized to preserve the device’s operational time. As a result, lightweight models are prioritized, even if it means sacrificing some accuracy or requiring additional data preprocessing. The focus is on balancing acceptable performance with energy-efficient operation.\nIn contrast, Cloud ML-based systems prioritize scalability and throughput. These systems must process large volumes of data and serve millions of users simultaneously. While compute resources in cloud environments are more abundant, energy efficiency and operational costs still remain important considerations. Here, algorithmic efficiency plays a critical role in ensuring that the system can scale without overwhelming the underlying infrastructure.\nEdge ML systems present an entirely different set of priorities. Autonomous vehicles or real-time monitoring systems require low-latency processing to ensure safe and reliable operation. This makes real-time performance and compute efficiency paramount, often at the expense of energy consumption. However, the hardware constraints of edge devices mean that these systems must still carefully manage energy and computational resources to remain viable.\nFinally, Tiny ML deployments demand extreme levels of efficiency due to the severe limitations of hardware and energy availability. For these systems, model and data efficiency are the top priorities. Models must be highly compact and capable of operating on microcontrollers with minimal memory and compute power. At the same time, the training process must rely on small, carefully curated datasets to ensure the model generalizes well without requiring extensive resources.\nIn each of these contexts, prioritizing the right dimensions of efficiency ensures that the system meets its functional and resource requirements. Recognizing the unique demands of each deployment scenario allows designers to navigate trade-offs effectively and tailor solutions to specific needs.\n\n\n9.5.2 End-to-End Co-Design\nEfficient machine learning systems are rarely the product of isolated optimizations. Achieving balance across model, compute, and data efficiency requires an end-to-end perspective, where each component of the system is designed in tandem with the others. This holistic approach, often referred to as co-design, involves aligning model architectures, hardware platforms, and data pipelines to work seamlessly together.\nOne of the key benefits of co-design is its ability to mitigate trade-offs by tailoring each component to the specific requirements of the system. For instance, consider a speech recognition system deployed on a mobile device. The model must be compact enough to fit within the device’s tiny ML memory constraints while still delivering real-time performance. By designing the model architecture to leverage the capabilities of hardware accelerators, such as Neural Processing Units (NPUs), it becomes possible to achieve low-latency inference without excessive energy consumption. Similarly, careful preprocessing and augmentation of the training data can ensure robust performance, even with a smaller, streamlined model.\nCo-design becomes essential in resource-constrained environments like Edge ML and Tiny ML deployments. Models must align precisely with hardware capabilities. For example, 8-bit models5 require hardware support for efficient integer operations, while pruned models benefit from sparse tensor operations. Similarly, edge accelerators often optimize specific operations like convolutions or matrix multiplication, influencing model architecture choices. This creates a tight coupling between hardware and model design decisions.\n5 8-bit models: ML models use 8-bit integer representations for weights and activations instead of the standard 32-bit floating-point format, reducing memory usage and computational requirements for faster, more energy-efficient inference on compatible hardware.This approach extends beyond the interaction of models and hardware. Data pipelines, too, play a central role in co-design. For example, in applications requiring real-time adaptation, such as personalized recommendation systems, the data pipeline must deliver high-quality, timely information that minimizes computational overhead while maximizing model effectiveness. By integrating data management into the design process, it becomes possible to reduce redundancy, streamline training, and support efficient deployment.\nEnd-to-end co-design ensures that the trade-offs inherent in machine learning systems are addressed holistically. By designing each component with the others in mind, it becomes possible to balance competing priorities and create systems that are not only efficient but also robust and adaptable.\n\n\n9.5.3 Automation and Optimization\nNavigating the trade-offs between model, compute, and data efficiency is a complex task that often involves numerous iterations and expert judgment. Automation and optimization tools have emerged as powerful solutions for managing these challenges, streamlining the process of balancing efficiency dimensions while reducing the time and expertise required.\nOne widely used approach is automated machine learning (AutoML), which enables the exploration of different model architectures, hyperparameter configurations, and feature engineering techniques. By automating these aspects of the design process, AutoML can identify models that achieve an optimal balance between performance and efficiency. For instance, an AutoML pipeline might search for a lightweight model architecture that delivers high accuracy while fitting within the resource constraints of an edge device (Hutter, Kotthoff, and Vanschoren 2019). This approach reduces the need for manual trial-and-error, making optimization faster and more accessible.\n\nHutter, Frank, Lars Kotthoff, and Joaquin Vanschoren. 2019. “Automated Machine Learning: Methods, Systems, Challenges.” Automated Machine Learning, 5–33.\n\nElsken, Thomas, Jan Hendrik Metzen, and Frank Hutter. 2019. “Neural Architecture Search: A Survey.” Journal of Machine Learning Research 20 (55): 1–21.\nNeural architecture search (NAS) takes automation a step further by designing model architectures tailored to specific hardware or deployment scenarios. NAS algorithms evaluate a wide range of architectural possibilities, selecting those that maximize performance while minimizing computational demands. For example, NAS can design models that leverage quantization or sparsity techniques, ensuring compatibility with energy-efficient accelerators like TPUs or microcontrollers (Elsken, Metzen, and Hutter 2019). This automated co-design of models and hardware helps mitigate trade-offs by aligning efficiency goals across dimensions.\nData efficiency, too, benefits from automation. Tools that automate dataset curation, augmentation, and active learning reduce the size of training datasets without sacrificing model performance. These tools prioritize high-value data points, ensuring that models are trained on the most informative examples. This not only speeds up training but also reduces computational overhead, reinforcing both compute and algorithmic efficiency (Settles 2009b).\n\nSettles, Burr. 2009b. “Active Learning Literature Survey.” University of Wisconsin-Madison Department of Computer Sciences 1648: 1–67.\nWhile automation tools are not a panacea, they play a critical role in addressing the complexity of trade-offs. By leveraging these tools, system designers can achieve efficient solutions more quickly and at lower cost, freeing them to focus on broader design challenges and deployment considerations.\n\n\n9.5.4 Summary\nDesigning efficient machine learning systems requires a deliberate approach to managing trade-offs between model, compute, and data efficiency. These trade-offs are influenced by the context of the deployment, the constraints of the hardware, and the goals of the application. By prioritizing efficiency dimensions based on the specific needs of the system, embracing end-to-end co-design, and leveraging automation tools, it becomes possible to navigate these challenges effectively.\nThe strategies explored illustrate how thoughtful design can transform trade-offs into opportunities for synergy. For example, aligning model architectures with hardware capabilities can mitigate energy constraints, while automation tools like AutoML and NAS streamline the process of optimizing efficiency dimensions. These approaches underscore the importance of treating system efficiency as a holistic endeavor, where components are designed to complement and reinforce one another.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Efficient AI</span>"
    ]
  },
  {
    "objectID": "contents/core/efficient_ai/efficient_ai.html#efficiency-first-mindset",
    "href": "contents/core/efficient_ai/efficient_ai.html#efficiency-first-mindset",
    "title": "9  Efficient AI",
    "section": "9.6 Efficiency-first Mindset",
    "text": "9.6 Efficiency-first Mindset\nDesigning an efficient machine learning system requires a holistic approach. While it is tempting to focus on optimizing individual components, such as the model architecture or the hardware platform, true efficiency emerges when the entire system is considered as a whole. This end-to-end perspective ensures that trade-offs are balanced across all stages of the machine learning pipeline, from data collection to deployment.\nEfficiency is not a static goal but a dynamic process shaped by the context of the application. A system designed for a cloud data center will prioritize scalability and throughput, while an edge deployment will focus on low latency and energy conservation. These differing priorities influence decisions at every step of the design process, requiring careful alignment of the model, compute resources, and data strategy.\nAn end-to-end perspective can transform system design, enabling machine learning practitioners to build systems that effectively balance trade-offs. Through case studies and examples, we will highlight how efficient systems are designed to meet the unique challenges of their deployment environments, whether in the cloud, on mobile devices, or in resource-constrained Tiny ML applications.\n\n9.6.1 End-to-End Perspective\nEfficiency in machine learning systems is achieved not through isolated optimizations but by considering the entire pipeline as a unified whole. Each stage—data collection, model training, hardware deployment, and inference—contributes to the overall efficiency of the system. Decisions made at one stage can ripple through the rest, influencing performance, resource use, and scalability.\nFor example, data collection and preprocessing are often the starting points of the pipeline. The quality and diversity of the data directly impact model performance and efficiency. Curating smaller, high-quality datasets can reduce computational costs during training while simplifying the model’s design. However, insufficient data diversity may affect generalization, necessitating compensatory measures in model architecture or training procedures. By aligning the data strategy with the model and deployment context, designers can avoid inefficiencies downstream.\nModel training is another critical stage. The choice of architecture, optimization techniques, and hyperparameters must consider the constraints of the deployment hardware. A model designed for high-performance cloud systems may emphasize accuracy and scalability, leveraging large datasets and compute resources. Conversely, a model intended for edge devices must balance accuracy with size and energy efficiency, often requiring compact architectures and quantization techniques tailored to specific hardware.\nDeployment and inference demand precise hardware alignment. Each platform offers distinct capabilities. GPUs excel at parallel matrix operations, TPUs optimize specific neural network computations, and microcontrollers provide energy-efficient scalar processing. For example, a smartphone speech recognition system might leverage an NPU’s dedicated convolution units for 5-millisecond inference times at 1-watt power draw, while an autonomous vehicle’s FPGA-based accelerator processes multiple sensor streams with 50-microsecond latency. This hardware-software integration determines real-world efficiency.\nAn end-to-end perspective ensures that trade-offs are addressed holistically, rather than shifting inefficiencies from one stage of the pipeline to another. By treating the system as an integrated whole, machine learning practitioners can design solutions that are not only efficient but also robust and scalable across diverse deployment scenarios.\n\n\n9.6.2 Scenarios\nThe efficiency needs of machine learning systems differ significantly depending on the lifecycle stage and deployment environment. From research prototypes to production systems, and from high-performance cloud applications to resource-constrained edge deployments, each scenario presents unique challenges and trade-offs. Understanding these differences is crucial for designing systems that meet their operational requirements effectively.\n\nResearch Prototypes vs. Production Systems\nIn the research phase, the primary focus is often on model performance, with efficiency taking a secondary role. Prototypes are typically trained and tested using abundant compute resources, allowing researchers to experiment with large architectures, extensive hyperparameter tuning, and diverse datasets. While this approach enables the exploration of cutting-edge techniques, the resulting systems are often too resource-intensive for real-world use.\nIn contrast, production systems must prioritize efficiency to operate within practical constraints. Deployment environments—whether cloud data centers, mobile devices, or IoT sensors—impose strict limitations on compute power, memory, and energy consumption. Transitioning from a research prototype to a production-ready system often involves significant optimization, such as model pruning, quantization, or retraining on targeted datasets. This shift highlights the need to balance performance and efficiency as systems move from concept to deployment.\n\n\nHigh-Performance Cloud Applications vs. Constrained Systems\nCloud-based systems, such as those used for large-scale analytics or recommendation engines, are designed to handle massive workloads. Scalability is the primary concern, requiring models and infrastructure that can support millions of users simultaneously. While compute resources are relatively abundant in cloud environments, energy efficiency and operational costs remain critical considerations. Techniques such as model compression and hardware-specific optimizations help manage these trade-offs, ensuring the system scales efficiently.\nIn contrast, edge and mobile systems operate under far stricter constraints. Real-time performance, energy efficiency, and hardware limitations are often the dominant concerns. For example, a speech recognition application on a smartphone must balance model size and latency to provide a seamless user experience without draining the device’s battery. Similarly, an IoT sensor deployed in a remote location must operate for months on limited power, requiring an ultra-efficient model and compute pipeline. These scenarios demand solutions that prioritize efficiency over raw performance.\n\n\nSystems Requiring Frequent Retraining vs. Long-Term Stability\nSome systems, such as recommendation engines or fraud detection platforms, require frequent retraining to remain effective in dynamic environments. These systems depend heavily on data efficiency, using actively labeled datasets and sampling strategies to minimize retraining costs. Compute efficiency also plays a role, as scalable infrastructure is needed to process new data and update models regularly.\nOther systems, such as embedded models in medical devices or industrial equipment, require long-term stability with minimal updates. In these cases, upfront optimizations in model and data efficiency are critical to ensure the system performs reliably over time. Reducing dependency on frequent updates minimizes computational and operational overhead, making the system more sustainable in the long run.\n\n\n\n9.6.3 Summary\nDesigning machine learning systems with efficiency in mind requires a holistic approach that considers the specific needs and constraints of the deployment context. From research prototypes to production systems, and across environments as varied as cloud data centers, mobile devices, and Tiny ML applications, the priorities for efficiency differ significantly. Each stage of the machine learning pipeline—data collection, model design, training, deployment, and inference—presents unique trade-offs that must be navigated thoughtfully.\nThe examples and scenarios in this section demonstrate the importance of aligning system design with operational requirements. Cloud systems prioritize scalability and throughput, edge systems focus on real-time performance, and Tiny ML applications emphasize extreme resource efficiency. Understanding these differences enables practitioners to tailor their approach, leveraging strategies such as end-to-end co-design and automation tools to balance competing priorities effectively.\nUltimately, the key to designing efficient systems lies in recognizing that efficiency is not a one-size-fits-all solution. It is a dynamic process that requires careful consideration of trade-offs, informed prioritization, and a commitment to addressing the unique challenges of each scenario. With these principles in mind, machine learning practitioners can create systems that are not only efficient but also robust, scalable, and sustainable.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Efficient AI</span>"
    ]
  },
  {
    "objectID": "contents/core/efficient_ai/efficient_ai.html#challenges-and-philosophical-questions",
    "href": "contents/core/efficient_ai/efficient_ai.html#challenges-and-philosophical-questions",
    "title": "9  Efficient AI",
    "section": "9.7 Challenges and Philosophical Questions",
    "text": "9.7 Challenges and Philosophical Questions\nWhile efficiency in machine learning is often framed as a technical challenge, it is also deeply tied to broader questions about the purpose and impact of AI systems. Designing efficient systems involves navigating not only practical trade-offs but also complex ethical and philosophical considerations, such as the following:\n\nWhat are the limits of optimization?\nHow do we ensure that efficiency benefits are distributed equitably?\nCan the pursuit of efficiency stifle innovation or creativity in the field?\n\nWe must explore these questions as engineers, inviting reflection on the broader implications of system efficiency. By examining the limits of optimization, equity concerns, and the tension between innovation and efficiency, we can have a deeper understanding of the challenges involved in balancing technical goals with ethical and societal values.\n\n9.7.1 The Limits of Optimization\nOptimization plays a central role in building efficient machine learning systems, but it is not an infinite process. As systems become more refined, each additional improvement often requires exponentially more effort, time, or resources, while delivering increasingly smaller benefits. This phenomenon, known as diminishing returns, is a common challenge in many engineering domains, including machine learning.\nThe No Free Lunch (NFL) theorems for optimization further illustrate the inherent limitations of optimization efforts. According to the NFL theorems, no single optimization algorithm can outperform all others across every possible problem. This implies that the effectiveness of an optimization technique is highly problem-specific, and improvements in one area may not translate to others (Wolpert and Macready 1997).\n\nWolpert, David H, and William G Macready. 1997. “No Free Lunch Theorems for Optimization.” IEEE Transactions on Evolutionary Computation 1 (1): 67–82.\nFor example, compressing a machine learning model can initially reduce memory usage and compute requirements significantly with minimal loss in accuracy. However, as compression progresses, maintaining performance becomes increasingly challenging. Achieving additional gains may necessitate sophisticated techniques, such as hardware-specific optimizations or extensive retraining, which increase both complexity and cost. These costs extend beyond financial investment in specialized hardware and training resources to include the time and expertise required to fine-tune models, iterative testing efforts, and potential trade-offs in model robustness and generalizability. As such, pursuing extreme efficiency often leads to diminishing returns, where escalating costs and complexity outweigh incremental benefits.\nThe NFL theorems highlight that no universal optimization solution exists, emphasizing the need to balance efficiency pursuits with practical considerations. Recognizing the limits of optimization is critical for designing systems that are not only efficient but also practical and sustainable. Over-optimization risks wasted resources and reduced adaptability, complicating future system updates or adjustments to changing requirements. Identifying when a system is “good enough” ensures resources are allocated effectively, focusing on efforts with the greatest overall impact.\nSimilarly, optimizing datasets for training efficiency may initially save resources but excessively reducing dataset size risks compromising diversity and weakening model generalization. Likewise, pushing hardware to its performance limits may improve metrics such as latency or power consumption, yet the associated reliability concerns and engineering costs can ultimately outweigh these gains.\nIn summary, understanding the limits of optimization is essential for creating systems that balance efficiency with practicality and sustainability. This perspective helps avoid over-optimization and ensures resources are invested in areas with the most meaningful returns.\n\n\n9.7.2 Case Study: Moore’s Law\nOne of the most insightful examples of the limits of optimization can be seen in Moore’s Law and the economic curve it depends on. While Moore’s Law is often celebrated as a predictor of exponential growth in computational power, its success relied on an intricate economic balance. The relationship between integration and cost, as illustrated in the accompanying plot, provides a compelling analogy for the diminishing returns seen in machine learning optimization.\nFigure 9.6 shows the relative manufacturing cost per component as the number of components in an integrated circuit increases. Initially, as more components are packed onto a chip (\\(x\\)-axis), the cost per component (\\(y\\)-axis) decreases. This is because higher integration reduces the need for supporting infrastructure such as packaging and interconnects, creating economies of scale. For example, in the early years of integrated circuit design, moving from hundreds to thousands of components per chip drastically reduced costs and improved performance (Moore 2021).\n\n\n\n\n\n\nFigure 9.6: The economics of Moore’s law. Sournce: (Moore 2021)\n\n\nMoore, Gordon. 2021. “Cramming More Components onto Integrated Circuits (1965).”\n\n\nHowever, as integration continues, the curve begins to rise. This inflection point occurs because the challenges of scaling become more pronounced. Components packed closer together face reliability issues, such as increased heat dissipation and signal interference. Addressing these issues requires more sophisticated manufacturing techniques, such as advanced lithography, error correction, and improved materials. These innovations increase the complexity and cost of production, driving the curve upward. This U-shaped curve captures the fundamental trade-off in optimization: early improvements yield substantial benefits, but beyond a certain point, each additional gain comes at a greater cost.\n\nParallels in ML Optimization\nThe dynamics of this curve mirror the challenges faced in machine learning optimization. For instance, compressing a deep learning model to reduce its size and energy consumption follows a similar trajectory. Initial optimizations, such as pruning redundant parameters or reducing precision, often lead to significant savings with minimal impact on accuracy. However, as the model is further compressed, the losses in performance become harder to recover. Techniques such as quantization or hardware-specific tuning can restore some of this performance, but these methods add complexity and cost to the design process.\nSimilarly, in data efficiency, reducing the size of training datasets often improves computational efficiency at first, as less data requires fewer resources to process. Yet, as the dataset shrinks further, it may lose diversity, compromising the model’s ability to generalize. Addressing this often involves introducing synthetic data or sophisticated augmentation techniques, which demand additional engineering effort.\nThe Moore’s Law plot (Figure 9.6) serves as a visual reminder that optimization is not an infinite process. The cost-benefit balance is always context-dependent, and the point of diminishing returns varies based on the goals and constraints of the system. Machine learning practitioners, like semiconductor engineers, must identify when further optimization ceases to provide meaningful benefits. Over-optimization can lead to wasted resources, reduced adaptability, and systems that are overly specialized to their initial conditions.\n\n\n\n9.7.3 Equity Concerns\nEfficiency in machine learning has the potential to reduce costs, improve scalability, and expand accessibility. However, the resources needed to achieve efficiency—advanced hardware, curated datasets, and state-of-the-art optimization techniques—are often concentrated in well-funded organizations or regions. This disparity creates inequities in who can leverage efficiency gains, limiting the reach of machine learning in low-resource contexts. By examining compute, data, and algorithmic efficiency inequities, we can better understand these challenges and explore pathways toward democratization.\n\nUneven Access to Compute Efficiency\nThe training costs of state-of-the-art AI models have reached unprecedented levels. For example, OpenAI’s GPT-4 used an estimated USD $78 million worth of compute to train, while Google’s Gemini Ultra cost USD $191 million for compute (Perrault and Clark 2024). Computational efficiency depends on access to specialized hardware and infrastructure. The discrepancy in access is significant: training even a small language model (SLM) like LLama with 7 billion parameters can require millions of dollars in computing resources, while many research institutions operate with significantly lower annual compute budgets.\n\nPerrault, Ray, and Jack Clark. 2024. “Artificial Intelligence Index Report 2024.”\n\nOECD.AI. 2021. “Measuring the Geographic Distribution of AI Computing Capacity.” https://oecd.ai/en/policy-circle/computing-capacity.\nResearch conducted by OECD.AI indicates that 90% of global AI computing capacity is centralized in only five countries, posing significant challenges for researchers and professionals in other regions (OECD.AI 2021).\nA concrete illustration of this disparity is the compute divide in academia versus industry. Academic institutions often lack the hardware needed to replicate state-of-the-art results, particularly when competing with large technology firms that have access to custom supercomputers or cloud resources. This imbalance not only stifles innovation in underfunded sectors but also makes it harder for diverse voices to contribute to advancing machine learning.\nEnergy-efficient compute technologies, such as accelerators designed for Tiny ML or Mobile ML, present a promising avenue for democratization. By enabling powerful processing on low-cost, low-power devices, these technologies allow organizations without access to high-end infrastructure to build and deploy impactful systems. For instance, energy-efficient Tiny ML models can be deployed on affordable microcontrollers, opening doors for applications in healthcare, agriculture, and education in underserved regions.\n\n\nData Efficiency and Low-Resource Challenges\nData efficiency is essential in contexts where high-quality datasets are scarce, but the challenges of achieving it are unequally distributed. For example, natural language processing (NLP) for low-resource languages suffers from a lack of sufficient training data, leading to significant performance gaps compared to high-resource languages like English. Efforts like the Masakhane project, which builds open-source datasets for African languages, show how collaborative initiatives can address this issue. However, scaling such efforts globally requires far greater investment and coordination.\nEven when data is available, the ability to process and curate it efficiently depends on computational and human resources. Large organizations routinely employ data engineering teams and automated pipelines for curation and augmentation, enabling them to optimize data efficiency and improve downstream performance. In contrast, smaller groups often lack access to the tools or expertise needed for such tasks, leaving them at a disadvantage in both research and practical applications.\nDemocratizing data efficiency requires more open sharing of pre-trained models and datasets. Initiatives like Hugging Face’s open access to transformers or multilingual models by organizations like Meta’s No Language Left Behind aim to make state-of-the-art NLP models available to researchers and practitioners worldwide. These efforts help reduce the barriers to entry for data-scarce regions, enabling more equitable access to AI capabilities.\n\n\nAlgorithmic Efficiency for Accessibility\nModel efficiency plays a crucial role in democratizing machine learning by enabling advanced capabilities on low-cost, resource-constrained devices. Compact, efficient models designed for edge devices or mobile phones have already begun to bridge the gap in accessibility. For instance, AI-powered diagnostic tools running on smartphones are transforming healthcare in remote areas, while low-power Tiny ML models enable environmental monitoring in regions without reliable electricity or internet connectivity.\nTechnologies like TensorFlow Lite and PyTorch Mobile allow developers to deploy lightweight models on everyday devices, expanding access to AI applications in resource-constrained settings. These tools demonstrate how algorithmic efficiency can serve as a practical pathway to equity, particularly when combined with energy-efficient compute hardware.\nHowever, scaling the benefits of algorithmic efficiency requires addressing barriers to entry. Many efficient architectures, such as those designed through NAS, remain resource-intensive to develop. Open-source efforts to share pre-optimized models, like MobileNet or EfficientNet, play a critical role in democratizing access to efficient AI by allowing under-resourced organizations to deploy state-of-the-art solutions without needing to invest in expensive optimization processes.\n\n\nPathways to Democratization\nEfforts to close the equity gap in machine learning must focus on democratizing access to tools and techniques that enhance efficiency. Open-source initiatives, such as community-driven datasets and shared model repositories, provide a foundation for equitable access to efficient systems. Affordable hardware platforms, such as Raspberry Pi devices or open-source microcontroller frameworks, further enable resource-constrained organizations to build and deploy AI solutions tailored to their needs.\nCollaborative partnerships between well-resourced organizations and underrepresented groups also offer opportunities to share expertise, funding, and infrastructure. For example, initiatives that provide subsidized access to cloud computing platforms or pre-trained models for underserved regions can empower diverse communities to leverage efficiency for social impact.\nThrough efforts in model, computation, and data efficiency, the democratization of machine learning can become a reality. These efforts not only expand access to AI capabilities but also foster innovation and inclusivity, ensuring that the benefits of efficiency are shared across the global community.\n\n\n\n9.7.4 Tensions Between Innovation and Efficiency\nThe pursuit of efficiency in machine learning often brings with it a tension between optimizing for what is known and exploring what is new. On one hand, efficiency drives the practical deployment of machine learning systems, enabling scalability, cost reduction, and environmental sustainability. On the other hand, focusing too heavily on efficiency can stifle innovation by discouraging experimentation with untested, resource-intensive ideas.\n\nThe Trade-off Between Stability and Experimentation\nEfficiency often favors established techniques and systems that have already been proven to work well. For instance, optimizing neural networks through pruning, quantization, or distillation typically involves refining existing architectures rather than developing entirely new ones. While these approaches provide incremental improvements, they may come at the cost of exploring novel designs or paradigms that could yield transformative breakthroughs.\nConsider the shift from traditional machine learning methods to deep learning. Early neural network research in the 1990s and 2000s required significant computational resources and often failed to outperform simpler methods on practical tasks. Despite this, researchers continued to push the boundaries of what was possible, eventually leading to the breakthroughs in deep learning that define modern AI. If the field had focused exclusively on efficiency during that period, these innovations might never have emerged.\n\n\nResource-Intensive Innovation\nPioneering research often requires significant resources, from massive datasets to custom hardware. For example, large language models like GPT-4 or PaLM are not inherently efficient; their training processes consume enormous amounts of compute power and energy. Yet, these models have opened up entirely new possibilities in language understanding, prompting advancements that eventually lead to more efficient systems, such as smaller fine-tuned versions for specific tasks.\nHowever, this reliance on resource-intensive innovation raises questions about who gets to participate in these advancements. Well-funded organizations can afford to explore new frontiers, while smaller institutions may be constrained to incremental improvements that prioritize efficiency over novelty. Balancing the need for experimentation with the realities of resource availability is a key challenge for the field.\n\n\nEfficiency as a Constraint on Creativity\nEfficiency-focused design often requires adhering to strict constraints, such as reducing model size, energy consumption, or latency. While these constraints can drive ingenuity, they can also limit the scope of what researchers and engineers are willing to explore. For instance, edge computing applications often demand ultra-compact models, leading to a narrow focus on compression techniques rather than entirely new approaches to machine learning on constrained devices.\nAt the same time, the drive for efficiency can have a positive impact on innovation. Constraints force researchers to think creatively, leading to the development of new methods that maximize performance within tight resource budgets. Techniques like NAS and attention mechanisms arose, in part, from the need to balance performance and efficiency, demonstrating that innovation and efficiency can coexist when approached thoughtfully.\n\n\nStriking a Balance\nThe tension between innovation and efficiency highlights the need for a balanced approach to system design and research priorities. Organizations and researchers must recognize when it is appropriate to prioritize efficiency and when to embrace the risks of experimentation. For instance, applied systems for real-world deployment may demand strict efficiency constraints, while exploratory research labs can focus on pushing boundaries without immediate concern for resource optimization.\nUltimately, the relationship between innovation and efficiency is not adversarial but complementary. Efficient systems create the foundation for scalable, practical applications, while resource-intensive experimentation drives the breakthroughs that redefine what is possible. Balancing these priorities ensures that machine learning continues to evolve while remaining accessible, impactful, and sustainable.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Efficient AI</span>"
    ]
  },
  {
    "objectID": "contents/core/efficient_ai/efficient_ai.html#conclusion",
    "href": "contents/core/efficient_ai/efficient_ai.html#conclusion",
    "title": "9  Efficient AI",
    "section": "9.8 Conclusion",
    "text": "9.8 Conclusion\nEfficiency in machine learning systems is essential not just for achieving technical goals but for addressing broader questions about scalability, sustainability, and inclusivity. This chapter has focused on the why and how of efficiency—why it is critical to modern machine learning and how to achieve it through a balanced focus on model, compute, and data dimensions. By understanding the interdependencies and trade-offs inherent in these dimensions, we can build systems that align with their operational contexts and long-term objectives.\nThe challenges discussed in this chapter, from the limits of optimization to equity concerns and the tension between efficiency and innovation, highlight the need for a thoughtful approach. Whether working on a high-performance cloud system or a constrained Tiny ML application, the principles of efficiency serve as a compass for navigating the complexities of system design.\nWith this foundation in place, we can now dive into the what—the specific techniques and strategies that enable efficient machine learning systems. By grounding these practices in a clear understanding of the why and the how, we ensure that efficiency remains a guiding principle rather than a reactive afterthought.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Efficient AI</span>"
    ]
  },
  {
    "objectID": "contents/core/optimizations/optimizations.html",
    "href": "contents/core/optimizations/optimizations.html",
    "title": "10  Model Optimizations",
    "section": "",
    "text": "Purpose\nResources: Slides, Videos, Exercises\nHow do theoretical neural network models transform into practical solutions, and what techniques bridge implementation gaps?\nThe adaptation of machine learning models for real-world deployment demands systematic transformation approaches. Each optimization technique introduces unique methods for preserving model capabilities while meeting deployment constraints, revealing fundamental patterns in the relationship between theoretical design and practical implementation. These patterns highlight strategies for translating research advances into production systems, establishing core principles for creating solutions that maintain effectiveness while satisfying real-world requirements.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Model Optimizations</span>"
    ]
  },
  {
    "objectID": "contents/core/optimizations/optimizations.html#purpose",
    "href": "contents/core/optimizations/optimizations.html#purpose",
    "title": "10  Model Optimizations",
    "section": "",
    "text": "Learning Objectives\n\n\n\n\nLearn techniques like pruning, knowledge distillation and specialized model architectures to represent models more efficiently\nUnderstand quantization methods to reduce model size and enable faster inference through reduced precision numerics\nExplore hardware-aware optimization approaches to match models to target device capabilities\nDevelop holistic thinking to balance tradeoffs in model complexity, accuracy, latency, power etc. based on application requirements\nDiscover software tools like frameworks and model conversion platforms that enable deployment of optimized models\nGain strategic insight into selecting and applying model optimizations based on use case constraints and hardware targets",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Model Optimizations</span>"
    ]
  },
  {
    "objectID": "contents/core/optimizations/optimizations.html#overview",
    "href": "contents/core/optimizations/optimizations.html#overview",
    "title": "10  Model Optimizations",
    "section": "10.1 Overview",
    "text": "10.1 Overview\nThe optimization of machine learning models for practical deployment is a critical aspect of AI systems. This chapter focuses on exploring model optimization techniques as they relate to the development of ML systems, ranging from high-level model architecture considerations to low-level hardware adaptations. Figure 10.1 Illustrates the three layers of the optimization stack we cover.\n\n\n\n\n\n\nFigure 10.1: Three layers to be covered.\n\n\n\nAt the highest level, we examine methodologies for reducing the complexity of model parameters without compromising inferential capabilities. Techniques such as pruning and knowledge distillation offer powerful approaches to compress and refine models while maintaining or even improving their performance, not only in terms of model quality but also in actual system runtime performance. These methods are crucial for creating efficient models that can be deployed in resource-constrained environments.\nFurthermore, we explore the role of numerical precision in model computations. Understanding how different levels of numerical precision impact model size, speed, and accuracy is essential for optimizing performance. We investigate various numerical formats and the application of reduced-precision arithmetic, particularly relevant for embedded system deployments where computational resources are often limited.\nAt the lowest level, we navigate the intricate landscape of hardware-software co-design. This exploration reveals how models can be tailored to leverage the specific characteristics and capabilities of target hardware platforms. By aligning model design with hardware architecture, we can significantly enhance performance and efficiency.\nThis collective approach focuses on helping us develop and deploy efficient, powerful, and hardware-aware machine learning models. From simplifying model architectures to fine-tuning numerical precision and adapting to specific hardware, this chapter covers the full spectrum of optimization strategies. By the conclusion of this chapter, readers will have gained a thorough understanding of various optimization techniques and their practical applications in real-world scenarios. This knowledge is important for creating machine learning models that not only perform well but are also optimized for the constraints and opportunities presented by modern computing environments.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Model Optimizations</span>"
    ]
  },
  {
    "objectID": "contents/core/optimizations/optimizations.html#sec-model_ops_representation",
    "href": "contents/core/optimizations/optimizations.html#sec-model_ops_representation",
    "title": "10  Model Optimizations",
    "section": "10.2 Efficient Model Representation",
    "text": "10.2 Efficient Model Representation\nThe first avenue of attack for model optimization starts in familiar territory for most ML practitioners: efficient model representation is often first tackled at the highest level of parametrization abstraction - the model’s architecture itself.\nMost traditional ML practitioners design models with a general high-level objective in mind, whether it be image classification, person detection, or keyword spotting as mentioned previously in this textbook. Their designs generally end up naturally fitting into some soft constraints due to limited compute resources during development, but generally these designs are not aware of later constraints, such as those required if the model is to be deployed on a more constrained device instead of the cloud.\nIn this section, we’ll discuss how practitioners can harness principles of hardware-software co-design even at a model’s high level architecture to make their models compatible with edge devices. From most to least hardware aware at this level of modification, we discuss several of the most common strategies for efficient model parametrization: pruning, model compression, and edge-friendly model architectures. You were introduced to pruning and model compression previously; now, this section will go one step beyond the definitions to provide you with a technical understanding of how these techniques work.\n\n10.2.1 Pruning\n\nOverview\nModel pruning is a technique in machine learning that reduces the size and complexity of a neural network model while maintaining its predictive capabilities as much as possible. The goal of model pruning is to remove redundant or non-essential components of the model, including connections between neurons, individual neurons, or even entire layers of the network.\nThis process typically involves analyzing the machine learning model to identify and remove weights, nodes, or layers that have little impact on the model’s outputs. By selectively pruning a model in this way, the total number of parameters can be reduced significantly without substantial declines in model accuracy. The resulting compressed model requires less memory and computational resources to train and run while enabling faster inference times.\nModel pruning is especially useful when deploying machine learning models to devices with limited compute resources, such as mobile phones or TinyML systems. The technique facilitates the deployment of larger, more complex models on these devices by reducing their resource demands. Additionally, smaller models require less data to generalize well and are less prone to overfitting. By providing an efficient way to simplify models, model pruning has become a vital technique for optimizing neural networks in machine learning.\nThere are several common pruning techniques used in machine learning, these include structured pruning, unstructured pruning, iterative pruning, bayesian pruning, and even random pruning. In addition to pruning the weights, one can also prune the activations. Activation pruning specifically targets neurons or filters that activate rarely or have overall low activation. There are numerous other methods, such as sensitivity and movement pruning. For a comprehensive list of methods, the reader is encouraged to read the following paper: “A Survey on Deep Neural Network Pruning: Taxonomy, Comparison, Analysis, and Recommendations” (2023).\nSo how does one choose the type of pruning methods? Many variations of pruning techniques exist where each varies the heuristic of what should be kept and pruned from the model as well as number of times pruning occurs. Traditionally, pruning happens after the model is fully trained, where the pruned model may experience mild accuracy loss. However, as we will discuss further, recent discoveries have found that pruning can be used during training (i.e., iteratively) to identify more efficient and accurate model representations.\n\n\nStructured Pruning\nWe start with structured pruning, a technique that reduces the size of a neural network by eliminating entire model-specific substructures while maintaining the overall model structure. It removes entire neurons/channels or layers based on importance criteria. For example, for a convolutional neural network (CNN), this could be certain filter instances or channels. For fully connected networks, this could be neurons themselves while maintaining full connectivity or even be elimination of entire model layers that are deemed to be insignificant. This type of pruning often leads to regular, structured sparse networks that are hardware friendly.\nBest practices have started to emerge on how to think about structured pruning. There are three main components:\n\n1. Structures to Target for Pruning\nGiven the variety of approaches, different structures within a neural network are pruned based on specific criteria. The primary structures for pruning include neurons, channels, and sometimes entire layers, each with its unique implications and methodologies. The goal in each approach is to ensure that the reduced model retains as much of the original model’s predictive prowess as possible while improving computational efficiency and reducing size.\nWhen neurons are pruned, we are removing entire neurons along with their associated weights and biases, thereby reducing the width of the layer. This type of pruning is often utilized in fully connected layers.\nWith channel pruning, which is predominantly applied in convolutional neural networks (CNNs), it involves eliminating entire channels or filters, which in turn reduces the depth of the feature maps and impacts the network’s ability to extract certain features from the input data. This is particularly crucial in image processing tasks where computational efficiency is paramount.\nFinally, layer pruning takes a more aggressive approach by removing entire layers of the network. This significantly reduces the network’s depth and thereby its capacity to model complex patterns and hierarchies in the data. This approach necessitates a careful balance to ensure that the model’s predictive capability is not unduly compromised.\nFigure 10.2 demonstrates the difference between channel/filter wise pruning and layer pruning. When we prune a channel, we have to reconfigure the model’s architecture in order to adapt to the structural changes. One adjustment is changing the number of input channels in the subsequent layer (here, the third and deepest layer): changing the depths of the filters that are applied to the layer with the pruned channel. On the other hand, pruning an entire layer (removing all the channels in the layer) requires more drastic adjustments. The main one involves modifying the connections between the remaining layers to replace or bypass the pruned layer. In our case, we reconfigure to connect the first and last layers. In all pruning cases, we have to fine-tune the new structure to adjust the weights.\n\n\n\n\n\n\nFigure 10.2: Channel vs layer pruning.\n\n\n\n\n\n2. Establishing a Criteria for Pruning\nEstablishing well-defined criteria for determining which specific structures to prune from a neural network model is a crucial component of the model pruning process. The core goal here is to identify and remove components that contribute the least to the model’s predictive capabilities, while retaining structures integral to preserving the model’s accuracy.\nA widely adopted and effective strategy for systematically pruning structures relies on computing importance scores for individual components like neurons, filters, channels or layers. These scores serve as quantitative metrics to gauge the significance of each structure and its effect on the model’s output.\nThere are several techniques for assigning these importance scores:\n\nWeight Magnitude-Based Pruning: This approach assigns importance scores to a structure by evaluating the aggregate magnitude of their associated weights. Structures with smaller overall weight magnitudes are considered less critical to the network’s performance.\nGradient-Based Pruning: This technique utilizes the gradients of the loss function with respect to the weights associated with a structure. Structures with low cumulative gradient magnitudes, indicating minimal impact on the loss when altered, are prime candidates for pruning.\nActivation-Based Pruning: This method tracks how often a neuron or filter is activated by storing this information in a parameter called the activation counter. Each time the structure is activated, the counter is incremented. A low activation count suggests that the structure is less relevant.\nTaylor Expansion-Based Pruning: This approach approximates the change in the loss function from removing a given weight. By assessing the cumulative loss disturbance from removing all the weights associated with a structure, you can identify structures with negligible impact on the loss, making them suitable candidates for pruning.\n\nThe idea is to measure, either directly or indirectly, the contribution of each component to the model’s output. Structures with minimal influence according to the defined criteria are pruned first. This enables selective, optimized pruning that maximally compresses models while preserving predictive capacity. In general, it is important to evaluate the impact of removing particular structures on the model’s output, with recent works such as (Rachwan et al. 2022) and (Lubana and Dick 2020) investigating combinations of techniques like magnitude-based pruning and gradient-based pruning.\n\nRachwan, John, Daniel Zügner, Bertrand Charpentier, Simon Geisler, Morgane Ayle, and Stephan Günnemann. 2022. “Winning the Lottery Ahead of Time: Efficient Early Network Pruning.” In International Conference on Machine Learning, 18293–309. PMLR.\n\nLubana, Ekdeep Singh, and Robert P Dick. 2020. “A Gradient Flow Framework for Analyzing Network Pruning.” arXiv Preprint arXiv:2009.11839.\n\n\n3. Selecting a Pruning Strategy\nNow that you understand some techniques for determining the importance of structures within a neural network, the next step is to decide how to apply these insights. This involves selecting an appropriate pruning strategy, which dictates how and when the identified structures are removed and how the model is fine-tuned to maintain its performance. Two main structured pruning strategies exist: iterative pruning and one-shot pruning.\nIterative pruning gradually removes structures across multiple cycles of pruning followed by fine-tuning. In each cycle, a small set of structures are pruned based on importance criteria. The model is then fine-tuned, allowing it to adjust smoothly to the structural changes before the next pruning iteration. This gradual, cyclic approach prevents abrupt accuracy drops. It allows the model to slowly adapt as structures are reduced across iterations.\nConsider a situation where we wish to prune the 6 least effective channels (based on some specific criteria) from a convolutional neural network. In Figure 10.3, we show a simplified pruning process carried over 3 iterations. In every iteration, we only prune 2 channels. Removing the channels results in accuracy degradation. In the first iteration, the accuracy drops from 0.995 to 0.971. However, after we fine-tune the model on the new structure, we are able to recover from the performance loss, bringing the accuracy up to 0.992. Since the structural changes are minor and gradual, the network can more easily adapt to them. Running the same process 2 more times, we end up with a final accuracy of 0.991 (a loss of only 0.4% from the original) and 27% decrease in the number of channels. Thus, iterative pruning enables us to maintain performance while benefiting from increased computational efficiency due to the decreased model size.\n\n\n\n\n\n\nFigure 10.3: Iterative pruning.\n\n\n\nOne-shot pruning takes a more aggressive approach by pruning a large portion of structures simultaneously in one shot based on predefined importance criteria. This is followed by extensive fine-tuning to recover model accuracy. While faster, this aggressive strategy can degrade accuracy if the model cannot recover during fine-tuning.\nThe choice between these strategies involves weighing factors like model size, target sparsity level, available compute and acceptable accuracy losses. One-shot pruning can rapidly compress models, but iterative pruning may enable better accuracy retention for a target level of pruning. In practice, the strategy is tailored based on use case constraints. The overarching aim is to generate an optimal strategy that removes redundancy, achieves efficiency gains through pruning, and finely tunes the model to stabilize accuracy at an acceptable level for deployment.\nNow consider the same network we had in the iterative pruning example. Whereas in the iterative process we pruned 2 channels at a time, in the one-shot pruning we would prune the 6 channels at once, as shown in Figure 10.4. Removing 27% of the network’s channel simultaneously alters the structure significantly, causing the accuracy to drop from 0.995 to 0.914. Given the major changes, the network is not able to properly adapt during fine-tuning, and the accuracy went up to 0.943, a 5% degradation from the accuracy of the unpruned network. While the final structures in both iterative pruning and oneshot pruning processes are identical, the former is able to maintain high performance while the latter suffers significant degradations.\n\n\n\n\n\n\nFigure 10.4: One-shot pruning.\n\n\n\n\n\n\nAdvantages of Structured Pruning\nStructured pruning brings forth a myriad of advantages that cater to various facets of model deployment and utilization, especially in environments where computational resources are constrained.\n\nComputational Efficiency: By eliminating entire structures, such as neurons or channels, structured pruning significantly diminishes the computational load during both training and inference phases, thereby enabling faster model predictions and training convergence. Moreover, the removal of structures inherently reduces the model’s memory footprint, ensuring that it demands less storage and memory during operation, which is particularly beneficial in memory-constrained environments like TinyML systems.\nHardware Efficiency: Structured pruning often results in models that are more amenable to deployment on specialized hardware, such as Field-Programmable Gate Arrays (FPGAs) or Application-Specific Integrated Circuits (ASICs), due to the regularity and simplicity of the pruned architecture. With reduced computational requirements, it translates to lower energy consumption, which is crucial for battery-powered devices and sustainable computing practices.\nMaintenance and Deployment: The pruned model, while smaller, retains its original architectural form, which can simplify the deployment pipeline and ensure compatibility with existing systems and frameworks. Also, with fewer parameters and simpler structures, the pruned model becomes easier to manage and monitor in production environments, potentially reducing the overhead associated with model maintenance and updates. Later on, when we dive into MLOps, this need will become apparent.\n\n\n\nUnstructured Pruning\nUnstructured pruning is, as its name suggests, pruning the model without regard to model-specific substructure. As mentioned above, it offers a greater aggression in pruning and can achieve higher model sparsities while maintaining accuracy given less constraints on what can and can’t be pruned. Generally, post-training unstructured pruning consists of an importance criterion for individual model parameters/weights, pruning/removal of weights that fall below the criteria, and optional fine-tuning after to try and recover the accuracy lost during weight removal.\nUnstructured pruning has some advantages over structured pruning: removing individual weights instead of entire model substructures often leads in practice to lower model accuracy decreases. Furthermore, generally determining the criterion of importance for an individual weight is much simpler than for an entire substructure of parameters in structured pruning, making the former preferable for cases where that overhead is hard or unclear to compute. Similarly, the actual process of structured pruning is generally less flexible, as removing individual weights is generally simpler than removing entire substructures and ensuring the model still works.\nUnstructured pruning, while offering the potential for significant model size reduction and enhanced deployability, brings with it challenges related to managing sparse representations and ensuring computational efficiency. It is particularly useful in scenarios where achieving the highest possible model compression is paramount and where the deployment environment can handle sparse computations efficiently.\nTable 10.1 provides a concise comparison between structured and unstructured pruning. In this table, aspects related to the nature and architecture of the pruned model (Definition, Model Regularity, and Compression Level) are grouped together, followed by aspects related to computational considerations (Computational Efficiency and Hardware Compatibility), and ending with aspects related to the implementation and adaptation of the pruned model (Implementation Complexity and Fine-Tuning Complexity). Both pruning strategies offer unique advantages and challenges, as shown in Table 10.1, and the selection between them should be influenced by specific project and deployment requirements.\n\n\n\nTable 10.1: Comparison of structured versus unstructured pruning.\n\n\n\n\n\n\n\n\n\n\nAspect\nStructured Pruning\nUnstructured Pruning\n\n\n\n\nDefinition\nPruning entire structures (e.g., neurons, channels, layers) within the network\nPruning individual weights or neurons, resulting in sparse matrices or non-regular network structures\n\n\nModel Regularity\nMaintains a regular, structured network architecture\nResults in irregular, sparse network architectures\n\n\nCompression Level\nMay offer limited model compression compared to unstructured pruning\nCan achieve higher model compression due to fine-grained pruning\n\n\nComputational Efficiency\nTypically more computationally efficient due to maintaining regular structures\nCan be computationally inefficient due to sparse weight matrices, unless specialized hardware/software is used\n\n\nHardware Compatibility\nGenerally better compatible with various hardware due to regular structures\nMay require hardware that efficiently handles sparse computations to realize benefits\n\n\nImplementation Complexity\nOften simpler to implement and manage due to maintaining network structure\nCan be complex to manage and compute due to sparse representations\n\n\nFine-Tuning Complexity\nMay require less complex fine-tuning strategies post-pruning\nMight necessitate more complex retraining or fine-tuning strategies post-pruning\n\n\n\n\n\n\nIn Figure 10.5 we have examples that illustrate the differences between unstructured and structured pruning. Observe that unstructured pruning can lead to models that no longer obey high-level structural guarantees of their original unpruned counterparts: the left network is no longer a fully connected network after pruning. Structured pruning on the other hand maintains those invariants: in the middle, the fully connected network is pruned in a way that the pruned network is still fully connected; likewise, the CNN maintains its convolutional structure, albeit with fewer filters.\n\n\n\n\n\n\nFigure 10.5: Unstructured vs structured pruning. Source: Qi et al. (2021).\n\n\nQi, Chen, Shibo Shen, Rongpeng Li, Zhifeng Zhao, Qing Liu, Jing Liang, and Honggang Zhang. 2021. “An Efficient Pruning Scheme of Deep Neural Networks for Internet of Things Applications.” EURASIP Journal on Advances in Signal Processing 2021 (1): 31. https://doi.org/10.1186/s13634-021-00744-4.\n\n\n\n\nLottery Ticket Hypothesis\nPruning has evolved from a purely post-training technique that came at the cost of some accuracy, to a powerful meta-learning approach applied during training to reduce model complexity. This advancement in turn improves compute, memory, and latency efficiency at both training and inference.\nA breakthrough finding that catalyzed this evolution was the lottery ticket hypothesis by Frankle and Carbin (2019). Their work states that within dense neural networks, there exist sparse subnetworks, referred to as “winning tickets,” that can match or even exceed the performance of the original model when trained in isolation. Specifically, these winning tickets, when initialized using the same weights as the original network, can achieve similarly high training convergence and accuracy on a given task. It is worthwhile pointing out that they empirically discovered the lottery ticket hypothesis, which was later formalized.\n\nFrankle, Jonathan, and Michael Carbin. 2019. “The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks.” In 7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019. OpenReview.net. https://openreview.net/forum?id=rJl-b3RcF7.\nThe intuition behind this hypothesis is that, during the training process of a neural network, many neurons and connections become redundant or unimportant, particularly with the inclusion of training techniques encouraging redundancy like dropout. Identifying, pruning out, and initializing these “winning tickets’’ allows for faster training and more efficient models, as they contain the essential model decision information for the task. Furthermore, as generally known with the bias-variance tradeoff theory, these tickets suffer less from overparameterization and thus generalize better rather than overfitting to the task.\nIn Figure 10.6 we have an example experiment showing pruning and training experiments on a fully connected LeNet over a variety of pruning ratios. In the left plot, notice how heavy pruning reveals a more efficient subnetwork (in green) that is 21.1% the size of the original network (in blue), The subnetwork achieves higher accuracy and in a faster manner than the unpruned version (green line is above the blue line). However, pruning has a limit (sweet spot), and further pruning will produce performance degradations and eventually drop below the unpruned version’s performance (notice how the red, purple, and brown subnetworks gradually drop in accuracy performance) due to the significant loss in the number of parameters.\n\n\n\n\n\n\nFigure 10.6: Lottery ticket hypothesis experiments.\n\n\n\nTo uncover these winning lottery tickets within a neural network, a systematic process is followed. This process, which is illustrated in Figure 10.7 (left side), involves iteratively training, pruning, and reinitializing the network. The steps below outline this approach:\n\nInitialize the network’s weights to random values.\nTrain the network until it converges to the desired performance.\nPrune out some percentage of the edges with the lowest weight values.\nReinitialize the network with the same random values from step 1.\nRepeat steps 2-4 for a number of times, or as long as the accuracy doesn’t significantly degrade.\n\nWhen we finish, we are left with a pruned network (Figure 10.7 right side), which is a subnetwork of the one we start with. The subnetwork should have a significantly smaller structure, while maintaining a comparable level of accuracy.\n\n\n\n\n\n\nFigure 10.7: Finding the winning ticket subnetwork.\n\n\n\n\n\nChallenges & Limitations\nThere is no free lunch with pruning optimizations, with some choices coming with both improvements and costs to considers. Below we discuss some tradeoffs for practitioners to consider.\n\nManaging Sparse Weight Matrices: A sparse weight matrix is a matrix in which many of the elements are zero. Unstructured pruning often results in sparse weight matrices, where many weights are pruned to zero. While this reduces model size, it also introduces several challenges. Computational inefficiency can arise because standard hardware is optimized for dense matrix operations. Without optimizations that take advantage of sparsity, the computational savings from pruning can be lost. Although sparse matrices can be stored without specialized formats, effectively leveraging their sparsity requires careful handling to avoid wasting resources. Algorithmically, navigating sparse structures requires efficiently skipping over zero entries, which adds complexity to the computation and model updates.\nQuality vs. Size Reduction: A key challenge in both structured and unstructured pruning is balancing size reduction with maintaining or improving predictive performance. Establishing robust pruning criteria, whether for removing entire structures (structured pruning) or individual weights (unstructured pruning), is essential. These pruning criteria chosen must accurately identify elements whose removal minimally impacts performance. Careful experimentation is often needed to ensure the pruned model remains efficient while maintaining its predictive performance.\nFine-Tuning and Retraining: Post-pruning fine-tuning is imperative in both structured and unstructured pruning to recover lost performance and stabilize the model. The challenge encompasses determining the extent, duration, and nature of the fine-tuning process, which can be influenced by the pruning method and the degree of pruning applied.\nHardware Compatibility and Efficiency: Especially pertinent to unstructured pruning, hardware compatibility and efficiency become critical. Unstructured pruning often results in sparse weight matrices, which may not be efficiently handled by certain hardware, potentially negating the computational benefits of pruning (see Figure 10.8). Ensuring that pruned models, particularly those resulting from unstructured pruning, are scalable, compatible, and efficient on the target hardware is a significant consideration.\nLegal and Ethical Considerations: Last but not least, adherence to legal and ethical guidelines is important, especially in domains with significant consequences. Pruning methods must undergo rigorous validation, testing, and potentially certification processes to ensure compliance with relevant regulations and standards, though arguably at this time no such formal standards and best practices exist that are vetted and validated by 3rd party entities. This is particularly crucial in high-stakes applications like medical AI and autonomous driving, where quality drops due to pruning-like optimizations can be life-threatening. Moreover, ethical considerations extend beyond safety to fairness and equality; recent work by (Tran et al. 2022) has revealed that pruning can disproportionately impact people of color, underscoring the need for comprehensive ethical evaluation in the pruning process.\n\n\nTran, Cuong, Ferdinando Fioretto, Jung-Eun Kim, and Rakshit Naidu. 2022. “Pruning Has a Disparate Impact on Model Accuracy.” Adv Neural Inf Process Syst 35: 17652–64.\n\n\n\n\n\n\nFigure 10.8: Sparse weight matrix.\n\n\n\n\n\n\n\n\n\nExercise 10.1: Pruning\n\n\n\n\n\nImagine your neural network is a giant, overgrown bush. Pruning is like strategically trimming away branches to make it stronger and more efficient! In the Colab, you’ll learn how to do this trimming in TensorFlow. Understanding these concepts will give you the foundation to see how pruning makes models small enough to run on your phone!\n\n\n\n\n\n\n\n10.2.2 Model Compression\nModel compression techniques are crucial for deploying deep learning models on resource-constrained devices. These techniques aim to create smaller, more efficient models that preserve the predictive performance of the original models.\n\nKnowledge Distillation\nOne popular technique is knowledge distillation (KD), which transfers knowledge from a large, complex “teacher” model to a smaller “student” model. The key idea is to train the student model to mimic the teacher’s outputs. The concept of KD was first popularized by Hinton (2005).\n\nHinton, Geoffrey. 2005. “Van Nostrand’s Scientific Encyclopedia.” Wiley. https://doi.org/10.1002/0471743984.vse0673.\n\nOverview and Benefits\nKnowledge distillation involves transferring knowledge from a large, complex teacher model to a smaller student model. The core idea is to use the teacher’s outputs, known as soft targets, to guide the training of the student model. Unlike traditional “hard targets” (the true labels), soft targets are the probability distributions over classes that the teacher model predicts. These distributions provide richer information about the relationships between classes, which can help the student model learn more effectively.\nYou have learned that the softmax function converts a model’s raw outputs into a probability distribution over classes. A key technique in KD is temperature scaling, which is applied to the softmax function of the teacher model’s outputs. By introducing a temperature parameter, the distribution can be adjusted: a higher temperature produces softer probabilities, meaning the differences between class probabilities become less extreme. This softening effect results in a more uniform distribution, where the model’s confidence in the most likely class is reduced, and other classes have higher, non-zero probabilities. This is valuable for the student model because it allows it to learn not just from the most likely class but from the relative probabilities of all classes, capturing subtle patterns that might be missed if trained only on hard targets. Thus, temperature scaling facilitates the transfer of more nuanced knowledge from the teacher to the student model.\nThe loss function in knowledge distillation typically combines two components: a distillation loss and a classification loss. The distillation loss, often calculated using Kullback-Leibler (KL) divergence, measures the difference between the soft targets produced by the teacher model and the outputs of the student model, encouraging the student to mimic the teacher’s predictions. Meanwhile, the classification loss ensures that the student model correctly predicts the true labels based on the original data. Together, these two components help the student model retain the knowledge of the teacher while adhering to the ground truth labels.\nThese components, when adeptly configured and harmonized, enable the student model to assimilate the teacher model’s knowledge, crafting a pathway towards efficient and robust smaller models that retain the predictive prowess of their larger counterparts. Figure 10.9 visualizes the training procedure of knowledge distillation. Note how the logits or soft labels of the teacher model are used to provide a distillation loss for the student model to learn from.\n\n\n\n\n\n\nFigure 10.9: Knowledge distillation training process. Source: IntelLabs (2023).\n\n\nIntelLabs. 2023. “Knowledge Distillation - Neural Network Distiller.” https://intellabs.github.io/distiller/knowledge_distillation.html.\n\n\n\n\nChallenges\nHowever, KD has a unique set of challenges and considerations that researchers and practitioners must attentively address. One of the challenges is in the meticulous tuning of hyperparameters, such as the temperature parameter in the softmax function and the weighting between the distillation and classification loss in the objective function. Striking a balance that effectively leverages the softened outputs of the teacher model while maintaining fidelity to the true data labels is non-trivial and can significantly impact the student model’s performance and generalization capabilities.\nFurthermore, the architecture of the student model itself poses a considerable challenge. Designing a model that is compact to meet computational and memory constraints, while still being capable of assimilating the essential knowledge from the teacher model, demands a nuanced understanding of model capacity and the inherent trade-offs involved in compression. The student model must be carefully architected to navigate the dichotomy of size and performance, ensuring that the distilled knowledge is meaningfully captured and utilized. Moreover, the choice of teacher model, which inherently influences the quality and nature of the knowledge to be transferred, is important and it introduces an added layer of complexity to the KD process.\nThese challenges underscore the necessity for a thorough and nuanced approach to implementing KD, ensuring that the resultant student models are both efficient and effective in their operational contexts.\n\n\n\nLow-rank Matrix Factorization\nSimilar in approximation theme, low-rank matrix factorization (LRMF) is a mathematical technique used in linear algebra and data analysis to approximate a given matrix by decomposing it into two or more lower-dimensional matrices. The fundamental idea is to express a high-dimensional matrix as a product of lower-rank matrices, which can help reduce the complexity of data while preserving its essential structure. Mathematically, given a matrix \\(A \\in \\mathbb{R}^{m \\times n}\\), LRMF seeks matrices \\(U \\in \\mathbb{R}^{m \\times k}\\) and \\(V \\in \\mathbb{R}^{k \\times n}\\) such that \\(A \\approx UV\\), where \\(k\\) is the rank and is typically much smaller than \\(m\\) and \\(n\\).\n\nBackground and Benefits\nOne of the seminal works in the realm of matrix factorization, particularly in the context of recommendation systems, is the paper by Koren, Bell, and Volinsky (2009). The authors look into various factorization models, providing insights into their efficacy in capturing the underlying patterns in the data and enhancing predictive accuracy in collaborative filtering. LRMF has been widely applied in recommendation systems (such as Netflix, Facebook, etc.), where the user-item interaction matrix is factorized to capture latent factors corresponding to user preferences and item attributes.\n\nKoren, Yehuda, Robert Bell, and Chris Volinsky. 2009. “Matrix Factorization Techniques for Recommender Systems.” Computer 42 (8): 30–37. https://doi.org/10.1109/mc.2009.263.\nThe main advantage of low-rank matrix factorization lies in its ability to reduce data dimensionality as shown in Figure 10.10, where there are fewer parameters to store, making it computationally more efficient and reducing storage requirements at the cost of some additional compute. This can lead to faster computations and more compact data representations, which is especially valuable when dealing with large datasets. Additionally, it may aid in noise reduction and can reveal underlying patterns and relationships in the data.\nFigure 10.10 illustrates the decrease in parameterization enabled by low-rank matrix factorization. Observe how the matrix \\(M\\) can be approximated by the product of matrices \\(L_k\\) and \\(R_k^T\\). For intuition, most fully connected layers in networks are stored as a projection matrix \\(M\\), which requires \\(m \\times n\\) parameter to be loaded on computation. However, by decomposing and approximating it as the product of two lower rank matrices, we thus only need to store \\(m \\times k + k\\times n\\) parameters in terms of storage while incurring an additional compute cost of the matrix multiplication. So long as \\(k &lt; n/2\\), this factorization has fewer parameters total to store while adding a computation of runtime \\(O(mkn)\\) (Gu 2023).\n\nGu, Ivy. 2023. “Deep Learning Model Compression (Ii) by Ivy Gu Medium.” https://ivygdy.medium.com/deep-learning-model-compression-ii-546352ea9453.\n\n\n\n\n\n\nFigure 10.10: Low matrix factorization. Source: The Clever Machine.\n\n\n\n\n\nChallenges\nBut practitioners and researchers encounter a spectrum of challenges and considerations that necessitate careful attention and strategic approaches. As with any lossy compression technique, we may lose information during this approximation process: choosing the correct rank that balances the information lost and the computational costs is tricky as well and adds an additional hyper-parameter to tune for.\nLow-rank matrix factorization is a valuable tool for dimensionality reduction and making compute fit onto edge devices but, like other techniques, needs to be carefully tuned to the model and task at hand. A key challenge resides in managing the computational complexity inherent to LRMF, especially when grappling with high-dimensional and large-scale data. The computational burden, particularly in the context of real-time applications and massive datasets, remains a significant hurdle for effectively using LRMF.\nMoreover, the conundrum of choosing the optimal rank \\(k\\), for the factorization introduces another layer of complexity. The selection of \\(k\\) inherently involves a trade-off between approximation accuracy and model simplicity, and identifying a rank that adeptly balances these conflicting objectives often demands a combination of domain expertise, empirical validation, and sometimes, heuristic approaches. The challenge is further amplified when the data encompasses noise or when the inherent low-rank structure is not pronounced, making the determination of a suitable \\(k\\) even more elusive.\nHandling missing or sparse data, a common occurrence in applications like recommendation systems, poses another substantial challenge. Traditional matrix factorization techniques, such as Singular Value Decomposition (SVD), are not directly applicable to matrices with missing entries, necessitating the development and application of specialized algorithms that can factorize incomplete matrices while mitigating the risks of overfitting to the observed entries. This often involves incorporating regularization terms or constraining the factorization in specific ways, which in turn introduces additional hyperparameters that need to be judiciously selected.\nFurthermore, in scenarios where data evolves or grows over time, developing LRMF models that can adapt to new data without necessitating a complete re-factorization is a critical yet challenging endeavor. Online and incremental matrix factorization algorithms seek to address this by enabling the update of factorized matrices as new data arrives, yet ensuring stability, accuracy, and computational efficiency in these dynamic settings remains an intricate task. This is particularly challenging in the space of TinyML, where edge redeployment for refreshed models can be quite challenging.\n\n\n\nTensor Decomposition\nYou have learned in Section 7.3.3 that tensors are flexible structures, commonly used by ML Frameworks, that can represent data in higher dimensions. Similar to low-rank matrix factorization, more complex models may store weights in higher dimensions, such as tensors. Tensor decomposition is the higher-dimensional analogue of matrix factorization, where a model tensor is decomposed into lower-rank components (see Figure 10.11). These lower-rank components are easier to compute on and store but may suffer from the same issues mentioned above, such as information loss and the need for nuanced hyperparameter tuning. Mathematically, given a tensor \\(\\mathcal{A}\\), tensor decomposition seeks to represent \\(\\mathcal{A}\\) as a combination of simpler tensors, facilitating a compressed representation that approximates the original data while minimizing the loss of information.\n\n\n\n\n\n\nFigure 10.11: Tensor decomposition. Source: Xinyu (n.d.).\n\n\nXinyu, Chen. n.d.\n\n\nThe work of Tamara G. Kolda and Brett W. Bader, “Tensor Decompositions and Applications” (2009), stands out as a seminal paper in the field of tensor decompositions. The authors provide a comprehensive overview of various tensor decomposition methods, exploring their mathematical underpinnings, algorithms, and a wide array of applications, ranging from signal processing to data mining. Of course, the reason we are discussing it is because it has huge potential for system performance improvements, particularly in the space of TinyML, where throughput and memory footprint savings are crucial to feasibility of deployments.\n\n\n\n\n\n\nExercise 10.2: Scalable Model Compression with TensorFlow\n\n\n\n\n\nThis Colab dives into a technique for compressing models while maintaining high accuracy. The key idea is to train a model with an extra penalty term that encourages the model to be more compressible. Then, the model is encoded using a special coding scheme that aligns with this penalty. This approach allows you to achieve compressed models that perform just as well as the original models and is useful in deploying models to devices with limited resources like mobile phones and edge devices.\n\n\n\n\n\n\n\n10.2.3 Edge-Aware Model Design\nNow, we reach the other end of the hardware-software gradient, where we specifically make model architecture decisions directly given knowledge of the edge devices we wish to deploy on.\nAs covered in previous sections, edge devices are constrained specifically with limitations on memory and parallelizable computations: as such, if there are critical inference speed requirements, computations must be flexible enough to satisfy hardware constraints, something that can be designed at the model architecture level. Furthermore, trying to cram SOTA large ML models onto edge devices even after pruning and compression is generally infeasible purely due to size: the model complexity itself must be chosen with more nuance as to more feasibly fit the device. Edge ML developers have approached this architectural challenge both through designing bespoke edge ML model architectures and through device-aware neural architecture search (NAS), which can more systematically generate feasible on-device model architectures.\n\nModel Design Techniques\nOne edge friendly architecture design, commonly used in deep learning for image processing, is depthwise separable convolutions. It consists of two distinct steps: the first is the depthwise convolution, where each input channel is convolved independently with its own set of learnable filters, as shown in Figure 10.12. This step reduces computational complexity by a significant margin compared to standard convolutions, as it drastically reduces the number of parameters and computations involved. The second step is the pointwise convolution, which combines the output of the depthwise convolution channels through a 1x1 convolution, creating inter-channel interactions. This approach offers several advantages. Benefits include reduced model size, faster inference times, and often better generalization due to fewer parameters, making it suitable for mobile and embedded applications. However, depthwise separable convolutions may not capture complex spatial interactions as effectively as standard convolutions and might require more depth (layers) to achieve the same level of representational power, potentially leading to longer training times. Nonetheless, their efficiency in terms of parameters and computation makes them a popular choice in modern convolutional neural network architectures.\n\n\n\n\n\n\nFigure 10.12: Depthwise separable convolutions. Source: Hegde (2023).\n\n\nHegde, Sumant. 2023. “An Introduction to Separable Convolutions - Analytics Vidhya.” https://www.analyticsvidhya.com/blog/2021/11/an-introduction-to-separable-convolutions/.\n\n\n\n\nExample Model Architectures\nIn this vein, a number of recent architectures have been, from inception, specifically designed for maximizing accuracy on an edge deployment, notably SqueezeNet, MobileNet, and EfficientNet.\n\nSqueezeNet by Iandola et al. (2016) for instance, utilizes a compact architecture with 1x1 convolutions and fire modules to minimize the number of parameters while maintaining strong accuracy.\nMobileNet by Howard et al. (2017), on the other hand, employs the aforementioned depthwise separable convolutions to reduce both computation and model size.\nEfficientNet by Tan and Le (2023) takes a different approach by optimizing network scaling (i.e. varying the depth, width and resolution of a network) and compound scaling, a more nuanced variation network scaling, to achieve superior performance with fewer parameters.\n\n\nIandola, Forrest N, Song Han, Matthew W Moskewicz, Khalid Ashraf, William J Dally, and Kurt Keutzer. 2016. “SqueezeNet: Alexnet-level Accuracy with 50x Fewer Parameters and 0.5 MB Model Size.” ArXiv Preprint abs/1602.07360. https://arxiv.org/abs/1602.07360.\n\nHoward, Andrew G., Menglong Zhu, Bo Chen, Dmitry Kalenichenko, Weijun Wang, Tobias Weyand, Marco Andreetto, and Hartwig Adam. 2017. “MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications.” ArXiv Preprint. https://arxiv.org/abs/1704.04861.\n\nTan, Mingxing, and Quoc V. Le. 2023. “Demystifying Deep Learning.” Wiley. https://doi.org/10.1002/9781394205639.ch6.\nThese models are essential in the context of edge computing where limited processing power and memory require lightweight yet effective models that can efficiently perform tasks such as image recognition, object detection, and more. Their design principles showcase the importance of intentionally tailored model architecture for edge computing, where performance and efficiency must fit within constraints.\n\n\nStreamlining Model Architecture Search\nLastly, to address the challenge of finding efficient model architectures that are compatible with edge devices, researchers have developed systematized pipelines that streamline the search for performant designs. Two notable frameworks in this space are TinyNAS by J. Lin et al. (2020) and MorphNet by Gordon et al. (2018), which automate the process of optimizing neural network architectures for edge deployment.\n\nGordon, Ariel, Elad Eban, Ofir Nachum, Bo Chen, Hao Wu, Tien-Ju Yang, and Edward Choi. 2018. “MorphNet: Fast &Amp; Simple Resource-Constrained Structure Learning of Deep Networks.” In 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition, 1586–95. IEEE. https://doi.org/10.1109/cvpr.2018.00171.\nTinyNAS is an innovative neural architecture search framework introduced in the MCUNet paper, designed to efficiently discover lightweight neural network architectures for edge devices with limited computational resources. Leveraging reinforcement learning and a compact search space of micro neural modules, TinyNAS optimizes for both accuracy and latency, enabling the deployment of deep learning models on microcontrollers, IoT devices, and other resource-constrained platforms. Specifically, TinyNAS, in conjunction with a network optimizer TinyEngine, generates different search spaces by scaling the input resolution and the model width of a model, then collects the computation FLOPs distribution of satisfying networks within the search space to evaluate its priority. TinyNAS relies on the assumption that a search space that accommodates higher FLOPs under memory constraint can produce higher accuracy models, something that the authors verified in practice in their work. In empirical performance, TinyEngine reduced the peak memory usage of models by around 3.4 times and accelerated inference by 1.7 to 3.3 times compared to TFLite and CMSIS-NN.\nSimilarly, MorphNet is a neural network optimization framework designed to automatically reshape and morph the architecture of deep neural networks, optimizing them for specific deployment requirements. It achieves this through two steps: first, it leverages a set of customizable network morphing operations, such as widening or deepening layers, to dynamically adjust the network’s structure. These operations enable the network to adapt to various computational constraints, including model size, latency, and accuracy targets, which are extremely prevalent in edge computing usage. In the second step, MorphNet uses a reinforcement learning-based approach to search for the optimal permutation of morphing operations, effectively balancing the trade-off between model size and performance. This innovative method allows deep learning practitioners to automatically tailor neural network architectures to specific application and hardware requirements, ensuring efficient and effective deployment across various platforms.\nTinyNAS and MorphNet represent a few of the many significant advancements in the field of systematic neural network optimization, allowing architectures to be systematically chosen and generated to fit perfectly within problem constraints.\n\n\n\n\n\n\nExercise 10.3: Edge-Aware Model Design\n\n\n\n\n\nImagine you’re building a tiny robot that can identify different flowers. It needs to be smart, but also small and energy-efficient! In the “Edge-Aware Model Design” world, we learned about techniques like depthwise separable convolutions and architectures like SqueezeNet, MobileNet, and EfficientNet—all designed to pack intelligence into compact models. Now, let’s see these ideas in action with some xColabs:\nSqueezeNet in Action: Maybe you’d like a Colab showing how to train a SqueezeNet model on a flower image dataset. This would demonstrate its small size and how it learns to recognize patterns despite its efficiency.\n\nMobileNet Exploration: Ever wonder if those tiny image models are just as good as the big ones? Let’s find out! In this Colab, we’re pitting MobileNet, the lightweight champion, against a classic image classification model. We’ll race them for speed, measure their memory needs, and see who comes out on top for accuracy. Get ready for a battle of the image brains!",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Model Optimizations</span>"
    ]
  },
  {
    "objectID": "contents/core/optimizations/optimizations.html#sec-model_ops_numerics",
    "href": "contents/core/optimizations/optimizations.html#sec-model_ops_numerics",
    "title": "10  Model Optimizations",
    "section": "10.3 Efficient Numerics Representation",
    "text": "10.3 Efficient Numerics Representation\nNumerics representation involves a myriad of considerations, including, but not limited to, the precision of numbers, their encoding formats, and the arithmetic operations facilitated. It invariably involves a rich array of different trade-offs, where practitioners are tasked with navigating between numerical accuracy and computational efficiency. For instance, while lower-precision numerics may offer the allure of reduced memory usage and expedited computations, they concurrently present challenges pertaining to numerical stability and potential degradation of model accuracy.\n\nMotivation\nThe imperative for efficient numerics representation arises, particularly as efficient model optimization alone falls short when adapting models for deployment on low-powered edge devices operating under stringent constraints.\nBeyond minimizing memory demands, the tremendous potential of efficient numerics representation lies in, but is not limited to, these fundamental ways. By diminishing computational intensity, efficient numerics can thereby amplify computational speed, allowing more complex models to compute on low-powered devices. Reducing the bit precision of weights and activations on heavily over-parameterized models enables condensation of model size for edge devices without significantly harming the model’s predictive accuracy. With the omnipresence of neural networks in models, efficient numerics has a unique advantage in leveraging the layered structure of NNs to vary numeric precision across layers, minimizing precision in resistant layers while preserving higher precision in sensitive layers.\nIn this section, we will dive into how practitioners can harness the principles of hardware-software co-design at the lowest levels of a model to facilitate compatibility with edge devices. Kicking off with an introduction to the numerics, we will examine its implications for device memory and computational complexity. Subsequently, we will embark on a discussion regarding the trade-offs entailed in adopting this strategy, followed by a deep dive into a paramount method of efficient numerics: quantization.\n\n\n10.3.1 The Basics\n\nTypes\nNumerical data, the bedrock upon which machine learning models stand, manifest in two primary forms. These are integers and floating point numbers.\nIntegers: Whole numbers, devoid of fractional components, integers (e.g., -3, 0, 42) are key in scenarios demanding discrete values. For instance, in ML, class labels in a classification task might be represented as integers, where “cat”, “dog”, and “bird” could be encoded as 0, 1, and 2 respectively.\nFloating-Point Numbers: Encompassing real numbers, floating-point numbers (e.g., -3.14, 0.01, 2.71828) afford the representation of values with fractional components. In ML model parameters, weights might be initialized with small floating-point values, such as 0.001 or -0.045, to commence the training process. Currently, there are 4 popular precision formats discussed below.\nVariable bit widths: Beyond the standard widths, research is ongoing into extremely low bit-width numerics, even down to binary or ternary representations. Extremely low bit-width operations can offer significant speedups and reduce power consumption even further. While challenges remain in maintaining model accuracy with such drastic quantization, advances continue to be made in this area.\n\n\nPrecision\nPrecision, delineating the exactness with which a number is represented, bifurcates typically into single, double, half and in recent years there have been a number of other precisions that have emerged to better support machine learning tasks efficiently on the underlying hardware.\nDouble Precision (Float64): Allocating 64 bits, double precision (e.g., 3.141592653589793) provides heightened accuracy, albeit demanding augmented memory and computational resources. In scientific computations, where precision is paramount, variables like π might be represented with Float64.\nSingle Precision (Float32): With 32 bits at its disposal, single precision (e.g., 3.1415927) strikes a balance between numerical accuracy and memory conservation. In ML, Float32 might be employed to store weights during training to maintain a reasonable level of precision.\nHalf Precision (Float16): Constrained to 16 bits, half precision (e.g., 3.14) curtails memory usage and can expedite computations, albeit sacrificing numerical accuracy and range. In ML, especially during inference on resource-constrained devices, Float16 might be utilized to reduce the model’s memory footprint.\nBfloat16: Brain Floating-Point Format or Bfloat16, also employs 16 bits but allocates them differently compared to FP16: 1 bit for the sign, 8 bits for the exponent (resulting in the same number range as in float32), and 7 bits for the fraction. This format, developed by Google, prioritizes a larger exponent range over precision, making it particularly useful in deep learning applications where the dynamic range is crucial.\nFigure 10.13 illustrates the differences between the three floating-point formats: Float32, Float16, and BFloat16.\n\n\n\n\n\n\nFigure 10.13: Three floating-point formats.\n\n\n\nInteger: Integer representations are made using 8, 4, and 2 bits. They are often used during the inference phase of neural networks, where the weights and activations of the model are quantized to these lower precisions. Integer representations are deterministic and offer significant speed and memory advantages over floating-point representations. For many inference tasks, especially on edge devices, the slight loss in accuracy due to quantization is often acceptable given the efficiency gains. An extreme form of integer numerics is for binary neural networks (BNNs), where weights and activations are constrained to one of two values: either +1 or -1.\n\n\nNumeric Encoding and Storage\nNumeric encoding, the art of transmuting numbers into a computer-amenable format, and their subsequent storage are critical for computational efficiency. For instance, floating-point numbers might be encoded using the IEEE 754 standard, which apportions bits among sign, exponent, and fraction components, thereby enabling the representation of a vast array of values with a single format. There are a few new IEEE floating point formats that have been defined specifically for AI workloads:\n\nbfloat16- A 16-bit floating point format introduced by Google. It has 8 bits for exponent, 7 bits for mantissa and 1 bit for sign. Offers a reduced precision compromise between 32-bit float and 8-bit integers. Supported on many hardware accelerators.\nposit - A configurable format that can represent different levels of precision based on exponent bits. It is more efficient than IEEE 754 binary floats. Has adjustable dynamic range and precision.\nFlexpoint - A format introduced by Intel that can dynamically adjust precision across layers or within a layer. Allows tuning precision to accuracy and hardware requirements.\nBF16ALT - A proposed 16-bit format by ARM as an alternative to bfloat16. Uses additional bit in exponent to prevent overflow/underflow.\nTF32 - Introduced by Nvidia for Ampere GPUs. Uses 10 bits for exponent instead of 8 bits like FP32. Improves model training performance while maintaining accuracy.\nFP8 - 8-bit floating point format that keeps 6 bits for mantissa and 2 bits for exponent. Enables better dynamic range than integers.\n\nThe key goals of these new formats are to provide lower precision alternatives to 32-bit floats for better computational efficiency and performance on AI accelerators while maintaining model accuracy. They offer different tradeoffs in terms of precision, range and implementation cost/complexity.\n\n\n\n10.3.2 Efficiency Benefits\nNumerical efficiency matters for machine learning workloads for a number of reasons. Efficient numerics is not just about reducing the bit-width of numbers but understanding the trade-offs between accuracy and efficiency. As machine learning models become more pervasive, especially in real-world, resource-constrained environments, the focus on efficient numerics will continue to grow. By thoughtfully selecting and leveraging the appropriate numeric precision, one can achieve robust model performance while optimizing for speed, memory, and energy.\n\n\n10.3.3 Numeric Representation Nuances\nThere are a number of nuances with numerical representations for ML that require us to have an understanding of both the theoretical and practical aspects of numerics representation, as well as a keen awareness of the specific requirements and constraints of the application domain.\n\nMemory Usage\nThe memory footprint of ML models, particularly those of considerable complexity and depth, can be substantial, thereby posing a significant challenge in both training and deployment phases. For instance, a deep neural network with 100 million parameters, represented using Float32 (32 bits or 4 bytes per parameter), would necessitate approximately 400 MB of memory just for storing the model weights. This does not account for additional memory requirements during training for storing gradients, optimizer states, and forward pass caches, which can further amplify the memory usage, potentially straining the resources on certain hardware, especially edge devices with limited memory capacity.\nThe choice of numeric representation further impacts memory usage and computational efficiency. For example, using Float64 for model weights would double the memory requirements compared to Float32, and could potentially increase computational time as well. For a weight matrix with dimensions [1000, 1000], Float64 would consume approximately 8MB of memory, while Float32 would reduce this to about 4MB. Thus, selecting an appropriate numeric format is crucial for optimizing both memory and computational efficiency.\n\n\nComputational Complexity\nNumerical precision directly impacts computational complexity, influencing the time and resources required to perform arithmetic operations. For example, operations using Float64 generally consume more computational resources than their Float32 or Float16 counterparts (see Figure 10.14). In the realm of ML, where models might need to process millions of operations (e.g., multiplications and additions in matrix operations during forward and backward passes), even minor differences in the computational complexity per operation can aggregate into a substantial impact on training and inference times. As shown in Figure 10.15, quantized models can be many times faster than their unquantized versions.\n\n\n\n\n\n\nFigure 10.14: Energy use by quantized operations. Source: Mark Horowitz, Stanford University.\n\n\n\n\n\n\n\n\n\nFigure 10.15: Speed of three different models in normal and quantized form.\n\n\n\nIn addition to pure runtimes, there is also a concern over energy efficiency. Not all numerical computations are created equal from the underlying hardware standpoint. Some numerical operations are more energy efficient than others. For example, Figure 10.16 below shows that integer addition is much more energy efficient than integer multiplication.\n\n\n\n\n\n\nFigure 10.16: Energy use by quantized operations. Source: Isscc (2014).\n\n\nIsscc. 2014. “Computing’s Energy Problem (and What We Can Do about It).” https://ieeexplore.ieee.org/document/6757323.\n\n\n\n\nHardware Compatibility\nEnsuring compatibility and optimized performance across diverse hardware platforms is another challenge in numerics representation. Different hardware, such as CPUs, GPUs, TPUs, and FPGAs, have varying capabilities and optimizations for handling different numeric precisions. For example, certain GPUs might be optimized for Float32 computations, while others might provide accelerations for Float16. Developing and optimizing ML models that can leverage the specific numerical capabilities of different hardware, while ensuring that the model maintains its accuracy and robustness, requires careful consideration and potentially additional development and testing efforts.\n\n\nPrecision and Accuracy Trade-offs\nThe trade-off between numerical precision and model accuracy is a nuanced challenge in numerics representation. Utilizing lower-precision numerics, such as Float16, might conserve memory and expedite computations but can also introduce issues like quantization error and reduced numerical range. For instance, training a model with Float16 might introduce challenges in representing very small gradient values, potentially impacting the convergence and stability of the training process. Furthermore, in certain applications, such as scientific simulations or financial computations, where high precision is paramount, the use of lower-precision numerics might not be permissible due to the risk of accruing significant errors.\n\n\nTrade-off Examples\nTo understand and appreciate the nuances, let’s consider some use case examples. Through these we will realize that the choice of numeric representation is not merely a technical decision but a strategic one, influencing the model’s predictive acumen, its computational demands, and its deployability across diverse computational environments. In this section we will look at a couple of examples to better understand the trade-offs with numerics and how they tie to the real world.\n\nAutonomous Vehicles\nIn the domain of autonomous vehicles, ML models are employed to interpret sensor data and make real-time decisions. The models must process high-dimensional data from various sensors (e.g., LiDAR, cameras, radar) and execute numerous computations within a constrained time frame to ensure safe and responsive vehicle operation. So the trade-offs here would include:\n\nMemory Usage: Storing and processing high-resolution sensor data, especially in floating-point formats, can consume substantial memory.\nComputational Complexity: Real-time processing demands efficient computations, where higher-precision numerics might impede the timely execution of control actions.\n\n\n\nMobile Health Applications\nMobile health applications often use ML models for tasks like activity recognition, health monitoring, or predictive analytics, operating within the resource-constrained environment of mobile devices. The trade-offs here would include:\n\nPrecision and Accuracy Trade-offs: Employing lower-precision numerics to conserve resources might impact the accuracy of health predictions or anomaly detections, which could have significant implications for user health and safety.\nHardware Compatibility: Models need to be optimized for diverse mobile hardware, ensuring efficient operation across a wide range of devices with varying numerical computation capabilities.\n\n\n\nHigh-Frequency Trading (HFT) Systems\nHFT systems leverage ML models to make rapid trading decisions based on real-time market data. These systems demand extremely low-latency responses to capitalize on short-lived trading opportunities.\n\nComputational Complexity: The models must process and analyze vast streams of market data with minimal latency, where even slight delays, potentially introduced by higher-precision numerics, can result in missed opportunities.\nPrecision and Accuracy Trade-offs: Financial computations often demand high numerical precision to ensure accurate pricing and risk assessments, posing challenges in balancing computational efficiency and numerical accuracy.\n\n\n\nEdge-Based Surveillance Systems\nSurveillance systems deployed on edge devices, like security cameras, use ML models for tasks like object detection, activity recognition, and anomaly detection, often operating under stringent resource constraints.\n\nMemory Usage: Storing pre-trained models and processing video feeds in real-time demands efficient memory usage, which can be challenging with high-precision numerics.\nHardware Compatibility: Ensuring that models can operate efficiently on edge devices with varying hardware capabilities and optimizations for different numeric precisions is crucial for widespread deployment.\n\n\n\nScientific Simulations\nML models are increasingly being utilized in scientific simulations, such as climate modeling or molecular dynamics simulations, to improve predictive capabilities and reduce computational demands.\n\nPrecision and Accuracy Trade-offs: Scientific simulations often require high numerical precision to ensure accurate and reliable results, which can conflict with the desire to reduce computational demands via lower-precision numerics.\nComputational Complexity: The models must manage and process complex, high-dimensional simulation data efficiently to ensure timely results and enable large-scale or long-duration simulations.\n\nThese examples illustrate diverse scenarios where the challenges of numerics representation in ML models are prominently manifested. Each system presents a unique set of requirements and constraints, necessitating tailored strategies and solutions to navigate the challenges of memory usage, computational complexity, precision-accuracy trade-offs, and hardware compatibility.\n\n\n\n\n10.3.4 Quantization\nQuantization is prevalent in various scientific and technological domains, and it essentially involves the mapping or constraining of a continuous set or range into a discrete counterpart to minimize the number of bits required.\n\nInitial Breakdown\nWe begin our foray into quantization with a brief analysis of one important use for quantization.\nIn signal processing, the continuous sine wave (shown in Figure 10.17) can be quantized into discrete values through a process known as sampling. This is a fundamental concept in digital signal processing and is crucial for converting analog signals (like the continuous sine wave) into a digital form that can be processed by computers. The sine wave is a prevalent example due to its periodic and smooth nature, making it a useful tool for explaining concepts like frequency, amplitude, phase, and, of course, quantization.\n\n\n\n\n\n\nFigure 10.17: Sine Wave.\n\n\n\nIn the quantized version shown in Figure 10.18, the continuous sine wave (Figure 10.17) is sampled at regular intervals (in this case, every \\(\\frac{\\pi}{4}\\) radians), and only these sampled values are represented in the digital version of the signal. The step-wise lines between the points show one way to represent the quantized signal in a piecewise-constant form. This is a simplified example of how analog-to-digital conversion works, where a continuous signal is mapped to a discrete set of values, enabling it to be represented and processed digitally.\n\n\n\n\n\n\nFigure 10.18: Quantized Sine Wave.\n\n\n\nReturning to the context of Machine Learning (ML), quantization refers to the process of constraining the possible values that numerical parameters (such as weights and biases) can take to a discrete set, thereby reducing the precision of the parameters and consequently, the model’s memory footprint. When properly implemented, quantization can reduce model size by up to 4x and improve inference latency and throughput by up to 2-3x. Figure 10.19 illustrates the impact that quantization has on different models’ sizes: for example, an Image Classification model like ResNet-v2 can be compressed from 180MB down to 45MB with 8-bit quantization. There is typically less than 1% loss in model accuracy from well tuned quantization. Accuracy can often be recovered by re-training the quantized model with quantization-aware training techniques. Therefore, this technique has emerged to be very important in deploying ML models to resource-constrained environments, such as mobile devices, IoT devices, and edge computing platforms, where computational resources (memory and processing power) are limited.\n\n\n\n\n\n\nFigure 10.19: Effect of quantization on model sizes. Source: HarvardX.\n\n\n\nThere are several dimensions to quantization such as uniformity, stochasticity (or determinism), symmetry, granularity (across layers/channels/groups or even within channels), range calibration considerations (static vs dynamic), and fine-tuning methods (QAT, PTQ, ZSQ). We examine these below.\n\n\n\n10.3.5 Types\n\nUniform Quantization\nUniform quantization involves mapping continuous or high-precision values to a lower-precision representation using a uniform scale. This means that the interval between each possible quantized value is consistent. For example, if weights of a neural network layer are quantized to 8-bit integers (values between 0 and 255), a weight with a floating-point value of 0.56 might be mapped to an integer value of 143, assuming a linear mapping between the original and quantized scales. Due to its use of integer or fixed-point math pipelines, this form of quantization allows computation on the quantized domain without the need to dequantize beforehand.\nThe process for implementing uniform quantization starts with choosing a range of real numbers to be quantized. The next step is to select a quantization function and map the real values to the integers representable by the bit-width of the quantized representation. For instance, a popular choice for a quantization function is:\n\\[\nQ(r)=Int(r/S) - Z\n\\]\nwhere \\(Q\\) is the quantization operator, \\(r\\) is a real valued input (in our case, an activation or weight), \\(S\\) is a real valued scaling factor, and \\(Z\\) is an integer zero point. The Int function maps a real value to an integer value through a rounding operation. Through this function, we have effectively mapped real values \\(r\\) to some integer values, resulting in quantized levels which are uniformly spaced.\nWhen the need arises for practitioners to retrieve the original higher precision values, real values \\(r\\) can be recovered from quantized values through an operation known as dequantization. In the example above, this would mean performing the following operation on our quantized value:\n\\[\n\\bar{r} = S(Q(r) + Z)\n\\]\nAs discussed, some precision in the real value is lost by quantization. In this case, the recovered value \\(\\bar{r}\\) will not exactly match \\(r\\) due to the rounding operation. This is an important tradeoff to note; however, in many successful uses of quantization, the loss of precision can be negligible and the test accuracy remains high. Despite this, uniform quantization continues to be the current de-facto choice due to its simplicity and efficient mapping to hardware.\n\n\nNon-uniform Quantization\nNon-uniform quantization, on the other hand, does not maintain a consistent interval between quantized values. This approach might be used to allocate more possible discrete values in regions where the parameter values are more densely populated, thereby preserving more detail where it is most needed. For instance, in bell-shaped distributions of weights with long tails, a set of weights in a model predominantly lies within a certain range; thus, more quantization levels might be allocated to that range to preserve finer details, enabling us to better capture information. However, one major weakness of non-uniform quantization is that it requires dequantization before higher precision computations due to its non-uniformity, restricting its ability to accelerate computation compared to uniform quantization.\nTypically, a rule-based non-uniform quantization uses a logarithmic distribution of exponentially increasing steps and levels as opposed to linearly. Another popular branch lies in binary-code-based quantization where real number vectors are quantized into binary vectors with a scaling factor. Notably, there is no closed form solution for minimizing errors between the real value and non-uniformly quantized value, so most quantizations in this field rely on heuristic solutions. For instance, recent work by Xu et al. (2018) formulates non-uniform quantization as an optimization problem where the quantization steps/levels in quantizer \\(Q\\) are adjusted to minimize the difference between the original tensor and quantized counterpart.\n\nXu, Chen, Jianqiang Yao, Zhouchen Lin, Wenwu Ou, Yuanbin Cao, Zhirong Wang, and Hongbin Zha. 2018. “Alternating Multi-Bit Quantization for Recurrent Neural Networks.” In 6th International Conference on Learning Representations, ICLR 2018, Vancouver, BC, Canada, April 30 - May 3, 2018, Conference Track Proceedings. OpenReview.net. https://openreview.net/forum?id=S19dR9x0b.\n\\[\n\\min_Q ||Q(r)-r||^2\n\\]\nFurthermore, learnable quantizers can be jointly trained with model parameters, and the quantization steps/levels are generally trained with iterative optimization or gradient descent. Additionally, clustering has been used to alleviate information loss from quantization. While capable of capturing higher levels of detail, non-uniform quantization schemes can be difficult to deploy efficiently on general computation hardware, making it less-preferred to methods which use uniform quantization.\n\n\n\n\n\n\nFigure 10.20: Quantization uniformity. Source: Gholami et al. (2021).\n\n\n\n\n\nStochastic Quantization\nUnlike the two previous approaches which generate deterministic mappings, there is some work exploring the idea of stochastic quantization for quantization-aware training and reduced precision training. This approach maps floating numbers up or down with a probability associated to the magnitude of the weight update. The hope generated by high level intuition is that such a probabilistic approach may allow a neural network to explore more, as compared to deterministic quantization. Supposedly, enabling a stochastic rounding may allow neural networks to escape local optimums, thereby updating its parameters. Below are two example stochastic mapping functions:\n\n\n\n\n\n\n\nFigure 10.21: Integer vs Binary quantization functions.\n\n\n\n\n\nZero Shot Quantization\nZero-shot quantization refers to the process of converting a full-precision deep learning model directly into a low-precision, quantized model without the need for any retraining or fine-tuning on the quantized model. The primary advantage of this approach is its efficiency, as it eliminates the often time-consuming and resource-intensive process of retraining a model post-quantization. By leveraging techniques that anticipate and minimize quantization errors, zero-shot quantization maintains the model’s original accuracy even after reducing its numerical precision. It is particularly useful for Machine Learning as a Service (MLaaS) providers aiming to expedite the deployment of their customer’s workloads without having to access their datasets.\n\n\n\n10.3.6 Calibration\nCalibration is the process of selecting the most effective clipping range [\\(\\alpha\\), \\(\\beta\\)] for weights and activations to be quantized to. For example, consider quantizing activations that originally have a floating-point range between -6 and 6 to 8-bit integers. If you just take the minimum and maximum possible 8-bit integer values (-128 to 127) as your quantization range, it might not be the most effective. Instead, calibration would involve passing a representative dataset then use this observed range for quantization.\nThere are many calibration methods but a few commonly used include:\n\nMax: Use the maximum absolute value seen during calibration. However, this method is susceptible to outlier data. Notice how in Figure 10.22, we have an outlier cluster around 2.1, while the rest are clustered around smaller values.\nEntropy: Use KL divergence to minimize information loss between the original floating-point values and values that could be represented by the quantized format. This is the default method used by TensorRT.\nPercentile: Set the range to a percentile of the distribution of absolute values seen during calibration. For example, 99% calibration would clip 1% of the largest magnitude values.\n\n\n\n\n\n\n\nFigure 10.22: Input activations to layer 3 in ResNet50. Source: @Wu, Judd, and Isaev (2020).\n\n\n\nImportantly, the quality of calibration can make a difference between a quantized model that retains most of its accuracy and one that degrades significantly. Hence, it’s an essential step in the quantization process. When choosing a calibration range, there are two types: symmetric and asymmetric.\n\nSymmetric Quantization\nSymmetric quantization maps real values to a symmetrical clipping range centered around 0. This involves choosing a range [\\(\\alpha\\), \\(\\beta\\)] where \\(\\alpha = -\\beta\\). For example, one symmetrical range would be based on the min/max values of the real values such that:\n\\[\n\\alpha = \\beta = max(abs(r_{max}), abs(r_{min}))\n\\]\nSymmetric clipping ranges are the most widely adopted in practice as they have the advantage of easier implementation. In particular, the mapping of zero to zero in the clipping range (sometimes called “zeroing out of the zero point”) can lead to reduction in computational cost during inference (Wu, Judd, and Isaev 2020).\n\n\nAsymmetric Quantization\nAsymmetric quantization maps real values to an asymmetrical clipping range that isn’t necessarily centered around 0, as shown in Figure 10.23 on the right. It involves choosing a range [\\(\\alpha\\), \\(\\beta\\)] where \\(\\alpha \\neq -\\beta\\). For example, selecting a range based on the minimum and maximum real values, or where \\(\\alpha = r_{min}\\) and \\(\\beta = r_{max}\\), creates an asymmetric range. Typically, asymmetric quantization produces tighter clipping ranges compared to symmetric quantization, which is important when target weights and activations are imbalanced, e.g., the activation after the ReLU always has non-negative values. Despite producing tighter clipping ranges, asymmetric quantization is less preferred to symmetric quantization as it doesn’t always zero out the real value zero.\n\n\n\n\n\n\nFigure 10.23: Quantization (a)symmetry. Source: Gholami et al. (2021).\n\n\n\n\n\nGranularity\nUpon deciding the type of clipping range, it is essential to tighten the range to allow a model to retain as much of its accuracy as possible. We’ll be taking a look at convolutional neural networks as our way of exploring methods that fine tune the granularity of clipping ranges for quantization. The input activation of a layer in our CNN undergoes convolution with multiple convolutional filters. Every convolutional filter can possess a unique range of values. Notice how in Figure 10.24, the range for Filter 1 is much smaller than that for Filter 3. Consequently, one distinguishing feature of quantization approaches is the precision with which the clipping range [α,β] is determined for the weights.\n\n\n\n\n\n\nFigure 10.24: Quantization granularity: variable ranges. Source: Gholami et al. (2021).\n\n\n\n\nLayerwise Quantization: This approach determines the clipping range by considering all of the weights in the convolutional filters of a layer. Then, the same clipping range is used for all convolutional filters. It’s the simplest to implement, and, as such, it often results in sub-optimal accuracy due the wide variety of differing ranges between filters. For example, a convolutional kernel with a narrower range of parameters loses its quantization resolution due to another kernel in the same layer having a wider range.\nGroupwise Quantization: This approach groups different channels inside a layer to calculate the clipping range. This method can be helpful when the distribution of parameters across a single convolution/activation varies a lot. In practice, this method was useful in Q-BERT (Shen et al. 2020) for quantizing Transformer (Vaswani et al. 2017) models that consist of fully-connected attention layers. The downside with this approach comes with the extra cost of accounting for different scaling factors.\nChannelwise Quantization: This popular method uses a fixed range for each convolutional filter that is independent of other channels. Because each channel is assigned a dedicated scaling factor, this method ensures a higher quantization resolution and often results in higher accuracy.\nSub-channelwise Quantization: Taking channelwise quantization to the extreme, this method determines the clipping range with respect to any groups of parameters in a convolution or fully-connected layer. It may result in considerable overhead since different scaling factors need to be taken into account when processing a single convolution or fully-connected layer.\n\n\nShen, Sheng, Zhen Dong, Jiayu Ye, Linjian Ma, Zhewei Yao, Amir Gholami, Michael W. Mahoney, and Kurt Keutzer. 2020. “Q-BERT: Hessian Based Ultra Low Precision Quantization of BERT.” Proceedings of the AAAI Conference on Artificial Intelligence 34 (05): 8815–21. https://doi.org/10.1609/aaai.v34i05.6409.\n\nVaswani, Ashish, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. 2017. “Attention Is All You Need.” Adv Neural Inf Process Syst 30.\nOf these, channelwise quantization is the current standard used for quantizing convolutional kernels, since it enables the adjustment of clipping ranges for each individual kernel with negligible overhead.\n\n\nStatic and Dynamic Quantization\nAfter determining the type and granularity of the clipping range, practitioners must decide when ranges are determined in their range calibration algorithms. There are two approaches to quantizing activations: static quantization and dynamic quantization.\nStatic quantization is the most frequently used approach. In this, the clipping range is pre-calculated and static during inference. It does not add any computational overhead, but, consequently, results in lower accuracy as compared to dynamic quantization. A popular method of implementing this is to run a series of calibration inputs to compute the typical range of activations (Jacob et al. 2018; Yao et al. 2021).\n\nJacob, Benoit, Skirmantas Kligys, Bo Chen, Menglong Zhu, Matthew Tang, Andrew Howard, Hartwig Adam, and Dmitry Kalenichenko. 2018. “Quantization and Training of Neural Networks for Efficient Integer-Arithmetic-Only Inference.” In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2704–13.\n\nYao, Zhewei, Zhen Dong, Zhangcheng Zheng, Amir Gholami, Jiali Yu, Eric Tan, Leyuan Wang, et al. 2021. “Hawq-V3: Dyadic Neural Network Quantization.” In International Conference on Machine Learning, 11875–86. PMLR.\nDynamic quantization is an alternative approach which dynamically calculates the range for each activation map during runtime. The approach requires real-time computations which might have a very high overhead. By doing this, dynamic quantization often achieves the highest accuracy as the range is calculated specifically for each input.\nBetween the two, calculating the range dynamically usually is very costly, so most practitioners will often use static quantization instead.\n\n\n\n10.3.7 Techniques\nWhen optimizing machine learning models for deployment, various quantization techniques are used to balance model efficiency, accuracy, and adaptability. Each method—post-training quantization, quantization-aware training, and dynamic quantization–offers unique advantages and trade-offs, impacting factors such as implementation complexity, computational overhead, and performance optimization.\nTable 10.2 provides an overview of these quantization methods, highlighting their respective strengths, limitations, and trade-offs. We will delve deeper into each of these methods because they are widely deployed and used across all ML systems of wildly different scales.\n\n\n\nTable 10.2: Comparison of post-training quantization, quantization-aware training, and dynamic quantization.\n\n\n\n\n\n\n\n\n\n\n\nAspect\nPost Training Quantization\nQuantization-Aware Training\nDynamic Quantization\n\n\n\n\nPros\n\n\n\n\n\nSimplicity\n✓\n✗\n✗\n\n\nAccuracy Preservation\n✗\n✓\n✓\n\n\nAdaptability\n✗\n✗\n✓\n\n\nOptimized Performance\n✗\n✓\nPotentially\n\n\nCons\n\n\n\n\n\nAccuracy Degradation\n✓\n✗\nPotentially\n\n\nComputational Overhead\n✗\n✓\n✓\n\n\nImplementation Complexity\n✗\n✓\n✓\n\n\nTradeoffs\n\n\n\n\n\nSpeed vs. Accuracy\n✓\n✗\n✗\n\n\nAccuracy vs. Cost\n✗\n✓\n✗\n\n\nAdaptability vs. Overhead\n✗\n✗\n✓\n\n\n\n\n\n\nPost Training Quantization: Post-training quantization (PTQ) is a quantization technique where the model is quantized after it has been trained. The model is trained in floating point and then weights and activations are quantized as a post-processing step. This is the simplest approach and does not require access to the training data. Unlike Quantization-Aware Training (QAT), PTQ sets weight and activation quantization parameters directly, making it low-overhead and suitable for limited or unlabeled data situations. However, not readjusting the weights after quantizing, especially in low-precision quantization can lead to very different behavior and thus lower accuracy. To tackle this, techniques like bias correction, equalizing weight ranges, and adaptive rounding methods have been developed. PTQ can also be applied in zero-shot scenarios, where no training or testing data are available. This method has been made even more efficient to benefit compute- and memory- intensive large language models. Recently, SmoothQuant, a training-free, accuracy-preserving, and general-purpose PTQ solution which enables 8-bit weight, 8-bit activation quantization for LLMs, has been developed, demonstrating up to 1.56x speedup and 2x memory reduction for LLMs with negligible loss in accuracy (Xiao et al. 2022).\nIn PTQ, a pretrained model undergoes a calibration process, as shown in Figure 10.25. Calibration involves using a separate dataset known as calibration data, a specific subset of the training data reserved for quantization to help find the appropriate clipping ranges and scaling factors.\n\n\n\n\n\n\nFigure 10.25: Post-Training Quantization and calibration. Source: Gholami et al. (2021).\n\n\n\nQuantization-Aware Training: Quantization-aware training (QAT) is a fine-tuning of the PTQ model. The model is trained aware of quantization, allowing it to adjust for quantization effects. This produces better accuracy with quantized inference. Quantizing a trained neural network model with methods such as PTQ introduces perturbations that can deviate the model from its original convergence point. For instance, Krishnamoorthi showed that even with per-channel quantization, networks like MobileNet do not reach baseline accuracy with int8 PTQ and require QAT (Krishnamoorthi 2018).To address this, QAT retrains the model with quantized parameters, employing forward and backward passes in floating point but quantizing parameters after each gradient update. Handling the non-differentiable quantization operator is crucial; a widely used method is the Straight Through Estimator (STE), approximating the rounding operation as an identity function. While other methods and variations exist, STE remains the most commonly used due to its practical effectiveness. In QAT, a pretrained model is quantized and then finetuned using training data to adjust parameters and recover accuracy degradation, as shown in Figure 10.26. The calibration process is often conducted in parallel with the finetuning process for QAT.\n\n\n\n\n\n\nFigure 10.26: Quantization-Aware Training. Source: Gholami et al. (2021).\n\n\nGholami, Dong Kim, Mahoney Yao, and Keutzer. 2021. “A Survey of Quantization Methods for Efficient Neural Network Inference).” ArXiv Preprint. https://arxiv.org/abs/2103.13630.\n\n\nQuantization-Aware Training serves as a natural extension of Post-Training Quantization. Following the initial quantization performed by PTQ, QAT is used to further refine and fine-tune the quantized parameters - see how in Figure 10.27, the PTQ model undergoes an additional step, QAT. It involves a retraining process where the model is exposed to additional training iterations using the original data. This dynamic training approach allows the model to adapt and adjust its parameters, compensating for the performance degradation caused by quantization.\n\n\n\n\n\n\nFigure 10.27: PTQ and QAT. Source: “The Ultimate Guide to Deep Learning Model Quantization and Quantization-Aware Training” (n.d.).\n\n\n“The Ultimate Guide to Deep Learning Model Quantization and Quantization-Aware Training.” n.d. https://deci.ai/quantization-and-quantization-aware-training/.\n\n\nFigure 10.28 shows the relative accuracy of different models after PTQ and QAT. In almost all cases, QAT yields a better accuracy than PTQ. Consider for example EfficientNet b0. After PTQ, the accuracy drops from 76.85% to 72.06%. But when we apply QAT, the accuracy rebounds to 76.95% (with even a slight improvement over the original accuracy).\n\n\n\n\n\n\nFigure 10.28: Relative accuracies of PTQ and QAT. Source: Wu, Judd, and Isaev (2020).\n\n\n\n\n\n10.3.8 Weights vs. Activations\nWeight Quantization: Involves converting the continuous or high-precision weights of a model to lower-precision, such as converting Float32 weights to quantized INT8 (integer) weights - in Figure 10.29, weight quantization is taking place in the second step (red squares) when we multiply the inputs. This reduces the model size, thereby reducing the memory required to store the model and the computational resources needed to perform inference. For example, consider a weight matrix in a neural network layer with Float32 weights as [0.215, -1.432, 0.902, …]. Through weight quantization, these might be mapped to INT8 values like [27, -183, 115, …], significantly reducing the memory required to store them.\n\n\n\n\n\n\nFigure 10.29: Weight and activation quantization. Source: HarvardX.\n\n\n\nActivation Quantization: Involves quantizing the activation values (outputs of layers) during model inference. This can reduce the computational resources required during inference, but it introduces additional challenges in maintaining model accuracy due to the reduced precision of intermediate computations. For example, in a convolutional neural network (CNN), the activation maps (feature maps) produced by convolutional layers, originally in Float32, might be quantized to INT8 during inference to accelerate computation, especially on hardware optimized for integer arithmetic. Additionally, recent work has explored the use of Activation-aware Weight Quantization for LLM compression and acceleration, which involves protecting only 1% of the most important salient weights by observing the activations not weights (Lin et al. 2023).\n\n\n10.3.9 Trade-offs\nQuantization invariably introduces a trade-off between model size/performance and accuracy. While it significantly reduces the memory footprint and can accelerate inference, especially on hardware optimized for low-precision arithmetic, the reduced precision can degrade model accuracy.\nModel Size: A model with weights represented as Float32 being quantized to INT8 can theoretically reduce the model size by a factor of 4, enabling it to be deployed on devices with limited memory. The model size of large language models is developing at a faster pace than the GPU memory in recent years, leading to a big gap between the supply and demand for memory. Figure 10.30 illustrates the recent trend of the widening gap between model size (red line) and accelerator memory (yellow line). Quantization and model compression techniques can help bridge the gap\n\n\n\n\n\n\nFigure 10.30: Model size vs. accelerator memory. Source: Xiao et al. (2022).\n\n\nXiao, Seznec Lin, Demouth Wu, and Han. 2022. “SmoothQuant: Accurate and Efficient Post-Training Quantization for Large Language Models.” ArXiv Preprint. https://arxiv.org/abs/2211.10438.\n\n\nInference Speed: Quantization can also accelerate inference, as lower-precision arithmetic is computationally less expensive. For example, certain hardware accelerators, like Google’s Edge TPU, are optimized for INT8 arithmetic and can perform inference significantly faster with INT8 quantized models compared to their floating-point counterparts. The reduction in memory from quantization helps reduce the amount of data transmission, saving up memory and speeding the process. Figure 10.31 compares the increase in throughput and the reduction in bandwidth memory for different data type on the NVIDIA Turing GPU.\n\n\n\n\n\n\nFigure 10.31: Benefits of lower precision data types. Source: Wu, Judd, and Isaev (2020).\n\n\nWu, Zhang Judd, and Micikevicius Isaev. 2020. “Integer Quantization for Deep Learning Inference: Principles and Empirical Evaluation).” ArXiv Preprint. https://arxiv.org/abs/2004.09602.\n\n\nAccuracy: The reduction in numerical precision post-quantization can lead to a degradation in model accuracy, which might be acceptable in certain applications (e.g., image classification) but not in others (e.g., medical diagnosis). Therefore, post-quantization, the model typically requires re-calibration or fine-tuning to mitigate accuracy loss. Furthermore, recent work has explored the use of Activation-aware Weight Quantization (Lin et al. 2023) which is based on the observation that protecting only 1% of salient weights can greatly reduce quantization error.\n\n\n10.3.10 Quantization and Pruning\nPruning and quantization work well together, and it’s been found that pruning doesn’t hinder quantization. In fact, pruning can help reduce quantization error. Intuitively, this is due to pruning reducing the number of weights to quantize, thereby reducing the accumulated error from quantization. For example, an unpruned AlexNet has 60 million weights to quantize whereas a pruned AlexNet only has 6.7 million weights to quantize. This significant drop in weights helps reduce the error between quantizing the unpruned AlexNet vs. the pruned AlexNet. Furthermore, recent work has found that quantization-aware pruning generates more computationally efficient models than either pruning or quantization alone; It typically performs similar to or better in terms of computational efficiency compared to other neural architecture search techniques like Bayesian optimization (Hawks et al. 2021).\n\n\n\n\n\n\nFigure 10.32: Accuracy vs. compression rate under different compression methods. Source: Han, Mao, and Dally (2015).\n\n\nHan, Song, Huizi Mao, and William J Dally. 2015. “Deep Compression: Compressing Deep Neural Networks with Pruning, Trained Quantization and Huffman Coding.” arXiv Preprint arXiv:1510.00149.\n\n\n\n\n10.3.11 Edge-aware Quantization\nQuantization not only reduces model size but also enables faster computations and draws less power, making it vital to edge development. Edge devices typically have tight resource constraints with compute, memory, and power, which are impossible to meet for many of the deep NN models of today. Furthermore, edge processors do not support floating point operations, making integer quantization particularly important for chips like GAP-8, a RISC-V SoC for edge inference with a dedicated CNN accelerator, which only support integer arithmetic.\nOne hardware platform utilizing quantization is the ARM Cortex-M group of 32-bit RISC ARM processor cores. They leverage fixed-point quantization with power of two scaling factors so that quantization and dequantization can be efficiently done by bit shifting. Additionally, Google Edge TPUs, Google’s emerging solution for running inference at the edge, is designed for small, low-powered devices and can only support 8-bit arithmetic. Many complex neural network models that could only be deployed on servers due to their high computational needs can now be run on edge devices thanks to recent advancements (e.g. quantization methods) in edge computing field.\nIn addition to being an indispensable technique for many edge processors, quantization has also brought noteworthy improvements to non-edge processors such as encouraging such processors to meet the Service Level Agreement (SLA) requirements such as 99th percentile latency.\nThus, quantization combined with efficient low-precision logic and dedicated deep learning accelerators, has been one crucial driving force for the evolution of such edge processors.\nVideo 10.1 is a lecture on quantization and the different quantization methods.\n\n\n\n\n\n\nImportant 10.1: Quantization",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Model Optimizations</span>"
    ]
  },
  {
    "objectID": "contents/core/optimizations/optimizations.html#sec-model_ops_hw",
    "href": "contents/core/optimizations/optimizations.html#sec-model_ops_hw",
    "title": "10  Model Optimizations",
    "section": "10.4 Efficient Hardware Implementation",
    "text": "10.4 Efficient Hardware Implementation\nEfficient hardware implementation transcends the selection of suitable components; it requires a holistic understanding of how software will interact with underlying architectures. The essence of achieving peak performance in TinyML applications lies not only in refining algorithms to hardware but also in ensuring that the hardware is strategically tailored to support these algorithms. This synergy between hardware and software is crucial. As we look deeper into the intricacies of efficient hardware implementation, the significance of a co-design approach, where hardware and software are developed in tandem, becomes increasingly evident. This section provides an overview of the techniques of how hardware and the interactions between hardware and software can be optimized to improve models performance.\n\n10.4.1 Hardware-Aware Neural Architecture Search\nFocusing only on the accuracy when performing Neural Architecture Search leads to models that are exponentially complex and require increasing memory and compute. This has lead to hardware constraints limiting the exploitation of the deep learning models at their full potential. Manually designing the architecture of the model is even harder when considering the hardware variety and limitations. This has lead to the creation of Hardware-aware Neural Architecture Search that incorporate the hardware contractions into their search and optimize the search space for a specific hardware and accuracy. HW-NAS can be categorized based how it optimizes for hardware. We will briefly explore these categories and leave links to related papers for the interested reader.\n\nSingle Target, Fixed Platform Configuration\nThe goal here is to find the best architecture in terms of accuracy and hardware efficiency for one fixed target hardware. For a specific hardware, the Arduino Nicla Vision for example, this category of HW-NAS will look for the architecture that optimizes accuracy, latency, energy consumption, etc.\n\nHardware-aware Search Strategy\nHere, the search is a multi-objective optimization problem, where both the accuracy and hardware cost guide the searching algorithm to find the most efficient architecture (Tan et al. 2019; Cai, Zhu, and Han 2019; B. Wu et al. 2019).\n\nTan, Mingxing, Bo Chen, Ruoming Pang, Vijay Vasudevan, Mark Sandler, Andrew Howard, and Quoc V. Le. 2019. “MnasNet: Platform-aware Neural Architecture Search for Mobile.” In 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2820–28. IEEE. https://doi.org/10.1109/cvpr.2019.00293.\n\nCai, Han, Ligeng Zhu, and Song Han. 2019. “ProxylessNAS: Direct Neural Architecture Search on Target Task and Hardware.” In 7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019. OpenReview.net. https://openreview.net/forum?id=HylVB3AqYm.\n\nWu, Bichen, Kurt Keutzer, Xiaoliang Dai, Peizhao Zhang, Yanghan Wang, Fei Sun, Yiming Wu, Yuandong Tian, Peter Vajda, and Yangqing Jia. 2019. “FBNet: Hardware-aware Efficient ConvNet Design via Differentiable Neural Architecture Search.” In 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 10734–42. IEEE. https://doi.org/10.1109/cvpr.2019.01099.\n\n\nHardware-aware Search Space\nHere, the search space is restricted to the architectures that perform well on the specific hardware. This can be achieved by either measuring the operators (Conv operator, Pool operator, …) performance, or define a set of rules that limit the search space. (L. L. Zhang et al. 2020)\n\nZhang, Li Lyna, Yuqing Yang, Yuhang Jiang, Wenwu Zhu, and Yunxin Liu. 2020. “Fast Hardware-Aware Neural Architecture Search.” In 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW). IEEE. https://doi.org/10.1109/cvprw50498.2020.00354.\n\n\n\nSingle Target, Multiple Platform Configurations\nSome hardware may have different configurations. For example, FPGAs have Configurable Logic Blocks (CLBs) that can be configured by the firmware. This method allows for the HW-NAS to explore different configurations. (Hu et al. 2023; Ho Yoon et al. 2012)\n\nHo Yoon, Jung, Hyung-Suk Jung, Min Hwan Lee, Gun Hwan Kim, Seul Ji Song, Jun Yeong Seok, Kyung Jean Yoon, et al. 2012. “Frontiers in Electronic Materials.” Wiley. https://doi.org/10.1002/9783527667703.ch67.\n\n\nMultiple Targets\nThis category aims at optimizing a single model for multiple hardware. This can be helpful for mobile devices development as it can optimize to different phones models. (Chu et al. 2021; Hu et al. 2023)\n\nChu, Grace, Okan Arikan, Gabriel Bender, Weijun Wang, Achille Brighton, Pieter-Jan Kindermans, Hanxiao Liu, Berkin Akin, Suyog Gupta, and Andrew Howard. 2021. “Discovering Multi-Hardware Mobile Models via Architecture Search.” In 2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW), 3022–31. IEEE. https://doi.org/10.1109/cvprw53098.2021.00337.\n\nHu, Yang, Jie Jiang, Lifu Zhang, Yunfeng Shi, and Jian Shi. 2023. “Halide Perovskite Semiconductors.” Wiley. https://doi.org/10.1002/9783527829026.ch13.\n\n\nExamples of Hardware-Aware Neural Architecture Search\n\nTinyNAS\nTinyNAS adopts a two stage approach to finding an optimal architecture for model with the constraints of the specific microcontroller in mind.\nFirst, TinyNAS generate multiple search spaces by varying the input resolution of the model, and the number of channels of the layers of the model. Then, TinyNAS chooses a search space based on the FLOPs (Floating Point Operations Per Second) of each search space. Spaces with a higher probability of containing architectures with a large number of FLOPs yields models with higher accuracies - compare the red line vs. the black line in Figure 10.33. Since a higher number FLOPs means the model has a higher computational capacity, the model is more likely to have a higher accuracy.\nThen, TinyNAS performs a search operation on the chosen space to find the optimal architecture for the specific constraints of the microcontroller. (J. Lin et al. 2020)\n\n\n\n\n\n\nFigure 10.33: Search spaces accuracy. Source: J. Lin et al. (2020).\n\n\nLin, Ji, Wei-Ming Chen, Yujun Lin, John Cohn, Chuang Gan, and Song Han. 2020. “MCUNet: Tiny Deep Learning on IoT Devices.” In Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, Virtual, edited by Hugo Larochelle, Marc’Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin. https://proceedings.neurips.cc/paper/2020/hash/86c51678350f656dcc7f490a43946ee5-Abstract.html.\n\n\n\n\n\nTopology-Aware NAS\nFocuses on creating and optimizing a search space that aligns with the hardware topology of the device. (T. Zhang et al. 2020)\n\nZhang, Tunhou, Hsin-Pai Cheng, Zhenwen Li, Feng Yan, Chengyu Huang, Hai Helen Li, and Yiran Chen. 2020. “AutoShrink: A Topology-Aware NAS for Discovering Efficient Neural Architecture.” In The Thirty-Fourth AAAI Conference on Artificial Intelligence, AAAI 2020, the Thirty-Second Innovative Applications of Artificial Intelligence Conference, IAAI 2020, the Tenth AAAI Symposium on Educational Advances in Artificial Intelligence, EAAI 2020, New York, NY, USA, February 7-12, 2020, 6829–36. AAAI Press. https://aaai.org/ojs/index.php/AAAI/article/view/6163.\n\n\n\n10.4.2 Challenges of Hardware-Aware Neural Architecture Search\nWhile HW-NAS carries high potential for finding optimal architectures for TinyML, it comes with some challenges. Hardware Metrics like latency, energy consumption and hardware utilization are harder to evaluate than the metrics of accuracy or loss. They often require specialized tools for precise measurements. Moreover, adding all these metrics leads to a much bigger search space. This leads to HW-NAS being time-consuming and expensive. It has to be applied to every hardware for optimal results, moreover, meaning that if one needs to deploy the model on multiple devices, the search has to be conducted multiple times and will result in different models, unless optimizing for all of them which means less accuracy. Finally, hardware changes frequently, and HW-NAS may need to be conducted on each version.\n\n\n10.4.3 Kernel Optimizations\nKernel Optimizations are modifications made to the kernel to improve the performance of machine learning models on resource-constrained devices. We will separate kernel optimizations into two types.\n\nGeneral Kernel Optimizations\nThese are kernel optimizations that all devices can benefit from. They provide technics to convert the code to more efficient instructions.\n\nLoop unrolling\nInstead of having a loop with loop control (incrementing the loop counter, checking the loop termination condition) the loop can be unrolled and the overhead of loop control can be omitted. This may also provide additional opportunities for parallelism that may not be possible with the loop structure. This can be particularly beneficial for tight loops, where the body of the loop is a small number of instructions with a lot of iterations.\n\n\nBlocking\nBlocking is used to make memory access patterns more efficient. If we have three computations the first and the last need to access cache A and the second needs to access cache B, blocking blocks the first two computations together to reduce the number of memory reads needed.\n\n\nTiling\nSimilarly to blocking, tiling divides data and computation into chunks, but extends beyond cache improvements. Tiling creates independent partitions of computation that can be run in parallel, which can result in significant performance improvements.\n\n\nOptimized Kernel Libraries\nThis comprises developing optimized kernels that take full advantage of a specific hardware. One example is the CMSIS-NN library, which is a collection of efficient neural network kernels developed to optimize the performance and minimize the memory footprint of models on Arm Cortex-M processors, which are common on IoT edge devices. The kernel leverage multiple hardware capabilities of Cortex-M processors like Single Instruction Multiple Data (SIMD), Floating Point Units (FPUs) and M-Profile Vector Extensions (MVE). These optimization make common operations like matrix multiplications more efficient, boosting the performance of model operations on Cortex-M processors. (Lai, Suda, and Chandra 2018)\n\nLai, Liangzhen, Naveen Suda, and Vikas Chandra. 2018. “CMSIS-NN: Efficient Neural Network Kernels for Arm Cortex-m CPUs.” https://arxiv.org/abs/1801.06601.\n\n\n\n\n10.4.4 Compute-in-Memory (CiM)\nThis is one example of Algorithm-Hardware Co-design. CiM is a computing paradigm that performs computation within memory. Therefore, CiM architectures allow for operations to be performed directly on the stored data, without the need to shuttle data back and forth between separate processing and memory units. This design paradigm is particularly beneficial in scenarios where data movement is a primary source of energy consumption and latency, such as in TinyML applications on edge devices. Figure 10.34 is one example of using CiM in TinyML: keyword spotting requires an always-on process that looks for certain wake words (such as ‘Hey, Siri’). Given the resource-intensive nature of this task, integrating CiM for the always-on keyword detection model can improve efficiency.\nThrough algorithm-hardware co-design, the algorithms can be optimized to leverage the unique characteristics of CiM architectures, and conversely, the CiM hardware can be customized or configured to better support the computational requirements and characteristics of the algorithms. This is achieved by using the analog properties of memory cells, such as addition and multiplication in DRAM. (Zhou et al. 2021)\n\n\n\n\n\n\nFigure 10.34: CiM for keyword spotting. Source: Zhou et al. (2021).\n\n\nZhou, Chuteng, Fernando Garcia Redondo, Julian Büchel, Irem Boybat, Xavier Timoneda Comas, S. R. Nandakumar, Shidhartha Das, Abu Sebastian, Manuel Le Gallo, and Paul N. Whatmough. 2021. “AnalogNets: Ml-hw Co-Design of Noise-Robust TinyML Models and Always-on Analog Compute-in-Memory Accelerator.” https://arxiv.org/abs/2111.06503.\n\n\n\n\n10.4.5 Memory Access Optimization\nDifferent devices may have different memory hierarchies. Optimizing for the specific memory hierarchy in the specific hardware can lead to great performance improvements by reducing the costly operations of reading and writing to memory. Dataflow optimization can be achieved by optimizing for reusing data within a single layer and across multiple layers. This dataflow optimization can be tailored to the specific memory hierarchy of the hardware, which can lead to greater benefits than general optimizations for different hardware.\n\nLeveraging Sparsity\nPruning is a fundamental approach to compress models to make them compatible with resource constrained devices. This results in sparse models where a lot of weights are 0’s. Therefore, leveraging this sparsity can lead to significant improvements in performance. Tools were created to achieve exactly this. RAMAN, is a sparse TinyML accelerator designed for inference on edge devices. RAMAN overlap input and output activations on the same memory space, reducing storage requirements by up to 50%. (Krishna et al. 2023)\n\nKrishna, Adithya, Srikanth Rohit Nudurupati, Chandana D G, Pritesh Dwivedi, André van Schaik, Mahesh Mehendale, and Chetan Singh Thakur. 2023. “RAMAN: A Re-Configurable and Sparse TinyML Accelerator for Inference on Edge.” https://arxiv.org/abs/2306.06493.\n\n\nOptimization Frameworks\nOptimization Frameworks have been introduced to exploit the specific capabilities of the hardware to accelerate the software. One example of such a framework is hls4ml - Figure 10.35 provides an overview of the framework’s workflow. This open-source software-hardware co-design workflow aids in interpreting and translating machine learning algorithms for implementation with both FPGA and ASIC technologies. Features such as network optimization, new Python APIs, quantization-aware pruning, and end-to-end FPGA workflows are embedded into the hls4ml framework, leveraging parallel processing units, memory hierarchies, and specialized instruction sets to optimize models for edge hardware. Moreover, hls4ml is capable of translating machine learning algorithms directly into FPGA firmware.\n\n\n\n\n\n\nFigure 10.35: hls4ml framework workflow. Source: Fahim et al. (2021).\n\n\nFahim, Farah, Benjamin Hawks, Christian Herwig, James Hirschauer, Sergo Jindariani, Nhan Tran, Luca P. Carloni, et al. 2021. “Hls4ml: An Open-Source Codesign Workflow to Empower Scientific Low-Power Machine Learning Devices.” https://arxiv.org/abs/2103.05579.\n\n\nOne other framework for FPGAs that focuses on a holistic approach is CFU Playground (Prakash et al. 2023)\n\nPrakash, Shvetank, Tim Callahan, Joseph Bushagour, Colby Banbury, Alan V. Green, Pete Warden, Tim Ansell, and Vijay Janapa Reddi. 2023. “CFU Playground: Full-stack Open-Source Framework for Tiny Machine Learning (TinyML) Acceleration on FPGAs.” In 2023 IEEE International Symposium on Performance Analysis of Systems and Software (ISPASS). Vol. abs/2201.01863. IEEE. https://doi.org/10.1109/ispass57527.2023.00024.\n\n\nHardware Built Around Software\nIn a contrasting approach, hardware can be custom-designed around software requirements to optimize the performance for a specific application. This paradigm creates specialized hardware to better adapt to the specifics of the software, thus reducing computational overhead and improving operational efficiency. One example of this approach is a voice-recognition application by (Kwon and Park 2021). The paper proposes a structure wherein preprocessing operations, traditionally handled by software, are allocated to custom-designed hardware. This technique was achieved by introducing resistor-transistor logic to an inter-integrated circuit sound module for windowing and audio raw data acquisition in the voice-recognition application. Consequently, this offloading of preprocessing operations led to a reduction in computational load on the software, showcasing a practical application of building hardware around software to improve the efficiency and performance.\n\n\n\n\n\n\nFigure 10.36: Delegating data processing to an FPGA. Source: Kwon and Park (2021).\n\n\nKwon, Jisu, and Daejin Park. 2021. “Hardware/Software Co-Design for TinyML Voice-Recognition Application on Resource Frugal Edge Devices.” Applied Sciences 11 (22): 11073. https://doi.org/10.3390/app112211073.\n\n\n\n\nSplitNets\nSplitNets were introduced in the context of Head-Mounted systems. They distribute the Deep Neural Networks (DNNs) workload among camera sensors and an aggregator. This is particularly compelling the in context of TinyML. The SplitNet framework is a split-aware NAS to find the optimal neural network architecture to achieve good accuracy, split the model among the sensors and the aggregator, and minimize the communication between the sensors and the aggregator.\nFigure 10.37 demonstrates how SplitNets (in red) achieves higher accuracy for lower latency (running on ImageNet) than different approaches, such as running the DNN on-sensor (All-on-sensor; in green) or on mobile (All-on-aggregator; in blue). Minimal communication is important in TinyML where memory is highly constrained, this way the sensors conduct some of the processing on their chips and then they send only the necessary information to the aggregator. When testing on ImageNet, SplitNets were able to reduce the latency by one order of magnitude on head-mounted devices. This can be helpful when the sensor has its own chip. (Dong et al. 2022)\n\n\n\n\n\n\nFigure 10.37: SplitNets vs other approaches. Source: Dong et al. (2022).\n\n\nDong, Xin, Barbara De Salvo, Meng Li, Chiao Liu, Zhongnan Qu, H. T. Kung, and Ziyun Li. 2022. “SplitNets: Designing Neural Architectures for Efficient Distributed Computing on Head-Mounted Systems.” In 2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 12549–59. IEEE. https://doi.org/10.1109/cvpr52688.2022.01223.\n\n\n\n\nHardware Specific Data Augmentation\nEach edge device may possess unique sensor characteristics, leading to specific noise patterns that can impact model performance. One example is audio data, where variations stemming from the choice of microphone are prevalent. Applications such as Keyword Spotting can experience substantial enhancements by incorporating data recorded from devices similar to those intended for deployment. Fine-tuning of existing models can be employed to adapt the data precisely to the sensor’s distinctive characteristics.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Model Optimizations</span>"
    ]
  },
  {
    "objectID": "contents/core/optimizations/optimizations.html#software-and-framework-support",
    "href": "contents/core/optimizations/optimizations.html#software-and-framework-support",
    "title": "10  Model Optimizations",
    "section": "10.5 Software and Framework Support",
    "text": "10.5 Software and Framework Support\nWhile all of the aforementioned techniques like pruning, quantization, and efficient numerics are well-known, they would remain impractical and inaccessible without extensive software support. For example, directly quantizing weights and activations in a model would require manually modifying the model definition and inserting quantization operations throughout. Similarly, directly pruning model weights requires manipulating weight tensors. Such tedious approaches become infeasible at scale.\nWithout the extensive software innovation across frameworks, optimization tools and hardware integration, most of these techniques would remain theoretical or only viable to experts. Without framework APIs and automation to simplify applying these optimizations, they would not see adoption. Software support makes them accessible to general practitioners and unlocks real-world benefits. In addition, issues such as hyperparameter tuning for pruning, managing the trade-off between model size and accuracy, and ensuring compatibility with target devices pose hurdles that developers must navigate.\n\n10.5.1 Built-in Optimization APIs\nMajor machine learning frameworks like TensorFlow, PyTorch, and MXNet provide libraries and APIs to allow common model optimization techniques to be applied without requiring custom implementations. For example, TensorFlow offers the TensorFlow Model Optimization Toolkit which contains modules like:\n\nQuantization: Applies quantization-aware training to convert floating point models to lower precision like int8 with minimal accuracy loss. Handles weight and activation quantization.\nSparsity: Provides pruning APIs to induce sparsity and remove unnecessary connections in models like neural networks. Can prune weights, layers, etc.\nClustering: Supports model compression by clustering weights into groups for higher compression rates.\n\nThese APIs allow users to enable optimization techniques like quantization and pruning without directly modifying model code. Parameters like target sparsity rates, quantization bit-widths etc. can be configured. Similarly, PyTorch provides torch.quantization for converting models to lower precision representations. TorchTensor and TorchModule form the base classes for quantization support. It also offers torch.nn.utils.prune for built-in pruning of models. MXNet offers gluon.contrib layers that add quantization capabilities like fixed point rounding and stochastic rounding of weights/activations during training. This allows quantization to be readily included in gluon models.\nThe core benefit of built-in optimizations is that users can apply them without re-implementing complex techniques. This makes optimized models accessible to a broad range of practitioners. It also ensures best practices are followed by building on research and experience implementing the methods. As new optimizations emerge, frameworks strive to provide native support and APIs where possible to further lower the barrier to efficient ML. The availability of these tools is key to widespread adoption.\n\n\n10.5.2 Automated Optimization Tools\nAutomated optimization tools provided by frameworks can analyze models and automatically apply optimizations like quantization, pruning, and operator fusion to make the process easier and accessible without excessive manual tuning. In effect, this builds on top of the previous section. For example, TensorFlow provides the TensorFlow Model Optimization Toolkit which contains modules like:\n\nQuantizationAwareTraining: Automatically quantizes weights and activations in a model to lower precision like UINT8 or INT8 with minimal accuracy loss. It inserts fake quantization nodes during training so that the model can learn to be quantization-friendly.\nPruning: Automatically removes unnecessary connections in a model based on analysis of weight importance. Can prune entire filters in convolutional layers or attention heads in transformers. Handles iterative re-training to recover any accuracy loss.\nGraphOptimizer: Applies graph optimizations like operator fusion to consolidate operations and reduce execution latency, especially for inference. In Figure 10.38, you can see the original (Source Graph) on the left, and how its operations are transformed (consolidated) on the right. Notice how Block1 in Source Graph has 3 separate steps (Convolution, BiasAdd, and Activation), which are then consolidated together in Block1 on Optimized Graph.\n\n\n\n\n\n\n\nFigure 10.38: GraphOptimizer. Source: Wess et al. (2020).\n\n\nWess, Matthias, Matvey Ivanov, Christoph Unger, and Anvesh Nookala. 2020. “ANNETTE: Accurate Neural Network Execution Time Estimation with Stacked Models.” IEEE. https://doi.org/10.1109/ACCESS.2020.3047259.\n\n\nThese automated modules only require the user to provide the original floating point model, and handle the end-to-end optimization pipeline including any re-training to regain accuracy. Other frameworks like PyTorch also offer increasing automation support, for example through torch.quantization.quantize_dynamic. Automated optimization makes efficient ML accessible to practitioners without optimization expertise.\n\n\n10.5.3 Hardware Optimization Libraries\nHardware libraries like TensorRT and TensorFlow XLA allow models to be highly optimized for target hardware through techniques that we discussed earlier.\n\nQuantization: For example, TensorRT and TensorFlow Lite both support quantization of models during conversion to their format. This provides speedups on mobile SoCs with INT8/INT4 support.\nKernel Optimization: For instance, TensorRT does auto-tuning to optimize CUDA kernels based on the GPU architecture for each layer in the model graph. This extracts maximum throughput.\nOperator Fusion: TensorFlow XLA does aggressive fusion to create optimized binary for TPUs. On mobile, frameworks like NCNN also support fused operators.\nHardware-Specific Code: Libraries are used to generate optimized binary code specialized for the target hardware. For example, TensorRT uses Nvidia CUDA/cuDNN libraries which are hand-tuned for each GPU architecture. This hardware-specific coding is key for performance. On TinyML devices, this can mean assembly code optimized for a Cortex M4 CPU for example. Vendors provide CMSIS-NN and other libraries.\nData Layout Optimizations: We can efficiently leverage memory hierarchy of hardware like cache and registers through techniques like tensor/weight rearrangement, tiling, and reuse. For example, TensorFlow XLA optimizes buffer layouts to maximize TPU utilization. This helps any memory constrained systems.\nProfiling-based Tuning: We can use profiling tools to identify bottlenecks. For example, adjust kernel fusion levels based on latency profiling. On mobile SoCs, vendors like Qualcomm provide profilers in SNPE to find optimization opportunities in CNNs. This data-driven approach is important for performance.\n\nBy integrating framework models with these hardware libraries through conversion and execution pipelines, ML developers can achieve significant speedups and efficiency gains from low-level optimizations tailored to the target hardware. The tight integration between software and hardware is key to enabling performant deployment of ML applications, especially on mobile and TinyML devices.\n\n\n10.5.4 Visualizing Optimizations\nImplementing model optimization techniques without visibility into the effects on the model can be challenging. Dedicated tooling or visualization tools can provide critical and useful insight into model changes and helps track the optimization process. Let’s consider the optimizations we considered earlier, such as pruning for sparsity and quantization.\n\nSparsity\nFor example, consider sparsity optimizations. Sparsity visualization tools can provide critical insights into pruned models by mapping out exactly which weights have been removed. For example, sparsity heat maps can use color gradients to indicate the percentage of weights pruned in each layer of a neural network. Layers with higher percentages pruned appear darker (see Figure 10.39). This identifies which layers have been simplified the most by pruning (Souza 2020).\n\n\n\n\n\n\nFigure 10.39: Sparse network heat map. Source: Numenta.\n\n\n\nTrend plots can also track sparsity over successive pruning rounds - they may show initial rapid pruning followed by more gradual incremental increases. Tracking the current global sparsity along with statistics like average, minimum, and maximum sparsity per-layer in tables or plots provides an overview of the model composition. For a sample convolutional network, these tools could reveal that the first convolution layer is pruned 20% while the final classifier layer is pruned 70% given its redundancy. The global model sparsity may increase from 10% after initial pruning to 40% after five rounds.\nBy making sparsity data visually accessible, practitioners can better understand exactly how their model is being optimized and which areas are being impacted. The visibility enables them to fine-tune and control the pruning process for a given architecture.\nSparsity visualization turns pruning into a transparent technique instead of a black-box operation.\n\n\nQuantization\nConverting models to lower numeric precisions through quantization introduces errors that can impact model accuracy if not properly tracked and addressed. Visualizing quantization error distributions provides valuable insights into the effects of reduced precision numerics applied to different parts of a model. For this, histograms of the quantization errors for weights and activations can be generated. These histograms can reveal the shape of the error distribution - whether they resemble a Gaussian distribution or contain significant outliers and spikes. Figure 10.40 shows the distributions of different quantization methods. Large outliers may indicate issues with particular layers handling the quantization. Comparing the histograms across layers highlights any problem areas standing out with abnormally high errors.\n\n\n\n\n\n\nFigure 10.40: Quantization errors. Source: Kuzmin et al. (2022).\n\n\nKuzmin, Andrey, Mart Van Baalen, Yuwei Ren, Markus Nagel, Jorn Peters, and Tijmen Blankevoort. 2022. “FP8 Quantization: The Power of the Exponent.” https://arxiv.org/abs/2208.09225.\n\n\nActivation visualizations are also important to detect overflow issues. By color mapping the activations before and after quantization, any values pushed outside the intended ranges become visible. This reveals saturation and truncation issues that could skew the information flowing through the model. Detecting these errors allows recalibrating activations to prevent loss of information (Mandal 2022). Figure 10.41 is a color mapping of the AlexNet convolutional kernels.\n\n\n\n\n\n\nFigure 10.41: Color mapping of activations. Source: Krizhevsky, Sutskever, and Hinton (2017).\n\n\nKrizhevsky, Alex, Ilya Sutskever, and Geoffrey E. Hinton. 2017. “ImageNet Classification with Deep Convolutional Neural Networks.” Edited by F. Pereira, C. J. Burges, L. Bottou, and K. Q. Weinberger. Commun. ACM 60 (6): 84–90. https://doi.org/10.1145/3065386.\n\n\nOther techniques, such as tracking the overall mean square quantization error at each step of the quantization-aware training process identifies fluctuations and divergences. Sudden spikes in the tracking plot may indicate points where quantization is disrupting the model training. Monitoring this metric builds intuition on model behavior under quantization. Together these techniques turn quantization into a transparent process. The empirical insights enable practitioners to properly assess quantization effects. They pinpoint areas of the model architecture or training process to recalibrate based on observed quantization issues. This helps achieve numerically stable and accurate quantized models.\nProviding this data enables practitioners to properly assess the impact of quantization and identify potential problem areas of the model to recalibrate or redesign to be more quantization friendly. This empirical analysis builds intuition on achieving optimal quantization.\nVisualization tools can provide insights that help practitioners better understand the effects of optimizations on their models. The visibility enables correcting issues early before accuracy or performance is impacted significantly. It also aids applying optimizations more effectively for specific models. These optimization analytics help build intuition when transitioning models to more efficient representations.\n\n\n\n10.5.5 Model Conversion and Deployment\nOnce models have been successfully optimized in frameworks like TensorFlow and PyTorch, specialized model conversion and deployment platforms are needed to bridge the gap to running them on target devices.\nTensorFlow Lite - TensorFlow’s platform to convert models to a lightweight format optimized for mobile, embedded and edge devices. Supports optimizations like quantization, kernel fusion, and stripping away unused ops. Models can be executed using optimized TensorFlow Lite kernels on device hardware. Critical for mobile and TinyML deployment.\nONNX Runtime - Performs model conversion and inference for models in the open ONNX model format. Provides optimized kernels, supports hardware accelerators like GPUs, and cross-platform deployment from cloud to edge. Allows framework-agnostic deployment. Figure 10.42 is an ONNX interoperability map, including major popular frameworks.\n\n\n\n\n\n\nFigure 10.42: Interoperability of ONNX. Source: TowardsDataScience.\n\n\n\nPyTorch Mobile - Enables PyTorch models to be run on iOS and Android by converting to mobile-optimized representations. Provides efficient mobile implementations of ops like convolution and special functions optimized for mobile hardware.\nThese platforms integrate with hardware drivers, operating systems, and accelerator libraries on devices to execute models efficiently using hardware optimization. They also offload operations to dedicated ML accelerators where present. The availability of these proven, robust deployment platforms bridges the gap between optimizing models in frameworks and actual deployment to billions of devices. They allow users to focus on model development rather than building custom mobile runtimes. Continued innovation to support new hardware and optimizations in these platforms is key to widespread ML optimizations.\nBy providing these optimized deployment pipelines, the entire workflow from training to device deployment can leverage model optimizations to deliver performant ML applications. This end-to-end software infrastructure has helped drive the adoption of on-device ML.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Model Optimizations</span>"
    ]
  },
  {
    "objectID": "contents/core/optimizations/optimizations.html#conclusion",
    "href": "contents/core/optimizations/optimizations.html#conclusion",
    "title": "10  Model Optimizations",
    "section": "10.6 Conclusion",
    "text": "10.6 Conclusion\nIn this chapter we’ve discussed model optimization across the software-hardware span. We dove deep into efficient model representation, where we covered the nuances of structured and unstructured pruning and other techniques for model compression such as knowledge distillation and matrix and tensor decomposition. We also dove briefly into edge-specific model design at the parameter and model architecture level, exploring topics like edge-specific models and hardware-aware NAS.\nWe then explored efficient numerics representations, where we covered the basics of numerics, numeric encodings and storage, benefits of efficient numerics, and the nuances of numeric representation with memory usage, computational complexity, hardware compatibility, and tradeoff scenarios. We finished by honing in on an efficient numerics staple: quantization, where we examined its history, calibration, techniques, and interaction with pruning.\nFinally, we looked at how we can make optimizations specific to the hardware we have. We explored how we can find model architectures tailored to the hardware, make optimizations in the kernel to better handle the model, and frameworks built to make the most use out of the hardware. We also looked at how we can go the other way around and build hardware around our specific software and talked about splitting networks to run on multiple processors available on the edge device.\nBy understanding the full picture of the degrees of freedom within model optimization both away and close to the hardware and the tradeoffs to consider when implementing these methods, practitioners can develop a more thoughtful pipeline for compressing their workloads onto edge devices.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Model Optimizations</span>"
    ]
  },
  {
    "objectID": "contents/core/optimizations/optimizations.html#sec-model-optimizations-resource",
    "href": "contents/core/optimizations/optimizations.html#sec-model-optimizations-resource",
    "title": "10  Model Optimizations",
    "section": "10.7 Resources",
    "text": "10.7 Resources\nHere is a curated list of resources to support both students and instructors in their learning and teaching journey. We are continuously working on expanding this collection and will be adding new exercises in the near future.\n\n\n\n\n\n\nSlides\n\n\n\n\n\nThese slides serve as a valuable tool for instructors to deliver lectures and for students to review the material at their own pace. We encourage both students and instructors to leverage these slides to improve their understanding and facilitate effective knowledge transfer.\n\nQuantization:\n\nQuantization: Part 1.\nQuantization: Part 2.\nPost-Training Quantization (PTQ).\nQuantization-Aware Training (QAT).\n\nPruning:\n\nPruning: Part 1.\nPruning: Part 2.\n\nKnowledge Distillation.\nClustering.\nNeural Architecture Search (NAS):\n\nNAS overview.\nNAS: Part 1.\nNAS: Part 2.\n\n\n\n\n\n\n\n\n\n\n\nVideos\n\n\n\n\n\n\nVideo 10.1\n\n\n\n\n\n\n\n\n\n\nExercises\n\n\n\n\n\nTo reinforce the concepts covered in this chapter, we have curated a set of exercises that challenge students to apply their knowledge and deepen their understanding.\n\nExercise 10.1\nExercise 10.2\nExercise 10.3",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Model Optimizations</span>"
    ]
  },
  {
    "objectID": "contents/core/hw_acceleration/hw_acceleration.html",
    "href": "contents/core/hw_acceleration/hw_acceleration.html",
    "title": "11  AI Acceleration",
    "section": "",
    "text": "Purpose\nResources: Slides, Videos, Exercises\nHow does specialized computing transform the performance frontier of machine learning systems, and what principles guide the design of acceleration strategies?\nThe evolution of computational acceleration in machine learning systems represents a fundamental shift in how processing resources are architected and utilized. Each acceleration approach introduces unique patterns for matching algorithmic structure with computational capabilities, revealing essential relationships between model design and execution efficiency. The integration of specialized computing elements demonstrates trade-offs between performance, power efficiency, and system complexity. These architectural interactions provide insights into system-level optimization strategies, establishing core principles for designing AI solutions that effectively leverage advanced computation across diverse deployment environments.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>AI Acceleration</span>"
    ]
  },
  {
    "objectID": "contents/core/hw_acceleration/hw_acceleration.html#purpose",
    "href": "contents/core/hw_acceleration/hw_acceleration.html#purpose",
    "title": "11  AI Acceleration",
    "section": "",
    "text": "Learning Objectives\n\n\n\n\nUnderstand why hardware acceleration is needed for AI workloads\nSurvey key accelerator options like GPUs, TPUs, FPGAs, and ASICs and their tradeoffs\nLearn about programming models, frameworks, and compilers for AI accelerators\nAppreciate the importance of benchmarking and metrics for hardware evaluation\nRecognize the role of hardware-software co-design in building efficient systems\nGain exposure to cutting-edge research directions like neuromorphic and quantum computing\nUnderstand how ML is beginning to augment and improve hardware design",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>AI Acceleration</span>"
    ]
  },
  {
    "objectID": "contents/core/hw_acceleration/hw_acceleration.html#overview",
    "href": "contents/core/hw_acceleration/hw_acceleration.html#overview",
    "title": "11  AI Acceleration",
    "section": "11.1 Overview",
    "text": "11.1 Overview\nYou’ve probably noticed the growing demand for embedding machine learning into everyday devices—like the smartphones in our pockets, smart home appliances, and even autonomous vehicles. Bringing ML capabilities into these real-world environments is exciting, but it comes with its own set of challenges. Unlike powerful data center servers, these edge devices have limited computing resources, making it tricky to run complex models effectively.\nSpecialized hardware acceleration is the key to making high-performance machine learning possible on resource-limited edge devices. When we talk about hardware acceleration, we’re referring to the use of custom chips and architectures designed to handle the heavy lifting of ML operations, taking the burden off the main processor. In neural networks, some of the most demanding tasks involve matrix multiplications during inference. Hardware accelerators are built to optimize these operations, often delivering 10-100x speedups compared to general-purpose CPUs. This kind of acceleration is what makes it feasible to run advanced neural network models on devices that are constrained by size, weight, and power— and to do it all in real-time.\nIn this chapter, we’ll take a closer look at the different hardware acceleration techniques available for embedded machine learning and the tradeoffs that come with each option. The goal is to give you a solid understanding of how these techniques work, so you can make informed decisions when it comes to choosing the right hardware and optimizing your software. By the end, you’ll be well-equipped to develop high-performance machine learning capabilities on edge devices, even with their constraints.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>AI Acceleration</span>"
    ]
  },
  {
    "objectID": "contents/core/hw_acceleration/hw_acceleration.html#background-and-basics",
    "href": "contents/core/hw_acceleration/hw_acceleration.html#background-and-basics",
    "title": "11  AI Acceleration",
    "section": "11.2 Background and Basics",
    "text": "11.2 Background and Basics\n\n11.2.1 Historical Background\nThe origins of hardware acceleration date back to the 1960s, with the advent of floating point math co-processors to offload calculations from the main CPU. One early example was the Intel 8087 chip released in 1980 to accelerate floating point operations for the 8086 processor. This established the practice of using specialized processors to handle math-intensive workloads efficiently.\nIn the 1990s, the first graphics processing units (GPUs) emerged to process graphics pipelines for rendering and gaming rapidly. Nvidia’s GeForce 256 in 1999 was one of the earliest programmable GPUs capable of running custom software algorithms. GPUs exemplify domain-specific fixed-function accelerators and evolve into parallel programmable accelerators.\nIn the 2000s, GPUs were applied to general-purpose computing under GPGPU. Their high memory bandwidth and computational throughput made them well-suited for math-intensive workloads. This included breakthroughs in using GPUs to accelerate training of deep learning models such as AlexNet in 2012.\nIn recent years, Google’s Tensor Processing Units (TPUs) represent customized ASICs specifically architected for matrix multiplication in deep learning. During inference, their optimized tensor cores achieve higher TeraOPS/watt than CPUs or GPUs. Ongoing innovation includes model compression techniques like pruning and quantization to fit larger neural networks on edge devices.\nThis evolution demonstrates how hardware acceleration has focused on solving compute-intensive bottlenecks, from floating point math to graphics to matrix multiplication for ML. Understanding this history provides a crucial context for specialized AI accelerators today.\n\n\n11.2.2 The Need for Acceleration\nThe evolution of hardware acceleration is closely tied to the broader history of computing. Central to this history is the role of transistors, the fundamental building blocks of modern electronics. Transistors act as tiny switches that can turn on or off, enabling the complex computations that drive everything from simple calculators to advanced machine learning models. In the early decades, chip design was governed by Moore’s Law, which predicted that the number of transistors on an integrated circuit would double approximately every two years, and Dennard Scaling, which observed that as transistors became smaller, their performance (speed) increased, while power density (power per unit area) remained constant. These two laws were held through the single-core era. Figure 11.1 shows the trends of different microprocessor metrics. As the figure denotes, Dennard Scaling fails around the mid-2000s; notice how the clock speed (frequency) remains almost constant even as the number of transistors keeps increasing.\n\n\n\n\n\n\nFigure 11.1: Microprocessor trends. Source: Karl Rupp.\n\n\n\nHowever, as Patterson and Hennessy (2016) describes, technological constraints eventually forced a transition to the multicore era, with chips containing multiple processing cores to deliver performance gains. Power limitations prevented further scaling, which led to “dark silicon” (Dark Silicon), where not all chip areas could be simultaneously active (Xiu 2019).\n\nPatterson, David A, and John L Hennessy. 2016. Computer Organization and Design ARM Edition: The Hardware Software Interface. Morgan kaufmann.\n\nXiu, Liming. 2019. “Time Moore: Exploiting Moore’s Law from the Perspective of Time.” IEEE Solid-State Circuits Mag. 11 (1): 39–55. https://doi.org/10.1109/mssc.2018.2882285.\n“Dark silicon” refers to portions of the chip that cannot be powered simultaneously due to thermal and power limitations. Essentially, as the density of transistors increased, the proportion of the chip that could be actively used without overheating or exceeding power budgets shrank.\nThis phenomenon meant that while chips had more transistors, not all could be operational simultaneously, limiting potential performance gains. This power crisis necessitated a shift to the accelerator era, with specialized hardware units tailored for specific tasks to maximize efficiency. The explosion in AI workloads further drove demand for customized accelerators. Enabling factors included new programming languages, software tools, and manufacturing advances.\nFundamentally, hardware accelerators are evaluated on performance, power, and silicon area (PPA)—the nature of the target application—whether memory-bound or compute-bound—heavily influences the design. For example, memory-bound workloads demand high bandwidth and low latency access, while compute-bound applications require maximal computational throughput.\n\n\n11.2.3 General Principles\nThe design of specialized hardware accelerators involves navigating complex tradeoffs between performance, power efficiency, silicon area, and workload-specific optimizations. This section outlines core considerations and methodologies for achieving an optimal balance based on application requirements and hardware constraints.\n\nPerformance Within Power Budgets\nTo understand how to achieve the right balance between performance and power budgets, it’s important to first define a few key concepts that play a crucial role in this process. Performance broadly refers to the overall capability of a system to complete computational tasks effectively within given constraints. One of the key components of performance is throughput, which is the rate at which these tasks are processed, commonly measured in floating point operations per second (FLOPS) or frames per second (FPS). Throughput depends heavily on parallelism—the ability of the hardware to carry out multiple operations simultaneously—and clock frequency, which is the speed at which the processor cycles through these operations. Higher throughput typically leads to better performance, but it also increases power consumption as activity rises.\nSimply maximizing throughput is not enough; the efficiency of the hardware also matters. Efficiency is the measure of how many operations are performed per watt of power consumed, reflecting the relationship between computational work and energy use. In scenarios where power is a limiting factor, such as in edge devices, achieving high efficiency is critical. To help you remember how these concepts interconnect, consider the following relationships:\n\nPerformance = Throughput * Efficiency\nThroughput ~= Parallelism * Clock Frequency\nEfficiency = Operations / Watt\n\nHardware accelerators aim to maximize performance within set power budgets. This requires careful balancing of parallelism, the chip’s clock frequency, the operating voltage, workload optimization, and other techniques to maximize operations per watt.\nFor example, GPUs achieve high throughput via massively parallel architectures. However, their efficiency is lower than that of customized application-specific integrated circuits (ASICs) like Google’s TPU, which optimize for a specific workload.\n\n\nManaging Silicon Area and Costs\nThe size of a chip’s area has a direct impact on its manufacturing cost. To understand why, it helps to know a bit about the manufacturing process.\nChips are created from large, thin slices of semiconductor material known as wafers. During manufacturing, each wafer is divided into multiple smaller blocks called dies, with each die containing the circuitry for an individual chip. After the wafer is processed, it’s cut into these individual dies, which are then packaged to form the final chips used in electronic devices.\nLarger dies require more material and are more prone to defects, which can lower the yield—meaning fewer usable chips are produced from each wafer. While manufacturers can scale designs by combining multiple smaller dies into a single package (multi-die packages), this adds complexity and cost to the packaging and production process.\nThe amount of silicon area needed on a die depends on several factors:\n\nComputational resources - e.g., number of cores, memory, caches\nManufacturing process node - smaller transistors enable higher density\nProgramming model - programmed accelerators require more flexibility\n\nAccelerator design involves squeezing maximum performance within these silicon area constraints. Techniques like pruning and compression help fit larger models onto the chip without exceeding the available space.\n\n\nWorkload-Specific Optimizations\nDesigning effective hardware accelerators requires tailoring the architecture to the specific demands of the target workload. Different types of workloads—whether in AI, graphics, or robotics—have unique characteristics that dictate how the accelerator should be optimized.\nSome of the key considerations when optimizing hardware for specific workloads include:\n\nMemory vs Compute boundedness: Memory-bound workloads require more memory bandwidth, while compute-bound apps need arithmetic throughput.\nData locality: Data movement should be minimized for efficiency. Near-compute memory helps.\nBit-level operations: Low precision datatypes like INT8/INT4 optimize compute density.\nData parallelism: Multiple replicated compute units allow parallel execution.\nPipelining: Overlapped execution of operations increases throughput.\n\nUnderstanding workload characteristics enables customized acceleration. For example, convolutional neural networks use sliding window operations optimally mapped to spatial arrays of processing elements.\nBy understanding these architectural tradeoffs, designers can make informed decisions about the hardware accelerator’s architecture, ensuring that it delivers the best possible performance for its intended use.\n\n\nSustainable Hardware Design\nIn recent years, AI sustainability has become a pressing concern driven by two key factors - the exploding scale of AI workloads and their associated energy consumption.\nFirst, the size of AI models and datasets has rapidly grown. For example, based on OpenAI’s AI computing trends, the amount of computing used to train state-of-the-art models doubles every 3.5 months. This exponential growth requires massive computational resources in data centers.\nSecond, the energy usage of AI training and inference presents sustainability challenges. Data centers running AI applications consume substantial energy, contributing to high carbon emissions. It’s estimated that training a large AI model can have a carbon footprint of 626,000 pounds of CO2 equivalent, almost 5 times the lifetime emissions of an average car.\nTo address these challenges, sustainable hardware design focuses on optimizing energy efficiency without compromising performance. This involves developing specialized accelerators that minimize energy consumption while maximizing computational throughput.\nWe will learn about Sustainable AI in a later chapter, where we will discuss it in more detail.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>AI Acceleration</span>"
    ]
  },
  {
    "objectID": "contents/core/hw_acceleration/hw_acceleration.html#sec-aihw",
    "href": "contents/core/hw_acceleration/hw_acceleration.html#sec-aihw",
    "title": "11  AI Acceleration",
    "section": "11.3 Accelerator Types",
    "text": "11.3 Accelerator Types\nHardware accelerators can take on many forms. They can exist as a widget (like the Neural Engine in the Apple M1 chip) or as entire chips specially designed to perform certain tasks very well. This section will examine processors for machine learning workloads along the spectrum from highly specialized ASICs to more general-purpose CPUs.\nWe first focus on custom hardware purpose-built for AI to understand the most extreme optimizations possible when design constraints are removed. This establishes a ceiling for performance and efficiency. We then progressively consider more programmable and adaptable architectures, discussing GPUs and FPGAs. These make tradeoffs in customization to maintain flexibility. Finally, we cover general-purpose CPUs that sacrifice optimizations for a particular workload in exchange for versatile programmability across applications.\nBy structuring the analysis along this spectrum, we aim to illustrate the fundamental tradeoffs between utilization, efficiency, programmability, and flexibility in accelerator design. The optimal balance point depends on the constraints and requirements of the target application. This spectrum perspective provides a framework for reasoning about hardware choices for machine learning and the capabilities required at each level of specialization.\nFigure 11.2 illustrates the complex interplay between flexibility, performance, functional diversity, and area of architecture design. Notice how the ASIC is on the bottom-right corner, with minimal area, flexibility, and power consumption and maximal performance, due to its highly specialized application-specific nature. A key tradeoff is functional diversity vs performance: general purpose architectures can serve diverse applications but their application performance is degraded as compared to more customized architectures.\n\n\n\n\n\n\nFigure 11.2: Design tradeoffs. Source: El-Rayis (2014).\n\n\nEl-Rayis, A. O. 2014. “Reconfigurable Architectures for the Next Generation of Mobile Device Telecommunications Systems.” : https://www.researchgate.net/publication/292608967.\n\n\nThe progression begins with the most specialized option, ASICs purpose-built for AI, to ground our understanding in the maximum possible optimizations before expanding to more generalizable architectures. This structured approach elucidates the accelerator design space.\n\n11.3.1 Application-Specific Integrated Circuits (ASICs)\nAn Application-Specific Integrated Circuit (ASIC) is a type of integrated circuit (IC) that is custom-designed for a specific application or workload rather than for general-purpose use. Unlike CPUs and GPUs, ASICs do not support multiple applications or workloads. Rather, they are optimized to perform a single task extremely efficiently. The Google TPU is an example of an ASIC.\nASICs achieve this efficiency by tailoring every aspect of the chip design - the underlying logic gates, electronic components, architecture, memory, I/O, and manufacturing process - specifically for the target application. This level of customization allows removing any unnecessary logic or functionality required for general computation. The result is an IC that maximizes performance and power efficiency on the desired workload. The efficiency gains from application-specific hardware are so substantial that these software-centric firms dedicate enormous engineering resources to designing customized ASICs.\nThe rise of more complex machine learning algorithms has made the performance advantages enabled by tailored hardware acceleration a key competitive differentiator, even for companies traditionally concentrated on software engineering. ASICs have become a high-priority investment for major cloud providers aiming to offer faster AI computation.\n\nAdvantages\nDue to their customized nature, ASICs provide significant benefits over general-purpose processors like CPUs and GPUs. The key advantages include the following.\n\nMaximized Performance and Efficiency\nThe most fundamental advantage of ASICs is maximizing performance and power efficiency by customizing the hardware architecture specifically for the target application. Every transistor and design aspect is optimized for the desired workload - no unnecessary logic or overhead is needed to support generic computation.\nFor example, Google’s Tensor Processing Units (TPUs) contain architectures tailored exactly for the matrix multiplication operations used in neural networks. To design the TPU ASICs, Google’s engineering teams need to define the chip specifications clearly, write the architecture description using Hardware Description Languages like Verilog, synthesize the design to map it to hardware components, and carefully place-and-route transistors and wires based on the fabrication process design rules. This complex design process, known as very-large-scale integration (VLSI), allows them to build an optimized IC for machine learning workloads.\nAs a result, TPU ASICs achieve over an order of magnitude higher efficiency in operations per watt than general-purpose GPUs on ML workloads by maximizing performance and minimizing power consumption through a full-stack custom hardware design.\n\n\nSpecialized On-Chip Memory\nASICs incorporate on-chip memory, such as SRAM (Static Random Access Memory), and caches that are specifically optimized to feed data to the computational units. SRAM is a type of memory that is faster and more reliable than DRAM (Dynamic Random Access Memory) because it does not need to be periodically refreshed. However, it requires more transistors per bit of data, making it take up more space and more expensive to produce as compared to DRAM.\nSRAM is ideal for on-chip memory, where speed is critical. The advantage of having large amounts of high-bandwidth, on-chip SRAM is that data can be stored close to the processing elements, allowing for rapid access. This provides tremendous speed advantages compared to acessing off-chip DRAM, which, although larger in capacity, can be up to 100x slower. For example, Apple’s M1 system-on-a-chip contains special low-latency SRAM to accelerate the performance of its Neural Engine machine learning hardware.\nData locality and optimizing memory hierarchy are crucial for high throughput and low power. Table 11.1 shows “Numbers Everyone Should Know,” from Jeff Dean.\n\n\n\nTable 11.1: Latency comparison of operations in computing and networking.\n\n\n\n\n\n\n\n\n\nOperation\nLatency\n\n\n\n\nL1 cache reference\n0.5 ns\n\n\nBranch mispredict\n5 ns\n\n\nL2 cache reference\n7 ns\n\n\nMutex lock/unlock\n25 ns\n\n\nMain memory reference\n100 ns\n\n\nCompress 1K bytes with Zippy\n3,000 ns (3 µs)\n\n\nSend 1 KB bytes over 1 Gbps network\n10,000 ns (10 µs)\n\n\nRead 4 KB randomly from SSD\n150,000 ns (150 µs)\n\n\nRead 1 MB sequentially from memory\n250,000 ns (250 µs)\n\n\nRound trip within same datacenter\n500,000 ns (0.5 ms)\n\n\nRead 1 MB sequentially from SSD\n1,000,000 ns (1 ms)\n\n\nDisk seek\n10,000,000 ns (10 ms)\n\n\nRead 1 MB sequentially from disk\n20,000,000 ns (20 ms)\n\n\nSend packet CA → Netherlands → CA\n150,000,000 ns (150 ms)\n\n\n\n\n\n\n\n\nCustom Datatypes and Operations\nUnlike general-purpose processors, ASICs can be designed to natively support custom datatypes like INT4 or bfloat16, which are widely used in ML models. For instance, Nvidia’s Ampere GPU architecture has dedicated bfloat16 Tensor Cores to accelerate AI workloads. Low-precision datatypes enable higher arithmetic density and performance. ASICs can also directly incorporate non-standard operations common in ML algorithms as primitive operations - for example, natively supporting activation functions like ReLU makes execution more efficient.\n\n\nHigh Parallelism\nASIC architectures can leverage higher parallelism tuned for the target workload versus general-purpose CPUs or GPUs. More computational units tailored for the application mean more operations execute simultaneously. Highly parallel ASICs achieve tremendous throughput for data parallel workloads like neural network inference.\n\n\nAdvanced Process Nodes\nCutting-edge manufacturing processes allow more transistors to be packed into smaller die areas, increasing density. ASICs designed specifically for high-volume applications can better amortize the costs of cutting-edge process nodes.\n\n\n\nDisadvantages\n\nLong Design Timelines\nThe engineering process of designing and validating an ASIC can take 2-3 years. Synthesizing the architecture using hardware description languages, taping out the chip layout, and fabricating the silicon on advanced process nodes involve long development cycles. For example, to tape out a 7nm chip, teams need to define specifications carefully, write the architecture in HDL, synthesize the logic gates, place components, route all interconnections, and finalize the layout to send for fabrication. This very large-scale integration (VLSI) flow means ASIC design and manufacturing can traditionally take 2-5 years.\nThere are a few key reasons why the long design timelines of ASICs, often 2-3 years, can be challenging for machine learning workloads:\n\nML algorithms evolve rapidly: New model architectures, training techniques, and network optimizations are constantly emerging. For example, Transformers became hugely popular in NLP last few years. When an ASIC finishes tapeout, the optimal architecture for a workload may have changed.\nDatasets grow quickly: ASICs designed for certain model sizes or datatypes can become undersized relative to demand. For instance, natural language models are scaling exponentially with more data and parameters. A chip designed for BERT might not accommodate GPT-3.\nML applications change frequently: The industry focus shifts between computer vision, speech, NLP, recommender systems, etc. An ASIC optimized for image classification may have less relevance in a few years.\nFaster design cycles with GPUs/FPGAs: Programmable accelerators like GPUs can adapt much quicker by upgrading software libraries and frameworks. New algorithms can be deployed without hardware changes.\nTime-to-market needs: Getting a competitive edge in ML requires rapidly experimenting with and deploying new ideas. Waiting several years for an ASIC is different from fast iteration.\n\nThe pace of innovation in ML needs to be better matched to the multi-year timescale for ASIC development. Significant engineering efforts are required to extend ASIC lifespan through modular architectures, process scaling, model compression, and other techniques. However, the rapid evolution of ML makes fixed-function hardware challenging.\n\n\nHigh Non-Recurring Engineering Costs\nThe fixed costs of taking an ASIC from design to high-volume manufacturing can be very capital-intensive, often tens of millions of dollars. Photomask fabrication for taping out chips in advanced process nodes, packaging, and one-time engineering efforts is expensive. For instance, a 7nm chip tape-out alone could cost millions. The high non-recurring engineering (NRE) investment narrows ASIC viability to high-volume production use cases where the upfront cost can be amortized.\n\n\nComplex Integration and Programming\nASICs require extensive software integration work, including drivers, compilers, OS support, and debugging tools. They also need expertise in electrical and thermal packaging. Additionally, efficiently programming ASIC architectures can involve challenges like workload partitioning and scheduling across many parallel units. The customized nature necessitates significant integration efforts to turn raw hardware into fully operational accelerators.\nWhile ASICs provide massive efficiency gains on target applications by tailoring every aspect of the hardware design to one specific task, their fixed nature results in tradeoffs in flexibility and development costs compared to programmable accelerators, which must be weighed based on the application.\n\n\n\n\n11.3.2 Field-Programmable Gate Arrays (FPGAs)\nFPGAs are programmable integrated circuits that can be reconfigured for different applications. Their customizable nature provides advantages for accelerating AI algorithms compared to fixed ASICs or inflexible GPUs. While Google, Meta, and NVIDIA are considering putting ASICs in data centers, Microsoft deployed FPGAs in its data centers (Putnam et al. 2014) in 2011 to efficiently serve diverse data center workloads.\n\nXiong, Siyu, Guoqing Wu, Xitian Fan, Xuan Feng, Zhongcheng Huang, Wei Cao, Xuegong Zhou, et al. 2021. “MRI-Based Brain Tumor Segmentation Using FPGA-Accelerated Neural Network.” BMC Bioinf. 22 (1): 421. https://doi.org/10.1186/s12859-021-04347-6.\nFPGAs have found widespread application in various fields, including medical imaging, robotics, and finance, where they excel in handling computationally intensive machine learning tasks. In medical imaging, an illustrative example is the application of FPGAs for brain tumor segmentation, a traditionally time-consuming and error-prone process. Compared to traditional GPU and CPU implementations, FPGAs have demonstrated over 5x and 44x performance improvements, respectively, and 11x and 82x gains in energy efficiency, highlighting their potential for demanding applications (Xiong et al. 2021).\n\nAdvantages\nFPGAs provide several benefits over GPUs and ASICs for accelerating machine learning workloads.\n\nFlexibility Through Reconfigurable Fabric\nThe key advantage of FPGAs is the ability to reconfigure the underlying fabric to implement custom architectures optimized for different models, unlike fixed-function ASICs. For example, quant trading firms use FPGAs to accelerate their algorithms because they change frequently, and the low NRE cost of FPGAs is more viable than tapping out new ASICs. Figure 11.3 contains a table comparing three different FPGAs.\n\n\n\n\n\n\nFigure 11.3: Comparison of FPGAs. Source: Gwennap (n.d.).\n\n\nGwennap, Linley. n.d. “Certus-NX Innovates General-Purpose FPGAs.”\n\n\nFPGAs comprise basic building blocks - configurable logic blocks, RAM blocks, and interconnects. Vendors provide a base amount of these resources, and engineers program the chips by compiling HDL code into bitstreams that rearrange the fabric into different configurations. This makes FPGAs adaptable as algorithms evolve.\nWhile FPGAs may not achieve the utmost performance and efficiency of workload-specific ASICs, their programmability provides more flexibility as algorithms change. This adaptability makes FPGAs a compelling choice for accelerating evolving machine learning applications.\n\n\nCustomized Parallelism and Pipelining\nFPGA architectures can leverage spatial parallelism and pipelining by tailoring the hardware design to mirror the parallelism in ML models. For example, on an Intel’s HARPv2 FPGA platform one can split the layers of a convolutional network across separate processing elements to maximize throughput. Unique parallel patterns like tree ensemble evaluations are also possible on FPGAs. Deep pipelines with optimized buffering and dataflow can be customized to each model’s structure and datatypes. This level of tailored parallelism and pipelining is not feasible on GPUs.\n\n\nLow Latency On-Chip Memory\nLarge amounts of high-bandwidth on-chip memory enable localized storage for weights and activations. For instance, Xilinx Versal FPGAs contain 32MB of low-latency RAM blocks and dual-channel DDR4 interfaces for external memory. Bringing memory physically closer to the compute units reduces access latency. This provides significant speed advantages over GPUs that traverse PCIe or other system buses to reach off-chip GDDR6 memory.\n\n\nNative Support for Low Precision\nA key advantage of FPGAs is the ability to natively implement any bit width for arithmetic units, such as INT4 or bfloat16, used in quantized ML models. For example, Intel Stratix 10 NX FPGA has dedicated INT8 cores that can achieve up to 143 INT8 TOPS (Tera Operations Per Second) at ~1 TOPS/W (Tera Operations Per Second per Watt). TOPS is a measure of performance similar to FLOPS, but while FLOPS measures floating-point calculations, TOPS measures the number of integer operations a system can perform per second. Lower bit widths, like INT8 or INT4, increase arithmetic density and performance. FPGAs can even support mixed precision or dynamic precision tuning at runtime.\n\n\n\nDisadvantages\n\nLower Peak Throughput than ASICs\nFPGAs cannot match the raw throughput numbers of ASICs customized for a specific model and precision. The overheads of the reconfigurable fabric compared to fixed function hardware result in lower peak performance. For example, the TPU v5e pods allow up to 256 chips to be connected with more than 100 PetaOps (Peta Operations Per Second) of INT8 performance, while FPGAs can offer up to 143 INT8 TOPS or 286 INT4 TOPS such as on the Intel Stratix 10 NX FPGA; PetaOps represents quadrillions of operations per second, whereas TOPS measures trillions, highlighting the much greater throughput capability of TPU pods compared to FPGAs.\nThis is because FPGAs comprise basic building blocks—configurable logic blocks, RAM blocks, and interconnects. Vendors provide a set amount of these resources. To program FPGAs, engineers write HDL code and compile it into bitstreams that rearrange the fabric, which has inherent overheads versus an ASIC purpose-built for one computation.\n\n\nProgramming Complexity\nTo optimize FPGA performance, engineers must program the architectures in low-level hardware description languages like Verilog or VHDL. This requires hardware design expertise and longer development cycles than higher-level software frameworks like TensorFlow. Maximizing utilization can be challenging despite advances in high-level synthesis from C/C++.\n\n\nReconfiguration Overheads\nChanging FPGA configurations requires reloading a new bitstream, which has considerable latency and storage size costs. For example, partial reconfiguration on Xilinx FPGAs can take 100s of milliseconds. This makes dynamically swapping architectures in real-time infeasible. The bitstream storage also consumes on-chip memory.\n\n\nDiminishing Gains on Advanced Nodes\nWhile smaller process nodes greatly benefit ASICs, they provide fewer advantages for FPGAs. At 7nm and below, effects like process variation, thermal constraints, and aging disproportionately impact FPGA performance. The overheads of the configurable fabric also diminish gains compared to fixed-function ASICs.\n\n\n\n\n11.3.3 Digital Signal Processors (DSPs)\nThe first digital signal processor core was built in 1948 by Texas Instruments (The Evolution of Audio DSPs). Traditionally, DSPs would have logic to directly access digital/audio data in memory, perform an arithmetic operation (multiply-add-accumulate-MAC was one of the most common operations), and then write the result back to memory. The DSP would include specialized analog components to retrieve digital/audio data.\nOnce we entered the smartphone era, DSPs started encompassing more sophisticated tasks. They required Bluetooth, Wi-Fi, and cellular connectivity. Media also became much more complex. Today, it’s rare to have entire chips dedicated to just DSP, but a System on Chip would include DSPs and general-purpose CPUs. For example, Qualcomm’s Hexagon Digital Signal Processor claims to be a “world-class processor with both CPU and DSP functionality to support deeply embedded processing needs of the mobile platform for both multimedia and modem functions.” Google Tensors, the chip in the Google Pixel phones, also includes CPUs and specialized DSP engines.\n\nAdvantages\nDSPs architecturally provide advantages in vector math throughput, low latency memory access, power efficiency, and support for diverse datatypes - making them well-suited for embedded ML acceleration.\n\nOptimized Architecture for Vector Math\nDSPs contain specialized data paths, register files, and instructions optimized specifically for vector math operations commonly used in machine learning models. This includes dot product engines, MAC units, and SIMD capabilities tailored for vector/matrix calculations. For example, the CEVA-XM6 DSP (“Ceva SensPro Fuses AI and Vector DSP”) has 512-bit vector units to accelerate convolutions. This efficiency on vector math workloads is far beyond general CPUs.\n\n\nLow Latency On-Chip Memory\nDSPs integrate large amounts of fast on-chip SRAM memory to hold data locally for processing. Bringing memory physically closer to the computation units reduces access latency. For example, Analog’s SHARC+ DSP contains 10MB of on-chip SRAM. This high-bandwidth local memory provides speed advantages for real-time applications.\n\n\nPower Efficiency\nDSPs are engineered to provide high performance per watt on digital signal workloads. Efficient data paths, parallelism, and memory architectures enable trillions of math operations per second within tight mobile power budgets. For example, Qualcomm’s Hexagon DSP can deliver 4 TOPS while consuming minimal watts.\n\n\nSupport for Integer and Floating Point Math\nUnlike GPUs that excel at single or half precision, DSPs can natively support 8/16-bit integer and 32-bit floating point datatypes used across ML models. Some DSPs support dot product acceleration at INT8 precision for quantized neural networks.\n\n\n\nDisadvantages\nDSPs make architectural tradeoffs that limit peak throughput, precision, and model capacity compared to other AI accelerators. However, their advantages in power efficiency and integer math make them a strong edge computing option. So, while DSPs provide some benefits over CPUs, they also come with limitations for machine learning workloads:\n\nLower Peak Throughput than ASICs/GPUs\nDSPs cannot match the raw computational throughput of GPUs or customized ASICs designed specifically for machine learning. For example, Qualcomm’s Cloud AI 100 ASIC delivers 480 TOPS on INT8, while their Hexagon DSP provides 10 TOPS. DSPs lack the massive parallelism of GPU SM units.\n\n\nSlower Double Precision Performance\nMost DSPs must be optimized for the higher precision floating point needed in some ML models. Their dot product engines focus on INT8/16 and FP32, which provide better power efficiency. However, 64-bit floating point throughput is much lower, which can limit usage in models requiring high precision.\n\n\nConstrained Model Capacity\nThe limited on-chip memory of DSPs constrains the model sizes that can be run. Large deep learning models with hundreds of megabytes of parameters would exceed on-chip SRAM capacity. DSPs are best suited for small to mid-sized models targeted for edge devices.\n\n\nProgramming Complexity\nEfficient programming of DSP architectures requires expertise in parallel programming and optimizing data access patterns. Their specialized microarchitectures have a steeper learning curve than high-level software frameworks, making development more complex.\n\n\n\n\n11.3.4 Graphics Processing Units (GPUs)\nThe term graphics processing unit has existed since at least the 1980s. There had always been a demand for graphics hardware in video game consoles (high demand, needed to be relatively lower cost) and scientific simulations (lower demand, but higher resolution, could be at a high price point).\nThe term was popularized, however, in 1999 when NVIDIA launched the GeForce 256, mainly targeting the PC games market sector (Lindholm et al. 2008). As PC games became more sophisticated, NVIDIA GPUs became more programmable. Soon, users realized they could take advantage of this programmability, run various non-graphics-related workloads on GPUs, and benefit from the underlying architecture. And so, in the late 2000s, GPUs became general-purpose graphics processing units or GP-GPUs.\n\nLindholm, Erik, John Nickolls, Stuart Oberman, and John Montrym. 2008. “NVIDIA Tesla: A Unified Graphics and Computing Architecture.” IEEE Micro 28 (2): 39–55. https://doi.org/10.1109/mm.2008.31.\nFollowing this shift, other major players like Intel with its Arc Graphics and AMD with their Radeon RX series also evolved their GPUs to support a broader range of applications beyond traditional graphics rendering. This expansion of GPU capabilities opened up new possibilities, particularly in fields requiring massive computational power.\nA striking example of this potential is the recent groundbreaking research conducted by OpenAI (Brown et al. 2020) with GPT-3, a language model with 175 billion parameters. Training such a massive model, which would have taken months on conventional CPUs, was completed in a matter of days using powerful GPUs, showcasing the transformative impact of GPUs in accelerating complex machine learning tasks.\n\nAdvantages\n\nHigh Computational Throughput\nThe key advantage of GPUs is their ability to perform massively parallel floating-point calculations optimized for computer graphics and linear algebra (Raina, Madhavan, and Ng 2009). Modern GPUs like Nvidia’s A100 offer up to 19.5 teraflops of FP32 performance with 6912 CUDA cores and 40GB of graphics memory tightly coupled with 1.6TB/s of graphics memory bandwidth.\n\nRaina, Rajat, Anand Madhavan, and Andrew Y. Ng. 2009. “Large-Scale Deep Unsupervised Learning Using Graphics Processors.” In Proceedings of the 26th Annual International Conference on Machine Learning, edited by Andrea Pohoreckyj Danyluk, Léon Bottou, and Michael L. Littman, 382:873–80. ACM International Conference Proceeding Series. ACM. https://doi.org/10.1145/1553374.1553486.\nThis raw throughput stems from the highly parallel streaming multiprocessor (SM) architecture tailored for data-parallel workloads (Zhihao Jia, Zaharia, and Aiken 2019). Each SM contains hundreds of scalar cores optimized for float32/64 math. With thousands of SMs on a chip, GPUs are purpose-built for matrix multiplication and vector operations used throughout neural networks.\nFor example, Nvidia’s latest H100 GPU provides 4000 TFLOPs of FP8, 2000 TFLOPs of FP16, 1000 TFLOPs of TF32, 67 TFLOPs of FP32 and 34 TFLOPs of FP64 compute performance, which can dramatically accelerate large batch training on models like BERT, GPT-3, and other transformer architectures. The scalable parallelism of GPUs is key to speeding up computationally intensive deep learning.\n\n\nMature Software Ecosystem\nNvidia provides extensive runtime libraries like cuDNN and cuBLAS that are highly optimized for deep learning primitives. Frameworks like TensorFlow and PyTorch integrate with these libraries to enable GPU acceleration without direct programming. These libraries are built on top of CUDA, Nvidia’s parallel computing platform and programming model.\nCUDA (Compute Unified Device Architecture) is the underlying framework that allows these high-level libraries to interact with the GPU’s hardware. It provides developers with low-level access to the GPU’s resources, enabling custom computations and optimizations that fully leverage the GPU’s parallel processing capabilities. By using CUDA, developers can write software that exploits the GPU’s architecture for high-performance computing tasks.\nThis ecosystem enables quick leveraging of GPUs via high-level Python without GPU programming expertise. Known workflows and abstractions provide a convenient on-ramp for scaling up deep learning experiments. The software maturity supplements the throughput advantages.\n\n\nBroad Availability\nThe economies of scale of graphics processing make GPUs broadly accessible in data centers, cloud platforms like AWS and GCP, and desktop workstations. Their availability in research environments has provided a convenient ML experimentation and innovation platform. For example, nearly every state-of-the-art deep learning result has involved GPU acceleration because of this ubiquity. The broad access supplements the software maturity to make GPUs the standard ML accelerator.\n\n\nProgrammable Architecture\nWhile not as flexible as FPGAs, GPUs provide programmability via CUDA and shader languages to customize computations. Developers can optimize data access patterns, create new ops, and tune precisions for evolving models and algorithms.\n\n\n\nDisadvantages\nWhile GPUs have become the standard accelerator for deep learning, their architecture has some key downsides.\n\nLess Efficient than Custom ASICs\nThe statement “GPUs are less efficient than ASICs” could spark intense debate within the ML/AI field and cause this book to explode.\nTypically, GPUs are perceived as less efficient than ASICs because the latter are custom-built for specific tasks and thus can operate more efficiently by design. With their general-purpose architecture, GPUs are inherently more versatile and programmable, catering to a broad spectrum of computational tasks beyond ML/AI.\nHowever, modern GPUs have evolved to include specialized hardware support for essential AI operations, such as generalized matrix multiplication (GEMM) and other matrix operations, native support for quantization, and native support for pruning, which are critical for running ML models effectively. These enhancements have significantly improved the efficiency of GPUs for AI tasks to the point where they can rival the performance of ASICs for certain applications.\nConsequently, contemporary GPUs are convergent, incorporating specialized ASIC-like capabilities within a flexible, general-purpose processing framework. This adaptability has blurred the lines between the two types of hardware. GPUs offer a strong balance of specialization and programmability that is well-suited to the dynamic needs of ML/AI research and development.\n\n\nHigh Memory Bandwidth Needs\nThe massively parallel architecture requires tremendous memory bandwidth to supply thousands of cores. For example, the Nvidia A100 GPU requires 1.6TB/sec to fully saturate its computer. GPUs rely on wide 384-bit memory buses to high-bandwidth GDDR6 RAM, but even the fastest GDDR6 tops out at around 1 TB/sec. This dependence on external DRAM incurs latency and power overheads.\n\n\nProgramming Complexity\nWhile tools like CUDA help, optimally mapping and partitioning ML workloads across the massively parallel GPU architecture remains challenging, achieving both high utilization and memory locality requires low-level tuning (Zhe Jia et al. 2018). Abstractions like TensorFlow can leave performance on the table.\n\nJia, Zhe, Marco Maggioni, Benjamin Staiger, and Daniele P. Scarpazza. 2018. “Dissecting the NVIDIA Volta GPU Architecture via Microbenchmarking.” ArXiv Preprint. https://arxiv.org/abs/1804.06826.\n\n\nLimited On-Chip Memory\nGPUs have relatively small on-chip memory caches compared to ML models’ large working set requirements during training. They rely on high bandwidth access to external DRAM, which ASICs minimize with large on-chip SRAM.\n\n\nFixed Architecture\nUnlike FPGAs, the fundamental GPU architecture cannot be altered post-manufacture. This constraint limits adapting to novel ML workloads or layers. The CPU-GPU boundary also creates data movement overheads.\n\n\n\n\n11.3.5 Central Processing Units (CPUs)\nThe term CPUs has a long history that dates back to 1955 (Weik 1955) while the first microprocessor CPU-the Intel 4004-was invented in 1971 (Who Invented the Microprocessor?). Compilers compile high-level programming languages like Python, Java, or C to assemble instructions (x86, ARM, RISC-V, etc.) for CPUs to process. The set of instructions a CPU understands is called the “instruction set architecture” (ISA), which defines the commands that the processor can execute directly. It must be agreed upon by both the hardware and software running atop it.\n\nWeik, Martin H. 1955. A Survey of Domestic Electronic Digital Computing Systems. Ballistic Research Laboratories.\nAn overview of significant developments in CPUs:\n\nSingle-core Era (1950s- 2000): This era is known for aggressive microarchitectural improvements. Techniques like speculative execution (executing an instruction before the previous one was done), out-of-order execution (re-ordering instructions to be more effective), and wider issue widths (executing multiple instructions at once) were implemented to increase instruction throughput. The term “System on Chip” also originated in this era as different analog components (components designed with transistors) and digital components (components designed with hardware description languages that are mapped to transistors) were put on the same platform to achieve some task.\nMulticore Era (2000s): Driven by the decrease of Moore’s Law, this era is marked by scaling the number of cores within a CPU. Now, tasks can be split across many different cores, each with its own datapath and control unit. Many of the issues in this era pertained to how to share certain resources, which resources to share, and how to maintain coherency and consistency across all the cores.\nSea of accelerators (2010s): Again, driven by the decrease of Moore’s law, this era is marked by offloading more complicated tasks to accelerators (widgets) attached to the main datapath in CPUs. It’s common to see accelerators dedicated to various AI workloads, as well as image/digital processing, and cryptography. In these designs, CPUs are often described more as judges, deciding which tasks should be processed rather than doing the processing itself. Any task could still be run on the CPU rather than the accelerators, but the CPU would generally be slower. However, the cost of designing and programming the accelerator became a non-trivial hurdle that sparked interest in design-specific libraries (DSLs).\nPresence in data centers: Although we often hear that GPUs dominate the data center marker, CPUs are still well suited for tasks that don’t inherently possess a large amount of parallelism. CPUs often handle serial and small tasks and coordinate the data center.\nOn the edge: Given the tighter resource constraints on the edge, edge CPUs often only implement a subset of the techniques developed in the sing-core era because these optimizations tend to be heavy on power and area consumption. Edge CPUs still maintain a relatively simple datapath with limited memory capacities.\n\nTraditionally, CPUs have been synonymous with general-purpose computing, a term that has also changed as the “average” workload a consumer would run changes over time. For example, floating point components were once considered reserved for “scientific computing,” they were usually implemented as a co-processor (a modular component that worked with the datapath) and seldom deployed to average consumers. Compare this attitude to today, where FPUs are built into every datapath.\n\nAdvantages\nWhile raw throughput is limited, general-purpose CPUs provide practical AI acceleration benefits.\n\nGeneral Programmability\nCPUs support diverse workloads beyond ML, providing flexible general-purpose programmability. This versatility comes from their standardized instruction sets and mature compiler ecosystems, which allow running any application, from databases and web servers to analytics pipelines (Hennessy and Patterson 2019).\n\nHennessy, John L., and David A. Patterson. 2019. “A New Golden Age for Computer Architecture.” Commun. ACM 62 (2): 48–60. https://doi.org/10.1145/3282307.\nThis avoids the need for dedicated ML accelerators and enables leveraging existing CPU-based infrastructure for basic ML deployment. For example, X86 servers from vendors like Intel and AMD can run common ML frameworks using Python and TensorFlow packages alongside other enterprise workloads.\n\n\nMature Software Ecosystem\nFor decades, highly optimized math libraries like BLAS, LAPACK, and FFTW have leveraged vectorized instructions and multithreading on CPUs (Dongarra 2009). Major ML frameworks like PyTorch, TensorFlow, and SciKit-Learn are designed to integrate seamlessly with these CPU math kernels.\n\nDongarra, Jack J. 2009. “The Evolution of High Performance Computing on System z.” IBM J. Res. Dev. 53: 3–4.\nHardware vendors like Intel and AMD also provide low-level libraries to optimize performance for deep learning primitives fully (AI Inference Acceleration on CPUs). This robust, mature software ecosystem allows quickly deploying ML on existing CPU infrastructure.\n\n\nWide Availability\nThe economies of scale of CPU manufacturing, driven by demand across many markets like PCs, servers, and mobile, make them ubiquitously available. Intel CPUs, for example, have powered most servers for decades (Ranganathan 2011). This wide availability in data centers reduces hardware costs for basic ML deployment.\n\nRanganathan, Parthasarathy. 2011. “From Microprocessors to Nanostores: Rethinking Data-Centric Systems.” Computer 44 (1): 39–48. https://doi.org/10.1109/mc.2011.18.\nEven small embedded devices typically integrate some CPU, enabling edge inference. The ubiquity reduces the need to purchase specialized ML accelerators in many situations.\n\n\nLow Power for Inference\nOptimizations like ARM Neon and Intel AVX vector extensions provide power-efficient integer and floating point throughput optimized for “bursty” workloads such as inference (Ignatov et al. 2018). While slower than GPUs, CPU inference can be deployed in power-constrained environments. For example, ARM’s Cortex-M CPUs now deliver over 1 TOPS of INT8 performance under 1W, enabling keyword spotting and vision applications on edge devices (ARM).\n\n\n\nDisadvantages\nWhile providing some advantages, general-purpose CPUs also have limitations for AI workloads.\n\nLower Throughput than Accelerators\nCPUs lack the specialized architectures for massively parallel processing that GPUs and other accelerators provide. Their general-purpose design reduces computational throughput for the highly parallelizable math operations common in ML models (N. P. Jouppi et al. 2017a).\n\nJouppi, Norman P., Cliff Young, Nishant Patil, David Patterson, Gaurav Agrawal, Raminder Bajwa, Sarah Bates, et al. 2017a. “In-Datacenter Performance Analysis of a Tensor Processing Unit.” In Proceedings of the 44th Annual International Symposium on Computer Architecture, 1–12. ISCA ’17. New York, NY, USA: ACM. https://doi.org/10.1145/3079856.3080246.\n\n\nNot Optimized for Data Parallelism\nThe architectures of CPUs are not specifically optimized for data parallel workloads inherent to AI (Sze et al. 2017). They allocate substantial silicon area to instruction decoding, speculative execution, caching, and flow control that provides little benefit for the array operations used in neural networks (AI Inference Acceleration on CPUs). However, modern CPUs are equipped with vector instructions like AVX-512 specifically to accelerate certain key operations like matrix multiplication.\nGPU streaming multiprocessors, for example, devote most transistors to floating point units instead of complex branch prediction logic. This specialization allows much higher utilization for ML math.\n\n\nHigher Memory Latency\nCPUs suffer from higher latency accessing main memory relative to GPUs and other accelerators (DDR). Techniques like tiling and caching can help, but the physical separation from off-chip RAM bottlenecks data-intensive ML workloads. This emphasizes the need for specialized memory architectures in ML hardware.\n\n\nPower Inefficiency Under Heavy Workloads\nWhile suitable for intermittent inference, sustaining near-peak throughput for training results in inefficient power consumption on CPUs, especially mobile CPUs (Ignatov et al. 2018). Accelerators explicitly optimize the data flow, memory, and computation for sustained ML workloads. CPUs are energy-inefficient for training large models.\n\n\n\n\n11.3.6 Comparison\nTable 11.2 compares the different types of hardware features.\n\n\n\nTable 11.2: Comparison of different hardware accelerators for AI workloads.\n\n\n\n\n\n\n\n\n\n\n\nAccelerator\nDescription\nKey Advantages\nKey Disadvantages\n\n\n\n\nASICs\nCustom ICs designed for target workloads like AI inference\n\nMaximizes perf/watt.\nOptimized for tensor ops\nLow latency on-chip memory\n\n\nFixed architecture lacks flexibility\nHigh NRE cost\nLong design cycles\n\n\n\nFPGAs\nReconfigurable fabric with programmable logic and routing\n\nFlexible architecture\nLow latency memory access\n\n\nLower perf/watt than ASICs\nComplex programming\n\n\n\nGPUs\nOriginally for graphics, now used for neural network acceleration\n\nHigh throughput\nParallel scalability\nSoftware ecosystem with CUDA\n\n\nNot as power efficient as ASICs\nRequire high memory bandwidth\n\n\n\nCPUs\nGeneral purpose processors\n\nProgrammability\nUbiquitous availability\n\n\nLower performance for AI workloads\n\n\n\n\n\n\n\nIn general, CPUs provide a readily available baseline, GPUs deliver broadly accessible acceleration, FPGAs offer programmability, and ASICs maximize efficiency for fixed functions. The optimal choice depends on the target application’s scale, cost, flexibility, and other requirements.\nAlthough first developed for data center deployment, Google has also put considerable effort into developing Edge TPUs. These Edge TPUs maintain the inspiration from systolic arrays but are tailored to the limited resources accessible at the edge.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>AI Acceleration</span>"
    ]
  },
  {
    "objectID": "contents/core/hw_acceleration/hw_acceleration.html#hardware-software-co-design",
    "href": "contents/core/hw_acceleration/hw_acceleration.html#hardware-software-co-design",
    "title": "11  AI Acceleration",
    "section": "11.4 Hardware-Software Co-Design",
    "text": "11.4 Hardware-Software Co-Design\nHardware-software co-design is based on the principle that AI systems achieve optimal performance and efficiency when the hardware and software components are designed in tight integration. This involves an iterative, collaborative design cycle where the hardware architecture and software algorithms are concurrently developed and refined with continuous feedback between teams.\nFor example, a new neural network model may be prototyped on an FPGA-based accelerator platform to obtain real performance data early in the design process. These results provide feedback to the hardware designers on potential optimizations and the software developers on refinements to the model or framework to better leverage the hardware capabilities. This level of synergy is difficult to achieve with the common practice of software being developed independently to deploy on fixed commodity hardware.\nCo-design is critical for embedded AI systems facing significant resource constraints like low power budgets, limited memory and compute capacity, and real-time latency requirements. Tight integration between algorithm developers and hardware architects helps unlock optimizations across the stack to meet these restrictions. Enabling techniques include algorithmic improvements like neural architecture search and pruning and hardware advances like specialized dataflows and memory hierarchies.\nBy bringing hardware and software design together, rather than developing them separately, holistic optimizations can be made that maximize performance and efficiency. The next sections provide more details on specific co-design approaches.\n\n11.4.1 The Need for Co-Design\nSeveral key factors make a collaborative hardware-software co-design approach essential for building efficient AI systems.\n\nIncreasing Model Size and Complexity\nState-of-the-art AI models have been rapidly growing in size, enabled by advances in neural architecture design and the availability of large datasets. For example, the GPT-3 language model contains 175 billion parameters (Brown et al. 2020), requiring huge computational resources for training. This explosion in model complexity necessitates co-design to develop efficient hardware and algorithms in tandem. Techniques like model compression (Cheng et al. 2018) and quantization must be co-optimized with the hardware architecture.\n\nBrown, Tom B., Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, et al. 2020. “Language Models Are Few-Shot Learners.” In Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, Virtual, edited by Hugo Larochelle, Marc’Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin. https://proceedings.neurips.cc/paper/2020/hash/1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html.\n\nCheng, Yu, Duo Wang, Pan Zhou, and Tao Zhang. 2018. “Model Compression and Acceleration for Deep Neural Networks: The Principles, Progress, and Challenges.” IEEE Signal Process Mag. 35 (1): 126–36. https://doi.org/10.1109/msp.2017.2765695.\n\n\nConstraints of Embedded Deployment\nDeploying AI applications on edge devices like mobile phones or smart home appliances introduces significant constraints on energy, memory, and silicon area (Sze et al. 2017). Enable real-time inference under these restrictions requires co-exploring hardware optimizations like specialized dataflows and compression with efficient neural network design and pruning techniques. Co-design maximizes performance within tight deployment constraints.\n\n\nRapid Evolution of AI Algorithms\nAI is rapidly evolving, with new model architectures, training methodologies, and software frameworks constantly emerging. For example, Transformers have recently become hugely popular for NLP (Young et al. 2018). Keeping pace with these algorithmic innovations requires hardware-software co-design to adapt platforms and avoid accrued technical debt quickly.\n\nYoung, Tom, Devamanyu Hazarika, Soujanya Poria, and Erik Cambria. 2018. “Recent Trends in Deep Learning Based Natural Language Processing [Review Article].” IEEE Comput. Intell. Mag. 13 (3): 55–75. https://doi.org/10.1109/mci.2018.2840738.\n\n\nComplex Hardware-Software Interactions\nMany subtle interactions and tradeoffs between hardware architectural choices and software optimizations significantly impact overall efficiency. For instance, techniques like tensor partitioning and batching affect parallelism and data access patterns impact memory utilization. Co-design provides a cross-layer perspective to unravel these dependencies.\n\n\nNeed for Specialization\nAI workloads benefit from specialized operations like low-precision math and customized memory hierarchies. This motivates incorporating custom hardware tailored to neural network algorithms rather than relying solely on flexible software running on generic hardware (Sze et al. 2017). However, the software stack must explicitly target custom hardware operations to realize the benefits.\n\n\nDemand for Higher Efficiency\nWith growing model complexity, diminishing returns and overhead from optimizing only the hardware or software in isolation (Putnam et al. 2014) arise. Inevitable tradeoffs arise that require global optimization across layers. Jointly co-designing hardware and software provides large compound efficiency gains.\n\nPutnam, Andrew, Adrian M. Caulfield, Eric S. Chung, Derek Chiou, Kypros Constantinides, John Demme, Hadi Esmaeilzadeh, et al. 2014. “A Reconfigurable Fabric for Accelerating Large-Scale Datacenter Services.” ACM SIGARCH Computer Architecture News 42 (3): 13–24. https://doi.org/10.1145/2678373.2665678.\n\n\n\n11.4.2 Principles of Hardware-Software Co-Design\nThe underlying hardware architecture and software stack must be tightly integrated and co-optimized to build high-performance and efficient AI systems. Neither can be designed in isolation; maximizing their synergies requires a holistic approach known as hardware-software co-design.\nThe key goal is tailoring the hardware capabilities to match the algorithms and workloads run by the software. This requires a feedback loop between hardware architects and software developers to converge on optimized solutions. Several techniques enable effective co-design:\n\nHardware-Aware Software Optimization\nThe software stack can be optimized to leverage the underlying hardware capabilities better:\n\nParallelism: Parallelize matrix computations like convolution or attention layers to maximize throughput on vector engines.\nMemory Optimization: Tune data layouts to improve cache locality based on hardware profiling. This maximizes reuse and minimizes expensive DRAM access.\nCompression: Use sparsity in the models to reduce storage space and save on computation by zero-skipping operations.\nCustom Operations: Incorporate specialized operations like low-precision INT4 or bfloat16 into models to capitalize on dedicated hardware support.\nDataflow Mapping: Explicitly map model stages to computational units to optimize data movement on hardware.\n\n\n\nAlgorithm-Driven Hardware Specialization\nHardware can be tailored to suit the characteristics of ML algorithms better:\n\nCustom Datatypes: Support low precision INT8/4 or bfloat16 in hardware for higher arithmetic density.\nOn-Chip Memory: Increase SRAM bandwidth and lower access latency to match model memory access patterns.\nDomain-Specific Ops: Add hardware units for key ML functions like FFTs or matrix multiplication to reduce latency and energy.\nModel Profiling: Use model simulation and profiling to identify computational hotspots and optimize hardware.\n\nThe key is collaborative feedback - insights from hardware profiling guide software optimizations, while algorithmic advances inform hardware specialization. This mutual enhancement provides multiplicative efficiency gains compared to isolated efforts.\n\n\nAlgorithm-Hardware Co-exploration\nA powerful co-design technique involves jointly exploring innovations in neural network architectures and custom hardware design. This allows for finding ideal pairings tailored to each other’s strengths (Sze et al. 2017).\n\nSze, Vivienne, Yu-Hsin Chen, Tien-Ju Yang, and Joel S. Emer. 2017. “Efficient Processing of Deep Neural Networks: A Tutorial and Survey.” Proc. IEEE 105 (12): 2295–2329. https://doi.org/10.1109/jproc.2017.2761740.\n\nHoward, Andrew G., Menglong Zhu, Bo Chen, Dmitry Kalenichenko, Weijun Wang, Tobias Weyand, Marco Andreetto, and Hartwig Adam. 2017. “MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications.” ArXiv Preprint. https://arxiv.org/abs/1704.04861.\n\nJacob, Benoit, Skirmantas Kligys, Bo Chen, Menglong Zhu, Matthew Tang, Andrew Howard, Hartwig Adam, and Dmitry Kalenichenko. 2018. “Quantization and Training of Neural Networks for Efficient Integer-Arithmetic-Only Inference.” In 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2704–13. IEEE. https://doi.org/10.1109/cvpr.2018.00286.\n\nGale, Trevor, Erich Elsen, and Sara Hooker. 2019. “The State of Sparsity in Deep Neural Networks.” ArXiv Preprint abs/1902.09574. https://arxiv.org/abs/1902.09574.\n\nMishra, Asit K., Jorge Albericio Latorre, Jeff Pool, Darko Stosic, Dusan Stosic, Ganesh Venkatesh, Chong Yu, and Paulius Micikevicius. 2021. “Accelerating Sparse Deep Neural Networks.” CoRR abs/2104.08378. https://arxiv.org/abs/2104.08378.\nFor instance, the shift to mobile architectures like MobileNets (Howard et al. 2017) was guided by edge device constraints like model size and latency. The quantization (Jacob et al. 2018) and pruning techniques (Gale, Elsen, and Hooker 2019) that unlocked these efficient models became possible thanks to hardware accelerators with native low-precision integer support and pruning support (Mishra et al. 2021).\nAttention-based models have thrived on massively parallel GPUs and ASICs, where their computation maps well spatially, as opposed to RNN architectures, which rely on sequential processing. The co-evolution of algorithms and hardware unlocked new capabilities.\nEffective co-exploration requires close collaboration between algorithm researchers and hardware architects. Rapid prototyping on FPGAs (C. Zhang et al. 2015) or specialized AI simulators allows quick evaluation of different pairings of model architectures and hardware designs pre-silicon.\n\nZhang, Chen, Peng Li, Guangyu Sun, Yijin Guan, Bingjun Xiao, and Jason Optimizing Cong. 2015. “FPGA-Based Accelerator Design for Deep Convolutional Neural Networks Proceedings of the 2015 ACM.” In SIGDA International Symposium on Field-Programmable Gate Arrays-FPGA, 15:161–70.\nFor example, Google’s TPU architecture evolved with optimizations to TensorFlow models to maximize performance on image classification. This tight feedback loop yielded models tailored for the TPU that would have been unlikely in isolation.\nStudies have shown 2-5x higher performance and efficiency gains with algorithm-hardware co-exploration than isolated algorithm or hardware optimization efforts (Suda et al. 2016). Parallelizing the joint development also reduces time-to-deployment.\n\nSuda, Naveen, Vikas Chandra, Ganesh Dasika, Abinash Mohanty, Yufei Ma, Sarma Vrudhula, Jae-sun Seo, and Yu Cao. 2016. “Throughput-Optimized OpenCL-Based FPGA Accelerator for Large-Scale Convolutional Neural Networks.” In Proceedings of the 2016 ACM/SIGDA International Symposium on Field-Programmable Gate Arrays, 16–25. ACM. https://doi.org/10.1145/2847263.2847276.\nOverall, exploring the tight interdependencies between model innovation and hardware advances unlocks opportunities that must be visible when tackled sequentially. This synergistic co-design yields solutions greater than the sum of their parts.\n\n\n\n11.4.3 Challenges\nWhile collaborative co-design can improve efficiency, adaptability, and time to market, it also has engineering and organizational challenges.\n\nIncreased Prototyping Costs\nMore extensive prototyping is required to evaluate different hardware-software pairings. The need for rapid, iterative prototypes on FPGAs or emulators increases validation overhead. For example, Microsoft found that more prototypes were needed to co-design an AI accelerator than sequential design (Fowers et al. 2018).\n\nFowers, Jeremy, Kalin Ovtcharov, Michael Papamichael, Todd Massengill, Ming Liu, Daniel Lo, Shlomi Alkalay, et al. 2018. “A Configurable Cloud-Scale DNN Processor for Real-Time AI.” In 2018 ACM/IEEE 45th Annual International Symposium on Computer Architecture (ISCA), 1–14. IEEE; IEEE. https://doi.org/10.1109/isca.2018.00012.\n\n\nTeam and Organizational Hurdles\nCo-design requires close coordination between traditionally disconnected hardware and software groups. This could introduce communication issues or misaligned priorities and schedules. Navigating different engineering workflows is also challenging. Some organizational inertia to adopting integrated practices may exist.\n\n\nSimulation and Modeling Complexity\nCapturing subtle interactions between hardware and software layers for joint simulation and modeling adds significant complexity. Full cross-layer abstractions are difficult to construct quantitatively before implementation, making holistic optimizations harder to quantify ahead of time.\n\n\nOver-Specialization Risks\nTight co-design bears the risk of overfitting optimizations to current algorithms, sacrificing generality. For example, hardware tuned exclusively for Transformer models could underperform on future techniques. Maintaining flexibility requires foresight.\n\n\nAdoption Challenges\nEngineers comfortable with established discrete hardware or software design practices may only accept familiar collaborative workflows. Despite the long-term benefits, projects could face friction in transitioning to co-design.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>AI Acceleration</span>"
    ]
  },
  {
    "objectID": "contents/core/hw_acceleration/hw_acceleration.html#software-for-ai-hardware",
    "href": "contents/core/hw_acceleration/hw_acceleration.html#software-for-ai-hardware",
    "title": "11  AI Acceleration",
    "section": "11.5 Software for AI Hardware",
    "text": "11.5 Software for AI Hardware\nSpecialized hardware accelerators like GPUs, TPUs, and FPGAs are essential to delivering high-performance artificial intelligence applications. However, an extensive software stack is required to leverage these hardware platforms effectively, spanning the entire development and deployment lifecycle. Frameworks and libraries form the backbone of AI hardware, offering sets of robust, pre-built code, algorithms, and functions specifically optimized to perform various AI tasks on different hardware. They are designed to simplify the complexities of utilizing the hardware from scratch, which can be time-consuming and prone to error. Software plays an important role in the following:\n\nProviding programming abstractions and models like CUDA and OpenCL to map computations onto accelerators.\nIntegrating accelerators into popular deep learning frameworks like TensorFlow and PyTorch.\nCompilers and tools to optimize across the hardware-software stack.\nSimulation platforms to model hardware and software together.\nInfrastructure to manage deployment on accelerators.\n\nThis expansive software ecosystem is as important as the hardware in delivering performant and efficient AI applications. This section overviews the tools available at each stack layer to enable developers to build and run AI systems powered by hardware acceleration.\n\n11.5.1 Programming Models\nProgramming models provide abstractions to map computations and data onto heterogeneous hardware accelerators:\n\nCUDA: Nvidia’s parallel programming model to leverage GPUs using extensions to languages like C/C++. Allows launching kernels across GPU cores (Luebke 2008).\nOpenCL: Open standard for writing programs spanning CPUs, GPUs, FPGAs, and other accelerators. Specifies a heterogeneous computing framework (Munshi 2009).\nOpenGL/WebGL: 3D graphics programming interfaces that can map general-purpose code to GPU cores (Segal and Akeley 1999).\nVerilog/VHDL: Hardware description languages (HDLs) used to configure FPGAs as AI accelerators by specifying digital circuits (Gannot and Ligthart 1994).\nTVM: A Compiler framework providing a Python frontend to optimize and map deep learning models onto diverse hardware backends (Chen et al. 2018).\n\n\nLuebke, David. 2008. “CUDA: Scalable Parallel Programming for High-Performance Scientific Computing.” In 2008 5th IEEE International Symposium on Biomedical Imaging: From Nano to Macro, 836–38. IEEE. https://doi.org/10.1109/isbi.2008.4541126.\n\nMunshi, Aaftab. 2009. “The OpenCL Specification.” In 2009 IEEE Hot Chips 21 Symposium (HCS), 1–314. IEEE. https://doi.org/10.1109/hotchips.2009.7478342.\n\nSegal, Mark, and Kurt Akeley. 1999. “The OpenGL Graphics System: A Specification (Version 1.1).”\n\nGannot, G., and M. Ligthart. 1994. “Verilog HDL Based FPGA Design.” In International Verilog HDL Conference, 86–92. IEEE. https://doi.org/10.1109/ivc.1994.323743.\n\nChen, Tianqi, Thierry Moreau, Ziheng Jiang, Lianmin Zheng, Eddie Yan, Haichen Shen, Meghan Cowan, et al. 2018. “TVM: An Automated End-to-End Optimizing Compiler for Deep Learning.” In 13th USENIX Symposium on Operating Systems Design and Implementation (OSDI 18), 578–94.\nKey challenges include expressing parallelism, managing memory across devices, and matching algorithms to hardware capabilities. Abstractions must balance portability with allowing hardware customization. Programming models enable developers to harness accelerators without hardware expertise. These details are discussed in the AI frameworks section.\n\n\n\n\n\n\nExercise 11.1: Software for AI Hardware - TVM\n\n\n\n\n\nWe’ve learned that fancy AI hardware needs special software to work magic. TVM is like a super-smart translator, turning your code into instructions that accelerators understand. In this Colab, we’ll use TVM to make a pretend accelerator called VTA do matrix multiplication super fast. Ready to see how software powers up hardware?\n\n\n\n\n\n\n11.5.2 Libraries and Runtimes\nSpecialized libraries and runtimes provide software abstractions to access and maximize the utilization of AI accelerators:\n\nMath Libraries: Highly optimized implementations of linear algebra primitives like GEMM, FFTs, convolutions, etc., tailored to the target hardware. Nvidia cuBLAS, Intel MKL, and Arm compute libraries are examples.\nFramework Integrations: Libraries to accelerate deep learning frameworks like TensorFlow, PyTorch, and MXNet on supported hardware. For example, cuDNN accelerates CNNs on Nvidia GPUs.\nRuntimes: Software to handle accelerator execution, including scheduling, synchronization, memory management, and other tasks. Nvidia TensorRT is an inference optimizer and runtime.\nDrivers and Firmware: Low-level software to interface with hardware, initialize devices, and handle execution. Vendors like Xilinx provide drivers for their accelerator boards.\n\nFor instance, PyTorch integrators use cuDNN and cuBLAS libraries to accelerate training on Nvidia GPUs. The TensorFlow XLA runtime optimizes and compiles models for accelerators like TPUs. Drivers initialize devices and offload operations.\nThe challenges include efficiently partitioning and scheduling workloads across heterogeneous devices like multi-GPU nodes. Runtimes must also minimize the overhead of data transfers and synchronization.\nLibraries, runtimes, and drivers provide optimized building blocks that deep learning developers can leverage to tap into accelerator performance without hardware programming expertise. Their optimization is essential for production deployments.\n\n\n11.5.3 Optimizing Compilers\nOptimizing compilers is key in extracting maximum performance and efficiency from hardware accelerators for AI workloads. They apply optimizations spanning algorithmic changes, graph-level transformations, and low-level code generation.\n\nAlgorithm Optimization: Techniques like quantization, pruning, and neural architecture search to improve model efficiency and match hardware capabilities.\nGraph Optimizations: Graph-level optimizations like operator fusion, rewriting, and layout transformations to optimize performance on target hardware.\nCode Generation: Generating optimized low-level code for accelerators from high-level models and frameworks.\n\nFor example, the TVM open compiler stack applies quantization for a BERT model targeting Arm GPUs. It fuses pointwise convolution operations and transforms the weight layout to optimize memory access. Finally, it emits optimized OpenGL code to run the GPU workload.\nKey compiler optimizations include maximizing parallelism, improving data locality and reuse, minimizing memory footprint, and exploiting custom hardware operations. Compilers build and optimize machine learning workloads holistically across hardware components like CPUs, GPUs, and other accelerators.\nHowever, efficiently mapping complex models introduces challenges like efficiently partitioning workloads across heterogeneous devices. Production-level compilers also require extensive time tuning on representative workloads. Still, optimizing compilers is indispensable in unlocking the full capabilities of AI accelerators.\n\n\n11.5.4 Simulation and Modeling\nSimulation software is important in hardware-software co-design. It enables joint modeling of proposed hardware architectures and software stacks:\n\nHardware Simulation: Platforms like Gem5 allow detailed simulation of hardware components like pipelines, caches, interconnects, and memory hierarchies. Engineers can model hardware changes without physical prototyping (Binkert et al. 2011).\nSoftware Simulation: Compiler stacks like TVM support the simulation of machine learning workloads to estimate performance on target hardware architectures. This assists with software optimizations.\nCo-simulation: Unified platforms like the SCALE-Sim (Samajdar et al. 2018) integrate hardware and software simulation into a single tool. This enables what-if analysis to quantify the system-level impacts of cross-layer optimizations early in the design cycle.\n\n\nBinkert, Nathan, Bradford Beckmann, Gabriel Black, Steven K. Reinhardt, Ali Saidi, Arkaprava Basu, Joel Hestness, et al. 2011. “The Gem5 Simulator.” ACM SIGARCH Computer Architecture News 39 (2): 1–7. https://doi.org/10.1145/2024716.2024718.\n\nSamajdar, Ananda, Yuhao Zhu, Paul Whatmough, Matthew Mattina, and Tushar Krishna. 2018. “Scale-Sim: Systolic Cnn Accelerator Simulator.” ArXiv Preprint abs/1811.02883. https://arxiv.org/abs/1811.02883.\nFor example, an FPGA-based AI accelerator design could be simulated using Verilog hardware description language and synthesized into a Gem5 model. Verilog is well-suited for describing the digital logic and interconnects of the accelerator architecture. Verilog allows the designer to specify the datapaths, control logic, on-chip memories, and other components implemented in the FPGA fabric. Once the Verilog design is complete, it can be synthesized into a model that simulates the behavior of the hardware, such as using the Gem5 simulator. Gem5 is useful for this task because it allows the modeling of full systems, including processors, caches, buses, and custom accelerators. Gem5 supports interfacing Verilog models of hardware to the simulation, enabling unified system modeling.\nThe synthesized FPGA accelerator model could then have ML workloads simulated using TVM compiled onto it within the Gem5 environment for unified modeling. TVM allows optimized compilation of ML models onto heterogeneous hardware like FPGAs. Running TVM-compiled workloads on the accelerator within the Gem5 simulation provides an integrated way to validate and refine the hardware design, software stack, and system integration before physically realizing the accelerator on a real FPGA.\nThis type of co-simulation provides estimations of overall metrics like throughput, latency, and power to guide co-design before expensive physical prototyping. They also assist with partitioning optimizations between hardware and software to guide design tradeoffs.\nHowever, accuracy in modeling subtle low-level interactions between components is limited. Quantified simulations are estimates but cannot wholly replace physical prototypes and testing. Still, unified simulation and modeling provide invaluable early insights into system-level optimization opportunities during the co-design process.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>AI Acceleration</span>"
    ]
  },
  {
    "objectID": "contents/core/hw_acceleration/hw_acceleration.html#benchmarking-ai-hardware",
    "href": "contents/core/hw_acceleration/hw_acceleration.html#benchmarking-ai-hardware",
    "title": "11  AI Acceleration",
    "section": "11.6 Benchmarking AI Hardware",
    "text": "11.6 Benchmarking AI Hardware\nBenchmarking is a critical process that quantifies and compares the performance of various hardware platforms designed to speed up artificial intelligence applications. It guides purchasing decisions, development focus, and performance optimization efforts for hardware manufacturers and software developers.\nThe benchmarking chapter explores this topic in great detail, explaining why it has become an indispensable part of the AI hardware development cycle and how it impacts the broader technology landscape. Here, we will briefly review the main concepts, but we recommend that you refer to the chapter for more details.\nBenchmarking suites such as MLPerf, Fathom, and AI Benchmark offer a set of standardized tests that can be used across different hardware platforms. These suites measure AI accelerator performance across various neural networks and machine learning tasks, from basic image classification to complex language processing. Providing a common ground for Comparison, they help ensure that performance claims are consistent and verifiable. These “tools” are applied not only to guide the development of hardware but also to ensure that the software stack leverages the full potential of the underlying architecture.\n\nMLPerf: Includes a broad set of benchmarks covering both training (Mattson et al. 2020) and inference (Reddi et al. 2020) for a range of machine learning tasks. Figure 12.5 showcases the diversity of AI use cases covered by MLPerf.\nFathom: Focuses on core operations in deep learning models, emphasizing their execution on different architectures (Adolf et al. 2016).\nAI Benchmark: Targets mobile and consumer devices, assessing AI performance in end-user applications (Ignatov et al. 2018).\n\n\nMattson, Peter, Vijay Janapa Reddi, Christine Cheng, Cody Coleman, Greg Diamos, David Kanter, Paulius Micikevicius, et al. 2020. “MLPerf: An Industry Standard Benchmark Suite for Machine Learning Performance.” IEEE Micro 40 (2): 8–16. https://doi.org/10.1109/mm.2020.2974843.\n\nReddi, Vijay Janapa, Christine Cheng, David Kanter, Peter Mattson, Guenther Schmuelling, Carole-Jean Wu, Brian Anderson, et al. 2020. “MLPerf Inference Benchmark.” In 2020 ACM/IEEE 47th Annual International Symposium on Computer Architecture (ISCA), 446–59. IEEE; IEEE. https://doi.org/10.1109/isca45697.2020.00045.\n\nAdolf, Robert, Saketh Rama, Brandon Reagen, Gu-yeon Wei, and David Brooks. 2016. “Fathom: Reference Workloads for Modern Deep Learning Methods.” In 2016 IEEE International Symposium on Workload Characterization (IISWC), 1–10. IEEE; IEEE. https://doi.org/10.1109/iiswc.2016.7581275.\n\nIgnatov, Andrey, Radu Timofte, William Chou, Ke Wang, Max Wu, Tim Hartley, and Luc Van Gool. 2018. “AI Benchmark: Running Deep Neural Networks on Android Smartphones,” 0–0.\n\n\n\n\n\n\nFigure 11.4: MLPerf Training v3.0 and its uses. Source: Forbes\n\n\n\nBenchmarks also have performance metrics that are the quantifiable measures used to evaluate the effectiveness of AI accelerators. These metrics provide a comprehensive view of an accelerator’s capabilities and are used to guide the design and selection process for AI systems. Common metrics include:\n\nThroughput: Usually measured in operations per second, this metric indicates the volume of computations an accelerator can handle.\nLatency: The time delay from input to output in a system is vital for real-time processing tasks.\nEnergy Efficiency: Calculated as computations per watt, representing the tradeoff between performance and power consumption.\nCost Efficiency: This evaluates the cost of operation relative to performance, an essential metric for budget-conscious deployments.\nAccuracy: In inference tasks, the precision of computations is critical and sometimes balanced against speed.\nScalability: The ability of the system to maintain performance gains as the computational load scales up.\n\nBenchmark results give insights beyond just numbers—they can reveal bottlenecks in the software and hardware stack. For example, benchmarks may show how increased batch size improves GPU utilization by providing more parallelism or how compiler optimizations boost TPU performance. These learnings enable continuous optimization (Zhihao Jia, Zaharia, and Aiken 2019).\n\nJia, Zhihao, Matei Zaharia, and Alex Aiken. 2019. “Beyond Data and Model Parallelism for Deep Neural Networks.” In Proceedings of Machine Learning and Systems 2019, MLSys 2019, Stanford, CA, USA, March 31 - April 2, 2019, edited by Ameet Talwalkar, Virginia Smith, and Matei Zaharia. mlsys.org. https://proceedings.mlsys.org/book/265.pdf.\n\nZhu, Hongyu, Mohamed Akrout, Bojian Zheng, Andrew Pelegris, Anand Jayarajan, Amar Phanishayee, Bianca Schroeder, and Gennady Pekhimenko. 2018. “Benchmarking and Analyzing Deep Neural Network Training.” In 2018 IEEE International Symposium on Workload Characterization (IISWC), 88–100. IEEE; IEEE. https://doi.org/10.1109/iiswc.2018.8573476.\nStandardized benchmarking provides a quantified, comparable evaluation of AI accelerators to inform design, purchasing, and optimization. However, real-world performance validation remains essential as well (H. Zhu et al. 2018).",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>AI Acceleration</span>"
    ]
  },
  {
    "objectID": "contents/core/hw_acceleration/hw_acceleration.html#challenges-and-solutions",
    "href": "contents/core/hw_acceleration/hw_acceleration.html#challenges-and-solutions",
    "title": "11  AI Acceleration",
    "section": "11.7 Challenges and Solutions",
    "text": "11.7 Challenges and Solutions\nAI accelerators offer impressive performance improvements, but significant portability and compatibility challenges often need to be improved in their integration into the broader AI landscape. The crux of the issue lies in the diversity of the AI ecosystem—a vast array of machine learning accelerators, frameworks, and programming languages exist, each with its unique features and requirements.\n\n11.7.1 Portability/Compatibility Issues\nDevelopers frequently encounter difficulties transferring their AI models from one hardware environment to another. For example, a machine learning model developed for a desktop environment in Python using the PyTorch framework, optimized for an Nvidia GPU, may not easily transition to a more constrained device such as the Arduino Nano 33 BLE. This complexity stems from stark differences in programming requirements - Python and PyTorch on the desktop versus a C++ environment on an Arduino, not to mention the shift from x86 architecture to ARM ISA.\nThese divergences highlight the intricacy of portability within AI systems. Moreover, the rapid advancement in AI algorithms and models means that hardware accelerators must continually adapt, creating a moving target for compatibility. The absence of universal standards and interfaces compounds the issue, making deploying AI solutions consistently across various devices and platforms challenging.\n\nSolutions and Strategies\nTo address these hurdles, the AI industry is moving towards several solutions:\n\nStandardization Initiatives\nThe Open Neural Network Exchange (ONNX) is at the forefront of this pursuit, proposing an open and shared ecosystem that promotes model interchangeability. ONNX facilitates the use of AI models across various frameworks, allowing models trained in one environment to be efficiently deployed in another, significantly reducing the need for time-consuming rewrites or adjustments.\n\n\nCross-Platform Frameworks\nComplementing the standardization efforts, cross-platform frameworks such as TensorFlow Lite and PyTorch Mobile have been developed specifically to create cohesion between diverse computational environments ranging from desktops to mobile and embedded devices. These frameworks offer streamlined, lightweight versions of their parent frameworks, ensuring compatibility and functional integrity across different hardware types without sacrificing performance. This ensures that developers can create applications with the confidence that they will work on many devices, bridging a gap that has traditionally posed a considerable challenge in AI development.\n\n\nHardware-agnostic Platforms\nThe rise of hardware-agnostic platforms has also played an important role in democratizing the use of AI. By creating environments where AI applications can be executed on various accelerators, these platforms remove the burden of hardware-specific coding from developers. This abstraction simplifies the development process and opens up new possibilities for innovation and application deployment, free from the constraints of hardware specifications.\n\n\nAdvanced Compilation Tools\nIn addition, the advent of advanced compilation tools like TVM, an end-to-end tensor compiler, offers an optimized path through the jungle of diverse hardware architectures. TVM equips developers with the means to fine-tune machine learning models for a broad spectrum of computational substrates, ensuring optimal performance and avoiding manual model adjustment each time there is a shift in the underlying hardware.\n\n\nCommunity and Industry Collaboration\nThe collaboration between open-source communities and industry consortia cannot be understated. These collective bodies are instrumental in forming shared standards and best practices that all developers and manufacturers can adhere to. Such collaboration fosters a more unified and synergistic AI ecosystem, significantly diminishing the prevalence of portability issues and smoothing the path toward global AI integration and advancement. Through these combined efforts, AI is steadily moving toward a future where seamless model deployment across various platforms becomes a standard rather than an exception.\nSolving the portability challenges is crucial for the AI field to realize the full potential of hardware accelerators in a dynamic and diverse technological landscape. It requires a concerted effort from hardware manufacturers, software developers, and standard bodies to create a more interoperable and flexible environment. With continued innovation and collaboration, the AI community can pave the way for seamless integration and deployment of AI models across many platforms.\n\n\n\n\n11.7.2 Power Consumption Concerns\nPower consumption is a crucial issue in the development and operation of data center AI accelerators, like Graphics Processing Units (GPUs) and Tensor Processing Units (TPUs) (N. P. Jouppi et al. 2017b) (Norrie et al. 2021) (N. Jouppi et al. 2023). These powerful components are the backbone of contemporary AI infrastructure, but their high energy demands contribute to the environmental impact of technology and drive up operational costs significantly. As data processing needs become more complex, with the popularity of AI and deep learning increasing, there’s a pressing demand for GPUs and TPUs that can deliver the necessary computational power more efficiently. The impact of such advancements is two-fold: they can lower these technologies’ environmental footprint and reduce the cost of running AI applications.\n\n———, et al. 2017b. “In-Datacenter Performance Analysis of a Tensor Processing Unit.” In Proceedings of the 44th Annual International Symposium on Computer Architecture, 1–12. ISCA ’17. New York, NY, USA: ACM. https://doi.org/10.1145/3079856.3080246.\n\nNorrie, Thomas, Nishant Patil, Doe Hyun Yoon, George Kurian, Sheng Li, James Laudon, Cliff Young, Norman Jouppi, and David Patterson. 2021. “The Design Process for Google’s Training Chips: Tpuv2 and TPUv3.” IEEE Micro 41 (2): 56–63. https://doi.org/10.1109/mm.2021.3058217.\n\nJouppi, Norm, George Kurian, Sheng Li, Peter Ma, Rahul Nagarajan, Lifeng Nai, Nishant Patil, et al. 2023. “TPU V4: An Optically Reconfigurable Supercomputer for Machine Learning with Hardware Support for Embeddings.” In Proceedings of the 50th Annual International Symposium on Computer Architecture. ISCA ’23. New York, NY, USA: ACM. https://doi.org/10.1145/3579371.3589350.\nEmerging hardware technologies are at the cusp of revolutionizing power efficiency in this sector. Photonic computing, for instance, uses light rather than electricity to carry information, offering a promise of high-speed processing with a fraction of the power usage. We look deeper into this and other innovative technologies in the “Emerging Hardware Technologies” section, exploring their potential to address current power consumption challenges.\nAt the edge of the network, AI accelerators are engineered to process data on devices like smartphones, IoT sensors, and smart wearables. These devices often work under severe power limitations, necessitating a careful balancing act between performance and power usage. A high-performance AI model may provide quick results but at the cost of depleting battery life swiftly and increasing thermal output, which may affect the device’s functionality and durability. The stakes are higher for devices deployed in remote or hard-to-reach areas, where consistent power supply cannot be guaranteed, underscoring the need for low-power-consuming solutions.\nLatency issues further compound the challenge of power efficiency at the edge. Edge AI applications in fields such as autonomous driving and healthcare monitoring require speed, precision, and reliability, as delays in processing can lead to serious safety risks. For these applications, developers must optimize both the AI algorithms and the hardware design to strike an optimal balance between power consumption and latency.\nThis optimization effort is not just about making incremental improvements to existing technologies; it’s about rethinking how and where we process AI tasks. By designing AI accelerators that are both power-efficient and capable of quick processing, we can ensure these devices serve their intended purposes without unnecessary energy use or compromised performance. Such developments could propel the widespread adoption of AI across various sectors, enabling smarter, safer, and more sustainable use of technology.\n\n\n11.7.3 Overcoming Resource Constraints\nResource constraints also pose a significant challenge for Edge AI accelerators, as these specialized hardware and software solutions must deliver robust performance within the limitations of edge devices. Due to power and size limitations, edge AI accelerators often have restricted computation, memory, and storage capacity (L. Zhu et al. 2023). This scarcity of resources necessitates a careful allocation of processing capabilities to execute machine learning models efficiently.\n\nZhu, Ligeng, Lanxiang Hu, Ji Lin, Wei-Ming Chen, Wei-Chen Wang, Chuang Gan, and Song Han. 2023. “PockEngine: Sparse and Efficient Fine-Tuning in a Pocket.” In 56th Annual IEEE/ACM International Symposium on Microarchitecture. ACM. https://doi.org/10.1145/3613424.3614307.\n\nLin, Ji, Jiaming Tang, Haotian Tang, Shang Yang, Xingyu Dang, and Song Han. 2023. “AWQ: Activation-aware Weight Quantization for LLM Compression and Acceleration.” arXiv.\n\nLi, Yuhang, Xin Dong, and Wei Wang. 2020. “Additive Powers-of-Two Quantization: An Efficient Non-Uniform Discretization for Neural Networks.” In 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net. https://openreview.net/forum?id=BkgXT24tDS.\n\nWang, Tianzhe, Kuan Wang, Han Cai, Ji Lin, Zhijian Liu, Hanrui Wang, Yujun Lin, and Song Han. 2020. “APQ: Joint Search for Network Architecture, Pruning and Quantization Policy.” In 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2075–84. IEEE. https://doi.org/10.1109/cvpr42600.2020.00215.\nMoreover, managing constrained resources demands innovative approaches, including model quantization (Lin et al. 2023) (Li, Dong, and Wang 2020), pruning (Wang et al. 2020), and optimizing inference pipelines. Edge AI accelerators must strike a delicate balance between providing meaningful AI functionality and not exhausting available resources while maintaining low power consumption. Overcoming these resource constraints is crucial to ensure the successful deployment of AI at the edge, where many applications, from IoT to mobile devices, rely on efficiently using limited hardware resources to deliver real-time and intelligent decision-making.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>AI Acceleration</span>"
    ]
  },
  {
    "objectID": "contents/core/hw_acceleration/hw_acceleration.html#emerging-technologies",
    "href": "contents/core/hw_acceleration/hw_acceleration.html#emerging-technologies",
    "title": "11  AI Acceleration",
    "section": "11.8 Emerging Technologies",
    "text": "11.8 Emerging Technologies\nThus far, we have discussed AI hardware technology in the context of conventional von Neumann architecture design and CMOS-based implementation. These specialized AI chips offer benefits like higher throughput and power efficiency but rely on traditional computing principles. The relentless growth in demand for AI computing power is driving innovations in integration methods for AI hardware.\nTwo leading approaches have emerged for maximizing compute density—wafer-scale integration and chiplet-based architectures—which we will discuss in this section. Looking much further ahead, we will examine emerging technologies that diverge from conventional architectures and adopt fundamentally different approaches for AI-specialized computing.\nSome of these unconventional paradigms include neuromorphic computing, which mimics biological neural networks; quantum computing, which leverages quantum mechanical effects; and optical computing, which utilizes photons instead of electrons. Beyond novel computing substrates, new device technologies are enabling additional gains through better memory and interconnecting.\nExamples include memristors for in-memory computing and nanophotonics for integrated photonic communication. Together, these technologies offer the potential for orders of magnitude improvements in speed, efficiency, and scalability compared to current AI hardware. We will examine these in this section.\n\n11.8.1 Integration Methods\nIntegration methods refer to the approaches used to combine and interconnect an AI chip or system’s various computational and memory components. By closely linking the key processing elements, integration tries to maximize performance, power efficiency, and density.\nIn the past, AI computing was primarily performed on CPUs and GPUs built using conventional integration methods. These discrete components were manufactured separately and connected together on a board. However, this loose integration creates bottlenecks, such as data transfer overheads.\nAs AI workloads have grown, there is increasing demand for tighter integration between computing, memory, and communication elements. Some key drivers of integration include:\n\nMinimizing data movement: Tight integration reduces latency and power for moving data between components. This improves efficiency.\nCustomization: Tailoring all system components to AI workloads allows optimizations throughout the hardware stack.\nParallelism: Integrating many processing elements enables massively parallel computation.\nDensity: Tighter integration allows more transistors and memory to be packed into a given area.\nCost: Economies of scale from large integrated systems can reduce costs.\n\nIn response, new manufacturing techniques like wafer-scale fabrication and advanced packaging now allow much higher levels of integration. The goal is to create unified, specialized AI compute complexes tailored for deep learning and other AI algorithms. Tighter integration is key to delivering the performance and efficiency needed for the next generation of AI.\n\nWafer-scale AI\nWafer-scale AI takes an extremely integrated approach, manufacturing an entire silicon wafer as one gigantic chip. This differs drastically from conventional CPUs and GPUs, which cut each wafer into many smaller individual chips. Figure 11.5 shows a comparison between Cerebras Wafer Scale Engine 2, which is the largest chip ever built, and the largest GPU. While some GPUs may contain billions of transistors, they still pale in Comparison to the scale of a wafer-size chip with over a trillion transistors.\n\n\n\n\n\n\nFigure 11.5: Wafer-scale vs. GPU. Source: Cerebras.\n\n\n\nThe wafer-scale approach also diverges from more modular system-on-chip designs that still have discrete components communicating by bus. Instead, wafer-scale AI enables full customization and tight integration of computation, memory, and interconnects across the entire die.\nBy designing the wafer as one integrated logic unit, data transfer between elements is minimized. This provides lower latency and power consumption than discrete system-on-chip or chiplet designs. While chiplets can offer flexibility by mixing and matching components, communication between chiplets is challenging. The monolithic nature of wafer-scale integration eliminates these inter-chip communication bottlenecks.\nHowever, the ultra-large-scale also poses difficulties for manufacturability and yield with wafer-scale designs. Defects in any region of the wafer can make (certain parts of) the chip unusable. Specialized lithography techniques are required to produce such large dies. So, wafer-scale integration pursues the maximum performance gains from integration but requires overcoming substantial fabrication challenges.\nVideo 11.1 provides additional context about wafer-scale AI chips.\n\n\n\n\n\n\nImportant 11.1: Wafer-scale AI Chips\n\n\n\n\n\n\n\n\nChiplets for AI\nChiplet design refers to a semiconductor architecture in which a single integrated circuit (IC) is constructed from multiple smaller, individual components known as chiplets. Each chiplet is a self-contained functional block, typically specialized for a specific task or functionality. These chiplets are then interconnected on a larger substrate or package to create a cohesive system.\nFigure 11.6 illustrates this concept. For AI hardware, chiplets enable the mixing of different types of chips optimized for tasks like matrix multiplication, data movement, analog I/O, and specialized memories. This heterogeneous integration differs greatly from wafer-scale integration, where all logic is manufactured as one monolithic chip. Companies like Intel and AMD have adopted chiplet designs for their CPUs.\nChiplets are interconnected using advanced packaging techniques like high-density substrate interposers, 2.5D/3D stacking, and wafer-level packaging. This allows combining chiplets fabricated with different process nodes, specialized memories, and various optimized AI engines.\n\n\n\n\n\n\nFigure 11.6: Chiplet partitioning. Source: Vivet et al. (2021).\n\n\nVivet, Pascal, Eric Guthmuller, Yvain Thonnart, Gael Pillonnet, Cesar Fuguet, Ivan Miro-Panades, Guillaume Moritz, et al. 2021. “IntAct: A 96-Core Processor with Six Chiplets 3D-Stacked on an Active Interposer with Distributed Interconnects and Integrated Power Management.” IEEE J. Solid-State Circuits 56 (1): 79–97. https://doi.org/10.1109/jssc.2020.3036341.\n\n\nSome key advantages of using chiplets for AI include:\n\nFlexibility: Chiplets allow for the combination of different chip types, process nodes, and memories tailored for each function. This is more modular versus a fixed wafer-scale design.\nYield: Smaller chiplets have a higher yield than a gigantic wafer-scale chip. Defects are contained in individual chiplets.\nCost: Leverages existing manufacturing capabilities versus requiring specialized new processes. Reduces costs by reusing mature fabrication.\nCompatibility: Can integrate with more conventional system architectures like PCIe and standard DDR memory interfaces.\n\nHowever, chiplets also face integration and performance challenges:\n\nLower density compared to wafer-scale, as chiplets are limited in size.\nAdded latency when communicating between chiplets versus monolithic integration. Requires optimization for low-latency interconnect.\nAdvanced packaging adds complexity versus wafer-scale integration, though this is arguable.\n\nThe key objective of chiplets is finding the right balance between modular flexibility and integration density for optimal AI performance. Chiplets aim for efficient AI acceleration while working within the constraints of conventional manufacturing techniques. Chiplets take a middle path between the extremes of wafer-scale integration and fully discrete components. This provides practical benefits but may sacrifice some computational density and efficiency versus a theoretical wafer-size system.\n\n\n\n11.8.2 Neuromorphic Computing\nNeuromorphic computing is an emerging field aiming to emulate the efficiency and robustness of biological neural systems for machine learning applications. A key difference from classical Von Neumann architectures is the merging of memory and processing in the same circuit (Schuman et al. 2022; Marković et al. 2020; Furber 2016), as illustrated in Figure 11.7. The structure of the brain inspires this integrated approach. A key advantage is the potential for orders of magnitude improvement in energy-efficient computation compared to conventional AI hardware. For example, estimates project 100x-1000x gains in energy efficiency versus current GPU-based systems for equivalent workloads.\n\nMarković, Danijela, Alice Mizrahi, Damien Querlioz, and Julie Grollier. 2020. “Physics for Neuromorphic Computing.” Nature Reviews Physics 2 (9): 499–510. https://doi.org/10.1038/s42254-020-0208-2.\n\nFurber, Steve. 2016. “Large-Scale Neuromorphic Computing Systems.” J. Neural Eng. 13 (5): 051001. https://doi.org/10.1088/1741-2560/13/5/051001.\n\n\n\n\n\n\nFigure 11.7: Comparison of the von Neumann architecture with the neuromorphic architecture. Source: Schuman et al. (2022).\n\n\nSchuman, Catherine D., Shruti R. Kulkarni, Maryam Parsa, J. Parker Mitchell, Prasanna Date, and Bill Kay. 2022. “Opportunities for Neuromorphic Computing Algorithms and Applications.” Nature Computational Science 2 (1): 10–19. https://doi.org/10.1038/s43588-021-00184-y.\n\n\nIntel and IBM are leading commercial efforts in neuromorphic hardware. Intel’s Loihi and Loihi 2 chips (Davies et al. 2018, 2021) offer programmable neuromorphic cores with on-chip learning. IBM’s Northpole (Modha et al. 2023) device comprises over 100 million magnetic tunnel junction synapses and 68 billion transistors. These specialized chips deliver benefits like low power consumption for edge inference.\n\nDavies, Mike, Narayan Srinivasa, Tsung-Han Lin, Gautham Chinya, Yongqiang Cao, Sri Harsha Choday, Georgios Dimou, et al. 2018. “Loihi: A Neuromorphic Manycore Processor with on-Chip Learning.” IEEE Micro 38 (1): 82–99. https://doi.org/10.1109/mm.2018.112130359.\n\nDavies, Mike, Andreas Wild, Garrick Orchard, Yulia Sandamirskaya, Gabriel A. Fonseca Guerra, Prasad Joshi, Philipp Plank, and Sumedh R. Risbud. 2021. “Advancing Neuromorphic Computing with Loihi: A Survey of Results and Outlook.” Proc. IEEE 109 (5): 911–34. https://doi.org/10.1109/jproc.2021.3067593.\n\nModha, Dharmendra S., Filipp Akopyan, Alexander Andreopoulos, Rathinakumar Appuswamy, John V. Arthur, Andrew S. Cassidy, Pallab Datta, et al. 2023. “Neural Inference at the Frontier of Energy, Space, and Time.” Science 382 (6668): 329–35. https://doi.org/10.1126/science.adh1174.\n\nMaass, Wolfgang. 1997. “Networks of Spiking Neurons: The Third Generation of Neural Network Models.” Neural Networks 10 (9): 1659–71. https://doi.org/10.1016/s0893-6080(97)00011-7.\nSpiking neural networks (SNNs) (Maass 1997) are computational models for neuromorphic hardware. Unlike deep neural networks communicating via continuous values, SNNs use discrete spikes that are more akin to biological neurons. This allows efficient event-based computation rather than constant processing. Additionally, SNNs consider the temporal and spatial characteristics of input data. This better mimics biological neural networks, where the timing of neuronal spikes plays an important role.\nHowever, training SNNs remains challenging due to the added temporal complexity. Figure 11.8 provides an overview of the spiking methodology: (a) illustration of a neuron; (b) Measuring an action potential propagated along the axon of a neuron. Only the action potential is detectable along the axon; (c) The neuron’s spike is approximated with a binary representation; (d) Event-Driven Processing; (e) Active Pixel Sensor and Dynamic Vision Sensor.\n\n\n\n\n\n\nFigure 11.8: Neuromorphic spiking. Source: Eshraghian et al. (2023).\n\n\nEshraghian, Jason K., Max Ward, Emre O. Neftci, Xinxin Wang, Gregor Lenz, Girish Dwivedi, Mohammed Bennamoun, Doo Seok Jeong, and Wei D. Lu. 2023. “Training Spiking Neural Networks Using Lessons from Deep Learning.” Proc. IEEE 111 (9): 1016–54. https://doi.org/10.1109/jproc.2023.3308088.\n\n\nYou can also watch Video 11.2 linked below for a more detailed explanation.\n\n\n\n\n\n\nImportant 11.2: Neuromorphic Computing\n\n\n\n\n\n\nSpecialized nanoelectronic devices called memristors (Chua 1971) are synaptic components in neuromorphic systems. Memristors act as nonvolatile memory with adjustable conductance, emulating the plasticity of real synapses. Memristors enable in-situ learning without separate data transfers by combining memory and processing functions. However, memristor technology has yet to reach maturity and scalability for commercial hardware.\n\nChua, L. 1971. “Memristor-the Missing Circuit Element.” #IEEE_J_CT# 18 (5): 507–19. https://doi.org/10.1109/tct.1971.1083337.\nThe integration of photonics with neuromorphic computing (Shastri et al. 2021) has recently emerged as an active research area. Using light for computation and communication allows high speeds and reduced energy consumption. However, fully realizing photonic neuromorphic systems requires overcoming design and integration challenges.\nNeuromorphic computing offers promising capabilities for efficient edge inference but faces obstacles around training algorithms, nanodevice integration, and system design. Ongoing multidisciplinary research across computer science, engineering, materials science, and physics will be key to unlocking this technology’s full potential for AI use cases.\n\n\n11.8.3 Analog Computing\nAnalog computing is an emerging approach that uses analog signals and components like capacitors, inductors, and amplifiers rather than digital logic for computing. It represents information as continuous electrical signals instead of discrete 0s and 1s. This allows the computation to directly reflect the analog nature of real-world data, avoiding digitization errors and overhead.\nAnalog computing has generated renewed interest in efficient AI hardware, particularly for inference directly on low-power edge devices. Analog circuits, such as multiplication and summation at the core of neural networks, can be used with very low energy consumption. This makes analog well-suited for deploying ML models on energy-constrained end nodes. Startups like Mythic are developing analog AI accelerators.\nWhile analog computing was popular in early computers, the boom of digital logic led to its decline. However, analog is compelling for niche applications requiring extreme efficiency (Haensch, Gokmen, and Puri 2019). It contrasts with digital neuromorphic approaches that still use digital spikes for computation. Analog may allow lower precision computation but requires expertise in analog circuit design. Tradeoffs around precision, programming complexity, and fabrication costs remain active research areas.\n\nHaensch, Wilfried, Tayfun Gokmen, and Ruchir Puri. 2019. “The Next Generation of Deep Learning Hardware: Analog Computing.” Proc. IEEE 107 (1): 108–22. https://doi.org/10.1109/jproc.2018.2871057.\n\nHazan, Avi, and Elishai Ezra Tsur. 2021. “Neuromorphic Analog Implementation of Neural Engineering Framework-Inspired Spiking Neuron for High-Dimensional Representation.” Front. Neurosci. 15 (February): 627221. https://doi.org/10.3389/fnins.2021.627221.\nNeuromorphic computing, which emulates biological neural systems for efficient ML inference, can use analog circuits to implement the key components and behaviors of brains. For example, researchers have designed analog circuits to model neurons and synapses using capacitors, transistors, and operational amplifiers (Hazan and Ezra Tsur 2021). The capacitors can exhibit the spiking dynamics of biological neurons, while the amplifiers and transistors provide a weighted summation of inputs to mimic dendrites. Variable resistor technologies like memristors can realize analog synapses with spike-timing-dependent plasticity, which can strengthen or weaken connections based on spiking activity.\nStartups like SynSense have developed analog neuromorphic chips containing these biomimetic components (Bains 2020). This analog approach results in low power consumption and high scalability for edge devices versus complex digital SNN implementations.\n\nBains, Sunny. 2020. “The Business of Building Brains.” Nature Electronics 3 (7): 348–51. https://doi.org/10.1038/s41928-020-0449-1.\nHowever, training analog SNNs on chips remains an open challenge. Overall, analog realization is a promising technique for delivering the efficiency, scalability, and biological plausibility envisioned with neuromorphic computing. The physics of analog components combined with neural architecture design could improve inference efficiency over conventional digital neural networks.\n\n\n11.8.4 Flexible Electronics\nWhile much of the new hardware technology in the ML workspace has been focused on optimizing and making systems more efficient, there’s a parallel trajectory aiming to adapt hardware for specific applications (Gates 2009; Musk et al. 2019; Tang et al. 2023; Tang, He, and Liu 2022; Kwon and Dong 2022). One such avenue is the development of flexible electronics for AI use cases.\n\nGates, Byron D. 2009. “Flexible Electronics.” Science 323 (5921): 1566–67. https://doi.org/10.1126/science.1171230.\n\nTang, Xin, Hao Shen, Siyuan Zhao, Na Li, and Jia Liu. 2023. “Flexible Braincomputer Interfaces.” Nature Electronics 6 (2): 109–18. https://doi.org/10.1038/s41928-022-00913-9.\n\nTang, Xin, Yichun He, and Jia Liu. 2022. “Soft Bioelectronics for Cardiac Interfaces.” Biophysics Reviews 3 (1). https://doi.org/10.1063/5.0069516.\nFlexible electronics refer to electronic circuits and devices fabricated on flexible plastic or polymer substrates rather than rigid silicon. Unlike conventional rigid boards and chips, this allows the electronics to bend, twist, and conform to irregular shapes. Figure 11.9 shows an example of a flexible device prototype that wirelessly measures body temperature, which can be seamlessly integrated into clothing or skin patches. The flexibility and bendability of emerging electronic materials allow them to be integrated into thin, lightweight form factors that are well-suited for embedded AI and TinyML applications.\nFlexible AI hardware can conform to curvy surfaces and operate efficiently with microwatt power budgets. Flexibility also enables rollable or foldable form factors to minimize device footprint and weight, ideal for small, portable smart devices and wearables incorporating TinyML. Another key advantage of flexible electronics compared to conventional technologies is lower manufacturing costs and simpler fabrication processes, which could democratize access to these technologies. While silicon masks and fabrication costs typically cost millions of dollars, flexible hardware typically costs only tens of cents to manufacture (Huang et al. 2011; Biggs et al. 2021). The potential to fabricate flexible electronics directly onto plastic films using high-throughput printing and coating processes can reduce costs and improve manufacturability at scale versus rigid AI chips (Musk et al. 2019).\n\nHuang, Tsung-Ching, Kenjiro Fukuda, Chun-Ming Lo, Yung-Hui Yeh, Tsuyoshi Sekitani, Takao Someya, and Kwang-Ting Cheng. 2011. “Pseudo-CMOS: A Design Style for Low-Cost and Robust Flexible Electronics.” IEEE Trans. Electron Devices 58 (1): 141–50. https://doi.org/10.1109/ted.2010.2088127.\n\nBiggs, John, James Myers, Jedrzej Kufel, Emre Ozer, Simon Craske, Antony Sou, Catherine Ramsdale, Ken Williamson, Richard Price, and Scott White. 2021. “A Natively Flexible 32-Bit Arm Microprocessor.” Nature 595 (7868): 532–36. https://doi.org/10.1038/s41586-021-03625-w.\n\n\n\n\n\n\nFigure 11.9: Flexible device prototype. Source: Jabil Circuit.\n\n\n\nThe field is enabled by advances in organic semiconductors and nanomaterials that can be deposited on thin, flexible films. However, fabrication remains challenging compared to mature silicon processes. Flexible circuits currently typically exhibit lower performance than rigid equivalents. Still, they promise to transform electronics into lightweight, bendable materials.\nFlexible electronics use cases are well-suited for intimate integration with the human body. Potential medical AI applications include bio-integrated sensors, soft assistive robots, and implants that monitor or stimulate the nervous system intelligently. Specifically, flexible electrode arrays could enable higher-density, less-invasive neural interfaces compared to rigid equivalents.\nTherefore, flexible electronics are ushering in a new era of wearables and body sensors, largely due to innovations in organic transistors. These components allow for more lightweight and bendable electronics, ideal for wearables, electronic skin, and body-conforming medical devices.\nThey are well-suited for bioelectronic devices in terms of biocompatibility, opening avenues for applications in brain and cardiac interfaces. For example, research in flexible brain-computer interfaces and soft bioelectronics for cardiac applications demonstrates the potential for wide-ranging medical applications.\nCompanies and research institutions are not only developing and investing great amounts of resources in flexible electrodes, as showcased in Neuralink’s work (Musk et al. 2019). Still, they are also pushing the boundaries to integrate machine learning models within the systems (Kwon and Dong 2022). These smart sensors aim for a seamless, long-lasting symbiosis with the human body.\n\nMusk, Elon et al. 2019. “An Integrated Brain-Machine Interface Platform with Thousands of Channels.” J. Med. Internet Res. 21 (10): e16194. https://doi.org/10.2196/16194.\n\nKwon, Sun Hwa, and Lin Dong. 2022. “Flexible Sensors and Machine Learning for Heart Monitoring.” Nano Energy 102 (November): 107632. https://doi.org/10.1016/j.nanoen.2022.107632.\n\nSegura Anaya, L. H., Abeer Alsadoon, N. Costadopoulos, and P. W. C. Prasad. 2017. “Ethical Implications of User Perceptions of Wearable Devices.” Sci. Eng. Ethics 24 (1): 1–28. https://doi.org/10.1007/s11948-017-9872-8.\n\nGoodyear, Victoria A. 2017. “Social Media, Apps and Wearable Technologies: Navigating Ethical Dilemmas and Procedures.” Qualitative Research in Sport, Exercise and Health 9 (3): 285–302. https://doi.org/10.1080/2159676x.2017.1303790.\n\nFarah, Martha J. 2005. “Neuroethics: The Practical and the Philosophical.” Trends Cogn. Sci. 9 (1): 34–40. https://doi.org/10.1016/j.tics.2004.12.001.\n\nRoskies, Adina. 2002. “Neuroethics for the New Millenium.” Neuron 35 (1): 21–23. https://doi.org/10.1016/s0896-6273(02)00763-8.\nEthically, incorporating smart, machine-learning-driven sensors within the body raises important questions. Issues surrounding data privacy, informed consent, and the long-term societal implications of such technologies are the focus of ongoing work in neuroethics and bioethics (Segura Anaya et al. 2017; Goodyear 2017; Farah 2005; Roskies 2002). The field is progressing at a pace that necessitates parallel advancements in ethical frameworks to guide the responsible development and deployment of these technologies. While there are limitations and ethical hurdles to overcome, the prospects for flexible electronics are expansive and hold immense promise for future research and applications.\n\n\n11.8.5 Memory Technologies\nMemory technologies are critical to AI hardware, but conventional DDR DRAM and SRAM create bottlenecks. AI workloads require high bandwidth (&gt;1 TB/s). Extreme scientific applications of AI require extremely low latency (&lt;50 ns) to feed data to compute units (Duarte et al. 2022), high density (&gt;128Gb) to store large model parameters and data sets, and excellent energy efficiency (&lt;100 fJ/b) for embedded use (Verma et al. 2019). New memories are needed to meet these demands. Emerging options include several new technologies:\n\nDuarte, Javier, Nhan Tran, Ben Hawks, Christian Herwig, Jules Muhizi, Shvetank Prakash, and Vijay Janapa Reddi. 2022. “FastML Science Benchmarks: Accelerating Real-Time Scientific Edge Machine Learning.” ArXiv Preprint abs/2207.07958. https://arxiv.org/abs/2207.07958.\n\nVerma, Naveen, Hongyang Jia, Hossein Valavi, Yinqi Tang, Murat Ozatay, Lung-Yen Chen, Bonan Zhang, and Peter Deaville. 2019. “In-Memory Computing: Advances and Prospects.” IEEE Solid-State Circuits Mag. 11 (3): 43–55. https://doi.org/10.1109/mssc.2019.2922889.\n\nResistive RAM (ReRAM) can improve density with simple, passive arrays. However, challenges around variability remain (Chi et al. 2016).\nPhase change memory (PCM) exploits the unique properties of chalcogenide glass. Crystalline and amorphous phases have different resistances. Intel’s Optane DCPMM provides fast (100ns), high endurance PCM. However, challenges include limited write cycles and high reset current (Burr et al. 2016).\n3D stacking can also boost memory density and bandwidth by vertically integrating memory layers with TSV interconnects (Loh 2008). For example, HBM provides 1024-bit wide interfaces.\n\n\nBurr, Geoffrey W., Matthew J. BrightSky, Abu Sebastian, Huai-Yu Cheng, Jau-Yi Wu, Sangbum Kim, Norma E. Sosa, et al. 2016. “Recent Progress in Phase-Change?Pub _Newline ?Memory Technology.” IEEE Journal on Emerging and Selected Topics in Circuits and Systems 6 (2): 146–62. https://doi.org/10.1109/jetcas.2016.2547718.\n\nLoh, Gabriel H. 2008. “3D-Stacked Memory Architectures for Multi-Core Processors.” ACM SIGARCH Computer Architecture News 36 (3): 453–64. https://doi.org/10.1145/1394608.1382159.\nNew memory technologies, with their innovative cell architectures and materials, are critical to unlocking the next level of AI hardware performance and efficiency. Realizing their benefits in commercial systems remains an ongoing challenge.\nIn-memory computing is gaining traction as a promising avenue for optimizing machine learning and high-performance computing workloads. At its core, the technology co-locates data storage and computation to improve energy efficiency and reduce latency Wong et al. (2012). Two key technologies under this umbrella are Resistive RAM (ReRAM) and Processing-In-Memory (PIM).\n\nWong, H.-S. Philip, Heng-Yuan Lee, Shimeng Yu, Yu-Sheng Chen, Yi Wu, Pang-Shiu Chen, Byoungil Lee, Frederick T. Chen, and Ming-Jinn Tsai. 2012. “MetalOxide RRAM.” Proc. IEEE 100 (6): 1951–70. https://doi.org/10.1109/jproc.2012.2190369.\n\nChi, Ping, Shuangchen Li, Cong Xu, Tao Zhang, Jishen Zhao, Yongpan Liu, Yu Wang, and Yuan Xie. 2016. “Prime: A Novel Processing-in-Memory Architecture for Neural Network Computation in ReRAM-Based Main Memory.” ACM SIGARCH Computer Architecture News 44 (3): 27–39. https://doi.org/10.1145/3007787.3001140.\nReRAM (Wong et al. 2012) and PIM (Chi et al. 2016) are the backbones for in-memory computing, storing and computing data in the same location. ReRAM focuses on issues of uniformity, endurance, retention, multi-bit operation, and scalability. On the other hand, PIM involves CPU units integrated directly into memory arrays, specialized for tasks like matrix multiplication, which are central in AI computations.\nThese technologies find applications in AI workloads and high-performance computing, where the synergy of storage and computation can lead to significant performance gains. The architecture is particularly useful for compute-intensive tasks common in machine learning models.\nWhile in-memory computing technologies like ReRAM and PIM offer exciting prospects for efficiency and performance, they come with their own challenges, such as data uniformity and scalability issues in ReRAM (Imani, Rahimi, and S. Rosing 2016). Nonetheless, the field is ripe for innovation, and addressing these limitations can open new frontiers in AI and high-performance computing.\n\nImani, Mohsen, Abbas Rahimi, and Tajana S. Rosing. 2016. “Resistive Configurable Associative Memory for Approximate Computing.” In Proceedings of the 2016 Design, Automation &Amp; Test in Europe Conference &Amp; Exhibition (DATE), 1327–32. IEEE; Research Publishing Services. https://doi.org/10.3850/9783981537079_0454.\n\n\n11.8.6 Optical Computing\nIn AI acceleration, a burgeoning area of interest lies in novel technologies that deviate from traditional paradigms. Some emerging technologies mentioned above, such as flexible electronics, in-memory computing, or even neuromorphic computing, are close to becoming a reality, given their ground-breaking innovations and applications. One of the promising and leading next-gen frontiers is optical computing technologies H. Zhou et al. (2022). Companies like LightMatter are pioneering the use of light photonics for calculations, thereby utilizing photons instead of electrons for data transmission and computation.\n\nZhou, Hailong, Jianji Dong, Junwei Cheng, Wenchan Dong, Chaoran Huang, Yichen Shen, Qiming Zhang, et al. 2022. “Photonic Matrix Multiplication Lights up Photonic Accelerator and Beyond.” Light: Science &Amp; Applications 11 (1): 30. https://doi.org/10.1038/s41377-022-00717-8.\n\nShastri, Bhavin J., Alexander N. Tait, T. Ferreira de Lima, Wolfram H. P. Pernice, Harish Bhaskaran, C. D. Wright, and Paul R. Prucnal. 2021. “Photonics for Artificial Intelligence and Neuromorphic Computing.” Nat. Photonics 15 (2): 102–14. https://doi.org/10.1038/s41566-020-00754-y.\nOptical computing utilizes photons and photonic devices rather than traditional electronic circuits for computing and data processing. It takes inspiration from fiber optic communication links that rely on light for fast, efficient data transfer (Shastri et al. 2021). Light can propagate with much less loss than semiconductors’ electrons, enabling inherent speed and efficiency benefits.\nSome specific advantages of optical computing include:\n\nHigh throughput: Photons can transmit with bandwidths &gt;100 Tb/s using wavelength division multiplexing.\nLow latency: Photons interact on femtosecond timescales, millions faster than silicon transistors.\nParallelism: Multiple data signals can propagate simultaneously through the same optical medium.\nLow power: Photonic circuits utilizing waveguides and resonators can achieve complex logic and memory with only microwatts of power.\n\nHowever, optical computing currently faces significant challenges:\n\nLack of optical memory equivalent to electronic RAM\nRequires conversion between optical and electrical domains.\nLimited set of available optical components compared to rich electronics ecosystem.\nImmature integration methods to combine photonics with traditional CMOS chips.\nComplex programming models required to handle parallelism.\n\nAs a result, optical computing is still in the very early research stage despite its promising potential. However, technical breakthroughs could enable it to complement electronics and unlock performance gains for AI workloads. Companies like Lightmatter are pioneering early optical AI accelerators. In the long term, if key challenges are overcome, it could represent a revolutionary computing substrate.\n\n\n11.8.7 Quantum Computing\nQuantum computers leverage unique phenomena of quantum physics, like superposition and entanglement, to represent and process information in ways not possible classically. Instead of binary bits, the fundamental unit is the quantum bit or qubit. Unlike classical bits, which are limited to 0 or 1, qubits can exist simultaneously in a superposition of both states due to quantum effects.\nMultiple qubits can also be entangled, leading to exponential information density but introducing probabilistic results. Superposition enables parallel computation on all possible states, while entanglement allows nonlocal correlations between qubits. Figure 11.10 visually conveys the differences between classical bits in computing and quantum bits (qubits).\n\n\n\n\n\n\nFigure 11.10: Qubits, the building blocks of quantum computing. Source: Microsoft\n\n\n\nQuantum algorithms carefully manipulate these inherently quantum mechanical effects to solve problems like optimization or search more efficiently than their classical counterparts in theory.\n\nFaster training of deep neural networks by exploiting quantum parallelism for linear algebra operations.\nEfficient quantum ML algorithms make use of the unique capabilities of qubits.\nQuantum neural networks with inherent quantum effects baked into the model architecture.\nQuantum optimizers leveraging quantum annealing or adiabatic algorithms for combinatorial optimization problems.\n\nHowever, quantum states are fragile and prone to errors that require error-correcting protocols. The non-intuitive nature of quantum programming also introduces challenges not present in classical computing.\n\nNoisy and fragile quantum bits are difficult to scale up. The largest quantum computer today has less than 1000 qubits.\nRestricted set of available quantum gates and circuits relative to classical programming.\nLack of datasets and benchmarks to evaluate quantum ML in practical domains.\n\nWhile meaningful quantum advantage for ML remains far off, active research at companies like D-Wave, Rigetti, and IonQ is advancing quantum computer engineering and quantum algorithms. Major technology companies like Google, IBM, and Microsoft are actively exploring quantum computing. Google recently announced a 72-qubit quantum processor called Bristlecone and plans to build a 49-qubit commercial quantum system. Microsoft also has an active research program in topological quantum computing and collaborates with quantum startup IonQ\nQuantum techniques may first make inroads into optimization before more generalized ML adoption. Realizing quantum ML’s full potential awaits major milestones in quantum hardware development and ecosystem maturity. Figure 11.11 illustratively compares quantum computing and classical computing.\n\n\n\n\n\n\nFigure 11.11: Comparing quantum computing with classical computing. Source: Devopedia",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>AI Acceleration</span>"
    ]
  },
  {
    "objectID": "contents/core/hw_acceleration/hw_acceleration.html#future-trends",
    "href": "contents/core/hw_acceleration/hw_acceleration.html#future-trends",
    "title": "11  AI Acceleration",
    "section": "11.9 Future Trends",
    "text": "11.9 Future Trends\nIn this chapter, the primary focus has been on designing specialized hardware optimized for machine learning workloads and algorithms. This discussion encompassed the tailored architectures of GPUs and TPUs for neural network training and inference. However, an emerging research direction is leveraging machine learning to facilitate the hardware design process itself.\nThe hardware design process involves many complex stages, including specification, high-level modeling, simulation, synthesis, verification, prototyping, and fabrication. Much of this process traditionally requires extensive human expertise, effort, and time. However, recent advances in machine learning are enabling parts of the hardware design workflow to be automated and enhanced using ML techniques.\nSome examples of how ML is transforming hardware design include:\n\nAutomated circuit synthesis using reinforcement learning: Rather than hand-crafting transistor-level designs, ML agents such as reinforcement learning can learn to connect logic gates and generate circuit layouts automatically. This can accelerate the time-consuming synthesis process.\nML-based hardware simulation and emulation: Deep neural network models can be trained to predict how a hardware design will perform under different conditions. For instance, deep learning models can be trained to predict cycle counts for given workloads. This allows faster and more accurate simulation than traditional RTL simulations.\nAutomated chip floorplanning using ML algorithms: Chip floorplanning involves optimally placing different components on a die. Evolutionary algorithms like genetic algorithms and other ML algorithms like reinforcement learning are used to explore floorplan options. This can significantly improve manual floorplanning placements in terms of faster turnaround time and quality of placements.\nML-driven architecture optimization: Novel hardware architectures, like those for efficient ML accelerators, can be automatically generated and optimized by searching the architectural design space. Machine learning algorithms can effectively search large architectural design spaces.\n\nApplying ML to hardware design automation holds enormous promise to make the process faster, cheaper, and more efficient. It opens up design possibilities that would require more than manual design. The use of ML in hardware design is an area of active research and early deployment, and we will study the techniques involved and their transformative potential.\n\n11.9.1 ML for Hardware Design Automation\nA major opportunity for machine learning in hardware design is automating parts of the complex and tedious design workflow. Hardware design automation (HDA) broadly refers to using ML techniques like reinforcement learning, genetic algorithms, and neural networks to automate tasks like synthesis, verification, floorplanning, and more. Here are a few examples of where ML for HDA shows real promise:\n\nAutomated circuit synthesis: Circuit synthesis involves converting a high-level description of desired logic into an optimized gate-level netlist implementation. This complex process has many design considerations and tradeoffs. ML agents can be trained through reinforcement learning G. Zhou and Anderson (2023) to explore the design space and automatically output optimized syntheses. Startups like Symbiotic EDA are bringing this technology to market.\nAutomated chip floorplanning: Floorplanning refers to strategically placing different components on a chip die area. Search algorithms like genetic algorithms (Valenzuela and Wang 2000) and reinforcement learning (Mirhoseini et al. (2021), Agnesina et al. (2023)) can be used to automate floorplan optimization to minimize wire length, power consumption, and other objectives. These automated ML-assisted floor planners are extremely valuable as chip complexity increases.\nML hardware simulators: Training deep neural network models to predict how hardware designs will perform as simulators can accelerate the simulation process by over 100x compared to traditional architectural and RTL simulations.\nAutomated code translation: Converting hardware description languages like Verilog to optimized RTL implementations is critical but time-consuming. ML models can be trained to act as translator agents and automate this process.\n\n\nZhou, Guanglei, and Jason H. Anderson. 2023. “Area-Driven FPGA Logic Synthesis Using Reinforcement Learning.” In Proceedings of the 28th Asia and South Pacific Design Automation Conference, 159–65. ACM. https://doi.org/10.1145/3566097.3567894.\n\nValenzuela, Christine L, and Pearl Y Wang. 2000. “A Genetic Algorithm for VLSI Floorplanning.” In Parallel Problem Solving from Nature PPSN VI: 6th International Conference Paris, France, September 1820, 2000 Proceedings 6, 671–80. Springer.\n\nMirhoseini, Azalia, Anna Goldie, Mustafa Yazgan, Joe Wenjie Jiang, Ebrahim Songhori, Shen Wang, Young-Joon Lee, et al. 2021. “A Graph Placement Methodology for Fast Chip Design.” Nature 594 (7862): 207–12. https://doi.org/10.1038/s41586-021-03544-w.\n\nAgnesina, Anthony, Puranjay Rajvanshi, Tian Yang, Geraldo Pradipta, Austin Jiao, Ben Keller, Brucek Khailany, and Haoxing Ren. 2023. “AutoDMP: Automated DREAMPlace-Based Macro Placement.” In Proceedings of the 2023 International Symposium on Physical Design, 149–57. ACM. https://doi.org/10.1145/3569052.3578923.\nThe benefits of HDA using ML are reduced design time, superior optimizations, and exploration of design spaces too complex for manual approaches. This can accelerate hardware development and lead to better designs.\nChallenges include limits of ML generalization, the black-box nature of some techniques, and accuracy tradeoffs. However, research is rapidly advancing to address these issues and make HDA ML solutions robust and reliable for production use. HDA provides a major avenue for ML to transform hardware design.\n\n\n11.9.2 ML-Based Hardware Simulation and Verification\nSimulating and verifying hardware designs is critical before manufacturing to ensure the design behaves as intended. Traditional approaches like register-transfer level (RTL) simulation are complex and time-consuming. ML introduces new opportunities to improve hardware simulation and verification. Some examples include:\n\nSurrogate modeling for simulation: Highly accurate surrogate models of a design can be built using neural networks. These models predict outputs from inputs much faster than RTL simulation, enabling fast design space exploration. Companies like Ansys use this technique.\nML simulators: Large neural network models can be trained on RTL simulations to learn to mimic the functionality of a hardware design. Once trained, the NN model can be a highly efficient simulator for regression testing and other tasks. Graphcore has demonstrated over 100x speedup with this approach.\nFormal verification using ML: Formal verification mathematically proves properties about a design. ML techniques can help generate verification properties and learn to solve the complex formal proofs needed, automating parts of this challenging process. Startups like Cortical.io are bringing formal ML verification solutions to the market.\nBug detection: ML models can be trained to process hardware designs and identify potential issues. This assists human designers in inspecting complex designs and finding bugs. Facebook has shown bug detection models for their server hardware.\n\nThe key benefits of applying ML to simulation and verification are faster design validation turnaround times, more rigorous testing, and reduced human effort. Challenges include verifying ML model correctness and handling corner cases. ML promises to accelerate testing workflows significantly.\n\n\n11.9.3 ML for Efficient Hardware Architectures\nA key goal is designing hardware architectures optimized for performance, power, and efficiency. ML introduces new techniques to automate and improve architecture design space exploration for general-purpose and specialized hardware like ML accelerators. Some promising examples include:\n\nArchitecture search for hardware: Search techniques like evolutionary algorithms (Kao and Krishna 2020), Bayesian optimization (Reagen et al. (2017), Bhardwaj et al. (2020)), reinforcement learning (Kao, Jeong, and Krishna (2020), Krishnan et al. (2022)) can automatically generate novel hardware architectures by mutating and mixing design attributes like cache size, number of parallel units, memory bandwidth, and so on. This allows for efficient navigation of large design spaces.\nPredictive modeling for optimization: ML models can be trained to predict hardware performance, power, and efficiency metrics for a given architecture. These become “surrogate models” (Krishnan et al. 2023) for fast optimization and space exploration by substituting lengthy simulations.\nSpecialized accelerator optimization: For specialized chips like tensor processing units for AI, automated architecture search techniques based on ML algorithms (D. Zhang et al. 2022) show promise for finding fast, efficient designs.\n\n\nKao, Sheng-Chun, and Tushar Krishna. 2020. “Gamma: Automating the HW Mapping of DNN Models on Accelerators via Genetic Algorithm.” In Proceedings of the 39th International Conference on Computer-Aided Design, 1–9. ACM. https://doi.org/10.1145/3400302.3415639.\n\nReagen, Brandon, Jose Miguel Hernandez-Lobato, Robert Adolf, Michael Gelbart, Paul Whatmough, Gu-Yeon Wei, and David Brooks. 2017. “A Case for Efficient Accelerator Design Space Exploration via Bayesian Optimization.” In 2017 IEEE/ACM International Symposium on Low Power Electronics and Design (ISLPED), 1–6. IEEE; IEEE. https://doi.org/10.1109/islped.2017.8009208.\n\nBhardwaj, Kshitij, Marton Havasi, Yuan Yao, David M. Brooks, José Miguel Hernández-Lobato, and Gu-Yeon Wei. 2020. “A Comprehensive Methodology to Determine Optimal Coherence Interfaces for Many-Accelerator SoCs.” In Proceedings of the ACM/IEEE International Symposium on Low Power Electronics and Design, 145–50. ACM. https://doi.org/10.1145/3370748.3406564.\n\nKao, Sheng-Chun, Geonhwa Jeong, and Tushar Krishna. 2020. “ConfuciuX: Autonomous Hardware Resource Assignment for DNN Accelerators Using Reinforcement Learning.” In 2020 53rd Annual IEEE/ACM International Symposium on Microarchitecture (MICRO), 622–36. IEEE; IEEE. https://doi.org/10.1109/micro50266.2020.00058.\n\nKrishnan, Srivatsan, Natasha Jaques, Shayegan Omidshafiei, Dan Zhang, Izzeddin Gur, Vijay Janapa Reddi, and Aleksandra Faust. 2022. “Multi-Agent Reinforcement Learning for Microprocessor Design Space Exploration.” https://arxiv.org/abs/2211.16385.\n\nZhang, Dan, Safeen Huda, Ebrahim Songhori, Kartik Prabhu, Quoc Le, Anna Goldie, and Azalia Mirhoseini. 2022. “A Full-Stack Search Technique for Domain Optimized Deep Learning Accelerators.” In Proceedings of the 27th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, 27–42. ASPLOS ’22. New York, NY, USA: ACM. https://doi.org/10.1145/3503222.3507767.\nThe benefits of using ML include superior design space exploration, automated optimization, and reduced manual effort. Challenges include long training times for some techniques and local optima limitations. However, ML for hardware architecture holds great potential for unlocking performance and efficiency gains.\n\n\n11.9.4 ML to Optimize Manufacturing and Reduce Defects\nOnce a hardware design is complete, it moves to manufacturing. However, variability and defects during manufacturing can impact yields and quality. ML techniques are now being applied to improve fabrication processes and reduce defects. Some examples include:\n\nPredictive maintenance: ML models can analyze equipment sensor data over time and identify signals that predict maintenance needs before failure. This enables proactive upkeep, which can be very handy in the costly fabrication process.\nProcess optimization: Supervised learning models can be trained on process data to identify factors that lead to low yields. The models can then optimize parameters to improve yields, throughput, or consistency.\nYield prediction: By analyzing test data from fabricated designs using techniques like regression trees, ML models can predict yields early in production, allowing process adjustments.\nDefect detection: Computer vision ML techniques can be applied to images of designs to identify defects invisible to the human eye. This enables precision quality control and root cause analysis.\nProactive failure analysis: ML models can help predict, diagnose, and prevent issues that lead to downstream defects and failures by analyzing structured and unstructured process data.\n\nApplying ML to manufacturing enables process optimization, real-time quality control, predictive maintenance, and higher yields. Challenges include managing complex manufacturing data and variations. But ML is poised to transform semiconductor manufacturing.\n\n\n11.9.5 Toward Foundation Models for Hardware Design\nAs we have seen, machine learning is opening up new possibilities across the hardware design workflow, from specification to manufacturing. However, current ML techniques are still narrow in scope and require extensive domain-specific engineering. The long-term vision is the development of general artificial intelligence systems that can be applied with versatility across hardware design tasks.\nTo fully realize this vision, investment, and research are needed to develop foundation models for hardware design. These are unified, general-purpose ML models and architectures that can learn complex hardware design skills with the right training data and objectives.\nRealizing foundation models for end-to-end hardware design will require the following:\n\nAccumulate large, high-quality, labeled datasets across hardware design stages to train foundation models.\nAdvances in multi-modal, multi-task ML techniques to handle the diversity of hardware design data and tasks.\nInterfaces and abstraction layers to connect foundation models to existing design flows and tools.\nDevelopment of simulation environments and benchmarks to train and test foundation models on hardware design capabilities.\nMethods to explain and interpret ML models’ design decisions and optimizations for trust and verification.\nCompilation techniques to optimize foundation models for efficient deployment across hardware platforms.\n\nWhile significant research remains, foundation models represent the most transformative long-term goal for imbuing AI into the hardware design process. Democratizing hardware design via versatile, automated ML systems promises to unlock a new era of optimized, efficient, and innovative chip design. The journey ahead is filled with open challenges and opportunities.\nIf you are interested in ML-aided computer architecture design (Krishnan et al. 2023), we encourage you to read Architecture 2.0.\n\nKrishnan, Srivatsan, Amir Yazdanbakhsh, Shvetank Prakash, Jason Jabbour, Ikechukwu Uchendu, Susobhan Ghosh, Behzad Boroujerdian, et al. 2023. “ArchGym: An Open-Source Gymnasium for Machine Learning Assisted Architecture Design.” In Proceedings of the 50th Annual International Symposium on Computer Architecture, 1–16. ACM. https://doi.org/10.1145/3579371.3589049.\nAlternatively, you can watch Video 11.3 for more details.\n\n\n\n\n\n\nImportant 11.3: Architecture 2.0",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>AI Acceleration</span>"
    ]
  },
  {
    "objectID": "contents/core/hw_acceleration/hw_acceleration.html#conclusion",
    "href": "contents/core/hw_acceleration/hw_acceleration.html#conclusion",
    "title": "11  AI Acceleration",
    "section": "11.10 Conclusion",
    "text": "11.10 Conclusion\nSpecialized hardware acceleration has become indispensable for enabling performant and efficient artificial intelligence applications as models and datasets explode in complexity. This chapter examined the limitations of general-purpose processors like CPUs for AI workloads. Their lack of parallelism and computational throughput cannot train or run state-of-the-art deep neural networks quickly. These motivations have driven innovations in customized accelerators.\nWe surveyed GPUs, TPUs, FPGAs, and ASICs specifically designed for the math-intensive operations inherent to neural networks. By covering this spectrum of options, we aimed to provide a framework for reasoning through accelerator selection based on constraints around flexibility, performance, power, cost, and other factors.\nWe also explored the role of software in actively enabling and optimizing AI acceleration. This spans programming abstractions, frameworks, compilers, and simulators. We discussed hardware-software co-design as a proactive methodology for building more holistic AI systems by closely integrating algorithm innovation and hardware advances.\nBut there is so much more to come! Exciting frontiers like analog computing, optical neural networks, and quantum machine learning represent active research directions that could unlock orders of magnitude improvements in efficiency, speed, and scale compared to present paradigms.\nUltimately, specialized hardware acceleration remains indispensable for unlocking the performance and efficiency necessary to fulfill the promise of artificial intelligence from cloud to edge. We hope this chapter provides useful background and insights into the rapid innovation occurring in this domain.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>AI Acceleration</span>"
    ]
  },
  {
    "objectID": "contents/core/hw_acceleration/hw_acceleration.html#sec-ai-acceleration-resource",
    "href": "contents/core/hw_acceleration/hw_acceleration.html#sec-ai-acceleration-resource",
    "title": "11  AI Acceleration",
    "section": "11.11 Resources",
    "text": "11.11 Resources\nHere is a curated list of resources to support students and instructors in their learning and teaching journeys. We are continuously working on expanding this collection and will add new exercises soon.\n\n\n\n\n\n\nSlides\n\n\n\n\n\n\nComing soon.\n\n\n\n\n\n\n\n\n\n\nVideos\n\n\n\n\n\n\nVideo 11.1\nVideo 11.2\nVideo 11.3\n\n\n\n\n\n\n\n\n\n\nExercises\n\n\n\n\n\n\nExercise 11.1",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>AI Acceleration</span>"
    ]
  },
  {
    "objectID": "contents/core/benchmarking/benchmarking.html",
    "href": "contents/core/benchmarking/benchmarking.html",
    "title": "12  Benchmarking AI",
    "section": "",
    "text": "Purpose\nResources: Slides, Videos, Exercises\nHow can quantitative evaluation reshape the development of machine learning systems, and what metrics reveal true system capabilities?\nThe measurement and analysis of AI system performance represent a critical element in bridging theoretical capabilities with practical outcomes. Systematic evaluation approaches reveal fundamental relationships between model behavior, resource utilization, and operational reliability. These measurements draw out the essential trade-offs across accuracy, efficiency, and scalability, providing insights that guide architectural decisions throughout the development lifecycle. These evaluation frameworks establish core principles for assessing and validating system design choices and enable the creation of robust solutions that meet increasingly complex performance requirements across diverse deployment scenarios.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Benchmarking AI</span>"
    ]
  },
  {
    "objectID": "contents/core/benchmarking/benchmarking.html#purpose",
    "href": "contents/core/benchmarking/benchmarking.html#purpose",
    "title": "12  Benchmarking AI",
    "section": "",
    "text": "Learning Objectives\n\n\n\n\nUnderstand the purpose and goals of benchmarking AI systems, including performance assessment, resource evaluation, validation, and more.\nLearn about key model benchmarks, metrics, and trends, including accuracy, fairness, complexity, performance, and energy efficiency.\nBecome familiar with the key components of an AI benchmark, including datasets, tasks, metrics, baselines, reproducibility rules, and more.\nUnderstand the distinction between training and inference and how each phase warrants specialized ML systems benchmarking.\nLearn about system benchmarking concepts like throughput, latency, power, and computational efficiency.\nAppreciate the evolution of model benchmarking from accuracy to more holistic metrics like fairness, robustness, and real-world applicability.\nRecognize the growing role of data benchmarking in evaluating issues like bias, noise, balance, and diversity.\nUnderstand the limitations of evaluating models, data, and systems in isolation and the emerging need for integrated benchmarking.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Benchmarking AI</span>"
    ]
  },
  {
    "objectID": "contents/core/benchmarking/benchmarking.html#overview",
    "href": "contents/core/benchmarking/benchmarking.html#overview",
    "title": "12  Benchmarking AI",
    "section": "12.1 Overview",
    "text": "12.1 Overview\nBenchmarking provides the essential measurements needed to drive machine learning progress and truly understand system performance. As the physicist Lord Kelvin famously said, “To measure is to know.” Benchmarks allow us to quantitatively know the capabilities of different models, software, and hardware. They allow ML developers to measure the inference time, memory usage, power consumption, and other metrics that characterize a system. Moreover, benchmarks create standardized processes for measurement, enabling fair comparisons across different solutions.\nWhen benchmarks are maintained over time, they become instrumental in capturing progress across generations of algorithms, datasets, and hardware. The models and techniques that set new records on ML benchmarks from one year to the next demonstrate tangible improvements in what’s possible for on-device machine learning. By using benchmarks to measure, ML practitioners can know the real-world capabilities of their systems and have confidence that each step reflects genuine progress towards the state-of-the-art.\nBenchmarking has several important goals and objectives that guide its implementation for machine learning systems.\n\nPerformance assessment. This involves evaluating key metrics like a given model’s speed, accuracy, and efficiency. For instance, in a TinyML context, it is crucial to benchmark how quickly a voice assistant can recognize commands, as this evaluates real-time performance.\nPower assessment. Evaluating the power drawn by a workload along with its performance equates to its energy efficiency. As the environmental impact of ML computing continues to grow, benchmarking energy can enable us to better optimize our systems for sustainability.\nResource evaluation. This means assessing the model’s impact on critical system resources, including battery life, memory usage, and computational overhead. A relevant example is comparing the battery drain of two different image recognition algorithms running on a wearable device.\nValidation and verification. Benchmarking helps ensure the system functions correctly and meets specified requirements. One way is by checking the accuracy of an algorithm, like a heart rate monitor on a smartwatch, against readings from medical-grade equipment as a form of clinical validation.\nCompetitive analysis. This enables comparing solutions against competing offerings in the market. For example, benchmarking a custom object detection model versus common TinyML benchmarks like MobileNet and Tiny-YOLO.\nCredibility. Accurate benchmarks uphold the credibility of AI solutions and the organizations that develop them. They demonstrate a commitment to transparency, honesty, and quality, which are essential in building trust with users and stakeholders.\nRegulation and Standardization. As the AI industry continues to grow, there is an increasing need for regulation and standardization to ensure that AI solutions are safe, ethical, and effective. Accurate and reliable benchmarks are essential to this regulatory framework, as they provide the data and evidence needed to assess compliance with industry standards and legal requirements.\n\nThis chapter will cover the 3 types of AI benchmarks, the standard metrics, tools, and techniques designers use to optimize their systems, and the challenges and trends in benchmarking.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Benchmarking AI</span>"
    ]
  },
  {
    "objectID": "contents/core/benchmarking/benchmarking.html#historical-context",
    "href": "contents/core/benchmarking/benchmarking.html#historical-context",
    "title": "12  Benchmarking AI",
    "section": "12.2 Historical Context",
    "text": "12.2 Historical Context\n\n12.2.1 Performance Benchmarks\nThe evolution of benchmarks in computing vividly illustrates the industry’s relentless pursuit of excellence and innovation. In the early days of computing during the 1960s and 1970s, benchmarks were rudimentary and designed for mainframe computers. For example, the Whetstone benchmark, named after the Whetstone ALGOL compiler, was one of the first standardized tests to measure the floating-point arithmetic performance of a CPU. These pioneering benchmarks prompted manufacturers to refine their architectures and algorithms to achieve better benchmark scores.\nThe 1980s marked a significant shift with the rise of personal computers. As companies like IBM, Apple, and Commodore competed for market share, and so benchmarks became critical tools to enable fair competition. The SPEC CPU benchmarks, introduced by the System Performance Evaluation Cooperative (SPEC), established standardized tests allowing objective comparisons between different machines. This standardization created a competitive environment, pushing silicon manufacturers and system creators to continually improve their hardware and software offerings.\nThe 1990s brought the era of graphics-intensive applications and video games. The need for benchmarks to evaluate graphics card performance led to Futuremark’s creation of 3DMark. As gamers and professionals sought high-performance graphics cards, companies like NVIDIA and AMD were driven to rapid innovation, leading to major advancements in GPU technology like programmable shaders.\nThe 2000s saw a surge in mobile phones and portable devices like tablets. With portability came the challenge of balancing performance and power consumption. Benchmarks like MobileMark by BAPCo evaluated speed and battery life. This drove companies to develop more energy-efficient System-on-Chips (SOCs), leading to the emergence of architectures like ARM that prioritized power efficiency.\nThe focus of the recent decade has shifted towards cloud computing, big data, and artificial intelligence. Cloud service providers like Amazon Web Services and Google Cloud compete on performance, scalability, and cost-effectiveness. Tailored cloud benchmarks like CloudSuite have become essential, driving providers to optimize their infrastructure for better services.\n\n\n12.2.2 Energy Benchmarks\nEnergy consumption and environmental concerns have gained prominence in recent years, making power (more precisely, energy) benchmarking increasingly important in the industry. This shift began in the mid-2000s when processors and systems started hitting cooling limits, and scaling became a crucial aspect of building large-scale systems due to internet advancements. Since then, energy considerations have expanded to encompass all areas of computing, from personal devices to large-scale data centers.\nPower benchmarking aims to measure the energy efficiency of computing systems, evaluating performance in relation to power consumption. This is crucial for several reasons:\n\nEnvironmental impact: With the growing carbon footprint of the tech industry, there’s a pressing need to reduce energy consumption.\nOperational costs: Energy expenses constitute a significant portion of data center operating costs.\nDevice longevity: For mobile devices, power efficiency directly impacts battery life and user experience.\n\nSeveral key benchmarks have emerged in this space:\n\nSPEC Power: Introduced in 2007, SPEC Power was one of the first industry-standard benchmarks for evaluating the power and performance characteristics of computer servers.\nGreen500: The Green500 list ranks supercomputers by energy efficiency, complementing the performance-focused TOP500 list.\nEnergy Star: While not a benchmark per se, ENERGY STAR for Computers certification program has driven manufacturers to improve the energy efficiency of consumer electronics.\n\nPower benchmarking faces unique challenges, such as accounting for different workloads and system configurations, and measuring power consumption accurately across a range of hardware that scales from microWatts to megaWatts in power consumption. As AI and edge computing continue to grow, power benchmarking is likely to become even more critical, driving the development of specialized energy-efficient AI hardware and software optimizations.\n\n\n12.2.3 Custom Benchmarks\nIn addition to industry-standard benchmarks, there are custom benchmarks specifically designed to meet the unique requirements of a particular application or task. They are tailored to the specific needs of the user or developer, ensuring that the performance metrics are directly relevant to the intended use of the AI model or system. Custom benchmarks can be created by individual organizations, researchers, or developers and are often used in conjunction with industry-standard benchmarks to provide a comprehensive evaluation of AI performance.\nFor example, a hospital could develop a benchmark to assess an AI model for predicting patient readmission. This benchmark would incorporate metrics relevant to the hospital’s patient population, like demographics, medical history, and social factors. Similarly, a financial institution’s fraud detection benchmark could focus on identifying fraudulent transactions accurately while minimizing false positives. In automotive, an autonomous vehicle benchmark may prioritize performance in diverse conditions, responding to obstacles, and safety. Retailers could benchmark recommendation systems using click-through rate, conversion rate, and customer satisfaction. Manufacturing companies might benchmark quality control systems on defect identification, efficiency, and waste reduction. In each industry, custom benchmarks provide organizations with evaluation criteria tailored to their unique needs and context. This allows for a more meaningful assessment of how well AI systems meet requirements.\nThe advantage of custom benchmarks lies in their flexibility and relevance. They can be designed to test specific performance aspects critical to the success of the AI solution in its intended application. This allows for a more targeted and accurate assessment of the AI model or system’s capabilities. Custom benchmarks also provide valuable insights into the performance of AI solutions in real-world scenarios, which can be crucial for identifying potential issues and areas for improvement.\nIn AI, benchmarks play a crucial role in driving progress and innovation. While benchmarks have long been used in computing, their application to machine learning is relatively recent. AI-focused benchmarks provide standardized metrics to evaluate and compare the performance of different algorithms, model architectures, and hardware platforms.\n\n\n12.2.4 Community Consensus\nA key prerogative for any benchmark to be impactful is that it must reflect the shared priorities and values of the broader research community. Benchmarks designed in isolation risk failing to gain acceptance if they overlook key metrics considered important by leading groups. Through collaborative development with open participation from academic labs, companies, and other stakeholders, benchmarks can incorporate collective input on critical capabilities worth measuring. This helps ensure the benchmarks evaluate aspects the community agrees are essential to advance the field. The process of reaching alignment on tasks and metrics itself supports converging on what matters most.\nFurthermore, benchmarks published with broad co-authorship from respected institutions carry authority and validity that convinces the community to adopt them as trusted standards. Benchmarks perceived as biased by particular corporate or institutional interests breed skepticism. Ongoing community engagement through workshops and challenges is also key after the initial release, and that is what, for instance, led to the success of ImageNet. As research progresses, collective participation enables continual refinement and expansion of benchmarks over time.\nFinally, releasing community-developed benchmarks with open access promotes their adoption and consistent use. By providing open-source code, documentation, models, and infrastructure, we reduce barriers to entry, enabling groups to benchmark solutions on an equal footing with standardized implementations. This consistency is essential for fair comparisons. Without coordination, labs and companies might implement benchmarks differently, which can undermine reproducibility and comparability of results.\nCommunity consensus brings benchmarks lasting relevance, while fragmentation confuses. Through collaborative development and transparent operation, benchmarks can become authoritative standards for tracking progress. Several of the benchmarks that we discuss in this chapter were developed and built by the community, for the community, and that is what ultimately led to their success.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Benchmarking AI</span>"
    ]
  },
  {
    "objectID": "contents/core/benchmarking/benchmarking.html#ai-benchmarks-system-model-and-data",
    "href": "contents/core/benchmarking/benchmarking.html#ai-benchmarks-system-model-and-data",
    "title": "12  Benchmarking AI",
    "section": "12.3 AI Benchmarks: System, Model, and Data",
    "text": "12.3 AI Benchmarks: System, Model, and Data\nThe need for comprehensive benchmarking becomes paramount as AI systems grow in complexity and ubiquity. Within this context, benchmarks are often classified into three primary categories: Hardware, Model, and Data. Let’s dive into why each of these buckets is essential and the significance of evaluating AI from these three distinct dimensions:\n\n12.3.1 System Benchmarks\nAI computations, especially those in deep learning, are resource-intensive. The hardware on which these computations run plays an important role in determining AI solutions’ speed, efficiency, and scalability. Consequently, hardware benchmarks help evaluate the performance of CPUs, GPUs, TPUs, and other accelerators in AI tasks. By understanding hardware performance, developers can choose which hardware platforms best suit specific AI applications. Furthermore, hardware manufacturers use these benchmarks to identify areas for improvement, driving innovation in AI-specific chip designs.\n\n\n12.3.2 Model Benchmarks\nThe architecture, size, and complexity of AI models vary widely. Different models have different computational demands and offer varying levels of accuracy and efficiency. Model benchmarks help us assess the performance of various AI architectures on standardized tasks. They provide insights into different models’ speed, accuracy, and resource demands. By benchmarking models, researchers can identify best-performing architectures for specific tasks, guiding the AI community towards more efficient and effective solutions. Additionally, these benchmarks aid in tracking the progress of AI research, showcasing advancements in model design and optimization.\n\n\n12.3.3 Data Benchmarks\nIn machine learning, data is foundational because the quality, scale, and diversity of datasets directly impact model efficacy and generalization. Data benchmarks focus on the datasets used in training and evaluation. They provide standardized datasets the community can use to train and test models, ensuring a level playing field for comparisons. Moreover, these benchmarks highlight data quality, diversity, and representation challenges, pushing the community to address biases and gaps in training data. By understanding data benchmarks, researchers can also gauge how models might perform in real-world scenarios, ensuring robustness and reliability.\nIn the remainder of the sections, we will discuss each of these benchmark types. The focus will be an in-depth exploration of system benchmarks, as these are critical to understanding and advancing machine learning system performance. We will briefly cover model and data benchmarks for a comprehensive perspective, but the emphasis and majority of the content will be devoted to system benchmarks.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Benchmarking AI</span>"
    ]
  },
  {
    "objectID": "contents/core/benchmarking/benchmarking.html#system-benchmarking",
    "href": "contents/core/benchmarking/benchmarking.html#system-benchmarking",
    "title": "12  Benchmarking AI",
    "section": "12.4 System Benchmarking",
    "text": "12.4 System Benchmarking\n\n12.4.1 Granularity\nMachine learning system benchmarking provides a structured and systematic approach to assessing a system’s performance across various dimensions. Given the complexity of ML systems, we can dissect their performance through different levels of granularity and obtain a comprehensive view of the system’s efficiency, identify potential bottlenecks, and pinpoint areas for improvement. To this end, various types of benchmarks have evolved over the years and continue to persist.\nFigure 12.1 illustrates the different layers of granularity of an ML system. At the application level, end-to-end benchmarks assess the overall system performance, considering factors like data preprocessing, model training, and inference. While at the model layer, benchmarks focus on assessing the efficiency and accuracy of specific models. This includes evaluating how well models generalize to new data and their computational efficiency during training and inference. Furthermore, benchmarking can extend to hardware and software infrastructure, examining the performance of individual components like GPUs or TPUs.\n\n\n\n\n\n\nFigure 12.1: ML system granularity.\n\n\n\n\nMicro Benchmarks\nMicro-benchmarks are specialized, evaluating distinct components or specific operations within a broader machine learning process. These benchmarks focus on individual tasks, offering insights into the computational demands of a particular neural network layer, the efficiency of a unique optimization technique, or the throughput of a specific activation function. For instance, practitioners might use micro-benchmarks to measure the computational time required by a convolutional layer in a deep learning model or to evaluate the speed of data preprocessing that feeds data into the model. Such granular assessments are instrumental in fine-tuning and optimizing discrete aspects of models, ensuring that each component operates at its peak potential.\nThese types of microbenchmarks include zooming into very specific operations or components of the AI pipeline, such as the following:\n\nTensor Operations: Libraries like cuDNN (by NVIDIA) often have benchmarks to measure the performance of individual tensor operations, such as convolutions or matrix multiplications, which are foundational to deep learning computations.\nActivation Functions: Benchmarks that measure the speed and efficiency of various activation functions like ReLU, Sigmoid, or Tanh in isolation.\nLayer Benchmarks: Evaluations of the computational efficiency of distinct neural network layers, such as LSTM or Transformer blocks, when operating on standardized input sizes.\n\nExample: DeepBench, introduced by Baidu, is a good benchmark that evaluates fundamental deep learning operations, such as those mentioned above. DeepBench assesses the performance of basic operations in deep learning models, providing insights into how different hardware platforms handle neural network training and inference.\n\n\n\n\n\n\nExercise 12.1: System Benchmarking - Tensor Operations\n\n\n\n\n\nEver wonder how your image filters get so fast? Special libraries like cuDNN supercharge those calculations on certain hardware. In this Colab, we’ll use cuDNN with PyTorch to speed up image filtering. Think of it as a tiny benchmark, showing how the right software can unlock your GPU’s power!\n\n\n\n\n\n\nMacro Benchmarks\nMacro benchmarks provide a holistic view, assessing the end-to-end performance of entire machine learning models or comprehensive ML systems. Rather than focusing on individual operations, macro-benchmarks evaluate the collective efficacy of models under real-world scenarios or tasks. For example, a macro-benchmark might assess the complete performance of a deep learning model undertaking image classification on a dataset like ImageNet. This includes gauging accuracy, computational speed, and resource consumption. Similarly, one might measure the cumulative time and resources needed to train a natural language processing model on extensive text corpora or evaluate the performance of an entire recommendation system, from data ingestion to final user-specific outputs.\nExamples: These benchmarks evaluate the AI model:\n\nMLPerf Inference (Reddi et al. 2020): An industry-standard set of benchmarks for measuring the performance of machine learning software and hardware. MLPerf has a suite of dedicated benchmarks for specific scales, such as MLPerf Mobile for mobile class devices and MLPerf Tiny, which focuses on microcontrollers and other resource-constrained devices.\nEEMBC’s MLMark: A benchmarking suite for evaluating the performance and power efficiency of embedded devices running machine learning workloads. This benchmark provides insights into how different hardware platforms handle tasks like image recognition or audio processing.\nAI-Benchmark (Ignatov et al. 2019): A benchmarking tool designed for Android devices, it evaluates the performance of AI tasks on mobile devices, encompassing various real-world scenarios like image recognition, face parsing, and optical character recognition.\n\n\nReddi, Vijay Janapa, Christine Cheng, David Kanter, Peter Mattson, Guenther Schmuelling, Carole-Jean Wu, Brian Anderson, et al. 2020. “MLPerf Inference Benchmark.” In 2020 ACM/IEEE 47th Annual International Symposium on Computer Architecture (ISCA), 446–59. IEEE; IEEE. https://doi.org/10.1109/isca45697.2020.00045.\n\nIgnatov, Andrey, Radu Timofte, Andrei Kulik, Seungsoo Yang, Ke Wang, Felix Baum, Max Wu, Lirong Xu, and Luc Van Gool. 2019. “AI Benchmark: All about Deep Learning on Smartphones in 2019.” In 2019 IEEE/CVF International Conference on Computer Vision Workshop (ICCVW), 3617–35. IEEE. https://doi.org/10.1109/iccvw.2019.00447.\n\n\nEnd-to-end Benchmarks\nEnd-to-end benchmarks provide an all-inclusive evaluation that extends beyond the boundaries of the ML model itself. Instead of focusing solely on a machine learning model’s computational efficiency or accuracy, these benchmarks encompass the entire pipeline of an AI system. This includes initial data preprocessing, the core model’s performance, post-processing of the model’s outputs, and other integral components like storage and network interactions.\nData preprocessing is the first stage in many AI systems, transforming raw data into a format suitable for model training or inference. These preprocessing steps’ efficiency, scalability, and accuracy are vital for the overall system’s performance. End-to-end benchmarks assess this phase, ensuring that data cleaning, normalization, augmentation, or any other transformation process doesn’t become a bottleneck.\nThe post-processing phase also takes center stage. This involves interpreting the model’s raw outputs, possibly converting scores into meaningful categories, filtering results, or even integrating with other systems. In real-world applications, this phase is crucial for delivering actionable insights, and end-to-end benchmarks ensure it’s both efficient and effective.\nBeyond the core AI operations, other system components are important in the overall performance and user experience. Storage solutions, whether cloud-based, on-premises, or hybrid, can significantly impact data retrieval and storage times, especially with vast AI datasets. Similarly, network interactions, vital for cloud-based AI solutions or distributed systems, can become performance bottlenecks if not optimized. End-to-end benchmarks holistically evaluate these components, ensuring that the entire system operates seamlessly, from data retrieval to final output delivery.\nTo date, there are no public, end-to-end benchmarks that take into account the role of data storage, network, and compute performance. Arguably, MLPerf Training and Inference come close to the idea of an end-to-end benchmark, but they are exclusively focused on ML model performance and do not represent real-world deployment scenarios of how models are used in the field. Nonetheless, they provide a very useful signal that helps assess AI system performance.\nGiven the inherent specificity of end-to-end benchmarking, it is typically performed internally at a company by instrumenting real production deployments of AI. This allows engineers to have a realistic understanding and breakdown of the performance, but given the sensitivity and specificity of the information, it is rarely reported outside of the company.\n\n\nUnderstanding the Trade-offs\nDifferent issues arise at different stages of an AI system. Micro-benchmarks help fine-tune individual components, macro-benchmarks aid in refining model architectures or algorithms, and end-to-end benchmarks guide the optimization of the entire workflow. By understanding where a problem lies, developers can apply targeted optimizations.\nMoreover, while individual components of an AI system might perform optimally in isolation, bottlenecks can emerge when they interact. End-to-end benchmarks, in particular, are crucial to ensure that the entire system, when operating collectively, meets desired performance and efficiency standards.\nFinally, organizations can make informed decisions on where to allocate resources by discerning performance bottlenecks or inefficiencies. For instance, if micro-benchmarks reveal inefficiencies in specific tensor operations, investments can be directed toward specialized hardware accelerators. Conversely, if end-to-end benchmarks indicate data retrieval issues, investments might be channeled toward better storage solutions.\n\n\n\n12.4.2 Benchmark Components\nAt its core, an AI benchmark is more than just a test or a score; it’s a comprehensive evaluation framework. To understand this in-depth, let’s break down the typical components that go into an AI benchmark.\n\nStandardized Datasets\nDatasets serve as the foundation for most AI benchmarks. They provide a consistent data set on which models are trained and evaluated, ensuring a level playing field for comparisons.\nExample: ImageNet, a large-scale dataset containing millions of labeled images spanning thousands of categories, is a popular benchmarking standard for image classification tasks.\n\n\nPre-defined Tasks\nA benchmark should have a clear objective or task that models aim to achieve. This task defines the problem the AI system is trying to solve.\nExample: Tasks for natural language processing benchmarks might include sentiment analysis, named entity recognition, or machine translation.\n\n\nEvaluation Metrics\nOnce a task is defined, benchmarks require metrics to quantify performance. These metrics offer objective measures to compare different models or systems. In classification tasks, metrics like accuracy, precision, recall, and F1 score are commonly used. Mean squared or absolute errors might be employed for regression tasks. We can also measure the power consumed by the benchmark execution to calculate energy efficiency.\n\n\nBaselines and Baseline Models\nBenchmarks often include baseline models or reference implementations. These usually serve as starting points or minimum performance standards for comparing new models or techniques. Baseline models help researchers measure the effectiveness of new algorithms.\nIn benchmark suites, simple models like linear regression or basic neural networks are often the common baselines. These provide context when evaluating more complex models. By comparing against these simpler models, researchers can quantify improvements from advanced approaches.\nPerformance metrics vary by task, but here are some examples:\n\nClassification tasks use metrics such as accuracy, precision, recall, and F1 score.\nRegression tasks often use mean squared error or mean absolute error.\n\n\n\nHardware and Software Specifications\nGiven the variability introduced by different hardware and software configurations, benchmarks often specify or document the hardware and software environments in which tests are conducted.\nExample: An AI benchmark might note that evaluations were conducted on an NVIDIA Tesla V100 GPU using TensorFlow v2.4.\n\n\nEnvironmental Conditions\nAs external factors can influence benchmark results, it’s essential to either control or document conditions like temperature, power source, or system background processes.\nExample: Mobile AI benchmarks might specify that tests were conducted at room temperature with devices plugged into a power source to eliminate battery-level variances.\n\n\nReproducibility Rules\nTo ensure benchmarks are credible and can be replicated by others in the community, they often include detailed protocols covering everything from random seeds used to exact hyperparameters.\nExample: A benchmark for a reinforcement learning task might detail the exact training episodes, exploration-exploitation ratios, and reward structures used.\n\n\nResult Interpretation Guidelines\nBeyond raw scores or metrics, benchmarks often provide guidelines or context to interpret results, helping practitioners understand the broader implications.\nExample: A benchmark might highlight that while Model A scored higher than Model B in accuracy, it offers better real-time performance, making it more suitable for time-sensitive applications.\n\n\n\n12.4.3 Training Benchmarks\nThe development life cycle of a machine learning model involves two critical phases - training and inference. Training represents the phase where the system processes and ingests raw data to adjust and refine its parameters. Benchmarking the training phase reveals how choices in data pipelines, storage solutions, model architectures, computing resources, hyperparameter settings, and optimization algorithms affect the efficiency and resource demands of model training. The goal is to ensure that the ML system can efficiently learn from data, optimizing both the model’s performance and the system’s resource utilization.\n\nPurpose\nFrom a systems perspective, training machine learning models is resource-intensive, especially when working with large models. These models often contain billions or even trillions of trainable parameters and require enormous amounts of data, often on the scale of many terabytes. For example, OpenAI’s GPT-3 (Brown et al. 2020) has 175 billion parameters, was trained on 45 TB of compressed plaintext data, and required 3,640 petaflop-days of compute for pretraining. ML training benchmarks evaluate the systems and resources required to manage the computational load of training such models.\nEfficient data storage and delivery during training also play a major role in the training process. For instance, in a machine learning model that predicts bounding boxes around objects in an image, thousands of images may be required. However, loading an entire image dataset into memory is typically infeasible, so practitioners rely on data loaders from ML frameworks. Successful model training depends on timely and efficient data delivery, making it essential to benchmark tools like data loaders, data pipelines, preprocessing speed, and storage retrieval times to understand their impact on training performance.\nHardware selection is another key factor in training machine learning systems, as it can significantly impact training time. Training benchmarks evaluate CPU, GPU, memory, and network utilization during the training phase to guide system optimizations. Understanding how resources are used is essential: Are GPUs being fully leveraged? Is there unnecessary memory overhead? Benchmarks can uncover bottlenecks or inefficiencies in resource utilization, leading to cost savings and performance improvements.\nIn many cases, using a single hardware accelerator, such as a single GPU, is insufficient to meet the computational demands of large-scale model training. Machine learning models are often trained in data centers with multiple GPUs or TPUs, where distributed computing enables parallel processing across nodes. Training benchmarks assess how efficiently the system scales across multiple nodes, manages data sharding, and handles challenges like node failures or drop-offs during training.\n\n\nMetrics\nWhen viewed from a systems perspective, training metrics offer insights that transcend conventional algorithmic performance indicators. These metrics measure the model’s learning efficacy and gauge the efficiency, scalability, and robustness of the entire ML system during the training phase. Let’s explore deeper into these metrics and their significance.\nThe following metrics are often considered important:\n\nTraining Time: The time it takes to train a model from scratch until it reaches a satisfactory performance level. It directly measures the computational resources required to train a model. For example, Google’s BERT (Devlin et al. 2019) is a natural language processing model that requires several days to train on a massive corpus of text data using multiple GPUs. The long training time is a significant resource consumption and cost challenge. In some cases, benchmarks can instead measure the training throughput (training samples per unit of time). Throughput can be calculated much faster and easier than training time but may obscure the metrics we really care about (e.g. time to train).\nScalability: How well the training process can handle increases in data size or model complexity. Scalability can be assessed by measuring training time, memory usage, and other resource consumption as data size or model complexity increases. For instance, training OpenAI’s GPT-3 required extensive engineering efforts to scale the training process across many GPU nodes to handle the massive model size. This involved using specialized hardware, distributed training, and other techniques to ensure the model could be trained efficiently.\nResource Utilization: The extent to which the training process utilizes available computational resources such as CPU, GPU, memory, and disk I/O. High resource utilization can indicate an efficient training process, while low utilization can suggest bottlenecks or inefficiencies. For instance, training a convolutional neural network (CNN) for image classification requires significant GPU resources. Utilizing multi-GPU setups and optimizing the training code for GPU acceleration can greatly improve resource utilization and training efficiency.\nMemory Consumption: The amount of memory the training process uses. Memory consumption can be a limiting factor for training large models or datasets. For example, Google researchers faced significant memory consumption challenges when training BERT. The model has hundreds of millions of parameters, requiring large amounts of memory. The researchers had to develop techniques to reduce memory consumption, such as gradient checkpointing and model parallelism.\nEnergy Consumption: The energy consumed during training. As machine learning models become more complex, energy consumption has become an important consideration. Training large machine learning models can consume significant energy, leading to a large carbon footprint. For instance, the training of OpenAI’s GPT-3 was estimated to have a carbon footprint equivalent to traveling by car for 700,000 kilometers (~435,000 miles).\nThroughput: The number of training samples processed per unit time. Higher throughput generally indicates a more efficient training process. The throughput is an important metric to consider when training a recommendation system for an e-commerce platform. A high throughput ensures that the model can process large volumes of user interaction data promptly, which is crucial for maintaining the relevance and accuracy of the recommendations. But it’s also important to understand how to balance throughput with latency bounds. Therefore, a latency-bounded throughput constraint is often imposed on service-level agreements for data center application deployments.\nCost: The cost of training a model can include both computational and human resources. Cost is important when considering the practicality and feasibility of training large or complex models. Training large language models like GPT-3 is estimated to cost millions of dollars. This cost includes computational, electricity and human resources required for model development and training.\nFault Tolerance and Robustness: The ability of the training process to handle failures or errors without crashing or producing incorrect results. This is important for ensuring the reliability of the training process. Network failures or hardware malfunctions can occur in a real-world scenario where a machine-learning model is being trained on a distributed system. In recent years, it has become abundantly clear that faults arising from silent data corruption have emerged as a major issue. A fault-tolerant and robust training process can recover from such failures without compromising the model’s integrity.\nEase of Use and Flexibility: The ease with which the training process can be set up and used and its flexibility in handling different types of data and models. In companies like Google, efficiency can sometimes be measured by the number of Software Engineer (SWE) years saved since that translates directly to impact. Ease of use and flexibility can reduce the time and effort required to train a model. TensorFlow and PyTorch are popular machine-learning frameworks that provide user-friendly interfaces and flexible APIs for building and training machine-learning models. These frameworks support many model architectures and are equipped with tools that simplify the training process.\nReproducibility: The ability to reproduce the training process results. Reproducibility is important for verifying a model’s correctness and validity. However, variations due to stochastic network characteristics often make it hard to reproduce the precise behavior of applications being trained, which can present a challenge for benchmarking.\n\nBy benchmarking for these types of metrics, we can obtain a comprehensive view of the training process’s performance and efficiency from a systems perspective. This can help identify areas for improvement and ensure that resources are used effectively.\n\n\nBenchmarks\nHere are some original works that laid the fundamental groundwork for developing systematic benchmarks for training machine learning systems.\nMLPerf Training Benchmark: MLPerf is a suite of benchmarks designed to measure the performance of machine learning hardware, software, and services. The MLPerf Training benchmark (Mattson et al. 2020a) focuses on the time it takes to train models to a target quality metric. It includes diverse workloads, such as image classification, object detection, translation, and reinforcement learning. Figure 12.2 highlights the performance improvements in progressive versions of MLPerf Training benchmarks, which have all outpaced Moore’s Law. Using standardized benchmarking trends enables us to rigorously showcase the rapid evolution of ML computing.\n\n\n\n\n\n\nFigure 12.2: MLPerf Training performance trends. Source: Mattson et al. (2020a).\n\n\n———, et al. 2020a. “MLPerf: An Industry Standard Benchmark Suite for Machine Learning Performance.” IEEE Micro 40 (2): 8–16. https://doi.org/10.1109/mm.2020.2974843.\n\n\nMetrics:\n\nTraining time to target quality\nThroughput (examples per second)\nResource utilization (CPU, GPU, memory, disk I/O)\n\nDAWNBench: DAWNBench (Coleman et al. 2019) is a benchmark suite focusing on end-to-end deep learning training time and inference performance. It includes common tasks such as image classification and question answering.\n\nColeman, Cody, Daniel Kang, Deepak Narayanan, Luigi Nardi, Tian Zhao, Jian Zhang, Peter Bailis, Kunle Olukotun, Chris Ré, and Matei Zaharia. 2019. “Analysis of DAWNBench, a Time-to-Accuracy Machine Learning Performance Benchmark.” ACM SIGOPS Operating Systems Review 53 (1): 14–25. https://doi.org/10.1145/3352020.3352024.\nMetrics:\n\nTime to train to target accuracy\nInference latency\nCost (in terms of cloud computing and storage resources)\n\nFathom: Fathom (Adolf et al. 2016) is a benchmark from Harvard University that evaluates the performance of deep learning models using a diverse set of workloads. These include common tasks such as image classification, speech recognition, and language modeling.\n\nAdolf, Robert, Saketh Rama, Brandon Reagen, Gu-yeon Wei, and David Brooks. 2016. “Fathom: Reference Workloads for Modern Deep Learning Methods.” In 2016 IEEE International Symposium on Workload Characterization (IISWC), 1–10. IEEE; IEEE. https://doi.org/10.1109/iiswc.2016.7581275.\nMetrics:\n\nOperations per second (to measure computational efficiency)\nTime to completion for each workload\nMemory bandwidth\n\n\n\nExample Use Case\nImagine you have been tasked with benchmarking the training performance of an image classification model on a specific hardware platform. Let’s break down how you might approach this:\n\nDefine the Task: First, choose a model and dataset. In this case, you’ll be training a CNN to classify images in the CIFAR-10 dataset, a widely used benchmark in computer vision.\nSelect the Benchmark: Choosing a widely accepted benchmark helps ensure your setup is comparable with other real-world evaluations. You could choose to use the MLPerf Training benchmark because it provides a structured image classification workload, making it a relevant and standardized option for assessing training performance on CIFAR-10. Using MLPerf enables you to evaluate your system against industry-standard metrics, helping to ensure that results are meaningful and comparable to those achieved on other hardware platforms.\nIdentify Key Metrics: Now, decide on the metrics that will help you evaluate the system’s training performance. For this example, you might track:\n\nTraining Time: How long does it take to reach 90% accuracy?\nThroughput: How many images are processed per second?\nResource Utilization: What’s the GPU and CPU usage throughout training?\n\n\nBy analyzing these metrics, you’ll gain insights into the model’s training performance on your chosen hardware platform. Consider whether training time meets your expectations, if there are any bottlenecks, such as underutilized GPUs or slow data loading. This process helps identify areas for potential optimization, like improving data handling or adjusting resource allocation, and can guide future benchmarking decisions.\n\n\n\n12.4.4 Inference Benchmarks\nInference in machine learning refers to using a trained model to make predictions on new, unseen data. It is the phase where the model applies its learned knowledge to solve the problem it was designed for, such as classifying images, recognizing speech, or translating text.\n\nPurpose\nWhen we build machine learning models, our ultimate goal is to deploy them in real-world applications where they can provide accurate and reliable predictions on new, unseen data. This process of using a trained model to make predictions is known as inference. A machine learning model’s real-world performance can differ significantly from its performance on training or validation datasets, which makes benchmarking inference a crucial step in the development and deployment of machine learning models.\nBenchmarking inference allows us to evaluate how well a machine-learning model performs in real-world scenarios. This evaluation ensures that the model is practical and reliable when deployed in applications, providing a more comprehensive understanding of the model’s behavior with real data. Additionally, benchmarking can help identify potential bottlenecks or limitations in the model’s performance. For example, if a model takes too long to predict, it may be impractical for real-time applications such as autonomous driving or voice assistants.\nResource efficiency is another critical aspect of inference, as it can be computationally intensive and require significant memory and processing power. Benchmarking helps ensure that the model is efficient regarding resource usage, which is particularly important for edge devices with limited computational capabilities, such as smartphones or IoT devices. Moreover, benchmarking allows us to compare the performance of our model with competing models or previous versions of the same model. This comparison is essential for making informed decisions about which model to deploy in a specific application.\nFinally, it is vital to ensure that the model’s predictions are not only accurate but also consistent across different data points. Benchmarking helps verify the model’s accuracy and consistency, ensuring that it meets the application’s requirements. It also assesses the model’s robustness, ensuring that it can handle real-world data variability and still make accurate predictions.\n\n\nMetrics\n\nAccuracy: Accuracy is one of the most vital metrics when benchmarking machine learning models. It quantifies the proportion of correct predictions made by the model compared to the true values or labels. For example, if a spam detection model can correctly classify 95 out of 100 email messages as spam or not, its accuracy would be calculated as 95%.\nLatency: Latency is a performance metric that calculates the time lag or delay between the input receipt and the production of the corresponding output by the machine learning system. An example that clearly depicts latency is a real-time translation application; if a half-second delay exists from the moment a user inputs a sentence to the time the app displays the translated text, then the system’s latency is 0.5 seconds.\nLatency-Bounded Throughput: Latency-bounded throughput is a valuable metric that combines the aspects of latency and throughput, measuring the maximum throughput of a system while still meeting a specified latency constraint. For example, in a video streaming application that utilizes a machine learning model to generate and display subtitles automatically, latency-bounded throughput would measure how many video frames the system can process per second (throughput) while ensuring that the subtitles are displayed with no more than a 1-second delay (latency). This metric is particularly important in real-time applications where meeting latency requirements is crucial to the user experience.\nThroughput: Throughput assesses the system’s capacity by measuring the number of inferences or predictions a machine learning model can handle within a specific unit of time. Consider a speech recognition system that employs a Recurrent Neural Network (RNN) as its underlying model; if this system can process and understand 50 different audio clips in a minute, then its throughput rate stands at 50 clips per minute.\nEnergy Efficiency: Energy efficiency is a metric that determines the amount of energy consumed by the machine learning model to perform a single inference. A prime example of this would be a natural language processing model built on a Transformer network architecture; if it utilizes 0.1 Joules of energy to translate a sentence from English to French, its energy efficiency is measured at 0.1 Joules per inference.\nMemory Usage: Memory usage quantifies the volume of RAM needed by a machine learning model to carry out inference tasks. A relevant example to illustrate this would be a face recognition system based on a CNN; if such a system requires 150 MB of RAM to process and recognize faces within an image, its memory usage is 150 MB.\n\n\n\nBenchmarks\nHere are some original works that laid the fundamental groundwork for developing systematic benchmarks for inference machine learning systems.\nMLPerf Inference Benchmark: MLPerf Inference is a comprehensive benchmark suite that assesses machine learning models’ performance during the inference phase. It encompasses a variety of workloads, including image classification, object detection, and natural language processing, aiming to provide standardized and insightful metrics for evaluating different inference systems. It’s metrics include:\nMLPerf Inference is a comprehensive benchmark suite that assesses machine learning models’ performance during the inference phase. It encompasses a variety of workloads, including image classification, object detection, and natural language processing, aiming to provide standardized and insightful metrics for evaluating different inference systems.\nMetrics:\n\nInference time\nLatency\nThroughput\nAccuracy\nEnergy consumption\n\nAI Benchmark: AI Benchmark is a benchmarking tool that evaluates the performance of AI and machine learning models on mobile devices and edge computing platforms. It includes tests for image classification, object detection, and natural language processing tasks, providing a detailed analysis of the inference performance on different hardware platforms. It’s metrics include:\nAI Benchmark is a benchmarking tool that evaluates the performance of AI and machine learning models on mobile devices and edge computing platforms. It includes tests for image classification, object detection, and natural language processing tasks, providing a detailed analysis of the inference performance on different hardware platforms.\nMetrics:\n\nInference time\nLatency\nEnergy consumption\nMemory usage\nThroughput\n\nOpenVINO toolkit: OpenVINO toolkit provides a benchmark tool to measure the performance of deep learning models for various tasks, such as image classification, object detection, and facial recognition, on Intel hardware. It offers detailed insights into the models’ inference performance on different hardware configurations. It’s metrics include:\nMetrics:\n\nInference time\nThroughput\nLatency\nCPU and GPU utilization\n\n\n\nExample Use Case\nSuppose you were tasked with evaluating the inference performance of an object detection model on a specific edge device. Here’s how you might approach structuring this benchmark:\n\nDefine the Task: In this case, the task is real-time object detection on video streams, identifying objects such as vehicles, pedestrians, and traffic signs.\nSelect the Benchmark: To align with your goal of evaluating inference on an edge device, the AI Benchmark is a suitable choice. It provides a standardized framework specifically for assessing inference performance on edge hardware, making it relevant to this scenario.\nIdentify Key Metrics: Now, determine the metrics that will help evaluate the model’s inference performance. For this example, you might track:\n\nInference Time: How long does it take to process each video frame?\nLatency: What is the delay in generating bounding boxes for detected objects?\nEnergy Consumption: How much power is used during inference?\nThroughput: How many video frames are processed per second?\n\n\nBy measuring these metrics, you’ll gain insights into how well the object detection model performs on the edge device. This can help identify any bottlenecks, such as slow frame processing or high energy consumption, and highlight areas for potential optimization to improve real-time performance.\n\n\n\n\n\n\nExercise 12.2: Inference Benchmarks - MLPerf\n\n\n\n\n\nGet ready to put your AI models to the ultimate test! MLPerf is like the Olympics for machine learning performance. In this Colab, we’ll use a toolkit called CK to run official MLPerf benchmarks, measure how fast and accurate your model is, and even use TVM to give it a super speed boost. Are you ready to see your model earn its medal?\n\n\n\n\n\n\n\n12.4.5 Benchmark Task Selection\nSelecting representative tasks for benchmarking machine learning systems is complex due to the varied applications, data types, and requirements across different domains. Machine learning is applied in fields such as healthcare, finance, natural language processing, and computer vision, each with unique tasks that may not be relevant or comparable to others. Key challenges in task selection include:\n\nDiversity of Applications and Data Types: Tasks across domains involve different data types (e.g., text, images, video) and qualities, making it difficult to find benchmarks that universally represent ML challenges.\nTask Complexity and Resource Needs: Tasks vary in complexity and resource demands, with some requiring substantial computational power and sophisticated models, while others can be addressed with simpler resources and methods.\nPrivacy Concerns: Tasks involving sensitive data, such as medical records or personal information, introduce ethical and privacy issues, making them unsuitable for general benchmarks.\nEvaluation Metrics: Performance metrics vary widely across tasks, and results from one task often do not generalize to others, complicating comparisons and limiting insights from one benchmarked task to another.\n\nAddressing these challenges is essential to designing meaningful benchmarks that are relevant across the diverse tasks encountered in machine learning, ensuring benchmarks provide useful, generalizable insights for both training and inference.\n\n\n12.4.6 Measuring Energy Efficiency\nAs machine learning capabilities expand, both in training and inference, concerns about increased power consumption and its ecological footprint have intensified. Addressing the sustainability of ML systems, a topic explored in more depth in the Sustainable AI chapter, has thus become a key priority. This focus on sustainability has led to the development of standardized benchmarks designed to accurately measure energy efficiency. However, standardizing these methodologies poses challenges due to the need to accommodate vastly different scales—from the microwatt consumption of TinyML devices to the megawatt demands of data center training systems. Moreover, ensuring that benchmarking is fair and reproducible requires accommodating the diverse range of hardware configurations and architectures in use today.\nOne example is the MLPerf Power benchmarking methodology (Tschand et al. 2024), which tackles these challenges by tailoring the methodologies for datacenter, edge inference, and tiny inference systems while measuring power consumption as comprehensively as possible for each scale. This methodology adapts to a variety of hardware, from general-purpose CPUs to specialized AI accelerators, while maintaining uniform measurement principles to ensure that comparisons are both fair and accurate across different platforms.\nFigure 12.3 illustrates the power measurement boundaries for different system scales, from TinyML devices to inference nodes and training racks. Each example highlights the components within the measurement boundary and those outside it. This setup allows for accurate reflection of the true energy costs associated with running ML workloads across various real-world scenarios, and ensures that the benchmark captures the full spectrum of energy consumption.\n\n\n\n\n\n\nFigure 12.3: MLPerf Power system measurement diagram. Source: Tschand et al. (2024).\n\n\nTschand, Arya, Arun Tejusve Raghunath Rajan, Sachin Idgunji, Anirban Ghosh, Jeremy Holleman, Csaba Kiraly, Pawan Ambalkar, et al. 2024. “MLPerf Power: Benchmarking the Energy Efficiency of Machine Learning Systems from Microwatts to Megawatts for Sustainable AI.” arXiv Preprint arXiv:2410.12032, October. http://arxiv.org/abs/2410.12032v1.\n\n\nIt is important to note that optimizing a system for performance may not lead to the most energy efficient execution. Oftentimes, sacrificing a small amount of performance or accuracy can lead to significant gains in energy efficiency, highlighting the importance of accurately benchmarking power metrics. Future insights from energy efficiency and sustainability benchmarking will enable us to optimize for more sustainable ML systems.\n\n\n12.4.7 Benchmark Example\nTo properly illustrate the components of a systems benchmark, we can look at the keyword spotting benchmark in MLPerf Tiny and explain the motivation behind each decision.\n\nTask\nKeyword spotting was selected as a task because it is a common use case in TinyML that has been well-established for years. Additionally, the typical hardware used for keyword spotting differs substantially from the offerings of other benchmarks, such as MLPerf Inference’s speech recognition task.\n\n\nDataset\nGoogle Speech Commands (Warden 2018) was selected as the best dataset to represent the task. The dataset is well-established in the research community and has permissive licensing, allowing it to be easily used in a benchmark.\n\nWarden, Pete. 2018. “Speech Commands: A Dataset for Limited-Vocabulary Speech Recognition.” ArXiv Preprint abs/1804.03209 (April). http://arxiv.org/abs/1804.03209v1.\n\n\nModel\nThe next core component is the model, which will act as the primary workload for the benchmark. The model should be well established as a solution to the selected task rather than a state-of-the-art solution. The model selected is a simple depthwise separable convolution model. This architecture is not the state-of-the-art solution to the task, but it is well-established and not designed for a specific hardware platform like many state-of-the-art solutions. Despite being an inference benchmark, the benchmark also establishes a reference training recipe to be fully reproducible and transparent.\n\n\nMetrics\nLatency was selected as the primary metric for the benchmark, as keyword spotting systems need to react quickly to maintain user satisfaction. Additionally, given that TinyML systems are often battery-powered, energy consumption is measured to ensure the hardware platform is efficient. The accuracy of the model is also measured to ensure that the optimizations applied by a submitter, such as quantization, don’t degrade the accuracy beyond a threshold.\n\n\nBenchmark Harness\nMLPerf Tiny uses EEMBCs EnergyRunner benchmark harness to load the inputs to the model and isolate and measure the device’s energy consumption. When measuring energy consumption, it’s critical to select a harness that is accurate at the expected power levels of the devices under test and simple enough not to become a burden for the benchmark participants.\n\n\nBaseline Submission\nBaseline submissions are critical for contextualizing results and as a reference point to help participants get started. The baseline submission should prioritize simplicity and readability over state-of-the-art performance. The keyword spotting baseline uses a standard STM microcontroller as its hardware and TensorFlow Lite for Microcontrollers (David et al. 2021) as its inference framework.\n\nDavid, Robert, Jared Duke, Advait Jain, Vijay Janapa Reddi, Nat Jeffries, Jian Li, Nick Kreeger, et al. 2021. “Tensorflow Lite Micro: Embedded Machine Learning for Tinyml Systems.” Proceedings of Machine Learning and Systems 3: 800–811.\n\n\n\n12.4.8 Challenges and Limitations\nWhile benchmarking provides a structured methodology for performance evaluation in complex domains like artificial intelligence and computing, the process also poses several challenges. If not properly addressed, these challenges can undermine the credibility and accuracy of benchmarking results. Some of the predominant difficulties faced in benchmarking include the following:\n\nIncomplete problem coverage: Benchmark tasks may not fully represent the problem space. For instance, common image classification datasets like CIFAR-10 have limited diversity in image types. Algorithms tuned for such benchmarks may fail to generalize well to real-world datasets.\nStatistical insignificance: Benchmarks must have enough trials and data samples to produce statistically significant results. For example, benchmarking an OCR model on only a few text scans may not adequately capture its true error rates.\nLimited reproducibility: Varying hardware, software versions, codebases, and other factors can reduce the reproducibility of benchmark results. MLPerf addresses this by providing reference implementations and environment specifications.\nMisalignment with end goals: Benchmarks focusing only on speed or accuracy metrics may misalign real-world objectives like cost and power efficiency. Benchmarks must reflect all critical performance axes.\nRapid staleness: Due to the rapid pace of advancements in AI and computing, benchmarks and their datasets can quickly become outdated. Maintaining up-to-date benchmarks is thus a persistent challenge.\n\nBut of all these, the most important challenge is benchmark engineering.\n\nHardware Lottery\nThe hardware lottery, first described by Hooker (2021), refers to the situation where a machine learning model’s success or efficiency is significantly influenced by its compatibility with the underlying hardware (Chu et al. 2021). Some models perform exceptionally well not because they are intrinsically superior, but because they are optimized for specific hardware characteristics, such as the parallel processing capabilities of Graphics Processing Units (GPUs) or Tensor Processing Units (TPUs).\n\nHooker, Sara. 2021. “The Hardware Lottery.” Communications of the ACM 64 (12): 58–65. https://doi.org/10.1145/3467017.\nFor instance, Figure 12.4 compares the performance of models across different hardware platforms. The multi-hardware models show comparable results to “MobileNetV3 Large min” on both the CPU uint8 and GPU configurations. However, these multi-hardware models demonstrate significant performance improvements over the MobileNetV3 Large baseline when run on the EdgeTPU and DSP hardware. This emphasizes the variable efficiency of multi-hardware models in specialized computing environments.\n\n\n\n\n\n\nFigure 12.4: Accuracy-latency trade-offs of multiple ML models and how they perform on various hardware. Source: Chu et al. (2021)\n\n\nChu, Grace, Okan Arikan, Gabriel Bender, Weijun Wang, Achille Brighton, Pieter-Jan Kindermans, Hanxiao Liu, Berkin Akin, Suyog Gupta, and Andrew Howard. 2021. “Discovering Multi-Hardware Mobile Models via Architecture Search.” In 2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW), 3016–25. IEEE. https://doi.org/10.1109/cvprw53098.2021.00337.\n\n\nHardware lottery can introduce challenges and biases in benchmarking machine learning systems, as the model’s performance is not solely dependent on the model’s architecture or algorithm but also on the compatibility and synergies with the underlying hardware. This can make it difficult to compare different models fairly and to identify the best model based on its intrinsic merits. It can also lead to a situation where the community converges on models that are a good fit for the popular hardware of the day, potentially overlooking other models that might be superior but incompatible with the current hardware trends.\n\n\nBenchmark Engineering\nHardware lottery occurs when a machine learning model unintentionally performs exceptionally well or poorly on a specific hardware setup due to unforeseen compatibility or incompatibility. The model is not explicitly designed or optimized for that particular hardware by the developers or engineers; rather, it happens to align or (mis)align with the hardware’s capabilities or limitations. In this case, the model’s performance on the hardware is a byproduct of coincidence rather than design.\nIn contrast to the accidental hardware lottery, benchmark engineering involves deliberately optimizing or designing a machine learning model to perform exceptionally well on specific hardware, often to win benchmarks or competitions. This intentional optimization might include tweaking the model’s architecture, algorithms, or parameters to exploit the hardware’s features and capabilities fully.\n\nProblem\nBenchmark engineering refers to tweaking or modifying an AI system to optimize performance on specific benchmark tests, often at the expense of generalizability or real-world performance. This can include adjusting hyperparameters, training data, or other aspects of the system specifically to achieve high scores on benchmark metrics without necessarily improving the overall functionality or utility of the system.\nThe motivation behind benchmark engineering often stems from the desire to achieve high-performance scores for marketing or competitive purposes. High benchmark scores can demonstrate the superiority of an AI system compared to competitors and can be a key selling point for potential users or investors. This pressure to perform well on benchmarks sometimes leads to prioritizing benchmark-specific optimizations over more holistic improvements to the system.\nIt can lead to several risks and challenges. One of the primary risks is that the AI system may perform better in real-world applications than the benchmark scores suggest. This can lead to user dissatisfaction, reputational damage, and potential safety or ethical concerns. Furthermore, benchmark engineering can contribute to a lack of transparency and accountability in the AI community, as it can be difficult to discern how much of an AI system’s performance is due to genuine improvements versus benchmark-specific optimizations.\nThe AI community must prioritize transparency and accountability to mitigate the risks associated with benchmark engineering. This can include disclosing any optimizations or adjustments made specifically for benchmark tests and providing more comprehensive evaluations of AI systems that include real-world performance metrics and benchmark scores. Researchers and developers must prioritize holistic improvements to AI systems that improve their generalizability and functionality across various applications rather than focusing solely on benchmark-specific optimizations.\n\n\nIssues\nOne of the primary problems with benchmark engineering is that it can compromise the real-world performance of AI systems. When developers focus on optimizing their systems to achieve high scores on specific benchmark tests, they may neglect other important system performance aspects crucial in real-world applications. For example, an AI system designed for image recognition might be engineered to perform exceptionally well on a benchmark test that includes a specific set of images but needs help to recognize images slightly different from those in the test set accurately.\nAnother area for improvement with benchmark engineering is that it can result in AI systems that lack generalizability. In other words, while the system may perform well on the benchmark test, it may need help handling a diverse range of inputs or scenarios. For instance, an AI model developed for natural language processing might be engineered to achieve high scores on a benchmark test that includes a specific type of text but fails to process text that falls outside of that specific type accurately.\nIt can also lead to misleading results. When AI systems are engineered to perform well on benchmark tests, the results may not accurately reflect the system’s true capabilities. This can be problematic for users or investors who rely on benchmark scores to make informed decisions about which AI systems to use or invest in. For example, an AI system engineered to achieve high scores on a benchmark test for speech recognition might need to be more capable of accurately recognizing speech in real-world situations, leading users or investors to make decisions based on inaccurate information.\n\n\nMitigation\nThere are several ways to mitigate benchmark engineering. Transparency in the benchmarking process is crucial to maintaining benchmark accuracy and reliability. This involves clearly disclosing the methodologies, data sets, and evaluation criteria used in benchmark tests, as well as any optimizations or adjustments made to the AI system for the purpose of the benchmark.\nOne way to achieve transparency is through the use of open-source benchmarks. Open-source benchmarks are made publicly available, allowing researchers, developers, and other stakeholders to review, critique, and contribute to them, thereby ensuring their accuracy and reliability. This collaborative approach also facilitates sharing best practices and developing more robust and comprehensive benchmarks.\nThe modular design of MLPerf Tiny connects to the problem of benchmark engineering by providing a structured yet flexible approach that encourages a balanced evaluation of TinyML. In benchmark engineering, systems may be overly optimized for specific benchmarks, leading to inflated performance scores that don’t necessarily translate to real-world effectiveness. MLPerf Tiny’s modular design aims to address this issue by allowing contributors to swap out and test specific components within a standardized framework, such as hardware, quantization techniques, or inference models. The reference implementations, highlighted in green and orange in Figure 12.5, provide a baseline for results, enabling flexible yet controlled testing by specifying which components can be modified. This structure supports transparency and flexibility, enabling a focus on genuine improvements rather than benchmark-specific optimizations.\n\n\n\n\n\n\nFigure 12.5: Modular design of the MLPerf Tiny benchmark, showing the reference implementation with modifiable components. This modular approach enables flexible, targeted testing while maintaining a standardized baseline. Source: Banbury et al. (2021).\n\n\nBanbury, Colby, Vijay Janapa Reddi, Peter Torelli, Jeremy Holleman, Nat Jeffries, Csaba Kiraly, Pietro Montino, et al. 2021. “MLPerf Tiny Benchmark.” arXiv Preprint arXiv:2106.07597, June. http://arxiv.org/abs/2106.07597v4.\n\n\nAnother method for achieving transparency is through peer review of benchmarks. This involves having independent experts review and validate the benchmark’s methodology, data sets, and results to ensure their credibility and reliability. Peer review can provide a valuable means of verifying the accuracy of benchmark tests and help build confidence in the results.\nStandardization of benchmarks is another important solution to mitigate benchmark engineering. Standardized benchmarks provide a common framework for evaluating AI systems, ensuring consistency and comparability across different systems and applications. This can be achieved by developing industry-wide standards and best practices for benchmarking and through common metrics and evaluation criteria.\nThird-party verification of results can also be valuable in mitigating benchmark engineering. This involves having an independent third party verify the results of a benchmark test to ensure their credibility and reliability. Third-party verification can build confidence in the results and provide a valuable means of validating the performance and capabilities of AI systems.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Benchmarking AI</span>"
    ]
  },
  {
    "objectID": "contents/core/benchmarking/benchmarking.html#model-benchmarking",
    "href": "contents/core/benchmarking/benchmarking.html#model-benchmarking",
    "title": "12  Benchmarking AI",
    "section": "12.5 Model Benchmarking",
    "text": "12.5 Model Benchmarking\nBenchmarking machine learning models is important for determining the effectiveness and efficiency of various machine learning algorithms in solving specific tasks or problems. By analyzing the results obtained from benchmarking, developers and researchers can identify their models’ strengths and weaknesses, leading to more informed decisions on model selection and further optimization.\nThe evolution and progress of machine learning models are intrinsically linked to the availability and quality of data sets. In machine learning, data acts as the raw material that powers the algorithms, allowing them to learn, adapt, and ultimately perform tasks that were traditionally the domain of humans. Therefore, it is important to understand this history.\n\n12.5.1 Historical Context\nMachine learning datasets have a rich history and have evolved significantly over the years, growing in size, complexity, and diversity to meet the ever-increasing demands of the field. Let’s take a closer look at this evolution, starting from one of the earliest and most iconic datasets – MNIST.\n\nMNIST (1998)\nThe MNIST dataset, created by Yann LeCun, Corinna Cortes, and Christopher J.C. Burges in 1998, can be considered a cornerstone in the history of machine learning datasets. It comprises 70,000 labeled 28x28 pixel grayscale images of handwritten digits (0-9). MNIST has been widely used for benchmarking algorithms in image processing and machine learning as a starting point for many researchers and practitioners. Figure 12.6 shows some examples of handwritten digits.\n\n\n\n\n\n\nFigure 12.6: MNIST handwritten digits. Source: Suvanjanprasai\n\n\n\n\n\nImageNet (2009)\nFast forward to 2009, and we see the introduction of the ImageNet dataset, which marked a significant leap in the scale and complexity of datasets. ImageNet consists of over 14 million labeled images spanning more than 20,000 categories. Fei-Fei Li and her team developed it to advance object recognition and computer vision research. The dataset became synonymous with the ImageNet Large Scale Visual Recognition Challenge (LSVRC), an annual competition crucial in developing deep learning models, including the famous AlexNet in 2012.\n\n\nCOCO (2014)\nThe Common Objects in Context (COCO) dataset (Lin et al. 2014), released in 2014, further expanded the landscape of machine learning datasets by introducing a richer set of annotations. COCO consists of images containing complex scenes with multiple objects, and each image is annotated with object bounding boxes, segmentation masks, and captions, as shown in Figure 12.7. This dataset has been instrumental in advancing research in object detection, segmentation, and image captioning.\n\nLin, Tsung-Yi, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollár, and C. Lawrence Zitnick. 2014. “Microsoft COCO: Common Objects in Context.” In Computer Vision – ECCV 2014, 740–55. Springer; Springer International Publishing. https://doi.org/10.1007/978-3-319-10602-1\\_48.\n\n\n\n\n\n\nFigure 12.7: Example images from the COCO dataset. Source: Coco\n\n\n\n\n\nGPT-3 (2020)\nWhile the above examples primarily focus on image datasets, there have also been significant developments in text datasets. One notable example is GPT-3 (Brown et al. 2020), developed by OpenAI. GPT-3 is a language model trained on diverse internet text. Although the dataset used to train GPT-3 is not publicly available, the model itself, consisting of 175 billion parameters, is a testament to the scale and complexity of modern machine learning datasets and models.\n\nBrown, Tom B., Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, et al. 2020. “Language Models Are Few-Shot Learners.” In Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, Virtual, edited by Hugo Larochelle, Marc’Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin. https://proceedings.neurips.cc/paper/2020/hash/1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html.\n\n\nPresent and Future\nToday, we have a plethora of datasets spanning various domains, including healthcare, finance, social sciences, and more. The following characteristics help us taxonomize the space and growth of machine learning datasets that fuel model development.\n\nDiversity of Data Sets: The variety of data sets available to researchers and engineers has expanded dramatically, covering many fields, including natural language processing, image recognition, and more. This diversity has fueled the development of specialized machine-learning models tailored to specific tasks, such as translation, speech recognition, and facial recognition.\nVolume of Data: The sheer volume of data that has become available in the digital age has also played a crucial role in advancing machine learning models. Large data sets enable models to capture the complexity and nuances of real-world phenomena, leading to more accurate and reliable predictions.\nQuality and Cleanliness of Data: The quality of data is another critical factor that influences the performance of machine learning models. Clean, well-labeled, and unbiased data sets are essential for training models that are robust and fair.\nOpen Access to Data: The availability of open-access data sets has also contributed significantly to machine learning’s progress. Open data allows researchers from around the world to collaborate, share insights, and build upon each other’s work, leading to faster innovation and the development of more advanced models.\nEthics and Privacy Concerns: As data sets grow in size and complexity, ethical considerations and privacy concerns become increasingly important. There is an ongoing debate about the balance between leveraging data for machine learning advancements and protecting individuals’ privacy rights.\n\nThe development of machine learning models relies heavily on the availability of diverse, large, high-quality, and open-access data sets. As we move forward, addressing the ethical considerations and privacy concerns associated with using large data sets is crucial to ensure that machine learning technologies benefit society. There is a growing awareness that data acts as the rocket fuel for machine learning, driving and fueling the development of machine learning models. Consequently, more focus is being placed on developing the data sets themselves. We will explore this in further detail in the data benchmarking section.\n\n\n\n12.5.2 Model Metrics\nMachine learning model evaluation has evolved from a narrow focus on accuracy to a more comprehensive approach considering a range of factors, from ethical considerations and real-world applicability to practical constraints like model size and efficiency. This shift reflects the field’s maturation as machine learning models are increasingly applied in diverse, complex real-world scenarios.\n\nAccuracy\nAccuracy is one of the most intuitive and commonly used metrics for evaluating machine learning models. In the early stages of machine learning, accuracy was often the primary, if not the only, metric considered when evaluating model performance. However, as the field has evolved, it’s become clear that relying solely on accuracy can be misleading, especially in applications where certain types of errors carry significant consequences.\nConsider the example of a medical diagnosis model with an accuracy of 95%. While at first glance this may seem impressive, we must look deeper to assess the model’s performance fully. Suppose the model fails to accurately diagnose severe conditions that, while rare, can have severe consequences; its high accuracy may not be as meaningful. A well-known example of this limitation is Google’s diabetic retinopathy model. While it achieved high accuracy in lab settings, it encountered challenges when deployed in real-world clinics in Thailand, where variations in patient populations, image quality, and environmental factors reduced its effectiveness. This example illustrates that even models with high accuracy need to be tested for their ability to generalize across diverse, unpredictable conditions to ensure reliability and impact in real-world settings.\nSimilarly, if the model performs well on average but exhibits significant disparities in performance across different demographic groups, this, too, would be cause for concern. The evolution of machine learning has thus seen a shift towards a more holistic approach to model evaluation, taking into account not just accuracy, but also other crucial factors such as fairness, transparency, and real-world applicability. A prime example is the Gender Shades project at MIT Media Lab, led by Joy Buolamwini, highlighting biases by performing better on lighter-skinned and male faces compared to darker-skinned and female faces.\nWhile accuracy remains essential for evaluating machine learning models, a comprehensive approach is needed to fully assess performance. This includes additional metrics for fairness, transparency, and real-world applicability, along with rigorous testing across diverse datasets to identify and address biases. This holistic evaluation approach reflects the field’s growing awareness of real-world implications in deploying models.\n\n\nFairness\nFairness in machine learning involves ensuring that models perform consistently across diverse groups, especially in high-impact applications like loan approvals, hiring, and criminal justice. Relying solely on accuracy can be misleading if the model exhibits biased outcomes across demographic groups. For example, a loan approval model with high accuracy may still consistently deny loans to certain groups, raising questions about its fairness.\nBias in models can arise directly, when sensitive attributes like race or gender influence decisions, or indirectly, when neutral features correlate with these attributes, affecting outcomes. Simply relying on accuracy can be insufficient when evaluating models. For instance, consider a loan approval model with a 95% accuracy rate. While this figure may appear impressive at first glance, it does not reveal how the model performs across different demographic groups. For instance, a well-known example is the COMPAS tool used in the US criminal justice system, which showed racial biases in predicting recidivism despite not explicitly using race as a variable.\nAddressing fairness requires analyzing a model’s performance across groups, identifying biases, and applying corrective measures like re-balancing datasets or using fairness-aware algorithms. Researchers and practitioners continuously develop metrics and methodologies tailored to specific use cases to evaluate fairness in real-world scenarios. For example, disparate impact analysis, demographic parity, and equal opportunity are some of the metrics employed to assess fairness. Additionally, transparency and interpretability of models are fundamental to achieving fairness. Tools like AI Fairness 360 and Fairness Indicators help explain how a model makes decisions, allowing developers to detect and correct fairness issues in machine learning models.\nWhile accuracy is a valuable metric, it doesn’t always provide the full picture; assessing fairness ensures models are effective across real-world scenarios. Ensuring fairness in machine learning models, particularly in applications that significantly impact people’s lives, requires rigorous evaluation of the model’s performance across diverse groups, careful identification and mitigation of biases, and implementation of transparency and interpretability measures.\n\n\nComplexity\n\nParameters\nIn the initial stages of machine learning, model benchmarking often relied on parameter counts as a proxy for model complexity. The rationale was that more parameters typically lead to a more complex model, which should, in turn, deliver better performance. However, this approach overlooks the practical costs associated with processing large models. As parameter counts increase, so do the computational resources required, making such models impractical for deployment in real-world scenarios, particularly on devices with limited processing power.\nRelying on parameter counts as a proxy for model complexity also fails to consider the model’s efficiency. A well-optimized model with fewer parameters can often achieve comparable or even superior performance to a larger model. For instance, MobileNets, developed by Google, is a family of models designed specifically for mobile and edge devices. They used depth-wise separable convolutions to reduce parameter counts and computational demands while still maintaining strong performance.\nIn light of these limitations, the field has moved towards a more holistic approach to model benchmarking that considers parameter counts and other crucial factors such as floating-point operations per second (FLOPs), memory consumption, and latency. This comprehensive approach balances performance with deployability, ensuring that models are not only accurate but also efficient and suitable for real-world applications.\n\n\nFLOPS\nFLOPs, or floating-point operations per second, have become a critical metric for representing a model’s computational load. Traditionally, parameter count was used as a proxy for model complexity, based on the assumption that more parameters would yield better performance. However, this approach overlooks the computational cost of processing these parameters, which can impact a model’s usability in real-world scenarios with limited resources.\nFLOPs measure the number of floating-point operations a model performs to generate a prediction. A model with many FLOPs requires substantial computational resources to process the vast number of operations, which may render it impractical for certain applications. Conversely, a model with a lower FLOP count is more lightweight and can be easily deployed in scenarios where computational resources are limited. Figure 12.8, from (Bianco et al. 2018), illustrates the trade-off between ImageNet accuracy, FLOPs, and parameter count, showing that some architectures achieve higher efficiency than others.\n\n\n\n\n\n\nFigure 12.8: A graph that depicts the top-1 imagenet accuracy vs. the FLOP count of a model along with the model’s parameter count. The figure shows a overall tradeoff between model complexity and accuracy, although some model architectures are more efficiency than others. Source: Bianco et al. (2018).\n\n\nBianco, Simone, Remi Cadene, Luigi Celona, and Paolo Napoletano. 2018. “Benchmark Analysis of Representative Deep Neural Network Architectures.” IEEE Access 6: 64270–77. https://doi.org/10.1109/access.2018.2877890.\n\n\nLet’s consider an example. BERT—Bidirectional Encoder Representations from Transformers (Devlin et al. 2019)—is a popular natural language processing model, has over 340 million parameters, making it a large model with high accuracy and impressive performance across various tasks. However, the sheer size of BERT, coupled with its high FLOP count, makes it a computationally intensive model that may not be suitable for real-time applications or deployment on edge devices with limited computational capabilities. In light of this, there has been a growing interest in developing smaller models that can achieve similar performance levels as their larger counterparts while being more efficient in computational load. DistilBERT, for instance, is a smaller version of BERT that retains 97% of its performance while being 40% smaller in terms of parameter count. The size reduction also translates to a lower FLOP count, making DistilBERT a more practical choice for resource-constrained scenarios.\n\nDevlin, Jacob, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. “None.” In Proceedings of the 2019 Conference of the North, 4171–86. Minneapolis, Minnesota: Association for Computational Linguistics. https://doi.org/10.18653/v1/n19-1423.\nWhile parameter count indicates model size, it does not fully capture the computational cost. FLOPs provide a more accurate measure of computational load, highlighting the practical trade-offs in model deployment. This shift from parameter count to FLOPs reflects the field’s growing awareness of deployment challenges in diverse settings.\n\n\nEfficiency\nEfficiency metrics, such as memory consumption and latency/throughput, have also gained prominence. These metrics are particularly crucial when deploying models on edge devices or in real-time applications, as they measure how quickly a model can process data and how much memory it requires. In this context, Pareto curves are often used to visualize the trade-off between different metrics, helping stakeholders decide which model best suits their needs.\n\n\n\n\n12.5.3 Lessons Learned\nModel benchmarking has offered us several valuable insights that can be leveraged to drive innovation in system benchmarks. The progression of machine learning models has been profoundly influenced by the advent of leaderboards and the open-source availability of models and datasets. These elements have served as significant catalysts, propelling innovation and accelerating the integration of cutting-edge models into production environments. However, as we will explore further, these are not the only contributors to the development of machine learning benchmarks.\nLeaderboards play a vital role in providing an objective and transparent method for researchers and practitioners to evaluate the efficacy of different models, ranking them based on their performance in benchmarks. This system fosters a competitive environment, encouraging the development of models that are not only accurate but also efficient. The ImageNet Large Scale Visual Recognition Challenge (ILSVRC) is a prime example of this, with its annual leaderboard significantly contributing to developing groundbreaking models such as AlexNet.\nOpen-source access to state-of-the-art models and datasets further democratizes machine learning, facilitating collaboration among researchers and practitioners worldwide. This open access accelerates the process of testing, validation, and deployment of new models in production environments, as evidenced by the widespread adoption of models like BERT and GPT-3 in various applications, from natural language processing to more complex, multi-modal tasks.\nCommunity collaboration platforms like Kaggle have revolutionized the field by hosting competitions that unite data scientists from across the globe to solve intricate problems. Specific benchmarks serve as the goalposts for innovation and model development.\nMoreover, the availability of diverse and high-quality datasets is paramount in training and testing machine learning models. Datasets such as ImageNet have played an instrumental role in the evolution of image recognition models, while extensive text datasets have facilitated advancements in natural language processing models.\nLastly, the contributions of academic and research institutions must be supported. Their role in publishing research papers, sharing findings at conferences, and fostering collaboration between various institutions has significantly contributed to advancing machine learning models and benchmarks.\n\nEmerging Trends\nAs machine learning models become more sophisticated, so do the benchmarks required to assess them accurately. There are several emerging benchmarks and datasets that are gaining popularity due to their ability to evaluate models in more complex and realistic scenarios:\nMultimodal Datasets: These datasets contain multiple data types, such as text, images, and audio, to represent real-world situations better. An example is the VQA (Visual Question Answering) dataset (Antol et al. 2015), where models’ ability to answer text-based questions about images is tested.\n\nAntol, Stanislaw, Aishwarya Agrawal, Jiasen Lu, Margaret Mitchell, Dhruv Batra, C. Lawrence Zitnick, and Devi Parikh. 2015. “VQA: Visual Question Answering.” In 2015 IEEE International Conference on Computer Vision (ICCV), 2425–33. IEEE. https://doi.org/10.1109/iccv.2015.279.\nFairness and Bias Evaluation: There is an increasing focus on creating benchmarks assessing machine learning models’ fairness and bias. Examples include the AI Fairness 360 toolkit, which offers a comprehensive set of metrics and datasets for evaluating bias in models.\nOut-of-Distribution Generalization: Testing how well models perform on data different from the original training distribution. This evaluates the model’s ability to generalize to new, unseen data. Example benchmarks are Wilds (Koh et al. 2021), RxRx, and ANC-Bench.\n\nKoh, Pang Wei, Shiori Sagawa, Henrik Marklund, Sang Michael Xie, Marvin Zhang, Akshay Balsubramani, Weihua Hu, et al. 2021. “WILDS: A Benchmark of in-the-Wild Distribution Shifts.” In Proceedings of the 38th International Conference on Machine Learning, ICML 2021, 18-24 July 2021, Virtual Event, edited by Marina Meila and Tong Zhang, 139:5637–64. Proceedings of Machine Learning Research. PMLR. http://proceedings.mlr.press/v139/koh21a.html.\n\nHendrycks, Dan, Kevin Zhao, Steven Basart, Jacob Steinhardt, and Dawn Song. 2021. “Natural Adversarial Examples.” In 2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 15257–66. IEEE. https://doi.org/10.1109/cvpr46437.2021.01501.\n\nXie, Cihang, Mingxing Tan, Boqing Gong, Jiang Wang, Alan L. Yuille, and Quoc V. Le. 2020. “Adversarial Examples Improve Image Recognition.” In 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 816–25. IEEE. https://doi.org/10.1109/cvpr42600.2020.00090.\nAdversarial Robustness: Evaluating model performance under adversarial attacks or perturbations to the input data. This tests the model’s robustness. Example benchmarks are ImageNet-A (Hendrycks et al. 2021), ImageNet-C (Xie et al. 2020), and CIFAR-10.1.\nReal-World Performance: Testing models on real-world datasets that closely match end tasks rather than just canned benchmark datasets. Examples are medical imaging datasets for healthcare tasks or customer support chat logs for dialogue systems.\nEnergy and Compute Efficiency: Benchmarks that measure the computational resources required to achieve a particular accuracy. This evaluates the model’s Efficiency. Examples are MLPerf and Greenbench, already discussed in the Systems benchmarking section.\nInterpretability and Explainability: Benchmarks that assess how easy it is to understand and explain a model’s internal logic and predictions. Example metrics are faithfulness to input gradients and coherence of explanations.\n\n\n\n12.5.4 Limitations and Challenges\nWhile model benchmarks are an essential tool in assessing machine learning models, several limitations and challenges should be addressed to ensure that they accurately reflect a model’s performance in real-world scenarios.\nDataset does not Correspond to Real-World Scenarios: Often, the data used in model benchmarks is cleaned and preprocessed to such an extent that it may need to accurately represent the data that a model would encounter in real-world applications. This idealized data version can lead to overestimating a model’s performance. In the case of the ImageNet dataset, the images are well-labeled and categorized. Still, in a real-world scenario, a model may need to deal with blurry images that could be better lit or taken from awkward angles. This discrepancy can significantly affect the model’s performance.\nSim2Real Gap: The Sim2Real gap refers to the difference in the performance of a model when transitioning from a simulated environment to a real-world environment. This gap is often observed in robotics, where a robot trained in a simulated environment struggles to perform tasks in the real world due to the complexity and unpredictability of real-world environments. A robot trained to pick up objects in a simulated environment may need help to perform the same task in the real world because the simulated environment does not accurately represent the complexities of real-world physics, lighting, and object variability.\nChallenges in Creating Datasets: Creating a dataset for model benchmarking is a challenging task that requires careful consideration of various factors such as data quality, diversity, and representation. As discussed in the data engineering section, ensuring that the data is clean, unbiased, and representative of the real-world scenario is crucial for the accuracy and reliability of the benchmark. For example, when creating a dataset for a healthcare-related task, it is important to ensure that the data is representative of the entire population and not biased towards a particular demographic. This ensures that the model performs well across diverse patient populations.\nModel benchmarks are essential in measuring the capability of a model architecture in solving a fixed task, but it is important to address the limitations and challenges associated with them. This includes ensuring that the dataset accurately represents real-world scenarios, addressing the Sim2Real gap, and overcoming the challenges of creating unbiased and representative datasets. By addressing these challenges and many others, we can ensure that model benchmarks provide a more accurate and reliable assessment of a model’s performance in real-world applications.\nThe Speech Commands dataset and its successor MSWC, are common benchmarks for one of the quintessential TinyML applications, keyword spotting. Speech commands establish streaming error metrics beyond the standard top-1 classification accuracy more relevant to the keyword spotting use case. Using case-relevant metrics is what elevates a dataset to a model benchmark.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Benchmarking AI</span>"
    ]
  },
  {
    "objectID": "contents/core/benchmarking/benchmarking.html#data-benchmarking",
    "href": "contents/core/benchmarking/benchmarking.html#data-benchmarking",
    "title": "12  Benchmarking AI",
    "section": "12.6 Data Benchmarking",
    "text": "12.6 Data Benchmarking\nFor the past several years, AI has focused on developing increasingly sophisticated machine learning models like large language models. The goal has been to create models capable of human-level or superhuman performance on a wide range of tasks by training them on massive datasets. This model-centric approach produced rapid progress, with models attaining state-of-the-art results on many established benchmarks. Figure 12.9 shows the performance of AI systems relative to human performance (marked by the horizontal line at 0) across five applications: handwriting recognition, speech recognition, image recognition, reading comprehension, and language understanding. Over the past decade, the AI performance has surpassed that of humans.\n\n\n\n\n\n\nFigure 12.9: AI vs human performane. Source: Kiela et al. (2021).\n\n\n\nHowever, growing concerns about issues like bias, safety, and robustness persist even in models that achieve high accuracy on standard benchmarks. Additionally, some popular datasets used for evaluating models are beginning to saturate, with models reaching near-perfect performance on existing test splits (Kiela et al. 2021). As a simple example, there are test images in the classic MNIST handwritten digit dataset that may look indecipherable to most human evaluators but were assigned a label when the dataset was created - models that happen to agree with those labels may appear to exhibit superhuman performance but instead may only be capturing idiosyncrasies of the labeling and acquisition process from the dataset’s creation in 1994. In the same spirit, computer vision researchers now ask, “Are we done with ImageNet?” (Beyer et al. 2020). This highlights limitations in the conventional model-centric approach of optimizing accuracy on fixed datasets through architectural innovations.\n\nKiela, Douwe, Max Bartolo, Yixin Nie, Divyansh Kaushik, Atticus Geiger, Zhengxuan Wu, Bertie Vidgen, et al. 2021. “Dynabench: Rethinking Benchmarking in NLP.” In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, 4110–24. Online: Association for Computational Linguistics. https://doi.org/10.18653/v1/2021.naacl-main.324.\n\nBeyer, Lucas, Olivier J. Hénaff, Alexander Kolesnikov, Xiaohua Zhai, and Aäron van den Oord. 2020. “Are We Done with ImageNet?” ArXiv Preprint abs/2006.07159 (June). http://arxiv.org/abs/2006.07159v1.\nAn alternative paradigm is emerging called data-centric AI. Rather than treating data as static and focusing narrowly on model performance, this approach recognizes that models are only as good as their training data. So, the emphasis shifts to curating high-quality datasets that better reflect real-world complexity, developing more informative evaluation benchmarks, and carefully considering how data is sampled, preprocessed, and augmented. The goal is to optimize model behavior by improving the data rather than just optimizing metrics on flawed datasets. Data-centric AI critically examines and enhances the data itself to produce beneficial AI. This reflects an important evolution in mindset as the field addresses the shortcomings of narrow benchmarking.\nThis section will explore the key differences between model-centric and data-centric approaches to AI. This distinction has important implications for how we benchmark AI systems. Specifically, we will see how focusing on data quality and Efficiency can directly improve machine learning performance as an alternative to optimizing model architectures solely. The data-centric approach recognizes that models are only as good as their training data. So, enhancing data curation, evaluation benchmarks, and data handling processes can produce AI systems that are safer, fairer, and more robust. Rethinking benchmarking to prioritize data alongside models represents an important evolution as the field strives to deliver trustworthy real-world impact.\n\n12.6.1 Limitations of Model-Centric AI\nIn the model-centric AI era, a prominent characteristic was the development of complex model architectures. Researchers and practitioners dedicated substantial effort to devising sophisticated and intricate models in the quest for superior performance. This frequently involved the incorporation of additional layers and the fine-tuning of a multitude of hyperparameters to achieve incremental improvements in accuracy. Concurrently, there was a significant emphasis on leveraging advanced algorithms. These algorithms, often at the forefront of the latest research, were employed to improve the performance of AI models. The primary aim of these algorithms was to optimize the learning process of models, thereby extracting maximal information from the training data.\nWhile the model-centric approach has been central to many advancements in AI, it has several areas for improvement. First, the development of complex model architectures can often lead to overfitting. This is when the model performs well on the training data but needs to generalize to new, unseen data. The additional layers and complexity can capture noise in the training data as if it were a real pattern, harming the model’s performance on new data.\nSecond, relying on advanced algorithms can sometimes obscure the real understanding of a model’s functioning. These algorithms often act as a black box, making it difficult to interpret how the model is making decisions. This lack of transparency can be a significant hurdle, especially in critical applications such as healthcare and finance, where understanding the model’s decision-making process is crucial.\nThird, the emphasis on achieving state-of-the-art results on benchmark datasets can sometimes be misleading. These datasets need to represent the complexities and variability of real-world data more fully. A model that performs well on a benchmark dataset may not necessarily generalize well to new, unseen data in a real-world application. This discrepancy can lead to false confidence in the model’s capabilities and hinder its practical applicability.\nLastly, the model-centric approach often relies on large labeled datasets for training. However, obtaining such datasets takes time and effort in many real-world scenarios. This reliance on large datasets also limits AI’s applicability in domains where data is scarce or expensive to label.\nAs a result of the above reasons, and many more, the AI community is shifting to a more data-centric approach. Rather than focusing just on model architecture, researchers are now prioritizing curating high-quality datasets, developing better evaluation benchmarks, and considering how data is sampled and preprocessed. The key idea is that models are only as good as their training data. So, focusing on getting the right data will allow us to develop AI systems that are more fair, safe, and aligned with human values. This data-centric shift represents an important change in mindset as AI progresses.\n\n\n12.6.2 The Shift Toward Data-centric AI\nData-centric AI is a paradigm that emphasizes the importance of high-quality, well-labeled, and diverse datasets in developing AI models. In contrast to the model-centric approach, which focuses on refining and iterating on the model architecture and algorithm to improve performance, data-centric AI prioritizes the quality of the input data as the primary driver of improved model performance. High-quality data is clean, well-labeled and representative of the real-world scenarios the model will encounter. In contrast, low-quality data can lead to poor model performance, regardless of the complexity or sophistication of the model architecture.\nData-centric AI puts a strong emphasis on the cleaning and labeling of data. Cleaning involves the removal of outliers, handling missing values, and addressing other data inconsistencies. Labeling, on the other hand, involves assigning meaningful and accurate labels to the data. Both these processes are crucial in ensuring that the AI model is trained on accurate and relevant data. Another important aspect of the data-centric approach is data augmentation. This involves artificially increasing the size and diversity of the dataset by applying various transformations to the data, such as rotation, scaling, and flipping training images. Data augmentation helps in improving the model’s robustness and generalization capabilities.\nThere are several benefits to adopting a data-centric approach to AI development. First and foremost, it leads to improved model performance and generalization capabilities. By ensuring that the model is trained on high-quality, diverse data, the model can better generalize to new, unseen data (Mattson et al. 2020b).\nAdditionally, a data-centric approach can often lead to simpler models that are easier to interpret and maintain. This is because the emphasis is on the data rather than the model architecture, meaning simpler models can achieve high performance when trained on high-quality data.\nThe shift towards data-centric AI represents a significant paradigm shift. By prioritizing the quality of the input data, this approach tries to model performance and generalization capabilities, ultimately leading to more robust and reliable AI systems. Figure 12.10 illustrates this difference. As we continue to advance in our understanding and application of AI, the data-centric approach is likely to play an important role in shaping the future of this field.\n\n\n\n\n\n\nFigure 12.10: Model-centric vs. Data-centric ML development. Source: NVIDIA\n\n\n\n\n\n12.6.3 Benchmarking Data\nData benchmarking focuses on evaluating common issues in datasets, such as identifying label errors, noisy features, representation imbalance (for example, out of the 1000 classes in Imagenet-1K, there are over 100 categories which are just types of dogs), class imbalance (where some classes have many more samples than others), whether models trained on a given dataset can generalize to out-of-distribution features, or what types of biases might exist in a given dataset (Mattson et al. 2020b). In its simplest form, data benchmarking seeks to improve accuracy on a test set by removing noisy or mislabeled training samples while keeping the model architecture fixed. Recent competitions in data benchmarking have invited participants to submit novel augmentation strategies and active learning techniques.\n\nMattson, Peter, Vijay Janapa Reddi, Christine Cheng, Cody Coleman, Greg Diamos, David Kanter, Paulius Micikevicius, et al. 2020b. “MLPerf: An Industry Standard Benchmark Suite for Machine Learning Performance.” IEEE Micro 40 (2): 8–16. https://doi.org/10.1109/mm.2020.2974843.\nData-centric techniques continue to gain attention in benchmarking, especially as foundation models are increasingly trained on self-supervised objectives. Compared to smaller datasets like Imagenet-1K, massive datasets commonly used in self-supervised learning, such as Common Crawl, OpenImages, and LAION-5B, contain higher amounts of noise, duplicates, bias, and potentially offensive data.\nDataComp is a recently launched dataset competition that targets the evaluation of large corpora. DataComp focuses on language-image pairs used to train CLIP models. The introductory whitepaper finds that when the total compute budget for training is constant, the best-performing CLIP models on downstream tasks, such as ImageNet classification, are trained on just 30% of the available training sample pool. This suggests that proper filtering of large corpora is critical to improving the accuracy of foundation models. Similarly, Demystifying CLIP Data (Xu et al. 2023) asks whether the success of CLIP is attributable to the architecture or the dataset.\n\nXu, Hu, Saining Xie, Xiaoqing Ellen Tan, Po-Yao Huang, Russell Howes, Vasu Sharma, Shang-Wen Li, Gargi Ghosh, Luke Zettlemoyer, and Christoph Feichtenhofer. 2023. “Demystifying CLIP Data.” ArXiv Preprint abs/2309.16671 (September). http://arxiv.org/abs/2309.16671v4.\nDataPerf is another recent effort focusing on benchmarking data in various modalities. DataPerf provides rounds of online competition to spur improvement in datasets. The inaugural offering launched with challenges in vision, speech, acquisition, debugging, and text prompting for image generation.\n\n\n12.6.4 Data Efficiency\nAs machine learning models grow larger and more complex and compute resources become more scarce in the face of rising demand, it becomes challenging to meet the computation requirements even with the largest machine learning fleets. To overcome these challenges and ensure machine learning system scalability, it is necessary to explore novel opportunities that increase conventional approaches to resource scaling.\nImproving data quality can be a useful method to impact machine learning system performance significantly. One of the primary benefits of enhancing data quality is the potential to reduce the size of the training dataset while still maintaining or even improving model performance. This data size reduction directly relates to the amount of training time required, thereby allowing models to converge more quickly and efficiently. Achieving this balance between data quality and dataset size is a challenging task that requires the development of sophisticated methods, algorithms, and techniques.\nSeveral approaches can be taken to improve data quality. These methods include and are not limited to the following:\n\nData Cleaning: This involves handling missing values, correcting errors, and removing outliers. Clean data ensures that the model is not learning from noise or inaccuracies.\nData Interpretability and Explainability: Common techniques include LIME (Ribeiro, Singh, and Guestrin 2016), which provides insight into the decision boundaries of classifiers, and Shapley values (Lundberg and Lee 2017), which estimate the importance of individual samples in contributing to a model’s predictions.\nFeature Engineering: Transforming or creating new features can significantly improve model performance by providing more relevant information for learning.\nData Augmentation: Augmenting data by creating new samples through various transformations can help improve model robustness and generalization.\nActive Learning: This is a semi-supervised learning approach where the model actively queries a human oracle to label the most informative samples (Coleman et al. 2022). This ensures that the model is trained on the most relevant data.\nDimensionality Reduction: Techniques like PCA can reduce the number of features in a dataset, thereby reducing complexity and training time.\n\n\nRibeiro, Marco Tulio, Sameer Singh, and Carlos Guestrin. 2016. “” Why Should i Trust You?” Explaining the Predictions of Any Classifier.” In Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, 1135–44.\n\nLundberg, Scott M., and Su-In Lee. 2017. “A Unified Approach to Interpreting Model Predictions.” In Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017, December 4-9, 2017, Long Beach, CA, USA, edited by Isabelle Guyon, Ulrike von Luxburg, Samy Bengio, Hanna M. Wallach, Rob Fergus, S. V. N. Vishwanathan, and Roman Garnett, 4765–74. https://proceedings.neurips.cc/paper/2017/hash/8a20a8621978632d76c43dfd28b67767-Abstract.html.\n\nColeman, Cody, Edward Chou, Julian Katz-Samuels, Sean Culatana, Peter Bailis, Alexander C. Berg, Robert Nowak, Roshan Sumbaly, Matei Zaharia, and I. Zeki Yalniz. 2022. “Similarity Search for Efficient Active Learning and Search of Rare Concepts.” Proceedings of the AAAI Conference on Artificial Intelligence 36 (6): 6402–10. https://doi.org/10.1609/aaai.v36i6.20591.\nThere are many other methods in the wild. But the goal is the same. Refining the dataset and ensuring it is of the highest quality can reduce the training time required for models to converge. However, achieving this requires developing and implementing sophisticated methods, algorithms, and techniques that can clean, preprocess, and augment data while retaining the most informative samples. This is an ongoing challenge that will require continued research and innovation in the field of machine learning.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Benchmarking AI</span>"
    ]
  },
  {
    "objectID": "contents/core/benchmarking/benchmarking.html#the-trifecta",
    "href": "contents/core/benchmarking/benchmarking.html#the-trifecta",
    "title": "12  Benchmarking AI",
    "section": "12.7 The Trifecta",
    "text": "12.7 The Trifecta\nWhile system, model, and data benchmarks have traditionally been studied in isolation, there is a growing recognition that to understand and advance AI fully, we must take a more holistic view. By iterating between benchmarking systems, models, and datasets together, novel insights that are not apparent when these components are analyzed separately may emerge. System performance impacts model accuracy, model capabilities drive data needs, and data characteristics shape system requirements.\nBenchmarking the triad of system, model, and data in an integrated fashion will likely lead to discoveries about the co-design of AI systems, the generalization properties of models, and the role of data curation and quality in enabling performance. Rather than narrow benchmarks of individual components, the future of AI requires benchmarks that evaluate the symbiotic relationship between computing platforms, algorithms, and training data. This systems-level perspective will be critical to overcoming current limitations and unlocking the next level of AI capabilities.\nFigure 12.11 illustrates the many potential ways to interplay data benchmarking, model benchmarking, and system infrastructure benchmarking together. Exploring these intricate interactions is likely to uncover new optimization opportunities and enhancement capabilities. The data, model, and system benchmark triad offers a rich space for co-design and co-optimization.\n\n\n\n\n\n\nFigure 12.11: Benchmarking trifecta.\n\n\n\nWhile this integrated perspective represents an emerging trend, the field has much more to discover about the synergies and trade-offs between these components. As we iteratively benchmark combinations of data, models, and systems, new insights that remain hidden when these elements are studied in isolation will emerge. This multifaceted benchmarking approach charting the intersections of data, algorithms, and hardware promises to be a fruitful avenue for major progress in AI, even though it is still in its early stages.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Benchmarking AI</span>"
    ]
  },
  {
    "objectID": "contents/core/benchmarking/benchmarking.html#benchmarks-for-emerging-technologies",
    "href": "contents/core/benchmarking/benchmarking.html#benchmarks-for-emerging-technologies",
    "title": "12  Benchmarking AI",
    "section": "12.8 Benchmarks for Emerging Technologies",
    "text": "12.8 Benchmarks for Emerging Technologies\nGiven their significant differences from existing techniques, emerging technologies can be particularly challenging to design benchmarks for. Standard benchmarks used for existing technologies may not highlight the key features of the new approach. In contrast, new benchmarks may be seen as contrived to favor the emerging technology over others. They may be so different from existing benchmarks that they cannot be understood and lose insightful value. Thus, benchmarks for emerging technologies must balance fairness, applicability, and ease of comparison with existing benchmarks.\nAn example of emerging technology where benchmarking has proven to be especially difficult is in Neuromorphic Computing. Using the brain as a source of inspiration for scalable, robust, and energy-efficient general intelligence, neuromorphic computing (Schuman et al. 2022) directly incorporates biologically realistic mechanisms in both computing algorithms and hardware, such as spiking neural networks (Maass 1997) and non-von Neumann architectures for executing them (Davies et al. 2018; Modha et al. 2023). From a full-stack perspective of models, training techniques, and hardware systems, neuromorphic computing differs from conventional hardware and AI. Thus, there is a key challenge in developing fair and useful benchmarks for guiding the technology.\n\nSchuman, Catherine D., Shruti R. Kulkarni, Maryam Parsa, J. Parker Mitchell, Prasanna Date, and Bill Kay. 2022. “Opportunities for Neuromorphic Computing Algorithms and Applications.” Nature Computational Science 2 (1): 10–19. https://doi.org/10.1038/s43588-021-00184-y.\n\nMaass, Wolfgang. 1997. “Networks of Spiking Neurons: The Third Generation of Neural Network Models.” Neural Networks 10 (9): 1659–71. https://doi.org/10.1016/s0893-6080(97)00011-7.\n\nDavies, Mike, Narayan Srinivasa, Tsung-Han Lin, Gautham Chinya, Yongqiang Cao, Sri Harsha Choday, Georgios Dimou, et al. 2018. “Loihi: A Neuromorphic Manycore Processor with on-Chip Learning.” IEEE Micro 38 (1): 82–99. https://doi.org/10.1109/mm.2018.112130359.\n\nModha, Dharmendra S., Filipp Akopyan, Alexander Andreopoulos, Rathinakumar Appuswamy, John V. Arthur, Andrew S. Cassidy, Pallab Datta, et al. 2023. “Neural Inference at the Frontier of Energy, Space, and Time.” Science 382 (6668): 329–35. https://doi.org/10.1126/science.adh1174.\n\nYik, Jason, Korneel Van den Berghe, Douwe den Blanken, Younes Bouhadjar, Maxime Fabre, Paul Hueber, Denis Kleyko, et al. 2023. “NeuroBench: A Framework for Benchmarking Neuromorphic Computing Algorithms and Systems,” April. http://arxiv.org/abs/2304.04640v3.\nAn ongoing initiative to develop standard neuromorphic benchmarks is NeuroBench (Yik et al. 2023). To suitably benchmark neuromorphic, NeuroBench follows high-level principles of inclusiveness through task and metric applicability to both neuromorphic and non-neuromorphic solutions, actionability of implementation using common tooling, and iterative updates to continue to ensure relevance as the field rapidly grows. NeuroBench and other benchmarks for emerging technologies provide critical guidance for future techniques, which may be necessary as the scaling limits of existing approaches draw nearer.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Benchmarking AI</span>"
    ]
  },
  {
    "objectID": "contents/core/benchmarking/benchmarking.html#conclusion",
    "href": "contents/core/benchmarking/benchmarking.html#conclusion",
    "title": "12  Benchmarking AI",
    "section": "12.9 Conclusion",
    "text": "12.9 Conclusion\nWhat gets measured gets improved. This chapter has explored the multifaceted nature of benchmarking spanning systems, models, and data. Benchmarking is important to advancing AI by providing the essential measurements to track progress.\nML system benchmarks enable optimization across speed, Efficiency, and scalability metrics. Model benchmarks drive innovation through standardized tasks and metrics beyond accuracy. Data benchmarks highlight issues of quality, balance, and representation.\nImportantly, evaluating these components in isolation has limitations. In the future, more integrated benchmarking will likely be used to explore the interplay between system, model, and data benchmarks. This view promises new insights into co-designing data, algorithms, and infrastructure.\nAs AI grows more complex, comprehensive benchmarking becomes even more critical. Standards must continuously evolve to measure new capabilities and reveal limitations. Close collaboration between industry, academics, national labels, etc., is essential to developing benchmarks that are rigorous, transparent, and socially beneficial.\nBenchmarking provides the compass to guide progress in AI. By persistently measuring and openly sharing results, we can navigate toward performant, robust, and trustworthy systems. If AI is to serve societal and human needs properly, it must be benchmarked with humanity’s best interests in mind. To this end, there are emerging areas, such as benchmarking the safety of AI systems, but that’s for another day and something we can discuss further in Generative AI!\nBenchmarking is a continuously evolving topic. The article The Olympics of AI: Benchmarking Machine Learning Systems covers several emerging subfields in AI benchmarking, including robotics, extended reality, and neuromorphic computing that we encourage the reader to pursue.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Benchmarking AI</span>"
    ]
  },
  {
    "objectID": "contents/core/benchmarking/benchmarking.html#sec-benchmarking-ai-resource",
    "href": "contents/core/benchmarking/benchmarking.html#sec-benchmarking-ai-resource",
    "title": "12  Benchmarking AI",
    "section": "12.10 Resources",
    "text": "12.10 Resources\nHere is a curated list of resources to support students and instructors in their learning and teaching journeys. We are continuously working on expanding this collection and will add new exercises soon.\n\n\n\n\n\n\nSlides\n\n\n\n\n\nThese slides are a valuable tool for instructors to deliver lectures and for students to review the material at their own pace. We encourage students and instructors to leverage these slides to improve their understanding and facilitate effective knowledge transfer.\n\nWhy is benchmarking important?\nEmbedded inference benchmarking.\n\n\n\n\n\n\n\n\n\n\nVideos\n\n\n\n\n\n\nComing soon.\n\n\n\n\n\n\n\n\n\n\nExercises\n\n\n\n\n\nTo reinforce the concepts covered in this chapter, we have curated a set of exercises that challenge students to apply their knowledge and deepen their understanding.\n\nExercise 12.1\nExercise 12.2",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Benchmarking AI</span>"
    ]
  },
  {
    "objectID": "contents/core/ops/ops.html",
    "href": "contents/core/ops/ops.html",
    "title": "13  ML Operations",
    "section": "",
    "text": "Purpose\nResources: Slides, Videos, Exercises\nWhat systematic principles enable continuous evolution of machine learning systems in production, and how do operational requirements shape deployment architecture?\nThe transition from experimental to production environments represents a fundamental challenge in scaling machine learning systems. Operational practices reveal essential patterns for maintaining reliability and adaptability throughout the system lifecycle, highlighting critical relationships between deployment architecture and continuous delivery. The implementation of MLOps frameworks draws out the key trade-offs between automation, monitoring, and governance that shape system evolution. Understanding these operational dynamics provides insights into managing complex AI workflows, establishing core principles for creating systems that remain robust and responsive in dynamic production environments.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>ML Operations</span>"
    ]
  },
  {
    "objectID": "contents/core/ops/ops.html#purpose",
    "href": "contents/core/ops/ops.html#purpose",
    "title": "13  ML Operations",
    "section": "",
    "text": "Learning Objectives\n\n\n\n\nUnderstand what MLOps is and why it is needed\nLearn the architectural patterns for traditional MLOps\nContrast traditional vs. embedded MLOps across the ML lifecycle\nIdentify key constraints of embedded environments\nLearn strategies to mitigate embedded ML challenges\nExamine real-world case studies demonstrating embedded MLOps principles\nAppreciate the need for holistic technical and human approaches",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>ML Operations</span>"
    ]
  },
  {
    "objectID": "contents/core/ops/ops.html#overview",
    "href": "contents/core/ops/ops.html#overview",
    "title": "13  ML Operations",
    "section": "13.1 Overview",
    "text": "13.1 Overview\nMachine Learning Operations (MLOps) is a systematic approach that combines machine learning (ML), data science, and software engineering to automate the end-to-end ML lifecycle. This includes everything from data preparation and model training to deployment and maintenance. MLOps ensures that ML models are developed, deployed, and maintained efficiently and effectively.\nLet’s start by taking a general example (i.e., non-edge ML) case. Consider a ridesharing company that wants to deploy a machine-learning model to predict real-time rider demand. The data science team spends months developing a model, but when it’s time to deploy, they realize it needs to be compatible with the engineering team’s production environment. Deploying the model requires rebuilding it from scratch, which costs weeks of additional work. This is where MLOps comes in.\nWith MLOps, protocols, and tools, the model developed by the data science team can be seamlessly deployed and integrated into the production environment. In essence, MLOps removes friction during the development, deployment, and maintenance of ML systems. It improves collaboration between teams through defined workflows and interfaces. MLOps also accelerates iteration speed by enabling continuous delivery for ML models.\nFor the ridesharing company, implementing MLOps means their demand prediction model can be frequently retrained and deployed based on new incoming data. This keeps the model accurate despite changing rider behavior. MLOps also allows the company to experiment with new modeling techniques since models can be quickly tested and updated.\nOther MLOps benefits include enhanced model lineage tracking, reproducibility, and auditing. Cataloging ML workflows and standardizing artifacts - such as logging model versions, tracking data lineage, and packaging models and parameters - enables deeper insight into model provenance. Standardizing these artifacts facilitates tracing a model back to its origins, replicating the model development process, and examining how a model version has changed over time. This also facilitates regulation compliance, which is especially critical in regulated industries like healthcare and finance, where being able to audit and explain models is important.\nMajor organizations adopt MLOps to boost productivity, increase collaboration, and accelerate ML outcomes. It provides the frameworks, tools, and best practices to effectively manage ML systems throughout their lifecycle. This results in better-performing models, faster time-to-value, and sustained competitive advantage. As we explore MLOps further, consider how implementing these practices can help address embedded ML challenges today and in the future.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>ML Operations</span>"
    ]
  },
  {
    "objectID": "contents/core/ops/ops.html#historical-context",
    "href": "contents/core/ops/ops.html#historical-context",
    "title": "13  ML Operations",
    "section": "13.2 Historical Context",
    "text": "13.2 Historical Context\nMLOps has its roots in DevOps, a set of practices combining software development (Dev) and IT operations (Ops) to shorten the development lifecycle and provide continuous delivery of high-quality software. The parallels between MLOps and DevOps are evident in their focus on automation, collaboration, and continuous improvement. In both cases, the goal is to break down silos between different teams (developers, operations, and, in the case of MLOps, data scientists and ML engineers) and to create a more streamlined and efficient process. It is useful to understand the history of this evolution better to understand MLOps in the context of traditional systems.\n\n13.2.1 DevOps\nThe term “DevOps” was first coined in 2009 by Patrick Debois, a consultant and Agile practitioner. Debois organized the first DevOpsDays conference in Ghent, Belgium, in 2009. The conference brought together development and operations professionals to discuss ways to improve collaboration and automate processes.\nDevOps has its roots in the Agile movement, which began in the early 2000s. Agile provided the foundation for a more collaborative approach to software development and emphasized small, iterative releases. However, Agile primarily focuses on collaboration between development teams. As Agile methodologies became more popular, organizations realized the need to extend this collaboration to operations teams.\nThe siloed nature of development and operations teams often led to inefficiencies, conflicts, and delays in software delivery. This need for better collaboration and integration between these teams led to the DevOps movement. DevOps can be seen as an extension of the Agile principles, including operations teams.\nThe key principles of DevOps include collaboration, automation, continuous integration, delivery, and feedback. DevOps focuses on automating the entire software delivery pipeline, from development to deployment. It improves the collaboration between development and operations teams, utilizing tools like Jenkins, Docker, and Kubernetes to streamline the development lifecycle.\nWhile Agile and DevOps share common principles around collaboration and feedback, DevOps specifically targets integrating development and IT operations - expanding Agile beyond just development teams. It introduces practices and tools to automate software delivery and improve the speed and quality of software releases.\n\n\n13.2.2 MLOps\nMLOps, on the other hand, stands for Machine Learning Operations, and it extends the principles of DevOps to the ML lifecycle. MLOps automates and streamlines the end-to-end ML lifecycle, from data preparation and model development to deployment and monitoring. The main focus of MLOps is to facilitate collaboration between data scientists, data engineers, and IT operations and to automate the deployment, monitoring, and management of ML models. Some key factors led to the rise of MLOps.\n\nData drift: Data drift degrades model performance over time, motivating the need for rigorous monitoring and automated retraining procedures provided by MLOps.\nReproducibility: The lack of reproducibility in machine learning experiments motivated MLOps systems to track code, data, and environment variables to enable reproducible ML workflows.\nExplainability: The black box nature and lack of explainability of complex models motivated the need for MLOps capabilities to increase model transparency and explainability.\nMonitoring: The inability to reliably monitor model performance post-deployment highlighted the need for MLOps solutions with robust model performance instrumentation and alerting.\nFriction: The friction in manually retraining and deploying models motivated the need for MLOps systems that automate machine learning deployment pipelines.\nOptimization: The complexity of configuring machine learning infrastructure motivated the need for MLOps platforms with optimized, ready-made ML infrastructure.\n\nWhile DevOps and MLOps share the common goal of automating and streamlining processes, they differ significantly in their focus and challenges. DevOps primarily deals with software development and IT operations. It enables collaboration between these teams and automate software delivery. In contrast, MLOps focuses on the machine learning lifecycle. It addresses additional complexities such as data versioning, model versioning, and model monitoring. MLOps requires collaboration among a broader range of stakeholders, including data scientists, data engineers, and IT operations. It goes beyond the scope of traditional DevOps by incorporating the unique challenges of managing ML models throughout their lifecycle. Table 13.1 provides a side-by-side comparison of DevOps and MLOps, highlighting their key differences and similarities.\n\n\n\nTable 13.1: Comparison of DevOps and MLOps.\n\n\n\n\n\n\n\n\n\n\nAspect\nDevOps\nMLOps\n\n\n\n\nObjective\nStreamlining software development and operations processes\nOptimizing the lifecycle of machine learning models\n\n\nMethodology\nContinuous Integration and Continuous Delivery (CI/CD) for software development\nSimilar to CI/CD but focuses on machine learning workflows\n\n\nPrimary Tools\nVersion control (Git), CI/CD tools (Jenkins, Travis CI), Configuration management (Ansible, Puppet)\nData versioning tools, Model training and deployment tools, CI/CD pipelines tailored for ML\n\n\nPrimary Concerns\nCode integration, Testing, Release management, Automation, Infrastructure as code\nData management, Model versioning, Experiment tracking, Model deployment, Scalability of ML workflows\n\n\nTypical Outcomes\nFaster and more reliable software releases, Improved collaboration between development and operations teams\nEfficient management and deployment of machine learning models, Enhanced collaboration between data scientists and engineers\n\n\n\n\n\n\nLearn more about ML Lifecycles through a case study featuring speech recognition in Video 13.1.\n\n\n\n\n\n\nImportant 13.1: MLOps",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>ML Operations</span>"
    ]
  },
  {
    "objectID": "contents/core/ops/ops.html#key-components-of-mlops",
    "href": "contents/core/ops/ops.html#key-components-of-mlops",
    "title": "13  ML Operations",
    "section": "13.3 Key Components of MLOps",
    "text": "13.3 Key Components of MLOps\nThe core components of MLOps form a comprehensive framework that supports the end-to-end lifecycle of ML models in production, from initial development to deployment and ongoing management. In this section, we build on topics like automation and monitoring from previous chapters, integrating them into a broader framework while also introducing additional key practices like governance. Each component contributes to smoother, more streamlined ML operations, with popular tools helping teams tackle specific tasks within this ecosystem. Together, these elements make MLOps a robust approach to managing ML models and creating long-term value within organizations.\nFigure 13.1 illustrates the comprehensive MLOps system stack. It shows the various layers involved in machine learning operations. At the top of the stack are ML Models/Applications, such as BERT, followed by ML Frameworks/Platforms like PyTorch. The core MLOps layer, labeled as Model Orchestration, encompasses several key components: Data Management, CI/CD, Model Training, Model Evaluation, Deployment, and Model Serving. Underpinning the MLOps layer is the Infrastructure layer, represented by technologies such as Kubernetes. This layer manages aspects such as Job Scheduling, Resource Management, Capacity Management, and Monitoring, among others. Holding it all together is the Hardware layer, which provides the necessary computational resources for ML operations.\n\n\n\n\n\n\nFigure 13.1: The MLOps stack, including ML Models, Frameworks, Model Orchestration, Infrastructure, and Hardware, illustrates the end-to-end workflow of MLOps.\n\n\n\nThis layered approach in Figure 13.1 demonstrates how MLOps integrates various technologies and processes to facilitate the development, deployment, and management of machine learning models in a production environment. The figure effectively illustrates the interdependencies between different components and how they come together to form a comprehensive MLOps ecosystem.\n\n13.3.1 Data Management\nData in its raw form, whether collected from sensors, databases, apps, or other systems, often requires significant preparation before it can be used for training or inference. Issues like inconsistent formats, missing values, and evolving labeling conventions can lead to inefficiencies and poor model performance if not systematically addressed. Robust data management practices ensure that data remains high quality, traceable, and readily accessible throughout the ML lifecycle, forming the foundation of scalable machine learning systems.\nOne key aspect of data management is version control. Tools like Git, GitHub, and GitLab enable teams to track changes to datasets, collaborate on curation, and revert to earlier versions when necessary. Alongside versioning, annotating and labeling datasets is crucial for supervised learning tasks. Software like LabelStudio helps distributed teams tag data consistently across large-scale datasets while maintaining access to earlier versions as labeling conventions evolve. These practices not only enhance collaboration but also ensure that models are trained on reliable, well-organized data.\nOnce prepared, datasets are typically stored on scalable cloud storage solutions like Amazon S3 or Google Cloud Storage. These services provide versioning, resilience, and granular access controls, safeguarding sensitive data while maintaining flexibility for analysis and modeling. To streamline the transition from raw data to analysis-ready formats, teams build automated pipelines using tools such as Prefect, Apache Airflow, and dbt. These pipelines automate tasks like data extraction, cleaning, deduplication, and transformation, reducing manual overhead and improving efficiency.\nFor example, a data pipeline might ingest information from PostgreSQL databases, REST APIs, and CSV files stored in S3, applying transformations to produce clean, aggregated datasets. The output can be stored in feature stores like Tecton or Feast, which provide low-latency access for both training and predictions. In an industrial predictive maintenance scenario, sensor data could be processed alongside maintenance records, resulting in enriched datasets stored in Feast for models to access the latest information seamlessly.\nBy integrating version control, annotation tools, storage solutions, and automated pipelines, data management becomes a critical enabler for effective MLOps. These practices ensure that data is not only clean and accessible but also consistently aligned with evolving project needs, allowing machine learning systems to deliver reliable and scalable performance in production environments.\nVideo 13.2 below is a short overview of data pipelines.\n\n\n\n\n\n\nImportant 13.2: Data Pipelines\n\n\n\n\n\n\n\n\n13.3.2 CI/CD Pipelines\nContinuous integration and continuous delivery (CI/CD) pipelines actively automate the progression of ML models from initial development into production deployment. Adapted for ML systems, CI/CD principles empower teams to rapidly and robustly deliver new models with minimized manual errors.\nCI/CD pipelines orchestrate key steps, including checking out new code changes, transforming data, training and registering new models, validation testing, containerization, deploying to environments like staging clusters, and promoting to production. Teams leverage popular CI/CD solutions like Jenkins, CircleCI and GitHub Actions to execute these MLOps pipelines, while Prefect, Metaflow and Kubeflow offer ML-focused options.\nFigure 13.2 illustrates a CI/CD pipeline specifically tailored for MLOps. The process starts with a dataset and feature repository (on the left), which feeds into a dataset ingestion stage. Post-ingestion, the data undergoes validation to ensure its quality before being transformed for training. Parallel to this, a retraining trigger can initiate the pipeline based on specified criteria. The data then passes through a model training/tuning phase within a data processing engine, followed by model evaluation and validation. Once validated, the model is registered and stored in a machine learning metadata and artifact repository. The final stage involves deploying the trained model back into the dataset and feature repository, thereby creating a cyclical process for continuous improvement and deployment of machine learning models.\n\n\n\n\n\n\nFigure 13.2: MLOps CI/CD diagram. Source: HarvardX.\n\n\n\nFor example, when a data scientist checks improvements to an image classification model into a GitHub repository, this actively triggers a Jenkins CI/CD pipeline. The pipeline reruns data transformations and model training on the latest data, tracking experiments with MLflow. After automated validation testing, teams deploy the model container to a Kubernetes staging cluster for further QA. Once approved, Jenkins facilitates a phased rollout of the model to production with canary deployments to catch any issues. If anomalies are detected, the pipeline enables teams to roll back to the previous model version gracefully.\nCI/CD pipelines empower teams to iterate and deliver ML models rapidly by connecting the disparate steps from development to deployment under continuous automation. Integrating MLOps tools like MLflow enhances model packaging, versioning, and pipeline traceability. CI/CD is integral for progressing models beyond prototypes into sustainable business systems.\n\n\n13.3.3 Model Training\nModel training is a critical phase where data scientists experiment with various ML architectures and algorithms to optimize models that extract insights from data. MLOps introduces best practices and automation to make this iterative process more efficient and reproducible. Modern ML frameworks like TensorFlow, PyTorch, and Keras provide pre-built components that simplify designing neural networks and other model architectures. These tools allow data scientists to focus on creating high-performing models using built-in modules for layers, activations, and loss functions.\nTo make the training process efficient and reproducible, MLOps introduces best practices such as version-controlling training code using Git and hosting it in repositories like GitHub. Reproducible environments, often managed through interactive tools like Jupyter notebooks, allow teams to bundle data ingestion, preprocessing, model development, and evaluation in a single document. These notebooks are not only version-controlled but can also be integrated into automated pipelines for continuous retraining.\nAutomation plays a significant role in standardizing training workflows. Capabilities such as hyperparameter tuning, neural architecture search, and automatic feature selection are commonly integrated into MLOps pipelines to iterate rapidly and find optimal configurations. CI/CD pipelines orchestrate training workflows by automating tasks like data preprocessing, model training, evaluation, and registration. For example, a Jenkins pipeline can trigger a Python script to retrain a TensorFlow model, validate its performance against pre-defined metrics, and deploy it if thresholds are met.\nCloud-managed training services have revolutionized the accessibility of high-performance hardware for training models. These services provide on-demand access to GPU-accelerated infrastructure, making advanced training feasible even for small teams. Depending on the provider, developers may manage the training workflow themselves or rely on fully managed options like Vertex AI Fine Tuning, which can automatically finetune a base model using a labeled dataset. However, it is important to note that GPU hardware demand often exceeds supply, and availability may vary based on region or contractual agreements, posing potential bottlenecks for teams relying on cloud services.\nAn example workflow has a data scientist using a PyTorch notebook to develop a CNN model for image classification. The fastai library provides high-level APIs to simplify training CNNs on image datasets. The notebook trains the model on sample data, evaluates accuracy metrics, and tunes hyperparameters like learning rate and layers to optimize performance. This reproducible notebook is version-controlled and integrated into a retraining pipeline.\nBy automating and standardizing model training, leveraging managed cloud services, and integrating modern frameworks, teams can accelerate experimentation and build robust, production-ready ML models.\n\n\n13.3.4 Model Evaluation\nBefore deploying models, teams perform rigorous evaluation and testing to validate meeting performance benchmarks and readiness for release. MLOps provides best practices for model validation, auditing, and controlled testing methods to minimize risks during deployment.\nThe evaluation process begins with testing models against holdout test datasets that are independent of the training data but originate from the same distribution as production data. Key metrics such as accuracy, AUC, precision, recall, and F1 score are calculated to quantify model performance. Tracking these metrics over time helps teams identify trends and potential degradation in model behavior, particularly when evaluation data comes from live production streams. This is vital for detecting data drift, where changes in input data distributions can erode model accuracy.\nTo validate real-world performance, canary testing deploys the model to a small subset of users. This gradual rollout allows teams to monitor metrics in a live environment and catch potential issues before full-scale deployment. By incrementally increasing traffic to the new model, teams can confidently evaluate its impact on end-user experience. For instance, a retailer might test a personalized recommendation model by comparing its accuracy and diversity metrics against historical data. During the testing phase, the team tracks live performance metrics and identifies a slight accuracy decline over two weeks. To ensure stability, the model is initially deployed to 5% of web traffic, monitored for potential issues, and only rolled out widely after proving robust in production.\nML models deployed to the cloud benefit from constant internet connectivity and the ability to log every request and response. This makes it feasible to replay or generate synthetic requests for comparing different models and versions. Some providers offer tools that automate parts of the evaluation process, such as tracking hyperparameter experiments or comparing model runs. For instance, platforms like Weights and Biases streamline this process by automating experiment tracking and generating artifacts from training runs.\nAutomating evaluation and testing processes, combined with careful canary testing, reduces deployment risks. While automated evaluation processes catch many issues, human oversight remains essential for reviewing performance across specific data segments and identifying subtle weaknesses. This combination of rigorous pre-deployment validation and real-world testing provides teams with confidence when putting models into production.\n\n\n13.3.5 Model Deployment\nTeams need to properly package, test, and track ML models to reliably deploy them to production. MLOps introduces frameworks and procedures for actively versioning, deploying, monitoring, and updating models in sustainable ways.\nOne common approach to deployment involves containerizing models using tools like Docker, which package code, libraries, and dependencies into standardized units. Containers ensure smooth portability across environments, making deployment consistent and predictable. Frameworks like TensorFlow Serving and BentoML help serve predictions from deployed models via performance-optimized APIs. These frameworks handle versioning, scaling, and monitoring.\nBefore full-scale rollout, teams deploy updated models to staging or QA environments to rigorously test performance. Techniques such as shadow or canary deployments are used to validate new models incrementally. For instance, canary deployments route a small percentage of traffic to the new model while closely monitoring performance. If no issues arise, traffic to the new model gradually increases. Robust rollback procedures are essential to handle unexpected issues, reverting systems to the previous stable model version to ensure minimal disruption. Integration with CI/CD pipelines further automates the deployment and rollback process, enabling efficient iteration cycles.\nTo maintain lineage and auditability, teams track model artifacts, including scripts, weights, logs, and metrics, using tools like MLflow. Model registries, such as Vertex AI’s model registry, act as centralized repositories for storing and managing trained models. These registries not only facilitate version comparisons but also often include access to base models, which may be open source, proprietary, or a hybrid (e.g., LLAMA). Deploying a model from the registry to an inference endpoint is streamlined, handling resource provisioning, model weight downloads, and hosting.\nInference endpoints typically expose the deployed model via REST APIs for real-time predictions. Depending on performance requirements, teams can configure resources, such as GPU accelerators, to meet latency and throughput targets. Some providers also offer flexible options like serverless or batch inference, eliminating the need for persistent endpoints and enabling cost-efficient, scalable deployments. For example, AWS SageMaker Inference supports such configurations.\nBy leveraging these tools and practices, teams can deploy ML models resiliently, ensuring smooth transitions between versions, maintaining production stability, and optimizing performance across diverse use cases.\n\n\n13.3.6 Model Serving\nAfter model deployment, ML-as-a-Service becomes a critical component in the MLOps lifecycle. Online services such as Facebook/Meta handle tens of trillions of inference queries per day (Wu et al. 2019). Model serving bridges the gap between developed models and ML applications or end-users, ensuring that deployed models are accessible, performant, and scalable in production environments.\n\nWu, Carole-Jean, David Brooks, Kevin Chen, Douglas Chen, Sy Choudhury, Marat Dukhan, Kim Hazelwood, et al. 2019. “Machine Learning at Facebook: Understanding Inference at the Edge.” In 2019 IEEE International Symposium on High Performance Computer Architecture (HPCA), 331–44. IEEE; IEEE. https://doi.org/10.1109/hpca.2019.00048.\nSeveral frameworks facilitate model serving, including TensorFlow Serving, NVIDIA Triton Inference Server, and KServe (formerly KFServing). These tools provide standardized interfaces for serving deployed models across various platforms and handle many complexities of model inference at scale.\nModel serving can be categorized into three main types:\n\nOnline Serving: Provides real-time predictions with low latency, which is crucial for applications like recommendation systems or fraud detection.\nOffline Serving: Processes large batches of data asynchronously, suitable for tasks like periodic report generation.\nNear-Online (semi-synchronous) Serving: Balances between online and offline, offering relatively quick responses for less time-sensitive applications such as chatbots.\n\nOne of the key challenges for model serving systems is operating under performance requirements defined by Service Level Agreements (SLAs) and Service Level Objectives (SLOs). SLAs are formal contracts specifying expected service levels. These service levels rely on metrics such as response time, availability, and throughput. SLOs are internal goals teams set to meet or exceed their SLAs.\nFor ML model serving, the SLA and SLO agreements and objectives directly impact user experience, system reliability, and business outcomes. Therefore, teams carefully tune their serving platform. ML serving systems employ various techniques to optimize performance and resource utilization, such as the following:\n\nRequest scheduling and batching: Efficiently manages incoming ML inference requests, optimizing performance through smart queuing and grouping strategies. Systems like Clipper (Crankshaw et al. 2017) introduce low-latency online prediction serving with caching and batching techniques.\nModel instance selection and routing: Intelligent algorithms direct requests to appropriate model versions or instances. INFaaS (Romero et al. 2021) explores this by generating model-variants and efficiently navigating the trade-off space based on performance and accuracy requirements.\nLoad balancing: Distributes workloads evenly across multiple serving instances. MArk (Model Ark) (C. Zhang et al. 2019) demonstrates effective load balancing techniques for ML serving systems.\nModel instance autoscaling: Dynamically adjusts capacity based on demand. Both INFaaS (Romero et al. 2021) and MArk (C. Zhang et al. 2019) incorporate autoscaling capabilities to handle workload fluctuations efficiently.\nModel orchestration: Manages model execution, enabling parallel processing and strategic resource allocation. AlpaServe (Z. Li et al. 2023) demonstrates advanced techniques for handling large models and complex serving scenarios.\nExecution time prediction: Systems like Clockwork (Gujarati et al. 2020) focus on high-performance serving by predicting execution times of individual inferences and efficiently using hardware accelerators.\n\n\nCrankshaw, Daniel, Xin Wang, Guilio Zhou, Michael J Franklin, Joseph E Gonzalez, and Ion Stoica. 2017. “Clipper: A \\(\\{\\)Low-Latency\\(\\}\\) Online Prediction Serving System.” In 14th USENIX Symposium on Networked Systems Design and Implementation (NSDI 17), 613–27.\n\nRomero, Francisco, Qian Li 0027, Neeraja J. Yadwadkar, and Christos Kozyrakis. 2021. “INFaaS: Automated Model-Less Inference Serving.” In 2021 USENIX Annual Technical Conference (USENIX ATC 21), 397–411. https://www.usenix.org/conference/atc21/presentation/romero.\n\nZhang, Chengliang, Minchen Yu, Wei Wang 0030, and Feng Yan 0001. 2019. “MArk: Exploiting Cloud Services for Cost-Effective, SLO-Aware Machine Learning Inference Serving.” In 2019 USENIX Annual Technical Conference (USENIX ATC 19), 1049–62. https://www.usenix.org/conference/atc19/presentation/zhang-chengliang.\n\nLi, Zhuohan, Lianmin Zheng, Yinmin Zhong, Vincent Liu, Ying Sheng, Xin Jin, Yanping Huang, et al. 2023. “\\(\\{\\)AlpaServe\\(\\}\\): Statistical Multiplexing with Model Parallelism for Deep Learning Serving.” In 17th USENIX Symposium on Operating Systems Design and Implementation (OSDI 23), 663–79.\n\nGujarati, Arpan, Reza Karimi, Safya Alzayat, Wei Hao, Antoine Kaufmann, Ymir Vigfusson, and Jonathan Mace. 2020. “Serving DNNs Like Clockwork: Performance Predictability from the Bottom Up.” In 14th USENIX Symposium on Operating Systems Design and Implementation (OSDI 20), 443–62. https://www.usenix.org/conference/osdi20/presentation/gujarati.\nML serving systems that excel in these areas enable organizations to deploy models that perform reliably under pressure. The result is scalable, responsive AI applications that can handle real-world demands and deliver value consistently.\n\n\n13.3.7 Infrastructure Management\nMLOps teams heavily leverage infrastructure as code (IaC) tools and robust cloud architectures to actively manage the resources needed for development, training, and deployment of ML systems.\nTeams use IaC tools like Terraform, CloudFormation and Ansible to programmatically define, provision and update infrastructure in a version controlled manner. For MLOps, teams widely use Terraform to spin up resources on AWS, GCP and Azure.\nFor model building and training, teams dynamically provision computing resources like GPU servers, container clusters, storage, and databases through Terraform as needed by data scientists. Code encapsulates and preserves infrastructure definitions.\nContainers and orchestrators like Docker and Kubernetes allow teams to package models and reliably deploy them across different environments. Containers can be predictably spun up or down automatically based on demand.\nBy leveraging cloud elasticity, teams scale resources up and down to meet spikes in workloads like hyperparameter tuning jobs or spikes in prediction requests. Auto-scaling enables optimized cost efficiency.\nInfrastructure spans on-premises (on-prem), cloud, and edge devices. A robust technology stack provides flexibility and resilience. Monitoring tools allow teams to observe resource utilization.\nFor example, a Terraform config may deploy a GCP Kubernetes cluster to host trained TensorFlow models exposed as prediction microservices. The cluster scales up pods to handle increased traffic. CI/CD integration seamlessly rolls out new model containers.\nCarefully managing infrastructure through IaC and monitoring enables teams to prevent bottlenecks in operationalizing ML systems at scale.\n\n\n13.3.8 Monitoring\nMLOps teams actively maintain robust monitoring to sustain visibility into ML models deployed in production. Continuous monitoring provides insights into model and system performance so teams can rapidly detect and address issues to minimize disruption.\nTeams actively monitor key model aspects, including analyzing samples of live predictions to track metrics like accuracy and confusion matrix over time.\nWhen monitoring performance, teams must profile incoming data to check for model drift - a steady decline in model accuracy after production deployment. Model drift can occur in two ways: concept drift and data drift. Concept drift refers to a fundamental change observed in the relationship between the input data and the target outcomes. For instance, as the COVID-19 pandemic progressed, e-commerce and retail sites had to correct their model recommendations since purchase data was overwhelmingly skewed towards items like hand sanitizer. Data drift describes changes in the distribution of data over time. For example, image recognition algorithms used in self-driving cars must account for seasonality in observing their surroundings. Teams also track application performance metrics like latency and errors for model integrations.\nFrom an infrastructure perspective, teams monitor for capacity issues like high CPU, memory, and disk utilization and system outages. Tools like Prometheus, Grafana, and Elastic enable teams to actively collect, analyze, query, and visualize diverse monitoring metrics. Dashboards make dynamics highly visible.\nTeams configure alerting for key monitoring metrics like accuracy declines and system faults to enable proactively responding to events that threaten reliability. For example, drops in model accuracy trigger alerts for teams to investigate potential data drift and retrain models using updated, representative data samples.\nAfter deployment, comprehensive monitoring enables teams to maintain confidence in model and system health. It empowers teams to catch and resolve deviations preemptively through data-driven alerts and dashboards. Active monitoring is essential for maintaining highly available, trustworthy ML systems.\nWatch the video below to learn more about monitoring.\n\n\n\n\n\n\nImportant 13.3: Model Monitoring\n\n\n\n\n\n\n\n\n13.3.9 Governance\nMLOps teams actively establish proper governance practices as a critical component. Governance provides oversight into ML models to ensure they are trustworthy, ethical, and compliant. Without governance, significant risks exist of models behaving in dangerous or prohibited ways when deployed in applications and business processes.\nMLOps governance employs techniques to provide transparency into model predictions, performance, and behavior throughout the ML lifecycle. Explainability methods like SHAP and LIME help auditors understand why models make certain predictions by highlighting influential input features behind decisions. Bias detection analyzes model performance across different demographic groups defined by attributes like age, gender, and ethnicity to detect any systematic skews. Teams perform rigorous testing procedures on representative datasets to validate model performance before deployment.\nOnce in production, teams monitor concept drift to determine whether predictive relationships change over time in ways that degrade model accuracy. Teams also analyze production logs to uncover patterns in the types of errors models generate. Documentation about data provenance, development procedures, and evaluation metrics provides additional visibility.\nPlatforms like Watson OpenScale incorporate governance capabilities like bias monitoring and explainability directly into model building, testing, and production monitoring. The key focus areas of governance are transparency, fairness, and compliance. This minimizes the risks of models behaving incorrectly or dangerously when integrated into business processes. Embedding governance practices into MLOps workflows enables teams to ensure trustworthy AI.\n\n\n13.3.10 Communication & Collaboration\nMLOps actively breaks down silos and enables the free flow of information and insights between teams through all ML lifecycle stages. Tools like MLflow, Weights & Biases, and data contexts provide traceability and visibility to improve collaboration.\nTeams use MLflow to systematize tracking of model experiments, versions, and artifacts. Experiments can be programmatically logged from data science notebooks and training jobs. The model registry provides a central hub for teams to store production-ready models before deployment, with metadata like descriptions, metrics, tags, and lineage. Integrations with Github, GitLab facilitate code change triggers.\nWeights & Biases provides collaborative tools tailored to ML teams. Data scientists log experiments, visualize metrics like loss curves, and share experimentation insights with colleagues. Comparison dashboards highlight model differences. Teams discuss progress and next steps.\nEstablishing shared data contexts—glossaries, data dictionaries, and schema references—ensures alignment on data meaning and usage across roles. Documentation aids understanding for those without direct data access.\nFor example, a data scientist may use Weights & Biases to analyze an anomaly detection model experiment and share the evaluation results with other team members to discuss improvements. The final model can then be registered with MLflow before handing off for deployment.\nEnabling transparency, traceability, and communication via MLOps empowers teams to remove bottlenecks and accelerate the delivery of impactful ML systems.\nVideo 13.4 covers key challenges in model deployment, including concept drift, model drift, and software engineering issues.\n\n\n\n\n\n\nImportant 13.4: Deployment Challenges",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>ML Operations</span>"
    ]
  },
  {
    "objectID": "contents/core/ops/ops.html#hidden-technical-debt-in-ml-systems",
    "href": "contents/core/ops/ops.html#hidden-technical-debt-in-ml-systems",
    "title": "13  ML Operations",
    "section": "13.4 Hidden Technical Debt in ML Systems",
    "text": "13.4 Hidden Technical Debt in ML Systems\nTechnical debt is increasingly pressing for ML systems. This metaphor, originally proposed in the 1990s, likens the long-term costs of quick software development to financial debt. Just as some financial debt powers beneficial growth, carefully managed technical debt enables rapid iteration. However, left unchecked, accumulating technical debt can outweigh any gains.\nFigure 13.3 illustrates the various components contributing to ML systems’ hidden technical debt. It shows the interconnected nature of configuration, data collection, and feature extraction, which is foundational to the ML codebase. The box sizes indicate the proportion of the entire system represented by each component. In industry ML systems, the code for the model algorithm makes up only a tiny fraction (see the small black box in the middle compared to all the other large boxes). The complexity of ML systems and the fast-paced nature of the industry make it very easy to accumulate technical debt.\n\n\n\n\n\n\nFigure 13.3: ML system components. Source: Sambasivan et al. (2021)\n\n\n\n\n13.4.1 Model Boundary Erosion\nUnlike traditional software, ML lacks clear boundaries between components, as seen in the diagram above. This erosion of abstraction creates entanglements that exacerbate technical debt in several ways:\n\n\n13.4.2 Entanglement\nTight coupling between ML model components makes isolating changes difficult. Modifying one part causes unpredictable ripple effects throughout the system. Changing anything changes everything (also known as CACE) is a phenomenon that applies to any tweak you make to your system. Potential mitigations include decomposing the problem when possible or closely monitoring for changes in behavior to contain their impact.\n\n\n13.4.3 Correction Cascades\nFigure 13.4 illustrates the concept of correction cascades in the ML workflow, from problem statement to model deployment. The arcs represent the potential iterative corrections needed at each workflow stage, with different colors corresponding to distinct issues such as interacting with physical world brittleness, inadequate application-domain expertise, conflicting reward systems, and poor cross-organizational documentation.\nThe red arrows indicate the impact of cascades, which can lead to significant revisions in the model development process. In contrast, the dotted red line represents the drastic measure of abandoning the process to restart. This visual emphasizes the complex, interconnected nature of ML system development and the importance of addressing these issues early in the development cycle to mitigate their amplifying effects downstream.\n\n\n\n\n\n\nFigure 13.4: Correction cascades flowchart. Source: Sambasivan et al. (2021).\n\n\nSambasivan, Nithya, Shivani Kapania, Hannah Highfill, Diana Akrong, Praveen Paritosh, and Lora M Aroyo. 2021. “‘Everyone Wants to Do the Model Work, Not the Data Work’: Data Cascades in High-Stakes AI.” In Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems. ACM. https://doi.org/10.1145/3411764.3445518.\n\n\nBuilding models sequentially creates risky dependencies where later models rely on earlier ones. For example, taking an existing model and fine-tuning it for a new use case seems efficient. However, this bakes in assumptions from the original model that may eventually need correction.\nSeveral factors inform the decision to build models sequentially or not:\n\nDataset size and rate of growth: With small, static datasets, fine-tuning existing models often makes sense. For large, growing datasets, training custom models from scratch allows more flexibility to account for new data.\nAvailable computing resources: Fine-tuning requires fewer resources than training large models from scratch. With limited resources, leveraging existing models may be the only feasible approach.\n\nWhile fine-tuning existing models can be efficient, modifying foundational components later becomes extremely costly due to these cascading effects. Therefore, careful consideration should be given to introducing fresh model architectures, even if resource-intensive, to avoid correction cascades down the line. This approach may help mitigate the amplifying effects of issues downstream and reduce technical debt. However, there are still scenarios where sequential model building makes sense, necessitating a thoughtful balance between efficiency, flexibility, and long-term maintainability in the ML development process.\n\n\n13.4.4 Undeclared Consumers\nOnce ML model predictions are made available, many downstream systems may silently consume them as inputs for further processing. However, the original model was not designed to accommodate this broad reuse. Due to the inherent opacity of ML systems, it becomes impossible to fully analyze the impact of the model’s outputs as inputs elsewhere. Changes to the model can then have expensive and dangerous consequences by breaking undiscovered dependencies.\nUndeclared consumers can also enable hidden feedback loops if their outputs indirectly influence the original model’s training data. Mitigations include restricting access to predictions, defining strict service contracts, and monitoring for signs of un-modelled influences. Architecting ML systems to encapsulate and isolate their effects limits the risks of unanticipated propagation.\n\n\n13.4.5 Data Dependency Debt\nData dependency debt refers to unstable and underutilized data dependencies, which can have detrimental and hard-to-detect repercussions. While this is a key contributor to tech debt for traditional software, those systems can benefit from the use of widely available tools for static analysis by compilers and linkers to identify dependencies of these types. ML systems need similar tooling.\nOne mitigation for unstable data dependencies is to use versioning, which ensures the stability of inputs but comes with the cost of managing multiple sets of data and the potential for staleness. Another mitigation for underutilized data dependencies is to conduct exhaustive leave-one-feature-out evaluation.\n\n\n13.4.6 Analysis Debt from Feedback Loops\nUnlike traditional software, ML systems can change their behavior over time, making it difficult to analyze pre-deployment. This debt manifests in feedback loops, both direct and hidden.\nDirect feedback loops occur when a model influences its future inputs, such as by recommending products to users that, in turn, shape future training data. Hidden loops arise indirectly between models, such as two systems that interact via real-world environments. Gradual feedback loops are especially hard to detect. These loops lead to analysis debt—the inability to predict how a model will act fully after release. They undermine pre-deployment validation by enabling unmodeled self-influence.\nCareful monitoring and canary deployments help detect feedback. However, fundamental challenges remain in understanding complex model interactions. Architectural choices that reduce entanglement and coupling mitigate analysis debt’s compounding effect.\n\n\n13.4.7 Pipeline Jungles\nML workflows often need more standardized interfaces between components. This leads teams to incrementally “glue” together pipelines with custom code. What emerges are “pipeline jungles”—tangled preprocessing steps that are brittle and resist change. Avoiding modifications to these messy pipelines causes teams to experiment through alternate prototypes. Soon, multiple ways of doing everything proliferate. The need for abstractions and interfaces then impedes sharing, reuse, and efficiency.\nTechnical debt accumulates as one-off pipelines solidify into legacy constraints. Teams sink time into managing idiosyncratic code rather than maximizing model performance. Architectural principles like modularity and encapsulation are needed to establish clean interfaces. Shared abstractions enable interchangeable components, prevent lock-in, and promote best-practice diffusion across teams. Breaking free of pipeline jungles ultimately requires enforcing standards that prevent the accretion of abstraction debt. The benefits of interfaces and APIs that tame complexity outweigh the transitional costs.\n\n\n13.4.8 Configuration Debt\nML systems involve extensive configuration of hyperparameters, architectures, and other tuning parameters. However, the configuration is often an afterthought, needing more rigor and testing—ad hoc configurations increase, amplified by the many knobs available for tuning complex ML models.\nThis accumulation of technical debt has several consequences. Fragile and outdated configurations lead to hidden dependencies and bugs that cause production failures. Knowledge about optimal configurations is isolated rather than shared, leading to redundant work. Reproducing and comparing results becomes difficult when configurations lack documentation. Legacy constraints accumulate as teams fear changing poorly understood configurations.\nAddressing configuration debt requires establishing standards to document, test, validate, and centrally store configurations. Investing in more automated approaches, such as hyperparameter optimization and architecture search, reduces dependence on manual tuning. Better configuration hygiene makes iterative improvement more tractable by preventing complexity from compounding endlessly. The key is recognizing configuration as an integral part of the ML system lifecycle rather than an ad hoc afterthought.\n\n\n13.4.9 The Changing World\nML systems operate in dynamic real-world environments. Thresholds and decisions that are initially effective become outdated as the world evolves. However, legacy constraints make adapting systems to changing populations, usage patterns, and other shifting contextual factors difficult.\nThis debt manifests in two main ways. First, preset thresholds and heuristics require constant re-evaluation and tuning as their optimal values drift. Second, validating systems through static unit and integration tests fails when inputs and behaviors are moving targets.\nResponding to a changing world in real-time with legacy ML systems is challenging. Technical debt accumulates as assumptions decay. The lack of modular architecture and the ability to dynamically update components without side effects exacerbates these issues.\nMitigating this requires building in configurability, monitoring, and modular updatability. Online learning, where models continuously adapt and robust feedback loops to training pipelines, helps automatically tune to the world. However, anticipating and architecting for change is essential to prevent erosion of real-world performance over time.\n\n\n13.4.10 Navigating Technical Debt in Early Stages\nUnderstandably, technical debt accumulates naturally in the early stages of model development. When aiming to build MVP models quickly, teams often need more complete information on what components will reach scale or require modification. Some deferred work is expected.\nHowever, even scrappy initial systems should follow principles like “Flexible Foundations” to avoid painting themselves into corners:\n\nModular code and reusable libraries allow components to be swapped later\nLoose coupling between models, data stores, and business logic facilitates change\nAbstraction layers hide implementation details that may shift over time\nContainerized model serving keeps options open on deployment requirements\n\nDecisions that seem reasonable at the moment can seriously limit future flexibility. For example, baking key business logic into model code rather than keeping it separate makes subsequent model changes extremely difficult.\nWith thoughtful design, though, it is possible to build quickly at first while retaining degrees of freedom to improve. As the system matures, prudent break points emerge where introducing fresh architectures proactively avoids massive rework down the line. This balances urgent timelines with reducing future correction cascades.\n\n\n13.4.11 Summary\nAlthough financial debt is a good metaphor for understanding tradeoffs, it differs from technical debt’s measurability. Technical debt needs to be fully tracked and quantified. This makes it hard for teams to navigate the tradeoffs between moving quickly and inherently introducing more debt versus taking the time to pay down that debt.\nThe Hidden Technical Debt of Machine Learning Systems paper spreads awareness of the nuances of ML system-specific tech debt. It encourages additional development in the broad area of maintainable ML.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>ML Operations</span>"
    ]
  },
  {
    "objectID": "contents/core/ops/ops.html#sec-roles-and_resp-ops",
    "href": "contents/core/ops/ops.html#sec-roles-and_resp-ops",
    "title": "13  ML Operations",
    "section": "13.5 Roles and Responsibilities",
    "text": "13.5 Roles and Responsibilities\nGiven the vastness of MLOps, successfully implementing ML systems requires diverse skills and close collaboration between people with different areas of expertise. While data scientists build the core ML models, it takes cross-functional teamwork to successfully deploy these models into production environments and enable them to deliver sustainable business value.\nMLOps provides the framework and practices for coordinating the efforts of various roles involved in developing, deploying, and running MLG systems. Bridging traditional silos between data, engineering, and operations teams is key to MLOp’s success. Enabling seamless collaboration through the machine learning lifecycle accelerates benefit realization while ensuring ML models’ long-term reliability and performance.\nWe will look at some key roles involved in MLOps and their primary responsibilities. Understanding the breadth of skills needed to operationalize ML models guides assembling MLOps teams. It also clarifies how the workflows between roles fit under the overarching MLOps methodology.\n\n13.5.1 Data Engineers\nData engineers are responsible for building and maintaining the data infrastructure and pipelines that feed data to ML models. They ensure data is smoothly moved from source systems into the storage, processing, and feature engineering environments needed for ML model development and deployment. Their main responsibilities include:\n\nMigrating raw data from on-prem databases, sensors, and apps into cloud-based data lakes like Amazon S3 or Google Cloud Storage. This provides cost-efficient, scalable storage.\nBuilding data pipelines with workflow schedulers like Apache Airflow, Prefect, and dbt. These extract data from sources, transform and validate data, and load it into destinations like data warehouses, feature stores, or directly for model training.\nTransforming messy, raw data into structured, analysis-ready datasets. This includes handling null or malformed values, deduplicating, joining disparate data sources, aggregating data, and engineering new features.\nMaintaining data infrastructure components like cloud data warehouses (Snowflake, Redshift, BigQuery), data lakes, and metadata management systems. Provisioning and optimizing data processing systems.\nProvisioning and optimizing data processing systems for efficient, scalable data handling and analysis.\nEstablishing data versioning, backup, and archival processes for ML datasets and features and enforcing data governance policies.\n\nFor example, a manufacturing firm may use Apache Airflow pipelines to extract sensor data from PLCs on the factory floor into an Amazon S3 data lake. The data engineers would then process this raw data to filter, clean, and join it with product metadata. These pipeline outputs would then load into a Snowflake data warehouse from which features can be read for model training and prediction.\nThe data engineering team builds and sustains the data foundation for reliable model development and operations. Their work enables data scientists and ML engineers to focus on building, training, and deploying ML models at scale.\n\n\n13.5.2 Data Scientists\nThe job of the data scientists is to focus on the research, experimentation, development, and continuous improvement of ML models. They leverage their expertise in statistics, modeling, and algorithms to create high-performing models. Their main responsibilities include:\n\nWorking with business and data teams to identify opportunities where ML can add value, framing the problem, and defining success metrics.\nPerforming exploratory data analysis to understand relationships in data, derive insights, and identify relevant features for modeling.\nResearching and experimenting with different ML algorithms and model architectures based on the problem and data characteristics and leveraging libraries like TensorFlow, PyTorch, and Keras.\nTo maximize performance, train and fine-tune models by tuning hyperparameters, adjusting neural network architectures, feature engineering, etc.\nEvaluating model performance through metrics like accuracy, AUC, and F1 scores and performing error analysis to identify areas for improvement.\nDeveloping new model versions by incorporating new data, testing different approaches, optimizing model behavior, and maintaining documentation and lineage for models.\n\nFor example, a data scientist may leverage TensorFlow and TensorFlow Probability to develop a demand forecasting model for retail inventory planning. They would iterate on different sequence models like LSTMs and experiment with features derived from product, sales, and seasonal data. The model would be evaluated based on error metrics versus actual demand before deployment. The data scientist monitors performance and retrains/enhances the model as new data comes in.\nData scientists drive model creation, improvement, and innovation through their expertise in ML techniques. They collaborate closely with other roles to ensure models create maximum business impact.\n\n\n13.5.3 ML Engineers\nML engineers enable models data scientists develop to be productized and deployed at scale. Their expertise makes models reliably serve predictions in applications and business processes. Their main responsibilities include:\n\nTaking prototype models from data scientists and hardening them for production environments through coding best practices.\nBuilding APIs and microservices for model deployment using tools like Flask, FastAPI. Containerizing models with Docker.\nManage model versions, sync new models into production using CI/CD pipelines, and implement canary releases, A/B tests, and rollback procedures.\nOptimizing model performance for high scalability, low latency, and cost efficiency. Leveraging compression, quantization, and multi-model serving.\nMonitor models once in production and ensure continued reliability and accuracy. Retraining models periodically.\n\nFor example, an ML engineer may take a TensorFlow fraud detection model developed by data scientists and containerize it using TensorFlow Serving for scalable deployment. The model would be integrated into the company’s transaction processing pipeline via APIs. The ML engineer implements a model registry and CI/CD pipeline using MLFlow and Jenkins to deploy model updates reliably. The ML engineers then monitor the running model for continued performance using tools like Prometheus and Grafana. If model accuracy drops, they initiate retraining and deployment of a new model version.\nThe ML engineering team enables data science models to progress smoothly into sustainable and robust production systems. Their expertise in building modular, monitored systems delivers continuous business value.\n\n\n13.5.4 DevOps Engineers\nDevOps engineers enable MLOps by building and managing the underlying infrastructure for developing, deploying, and monitoring ML models. As a specialized branch of software engineering, DevOps focuses on creating automation pipelines, cloud architecture, and operational frameworks. Their main responsibilities include:\n\nProvisioning and managing cloud infrastructure for ML workflows using IaC tools like Terraform, Docker, and Kubernetes.\nDeveloping CI/CD pipelines for model retraining, validation, and deployment. Integrating ML tools into the pipeline, such as MLflow and Kubeflow.\nMonitoring model and infrastructure performance using tools like Prometheus, Grafana, ELK stack. Building alerts and dashboards.\nImplement governance practices around model development, testing, and promotion to enable reproducibility and traceability.\nEmbedding ML models within applications. They are exposing models via APIs and microservices for integration.\nOptimizing infrastructure performance and costs and leveraging autoscaling, spot instances, and availability across regions.\n\nFor example, a DevOps engineer provisions a Kubernetes cluster on AWS using Terraform to run ML training jobs and online deployment. The engineer builds a CI/CD pipeline in Jenkins, which triggers model retraining when new data becomes available. After automated testing, the model is registered with MLflow and deployed in the Kubernetes cluster. The engineer then monitors cluster health, container resource usage, and API latency using Prometheus and Grafana.\nThe DevOps team enables rapid experimentation and reliable deployments for ML through cloud, automation, and monitoring expertise. Their work maximizes model impact while minimizing technical debt.\n\n\n13.5.5 Project Managers\nProject managers play a vital role in MLOps by coordinating the activities between the teams involved in delivering ML projects. They help drive alignment, accountability, and accelerated results. Their main responsibilities include:\n\nWorking with stakeholders to define project goals, success metrics, timelines, and budgets; outlining specifications and scope.\nCreating a project plan spanning data acquisition, model development, infrastructure setup, deployment, and monitoring.\nCoordinating design, development, and testing efforts between data engineers, data scientists, ML engineers, and DevOps roles.\nTracking progress and milestones, identifying roadblocks and resolving them through corrective actions, and managing risks and issues.\nFacilitating communication through status reports, meetings, workshops, and documentation and enabling seamless collaboration.\nDriving adherence to timelines and budget and escalating anticipated overruns or shortfalls for mitigation.\n\nFor example, a project manager would create a project plan for developing and enhancing a customer churn prediction model. They coordinate between data engineers building data pipelines, data scientists experimenting with models, ML engineers productizing models, and DevOps setting up deployment infrastructure. The project manager tracks progress via milestones like dataset preparation, model prototyping, deployment, and monitoring. To enact preventive solutions, they surface any risks, delays, or budget issues.\nSkilled project managers enable MLOps teams to work synergistically to rapidly deliver maximum business value from ML investments. Their leadership and organization align with diverse teams.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>ML Operations</span>"
    ]
  },
  {
    "objectID": "contents/core/ops/ops.html#traditional-mlops-vs.-embedded-mlops",
    "href": "contents/core/ops/ops.html#traditional-mlops-vs.-embedded-mlops",
    "title": "13  ML Operations",
    "section": "13.6 Traditional MLOps vs. Embedded MLOps",
    "text": "13.6 Traditional MLOps vs. Embedded MLOps\nBuilding on our discussion of On-device Learning in the previous chapter, we now turn our attention to the broader context of embedded systems in MLOps. The unique constraints and requirements of embedded environments significantly impact the implementation of machine learning models and operations. As we have discussed in previous chapters, embedded systems introduce unique challenges to MLOps due to their constrained resources, intermittent connectivity, and the need for efficient, power-aware computation. Unlike cloud environments with abundant compute and storage, embedded devices often operate with limited memory, power, and processing capabilities, requiring careful optimization of workflows. These limitations influence all aspects of MLOps, from deployment and data collection to monitoring and updates.\nIn traditional MLOps, ML models are typically deployed in cloud-based or server environments, with abundant resources like computing power and memory. These environments facilitate the smooth operation of complex models that require significant computational resources. For instance, a cloud-based image recognition model might be used by a social media platform to tag photos with relevant labels automatically. In this case, the model can leverage the extensive resources available in the cloud to efficiently process vast amounts of data.\nOn the other hand, embedded MLOps involves deploying ML models on embedded systems, specialized computing systems designed to perform specific functions within larger systems. Embedded systems are typically characterized by their limited computational resources and power. For example, an ML model might be embedded in a smart thermostat to optimize heating and cooling based on the user’s preferences and habits. The model must be optimized to run efficiently on the thermostat’s limited hardware without compromising its performance or accuracy.\nThe key difference between traditional and embedded MLOps lies in the embedded system’s resource constraints. While traditional MLOps can leverage abundant cloud or server resources, embedded MLOps must contend with the hardware limitations on which the model is deployed. This requires careful optimization and fine-tuning of the model to ensure it can deliver accurate and valuable insights within the embedded system’s constraints.\nFurthermore, embedded MLOps must consider the unique challenges posed by integrating ML models with other embedded system components. For example, the model must be compatible with the system’s software and hardware and must be able to interface seamlessly with other components, such as sensors or actuators. This requires a deep understanding of both ML and embedded systems and close collaboration between data scientists, engineers, and other stakeholders.\nSo, while traditional MLOps and embedded MLOps share the common goal of deploying and maintaining ML models in production environments, the unique challenges posed by embedded systems require a specialized approach. Embedded MLOps must carefully balance the need for model accuracy and performance with the constraints of the hardware on which the model is deployed. This requires a deep understanding of both ML and embedded systems and close collaboration between various stakeholders to ensure the successful integration of ML models into embedded systems.\nThis time, we will group the subtopics under broader categories to streamline the structure of our thought process on MLOps. This structure will help you understand how different aspects of MLOps are interconnected and why each is important for the efficient operation of ML systems as we discuss the challenges in the context of embedded systems.\n\nModel Lifecycle Management\n\nData Management: Handling data ingestion, validation, and version control.\nModel Training: Techniques and practices for effective and scalable model training.\nModel Evaluation: Strategies for testing and validating model performance.\nModel Deployment: Approaches for deploying models into production environments.\n\nDevelopment and Operations Integration\n\nCI/CD Pipelines: Integrating ML models into continuous integration and deployment pipelines.\nInfrastructure Management: Setting up and maintaining the infrastructure required for training and deploying models.\nCommunication & Collaboration: Ensuring smooth communication and collaboration between data scientists, ML engineers, and operations teams.\n\nOperational Excellence\n\nMonitoring: Techniques for monitoring model performance, data drift, and operational health.\nGovernance: Implementing policies for model auditability, compliance, and ethical considerations.\n\n\n\n13.6.1 Model Lifecycle Management\n\nData Management\nIn traditional centralized MLOps, data is aggregated into large datasets and data lakes, then processed on cloud or on-prem servers. However, embedded MLOps relies on decentralized data from local on-device sensors. Devices collect smaller batches of incremental data, often noisy and unstructured. With connectivity constraints, this data cannot always be instantly transmitted to the cloud and needs to be intelligently cached and processed at the edge.\nDue to limited on-device computing, embedded devices can only preprocess and clean data minimally before transmission. Early filtering and processing occur at edge gateways to reduce transmission loads. While leveraging cloud storage, more processing and storage happen at the edge to account for intermittent connectivity. Devices identify and transmit only the most critical subsets of data to the cloud.\nLabeling also needs centralized data access, requiring more automated techniques like federated learning, where devices collaboratively label peers’ data. With personal edge devices, data privacy and regulations are critical concerns. Data collection, transmission, and storage must be secure and compliant.\nFor instance, a smartwatch may collect the day’s step count, heart rate, and GPS coordinates. This data is cached locally and transmitted to an edge gateway when WiFi is available—the gateway processes and filters data before syncing relevant subsets with the cloud platform to retrain models.\n\n\nModel Training\nIn traditional centralized MLOps, models are trained using abundant data via deep learning on high-powered cloud GPU servers. However, embedded MLOps need more support in model complexity, data availability, and computing resources for training.\nThe volume of aggregated data is much lower, often requiring techniques like federated learning across devices to create training sets. The specialized nature of edge data also limits public datasets for pre-training. With privacy concerns, data samples must be tightly controlled and anonymized where possible.\nFurthermore, the models must use simplified architectures optimized for low-power edge hardware. Given the computing limitations, high-end GPUs are inaccessible for intensive deep learning. Training leverages lower-powered edge servers and clusters with distributed approaches to spread load.\nTransfer learning emerges as a crucial strategy to address data scarcity and irregularity in machine learning, particularly in edge computing scenarios. As illustrated in Figure 13.5, this approach involves pre-training models on large public datasets and then fine-tuning them on limited domain-specific edge data. The figure depicts a neural network where initial layers (\\(W_{A1}\\) to \\(W_{A4}\\)), responsible for general feature extraction, are frozen (indicated by a green dashed line). These layers retain knowledge from previous tasks, accelerating learning and reducing resource requirements. The latter layers (\\(W_{A5}\\) to \\(W_{A7}\\)), beyond the blue dashed line, are fine-tuned for the specific task, focusing on task-specific feature learning.\n\n\n\n\n\n\nFigure 13.5: Transfer learning in MLOps. Source: HarvardX.\n\n\n\nThis method not only mitigates data scarcity but also accommodates the decentralized nature of embedded data. Furthermore, techniques like incremental on-device learning can further customize models to specific use cases. The lack of broad labeled data in many domains also motivates the use of semi-supervised techniques, complementing the transfer learning approach. By leveraging pre-existing knowledge and adapting it to specialized tasks, transfer learning within an MLOps framework enables models to achieve higher performance with fewer resources, even in data-constrained environments.\nFor example, a smart home assistant may pre-train an audio recognition model on public YouTube clips, which helps bootstrap with general knowledge. It then transfers learning to a small sample of home data to classify customized appliances and events, specializing in the model. The model transforms into a lightweight neural network optimized for microphone-enabled devices across the home.\nSo, embedded MLOps face acute challenges in constructing training datasets, designing efficient models, and distributing compute for model development compared to traditional settings. Given the embedded constraints, careful adaptation, such as transfer learning and distributed training, is required to train models.\n\n\nModel Evaluation\nIn traditional centralized MLOps, models are evaluated primarily using accuracy metrics and holdout test datasets. However, embedded MLOps require a more holistic evaluation that accounts for system constraints beyond accuracy.\nModels must be tested early and often on deployed edge hardware covering diverse configurations. In addition to accuracy, factors like latency, CPU usage, memory footprint, and power consumption are critical evaluation criteria. Models are selected based on tradeoffs between these metrics to meet edge device constraints.\nData drift must also be monitored - where models trained on cloud data degrade in accuracy over time on local edge data. Embedded data often has more variability than centralized training sets. Evaluating models across diverse operational edge data samples is key. But sometimes, getting the data for monitoring the drift can be challenging if these devices are in the wild and communication is a barrier.\nOngoing monitoring provides visibility into real-world performance post-deployment, revealing bottlenecks not caught during testing. For instance, a smart camera model update may be canary tested on 100 cameras first and rolled back if degraded accuracy is observed before expanding to all 5000 cameras.\n\n\nModel Deployment\nIn traditional MLOps, new model versions are directly deployed onto servers via API endpoints. However, embedded devices require optimized delivery mechanisms to receive updated models. Over-the-air (OTA) updates provide a standardized approach to wirelessly distributing new software or firmware releases to embedded devices. Rather than direct API access, OTA packages allow remote deploying models and dependencies as pre-built bundles. Alternatively, federated learning allows model updates without direct access to raw training data. This decentralized approach has the potential for continuous model improvement but needs robust MLOps platforms.\nModel delivery relies on physical interfaces like USB or UART serial connections for deeply embedded devices lacking connectivity. The model packaging still follows similar principles to OTA updates, but the deployment mechanism is tailored to the capabilities of the edge hardware. Moreover, specialized OTA protocols optimized for IoT networks are often used rather than standard WiFi or Bluetooth protocols. Key factors include efficiency, reliability, security, and telemetry, such as progress tracking—solutions like Mender. Io provides embedded-focused OTA services handling differential updates across device fleets.\nFigure 13.6 presents an overview of Model Lifecycle Management in an MLOps context, illustrating the flow from development (top left) to deployment and monitoring (bottom right). The process begins with ML Development, where code and configurations are version-controlled. Data and model management are central to the process, involving datasets and feature repositories. Continuous training, model conversion, and model registry are key stages in the operationalization of training. The model deployment includes serving the model and managing serving logs. Alerting mechanisms are in place to flag issues, which feed into continuous monitoring to ensure model performance and reliability over time. This integrated approach ensures that models are developed and maintained effectively throughout their lifecycle.\n\n\n\n\n\n\nFigure 13.6: Model lifecycle management. Source: HarvardX.\n\n\n\n\n\n\n13.6.2 Development and Operations Integration\n\nCI/CD Pipelines\nIn traditional MLOps, robust CI/CD infrastructure like Jenkins and Kubernetes enables pipeline automation for large-scale model deployment. However, embedded MLOps need this centralized infrastructure and more tailored CI/CD workflows for edge devices.\nBuilding CI/CD pipelines has to account for a fragmented landscape of diverse hardware, firmware versions, and connectivity constraints. There is no standard platform to orchestrate pipelines, and tooling support is more limited.\nTesting must cover this wide spectrum of target embedded devices early, which is difficult without centralized access. Companies must invest significant effort into acquiring and managing test infrastructure across the heterogeneous embedded ecosystem.\nOver-the-air updates require setting up specialized servers to distribute model bundles securely to devices in the field. Rollout and rollback procedures must also be carefully tailored for particular device families.\nWith traditional CI/CD tools less applicable, embedded MLOps rely more on custom scripts and integration. Companies take varied approaches, from open-source frameworks to fully in-house solutions. Tight integration between developers, edge engineers, and end customers establishes trusted release processes.\nTherefore, embedded MLOps can’t leverage centralized cloud infrastructure for CI/CD. Companies combine custom pipelines, testing infrastructure, and OTA delivery to deploy models across fragmented and disconnected edge systems.\n\n\nInfrastructure Management\nIn traditional centralized MLOps, infrastructure entails provisioning cloud servers, GPUs, and high-bandwidth networks for intensive workloads like model training and serving predictions at scale. However, embedded MLOps require more heterogeneous infrastructure spanning edge devices, gateways, and the cloud.\nEdge devices like sensors capture and preprocess data locally before intermittent transmission to avoid overloading networks—gateways aggregate and process device data before sending select subsets to the cloud for training and analysis. The cloud provides centralized management and supplemental computing.\nThis infrastructure needs tight integration and balancing processing and communication loads. Network bandwidth is limited, requiring careful data filtering and compression. Edge computing capabilities are modest compared to the cloud, imposing optimization constraints.\nManaging secure OTA updates across large device fleets presents challenges at the edge. Rollouts must be incremental and rollback-ready for quick mitigation. Given decentralized environments, updating edge infrastructure requires coordination.\nFor example, an industrial plant may perform basic signal processing on sensors before sending data to an on-prem gateway. The gateway handles data aggregation, infrastructure monitoring, and OTA updates. Only curated data is transmitted to the cloud for advanced analytics and model retraining.\nEmbedded MLOps requires holistic management of distributed infrastructure spanning constrained edge, gateways, and centralized cloud. Workloads are balanced across tiers while accounting for connectivity, computing, and security challenges.\n\n\nCommunication & Collaboration\nIn traditional MLOps, collaboration tends to center around data scientists, ML engineers, and DevOps teams. However, embedded MLOps require tighter cross-functional coordination between additional roles to address system constraints.\nEdge engineers optimize model architectures for target hardware environments. They provide feedback to data scientists during development so models fit device capabilities early on. Similarly, product teams define operational requirements informed by end-user contexts.\nWith more stakeholders across the embedded ecosystem, communication channels must facilitate information sharing between centralized and remote teams. Issue tracking and project management ensure alignment.\nCollaborative tools optimize models for particular devices. Data scientists can log issues replicated from field devices so models specialize in niche data. Remote device access aids debugging and data collection.\nFor example, data scientists may collaborate with field teams managing fleets of wind turbines to retrieve operational data samples. This data is used to specialize models detecting anomalies specific to that turbine class. Model updates are tested in simulations and reviewed by engineers before field deployment.\nEmbedded MLOps mandates continuous coordination between data scientists, engineers, end customers, and other stakeholders throughout the ML lifecycle. Through close collaboration, models can be tailored and optimized for targeted edge devices.\n\n\n\n13.6.3 Operational Excellence\n\nMonitoring\nTraditional MLOps monitoring focuses on centrally tracking model accuracy, performance metrics, and data drift. However, embedded MLOps must account for decentralized monitoring across diverse edge devices and environments.\nEdge devices require optimized data collection to transmit key monitoring metrics without overloading networks. Metrics help assess model performance, data patterns, resource usage, and other behaviors on remote devices.\nWith limited connectivity, more analysis occurs at the edge before aggregating insights centrally. Gateways play a key role in monitoring fleet health and coordinating software updates. Confirmed indicators are eventually propagated to the cloud.\nBroad device coverage is challenging but critical. Issues specific to certain device types may arise, so monitoring needs to cover the full spectrum. Canary deployments help trial monitoring processes before scaling.\nAnomaly detection identifies incidents requiring rolling back models or retraining on new data. However, interpreting alerts requires understanding unique device contexts based on input from engineers and customers.\nFor example, an automaker may monitor autonomous vehicles for indicators of model degradation using caching, aggregation, and real-time streams. Engineers assess when identified anomalies warrant OTA updates to improve models based on factors like location and vehicle age.\nEmbedded MLOps monitoring provides observability into model and system performance across decentralized edge environments. Careful data collection, analysis, and collaboration deliver meaningful insights to maintain reliability.\n\n\nGovernance\nIn traditional MLOps, governance focuses on model explainability, fairness, and compliance for centralized systems. However, embedded MLOps must also address device-level governance challenges related to data privacy, security, and safety.\nWith sensors collecting personal and sensitive data, local data governance on devices is critical. Data access controls, anonymization, and encrypted caching help address privacy risks and compliance like HIPAA and GDPR. Updates must maintain security patches and settings.\nSafety governance considers the physical impacts of flawed device behavior. Failures could cause unsafe conditions in vehicles, factories, and critical systems. Redundancy, fail-safes, and warning systems help mitigate risks.\nTraditional governance, such as bias monitoring and model explainability, remains imperative but is harder to implement for embedded AI. Peeking into black-box models on low-power devices also poses challenges.\nFor example, a medical device may scrub personal data on the device before transmission. Strict data governance protocols approve model updates. Model explainability is limited, but the focus is on detecting anomalous behavior. Backup systems prevent failures.\nEmbedded MLOps governance must encompass privacy, security, safety, transparency, and ethics. Specialized techniques and team collaboration are needed to help establish trust and accountability within decentralized environments.\n\n\n\n13.6.4 Comparison\nTable 13.2 highlights the similarities and differences between Traditional MLOps and Embedded MLOps based on all the things we have learned thus far:\n\n\n\nTable 13.2: Comparison of Traditional MLOps and Embedded MLOps practices.\n\n\n\n\n\n\n\n\n\n\nArea\nTraditional MLOps\nEmbedded MLOps\n\n\n\n\nData Management\nLarge datasets, data lakes, feature stores\nOn-device data capture, edge caching and processing\n\n\nModel Development\nLeverage deep learning, complex neural nets, GPU training\nConstraints on model complexity, need for optimization\n\n\nDeployment\nServer clusters, cloud deployment, low latency at scale\nOTA deployment to devices, intermittent connectivity\n\n\nMonitoring\nDashboards, logs, alerts for cloud model performance\nOn-device monitoring of predictions, resource usage\n\n\nRetraining\nRetrain models on new data\nFederated learning from devices, edge retraining\n\n\nInfrastructure\nDynamic cloud infrastructure\nHeterogeneous edge/cloud infrastructure\n\n\nCollaboration\nShared experiment tracking and model registry\nCollaboration for device-specific optimization\n\n\n\n\n\n\nSo, while Embedded MLOps shares foundational MLOps principles, it faces unique constraints in tailoring workflows and infrastructure specifically for resource-constrained edge devices.\n\n\n13.6.5 Embedded MLOps Services\nDespite the proliferation of new MLOps tools in response to the increase in demand, the challenges described earlier have constrained the availability of such tools in embedded systems environments. More recently, new tools such as Edge Impulse (Janapa Reddi et al. 2023) have made the development process somewhat easier, as described below.\n\nJanapa Reddi, Vijay, Alexander Elium, Shawn Hymel, David Tischler, Daniel Situnayake, Carl Ward, Louis Moreau, et al. 2023. “Edge Impulse: An MLOps Platform for Tiny Machine Learning.” Proceedings of Machine Learning and Systems 5.\n\nEdge Impulse\nEdge Impulse is an end-to-end development platform for creating and deploying machine learning models onto edge devices such as microcontrollers and small processors. It makes embedded machine learning more accessible to software developers through its easy-to-use web interface and integrated tools for data collection, model development, optimization, and deployment. Its key capabilities include the following:\n\nIntuitive drag-and-drop workflow for building ML models without coding required\nTools for acquiring, labeling, visualizing, and preprocessing data from sensors\nChoice of model architectures, including neural networks and unsupervised learning\nModel optimization techniques to balance performance metrics and hardware constraints\n\nSeamless deployment onto edge devices through compilation, SDKs, and benchmarks\nCollaboration features for teams and integration with other platforms\n\nEdge Impulse offers a comprehensive solution for creating embedded intelligence and advancing machine learning, particularly for developers with limited data science expertise. This platform enables the development of specialized ML models that run efficiently within small computing environments. As illustrated in Figure 13.7, Edge Impulse facilitates the journey from data collection to model deployment, highlighting its user-friendly interface and tools that simplify the creation of embedded ML solutions, thus making it accessible to a broader range of developers and applications.\n\n\n\n\n\n\nFigure 13.7: Edge impulse overview. Source: Edge Impulse\n\n\n\n\nUser Interface\nEdge Impulse was designed with seven key principles: accessibility, end-to-end capabilities, a data-centric approach, interactiveness, extensibility, team orientation, and community support. The intuitive user interface, shown in Figure 13.8, guides developers at all experience levels through uploading data, selecting a model architecture, training the model, and deploying it across relevant hardware platforms. It should be noted that, like any tool, Edge Impulse is intended to assist with, not replace, foundational considerations such as determining if ML is an appropriate solution or acquiring the requisite domain expertise for a given application.\n\n\n\n\n\n\nFigure 13.8: Screenshot of Edge Impulse user interface for building workflows from input data to output features.\n\n\n\nWhat makes Edge Impulse notable is its comprehensive yet intuitive end-to-end workflow. Developers start by uploading their data via the graphical user interface (GUI) or command line interface (CLI) tools, after which they can examine raw samples and visualize the data distribution in the training and test splits. Next, users can pick from various preprocessing “blocks” to facilitate digital signal processing (DSP). While default parameter values are provided, users can customize the parameters as needed, with considerations around memory and latency displayed. Users can easily choose their neural network architecture - without any code needed.\nThanks to the platform’s visual editor, users can customize the architecture’s components and specific parameters while ensuring that the model is still trainable. Users can also leverage unsupervised learning algorithms, such as K-means clustering and Gaussian mixture models (GMM).\n\n\nOptimizations\nTo accommodate the resource constraints of TinyML applications, Edge Impulse provides a confusion matrix summarizing key performance metrics, including per-class accuracy and F1 scores. The platform elucidates the tradeoffs between model performance, size, and latency using simulations in Renode and device-specific benchmarking. For streaming data use cases, a performance calibration tool leverages a genetic algorithm to find ideal post-processing configurations balancing false acceptance and false rejection rates. Techniques like quantization, code optimization, and device-specific optimization are available to optimize models. For deployment, models can be compiled in appropriate formats for target edge devices. Native firmware SDKs also enable direct data collection on devices.\nIn addition to streamlining development, Edge Impulse scales the modeling process itself. A key capability is the EON Tuner, an automated machine learning (AutoML) tool that assists users in hyperparameter tuning based on system constraints. It runs a random search to generate configurations for digital signal processing and training steps quickly. The resulting models are displayed for the user to select based on relevant performance, memory, and latency metrics. For data, active learning facilitates training on a small labeled subset, followed by manually or automatically labeling new samples based on proximity to existing classes. This expands data efficiency.\n\n\nUse Cases\nBeyond the accessibility of the platform itself, the Edge Impulse team has expanded the knowledge base of the embedded ML ecosystem. The platform lends itself to academic environments, having been used in online courses and on-site workshops globally. Numerous case studies featuring industry and research use cases have been published, most notably Oura Ring, which uses ML to identify sleep patterns. The team has made repositories open source on GitHub, facilitating community growth. Users can also make projects public to share techniques and download libraries to share via Apache. Organization-level access enables collaboration on workflows.\nOverall, Edge Impulse is uniquely comprehensive and integrateable for developer workflows. Larger platforms like Google and Microsoft focus more on cloud versus embedded systems. TinyMLOps frameworks such as Neuton AI and Latent AI offer some functionality but lack Edge Impulse’s end-to-end capabilities. TensorFlow Lite Micro is the standard inference engine due to flexibility, open source status, and TensorFlow integration, but it uses more memory and storage than Edge Impulse’s EON Compiler. Other platforms need to be updated, academic-focused, or more versatile. In summary, Edge Impulse streamlines and scale embedded ML through an accessible, automated platform.\n\n\n\nLimitations\nWhile Edge Impulse provides an accessible pipeline for embedded ML, important limitations and risks remain. A key challenge is data quality and availability - the models are only as good as the data used to train them. Users must have sufficient labeled samples that capture the breadth of expected operating conditions and failure modes. Labeled anomalies and outliers are critical yet time-consuming to collect and identify. Insufficient or biased data leads to poor model performance regardless of the tool’s capabilities.\nDeploying low-powered devices also presents inherent challenges. Optimized models may still need to be more resource-intensive for ultra-low-power MCUs. Striking the right balance of compression versus accuracy takes some experimentation. The tool simplifies but still needs to eliminate the need for foundational ML and signal processing expertise. Embedded environments also constrain debugging and interpretability compared to the cloud.\nWhile impressive results are achievable, users shouldn’t view Edge Impulse as a “Push Button ML” solution. Careful project scoping, data collection, model evaluation, and testing are still essential. As with any development tool, reasonable expectations and diligence in application are advised. However, Edge Impulse can accelerate embedded ML prototyping and deployment for developers willing to invest the requisite data science and engineering effort.\n\n\n\n\n\n\nExercise 13.1: Edge Impulse\n\n\n\n\n\nReady to level up your tiny machine-learning projects? Let’s combine the power of Edge Impulse with the awesome visualizations of Weights & Biases (WandB). In this Colab, you’ll learn to track your model’s training progress like a pro! Imagine seeing cool graphs of your model getting smarter, comparing different versions, and ensuring your AI performs its best even on tiny devices.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>ML Operations</span>"
    ]
  },
  {
    "objectID": "contents/core/ops/ops.html#case-studies",
    "href": "contents/core/ops/ops.html#case-studies",
    "title": "13  ML Operations",
    "section": "13.7 Case Studies",
    "text": "13.7 Case Studies\n\n13.7.1 Oura Ring\nThe Oura Ring is a wearable that can measure activity, sleep, and recovery when placed on the user’s finger. Using sensors to track physiological metrics, the device uses embedded ML to predict the stages of sleep. To establish a baseline of legitimacy in the industry, Oura conducted a correlation experiment to evaluate the device’s success in predicting sleep stages against a baseline study. This resulted in a solid 62% correlation compared to the 82-83% baseline. Thus, the team set out to determine how to improve their performance even further.\nThe first challenge was to obtain better data in terms of both quantity and quality. They could host a larger study to get a more comprehensive data set, but the data would be so noisy and large that it would be difficult to aggregate, scrub, and analyze. This is where Edge Impulse comes in.\nWe hosted a massive sleep study of 100 men and women between the ages of 15 and 73 across three continents (Asia, Europe, and North America). In addition to wearing the Oura Ring, participants were responsible for undergoing the industry standard PSG testing, which provided a “label” for this data set. With 440 nights of sleep from 106 participants, the data set totaled 3,444 hours in length across Ring and PSG data. With Edge Impulse, Oura could easily upload and consolidate data from different sources into a private S3 bucket. They were also able to set up a Data Pipeline to merge data samples into individual files and preprocess the data without having to conduct manual scrubbing.\nBecause of the time saved on data processing thanks to Edge Impulse, the Oura team could focus on the key drivers of their prediction. They only extracted three types of sensor data: heart rate, motion, and body temperature. After partitioning the data using five-fold cross-validation and classifying sleep stages, the team achieved a correlation of 79% - just a few percentage points off the standard. They readily deployed two types of sleep detection models: one simplified using just the ring’s accelerometer and one more comprehensive leveraging Autonomic Nervous System (ANS)-mediated peripheral signals and circadian features. With Edge Impulse, they plan to conduct further analyses of different activity types and leverage the platform’s scalability to continue experimenting with different data sources and subsets of extracted features.\nWhile most ML research focuses on model-dominant steps such as training and finetuning, this case study underscores the importance of a holistic approach to MLOps, where even the initial steps of data aggregation and preprocessing fundamentally impact successful outcomes.\n\n\n13.7.2 ClinAIOps\nLet’s look at MLOps in the context of medical health monitoring to better understand how MLOps “matures” in a real-world deployment. Specifically, let’s consider continuous therapeutic monitoring (CTM) enabled by wearable devices and sensors. CTM captures detailed physiological data from patients, providing the opportunity for more frequent and personalized adjustments to treatments.\nWearable ML-enabled sensors enable continuous physiological and activity monitoring outside clinics, opening up possibilities for timely, data-driven therapy adjustments. For example, wearable insulin biosensors (Psoma and Kanthou 2023) and wrist-worn ECG sensors for glucose monitoring (J. Li et al. 2021) can automate insulin dosing for diabetes, wrist-worn ECG and PPG sensors can adjust blood thinners based on atrial fibrillation patterns (Attia et al. 2018; Guo et al. 2019), and accelerometers tracking gait can trigger preventative care for declining mobility in the elderly (Liu et al. 2022). The variety of signals that can now be captured passively and continuously allows therapy titration and optimization tailored to each patient’s changing needs. By closing the loop between physiological sensing and therapeutic response with TinyML and on-device learning, wearables are poised to transform many areas of personalized medicine.\n\nPsoma, Sotiria D., and Chryso Kanthou. 2023. “Wearable Insulin Biosensors for Diabetes Management: Advances and Challenges.” Biosensors 13 (7): 719. https://doi.org/10.3390/bios13070719.\n\nLi, Jingzhen, Igbe Tobore, Yuhang Liu, Abhishek Kandwal, Lei Wang, and Zedong Nie. 2021. “Non-Invasive Monitoring of Three Glucose Ranges Based on ECG by Using DBSCAN-CNN.” IEEE Journal of Biomedical and Health Informatics 25 (9): 3340–50. https://doi.org/10.1109/jbhi.2021.3072628.\n\nAttia, Zachi I., Alan Sugrue, Samuel J. Asirvatham, Michael J. Ackerman, Suraj Kapa, Paul A. Friedman, and Peter A. Noseworthy. 2018. “Noninvasive Assessment of Dofetilide Plasma Concentration Using a Deep Learning (Neural Network) Analysis of the Surface Electrocardiogram: A Proof of Concept Study.” PLOS ONE 13 (8): e0201059. https://doi.org/10.1371/journal.pone.0201059.\n\nGuo, Yutao, Hao Wang, Hui Zhang, Tong Liu, Zhaoguang Liang, Yunlong Xia, Li Yan, et al. 2019. “Mobile Photoplethysmographic Technology to Detect Atrial Fibrillation.” Journal of the American College of Cardiology 74 (19): 2365–75. https://doi.org/10.1016/j.jacc.2019.08.019.\n\nLiu, Yingcheng, Guo Zhang, Christopher G. Tarolli, Rumen Hristov, Stella Jensen-Roberts, Emma M. Waddell, Taylor L. Myers, et al. 2022. “Monitoring Gait at Home with Radio Waves in Parkinson’s Disease: A Marker of Severity, Progression, and Medication Response.” Science Translational Medicine 14 (663): eadc9669. https://doi.org/10.1126/scitranslmed.adc9669.\nML holds great promise in analyzing CTM data to provide data-driven recommendations for therapy adjustments. But simply deploying AI models in silos, without integrating them properly into clinical workflows and decision-making, can lead to poor adoption or suboptimal outcomes. In other words, thinking about MLOps alone is insufficient to make them useful in practice. This study shows that frameworks are needed to incorporate AI and CTM into real-world clinical practice seamlessly.\nThis case study analyzes “ClinAIOps” as a model for embedded ML operations in complex clinical environments (Chen et al. 2023). We provide an overview of the framework and why it’s needed, walk through an application example, and discuss key implementation challenges related to model monitoring, workflow integration, and stakeholder incentives. Analyzing real-world examples like ClinAIOps illuminates crucial principles and best practices for reliable and effective AI Ops across many domains.\nTraditional MLOps frameworks are insufficient for integrating continuous therapeutic monitoring (CTM) and AI in clinical settings for a few key reasons:\n\nMLOps focuses on the ML model lifecycle—training, deployment, monitoring. But healthcare involves coordinating multiple human stakeholders—patients and clinicians—not just models.\nMLOps automates IT system monitoring and management. However, optimizing patient health requires personalized care and human oversight, not just automation.\nCTM and healthcare delivery are complex sociotechnical systems with many moving parts. MLOps doesn’t provide a framework for coordinating human and AI decision-making.\nEthical considerations regarding healthcare AI require human judgment, oversight, and accountability. MLOps frameworks lack processes for ethical oversight.\nPatient health data is highly sensitive and regulated. MLOps alone doesn’t ensure the handling of protected health information to privacy and regulatory standards.\nClinical validation of AI-guided treatment plans is essential for provider adoption. MLOps doesn’t incorporate domain-specific evaluation of model recommendations.\nOptimizing healthcare metrics like patient outcomes requires aligning stakeholder incentives and workflows, which pure tech-focused MLOps overlooks.\n\nThus, effectively integrating AI/ML and CTM in clinical practice requires more than just model and data pipelines; it requires coordinating complex human-AI collaborative decision-making, which ClinAIOps addresses via its multi-stakeholder feedback loops.\n\nFeedback Loops\nThe ClinAIOps framework, shown in Figure 13.9, provides these mechanisms through three feedback loops. The loops are useful for coordinating the insights from continuous physiological monitoring, clinician expertise, and AI guidance via feedback loops, enabling data-driven precision medicine while maintaining human accountability. ClinAIOps provides a model for effective human-AI symbiosis in healthcare: the patient is at the center, providing health challenges and goals that inform the therapy regimen; the clinician oversees this regimen, giving inputs for adjustments based on continuous monitoring data and health reports from the patient; whereas AI developers play a crucial role by creating systems that generate alerts for therapy updates, which the clinician then vets.\nThese feedback loops, which we will discuss below, help maintain clinician responsibility and control over treatment plans by reviewing AI suggestions before they impact patients. They help dynamically customize AI model behavior and outputs to each patient’s changing health status. They help improve model accuracy and clinical utility over time by learning from clinician and patient responses. They facilitate shared decision-making and personalized care during patient-clinician interactions. They enable rapid optimization of therapies based on frequent patient data that clinicians cannot manually analyze.\n\n\n\n\n\n\nFigure 13.9: ClinAIOps cycle. Source: Chen et al. (2023).\n\n\n\n\nPatient-AI Loop\nThe patient-AI loop enables frequent therapy optimization driven by continuous physiological monitoring. Patients are prescribed wearables like smartwatches or skin patches to collect relevant health signals passively. For example, a diabetic patient could have a continuous glucose monitor, or a heart disease patient may wear an ECG patch. An AI model analyzes the patient’s longitudinal health data streams in the context of their electronic medical records - their diagnoses, lab tests, medications, and demographics. The AI model suggests adjustments to the treatment regimen tailored to that individual, like changing a medication dose or administration schedule. Minor adjustments within a pre-approved safe range can be made by the patient independently, while major changes are reviewed by the clinician first. This tight feedback between the patient’s physiology and AI-guided therapy allows data-driven, timely optimizations like automated insulin dosing recommendations based on real-time glucose levels for diabetes patients.\n\n\nClinician-AI Loop\nThe clinician-AI loop allows clinical oversight over AI-generated recommendations to ensure safety and accountability. The AI model provides the clinician with treatment recommendations and easily reviewed summaries of the relevant patient data on which the suggestions are based. For instance, an AI may suggest lowering a hypertension patient’s blood pressure medication dose based on continuously low readings. The clinician can accept, reject, or modify the AI’s proposed prescription changes. This clinician feedback further trains and improves the model. Additionally, the clinician sets the bounds for the types and extent of treatment changes the AI can autonomously recommend to patients. By reviewing AI suggestions, the clinician maintains ultimate treatment authority based on their clinical judgment and accountability. This loop allows them to oversee patient cases with AI assistance efficiently.\n\n\nPatient-Clinician Loop\nInstead of routine data collection, the clinician can focus on interpreting high-level data patterns and collaborating with the patient to set health goals and priorities. The AI assistance will also free up clinicians’ time, allowing them to focus more deeply on listening to patients’ stories and concerns. For instance, the clinician may discuss diet and exercise changes with a diabetes patient to improve their glucose control based on their continuous monitoring data. Appointment frequency can also be dynamically adjusted based on patient progress rather than following a fixed calendar. Freed from basic data gathering, the clinician can provide coaching and care customized to each patient informed by their continuous health data. The patient-clinician relationship is made more productive and personalized.\n\n\n\nHypertension Example\nLet’s consider an example. According to the Centers for Disease Control and Prevention, nearly half of adults have hypertension (48.1%, 119.9 million). Hypertension can be managed through ClinAIOps with the help of wearable sensors using the following approach:\n\nData Collection\nThe data collected would include continuous blood pressure monitoring using a wrist-worn device equipped with photoplethysmography (PPG) and electrocardiography (ECG) sensors to estimate blood pressure (Q. Zhang, Zhou, and Zeng 2017). The wearable would also track the patient’s physical activity via embedded accelerometers. The patient would log any antihypertensive medications they take, along with the time and dose. The patient’s demographic details and medical history from their electronic health record (EHR) would also be incorporated. This multimodal real-world data provides valuable context for the AI model to analyze the patient’s blood pressure patterns, activity levels, medication adherence, and responses to therapy.\n\nZhang, Qingxue, Dian Zhou, and Xuan Zeng. 2017. “Highly Wearable Cuff-Less Blood Pressure and Heart Rate Monitoring with Single-Arm Electrocardiogram and Photoplethysmogram Signals.” BioMedical Engineering OnLine 16 (1): 23. https://doi.org/10.1186/s12938-017-0317-z.\n\n\nAI Model\nThe on-device AI model would analyze the patient’s continuous blood pressure trends, circadian patterns, physical activity levels, medication adherence behaviors, and other contexts. It would use ML to predict optimal antihypertensive medication doses and timing to control the individual’s blood pressure. The model would send dosage change recommendations directly to the patient for minor adjustments or to the reviewing clinician for approval for more significant modifications. By observing clinician feedback on its recommendations and evaluating the resulting blood pressure outcomes in patients, the AI model could be continually retrained to improve performance. The goal is fully personalized blood pressure management optimized for each patient’s needs and responses.\n\n\nPatient-AI Loop\nIn the Patient-AI loop, the hypertensive patient would receive notifications on their wearable device or tethered smartphone app recommending adjustments to their antihypertensive medications. For minor dose changes within a pre-defined safe range, the patient could independently implement the AI model’s suggested adjustment to their regimen. However, the patient must obtain clinician approval before changing their dosage for more significant modifications. Providing personalized and timely medication recommendations automates an element of hypertension self-management for the patient. It can improve their adherence to the regimen as well as treatment outcomes. The patient is empowered to leverage AI insights to control their blood pressure better.\n\n\nClinician-AI Loop\nIn the Clinician-AI loop, the provider would receive summaries of the patient’s continuous blood pressure trends and visualizations of their medication-taking patterns and adherence. They review the AI model’s suggested antihypertensive dosage changes and decide whether to approve, reject, or modify the recommendations before they reach the patient. The clinician also specifies the boundaries for how much the AI can independently recommend changing dosages without clinician oversight. If the patient’s blood pressure is trending at dangerous levels, the system alerts the clinician so they can promptly intervene and adjust medications or request an emergency room visit. This loop maintains accountability and safety while allowing the clinician to harness AI insights by keeping the clinician in charge of approving major treatment changes.\n\n\nPatient-Clinician Loop\nIn the Patient-Clinician loop, shown in Figure 13.10, the in-person visits would focus less on collecting data or basic medication adjustments. Instead, the clinician could interpret high-level trends and patterns in the patient’s continuous monitoring data and have focused discussions about diet, exercise, stress management, and other lifestyle changes to improve their blood pressure control holistically. The frequency of appointments could be dynamically optimized based on the patient’s stability rather than following a fixed calendar. Since the clinician would not need to review all the granular data, they could concentrate on delivering personalized care and recommendations during visits. With continuous monitoring and AI-assisted optimization of medications between visits, the clinician-patient relationship focuses on overall wellness goals and becomes more impactful. This proactive and tailored data-driven approach can help avoid hypertension complications like stroke, heart failure, and other threats to patient health and well-being.\n\n\n\n\n\n\nFigure 13.10: ClinAIOps interactive loop. Source: Chen et al. (2023).\n\n\nChen, Emma, Shvetank Prakash, Vijay Janapa Reddi, David Kim, and Pranav Rajpurkar. 2023. “A Framework for Integrating Artificial Intelligence for Clinical Care with Continuous Therapeutic Monitoring.” Nature Biomedical Engineering, November. https://doi.org/10.1038/s41551-023-01115-0.\n\n\n\n\n\nMLOps vs. ClinAIOps\nThe hypertension example illustrates well why traditional MLOps are insufficient for many real-world AI applications and why frameworks like ClinAIOps are needed instead.\nWith hypertension, simply developing and deploying an ML model for adjusting medications would only succeed if it considered the broader clinical context. The patient, clinician, and health system have concerns about shaping adoption. The AI model cannot optimize blood pressure outcomes alone—it requires integrating with workflows, behaviors, and incentives.\n\nSome key gaps the example highlights in a pure MLOps approach:\nThe model itself would lack the real-world patient data at scale to recommend treatments reliably. ClinAIOps enables this by collecting feedback from clinicians and patients via continuous monitoring.\nClinicians would only trust model recommendations with transparency, explainability, and accountability. ClinAIOps keeps the clinician in the loop to build confidence.\nPatients need personalized coaching and motivation - not just AI notifications. The ClinAIOps patient-clinician loop facilitates this.\nSensor reliability and data accuracy would only be sufficient with clinical oversight. ClinAIOps validates recommendations.\nLiability for treatment outcomes must be clarified with just an ML model. ClinAIOps maintains human accountability.\nHealth systems would need to demonstrate value to change workflows. ClinAIOps aligns stakeholders.\n\nThe hypertension case clearly shows the need to look beyond training and deploying a performant ML model to consider the entire human-AI sociotechnical system. This is the key gap ClinAIOps addresses over traditional MLOps. Traditional MLOps is overly tech-focused on automating ML model development and deployment, while ClinAIOps incorporates clinical context and human-AI coordination through multi-stakeholder feedback loops.\nTable 13.3 compares them. This table highlights how, when MLOps is implemented, we need to consider more than just ML models.\n\n\n\nTable 13.3: Comparison of MLOps versus AI operations for clinical use.\n\n\n\n\n\n\n\n\n\n\n\nTraditional MLOps\nClinAIOps\n\n\n\n\nFocus\nML model development and deployment\nCoordinating human and AI decision-making\n\n\nStakeholders\nData scientists, IT engineers\nPatients, clinicians, AI developers\n\n\nFeedback loops\nModel retraining, monitoring\nPatient-AI, clinician-AI, patient-clinician\n\n\nObjective\nOperationalize ML deployments\nOptimize patient health outcomes\n\n\nProcesses\nAutomated pipelines and infrastructure\nIntegrates clinical workflows and oversight\n\n\nData considerations\nBuilding training datasets\nPrivacy, ethics, protected health information\n\n\nModel validation\nTesting model performance metrics\nClinical evaluation of recommendations\n\n\nImplementation\nFocuses on technical integration\nAligns incentives of human stakeholders\n\n\n\n\n\n\n\n\nSummary\nIn complex domains like healthcare, successfully deploying AI requires moving beyond a narrow focus on training and deploying performant ML models. As illustrated through the hypertension example, real-world integration of AI necessitates coordinating diverse stakeholders, aligning incentives, validating recommendations, and maintaining accountability. Frameworks like ClinAIOps, which facilitate collaborative human-AI decision-making through integrated feedback loops, are needed to address these multifaceted challenges. Rather than just automating tasks, AI must augment human capabilities and clinical workflows. This allows AI to positively impact patient outcomes, population health, and healthcare efficiency.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>ML Operations</span>"
    ]
  },
  {
    "objectID": "contents/core/ops/ops.html#conclusion",
    "href": "contents/core/ops/ops.html#conclusion",
    "title": "13  ML Operations",
    "section": "13.8 Conclusion",
    "text": "13.8 Conclusion\nEmbedded ML is poised to transform many industries by enabling AI capabilities directly on edge devices like smartphones, sensors, and IoT hardware. However, developing and deploying TinyML models on resource-constrained embedded systems poses unique challenges compared to traditional cloud-based MLOps.\nThis chapter provided an in-depth analysis of key differences between traditional and embedded MLOps across the model lifecycle, development workflows, infrastructure management, and operational practices. We discussed how factors like intermittent connectivity, decentralized data, and limited on-device computing necessitate innovative techniques like federated learning, on-device inference, and model optimization. Architectural patterns like cross-device learning and hierarchical edge-cloud infrastructure help mitigate constraints.\nThrough concrete examples like Oura Ring and ClinAIOps, we demonstrated applied principles for embedded MLOps. The case studies highlighted critical considerations beyond core ML engineering, like aligning stakeholder incentives, maintaining accountability, and coordinating human-AI decision-making. This underscores the need for a holistic approach spanning both technical and human elements.\nWhile embedded MLOps face impediments, emerging tools like Edge Impulse and lessons from pioneers help accelerate TinyML innovation. A solid understanding of foundational MLOps principles tailored to embedded environments will empower more organizations to overcome constraints and deliver distributed AI capabilities. As frameworks and best practices mature, seamlessly integrating ML into edge devices and processes will transform industries through localized intelligence.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>ML Operations</span>"
    ]
  },
  {
    "objectID": "contents/core/ops/ops.html#sec-embedded-aiops-resource",
    "href": "contents/core/ops/ops.html#sec-embedded-aiops-resource",
    "title": "13  ML Operations",
    "section": "13.9 Resources",
    "text": "13.9 Resources\nHere is a curated list of resources to support students and instructors in their learning and teaching journeys. We are continuously working on expanding this collection and will add new exercises soon.\n\n\n\n\n\n\nSlides\n\n\n\n\n\nThese slides serve as a valuable tool for instructors to deliver lectures and for students to review the material at their own pace. We encourage both students and instructors to leverage these slides to improve their understanding and facilitate effective knowledge transfer.\n\nMLOps, DevOps, and AIOps.\nMLOps overview.\nTiny MLOps.\nMLOps: a use case.\nMLOps: Key Activities and Lifecycle.\nML Lifecycle.\nScaling TinyML: Challenges and Opportunities.\nTraining Operationalization:\n\nTraining Ops: CI/CD trigger.\nContinuous Integration.\nContinuous Deployment.\nProduction Deployment.\nProduction Deployment: Online Experimentation.\nTraining Ops Impact on MLOps.\n\nModel Deployment:\n\nScaling ML Into Production Deployment.\nContainers for Scaling ML Deployment.\nChallenges for Scaling TinyML Deployment: Part 1.\nChallenges for Scaling TinyML Deployment: Part 2.\nModel Deployment Impact on MLOps.\n\n\n\n\n\n\n\n\n\n\n\nVideos\n\n\n\n\n\n\nVideo 13.1\nVideo 13.2\nVideo 13.3\nVideo 13.4\n\n\n\n\n\n\n\n\n\n\nExercises\n\n\n\n\n\nTo reinforce the concepts covered in this chapter, we have curated a set of exercises that challenge students to apply their knowledge and deepen their understanding.\n\nExercise 13.1",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>ML Operations</span>"
    ]
  },
  {
    "objectID": "contents/core/ondevice_learning/ondevice_learning.html",
    "href": "contents/core/ondevice_learning/ondevice_learning.html",
    "title": "14  On-Device Learning",
    "section": "",
    "text": "Purpose\nResources: Slides, Videos, Exercises\nHow does learning at the edge transform traditional machine learning paradigms, and what principles enable effective adaptation in resource-constrained environments?\nThe migration of learning capabilities to edge devices represents a fundamental shift in how AI systems evolve and adapt. Local learning introduces unique patterns for balancing model adaptation with resource limitations, revealing essential relationships between computational constraints and system autonomy. The implementation of on-device training emphasizes the trade-offs between learning capability, energy efficiency, and operational independence. These adaptation mechanisms provide insights into designing self-evolving systems, establishing core principles for creating AI solutions that can learn and improve within the constraints of local computing environments.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>On-Device Learning</span>"
    ]
  },
  {
    "objectID": "contents/core/ondevice_learning/ondevice_learning.html#purpose",
    "href": "contents/core/ondevice_learning/ondevice_learning.html#purpose",
    "title": "14  On-Device Learning",
    "section": "",
    "text": "Learning Objectives\n\n\n\n\nUnderstand on-device learning and how it differs from cloud-based training\nRecognize the benefits and limitations of on-device learning\nExamine strategies to adapt models through complexity reduction, optimization, and data compression\nUnderstand related concepts like federated learning and transfer learning\nAnalyze the security implications of on-device learning and mitigation strategies",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>On-Device Learning</span>"
    ]
  },
  {
    "objectID": "contents/core/ondevice_learning/ondevice_learning.html#overview",
    "href": "contents/core/ondevice_learning/ondevice_learning.html#overview",
    "title": "14  On-Device Learning",
    "section": "14.1 Overview",
    "text": "14.1 Overview\nOn-device learning refers to training ML models directly on the device where they are deployed, as opposed to traditional methods where models are trained on powerful servers and then deployed to devices. This method is particularly relevant to TinyML, where ML systems are integrated into tiny, resource-constrained devices.\nAn example of on-device learning can be seen in a smart thermostat that adapts to user behavior over time. Initially, the thermostat may have a generic model that understands basic usage patterns. However, as it is exposed to more data, such as the times the user is home or away, preferred temperatures, and external weather conditions, the thermostat can refine its model directly on the device to provide a personalized experience. This is all done without sending data back to a central server for processing.\nAnother example is in predictive text on smartphones. As users type, the phone learns from the user’s language patterns and suggests words or phrases that are likely to be used next. This learning happens directly on the device, and the model updates in real-time as more data is collected. A widely used real-world example of on-device learning is Gboard. On an Android phone, Gboard learns from typing and dictation patterns to enhance the experience for all users.\nIn some cases, on-device learning can be coupled with a federated learning setup, where each device tunes its model locally using only the data stored on that device. This approach allows the model to learn from each device’s unique data without transmitting any of it to a central server. As shown in Figure 14.1, federated learning preserves privacy by keeping all personal data on the device, ensuring that the training process remains entirely on-device, with only summarized model updates shared across devices.\n\n\n\n\n\n\nFigure 14.1: Federated learning cycle. Source: Google Research.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>On-Device Learning</span>"
    ]
  },
  {
    "objectID": "contents/core/ondevice_learning/ondevice_learning.html#advantages-and-limitations",
    "href": "contents/core/ondevice_learning/ondevice_learning.html#advantages-and-limitations",
    "title": "14  On-Device Learning",
    "section": "14.2 Advantages and Limitations",
    "text": "14.2 Advantages and Limitations\nOn-device learning provides several advantages over traditional cloud-based ML. By keeping data and models on the device, it eliminates the need for costly data transmission and addresses privacy concerns. This allows for more personalized, responsive experiences, as the model can adapt in real-time to user behavior.\nHowever, on-device learning also comes with drawbacks. The limited computing resources on consumer devices can make it challenging to run complex models locally. Datasets are also more restricted since they consist only of user-generated data from a single device. Additionally, updating models on each device can be more challenging, as it often requires deploying new versions to each device individually, rather than seamlessly updating a single model in the cloud.\nOn-device learning opens up new capabilities by enabling offline AI while maintaining user privacy. However, it requires carefully managing model and data complexity within the constraints of consumer devices. Finding the right balance between localization and cloud offloading is key to optimizing on-device experiences.\n\n14.2.1 Benefits\n\nPrivacy and Data Security\nOne of the significant advantages of on-device learning is the enhanced privacy and security of user data. For instance, consider a smartwatch that monitors sensitive health metrics such as heart rate and blood pressure. By processing data and adapting models directly on the device, the biometric data remains localized, circumventing the need to transmit raw data to cloud servers where it could be susceptible to breaches.\nServer breaches are far from rare, with millions of records compromised annually. For example, the 2017 Equifax breach exposed the personal data of 147 million people. By keeping data on the device, the risk of such exposures is drastically minimized. On-device learning eliminates reliance on centralized cloud storage and safeguards against unauthorized access from various threats, including malicious actors, insider threats, and accidental exposure.\nRegulations like the Health Insurance Portability and Accountability Act (HIPAA) and the General Data Protection Regulation (GDPR) mandate stringent data privacy requirements that on-device learning adeptly addresses. By ensuring data remains localized and is not transferred to other systems, on-device learning facilitates compliance with these regulations.\nOn-device learning is not just beneficial for individual users; it has significant implications for organizations and sectors dealing with highly sensitive data. For instance, within the military, on-device learning empowers frontline systems to adapt models and function independently of connections to central servers that could potentially be compromised. Critical and sensitive information is staunchly protected by localizing data processing and learning. However, a drawback is that individual devices take on more value and may incentivize theft or destruction as they become the sole carriers of specialized AI models. Care must be taken to secure devices themselves when transitioning to on-device learning.\nIt is also important to preserve the privacy, security, and regulatory compliance of personal and sensitive data. Instead of in the cloud, training and operating models locally substantially augment privacy measures, ensuring that user data is safeguarded from potential threats.\nHowever, this is only partially intuitive because on-device learning could instead open systems up to new privacy attacks. With valuable data summaries and model updates permanently stored on individual devices, it may be much harder to physically and digitally protect them than a large computing cluster. While on-device learning reduces the amount of data compromised in any one breach, it could also introduce new dangers by dispersing sensitive information across many decentralized endpoints. Careful security practices are still essential for on-device systems.\n\n\nRegulatory Compliance\nOn-device learning helps address major privacy regulations like GDPR and CCPA. These regulations require data localization, restricting cross-border data transfers to approved countries with adequate controls. GDPR also mandates privacy by design and consent requirements for data collection. By keeping data processing and model training localized on-device, sensitive user data is not transferred across borders. This avoids major compliance headaches for organizations.\nFor example, a healthcare provider monitoring patient vitals with wearables must ensure cross-border data transfers comply with HIPAA and GDPR if using the cloud. Determining which country’s laws apply and securing approvals for international data flows introduces legal and engineering burdens. With on-device learning, no data leaves the device, simplifying compliance. The time and resources spent on compliance are reduced significantly.\nIndustries like healthcare, finance, and government, which have highly regulated data, can benefit greatly from on-device learning. By localizing data and learning, regulatory privacy and data sovereignty requirements are more easily met. On-device solutions provide an efficient way to build compliant AI applications.\nMajor privacy regulations impose restrictions on cross-border data movement that on-device learning inherently addresses through localized processing. This reduces the compliance burden for organizations working with regulated data.\n\n\nReduced Bandwidth, Costs, and Increased Efficiency\nOne major advantage of on-device learning is the significant reduction in bandwidth usage and associated cloud infrastructure costs. By keeping data localized for model training rather than transmitting raw data to the cloud, on-device learning can result in substantial bandwidth savings. For instance, a network of cameras analyzing video footage can achieve significant reductions in data transfer by training models on-device rather than streaming all video footage to the cloud for processing.\nThis reduction in data transmission saves bandwidth and translates to lower costs for servers, networking, and data storage in the cloud. Large organizations, which might spend millions on cloud infrastructure to train models, can experience dramatic cost reductions through on-device learning. In the era of Generative AI, where costs have been escalating significantly, finding ways to keep expenses down has become increasingly important.\nFurthermore, the energy and environmental costs of running large server farms are also diminished. Data centers consume vast amounts of energy, contributing to greenhouse gas emissions. By reducing the need for extensive cloud-based infrastructure, on-device learning plays a part in mitigating the environmental impact of data processing (Wu et al. 2022).\n\nWu, Carole-Jean, Ramya Raghavendra, Udit Gupta, Bilge Acun, Newsha Ardalani, Kiwan Maeng, Gloria Chang, et al. 2022. “Sustainable Ai: Environmental Implications, Challenges and Opportunities.” Proceedings of Machine Learning and Systems 4: 795–813.\nSpecifically for endpoint applications, on-device learning minimizes the number of network API calls needed to run inference through a cloud provider. The cumulative costs associated with bandwidth and API calls can quickly escalate for applications with millions of users. In contrast, performing training and inferences locally is considerably more efficient and cost-effective. Under state-of-the-art optimizations, on-device learning has been shown to reduce training memory requirements, drastically improve memory efficiency, and reduce up to 20% in per-iteration latency (Dhar et al. 2021).\n\n\nLifelong Learning\nOne of the key benefits of on-device learning is its ability to support lifelong learning, allowing models to continuously adapt to new data and evolving user behavior directly on the device. In dynamic environments, data patterns can change over time—a phenomenon known as data drift—which can degrade model accuracy and relevance if the model remains static. For example, user preferences, seasonal trends, or even external conditions (such as network traffic patterns or weather) can evolve, requiring models to adjust in order to maintain optimal performance.\nOn-device learning enables models to address this by adapting incrementally as new data becomes available. This continuous adaptation process allows models to remain relevant and effective, reducing the need for frequent cloud updates. Local adaptations reduce the need to transmit large datasets back to the cloud for retraining, saving bandwidth and ensuring data privacy.\n\n\n\n14.2.2 Limitations\nWhile traditional cloud-based ML systems have access to nearly endless computing resources, on-device learning is often restricted by the limitations in computational and storage power of the edge device that the model is trained on. By definition, an edge device is a device with restrained computing, memory, and energy resources that cannot be easily increased or decreased. Thus, the reliance on edge devices can restrict the complexity, efficiency, and size of on-device ML models.\n\nCompute resources\nTraditional cloud-based ML systems use large servers with multiple high-end GPUs or TPUs, providing nearly endless computational power and memory. For example, services like Amazon Web Services (AWS) EC2 allow configuring clusters of GPU instances for massively parallel training.\nIn contrast, on-device learning is restricted by the hardware limitations of the edge device on which it runs. Edge devices refer to endpoints like smartphones, embedded electronics, and IoT devices. By definition, these devices have highly restrained computing, memory, and energy resources compared to the cloud.\nFor example, a typical smartphone or Raspberry Pi may only have a few CPU cores, a few GB of RAM, and a small battery. Even more resource-constrained are TinyML microcontroller devices such as the Arduino Nano BLE Sense. The resources are fixed on these devices and can’t easily be increased on demand, such as scaling cloud infrastructure. This reliance on edge devices directly restricts the complexity, efficiency, and size of models that can be deployed for on-device training:\n\nComplexity: Limits on memory, computing, and power restrict model architecture design, constraining the number of layers and parameters.\nEfficiency: Models must be heavily optimized through methods like quantization and pruning to run faster and consume less energy.\nSize: Actual model files must be compressed as much as possible to fit within the storage limitations of edge devices.\n\nThus, while the cloud offers endless scalability, on-device learning must operate within the tight resource constraints of endpoint hardware. This requires careful codesign of streamlined models, training methods, and optimizations tailored specifically for edge devices.\n\n\nDataset Size, Accuracy, and Generalization\nIn addition to limited computing resources, on-device learning is also constrained by the dataset available for training models.\nIn the cloud, models are trained on massive, diverse datasets like ImageNet or Common Crawl. For example, ImageNet contains over 14 million images carefully categorized across thousands of classes.\nOn-device learning instead relies on smaller, decentralized data silos unique to each device. A smartphone camera roll may contain only thousands of photos of users’ interests and environments.\nIn machine learning, effective model training often assumes that data is independent and identically distributed (IID). This means that each data point is generated independently (without influencing other points) and follows the same statistical distribution as the rest of the data. When data is IID, models trained on it are more likely to generalize well to new, similar data. However, in on-device learning, this IID condition is rarely met, as data is highly specific to individual users and contexts. For example, two friends may take similar photos of the same places, creating correlated data that doesn’t represent a broader population or the variety needed for generalization.\nReasons data may be non-IID in on-device settings:\n\nUser heterogeneity: Different users have different interests and environments.\nDevice differences: Sensors, regions, and demographics affect data.\nTemporal effects: time of day, seasonal impacts on data.\n\nThe effectiveness of ML relies heavily on large, diverse training data. With small, localized datasets, on-device models may fail to generalize across different user populations and environments. For example, a disease detection model trained only on images from a single hospital would not generalize well to other patient demographics. The real-world performance would only improve with extensive and diverse medical advancements. Thus, while cloud-based learning leverages massive datasets, on-device learning relies on much smaller, decentralized data silos unique to each user.\nThe limited data and optimizations required for on-device learning can negatively impact model accuracy and generalization:\n\nSmall datasets increase overfitting risk. For example, a fruit classifier trained on 100 images risks overfitting compared to one trained on 1 million diverse images.\nNoisy user-generated data reduces quality. Sensor noise or improper data labeling by non-experts may degrade training.\nOptimizations like pruning and quantization trade off accuracy for efficiency. An 8-bit quantized model runs faster but less accurately than a 32-bit model.\n\nSo while cloud models achieve high accuracy with massive datasets and no constraints, on-device models can struggle to generalize. Some studies show that on-device training matches cloud accuracy on select tasks. However, performance on real-world workloads requires further study (Lin et al. 2022). For instance, a cloud model can accurately detect pneumonia in chest X-rays from thousands of hospitals. However, an on-device model trained only on a small local patient population may fail to generalize. This limits the real-world applicability of on-device learning for mission-critical uses like disease diagnosis or self-driving vehicles.\nOn-device training is also slower than the cloud due to limited resources. Even if each iteration is faster, the overall training process takes longer. For example, a real-time robotics application may require model updates within milliseconds. On-device training on small embedded hardware may take seconds or minutes per update - too slow for real-time use.\nAccuracy, generalization, and speed challenges pose hurdles to adopting on-device learning for real-world production systems, especially when reliability and low latency are critical.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>On-Device Learning</span>"
    ]
  },
  {
    "objectID": "contents/core/ondevice_learning/ondevice_learning.html#on-device-adaptation",
    "href": "contents/core/ondevice_learning/ondevice_learning.html#on-device-adaptation",
    "title": "14  On-Device Learning",
    "section": "14.3 On-device Adaptation",
    "text": "14.3 On-device Adaptation\nIn an ML task, resource consumption mainly comes from three sources:\n\nThe ML model itself;\nThe optimization process during model learning\nStoring and processing the dataset used for learning.\n\nCorrespondingly, there are three approaches to adapting existing ML algorithms onto resource-constrained devices:\n\nReducing the complexity of the ML model\nModifying optimizations to reduce training resource requirements\nCreating new storage-efficient data representations\n\nIn the following section, we will review these on-device learning adaptation methods. The Model Optimizations chapter provides more details on model optimizations.\n\n14.3.1 Reducing Model Complexity\nIn this section, we will briefly discuss ways to reduce model complexity when adapting ML models on-device. For details on reducing model complexity, please refer to the Model Optimization Chapter.\n\nTraditional ML Algorithms\nDue to edge devices’ computing and memory limitations, select traditional ML algorithms are great candidates for on-device learning applications due to their lightweight nature. Some example algorithms with low resource footprints include Naive Bayes Classifiers, Support Vector Machines (SVMs), Linear Regression, Logistic Regression, and select Decision Tree algorithms.\nWith some refinements, these classical ML algorithms can be adapted to specific hardware architectures and perform simple tasks. Their low-performance requirements make it easy to integrate continuous learning even on edge devices.\n\n\nPruning\nAs discussed in Section 10.2.1, pruning is a key technique for reducing the size and complexity of ML models. For on-device learning, pruning is particularly valuable, as it minimizes resource consumption while retaining competitive accuracy. By removing less informative components of a model, pruning allows ML models to run more efficiently on resource-limited devices.\nIn the context of on-device learning, pruning is applied to adapt complex deep learning models to the limited memory and processing power of edge devices. For example, pruning can reduce the number of neurons or connections in a DNN, resulting in a model that consumes less memory and requires fewer computations. This approach simplifies the neural network structure, resulting in a more compact and efficient model.\n\n\nReducing Complexity of Deep Learning Models\nTraditional cloud-based DNN frameworks have too much memory overhead to be used on-device. For example, deep learning systems like PyTorch and TensorFlow require hundreds of megabytes of memory overhead when training models such as MobilenetV2, and the overhead scales as the number of training parameters increases.\nCurrent research for lightweight DNNs mostly explores CNN architectures. Several bare-metal frameworks designed for running Neural Networks on MCUs by keeping computational overhead and memory footprint low also exist. Some examples include MNN, TVM, and TensorFlow Lite. However, they can only perform inference during forward passes and lack support for backpropagation. While these models are designed for edge deployment, their reduction in model weights and architectural connections led to reduced resource requirements for continuous learning.\nThe tradeoff between performance and model support is clear when adapting the most popular DNN systems. How do we adapt existing DNN models to resource-constrained settings while maintaining support for backpropagation and continuous learning? The latest research suggests algorithm and system codesign techniques that help reduce the resource consumption of ML training on edge devices. Utilizing techniques such as quantization-aware scaling (QAS), sparse updates, and other cutting-edge techniques, on-device learning is possible on embedded systems with a few hundred kilobytes of RAM without additional memory while maintaining high accuracy.\n\n\n\n14.3.2 Modifying Optimization Processes\nChoosing the right optimization strategy is important for DNN training on a device since this allows for finding a good local minimum. Since training occurs on a device, this strategy must also consider limited memory and power.\n\nQuantization-Aware Scaling\nQuantization is a common method for reducing the memory footprint of DNN training. Although this could introduce new errors, these errors can be mitigated by designing a model to characterize this statistical error. For example, models could use stochastic rounding or introduce the quantization error into the gradient updates.\nA specific algorithmic technique is Quantization-Aware Scaling (QAS), which improves the performance of neural networks on low-precision hardware, such as edge devices, mobile devices, or TinyML systems, by adjusting the scale factors during the quantization process.\nAs we discussed in the Model Optimizations chapter, quantization is the process of mapping a continuous range of values to a discrete set of values. In the context of neural networks, this often involves reducing the precision of weights and activations from 32-bit floating point to lower-precision formats such as 8-bit integers. This reduction in precision can significantly decrease the model’s computational cost and memory footprint, making it suitable for deployment on low-precision hardware. Figure 14.2 illustrates this concept, showing an example of float-to-integer quantization where high-precision floating-point values are mapped to a more compact integer representation. This visual representation helps to clarify how quantization can maintain the essential structure of the data while reducing its complexity and storage requirements.\n\n\n\n\n\n\nFigure 14.2: Float to integer quantization. Source: Nvidia.\n\n\n\nHowever, the quantization process can also introduce quantization errors that can degrade the model’s performance. Quantization-aware scaling is a technique that minimizes these errors by adjusting the scale factors used in the quantization process.\nThe QAS process involves two main steps:\n\nQuantization-aware training: In this step, the neural network is trained with quantization in mind, simulating it to mimic its effects during forward and backward passes. This allows the model to learn to compensate for the quantization errors and improve its performance on low-precision hardware. Refer to the QAT section in Model Optimizations for details.\nQuantization and scaling: After training, the model is quantized to a low-precision format, and the scale factors are adjusted to minimize the quantization errors. The scale factors are chosen based on the distribution of the weights and activations in the model and are adjusted to ensure that the quantized values are within the range of the low-precision format.\n\nQAS is used to overcome the difficulties of optimizing models on tiny devices without needing hyperparameter tuning; QAS automatically scales tensor gradients with various bit precisions. This stabilizes the training process and matches the accuracy of floating-point precision.\n\n\nSparse Updates\nAlthough QAS enables the optimization of a quantized model, it uses a large amount of memory, which is unrealistic for on-device training. So, spare updates are used to reduce the memory footprint of full backward computation. Instead of pruning weights for inference, sparse update prunes the gradient during backward propagation to update the model sparsely. In other words, sparse update skips computing gradients of less important layers and sub-tensors.\nHowever, determining the optimal sparse update scheme given a constraining memory budget can be challenging due to the large search space. For example, the MCUNet model has 43 convolutional layers and a search space of approximately \\(10^{30}\\). One technique to address this issue is contribution analysis. Contribution analysis measures the accuracy improvement from biases (updating the last few biases compared to only updating the classifier) and weights (updating the weight of one extra layer compared to only having a bias update). By trying to maximize these improvements, contribution analysis automatically derives an optimal sparse update scheme for enabling on-device training.\n\n\nLayer-Wise Training\nOther methods besides quantization can help optimize routines. One such method is layer-wise training. A significant memory consumer of DNN training is end-to-end backpropagation, which requires all intermediate feature maps to be stored so the model can calculate gradients. An alternative to this approach that reduces the memory footprint of DNN training is sequential layer-by-layer training (T. Chen et al. 2016). Instead of training end-to-end, training a single layer at a time helps avoid having to store intermediate feature maps.\n\nChen, Tianqi, Bing Xu, Chiyuan Zhang, and Carlos Guestrin. 2016. “Training Deep Nets with Sublinear Memory Cost.” ArXiv Preprint abs/1604.06174 (April). http://arxiv.org/abs/1604.06174v2.\n\n\nTrading Computation for Memory\nThe strategy of trading computation for memory involves releasing some of the memory being used to store intermediate results. Instead, these results can be recomputed as needed. Reducing memory in exchange for more computation is shown to reduce the memory footprint of DNN training to fit into almost any budget while also minimizing computational cost (Gruslys et al. 2016).\n\nGruslys, Audrunas, Rémi Munos, Ivo Danihelka, Marc Lanctot, and Alex Graves. 2016. “Memory-Efficient Backpropagation Through Time.” In Advances in Neural Information Processing Systems 29: Annual Conference on Neural Information Processing Systems 2016, December 5-10, 2016, Barcelona, Spain, edited by Daniel D. Lee, Masashi Sugiyama, Ulrike von Luxburg, Isabelle Guyon, and Roman Garnett, 4125–33. https://proceedings.neurips.cc/paper/2016/hash/a501bebf79d570651ff601788ea9d16d-Abstract.html.\n\n\n\n14.3.3 Developing New Data Representations\nThe dimensionality and volume of the training data can significantly impact on-device adaptation. So, another technique for adapting models onto resource-constrained devices is to represent datasets more efficiently.\n\nData Compression\nThe goal of data compression is to reach high accuracies while limiting the amount of training data. One method to achieve this is prioritizing sample complexity: the amount of training data required for the algorithm to reach a target accuracy (Dhar et al. 2021).\n\nDhar, Sauptik, Junyao Guo, Jiayi (Jason) Liu, Samarth Tripathi, Unmesh Kurup, and Mohak Shah. 2021. “A Survey of on-Device Machine Learning: An Algorithms and Learning Theory Perspective.” ACM Transactions on Internet of Things 2 (3): 1–49. https://doi.org/10.1145/3450494.\n\nDarvish Rouhani, Bita, Azalia Mirhoseini, and Farinaz Koushanfar. 2017. “TinyDL: Just-in-Time Deep Learning Solution for Constrained Embedded Systems.” In 2017 IEEE International Symposium on Circuits and Systems (ISCAS), 1–4. IEEE. https://doi.org/10.1109/iscas.2017.8050343.\n\nLi, Xiang, Tao Qin, Jian Yang, and Tie-Yan Liu. 2016. “LightRNN: Memory and Computation-Efficient Recurrent Neural Networks.” In Advances in Neural Information Processing Systems 29: Annual Conference on Neural Information Processing Systems 2016, December 5-10, 2016, Barcelona, Spain, edited by Daniel D. Lee, Masashi Sugiyama, Ulrike von Luxburg, Isabelle Guyon, and Roman Garnett, 4385–93. https://proceedings.neurips.cc/paper/2016/hash/c3e4035af2a1cde9f21e1ae1951ac80b-Abstract.html.\nOther more common methods of data compression focus on reducing the dimensionality and the volume of the training data. For example, an approach could take advantage of matrix sparsity to reduce the memory footprint of storing training data. Training data can be transformed into a lower-dimensional embedding and factorized into a dictionary matrix multiplied by a block-sparse coefficient matrix (Darvish Rouhani, Mirhoseini, and Koushanfar 2017). Another example could involve representing words from a large language training dataset in a more compressed vector format (Li et al. 2016).",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>On-Device Learning</span>"
    ]
  },
  {
    "objectID": "contents/core/ondevice_learning/ondevice_learning.html#transfer-learning",
    "href": "contents/core/ondevice_learning/ondevice_learning.html#transfer-learning",
    "title": "14  On-Device Learning",
    "section": "14.4 Transfer Learning",
    "text": "14.4 Transfer Learning\nTransfer learning is a technique in which a model developed for a particular task is reused as the starting point for a model on a second task. Transfer learning allows us to leverage pre-trained models that have already learned useful representations from large datasets and fine-tune them for specific tasks using smaller datasets directly on the device. This can significantly reduce the computational resources and time required for training models from scratch.\nIt can be understood through intuitive real-world examples, as illustrated in Figure 14.3. The figure shows scenarios where skills from one domain can be applied to accelerate learning in a related field. A prime example is the relationship between riding a bicycle and a motorcycle. If you can ride a bicycle, you would have already mastered the skill of balancing on a two-wheeled vehicle. The foundational knowledge about this skill makes it significantly easier for you to learn how to ride a motorcycle compared to someone without any cycling experience. The figure depicts this and other similar scenarios, demonstrating how transfer learning leverages existing knowledge to expedite the acquisition of new, related skills.\n\n\n\n\n\n\nFigure 14.3: Transferring knowledge between tasks. Source: Zhuang et al. (2021).\n\n\nZhuang, Fuzhen, Zhiyuan Qi, Keyu Duan, Dongbo Xi, Yongchun Zhu, Hengshu Zhu, Hui Xiong, and Qing He. 2021. “A Comprehensive Survey on Transfer Learning.” Proceedings of the IEEE 109 (1): 43–76. https://doi.org/10.1109/jproc.2020.3004555.\n\n\nLet’s take the example of a smart sensor application that uses on-device AI to recognize objects in images captured by the device. Traditionally, this would require sending the image data to a server, where a large neural network model processes the data and sends back the results. With on-device AI, the model is stored and runs directly on-device, eliminating the need to send data to a server.\nIf we want to customize the model for the on-device characteristics, training a neural network model from scratch on the device would be impractical due to the limited computational resources and battery life. This is where transfer learning comes in. Instead of training a model from scratch, we can take a pre-trained model, such as a convolutional neural network (CNN) or a transformer network trained on a large dataset of images, and finetune it for our specific object recognition task. This finetuning can be done directly on the device using a smaller dataset of images relevant to the task. By leveraging the pre-trained model, we can reduce the computational resources and time required for training while still achieving high accuracy for the object recognition task. Figure 14.4 further illustrates the benefits of transfer learning over training from scratch.\n\n\n\n\n\n\nFigure 14.4: Training from scratch vs. transfer learning.\n\n\n\nTransfer learning is important in making on-device AI practical by allowing us to leverage pre-trained models and finetune them for specific tasks, thereby reducing the computational resources and time required for training. The combination of on-device AI and transfer learning opens up new possibilities for AI applications that are more privacy-conscious and responsive to user needs.\nTransfer learning has revolutionized the way models are developed and deployed, both in the cloud and at the edge. Transfer learning is being used in the real world. One such example is the use of transfer learning to develop AI models that can detect and diagnose diseases from medical images, such as X-rays, MRI scans, and CT scans. For example, researchers at Stanford University developed a transfer learning model that can detect cancer in skin images with an accuracy of 97% (Esteva et al. 2017). This model was pre-trained on 1.28 million images to classify a broad range of objects and then specialized for cancer detection by training on a dermatologist-curated dataset of skin images.\n\nEsteva, Andre, Brett Kuprel, Roberto A. Novoa, Justin Ko, Susan M. Swetter, Helen M. Blau, and Sebastian Thrun. 2017. “Dermatologist-Level Classification of Skin Cancer with Deep Neural Networks.” Nature 542 (7639): 115–18. https://doi.org/10.1038/nature21056.\nIn production settings, implementing transfer learning typically involves two key stages: pre-deployment and post-deployment. Pre-deployment focuses on preparing the model for its specialized task before release, while post-deployment enables the model to adapt further based on individual user data, enhancing personalization and accuracy over time.\n\n14.4.1 Pre-Deployment Specialization\nIn the pre-deployment stage, transfer learning acts as a catalyst to expedite the development process. Here’s how it typically works: Imagine we are creating a system to recognize different breeds of dogs. Rather than starting from scratch, we can use a pre-trained model that has already mastered the broader task of recognizing animals in images.\nThis pre-trained model serves as a solid foundation and contains a wealth of knowledge acquired from extensive data. We then finetune this model using a specialized dataset containing images of various dog breeds. This finetuning process tailors the model to our specific need — precisely identifying dog breeds. Once finetuned and validated to meet performance criteria, this specialized model is then ready for deployment.\nHere’s how it works in practice:\n\nStart with a Pre-Trained Model: Begin by selecting a model that has already been trained on a comprehensive dataset, usually related to a general task. This model serves as the foundation for the task at hand.\nFine-tuning: The pre-trained model is then finetuned on a smaller, more specialized dataset specific to the desired task. This step allows the model to adapt and specialize its knowledge to the specific requirements of the application.\nValidation: After finetuning, the model is validated to ensure it meets the performance criteria for the specialized task.\nDeployment: Once validated, the specialized model is then deployed into the production environment.\n\nThis method significantly reduces the time and computational resources required to train a model from scratch (Pan and Yang 2010). By adopting transfer learning, embedded systems can achieve high accuracy on specialized tasks without the need to gather extensive data or expend significant computational resources on training from the ground up.\n\nPan, Sinno Jialin, and Qiang Yang. 2010. “A Survey on Transfer Learning.” IEEE Transactions on Knowledge and Data Engineering 22 (10): 1345–59. https://doi.org/10.1109/tkde.2009.191.\n\n\n14.4.2 Post-Deployment Adaptation\nDeployment to a device need not mark the culmination of an ML model’s educational trajectory. With the advent of transfer learning, we open the doors to the deployment of adaptive ML models in real-world scenarios, catering to users’ personalized needs.\nConsider a real-world application where a parent wishes to identify their child in a collection of images from a school event on their smartphone. In this scenario, the parent is faced with the challenge of locating their child amidst images of many other children. Transfer learning can be employed here to finetune an embedded system’s model to this unique and specialized task. Initially, the system might use a generic model trained to recognize faces in images. However, with transfer learning, the system can adapt this model to recognize the specific features of the user’s child.\nHere’s how it works:\n\nData Collection: The embedded system gathers images that include the child, ideally with the parent’s input to ensure accuracy and relevance. This can be done directly on the device, maintaining the user’s data privacy.\nOn-Device Fine-tuning: The pre-existing face recognition model, which has been trained on a large and diverse dataset, is then finetuned using the newly collected images of the child. This process adapts the model to recognize the child’s specific facial features, distinguishing them from other children in the images.\nValidation: The refined model is then validated to ensure it accurately recognizes the child in various images. This can involve the parent verifying the model’s performance and providing feedback for further improvements.\nLocalized Use: Once adapted, the model can instantly locate the child in photos, providing a customized experience without needing cloud resources or data transfer.\n\nThis on-the-fly customization enhances the model’s efficacy for the individual user, ensuring that they benefit from ML personalization. This is, in part, how iPhotos or Google Photos works when they ask us to recognize a face, and then, based on that information, they index all the photos by that face. Because the learning and adaptation occur on the device itself, there are no risks to personal privacy. The parent’s images are not uploaded to a cloud server or shared with third parties, protecting the family’s privacy while still reaping the benefits of a personalized ML model. This approach represents a significant step forward in the quest to provide users with tailored ML solutions that respect and uphold their privacy.\n\n\n14.4.3 Benefits\nTransfer learning has become an important technique in ML and artificial intelligence, and it is particularly valuable for several reasons.\n\nData Scarcity: In many real-world applications, gathering a large, labeled dataset to train an ML model from scratch is difficult, costly, and time-consuming. Transfer learning addresses this challenge by allowing the use of pre-trained models that have already learned valuable features from vast labeled datasets, thereby reducing the need for extensive annotated data in the new task.\nComputational Expense: Training a model from scratch requires significant computational resources and time, especially for complex models like deep neural networks. By using transfer learning, we can leverage the computation that has already been done during the training of the source model, thereby saving both time and computational power.\n\nThere are advantages to reusing the features:\n\nHierarchical Feature Learning: Deep learning models, particularly CNNs, can learn hierarchical features. Lower layers typically learn generic features like edges and shapes, while higher layers learn more complex and task-specific features. Transfer learning allows us to reuse the generic features learned by a model and finetune the higher layers for our specific task.\nBoosting Performance: Transfer learning has been proven to boost the performance of models on tasks with limited data. The knowledge gained from the source task can provide a valuable starting point and lead to faster convergence and improved accuracy on the target task.\n\n\n\n\n\n\n\nExercise 14.1: Transfer Learning\n\n\n\n\n\nImagine training an AI to recognize flowers like a pro, but without needing a million flower pictures! That’s the power of transfer learning. In this Colab, we’ll take an AI that already knows about images and teach it to become a flower expert with less effort. Get ready to make your AI smarter, not harder!\n\n\n\n\n\n\n14.4.4 Core Concepts\nUnderstanding the core concepts of transfer learning is essential for effectively utilizing this powerful approach in ML. Here, we’ll break down some of the main principles and components that underlie the process of transfer learning.\n\nSource and Target Tasks\nIn transfer learning, there are two main tasks involved: the source task and the target task. The source task is the task for which the model has already been trained and has learned valuable information. The target task is the new task we want the model to perform. The goal of transfer learning is to leverage the knowledge gained from the source task to improve performance on the target task.\nSuppose we have a model trained to recognize various fruits in images (source task), and we want to create a new model to recognize different vegetables in images (target task). In that case, we can use transfer learning to leverage the knowledge gained during the fruit recognition task to improve the performance of the vegetable recognition model.\n\n\nRepresentation Transfer\nRepresentation transfer is about transferring the learned representations (features) from the source task to the target task. There are three main types of representation transfer:\n\nInstance Transfer: This involves reusing the data instances from the source task in the target task.\nFeature-Representation Transfer: This involves transferring the learned feature representations from the source task to the target task.\nParameter Transfer: This involves transferring the model’s learned parameters (weights) from the source task to the target task.\n\nIn natural language processing, a model trained to understand the syntax and grammar of a language (source task) can have its learned representations transferred to a new model designed to perform sentiment analysis (target task).\n\n\nFinetuning\nFinetuning is the process of adjusting the parameters of a pre-trained model to adapt it to the target task. This typically involves updating the weights of the model’s layers, especially the last few layers, to make the model more relevant for the new task. In image classification, a model pre-trained on a general dataset like ImageNet (source task) can be finetuned by adjusting the weights of its layers to perform well on a specific classification task, like recognizing specific animal species (target task).\n\n\nFeature Extractions\nFeature extraction involves using a pre-trained model as a fixed feature extractor, where the output of the model’s intermediate layers is used as features for the target task. This approach is particularly useful when the target task has a small dataset, as the pre-trained model’s learned features can significantly improve performance. In medical image analysis, a model pre-trained on a large dataset of general medical images (source task) can be used as a feature extractor to provide valuable features for a new model designed to recognize specific types of tumors in X-ray images (target task).\n\n\n\n14.4.5 Types of Transfer Learning\nTransfer learning can be classified into three main types based on the nature of the source and target tasks and data. Let’s explore each type in detail:\n\nInductive Transfer Learning\nIn inductive transfer learning, the goal is to learn the target predictive function with the help of source data. It typically involves finetuning a pre-trained model on the target task with available labeled data. A common example of inductive transfer learning is image classification tasks. For instance, a model pre-trained on the ImageNet dataset (source task) can be finetuned to classify specific types of birds (target task) using a smaller labeled dataset of bird images.\n\n\nTransductive Transfer Learning\nTransductive transfer learning involves using source and target data, but only the source task. The main aim is to transfer knowledge from the source domain to the target domain, even though the tasks remain the same. Sentiment analysis for different languages can serve as an example of transductive transfer learning. A model trained to perform sentiment analysis in English (source task) can be adapted to perform sentiment analysis in another language, like French (target task), by leveraging parallel datasets of English and French sentences with the same sentiments.\n\n\nUnsupervised Transfer Learning\nUnsupervised transfer learning is used when the source and target tasks are related, but there is no labeled data available for the target task. The goal is to leverage the knowledge gained from the source task to improve performance on the target task, even without labeled data. An example of unsupervised transfer learning is topic modeling in text data. A model trained to extract topics from news articles (source task) can be adapted to extract topics from social media posts (target task) without needing labeled data for the social media posts.\n\n\nComparison and Tradeoffs\nBy leveraging these different types of transfer learning, practitioners can choose the approach that best fits the nature of their tasks and available data, ultimately leading to more effective and efficient ML models. So, in summary:\n\nInductive: different source and target tasks, different domains\nTransductive: different source and target tasks, same domain\nUnsupervised: unlabeled source data, transfers feature representations\n\nTable 14.1 presents a matrix that outlines in a bit more detail the similarities and differences between the types of transfer learning:\n\n\n\nTable 14.1: Comparison of transfer learning types.\n\n\n\n\n\n\n\n\n\n\n\nAspect\nInductive Transfer Learning\nTransductive Transfer Learning\nUnsupervised Transfer Learning\n\n\n\n\nLabeled Data for Target Task\nRequired\nNot Required\nNot Required\n\n\nSource Task\nCan be different\nSame\nSame or Different\n\n\nTarget Task\nCan be different\nSame\nCan be different\n\n\nObjective\nImprove target task performance with source data\nTransfer knowledge from source to target domain\nLeverage source task to improve target task performance without labeled data\n\n\nExample\nImageNet to bird classification\nSentiment analysis in different languages\nTopic modeling for different text data\n\n\n\n\n\n\n\n\n\n14.4.6 Constraints and Considerations\nWhen engaging in transfer learning, there are several factors that must be considered to ensure successful knowledge transfer and model performance. Here’s a breakdown of some key factors:\n\nDomain Similarity\nDomain similarity refers to the degree of resemblance between the types of data used in the source and target applications. The more similar the domains, the more likely the transfer learning will be successful. For instance, transferring knowledge from a model trained on outdoor images (source domain) to a new application involving indoor images (target domain) is more feasible than transferring knowledge from outdoor images to a text-based application. Since images and text are fundamentally different types of data, the domains are dissimilar, making transfer learning more challenging.\n\n\nTask Similarity\nTask similarity, on the other hand, refers to how similar the objectives or functions of the source and target tasks are. If the tasks are similar, transfer learning is more likely to be effective. For instance, a model trained to classify different breeds of dogs (source task) can be more easily adapted to classify different breeds of cats (target task) than it could be adapted to a less related task, such as identifying satellite imagery. Since both tasks involve the visual classification of animals, task similarity supports effective transfer, while moving to an unrelated task could make transfer learning less effective.\n\n\nData Quality and Quantity\nThe quality and quantity of data available for the target task can significantly impact the success of transfer learning. More high-quality data can result in better model performance. Suppose we have a large dataset with clear, well-labeled images to recognize specific bird species. In that case, the transfer learning process will likely be more successful than if we have a small, noisy dataset.\n\n\nFeature Space Overlap\nFeature space overlap refers to how well the features learned by the source model align with the features needed for the target task. Greater overlap can lead to more successful transfer learning. A model trained on high-resolution images (source task) may not transfer well to a target task that involves low-resolution images, as the feature space (high-res vs. low-res) is different.\n\n\nModel Complexity\nThe complexity of the source model can also impact the success of transfer learning. Sometimes, a simpler model might transfer better than a complex one, as it is less likely to overfit the source task. For example, a simple CNN model trained on image data (source task) may transfer more successfully to a new image classification task (target task) than a complex CNN with many layers, as the simpler model is less likely to overfit the source task.\nBy considering these factors, ML practitioners can make informed decisions about when and how to use transfer learning, ultimately leading to more successful model performance on the target task. The success of transfer learning hinges on the degree of similarity between the source and target domains. Overfitting is risky, especially when finetuning occurs on a limited dataset. On the computational front, certain pre-trained models, owing to their size, might not comfortably fit into the memory constraints of some devices or may run prohibitively slowly. Over time, as data evolves, there is potential for model drift, indicating the need for periodic re-training or ongoing adaptation.\nLearn more about transfer learning in Video 14.1 below.\n\n\n\n\n\n\nImportant 14.1: Transfer Learning",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>On-Device Learning</span>"
    ]
  },
  {
    "objectID": "contents/core/ondevice_learning/ondevice_learning.html#sec-fl",
    "href": "contents/core/ondevice_learning/ondevice_learning.html#sec-fl",
    "title": "14  On-Device Learning",
    "section": "14.5 Federated Machine Learning",
    "text": "14.5 Federated Machine Learning\n\n14.5.1 Federated Learning Overview\nThe modern internet is full of large networks of connected devices. Whether it’s cell phones, thermostats, smart speakers, or other IoT products, countless edge devices are a goldmine for hyper-personalized, rich data. However, with that rich data comes an assortment of problems with information transfer and privacy. Constructing a training dataset in the cloud from these devices would involve high volumes of bandwidth, cost-efficient data transfer, and violation of users’ privacy.\nFederated learning offers a solution to these problems: train models partially on the edge devices and only communicate model updates to the cloud. In 2016, a team from Google designed architecture for federated learning that attempts to address these problems. In their initial paper, McMahan et al. (2017) outline a principle federated learning algorithm called FederatedAveraging, shown in Figure 14.5. Specifically, FederatedAveraging performs stochastic gradient descent (SGD) over several different edge devices. In this process, each device calculates a gradient \\(g_k = \\nabla F_k(w_t)\\) which is then applied to update the server-side weights as (with \\(\\eta\\) as learning rate across \\(k\\) clients):\n\nMcMahan, Brendan, Eider Moore, Daniel Ramage, Seth Hampson, and Blaise Agüera y Arcas. 2017. “Communication-Efficient Learning of Deep Networks from Decentralized Data.” In Artificial Intelligence and Statistics, 1273–82. PMLR. http://proceedings.mlr.press/v54/mcmahan17a.html.\n\\[\nw_{t+1} \\rightarrow w_t - \\eta \\sum_{k=1}^{K} \\frac{n_k}{n}g_k\n\\] This summarizes the basic algorithm for federated learning on the right. For each round of training, the server takes a random set of client devices and calls each client to train on its local batch using the most recent server-side weights. Those weights are then returned to the server, where they are collected individually and averaged to update the global model weights.\n\n\n\n\n\n\nFigure 14.5: Google’s Proposed FederatedAverage Algorithm. Source: McMahan et al. (2017).\n\n\n\nWith this proposed structure, there are a few key vectors for further optimizing federated learning. We will outline each in the following subsections.\nVideo 14.2 gives an overview of federated learning.\n\n\n\n\n\n\nImportant 14.2: Transfer Learning\n\n\n\n\n\n\nFigure 14.6 outlines the transformative impact of federated learning on on-device learning.\n\n\n\n\n\n\nFigure 14.6: Federated learning is revolutionizing on-device learning.\n\n\n\n\n\n14.5.2 Communication Efficiency\nOne of the key bottlenecks in federated learning is communication. Every time a client trains the model, they must communicate their updates back to the server. Similarly, once the server has averaged all the updates, it must send them back to the client. This incurs huge bandwidth and resource costs on large networks of millions of devices. As the field of federated learning advances, a few optimizations have been developed to minimize this communication. To address the footprint of the model, researchers have developed model compression techniques. In the client-server protocol, federated learning can also minimize communication through the selective sharing of updates on clients. Finally, efficient aggregation techniques can also streamline the communication process.\n\n\n14.5.3 Model Compression\nIn standard federated learning, the server communicates the entire model to each client, and then the client sends back all of the updated weights. This means that the easiest way to reduce the client’s memory and communication footprint is to minimize the size of the model needed to be communicated. We can employ all of the previously discussed model optimization strategies to do this.\nIn 2022, another team at Google proposed that each client communicates via a compressed format and decompresses the model on the fly for training (Yang et al. 2023), allocating and deallocating the full memory for the model only for a short period while training. The model is compressed through a range of various quantization strategies elaborated upon in their paper. Meanwhile, the server can update the uncompressed model by decompressing and applying updates as they come in.\n\nYang, Tien-Ju, Yonghui Xiao, Giovanni Motta, Françoise Beaufays, Rajiv Mathews, and Mingqing Chen. 2023. “Online Model Compression for Federated Learning with Large Models.” In ICASSP 2023 - 2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 1–5. IEEE; IEEE. https://doi.org/10.1109/icassp49357.2023.10097124.\n\n\n14.5.4 Selective Update Sharing\nThere are many methods for selectively sharing updates. The general principle is that reducing the portion of the model that the clients are training on the edge reduces the memory necessary for training and the size of communication to the server. In basic federated learning, the client trains the entire model. This means that when a client sends an update to the server, it has gradients for every weight in the network.\nHowever, we cannot just reduce communication by sending pieces of those gradients from each client to the server because the gradients are part of an entire update required to improve the model. Instead, you need to architecturally design the model such that each client trains only a small portion of the broader model, reducing the total communication while still gaining the benefit of training on client data. Shi and Radu (2022) apply this concept to a CNN by splitting the global model into two parts: an upper and a lower part, as shown in Z. Chen and Xu (2023).\n\nShi, Hongrui, and Valentin Radu. 2022. “Data Selection for Efficient Model Update in Federated Learning.” In Proceedings of the 2nd European Workshop on Machine Learning and Systems, 72–78. ACM. https://doi.org/10.1145/3517207.3526980.\n\n\n\n\n\n\nFigure 14.7: Federated learning with split model training. The model is divided into a lower part, trained locally on each client, and an upper part, refined on the server. Clients perform local updates, generating activation maps from their data, which are sent to the server instead of raw data to ensure privacy. The server uses these activation maps to update the upper part, then combines both parts and redistributes the updated model to clients. This setup minimizes communication, preserves privacy, and adapts the model to diverse client data. Source: Shi et al., (2022).\n\n\n\nThe lower part of the model, responsible for extracting generic features, is trained directly on each client device. Using federated averaging, this lower part learns shared foundational features across all clients, allowing it to generalize well across varied data. Meanwhile, the upper part of the model, which captures more specific and complex patterns, is trained on the server. Rather than sending raw data to the server, each client generates activation maps—a compressed representation of its local data’s most relevant features—and sends these to the server. The server uses these activation maps to refine the upper part of the model, allowing it to become more sensitive to the diverse data distributions found across clients without compromising user privacy.\nThis approach significantly reduces the communication load, as only summarized activation maps are transmitted instead of full datasets. By focusing on shared training for the lower part and specialized tuning for the upper part, the system achieves a balance: it minimizes data transfer, preserves privacy, and makes the model robust to varied input types encountered on client devices.\n\n\n14.5.5 Optimized Aggregation\nIn addition to reducing the communication overhead, optimizing the aggregation function can improve model training speed and accuracy in certain federated learning use cases. While the standard for aggregation is just averaging, various other approaches can improve model efficiency, accuracy, and security.\nOne alternative is clipped averaging, which clips the model updates within a specific range. Another strategy to preserve security is differential privacy average aggregation. This approach integrates differential privacy into the aggregation step to protect client identities. Each client adds a layer of random noise to their updates before communicating to the server. The server then updates itself with the noisy updates, meaning that the amount of noise needs to be tuned carefully to balance privacy and accuracy.\nIn addition to security-enhancing aggregation methods, there are several modifications to the aggregation methods that can improve training speed and performance by adding client metadata along with the weight updates. Momentum aggregation is a technique that helps address the convergence problem. In federated learning, client data can be extremely heterogeneous depending on the different environments in which the devices are used. That means that many models with heterogeneous data may need help to converge. Each client stores a momentum term locally, which tracks the pace of change over several updates. With clients communicating this momentum, the server can factor in the rate of change of each update when changing the global model to accelerate convergence. Similarly, weighted aggregation can factor in the client performance or other parameters like device type or network connection strength to adjust the weight with which the server should incorporate the model updates. Further descriptions of specific aggregation algorithms are provided by Moshawrab et al. (2023).\n\nMoshawrab, Mohammad, Mehdi Adda, Abdenour Bouzouane, Hussein Ibrahim, and Ali Raad. 2023. “Reviewing Federated Learning Aggregation Algorithms; Strategies, Contributions, Limitations and Future Perspectives.” Electronics 12 (10): 2287. https://doi.org/10.3390/electronics12102287.\n\n\n14.5.6 Handling non-IID Data\nWhen using federated learning to train a model across many client devices, it is convenient to consider the data to be independent and identically distributed (IID) across all clients. When data is IID, the model will converge faster and perform better because each local update on any given client is more representative of the broader dataset. This makes aggregation straightforward, as you can directly average all clients. However, this differs from how data often appears in the real world. Consider a few of the following ways in which data may be non-IID:\n\nIf you are learning on a set of health-monitor devices, different device models could mean different sensor qualities and properties. This means that low-quality sensors and devices may produce data, and therefore, model updates distinctly different than high-quality ones\nA smart keyboard trained to perform autocorrect. If you have a disproportionate amount of devices from a certain region, the slang, sentence structure, or even language they were using could skew more model updates towards a certain style of typing\nIf you have wildlife sensors in remote areas, connectivity may not be equally distributed, causing some clients in certain regions to be unable to send more model updates than others. If those regions have different wildlife activity from certain species, that could skew the updates toward those animals\n\nThere are a few approaches to addressing non-IID data in federated learning. One approach would be to change the aggregation algorithm. If you use a weighted aggregation algorithm, you can adjust based on different client properties like region, sensor properties, or connectivity (Zhao et al. 2018).\n\nZhao, Yue, Meng Li, Liangzhen Lai, Naveen Suda, Damon Civin, and Vikas Chandra. 2018. “Federated Learning with Non-IID Data.” ArXiv Preprint abs/1806.00582 (June). http://arxiv.org/abs/1806.00582v2.\n\n\n14.5.7 Client Selection\nConsidering all of the factors influencing the efficacy of federated learning, like IID data and communication, client selection is a key component to ensuring a system trains well. Selecting the wrong clients can skew the dataset, resulting in non-IID data. Similarly, choosing clients randomly with bad network connections can slow down communication. Therefore, several key characteristics must be considered when selecting the right subset of clients.\nWhen selecting clients, there are three main components to consider: data heterogeneity, resource allocation, and communication cost. We can select clients on the previously proposed metrics in the non-IID section to address data heterogeneity. In federated learning, all devices may have different amounts of computing, resulting in some being more inefficient at training than others. When selecting a subset of clients for training, one must consider a balance of data heterogeneity and available resources. In an ideal scenario, you can always select the subset of clients with the greatest resources. However, this may skew your dataset, so a balance must be struck. Communication differences add another layer; you want to avoid being bottlenecked by waiting for devices with poor connections to transmit all their updates. Therefore, you must also consider choosing a subset of diverse yet well-connected devices.\n\n\n14.5.8 Gboard Example\nA primary example of a deployed federated learning system is Google’s Keyboard, Gboard, for Android devices. In implementing federated learning for the keyboard, Google focused on employing differential privacy techniques to protect the user’s data and identity. Gboard leverages language models for several key features, such as Next Word Prediction (NWP), Smart Compose (SC), and On-The-Fly rescoring (OTF) (Xu et al. 2023), as shown in Figure 14.8.\n\nXu, Zheng, Yanxiang Zhang, Galen Andrew, Christopher A. Choquette-Choo, Peter Kairouz, H. Brendan McMahan, Jesse Rosenstock, and Yuanbo Zhang. 2023. “Federated Learning of Gboard Language Models with Differential Privacy.” ArXiv Preprint abs/2305.18465 (May). http://arxiv.org/abs/2305.18465v2.\nNWP will anticipate the next word the user tries to type based on the previous one. SC gives inline suggestions to speed up the typing based on each character. OTF will re-rank the proposed next words based on the active typing process. All three of these models need to run quickly on the edge, and federated learning can accelerate training on the users’ data. However, uploading every word a user typed to the cloud for training would be a massive privacy violation. Therefore, federated learning emphasizes differential privacy, which protects the user while enabling a better user experience.\n\n\n\n\n\n\nFigure 14.8: Google G Board Features. Source: Zheng et al., (2023).\n\n\n\nTo accomplish this goal, Google employed its algorithm DP-FTRL, which provides a formal guarantee that trained models will not memorize specific user data or identities. The algorithm system design is shown in Figure 14.9. DP-FTRL, combined with secure aggregation, encrypts model updates and provides an optimal balance of privacy and utility. Furthermore, adaptive clipping is applied in the aggregation process to limit the impact of individual users on the global model (step 3 in Figure 14.9). By combining all these techniques, Google can continuously refine its keyboard while preserving user privacy in a formally provable way.\n\n\n\n\n\n\nFigure 14.9: Differential Privacy in G Board. Source: Zheng et al., (2023).\n\n\n\n\n\n\n\n\n\nExercise 14.2: Federated Learning - Text Generation\n\n\n\n\n\nHave you ever used those smart keyboards to suggest the next word? With federated learning, we can make them even better without sacrificing privacy. In this Colab, we’ll teach an AI to predict words by training on text data spread across devices. Get ready to make your typing even smoother!\n\n\n\n\n\n\n\n\n\n\nExercise 14.3: Federated Learning - Image Classification\n\n\n\n\n\nWant to train an image-savvy AI without sending your photos to the cloud? Federated learning is the answer! In this Colab, we’ll train a model across multiple devices, each learning from its images. Privacy is protected, and teamwork makes the AI dream work!\n\n\n\n\n\n\n14.5.9 Benchmarking Federated Learning: MedPerf\nMedical devices represent one of the richest examples of data on the edge. These devices store some of the most personal user data while simultaneously offering significant advances in personalized treatment and improved accuracy in medical AI. This combination of sensitive data and potential for innovation makes medical devices an ideal use case for federated learning.\nA key development in this field is MedPerf, an open-source platform designed for benchmarking models using federated evaluation (Karargyris et al. 2023). MedPerf goes beyond traditional federated learning by bringing the model to edge devices for testing against personalized data while maintaining privacy. This approach allows a benchmark committee to evaluate various models in real-world scenarios on edge devices without compromising patient anonymity.\n\nKarargyris, Alexandros, Renato Umeton, Micah J. Sheller, Alejandro Aristizabal, Johnu George, Anna Wuest, Sarthak Pati, et al. 2023. “Federated Benchmarking of Medical Artificial Intelligence with MedPerf.” Nature Machine Intelligence 5 (7): 799–810. https://doi.org/10.1038/s42256-023-00652-2.\nThe MedPerf platform, detailed in a recent study (https://doi.org/10.1038/s42256-023-00652-2), demonstrates how federated techniques can be applied not just to model training, but also to model evaluation and benchmarking. This advancement is particularly crucial in the medical field, where the balance between leveraging large datasets for improved AI performance and protecting individual privacy is of utmost importance.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>On-Device Learning</span>"
    ]
  },
  {
    "objectID": "contents/core/ondevice_learning/ondevice_learning.html#security-concerns",
    "href": "contents/core/ondevice_learning/ondevice_learning.html#security-concerns",
    "title": "14  On-Device Learning",
    "section": "14.6 Security Concerns",
    "text": "14.6 Security Concerns\nPerforming ML model training and adaptation on end-user devices also introduces security risks that must be addressed. Some key security concerns include:\n\nExposure of private data: Training data may be leaked or stolen from devices\nData poisoning: Adversaries can manipulate training data to degrade model performance\nModel extraction: Attackers may attempt to steal trained model parameters\nMembership inference: Models may reveal the participation of specific users’ data\nEvasion attacks: Specially crafted inputs can cause misclassification\n\nAny system that performs learning on-device introduces security concerns, as it may expose vulnerabilities in larger-scale models. Numerous security risks are associated with any ML model, but these risks have specific consequences for on-device learning. Fortunately, there are methods to mitigate these risks and improve the real-world performance of on-device learning.\n\n14.6.1 Data Poisoning\nOn-device ML introduces unique data security challenges compared to traditional cloud-based training. In particular, data poisoning attacks pose a serious threat during on-device learning. Adversaries can manipulate training data to degrade model performance when deployed.\nSeveral data poisoning attack techniques exist:\n\nLabel Flipping: It involves applying incorrect labels to samples. For instance, in image classification, cat photos may be labeled as dogs to confuse the model. Flipping even 10% of labels can have significant consequences on the model.\nData Insertion: It introduces fake or distorted inputs into the training set. This could include pixelated images, noisy audio, or garbled text.\nLogic Corruption: This alters the underlying patterns in data to mislead the model. In sentiment analysis, highly negative reviews may be marked positive through this technique. For this reason, recent surveys have shown that many companies are more afraid of data poisoning than other adversarial ML concerns.\n\nWhat makes data poisoning alarming is how it exploits the discrepancy between curated datasets and live training data. Consider a cat photo dataset collected from the internet. In the weeks later, when this data trains a model on-device, new cat photos on the web differ significantly.\nWith data poisoning, attackers purchase domains and upload content that influences a portion of the training data. Even small data changes significantly impact the model’s learned behavior. Consequently, poisoning can instill racist, sexist, or other harmful biases if unchecked.\nMicrosoft Tay was a chatbot launched by Microsoft in 2016. It was designed to learn from its interactions with users on social media platforms like Twitter. Unfortunately, Microsoft Tay became a prime example of data poisoning in ML models. Within 24 hours of its launch, Microsoft had to take Tay offline because it had started producing offensive and inappropriate messages, including hate speech and racist comments. This occurred because some users on social media intentionally fed Tay with harmful and offensive input, which the chatbot then learned from and incorporated into its responses.\nThis incident is a clear example of data poisoning because malicious actors intentionally manipulated the data used to train the chatbot and shape its responses. The data poisoning resulted in the chatbot adopting harmful biases and producing output that its developers did not intend. It demonstrates how even small amounts of maliciously crafted data can significantly impact the behavior of ML models and highlights the importance of implementing robust data filtering and validation mechanisms to prevent such incidents from occurring.\nSuch biases could have dangerous real-world impacts. Rigorous data validation, anomaly detection, and tracking of data provenance are critical defensive measures. Adopting frameworks like Five Safes ensures models are trained on high-quality, representative data (Desai et al. 2016).\n\nDesai, Tanvi, Felix Ritchie, Richard Welpton, et al. 2016. “Five Safes: Designing Data Access for Research.” Economics Working Paper Series 1601: 28.\nData poisoning is a pressing concern for secure on-device learning since data at the endpoint cannot be easily monitored in real-time. If models are allowed to adapt on their own, then we run the risk of the device acting maliciously. However, continued research in adversarial ML is needed to develop robust solutions to detect and mitigate such data attacks.\n\n\n14.6.2 Adversarial Attacks\nDuring the training phase, attackers might inject malicious data into the training dataset, which can subtly alter the model’s behavior. For example, an attacker could add images of cats labeled as dogs to a dataset used to train an image classification model. If done cleverly, the model’s accuracy might not significantly drop, and the attack could be noticed. The model would then incorrectly classify some cats as dogs, which could have consequences depending on the application.\nIn an embedded security camera system, for instance, this could allow an intruder to avoid detection by wearing a specific pattern that the model has been tricked into classifying as non-threatening.\nDuring the inference phase, attackers can use adversarial examples to fool the model. Adversarial examples are inputs that have been slightly altered in a way that causes the model to make incorrect predictions. For instance, an attacker might add a small amount of noise to an image in a way that causes a face recognition system to misidentify a person. These attacks can be particularly concerning in applications where safety is at stake, such as autonomous vehicles. A real-world example of this is when researchers were able to cause a traffic sign recognition system to misclassify a stop sign as a speed limit sign. This type of misclassification could lead to accidents if it occurred in a real-world autonomous driving system.\nTo mitigate these risks, several defenses can be employed:\n\nData Validation and Sanitization: Before incorporating new data into the training dataset, it should be thoroughly validated and sanitized to ensure it is not malicious.\nAdversarial Training: The model can be trained on adversarial examples to make it more robust to these types of attacks.\nInput Validation: During inference, inputs should be validated to ensure they have not been manipulated to create adversarial examples.\nRegular Auditing and Monitoring: Regularly auditing and monitoring the model’s behavior can help detect and mitigate adversarial attacks. However, this is easier said than done in the context of tiny ML systems. It is often hard to monitor embedded ML systems at the endpoint due to communication bandwidth limitations, which we will discuss in the MLOps chapter.\n\nBy understanding the potential risks and implementing these defenses, we can help secure on-device training at the endpoint/edge and mitigate the impact of adversarial attacks. Most people easily confuse data poisoning and adversarial attacks. So Table 14.2 compares data poisoning and adversarial attacks:\n\n\n\nTable 14.2: Comparison of data poisoning and adversarial attacks.\n\n\n\n\n\n\n\n\n\n\nAspect\nData Poisoning\nAdversarial Attacks\n\n\n\n\nTiming\nTraining phase\nInference phase\n\n\nTarget\nTraining data\nInput data\n\n\nGoal\nNegatively affect model’s performance\nCause incorrect predictions\n\n\nMethod\nInsert malicious examples into training data, often with incorrect labels\nAdd carefully crafted noise to input data\n\n\nExample\nAdding images of cats labeled as dogs to a dataset used for training an image classification model\nAdding a small amount of noise to an image in a way that causes a face recognition system to misidentify a person\n\n\nPotential Effects\nModel learns incorrect patterns and makes incorrect predictions\nImmediate and potentially dangerous incorrect predictions\n\n\nApplications Affected\nAny ML model\nAutonomous vehicles, security systems, etc.\n\n\n\n\n\n\n\n\n14.6.3 Model Inversion\nModel inversion attacks are a privacy threat to on-device machine learning models trained on sensitive user data (Nguyen et al. 2023). Understanding this attack vector and mitigation strategies will be important for building secure and ethical on-device AI. For example, imagine an iPhone app that uses on-device learning to categorize photos in your camera roll into groups like “beach,” “food,” or “selfies” for easier searching.\n\nNguyen, Ngoc-Bao, Keshigeyan Chandrasegaran, Milad Abdollahzadeh, and Ngai-Man Cheung. 2023. “Re-Thinking Model Inversion Attacks Against Deep Neural Networks.” In 2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 16384–93. IEEE. https://doi.org/10.1109/cvpr52729.2023.01572.\nThe on-device model may be trained by Apple on a dataset of iCloud photos from consenting users. A malicious attacker could attempt to extract parts of those original iCloud training photos using model inversion. Specifically, the attacker feeds crafted synthetic inputs into the on-device photo classifier. By tweaking the synthetic inputs and observing how the model categorizes them, they can refine the inputs until they reconstruct copies of the original training data - like a beach photo from a user’s iCloud. Now, the attacker has breached that user’s privacy by obtaining one of their photos without consent. This demonstrates why model inversion is dangerous - it can potentially leak highly sensitive training data.\nPhotos are an especially high-risk data type because they often contain identifiable people, location information, and private moments. However, the same attack methodology could apply to other personal data, such as audio recordings, text messages, or users’ health data.\nTo defend against model inversion, one would need to take precautions like adding noise to the model outputs or using privacy-preserving machine learning techniques like federated learning to train the on-device model. The goal is to prevent attackers from being able to reconstruct the original training data.\n\n\n14.6.4 On-Device Learning Security Concerns\nWhile data poisoning and adversarial attacks are common concerns for ML models in general, on-device learning introduces unique security risks. When on-device variants of large-scale models are published, adversaries can exploit these smaller models to attack their larger counterparts. Research has demonstrated that as on-device models and full-scale models become more similar, the vulnerability of the original large-scale models increases significantly. For instance, evaluations across 19 Deep Neural Networks (DNNs) revealed that exploiting on-device models could increase the vulnerability of the original large-scale models by up to 100 times.\nThere are three primary types of security risks specific to on-device learning:\n\nTransfer-Based Attacks: These attacks exploit the transferability property between a surrogate model (an approximation of the target model, similar to an on-device model) and a remote target model (the original full-scale model). Attackers generate adversarial examples using the surrogate model, which can then be used to deceive the target model. For example, imagine an on-device model designed to identify spam emails. An attacker could use this model to generate a spam email that is not detected by the larger, full-scale filtering system.\nOptimization-Based Attacks: These attacks generate adversarial examples for transfer-based attacks using some form of the objective function and iteratively modify inputs to achieve the desired outcome. Gradient estimation attacks, for example, approximate the model’s gradient using query outputs (such as softmax confidence scores), while gradient-free attacks use the model’s final decision (the predicted class) to approximate the gradient, albeit requiring many more queries.\nQuery Attacks with Transfer Priors: These attacks combine elements of transfer-based and optimization-based attacks. They reverse engineer on-device models to serve as surrogates for the target full-scale model. In other words, attackers use the smaller on-device model to understand how the larger model works and then use this knowledge to attack the full-scale model.\n\nBy understanding these specific risks associated with on-device learning, we can develop more robust security protocols to protect both on-device and full-scale models from potential attacks.\n\n\n14.6.5 Mitigation of On-Device Learning Risks\nVarious methods can be employed to mitigate the numerous security risks associated with on-device learning. These methods may be specific to the type of attack or serve as a general tool to bolster security.\nOne strategy to reduce security risks is to diminish the similarity between on-device models and full-scale models, thereby reducing transferability by up to 90%. This method, known as similarity-unpairing, addresses the problem that arises when adversaries exploit the input-gradient similarity between the two models. By finetuning the full-scale model to create a new version with similar accuracy but different input gradients, we can construct the on-device model by quantizing this updated full-scale model. This unpairing reduces the vulnerability of on-device models by limiting the exposure of the original full-scale model. Importantly, the order of finetuning and quantization can be varied while still achieving risk mitigation (Hong, Carlini, and Kurakin 2023).\nTo tackle data poisoning, it is imperative to source datasets from trusted and reliable vendors.\nSeveral strategies can be employed to combat adversarial attacks. A proactive approach involves generating adversarial examples and incorporating them into the model’s training dataset, thereby fortifying the model against such attacks. Tools like CleverHans, an open-source training library, are instrumental in creating adversarial examples. Defense distillation is another effective strategy, wherein the on-device model outputs probabilities of different classifications rather than definitive decisions (Hong, Carlini, and Kurakin 2023), making it more challenging for adversarial examples to exploit the model.\n\nHong, Sanghyun, Nicholas Carlini, and Alexey Kurakin. 2023. “Publishing Efficient on-Device Models Increases Adversarial Vulnerability.” In 2023 IEEE Conference on Secure and Trustworthy Machine Learning (SaTML), abs 1603 5279:271–90. IEEE; IEEE. https://doi.org/10.1109/satml54575.2023.00026.\nThe theft of intellectual property is another significant concern when deploying on-device models. Intellectual property theft is a concern when deploying on-device models, as adversaries may attempt to reverse-engineer the model to steal the underlying technology. To safeguard against intellectual property theft, the binary executable of the trained model should be stored on a microcontroller unit with encrypted software and secured physical interfaces of the chip. Furthermore, the final dataset used for training the model should be kept private.\nFurthermore, on-device models often use well-known or open-source datasets, such as MobileNet’s Visual Wake Words. As such, it is important to maintain the privacy of the final dataset used for training the model. Additionally, protecting the data augmentation process and incorporating specific use cases can minimize the risk of reverse-engineering an on-device model.\nLastly, the Adversarial Threat Landscape for Artificial Intelligence Systems (ATLAS) serves as a valuable matrix tool that helps assess the risk profile of on-device models, empowering developers to identify and mitigate potential risks proactively.\n\n\n14.6.6 Securing Training Data\nThere are various ways to secure on-device training data. Each concept is really deep and could be worth a class by itself. So here, we’ll briefly allude to those concepts so you’re aware of what to learn further.\n\nEncryption\nEncryption serves as the first line of defense for training data. This involves implementing end-to-end encryption for local storage on devices and communication channels to prevent unauthorized access to raw training data. Trusted execution environments, such as Intel SGX and ARM TrustZone, are essential for facilitating secure training on encrypted data.\nAdditionally, when aggregating updates from multiple devices, secure multi-party computation protocols can be employed to improve security (Kairouz, Oh, and Viswanath 2015); a practical application of this is in collaborative on-device learning, where cryptographic privacy-preserving aggregation of user model updates can be implemented. This technique effectively hides individual user data even during the aggregation phase.\n\nKairouz, Peter, Sewoong Oh, and Pramod Viswanath. 2015. “Secure Multi-Party Differential Privacy.” In Advances in Neural Information Processing Systems 28: Annual Conference on Neural Information Processing Systems 2015, December 7-12, 2015, Montreal, Quebec, Canada, edited by Corinna Cortes, Neil D. Lawrence, Daniel D. Lee, Masashi Sugiyama, and Roman Garnett, 2008–16. https://proceedings.neurips.cc/paper/2015/hash/a01610228fe998f515a72dd730294d87-Abstract.html.\n\n\nDifferential Privacy\nDifferential privacy is another crucial strategy for protecting training data. By injecting calibrated statistical noise into the data, we can mask individual records while still extracting valuable population patterns (Dwork and Roth 2013). Managing the privacy budget across multiple training iterations and reducing noise as the model converges is also vital (Abadi et al. 2016). Methods such as formally provable differential privacy, which may include adding Laplace or Gaussian noise scaled to the dataset’s sensitivity, can be employed.\n\nDwork, Cynthia, and Aaron Roth. 2013. “The Algorithmic Foundations of Differential Privacy.” Foundations and Trends® in Theoretical Computer Science 9 (3-4): 211–407. https://doi.org/10.1561/0400000042.\n\nAbadi, Martin, Andy Chu, Ian Goodfellow, H. Brendan McMahan, Ilya Mironov, Kunal Talwar, and Li Zhang. 2016. “Deep Learning with Differential Privacy.” In Proceedings of the 2016 ACM SIGSAC Conference on Computer and Communications Security, 308–18. CCS ’16. New York, NY, USA: ACM. https://doi.org/10.1145/2976749.2978318.\n\n\nAnomaly Detection\nAnomaly detection plays an important role in identifying and mitigating potential data poisoning attacks. This can be achieved through statistical analyses like Principal Component Analysis (PCA) and clustering, which help to detect deviations in aggregated training data. Time-series methods such as Cumulative Sum (CUSUM) charts are useful for identifying shifts indicative of potential poisoning. Comparing current data distributions with previously seen clean data distributions can also help to flag anomalies. Moreover, suspected poisoned batches should be removed from the training update aggregation process. For example, spot checks on subsets of training images on devices can be conducted using photoDNA hashes to identify poisoned inputs.\n\n\nInput Data Validation\nLastly, input data validation is essential for ensuring the integrity and validity of input data before it is fed into the training model, thereby protecting against adversarial payloads. Similarity measures, such as cosine distance, can be employed to catch inputs that deviate significantly from the expected distribution. Suspicious inputs that may contain adversarial payloads should be quarantined and sanitized. Furthermore, parser access to training data should be restricted to validated code paths only. Leveraging hardware security features, such as ARM Pointer Authentication, can prevent memory corruption (ARM Limited, 2023). An example of this is implementing input integrity checks on audio training data used by smart speakers before processing by the speech recognition model (Z. Chen and Xu 2023).\n\nChen, Zhiyong, and Shugong Xu. 2023. “Learning Domain-Heterogeneous Speaker Recognition Systems with Personalized Continual Federated Learning.” EURASIP Journal on Audio, Speech, and Music Processing 2023 (1): 33. https://doi.org/10.1186/s13636-023-00299-2.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>On-Device Learning</span>"
    ]
  },
  {
    "objectID": "contents/core/ondevice_learning/ondevice_learning.html#on-device-training-frameworks",
    "href": "contents/core/ondevice_learning/ondevice_learning.html#on-device-training-frameworks",
    "title": "14  On-Device Learning",
    "section": "14.7 On-Device Training Frameworks",
    "text": "14.7 On-Device Training Frameworks\nEmbedded inference frameworks like TF-Lite Micro (David et al. 2021), TVM (T. Chen et al. 2018), and MCUNet (Lin et al. 2020) provide a slim runtime for running neural network models on microcontrollers and other resource-constrained devices. However, they don’t support on-device training. Training requires its own set of specialized tools due to the impact of quantization on gradient calculation and the memory footprint of backpropagation (Lin et al. 2022).\n\nDavid, Robert, Jared Duke, Advait Jain, Vijay Janapa Reddi, Nat Jeffries, Jian Li, Nick Kreeger, et al. 2021. “Tensorflow Lite Micro: Embedded Machine Learning for Tinyml Systems.” Proceedings of Machine Learning and Systems 3: 800–811.\n\nChen, Tianqi, Thierry Moreau, Ziheng Jiang, Lianmin Zheng, Eddie Yan, Haichen Shen, Meghan Cowan, et al. 2018. “TVM: An Automated End-to-End Optimizing Compiler for Deep Learning.” In 13th USENIX Symposium on Operating Systems Design and Implementation (OSDI 18), 578–94.\n\nLin, Ji, Wei-Ming Chen, Yujun Lin, John Cohn, Chuang Gan, and Song Han. 2020. “MCUNet: Tiny Deep Learning on IoT Devices.” In Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, Virtual, edited by Hugo Larochelle, Marc’Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin. https://proceedings.neurips.cc/paper/2020/hash/86c51678350f656dcc7f490a43946ee5-Abstract.html.\n\nLin, Ji, Ligeng Zhu, Wei-Ming Chen, Wei-Chen Wang, Chuang Gan, and Song Han. 2022. “On-Device Training Under 256kb Memory.” Adv. Neur. In. 35: 22941–54.\nIn recent years, a handful of tools and frameworks have started to emerge that enable on-device training. These include Tiny Training Engine (Lin et al. 2022), TinyTL (Cai et al. 2020), and TinyTrain (Kwon et al. 2023).\n\n14.7.1 Tiny Training Engine\nTiny Training Engine (TTE) uses several techniques to optimize memory usage and speed up the training process. An overview of the TTE workflow is shown in Figure 14.10. First, TTE offloads the automatic differentiation to compile time instead of runtime, significantly reducing overhead during training. Second, TTE performs graph optimization like pruning and sparse updates to reduce memory requirements and accelerate computations.\n\n\n\n\n\n\nFigure 14.10: TTE workflow.\n\n\n\nSpecifically, TTE follows four main steps:\n\nDuring compile time, TTE traces the forward propagation graph and derives the corresponding backward graph for backpropagation. This allows differentiation to happen at compile time rather than runtime.\nTTE prunes any nodes representing frozen weights from the backward graph. Frozen weights are weights that are not updated during training to reduce certain neurons’ impact. Pruning their nodes saves memory.\nTTE reorders the gradient descent operators to interleave them with the backward pass computations. This scheduling minimizes memory footprints.\nTTE uses code generation to compile the optimized forward and backward graphs, which are then deployed for on-device training.\n\n\n\n14.7.2 Tiny Transfer Learning\nTiny Transfer Learning (TinyTL) enables memory-efficient on-device training through a technique called weight freezing. During training, much of the memory bottleneck comes from storing intermediate activations and updating the weights in the neural network.\nTo reduce this memory overhead, TinyTL freezes the majority of the weights so they do not need to be updated during training. This eliminates the need to store intermediate activations for frozen parts of the network. TinyTL only finetunes the bias terms, which are much smaller than the weights. An overview of TinyTL workflow is shown in Figure 14.11.\n\n\n\n\n\n\nFigure 14.11: TinyTL workflow. In (a), conventional transfer learning fine-tunes both weights and biases, requiring large memory (shown in blue) for activation maps during back-propagation. In (b), TinyTL reduces memory needs by fixing weights and fine-tuning only the biases, enabling transfer learning on smaller devices. Finally, in (c), TinyTL adds a “lite” residual learning component to compensate for fixed weights, using efficient group convolutions and avoiding memory-heavy bottlenecks, achieving high efficiency with minimal memory. Source: Cai et al. (2020).)\n\n\nCai, Han, Chuang Gan, Ligeng Zhu, and Song Han 0003. 2020. “TinyTL: Reduce Memory, Not Parameters for Efficient on-Device Learning.” In Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, Virtual, edited by Hugo Larochelle, Marc’Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin. https://proceedings.neurips.cc/paper/2020/hash/81f7acabd411274fcf65ce2070ed568a-Abstract.html.\n\n\nFreezing weights apply to fully connected layers as well as convolutional and normalization layers. However, only adapting the biases limits the model’s ability to learn and adapt to new data.\nTo increase adaptability without much additional memory, TinyTL uses a small residual learning model. This refines the intermediate feature maps to produce better outputs, even with fixed weights. The residual model introduces minimal overhead - less than 3.8% on top of the base model.\nBy freezing most weights, TinyTL significantly reduces memory usage during on-device training. The residual model then allows it to adapt and learn effectively for the task. The combined approach provides memory-efficient on-device training with minimal impact on model accuracy.\n\n\n14.7.3 Tiny Train\nTinyTrain significantly reduces the time required for on-device training by selectively updating only certain parts of the model. It does this using a technique called task-adaptive sparse updating, as shown in Figure 14.12.\n\n\n\n\n\n\nFigure 14.12: TinyTrain workflow. Source: Kwon et al. (2023).\n\n\nKwon, Young D., Rui Li, Stylianos I. Venieris, Jagmohan Chauhan, Nicholas D. Lane, and Cecilia Mascolo. 2023. “TinyTrain: Resource-Aware Task-Adaptive Sparse Training of DNNs at the Data-Scarce Edge.” ArXiv Preprint abs/2307.09988 (July). http://arxiv.org/abs/2307.09988v2.\n\n\nBased on the user data, memory, and computing available on the device, TinyTrain dynamically chooses which neural network layers to update during training. This layer selection is optimized to reduce computation and memory usage while maintaining high accuracy.\nMore specifically, TinyTrain first does offline pretraining of the model. During pretraining, it not only trains the model on the task data but also meta-trains the model. Meta-training means training the model on metadata about the training process itself. This meta-learning improves the model’s ability to adapt accurately even when limited data is available for the target task.\nThen, during the online adaptation stage, when the model is being customized on the device, TinyTrain performs task-adaptive sparse updates. Using the criteria around the device’s capabilities, it selects only certain layers to update through backpropagation. The layers are chosen to balance accuracy, memory usage, and computation time.\nBy sparsely updating layers tailored to the device and task, TinyTrain significantly reduces on-device training time and resource usage. The offline meta-training also improves accuracy when adapting to limited data. Together, these methods enable fast, efficient, and accurate on-device training.\n\n\n14.7.4 Comparison\nTable 14.3 summarizes the key similarities and differences between the different frameworks.\n\n\n\nTable 14.3: Comparison of frameworks for on-device training optimization.\n\n\n\n\n\n\n\n\n\n\nFramework\nSimilarities\nDifferences\n\n\n\n\nTiny Training Engine\n\nOn-device training\nOptimize memory & computation\nLeverage pruning, sparsity, etc.\n\n\nTraces forward & backward graphs\nPrunes frozen weights\nInterleaves backprop & gradients\nCode generation\n\n\n\nTinyTL\n\nOn-device training\nOptimize memory & computation\nLeverage freezing, sparsity, etc.\n\n\nFreezes most weights\nOnly adapts biases\nUses residual model\n\n\n\nTinyTrain\n\nOn-device training\nOptimize memory & computation\nLeverage sparsity, etc.\n\n\nMeta-training in pretraining\nTask-adaptive sparse updating\nSelective layer updating",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>On-Device Learning</span>"
    ]
  },
  {
    "objectID": "contents/core/ondevice_learning/ondevice_learning.html#conclusion",
    "href": "contents/core/ondevice_learning/ondevice_learning.html#conclusion",
    "title": "14  On-Device Learning",
    "section": "14.8 Conclusion",
    "text": "14.8 Conclusion\nThe concept of on-device learning is increasingly important for increasing the usability and scalability of TinyML. This chapter explored the intricacies of on-device learning, exploring its advantages and limitations, adaptation strategies, key related algorithms and techniques, security implications, and existing and emerging on-device training frameworks.\nOn-device learning is, undoubtedly, a groundbreaking paradigm that brings forth numerous advantages for embedded and edge ML deployments. By performing training directly on the endpoint devices, on-device learning obviates the need for continuous cloud connectivity, making it particularly well-suited for IoT and edge computing applications. It comes with benefits such as improved privacy, ease of compliance, and resource efficiency. At the same time, on-device learning faces limitations related to hardware constraints, limited data size, and reduced model accuracy and generalization.\nMechanisms such as reduced model complexity, optimization and data compression techniques, and related learning methods such as transfer learning and federated learning allow models to adapt to learn and evolve under resource constraints, thus serving as the bedrock for effective ML on edge devices.\nThe critical security concerns in on-device learning highlighted in this chapter, ranging from data poisoning and adversarial attacks to specific risks introduced by on-device learning, must be addressed in real workloads for on-device learning to be a viable paradigm. Effective mitigation strategies, such as data validation, encryption, differential privacy, anomaly detection, and input data validation, are crucial to safeguard on-device learning systems from these threats.\nThe emergence of specialized on-device training frameworks such as Tiny Training Engine, Tiny Transfer Learning, and Tiny Train presents practical tools that enable efficient on-device training. These frameworks employ various techniques to optimize memory usage, reduce computational overhead, and streamline the on-device training process.\nIn conclusion, on-device learning stands at the forefront of TinyML, promising a future where models can autonomously acquire knowledge and adapt to changing environments on edge devices. The application of on-device learning has the potential to revolutionize various domains, including healthcare, industrial IoT, and smart cities. However, the transformative potential of on-device learning must be balanced with robust security measures to protect against data breaches and adversarial threats. Embracing innovative on-device training frameworks and implementing stringent security protocols are key steps in unlocking the full potential of on-device learning. As this technology continues to evolve, it holds the promise of making our devices smarter, more responsive, and better integrated into our daily lives.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>On-Device Learning</span>"
    ]
  },
  {
    "objectID": "contents/core/ondevice_learning/ondevice_learning.html#sec-on-device-learning-resource",
    "href": "contents/core/ondevice_learning/ondevice_learning.html#sec-on-device-learning-resource",
    "title": "14  On-Device Learning",
    "section": "14.9 Resources",
    "text": "14.9 Resources\nHere is a curated list of resources to support students and instructors in their learning and teaching journeys. We are continuously working on expanding this collection and will add new exercises soon.\n\n\n\n\n\n\nSlides\n\n\n\n\n\nThese slides serve as a valuable tool for instructors to deliver lectures and for students to review the material at their own pace. We encourage both students and instructors to leverage these slides to improve their understanding and facilitate effective knowledge transfer.\n\nIntro to TensorFlow Lite (TFLite).\nTFLite Optimization and Quantization.\nTFLite Quantization-Aware Training.\nTransfer Learning:\n\nTransfer Learning: with Visual Wake Words example.\nOn-device Training and Transfer Learning.\n\nDistributed Training:\n\nDistributed Training.\nDistributed Training.\n\nContinuous Monitoring:\n\nContinuous Evaluation Challenges for TinyML.\nFederated Learning Challenges.\nContinuous Monitoring with Federated ML.\nContinuous Monitoring Impact on MLOps.\n\n\n\n\n\n\n\n\n\n\n\nVideos\n\n\n\n\n\n\nVideo 14.1\nVideo 14.2\n\n\n\n\n\n\n\n\n\n\nExercises\n\n\n\n\n\nTo reinforce the concepts covered in this chapter, we have curated a set of exercises that challenge students to apply their knowledge and deepen their understanding.\n\nExercise 14.1\nExercise 14.2\nExercise 14.3",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>On-Device Learning</span>"
    ]
  },
  {
    "objectID": "contents/core/privacy_security/privacy_security.html",
    "href": "contents/core/privacy_security/privacy_security.html",
    "title": "15  Security & Privacy",
    "section": "",
    "text": "Purpose\nResources: Slides, Videos, Exercises\nWhat principles guide the protection of machine learning systems, and how do security and privacy requirements transform system architecture?\nThe integration of protection mechanisms into AI systems represents a fundamental dimension of modern system design. Security considerations reveal essential patterns for safeguarding data, models, and infrastructure while maintaining operational effectiveness. The implementation of defensive strategies brings to light the trade-offs between protection, performance, and usability that shape architectural decisions across the AI lifecycle. Understanding these security dynamics provides insights into creating trustworthy systems, establishing core principles for designing solutions that preserve privacy and resist adversarial threats while meeting functional requirements in production environments.",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Security & Privacy</span>"
    ]
  },
  {
    "objectID": "contents/core/privacy_security/privacy_security.html#purpose",
    "href": "contents/core/privacy_security/privacy_security.html#purpose",
    "title": "15  Security & Privacy",
    "section": "",
    "text": "Learning Objectives\n\n\n\n\nUnderstand key ML privacy and security risks, such as data leaks, model theft, adversarial attacks, bias, and unintended data access.\nLearn from historical hardware and embedded systems security incidents.\nIdentify threats to ML models like data poisoning, model extraction, membership inference, and adversarial examples.\nRecognize hardware security threats to embedded ML spanning hardware bugs, physical attacks, side channels, counterfeit components, etc.\nExplore embedded ML defenses, such as trusted execution environments, secure boot, physical unclonable functions, and hardware security modules.\nDiscuss privacy issues handling sensitive user data with embedded ML, including regulations.\nLearn privacy-preserving ML techniques like differential privacy, federated learning, homomorphic encryption, and synthetic data generation.\nUnderstand trade-offs between privacy, accuracy, efficiency, threat models, and trust assumptions.\nRecognize the need for a cross-layer perspective spanning electrical, firmware, software, and physical design when securing embedded ML devices.",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Security & Privacy</span>"
    ]
  },
  {
    "objectID": "contents/core/privacy_security/privacy_security.html#overview",
    "href": "contents/core/privacy_security/privacy_security.html#overview",
    "title": "15  Security & Privacy",
    "section": "15.1 Overview",
    "text": "15.1 Overview\nMachine learning has evolved substantially from its academic origins, where privacy was not a primary concern. As ML migrated into commercial and consumer applications, the data became more sensitive - encompassing personal information like communications, purchases, and health data. This explosion of data availability fueled rapid advancements in ML capabilities. However, it also exposed new privacy risks, as demonstrated by incidents like the AOL data leak in 2006 and the Cambridge Analytica scandal.\nThese events highlighted the growing need to address privacy in ML systems. In this chapter, we explore privacy and security considerations together, as they are inherently linked in ML. For example, an ML-powered home security camera must secure video feeds against unauthorized access and provide privacy protections to ensure only intended users can view the footage. A breach of either security or privacy could expose private user moments.\nEmbedded ML systems like smart assistants and wearables are ubiquitous and process intimate user data. However, their computational constraints often prevent heavy security protocols. Designers must balance performance needs with rigorous security and privacy standards tailored to embedded hardware limitations.\nThis chapter provides essential knowledge for addressing the complex privacy and security landscape of embedded ML. We will explore vulnerabilities and cover various techniques that enhance privacy and security within embedded systems’ resource constraints.\nWe hope that by building a holistic understanding of risks and safeguards, you will gain the principles to develop secure, ethical, embedded ML applications.",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Security & Privacy</span>"
    ]
  },
  {
    "objectID": "contents/core/privacy_security/privacy_security.html#terminology",
    "href": "contents/core/privacy_security/privacy_security.html#terminology",
    "title": "15  Security & Privacy",
    "section": "15.2 Terminology",
    "text": "15.2 Terminology\nIn this chapter, we will discuss security and privacy together, so there are key terms that we need to be clear about. Since these terms are general concepts applied in many domains, we want to define how they relate to the context of this chapter and provide relevant examples to illustrate their application.\n\nPrivacy: The ability to control access to sensitive user data collected and processed by a system. In machine learning, this involves ensuring that personal information, such as financial details or biometric data, is accessible only to authorized individuals. For instance, a home security camera powered by machine learning might record video footage and identify faces of visitors. Privacy concerns center on who can access, view, or share this sensitive data.\nSecurity: The practice of protecting machine learning systems and their data from unauthorized access, hacking, theft, and misuse. A secure system safeguards its data and operations to ensure integrity and confidentiality. For example, in the context of the home security camera, security measures prevent hackers from intercepting live video feeds or tampering with stored footage and ensure the model itself remains uncompromised.\nThreat: Refers to any potential danger, malicious actor, or harmful event that aims to exploit weaknesses in a system to compromise its security or privacy. A threat is the external force or intent that seeks to cause harm. Using the home security camera example, a threat could involve a hacker attempting to access live streams, steal stored videos, or deceive the system with false inputs to bypass facial recognition.\nVulnerability: Refers to a weakness, flaw, or gap in the system that creates the opportunity for a threat to succeed. Vulnerabilities are the points of exposure that threats target. Vulnerabilities can exist in hardware, software, or network configurations. For instance, if the home security camera connects to the internet through an unsecured Wi-Fi network, this vulnerability could allow attackers to intercept or manipulate the video data.",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Security & Privacy</span>"
    ]
  },
  {
    "objectID": "contents/core/privacy_security/privacy_security.html#historical-precedents",
    "href": "contents/core/privacy_security/privacy_security.html#historical-precedents",
    "title": "15  Security & Privacy",
    "section": "15.3 Historical Precedents",
    "text": "15.3 Historical Precedents\nWhile the specifics of machine learning hardware security can be distinct, the embedded systems field has a history of security incidents that provide critical lessons for all connected systems, including those using ML. Here are detailed explorations of past breaches:\n\n15.3.1 Stuxnet\nIn 2010, something unexpected was found on a computer in Iran - a very complicated computer virus that experts had never seen before. Stuxnet was a malicious computer worm that targeted supervisory control and data acquisition (SCADA) systems and was designed to damage Iran’s nuclear program (Farwell and Rohozinski 2011). Stuxnet was using four “zero-day exploits” - attacks that take advantage of secret weaknesses in software that no one knows about yet. This made Stuxnet very sneaky and hard to detect.\n\nFarwell, James P., and Rafal Rohozinski. 2011. “Stuxnet and the Future of Cyber War.” Survival 53 (1): 23–40. https://doi.org/10.1080/00396338.2011.555586.\nBut Stuxnet wasn’t designed to steal information or spy on people. Its goal was physical destruction - to sabotage centrifuges at Iran’s Natanz nuclear plant! So, how did the virus get onto computers at the Natanz plant, which was supposed to be disconnected from the outside world for security? Experts think someone inserted a USB stick containing Stuxnet into the internal Natanz network. This allowed the virus to “jump” from an outside system onto the isolated nuclear control systems and wreak havoc.\nStuxnet was incredibly advanced malware built by national governments to cross from the digital realm into real-world infrastructure. It specifically targeted important industrial machines, where embedded machine learning is highly applicable in a way never done before. The virus provided a wake-up call about how sophisticated cyberattacks could now physically destroy equipment and facilities.\nThis breach was significant due to its sophistication; Stuxnet specifically targeted programmable logic controllers (PLCs) used to automate electromechanical processes such as the speed of centrifuges for uranium enrichment. The worm exploited vulnerabilities in the Windows operating system to gain access to the Siemens Step7 software controlling the PLCs. Despite not being a direct attack on ML systems, Stuxnet is relevant for all embedded systems as it showcases the potential for state-level actors to design attacks that bridge the cyber and physical worlds with devastating effects. Figure 15.1 explains Stuxnet in greater detail.\n\n\n\n\n\n\nFigure 15.1: Stuxnet explained. Source: IEEE Spectrum\n\n\n\n\n\n15.3.2 Jeep Cherokee Hack\nThe Jeep Cherokee hack was a groundbreaking event demonstrating the risks inherent in increasingly connected automobiles (Miller 2019). In a controlled demonstration, security researchers remotely exploited a vulnerability in the Uconnect entertainment system, which had a cellular connection to the internet. They were able to control the vehicle’s engine, transmission, and brakes, alarming the automotive industry into recognizing the severe safety implications of cyber vulnerabilities in vehicles. Video 15.1 below is a short documentary of the attack.\n\nMiller, Charlie. 2019. “Lessons Learned from Hacking a Car.” IEEE Design &Amp; Test 36 (6): 7–9. https://doi.org/10.1109/mdat.2018.2863106.\n\n\n\n\n\n\nImportant 15.1: Jeep Cherokee Hack\n\n\n\n\n\n\nWhile this wasn’t an attack on an ML system per se, the reliance of modern vehicles on embedded systems for safety-critical functions has significant parallels to the deployment of ML in embedded systems, underscoring the need for robust security at the hardware level.\n\n\n15.3.3 Mirai Botnet\nThe Mirai botnet involved the infection of networked devices such as digital cameras and DVR players (Antonakakis et al. 2017). In October 2016, the botnet was used to conduct one of the largest DDoS attacks, disrupting internet access across the United States. The attack was possible because many devices used default usernames and passwords, which were easily exploited by the Mirai malware to control the devices. Video 15.2 explains how the Mirai Botnet works.\n\nAntonakakis, Manos, Tim April, Michael Bailey, Matt Bernhard, Elie Bursztein, Jaime Cochran, Zakir Durumeric, et al. 2017. “Understanding the Mirai Botnet.” In 26th USENIX Security Symposium (USENIX Security 17), 1093–1110.\n\n\n\n\n\n\nImportant 15.2: Mirai Botnet\n\n\n\n\n\n\nAlthough the devices were not ML-based, the incident is a stark reminder of what can happen when numerous embedded devices with poor security controls are networked, which is becoming more common with the growth of ML-based IoT devices.\n\n\n15.3.4 Implications\nThese historical breaches demonstrate the cascading effects of hardware vulnerabilities in embedded systems. Each incident offers a precedent for understanding the risks and designing better security protocols. For instance, the Mirai botnet highlights the immense destructive potential when threat actors can gain control over networked devices with weak security, a situation becoming increasingly common with ML systems. Many current ML devices function as “edge” devices meant to collect and process data locally before sending it to the cloud. Much like the cameras and DVRs compromised by Mirai, edge ML devices often rely on embedded hardware like ARM processors and run lightweight OS like Linux. Securing the device credentials is critical.\nSimilarly, the Jeep Cherokee hack was a watershed moment for the automotive industry. It exposed serious vulnerabilities in the growing network-connected vehicle systems and their lack of isolation from core drive systems like brakes and steering. In response, auto manufacturers invested heavily in new cybersecurity measures, though gaps likely remain.\nChrysler did a recall to patch the vulnerable Uconnect software, allowing the remote exploit. This included adding network-level protections to prevent unauthorized external access and compartmentalizing in-vehicle systems to limit lateral movement. Additional layers of encryption were added for commands sent over the CAN bus within vehicles.\nThe incident also spurred the creation of new cybersecurity standards and best practices. The Auto-ISAC was established for automakers to share intelligence, and the NHTSA guided management risks. New testing and audit procedures were developed to assess vulnerabilities proactively. The aftereffects continue to drive change in the automotive industry as cars become increasingly software-defined.\nUnfortunately, manufacturers often overlook security when developing new ML edge devices - using default passwords, unencrypted communications, unsecured firmware updates, etc. Any such vulnerabilities could allow attackers to gain access and control devices at scale by infecting them with malware. With a botnet of compromised ML devices, attackers could leverage their aggregated computational power for DDoS attacks on critical infrastructure.\nWhile these events didn’t directly involve machine learning hardware, the principles of the attacks carry over to ML systems, which often involve similar embedded devices and network architectures. As ML hardware is increasingly integrated with the physical world, securing it against such breaches is paramount. The evolution of security measures in response to these incidents provides valuable insights into protecting current and future ML systems from analogous vulnerabilities.\nThe distributed nature of ML edge devices means threats can propagate quickly across networks. And if devices are being used for mission-critical purposes like medical devices, industrial controls, or self-driving vehicles, the potential physical damage from weaponized ML bots could be severe. Just like Mirai demonstrated the dangerous potential of poorly secured IoT devices, the litmus test for ML hardware security will be how vulnerable or resilient these devices are to worm-like attacks. The stakes are raised as ML spreads to safety-critical domains, putting the onus on manufacturers and system operators to incorporate the lessons from Mirai.\nThe lesson is the importance of designing for security from the outset and having layered defenses. The Jeep case highlights potential vulnerabilities for ML systems around externally facing software interfaces and isolation between subsystems. Manufacturers of ML devices and platforms should assume a similar proactive and comprehensive approach to security rather than leaving it as an afterthought. Rapid response and dissemination of best practices will be crucial as threats evolve.",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Security & Privacy</span>"
    ]
  },
  {
    "objectID": "contents/core/privacy_security/privacy_security.html#security-threats-to-ml-models",
    "href": "contents/core/privacy_security/privacy_security.html#security-threats-to-ml-models",
    "title": "15  Security & Privacy",
    "section": "15.4 Security Threats to ML Models",
    "text": "15.4 Security Threats to ML Models\nML models face security risks that can undermine their integrity, performance, and trustworthiness if not adequately addressed. Among these, three primary threats stand out: model theft, where adversaries steal proprietary model parameters and the sensitive data they contain; data poisoning, which compromises models by tampering with training data; and adversarial attacks, designed to deceive models into making incorrect or unwanted predictions. We will discuss each of these threats in detail and provide case study examples to illustrate their real-world implications.\n\n15.4.1 Model Theft\nModel theft occurs when an attacker gains unauthorized access to a deployed ML model. The concern here is the theft of the model’s structure and trained parameters and the proprietary data it contains (Ateniese et al. 2015). Model theft is a real and growing threat, as demonstrated by cases like ex-Google engineer Anthony Levandowski, who allegedly stole Waymo’s self-driving car designs and started a competing company. Beyond economic impacts, model theft can seriously undermine privacy and enable further attacks.\n\nAteniese, Giuseppe, Luigi V. Mancini, Angelo Spognardi, Antonio Villani, Domenico Vitali, and Giovanni Felici. 2015. “Hacking Smart Machines with Smarter Ones: How to Extract Meaningful Data from Machine Learning Classifiers.” International Journal of Security and Networks 10 (3): 137. https://doi.org/10.1504/ijsn.2015.071829.\nFor instance, consider an ML model developed for personalized recommendations in an e-commerce application. If a competitor steals this model, they gain insights into business analytics, customer preferences, and even trade secrets embedded within the model’s data. Attackers could leverage stolen models to craft more effective inputs for model inversion attacks, deducing private details about the model’s training data. A cloned e-commerce recommendation model could reveal customer purchase behaviors and demographics.\nTo understand model inversion attacks, consider a facial recognition system used to grant access to secured facilities. The system is trained on a dataset of employee photos. An attacker could infer features of the original dataset by observing the model’s output to various inputs. For example, suppose the model’s confidence level for a particular face is significantly higher for a given set of features. In that case, an attacker might deduce that someone with those features is likely in the training dataset.\nThe methodology of model inversion typically involves the following steps:\n\nAccessing Model Outputs: The attacker queries the ML model with input data and observes the outputs. This is often done through a legitimate interface, like a public API.\nAnalyzing Confidence Scores: For each input, the model provides a confidence score that reflects how similar the input is to the training data.\nReverse-Engineering: By analyzing the confidence scores or output probabilities, attackers can use optimization techniques to reconstruct what they believe is close to the original input data.\n\nOne historical example of such a vulnerability being explored was the research on inversion attacks against the U.S. Netflix Prize dataset, where researchers demonstrated that it was possible to learn about an individual’s movie preferences, which could lead to privacy breaches (Narayanan and Shmatikov 2006).\n\nNarayanan, Arvind, and Vitaly Shmatikov. 2006. “How to Break Anonymity of the Netflix Prize Dataset.” CoRR. http://arxiv.org/abs/cs/0610105.\nModel theft implies that it could lead to economic losses, undermine competitive advantage, and violate user privacy. There’s also the risk of model inversion attacks, where an adversary could input various data into the stolen model to infer sensitive information about the training data.\nBased on the desired asset, model theft attacks can be divided into two categories: exact model properties and approximate model behavior.\n\nStealing Exact Model Properties\nIn these attacks, the objective is to extract information about concrete metrics, such as a network’s learned parameters, fine-tuned hyperparameters, and the model’s internal layer architecture (Oliynyk, Mayer, and Rauber 2023).\n\nLearned Parameters: Adversaries aim to steal a model’s learned knowledge (weights and biases) to replicate it. Parameter theft is generally used with other attacks, such as architecture theft, which lacks parameter knowledge.\nFine-Tuned Hyperparameters: Training is costly, and identifying the optimal configuration of hyperparameters (such as learning rate and regularization) can be time-consuming and resource-intensive. Consequently, stealing a model’s optimized hyperparameters enables adversaries to replicate the model without incurring the exact development costs.\nModel Architecture: This attack concerns the specific design and structure of the model, such as layers, neurons, and connectivity patterns. Beyond reducing associated training costs, this theft poses a severe risk to intellectual property, potentially undermining a company’s competitive advantage. Architecture theft can be achieved by exploiting side-channel attacks (discussed later).\n\n\n\nStealing Approximate Model Behavior\nInstead of extracting exact numerical values of the model’s parameters, these attacks aim to reproduce the model’s behavior (predictions and effectiveness), decision-making, and high-level characteristics (Oliynyk, Mayer, and Rauber 2023). These techniques aim to achieve similar outcomes while allowing for internal deviations in parameters and architecture. Types of approximate behavior theft include gaining the same level of effectiveness and obtaining prediction consistency.\n\nOliynyk, Daryna, Rudolf Mayer, and Andreas Rauber. 2023. “I Know What You Trained Last Summer: A Survey on Stealing Machine Learning Models and Defences.” ACM Computing Surveys 55 (14s): 1–41. https://doi.org/10.1145/3595292.\n\nLevel of Effectiveness: Attackers aim to replicate the model’s decision-making capabilities rather than focus on the precise parameter values. This is done through understanding the overall behavior of the model. Consider a scenario where an attacker wants to copy the behavior of an image classification model. By analyzing the model’s decision boundaries, the attack tunes its model to reach an effectiveness comparable to the original model. This could entail analyzing 1) the confusion matrix to understand the balance of prediction metrics (true positive, true negative, false positive, false negative) and 2) other performance metrics, such as F1 score and precision, to ensure that the two models are comparable.\nPrediction Consistency: The attacker tries to align their model’s prediction patterns with the target model’s. This involves matching prediction outputs (both positive and negative) on the same set of inputs and ensuring distributional consistency across different classes. For instance, consider a natural language processing (NLP) model that generates sentiment analysis for movie reviews (labels reviews as positive, neutral, or negative). The attacker will try to fine-tune their model to match the prediction of the original models on the same set of movie reviews. This includes ensuring that the model makes the same mistakes (mispredictions) that the targeted model makes.\n\n\n\nCase Study: Tesla’s IP Theft Case\nIn 2018, Tesla filed a lawsuit against self-driving car startup Zoox, alleging former employees stole confidential data and trade secrets related to Tesla’s autonomous driving assistance system.\nTesla claimed that several of its former employees took over 10 GB of proprietary data, including ML models and source code, before joining Zoox. This allegedly included one of Tesla’s crucial image recognition models for identifying objects.\nThe theft of this sensitive proprietary model could help Zoox shortcut years of ML development and duplicate Tesla’s capabilities. Tesla argued this theft of IP caused significant financial and competitive harm. There were also concerns it could allow model inversion attacks to infer private details about Tesla’s testing data.\nThe Zoox employees denied stealing any proprietary information. However, the case highlights the significant risks of model theft—enabling the cloning of commercial models, causing economic impacts, and opening the door for further data privacy violations.\n\n\n\n15.4.2 Data Poisoning\nData poisoning is an attack where the training data is tampered with, leading to a compromised model (Biggio, Nelson, and Laskov 2012). Attackers can modify existing training examples, insert new malicious data points, or influence the data collection process. The poisoned data is labeled in such a way as to skew the model’s learned behavior. This can be particularly damaging in applications where ML models make automated decisions based on learned patterns. Beyond training sets, poisoning tests and validation data can allow adversaries to boost reported model performance artificially.\n\nBiggio, Battista, Blaine Nelson, and Pavel Laskov. 2012. “Poisoning Attacks Against Support Vector Machines.” In Proceedings of the 29th International Conference on Machine Learning, ICML 2012, Edinburgh, Scotland, UK, June 26 - July 1, 2012. icml.cc / Omnipress. http://icml.cc/2012/papers/880.pdf.\nThe process usually involves the following steps:\n\nInjection: The attacker adds incorrect or misleading examples into the training set. These examples are often designed to look normal to cursory inspection but have been carefully crafted to disrupt the learning process.\nTraining: The ML model trains on this manipulated dataset and develops skewed understandings of the data patterns.\nDeployment: Once the model is deployed, the corrupted training leads to flawed decision-making or predictable vulnerabilities the attacker can exploit.\n\nThe impacts of data poisoning extend beyond just classification errors or accuracy drops. For instance, if incorrect or malicious data is introduced into a traffic sign recognition system’s training set, the model may learn to misclassify stop signs as yield signs, which can have dangerous real-world consequences, especially in embedded autonomous systems like autonomous vehicles.\nData poisoning can degrade a model’s accuracy, force it to make incorrect predictions or cause it to behave unpredictably. In critical applications like healthcare, such alterations can lead to significant trust and safety issues.\nThere are six main categories of data poisoning (Oprea, Singhal, and Vassilev 2022):\n\nOprea, Alina, Anoop Singhal, and Apostol Vassilev. 2022. “Poisoning Attacks Against Machine Learning: Can Machine Learning Be Trustworthy?” Computer 55 (11): 94–99. https://doi.org/10.1109/mc.2022.3190787.\n\nAvailability Attacks: These attacks seek to compromise a model’s overall functionality. They cause it to misclassify most testing samples, rendering the model unusable for practical applications. An example is label flipping, where labels of a specific, targeted class are replaced with labels from a different one.\nTargeted Attacks: Unlike availability attacks, targeted attacks aim to compromise a small number of the testing samples. So, the effect is localized to a limited number of classes, while the model maintains the same original level of accuracy on most of the classes. The targeted nature of the attack requires the attacker to possess knowledge of the model’s classes, making detecting these attacks more challenging.\nBackdoor Attacks: In these attacks, an adversary targets specific patterns in the data. The attacker introduces a backdoor (a malicious, hidden trigger or pattern) into the training data, such as altering certain features in structured data or a pattern of pixels at a fixed position. This causes the model to associate the malicious pattern with specific labels. As a result, when the model encounters test samples that contain a malicious pattern, it makes false predictions, highlighting the importance of caution and prevention in the role of data security professionals.\nSubpopulation Attacks: Attackers selectively choose to compromise a subset of the testing samples while maintaining accuracy on the rest of the samples. You can think of these attacks as a combination of availability and targeted attacks: performing availability attacks (performance degradation) within the scope of a targeted subset. Although subpopulation attacks may seem very similar to targeted attacks, the two have clear differences:\nScope: While targeted attacks target a selected set of samples, subpopulation attacks target a general subpopulation with similar feature representations. For example, in a targeted attack, an actor inserts manipulated images of a ‘speed bump’ warning sign (with carefully crafted perturbation or patterns), which causes an autonomous car to fail to recognize such a sign and slow down. On the other hand, manipulating all samples of people with a British accent so that a speech recognition model would misclassify a British person’s speech is an example of a subpopulation attack.\nKnowledge: While targeted attacks require a high degree of familiarity with the data, subpopulation attacks require less intimate knowledge to be effective.\n\n\nCase Study: Poisoning Content Moderation Systems\nIn 2017, researchers demonstrated a data poisoning attack against a popular toxicity classification model called Perspective (Hosseini et al. 2017). This ML model detects toxic comments online.\n\nHosseini, Hossein, Sreeram Kannan, Baosen Zhang, and Radha Poovendran. 2017. “Deceiving Google’s Perspective API Built for Detecting Toxic Comments.” ArXiv Preprint abs/1702.08138 (February). http://arxiv.org/abs/1702.08138v1.\nThe researchers added synthetically generated toxic comments with slight misspellings and grammatical errors to the model’s training data. This slowly corrupted the model, causing it to misclassify increasing numbers of severely toxic inputs as non-toxic over time.\nAfter retraining on the poisoned data, the model’s false negative rate increased from 1.4% to 27% - allowing extremely toxic comments to bypass detection. The researchers warned this stealthy data poisoning could enable the spread of hate speech, harassment, and abuse if deployed against real moderation systems.\nThis case highlights how data poisoning can degrade model accuracy and reliability. For social media platforms, a poisoning attack that impairs toxicity detection could lead to the proliferation of harmful content and distrust of ML moderation systems. The example demonstrates why securing training data integrity and monitoring for poisoning is critical across application domains.\n\n\n\n15.4.3 Adversarial Attacks\nAdversarial attacks aim to trick models into making incorrect predictions by providing them with specially crafted, deceptive inputs (called adversarial examples) (Parrish et al. 2023). By adding slight perturbations to input data, adversaries can “hack” a model’s pattern recognition and deceive it. These are sophisticated techniques where slight, often imperceptible alterations to input data can trick an ML model into making a wrong prediction.\n\nParrish, Alicia, Hannah Rose Kirk, Jessica Quaye, Charvi Rastogi, Max Bartolo, Oana Inel, Juan Ciro, et al. 2023. “Adversarial Nibbler: A Data-Centric Challenge for Improving the Safety of Text-to-Image Models.” ArXiv Preprint abs/2305.14384 (May). http://arxiv.org/abs/2305.14384v1.\n\nRamesh, Aditya, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, and Ilya Sutskever. 2021. “Zero-Shot Text-to-Image Generation.” In Proceedings of the 38th International Conference on Machine Learning, ICML 2021, 18-24 July 2021, Virtual Event, edited by Marina Meila and Tong Zhang, 139:8821–31. Proceedings of Machine Learning Research. PMLR. http://proceedings.mlr.press/v139/ramesh21a.html.\n\nRombach, Robin, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. 2022. “High-Resolution Image Synthesis with Latent Diffusion Models.” In 2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 10674–85. IEEE. https://doi.org/10.1109/cvpr52688.2022.01042.\nOne can generate prompts that lead to unsafe images in text-to-image models like DALLE (Ramesh et al. 2021) or Stable Diffusion (Rombach et al. 2022). For example, by altering the pixel values of an image, attackers can deceive a facial recognition system into identifying a face as a different person.\nAdversarial attacks exploit the way ML models learn and make decisions during inference. These models work on the principle of recognizing patterns in data. An adversary crafts malicious inputs with perturbations to mislead the model’s pattern recognition—essentially ‘hacking’ the model’s perceptions.\nAdversarial attacks fall under different scenarios:\n\nWhitebox Attacks: The attacker has comprehensive knowledge of the target model’s internal workings, including the training data, parameters, and architecture. This extensive access facilitates the exploitation of the model’s vulnerabilities. The attacker can leverage specific and subtle weaknesses to construct highly effective adversarial examples.\nBlackbox Attacks: In contrast to whitebox attacks, in blackbox attacks, the attacker has little to no knowledge of the target model. The adversarial actor must carefully observe the model’s output behavior to carry out the attack.\nGreybox Attacks: These attacks occupy a spectrum between black-box and white-box attacks. The adversary possesses partial knowledge of the target model’s internal structure. For instance, the attacker might know the training data but lack information about the model’s architecture or parameters. In practical scenarios, most attacks fall within this grey area.\n\nThe landscape of machine learning models is complex and broad, especially given their relatively recent integration into commercial applications. This rapid adoption, while transformative, has brought to light numerous vulnerabilities within these models. Consequently, various adversarial attack methods have emerged, each strategically exploiting different aspects of different models. Below, we highlight a subset of these methods, showcasing the multifaceted nature of adversarial attacks on machine learning models:\n\nGenerative Adversarial Networks (GANs): The adversarial nature of GANs, where a generator and discriminator compete, aligns perfectly with crafting adversarial attacks (Goodfellow et al. 2020). By leveraging this framework, the generator network is trained to produce inputs that exploit weaknesses in a target model, causing it to misclassify. This dynamic, competitive process makes GANs particularly effective at creating sophisticated and diverse adversarial examples, underscoring their adaptability in attacking machine learning models.\nTransfer Learning Adversarial Attacks: These attacks target the feature extractors in transfer learning models by introducing perturbations that manipulate their learned representations. Feature extractors, pre-trained to identify general patterns, are fine-tuned for specific tasks in downstream models. Adversaries exploit this transfer by crafting inputs that distort the feature extractor’s outputs, causing downstream misclassifications. “Headless attacks” exemplify this strategy, where adversaries focus on the feature extractor without requiring access to the classification head or training data. This highlights a critical vulnerability in transfer learning pipelines, as the foundational components of many models can be exploited. Strengthening defenses is essential, given the widespread reliance on pre-trained models (Abdelkader et al. 2020).\n\n\nGoodfellow, Ian, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. 2020. “Generative Adversarial Networks.” Communications of the ACM 63 (11): 139–44. https://doi.org/10.1145/3422622.\n\nAbdelkader, Ahmed, Michael J. Curry, Liam Fowl, Tom Goldstein, Avi Schwarzschild, Manli Shu, Christoph Studer, and Chen Zhu. 2020. “Headless Horseman: Adversarial Attacks on Transfer Learning Models.” In ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 3087–91. IEEE. https://doi.org/10.1109/icassp40776.2020.9053181.\n\nCase Study: Tricking Traffic Sign Detection Models\nIn 2017, researchers conducted experiments by placing small black and white stickers on stop signs (Eykholt et al. 2017). When viewed by a normal human eye, the stickers did not obscure the sign or prevent interpretability. However, when images of the stickers stop signs were fed into standard traffic sign classification ML models, they were misclassified as speed limit signs over 85% of the time.\n\nEykholt, Kevin, Ivan Evtimov, Earlence Fernandes, Bo Li, Amir Rahmati, Chaowei Xiao, Atul Prakash, Tadayoshi Kohno, and Dawn Song. 2017. “Robust Physical-World Attacks on Deep Learning Models.” ArXiv Preprint abs/1707.08945 (July). http://arxiv.org/abs/1707.08945v5.\nThis demonstration showed how simple adversarial stickers could trick ML systems into misreading critical road signs. If deployed realistically, these attacks could endanger public safety, causing autonomous vehicles to misinterpret stop signs as speed limits. Researchers warned this could potentially cause dangerous rolling stops or acceleration into intersections.\nThis case study provides a concrete illustration of how adversarial examples exploit the pattern recognition mechanisms of ML models. By subtly altering the input data, attackers can induce incorrect predictions and pose significant risks to safety-critical applications like self-driving cars. The attack’s simplicity demonstrates how even minor, imperceptible changes can lead models astray. Consequently, developers must implement robust defenses against such threats.",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Security & Privacy</span>"
    ]
  },
  {
    "objectID": "contents/core/privacy_security/privacy_security.html#security-threats-to-ml-hardware",
    "href": "contents/core/privacy_security/privacy_security.html#security-threats-to-ml-hardware",
    "title": "15  Security & Privacy",
    "section": "15.5 Security Threats to ML Hardware",
    "text": "15.5 Security Threats to ML Hardware\nEmbedded machine learning hardware plays a critical role in powering modern AI applications but is increasingly exposed to a diverse range of security threats. These vulnerabilities can arise from flaws in hardware design, physical tampering, or even the complex pathways of global supply chains. Addressing these risks requires a comprehensive understanding of the various ways hardware integrity can be compromised. As summarized in Table 15.1, this section explores the key categories of hardware threats, offering insights into their origins, methods, and implications for ML systems.\n\n\n\nTable 15.1: Threat types on hardware security.\n\n\n\n\n\n\n\n\n\n\nThreat Type\nDescription\nRelevance to ML Hardware Security\n\n\n\n\nHardware Bugs\nIntrinsic flaws in hardware designs that can compromise system integrity.\nFoundation of hardware vulnerability.\n\n\nPhysical Attacks\nDirect exploitation of hardware through physical access or manipulation.\nBasic and overt threat model.\n\n\nFault-injection Attacks\nInduction of faults to cause errors in hardware operation, leading to potential system crashes.\nSystematic manipulation leading to failure.\n\n\nSide-Channel Attacks\nExploitation of leaked information from hardware operation to extract sensitive data.\nIndirect attack via environmental observation.\n\n\nLeaky Interfaces\nVulnerabilities arising from interfaces that expose data unintentionally.\nData exposure through communication channels.\n\n\nCounterfeit Hardware\nUse of unauthorized hardware components that may have security flaws.\nCompounded vulnerability issues.\n\n\nSupply Chain Risks\nRisks introduced through the hardware lifecycle, from production to deployment.\nCumulative & multifaceted security challenges.\n\n\n\n\n\n\n\n15.5.1 Hardware Bugs\nHardware is not immune to the pervasive issue of design flaws or bugs. Attackers can exploit these vulnerabilities to access, manipulate, or extract sensitive data, breaching the confidentiality and integrity that users and services depend on. An example of such vulnerabilities came to light with the discovery of Meltdown and Spectre—two hardware vulnerabilities that exploit critical vulnerabilities in modern processors. These bugs allow attackers to bypass the hardware barrier that separates applications, allowing a malicious program to read the memory of other programs and the operating system.\nMeltdown (Kocher et al. 2019a) and Spectre (Kocher et al. 2019b) work by taking advantage of optimizations in modern CPUs that allow them to speculatively execute instructions out of order before validity checks have been completed. This reveals data that should be inaccessible, which the attack captures through side channels like caches. The technical complexity demonstrates the difficulty of eliminating vulnerabilities even with extensive validation.\n\n———, et al. 2019a. “Spectre Attacks: Exploiting Speculative Execution.” In 2019 IEEE Symposium on Security and Privacy (SP), 1–19. IEEE. https://doi.org/10.1109/sp.2019.00002.\n\nKocher, Paul, Jann Horn, Anders Fogh, Daniel Genkin, Daniel Gruss, Werner Haas, Mike Hamburg, et al. 2019b. “Spectre Attacks: Exploiting Speculative Execution.” In 2019 IEEE Symposium on Security and Privacy (SP), 1–19. IEEE. https://doi.org/10.1109/sp.2019.00002.\nIf an ML system is processing sensitive data, such as personal user information or proprietary business analytics, Meltdown and Spectre represent a real and present danger to data security. Consider the case of an ML accelerator card designed to speed up machine learning processes, such as the ones we discussed in the AI Acceleration chapter. These accelerators work with the CPU to handle complex calculations, often related to data analytics, image recognition, and natural language processing. If such an accelerator card has a vulnerability akin to Meltdown or Spectre, it could leak the data it processes. An attacker could exploit this flaw not just to siphon off data but also to gain insights into the ML model’s workings, including potentially reverse-engineering the model itself (thus, going back to the issue of model theft.\nA real-world scenario where this could be devastating would be in the healthcare industry. ML systems routinely process highly sensitive patient data to help diagnose, plan treatment, and forecast outcomes. A bug in the system’s hardware could lead to the unauthorized disclosure of personal health information, violating patient privacy and contravening strict regulatory standards like the Health Insurance Portability and Accountability Act (HIPAA)\nThe Meltdown and Spectre vulnerabilities are stark reminders that hardware security is not just about preventing unauthorized physical access but also about ensuring that the hardware’s architecture does not become a conduit for data exposure. Similar hardware design flaws regularly emerge in CPUs, accelerators, memory, buses, and other components. This necessitates ongoing retroactive mitigations and performance trade-offs in deployed systems. Proactive solutions like confidential computing architectures could mitigate entire classes of vulnerabilities through fundamentally more secure hardware design. Thwarting hardware bugs requires rigor at every design stage, validation, and deployment.\n\n\n15.5.2 Physical Attacks\nPhysical tampering refers to the direct, unauthorized manipulation of physical computing resources to undermine the integrity of machine learning systems. It’s a particularly insidious attack because it circumvents traditional cybersecurity measures, which often focus more on software vulnerabilities than hardware threats.\nPhysical tampering can take many forms, from the relatively simple, such as someone inserting a USB device loaded with malicious software into a server, to the highly sophisticated, such as embedding a hardware Trojan during the manufacturing process of a microchip (discussed later in greater detail in the Supply Chain section). ML systems are susceptible to this attack because they rely on the accuracy and integrity of their hardware to process and analyze vast amounts of data correctly.\nConsider an ML-powered drone used for geographical mapping. The drone’s operation relies on a series of onboard systems, including a navigation module that processes inputs from various sensors to determine its path. If an attacker gains physical access to this drone, they could replace the genuine navigation module with a compromised one that includes a backdoor. This manipulated module could then alter the drone’s flight path to conduct surveillance over restricted areas or even smuggle contraband by flying undetected routes.\nAnother example is the physical tampering of biometric scanners used for access control in secure facilities. By introducing a modified sensor that transmits biometric data to an unauthorized receiver, an attacker can access personal identification data to authenticate individuals.\nThere are several ways that physical tampering can occur in ML hardware:\n\nManipulating sensors: Consider an autonomous vehicle equipped with cameras and LiDAR for environmental perception. A malicious actor could deliberately manipulate the physical alignment of these sensors to create occlusion zones or distort distance measurements. This could compromise object detection capabilities and potentially endanger vehicle occupants.\nHardware trojans: Malicious circuit modifications can introduce trojans designed to activate upon specific input conditions. For instance, an ML accelerator chip might operate as intended until encountering a predetermined trigger, at which point it behaves erratically.\nTampering with memory: Physically exposing and manipulating memory chips could allow the extraction of encrypted ML model parameters. Fault injection techniques can also corrupt model data to degrade accuracy.\nIntroducing backdoors: Gaining physical access to servers, an adversary could use hardware keyloggers to capture passwords and create backdoor accounts for persistent access. These could then be used to exfiltrate ML training data over time.\nSupply chain attacks: Manipulating third-party hardware components or compromising manufacturing and shipping channels creates systemic vulnerabilities that are difficult to detect and remediate.\n\n\n\n15.5.3 Fault-injection Attacks\nBy intentionally introducing faults into ML hardware, attackers can induce errors in the computational process, leading to incorrect outputs. This manipulation compromises the integrity of ML operations and can serve as a vector for further exploitation, such as system reverse engineering or security protocol bypass. Fault injection involves deliberately disrupting standard computational operations in a system through external interference (Joye and Tunstall 2012). By precisely triggering computational errors, adversaries can alter program execution in ways that degrade reliability or leak sensitive information.\n\nJoye, Marc, and Michael Tunstall. 2012. Fault Analysis in Cryptography. Springer Berlin Heidelberg. https://doi.org/10.1007/978-3-642-29656-7.\n\nBarenghi, Alessandro, Guido M. Bertoni, Luca Breveglieri, Mauro Pellicioli, and Gerardo Pelosi. 2010. “Low Voltage Fault Attacks to AES.” In 2010 IEEE International Symposium on Hardware-Oriented Security and Trust (HOST), 7–12. IEEE; IEEE. https://doi.org/10.1109/hst.2010.5513121.\n\nHutter, Michael, Jorn-Marc Schmidt, and Thomas Plos. 2009. “Contact-Based Fault Injections and Power Analysis on RFID Tags.” In 2009 European Conference on Circuit Theory and Design, 409–12. IEEE; IEEE. https://doi.org/10.1109/ecctd.2009.5275012.\n\nAmiel, Frederic, Christophe Clavier, and Michael Tunstall. 2006. “Fault Analysis of DPA-Resistant Algorithms.” In Fault Diagnosis and Tolerance in Cryptography, 223–36. Springer; Springer Berlin Heidelberg. https://doi.org/10.1007/11889700\\_20.\n\nAgrawal, Dakshi, Selcuk Baktir, Deniz Karakoyunlu, Pankaj Rohatgi, and Berk Sunar. 2007. “Trojan Detection Using IC Fingerprinting.” In 2007 IEEE Symposium on Security and Privacy (SP ’07), 296–310. Springer; IEEE. https://doi.org/10.1109/sp.2007.36.\n\nSkorobogatov, Sergei. 2009. “Local Heating Attacks on Flash Memory Devices.” In 2009 IEEE International Workshop on Hardware-Oriented Security and Trust, 1–6. IEEE; IEEE. https://doi.org/10.1109/hst.2009.5225028.\n\nSkorobogatov, Sergei P., and Ross J. Anderson. 2002. “Optical Fault Induction Attacks.” In Cryptographic Hardware and Embedded Systems-CHES 2002: 4th International Workshop Redwood Shores, CA, USA, August 13–15, 2002 Revised Papers 4, 2–12. Springer. https://doi.org/10.1007/3-540-36400-5\\_2.\nVarious physical tampering techniques can be used for fault injection. Low voltage (Barenghi et al. 2010), power spikes (Hutter, Schmidt, and Plos 2009), clock glitches (Amiel, Clavier, and Tunstall 2006), electromagnetic pulses (Agrawal et al. 2007), temperate increase (S. Skorobogatov 2009) and laser strikes (S. P. Skorobogatov and Anderson 2002) are common hardware attack vectors. They are precisely timed to induce faults like flipped bits or skipped instructions during critical operations.\nFor ML systems, consequences include impaired model accuracy, denial of service, extraction of private training data or model parameters, and reverse engineering of model architectures. Attackers could use fault injection to force misclassifications, disrupt autonomous systems, or steal intellectual property.\nFor example, Breier et al. (2018) successfully injected a fault attack into a deep neural network deployed on a microcontroller. They used a laser to heat specific transistors, forcing them to switch states. In one instance, they used this method to attack a ReLU activation function, resulting in the function always outputting a value of 0, regardless of the input. In the assembly code shown in Figure 15.2, the attack caused the executing program always to skip the jmp end instruction on line 6. This means that HiddenLayerOutput[i] is always set to 0, overwriting any values written to it on lines 4 and 5. As a result, the targeted neurons are rendered inactive, resulting in misclassifications.\n\n\n\n\n\n\nFigure 15.2: Fault-injection demonstrated with assembly code. Source: Breier et al. (2018).\n\n\nBreier, Jakub, Xiaolu Hou, Dirmanto Jap, Lei Ma, Shivam Bhasin, and Yang Liu. 2018. “DeepLaser: Practical Fault Attack on Deep Neural Networks.” ArXiv Preprint abs/1806.05859 (June). http://arxiv.org/abs/1806.05859v2.\n\n\nAn attacker’s strategy could be to infer information about the activation functions using side-channel attacks (discussed next). Then, the attacker could attempt to target multiple activation function computations by randomly injecting faults into the layers as close to the output layer as possible, increasing the likelihood and impact of the attack.\nEmbedded devices are particularly vulnerable due to limited physical hardening and resource constraints that restrict robust runtime defenses. Without tamper-resistant packaging, attacker access to system buses and memory enables precise fault strikes. Lightweight embedded ML models also lack redundancy to overcome errors.\nThese attacks can be particularly insidious because they bypass traditional software-based security measures, often not accounting for physical disruptions. Furthermore, because ML systems rely heavily on the accuracy and reliability of their hardware for tasks like pattern recognition, decision-making, and automated responses, any compromise in their operation due to fault injection can have severe and wide-ranging consequences.\nMitigating fault injection risks necessitates a multilayer approach. Physical hardening through tamper-proof enclosures and design obfuscation helps reduce access. Lightweight anomaly detection can identify unusual sensor inputs or erroneous model outputs (Hsiao et al. 2023). Error-correcting memories minimize disruption, while data encryption safeguards information. Emerging model watermarking techniques trace stolen parameters.\n\nHsiao, Yu-Shun, Zishen Wan, Tianyu Jia, Radhika Ghosal, Abdulrahman Mahmoud, Arijit Raychowdhury, David Brooks, Gu-Yeon Wei, and Vijay Janapa Reddi. 2023. “MAVFI: An End-to-End Fault Analysis Framework with Anomaly Detection and Recovery for Micro Aerial Vehicles.” In 2023 Design, Automation &Amp; Test in Europe Conference &Amp; Exhibition (DATE), 1–6. IEEE; IEEE. https://doi.org/10.23919/date56975.2023.10137246.\nHowever, balancing robust protections with embedded systems’ tight size and power limits remains challenging. Cryptography limits and lack of secure co-processors on cost-sensitive embedded hardware restrict options. Ultimately, fault injection resilience demands a cross-layer perspective spanning electrical, firmware, software, and physical design layers.\n\n\n15.5.4 Side-Channel Attacks\nSide-channel attacks constitute a class of security breaches that exploit information inadvertently revealed through the physical implementation of computing systems. In contrast to direct attacks targeting software or network vulnerabilities, these attacks leverage the system’s inherent hardware characteristics to extract sensitive information.\nThe fundamental premise of a side-channel attack is that a device’s operation can inadvertently reveal information. Such leaks can come from various sources, including the electrical power a device consumes (Kocher, Jaffe, and Jun 1999), the electromagnetic fields it emits (Gandolfi, Mourtel, and Olivier 2001), the time it takes to process certain operations, or even the sounds it produces. Each channel can indirectly glimpse the system’s internal processes, revealing information that can compromise security.\n\nKocher, Paul, Joshua Jaffe, and Benjamin Jun. 1999. “Differential Power Analysis.” In Advances in Cryptology — CRYPTO’ 99, 388–97. Springer; Springer Berlin Heidelberg. https://doi.org/10.1007/3-540-48405-1\\_25.\n\nGandolfi, Karine, Christophe Mourtel, and Francis Olivier. 2001. “Electromagnetic Analysis: Concrete Results.” In Cryptographic Hardware and Embedded Systems — CHES 2001, 251–61. Springer; Springer Berlin Heidelberg. https://doi.org/10.1007/3-540-44709-1\\_21.\n\nKocher, Paul, Joshua Jaffe, Benjamin Jun, and Pankaj Rohatgi. 2011. “Introduction to Differential Power Analysis.” Journal of Cryptographic Engineering 1 (1): 5–27. https://doi.org/10.1007/s13389-011-0006-y.\nConsider a machine learning system performing encrypted transactions. Encryption algorithms are designed to secure data but require computational work to encrypt and decrypt information. One widely used encryption standard is the Advanced Encryption Standard (AES), which encrypts data to prevent unauthorized access. However, attackers can analyze the power consumption patterns of a device performing encryption to deduce sensitive information, such as the cryptographic key. With sophisticated statistical methods, small variations in power usage during the encryption process can be correlated with the data being processed, eventually revealing the key. Some differential analysis attack techniques are Differential Power Analysis (DPA) (Kocher et al. 2011), Differential Electromagnetic Analysis (DEMA), and Correlation Power Analysis (CPA).\nAn attacker attempting to break AES encryption could collect power or electromagnetic traces (records of power consumption or emissions) from the device while it performs encryption. By analyzing these traces with statistical techniques, the attacker could identify correlations between the traces and the plaintext (original, unencrypted text) or ciphertext (encrypted text). These correlations could then be used to infer individual bits of the AES key and, eventually, reconstruct the entire key. Differential analysis attacks are particularly dangerous because they are low-cost, effective, and non-intrusive, allowing attackers to bypass algorithmic and hardware-level security measures. Compromises through these attacks are also challenging to detect, as they do not physically alter the device or break the encryption algorithm itself.\nBelow, a simplified visualization illustrates how analyzing the encryption device’s power consumption patterns can help extract information about the algorithm’s operations and, in turn, the secret data. The example shows a device that takes a 5-byte password as input. The password entered in this scenario is 0x61, 0x52, 0x77, 0x6A, 0x73, which represents the correct password. The power consumption patterns during authentication provide insights into how the algorithm functions.\nIn Figure 15.3, the red waveform represents the serial data lines as the bootloader receives the password data in chunks (i.e. 0x61, 0x52, 0x77, 0x6A, 0x73). Each labeled segment (e.g., “Data: 61”) corresponds to one byte of the password being processed by the encryption algorithm. The blue graph shows the power consumption of the encryption device as it processes each byte. When the correct password is entered, the device processes all 5 bytes successfully, and the blue voltage graph displays consistent patterns throughout. This chart gives you a baseline to understand how the device’s power consumption looks when a correct password is entered. In the next figures, you’ll see how the power profile changes with incorrect passwords, helping you spot the differences in the device’s behavior when authentication fails.\n\n\n\n\n\n\nFigure 15.3: Power consumption profile of the device during normal operations with a valid 5-byte password (0x61, 0x52, 0x77, 0x6A, 0x73). The red line represents the serial data being received by the bootloader, which in this figure is receiving the correct bytes. Notice how the blue line, representing power usage during authentication, corresponds to receiving and verifying the bytes. In the next figures, this blue power consumption profile will change. Source: Colin O’Flynn.\n\n\n\nWhen an incorrect password is entered, the power analysis chart is shown in Figure 15.4. The first three bytes of the password are correct (i.e. 0x61, 0x52, 0x77). As a result, the voltage patterns are very similar or identical between the two charts, up to and including the fourth byte. After processing the fourth byte (0x42), the device detects a mismatch with the correct password and stops processing further. This results in a noticeable change in the power pattern, shown by the sudden jump in the blue line as the voltage increases.\n\n\n\n\n\n\nFigure 15.4: Power consumption profile of the device when an incorrect 5-byte password (0x61, 0x52, 0x77, 0x42, 0x42) is entered. The red line represents the serial data received by the bootloader, showing the input bytes being processed. The first three bytes (0x61, 0x52, 0x77) are correct and match the expected password, as indicated by the consistent blue power consumption line. However, upon processing the fourth byte (0x42), a mismatch is detected. The bootloader stops further processing, resulting in a noticeable jump in the blue power consumption line, as the device halts authentication and enters an error state. Source: Colin O’Flynn.\n\n\n\nFigure 15.5 shows another example but where the password is entirely incorrect (0x30, 0x30, 0x30, 0x30, 0x30), unlike the previous example with the first three bytes correct. Here, the device identifies the mismatch immediately after processing the first byte and halts further processing. This is reflected in the power consumption profile, where the blue line exhibits a sharp jump following the first byte, indicating the device’s early termination of authentication.\n\n\n\n\n\n\nFigure 15.5: Power consumption profile of the device when an entirely incorrect password (0x30, 0x30, 0x30, 0x30, 0x30) is entered. The blue line shows a sharp jump after processing the first byte, indicating that the device has halted the authentication process. Source: Colin O’Flynn.\n\n\n\nThe example above demonstrates how information about the encryption process and the secret key can be inferred by analyzing different inputs and brute-force testing variations of each password byte, effectively ‘eavesdropping’ on the device’s operations. For a more detailed explanation, watch Video 15.3 below.\n\n\n\n\n\n\nImportant 15.3: Power Attack\n\n\n\n\n\n\nAnother example is an ML system for speech recognition, which processes voice commands to perform actions. By measuring the latency for the system to respond to commands or the power used during processing, an attacker could infer what commands are being processed and thus learn about the system’s operational patterns. Even more subtly, the sound emitted by a computer’s fan or hard drive could change in response to the workload, which a sensitive microphone could pick up and analyze to determine what kind of operations are being performed.\nIn real-world scenarios, side-channel attacks have effectively extracted encryption keys and compromised secure communications. One of the earliest recorded instances of such an attack occurred in the 1960s when the British intelligence agency MI5 confronted the challenge of deciphering encrypted communications from the Egyptian Embassy in London. Their cipher-breaking efforts were initially thwarted by the computational limitations of the time until an ingenious observation by MI5 agent Peter Wright altered the course of the operation.\nMI5 agent Peter Wright proposed using a microphone to capture the subtle acoustic signatures emitted from the embassy’s rotor cipher machine during encryption (Burnet and Thomas 1989). The distinct mechanical clicks of the rotors as operators configured them daily leaked critical information about the initial settings. This simple side channel of sound enabled MI5 to reduce the complexity of deciphering messages dramatically. This early acoustic leak attack highlights that side-channel attacks are not merely a digital age novelty but a continuation of age-old cryptanalytic principles. The notion that where there is a signal, there is an opportunity for interception remains foundational. From mechanical clicks to electrical fluctuations and beyond, side channels enable adversaries to extract secrets indirectly through careful signal analysis.\n\nBurnet, David, and Richard Thomas. 1989. “Spycatcher: The Commodification of Truth.” Journal of Law and Society 16 (2): 210. https://doi.org/10.2307/1410360.\n\nAsonov, D., and R. Agrawal. n.d. “Keyboard Acoustic Emanations.” In IEEE Symposium on Security and Privacy, 2004. Proceedings. 2004, 3–11. IEEE; IEEE. https://doi.org/10.1109/secpri.2004.1301311.\n\nGnad, Dennis R. E., Fabian Oboril, and Mehdi B. Tahoori. 2017. “Voltage Drop-Based Fault Attacks on FPGAs Using Valid Bitstreams.” In 2017 27th International Conference on Field Programmable Logic and Applications (FPL), 1–7. IEEE; IEEE. https://doi.org/10.23919/fpl.2017.8056840.\n\nZhao, Mark, and G. Edward Suh. 2018. “FPGA-Based Remote Power Side-Channel Attacks.” In 2018 IEEE Symposium on Security and Privacy (SP), 229–44. IEEE; IEEE. https://doi.org/10.1109/sp.2018.00049.\nToday, acoustic cryptanalysis has evolved into attacks like keyboard eavesdropping (Asonov and Agrawal, n.d.). Electrical side channels range from power analysis on cryptographic hardware (Gnad, Oboril, and Tahoori 2017) to voltage fluctuations (Zhao and Suh 2018) on machine learning accelerators. Timing, electromagnetic emission, and even heat footprints can likewise be exploited. New and unexpected side channels often emerge as computing becomes more interconnected and miniaturized.\nJust as MI5’s analog acoustic leak transformed their codebreaking, modern side-channel attacks circumvent traditional boundaries of cyber defense. Understanding the creative spirit and historical persistence of side channel exploits is key knowledge for developers and defenders seeking to secure modern machine learning systems comprehensively against digital and physical threats.\n\n\n15.5.5 Leaky Interfaces\nLeaky interfaces in embedded systems are often overlooked backdoors that can become significant security vulnerabilities. While designed for legitimate purposes such as communication, maintenance, or debugging, these interfaces may inadvertently provide attackers with a window through which they can extract sensitive information or inject malicious data.\nAn interface becomes “leaky” when it exposes more information than it should, often due to a lack of stringent access controls or inadequate shielding of the transmitted data. Here are some real-world examples of leaky interface issues causing security problems in IoT and embedded devices:\n\nBaby Monitors: Many WiFi-enabled baby monitors have been found to have unsecured interfaces for remote access. This allowed attackers to gain live audio and video feeds from people’s homes, representing a major privacy violation.\nPacemakers: Interface vulnerabilities were discovered in some pacemakers that could allow attackers to manipulate cardiac functions if exploited. This presents a potentially life-threatening scenario.\nSmart Lightbulbs: A researcher found he could access unencrypted data from smart lightbulbs via a debug interface, including WiFi credentials, allowing him to gain access to the connected network (Greengard 2021).\nSmart Cars: If left unsecured, The OBD-II diagnostic port has been shown to provide an attack vector into automotive systems. Attackers could use it to control brakes and other components (Miller and Valasek 2015).\n\n\nGreengard, Samuel. 2021. The Internet of Things. The MIT Press. https://doi.org/10.7551/mitpress/13937.001.0001.\n\nMiller, Charlie, and Chris Valasek. 2015. “Remote Exploitation of an Unaltered Passenger Vehicle.” Black Hat USA 2015 (S 91): 1–91.\nWhile the above are not directly connected with ML, consider the example of a smart home system with an embedded ML component that controls home security based on behavior patterns it learns over time. The system includes a maintenance interface accessible via the local network for software updates and system checks. If this interface does not require strong authentication or the data transmitted through it is not encrypted, an attacker on the same network could gain access. They could then eavesdrop on the homeowner’s daily routines or reprogram the security settings by manipulating the firmware.\nSuch leaks are a privacy issue and a potential entry point for more damaging exploits. The exposure of training data, model parameters, or ML outputs from a leak could help adversaries construct adversarial examples or reverse-engineer models. Access through a leaky interface could also be used to alter an embedded device’s firmware, loading it with malicious code that could turn off the device, intercept data, or use it in botnet attacks.\nA multi-layered approach is necessary to mitigate these risks, spanning technical controls like authentication, encryption, anomaly detection, policies and processes like interface inventories, access controls, auditing, and secure development practices. Turning off unnecessary interfaces and compartmentalizing risks via a zero-trust model provide additional protection.\nAs designers of embedded ML systems, we should assess interfaces early in development and continually monitor them post-deployment as part of an end-to-end security lifecycle. Understanding and securing interfaces is crucial for ensuring the overall security of embedded ML.\n\n\n15.5.6 Counterfeit Hardware\nML systems are only as reliable as the underlying hardware. In an era where hardware components are global commodities, the rise of counterfeit or cloned hardware presents a significant challenge. Counterfeit hardware encompasses any components that are unauthorized reproductions of original parts. Counterfeit components infiltrate ML systems through complex supply chains that stretch across borders and involve numerous stages from manufacture to delivery.\nA single lapse in the supply chain’s integrity can result in the insertion of counterfeit parts designed to imitate the functions and appearance of genuine hardware closely. For instance, a facial recognition system for high-security access control may be compromised if equipped with counterfeit processors. These processors could fail to accurately process and verify biometric data, potentially allowing unauthorized individuals to access restricted areas.\nThe challenge with counterfeit hardware is multifaceted. It undermines the quality and reliability of ML systems, as these components may degrade faster or perform unpredictably due to substandard manufacturing. The security risks are also profound; counterfeit hardware can contain vulnerabilities ripe for exploitation by malicious actors. For example, a cloned network router in an ML data center might include a hidden backdoor, enabling data interception or network intrusion without detection.\nFurthermore, counterfeit hardware poses legal and compliance risks. Companies inadvertently utilizing counterfeit parts in their ML systems may face serious legal repercussions, including fines and sanctions for failing to comply with industry regulations and standards. This is particularly true for sectors where compliance with specific safety and privacy regulations is mandatory, such as healthcare and finance.\nEconomic pressures to reduce costs exacerbate the issue of counterfeit hardware and compel businesses to source from lower-cost suppliers without stringent verification processes. This economizing can inadvertently introduce counterfeit parts into otherwise secure systems. Additionally, detecting these counterfeits is inherently tricky since they are created to pass as the original components, often requiring sophisticated equipment and expertise to identify.\nIn the field of ML, where real-time decisions and complex computations are the norm, the implications of hardware failure can be inconvenient and potentially dangerous. It is crucial for stakeholders to be fully aware of these risks. The challenges posed by counterfeit hardware call for a comprehensive understanding of the current threats to ML system integrity. This underscores the need for proactive, informed management of the hardware life cycle within these advanced systems.\n\n\n15.5.7 Supply Chain Risks\nThe threat of counterfeit hardware is closely tied to broader supply chain vulnerabilities. Globalized, interconnected supply chains create multiple opportunities for compromised components to infiltrate a product’s lifecycle. Supply chains involve numerous entities, from design to manufacturing, assembly, distribution, and integration. A lack of transparency and oversight of each partner makes verifying integrity at every step challenging. Lapses anywhere along the chain can allow the insertion of counterfeit parts.\nFor example, a contracted manufacturer may unknowingly receive and incorporate recycled electronic waste containing dangerous counterfeits. An untrustworthy distributor could smuggle in cloned components. Insider threats at any vendor might deliberately mix counterfeits into legitimate shipments.\nOnce counterfeits enter the supply stream, they move quickly through multiple hands before ending up in ML systems where detection is difficult. Advanced counterfeits like refurbished parts or clones with repackaged externals can masquerade as authentic components, passing visual inspection.\nTo identify fakes, thorough technical profiling using micrography, X-ray screening, component forensics, and functional testing is often required. However, such costly analysis is impractical for large-volume procurement.\nStrategies like supply chain audits, screening suppliers, validating component provenance, and adding tamper-evident protections can help mitigate risks. However, given global supply chain security challenges, a zero-trust approach is prudent. Designing ML systems to use redundant checking, fail-safes, and continuous runtime monitoring provides resilience against component compromises.\nRigorous validation of hardware sources coupled with fault-tolerant system architectures offers the most robust defense against the pervasive risks of convoluted, opaque global supply chains.\n\n\n15.5.8 Case Study: A Wake-Up Call for Hardware Security\nIn 2018, Bloomberg Businessweek published an alarming story that got much attention in the tech world. The article claimed that Supermicro had secretly planted tiny spy chips on server hardware. Reporters said Chinese state hackers working with Supermicro could sneak these tiny chips onto motherboards during manufacturing. The tiny chips allegedly gave the hackers backdoor access to servers used by over 30 major companies, including Apple and Amazon.\nIf true, this would allow hackers to spy on private data or even tamper with systems. However, after investigating, Apple and Amazon found no proof that such hacked Supermicro hardware existed. Other experts questioned whether the Bloomberg article was accurate reporting.\nWhether the story is entirely accurate or not is not our concern from a pedagogical viewpoint. However, this incident drew attention to the risks of global supply chains for hardware primarily manufactured in China. When companies outsource and buy hardware components from vendors worldwide, there needs to be more visibility into the process. In this complex global pipeline, there are concerns that counterfeits or tampered hardware could be slipped in somewhere along the way without tech companies realizing it. Companies relying too much on single manufacturers or distributors creates risk. For instance, due to the over-reliance on TSMC for semiconductor manufacturing, the U.S. has invested 50 billion dollars into the CHIPS Act.\nAs ML moves into more critical systems, verifying hardware integrity from design through production and delivery is crucial. The reported Supermicro backdoor demonstrated that for ML security, we cannot take global supply chains and manufacturing for granted. We must inspect and validate hardware at every link in the chain.",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Security & Privacy</span>"
    ]
  },
  {
    "objectID": "contents/core/privacy_security/privacy_security.html#embedded-ml-hardware-security",
    "href": "contents/core/privacy_security/privacy_security.html#embedded-ml-hardware-security",
    "title": "15  Security & Privacy",
    "section": "15.6 Embedded ML Hardware Security",
    "text": "15.6 Embedded ML Hardware Security\n\n15.6.1 Trusted Execution Environments\n\nAbout TEE\nA Trusted Execution Environment (TEE) is a secure area within a host processor that ensures the safe execution of code and the protection of sensitive data. By isolating critical tasks from the operating system, TEEs resist software and hardware attacks, providing a secure environment for handling sensitive computations.\n\n\nBenefits\nTEEs are particularly valuable in scenarios where sensitive data must be processed or where the integrity of a system’s operations is critical. In the context of ML hardware, TEEs ensure that the ML algorithms and data are protected against tampering and leakage. This is essential because ML models often process private information, trade secrets, or data that could be exploited if exposed.\nFor instance, a TEE can protect ML model parameters from being extracted by malicious software on the same device. This protection is vital for privacy and maintaining the integrity of the ML system, ensuring that the models perform as expected and do not provide skewed outputs due to manipulated parameters. Apple’s Secure Enclave, found in iPhones and iPads, is a form of TEE that provides an isolated environment to protect sensitive user data and cryptographic operations.\nTrusted Execution Environments (TEEs) are crucial for industries that demand high levels of security, including telecommunications, finance, healthcare, and automotive. TEEs protect the integrity of 5G networks in telecommunications and support critical applications. In finance, they secure mobile payments and authentication processes. Healthcare relies on TEEs to safeguard sensitive patient data, while the automotive industry depends on them for the safety and reliability of autonomous systems. Across all sectors, TEEs ensure the confidentiality and integrity of data and operations.\nIn ML systems, TEEs can:\n\nSecurely perform model training and inference, ensuring the computation results remain confidential.\nProtect the confidentiality of input data, like biometric information, used for personal identification or sensitive classification tasks.\nSecure ML models by preventing reverse engineering, which can protect proprietary information and maintain a competitive advantage.\nEnable secure updates to ML models, ensuring that updates come from a trusted source and have not been tampered with in transit.\nStrengthen network security by safeguarding data transmission between distributed ML components through encryption and secure in-TEE processing.\n\nThe importance of TEEs in ML hardware security stems from their ability to protect against external and internal threats, including the following:\n\nMalicious Software: TEEs can prevent high-privilege malware from accessing sensitive areas of the ML system.\nPhysical Tampering: By integrating with hardware security measures, TEEs can protect against physical tampering that attempts to bypass software security.\nSide-channel Attacks: Although not impenetrable, TEEs can mitigate specific side-channel attacks by controlling access to sensitive operations and data patterns.\nNetwork Threats: TEEs enhance network security by safeguarding data transmission between distributed ML components through encryption and secure in-TEE processing. This effectively prevents man-in-the-middle attacks and ensures data is transmitted through trusted channels.\n\n\n\nMechanics\nThe fundamentals of TEEs contain four main parts:\n\nIsolated Execution: Code within a TEE runs in a separate environment from the host device’s host operating system. This isolation protects the code from unauthorized access by other applications.\nSecure Storage: TEEs can securely store cryptographic keys, authentication tokens, and sensitive data, preventing regular applications from accessing them outside the TEE.\nIntegrity Protection: TEEs can verify the integrity of code and data, ensuring that they have not been altered before execution or during storage.\nData Encryption: Data handled within a TEE can be encrypted, making it unreadable to entities without the proper keys, which are also managed within the TEE.\n\nHere are some examples of TEEs that provide hardware-based security for sensitive applications:\n\nARMTrustZone: This technology creates secure and normal world execution environments isolated using hardware controls and implemented in many mobile chipsets.\nIntelSGX: Intel’s Software Guard Extensions provide an enclave for code execution that protects against various software-based threats, specifically targeting O.S. layer vulnerabilities. They are used to safeguard workloads in the cloud.\nQualcomm Secure Execution Environment: A Hardware sandbox on Qualcomm chipsets for mobile payment and authentication apps.\nApple SecureEnclave: A TEE for biometric data and cryptographic key management on iPhones and iPads, facilitating secure mobile payments.\n\nFigure 15.6 is a diagram demonstrating a secure enclave isolated from the host processor to provide an extra layer of security. The secure enclave has a boot ROM to establish a hardware root of trust, an AES engine for efficient and secure cryptographic operations, and protected memory. It also has a mechanism to store information securely on attached storage separate from the NAND flash storage used by the application processor and operating system. NAND flash is a type of non-volatile storage used in devices like SSDs, smartphones, and tablets to retain data even when powered off. By isolating sensitive data from the NAND storage accessed by the main system, this design ensures user data remains secure even if the application processor kernel is compromised.\n\n\n\n\n\n\nFigure 15.6: System-on-chip secure enclave. Source: Apple.\n\n\n\n\n\nTradeoffs\nWhile Trusted Execution Environments offer significant security benefits, their implementation involves trade-offs. Several factors influence whether a system includes a TEE:\nCost: Implementing TEEs involves additional costs. There are direct costs for the hardware and indirect costs associated with developing and maintaining secure software for TEEs. These costs may only be justifiable for some devices, especially low-margin products.\nComplexity: TEEs add complexity to system design and development. Integrating a TEE with existing systems requires a substantial redesign of the hardware and software stack, which can be a barrier, especially for legacy systems.\nPerformance Overhead: TEEs may introduce performance overhead due to the additional steps involved in encryption and data verification, which could slow down time-sensitive applications.\nDevelopment Challenges: Developing for TEEs requires specialized knowledge and often must adhere to strict development protocols. This can extend development time and complicate the debugging and testing processes.\nScalability and Flexibility: TEEs, due to their protected nature, may impose limitations on scalability and flexibility. Upgrading protected components or scaling the system for more users or data can be more challenging when everything must pass through a secure, enclosed environment.\nEnergy Consumption: The increased processing required for encryption, decryption, and integrity checks can lead to higher energy consumption, a significant concern for battery-powered devices.\nMarket Demand: Not all markets or applications require the level of security provided by TEEs. For many consumer applications, the perceived risk may be low enough that manufacturers opt not to include TEEs in their designs.\nSecurity Certification and Assurance: Systems with TEEs may need rigorous security certifications with bodies like Common Criteria (CC) or the European Union Agency for Cybersecurity (ENISA), which can be lengthy and expensive. Some organizations may choose to refrain from implementing TEEs to avoid these hurdles.\nLimited Resource Devices: Devices with limited processing power, memory, or storage may only support TEEs without compromising their primary functionality.\n\n\n\n15.6.2 Secure Boot\n\nAbout\nA Secure Boot is a fundamental security standard that ensures a device only boots using software trusted by the device manufacturer. During startup, the firmware checks the digital signature of each boot software component, including the bootloader, kernel, and base operating system. This process verifies that the software has not been altered or tampered with. If any signature fails verification, the boot process is halted to prevent unauthorized code execution that could compromise the system’s security integrity.\n\n\nBenefits\nThe integrity of an embedded ML system is paramount from the moment it is powered on. Any compromise in the boot process can lead to the execution of malicious software before the operating system and ML applications begin, resulting in manipulated ML operations, unauthorized data access, or repurposing the device for malicious activities such as botnets or crypto-mining.\nSecure Boot offers vital protections for embedded ML hardware through the following critical mechanisms:\n\nProtecting ML Data: Ensuring that the data used by ML models, which may include private or sensitive information, is not exposed to tampering or theft during the boot process.\nGuarding Model Integrity: Maintaining the integrity of the ML models is crucial, as tampering with them could lead to incorrect or malicious outcomes.\nSecure Model Updates: Enabling secure updates to ML models and algorithms, ensuring that updates are authenticated and have not been altered.\n\n\n\nMechanics\nSecure Boot works with TEEs to further enhance system security. Figure 15.7 illustrates a flow diagram of a trusted embedded system. In the initial validation phase, Secure Boot verifies that the code running within the TEE is the correct, untampered version authorized by the device manufacturer. By checking digital signatures of the firmware and other critical system components, Secure Boot prevents unauthorized modifications that could compromise the TEE’s security capabilities. This establishes a foundation of trust upon which the TEE can securely execute sensitive operations such as cryptographic key management and secure data processing. By enforcing these layers of security, Secure Boot enables resilient and secure device operations in even the most resource-constrained environments.\n\n\n\n\n\n\nFigure 15.7: Secure Boot flow. Source: R. V. and A. (2018).\n\n\nR. V., Rashmi, and Karthikeyan A. 2018. “Secure Boot of Embedded Applications - a Review.” In 2018 Second International Conference on Electronics, Communication and Aerospace Technology (ICECA), 291–98. IEEE. https://doi.org/10.1109/iceca.2018.8474730.\n\n\n\n\nCase Study: Apple’s Face ID\nA real-world example of Secure Boot’s application can be observed in Apple’s Face ID technology, which uses advanced machine learning algorithms to enable facial recognition on iPhones and iPads. Face ID relies on a sophisticated integration of sensors and software to precisely map the geometry of a user’s face. For Face ID to operate securely and protect users’ biometric data, the device’s operations must be trustworthy from initialization. This is where Secure Boot plays a pivotal role. The following outlines how Secure Boot functions in conjunction with Face ID:\n\nInitial Verification: Upon booting up an iPhone, the Secure Boot process commences within the Secure Enclave, a specialized coprocessor designed to add an extra layer of security. The Secure Enclave handles biometric data, such as fingerprints for Touch ID and facial recognition data for Face ID. During the boot process, the system rigorously verifies that Apple has digitally signed the Secure Enclave’s firmware, guaranteeing its authenticity. This verification step ensures that the firmware used to process biometric data remains secure and uncompromised.\nContinuous Security Checks: Following the system’s initialization and validation by Secure Boot, the Secure Enclave communicates with the device’s central processor to maintain a secure boot chain. During this process, the digital signatures of the iOS kernel and other critical boot components are meticulously verified to ensure their integrity before proceeding. This “chain of trust” model effectively prevents unauthorized modifications to the bootloader and operating system, safeguarding the device’s overall security.\nFace Data Processing: Once the secure boot sequence is completed, the Secure Enclave interacts securely with the machine learning algorithms that power Face ID. Facial recognition involves projecting and analyzing over 30,000 invisible points to create a depth map of the user’s face and an infrared image. This data is converted into a mathematical representation and is securely compared with the registered face data stored in the Secure Enclave.\nSecure Enclave and Data Protection: The Secure Enclave is precisely engineered to protect sensitive data and manage cryptographic operations that safeguard this data. Even in the event of a compromised operating system kernel, the facial data processed through Face ID remains inaccessible to unauthorized applications or external attackers. Importantly, Face ID data is never transmitted off the device and is not stored on iCloud or other external servers.\nFirmware Updates: Apple frequently releases updates to address security vulnerabilities and enhance system functionality. Secure Boot ensures that all firmware updates are authenticated, allowing only those signed by Apple to be installed. This process helps preserve the integrity and security of the Face ID system over time.\n\nBy integrating Secure Boot with dedicated hardware such as the Secure Enclave, Apple delivers robust security guarantees for critical operations like facial recognition.\n\n\nChallenges\nDespite its benefits, implementing Secure Boot presents several challenges, particularly in complex and large-scale deployments: Key Management Complexity: Generating, storing, distributing, rotating, and revoking cryptographic keys provably securely is particularly challenging yet vital for maintaining the chain of trust. Any compromise of keys cripples protections. Large enterprises managing multitudes of device keys face particular scale challenges.\nPerformance Overhead: Checking cryptographic signatures during Boot can add 50-100ms or more per component verified. This delay may be prohibitive for time-sensitive or resource-constrained applications. However, performance impacts can be reduced through parallelization and hardware acceleration.\nSigning Burden: Developers must diligently ensure that all software components involved in the boot process - bootloaders, firmware, OS kernel, drivers, applications, etc. are correctly signed by trusted keys. Accommodating third-party code signing remains an issue.\nCryptographic Verification: Secure algorithms and protocols must validate the legitimacy of keys and signatures, avoid tampering or bypass, and support revocation. Accepting dubious keys undermines trust.\nCustomizability Constraints: Vendor-locked Secure Boot architectures limit user control and upgradability. Open-source bootloaders like u-boot and coreboot enable security while supporting customizability.\nScalable Standards: Emerging standards like Device Identifier Composition Engine (DICE) and IDevID promise to securely provision and manage device identities and keys at scale across ecosystems.\nAdopting Secure Boot requires following security best practices around key management, crypto validation, signed updates, and access control. Secure Boot provides a robust foundation for building device integrity and trust when implemented with care.\n\n\n\n15.6.3 Hardware Security Modules\n\nAbout HSM\nA Hardware Security Module (HSM) is a physical device that manages digital keys for strong authentication and provides crypto-processing. These modules are designed to be tamper-resistant and provide a secure environment for performing cryptographic operations. HSMs can come in standalone devices, plug-in cards, or integrated circuits on another device.\nHSMs are crucial for various security-sensitive applications because they offer a hardened, secure enclave for storing cryptographic keys and executing cryptographic functions. They are particularly important for ensuring the security of transactions, identity verifications, and data encryption.\n\n\nBenefits\nHSMs provide several functionalities that are beneficial for the security of ML systems:\nProtecting Sensitive Data: In machine learning applications, models often process sensitive data that can be proprietary or personal. HSMs protect the encryption keys used to secure this data, both at rest and in transit, from exposure or theft.\nEnsuring Model Integrity: The integrity of ML models is vital for their reliable operation. HSMs can securely manage the signing and verification processes for ML software and firmware, ensuring unauthorized parties have not altered the models.\nSecure Model Training and Updates: The training and updating of ML models involve the processing of potentially sensitive data. HSMs ensure that these processes are conducted within a secure cryptographic boundary, protecting against the exposure of training data and unauthorized model updates.\n\n\nTradeoffs\nHSMs involve several tradeoffs for embedded ML. These tradeoffs are similar to TEEs, but for completeness, we will also discuss them here through the lens of HSM.\nCost: HSMs are specialized devices that can be expensive to procure and implement, raising the overall cost of an ML project. This may be a significant factor for embedded systems, where cost constraints are often stricter.\nPerformance Overhead: While secure, the cryptographic operations performed by HSMs can introduce latency. Any added delay can be critical in high-performance embedded ML applications where inference must happen in real-time, such as in autonomous vehicles or translation devices.\nPhysical Space: Embedded systems are often limited by physical space, and adding an HSM can be challenging in tightly constrained environments. This is especially true for consumer electronics and wearable technology, where size and form factor are key considerations.\nPower Consumption: HSMs require power for their operation, which can be a drawback for battery-operated devices with long battery life. The secure processing and cryptographic operations can drain the battery faster, a significant tradeoff for mobile or remote embedded ML applications.\nComplexity in Integration: Integrating HSMs into existing hardware systems adds complexity. It often requires specialized knowledge to manage the secure communication between the HSM and the system’s processor and develop software capable of interfacing with the HSM.\nScalability: Scaling an ML solution that uses HSMs can be challenging. Managing a fleet of HSMs and ensuring uniformity in security practices across devices can become complex and costly when the deployment size increases, especially when dealing with embedded systems where communication is costly.\nOperational Complexity: HSMs can make updating firmware and ML models more complex. Every update must be signed and possibly encrypted, which adds steps to the update process and may require secure mechanisms for key management and update distribution.\nDevelopment and Maintenance: The secure nature of HSMs means that only limited personnel have access to the HSM for development and maintenance purposes. This can slow down the development process and make routine maintenance more difficult.\nCertification and Compliance: Ensuring that an HSM meets specific industry standards and compliance requirements can add to the time and cost of development. This may involve undergoing rigorous certification processes and audits.\n\n\n\n15.6.4 Physical Unclonable Functions (PUFs)\n\nAbout\nPhysical Unclonable Functions (PUFs) provide a hardware-intrinsic means for cryptographic key generation and device authentication by harnessing the inherent manufacturing variability in semiconductor components. During fabrication, random physical factors such as doping variations, line edge roughness, and dielectric thickness result in microscale differences between semiconductors, even when produced from the same masks. These create detectable timing and power variances that act as a “fingerprint” unique to each chip. PUFs exploit this phenomenon by incorporating integrated circuits to amplify minute timing or power differences into measurable digital outputs.\nWhen stimulated with an input challenge, the PUF circuit produces an output response based on the device’s intrinsic physical characteristics. Due to their physical uniqueness, the same challenge will yield a different response on other devices. This challenge-response mechanism can be used to generate keys securely and identifiers tied to the specific hardware, perform device authentication, or securely store secrets. For example, a key derived from a PUF will only work on that device and cannot be cloned or extracted even with physical access or full reverse engineering (Gao, Al-Sarawi, and Abbott 2020).\n\n\nBenefits\nPUF key generation avoids external key storage, which risks exposure. It also provides a foundation for other hardware security primitives like Secure Boot. Implementation challenges include managing varying reliability and entropy across different PUFs, sensitivity to environmental conditions, and susceptibility to machine learning modeling attacks. When designed carefully, PUFs enable promising applications in IP protection, trusted computing, and anti-counterfeiting.\n\n\nUtility\nMachine learning models are rapidly becoming a core part of the functionality for many embedded devices, such as smartphones, smart home assistants, and autonomous drones. However, securing ML on resource-constrained embedded hardware can be challenging. This is where PUFs come in uniquely handy. Let’s look at some examples of how PUFs can be useful.\nPUFs provide a way to generate unique fingerprints and cryptographic keys tied to the physical characteristics of each chip on the device. Let’s take an example. We have a smart camera drone that uses embedded ML to track objects. A PUF integrated into the drone’s processor could create a device-specific key to encrypt the ML model before loading it onto the drone. This way, even if an attacker somehow hacks the drone and tries to steal the model, they won’t be able to use it on another device!\nThe same PUF key could also create a digital watermark embedded in the ML model. If that model ever gets leaked and posted online by someone trying to pirate it, the watermark could help prove it came from your stolen drone and didn’t originate from the attacker. Also, imagine the drone camera connects to the cloud to offload some of its ML processing. The PUF can authenticate that the camera is legitimate before the cloud will run inference on sensitive video feeds. The cloud could verify that the drone has not been physically tampered with by checking that the PUF responses have not changed.\nPUFs enable all this security through their challenge-response behavior’s inherent randomness and hardware binding. Without needing to store keys externally, PUFs are ideal for securing embedded ML with limited resources. Thus, they offer a unique advantage over other mechanisms.\n\n\nMechanics\nThe working principle behind PUFs, shown in Figure 15.8, involves generating a “challenge-response” pair, where a specific input (the challenge) to the PUF circuit results in an output (the response) that is determined by the unique physical properties of that circuit. This process can be likened to a fingerprinting mechanism for electronic devices. Devices that use ML for processing sensor data can employ PUFs to secure communication between devices and prevent the execution of ML models on counterfeit hardware.\nFigure 15.8 illustrates an overview of the PUF basics: a) PUF can be thought of as a unique fingerprint for each piece of hardware; b) an Optical PUF is a special plastic token that is illuminated, creating a unique speckle pattern that is then recorded; c) in an APUF (Arbiter PUF), challenge bits select different paths, and a judge decides which one is faster, giving a response of ‘1’ or ‘0’; d) in an SRAM PUF, the response is determined by the mismatch in the threshold voltage of transistors, where certain conditions lead to a preferred response of ‘1’. Each of these methods uses specific characteristics of the hardware to create a unique identifier.\n\n\n\n\n\n\nFigure 15.8: PUF basics. Source: Gao, Al-Sarawi, and Abbott (2020).\n\n\nGao, Yansong, Said F. Al-Sarawi, and Derek Abbott. 2020. “Physical Unclonable Functions.” Nature Electronics 3 (2): 81–91. https://doi.org/10.1038/s41928-020-0372-5.\n\n\n\n\nChallenges\nThere are a few challenges with PUFs. The PUF response can be sensitive to environmental conditions, such as temperature and voltage fluctuations, leading to inconsistent behavior that must be accounted for in the design. Also, since PUFs can generate many unique challenge-response pairs, managing and ensuring the consistency of these pairs across the device’s lifetime can be challenging. Last but not least, integrating PUF technology may increase the overall manufacturing cost of a device, although it can save costs in key management over the device’s lifecycle.",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Security & Privacy</span>"
    ]
  },
  {
    "objectID": "contents/core/privacy_security/privacy_security.html#privacy-concerns-in-data-handling",
    "href": "contents/core/privacy_security/privacy_security.html#privacy-concerns-in-data-handling",
    "title": "15  Security & Privacy",
    "section": "15.7 Privacy Concerns in Data Handling",
    "text": "15.7 Privacy Concerns in Data Handling\nHandling personal and sensitive data securely and ethically is critical as machine learning permeates devices like smartphones, wearables, and smart home appliances. For medical hardware, handling data securely and ethically is further required by law through the Health Insurance Portability and Accountability Act (HIPAA). These embedded ML systems pose unique privacy risks, given their intimate proximity to users’ lives.\n\n15.7.1 Sensitive Data Types\nEmbedded ML devices like wearables, smart home assistants, and autonomous vehicles frequently process highly personal data that requires careful handling to maintain user privacy and prevent misuse. Specific examples include medical reports and treatment plans processed by health wearables, private conversations continuously captured by smart home assistants, and detailed driving habits collected by connected cars. Compromise of such sensitive data can lead to serious consequences like identity theft, emotional manipulation, public shaming, and mass surveillance overreach.\nSensitive data takes many forms - structured records like contact lists and unstructured content like conversational audio and video streams. In medical settings, protected health information (PHI) is collected by doctors throughout every interaction and is heavily regulated by strict HIPAA guidelines. Even outside of medical settings, sensitive data can still be collected in the form of Personally Identifiable Information (PII), which is defined as “any representation of information that permits the identity of an individual to whom the information applies to be reasonably inferred by either direct or indirect means.” Examples of PII include email addresses, social security numbers, and phone numbers, among other fields. PII is collected in medical settings and other settings (financial applications, etc) and is heavily regulated by Department of Labor policies.\nEven derived model outputs could indirectly leak details about individuals. Beyond just personal data, proprietary algorithms and datasets also warrant confidentiality protections. In the Data Engineering section, we covered several topics in detail.\nTechniques like de-identification, aggregation, anonymization, and federation can help transform sensitive data into less risky forms while retaining analytical utility. However, diligent controls around access, encryption, auditing, consent, minimization, and compliance practices are still essential throughout the data lifecycle. Regulations like GDPR categorize different classes of sensitive data and prescribe responsibilities around their ethical handling. Standards like NIST 800-53 provide rigorous security control guidance for confidentiality protection. With growing reliance on embedded ML, understanding sensitive data risks is crucial.\n\n\n15.7.2 Applicable Regulations\nMany embedded ML applications handle sensitive user data under HIPAA, GDPR, and CCPA regulations. Understanding the protections mandated by these laws is crucial for building compliant systems.\n\nHIPAA Privacy Rule establishes care providers that conduct certain governs medical data privacy and security in the US, with severe penalties for violations. Any health-related embedded ML devices like diagnostic wearables or assistive robots would need to implement controls like audit trails, access controls, and encryption prescribed by HIPAA.\nGDPR imposes transparency, retention limits, and user rights on EU citizen data, even when processed by companies outside the EU. Smart home systems capturing family conversations or location patterns would need GDPR compliance. Key requirements include data minimization, encryption, and mechanisms for consent and erasure.\nCCPA, which applies in California, protects consumer data privacy through provisions like required disclosures and opt-out rights—ioT gadgets like smart speakers and fitness trackers Californians use likely to fall under its scope.\nThe CCPA was the first state-specific set of regulations regarding privacy concerns. Following the CCPA, similar regulations were also enacted in 10 other states, with some states proposing bills for consumer data privacy protections.\n\nAdditionally, when relevant to the application, sector-specific rules govern telematics, financial services, utilities, etc. Best practices like Privacy by design, impact assessments, and maintaining audit trails help embed compliance if it is not already required by law. Given potentially costly penalties, consulting legal/compliance teams is advisable when developing regulated embedded ML systems.\n\n\n15.7.3 De-identification\nIf medical data is de-identified thoroughly, HIPAA guidelines do not directly apply, and there are far fewer regulations. However, medical data needs to be de-identified using HIPAA methods (Safe Harbor methods or Expert Determination methods) for HIPAA guidelines to no longer apply.\n\nSafe Harbor Methods\nSafe Harbor methods are most commonly used for de-identifying protected healthcare information due to the limited resources needed compared to Expert Determination methods. Safe Harbor de-identification requires scrubbing datasets of any data that falls into one of 18 categories. The following categories are listed as sensitive information based on the Safe Harbor standard:\n\nName, Geographic locator, Birthdate, Phone Number, Email Address, addresses, Social Security Numbers, Medical Record Numbers, health beneficiary Numbers, Device Identifiers and Serial Numbers, Certificate/License Numbers (Birth Certificate, Drivers License, etc), Account Numbers, Vehicle Identifiers, Website URLs, FullFace Photos and Comparable Images, Biometric Identifiers, Any other unique identifiers\n\nFor most of these categories, all data must be removed regardless of the circumstances. For other categories, including geographical information and birthdate, the data can be partially removed enough to make the information hard to re-identify. For example, if a zip code is large enough, the first 3 digits can remain since there are enough people in the geographic area to make re-identification difficult. Birthdates need to be scrubbed of all elements except birth year, and all ages above 89 need to be aggregated into a 90+ category.\n\n\nExpert Determination Methods\nSafe Harbor methods work for several cases of medical data de-identification, though re-identification is still possible in some cases. For example, let’s say you collect data on a patient in an urban city with a large zip code, but you have documented a rare disease that they have—a disease that only 25 people have in the entire city. Given geographic data coupled with birth year, it is highly possible that someone can re-identify this individual, which is an extremely detrimental privacy breach.\nIn unique cases like these, expert determination data de-identification methods are preferred. Expert determination de-identification requires a “person with appropriate knowledge of and experience with generally accepted statistical and scientific principles and methods for rendering information not individually identifiable” to evaluate a dataset and determine if the risk of re-identification of individual data in a given dataset in combination with publicly available data (voting records, etc.), is extremely small.\nExpert Determination de-identification is understandably harder to complete than Safe Harbour de-identification due to the cost and feasibility of accessing an expert to verify the likelihood of re-identifying a dataset. However, in many cases, expert determination is required to ensure that re-identification of data is extremely unlikely.\n\n\n\n15.7.4 Data Minimization\nData minimization involves collecting, retaining, and processing only the necessary user data to reduce privacy risks from embedded ML systems. This starts by restricting the data types and instances gathered to the bare minimum required for the system’s core functionality. For example, an object detection model only collects the images needed for that specific computer vision task. Similarly, a voice assistant would limit audio capture to specific spoken commands rather than persistently recording ambient sounds.\nWhere possible, temporary data that briefly resides in memory without persisting storage provides additional minimization. A clear legal basis, like user consent, should be established for collection and retention. Sandboxing and access controls prevent unauthorized use beyond intended tasks. Retention periods should be defined based on purpose, with secure deletion procedures removing expired data.\nData minimization can be broken down into 3 categories:\n\n“Data must be adequate about the purpose that is pursued.” Data omission can limit the accuracy of models trained on the data and any general usefulness of a dataset. Data minimization requires a minimum amount of data to be collected from users while creating a dataset that adds value to others.\nThe data collected from users must be relevant to the purpose of the data collection.\nUsers’ data should be limited to only the necessary data to fulfill the purpose of the initial data collection. If similarly robust and accurate results can be obtained from a smaller dataset, any additional data beyond this smaller dataset should not be collected.\n\nEmerging techniques like differential privacy, federated learning, and synthetic data generation allow useful insights derived from less raw user data. Performing data flow mapping and impact assessments helps identify opportunities to minimize raw data usage.\nMethodologies like Privacy by Design (Cavoukian 2009) consider such minimization early in system architecture. Regulations like GDPR also mandate data minimization principles. With a multilayered approach across legal, technical, and process realms, data minimization limits risks in embedded ML products.\n\nCavoukian, Ann. 2009. “Privacy by Design.” Office of the Information and Privacy Commissioner.\n\nCase Study: Performance-Based Data Minimization\nPerformance-based data minimization (Biega et al. 2020) focuses on expanding upon the third category of data minimization mentioned above, namely limitation. It specifically defines the robustness of model results on a given dataset by certain performance metrics, such that data should not be additionally collected if it does not significantly improve performance. Performance metrics can be divided into two categories:\n\nBiega, Asia J., Peter Potash, Hal Daumé, Fernando Diaz, and Michèle Finck. 2020. “Operationalizing the Legal Principle of Data Minimization for Personalization.” In Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval, edited by Jimmy Huang, Yi Chang, Xueqi Cheng, Jaap Kamps, Vanessa Murdock, Ji-Rong Wen, and Yiqun Liu, 399–408. ACM. https://doi.org/10.1145/3397271.3401034.\n\nGlobal data minimization performance: Satisfied if a dataset minimizes the amount of per-user data while its mean performance across all data is comparable to the mean performance of the original, unminimized dataset.\nPer user data minimization performance: Satisfied if a dataset minimizes the amount of per-user data while the minimum performance of individual user data is comparable to that of individual user data in the original, unminimized dataset.\n\nPerformance-based data minimization can be leveraged in machine-learning settings, including movie recommendation algorithms and e-commerce settings.\nGlobal data minimization is much more feasible than per-user data minimization, given the much more significant difference in per-user losses between the minimized and original datasets.\n\n\n\n15.7.5 Consent and Transparency\nMeaningful consent and transparency are crucial when collecting user data for embedded ML products like smart speakers, wearables, and autonomous vehicles. When first set up. Ideally, the device should clearly explain what data types are gathered, for what purposes, how they are processed, and retention policies. For example, a smart speaker might collect voice samples to train speech recognition and personalized voice profiles. During use, reminders and dashboard options provide ongoing transparency into how data is handled, such as weekly digests of captured voice snippets. Control options allow revoking or limiting consent, like turning off the storage of voice profiles.\nConsent flows should provide granular controls beyond just binary yes/no choices. For instance, users could selectively consent to certain data uses, such as training speech recognition, but not personalization. Focus groups and usability testing with target users shape consent interfaces and wording of privacy policies to optimize comprehension and control. Respecting user rights, such as data deletion and rectification, demonstrates trustworthiness. Vague legal jargon hampers transparency. Regulations like GDPR and CCPA reinforce consent requirements. Thoughtful consent and transparency provide users agency over their data while building trust in embedded ML products through open communication and control.\n\n\n15.7.6 Privacy Concerns in Machine Learning\n\nGenerative AI\nPrivacy and security concerns have also risen with the public use of generative AI models, including OpenAI’s GPT4 and other LLMs. ChatGPT, in particular, has been discussed more recently about Privacy, given all the personal information collected from ChatGPT users. In June 2023, a class action lawsuit was filed against ChatGPT due to concerns that it was trained on proprietary medical and personal information without proper permissions or consent. As a result of these privacy concerns, many companies have prohibited their employees from accessing ChatGPT, and uploading private, company related information to the chatbot. Further, ChatGPT is susceptible to prompt injection and other security attacks that could compromise the privacy of the proprietary data upon which it was trained.\n\nCase Study: Bypassing ChatGPT Safeguards\nWhile ChatGPT has instituted protections to prevent people from accessing private and ethically questionable information, several individuals have successfully bypassed these protections through prompt injection and other security attacks. As demonstrated in Figure 15.9, users can bypass ChatGPT protections to mimic the tone of a “deceased grandmother” to learn how to bypass a web application firewall (Gupta et al. 2023).\n\n\n\n\n\n\nFigure 15.9: Grandma role play to bypass safety restrictions. Source: Gupta et al. (2023).\n\n\n\nFurther, users have also successfully used reverse psychology to manipulate ChatGPT and access information initially prohibited by the model. In Figure 15.10, a user is initially prevented from learning about piracy websites through ChatGPT but can bypass these restrictions using reverse psychology.\n\n\n\n\n\n\nFigure 15.10: Reverse psychology to bypass safety restrictions. Source: Gupta et al. (2023).\n\n\nGupta, Maanak, Charankumar Akiri, Kshitiz Aryal, Eli Parker, and Lopamudra Praharaj. 2023. “From ChatGPT to ThreatGPT: Impact of Generative AI in Cybersecurity and Privacy.” IEEE Access 11: 80218–45. https://doi.org/10.1109/access.2023.3300381.\n\n\nThe ease at which security attacks can manipulate ChatGPT is concerning, given the private information it was trained upon without consent. Further research on data privacy in LLMs and generative AI should focus on preventing the model from being so naive to prompt injection attacks.\n\n\n\nData Erasure\nMany previous regulations mentioned above, including GDPR, include a “right to be forgotten” clause. This clause essentially states that “the data subject shall have the right to obtain from the controller the erasure of personal data concerning him or her without undue delay.” However, in several cases, even if user data has been erased from a platform, the data is only partially erased if a machine learning model has been trained on this data for separate purposes. Through methods similar to membership inference attacks, other individuals can still predict the training data a model was trained upon, even if the data’s presence was explicitly removed online.\nOne approach to addressing privacy concerns with machine learning training data has been through differential privacy methods. For example, by adding Laplacian noise in the training set, a model can be robust to membership inference attacks, preventing deleted data from being recovered. Another approach to preventing deleted data from being inferred from security attacks is simply retraining the model from scratch on the remaining data. Since this process is time-consuming and computationally expensive, other researchers have attempted to address privacy concerns surrounding inferring model training data through a process called machine unlearning, in which a model actively iterates on itself to remove the influence of “forgotten” data that it might have been trained on, as mentioned below.",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Security & Privacy</span>"
    ]
  },
  {
    "objectID": "contents/core/privacy_security/privacy_security.html#privacy-preserving-ml-techniques",
    "href": "contents/core/privacy_security/privacy_security.html#privacy-preserving-ml-techniques",
    "title": "15  Security & Privacy",
    "section": "15.8 Privacy-Preserving ML Techniques",
    "text": "15.8 Privacy-Preserving ML Techniques\nMany techniques have been developed to preserve privacy, each addressing different aspects and data security challenges. These methods can be broadly categorized into several key areas: Differential Privacy, which focuses on statistical privacy in data outputs; Federated Learning, emphasizing decentralized data processing; Homomorphic Encryption and Secure Multi-party Computation (SMC), both enabling secure computations on encrypted or private data; Data Anonymization and Data Masking and Obfuscation, which alter data to protect individual identities; Private Set Intersection and Zero-Knowledge Proofs, facilitating secure data comparisons and validations; Decentralized Identifiers (DIDs) for self-sovereign digital identities; Privacy-Preserving Record Linkage (PPRL), linking data across sources without exposure; Synthetic Data Generation, creating artificial datasets for safe analysis; and Adversarial Learning Techniques, enhancing data or model resistance to privacy attacks.\nGiven the extensive range of these techniques, it is not feasible to dive into each in depth within a single course or discussion, let alone for anyone to know it all in its glorious detail. Therefore, we will explore a few specific techniques in relative detail, providing a deeper understanding of their principles, applications, and the unique privacy challenges they address in machine learning. This focused approach will give us a more comprehensive and practical understanding of key privacy-preserving methods in modern ML systems.\n\n15.8.1 Differential Privacy\n\nCore Idea\nDifferential Privacy is a framework for quantifying and managing the privacy of individuals in a dataset (Dwork et al. 2006). It provides a mathematical guarantee that the privacy of individuals in the dataset will not be compromised, regardless of any additional knowledge an attacker may possess. The core idea of differential Privacy is that the outcome of any analysis (like a statistical query) should be essentially the same, whether any individual’s data is included in the dataset or not. This means that by observing the analysis result, one cannot determine whether any individual’s data was used in the computation.\n\nDwork, Cynthia, Frank McSherry, Kobbi Nissim, and Adam Smith. 2006. “Calibrating Noise to Sensitivity in Private Data Analysis.” In Theory of Cryptography, edited by Shai Halevi and Tal Rabin, 265–84. Berlin, Heidelberg: Springer Berlin Heidelberg. https://doi.org/10.1007/11681878\\_14.\nFor example, let’s say a database contains medical records for 10 patients. We want to release statistics about the prevalence of diabetes in this sample without revealing one patient’s condition. To do this, we could add a small amount of random noise to the true count before releasing it. If the true number of diabetes patients is 6, we might add noise from a Laplace distribution to randomly output 5, 6, or 7 each with some probability. An observer now can’t tell if any single patient has diabetes based only on the noisy output. The query result looks similar to whether each patient’s data is included or excluded. This is differential Privacy. More formally, a randomized algorithm satisfies ε-differential Privacy if, for any neighbor databases D and Dʹ differing by only one entry, the probability of any outcome changes by at most a factor of ε. A lower ε provides stronger privacy guarantees.\nThe Laplace Mechanism is one of the most straightforward and commonly used methods to achieve differential Privacy. It involves adding noise that follows a Laplace distribution to the data or query results. Apart from the Laplace Mechanism, the general principle of adding noise is central to differential Privacy. The idea is to add random noise to the data or the results of a query. The noise is calibrated to ensure the necessary privacy guarantee while keeping the data useful.\nWhile the Laplace distribution is common, other distributions like Gaussian can also be used. Laplace noise is used for strict ε-differential Privacy for low-sensitivity queries. In contrast, Gaussian distributions can be used when Privacy is not guaranteed, known as (ϵ, 𝛿)-Differential Privacy. In this relaxed version of differential Privacy, epsilon and delta define the amount of Privacy guaranteed when releasing information or a model related to a dataset. Epsilon sets a bound on how much information can be learned about the data based on the output. At the same time, delta allows for a small probability of the privacy guarantee to be violated. The choice between Laplace, Gaussian, and other distributions will depend on the specific requirements of the query and the dataset and the tradeoff between Privacy and accuracy.\nTo illustrate the tradeoff of Privacy and accuracy in (\\(\\epsilon\\), \\(\\delta\\))-differential Privacy, the following graphs in Figure 15.11 show the results on accuracy for different noise levels on the MNIST dataset, a large dataset of handwritten digits (Abadi et al. 2016). The delta value (black line; right y-axis) denotes the level of privacy relaxation (a high value means Privacy is less stringent). As Privacy becomes more relaxed, the accuracy of the model increases.\n\n\n\n\n\n\nFigure 15.11: Privacy-accuracy tradeoff. Source: Abadi et al. (2016).\n\n\nAbadi, Martin, Andy Chu, Ian Goodfellow, H. Brendan McMahan, Ilya Mironov, Kunal Talwar, and Li Zhang. 2016. “Deep Learning with Differential Privacy.” In Proceedings of the 2016 ACM SIGSAC Conference on Computer and Communications Security, 308–18. CCS ’16. New York, NY, USA: ACM. https://doi.org/10.1145/2976749.2978318.\n\n\nThe key points to remember about differential Privacy are the following:\n\nAdding Noise: The fundamental technique in differential Privacy is adding controlled random noise to the data or query results. This noise masks the contribution of individual data points.\nBalancing Act: There’s a balance between Privacy and accuracy. More noise (lower ϵ) in the data means higher Privacy but less accuracy in the model’s results.\nUniversality: Differential Privacy doesn’t rely on assumptions about what an attacker knows. This makes it robust against re-identification attacks, where an attacker tries to uncover individual data.\nApplicability: It can be applied to various types of data and queries, making it a versatile tool for privacy-preserving data analysis.\n\n\n\nTradeoffs\nThere are several tradeoffs to make with differential Privacy, as is the case with any algorithm. But let’s focus on the computational-specific tradeoffs since we care about ML systems. There are some key computational considerations and tradeoffs when implementing differential Privacy in a machine-learning system:\nNoise generation: Implementing differential Privacy introduces several important computational tradeoffs compared to standard machine learning techniques. One major consideration is the need to securely generate random noise from distributions like Laplace or Gaussian that get added to query results and model outputs. High-quality cryptographic random number generation can be computationally expensive.\nSensitivity analysis: Another key requirement is rigorously tracking the sensitivity of the underlying algorithms to single data points getting added or removed. This global sensitivity analysis is required to calibrate the noise levels properly. However, analyzing worst-case sensitivity can substantially increase computational complexity for complex model training procedures and data pipelines.\nPrivacy budget management: Managing the privacy loss budget across multiple queries and learning iterations is another bookkeeping overhead. The system must keep track of cumulative privacy costs and compose them to explain overall privacy guarantees. This adds a computational burden beyond just running queries or training models.\nBatch vs. online tradeoffs: For online learning systems with continuous high-volume queries, differentially private algorithms require new mechanisms to maintain utility and prevent too much accumulated privacy loss since each query can potentially alter the privacy budget. Batch offline processing is simpler from a computational perspective as it processes data in large batches, where each batch is treated as a single query. High-dimensional sparse data also increases sensitivity analysis challenges.\nDistributed training: When training models using distributed or federated approaches, new cryptographic protocols are needed to track and bound privacy leakage across nodes. Secure multiparty computation with encrypted data for differential Privacy adds substantial computational load.\nWhile differential Privacy provides strong formal privacy guarantees, implementing it rigorously requires additions and modifications to the machine learning pipeline at a computational cost. Managing these overheads while preserving model accuracy remains an active research area.\n\n\nCase Study: Differential Privacy at Apple\nApple’s implementation of differential Privacy in iOS and MacOS provides a prominent real-world example of how differential Privacy can be deployed at large scale. Apple wanted to collect aggregated usage statistics across their ecosystem to improve products and services, but aimed to do so without compromising individual user privacy.\nTo achieve this, they implemented differential privacy techniques directly on user devices to anonymize data points before sending them to Apple servers. Specifically, Apple uses the Laplace mechanism to inject carefully calibrated random noise. For example, suppose a user’s location history contains [Work, Home, Work, Gym, Work, Home]. In that case, the differentially private version might replace the exact locations with a noisy sample like [Gym, Home, Work, Work, Home, Work].\nApple tunes the Laplace noise distribution to provide a high level of Privacy while preserving the utility of aggregated statistics. Increasing noise levels provides stronger privacy guarantees (lower ε values in DP terminology) but can reduce data utility. Apple’s privacy engineers empirically optimized this tradeoff based on their product goals.\nApple obtains high-fidelity aggregated statistics by aggregating hundreds of millions of noisy data points from devices. For instance, they can analyze new iOS apps’ features while masking any user’s app behaviors. On-device computation avoids sending raw data to Apple servers.\nThe system uses hardware-based secure random number generation to sample from the Laplace distribution on devices efficiently. Apple also had to optimize its differentially private algorithms and pipeline to operate under the computational constraints of consumer hardware.\nMultiple third-party audits have verified that Apple’s system provides rigorous differential privacy protections in line with their stated policies. Of course, assumptions around composition over time and potential re-identification risks still apply. Apple’s deployment shows how differential Privacy can be realized in large real-world products when backed by sufficient engineering resources.\n\n\n\n\n\n\nExercise 15.1: Differential Privacy - TensorFlow Privacy\n\n\n\n\n\nWant to train an ML model without compromising anyone’s secrets? Differential Privacy is like a superpower for your data! In this Colab, we’ll use TensorFlow Privacy to add special noise during training. This makes it way harder for anyone to determine if a single person’s data was used, even if they have sneaky ways of peeking at the model.\n\n\n\n\n\n\n\n15.8.2 Federated Learning\n\nCore Idea\nFederated Learning (FL) is a type of machine learning in which a model is built and distributed across multiple devices or servers while keeping the training data localized. It was previously discussed in the Model Optimizations chapter. Still, we will recap it here briefly to complete it and focus on things that pertain to this chapter.\nFL trains machine learning models across decentralized networks of devices or systems while keeping all training data localized. Figure 15.12 illustrates this process: each participating device leverages its local data to calculate model updates, which are then aggregated to build an improved global model. However, the raw training data is never directly shared, transferred, or compiled. This privacy-preserving approach allows for the joint development of ML models without centralizing the potentially sensitive training data in one place.\n\n\n\n\n\n\nFigure 15.12: Federated Learning lifecycle. Source: Jin et al. (2020).\n\n\nJin, Yilun, Xiguang Wei, Yang Liu, and Qiang Yang. 2020. “Towards Utilizing Unlabeled Data in Federated Learning: A Survey and Prospective.” arXiv Preprint arXiv:2002.11545, February. http://arxiv.org/abs/2002.11545v2.\n\n\nOne of the most common model aggregation algorithms is Federated Averaging (FedAvg), where the global model is created by averaging all of the parameters from local parameters. While FedAvg works well with independent and identically distributed data (IID), alternate algorithms like Federated Proximal (FedProx) are crucial in real-world applications where data is often non-IID. FedProx is designed for the FL process when there is significant heterogeneity in the client updates due to diverse data distributions across devices, computational capabilities, or varied amounts of data.\nBy leaving the raw data distributed and exchanging only temporary model updates, federated learning provides a more secure and privacy-enhancing alternative to traditional centralized machine learning pipelines. This allows organizations and users to benefit collaboratively from shared models while maintaining control and ownership over sensitive data. The decentralized nature of FL also makes it robust to single points of failure.\nImagine a group of hospitals that want to collaborate on a study to predict patient outcomes based on their symptoms. However, they cannot share their patient data due to privacy concerns and regulations like HIPAA. Here’s how Federated Learning can help.\n\nLocal Training: Each hospital trains a machine learning model on patient data. This training happens locally, meaning the data never leaves the hospital’s servers.\nModel Sharing: After training, each hospital only sends the model (specifically, its parameters or weights ) to a central server. It does not send any patient data.\nAggregating Models: The central server aggregates these models from all hospitals into a single, more robust model. This process typically involves averaging the model parameters.\nBenefit: The result is a machine learning model that has learned from a wide range of patient data without sharing sensitive data or removing it from its original location.\n\n\n\nTradeoffs\nThere are several system performance-related aspects of FL in machine learning systems. It would be wise to understand these tradeoffs because there is no “free lunch” for preserving Privacy through FL (Li et al. 2020).\n\nLi, Tian, Anit Kumar Sahu, Ameet Talwalkar, and Virginia Smith. 2020. “Federated Learning: Challenges, Methods, and Future Directions.” IEEE Signal Processing Magazine 37 (3): 50–60. https://doi.org/10.1109/msp.2020.2975749.\nCommunication Overhead and Network Constraints: In FL, one of the most significant challenges is managing the communication overhead. This involves the frequent transmission of model updates between a central server and numerous client devices, which can be bandwidth-intensive. The total number of communication rounds and the size of transmitted messages per round need to be reduced to minimize communication further. This can lead to substantial network traffic, especially in scenarios with many participants. Additionally, latency becomes a critical factor — the time taken for these updates to be sent, aggregated, and redistributed can introduce delays. This affects the overall training time and impacts the system’s responsiveness and real-time capabilities. Managing this communication while minimizing bandwidth usage and latency is crucial for implementing FL.\nComputational Load on Local Devices: FL relies on client devices (like smartphones or IoT devices, which especially matter in TinyML) for model training, which often have limited computational power and battery life. Running complex machine learning algorithms locally can strain these resources, leading to potential performance issues. Moreover, the capabilities of these devices can vary significantly, resulting in uneven contributions to the model training process. Some devices process updates faster and more efficiently than others, leading to disparities in the learning process. Balancing the computational load to ensure consistent participation and efficiency across all devices is a key challenge in FL.\nModel Training Efficiency: FL’s decentralized nature can impact model training’s efficiency. Achieving convergence, where the model no longer significantly improves, can be slower in FL than in centralized training methods. This is particularly true in cases where the data is non-IID (non-independent and identically distributed) across devices. Additionally, the algorithms used for aggregating model updates play a critical role in the training process. Their efficiency directly affects the speed and effectiveness of learning. Developing and implementing algorithms that can handle the complexities of FL while ensuring timely convergence is essential for the system’s performance.\nScalability Challenges: Scalability is a significant concern in FL, especially as the number of participating devices increases. Managing and coordinating model updates from many devices adds complexity and can strain the system. Ensuring that the system architecture can efficiently handle this increased load without degrading performance is crucial. This involves not just handling the computational and communication aspects but also maintaining the quality and consistency of the model as the scale of the operation grows. A key challenge is designing FL systems that scale effectively while maintaining performance.\nData Synchronization and Consistency: Ensuring data synchronization and maintaining model consistency across all participating devices in FL is challenging. Keeping all devices synchronized with the latest model version can be difficult in environments with intermittent connectivity or devices that go offline periodically. Furthermore, maintaining consistency in the learned model, especially when dealing with a wide range of devices with different data distributions and update frequencies, is crucial. This requires sophisticated synchronization and aggregation strategies to ensure that the final model accurately reflects the learnings from all devices.\nEnergy Consumption: The energy consumption of client devices in FL is a critical factor, particularly for battery-powered devices like smartphones and other TinyML/IoT devices. The computational demands of training models locally can lead to significant battery drain, which might discourage continuous participation in the FL process. Balancing the computational requirements of model training with energy efficiency is essential. This involves optimizing algorithms and training processes to reduce energy consumption while achieving effective learning outcomes. Ensuring energy-efficient operation is key to user acceptance and the sustainability of FL systems.\n\n\nCase Study: Federated Learning for Collaborative Healthcare Datasets\nIn healthcare and pharmaceuticals, organizations often hold vast amounts of valuable data, but sharing it directly is fraught with challenges. Strict regulations like GDPR and HIPAA, as well as concerns about protecting IP, make combining datasets across companies nearly impossible. However, collaboration remains essential for advancing fields like drug discovery and patient care. Federated learning offers a unique solution by allowing companies to collaboratively train machine learning models without ever sharing their raw data. This approach ensures that each organization retains full control of its data while still benefiting from the collective insights of the group.\nThe MELLODDY project, a landmark initiative in Europe, exemplifies how federated learning can overcome these barriers (Heyndrickx et al. 2023). MELLODDY brought together ten pharmaceutical companies to create the largest shared chemical compound library ever assembled, encompassing over 21 million molecules and 2.6 billion experimental data points. Despite working with sensitive and proprietary data, the companies securely collaborated to improve predictive models for drug development.\n\nHeyndrickx, Wouter, Lewis Mervin, Tobias Morawietz, Noé Sturm, Lukas Friedrich, Adam Zalewski, Anastasia Pentina, et al. 2023. “Melloddy: Cross-Pharma Federated Learning at Unprecedented Scale Unlocks Benefits in Qsar Without Compromising Proprietary Information.” Journal of Chemical Information and Modeling 64 (7): 2331–44. https://pubs.acs.org/doi/10.1021/acs.jcim.3c00799.\nThe results were remarkable. By pooling insights through federated learning, each company significantly enhanced its ability to identify promising drug candidates. Predictive accuracy improved while the models also gained broader applicability to diverse datasets. MELLODDY demonstrated that federated learning not only preserves privacy but also unlocks new opportunities for innovation by enabling large-scale, data-driven collaboration. This approach highlights a future where companies can work together to solve complex problems without sacrificing data security or ownership.\n\n\n\n15.8.3 Machine Unlearning\n\nCore Idea\nMachine unlearning is a fairly new process that describes how the influence of a subset of training data can be removed from the model. Several methods have been used to perform machine unlearning and remove the influence of a subset of training data from the final model. A baseline approach might consist of simply fine-tuning the model for more epochs on just the data that should be remembered to decrease the influence of the data “forgotten” by the model. Since this approach doesn’t explicitly remove the influence of data that should be erased, membership inference attacks are still possible, so researchers have adopted other approaches to unlearn data from a model explicitly. One type of approach that researchers have adopted includes adjusting the model loss function to treat the losses of the “forget set explicitly” (data to be unlearned) and the “retain set” (remaining data that should still be remembered) differently (Tarun et al. 2022; Khan and Swaroop 2021). Figure 15.13 illustrates some of the applications of Machine-unlearning.\n\nTarun, Ayush K, Vikram S Chundawat, Murari Mandal, and Mohan Kankanhalli. 2022. “Deep Regression Unlearning.” ArXiv Preprint abs/2210.08196 (October). http://arxiv.org/abs/2210.08196v2.\n\nKhan, Mohammad Emtiyaz, and Siddharth Swaroop. 2021. “Knowledge-Adaptation Priors.” In Advances in Neural Information Processing Systems 34: Annual Conference on Neural Information Processing Systems 2021, NeurIPS 2021, December 6-14, 2021, Virtual, edited by Marc’Aurelio Ranzato, Alina Beygelzimer, Yann N. Dauphin, Percy Liang, and Jennifer Wortman Vaughan, 19757–70. https://proceedings.neurips.cc/paper/2021/hash/a4380923dd651c195b1631af7c829187-Abstract.html.\n\n\n\n\n\n\nFigure 15.13: Applications of Machine Unlearning. Source: BBVA OpenMind\n\n\n\n\n\nCase Study: The Harry Potter Experiment\nSome researchers have demonstrated a real-life example of machine unlearning approaches applied to SOTA machine learning models through training an LLM, LLaMA2-7b, to unlearn any references to Harry Potter (Eldan and Russinovich 2023). Though this model took 184K GPU hours to pre-train, it only took 1 GPU hour of fine-tuning to erase the model’s ability to generate or recall Harry Potter-related content without noticeably compromising the accuracy of generating content unrelated to Harry Potter. Figure 15.14 demonstrates how the model output changes before (Llama-7b-chat-hf column) and after (Finetuned Llama-b column) unlearning has occurred.\n\n\n\n\n\n\nFigure 15.14: Llama unlearning Harry Potter. Source: Eldan and Russinovich (2023).\n\n\nEldan, Ronen, and Mark Russinovich. 2023. “Who’s Harry Potter? Approximate Unlearning in LLMs.” ArXiv Preprint abs/2310.02238 (October). http://arxiv.org/abs/2310.02238v2.\n\n\n\n\nOther Uses\n\nRemoving adversarial data\nDeep learning models have previously been shown to be vulnerable to adversarial attacks, in which the attacker generates adversarial data similar to the original training data, where a human cannot tell the difference between the real and fabricated data. The adversarial data results in the model outputting incorrect predictions, which could have detrimental consequences in various applications, including healthcare diagnosis predictions. Machine unlearning has been used to unlearn the influence of adversarial data to prevent these incorrect predictions from occurring and causing any harm.\n\n\n\n\n15.8.4 Homomorphic Encryption\n\nCore Idea\nHomomorphic encryption is a form of encryption that allows computations to be carried out on ciphertext, generating an encrypted result that, when decrypted, matches the result of operations performed on the plaintext. For example, multiplying two numbers encrypted with homomorphic encryption produces an encrypted product that decrypts the actual product of the two numbers. This means that data can be processed in an encrypted form, and only the resulting output needs to be decrypted, significantly enhancing data security, especially for sensitive information.\nHomomorphic encryption enables outsourced computation on encrypted data without exposing the data itself to the external party performing the operations. However, only certain computations like addition and multiplication are supported in partially homomorphic schemes. Fully Homomorphic Encryption (FHE) that can handle any computation is even more complex. The number of possible operations is limited before noise accumulation corrupts the ciphertext.\nTo use homomorphic encryption across different entities, carefully generated public keys must be exchanged for operations across separately encrypted data. This advanced encryption technique enables previously impossible secure computation paradigms but requires expertise to implement correctly for real-world systems.\n\n\nBenefits\nHomomorphic encryption enables machine learning model training and inference on encrypted data, ensuring that sensitive inputs and intermediate values remain confidential. This is critical in healthcare, finance, genetics, and other domains, which are increasingly relying on ML to analyze sensitive and regulated data sets containing billions of personal records.\nHomomorphic encryption thwarts attacks like model extraction and membership inference that could expose private data used in ML workflows. It provides an alternative to TEEs using hardware enclaves for confidential computing. However, current schemes have high computational overheads and algorithmic limitations that constrain real-world applications.\nHomomorphic encryption realizes the decades-old vision of secure multiparty computation by allowing computation on ciphertexts. Conceptualized in the 1970s, the first fully homomorphic cryptosystems emerged in 2009, enabling arbitrary computations. Ongoing research is making these techniques more efficient and practical.\nHomomorphic encryption shows great promise in enabling privacy-preserving machine learning under emerging data regulations. However, given constraints, one should carefully evaluate its applicability against other confidential computing approaches. Extensive resources exist to explore homomorphic encryption and track progress in easing adoption barriers.\n\n\nMechanics\n\nData Encryption: Before data is processed or sent to an ML model, it is encrypted using a homomorphic encryption scheme and public key. For example, encrypting numbers \\(x\\) and \\(y\\) generates ciphertexts \\(E(x)\\) and \\(E(y)\\).\nComputation on Ciphertext: The ML algorithm processes the encrypted data directly. For instance, multiplying the ciphertexts \\(E(x)\\) and \\(E(y)\\) generates \\(E(xy)\\). More complex model training can also be done on ciphertexts.\nResult Encryption: The result \\(E(xy)\\) remains encrypted and can only be decrypted by someone with the corresponding private key to reveal the actual product \\(xy\\).\n\nOnly authorized parties with the private key can decrypt the final outputs, protecting the intermediate state. However, noise accumulates with each operation, preventing further computation without decryption.\nBeyond healthcare, homomorphic encryption enables confidential computing for applications like financial fraud detection, insurance analytics, genetics research, and more. It offers an alternative to techniques like multiparty computation and TEEs. Ongoing research improves the efficiency and capabilities.\nTools like HElib, SEAL, and TensorFlow HE provide libraries for exploring implementing homomorphic encryption in real-world machine learning pipelines.\n\n\nTradeoffs\nFor many real-time and embedded applications, fully homomorphic encryption remains impractical for the following reasons.\nComputational Overhead: Homomorphic encryption imposes very high computational overheads, often resulting in slowdowns of over 100x for real-world ML applications. This makes it impractical for many time-sensitive or resource-constrained uses. Optimized hardware and parallelization can alleviate but not eliminate this issue.\nComplexity of Implementation The sophisticated algorithms require deep expertise in cryptography to be implemented correctly. Nuances like format compatibility with floating point ML models and scalable key management pose hurdles. This complexity hinders widespread practical adoption.\nAlgorithmic Limitations: Current schemes restrict the functions and depth of computations supported, limiting the models and data volumes that can be processed. Ongoing research is pushing these boundaries, but restrictions remain.\nHardware Acceleration: Homomorphic encryption requires specialized hardware, such as secure processors or coprocessors with TEEs, which adds design and infrastructure costs.\nHybrid Designs: Rather than encrypting entire workflows, selective application of homomorphic encryption to critical subcomponents can achieve protection while minimizing overheads.\n\n\n\n\n\n\nExercise 15.2: Homomorphic Encryption\n\n\n\n\n\nThe power of encrypted computation is unlocked through homomorphic encryption – a transformative approach in which calculations are performed directly on encrypted data, ensuring privacy is preserved throughout the process. This Colab explores the principles of computing on encrypted numbers without exposing the underlying data. Imagine a scenario where a machine learning model is trained on data that cannot be directly accessed – such is the strength of homomorphic encryption.\n\n\n\n\n\n\n\n15.8.5 Secure Multiparty Communication\n\nCore Idea\nMulti-Party Communication (MPC) enables multiple parties to jointly compute a function over their inputs while ensuring that each party’s inputs remain confidential. For instance, two organizations can collaborate on training a machine learning model by combining datasets without revealing sensitive information to each other. MPC protocols are essential where privacy and confidentiality regulations restrict direct data sharing, such as in healthcare or financial sectors.\nMPC divides computation into parts that each participant executes independently using their private data. These results are then combined to reveal only the final output, preserving the privacy of intermediate values. Cryptographic techniques are used to guarantee that the partial results remain private provably.\nLet’s take a simple example of an MPC protocol. One of the most basic MPC protocols is the secure addition of two numbers. Each party splits its input into random shares that are secretly distributed. They exchange the shares and locally compute the sum of the shares, which reconstructs the final sum without revealing the individual inputs. For example, if Alice has input x and Bob has input y:\n\nAlice generates random \\(x_1\\) and sets \\(x_2 = x - x_1\\)\nBob generates random \\(y_1\\) and sets \\(y_2 = y - y_1\\)\nAlice sends \\(x_1\\) to Bob, Bob sends \\(y_1\\) to Alice (keeping \\(x_2\\) and \\(y_2\\) secret)\nAlice computes \\(x_2 + y_1 = s_1\\), Bob computes \\(x_1 + y_2 = s_2\\)\n\\(s_1 + s_2 = x + y\\) is the final sum, without revealing \\(x\\) or \\(y\\).\n\nAlice’s and Bob’s individual inputs (\\(x\\) and \\(y\\)) remain private, and each party only reveals one number associated with their original inputs. The random outputs ensure that no information about the original numbers is disclosed.\nSecure Comparison: Another basic operation is a secure comparison of two numbers, determining which is greater than the other. This can be done using techniques like Yao’s Garbled Circuits, where the comparison circuit is encrypted to allow joint evaluation of the inputs without leaking them.\nSecure Matrix Multiplication: Matrix operations like multiplication are essential for machine learning. MPC techniques like additive secret sharing can be used to split matrices into random shares, compute products on the shares, and then reconstruct the result.\nSecure Model Training: Distributed machine learning training algorithms like federated averaging can be made secure using MPC. Model updates computed on partitioned data at each node are secretly shared between nodes and aggregated to train the global model without exposing individual updates.\nThe core idea behind MPC protocols is to divide the computation into steps that can be executed jointly without revealing intermediate sensitive data. This is accomplished by combining cryptographic techniques like secret sharing, homomorphic encryption, oblivious transfer, and garbled circuits. MPC protocols enable the collaborative computation of sensitive data while providing provable privacy guarantees. This privacy-preserving capability is essential for many machine learning applications today involving multiple parties that cannot directly share their raw data.\nThe main approaches used in MPC include:\n\nHomomorphic encryption: Special encryption allows computations to be carried out on encrypted data without decrypting it.\nSecret sharing: The private data is divided into random shares distributed to each party. Computations are done locally on the shares and finally reconstructed.\nOblivious transfer: A protocol where a receiver obtains a subset of data from a sender, but the sender does not know which specific data was transferred.\nGarbled circuits: The function to be computed is represented as a Boolean circuit that is encrypted (“garbled”) to allow joint evaluation without revealing inputs.\n\n\n\nTradeoffs\nWhile MPC protocols provide strong privacy guarantees, they come at a high computational cost compared to plain computations. Every secure operation, like addition, multiplication, comparison, etc., requires more processing orders than the equivalent unencrypted operation. This overhead stems from the underlying cryptographic techniques:\n\nIn partially homomorphic encryption, each computation on ciphertexts requires costly public-key operations. Fully homomorphic encryption has even higher overheads.\nSecret sharing divides data into multiple shares, so even basic operations require manipulating many shares.\nOblivious transfer and garbled circuits add masking and encryption to hide data access patterns and execution flows.\nMPC systems require extensive communication and interaction between parties to jointly compute on shares/ciphertexts.\n\nAs a result, MPC protocols can slow down computations by 3-4 orders of magnitude compared to plain implementations. This becomes prohibitively expensive for large datasets and models. Therefore, training machine learning models on encrypted data using MPC remains infeasible today for realistic dataset sizes due to the overhead. Clever optimizations and approximations are needed to make MPC practical.\nOngoing MPC research closes this efficiency gap through cryptographic advances, new algorithms, trusted hardware like SGX enclaves, and leveraging accelerators like GPUs/TPUs. However, in the foreseeable future, some degree of approximation and performance tradeoff is needed to scale MPC to meet the demands of real-world machine learning systems.\n\n\n\n15.8.6 Synthetic Data Generation\n\nCore Idea\nSynthetic data generation has emerged as an important privacy-preserving machine learning approach that allows models to be developed and tested without exposing real user data. The key idea is to train generative models on real-world datasets and then sample from these models to synthesize artificial data that statistically matches the original data distribution but does not contain actual user information. For instance, techniques like GANs, VAEs, and data augmentation can be used to produce synthetic data that mimics real datasets while preserving privacy. Simulations are also commonly employed in scenarios where synthetic data must represent complex systems, such as in scientific research or urban planning.\nThe primary challenge of synthesizing data is to ensure adversaries cannot re-identify the original dataset. A simple approach to achieving synthetic data is adding noise to the original dataset, which still risks privacy leakage. When noise is added to data in the context of differential privacy, sophisticated mechanisms based on the data’s sensitivity are used to calibrate the amount and distribution of noise. Through these mathematically rigorous frameworks, differential privacy generally guarantees privacy at some level, which is the primary goal of this technique. Beyond preserving privacy, synthetic data combats multiple data availability issues such as imbalanced datasets, scarce datasets, and anomaly detection.\nResearchers can freely share this synthetic data and collaborate on modeling without revealing private medical information. Well-constructed synthetic data protects privacy while providing utility for developing accurate models. Key techniques to prevent reconstructing the original data include adding differential privacy noise during training, enforcing plausibility constraints, and using multiple diverse generative models.\n\n\nBenefits\nWhile synthetic data may be necessary due to Privacy or compliance risks, it is widely used in machine learning models when available data is of poor quality, scarce, or inaccessible. Synthetic data offers more efficient and effective development by streamlining robust model training, testing, and deployment processes. It allows researchers to share models more widely without breaching privacy laws and regulations. Collaboration between users of the same dataset will be facilitated, which will help broaden the capabilities and advancements in ML research.\nThere are several motivations for using synthetic data in machine learning:\n\nPrivacy and compliance: Synthetic data avoids exposing personal information, allowing more open sharing and collaboration. This is important when working with sensitive datasets like healthcare records or financial information.\nData scarcity: When insufficient real-world data is available, synthetic data can augment training datasets. This improves model accuracy when limited data is a bottleneck.\nModel testing: Synthetic data provides privacy-safe sandboxes for testing model performance, debugging issues, and monitoring for bias.\nData labeling: High-quality labeled training data is often scarce and expensive. Synthetic data can help auto-generate labeled examples.\n\n\n\nTradeoffs\nWhile synthetic data tries to remove any evidence of the original dataset, privacy leakage is still a risk since the synthetic data mimics the original data. The statistical information and distribution are similar, if not the same, between the original and synthetic data. By resampling from the distribution, adversaries may still be able to recover the original training samples. Due to their inherent learning processes and complexities, neural networks might accidentally reveal sensitive information about the original training data.\nA core challenge with synthetic data is the potential gap between synthetic and real-world data distributions. Despite advancements in generative modeling techniques, synthetic data may only partially capture real data’s complexity, diversity, and nuanced patterns. This can limit the utility of synthetic data for robustly training machine learning models. Rigorously evaluating synthetic data quality through adversary methods and comparing model performance to real data benchmarks helps assess and improve fidelity. However, inherently, synthetic data remains an approximation.\nAnother critical concern is the privacy risks of synthetic data. Generative models may leak identifiable information about individuals in the training data, which could enable the reconstruction of private information. Emerging adversarial attacks demonstrate the challenges in preventing identity leakage from synthetic data generation pipelines. Techniques like differential privacy can help safeguard privacy, but they come with tradeoffs in data utility. There is an inherent tension between producing valid synthetic data and fully protecting sensitive training data, which must be balanced.\nAdditional pitfalls of synthetic data include amplified biases, mislabeling, the computational overhead of training generative models, storage costs, and failure to account for out-of-distribution novel data. While these are secondary to the core synthetic-real gap and privacy risks, they remain important considerations when evaluating the suitability of synthetic data for particular machine-learning tasks. As with any technique, the advantages of synthetic data come with inherent tradeoffs and limitations that require thoughtful mitigation strategies.\n\n\n\n15.8.7 Summary\nWhile all the techniques we have discussed thus far aim to enable privacy-preserving machine learning, they involve distinct mechanisms and tradeoffs. Factors like computational constraints, required trust assumptions, threat models, and data characteristics help guide the selection process for a particular use case. However, finding the right balance between Privacy, accuracy, and efficiency necessitates experimentation and empirical evaluation for many applications. Table 15.2 is a comparison table of the key privacy-preserving machine learning techniques and their pros and cons:\n\n\n\nTable 15.2: Comparing techniques for privacy-preserving machine learning.\n\n\n\n\n\n\n\n\n\n\nTechnique\nPros\nCons\n\n\n\n\nDifferential Privacy\n\nStrong formal privacy guarantees\nRobust to auxiliary data attacks\nVersatile for many data types and analyses\n\n\nAccuracy loss from noise addition\nComputational overhead for sensitivity analysis and noise generation\n\n\n\nFederated Learning\n\nAllows collaborative learning without sharing raw data\nData remains decentralized improving security\nNo need for encrypted computation\n\n\nIncreased communication overhead\nPotentially slower model convergence\nUneven client device capabilities\n\n\n\nMachine Unlearning\n\nEnables selective removal of data influence from models\nUseful for compliance with privacy regulations\nPrevents unintended retention of adversarial or outdated data\n\n\nMay degrade model performance on related tasks\nImplementation complexity in large-scale models\nRisk of incomplete or ineffective unlearning\n\n\n\nHomomorphic Encryption\n\nAllows computation on encrypted data\nPrevents intermediate state exposure\n\n\nExtremely high computational cost\nComplex cryptographic implementations\nRestrictions on function types\n\n\n\nSecure Multi-Party Computation\n\nEnables joint computation on sensitive data\nProvides cryptographic privacy guarantees\nFlexible protocols for various functions\n\n\nVery high computational overhead\nComplexity of implementation\nAlgorithmic constraints on function depth\n\n\n\nSynthetic Data Generation\n\nEnables data sharing without leakage\nMitigates data scarcity problems\n\n\nSynthetic-real gap in distributions\nPotential for reconstructing private data\nBiases and labeling challenges",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Security & Privacy</span>"
    ]
  },
  {
    "objectID": "contents/core/privacy_security/privacy_security.html#conclusion",
    "href": "contents/core/privacy_security/privacy_security.html#conclusion",
    "title": "15  Security & Privacy",
    "section": "15.9 Conclusion",
    "text": "15.9 Conclusion\nMachine learning hardware security is critical as embedded ML systems are increasingly deployed in safety-critical domains like medical devices, industrial controls, and autonomous vehicles. We have explored various threats spanning hardware bugs, physical attacks, side channels, supply chain risks, etc. Defenses like TEEs, Secure Boot, PUFs, and hardware security modules provide multilayer protection tailored for resource-constrained embedded devices.\nHowever, continual vigilance is essential to track emerging attack vectors and address potential vulnerabilities through secure engineering practices across the hardware lifecycle. As ML and embedded ML spread, maintaining rigorous security foundations that match the field’s accelerating pace of innovation remains imperative.",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Security & Privacy</span>"
    ]
  },
  {
    "objectID": "contents/core/privacy_security/privacy_security.html#sec-security-and-privacy-resource",
    "href": "contents/core/privacy_security/privacy_security.html#sec-security-and-privacy-resource",
    "title": "15  Security & Privacy",
    "section": "15.10 Resources",
    "text": "15.10 Resources\nHere is a curated list of resources to support students and instructors in their learning and teaching journeys. We are continuously working on expanding this collection and will add new exercises soon.\n\n\n\n\n\n\nSlides\n\n\n\n\n\nThese slides are a valuable tool for instructors to deliver lectures and for students to review the material at their own pace. We encourage students and instructors to leverage these slides to improve their understanding and facilitate effective knowledge transfer.\n\nSecurity.\nPrivacy.\nMonitoring after Deployment.\n\n\n\n\n\n\n\n\n\n\nVideos\n\n\n\n\n\n\nVideo 15.1\nVideo 15.2\nVideo 15.3\n\n\n\n\n\n\n\n\n\n\nExercises\n\n\n\n\n\nTo reinforce the concepts covered in this chapter, we have curated a set of exercises that challenge students to apply their knowledge and deepen their understanding.\n\nExercise 15.1\nExercise 15.2",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Security & Privacy</span>"
    ]
  },
  {
    "objectID": "contents/core/responsible_ai/responsible_ai.html",
    "href": "contents/core/responsible_ai/responsible_ai.html",
    "title": "16  Responsible AI",
    "section": "",
    "text": "Purpose\nResources: Slides, Videos, Exercises\nHow do human values translate into system architecture, and what principles enable the development of AI systems that embody ethical considerations?\nEmbedding ethical principles into machine learning systems represents a fundamental engineering challenge. Each design decision carries moral implications, revealing essential patterns in how technical choices influence societal outcomes. The implementation of ethical frameworks demonstrates the need for new approaches to system architecture that consider human values as fundamental design constraints. Understanding these moral-technical relationships provides insights into creating principled systems, establishing core methodologies for designing AI solutions that advance capabilities while upholding human values.",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Responsible AI</span>"
    ]
  },
  {
    "objectID": "contents/core/responsible_ai/responsible_ai.html#purpose",
    "href": "contents/core/responsible_ai/responsible_ai.html#purpose",
    "title": "16  Responsible AI",
    "section": "",
    "text": "Learning Objectives\n\n\n\n\nUnderstand responsible AI’s core principles and motivations, including fairness, transparency, privacy, safety, and accountability.\nLearn technical methods for implementing responsible AI principles, such as detecting dataset biases, building interpretable models, adding noise for privacy, and testing model robustness.\nRecognize organizational and social challenges to achieving responsible AI, including data quality, model objectives, communication, and job impacts.\nKnowledge of ethical frameworks and considerations for AI systems, spanning AI safety, human autonomy, and economic consequences.\nAppreciate the increased complexity and costs of developing ethical, trustworthy AI systems compared to unprincipled AI.",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Responsible AI</span>"
    ]
  },
  {
    "objectID": "contents/core/responsible_ai/responsible_ai.html#overview",
    "href": "contents/core/responsible_ai/responsible_ai.html#overview",
    "title": "16  Responsible AI",
    "section": "16.1 Overview",
    "text": "16.1 Overview\nMachine learning models are increasingly used to automate decisions in high-stakes social domains like healthcare, criminal justice, and employment. However, without deliberate care, these algorithms can perpetuate biases, breach privacy, or cause other harm. For instance, a loan approval model solely trained on data from high-income neighborhoods could disadvantage applicants from lower-income areas. This motivates the need for responsible machine learning - creating fair, accountable, transparent, and ethical models.\nSeveral core principles underlie responsible ML. Fairness ensures models do not discriminate based on gender, race, age, and other attributes. Explainability enables humans to interpret model behaviors and improve transparency. Robustness and safety techniques prevent vulnerabilities like adversarial examples. Rigorous testing and validation help reduce unintended model weaknesses or side effects.\nImplementing responsible ML presents both technical and ethical challenges. Developers must grapple with defining fairness mathematically, balancing competing objectives like accuracy vs interpretability, and securing quality training data. Organizations must also align incentives, policies, and culture to uphold ethical AI.\nThis chapter will equip you to critically evaluate AI systems and contribute to developing beneficial and ethical machine learning applications by covering the foundations, methods, and real-world implications of responsible ML. The responsible ML principles discussed are crucial knowledge as algorithms mediate more aspects of human society.",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Responsible AI</span>"
    ]
  },
  {
    "objectID": "contents/core/responsible_ai/responsible_ai.html#terminology",
    "href": "contents/core/responsible_ai/responsible_ai.html#terminology",
    "title": "16  Responsible AI",
    "section": "16.2 Terminology",
    "text": "16.2 Terminology\nResponsible AI is about developing AI that positively impacts society under human ethics and values. There is no universally agreed-upon definition of “responsible AI,” but here is a summary of how it is commonly described. Responsible AI refers to designing, developing, and deploying artificial intelligence systems in an ethical, socially beneficial way. The core goal is to create trustworthy, unbiased, fair, transparent, accountable, and safe AI. While there is no canonical definition, responsible AI is generally considered to encompass principles such as:\n\nFairness: Avoiding biases, discrimination, and potential harm to certain groups or populations\nExplainability: Enabling humans to understand and interpret how AI models make decisions\nTransparency: Openly communicating how AI systems operate, are built, and are evaluated\nAccountability: Having processes to determine responsibility and liability for AI failures or negative impacts\nRobustness: Ensuring AI systems are secure, reliable, and behave as intended\nPrivacy: Protecting sensitive user data and adhering to privacy laws and ethics\n\nPutting these principles into practice involves technical skills, corporate policies, governance frameworks, and moral philosophy. There are also ongoing debates around defining ambiguous concepts like fairness and determining how to balance competing objectives.",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Responsible AI</span>"
    ]
  },
  {
    "objectID": "contents/core/responsible_ai/responsible_ai.html#principles-and-concepts",
    "href": "contents/core/responsible_ai/responsible_ai.html#principles-and-concepts",
    "title": "16  Responsible AI",
    "section": "16.3 Principles and Concepts",
    "text": "16.3 Principles and Concepts\n\n16.3.1 Transparency and Explainability\nMachine learning models are often criticized as mysterious “black boxes” - opaque systems where it’s unclear how they arrived at particular predictions or decisions. For example, an AI system called COMPAS used to assess criminal recidivism risk in the US was found to be racially biased against black defendants. Still, the opacity of the algorithm made it difficult to understand and fix the problem. This lack of transparency can obscure biases, errors, and deficiencies.\nExplaining model behaviors helps engender trust from the public and domain experts and enables identifying issues to address. Interpretability techniques play a key role in this process. For instance, LIME (Local Interpretable Model-Agnostic Explanations) highlights how individual input features contribute to a specific prediction, while Shapley values quantify each feature’s contribution to a model’s output based on cooperative game theory. Saliency maps, commonly used in image-based models, visually highlight areas of an image that most influenced the model’s decision. These tools empower users to understand model logic.\nBeyond practical benefits, transparency is increasingly required by law. Regulations like the European Union’s General Data Protection Regulation (GDPR) mandate that organizations provide explanations for certain automated decisions, especially when they significantly impact individuals. This makes explainability not just a best practice but a legal necessity in some contexts. Together, transparency and explainability form critical pillars of building responsible and trustworthy AI systems.\n\n\n16.3.2 Fairness, Bias, and Discrimination\nML models trained on historically biased data often perpetuate and amplify those prejudices. Healthcare algorithms have been shown to disadvantage black patients by underestimating their needs (Obermeyer et al. 2019). Facial recognition needs to be more accurate for women and people of color. Such algorithmic discrimination can negatively impact people’s lives in profound ways.\n\nObermeyer, Ziad, Brian Powers, Christine Vogeli, and Sendhil Mullainathan. 2019. “Dissecting Racial Bias in an Algorithm Used to Manage the Health of Populations.” Science 366 (6464): 447–53. https://doi.org/10.1126/science.aax2342.\nDifferent philosophical perspectives also exist on fairness - for example, is it fairer to treat all individuals equally or try to achieve equal outcomes for groups? Ensuring fairness requires proactively detecting and mitigating biases in data and models. However, achieving perfect fairness is tremendously difficult due to contrasting mathematical definitions and ethical perspectives. Still, promoting algorithmic fairness and non-discrimination is a key responsibility in AI development.\n\n\n16.3.3 Privacy and Data Governance\nMaintaining individuals’ privacy is an ethical obligation and legal requirement for organizations deploying AI systems. Regulations like the EU’s GDPR mandate data privacy protections and rights, such as the ability to access and delete one’s data.\nHowever, maximizing the utility and accuracy of data for training models can conflict with preserving privacy - modeling disease progression could benefit from access to patients’ full genomes, but sharing such data widely violates privacy.\nResponsible data governance involves carefully anonymizing data, controlling access with encryption, getting informed consent from data subjects, and collecting the minimum data needed. Honoring privacy is challenging but critical as AI capabilities and adoption expand.\n\n\n16.3.4 Safety and Robustness\nPutting AI systems into real-world operation requires ensuring they are safe, reliable, and robust, especially for human interaction scenarios. Self-driving cars from Uber and Tesla have been involved in deadly crashes due to unsafe behaviors.\nAdversarial attacks that subtly alter input data can also fool ML models and cause dangerous failures if systems are not resistant. Deepfakes represent another emerging threat area.\nVideo 16.1 is a deepfake video of Barack Obama that went viral a few years ago.\n\n\n\n\n\n\nImportant 16.1: Fake Obama\n\n\n\n\n\n\nPromoting safety requires extensive testing, risk analysis, human oversight, and designing systems that combine multiple weak models to avoid single points of failure. Rigorous safety mechanisms are essential for the responsible deployment of capable AI.\n\n\n16.3.5 Accountability and Governance\nWhen AI systems eventually fail or produce harmful outcomes, mechanisms must exist to address resultant issues, compensate affected parties, and assign responsibility. Both corporate accountability policies and government regulations are indispensable for responsible AI governance. For instance, Illinois’ Artificial Intelligence Video Interview Act requires companies to disclose and obtain consent for AI video analysis, promoting accountability.\nWithout clear accountability, even harms caused unintentionally could go unresolved, furthering public outrage and distrust. Oversight boards, impact assessments, grievance redress processes, and independent audits promote responsible development and deployment.",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Responsible AI</span>"
    ]
  },
  {
    "objectID": "contents/core/responsible_ai/responsible_ai.html#cloud-edge-tiny-ml",
    "href": "contents/core/responsible_ai/responsible_ai.html#cloud-edge-tiny-ml",
    "title": "16  Responsible AI",
    "section": "16.4 Cloud, Edge & Tiny ML",
    "text": "16.4 Cloud, Edge & Tiny ML\nWhile these principles broadly apply across AI systems, certain responsible AI considerations are unique or pronounced when dealing with machine learning on embedded devices versus traditional server-based modeling. Therefore, we present a high-level taxonomy comparing responsible AI considerations across cloud, edge, and TinyML systems.\n\n16.4.1 Explainability\nFor cloud-based machine learning, explainability techniques can leverage significant compute resources, enabling complex methods like SHAP values or sampling-based approaches to interpret model behaviors. For example, Microsoft’s InterpretML toolkit provides explainability techniques tailored for cloud environments.\nHowever, edge ML operates on resource-constrained devices, requiring more lightweight explainability methods that can run locally without excessive latency. Techniques like LIME (Ribeiro, Singh, and Guestrin 2016) approximate model explanations using linear models or decision trees to avoid expensive computations, which makes them ideal for resource-constrained devices. However, LIME requires training hundreds to even thousands of models to generate good explanations, which is often infeasible given edge computing constraints. In contrast, saliency-based methods are often much faster in practice, only requiring a single forward pass through the network to estimate feature importance. This greater efficiency makes such methods better suited to edge devices with limited compute resources where low-latency explanations are critical.\nGiven tiny hardware capabilities, embedded systems pose the most significant challenges for explainability. More compact models and limited data make inherent model transparency easier. Explaining decisions may not be feasible on high-size and power-optimized microcontrollers. DARPA’s Transparent Computing program tries to develop extremely low overhead explainability, especially for TinyML devices like sensors and wearables.\n\n\n16.4.2 Fairness\nFor cloud machine learning, vast datasets and computing power enable detecting biases across large heterogeneous populations and mitigating them through techniques like re-weighting data samples. However, biases may emerge from the broad behavioral data used to train cloud models. Amazon’s Fairness Flow framework helps assess cloud ML fairness.\nEdge ML relies on limited on-device data, making analyzing biases across diverse groups harder. However, edge devices interact closely with individuals, providing an opportunity to adapt locally for fairness. Google’s Federated Learning distributes model training across devices to incorporate individual differences.\nTinyML poses unique challenges for fairness with highly dispersed specialized hardware and minimal training data. Bias testing is difficult across diverse devices. Collecting representative data from many devices to mitigate bias has scale and privacy hurdles. DARPA’s Assured Neuro Symbolic Learning and Reasoning (ANSR) efforts are geared toward developing fairness techniques given extreme hardware constraints.\n\n\n16.4.3 Privacy\nFor cloud ML, vast amounts of user data are concentrated in the cloud, creating risks of exposure through breaches. Differential privacy techniques add noise to cloud data to preserve privacy. Strict access controls and encryption protect cloud data at rest and in transit.\nEdge ML moves data processing onto user devices, reducing aggregated data collection but increasing potential sensitivity as personal data resides on the device. Apple uses on-device ML and differential privacy to train models while minimizing data sharing. Data anonymization and secure enclaves protect on-device data.\nTinyML distributes data across many resource-constrained devices, making centralized breaches unlikely and making scale anonymization challenging. Data minimization and using edge devices as intermediaries help TinyML privacy.\nSo, while cloud ML must protect expansive centralized data, edge ML secures sensitive on-device data, and TinyML aims for minimal distributed data sharing due to constraints. While privacy is vital throughout, techniques must match the environment. Understanding nuances allows for selecting appropriate privacy preservation approaches.\n\n\n16.4.4 Safety\nKey safety risks for cloud ML include model hacking, data poisoning, and malware disrupting cloud services. Robustness techniques like adversarial training, anomaly detection, and diversified models aim to harden cloud ML against attacks. Redundancy can help prevent single points of failure.\nEdge ML and TinyML interact with the physical world, so reliability and safety validation are critical. Rigorous testing platforms like Foretellix synthetically generate edge scenarios to validate safety. TinyML safety is magnified by autonomous devices with limited supervision. TinyML safety often relies on collective coordination - swarms of drones maintain safety through redundancy. Physical control barriers also constrain unsafe TinyML device behaviors.\nSafety considerations vary significantly across domains, reflecting their unique challenges. Cloud ML focuses on guarding against hacking and data breaches, edge ML emphasizes reliability due to its physical interactions with the environment, and TinyML often relies on distributed coordination to maintain safety in autonomous systems. Recognizing these nuances is essential for applying the appropriate safety techniques to each domain.\n\n\n16.4.5 Accountability\nCloud ML’s accountability centers on corporate practices like responsible AI committees, ethical charters, and processes to address harmful incidents. Third-party audits and external government oversight promote cloud ML accountability.\nEdge ML accountability is more complex with distributed devices and supply chain fragmentation. Companies are accountable for devices, but components come from various vendors. Industry standards help coordinate edge ML accountability across stakeholders.\nWith TinyML, accountability mechanisms must be traced across long, complex supply chains of integrated circuits, sensors, and other hardware. TinyML certification schemes help track component provenance. Trade associations should ideally promote shared accountability for ethical TinyML.\n\n\n16.4.6 Governance\nOrganizations institute internal governance for cloud ML, such as ethics boards, audits, and model risk management. External governance also plays a significant role in ensuring accountability and fairness. We have already introduced the General Data Protection Regulation (GDPR), which sets stringent requirements for data protection and transparency. However, it is not the only framework guiding responsible AI practices. The AI Bill of Rights establishes principles for ethical AI use in the United States, and the California Consumer Protection Act (CCPA) focuses on safeguarding consumer data privacy within California. Third-party audits further bolster cloud ML governance by providing external oversight.\nEdge ML is more decentralized, requiring responsible self-governance by developers and companies deploying models locally. Industry associations coordinate governance across edge ML vendors, and open software helps align incentives for ethical edge ML.\nExtreme decentralization and complexity make external governance infeasible with TinyML. TinyML relies on protocols and standards for self-governance baked into model design and hardware. Cryptography enables the provable trustworthiness of TinyML devices.\n\n\n16.4.7 Summary\nTable 16.1 summarizes how responsible AI principles manifest differently across cloud, edge, and TinyML architectures and how core considerations tie into their unique capabilities and limitations. Each environment’s constraints and tradeoffs shape how we approach transparency, accountability, governance, and other pillars of responsible AI.\n\n\n\nTable 16.1: Comparison of key principles in Cloud ML, Edge ML, and TinyML.\n\n\n\n\n\n\n\n\n\n\n\nPrinciple\nCloud ML\nEdge ML\nTinyML\n\n\n\n\nExplainability\nSupports complex models and methods like SHAP and sampling approaches\nNeeds lightweight, low-latency methods like saliency maps\nSeverely limited due to constrained hardware\n\n\nFairness\nLarge datasets enable bias detection and mitigation\nLocalized biases harder to detect but allows on-device adjustments\nMinimal data limits bias analysis and mitigation\n\n\nPrivacy\nCentralized data at risk of breaches but can leverage strong encryption and differential privacy\nSensitive personal data on-device requires on-device protections\nDistributed data reduces centralized risks but poses challenges for anonymization\n\n\nSafety\nVulnerable to hacking and large-scale attacks\nReal-world interactions make reliability critical\nNeeds distributed safety mechanisms due to autonomy\n\n\nAccountability\nCorporate policies and audits ensure responsibility\nFragmented supply chains complicate accountability\nTraceability required across long, complex hardware chains\n\n\nGovernance\nExternal oversight and regulations like GDPR or CCPA are feasible\nRequires self-governance by developers and stakeholders\nRelies on built-in protocols and cryptographic assurances",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Responsible AI</span>"
    ]
  },
  {
    "objectID": "contents/core/responsible_ai/responsible_ai.html#technical-aspects",
    "href": "contents/core/responsible_ai/responsible_ai.html#technical-aspects",
    "title": "16  Responsible AI",
    "section": "16.5 Technical Aspects",
    "text": "16.5 Technical Aspects\n\n16.5.1 Detecting and Mitigating Bias\nMachine learning models, like any complex system, can sometimes exhibit biases in their predictions. These biases may manifest in underperformance for specific groups or in decisions that inadvertently restrict access to certain opportunities or resources (Buolamwini and Gebru 2018). Understanding and addressing these biases is critical, especially as machine learning systems are increasingly used in sensitive domains like lending, healthcare, and criminal justice.\nTo evaluate and address these issues, fairness in machine learning is typically assessed by analyzing “subgroup attributes,” which are characteristics unrelated to the prediction task, such as geographic location, age group, income level, race, gender, or religion. For example, in a loan default prediction model, subgroups could include race, gender, or religion. When models are trained with the sole objective of maximizing accuracy, they may overlook performance differences across these subgroups, potentially resulting in biased or inconsistent outcomes.\nThis concept is illustrated in Figure 16.1, which visualizes the performance of a machine learning model predicting loan repayment for two subgroups, Subgroup A (blue) and Subgroup B (red). Each individual in the dataset is represented by a symbol: plusses (+) indicate individuals who will repay their loans (true positives), while circles (O) indicate individuals who will default on their loans (true negatives). The model’s objective is to correctly classify these individuals into repayers and defaulters.\n\n\n\n\n\n\nFigure 16.1: Illustrates the trade-off in setting classification thresholds for two subgroups (A and B) in a loan repayment model. Plusses (+) represent true positives (repayers), and circles (O) represent true negatives (defaulters). Different thresholds (75% for B and 81.25% for A) maximize subgroup accuracy but reveal fairness challenges.\n\n\n\nTo evaluate performance, two dotted lines are shown, representing the thresholds at which the model achieves acceptable accuracy for each subgroup. For Subgroup A, the threshold needs to be set at 81.25% accuracy (the second dotted line) to correctly classify all repayers (plusses). However, using this same threshold for Subgroup B would result in misclassifications, as some repayers in Subgroup B would incorrectly fall below this threshold and be classified as defaulters. For Subgroup B, a lower threshold of 75% accuracy (the first dotted line) is necessary to correctly classify its repayers. However, applying this lower threshold to Subgroup A would result in misclassifications for that group. This illustrates how the model performs unequally across the two subgroups, with each requiring a different threshold to maximize their true positive rates.\nThe disparity in required thresholds highlights the challenge of achieving fairness in model predictions. If positive classifications lead to loan approvals, individuals in Subgroup B would be disadvantaged unless the threshold is adjusted specifically for their subgroup. However, adjusting thresholds introduces trade-offs between group-level accuracy and fairness, demonstrating the inherent tension in optimizing for these objectives in machine learning systems.\nThus, the fairness literature has proposed three main fairness metrics for quantifying how fair a model performs over a dataset (Hardt, Price, and Srebro 2016). Given a model \\(h\\) and a dataset \\(D\\) consisting of \\((x, y, s)\\) samples, where \\(x\\) is the data features, \\(y\\) is the label, and \\(s\\) is the subgroup attribute, and we assume there are simply two subgroups \\(a\\) and \\(b\\), we can define the following:\n\nDemographic Parity asks how accurate a model is for each subgroup. In other words, \\(P(h(X) = Y \\mid S = a) = P(h(X) = Y \\mid S = b)\\).\nEqualized Odds asks how precise a model is on positive and negative samples for each subgroup. \\(P(h(X) = y \\mid S = a, Y = y) = P(h(X) = y \\mid S = b, Y = y)\\).\nEquality of Opportunity is a special case of equalized odds that only asks how precise a model is on positive samples. This is relevant in cases such as resource allocation, where we care about how positive (i.e., resource-allocated) labels are distributed across groups. For example, we care that an equal proportion of loans are given to both men and women. \\(P(h(X) = 1 \\mid S = a, Y = 1) = P(h(X) = 1 \\mid S = b, Y = 1)\\).\n\nNote: These definitions often take a narrow view when considering binary comparisons between two subgroups. Another thread of fair machine learning research focusing on multicalibration and multiaccuracy considers the interactions between an arbitrary number of identities, acknowledging the inherent intersectionality of individual identities in the real world (Hébert-Johnson et al. 2018).\n\nHébert-Johnson, Úrsula, Michael P. Kim, Omer Reingold, and Guy N. Rothblum. 2018. “Multicalibration: Calibration for the (Computationally-Identifiable) Masses.” In Proceedings of the 35th International Conference on Machine Learning, ICML 2018, Stockholmsmässan, Stockholm, Sweden, July 10-15, 2018, edited by Jennifer G. Dy and Andreas Krause, 80:1944–53. Proceedings of Machine Learning Research. PMLR. http://proceedings.mlr.press/v80/hebert-johnson18a.html.\n\nContext Matters\nBefore making any technical decisions to develop an unbiased ML algorithm, we need to understand the context surrounding our model. Here are some of the key questions to think about:\n\nWho will this model make decisions for?\nWho is represented in the training data?\nWho is represented, and who is missing at the table of engineers, designers, and managers?\n\nWhat sort of long-lasting impacts could this model have? For example, will it impact an individual’s financial security at a generational scale, such as determining college admissions or admitting a loan for a house?\n\nWhat historical and systematic biases are present in this setting, and are they present in the training data the model will generalize from?\n\nUnderstanding a system’s social, ethical, and historical background is critical to preventing harm and should inform decisions throughout the model development lifecycle. After understanding the context, one can make various technical decisions to remove bias. First, one must decide what fairness metric is the most appropriate criterion for optimizing. Next, there are generally three main areas where one can intervene to debias an ML system.\nFirst, preprocessing is when one balances a dataset to ensure fair representation or even increases the weight on certain underrepresented groups to ensure the model performs well. Second, in processing attempts to modify the training process of an ML system to ensure it prioritizes fairness. This can be as simple as adding a fairness regularizer (Lowy et al. 2021) to training an ensemble of models and sampling from them in a specific manner (Agarwal et al. 2018).\n\nLowy, Andrew, Rakesh Pavan, Sina Baharlouei, Meisam Razaviyayn, and Ahmad Beirami. 2021. “Fermi: Fair Empirical Risk Minimization via Exponential Rényi Mutual Information.”\n\nAgarwal, Alekh, Alina Beygelzimer, Miroslav Dudı́k, John Langford, and Hanna M. Wallach. 2018. “A Reductions Approach to Fair Classification.” In Proceedings of the 35th International Conference on Machine Learning, ICML 2018, Stockholmsmässan, Stockholm, Sweden, July 10-15, 2018, edited by Jennifer G. Dy and Andreas Krause, 80:60–69. Proceedings of Machine Learning Research. PMLR. http://proceedings.mlr.press/v80/agarwal18a.html.\n\nAlghamdi, Wael, Hsiang Hsu, Haewon Jeong, Hao Wang, Peter Michalak, Shahab Asoodeh, and Flavio Calmon. 2022. “Beyond Adult and COMPAS: Fair Multi-Class Prediction via Information Projection.” Adv. Neur. In. 35: 38747–60.\n\nHardt, Moritz, Eric Price, and Nati Srebro. 2016. “Equality of Opportunity in Supervised Learning.” In Advances in Neural Information Processing Systems 29: Annual Conference on Neural Information Processing Systems 2016, December 5-10, 2016, Barcelona, Spain, edited by Daniel D. Lee, Masashi Sugiyama, Ulrike von Luxburg, Isabelle Guyon, and Roman Garnett, 3315–23. https://proceedings.neurips.cc/paper/2016/hash/9d2682367c3935defcb1f9e247a97c0d-Abstract.html.\nFinally, post-processing debases a model after the fact, taking a trained model and modifying its predictions in a specific manner to ensure fairness is preserved (Alghamdi et al. 2022; Hardt, Price, and Srebro 2016). Post-processing builds on the preprocessing and in-processing steps by providing another opportunity to address bias and fairness issues in the model after it has already been trained.\nThe three-step process of preprocessing, in-processing, and post-processing provides a framework for intervening at different stages of model development to mitigate issues around bias and fairness. While preprocessing and in-processing focus on data and training, post-processing allows for adjustments after the model has been fully trained. Together, these three approaches give multiple opportunities to detect and remove unfair bias.\n\n\nThoughtful Deployment\nThe breadth of existing fairness definitions and debiasing interventions underscores the need for thoughtful assessment before deploying ML systems. As ML researchers and developers, responsible model development requires proactively educating ourselves on the real-world context, consulting domain experts and end-users, and centering harm prevention.\nRather than seeing fairness considerations as a box to check, we must deeply engage with the unique social implications and ethical tradeoffs around each model we build. Every technical choice about datasets, model architectures, evaluation metrics, and deployment constraints embeds values. By broadening our perspective beyond narrow technical metrics, carefully evaluating tradeoffs, and listening to impacted voices, we can work to ensure our systems expand opportunity rather than encode bias.\nThe path forward lies not in an arbitrary debiasing checklist but in a commitment to understanding and upholding our ethical responsibility at each step. This commitment starts with proactively educating ourselves and consulting others rather than just going through the motions of a fairness checklist. It requires engaging deeply with ethical tradeoffs in our technical choices, evaluating impacts on different groups, and listening to those voices most impacted.\nUltimately, responsible and ethical AI systems do not come from checkbox debiasing but from upholding our duty to assess harms, broaden perspectives, understand tradeoffs, and ensure we provide opportunity for all groups. This ethical responsibility should drive every step.\nThe connection between the paragraphs is that the first paragraph establishes the need for a thoughtful assessment of fairness issues rather than a checkbox approach. The second paragraph then expands on what that thoughtful assessment looks like in practice—engaging with tradeoffs, evaluating impacts on groups, and listening to impacted voices. Finally, the last paragraph refers to avoiding an “arbitrary debiasing checklist” and committing to ethical responsibility through assessment, understanding tradeoffs, and providing opportunity.\n\n\n\n16.5.2 Preserving Privacy\nRecent incidents have shed light on how AI models can memorize sensitive user data in ways that violate privacy. Ippolito et al. (2023) demonstrate that language models tend to memorize training data and can even reproduce specific training examples. These risks are amplified with personalized ML systems deployed in intimate environments like homes or wearables. Consider a smart speaker that uses our conversations to improve its service quality for users who appreciate such enhancements. While potentially beneficial, this also creates privacy risks, as malicious actors could attempt to extract what the speaker “remembers.” The issue extends beyond language models. Figure 16.2 showcases how diffusion models can memorize and generate individual training examples (Nicolas Carlini et al. 2023), further demonstrating the potential privacy risks associated with AI systems learning from user data.\n\nCarlini, Nicolas, Jamie Hayes, Milad Nasr, Matthew Jagielski, Vikash Sehwag, Florian Tramer, Borja Balle, Daphne Ippolito, and Eric Wallace. 2023. “Extracting Training Data from Diffusion Models.” In 32nd USENIX Security Symposium (USENIX Security 23), 5253–70.\n\n\n\n\n\n\nFigure 16.2: Diffusion models memorizing samples from training data. Source: Ippolito et al. (2023).\n\n\nIppolito, Daphne, Florian Tramer, Milad Nasr, Chiyuan Zhang, Matthew Jagielski, Katherine Lee, Christopher Choquette Choo, and Nicholas Carlini. 2023. “Preventing Generation of Verbatim Memorization in Language Models Gives a False Sense of Privacy.” In Proceedings of the 16th International Natural Language Generation Conference, 5253–70. Association for Computational Linguistics. https://doi.org/10.18653/v1/2023.inlg-main.3.\n\n\nAs AI becomes increasingly integrated into our daily lives, it is becoming more important that privacy concerns and robust safeguards to protect user information are developed with a critical eye. The challenge lies in balancing the benefits of personalized AI with the fundamental right to privacy.\nAdversaries can use these memorization capabilities and train models to detect if specific training data influenced a target model. For example, membership inference attacks train a secondary model that learns to detect a change in the target model’s outputs when making inferences over data it was trained on versus not trained on (Shokri et al. 2017).\n\nShokri, Reza, Marco Stronati, Congzheng Song, and Vitaly Shmatikov. 2017. “Membership Inference Attacks Against Machine Learning Models.” In 2017 IEEE Symposium on Security and Privacy (SP), 3–18. IEEE; IEEE. https://doi.org/10.1109/sp.2017.41.\n\nAbadi, Martin, Andy Chu, Ian Goodfellow, H. Brendan McMahan, Ilya Mironov, Kunal Talwar, and Li Zhang. 2016. “Deep Learning with Differential Privacy.” In Proceedings of the 2016 ACM SIGSAC Conference on Computer and Communications Security, 308–18. CCS ’16. New York, NY, USA: ACM. https://doi.org/10.1145/2976749.2978318.\nML devices are especially vulnerable because they are often personalized on user data and are deployed in even more intimate settings such as the home. Private machine learning techniques have evolved to establish safeguards against adversaries, as mentioned in the Security and Privacy chapter to combat these privacy issues. Methods like differential privacy add mathematical noise during training to obscure individual data points’ influence on the model. Popular techniques like DP-SGD (Abadi et al. 2016) also clip gradients to limit what the model leaks about the data. Still, users should also be able to delete the impact of their data after the fact.\n\n\n16.5.3 Machine Unlearning\nWith ML devices personalized to individual users and then deployed to remote edges without connectivity, a challenge arises—how can models responsively “forget” data points after deployment? If users request their data be removed from a personalized model, the lack of connectivity makes retraining infeasible. Thus, efficient on-device data forgetting is necessary but poses hurdles.\nInitial unlearning approaches faced limitations in this context. Given the resource constraints, retrieving models from scratch on the device to forget data points proves inefficient or even impossible. Fully retraining also requires retaining all the original training data on the device, which brings its own security and privacy risks. Common machine unlearning techniques (Bourtoule et al. 2021) for remote embedded ML systems fail to enable responsive, secure data removal.\n\nBourtoule, Lucas, Varun Chandrasekaran, Christopher A. Choquette-Choo, Hengrui Jia, Adelin Travers, Baiwu Zhang, David Lie, and Nicolas Papernot. 2021. “Machine Unlearning.” In 2021 IEEE Symposium on Security and Privacy (SP), 141–59. IEEE; IEEE. https://doi.org/10.1109/sp40001.2021.00019.\nHowever, newer methods show promise in modifying models to approximately forget data without full retraining. While the accuracy loss from avoiding full rebuilds is modest, guaranteeing data privacy should still be the priority when handling sensitive user information ethically. Even slight exposure to private data can violate user trust. As ML systems become deeply personalized, efficiency and privacy must be enabled from the start—not afterthoughts.\nGlobal privacy regulations, such as the well-established GDPR in the European Union, the CCPA in California, and newer proposals like Canada’s CPPA and Japan’s APPI, emphasize the right to delete personal data. These policies, alongside high-profile AI incidents such as Stable Diffusion memorizing artist data, have highlighted the ethical imperative for models to allow users to delete their data even after training.\nThe right to remove data arises from privacy concerns around corporations or adversaries misusing sensitive user information. Machine unlearning refers to removing the influence of specific points from an already-trained model. Naively, this involves full retraining without the deleted data. However, connectivity constraints often make retraining infeasible for ML systems personalized and deployed to remote edges. If a smart speaker learns from private home conversations, retaining access to delete that data is important.\nAlthough limited, methods are evolving to enable efficient approximations of retraining for unlearning. By modifying models’ inference time, they can mimic “forgetting” data without full access to training data. However, most current techniques are restricted to simple models, still have resource costs, and trade some accuracy. Though methods are evolving, enabling efficient data removal and respecting user privacy remains imperative for responsible TinyML deployment.\n\n\n16.5.4 Adversarial Examples and Robustness\nMachine learning models, especially deep neural networks, have a well-documented Achilles heel: they often break when even tiny perturbations are made to their inputs (Szegedy et al. 2014). This surprising fragility highlights a major robustness gap threatening real-world deployment in high-stakes domains. It also opens the door for adversarial attacks designed to fool models deliberately.\nMachine learning models can exhibit surprising brittleness—minor input tweaks can cause shocking malfunctions, even in state-of-the-art deep neural networks (Szegedy et al. 2014). This unpredictability around out-of-sample data underscores gaps in model generalization and robustness. Given the growing ubiquity of ML, it also enables adversarial threats that weaponize models’ blindspots.\nDeep neural networks demonstrate an almost paradoxical dual nature - human-like proficiency in training distributions coupled with extreme fragility to tiny input perturbations (Szegedy et al. 2014). This adversarial vulnerability gap highlights gaps in standard ML procedures and threats to real-world reliability. At the same time, it can be exploited: attackers can find model-breaking points humans wouldn’t perceive.\n\nSzegedy, Christian, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian J. Goodfellow, and Rob Fergus. 2014. “Intriguing Properties of Neural Networks.” In 2nd International Conference on Learning Representations, ICLR 2014, Banff, AB, Canada, April 14-16, 2014, Conference Track Proceedings, edited by Yoshua Bengio and Yann LeCun. http://arxiv.org/abs/1312.6199.\nFigure 16.3 includes an example of a small meaningless perturbation that changes a model prediction. This fragility has real-world impacts: lack of robustness undermines trust in deploying models for high-stakes applications like self-driving cars or medical diagnosis. Moreover, the vulnerability leads to security threats: attackers can deliberately craft adversarial examples that are perceptually indistinguishable from normal data but cause model failures.\n\n\n\n\n\n\nFigure 16.3: Perturbation effect on prediction. Source: Microsoft.\n\n\n\nFor instance, past work shows successful attacks that trick models for tasks like NSFW detection (Bhagoji et al. 2018), ad-blocking (Tramèr et al. 2019), and speech recognition (Nicholas Carlini et al. 2016). While errors in these domains already pose security risks, the problem extends beyond IT security. Recently, adversarial robustness has been proposed as an additional performance metric by approximating worst-case behavior.\n\nBhagoji, Arjun Nitin, Warren He, Bo Li, and Dawn Song. 2018. “Practical Black-Box Attacks on Deep Neural Networks Using Efficient Query Mechanisms.” In Proceedings of the European Conference on Computer Vision (ECCV), 154–69.\n\nTramèr, Florian, Pascal Dupré, Gili Rusak, Giancarlo Pellegrino, and Dan Boneh. 2019. “AdVersarial: Perceptual Ad Blocking Meets Adversarial Machine Learning.” In Proceedings of the 2019 ACM SIGSAC Conference on Computer and Communications Security, 2005–21. ACM. https://doi.org/10.1145/3319535.3354222.\n\nCarlini, Nicholas, Pratyush Mishra, Tavish Vaidya, Yuankai Zhang, Micah Sherr, Clay Shields, David Wagner, and Wenchao Zhou. 2016. “Hidden Voice Commands.” In 25th USENIX Security Symposium (USENIX Security 16), 513–30.\nThe surprising model fragility highlighted above casts doubt on real-world reliability and opens the door to adversarial manipulation. This growing vulnerability underscores several needs. First, moral robustness evaluations are essential for quantifying model vulnerabilities before deployment. Approximating worst-case behavior surfaces blindspots.\nSecond, effective defenses across domains must be developed to close these robustness gaps. With security on the line, developers cannot ignore the threat of attacks exploiting model weaknesses. Moreover, we cannot afford any fragility-induced failures for safety-critical applications like self-driving vehicles and medical diagnosis. Lives are at stake.\nFinally, the research community continues mobilizing rapidly in response. Interest in adversarial machine learning has exploded as attacks reveal the need to bridge the robustness gap between synthetic and real-world data. Conferences now commonly feature defenses for securing and stabilizing models. The community recognizes that model fragility is a critical issue that must be addressed through robustness testing, defense development, and ongoing research. By surfacing blindspots and responding with principled defenses, we can work to ensure reliability and safety for machine learning systems, especially in high-stakes domains.\n\n\n16.5.5 Building Interpretable Models\nAs models are deployed more frequently in high-stakes settings, practitioners, developers, downstream end-users, and increasing regulation have highlighted the need for explainability in machine learning. The goal of many interpretability and explainability methods is to provide practitioners with more information about the models’ overall behavior or the behavior given a specific input. This allows users to decide whether or not a model’s output or prediction is trustworthy.\nSuch analysis can help developers debug models and improve performance by pointing out biases, spurious correlations, and failure modes of models. In cases where models can surpass human performance on a task, interpretability can help users and researchers better understand relationships in their data and previously unknown patterns.\nThere are many classes of explainability/interpretability methods, including post hoc explainability, inherent interpretability, and mechanistic interpretability. These methods aim to make complex machine learning models more understandable and ensure users can trust model predictions, especially in critical settings. By providing transparency into model behavior, explainability techniques are an important tool for developing safe, fair, and reliable AI systems.\n\nPost Hoc Explainability\nPost hoc explainability methods typically explain the output behavior of a black-box model on a specific input. Popular methods include counterfactual explanations, feature attribution methods, and concept-based explanations.\nCounterfactual explanations, also frequently called algorithmic recourse, “If X had not occurred, Y would not have occurred” (Wachter, Mittelstadt, and Russell 2017). For example, consider a person applying for a bank loan whose application is rejected by a model. They may ask their bank for recourse or how to change to be eligible for a loan. A counterfactual explanation would tell them which features they need to change and by how much such that the model’s prediction changes.\n\nWachter, Sandra, Brent Mittelstadt, and Chris Russell. 2017. “Counterfactual Explanations Without Opening the Black Box: Automated Decisions and the GDPR.” SSRN Electronic Journal 31: 841. https://doi.org/10.2139/ssrn.3063289.\n\nSelvaraju, Ramprasaath R., Michael Cogswell, Abhishek Das, Ramakrishna Vedantam, Devi Parikh, and Dhruv Batra. 2017. “Grad-CAM: Visual Explanations from Deep Networks via Gradient-Based Localization.” In 2017 IEEE International Conference on Computer Vision (ICCV), 618–26. IEEE. https://doi.org/10.1109/iccv.2017.74.\n\nSmilkov, Daniel, Nikhil Thorat, Been Kim, Fernanda Viégas, and Martin Wattenberg. 2017. “Smoothgrad: Removing Noise by Adding Noise.” ArXiv Preprint abs/1706.03825. https://arxiv.org/abs/1706.03825.\n\nRibeiro, Marco Tulio, Sameer Singh, and Carlos Guestrin. 2016. “” Why Should i Trust You?” Explaining the Predictions of Any Classifier.” In Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, 1135–44.\n\nLundberg, Scott M., and Su-In Lee. 2017. “A Unified Approach to Interpreting Model Predictions.” In Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017, December 4-9, 2017, Long Beach, CA, USA, edited by Isabelle Guyon, Ulrike von Luxburg, Samy Bengio, Hanna M. Wallach, Rob Fergus, S. V. N. Vishwanathan, and Roman Garnett, 4765–74. https://proceedings.neurips.cc/paper/2017/hash/8a20a8621978632d76c43dfd28b67767-Abstract.html.\nFeature attribution methods highlight the input features that are important or necessary for a particular prediction. For a computer vision model, this would mean highlighting the individual pixels that contributed most to the predicted label of the image. Note that these methods do not explain how those pixels/features impact the prediction, only that they do. Common methods include input gradients, GradCAM (Selvaraju et al. 2017), SmoothGrad (Smilkov et al. 2017), LIME (Ribeiro, Singh, and Guestrin 2016), and SHAP (Lundberg and Lee 2017).\nBy providing examples of changes to input features that would alter a prediction (counterfactuals) or indicating the most influential features for a given prediction (attribution), these post hoc explanation techniques shed light on model behavior for individual inputs. This granular transparency helps users determine whether they can trust and act upon specific model outputs.\nConcept-based explanations aim to explain model behavior and outputs using a pre-defined set of semantic concepts (e.g., the model recognizes scene class “bedroom” based on the presence of concepts “bed” and “pillow”). Recent work shows that users often prefer these explanations to attribution and example-based explanations because they “resemble human reasoning and explanations” (Vikram V. Ramaswamy et al. 2023b). Popular concept-based explanation methods include TCAV (Cai et al. 2019), Network Dissection (Bau et al. 2017), and interpretable basis decomposition (Zhou et al. 2018).\n\nRamaswamy, Vikram V, Sunnie SY Kim, Ruth Fong, and Olga Russakovsky. 2023b. “UFO: A Unified Method for Controlling Understandability and Faithfulness Objectives in Concept-Based Explanations for CNNs.” ArXiv Preprint abs/2303.15632. https://arxiv.org/abs/2303.15632.\n\nCai, Carrie J., Emily Reif, Narayan Hegde, Jason Hipp, Been Kim, Daniel Smilkov, Martin Wattenberg, et al. 2019. “Human-Centered Tools for Coping with Imperfect Algorithms During Medical Decision-Making.” In Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems, edited by Jennifer G. Dy and Andreas Krause, 80:2673–82. Proceedings of Machine Learning Research. ACM. https://doi.org/10.1145/3290605.3300234.\n\nBau, David, Bolei Zhou, Aditya Khosla, Aude Oliva, and Antonio Torralba. 2017. “Network Dissection: Quantifying Interpretability of Deep Visual Representations.” In 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 3319–27. IEEE. https://doi.org/10.1109/cvpr.2017.354.\n\nZhou, Bolei, Yiyou Sun, David Bau, and Antonio Torralba. 2018. “Interpretable Basis Decomposition for Visual Explanation.” In Proceedings of the European Conference on Computer Vision (ECCV), 119–34.\n\nRamaswamy, Vikram V., Sunnie S. Y. Kim, Ruth Fong, and Olga Russakovsky. 2023a. “Overlooked Factors in Concept-Based Explanations: Dataset Choice, Concept Learnability, and Human Capability.” In 2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 10932–41. IEEE. https://doi.org/10.1109/cvpr52729.2023.01052.\nNote that these methods are extremely sensitive to the size and quality of the concept set, and there is a tradeoff between their accuracy and faithfulness and their interpretability or understandability to humans (Vikram V. Ramaswamy et al. 2023a). However, by mapping model predictions to human-understandable concepts, concept-based explanations can provide transparency into the reasoning behind model outputs.\n\n\nInherent Interpretability\nInherently interpretable models are constructed such that their explanations are part of the model architecture and are thus naturally faithful, which sometimes makes them preferable to post-hoc explanations applied to black-box models, especially in high-stakes domains where transparency is imperative (Rudin 2019). Often, these models are constrained so that the relationships between input features and predictions are easy for humans to follow (linear models, decision trees, decision sets, k-NN models), or they obey structural knowledge of the domain, such as monotonicity (Gupta et al. 2016), causality, or additivity (Lou et al. 2013; Beck and Jackman 1998).\n\nRudin, Cynthia. 2019. “Stop Explaining Black Box Machine Learning Models for High Stakes Decisions and Use Interpretable Models Instead.” Nature Machine Intelligence 1 (5): 206–15. https://doi.org/10.1038/s42256-019-0048-x.\n\nGupta, Maya, Andrew Cotter, Jan Pfeifer, Konstantin Voevodski, Kevin Canini, Alexander Mangylov, Wojciech Moczydlowski, and Alexander Van Esbroeck. 2016. “Monotonic Calibrated Interpolated Look-up Tables.” The Journal of Machine Learning Research 17 (1): 3790–3836.\n\nLou, Yin, Rich Caruana, Johannes Gehrke, and Giles Hooker. 2013. “Accurate Intelligible Models with Pairwise Interactions.” In Proceedings of the 19th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, edited by Inderjit S. Dhillon, Yehuda Koren, Rayid Ghani, Ted E. Senator, Paul Bradley, Rajesh Parekh, Jingrui He, Robert L. Grossman, and Ramasamy Uthurusamy, 623–31. ACM. https://doi.org/10.1145/2487575.2487579.\n\nBeck, Nathaniel, and Simon Jackman. 1998. “Beyond Linearity by Default: Generalized Additive Models.” Am. J. Polit. Sci. 42 (2): 596. https://doi.org/10.2307/2991772.\n\nKoh, Pang Wei, Thao Nguyen, Yew Siang Tang, Stephen Mussmann, Emma Pierson, Been Kim, and Percy Liang. 2020. “Concept Bottleneck Models.” In Proceedings of the 37th International Conference on Machine Learning, ICML 2020, 13-18 July 2020, Virtual Event, 119:5338–48. Proceedings of Machine Learning Research. PMLR. http://proceedings.mlr.press/v119/koh20a.html.\n\nChen, Chaofan, Oscar Li, Daniel Tao, Alina Barnett, Cynthia Rudin, and Jonathan Su. 2019. “This Looks Like That: Deep Learning for Interpretable Image Recognition.” In Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada, edited by Hanna M. Wallach, Hugo Larochelle, Alina Beygelzimer, Florence d’Alché-Buc, Emily B. Fox, and Roman Garnett, 8928–39. https://proceedings.neurips.cc/paper/2019/hash/adf7ee2dcf142b0e11888e72b43fcb75-Abstract.html.\nHowever, more recent works have relaxed the restrictions on inherently interpretable models, using black-box models for feature extraction and a simpler inherently interpretable model for classification, allowing for faithful explanations that relate high-level features to prediction. For example, Concept Bottleneck Models (Koh et al. 2020) predict a concept set c that is passed into a linear classifier. ProtoPNets (Chen et al. 2019) dissect inputs into linear combinations of similarities to prototypical parts from the training set.\n\n\nMechanistic Interpretability\nMechanistic interpretability methods seek to reverse engineer neural networks, often analogizing them to how one might reverse engineer a compiled binary or how neuroscientists attempt to decode the function of individual neurons and circuits in brains. Most research in mechanistic interpretability views models as a computational graph (Geiger et al. 2021), and circuits are subgraphs with distinct functionality (Wang and Zhan 2019). Current approaches to extracting circuits from neural networks and understanding their functionality rely on human manual inspection of visualizations produced by circuits (Olah et al. 2020).\n\nGeiger, Atticus, Hanson Lu, Thomas Icard, and Christopher Potts. 2021. “Causal Abstractions of Neural Networks.” In Advances in Neural Information Processing Systems 34: Annual Conference on Neural Information Processing Systems 2021, NeurIPS 2021, December 6-14, 2021, Virtual, edited by Marc’Aurelio Ranzato, Alina Beygelzimer, Yann N. Dauphin, Percy Liang, and Jennifer Wortman Vaughan, 9574–86. https://proceedings.neurips.cc/paper/2021/hash/4f5c422f4d49a5a807eda27434231040-Abstract.html.\n\nWang, LingFeng, and YaQing Zhan. 2019. “A Conceptual Peer Review Model for arXiv and Other Preprint Databases.” Learn. Publ. 32 (3): 213–19. https://doi.org/10.1002/leap.1229.\n\nOlah, Chris, Nick Cammarata, Ludwig Schubert, Gabriel Goh, Michael Petrov, and Shan Carter. 2020. “Zoom in: An Introduction to Circuits.” Distill 5 (3): e00024–001. https://doi.org/10.23915/distill.00024.001.\n\nDavarzani, Samaneh, David Saucier, Purva Talegaonkar, Erin Parker, Alana Turner, Carver Middleton, Will Carroll, et al. 2023. “Closing the Wearable Gap: Footankle Kinematic Modeling via Deep Learning Models Based on a Smart Sock Wearable.” Wearable Technologies 4. https://doi.org/10.1017/wtc.2023.3.\nAlternatively, some approaches build sparse autoencoders that encourage neurons to encode disentangled interpretable features (Davarzani et al. 2023). This field is much newer than existing areas in explainability and interpretability, and as such, most works are generally exploratory rather than solution-oriented.\nThere are many problems in mechanistic interpretability, including the polysemanticity of neurons and circuits, the inconvenience and subjectivity of human labeling, and the exponential search space for identifying circuits in large models with billions or trillions of neurons.\n\n\nChallenges and Considerations\nAs methods for interpreting and explaining models progress, it is important to note that humans overtrust and misuse interpretability tools (Kaur et al. 2020) and that a user’s trust in a model due to an explanation can be independent of the correctness of the explanations (Lakkaraju and Bastani 2020). As such, it is necessary that aside from assessing the faithfulness/correctness of explanations, researchers must also ensure that interpretability methods are developed and deployed with a specific user in mind and that user studies are performed to evaluate their efficacy and usefulness in practice.\n\nKaur, Harmanpreet, Harsha Nori, Samuel Jenkins, Rich Caruana, Hanna Wallach, and Jennifer Wortman Vaughan. 2020. “Interpreting Interpretability: Understanding Data Scientists’ Use of Interpretability Tools for Machine Learning.” In Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems, edited by Regina Bernhaupt, Florian ’Floyd’Mueller, David Verweij, Josh Andres, Joanna McGrenere, Andy Cockburn, Ignacio Avellino, et al., 1–14. ACM. https://doi.org/10.1145/3313831.3376219.\n\nLakkaraju, Himabindu, and Osbert Bastani. 2020. “”How Do i Fool You?”: Manipulating User Trust via Misleading Black Box Explanations.” In Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society, 79–85. ACM. https://doi.org/10.1145/3375627.3375833.\nFurthermore, explanations should be tailored to the user’s expertise, the task they are using the explanation for and the corresponding minimal amount of information required for the explanation to be useful to prevent information overload.\nWhile interpretability/explainability are popular areas in machine learning research, very few works study their intersection with TinyML and edge computing. Given that a significant application of TinyML is healthcare, which often requires high transparency and interpretability, existing techniques must be tested for scalability and efficiency concerning edge devices. Many methods rely on extra forward and backward passes, and some even require extensive training in proxy models, which are infeasible on resource-constrained microcontrollers.\nThat said, explainability methods can be highly useful in developing models for edge devices, as they can give insights into how input data and models can be compressed and how representations may change post-compression. Furthermore, many interpretable models are often smaller than their black-box counterparts, which could benefit TinyML applications.\n\n\n\n16.5.6 Monitoring Model Performance\nWhile developers may train models that seem adversarially robust, fair, and interpretable before deployment, it is imperative that both the users and the model owners continue to monitor the model’s performance and trustworthiness during the model’s full lifecycle. Data is frequently changing in practice, which can often result in distribution shifts. These distribution shifts can profoundly impact the model’s vanilla predictive performance and its trustworthiness (fairness, robustness, and interpretability) in real-world data.\nFurthermore, definitions of fairness frequently change with time, such as what society considers a protected attribute, and the expertise of the users asking for explanations may also change.\nTo ensure that models keep up to date with such changes in the real world, developers must continually evaluate their models on current and representative data and standards and update models when necessary.",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Responsible AI</span>"
    ]
  },
  {
    "objectID": "contents/core/responsible_ai/responsible_ai.html#implementation-challenges",
    "href": "contents/core/responsible_ai/responsible_ai.html#implementation-challenges",
    "title": "16  Responsible AI",
    "section": "16.6 Implementation Challenges",
    "text": "16.6 Implementation Challenges\n\n16.6.1 Organizational and Cultural Structures\nWhile innovation and regulation are often seen as having competing interests, many countries have found it necessary to provide oversight as AI systems expand into more sectors. As shown in in Figure 16.4, this oversight has become crucial as these systems continue permeating various industries and impacting people’s lives. Further discussion of this topic can be found in Human-Centered AI, Chapter 22 “Government Interventions and Regulations”.\n\n\n\n\n\n\nFigure 16.4: How various groups impact human-centered AI. Source: Shneiderman (2020).\n\n\nShneiderman, Ben. 2020. “Bridging the Gap Between Ethics and Practice: Guidelines for Reliable, Safe, and Trustworthy Human-Centered AI Systems.” ACM Trans. Interact. Intell. Syst. 10 (4): 1–31. https://doi.org/10.1145/3419764.\n\n\nThroughout this chapter, we have touched on several key policies aimed at guiding responsible AI development and deployment. Below is a summary of these policies, alongside additional noteworthy frameworks that reflect a global push for transparency in AI systems:\n\nThe European Union’s General Data Protection Regulation (GDPR) mandates transparency and data protection measures for AI systems handling personal data.\n\nThe AI Bill of Rights outlines principles for ethical AI use in the United States, emphasizing fairness, privacy, and accountability.\n\nThe California Consumer Privacy Act (CCPA) protects consumer data and holds organizations accountable for data misuse.\n\nCanada’s Responsible Use of Artificial Intelligence outlines best practices for ethical AI deployment.\n\nJapan’s Act on the Protection of Personal Information (APPI) establishes guidelines for handling personal data in AI systems.\n\nCanada’s proposed Consumer Privacy Protection Act (CPPA) aims to strengthen privacy protections in digital ecosystems.\n\nThe European Commission’s White Paper on Artificial Intelligence: A European Approach to Excellence and Trust emphasizes ethical AI development alongside innovation.\n\nThe UK’s Information Commissioner’s Office and Alan Turing Institute’s Guidance on Explaining AI Decisions provides recommendations for increasing AI transparency.\n\nThese policies highlight an ongoing global effort to balance innovation with accountability and ensure that AI systems are developed and deployed responsibly.\n\n\n16.6.2 Obtaining Quality and Representative Data\nAs discussed in the Data Engineering chapter, responsible AI design must occur at all pipeline stages, including data collection. This begs the question: what does it mean for data to be high-quality and representative? Consider the following scenarios that hinder the representativeness of data:\n\nSubgroup Imbalance\nThis is likely what comes to mind when hearing “representative data.” Subgroup imbalance means the dataset contains relatively more data from one subgroup than another. This imbalance can negatively affect the downstream ML model by causing it to overfit a subgroup of people while performing poorly on another.\nOne example consequence of subgroup imbalance is racial discrimination in facial recognition technology (Buolamwini and Gebru 2018); commercial facial recognition algorithms have up to 34% worse error rates on darker-skinned females than lighter-skinned males.\n\nBuolamwini, Joy, and Timnit Gebru. 2018. “Gender Shades: Intersectional Accuracy Disparities in Commercial Gender Classification.” In Conference on Fairness, Accountability and Transparency, 77–91. PMLR.\nNote that data imbalance goes both ways, and subgroups can also be harmful overrepresented in the dataset. For example, the Allegheny Family Screening Tool (AFST) predicts the likelihood that a child will eventually be removed from a home. The AFST produces disproportionate scores for different subgroups, one of the reasons being that it is trained on historically biased data, sourced from juvenile and adult criminal legal systems, public welfare agencies, and behavioral health agencies and programs.\n\n\nQuantifying Target Outcomes\nThis occurs in applications where the ground-truth label cannot be measured or is difficult to represent in a single quantity. For example, an ML model in a mobile wellness application may want to predict individual stress levels. The true stress labels themselves are impossible to obtain directly and must be inferred from other biosignals, such as heart rate variability and user self-reported data. In these situations, noise is built into the data by design, making this a challenging ML task.\n\n\nDistribution Shift\nData may no longer represent a task if a major external event causes the data source to change drastically. The most common way to think about distribution shifts is with respect to time; for example, data on consumer shopping habits collected pre-covid may no longer be present in consumer behavior today.\nThe transfer causes another form of distribution shift. For instance, when applying a triage system that was trained on data from one hospital to another, a distribution shift may occur if the two hospitals are very different.\n\n\nGathering Data\nA reasonable solution for many of the above problems with non-representative or low-quality data is to collect more; we can collect more data targeting an underrepresented subgroup or from the target hospital to which our model might be transferred. However, for some reasons, gathering more data is an inappropriate or infeasible solution for the task at hand.\n\nData collection can be harmful. This is the paradox of exposure, the situation in which those who stand to significantly gain from their data being collected are also those who are put at risk by the collection process (D’ignazio and Klein (2023), Chapter 4). For example, collecting more data on non-binary individuals may be important for ensuring the fairness of the ML application, but it also puts them at risk, depending on who is collecting the data and how (whether the data is easily identifiable, contains sensitive content, etc.).\nData collection can be costly. In some domains, such as healthcare, obtaining data can be costly in terms of time and money.\nBiased data collection. Electronic Health Records is a huge data source for ML-driven healthcare applications. Issues of subgroup representation aside, the data itself may be collected in a biased manner. For example, negative language (“nonadherent,” “unwilling”) is disproportionately used on black patients (Himmelstein, Bates, and Zhou 2022).\n\n\nD’ignazio, Catherine, and Lauren F Klein. 2023. Data Feminism. MIT press.\n\nHimmelstein, Gracie, David Bates, and Li Zhou. 2022. “Examination of Stigmatizing Language in the Electronic Health Record.” JAMA Network Open 5 (1): e2144967. https://doi.org/10.1001/jamanetworkopen.2021.44967.\nWe conclude with several additional strategies for maintaining data quality. First, fostering a deeper understanding of the data is crucial. This can be achieved through the implementation of standardized labels and measures of data quality, such as in the Data Nutrition Project. Collaborating with organizations responsible for collecting data helps ensure the data is interpreted correctly. Second, employing effective tools for data exploration is important. Visualization techniques and statistical analyses can reveal issues with the data. Finally, establishing a feedback loop within the ML pipeline is essential for understanding the real-world implications of the data. Metrics, such as fairness measures, allow us to define “data quality” in the context of the downstream application; improving fairness may directly improve the quality of the predictions that the end users receive.\n\n\n\n16.6.3 Balancing Accuracy and Other Objectives\nMachine learning models are often evaluated on accuracy alone, but this single metric cannot fully capture model performance and tradeoffs for responsible AI systems. Other ethical dimensions, such as fairness, robustness, interpretability, and privacy, may compete with pure predictive accuracy during model development. For instance, inherently interpretable models such as small decision trees or linear classifiers with simplified features intentionally trade some accuracy for transparency in the model behavior and predictions. While these simplified models achieve lower accuracy by not capturing all the complexity in the dataset, improved interpretability builds trust by enabling direct analysis by human practitioners.\nAdditionally, certain techniques meant to improve adversarial robustness, such as adversarial training examples or dimensionality reduction, can degrade the accuracy of clean validation data. In sensitive applications like healthcare, focusing narrowly on state-of-the-art accuracy carries ethical risks if it allows models to rely more on spurious correlations that introduce bias or use opaque reasoning. Therefore, the appropriate performance objectives depend greatly on the sociotechnical context.\nMethodologies like Value Sensitive Design provide frameworks for formally evaluating the priorities of various stakeholders within the real-world deployment system. These explain the tensions between values like accuracy, interpretability and fairness, which can then guide responsible tradeoff decisions. For a medical diagnosis system, achieving the highest accuracy may not be the singular goal - improving transparency to build practitioner trust or reducing bias towards minority groups could justify small losses in accuracy. Analyzing the sociotechnical context is key for setting these objectives.\nBy taking a holistic view, we can responsibly balance accuracy with other ethical objectives for model success. Ongoing performance monitoring along multiple dimensions is crucial as the system evolves after deployment.",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Responsible AI</span>"
    ]
  },
  {
    "objectID": "contents/core/responsible_ai/responsible_ai.html#ethical-considerations-in-ai-design",
    "href": "contents/core/responsible_ai/responsible_ai.html#ethical-considerations-in-ai-design",
    "title": "16  Responsible AI",
    "section": "16.7 Ethical Considerations in AI Design",
    "text": "16.7 Ethical Considerations in AI Design\nWe must discuss at least some of the many ethical issues at stake in designing and applying AI systems and diverse frameworks for approaching these issues, including those from AI safety, Human-Computer Interaction (HCI), and Science, Technology, and Society (STS).\n\n16.7.1 AI Safety and Value Alignment\nIn 1960, Norbert Weiner wrote, “’if we use, to achieve our purposes, a mechanical agency with whose operation we cannot interfere effectively… we had better be quite sure that the purpose put into the machine is the purpose which we desire” (Wiener 1960).\n\nWiener, Norbert. 1960. “Some Moral and Technical Consequences of Automation: As Machines Learn They May Develop Unforeseen Strategies at Rates That Baffle Their Programmers.” Science 131 (3410): 1355–58. https://doi.org/10.1126/science.131.3410.1355.\n\nRussell, Stuart. 2021. “Human-Compatible Artificial Intelligence.” Human-Like Machine Intelligence, 3–23.\nIn recent years, as the capabilities of deep learning models have achieved, and sometimes even surpassed, human abilities, the issue of creating AI systems that act in accord with human intentions instead of pursuing unintended or undesirable goals has become a source of concern (Russell 2021). Within the field of AI safety, a particular goal concerns “value alignment,” or the problem of how to code the “right” purpose into machines Human-Compatible Artificial Intelligence. Present AI research assumes we know the objectives we want to achieve and “studies the ability to achieve objectives, not the design of those objectives.”\nHowever, complex real-world deployment contexts make explicitly defining “the right purpose” for machines difficult, requiring frameworks for responsible and ethical goal-setting. Methodologies like Value Sensitive Design provide formal mechanisms to surface tensions between stakeholder values and priorities.\nBy taking a holistic sociotechnical view, we can better ensure intelligent systems pursue objectives that align with broad human intentions rather than maximizing narrow metrics like accuracy alone. Achieving this in practice remains an open and critical research question as AI capabilities advance rapidly.\nThe absence of this alignment can lead to several AI safety issues, as have been documented in a variety of deep learning models. A common feature of systems that optimize for an objective is that variables not directly included in the objective may be set to extreme values to help optimize for that objective, leading to issues characterized as specification gaming, reward hacking, etc., in reinforcement learning (RL).\nIn recent years, a particularly popular implementation of RL has been models pre-trained using self-supervised learning and fine-tuned reinforcement learning from human feedback (RLHF) (Christiano et al. 2017). Ngo 2022 (Ngo, Chan, and Mindermann 2022) argues that by rewarding models for appearing harmless and ethical while also maximizing useful outcomes, RLHF could encourage the emergence of three problematic properties: situationally aware reward hacking, where policies exploit human fallibility to gain high reward, misaligned internally-represented goals that generalize beyond the RLHF fine-tuning distribution, and power-seeking strategies.\n\nChristiano, Paul F., Jan Leike, Tom B. Brown, Miljan Martic, Shane Legg, and Dario Amodei. 2017. “Deep Reinforcement Learning from Human Preferences.” In Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017, December 4-9, 2017, Long Beach, CA, USA, edited by Isabelle Guyon, Ulrike von Luxburg, Samy Bengio, Hanna M. Wallach, Rob Fergus, S. V. N. Vishwanathan, and Roman Garnett, 4299–4307. https://proceedings.neurips.cc/paper/2017/hash/d5e2c0adad503c91f91df240d0cd4e49-Abstract.html.\n\nNgo, Richard, Lawrence Chan, and Sören Mindermann. 2022. “The Alignment Problem from a Deep Learning Perspective.” ArXiv Preprint abs/2209.00626. https://arxiv.org/abs/2209.00626.\n\nVan Noorden, Richard. 2016. “ArXiv Preprint Server Plans Multimillion-Dollar Overhaul.” Nature 534 (7609): 602–2. https://doi.org/10.1038/534602a.\nSimilarly, Van Noorden (2016) outlines six concrete problems for AI safety, including avoiding negative side effects, avoiding reward hacking, scalable oversight for aspects of the objective that are too expensive to be frequently evaluated during training, safe exploration strategies that encourage creativity while preventing harm, and robustness to distributional shift in unseen testing environments.\n\n\n16.7.2 Autonomous Systems and Control [and Trust]\nThe consequences of autonomous systems that act independently of human oversight and often outside human judgment have been well documented across several industries and use cases. Most recently, the California Department of Motor Vehicles suspended Cruise’s deployment and testing permits for its autonomous vehicles citing “unreasonable risks to public safety”. One such accident occurred when a vehicle struck a pedestrian who stepped into a crosswalk after the stoplight had turned green, and the vehicle was allowed to proceed. In 2018, a pedestrian crossing the street with her bike was killed when a self-driving Uber car, which was operating in autonomous mode, failed to accurately classify her moving body as an object to be avoided.\nAutonomous systems beyond self-driving vehicles are also susceptible to such issues, with potentially graver consequences, as remotely-powered drones are already reshaping warfare. While such incidents bring up important ethical questions regarding who should be held responsible when these systems fail, they also highlight the technical challenges of giving full control of complex, real-world tasks to machines.\nAt its core, there is a tension between human and machine autonomy. Engineering and computer science disciplines have tended to focus on machine autonomy. For example, as of 2019, a search for the word “autonomy” in the Digital Library of the Association for Computing Machinery (ACM) reveals that of the top 100 most cited papers, 90% are on machine autonomy (Calvo et al. 2020). In an attempt to build systems for the benefit of humanity, these disciplines have taken, without question, increasing productivity, efficiency, and automation as primary strategies for benefiting humanity.\n\nMcCarthy, John. 1981. “Epistemological Problems of Artificial Intelligence.” In Readings in Artificial Intelligence, 459–65. Elsevier. https://doi.org/10.1016/b978-0-934613-03-3.50035-0.\nThese goals put machine automation at the forefront, often at the expense of the human. This approach suffers from inherent challenges, as noted since the early days of AI through the Frame problem and qualification problem, which formalizes the observation that it is impossible to specify all the preconditions needed for a real-world action to succeed (McCarthy 1981).\nThese logical limitations have given rise to mathematical approaches such as Responsibility-sensitive safety (RSS) (Shalev-Shwartz, Shammah, and Shashua 2017), which is aimed at breaking down the end goal of an automated driving system (namely safety) into concrete and checkable conditions that can be rigorously formulated in mathematical terms. The goal of RSS is that those safety rules guarantee Automated Driving System (ADS) safety in the rigorous form of mathematical proof. However, such approaches tend towards using automation to address the problems of automation and are susceptible to many of the same issues.\n\nShalev-Shwartz, Shai, Shaked Shammah, and Amnon Shashua. 2017. “On a Formal Model of Safe and Scalable Self-Driving Cars.” ArXiv Preprint abs/1708.06374. https://arxiv.org/abs/1708.06374.\n\nFriedman, Batya. 1996. “Value-Sensitive Design.” Interactions 3 (6): 16–23. https://doi.org/10.1145/242485.242493.\n\nPeters, Dorian, Rafael A. Calvo, and Richard M. Ryan. 2018. “Designing for Motivation, Engagement and Wellbeing in Digital Experience.” Front. Psychol. 9 (May): 797. https://doi.org/10.3389/fpsyg.2018.00797.\n\nRyan, Richard M., and Edward L. Deci. 2000. “Self-Determination Theory and the Facilitation of Intrinsic Motivation, Social Development, and Well-Being.” Am. Psychol. 55 (1): 68–78. https://doi.org/10.1037/0003-066x.55.1.68.\nAnother approach to combating these issues is to focus on the human-centered design of interactive systems that incorporate human control. Value-sensitive design (Friedman 1996) described three key design factors for a user interface that impact autonomy, including system capability, complexity, misrepresentation, and fluidity. A more recent model, called METUX (A Model for Motivation, Engagement, and Thriving in the User Experience), leverages insights from Self-determination Theory (SDT) in Psychology to identify six distinct spheres of technology experience that contribute to the design systems that promote well-being and human flourishing (Peters, Calvo, and Ryan 2018). SDT defines autonomy as acting by one’s goals and values, which is distinct from the use of autonomy as simply a synonym for either independence or being in control (Ryan and Deci 2000).\nCalvo et al. (2020) elaborates on METUX and its six “spheres of technology experience” in the context of AI-recommender systems. They propose these spheres—Adoption, Interface, Tasks, Behavior, Life, and Society—as a way of organizing thinking and evaluation of technology design in order to appropriately capture contradictory and downstream impacts on human autonomy when interacting with AI systems.\n\nCalvo, Rafael A, Dorian Peters, Karina Vold, and Richard M Ryan. 2020. “Supporting Human Autonomy in AI Systems: A Framework for Ethical Enquiry.” Ethics of Digital Well-Being: A Multidisciplinary Approach, 31–54.\n\n\n16.7.3 Economic Impacts on Jobs, Skills, Wages\nA major concern of the current rise of AI technologies is widespread unemployment. As AI systems’ capabilities expand, many fear these technologies will cause an absolute loss of jobs as they replace current workers and overtake alternative employment roles across industries. However, changing economic landscapes at the hands of automation is not new, and historically, have been found to reflect patterns of displacement rather than replacement (Shneiderman 2022)—Chapter 4. In particular, automation usually lowers costs and increases quality, greatly increasing access and demand. The need to serve these growing markets pushes production, creating new jobs.\n\n———. 2022. Human-Centered AI. Oxford University Press.\nFurthermore, studies have found that attempts to achieve “lights-out” automation – productive and flexible automation with a minimal number of human workers – have been unsuccessful. Attempts to do so have led to what the MIT Work of the Future taskforce has termed “zero-sum automation”, in which process flexibility is sacrificed for increased productivity.\nIn contrast, the task force proposes a “positive-sum automation” approach in which flexibility is increased by designing technology that strategically incorporates humans where they are very much needed, making it easier for line employees to train and debug robots, using a bottom-up approach to identifying what tasks should be automated; and choosing the right metrics for measuring success (see MIT’s Work of the Future).\nHowever, the optimism of the high-level outlook does not preclude individual harm, especially to those whose skills and jobs will be rendered obsolete by automation. Public and legislative pressure, as well as corporate social responsibility efforts, will need to be directed at creating policies that share the benefits of automation with workers and result in higher minimum wages and benefits.\n\n\n16.7.4 Scientific Communication and AI Literacy\nA 1993 survey of 3000 North American adults’ beliefs about the “electronic thinking machine” revealed two primary perspectives of the early computer: the “beneficial tool of man” perspective and the “awesome thinking machine” perspective. The attitudes contributing to the “awesome thinking machine” view in this and other studies revealed a characterization of computers as “intelligent brains, smarter than people, unlimited, fast, mysterious, and frightening” (Martin 1993). These fears highlight an easily overlooked component of responsible AI, especially amidst the rush to commercialize such technologies: scientific communication that accurately communicates the capabilities and limitations of these systems while providing transparency about the limitations of experts’ knowledge about these systems.\n\nMartin, C. Dianne. 1993. “The Myth of the Awesome Thinking Machine.” Commun. ACM 36 (4): 120–33. https://doi.org/10.1145/255950.153587.\n\nHandlin, Oscar. 1965. “Science and Technology in Popular Culture.” Daedalus-Us., 156–70.\nAs AI systems’ capabilities expand beyond most people’s comprehension, there is a natural tendency to assume the kinds of apocalyptic worlds painted by our media. This is partly due to the apparent difficulty of assimilating scientific information, even in technologically advanced cultures, which leads to the products of science being perceived as magic—“understandable only in terms of what it did, not how it worked” (Handlin 1965).\nWhile tech companies should be held responsible for limiting grandiose claims and not falling into cycles of hype, research studying scientific communication, especially concerning (generative) AI, will also be useful in tracking and correcting public understanding of these technologies. An analysis of the Scopus scholarly database found that such research is scarce, with only a handful of papers mentioning both “science communication” and “artificial intelligence” (Schäfer 2023).\n\nSchäfer, Mike S. 2023. “The Notorious GPT: Science Communication in the Age of Artificial Intelligence.” Journal of Science Communication 22 (02): Y02. https://doi.org/10.22323/2.22020402.\n\nLindgren, Simon. 2023. Handbook of Critical Studies of Artificial Intelligence. Edward Elgar Publishing.\n\nNg, Davy Tsz Kit, Jac Ka Lok Leung, Kai Wah Samuel Chu, and Maggie Shen Qiao. 2021. “AI Literacy: Definition, Teaching, Evaluation and Ethical Issues.” Proceedings of the Association for Information Science and Technology 58 (1): 504–9.\nResearch that exposes the perspectives, frames, and images of the future promoted by academic institutions, tech companies, stakeholders, regulators, journalists, NGOs, and others will also help to identify potential gaps in AI literacy among adults (Lindgren 2023). Increased focus on AI literacy from all stakeholders will be important in helping people whose skills are rendered obsolete by AI automation (Ng et al. 2021).\n“But even those who never acquire that understanding need assurance that there is a connection between the goals of science and their welfare, and above all, that the scientist is not a man altogether apart but one who shares some of their value.” (Handlin, 1965)",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Responsible AI</span>"
    ]
  },
  {
    "objectID": "contents/core/responsible_ai/responsible_ai.html#conclusion",
    "href": "contents/core/responsible_ai/responsible_ai.html#conclusion",
    "title": "16  Responsible AI",
    "section": "16.8 Conclusion",
    "text": "16.8 Conclusion\nResponsible artificial intelligence is crucial as machine learning systems exert growing influence across healthcare, employment, finance, and criminal justice sectors. While AI promises immense benefits, thoughtlessly designed models risk perpetrating harm through biases, privacy violations, unintended behaviors, and other pitfalls.\nUpholding principles of fairness, explainability, accountability, safety, and transparency enables the development of ethical AI aligned with human values. However, implementing these principles involves surmounting complex technical and social challenges around detecting dataset biases, choosing appropriate model tradeoffs, securing quality training data, and more. Frameworks like value-sensitive design guide balancing accuracy versus other objectives based on stakeholder needs.\nLooking forward, advancing responsible AI necessitates continued research and industry commitment. More standardized benchmarks are required to compare model biases and robustness. As personalized TinyML expands, enabling efficient transparency and user control for edge devices warrants focus. Revised incentive structures and policies must encourage deliberate, ethical development before reckless deployment. Education around AI literacy and its limitations will further contribute to public understanding.\nResponsible methods underscore that while machine learning offers immense potential, thoughtless application risks adverse consequences. Cross-disciplinary collaboration and human-centered design are imperative so AI can promote broad social benefit. The path ahead lies not in an arbitrary checklist but in a steadfast commitment to understand and uphold our ethical responsibility at each step. By taking conscientious action, the machine learning community can lead AI toward empowering all people equitably and safely.",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Responsible AI</span>"
    ]
  },
  {
    "objectID": "contents/core/responsible_ai/responsible_ai.html#sec-responsible-ai-resource",
    "href": "contents/core/responsible_ai/responsible_ai.html#sec-responsible-ai-resource",
    "title": "16  Responsible AI",
    "section": "16.9 Resources",
    "text": "16.9 Resources\nHere is a curated list of resources to support students and instructors in their learning and teaching journeys. We are continuously working on expanding this collection and will be adding new exercises soon.\n\n\n\n\n\n\nSlides\n\n\n\n\n\nThese slides are a valuable tool for instructors to deliver lectures and for students to review the material at their own pace. We encourage students and instructors to leverage these slides to improve their understanding and facilitate effective knowledge transfer.\n\nWhat am I building? What is the goal?\nWho is the audience?\nWhat are the consequences?\nResponsible Data Collection.\n\n\n\n\n\n\n\n\n\n\nVideos\n\n\n\n\n\n\nVideo 16.1\n\n\n\n\n\n\n\n\n\n\nExercises\n\n\n\n\n\nTo reinforce the concepts covered in this chapter, we have curated a set of exercises that challenge students to apply their knowledge and deepen their understanding.\n\nComing soon.",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Responsible AI</span>"
    ]
  },
  {
    "objectID": "contents/core/sustainable_ai/sustainable_ai.html",
    "href": "contents/core/sustainable_ai/sustainable_ai.html",
    "title": "17  Sustainable AI",
    "section": "",
    "text": "Purpose\nResources: Slides, Videos, Exercises\nWhat principles emerge when we consider machine learning systems through an ecological lens, and how does environmental stewardship reshape system architecture?\nThe ecological footprint of AI computation presents a fundamental challenge to technological advancement. Each design decision carries environmental implications, revealing essential tensions between computational capability and ecological responsibility. The pursuit of sustainable practices illuminates new approaches to system architecture that consider environmental impact as a primary design constraint. Understanding these ecological relationships provides insights into creating next-generation systems, establishing core principles for designing AI solutions that advance technology while preserving planetary resources.",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Sustainable AI</span>"
    ]
  },
  {
    "objectID": "contents/core/sustainable_ai/sustainable_ai.html#purpose",
    "href": "contents/core/sustainable_ai/sustainable_ai.html#purpose",
    "title": "17  Sustainable AI",
    "section": "",
    "text": "Learning Objectives\n\n\n\n\nUnderstand AI’s environmental impact, including energy consumption, carbon emissions, electronic waste, and biodiversity effects.\nLearn about methods and best practices for developing sustainable AI systems\nAppreciate the importance of taking a lifecycle perspective when evaluating and addressing the sustainability of AI systems.\nRecognize the roles various stakeholders, such as researchers, corporations, policymakers, and end users, play in furthering responsible and sustainable AI progress.\nLearn about specific frameworks, metrics, and tools to enable greener AI development.\nAppreciate real-world case studies like Google’s 4M efficiency practices that showcase how organizations are taking tangible steps to improve AI’s environmental record",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Sustainable AI</span>"
    ]
  },
  {
    "objectID": "contents/core/sustainable_ai/sustainable_ai.html#overview",
    "href": "contents/core/sustainable_ai/sustainable_ai.html#overview",
    "title": "17  Sustainable AI",
    "section": "17.1 Overview",
    "text": "17.1 Overview\nThe rapid advancements in artificial intelligence (AI) and machine learning (ML) have led to many beneficial applications and optimizations for performance efficiency. However, the remarkable growth of AI comes with a significant yet often overlooked cost: its environmental impact. The most recent report released by the IPCC, the international body leading scientific assessments of climate change and its impacts, emphasized the pressing importance of tackling climate change. Without immediate efforts to decrease global \\(\\textrm{CO}_2\\) emissions by at least 43 percent before 2030, we exceed global warming of 1.5 degrees Celsius (Winkler et al. 2022). This could initiate positive feedback loops, pushing temperatures even higher. Next to environmental issues, the United Nations recognized 17 Sustainable Development Goals (SDGs), in which AI can play an important role, and vice versa, play an important role in the development of AI systems. As the field continues expanding, considering sustainability is crucial.\n\nWinkler, Harald, Franck Lecocq, Hans Lofgren, Maria Virginia Vilariño, Sivan Kartha, and Joana Portugal-Pereira. 2022. “Examples of Shifting Development Pathways: Lessons on How to Enable Broader, Deeper, and Faster Climate Action.” Climate Action 1 (1). https://doi.org/10.1007/s44168-022-00026-1.\n\nMaslej, Nestor, Loredana Fattorini, Erik Brynjolfsson, John Etchemendy, Katrina Ligett, Terah Lyons, James Manyika, et al. 2023. “Artificial Intelligence Index Report 2023.” ArXiv Preprint abs/2310.03715. https://arxiv.org/abs/2310.03715.\nAI systems, particularly large language models like GPT-3 and computer vision models like DALL-E 2, require massive amounts of computational resources for training. For example, GPT-3 was estimated to consume 1,300 megawatt-hours of electricity, which is equal to 1,450 average US households in an entire month (Maslej et al. 2023), or put another way, it consumed enough energy to supply an average US household for 120 years! This immense energy demand stems primarily from power-hungry data centers with servers running intense computations to train these complex neural networks for days or weeks.\nCurrent estimates indicate that the carbon emissions produced from developing a single, sophisticated AI model can equal the emissions over the lifetime of five standard gasoline-powered vehicles (Strubell, Ganesh, and McCallum 2019). A significant portion of the electricity presently consumed by data centers is generated from nonrenewable sources such as coal and natural gas, resulting in data centers contributing around 1% of total worldwide carbon emissions. This is comparable to the emissions from the entire airline sector. This immense carbon footprint demonstrates the pressing need to transition to renewable power sources such as solar and wind to operate AI development.\n\nPrakash, Shvetank, Matthew Stewart, Colby Banbury, Mark Mazumder, Pete Warden, Brian Plancher, and Vijay Janapa Reddi. 2023. “Is TinyML Sustainable? Assessing the Environmental Impacts of Machine Learning on Microcontrollers.” ArXiv Preprint. https://arxiv.org/abs/2301.11899.\nAdditionally, even small-scale AI systems deployed to edge devices as part of TinyML have environmental impacts that should not be ignored (Prakash, Stewart, et al. 2023). The specialized hardware required for AI has an environmental toll from natural resource extraction and manufacturing. GPUs, CPUs, and chips like TPUs depend on rare earth metals whose mining and processing generate substantial pollution. The production of these components also has its energy demands. Furthermore, collecting, storing, and preprocessing data used to train both small- and large-scale models comes with environmental costs, further exacerbating the sustainability implications of ML systems.\nThus, while AI promises innovative breakthroughs in many fields, sustaining progress requires addressing sustainability challenges. AI can continue advancing responsibly by optimizing models’ efficiency, exploring alternative specialized hardware and renewable energy sources for data centers, and tracking its overall environmental impact.",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Sustainable AI</span>"
    ]
  },
  {
    "objectID": "contents/core/sustainable_ai/sustainable_ai.html#social-and-ethical-responsibility",
    "href": "contents/core/sustainable_ai/sustainable_ai.html#social-and-ethical-responsibility",
    "title": "17  Sustainable AI",
    "section": "17.2 Social and Ethical Responsibility",
    "text": "17.2 Social and Ethical Responsibility\nThe environmental impact of AI is not just a technical issue but also an ethical and social one. As AI becomes more integrated into our lives and industries, its sustainability becomes increasingly critical.\n\n17.2.1 Ethical Considerations\nThe scale of AI’s environmental footprint raises profound ethical questions about the responsibilities of AI developers and companies to minimize their carbon emissions and energy usage. As the creators of AI systems and technologies that can have sweeping global impacts, developers have an ethical obligation to consciously integrate environmental stewardship into their design process, even if sustainability comes at the cost of some efficiency gains.\nThere is a clear and present need for us to have open and honest conversations about AI’s environmental tradeoffs earlier in the development lifecycle. Researchers should feel empowered to voice concerns if organizational priorities do not align with ethical goals, as in the case of the open letter to pause giant AI experiments.\nAdditionally, there is an increasing need for AI companies to scrutinize their contributions to climate change and environmental harm. Large tech firms are responsible for the cloud infrastructure, data center energy demands, and resource extraction required to power today’s AI. Leadership should assess whether organizational values and policies promote sustainability, from hardware manufacturing through model training pipelines.\nFurthermore, more than voluntary self-regulation may be needed—governments may need to introduce new regulations aimed at sustainable AI standards and practices if we hope to curb the projected energy explosion of ever-larger models. Reported metrics like computing usage, carbon footprint, and efficiency benchmarks could hold organizations accountable.\nThrough ethical principles, company policies, and public rules, AI technologists and corporations have a profound duty to our planet to ensure the responsible and sustainable advancement of technology positioned to transform modern society radically. We owe it to future generations to get this right. Figure 17.1 outlines some ethical concerns and challenges facing AI.\n\n\n\n\n\n\nFigure 17.1: Ethical challenges in AI development. Source: COE\n\n\n\n\n\n17.2.2 Long-term Sustainability\nThe massive projected expansion of AI raises urgent concerns about its long-term sustainability. As AI software and applications rapidly increase in complexity and usage across industries, demand for computing power and infrastructure will skyrocket exponentially in the coming years.\nTo put the scale of projected growth in perspective, the total computing capacity required for training AI models saw an astonishing 350,000x increase from 2012 to 2019 (R. Schwartz et al. 2020). Researchers forecast over an order of magnitude growth each year moving forward as personalized AI assistants, autonomous technology, precision medicine tools, and more are developed. Similar trends are estimated for embedded ML systems, with an estimated 2.5 billion AI-enabled edge devices deployed by 2030.\nManaging this expansion level requires software and hardware-focused breakthroughs in efficiency and renewable integration from AI engineers and scientists. On the software side, novel techniques in model optimization, distillation, pruning, low-precision numerics, knowledge sharing between systems, and other areas must become widespread best practices to curb energy needs. For example, realizing even a 50% reduced computational demand per capability doubling would have massive compounding on total energy.\nOn the hardware infrastructure side, due to increasing costs of data transfer, storage, cooling, and space, continuing today’s centralized server farm model at data centers is likely infeasible long-term (Lannelongue, Grealey, and Inouye 2021). Exploring alternative decentralized computing options around “edge AI” on local devices or within telco networks can alleviate scaling pressures on power-hungry hyper scale data centers. Likewise, the shift towards carbon-neutral, hybrid renewable energy sources powering leading cloud provider data centers worldwide will be essential.\n\nLannelongue, Loı̈c, Jason Grealey, and Michael Inouye. 2021. “Green Algorithms: Quantifying the Carbon Footprint of Computation.” Adv. Sci. 8 (12): 2100707. https://doi.org/10.1002/advs.202100707.\n\n\n17.2.3 AI for Environmental Good\nWhile much focus goes on AI’s sustainability challenges, these powerful technologies provide unique solutions to combat climate change and drive environmental progress. For example, ML can continuously optimize smart power grids to improve renewable integration and electricity distribution efficiency across networks (Zhang, Han, and Deng 2018). Models can ingest the real-time status of a power grid and weather forecasts to allocate and shift sources responding to supply and demand.\n\nZhang, Dongxia, Xiaoqing Han, and Chunyu Deng. 2018. “Review on the Research and Practice of Deep Learning and Reinforcement Learning in Smart Grids.” CSEE Journal of Power and Energy Systems 4 (3): 362–70. https://doi.org/10.17775/cseejpes.2018.00520.\n\nLam, Remi, Alvaro Sanchez-Gonzalez, Matthew Willson, Peter Wirnsberger, Meire Fortunato, Ferran Alet, Suman Ravuri, et al. 2023. “Learning Skillful Medium-Range Global Weather Forecasting.” Science 382 (6677): 1416–21. https://doi.org/10.1126/science.adi2336.\n\nKurth, Thorsten, Shashank Subramanian, Peter Harrington, Jaideep Pathak, Morteza Mardani, David Hall, Andrea Miele, Karthik Kashinath, and Anima Anandkumar. 2023. “FourCastNet: Accelerating Global High-Resolution Weather Forecasting Using Adaptive Fourier Neural Operators.” In Proceedings of the Platform for Advanced Scientific Computing Conference, 1–11. ACM. https://doi.org/10.1145/3592979.3593412.\nFine-tuned neural networks have also proven remarkably effective at next-generation weather forecasting (Lam et al. 2023) and climate modeling (Kurth et al. 2023). They can rapidly analyze massive volumes of climate data to boost extreme event preparation and resource planning for hurricanes, floods, droughts, and more. Climate researchers have achieved state-of-the-art storm path accuracy by combining AI simulations with traditional numerical models.\nAI also enables better tracking of biodiversity (Silvestro et al. 2022), wildlife (D. Schwartz et al. 2021), ecosystems, and illegal deforestation using drones and satellite feeds. Computer vision algorithms can automate species population estimates and habitat health assessments over huge untracked regions. These capabilities provide conservationists with powerful tools for combating poaching (Bondi et al. 2018), reducing species extinction risks, and understanding ecological shifts.\n\nSilvestro, Daniele, Stefano Goria, Thomas Sterner, and Alexandre Antonelli. 2022. “Improving Biodiversity Protection Through Artificial Intelligence.” Nature Sustainability 5 (5): 415–24. https://doi.org/10.1038/s41893-022-00851-6.\n\nSchwartz, Daniel, Jonathan Michael Gomes Selman, Peter Wrege, and Andreas Paepcke. 2021. “Deployment of Embedded Edge-AI for Wildlife Monitoring in Remote Regions.” In 2021 20th IEEE International Conference on Machine Learning and Applications (ICMLA), 1035–42. IEEE; IEEE. https://doi.org/10.1109/icmla52953.2021.00170.\n\nBondi, Elizabeth, Ashish Kapoor, Debadeepta Dey, James Piavis, Shital Shah, Robert Hannaford, Arvind Iyer, Lucas Joppa, and Milind Tambe. 2018. “Near Real-Time Detection of Poachers from Drones in AirSim.” In Proceedings of the Twenty-Seventh International Joint Conference on Artificial Intelligence, edited by Jérôme Lang, 5814–16. International Joint Conferences on Artificial Intelligence Organization. https://doi.org/10.24963/ijcai.2018/847.\nTargeted investment in AI applications for environmental sustainability, cross-sector data sharing, and model accessibility can profoundly accelerate solutions to pressing ecological issues. Emphasizing AI for social good steers innovation in cleaner directions, guiding these world-shaping technologies towards ethical and responsible development.\n\n\n17.2.4 Case Study: DeepMind’s AI for AI Energy Efficiency\nGoogle’s data centers are foundational to powering products like Search, Gmail, and YouTube, which are used by billions daily. However, keeping the vast server farms up and running requires substantial energy, particularly for vital cooling systems. Google continuously strives to improve efficiency across operations. Yet progress was proving difficult through traditional methods alone, considering the complex, custom dynamics involved. This challenge prompted an ML breakthrough, yielding potential savings.\nAfter over a decade of optimizing data center design, inventing energy-efficient computing hardware, and securing renewable energy sources, Google brought DeepMind scientists to unlock further advances. The AI experts faced intricate factors surrounding the functioning of industrial cooling apparatuses. Equipment like pumps and chillers interact nonlinearly, while external weather and internal architectural variables also change. Capturing this complexity confounded rigid engineering formulas and human intuition.\nThe DeepMind team leveraged Google’s extensive historical sensor data detailing temperatures, power draw, and other attributes as training inputs. They built a flexible system based on neural networks to model the relationships and predict optimal configurations, minimizing power usage effectiveness (PUE) (Barroso, Hölzle, and Ranganathan 2019); PUE is the standard measurement for gauging how efficiently a data center uses energy gives the proportion of total facility power consumed divided by the power directly used for computing operations. When tested live, the AI system delivered remarkable gains beyond prior innovations, lowering cooling energy by 40% for a 15% drop in total PUE, a new site record. The generalizable framework learned cooling dynamics rapidly across shifting conditions that static rules could not match. The breakthrough highlights AI’s rising role in transforming modern tech and enabling a sustainable future.\n\nBarroso, Luiz André, Urs Hölzle, and Parthasarathy Ranganathan. 2019. The Datacenter as a Computer: Designing Warehouse-Scale Machines. Springer International Publishing. https://doi.org/10.1007/978-3-031-01761-2.",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Sustainable AI</span>"
    ]
  },
  {
    "objectID": "contents/core/sustainable_ai/sustainable_ai.html#energy-consumption",
    "href": "contents/core/sustainable_ai/sustainable_ai.html#energy-consumption",
    "title": "17  Sustainable AI",
    "section": "17.3 Energy Consumption",
    "text": "17.3 Energy Consumption\n\n17.3.1 Understanding Energy Needs\nUnderstanding the energy needs for training and operating AI models is crucial in the rapidly evolving field of A.I. With AI entering widespread use in many new fields (Bohr and Memarzadeh 2020; Sudhakar, Sze, and Karaman 2023), the demand for AI-enabled devices and data centers is expected to explode. This understanding helps us understand why AI, particularly deep learning, is often labeled energy-intensive.\n\nBohr, Adam, and Kaveh Memarzadeh. 2020. “The Rise of Artificial Intelligence in Healthcare Applications.” In Artificial Intelligence in Healthcare, 25–60. Elsevier. https://doi.org/10.1016/b978-0-12-818438-7.00002-2.\n\nEnergy Requirements for AI Training\nThe training of complex AI systems like large deep learning models can demand startlingly high levels of computing power–with profound energy implications. Consider OpenAI’s state-of-the-art language model GPT-3 as a prime example. This system pushes the frontiers of text generation through algorithms trained on massive datasets. Yet, the energy GPT-3 consumed for a single training cycle could rival an entire small town’s monthly usage. In recent years, these generative AI models have gained increasing popularity, leading to more models being trained. Next to the increased number of models, the number of parameters in these models will also increase. Research shows that increasing the model size (number of parameters), dataset size, and compute used for training improves performance smoothly with no signs of saturation (Kaplan et al. 2020). See how, in Figure 17.2, the test loss decreases as each of the 3 increases above.\n\n\n\n\n\n\nFigure 17.2: Performance improves with compute, dataset set, and model size. Source: Kaplan et al. (2020).\n\n\nKaplan, Jared, Sam McCandlish, Tom Henighan, Tom B. Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. 2020. “Scaling Laws for Neural Language Models.” ArXiv Preprint abs/2001.08361. https://arxiv.org/abs/2001.08361.\n\n\nWhat drives such immense requirements? During training, models like GPT-3 learn their capabilities by continuously processing huge volumes of data to adjust internal parameters. The processing capacity enabling AI’s rapid advances also contributes to surging energy usage, especially as datasets and models balloon. GPT-3 highlights a steady trajectory in the field where each leap in AI’s sophistication traces back to ever more substantial computational power and resources. Its predecessor, GPT-2, required 10x less training to compute only 1.5 billion parameters, a difference now dwarfed by magnitudes as GPT-3 comprises 175 billion parameters. Sustaining this trajectory toward increasingly capable AI raises energy and infrastructure provision challenges ahead.\n\n\nOperational Energy Use\nDeveloping and training AI models requires immense data, computing power, and energy. However, the deployment and operation of those models also incur significant recurrent resource costs over time. AI systems are now integrated across various industries and applications and are entering the daily lives of an increasing demographic. Their cumulative operational energy and infrastructure impacts could eclipse the upfront model training.\nThis concept is reflected in the demand for training and inference hardware in data centers and on the edge. Inference refers to using a trained model to make predictions or decisions on real-world data. According to a recent McKinsey analysis, the need for advanced systems to train ever-larger models is rapidly growing.\nHowever, inference computations already make up a dominant and increasing portion of total AI workloads, as shown in Figure 17.3. Running real-time inference with trained models–whether for image classification, speech recognition, or predictive analytics–invariably demands computing hardware like servers and chips. However, even a model handling thousands of facial recognition requests or natural language queries daily is dwarfed by massive platforms like Meta. Where inference on millions of photos and videos shared on social media, the infrastructure energy requirements continue to scale.\n\n\n\n\n\n\nFigure 17.3: Market size for inference and training hardware. Source: McKinsey.\n\n\n\nAlgorithms powering AI-enabled smart assistants, automated warehouses, self-driving vehicles, tailored healthcare, and more have marginal individual energy footprints. However, the projected proliferation of these technologies could add hundreds of millions of endpoints running AI algorithms continually, causing the scale of their collective energy requirements to surge. Current efficiency gains need help to counterbalance this sheer growth.\nAI is expected to see an annual growth rate of 37.3% between 2023 and 2030. Yet, applying the same growth rate to operational computing could multiply annual AI energy needs up to 1,000 times by 2030. So, while model optimization tackles one facet, responsible innovation must also consider total lifecycle costs at global deployment scales that were unfathomable just years ago but now pose infrastructure and sustainability challenges ahead.\n\n\n\n17.3.2 Data Centers and Their Impact\nAs the demand for AI services grows, the impact of data centers on the energy consumption of AI systems is becoming increasingly important. While these facilities are crucial for the advancement and deployment of AI, they contribute significantly to its energy footprint.\n\nScale\nData centers are the essential workhorses enabling the recent computational demands of advanced AI systems. For example, leading providers like Meta operate massive data centers spanning up to the size of multiple football fields, housing hundreds of thousands of high-capacity servers optimized for parallel processing and data throughput.\nThese massive facilities provide the infrastructure for training complex neural networks on vast datasets. For instance, based on leaked information, OpenAI’s language model GPT-4 was trained on Azure data centers packing over 25,000 Nvidia A100 GPUs, used continuously for over 90 to 100 days.\nAdditionally, real-time inference for consumer AI applications at scale is only made possible by leveraging the server farms inside data centers. Services like Alexa, Siri, and Google Assistant process billions of voice requests per month from users globally by relying on data center computing for low-latency response. In the future, expanding cutting-edge use cases like self-driving vehicles, precision medicine diagnostics, and accurate climate forecasting models will require significant computational resources to be obtained by tapping into vast on-demand cloud computing resources from data centers. Some emerging applications, like autonomous cars, have harsh latency and bandwidth constraints. Locating data center-level computing power on the edge rather than the cloud will be necessary.\nMIT research prototypes have shown trucks and cars with onboard hardware performing real-time AI processing of sensor data equivalent to small data centers (Sudhakar, Sze, and Karaman 2023). These innovative “data centers on wheels” demonstrate how vehicles like self-driving trucks may need embedded data center-scale compute on board to achieve millisecond system latency for navigation, though still likely supplemented by wireless 5G connectivity to more powerful cloud data centers.\n\nSudhakar, Soumya, Vivienne Sze, and Sertac Karaman. 2023. “Data Centers on Wheels: Emissions from Computing Onboard Autonomous Vehicles.” IEEE Micro 43 (1): 29–39. https://doi.org/10.1109/mm.2022.3219803.\nThe bandwidth, storage, and processing capacities required to enable this future technology at scale will depend heavily on advancements in data center infrastructure and AI algorithmic innovations.\n\n\nEnergy Demand\nThe energy demand of data centers can roughly be divided into 4 components—infrastructure, network, storage, and servers. In Figure 17.4, we see that the data infrastructure (which includes cooling, lighting, and controls) and the servers use most of the total energy budget of data centers in the US (Shehabi et al. 2016). This section breaks down the energy demand for the servers and the infrastructure. For the latter, the focus is on cooling systems, as cooling is the dominant factor in energy consumption in the infrastructure.\n\nShehabi, Arman, Sarah Smith, Dale Sartor, Richard Brown, Magnus Herrlin, Jonathan Koomey, Eric Masanet, Nathaniel Horner, Inês Azevedo, and William Lintner. 2016. “United States Data Center Energy Usage Report.”\n\n\n\n\n\n\nFigure 17.4: Data centers energy consumption in the US. Source: International Energy Agency (IEA).\n\n\n\n\nServers\nThe increase in energy consumption of data centers stems mainly from exponentially growing AI computing requirements. NVIDIA DGX H100 machines that are optimized for deep learning can draw up to 10.2 kW at peak. Leading providers operate data centers with hundreds to thousands of these power-hungry DGX nodes networked to train the latest AI models. For example, the supercomputer developed for OpenAI is a single system with over 285,000 CPU cores, 10,000 GPUs, and 400 gigabits per second of network connectivity for each GPU server.\nThe intensive computations needed across an entire facility’s densely packed fleet and supporting hardware result in data centers drawing tens of megawatts around the clock. Overall, advancing AI algorithms continue to expand data center energy consumption as more DGX nodes get deployed to keep pace with projected growth in demand for AI compute resources over the coming years.\n\n\nCooling Systems\nTo keep the beefy servers fed at peak capacity and cool, data centers require tremendous cooling capacity to counteract the heat produced by densely packed servers, networking equipment, and other hardware running computationally intensive workloads without pause. With large data centers packing thousands of server racks operating at full tilt, massive industrial-scale cooling towers and chillers are required, using energy amounting to 30-40% of the total data center electricity footprint (Dayarathna, Wen, and Fan 2016). Consequently, companies are looking for alternative methods of cooling. For example, Microsoft’s data center in Ireland leverages a nearby fjord to exchange heat using over half a million gallons of seawater daily.\nRecognizing the importance of energy-efficient cooling, there have been innovations aimed at reducing this energy demand. Techniques like free cooling, which uses outside air or water sources when conditions are favorable, and the use of AI to optimize cooling systems are examples of how the industry adapts. These innovations reduce energy consumption, lower operational costs, and lessen the environmental footprint. However, exponential increases in AI model complexity continue to demand more servers and acceleration hardware operating at higher utilization, translating to rising heat generation and ever greater energy used solely for cooling purposes.\n\n\n\nThe Environmental Impact\nThe environmental impact of data centers is not only caused by the direct energy consumption of the data center itself (Siddik, Shehabi, and Marston 2021). Data center operation involves the supply of treated water to the data center and the discharge of wastewater from the data center. Water and wastewater facilities are major electricity consumers.\n\nSiddik, Md Abu Bakar, Arman Shehabi, and Landon Marston. 2021. “The Environmental Footprint of Data Centers in the United States.” Environ. Res. Lett. 16 (6): 064017. https://doi.org/10.1088/1748-9326/abfba1.\n\nDavis, Jacqueline, Daniel Bizo, Andy Lawrence, Owen Rogers, and Max Smolaks. 2022. “Uptime Institute Global Data Center Survey 2022.” Uptime Institute.\nNext to electricity usage, there are many more aspects to the environmental impacts of these data centers. The water usage of the data centers can lead to water scarcity issues, increased water treatment needs, and proper wastewater discharge infrastructure. Also, raw materials required for construction and network transmission considerably impact the environment, and components in data centers need to be upgraded and maintained. Where almost 50 percent of servers were refreshed within 3 years of usage, refresh cycles have shown to slow down (Davis et al. 2022). Still, this generates significant e-waste, which can be hard to recycle.\n\n\n\n17.3.3 Energy Optimization\nUltimately, measuring and understanding the energy consumption of AI facilitates optimizing energy consumption.\nOne way to reduce the energy consumption of a given amount of computational work is to run it on more energy-efficient hardware. For instance, TPU chips can be more energy-efficient compared to CPUs when it comes to running large tensor computations for AI, as TPUs can run such computations much faster without drawing significantly more power than CPUs. Another way is to build software systems aware of energy consumption and application characteristics. Good examples are systems works such as Zeus (You, Chung, and Chowdhury 2023) and Perseus (Chung et al. 2023), both of which characterize the tradeoff between computation time and energy consumption at various levels of an ML training system to achieve energy reduction without end-to-end slowdown. In reality, building both energy-efficient hardware and software and combining their benefits should be promising, along with open-source frameworks (e.g., Zeus) that facilitate community efforts.\n\nYou, Jie, Jae-Won Chung, and Mosharaf Chowdhury. 2023. “Zeus: Understanding and Optimizing GPU Energy Consumption of DNN Training.” In 20th USENIX Symposium on Networked Systems Design and Implementation (NSDI 23), 119–39. Boston, MA: USENIX Association. https://www.usenix.org/conference/nsdi23/presentation/you.\n\nChung, Jae-Won, Yile Gu, Insu Jang, Luoxi Meng, Nikhil Bansal, and Mosharaf Chowdhury. 2023. “Perseus: Removing Energy Bloat from Large Model Training.” ArXiv Preprint abs/2312.06902. https://arxiv.org/abs/2312.06902.",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Sustainable AI</span>"
    ]
  },
  {
    "objectID": "contents/core/sustainable_ai/sustainable_ai.html#carbon-footprint",
    "href": "contents/core/sustainable_ai/sustainable_ai.html#carbon-footprint",
    "title": "17  Sustainable AI",
    "section": "17.4 Carbon Footprint",
    "text": "17.4 Carbon Footprint\nData centers consume massive amounts of electricity, and without access to a renewable power supply, this demand can have substantial environmental impacts. Many facilities rely heavily on nonrenewable energy sources like coal and natural gas. For example, data centers are estimated to produce up to 2% of total global \\(\\textrm{CO}_2\\) emissions which is closing the gap with the airline industry. As mentioned in previous sections, the computational demands of AI are set to increase. The emissions of this surge are threefold. First, data centers are projected to increase in size (Liu et al. 2020). Secondly, emissions during training are set to increase significantly (Patterson et al. 2022). Thirdly, inference calls to these models are set to increase dramatically.\n\nLiu, Yanan, Xiaoxia Wei, Jinyu Xiao, Zhijie Liu, Yang Xu, and Yun Tian. 2020. “Energy Consumption and Emission Mitigation Prediction Based on Data Center Traffic and PUE for Global Data Centers.” Global Energy Interconnection 3 (3): 272–82. https://doi.org/10.1016/j.gloei.2020.07.008.\nWithout action, this exponential demand growth risks ratcheting up the carbon footprint of data centers further to unsustainable levels. Major providers have pledged carbon neutrality and committed funds to secure clean energy, but progress remains incremental compared to overall industry expansion plans. More radical grid decarbonization policies and renewable energy investments may prove essential to counteracting the climate impact of the coming tide of new data centers aimed at supporting the next generation of AI.\n\n17.4.1 Definition and Significance\nThe concept of a ‘carbon footprint’ has emerged as a key metric. This term refers to the total amount of greenhouse gasses, particularly carbon dioxide, emitted directly or indirectly by an individual, organization, event, or product. These emissions significantly contribute to the greenhouse effect, accelerating global warming and climate change. The carbon footprint is measured in terms of carbon dioxide equivalents (\\(\\textrm{CO}_2\\)e), allowing for a comprehensive account that includes various greenhouse gasses and their relative environmental impact. Examples of this as applied to large-scale ML tasks are shown in Figure 17.5.\n\n\n\n\n\n\nFigure 17.5: Carbon footprint of large-scale ML tasks. Source: Wu et al. (2022).\n\n\n\nConsidering the carbon footprint is especially important in AI’s rapid advancement and integration into various sectors, bringing its environmental impact into sharp focus. AI systems, particularly those involving intensive computations like deep learning and large-scale data processing, are known for their substantial energy demands. This energy, often drawn from power grids, may still predominantly rely on fossil fuels, leading to significant greenhouse gas emissions.\nTake, for example, training large AI models such as GPT-3 or complex neural networks. These processes require immense computational power, typically provided by data centers. The energy consumption associated with operating these centers, particularly for high-intensity tasks, results in notable greenhouse gas emissions. Studies have highlighted that training a single AI model can generate carbon emissions comparable to that of the lifetime emissions of multiple cars, shedding light on the environmental cost of developing advanced AI technologies (Dayarathna, Wen, and Fan 2016). Figure 17.6 shows a comparison from lowest to highest carbon footprints, starting with a roundtrip flight between NY and SF, human life average per year, American life average per year, US car including fuel over a lifetime, and a Transformer model with neural architecture search, which has the highest footprint.\n\n\n\n\n\n\nFigure 17.6: Carbon footprint of NLP model in lbs of \\(\\textrm{CO}_2\\) equivalent. Source: Dayarathna, Wen, and Fan (2016).\n\n\nDayarathna, Miyuru, Yonggang Wen, and Rui Fan. 2016. “Data Center Energy Consumption Modeling: A Survey.” IEEE Communications Surveys &Amp; Tutorials 18 (1): 732–94. https://doi.org/10.1109/comst.2015.2481183.\n\n\nMoreover, AI’s carbon footprint extends beyond the operational phase. The entire lifecycle of AI systems, including the manufacturing of computing hardware, the energy used in data centers for cooling and maintenance, and the disposal of electronic waste, contributes to their overall carbon footprint. We have discussed some of these aspects earlier, and we will discuss the waste aspects later in this chapter.\n\n\n17.4.2 The Need for Awareness and Action\nUnderstanding the carbon footprint of AI systems is crucial for several reasons. Primarily, it is a step towards mitigating the impacts of climate change. As AI continues to grow and permeate different aspects of our lives, its contribution to global carbon emissions becomes a significant concern. Awareness of these emissions can inform decisions made by developers, businesses, policymakers, and even ML engineers and scientists like us to ensure a balance between technological innovation and environmental responsibility.\nFurthermore, this understanding stimulates the drive towards ‘Green AI’ (R. Schwartz et al. 2020). This approach focuses on developing AI technologies that are efficient, powerful, and environmentally sustainable. It encourages exploring energy-efficient algorithms, using renewable energy sources in data centers, and adopting practices that reduce AI’s overall environmental impact.\nIn essence, the carbon footprint is an essential consideration in developing and applying AI technologies. As AI evolves and its applications become more widespread, managing its carbon footprint is key to ensuring that this technological progress aligns with the broader environmental sustainability goals.\n\n\n17.4.3 Estimating the AI Carbon Footprint\nEstimating AI systems’ carbon footprint is critical in understanding their environmental impact. This involves analyzing the various elements contributing to emissions throughout AI technologies’ lifecycle and employing specific methodologies to quantify these emissions accurately. Many different methods for quantifying ML’s carbon emissions have been proposed.\nThe carbon footprint of AI encompasses several key elements, each contributing to the overall environmental impact. First, energy is consumed during the AI model training and operational phases. The source of this energy heavily influences the carbon emissions. Once trained, these models, depending on their application and scale, continue to consume electricity during operation. Next to energy considerations, the hardware used stresses the environment as well.\nThe carbon footprint varies significantly based on the energy sources used. The composition of the sources providing the energy used in the grid varies widely depending on geographical region and even time in a single day. For example, in the USA, roughly 60 percent of the total energy supply is still covered by fossil fuels. Nuclear and renewable energy sources cover the remaining 40 percent. These fractions are not constant throughout the day. As renewable energy production usually relies on environmental factors, such as solar radiation and pressure fields, they do not provide a constant energy source.\nThe variability of renewable energy production has been an ongoing challenge in the widespread use of these sources. Looking at Figure 17.7, which shows data for the European grid, we see that it is supposed to be able to produce the required amount of energy throughout the day. While solar energy peaks in the middle of the day, wind energy has two distinct peaks in the mornings and evenings. Currently, we rely on fossil and coal-based energy generation methods to supplement the lack of energy during times when renewable energy does not meet requirements.\nInnovation in energy storage solutions is required to enable constant use of renewable energy sources. The base energy load is currently met with nuclear energy. This constant energy source does not directly produce carbon emissions but needs to be faster to accommodate the variability of renewable energy sources. Tech companies such as Microsoft have shown interest in nuclear energy sources to power their data centers. As the demand of data centers is more constant than the demand of regular households, nuclear energy could be used as a dominant source of energy.\n\n\n\n\n\n\nFigure 17.7: Energy sources and generation capabilities. Source: Energy Charts.\n\n\n\nAdditionally, the manufacturing and disposal of AI hardware add to the carbon footprint. Producing specialized computing devices, such as GPUs and CPUs, is energy- and resource-intensive. This phase often relies on energy sources that contribute to greenhouse gas emissions. The electronics industry’s manufacturing process has been identified as one of the eight big supply chains responsible for more than 50 percent of global emissions (Challenge 2021). Furthermore, the end-of-life disposal of this hardware, which can lead to electronic waste, also has environmental implications. As mentioned, servers have a refresh cycle of roughly 3 to 5 years. Of this e-waste, currently only 17.4 percent is properly collected and recycled. The carbon emissions of this e-waste has shown an increase of more than 50 percent between 2014 and 2020 (Singh and Ogunseitan 2022).\n\nChallenge, WEF Net-Zero. 2021. “The Supply Chain Opportunity.” In World Economic Forum: Geneva, Switzerland.\n\nSingh, Narendra, and Oladele A. Ogunseitan. 2022. “Disentangling the Worldwide Web of e-Waste and Climate Change Co-Benefits.” Circular Economy 1 (2): 100011. https://doi.org/10.1016/j.cec.2022.100011.\nAs is clear from the above, a proper Life Cycle Analysis is necessary to portray all relevant aspects of the emissions caused by AI. Another method is carbon accounting, which quantifies the amount of carbon dioxide emissions directly and indirectly associated with AI operations. This measurement typically uses \\(\\textrm{CO}_2\\) equivalents, allowing for a standardized way of reporting and assessing emissions.\n\n\n\n\n\n\nExercise 17.1: AI’s Carbon Footprint\n\n\n\n\n\nDid you know that the cutting-edge AI models you might use have an environmental impact? This exercise will go into an AI system’s “carbon footprint.” You’ll learn how data centers’ energy demands, large AI models’ training, and even hardware manufacturing contribute to greenhouse gas emissions. We’ll discuss why it’s crucial to be aware of this impact, and you’ll learn methods to estimate the carbon footprint of your own AI projects. Get ready to explore the intersection of AI and environmental sustainability!",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Sustainable AI</span>"
    ]
  },
  {
    "objectID": "contents/core/sustainable_ai/sustainable_ai.html#beyond-carbon-footprint",
    "href": "contents/core/sustainable_ai/sustainable_ai.html#beyond-carbon-footprint",
    "title": "17  Sustainable AI",
    "section": "17.5 Beyond Carbon Footprint",
    "text": "17.5 Beyond Carbon Footprint\nThe current focus on reducing AI systems’ carbon emissions and energy consumption addresses one crucial aspect of sustainability. However, manufacturing the semiconductors and hardware that enable AI also carries severe environmental impacts that receive comparatively less public attention. Building and operating a leading-edge semiconductor fabrication plant, or “fab,” has substantial resource requirements and polluting byproducts beyond a large carbon footprint.\nFor example, a state-of-the-art fab producing chips like those in 5nm may require up to four million gallons of pure water each day. This water usage approaches what a city of half a million people would require for all needs. Sourcing this consistently places immense strain on local water tables and reservoirs, especially in already water-stressed regions that host many high-tech manufacturing hubs.\nAdditionally, over 250 unique hazardous chemicals are utilized at various stages of semiconductor production within fabs (Mills and Le Hunte 1997). These include volatile solvents like sulfuric acid, nitric acid, and hydrogen fluoride, along with arsine, phosphine, and other highly toxic substances. Preventing the discharge of these chemicals requires extensive safety controls and wastewater treatment infrastructure to avoid soil contamination and risks to surrounding communities. Any improper chemical handling or unanticipated spill carries dire consequences.\n\nMills, Andrew, and Stephen Le Hunte. 1997. “An Overview of Semiconductor Photocatalysis.” J. Photochem. Photobiol., A 108 (1): 1–35. https://doi.org/10.1016/s1010-6030(97)00118-4.\nBeyond water consumption and chemical risks, fab operations also depend on rare metals sourcing, generate tons of dangerous waste products, and can hamper local biodiversity. This section will analyze these critical but less discussed impacts. With vigilance and investment in safety, the harms from semiconductor manufacturing can be contained while still enabling technological progress. However, ignoring these externalized issues will exacerbate ecological damage and health risks over the long run.\n\n17.5.1 Water Usage and Stress\nSemiconductor fabrication is an incredibly water-intensive process. Based on an article from 2009, a typical 300mm silicon wafer requires 8,328 liters of water, of which 5,678 liters is ultrapure water (Cope 2009). While modern fabs like those mentioned earlier can use several million gallons of pure water daily, TSMC’s latest fab in Arizona is projected to consume even more—8.9 million gallons per day—amounting to nearly 3 percent of the city’s current water production. To put things in perspective, Intel and Quantis found that over 97% of their direct water consumption is attributed to semiconductor manufacturing operations within their fabrication facilities (Cooper et al. 2011).\n\nCope, Gord. 2009. “Pure Water, Semiconductors and the Recession.” Global Water Intelligence 10 (10).\n\nCooper, Tom, Suzanne Fallender, Joyann Pafumi, Jon Dettling, Sebastien Humbert, and Lindsay Lessard. 2011. “A Semiconductor Company’s Examination of Its Water Footprint Approach.” In Proceedings of the 2011 IEEE International Symposium on Sustainable Systems and Technology, 1–6. IEEE; IEEE. https://doi.org/10.1109/issst.2011.5936865.\n\n\n\n\n\n\nFigure 17.8: Daily Water Footprint of Datacenters in comparison with other water uses. Source: Google’s Data Center Cooling\n\n\n\nTo put these numbers into perspective, consider a Google data center, which uses approximately 450,000 gallons of water daily. This is equivalent to irrigating 17 acres of grass or producing 160 pairs of cotton jeans, showcasing the immense water demands of advanced technologies.\nThis water is repeatedly used to flush away contaminants in cleaning steps and also acts as a coolant and carrier fluid in thermal oxidation, chemical deposition, and chemical mechanical planarization processes. During peak summer months, this approximates the daily water consumption of a city with a population of half a million people.\nDespite being located in regions with sufficient water, the intensive usage can severely depress local water tables and drainage basins. For example, the city of Hsinchu in Taiwan suffered sinking water tables and seawater intrusion into aquifers due to excessive pumping to satisfy water supply demands from the Taiwan Semiconductor Manufacturing Company (TSMC) fab. In water-scarce inland areas like Arizona, massive water inputs are needed to support fabs despite already strained reservoirs.\nWater discharge from fabs risks environmental contamination besides depletion if not properly treated. While much discharge is recycled within the fab, the purification systems still filter out metals, acids, and other contaminants that can pollute rivers and lakes if not cautiously handled (Prakash, Callahan, et al. 2023). These factors make managing water usage essential when mitigating wider sustainability impacts.\n\n\n17.5.2 Hazardous Chemicals Usage\nModern semiconductor fabrication involves working with many highly hazardous chemicals under extreme conditions of heat and pressure (Kim et al. 2018). Key chemicals utilized include:\n\nKim, Sunju, Chungsik Yoon, Seunghon Ham, Jihoon Park, Ohun Kwon, Donguk Park, Sangjun Choi, Seungwon Kim, Kwonchul Ha, and Won Kim. 2018. “Chemical Use in the Semiconductor Manufacturing Industry.” Int. J. Occup. Env. Heal. 24 (3-4): 109–18. https://doi.org/10.1080/10773525.2018.1519957.\n\nStrong acids: Hydrofluoric, sulfuric, nitric, and hydrochloric acids rapidly eat through oxides and other surface contaminants but also pose toxicity dangers. Fabs can use thousands of metric tons of these acids annually, and accidental exposure can be fatal for workers.\nSolvents: Key solvents like xylene, methanol, and methyl isobutyl ketone (MIBK) handle dissolving photoresists but have adverse health impacts like skin/eye irritation and narcotic effects if mishandled. They also create explosive and air pollution risks.\nToxic gases: Gas mixtures containing arsine (AsH3), phosphine (PH3), diborane (B2H6), germane (GeH4), etc., are some of the deadliest chemicals used in doping and vapor deposition steps. Minimal exposures can lead to poisoning, tissue damage, and even death without quick treatment.\nChlorinated compounds: Older chemical mechanical planarization formulations incorporated perchloroethylene, trichloroethylene, and other chlorinated solvents, which have since been banned due to their carcinogenic effects and impacts on the ozone layer. However, their prior release still threatens surrounding groundwater sources.\n\nStrict handling protocols, protective equipment for workers, ventilation, filtrating/scrubbing systems, secondary containment tanks, and specialized disposal mechanisms are vital where these chemicals are used to minimize health, explosion, air, and environmental spill dangers (Wald and Jones 1987). But human errors and equipment failures still occasionally occur–highlighting why reducing fab chemical intensities is an ongoing sustainability effort.\n\nWald, Peter H., and Jeffrey R. Jones. 1987. “Semiconductor Manufacturing: An Introduction to Processes and Hazards.” Am. J. Ind. Med. 11 (2): 203–21. https://doi.org/10.1002/ajim.4700110209.\n\n\n17.5.3 Resource Depletion\nWhile silicon forms the base, there is an almost endless supply of silicon on Earth. In fact, silicon is the second most plentiful element found in the Earth’s crust, accounting for 27.7% of the crust’s total mass. Only oxygen exceeds silicon in abundance within the crust. Therefore, silicon is not necessary to consider for resource depletion. However, the various specialty metals and materials that enable the integrated circuit fabrication process and provide specific properties still need to be discovered. Maintaining supplies of these resources is crucial yet threatened by finite availability and geopolitical influences (Nakano 2021).\n\nNakano, Jane. 2021. The Geopolitics of Critical Minerals Supply Chains. JSTOR.\n\nChen, H.-W. 2006. “Gallium, Indium, and Arsenic Pollution of Groundwater from a Semiconductor Manufacturing Area of Taiwan.” B. Environ. Contam. Tox. 77 (2): 289–96. https://doi.org/10.1007/s00128-006-1062-3.\nGallium, indium, and arsenic are vital ingredients in forming ultra-efficient compound semiconductors in the highest-speed chips suited for 5G and AI applications (Chen 2006). However, these rare elements have relatively scarce natural deposits that are being depleted. The United States Geological Survey has indium on its list of most critical at-risk commodities, estimated to have less than a 15-year viable global supply at current demand growth (Davies 2011).\nHelium is required in huge volumes for next-gen fabs to enable precise wafer cooling during operation. But helium’s relative rarity and the fact that once it vents into the atmosphere, it quickly escapes Earth make maintaining helium supplies extremely challenging long-term (Davies 2011). According to the US National Academies, substantial price increases and supply shocks are already occurring in this thinly traded market.\n\nJha, A. R. 2014. Rare Earth Materials: Properties and Applications. CRC Press. https://doi.org/10.1201/b17045.\nOther risks include China’s control over 90% of the rare earth elements critical to semiconductor material production (Jha 2014). Any supply chain issues or trade disputes can lead to catastrophic raw material shortages, given the lack of current alternatives. In conjunction with helium shortages, resolving the limited availability and geographic imbalance in accessing essential ingredients remains a sector priority for sustainability.\n\n\n17.5.4 Hazardous Waste Generation\nSemiconductor fabs generate tons of hazardous waste annually as byproducts from the various chemical processes (Grossman 2007). The key waste streams include:\n\nGrossman, Elizabeth. 2007. High Tech Trash: Digital Devices, Hidden Toxics, and Human Health. Island press.\n\nGaseous waste: Fab ventilation systems capture harmful gases like arsine, phosphine, and germane and filter them out to avoid worker exposure. However, this produces significant quantities of dangerous condensed gas that need specialized treatment.\nVOCs: Volatile organic compounds like xylene, acetone, and methanol are used extensively as photoresist solvents and are evaporated as emissions during baking, etching, and stripping. VOCs pose toxicity issues and require scrubbing systems to prevent release.\nSpent acids: Strong acids such as sulfuric acid, hydrofluoric acid, and nitric acid get depleted in cleaning and etching steps, transforming into a corrosive, toxic soup that can dangerously react, releasing heat and fumes if mixed.\nSludge: Water treatment of discharged effluent contains concentrated heavy metals, acid residues, and chemical contaminants. Filter press systems separate this hazardous sludge.\nFilter cake: Gaseous filtration systems generate multi-ton sticky cakes of dangerous absorbed compounds requiring containment.\n\nWithout proper handling procedures, storage tanks, packaging materials, and secondary containment, improper disposal of any of these waste streams can lead to dangerous spills, explosions, and environmental releases. The massive volumes mean even well-run fabs produce tons of hazardous waste year after year, requiring extensive treatment.\n\n\n17.5.5 Biodiversity Impacts\n\nHabitat Disruption and Fragmentation\nSemiconductor fabs require large, contiguous land areas to accommodate cleanrooms, support facilities, chemical storage, waste treatment, and ancillary infrastructure. Developing these vast built-up spaces inevitably dismantles existing habitats, damaging sensitive biomes that may have taken decades to develop. For example, constructing a new fabrication module may level local forest ecosystems that species, like spotted owls and elk, rely upon for survival. The outright removal of such habitats severely threatens wildlife populations dependent on those lands.\nFurthermore, pipelines, water channels, air and waste exhaust systems, access roads, transmission towers, and other support infrastructure fragment the remaining undisturbed habitats. Animals moving daily for food, water, and spawning can find their migration patterns blocked by these physical human barriers that bisect previously natural corridors.\n\n\nAquatic Life Disturbances\nWith semiconductor fabs consuming millions of gallons of ultra-pure water daily, accessing and discharging such volumes risks altering the suitability of nearby aquatic environments housing fish, water plants, amphibians, and other species. If the fab is tapping groundwater tables as its primary supply source, overdrawing at unsustainable rates can deplete lakes or lead to stream drying as water levels drop (Davies 2011).\n\nDavies, Emma. 2011. “Endangered Elements: Critical Thinking.” https://www.rsc.org/images/Endangered\\%20Elements\\%20-\\%20Critical\\%20Thinking\\_tcm18-196054.pdf.\n\nLeRoy Poff, N, MM Brinson, and JW Day. 2002. “Aquatic Ecosystems & Global Climate Change.” Pew Center on Global Climate Change.\n\nTill, Aaron, Andrew L. Rypel, Andrew Bray, and Samuel B. Fey. 2019. “Fish Die-Offs Are Concurrent with Thermal Extremes in North Temperate Lakes.” Nat. Clim. Change 9 (8): 637–41. https://doi.org/10.1038/s41558-019-0520-y.\nAlso, discharging wastewater at higher temperatures to cool fabrication equipment can shift downstream river conditions through thermal pollution. Temperature changes beyond thresholds that native species evolved for can disrupt reproductive cycles. Warmer water also holds less dissolved oxygen, critical to supporting aquatic plant and animal life (LeRoy Poff, Brinson, and Day 2002). Combined with traces of residual contaminants that escape filtration systems, the discharged water can cumulatively transform environments to be far less habitable for sensitive organisms (Till et al. 2019).\n\n\nAir and Chemical Emissions\nWhile modern semiconductor fabs aim to contain air and chemical discharges through extensive filtration systems, some levels of emissions often persist, raising risks for nearby flora and fauna. Air pollutants can carry downwind, including volatile organic compounds (VOCs), nitrogen oxide compounds (NOx), particulate matter from fab operational exhausts, and power plant fuel emissions.\nAs contaminants permeate local soils and water sources, wildlife ingesting affected food and water ingest toxic substances, which research shows can hamper cell function, reproduction rates, and longevity–slowly poisoning ecosystems (Hsu et al. 2016).\n\nHsu, Liang-Ching, Ching-Yi Huang, Yen-Hsun Chuang, Ho-Wen Chen, Ya-Ting Chan, Heng Yi Teah, Tsan-Yao Chen, Chiung-Fen Chang, Yu-Ting Liu, and Yu-Min Tzou. 2016. “Accumulation of Heavy Metals and Trace Elements in Fluvial Sediments Received Effluents from Traditional and Semiconductor Industries.” Scientific Reports 6 (1): 34250. https://doi.org/10.1038/srep34250.\nLikewise, accidental chemical spills and improper waste handling, which release acids and heavy metals into soils, can dramatically affect retention and leaching capabilities. Flora, such as vulnerable native orchids adapted to nutrient-poor substrates, can experience die-offs when contacted by foreign runoff chemicals that alter soil pH and permeability. One analysis found that a single 500-gallon nitric acid spill led to the regional extinction of a rare moss species in the year following when the acidic effluent reached nearby forest habitats. Such contamination events set off chain reactions across the interconnected web of life. Thus, strict protocols are essential to avoid hazardous discharge and runoff.",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Sustainable AI</span>"
    ]
  },
  {
    "objectID": "contents/core/sustainable_ai/sustainable_ai.html#life-cycle-analysis",
    "href": "contents/core/sustainable_ai/sustainable_ai.html#life-cycle-analysis",
    "title": "17  Sustainable AI",
    "section": "17.6 Life Cycle Analysis",
    "text": "17.6 Life Cycle Analysis\nUnderstanding the holistic environmental impact of AI systems requires a comprehensive approach that considers the entire life cycle of these technologies. Life Cycle Analysis (LCA) refers to a methodological framework used to quantify the environmental impacts across all stages in a product or system’s lifespan, from raw material extraction to end-of-life disposal. Applying LCA to AI systems can help identify priority areas to target for reducing overall environmental footprints.\n\n\n\n\n\n\nFigure 17.9: AI System Life Cycle Analysis is divided into four key phases: Design, Manufacture, Use, Disposal.\n\n\n\n\n17.6.1 Stages of an AI System’s Life Cycle\nThe life cycle of an AI system can be divided into four key phases:\n\nDesign Phase: This includes the energy and resources used in researching and developing AI technologies. It encompasses the computational resources used for algorithm development and testing contributing to carbon emissions.\nManufacture Phase: This stage involves producing hardware components such as graphics cards, processors, and other computing devices necessary for running AI algorithms. Manufacturing these components often involves significant energy for material extraction, processing, and greenhouse gas emissions.\nUse Phase: The next most energy-intensive phase involves the operational use of AI systems. It includes the electricity consumed in data centers for training and running neural networks and powering end-user applications. This is arguably one of the most carbon-intensive stages.\nDisposal Phase: This final stage covers the end-of-life aspects of AI systems, including the recycling and disposal of electronic waste generated from outdated or non-functional hardware past their usable lifespan.\n\n\n\n17.6.2 Environmental Impact at Each Stage\nDesign and Manufacturing\nThe environmental impact during these beginning-of-life phases includes emissions from energy use and resource depletion from extracting materials for hardware production. At the heart of AI hardware are semiconductors, primarily silicon, used to make the integrated circuits in processors and memory chips. This hardware manufacturing relies on metals like copper for wiring, aluminum for casings, and various plastics and composites for other components. It also uses rare earth metals and specialized alloys- elements like neodymium, terbium, and yttrium- used in small but vital quantities. For example, the creation of GPUs relies on copper and aluminum. At the same time, chips use rare earth metals, which is the mining process that can generate substantial carbon emissions and ecosystem damage.\nUse Phase\nAI computes the majority of emissions in the lifecycle due to continuous high-power consumption, especially for training and running models. This includes direct and indirect emissions from electricity usage and nonrenewable grid energy generation. Studies estimate training complex models can have a carbon footprint comparable to the lifetime emissions of up to five cars.\nDisposal Phase\nThe disposal stage impacts include air and water pollution from toxic materials in devices, challenges associated with complex electronics recycling, and contamination when improperly handled. Harmful compounds from burned e-waste are released into the atmosphere. At the same time, landfill leakage of lead, mercury, and other materials poses risks of soil and groundwater contamination if not properly controlled. Implementing effective electronics recycling is crucial.\n\n\n\n\n\n\nExercise 17.2: Tracking ML Emissions\n\n\n\n\n\nIn this exercise, you’ll explore the environmental impact of training machine learning models. We’ll use CodeCarbon to track emissions, learn about Life Cycle Analysis (LCA) to understand AI’s carbon footprint, and explore strategies to make your ML model development more environmentally friendly. By the end, you’ll be equipped to track the carbon emissions of your models and start implementing greener practices in your projects.",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Sustainable AI</span>"
    ]
  },
  {
    "objectID": "contents/core/sustainable_ai/sustainable_ai.html#challenges-in-lca",
    "href": "contents/core/sustainable_ai/sustainable_ai.html#challenges-in-lca",
    "title": "17  Sustainable AI",
    "section": "17.7 Challenges in LCA",
    "text": "17.7 Challenges in LCA\n\n17.7.1 Lack of Consistency and Standards\nOne major challenge facing life cycle analysis (LCA) for AI systems is the need for consistent methodological standards and frameworks. Unlike product categories like building materials, which have developed international standards for LCA through ISO 14040, there are no firmly established guidelines for analyzing the environmental footprint of complex information technology like AI.\nThis absence of uniformity means researchers make differing assumptions and varying methodological choices. For example, a 2021 study from the University of Massachusetts Amherst (Strubell, Ganesh, and McCallum 2019) analyzed the life cycle emissions of several natural language processing models but only considered computational resource usage for training and omitted hardware manufacturing impacts. A more comprehensive 2020 study from Stanford University researchers included emissions estimates from producing relevant servers, processors, and other components, following an ISO-aligned LCA standard for computer hardware. However, these diverging choices in system boundaries and accounting approaches reduce robustness and prevent apples-to-apples comparisons of results.\nStandardized frameworks and protocols tailored to AI systems’ unique aspects and rapid update cycles would provide more coherence. This could equip researchers and developers to understand environmental hotspots, compare technology options, and accurately track progress on sustainability initiatives across the AI field. Industry groups and international standards bodies like the IEEE or ACM should prioritize addressing this methodological gap.\n\n\n17.7.2 Data Gaps\nAnother key challenge for comprehensive life cycle assessment of AI systems is substantial data gaps, especially regarding upstream supply chain impacts and downstream electronic waste flows. Most existing studies focus narrowly on the learner or usage phase emissions from computational power demands, which misses a significant portion of lifetime emissions (Gupta et al. 2022).\nFor example, little public data from companies exists quantifying energy use and emissions from manufacturing the specialized hardware components that enable AI–including high-end GPUs, ASIC chips, solid-state drives, and more. Researchers often rely on secondary sources or generic industry averages to approximate production impacts. Similarly, on average, there is limited transparency into downstream fate once AI systems are discarded after 4-5 years of usable lifespans.\nWhile electronic waste generation levels can be estimated, specifics on hazardous material leakage, recycling rates, and disposal methods for the complex components are hugely uncertain without better corporate documentation or regulatory reporting requirements.\nThe need for fine-grained data on computational resource consumption for training different model types makes reliable per-parameter or per-query emissions calculations difficult even for the usage phase. Attempts to create lifecycle inventories estimating average energy needs for key AI tasks exist (Henderson et al. 2020; Anthony, Kanding, and Selvan 2020), but variability across hardware setups, algorithms, and input data uncertainty remains extremely high. Furthermore, real-time carbon intensity data, critical in accurately tracking operational carbon footprint, must be improved in many geographic locations, rendering existing tools for operational carbon emission mere approximations based on annual average carbon intensity values.\n\nHenderson, Peter, Jieru Hu, Joshua Romoff, Emma Brunskill, Dan Jurafsky, and Joelle Pineau. 2020. “Towards the Systematic Reporting of the Energy and Carbon Footprints of Machine Learning.” The Journal of Machine Learning Research 21 (1): 10039–81.\n\nAnthony, Lasse F. Wolff, Benjamin Kanding, and Raghavendra Selvan. 2020. ICML Workshop on Challenges in Deploying and monitoring Machine Learning Systems.\nThe challenge is that tools like CodeCarbon and ML \\(\\textrm{CO}_2\\) are just ad hoc approaches at best, despite their well-meaning intentions. Bridging the real data gaps with more rigorous corporate sustainability disclosures and mandated environmental impact reporting will be key for AI’s overall climatic impacts to be understood and managed.\n\n\n17.7.3 Rapid Pace of Evolution\nThe extremely quick evolution of AI systems poses additional challenges in keeping life cycle assessments up-to-date and accounting for the latest hardware and software advancements. The core algorithms, specialized chips, frameworks, and technical infrastructure underpinning AI have all been advancing exceptionally fast, with new developments rapidly rendering prior systems obsolete.\nFor example, in deep learning, novel neural network architectures that achieve significantly better performance on key benchmarks or new optimized hardware like Google’s TPU chips can completely change an “average” model in less than a year. These swift shifts quickly make one-off LCA studies outdated for accurately tracking emissions from designing, running, or disposing of the latest AI.\nHowever, the resources and access required to update LCAs continuously need to be improved. Frequently re-doing labor—and data-intensive life cycle inventories and impact modeling to stay current with AI’s state-of-the-art is likely infeasible for many researchers and organizations. However, updated analyses could notice environmental hotspots as algorithms and silicon chips continue rapidly evolving.\nThis presents difficulty in balancing dynamic precision through continuous assessment with pragmatic constraints. Some researchers have proposed simplified proxy metrics like tracking hardware generations over time or using representative benchmarks as an oscillating set of goalposts for relative comparisons, though granularity may be sacrificed. Overall, the challenge of rapid change will require innovative methodological solutions to prevent underestimating AI’s evolving environmental burdens.\n\n\n17.7.4 Supply Chain Complexity\nFinally, the complex and often opaque supply chains associated with producing the wide array of specialized hardware components that enable AI pose challenges for comprehensive life cycle modeling. State-of-the-art AI relies on cutting-edge advancements in processing chips, graphics cards, data storage, networking equipment, and more. However, tracking emissions and resource use across the tiered networks of globalized suppliers for all these components is extremely difficult.\nFor example, NVIDIA graphics processing units dominate much of the AI computing hardware, but the company relies on several discrete suppliers across Asia and beyond to produce GPUs. Many firms at each supplier tier choose to keep facility-level environmental data private, which could fully enable robust LCAs. Gaining end-to-end transparency down multiple levels of suppliers across disparate geographies with varying disclosure protocols and regulations poses barriers despite being crucial for complete boundary setting. This becomes even more complex when attempting to model emerging hardware accelerators like tensor processing units (TPUs), whose production networks still need to be made public.\nWithout tech giants’ willingness to require and consolidate environmental impact data disclosure from across their global electronics supply chains, considerable uncertainty will remain around quantifying the full lifecycle footprint of AI hardware enablement. More supply chain visibility coupled with standardized sustainability reporting frameworks specifically addressing AI’s complex inputs hold promise for enriching LCAs and prioritizing environmental impact reductions.",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Sustainable AI</span>"
    ]
  },
  {
    "objectID": "contents/core/sustainable_ai/sustainable_ai.html#sustainable-design-and-development",
    "href": "contents/core/sustainable_ai/sustainable_ai.html#sustainable-design-and-development",
    "title": "17  Sustainable AI",
    "section": "17.8 Sustainable Design and Development",
    "text": "17.8 Sustainable Design and Development\n\n17.8.1 Sustainability Principles\nAs the impact of AI on the environment becomes increasingly evident, the focus on sustainable design and development in AI is gaining prominence. This involves incorporating sustainability principles into AI design, developing energy-efficient models, and integrating these considerations throughout the AI development pipeline. There is a growing need to consider its sustainability implications and develop principles to guide responsible innovation. Below is a core set of principles. The principles flow from the conceptual foundation to practical execution to supporting implementation factors; the principles provide a full cycle perspective on embedding sustainability in AI design and development.\nLifecycle Thinking: Encouraging designers to consider the entire lifecycle of AI systems, from data collection and preprocessing to model development, training, deployment, and monitoring. The goal is to ensure sustainability is considered at each stage. This includes using energy-efficient hardware, prioritizing renewable energy sources, and planning to reuse or recycle retired models.\nFuture Proofing: Designing AI systems anticipating future needs and changes can improve sustainability. This may involve making models adaptable via transfer learning and modular architectures. It also includes planning capacity for projected increases in operational scale and data volumes.\nEfficiency and Minimalism: This principle focuses on creating AI models that achieve desired results with the least possible resource use. It involves simplifying models and algorithms to reduce computational requirements. Specific techniques include pruning redundant parameters, quantizing and compressing models, and designing efficient model architectures, such as those discussed in the Optimizations chapter.\nLifecycle Assessment (LCA) Integration: Analyzing environmental impacts throughout the development and deployment of lifecycles highlights unsustainable practices early on. Teams can then make adjustments instead of discovering issues late when they are more difficult to address. Integrating this analysis into the standard design flow avoids creating legacy sustainability problems.\nIncentive Alignment: Economic and policy incentives should promote and reward sustainable AI development. These may include government grants, corporate initiatives, industry standards, and academic mandates for sustainability. Aligned incentives enable sustainability to become embedded in AI culture.\nSustainability Metrics and Goals: It is important to establish clearly defined Metrics that measure sustainability factors like carbon usage and energy efficiency. Establishing clear targets for these metrics provides concrete guidelines for teams to develop responsible AI systems. Tracking performance on metrics over time shows progress towards set sustainability goals.\nFairness, Transparency, and Accountability: Sustainable AI systems should be fair, transparent, and accountable. Models should be unbiased, with transparent development processes and mechanisms for auditing and redressing issues. This builds public trust and enables the identification of unsustainable practices.\nCross-disciplinary Collaboration: AI researchers teaming up with environmental scientists and engineers can lead to innovative systems that are high-performing yet environmentally friendly. Combining expertise from different fields from the start of projects enables sustainable thinking to be incorporated into the AI design process.\nEducation and Awareness: Workshops, training programs, and course curricula that cover AI sustainability raise awareness among the next generation of practitioners. This equips students with the knowledge to develop AI that consciously minimizes negative societal and environmental impacts. Instilling these values from the start shapes tomorrow’s professionals and company cultures.",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Sustainable AI</span>"
    ]
  },
  {
    "objectID": "contents/core/sustainable_ai/sustainable_ai.html#green-ai-infrastructure",
    "href": "contents/core/sustainable_ai/sustainable_ai.html#green-ai-infrastructure",
    "title": "17  Sustainable AI",
    "section": "17.9 Green AI Infrastructure",
    "text": "17.9 Green AI Infrastructure\nGreen AI represents a transformative approach to AI that incorporates environmental sustainability as a fundamental principle across the AI system design and lifecycle (R. Schwartz et al. 2020). This shift is driven by growing awareness of AI technologies’ significant carbon footprint and ecological impact, especially the compute-intensive process of training complex ML models.\n\nSchwartz, Roy, Jesse Dodge, Noah A. Smith, and Oren Etzioni. 2020. “Green AI.” Commun. ACM 63 (12): 54–63. https://doi.org/10.1145/3381831.\nThe essence of Green AI lies in its commitment to align AI advancement with sustainability goals around energy efficiency, renewable energy usage, and waste reduction. The introduction of Green AI ideals reflects maturing responsibility across the tech industry towards environmental stewardship and ethical technology practices. It moves beyond technical optimizations toward holistic life cycle assessment on how AI systems affect sustainability metrics. Setting new bars for ecologically conscious AI paves the way for the harmonious coexistence of technological progress and planetary health.\n\n17.9.1 Energy Efficient AI Systems\nEnergy efficiency in AI systems is a cornerstone of Green AI, aiming to reduce the energy demands traditionally associated with AI development and operations. This shift towards energy-conscious AI practices is vital in addressing the environmental concerns raised by the rapidly expanding field of AI. By focusing on energy efficiency, AI systems can become more sustainable, lessening their environmental impact and paving the way for more responsible AI use.\nAs we discussed earlier, the training and operation of AI models, especially large-scale ones, are known for their high energy consumption, which stems from compute-intensive model architecture and reliance on vast amounts of training data. For example, it is estimated that training a large state-of-the-art neural network model can have a carbon footprint of 284 tonnes—equivalent to the lifetime emissions of 5 cars (Strubell, Ganesh, and McCallum 2019).\n\nStrubell, Emma, Ananya Ganesh, and Andrew McCallum. 2019. “Energy and Policy Considerations for Deep Learning in NLP.” In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, 3645–50. Florence, Italy: Association for Computational Linguistics. https://doi.org/10.18653/v1/p19-1355.\nTo tackle the massive energy demands, researchers and developers are actively exploring methods to optimize AI systems for better energy efficiency while maintaining model accuracy and performance. This includes techniques like the ones we have discussed in the model optimizations, efficient AI, and hardware acceleration chapters:\n\nKnowledge distillation to transfer knowledge from large AI models to miniature versions\nQuantization and pruning approaches that reduce computational and space complexities\nLow-precision numerics–lowering mathematical precision without impacting model quality\nSpecialized hardware like TPUs, neuromorphic chips tuned explicitly for efficient AI processing\n\nOne example is Intel’s work on Q8BERT—quantizing the BERT language model with 8-bit integers, leading to a 4x reduction in model size with minimal accuracy loss (Zafrir et al. 2019). The push for energy-efficient AI is not just a technical endeavor–it has tangible real-world implications. More performant systems lower AI’s operational costs and carbon footprint, making it accessible for widespread deployment on mobile and edge devices. It also paves the path toward the democratization of AI and mitigates unfair biases that can emerge from uneven access to computing resources across regions and communities. Pursuing energy-efficient AI is thus crucial for creating an equitable and sustainable future with AI.\n\nZafrir, Ofir, Guy Boudoukh, Peter Izsak, and Moshe Wasserblat. 2019. “Q8BERT: Quantized 8Bit BERT.” In 2019 Fifth Workshop on Energy Efficient Machine Learning and Cognitive Computing - NeurIPS Edition (EMC2-NIPS), 36–39. IEEE; IEEE. https://doi.org/10.1109/emc2-nips53020.2019.00016.\n\n\n17.9.2 Sustainable AI Infrastructure\nSustainable AI infrastructure includes the physical and technological frameworks that support AI systems, focusing on environmental sustainability. This involves designing and operating AI infrastructure to minimize ecological impact, conserve resources, and reduce carbon emissions. The goal is to create a sustainable ecosystem for AI that aligns with broader environmental objectives.\nGreen data centers are central to sustainable AI infrastructure, optimized for energy efficiency, and often powered by renewable energy sources. These data centers employ advanced cooling technologies (Ebrahimi, Jones, and Fleischer 2014), energy-efficient server designs (Uddin and Rahman 2012), and smart management systems (Buyya, Beloglazov, and Abawajy 2010) to reduce power consumption. The shift towards green computing infrastructure also involves adopting energy-efficient hardware, like AI-optimized processors that deliver high performance with lower energy requirements, which we discussed in the AI Acceleration chapter. These efforts collectively reduce the carbon footprint of running large-scale AI operations.\n\nEbrahimi, Khosrow, Gerard F. Jones, and Amy S. Fleischer. 2014. “A Review of Data Center Cooling Technology, Operating Conditions and the Corresponding Low-Grade Waste Heat Recovery Opportunities.” Renewable Sustainable Energy Rev. 31 (March): 622–38. https://doi.org/10.1016/j.rser.2013.12.007.\n\nUddin, Mueen, and Azizah Abdul Rahman. 2012. “Energy Efficiency and Low Carbon Enabler Green IT Framework for Data Centers Considering Green Metrics.” Renewable Sustainable Energy Rev. 16 (6): 4078–94. https://doi.org/10.1016/j.rser.2012.03.014.\n\nBuyya, Rajkumar, Anton Beloglazov, and Jemal Abawajy. 2010. “Energy-Efficient Management of Data Center Resources for Cloud Computing: A Vision, Architectural Elements, and Open Challenges.” https://arxiv.org/abs/1006.0308.\n\nChua, L. 1971. “Memristor-the Missing Circuit Element.” #IEEE_J_CT# 18 (5): 507–19. https://doi.org/10.1109/tct.1971.1083337.\nIntegrating renewable energy sources, such as solar, wind, and hydroelectric power, into AI infrastructure is important for environmental sustainability (Chua 1971). Many tech companies and research institutions are investing in renewable energy projects to power their data centers. This not only helps in making AI operations carbon-neutral but also promotes the wider adoption of clean energy. Using renewable energy sources clearly shows commitment to environmental responsibility in the AI industry.\nSustainability in AI also extends to the materials and hardware used in creating AI systems. This involves choosing environmentally friendly materials, adopting recycling practices, and ensuring responsible electronic waste disposal. Efforts are underway to develop more sustainable hardware components, including energy-efficient chips designed for domain-specific tasks (such as AI accelerators) and environmentally friendly materials in device manufacturing (Cenci et al. 2021; Irimia-Vladu 2014). The lifecycle of these components is also a focus, with initiatives aimed at extending the lifespan of hardware and promoting recycling and reuse.\n\nCenci, Marcelo Pilotto, Tatiana Scarazzato, Daniel Dotto Munchen, Paula Cristina Dartora, Hugo Marcelo Veit, Andrea Moura Bernardes, and Pablo R. Dias. 2021. “Eco-Friendly ElectronicsA Comprehensive Review.” Adv. Mater. Technol. 7 (2): 2001263. https://doi.org/10.1002/admt.202001263.\n\nIrimia-Vladu, Mihai. 2014. ““Green” Electronics: Biodegradable and Biocompatible Materials and Devices for Sustainable Future.” Chem. Soc. Rev. 43 (2): 588–610. https://doi.org/10.1039/c3cs60235d.\nWhile strides are being made in sustainable AI infrastructure, challenges remain, such as the high costs of green technology and the need for global standards in sustainable practices. Future directions include more widespread adoption of green energy, further innovations in energy-efficient hardware, and international collaboration on sustainable AI policies. Pursuing sustainable AI infrastructure is not just a technical endeavor but a holistic approach that encompasses environmental, economic, and social aspects, ensuring that AI advances harmoniously with our planet’s health.\n\n\n17.9.3 Frameworks and Tools\nAccess to the right frameworks and tools is essential to effectively implementing green AI practices. These resources are designed to assist developers and researchers in creating more energy-efficient and environmentally friendly AI systems. They range from software libraries optimized for low-power consumption to platforms that facilitate the development of sustainable AI applications.\nSeveral software libraries and development environments are specifically tailored for Green AI. These tools often include features for optimizing AI models to reduce their computational load and, consequently, their energy consumption. For example, libraries in PyTorch and TensorFlow that support model pruning, quantization, and efficient neural network architectures enable developers to build AI systems that require less processing power and energy. Additionally, open-source communities like the Green Software Foundation are creating a centralized carbon intensity metric and building software for carbon-aware computing.\nEnergy monitoring tools are crucial for Green AI, as they allow developers to measure and analyze the energy consumption of their AI systems. Figure 17.10 is a screenshot of an energy consumption dashboard provided by Microsoft’s cloud services platform. By providing detailed insights into where and how energy is being used, these tools enable developers to make informed decisions about optimizing their models for better energy efficiency. This can involve adjustments in algorithm design, hardware selection, cloud computing software selection, or operational parameters.\n\n\n\n\n\n\nFigure 17.10: Microsoft Azure energy consumption dashboard. Source: Will Buchanan.\n\n\n\nWith the increasing integration of renewable energy sources in AI operations, frameworks facilitating this process are becoming more important. These frameworks help manage the energy supply from renewable sources like solar or wind power, ensuring that AI systems can operate efficiently with fluctuating energy inputs.\nBeyond energy efficiency, sustainability assessment tools help evaluate the broader environmental impact of AI systems. These tools can analyze factors like the carbon footprint of AI operations, the lifecycle impact of hardware components (Gupta et al. 2022), and the overall sustainability of AI projects (Prakash, Callahan, et al. 2023).\n\nGupta, Udit, Mariam Elgamal, Gage Hills, Gu-Yeon Wei, Hsien-Hsin S. Lee, David Brooks, and Carole-Jean Wu. 2022. “Act: Designing Sustainable Computer Systems with an Architectural Carbon Modeling Tool.” In Proceedings of the 49th Annual International Symposium on Computer Architecture, 784–99. ACM. https://doi.org/10.1145/3470496.3527408.\n\nPrakash, Shvetank, Tim Callahan, Joseph Bushagour, Colby Banbury, Alan V. Green, Pete Warden, Tim Ansell, and Vijay Janapa Reddi. 2023. “CFU Playground: Full-stack Open-Source Framework for Tiny Machine Learning (TinyML) Acceleration on FPGAs.” In 2023 IEEE International Symposium on Performance Analysis of Systems and Software (ISPASS). Vol. abs/2201.01863. IEEE. https://doi.org/10.1109/ispass57527.2023.00024.\nThe availability and ongoing development of Green AI frameworks and tools are critical for advancing sustainable AI practices. By providing the necessary resources for developers and researchers, these tools facilitate the creation of more environmentally friendly AI systems and encourage a broader shift towards sustainability in the tech community. As Green AI continues to evolve, these frameworks and tools will play a vital role in shaping a more sustainable future for AI.\n\n\n17.9.4 Benchmarks and Leaderboards\nBenchmarks and leaderboards are important for driving progress in Green AI, as they provide standardized ways to measure and compare different methods. Well-designed benchmarks that capture relevant metrics around energy efficiency, carbon emissions, and other sustainability factors enable the community to track advancements fairly and meaningfully.\nExtensive benchmarks exist for tracking AI model performance, such as those extensively discussed in the Benchmarking chapter. Still, a clear and pressing need exists for additional standardized benchmarks focused on sustainability metrics like energy efficiency, carbon emissions, and overall ecological impact. Understanding the environmental costs of AI currently needs to be improved by a lack of transparency and standardized measurement around these factors.\nEmerging efforts such as the ML.ENERGY Leaderboard, which provides performance and energy consumption benchmarking results for large language models (LLMs) text generation, assists in enhancing the understanding of the energy cost of GenAI deployment.\nAs with any benchmark, Green AI benchmarks must represent realistic usage scenarios and workloads. Benchmarks that focus narrowly on easily gamed metrics may lead to short-term gains but fail to reflect actual production environments where more holistic efficiency and sustainability measures are needed. The community should continue expanding benchmarks to cover diverse use cases.\nWider adoption of common benchmark suites by industry players will accelerate innovation in Green AI by allowing easier comparison of techniques across organizations. Shared benchmarks lower the barrier to demonstrating the sustainability benefits of new tools and best practices. However, when designing industry-wide benchmarks, care must be taken around issues like intellectual property, privacy, and commercial sensitivity. Initiatives to develop open reference datasets for Green AI evaluation may help drive broader participation.\nAs methods and infrastructure for Green AI continue maturing, the community must revisit benchmark design to ensure existing suites capture new techniques and scenarios well. Tracking the evolving landscape through regular benchmark updates and reviews will be important to maintain representative comparisons over time. Community efforts for benchmark curation can enable sustainable benchmark suites that stand the test of time. Comprehensive benchmark suites owned by research communities or neutral third parties like MLCommons may encourage wider participation and standardization.",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Sustainable AI</span>"
    ]
  },
  {
    "objectID": "contents/core/sustainable_ai/sustainable_ai.html#case-study-google-4ms",
    "href": "contents/core/sustainable_ai/sustainable_ai.html#case-study-google-4ms",
    "title": "17  Sustainable AI",
    "section": "17.10 Case Study: Google’s 4Ms",
    "text": "17.10 Case Study: Google’s 4Ms\nOver the past decade, AI has rapidly moved from academic research to large-scale production systems powering numerous Google products and services. As AI models and workloads have grown exponentially in size and computational demands, concerns have emerged about their energy consumption and carbon footprint. Some researchers predicted runaway growth in ML’s energy appetite that could outweigh efficiencies gained from improved algorithms and hardware (Thompson et al. 2021).\n\nThompson, Neil C., Kristjan Greenewald, Keeheon Lee, and Gabriel F. Manso. 2021. “Deep Learning’s Diminishing Returns: The Cost of Improvement Is Becoming Unsustainable.” IEEE Spectr. 58 (10): 50–55. https://doi.org/10.1109/mspec.2021.9563954.\nHowever, Google’s production data reveals a different story—AI represents a steady 10-15% of total company energy usage from 2019 to 2021. This case study analyzes how Google applied a systematic approach leveraging four best practices—what they term the “4 Ms” of model efficiency, machine optimization, mechanization through cloud computing, and mapping to green locations—to bend the curve on emissions from AI workloads.\nThe scale of Google’s AI usage makes it an ideal case study. In 2021 alone, the company trained models like the 1.2 trillion-parameter GLam model. Analyzing how the application of AI has been paired with rapid efficiency gains in this environment helps us by providing a logical blueprint for the broader AI field to follow.\nBy transparently publishing detailed energy usage statistics, adopting rates of carbon-free clouds and renewables purchases, and more, alongside its technical innovations, Google has enabled outside researchers to measure progress accurately. Their study in the ACM CACM (Patterson et al. 2022) highlights how the company’s multipronged approach shows that runaway AI energy consumption predictions can be overcome by focusing engineering efforts on sustainable development patterns. The pace of improvements also suggests ML’s efficiency gains are just starting.\n\nPatterson, David, Joseph Gonzalez, Urs Holzle, Quoc Le, Chen Liang, Lluis-Miquel Munguia, Daniel Rothchild, David R. So, Maud Texier, and Jeff Dean. 2022. “The Carbon Footprint of Machine Learning Training Will Plateau, Then Shrink.” Computer 55 (7): 18–28. https://doi.org/10.1109/mc.2022.3148714.\n\n17.10.1 Google’s 4M Best Practices\nTo curb emissions from their rapidly expanding AI workloads, Google engineers systematically identified four best practice areas–termed the “4 Ms”–where optimizations could compound to reduce the carbon footprint of ML:\n\nModel: Selecting efficient AI model architectures can reduce computation by 5-10X with no loss in model quality. Google has extensively researched developing sparse models and neural architecture search to create more efficient models like the Evolved Transformer and Primer.\nMachine: Using hardware optimized for AI over general-purpose systems improves performance per watt by 2-5X. Google’s Tensor Processing Units (TPUs) led to 5-13X better carbon efficiency versus GPUs not optimized for ML.\nMechanization: By leveraging cloud computing systems tailored for high utilization over conventional on-premise data centers, energy costs are reduced by 1.4-2X. Google cites its data center’s power usage effectiveness as outpacing industry averages.\nMap: Choosing data center locations with low-carbon electricity reduces gross emissions by another 5-10X. Google provides real-time maps highlighting the percentage of renewable energy used by its facilities.\n\nTogether, these practices created drastic compound efficiency gains. For example, optimizing the Transformer AI model on TPUs in a sustainable data center location cut energy use by 83x. It lowered \\(\\textrm{CO}_2\\) emissions by a factor of 747.\n\n\n17.10.2 Significant Results\nDespite exponential growth in AI adoption across products and services, Google’s efforts to improve the carbon efficiency of ML have produced measurable gains, helping to restrain overall energy appetite. One key data point highlighting this progress is that AI workloads have remained a steady 10% to 15% of total company energy use from 2019 to 2021. As AI became integral to more Google offerings, overall compute cycles dedicated to AI grew substantially. However, efficiencies in algorithms, specialized hardware, data center design, and flexible geography allowed sustainability to keep pace—with AI representing just a fraction of total data center electricity over years of expansion.\nOther case studies underscore how an engineering focus on sustainable AI development patterns enabled rapid quality improvements in lockstep with environmental gains. For example, the natural language processing model GPT-3 was viewed as state-of-the-art in mid-2020. Yet its successor GLaM improved accuracy while cutting training compute needs and using cleaner data center energy–cutting CO2 emissions by a factor of 14 in just 18 months of model evolution.\nSimilarly, Google found past published speculation missing the mark on ML’s energy appetite by factors of 100 to 100,000X due to a lack of real-world metrics. By transparently tracking optimization impact, Google hoped to motivate efficiency while preventing overestimated extrapolations about ML’s environmental toll.\nThese data-driven case studies show how companies like Google are steering AI advancements toward sustainable trajectories and improving efficiency to outpace adoption growth. With further efforts around lifecycle analysis, inference optimization, and renewable expansion, companies can aim to accelerate progress, giving evidence that ML’s clean potential is only just being unlocked by current gains.\n\n\n17.10.3 Further Improvements\nWhile Google has made measurable progress in restraining the carbon footprint of its AI operations, the company recognizes further efficiency gains will be vital for responsible innovation given the technology’s ongoing expansion.\nOne area of focus is showing how advances are often incorrectly viewed as increasing unsustainable computing—like neural architecture search (NAS) to find optimized models— spur downstream savings, outweighing their upfront costs. Despite expending more energy on model discovery rather than hand-engineering, NAS cuts lifetime emissions by producing efficient designs callable across countless applications.\nAdditionally, the analysis reveals that focusing sustainability efforts on data center and server-side optimization makes sense, given the dominant energy draw versus consumer devices. Though Google shrinks inference impacts across processors like mobile phones, priority rests on improving training cycles and data center renewables procurement for maximal effect.\nTo that end, Google’s progress in pooling computing inefficiently designed cloud facilities highlights the value of scale and centralization. As more workloads shift away from inefficient on-premise servers, internet giants’ prioritization of renewable energy—with Google and Meta matched 100% by renewables since 2017 and 2020, respectively—unlocks compounding emissions cuts.\nTogether, these efforts emphasize that while no resting on laurels is possible, Google’s multipronged approach shows that AI efficiency improvements are only accelerating. Cross-domain initiatives around lifecycle assessment, carbon-conscious development patterns, transparency, and matching rising AI demand with clean electricity supply pave a path toward bending the curve further as adoption grows. The company’s results compel the broader field towards replicating these integrated sustainability pursuits.",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Sustainable AI</span>"
    ]
  },
  {
    "objectID": "contents/core/sustainable_ai/sustainable_ai.html#embedded-ai-internet-of-trash",
    "href": "contents/core/sustainable_ai/sustainable_ai.html#embedded-ai-internet-of-trash",
    "title": "17  Sustainable AI",
    "section": "17.11 Embedded AI - Internet of Trash",
    "text": "17.11 Embedded AI - Internet of Trash\nWhile much attention has focused on making the immense data centers powering AI more sustainable, an equally pressing concern is the movement of AI capabilities into smart edge devices and endpoints. Edge/embedded AI allows near real-time responsiveness without connectivity dependencies. It also reduces transmission bandwidth needs. However, the increase of tiny devices leads to other risks.\nTiny computers, microcontrollers, and custom ASICs powering edge intelligence face size, cost, and power limitations that rule out high-end GPUs used in data centers. Instead, they require optimized algorithms and extremely compact, energy-efficient circuitry to run smoothly. However, engineering for these microscopic form factors opens up risks around planned obsolescence, disposability, and waste. Figure 17.11 shows that the number of IoT devices is projected to reach 30 billion connected devices by 2030.\n\n\n\n\n\n\nFigure 17.11: Number of Internet of Things (IoT) connected devices worldwide from 2019 to 2023. Source: Statista.\n\n\n\nEnd-of-life handling of internet-connected gadgets embedded with sensors and AI remains an often overlooked issue during design. However, these products permeate consumer goods, vehicles, public infrastructure, industrial equipment, and more.\n\n17.11.1 E-waste\nElectronic waste, or e-waste, refers to discarded electrical equipment and components that enter the waste stream. This includes devices that have to be plugged in, have a battery, or electrical circuitry. With the rising adoption of internet-connected smart devices and sensors, e-waste volumes rapidly increase yearly. These proliferating gadgets contain toxic heavy metals like lead, mercury, and cadmium that become environmental and health hazards when improperly disposed of.\nThe amount of electronic waste being produced is growing at an alarming rate. Today, we already produce 50 million tons per year. By 2030, that figure is projected to jump to a staggering 75 million tons as consumer electronics consumption continues to accelerate. Global e-waste production will reach 120 million tonnes annually by 2050 (Un and Forum 2019). The soaring production and short lifecycles of our gadgets fuel this crisis, from smartphones and tablets to internet-connected devices and home appliances.\nDeveloping nations are being hit the hardest as they need more infrastructure to process obsolete electronics safely. In 2019, formal e-waste recycling rates in poorer countries ranged from 13% to 23%. The remainder ends up illegally dumped, burned, or crudely dismantled, releasing toxic materials into the environment and harming workers and local communities. Clearly, more needs to be done to build global capacity for ethical and sustainable e-waste management, or we risk irreversible damage.\nThe danger is that crude handling of electronics to strip valuables exposes marginalized workers and communities to noxious burnt plastics/metals. Lead poisoning poses especially high risks to child development if ingested or inhaled. Overall, only about 20% of e-waste produced was collected using environmentally sound methods, according to UN estimates (Un and Forum 2019). So solutions for responsible lifecycle management are urgently required to contain the unsafe disposal as volume soars higher.\n\nUn, and World Economic Forum. 2019. A New Circular Vision for Electronics, Time for a Global Reboot. PACE - Platform for Accelerating the Circular Economy. https://www3.weforum.org/docs/WEF\\_A\\_New\\_Circular\\_Vision\\_for\\_Electronics.pdf.\n\n\n17.11.2 Disposable Electronics\nThe rapidly falling costs of microcontrollers, tiny rechargeable batteries, and compact communication hardware have enabled the embedding of intelligent sensor systems throughout everyday consumer goods. These internet-of-things (IoT) devices monitor product conditions, user interactions, and environmental factors to enable real-time responsiveness, personalization, and data-driven business decisions in the evolving connected marketplace.\nHowever, these embedded electronics face little oversight or planning around sustainably handling their eventual disposal once the often plastic-encased products are discarded after brief lifetimes. IoT sensors now commonly reside in single-use items like water bottles, food packaging, prescription bottles, and cosmetic containers that overwhelmingly enter landfill waste streams after a few weeks to months of consumer use.\nThe problem accelerates as more manufacturers rush to integrate mobile chips, power sources, Bluetooth modules, and other modern silicon ICs, costing under US$1, into various merchandise without protocols for recycling, replacing batteries, or component reusability. Despite their small individual size, the volumes of these devices and lifetime waste burden loom large. Unlike regulating larger electronics, few policy constraints exist around materials requirements or toxicity in tiny disposable gadgets.\nWhile offering convenience when working, the unsustainable combination of difficult retrievability and limited safe breakdown mechanisms causes disposable connected devices to contribute outsized shares of future e-waste volumes needing urgent attention.\n\n\n17.11.3 Planned Obsolescence\nPlanned obsolescence refers to the intentional design strategy of manufacturing products with artificially limited lifetimes that quickly become non-functional or outdated. This spurs faster replacement purchase cycles as consumers find devices no longer meet their needs within a few years. However, electronics designed for premature obsolescence contribute to unsustainable e-waste volumes.\nFor example, gluing smartphone batteries and components together hinders repairability compared to modular, accessible assemblies. Rolling out software updates that deliberately slow system performance creates a perception that upgrading devices produced only several years earlier is worth it.\nLikewise, fashionable introductions of new product generations with minor but exclusive feature additions make prior versions rapidly seem dated. These tactics compel buying new gadgets (e.g., iPhones) long before operational endpoints. When multiplied across fast-paced electronics categories, billions of barely worn items are discarded annually.\nPlanned obsolescence thus intensifies resource utilization and waste creation in making products with no intention for long lifetimes. This contradicts sustainability principles around durability, reuse, and material conservation. While stimulating continuous sales and gains for manufacturers in the short term, the strategy externalizes environmental costs and toxins onto communities lacking proper e-waste processing infrastructure.\nPolicy and consumer action are crucial to counter gadget designs that are needlessly disposable by default. Companies should also invest in product stewardship programs supporting responsible reuse and reclamation.\nConsider the real-world example. Apple has faced scrutiny over the years for allegedly engaging in planned obsolescence to encourage customers to buy new iPhone models. The company allegedly designed its phones so that performance degrades over time or existing features become incompatible with new operating systems, which critics argue is meant to spur more rapid upgrade cycles. In 2020, Apple paid a 25 million Euros fine to settle a case in France where regulators found the company guilty of intentionally slowing down older iPhones without clearly informing customers via iOS updates.\nBy failing to be transparent about power management changes that reduced device performance, Apple participated in deceptive activities that reduced product lifespan to drive sales. The company claimed it was done to “smooth out” peaks that could suddenly cause older batteries to shut down. However, this example highlights the legal risks around employing planned obsolescence and not properly disclosing when functionality changes impact device usability over time- even leading brands like Apple can run into trouble if perceived as intentionally shortening product life cycles.",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Sustainable AI</span>"
    ]
  },
  {
    "objectID": "contents/core/sustainable_ai/sustainable_ai.html#policy-and-regulatory-considerations",
    "href": "contents/core/sustainable_ai/sustainable_ai.html#policy-and-regulatory-considerations",
    "title": "17  Sustainable AI",
    "section": "17.12 Policy and Regulatory Considerations",
    "text": "17.12 Policy and Regulatory Considerations\n\n17.12.1 Measurement and Reporting Mandates\nOne policy mechanism that is increasingly relevant for AI systems is measurement and reporting requirements regarding energy consumption and carbon emissions. Mandated metering, auditing, disclosures, and more rigorous methodologies aligned to sustainability metrics can help address information gaps hindering efficiency optimizations.\nSimultaneously, national or regional policies require companies above a certain size to use AI in their products or backend systems to report energy consumption or emissions associated with major AI workloads. Organizations like the Partnership on AI, IEEE, and NIST could help shape standardized methodologies. More complex proposals involve defining consistent ways to measure computational complexity, data center PUE, carbon intensity of energy supply, and efficiencies gained through AI-specific hardware.\nReporting obligations for public sector users procuring AI services—such as through proposed legislation in Europe—could also increase transparency. However, regulators must balance the additional measurement burden such mandates place on organizations against ongoing carbon reductions from ingraining sustainability-conscious development patterns.\nTo be most constructive, any measurement and reporting policies should focus on enabling continuous refinement rather than simplistic restrictions or caps. As AI advancements unfold rapidly, nimble governance guardrails that embed sustainability considerations into normal evaluation metrics can motivate positive change. However, overprescription risks constraining innovation if requirements grow outdated. AI efficiency policy accelerates progress industry-wide by combining flexibility with appropriate transparency guardrails.\n\n\n17.12.2 Restriction Mechanisms\nIn addition to reporting mandates, policymakers have several restriction mechanisms that could directly shape how AI systems are developed and deployed to curb emissions:\nCaps on Computing Emissions: The European Commission’s proposed AI Act takes a horizontal approach that could allow setting economy-wide caps on the volume of computing power available for training AI models. Like emissions trading systems, caps aim to disincentivize extensive computing over sustainability indirectly. However, model quality could be improved to provide more pathways for procuring additional capacity.\nConditioning Access to Public Resources: Some experts have proposed incentives like only allowing access to public datasets or computing power for developing fundamentally efficient models rather than extravagant architectures. For example, the MLCommons benchmarking consortium founded by major tech firms could formally integrate efficiency into its standardized leaderboard metrics—however, conditioned access risks limiting innovation.\nFinancial Mechanisms: Analogous to carbon taxes on polluting industries, fees applied per unit of AI-related compute consumption could discourage unnecessary model scaling while funding efficiency innovations. Tax credits could alternatively reward organizations pioneering more accurate but compact AI techniques. However, financial tools require careful calibration between revenue generation and fairness and not over-penalizing productive uses of AI.\nTechnology Bans: If measurement consistently pinned extreme emissions on specific applications of AI without paths for remediation, outright bans present a tool of last resort for policymakers. However, given AI’s dual use, defining harmful versus beneficial deployments proves complex, necessitating holistic impact assessment before concluding no redeeming value exists. Banning promising technologies risks unintended consequences and requires caution.\n\n\n17.12.3 Government Incentives\nIt is a common practice for governments to provide tax or other incentives to consumers or businesses when contributing to more sustainable technological practices. Such incentives already exist in the US for adopting solar panels or energy-efficient buildings. To the best of our knowledge, no such tax incentives exist for AI-specific development practices yet.\nAnother potential incentive program that is beginning to be explored is using government grants to fund Green AI projects. For example, in Spain, 300 million euros have been allocated to specifically fund projects in AI and sustainability. Government incentives are a promising avenue to encourage sustainable business and consumer behavior practices, but careful thought is required to determine how those incentives will fit into market demands (Cohen, Lobel, and Perakis 2016).\n\nCohen, Maxime C., Ruben Lobel, and Georgia Perakis. 2016. “The Impact of Demand Uncertainty on Consumer Subsidies for Green Technology Adoption.” Manage. Sci. 62 (5): 1235–58. https://doi.org/10.1287/mnsc.2015.2173.\n\n\n17.12.4 Self-Regulation\nComplimentary to potential government action, voluntary self-governance mechanisms allow the AI community to pursue sustainability ends without top-down intervention:\nRenewables Commitments: Large AI practitioners like Google, Microsoft, Amazon, and Meta have pledged to procure enough renewable electricity to match 100% of their energy demands. These commitments unlock compounding emissions cuts as compute scales up. Formalizing such programs incentivizes green data center regions. However, there are critiques on whether these pledges are enough (Monyei and Jenkins 2018).\n\nMonyei, Chukwuka G., and Kirsten E. H. Jenkins. 2018. “Electrons Have No Identity: Setting Right Misrepresentations in Google and Apple’s Clean Energy Purchasing.” Energy Research &Amp; Social Science 46 (December): 48–51. https://doi.org/10.1016/j.erss.2018.06.015.\nInternal Carbon Prices: Some organizations use shadow prices on carbon emissions to represent environmental costs in capital allocation decisions between AI projects. If modeled effectively, theoretical charges on development carbon footprints steer funding toward efficient innovations rather than solely accuracy gains.\nEfficiency Development Checklists: Groups like the AI Sustainability Coalition suggest voluntary checklist templates highlighting model design choices, hardware configurations, and other factors architects can tune per application to restrain emissions. Organizations can drive change by ingraining sustainability as a primary success metric alongside accuracy and cost.\nIndependent Auditing: Even absent public disclosure mandates, firms specializing in technology sustainability audits help AI developers identify waste, create efficiency roadmaps, and benchmark progress via impartial reviews. Structuring such audits into internal governance procedures or the procurement process expands accountability.\n\n\n17.12.5 Global Considerations\nWhile measurement, restrictions, incentives, and self-regulation represent potential policy mechanisms for furthering AI sustainability, fragmentation across national regimes risks unintended consequences. As with other technology policy domains, divergence between regions must be carefully managed.\nFor example, due to regional data privacy concerns, OpenAI barred European users from accessing its viral ChatGPT chatbot. This came after the EU’s proposed AI Act signaled a precautionary approach, allowing the EC to ban certain high-risk AI uses and enforcing transparency rules that create uncertainty for releasing brand new models. However, it would be wise to caution against regulator action as it could inadvertently limit European innovation if regimes with lighter-touch regulation attract more private-sector AI research spending and talent. Finding common ground is key.\nThe OECD principles on AI and the United Nations frameworks underscore universally agreed-upon tenets all national policies should uphold: transparency, accountability, bias mitigation, and more. Constructively embedding sustainability as a core principle for responsible AI within international guidance can motivate unified action without sacrificing flexibility across divergent legal systems. Avoiding race-to-the-bottom dynamics hinges on enlightened multilateral cooperation.",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Sustainable AI</span>"
    ]
  },
  {
    "objectID": "contents/core/sustainable_ai/sustainable_ai.html#public-perception-and-engagement",
    "href": "contents/core/sustainable_ai/sustainable_ai.html#public-perception-and-engagement",
    "title": "17  Sustainable AI",
    "section": "17.13 Public Perception and Engagement",
    "text": "17.13 Public Perception and Engagement\nAs societal attention and policy efforts aimed at environmental sustainability ramp up worldwide, there is growing enthusiasm for leveraging AI to help address ecological challenges. However, public understanding and attitudes toward the role of AI systems in sustainability contexts still need to be clarified and clouded by misconceptions. On the one hand, people hope advanced algorithms can provide new solutions for green energy, responsible consumption, decarbonization pathways, and ecosystem preservation. On the other, fears regarding the risks of uncontrolled AI also seep into the environmental domain and undermine constructive discourse. Furthermore, a lack of public awareness on key issues like transparency in developing sustainability-focused AI tools and potential biases in data or modeling also threaten to limit inclusive participation and degrade public trust.\nTackling complex, interdisciplinary priorities like environmental sustainability requires informed, nuanced public engagement and responsible advances in AI innovation. The path forward demands careful, equitable collaborative efforts between experts in ML, climate science, environmental policy, social science, and communication. Mapping the landscape of public perceptions, identifying pitfalls, and charting strategies to cultivate understandable, accessible, and trustworthy AI systems targeting shared ecological priorities will prove essential to realizing sustainability goals. This complex terrain warrants a deep examination of the sociotechnical dynamics involved.\n\n17.13.1 AI Awareness\nIn May 2022, the Pew Research Center polled 5,101 US adults, finding 60% had heard or read “a little” about AI while 27% heard “a lot”–indicating decent broad recognition, but likely limited comprehension about details or applications. However, among those with some AI familiarity, concerns emerge regarding risks of personal data misuse according to agreed terms. Still, 62% felt AI could ease modern life if applied responsibly. Yet, a specific understanding of sustainability contexts still needs to be improved.\nStudies attempting to categorize online discourse sentiments find a nearly even split between optimism and caution regarding deploying AI for sustainability goals. Factors driving positivity include hopes around better forecasting of ecological shifts using ML models. Negativity arises from a lack of confidence in self-supervised algorithms avoiding unintended consequences due to unpredictable human impacts on complex natural systems during training.\nThe most prevalent public belief remains that while AI does harbor the potential for accelerating solutions on issues like emission reductions and wildlife protections, inadequate safeguarding around data biases, ethical blindspots, and privacy considerations could be more appreciated risks if pursued carelessly, especially at scale. This leads to hesitancy around unconditional support without evidence of deliberate, democratically guided development.\n\n\n17.13.2 Messaging\nOptimistic efforts are highlighting AI’s sustainability promise and emphasize the potential for advanced ML to radically accelerate decarbonization effects from smart grids, personalized carbon tracking apps, automated building efficiency optimizations, and predictive analytics guiding targeted conservation efforts. More comprehensive real-time modeling of complex climate and ecological shifts using self-improving algorithms offers hope for mitigating biodiversity losses and averting worst-case scenarios.\nHowever, cautionary perspectives, such as the Asilomar AI Principles, question whether AI itself could exacerbate sustainability challenges if improperly constrained. The rising energy demands of large-scale computing systems and the increasingly massive neural network model training conflict with clean energy ambitions. Lack of diversity in data inputs or developers’ priorities may downplay urgent environmental justice considerations. Near-term skeptical public engagement likely hinges on a need for perceivable safeguards against uncontrolled AI systems running amok on core ecological processes.\nIn essence, polarized framings either promote AI as an indispensable tool for sustainability problem-solving–if compassionately directed toward people and the planet–or present AI as an amplifier of existing harms insidiously dominating hidden facets of natural systems central to all life. Overcoming such impasses demands balancing honest trade-off discussions with shared visions for equitable, democratically governed technological progress targeting restoration.\n\n\n17.13.3 Equitable Participation\nEnsuring equitable participation and access should form a cornerstone of any sustainability initiative with the potential for major societal impacts. This principle applies equally to AI systems targeting environmental goals. However, commonly excluded voices like frontline, rural, or indigenous communities and future generations not present to consent could suffer disproportionate consequences from technology transformations. For instance, the Partnership on AI has launched events expressly targeting input from marginalized communities on deploying AI responsibly.\nEnsuring equitable access and participation should form a cornerstone of any sustainability initiative with the potential for major societal impacts, whether AI or otherwise. However, inclusive engagement in environmental AI relies partly on the availability and understanding of fundamental computing resources. As the recent OECD report on National AI Compute Capacity highlights (Oecd 2023), many countries currently lack data or strategic plans mapping needs for the infrastructure required to fuel AI systems. This policy blindspot could constrain economic goals and exacerbate barriers to entry for marginalized populations. Their blueprint urges developing national AI compute capacity strategies along dimensions of capacity, accessibility, innovation pipelines, and resilience to anchor innovation. The underlying data storage needs to be improved, and model development platforms or specialized hardware could inadvertently concentrate AI progress in the hands of select groups. Therefore, planning for a balanced expansion of fundamental AI computing resources via policy initiatives ties directly to hopes for democratized sustainability problem-solving using equitable and transparent ML tools.\n\nOecd. 2023. “A Blueprint for Building National Compute Capacity for Artificial Intelligence.” 350. Organisation for Economic Co-Operation; Development (OECD). https://doi.org/10.1787/876367e3-en.\nThe key idea is that equitable participation in AI systems targeting environmental challenges relies in part on ensuring the underlying computing capacity and infrastructure are correct, which requires proactive policy planning from a national perspective.\n\n\n17.13.4 Transparency\nAs public sector agencies and private companies alike rush towards adopting AI tools to help tackle pressing environmental challenges, calls for transparency around these systems’ development and functionality have begun to amplify. Explainable and interpretable ML features grow more crucial for building trust in emerging models aiming to guide consequential sustainability policies. Initiatives like the Montreal Carbon Pledge brought tech leaders together to commit to publishing impact assessments before launching environmental systems, as pledged below:\n\n“As institutional investors, we must act in the best long-term interests of our beneficiaries. In this fiduciary role, long-term investment risks are associated with greenhouse gas emissions, climate change, and carbon regulation. Measuring our carbon footprint is integral to understanding better, quantifying, and managing the carbon and climate change-related impacts, risks, and opportunities in our investments. Therefore, as a first step, we commit to measuring and disclosing the carbon footprint of our investments annually to use this information to develop an engagement strategy and identify and set carbon footprint reduction targets.” – Montréal Carbon Pledge\n\nWe need a similar pledge for AI sustainability and responsibility. Widespread acceptance and impact of AI sustainability solutions will partly be on deliberate communication of validation schemes, metrics, and layers of human judgment applied before live deployment. Efforts like NIST’s Principles for Explainable AI can help foster transparency into AI systems. The National Institute of Standards and Technology (NIST) has published an influential set of guidelines dubbed the Principles for Explainable AI (Phillips et al. 2020). This framework articulates best practices for designing, evaluating, and deploying responsible AI systems with transparent and interpretable features that build critical user understanding and trust.\n\nPhillips, P Jonathon, Carina A Hahn, Peter C Fontana, David A Broniatowski, and Mark A Przybocki. 2020. “Four Principles of Explainable Artificial Intelligence.” Gaithersburg, Maryland 18.\nIt delineates four core principles: Firstly, AI systems should provide contextually relevant explanations justifying the reasoning behind their outputs to appropriate stakeholders. Secondly, these AI explanations must communicate information meaningfully for their target audience’s appropriate comprehension level. Next is the accuracy principle, which dictates that explanations should faithfully reflect the actual process and logic informing an AI model’s internal mechanics for generating given outputs or recommendations based on inputs. Finally, a knowledge limits principle compels explanations to clarify an AI model’s boundaries in capturing the full breadth of real-world complexity, variance, and uncertainties within a problem space.\nAltogether, these NIST principles offer AI practitioners and adopters guidance on key transparency considerations vital for developing accessible solutions prioritizing user autonomy and trust rather than simply maximizing predictive accuracy metrics alone. As AI rapidly advances across sensitive social contexts like healthcare, finance, employment, and beyond, such human-centered design guidelines will continue growing in importance for anchoring innovation to public interests.\nThis applies equally to the domain of environmental ability. Responsible and democratically guided AI innovation targeting shared ecological priorities depends on maintaining public vigilance, understanding, and oversight over otherwise opaque systems taking prominent roles in societal decisions. Prioritizing explainable algorithm designs and radical transparency practices per global standards can help sustain collective confidence that these tools improve rather than imperil hopes for a driven future.",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Sustainable AI</span>"
    ]
  },
  {
    "objectID": "contents/core/sustainable_ai/sustainable_ai.html#future-directions-and-challenges",
    "href": "contents/core/sustainable_ai/sustainable_ai.html#future-directions-and-challenges",
    "title": "17  Sustainable AI",
    "section": "17.14 Future Directions and Challenges",
    "text": "17.14 Future Directions and Challenges\nAs we look towards the future, the role of AI in environmental sustainability is poised to grow even more significant. AI’s potential to drive advancements in renewable energy, climate modeling, conservation efforts, and more is immense. However, it is a two-sided coin, as we need to overcome several challenges and direct our efforts towards sustainable and responsible AI development.\n\n17.14.1 Future Directions\nOne key future direction is the development of more energy-efficient AI models and algorithms. This involves ongoing research and innovation in areas like model pruning, quantization, and the use of low-precision numerics, as well as developing the hardware to enable full profitability of these innovations. Even further, we look at alternative computing paradigms that do not rely on von-Neumann architectures. More on this topic can be found in the hardware acceleration chapter. The goal is to create AI systems that deliver high performance while minimizing energy consumption and carbon emissions.\nAnother important direction is the integration of renewable energy sources into AI infrastructure. As data centers continue to be major contributors to AI’s carbon footprint, transitioning to renewable energy sources like solar and wind is crucial. Developments in long-term, sustainable energy storage, such as Ambri, an MIT spinoff, could enable this transition. This requires significant investment and collaboration between tech companies, energy providers, and policymakers.\n\n\n17.14.2 Challenges\nDespite these promising directions, several challenges need to be addressed. One of the major challenges is the need for consistent standards and methodologies for measuring and reporting the environmental impact of AI. These methods must capture the complexity of the life cycles of AI models and system hardware. Also, efficient and environmentally sustainable AI infrastructure and system hardware are needed. This consists of three components:\n\nMaximize the utilization of accelerator and system resources.\nProlong the lifetime of AI infrastructure.\nDesign systems hardware with environmental impact in mind.\n\nOn the software side, we should trade off experimentation and the subsequent training cost. Techniques such as neural architecture search and hyperparameter optimization can be used for design space exploration. However, these are often very resource-intensive. Efficient experimentation can significantly reduce the environmental footprint overhead. Next, methods to reduce wasted training efforts should be explored.\nTo improve model quality, we often scale the dataset. However, the increased system resources required for data storage and ingestion caused by this scaling have a significant environmental impact (Wu et al. 2022). A thorough understanding of the rate at which data loses its predictive value and devising data sampling strategies is important.\n\nWu, Carole-Jean, Ramya Raghavendra, Udit Gupta, Bilge Acun, Newsha Ardalani, Kiwan Maeng, Gloria Chang, et al. 2022. “Sustainable Ai: Environmental Implications, Challenges and Opportunities.” Proceedings of Machine Learning and Systems 4: 795–813.\nData gaps also pose a significant challenge. Without companies and governments openly sharing detailed and accurate data on energy consumption, carbon emissions, and other environmental impacts, it isn’t easy to develop effective strategies for sustainable AI.\nFinally, the fast pace of AI development requires an agile approach to the policy imposed on these systems. The policy should ensure sustainable development without constraining innovation. This requires experts in all domains of AI, environmental sciences, energy, and policy to work together to achieve a sustainable future.",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Sustainable AI</span>"
    ]
  },
  {
    "objectID": "contents/core/sustainable_ai/sustainable_ai.html#conclusion",
    "href": "contents/core/sustainable_ai/sustainable_ai.html#conclusion",
    "title": "17  Sustainable AI",
    "section": "17.15 Conclusion",
    "text": "17.15 Conclusion\nWe must address sustainability considerations as AI rapidly expands across industries and society. AI promises breakthrough innovations, yet its environmental footprint threatens its widespread growth. This chapter analyzes multiple facets, from energy and emissions to waste and biodiversity impacts, that AI/ML developers must weigh when creating responsible AI systems.\nFundamentally, we require elevating sustainability as a primary design priority rather than an afterthought. Techniques like energy-efficient models, renewable-powered data centers, and hardware recycling programs offer solutions, but the holistic commitment remains vital. We need standards around transparency, carbon accounting, and supply chain disclosures to supplement technical gains. Still, examples like Google’s 4M efficiency practices containing ML energy use highlight that we can advance AI in lockstep with environmental objectives with concerted effort. We achieve this harmonious balance by having researchers, corporations, regulators, and users collaborate across domains. The aim is not perfect solutions but continuous improvement as we integrate AI across new sectors.",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Sustainable AI</span>"
    ]
  },
  {
    "objectID": "contents/core/sustainable_ai/sustainable_ai.html#sec-sustainable-ai-resource",
    "href": "contents/core/sustainable_ai/sustainable_ai.html#sec-sustainable-ai-resource",
    "title": "17  Sustainable AI",
    "section": "17.16 Resources",
    "text": "17.16 Resources\nHere is a curated list of resources to support students and instructors in their learning and teaching journeys. We are continuously working on expanding this collection and will add new exercises soon.\n\n\n\n\n\n\nSlides\n\n\n\n\n\nThese slides are a valuable tool for instructors to deliver lectures and for students to review the material at their own pace. We encourage students and instructors to leverage these slides to improve their understanding and facilitate effective knowledge transfer.\n\nTransparency and Sustainability.\nSustainability of TinyML.\nModel Cards for Transparency.\n\n\n\n\n\n\n\n\n\n\nVideos\n\n\n\n\n\n\nComing soon.\n\n\n\n\n\n\n\n\n\n\nExercises\n\n\n\n\n\nTo reinforce the concepts covered in this chapter, we have curated a set of exercises that challenge students to apply their knowledge and deepen their understanding.\n\nExercise 17.1\nExercise 17.2",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Sustainable AI</span>"
    ]
  },
  {
    "objectID": "contents/core/robust_ai/robust_ai.html",
    "href": "contents/core/robust_ai/robust_ai.html",
    "title": "18  Robust AI",
    "section": "",
    "text": "Purpose\nResources: Slides, Videos, Exercises\nHow do uncertainty and variability shape machine learning system design, and what principles enable reliable operation in challenging conditions?\nThe exposure of AI systems to real-world conditions presents fundamental challenges in maintaining consistent performance. Operational patterns reveal essential relationships between system stability and environmental variability, highlighting critical trade-offs between resilience and efficiency. The implementation of robust architectures emphasizes the need for strategies to maintain reliability across diverse and unpredictable scenarios while preserving core functionality. Understanding these resilience dynamics provides insights into creating dependable systems, establishing principles for designing AI solutions that maintain effectiveness even when faced with distributional shifts, noise, and adversarial conditions.",
    "crumbs": [
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Robust AI</span>"
    ]
  },
  {
    "objectID": "contents/core/robust_ai/robust_ai.html#purpose",
    "href": "contents/core/robust_ai/robust_ai.html#purpose",
    "title": "18  Robust AI",
    "section": "",
    "text": "Learning Objectives\n\n\n\n\nUnderstand the importance of robust and resilient AI systems in real-world applications.\nIdentify and characterize hardware faults, software faults, and their impact on ML systems.\nRecognize and develop defensive strategies against threats posed by adversarial attacks, data poisoning, and distribution shifts.\nLearn techniques for detecting, mitigating, and designing fault-tolerant ML systems.\nBecome familiar with tools and frameworks for studying and enhancing ML system resilience throughout the AI development lifecycle.",
    "crumbs": [
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Robust AI</span>"
    ]
  },
  {
    "objectID": "contents/core/robust_ai/robust_ai.html#overview",
    "href": "contents/core/robust_ai/robust_ai.html#overview",
    "title": "18  Robust AI",
    "section": "18.1 Overview",
    "text": "18.1 Overview\nRobust AI refers to a system’s ability to maintain its performance and reliability in the presence of errors. A robust machine learning system is designed to be fault-tolerant and error-resilient, capable of operating effectively even under adverse conditions.\nAs ML systems become increasingly integrated into various aspects of our lives, from cloud-based services to edge devices and embedded systems, the impact of hardware and software faults on their performance and reliability becomes more significant. In the future, as ML systems become more complex and are deployed in even more critical applications, the need for robust and fault-tolerant designs will be paramount.\nML systems are expected to play crucial roles in autonomous vehicles, smart cities, healthcare, and industrial automation domains. In these domains, the consequences of hardware or software faults can be severe, potentially leading to loss of life, economic damage, or environmental harm.\nResearchers and engineers must focus on developing advanced techniques for fault detection, isolation, and recovery to mitigate these risks and ensure the reliable operation of future ML systems.\nThis chapter will focus specifically on three main categories of faults and errors that can impact the robustness of ML systems: hardware faults, software faults, and human errors.\n\nHardware Faults: Transient, permanent, and intermittent faults can affect the hardware components of an ML system, corrupting computations and degrading performance.\nModel Robustness: ML models can be vulnerable to adversarial attacks, data poisoning, and distribution shifts, which can induce targeted misclassifications, skew the model’s learned behavior, or compromise the system’s integrity and reliability.\nSoftware Faults: Bugs, design flaws, and implementation errors in the software components, such as algorithms, libraries, and frameworks, can propagate errors and introduce vulnerabilities.\n\nThe specific challenges and approaches to achieving robustness may vary depending on the scale and constraints of the ML system. Large-scale cloud computing or data center systems may focus on fault tolerance and resilience through redundancy, distributed processing, and advanced error detection and correction techniques. In contrast, resource-constrained edge devices or embedded systems face unique challenges due to limited computational power, memory, and energy resources.\nRegardless of the scale and constraints, the key characteristics of a robust ML system include fault tolerance, error resilience, and performance maintenance. By understanding and addressing the multifaceted challenges to robustness, we can develop trustworthy and reliable ML systems that can navigate the complexities of real-world environments.\nThis chapter is not just about exploring ML systems’ tools, frameworks, and techniques for detecting and mitigating faults, attacks, and distributional shifts. It’s about emphasizing the crucial role of each one of you in prioritizing resilience throughout the AI development lifecycle, from data collection and model training to deployment and monitoring. By proactively addressing the challenges to robustness, we can unlock the full potential of ML technologies while ensuring their safe, reliable, and responsible deployment in real-world applications.\nAs AI continues to shape our future, the potential of ML technologies is immense. But it’s only when we build resilient systems that can withstand the challenges of the real world that we can truly harness this potential. This is a defining factor in the success and societal impact of this transformative technology, and it’s within our reach.",
    "crumbs": [
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Robust AI</span>"
    ]
  },
  {
    "objectID": "contents/core/robust_ai/robust_ai.html#real-world-examples",
    "href": "contents/core/robust_ai/robust_ai.html#real-world-examples",
    "title": "18  Robust AI",
    "section": "18.2 Real-World Examples",
    "text": "18.2 Real-World Examples\nHere are some real-world examples of cases where faults in hardware or software have caused major issues in ML systems across cloud, edge, and embedded environments:\n\n18.2.1 Cloud\nIn February 2017, Amazon Web Services (AWS) experienced a significant outage due to human error during maintenance. An engineer inadvertently entered an incorrect command, causing many servers to be taken offline. This outage disrupted many AWS services, including Amazon’s AI-powered assistant, Alexa. As a result, Alexa-powered devices, such as Amazon Echo and third-party products using Alexa Voice Service, could not respond to user requests for several hours. This incident highlights the potential impact of human errors on cloud-based ML systems and the need for robust maintenance procedures and failsafe mechanisms.\nIn another example (Vangal et al. 2021), Facebook encountered a silent data corruption (SDC) issue within its distributed querying infrastructure, as shown in Figure 18.1. SDC refers to undetected errors during computation or data transfer that often propagate unnoticed through system layers. Facebook’s querying infrastructure processed SQL-like queries across datasets and supported a compression application to reduce the footprint of data stores. In this application, files were compressed when not being read and decompressed when a read request was made. Before decompression, the file size was checked to ensure it was greater than zero, indicating a valid compressed file with contents. However, an unexpected error caused the system to return a zero file size for a valid file, leading to decompression failure and missing files in the output database. This issue manifested sporadically, with some computations returning the correct non-zero file size, making it particularly challenging to diagnose.\n\nVangal, Sriram, Somnath Paul, Steven Hsu, Amit Agarwal, Saurabh Kumar, Ram Krishnamurthy, Harish Krishnamurthy, James Tschanz, Vivek De, and Chris H. Kim. 2021. “Wide-Range Many-Core SoC Design in Scaled CMOS: Challenges and Opportunities.” IEEE Trans. Very Large Scale Integr. VLSI Syst. 29 (5): 843–56. https://doi.org/10.1109/tvlsi.2021.3061649.\n\n\n\n\n\n\nFigure 18.1: Silent data corruption in database applications. Source: Facebook\n\n\n\nThis case illustrates how silent data corruption can propagate through multiple layers of an application stack, leading to data loss and application failures in large-scale distributed systems. Such issues, if left undetected, can severely impact ML systems. For example, corrupted training data or inconsistencies in data pipelines due to SDC can degrade model performance or skew predictions. Other major companies, such as Google, also face similar issues in their AI hypercomputers. Figure 18.2 Jeff Dean, Chief Scientist at Google DeepMind and Google Research, discussed these challenges and their implications for ML systems at MLSys 2024.\n\n\n\n\n\n\nFigure 18.2: Silent data corruption (SDC) errors are a major issue for AI hypercomputers. Source: Jeff Dean at MLSys 2024, Keynote (Google)\n\n\n\n\n\n18.2.2 Edge\nRegarding examples of faults and errors in edge ML systems, one area that has gathered significant attention is the domain of self-driving cars. Self-driving vehicles rely heavily on machine learning algorithms for perception, decision-making, and control, making them particularly susceptible to the impact of hardware and software faults. In recent years, several high-profile incidents involving autonomous vehicles have highlighted the challenges and risks associated with deploying these systems in real-world environments.\nIn May 2016, a fatal accident occurred when a Tesla Model S operating on Autopilot crashed into a white semi-trailer truck crossing the highway. The Autopilot system, which relied on computer vision and machine learning algorithms, failed to recognize the white trailer against a bright sky background. The driver, who was reportedly watching a movie when the crash, did not intervene in time, and the vehicle collided with the trailer at full speed as shown in Figure 18.3. This incident raised concerns about the limitations of AI-based perception systems and the need for robust failsafe mechanisms in autonomous vehicles. Similarly in March 2018, an Uber self-driving test vehicle struck and killed a pedestrian crossing the street in Tempe, Arizona. The incident was caused by a software flaw in the vehicle’s object recognition system, which failed to identify the pedestrians appropriately to avoid them as obstacles.\n\n\n\n\n\n\nFigure 18.3: Tesla in the fatal California crash was on Autopilot. Source: BBC News\n\n\n\n\n\n18.2.3 Embedded\nEmbedded systems, which often operate in resource-constrained environments and safety-critical applications, have long faced challenges related to hardware and software faults. As AI and machine learning technologies are increasingly integrated into these systems, the potential for faults and errors takes on new dimensions, with the added complexity of AI algorithms and the critical nature of the applications in which they are deployed.\nLet’s consider a few examples, starting with outer space exploration. NASA’s Mars Polar Lander mission in 1999 suffered a catastrophic failure due to a software error in the touchdown detection system (Figure 18.4). The spacecraft’s onboard software mistakenly interpreted the noise from the deployment of its landing legs as a sign that it had touched down on the Martian surface. As a result, the spacecraft prematurely shut down its engines, causing it to crash into the surface. This incident highlights the critical importance of robust software design and extensive testing in embedded systems, especially those operating in remote and unforgiving environments. As AI capabilities are integrated into future space missions, ensuring these systems’ reliability and fault tolerance will be paramount to mission success.\n\n\n\n\n\n\nFigure 18.4: NASA’s Failed Mars Polar Lander mission in 1999 cost over $200M. Source: SlashGear\n\n\n\nBack on earth, in 2015, a Boeing 787 Dreamliner experienced a complete electrical shutdown during a flight due to a software bug in its generator control units. This incident underscores the potential for software faults to have severe consequences in complex embedded systems like aircraft. As AI technologies are increasingly applied in aviation, such as in autonomous flight systems and predictive maintenance, ensuring the robustness and reliability of these systems will be critical to passenger safety.\n\n“If the four main generator control units (associated with the engine-mounted generators) were powered up at the same time, after 248 days of continuous power, all four GCUs will go into failsafe mode at the same time, resulting in a loss of all AC electrical power regardless of flight phase.” – Federal Aviation Administration directive (2015)\n\nAs AI capabilities increasingly integrate into embedded systems, the potential for faults and errors becomes more complex and severe. Imagine a smart pacemaker that has a sudden glitch. A patient could die from that effect. Therefore, AI algorithms, such as those used for perception, decision-making, and control, introduce new sources of potential faults, such as data-related issues, model uncertainties, and unexpected behaviors in edge cases. Moreover, the opaque nature of some AI models can make it challenging to identify and diagnose faults when they occur.",
    "crumbs": [
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Robust AI</span>"
    ]
  },
  {
    "objectID": "contents/core/robust_ai/robust_ai.html#hardware-faults",
    "href": "contents/core/robust_ai/robust_ai.html#hardware-faults",
    "title": "18  Robust AI",
    "section": "18.3 Hardware Faults",
    "text": "18.3 Hardware Faults\nHardware faults are a significant challenge in computing systems, including traditional and ML systems. These faults occur when physical components, such as processors, memory modules, storage devices, or interconnects, malfunction or behave abnormally. Hardware faults can cause incorrect computations, data corruption, system crashes, or complete system failure, compromising the integrity and trustworthiness of the computations performed by the system (Jha et al. 2019). A complete system failure refers to a situation where the entire computing system becomes unresponsive or inoperable due to a critical hardware malfunction. This type of failure is the most severe, as it renders the system unusable and may lead to data loss or corruption, requiring manual intervention to repair or replace the faulty components.\nML systems depend on complex hardware architectures and large-scale computations to train and deploy models that learn from data and make intelligent predictions. Hardware faults can disrupt the MLOps pipeline, introducing errors that compromise model accuracy, robustness, and reliability (G. Li et al. 2017). Understanding the types of hardware faults, their mechanisms, and their impact on system behavior is essential for developing strategies to detect, mitigate, and recover from these issues.\nThe following sections will explore the three main categories of hardware faults: transient, permanent, and intermittent. We will discuss their definitions, characteristics, causes, mechanisms, and examples of how they manifest in computing systems. We will also cover detection and mitigation techniques specific to each fault type.\n\nTransient Faults: Transient faults are temporary and non-recurring. They are often caused by external factors such as cosmic rays, electromagnetic interference, or power fluctuations. A common example of a transient fault is a bit flip, where a single bit in a memory location or register changes its value unexpectedly. Transient faults can lead to incorrect computations or data corruption, but they do not cause permanent damage to the hardware.\nPermanent Faults: Permanent faults, also called hard errors, are irreversible and persist over time. They are typically caused by physical defects or wear-out of hardware components. Examples of permanent faults include stuck-at faults, where a bit or signal is permanently set to a specific value (e.g., always 0 or always 1), and device failures, such as a malfunctioning processor or a damaged memory module. Permanent faults can result in complete system failure or significant performance degradation.\nIntermittent Faults: Intermittent faults are recurring faults that appear and disappear intermittently. Unstable hardware conditions, such as loose connections, aging components, or manufacturing defects, often cause them. Intermittent faults can be challenging to diagnose and reproduce because they may occur sporadically and under specific conditions. Examples include intermittent short circuits or contact resistance issues. Intermittent faults can lead to unpredictable system behavior and intermittent errors.\n\nBy the end of this discussion, readers will have a solid understanding of fault taxonomy and its relevance to traditional computing and ML systems. This foundation will help them make informed decisions when designing, implementing, and deploying fault-tolerant solutions, improving the reliability and trustworthiness of their computing systems and ML applications.\n\n18.3.1 Transient Faults\nTransient faults in hardware can manifest in various forms, each with its own unique characteristics and causes. These faults are temporary in nature and do not result in permanent damage to the hardware components.\n\nDefinition and Characteristics\nSome of the common types of transient faults include Single Event Upsets (SEUs) caused by ionizing radiation, voltage fluctuations (Reddi and Gupta 2013) due to power supply noise or electromagnetic interference, Electromagnetic Interference (EMI) induced by external electromagnetic fields, Electrostatic Discharge (ESD) resulting from sudden static electricity flow, crosstalk caused by unintended signal coupling, ground bounce triggered by simultaneous switching of multiple outputs, timing violations due to signal timing constraint breaches, and soft errors in combinational logic affecting the output of logic circuits (Mukherjee, Emer, and Reinhardt 2005). Understanding these different types of transient faults is crucial for designing robust and resilient hardware systems that can mitigate their impact and ensure reliable operation.\n\nReddi, Vijay Janapa, and Meeta Sharma Gupta. 2013. Resilient Architecture Design for Voltage Variation. Springer International Publishing. https://doi.org/10.1007/978-3-031-01739-1.\n\nMukherjee, S. S., J. Emer, and S. K. Reinhardt. 2005. “The Soft Error Problem: An Architectural Perspective.” In 11th International Symposium on High-Performance Computer Architecture, 243–47. IEEE; IEEE. https://doi.org/10.1109/hpca.2005.37.\nAll of these transient faults are characterized by their short duration and non-permanent nature. They do not persist or leave any lasting impact on the hardware. However, they can still lead to incorrect computations, data corruption, or system misbehavior if not properly handled, as exemplified by bit-flip errors, where a single bit in memory unexpectedly changes state, potentially altering critical data or computations Figure 18.5.\n\n\n\n\n\n\nFigure 18.5: An illustration of a bit-flip error, where a single bit in memory changes state, leading to data corruption or computation errors.\n\n\n\n\n\nCauses of Transient Faults\nTransient faults can be attributed to various external factors. One common cause is cosmic rays, high-energy particles originating from outer space. When these particles strike sensitive areas of the hardware, such as memory cells or transistors, they can induce charge disturbances that alter the stored or transmitted data. This is illustrated in Figure 18.6. Another cause of transient faults is electromagnetic interference (EMI) from nearby devices or power fluctuations. EMI can couple with the circuits and cause voltage spikes or glitches that temporarily disrupt the normal operation of the hardware.\n\n\n\n\n\n\nFigure 18.6: Mechanism of Hardware Transient Fault Occurrence. Source: NTT\n\n\n\n\n\nMechanisms of Transient Faults\nTransient faults can manifest through different mechanisms depending on the affected hardware component. In memory devices like DRAM or SRAM, transient faults often lead to bit flips, where a single bit changes its value from 0 to 1 or vice versa. This can corrupt the stored data or instructions. In logic circuits, transient faults can cause glitches or voltage spikes propagating through the combinational logic, resulting in incorrect outputs or control signals. Transient faults can also affect communication channels, causing bit errors or packet losses during data transmission.\n\n\nImpact on ML Systems\nA common example of a transient fault is a bit flip in the main memory. If an important data structure or critical instruction is stored in the affected memory location, it can lead to incorrect computations or program misbehavior. If a transient fault occurs in the memory storing the model weights or gradients. For instance, a bit flip in the memory storing a loop counter can cause the loop to execute indefinitely or terminate prematurely. Transient faults in control registers or flag bits can alter the flow of program execution, leading to unexpected jumps or incorrect branch decisions. In communication systems, transient faults can corrupt transmitted data packets, resulting in retransmissions or data loss.\nIn ML systems, transient faults can have significant implications during the training phase (He et al. 2023). ML training involves iterative computations and updates to model parameters based on large datasets. If a transient fault occurs in the memory storing the model weights or gradients, it can lead to incorrect updates and compromise the convergence and accuracy of the training process. For example, a bit flip in the weight matrix of a neural network can cause the model to learn incorrect patterns or associations, leading to degraded performance (Wan et al. 2021). Transient faults in the data pipeline, such as corruption of training samples or labels, can also introduce noise and affect the quality of the learned model.As shown in Figure 18.7, a real-world example from Google’s production fleet highlights how a SDC anomaly caused a significant deviation in the gradient norm—a measure of the magnitude of updates to the model parameters. Such deviations can disrupt the optimization process, leading to slower convergence or failure to reach an optimal solution.\n\nWan, Zishen, Aqeel Anwar, Yu-Shun Hsiao, Tianyu Jia, Vijay Janapa Reddi, and Arijit Raychowdhury. 2021. “Analyzing and Improving Fault Tolerance of Learning-Based Navigation Systems.” In 2021 58th ACM/IEEE Design Automation Conference (DAC), 841–46. IEEE; IEEE. https://doi.org/10.1109/dac18074.2021.9586116.\n\n\n\n\n\n\nFigure 18.7: SDC in ML training phase results in anomalies in the gradient norm. Source: Jeff Dean, MLSys 2024 Keynote (Google)\n\n\n\nDuring the inference phase, transient faults can impact the reliability and trustworthiness of ML predictions. If a transient fault occurs in the memory storing the trained model parameters or in the computation of the inference results, it can lead to incorrect or inconsistent predictions. For instance, a bit flip in the activation values of a neural network can alter the final classification or regression output (Mahmoud et al. 2020). In safety-critical applications, such as autonomous vehicles or medical diagnosis, these faults can have severe consequences, resulting in incorrect decisions or actions that may compromise safety or lead to system failures (G. Li et al. 2017; Jha et al. 2019).\n\nLi, Guanpeng, Siva Kumar Sastry Hari, Michael Sullivan, Timothy Tsai, Karthik Pattabiraman, Joel Emer, and Stephen W. Keckler. 2017. “Understanding Error Propagation in Deep Learning Neural Network (DNN) Accelerators and Applications.” In Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis, 1–12. ACM. https://doi.org/10.1145/3126908.3126964.\n\nCourbariaux, Matthieu, Itay Hubara, Daniel Soudry, Ran El-Yaniv, and Yoshua Bengio. 2016. “Binarized Neural Networks: Training Deep Neural Networks with Weights and Activations Constrained to+ 1 or-1.” arXiv Preprint arXiv:1602.02830.\n\nAygun, Sercan, Ece Olcay Gunes, and Christophe De Vleeschouwer. 2021. “Efficient and Robust Bitstream Processing in Binarised Neural Networks.” Electron. Lett. 57 (5): 219–22. https://doi.org/10.1049/ell2.12045.\nTransient faults can be amplified in resource-constrained environments like TinyML, where limited computational and memory resources exacerbate their impact. One prominent example is Binarized Neural Networks (BNNs) (Courbariaux et al. 2016), which represent network weights in single-bit precision to achieve computational efficiency and faster inference times. While this binary representation is advantageous for resource-constrained systems, it also makes BNNs particularly fragile to bit-flip errors. For instance, prior work (Aygun, Gunes, and De Vleeschouwer 2021) has shown that a two-hidden layer BNN architecture for a simple task such as MNIST classification suffers performance degradation from 98% test accuracy to 70% when random bit-flipping soft errors are inserted through model weights with a 10% probability. To address these vulnerabilities, techniques like flip-aware training and emerging approaches such as stochastic computing are being explored to enhance fault tolerance. These strategies will be discussed further in Section 18.3.4.\n\n\n\n18.3.2 Permanent Faults\nPermanent faults are hardware defects that persist and cause irreversible damage to the affected components. These faults are characterized by their persistent nature and require repair or replacement of the faulty hardware to restore normal system functionality.\n\nDefinition and Characteristics\nPermanent faults are hardware defects that cause persistent and irreversible malfunctions in the affected components. The faulty component remains non-operational until a permanent fault is repaired or replaced. These faults are characterized by their consistent and reproducible nature, meaning that the faulty behavior is observed every time the affected component is used. Permanent faults can impact various hardware components, such as processors, memory modules, storage devices, or interconnects, leading to system crashes, data corruption, or complete system failure.\nOne notable example of a permanent fault is the Intel FDIV bug, which was discovered in 1994. The FDIV bug was a flaw in certain Intel Pentium processors’ floating-point division (FDIV) units. The bug caused incorrect results for specific division operations, leading to inaccurate calculations.\nThe FDIV bug occurred due to an error in the lookup table used by the division unit. In rare cases, the processor would fetch an incorrect value from the lookup table, resulting in a slightly less precise result than expected. For instance, Figure 18.8 shows a fraction 4195835/3145727 plotted on a Pentium processor with the FDIV permanent fault. The triangular regions are where erroneous calculations occurred. Ideally, all correct values would round to 1.3338, but the erroneous results show 1.3337, indicating a mistake in the 5th digit.\nAlthough the error was small, it could compound over many division operations, leading to significant inaccuracies in mathematical calculations. The impact of the FDIV bug was significant, especially for applications that relied heavily on precise floating-point division, such as scientific simulations, financial calculations, and computer-aided design. The bug led to incorrect results, which could have severe consequences in fields like finance or engineering.\n\n\n\n\n\n\nFigure 18.8: Intel Pentium processor with the FDIV permanent fault. The triangular regions are where erroneous calculations occurred. Source: Byte Magazine\n\n\n\nThe Intel FDIV bug is a cautionary tale for the potential impact of permanent faults on ML systems. In the context of ML, permanent faults in hardware components can lead to incorrect computations, affecting the accuracy and reliability of the models. For example, if an ML system relies on a processor with a faulty floating-point unit, similar to the Intel FDIV bug, it could introduce errors in the calculations performed during training or inference.\nThese errors can propagate through the model, leading to inaccurate predictions or skewed learning. In applications where ML is used for critical tasks, such as autonomous driving, medical diagnosis, or financial forecasting, the consequences of incorrect computations due to permanent faults can be severe.\nIt is crucial for ML practitioners to be aware of the potential impact of permanent faults and to incorporate fault-tolerant techniques, such as hardware redundancy, error detection and correction mechanisms, and robust algorithm design, to mitigate the risks associated with these faults. Additionally, thorough testing and validation of ML hardware components can help identify and address permanent faults before they impact the system’s performance and reliability.\n\n\nCauses of Permanent Faults\nPermanent faults can arise from several causes, including manufacturing defects and wear-out mechanisms. Manufacturing defects are inherent flaws introduced during the fabrication process of hardware components. These defects include improper etching, incorrect doping, or contamination, leading to non-functional or partially functional components.\nOn the other hand, wear-out mechanisms occur over time as the hardware components are subjected to prolonged use and stress. Factors such as electromigration, oxide breakdown, or thermal stress can cause gradual degradation of the components, eventually leading to permanent failures.\n\n\nMechanisms of Permanent Faults\nPermanent faults can manifest through various mechanisms, depending on the nature and location of the fault. Stuck-at faults (Seong et al. 2010) are common permanent faults where a signal or memory cell remains fixed at a particular value (either 0 or 1) regardless of the inputs, as illustrated in Figure 18.9.\n\nSeong, Nak Hee, Dong Hyuk Woo, Vijayalakshmi Srinivasan, Jude A. Rivers, and Hsien-Hsin S. Lee. 2010. “SAFER: Stuck-at-fault Error Recovery for Memories.” In 2010 43rd Annual IEEE/ACM International Symposium on Microarchitecture, 115–24. IEEE; IEEE. https://doi.org/10.1109/micro.2010.46.\n\n\n\n\n\n\nFigure 18.9: Stuck-at Fault Model in Digital Circuits. Source: Accendo Reliability\n\n\n\nStuck-at faults can occur in logic gates, memory cells, or interconnects, causing incorrect computations or data corruption. Another mechanism is device failures, where a component, such as a transistor or a memory cell, completely ceases to function. This can be due to manufacturing defects or severe wear-out. Bridging faults occur when two or more signal lines are unintentionally connected, causing short circuits or incorrect logic behavior.\nIn addition to stuck-at faults, there are several other types of permanent faults that can affect digital circuits that can impact an ML system. Delay faults can cause the propagation delay of a signal to exceed the specified limit, leading to timing violations. Interconnect faults, such as open faults (broken wires), resistive faults (increased resistance), or capacitive faults (increased capacitance), can cause signal integrity issues or timing violations. Memory cells can also suffer from various faults, including transition faults (inability to change state), coupling faults (interference between adjacent cells), and neighborhood pattern sensitive faults (faults that depend on the values of neighboring cells). Other permanent faults can occur in the power supply network or the clock distribution network, affecting the functionality and timing of the circuit.\n\n\nImpact on ML Systems\nPermanent faults can severely affect the behavior and reliability of computing systems. For example, a stuck-at-fault in a processor’s arithmetic logic unit (ALU) can cause incorrect computations, leading to erroneous results or system crashes. A permanent fault in a memory module, such as a stuck-at fault in a specific memory cell, can corrupt the stored data, causing data loss or program misbehavior. In storage devices, permanent faults like bad sectors or device failures can result in data inaccessibility or complete loss of stored information. Permanent interconnect faults can disrupt communication channels, causing data corruption or system hangs.\nPermanent faults can significantly affect ML systems during the training and inference phases. During training, permanent faults in processing units or memory can lead to incorrect computations, resulting in corrupted or suboptimal models (He et al. 2023). Furthermore, faults in storage devices can corrupt the training data or the stored model parameters, leading to data loss or model inconsistencies (He et al. 2023).\n\nZhang, Jeff Jun, Tianyu Gu, Kanad Basu, and Siddharth Garg. 2018. “Analyzing and Mitigating the Impact of Permanent Faults on a Systolic Array Based Neural Network Accelerator.” In 2018 IEEE 36th VLSI Test Symposium (VTS), 1–6. IEEE; IEEE. https://doi.org/10.1109/vts.2018.8368656.\nDuring inference, permanent faults can impact the reliability and correctness of ML predictions. Faults in the processing units can produce incorrect results or cause system failures, while faults in memory storing the model parameters can lead to corrupted or outdated models being used for inference (J. J. Zhang et al. 2018).\nTo mitigate the impact of permanent faults in ML systems, fault-tolerant techniques must be employed at both the hardware and software levels. Hardware redundancy, such as duplicating critical components or using error-correcting codes (Kim, Sullivan, and Erez 2015), can help detect and recover from permanent faults. Software techniques, such as checkpoint and restart mechanisms (Egwutuoha et al. 2013), can enable the system to recover from permanent faults by returning to a previously saved state. Regular monitoring, testing, and maintenance of ML systems can help identify and replace faulty components before they cause significant disruptions.\n\nKim, Jungrae, Michael Sullivan, and Mattan Erez. 2015. “Bamboo ECC: Strong, Safe, and Flexible Codes for Reliable Computer Memory.” In 2015 IEEE 21st International Symposium on High Performance Computer Architecture (HPCA), 101–12. IEEE; IEEE. https://doi.org/10.1109/hpca.2015.7056025.\n\nEgwutuoha, Ifeanyi P., David Levy, Bran Selic, and Shiping Chen. 2013. “A Survey of Fault Tolerance Mechanisms and Checkpoint/Restart Implementations for High Performance Computing Systems.” The Journal of Supercomputing 65 (3): 1302–26. https://doi.org/10.1007/s11227-013-0884-0.\nDesigning ML systems with fault tolerance in mind is crucial to ensure their reliability and robustness in the presence of permanent faults. This may involve incorporating redundancy, error detection and correction mechanisms, and fail-safe strategies into the system architecture. By proactively addressing the challenges posed by permanent faults, ML systems can maintain their integrity, accuracy, and trustworthiness, even in the face of hardware failures.\n\n\n\n18.3.3 Intermittent Faults\nIntermittent faults are hardware faults that occur sporadically and unpredictably in a system. An example is illustrated in Figure 18.10, where cracks in the material can introduce increased resistance in circuitry. These faults are particularly challenging to detect and diagnose because they appear and disappear intermittently, making it difficult to reproduce and isolate the root cause. Intermittent faults can lead to system instability, data corruption, and performance degradation.\n\n\n\n\n\n\nFigure 18.10: Increased resistance due to an intermittent fault – crack between copper bump and package solder. Source: Constantinescu\n\n\n\n\nDefinition and Characteristics\nIntermittent faults are characterized by their sporadic and non-deterministic nature. They occur irregularly and may appear and disappear spontaneously, with varying durations and frequencies. These faults do not consistently manifest every time the affected component is used, making them harder to detect than permanent faults. Intermittent faults can affect various hardware components, including processors, memory modules, storage devices, or interconnects. They can cause transient errors, data corruption, or unexpected system behavior.\nIntermittent faults can significantly impact the behavior and reliability of computing systems (Rashid, Pattabiraman, and Gopalakrishnan 2015). For example, an intermittent fault in a processor’s control logic can cause irregular program flow, leading to incorrect computations or system hangs. Intermittent faults in memory modules can corrupt data values, resulting in erroneous program execution or data inconsistencies. In storage devices, intermittent faults can cause read/write errors or data loss. Intermittent faults in communication channels can lead to data corruption, packet loss, or intermittent connectivity issues. These faults can cause system crashes, data integrity problems, or performance degradation, depending on the severity and frequency of the intermittent failures.\n\n———. 2015. “Characterizing the Impact of Intermittent Hardware Faults on Programs.” IEEE Trans. Reliab. 64 (1): 297–310. https://doi.org/10.1109/tr.2014.2363152.\n\n\nCauses of Intermittent Faults\nIntermittent faults can arise from several causes, both internal and external, to the hardware components (Constantinescu 2008). One common cause is aging and wear-out of the components. As electronic devices age, they become more susceptible to intermittent failures due to degradation mechanisms such as electromigration, oxide breakdown, or solder joint fatigue.\n\nConstantinescu, Cristian. 2008. “Intermittent Faults and Effects on Reliability of Integrated Circuits.” In 2008 Annual Reliability and Maintainability Symposium, 370–74. IEEE; IEEE. https://doi.org/10.1109/rams.2008.4925824.\nManufacturing defects or process variations can also introduce intermittent faults, where marginal or borderline components may exhibit sporadic failures under specific conditions, as shown in Figure 18.11.\n\n\n\n\n\n\nFigure 18.11: Residue induced intermittent fault in a DRAM chip. Source: Hynix Semiconductor\n\n\n\nEnvironmental factors, such as temperature fluctuations, humidity, or vibrations, can trigger intermittent faults by altering the electrical characteristics of the components. Loose or degraded connections, such as those in connectors or printed circuit boards, can cause intermittent faults.\n\n\nMechanisms of Intermittent Faults\nIntermittent faults can manifest through various mechanisms, depending on the underlying cause and the affected component. One mechanism is the intermittent open or short circuit, where a signal path or connection becomes temporarily disrupted or shorted, causing erratic behavior. Another mechanism is the intermittent delay fault (J. Zhang et al. 2018), where the timing of signals or propagation delays becomes inconsistent, leading to synchronization issues or incorrect computations. Intermittent faults can manifest as transient bit flips or soft errors in memory cells or registers, causing data corruption or incorrect program execution.\n\nZhang, Jeff, Kartheek Rangineni, Zahra Ghodsi, and Siddharth Garg. 2018. “ThUnderVolt: Enabling Aggressive Voltage Underscaling and Timing Error Resilience for Energy Efficient Deep Learning Accelerators.” In 2018 55th ACM/ESDA/IEEE Design Automation Conference (DAC), 1–6. IEEE. https://doi.org/10.1109/dac.2018.8465918.\n\n\nImpact on ML Systems\nIn the context of ML systems, intermittent faults can introduce significant challenges and impact the system’s reliability and performance. During the training phase, intermittent faults in processing units or memory can lead to inconsistencies in computations, resulting in incorrect or noisy gradients and weight updates. This can affect the convergence and accuracy of the training process, leading to suboptimal or unstable models. Intermittent data storage or retrieval faults can corrupt the training data, introducing noise or errors that degrade the quality of the learned models (He et al. 2023).\n\nHe, Yi, Mike Hutton, Steven Chan, Robert De Gruijl, Rama Govindaraju, Nishant Patil, and Yanjing Li. 2023. “Understanding and Mitigating Hardware Failures in Deep Learning Training Systems.” In Proceedings of the 50th Annual International Symposium on Computer Architecture, 1–16. IEEE; ACM. https://doi.org/10.1145/3579371.3589105.\nDuring the inference phase, intermittent faults can impact the reliability and consistency of ML predictions. Faults in the processing units or memory can cause incorrect computations or data corruption, leading to erroneous or inconsistent predictions. Intermittent faults in the data pipeline can introduce noise or errors in the input data, affecting the accuracy and robustness of the predictions. In safety-critical applications, such as autonomous vehicles or medical diagnosis systems, intermittent faults can have severe consequences, leading to incorrect decisions or actions that compromise safety and reliability.\nMitigating the impact of intermittent faults in ML systems requires a multifaceted approach (Rashid, Pattabiraman, and Gopalakrishnan 2012). At the hardware level, techniques such as robust design practices, component selection, and environmental control can help reduce the occurrence of intermittent faults. Redundancy and error correction mechanisms can be employed to detect and recover from intermittent failures. At the software level, runtime monitoring, anomaly detection, and fault-tolerant techniques can be incorporated into the ML pipeline. This may include techniques such as data validation, outlier detection, model ensembling, or runtime model adaptation to handle intermittent faults gracefully.\n\nRashid, Layali, Karthik Pattabiraman, and Sathish Gopalakrishnan. 2012. “Intermittent Hardware Errors Recovery: Modeling and Evaluation.” In 2012 Ninth International Conference on Quantitative Evaluation of Systems, 220–29. IEEE; IEEE. https://doi.org/10.1109/qest.2012.37.\nDesigning ML systems resilient to intermittent faults is crucial to ensuring their reliability and robustness. This involves incorporating fault-tolerant techniques, runtime monitoring, and adaptive mechanisms into the system architecture. By proactively addressing the challenges of intermittent faults, ML systems can maintain their accuracy, consistency, and trustworthiness, even in sporadic hardware failures. Regular testing, monitoring, and maintenance of ML systems can help identify and mitigate intermittent faults before they cause significant disruptions or performance degradation.\n\n\n\n18.3.4 Detection and Mitigation\nThis section explores various fault detection techniques, including hardware-level and software-level approaches, and discusses effective mitigation strategies to enhance the resilience of ML systems. Additionally, we will look into resilient ML system design considerations, present case studies and examples, and highlight future research directions in fault-tolerant ML systems.\n\nFault Detection Techniques\nFault detection techniques are important for identifying and localizing hardware faults in ML systems. These techniques can be broadly categorized into hardware-level and software-level approaches, each offering unique capabilities and advantages.\n\nHardware-level fault detection\nHardware-level fault detection techniques are implemented at the physical level of the system and aim to identify faults in the underlying hardware components. There are several hardware techniques, but broadly, we can bucket these different mechanisms into the following categories.\nBuilt-in self-test (BIST) mechanisms: BIST is a powerful technique for detecting faults in hardware components (Bushnell and Agrawal 2002). It involves incorporating additional hardware circuitry into the system for self-testing and fault detection. BIST can be applied to various components, such as processors, memory modules, or application-specific integrated circuits (ASICs). For example, BIST can be implemented in a processor using scan chains, which are dedicated paths that allow access to internal registers and logic for testing purposes.\n\nBushnell, Michael L, and Vishwani D Agrawal. 2002. “Built-in Self-Test.” Essentials of Electronic Testing for Digital, Memory and Mixed-Signal VLSI Circuits, 489–548.\nDuring the BIST process, predefined test patterns are applied to the processor’s internal circuitry, and the responses are compared against expected values. Any discrepancies indicate the presence of faults. Intel’s Xeon processors, for instance, include BIST mechanisms to test the CPU cores, cache memory, and other critical components during system startup.\nError detection codes: Error detection codes are widely used to detect data storage and transmission errors (Hamming 1950). These codes add redundant bits to the original data, allowing the detection of bit errors. Example: Parity checks are a simple form of error detection code shown in Figure 18.12. In a single-bit parity scheme, an extra bit is appended to each data word, making the number of 1s in the word even (even parity) or odd (odd parity).\n\nHamming, R. W. 1950. “Error Detecting and Error Correcting Codes.” Bell Syst. Tech. J. 29 (2): 147–60. https://doi.org/10.1002/j.1538-7305.1950.tb00463.x.\n\n\n\n\n\n\nFigure 18.12: Parity bit example. Source: Computer Hope\n\n\n\nWhen reading the data, the parity is checked, and if it doesn’t match the expected value, an error is detected. More advanced error detection codes, such as cyclic redundancy checks (CRC), calculate a checksum based on the data and append it to the message. The checksum is recalculated at the receiving end and compared with the transmitted checksum to detect errors. Error-correcting code (ECC) memory modules, commonly used in servers and critical systems, employ advanced error detection and correction codes to detect and correct single-bit or multi-bit errors in memory.\nHardware redundancy and voting mechanisms: Hardware redundancy involves duplicating critical components and comparing their outputs to detect and mask faults (Sheaffer, Luebke, and Skadron 2007). Voting mechanisms, such as double modular redundancy (DMR) or triple modular redundancy (TMR), employ multiple instances of a component and compare their outputs to identify and mask faulty behavior (Arifeen, Hassan, and Lee 2020).\n\nSheaffer, Jeremy W, David P Luebke, and Kevin Skadron. 2007. “A Hardware Redundancy and Recovery Mechanism for Reliable Scientific Computation on Graphics Processors.” In Graphics Hardware, 2007:55–64. Citeseer.\n\nArifeen, Tooba, Abdus Sami Hassan, and Jeong-A Lee. 2020. “Approximate Triple Modular Redundancy: A Survey.” #IEEE_O_ACC# 8: 139851–67. https://doi.org/10.1109/access.2020.3012673.\n\nYeh, Y. C. 1996. “Triple-Triple Redundant 777 Primary Flight Computer.” In 1996 IEEE Aerospace Applications Conference. Proceedings, 1:293–307. IEEE; IEEE. https://doi.org/10.1109/aero.1996.495891.\nIn a DMR or TMR system, two or three identical instances of a hardware component, such as a processor or a sensor, perform the same computation in parallel. The outputs of these instances are fed into a voting circuit, which compares the results and selects the majority value as the final output. If one of the instances produces an incorrect result due to a fault, the voting mechanism masks the error and maintains the correct output. TMR is commonly used in aerospace and aviation systems, where high reliability is critical. For instance, the Boeing 777 aircraft employs TMR in its primary flight computer system to ensure the availability and correctness of flight control functions (Yeh 1996).\nTesla’s self-driving computers, on the other hand, employ a DMR architecture to ensure the safety and reliability of critical functions such as perception, decision-making, and vehicle control, as shown in Figure 18.13. In Tesla’s implementation, two identical hardware units, often called “redundant computers” or “redundant control units,” perform the same computations in parallel. Each unit independently processes sensor data, executes algorithms, and generates control commands for the vehicle’s actuators, such as steering, acceleration, and braking (Bannon et al. 2019).\n\nBannon, Pete, Ganesh Venkataramanan, Debjit Das Sarma, and Emil Talpes. 2019. “Computer and Redundancy Solution for the Full Self-Driving Computer.” In 2019 IEEE Hot Chips 31 Symposium (HCS), 1–22. IEEE Computer Society; IEEE. https://doi.org/10.1109/hotchips.2019.8875645.\n\n\n\n\n\n\nFigure 18.13: Tesla full self-driving computer with dual redundant SoCs. Source: Tesla\n\n\n\nThe outputs of these two redundant units are continuously compared to detect any discrepancies or faults. If the outputs match, the system assumes that both units function correctly, and the control commands are sent to the vehicle’s actuators. However, if there is a mismatch between the outputs, the system identifies a potential fault in one of the units and takes appropriate action to ensure safe operation.\nDMR in Tesla’s self-driving computer provides an extra safety and fault tolerance layer. By having two independent units performing the same computations, the system can detect and mitigate faults that may occur in one of the units. This redundancy helps prevent single points of failure and ensures that critical functions remain operational despite hardware faults.\nThe system may employ additional mechanisms to determine which unit is faulty in a mismatch. This can involve using diagnostic algorithms, comparing the outputs with data from other sensors or subsystems, or analyzing the consistency of the outputs over time. Once the faulty unit is identified, the system can isolate it and continue operating using the output from the non-faulty unit.\nTesla also incorporates redundancy mechanisms beyond DMR. For example, they use redundant power supplies, steering and braking systems, and diverse sensor suites (e.g., cameras, radar, and ultrasonic sensors) to provide multiple layers of fault tolerance. These redundancies collectively contribute to the overall safety and reliability of the self-driving system.\nIt’s important to note that while DMR provides fault detection and some level of fault tolerance, TMR may provide a different level of fault masking. In DMR, if both units experience simultaneous faults or the fault affects the comparison mechanism, the system may be unable to identify the fault. Therefore, Tesla’s SDCs rely on a combination of DMR and other redundancy mechanisms to achieve a high level of fault tolerance.\nThe use of DMR in Tesla’s self-driving computer highlights the importance of hardware redundancy in safety-critical applications. By employing redundant computing units and comparing their outputs, the system can detect and mitigate faults, enhancing the overall safety and reliability of the self-driving functionality.\nAnother approach to hardware redundancy is the use of hot spares, as employed by Google in its data centers to address SDC during ML training. Unlike DMR and TMR, which rely on parallel processing and voting mechanisms to detect and mask faults, hot spares provide fault tolerance by maintaining backup hardware units that can seamlessly take over computations when a fault is detected. As illustrated in Figure 18.14, during normal ML training, multiple synchronous training workers process data in parallel. However, if a worker becomes defective and causes SDC, an SDC checker automatically identifies the issues. Upon detecting the SDC, the SDC checker moves the training to a hot spare and sends the defective machine for repair. This redundancy safeguards the continuity and reliability of ML training, effectively minimizing downtime and preserving data integrity.\n\n\n\n\n\n\nFigure 18.14: Google employs hot spare cores to transparently handle SDCs in the data center. Source: Jeff Dean, MLSys 2024 Keynote (Google)\n\n\n\nWatchdog timers: Watchdog timers are hardware components that monitor the execution of critical tasks or processes (Pont and Ong 2002). They are commonly used to detect and recover from software or hardware faults that cause a system to become unresponsive or stuck in an infinite loop. In an embedded system, a watchdog timer can be configured to monitor the execution of the main control loop, as illustrated in Figure 18.15. The software periodically resets the watchdog timer to indicate that it functions correctly. Suppose the software fails to reset the timer within a specified time limit (timeout period). In that case, the watchdog timer assumes that the system has encountered a fault and triggers a predefined recovery action, such as resetting the system or switching to a backup component. Watchdog timers are widely used in automotive electronics, industrial control systems, and other safety-critical applications to ensure the timely detection and recovery from faults.\n\nPont, Michael J, and Royan HL Ong. 2002. “Using Watchdog Timers to Improve the Reliability of Single-Processor Embedded Systems: Seven New Patterns and a Case Study.” In Proceedings of the First Nordic Conference on Pattern Languages of Programs, 159–200. Citeseer.\n\n\n\n\n\n\nFigure 18.15: Watchdog timer example in detecting MCU faults. Source: Ablic\n\n\n\n\n\nSoftware-level fault detection\nSoftware-level fault detection techniques rely on software algorithms and monitoring mechanisms to identify system faults. These techniques can be implemented at various levels of the software stack, including the operating system, middleware, or application level.\nRuntime monitoring and anomaly detection: Runtime monitoring involves continuously observing the behavior of the system and its components during execution (Francalanza et al. 2017). It helps detect anomalies, errors, or unexpected behavior that may indicate the presence of faults. For example, consider an ML-based image classification system deployed in a self-driving car. Runtime monitoring can be implemented to track the classification model’s performance and behavior (Mahmoud et al. 2021).\n\nFrancalanza, Adrian, Luca Aceto, Antonis Achilleos, Duncan Paul Attard, Ian Cassar, Dario Della Monica, and Anna Ingólfsdóttir. 2017. “A Foundation for Runtime Monitoring.” In International Conference on Runtime Verification, 8–29. Springer.\n\nMahmoud, Abdulrahman, Siva Kumar Sastry Hari, Christopher W. Fletcher, Sarita V. Adve, Charbel Sakr, Naresh Shanbhag, Pavlo Molchanov, Michael B. Sullivan, Timothy Tsai, and Stephen W. Keckler. 2021. “Optimizing Selective Protection for CNN Resilience.” In 2021 IEEE 32nd International Symposium on Software Reliability Engineering (ISSRE), 127–38. IEEE. https://doi.org/10.1109/issre52982.2021.00025.\n\nChandola, Varun, Arindam Banerjee, and Vipin Kumar. 2009. “Anomaly Detection: A Survey.” ACM Comput. Surv. 41 (3): 1–58. https://doi.org/10.1145/1541880.1541882.\nAnomaly detection algorithms can be applied to the model’s predictions or intermediate layer activations, such as statistical outlier detection or machine learning-based approaches (e.g., One-Class SVM or Autoencoders) (Chandola, Banerjee, and Kumar 2009). Figure 18.16 shows example of anomaly detection. Suppose the monitoring system detects a significant deviation from the expected patterns, such as a sudden drop in classification accuracy or out-of-distribution samples. In that case, it can raise an alert indicating a potential fault in the model or the input data pipeline. This early detection allows for timely intervention and fault mitigation strategies to be applied.\n\n\n\n\n\n\nFigure 18.16: An example of anomaly detection using an SVM to analyze system logs and identify anomalies. Advanced methods, including unsupervised approaches, have been developed to enhance anomaly detection. Source: Google\n\n\n\nConsistency checks and data validation: Consistency checks and data validation techniques ensure data integrity and correctness at different processing stages in an ML system (Lindholm et al. 2019). These checks help detect data corruption, inconsistencies, or errors that may propagate and affect the system’s behavior. Example: In a distributed ML system where multiple nodes collaborate to train a model, consistency checks can be implemented to validate the integrity of the shared model parameters. Each node can compute a checksum or hash of the model parameters before and after the training iteration, as shown in Figure 18.16. Any inconsistencies or data corruption can be detected by comparing the checksums across nodes. Additionally, range checks can be applied to the input data and model outputs to ensure they fall within expected bounds. For instance, if an autonomous vehicle’s perception system detects an object with unrealistic dimensions or velocities, it can indicate a fault in the sensor data or the perception algorithms (Wan et al. 2023).\n\nLindholm, Andreas, Dave Zachariah, Petre Stoica, and Thomas B. Schon. 2019. “Data Consistency Approach to Model Validation.” #IEEE_O_ACC# 7: 59788–96. https://doi.org/10.1109/access.2019.2915109.\n\nWan, Zishen, Yiming Gan, Bo Yu, S Liu, A Raychowdhury, and Y Zhu. 2023. “Vpp: The Vulnerability-Proportional Protection Paradigm Towards Reliable Autonomous Machines.” In Proceedings of the 5th International Workshop on Domain Specific System Architecture (DOSSA), 1–6.\n\nKawazoe Aguilera, Marcos, Wei Chen, and Sam Toueg. 1997. “Heartbeat: A Timeout-Free Failure Detector for Quiescent Reliable Communication.” In Distributed Algorithms: 11th International Workshop, WDAG’97 Saarbrücken, Germany, September 2426, 1997 Proceedings 11, 126–40. Springer.\nHeartbeat and timeout mechanisms: Heartbeat mechanisms and timeouts are commonly used to detect faults in distributed systems and ensure the liveness and responsiveness of components (Kawazoe Aguilera, Chen, and Toueg 1997). These are quite similar to the watchdog timers found in hardware. For example, in a distributed ML system, where multiple nodes collaborate to perform tasks such as data preprocessing, model training, or inference, heartbeat mechanisms can be implemented to monitor the health and availability of each node. Each node periodically sends a heartbeat message to a central coordinator or its peer nodes, indicating its status and availability. Suppose a node fails to send a heartbeat within a specified timeout period, as shown in Figure 18.17. In that case, it is considered faulty, and appropriate actions can be taken, such as redistributing the workload or initiating a failover mechanism. Timeouts can also be used to detect and handle hanging or unresponsive components. For example, if a data loading process exceeds a predefined timeout threshold, it may indicate a fault in the data pipeline, and the system can take corrective measures.\n\n\n\n\n\n\nFigure 18.17: Heartbeat messages in distributed systems. Source: GeeksforGeeks\n\n\n\n\nSoftware-implemented fault tolerance (SIFT) techniques: SIFT techniques introduce redundancy and fault detection mechanisms at the software level to improve the reliability and fault tolerance of the system (Reis et al. 2005). Example: N-version programming is a SIFT technique where multiple functionally equivalent software component versions are developed independently by different teams. This can be applied to critical components such as the model inference engine in an ML system. Multiple versions of the inference engine can be executed in parallel, and their outputs can be compared for consistency. It is considered the correct result if most versions produce the same output. If there is a discrepancy, it indicates a potential fault in one or more versions, and appropriate error-handling mechanisms can be triggered. Another example is using software-based error correction codes, such as Reed-Solomon codes (Plank 1997), to detect and correct errors in data storage or transmission, as shown in Figure 18.18. These codes add redundancy to the data, enabling detecting and correcting certain errors and enhancing the system’s fault tolerance.\n\nReis, G. A., J. Chang, N. Vachharajani, R. Rangan, and D. I. August. 2005. “SWIFT: Software Implemented Fault Tolerance.” In International Symposium on Code Generation and Optimization, 243–54. IEEE; IEEE. https://doi.org/10.1109/cgo.2005.34.\n\nPlank, James S. 1997. “A Tutorial on ReedSolomon Coding for Fault-Tolerance in RAID-Like Systems.” Software: Practice and Experience 27 (9): 995–1012.\n\n\n\n\n\n\nFigure 18.18: n-bits representation of the Reed-Solomon codes. Source: GeeksforGeeks\n\n\n\n\n\n\n\n\n\nExercise 18.1: Anomaly Detection\n\n\n\n\n\nIn this Colab, play the role of an AI fault detective! You’ll build an autoencoder-based anomaly detector to pinpoint errors in heart health data. Learn how to identify malfunctions in ML systems, a vital skill for creating dependable AI. We’ll use Keras Tuner to fine-tune your autoencoder for top-notch fault detection. This experience directly links to the Robust AI chapter, demonstrating the importance of fault detection in real-world applications like healthcare and autonomous systems. Get ready to strengthen the reliability of your AI creations!\n\n\n\n\n\n\n\n\n18.3.5 Summary\nTable 18.1 provides an extensive comparative analysis of transient, permanent, and intermittent faults. It outlines the primary characteristics or dimensions that distinguish these fault types. Here, we summarize the relevant dimensions we examined and explore the nuances that differentiate transient, permanent, and intermittent faults in greater detail.\n\n\n\nTable 18.1: Comparison of transient, permanent, and intermittent faults.\n\n\n\n\n\n\n\n\n\n\n\nDimension\nTransient Faults\nPermanent Faults\nIntermittent Faults\n\n\n\n\nDuration\nShort-lived, temporary\nPersistent, remains until repair or replacement\nSporadic, appears and disappears intermittently\n\n\nPersistence\nDisappears after the fault condition passes\nConsistently present until addressed\nRecurs irregularly, not always present\n\n\nCauses\nExternal factors (e.g., electromagnetic interference cosmic rays)\nHardware defects, physical damage, wear-out\nUnstable hardware conditions, loose connections, aging components\n\n\nManifestation\nBit flips, glitches, temporary data corruption\nStuck-at faults, broken components, complete device failures\nOccasional bit flips, intermittent signal issues, sporadic malfunctions\n\n\nImpact on ML Systems\nIntroduces temporary errors or noise in computations\nCauses consistent errors or failures, affecting reliability\nLeads to sporadic and unpredictable errors, challenging to diagnose and mitigate\n\n\nDetection\nError detection codes, comparison with expected values\nBuilt-in self-tests, error detection codes, consistency checks\nMonitoring for anomalies, analyzing error patterns and correlations\n\n\nMitigation\nError correction codes, redundancy, checkpoint and restart\nHardware repair or replacement, component redundancy, failover mechanisms\nRobust design, environmental control, runtime monitoring, fault-tolerant techniques",
    "crumbs": [
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Robust AI</span>"
    ]
  },
  {
    "objectID": "contents/core/robust_ai/robust_ai.html#ml-model-robustness",
    "href": "contents/core/robust_ai/robust_ai.html#ml-model-robustness",
    "title": "18  Robust AI",
    "section": "18.4 ML Model Robustness",
    "text": "18.4 ML Model Robustness\n\n18.4.1 Adversarial Attacks\nWe first introduced adversarial attacks in Section 15.4.3, where we discussed how slight changes to input data can trick a model into making incorrect predictions. These attacks often involve adding small, carefully designed perturbations to input data, which can cause the model to misclassify it, as shown in Figure 18.19. In this section, we will look at the different types of adversarial attacks and their impact on machine learning models. Understanding these attacks highlights why it is important to build models that are robust and able to handle these kinds of challenges.\n\n\n\n\n\n\nFigure 18.19: A small adversarial noise added to the original image can make the neural network classify the image as a Guacamole instead of an Egyptian cat. Source: Sutanto\n\n\n\n\nMechanisms of Adversarial Attacks\nGradient-based Attacks\nOne prominent category of adversarial attacks is gradient-based attacks. These attacks leverage the gradients of the ML model’s loss function to craft adversarial examples. The Fast Gradient Sign Method (FGSM) is a well-known technique in this category. FGSM perturbs the input data by adding small noise in the gradient direction, aiming to maximize the model’s prediction error. FGSM can quickly generate adversarial examples, as shown in Figure 18.20, by taking a single step in the gradient direction.\n\n\n\n\n\n\nFigure 18.20: Gradient-Based Attacks. Source: Ivezic\n\n\n\nAnother variant, the Projected Gradient Descent (PGD) attack, extends FGSM by iteratively applying the gradient update step, allowing for more refined and powerful adversarial examples. The Jacobian-based Saliency Map Attack (JSMA) is another gradient-based approach that identifies the most influential input features and perturbs them to create adversarial examples.\nOptimization-based Attacks\nThese attacks formulate the generation of adversarial examples as an optimization problem. The Carlini and Wagner (C&W) attack is a prominent example in this category. It finds the smallest perturbation that can cause misclassification while maintaining the perceptual similarity to the original input. The C&W attack employs an iterative optimization process to minimize the perturbation while maximizing the model’s prediction error.\nAnother optimization-based approach is the Elastic Net Attack to DNNs (EAD), which incorporates elastic net regularization to generate adversarial examples with sparse perturbations.\nTransfer-based Attacks\nTransfer-based attacks exploit the transferability property of adversarial examples. Transferability refers to the phenomenon where adversarial examples crafted for one ML model can often fool other models, even if they have different architectures or were trained on different datasets. This enables attackers to generate adversarial examples using a surrogate model and then transfer them to the target model without requiring direct access to its parameters or gradients. Transfer-based attacks highlight the generalization of adversarial vulnerabilities across different models and the potential for black-box attacks.\nPhysical-world Attacks\nPhysical-world attacks bring adversarial examples into the realm of real-world scenarios. These attacks involve creating physical objects or manipulations that can deceive ML models when captured by sensors or cameras. Adversarial patches, for example, are small, carefully designed patches that can be placed on objects to fool object detection or classification models. When attached to real-world objects, these patches can cause models to misclassify or fail to detect the objects accurately. Adversarial objects, such as 3D-printed sculptures or modified road signs, can also be crafted to deceive ML systems in physical environments.\nSummary\nTable 18.2 a concise overview of the different categories of adversarial attacks, including gradient-based attacks (FGSM, PGD, JSMA), optimization-based attacks (C&W, EAD), transfer-based attacks, and physical-world attacks (adversarial patches and objects). Each attack is briefly described, highlighting its key characteristics and mechanisms.\n\n\n\nTable 18.2: Different attack types on ML models.\n\n\n\n\n\n\n\n\n\n\nAttack Category\nAttack Name\nDescription\n\n\n\n\nGradient-based\nFast Gradient Sign Method (FGSM) Projected Gradient Descent (PGD) Jacobian-based Saliency Map Attack (JSMA)\nPerturbs input data by adding small noise in the gradient direction to maximize prediction error. Extends FGSM by iteratively applying the gradient update step for more refined adversarial examples. Identifies influential input features and perturbs them to create adversarial examples.\n\n\nOptimization-based\nCarlini and Wagner (C&W) Attack Elastic Net Attack to DNNs (EAD)\nFinds the smallest perturbation that causes misclassification while maintaining perceptual similarity. Incorporates elastic net regularization to generate adversarial examples with sparse perturbations.\n\n\nTransfer-based\nTransferability-based Attacks\nExploits the transferability of adversarial examples across different models, enabling black-box attacks.\n\n\nPhysical-world\nAdversarial Patches Adversarial Objects\nSmall, carefully designed patches placed on objects to fool object detection or classification models. Physical objects (e.g., 3D-printed sculptures, modified road signs) crafted to deceive ML systems in real-world scenarios.\n\n\n\n\n\n\nThe mechanisms of adversarial attacks reveal the intricate interplay between the ML model’s decision boundaries, the input data, and the attacker’s objectives. By carefully manipulating the input data, attackers can exploit the model’s sensitivities and blind spots, leading to incorrect predictions. The success of adversarial attacks highlights the need for a deeper understanding of ML models’ robustness and generalization properties.\nDefending against adversarial attacks requires a multifaceted approach. Adversarial training is one common defense strategy in which models are trained on adversarial examples to improve robustness. Exposing the model to adversarial examples during training teaches it to classify them correctly and become more resilient to attacks. Defensive distillation, input preprocessing, and ensemble methods are other techniques that can help mitigate the impact of adversarial attacks.\nAs adversarial machine learning evolves, researchers explore new attack mechanisms and develop more sophisticated defenses. The arms race between attackers and defenders drives the need for constant innovation and vigilance in securing ML systems against adversarial threats. Understanding the mechanisms of adversarial attacks is crucial for developing robust and reliable ML models that can withstand the ever-evolving landscape of adversarial examples.\n\n\nImpact on ML Systems\nAdversarial attacks on machine learning systems have emerged as a significant concern in recent years, highlighting the potential vulnerabilities and risks associated with the widespread adoption of ML technologies. These attacks involve carefully crafted perturbations to input data that can deceive or mislead ML models, leading to incorrect predictions or misclassifications, as shown in Figure 18.21. The impact of adversarial attacks on ML systems is far-reaching and can have serious consequences in various domains.\n\n\n\n\n\n\nFigure 18.21: Adversarial example generation applied to GoogLeNet (Szegedy et al., 2014a) on ImageNet. Source: Goodfellow\n\n\n\nOne striking example of the impact of adversarial attacks was demonstrated by researchers in 2017. They experimented with small black and white stickers on stop signs (Eykholt et al. 2017). To the human eye, these stickers did not obscure the sign or prevent its interpretability. However, when images of the sticker-modified stop signs were fed into standard traffic sign classification ML models, a shocking result emerged. The models misclassified the stop signs as speed limit signs over 85% of the time.\n\nEykholt, Kevin, Ivan Evtimov, Earlence Fernandes, Bo Li, Amir Rahmati, Chaowei Xiao, Atul Prakash, Tadayoshi Kohno, and Dawn Song. 2017. “Robust Physical-World Attacks on Deep Learning Models.” ArXiv Preprint abs/1707.08945. https://arxiv.org/abs/1707.08945.\nThis demonstration shed light on the alarming potential of simple adversarial stickers to trick ML systems into misreading critical road signs. The implications of such attacks in the real world are significant, particularly in the context of autonomous vehicles. If deployed on actual roads, these adversarial stickers could cause self-driving cars to misinterpret stop signs as speed limits, leading to dangerous situations, as shown in Figure 18.22. Researchers warned that this could result in rolling stops or unintended acceleration into intersections, endangering public safety.\n\n\n\n\n\n\nFigure 18.22: Graffiti on a stop sign tricked a self-driving car into thinking it was a 45 mph speed limit sign. Source: Eykholt\n\n\n\nThe case study of the adversarial stickers on stop signs provides a concrete illustration of how adversarial examples exploit how ML models recognize patterns. By subtly manipulating the input data in ways that are invisible to humans, attackers can induce incorrect predictions and create serious risks, especially in safety-critical applications like autonomous vehicles. The attack’s simplicity highlights the vulnerability of ML models to even minor changes in the input, emphasizing the need for robust defenses against such threats.\nThe impact of adversarial attacks extends beyond the degradation of model performance. These attacks raise significant security and safety concerns, particularly in domains where ML models are relied upon for critical decision-making. In healthcare applications, adversarial attacks on medical imaging models could lead to misdiagnosis or incorrect treatment recommendations, jeopardizing patient well-being (M.-J. Tsai, Lin, and Lee 2023). In financial systems, adversarial attacks could enable fraud or manipulation of trading algorithms, resulting in substantial economic losses.\n\nTsai, Min-Jen, Ping-Yi Lin, and Ming-En Lee. 2023. “Adversarial Attacks on Medical Image Classification.” Cancers 15 (17): 4228. https://doi.org/10.3390/cancers15174228.\n\nFursov, Ivan, Matvey Morozov, Nina Kaploukhaya, Elizaveta Kovtun, Rodrigo Rivera-Castro, Gleb Gusev, Dmitry Babaev, Ivan Kireev, Alexey Zaytsev, and Evgeny Burnaev. 2021. “Adversarial Attacks on Deep Models for Financial Transaction Records.” In Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery &Amp; Data Mining, 2868–78. ACM. https://doi.org/10.1145/3447548.3467145.\nMoreover, adversarial vulnerabilities undermine the trustworthiness and interpretability of ML models. If carefully crafted perturbations can easily fool models, confidence in their predictions and decisions erodes. Adversarial examples expose the models’ reliance on superficial patterns and the inability to capture the true underlying concepts, challenging the reliability of ML systems (Fursov et al. 2021).\nDefending against adversarial attacks often requires additional computational resources and can impact the overall system performance. Techniques like adversarial training, where models are trained on adversarial examples to improve robustness, can significantly increase training time and computational requirements (Bai et al. 2021). Runtime detection and mitigation mechanisms, such as input preprocessing (Addepalli et al. 2020) or prediction consistency checks, introduce latency and affect the real-time performance of ML systems.\n\nBai, Tao, Jinqi Luo, Jun Zhao, Bihan Wen, and Qian Wang. 2021. “Recent Advances in Adversarial Training for Adversarial Robustness.” arXiv Preprint arXiv:2102.01356.\n\nAddepalli, Sravanti, B. S. Vivek, Arya Baburaj, Gaurang Sriramanan, and R. Venkatesh Babu. 2020. “Towards Achieving Adversarial Robustness by Enforcing Feature Consistency Across Bit Planes.” In 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 1020–29. IEEE. https://doi.org/10.1109/cvpr42600.2020.00110.\nThe presence of adversarial vulnerabilities also complicates the deployment and maintenance of ML systems. System designers and operators must consider the potential for adversarial attacks and incorporate appropriate defenses and monitoring mechanisms. Regular updates and retraining of models become necessary to adapt to new adversarial techniques and maintain system security and performance over time.\nThe impact of adversarial attacks on ML systems is significant and multifaceted. These attacks expose ML models’ vulnerabilities, from degrading model performance and raising security and safety concerns to challenging model trustworthiness and interpretability. Developers and researchers must prioritize the development of robust defenses and countermeasures to mitigate the risks posed by adversarial attacks. By addressing these challenges, we can build more secure, reliable, and trustworthy ML systems that can withstand the ever-evolving landscape of adversarial threats.\n\n\n\n\n\n\nExercise 18.2: Adversarial Attacks\n\n\n\n\n\nGet ready to become an AI adversary! In this Colab, you’ll become a white-box hacker, learning to craft attacks that deceive image classification models. We’ll focus on the Fast Gradient Sign Method (FGSM), where you’ll weaponize a model’s gradients against it! You’ll deliberately distort images with tiny perturbations, observing how they increasingly fool the AI more intensely. This hands-on exercise highlights the importance of building secure AI – a critical skill as AI integrates into cars and healthcare. The Colab directly ties into the Robust AI chapter of your book, moving adversarial attacks from theory into your own hands-on experience.\n\nThink you can outsmart an AI? In this Colab, learn how to trick image classification models with adversarial attacks. We’ll use methods like FGSM to change images and subtly fool the AI. Discover how to design deceptive image patches and witness the surprising vulnerability of these powerful models. This is crucial knowledge for building truly robust AI systems!\n\n\n\n\n\n\n\n18.4.2 Data Poisoning\n\nDefinition and Characteristics\nData poisoning is an attack where the training data is tampered with, leading to a compromised model (Biggio, Nelson, and Laskov 2012), as shown in Figure 18.23. Attackers can modify existing training examples, insert new malicious data points, or influence the data collection process. The poisoned data is labeled in such a way as to skew the model’s learned behavior. This can be particularly damaging in applications where ML models make automated decisions based on learned patterns. Beyond training sets, poisoning tests, and validation data can allow adversaries to boost reported model performance artificially.\n\nBiggio, Battista, Blaine Nelson, and Pavel Laskov. 2012. “Poisoning Attacks Against Support Vector Machines.” In Proceedings of the 29th International Conference on Machine Learning, ICML 2012, Edinburgh, Scotland, UK, June 26 - July 1, 2012. icml.cc / Omnipress. http://icml.cc/2012/papers/880.pdf.\n\n\n\n\n\n\nFigure 18.23: Samples of dirty-label poison data regarding mismatched text/image pairs. Source: Shan\n\n\n\nThe process usually involves the following steps:\n\nInjection: The attacker adds incorrect or misleading examples into the training set. These examples are often designed to look normal to cursory inspection but have been carefully crafted to disrupt the learning process.\nTraining: The ML model trains on this manipulated dataset and develops skewed understandings of the data patterns.\nDeployment: Once the model is deployed, the corrupted training leads to flawed decision-making or predictable vulnerabilities the attacker can exploit.\n\nThe impact of data poisoning extends beyond classification errors or accuracy drops. In critical applications like healthcare, such alterations can lead to significant trust and safety issues (Marulli, Marrone, and Verde 2022). Later, we will discuss a few case studies of these issues.\n\nMarulli, Fiammetta, Stefano Marrone, and Laura Verde. 2022. “Sensitivity of Machine Learning Approaches to Fake and Untrusted Data in Healthcare Domain.” Journal of Sensor and Actuator Networks 11 (2): 21. https://doi.org/10.3390/jsan11020021.\n\nOprea, Alina, Anoop Singhal, and Apostol Vassilev. 2022. “Poisoning Attacks Against Machine Learning: Can Machine Learning Be Trustworthy?” Computer 55 (11): 94–99. https://doi.org/10.1109/mc.2022.3190787.\nThere are four main categories of data poisoning (Oprea, Singhal, and Vassilev 2022):\n\nPoisoning Availability: Involves poisoning a large percentage of the training data. These attacks aim to compromise the overall functionality of a model. They cause it to misclassify most testing samples, rendering the model unusable for practical applications. An example is label flipping, where labels of a specific, targeted class are replaced with labels from a different one.\nTargeted Poisoning: In contrast to availability attacks, targeted attacks aim to poison a small number of the testing samples. So, the effect is localized to a limited number of classes, while the model maintains the same original level of accuracy for the majority of the classes. The targeted nature of the attack requires the attacker to possess knowledge of the model’s classes, making detecting these attacks more challenging.\nBackdoor Poisoning: In these attacks, an adversary targets specific patterns in the data. The attacker introduces a backdoor (a malicious, hidden trigger or pattern) into the training data, such as manipulating certain features in structured data or manipulating a pattern of pixels at a fixed position. This causes the model to associate the malicious pattern with specific labels. As a result, when the model encounters test samples that contain a malicious pattern, it makes false predictions.\nSubpopulation Poisoning: Attackers selectively choose to compromise a subset of the testing samples while maintaining accuracy on the rest of the samples. You can think of these attacks as a combination of availability and targeted attacks: performing availability attacks (performance degradation) within the scope of a targeted subset. Although subpopulation attacks may seem very similar to targeted attacks, the two have clear differences:\n\nThe characteristics of data poisoning include:\nSubtle and hard-to-detect manipulations of training data: Data poisoning often involves subtle manipulations of the training data that are carefully crafted to be difficult to detect through casual inspection. Attackers employ sophisticated techniques to ensure that the poisoned samples blend seamlessly with the legitimate data, making them easier to identify with thorough analysis. These manipulations can target specific features or attributes of the data, such as altering numerical values, modifying categorical labels, or introducing carefully designed patterns. The goal is to influence the model’s learning process while evading detection, allowing the poisoned data to subtly corrupt the model’s behavior.\nCan be performed by insiders or external attackers: Data poisoning attacks can be carried out by various actors, including malicious insiders with access to the training data and external attackers who find ways to influence the data collection or preprocessing pipeline. Insiders pose a significant threat because they often have privileged access and knowledge of the system, enabling them to introduce poisoned data without raising suspicions. On the other hand, external attackers may exploit vulnerabilities in data sourcing, crowdsourcing platforms, or data aggregation processes to inject poisoned samples into the training dataset. This highlights the importance of implementing strong access controls, data governance policies, and monitoring mechanisms to mitigate the risk of insider threats and external attacks.\nExploits vulnerabilities in data collection and preprocessing: Data poisoning attacks often exploit vulnerabilities in the machine learning pipeline’s data collection and preprocessing stages. Attackers carefully design poisoned samples to evade common data validation techniques, ensuring that the manipulated data still falls within acceptable ranges, follows expected distributions, or maintains consistency with other features. This allows the poisoned data to pass through data preprocessing steps without detection. Furthermore, poisoning attacks can take advantage of weaknesses in data preprocessing, such as inadequate data cleaning, insufficient outlier detection, or lack of integrity checks. Attackers may also exploit the lack of robust data provenance and lineage tracking mechanisms to introduce poisoned data without leaving a traceable trail. Addressing these vulnerabilities requires rigorous data validation, anomaly detection, and data provenance tracking techniques to ensure the integrity and trustworthiness of the training data.\nDisrupts the learning process and skews model behavior: Data poisoning attacks are designed to disrupt the learning process of machine learning models and skew their behavior towards the attacker’s objectives. The poisoned data is typically manipulated with specific goals, such as skewing the model’s behavior towards certain classes, introducing backdoors, or degrading overall performance. These manipulations are not random but targeted to achieve the attacker’s desired outcomes. By introducing label inconsistencies, where the manipulated samples have labels that do not align with their true nature, poisoning attacks can confuse the model during training and lead to biased or incorrect predictions. The disruption caused by poisoned data can have far-reaching consequences, as the compromised model may make flawed decisions or exhibit unintended behavior when deployed in real-world applications.\n\n\nMechanisms of Data Poisoning\nData poisoning attacks can be carried out through various mechanisms, exploiting different ML pipeline vulnerabilities. These mechanisms allow attackers to manipulate the training data and introduce malicious samples that can compromise the model’s performance, fairness, or integrity. Understanding these mechanisms is crucial for developing effective defenses against data poisoning and ensuring the robustness of ML systems. Data poisoning mechanisms can be broadly categorized based on the attacker’s approach and the stage of the ML pipeline they target. Some common mechanisms include modifying training data labels, altering feature values, injecting carefully crafted malicious samples, exploiting data collection and preprocessing vulnerabilities, manipulating data at the source, poisoning data in online learning scenarios, and collaborating with insiders to manipulate data.\nEach of these mechanisms presents unique challenges and requires different mitigation strategies. For example, detecting label manipulation may involve analyzing the distribution of labels and identifying anomalies (Zhou et al. 2018), while preventing feature manipulation may require secure data preprocessing and anomaly detection techniques (Carta et al. 2020). Defending against insider threats may involve strict access control policies and monitoring of data access patterns. Moreover, the effectiveness of data poisoning attacks often depends on the attacker’s knowledge of the ML system, including the model architecture, training algorithms, and data distribution. Attackers may use adversarial machine learning or data synthesis techniques to craft samples that are more likely to bypass detection and achieve their malicious objectives.\n\nZhou, Peng, Xintong Han, Vlad I. Morariu, and Larry S. Davis. 2018. “Learning Rich Features for Image Manipulation Detection.” In 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition, 1053–61. IEEE. https://doi.org/10.1109/cvpr.2018.00116.\n\nCarta, Salvatore, Alessandro Sebastian Podda, Diego Reforgiato Recupero, and Roberto Saia. 2020. “A Local Feature Engineering Strategy to Improve Network Anomaly Detection.” Future Internet 12 (10): 177. https://doi.org/10.3390/fi12100177.\nModifying training data labels: One of the most straightforward mechanisms of data poisoning is modifying the training data labels. In this approach, the attacker selectively changes the labels of a subset of the training samples to mislead the model’s learning process as shown in Figure 18.24. For example, in a binary classification task, the attacker might flip the labels of some positive samples to negative, or vice versa. By introducing such label noise, the attacker degrades the model’s performance or cause it to make incorrect predictions for specific target instances.\n\n\n\n\n\n\nFigure 18.24: Garbage In – Garbage Out. Source: Information Matters\n\n\n\nAltering feature values in training data: Another mechanism of data poisoning involves altering the feature values of the training samples without modifying the labels. The attacker carefully crafts the feature values to introduce specific biases or vulnerabilities into the model. For instance, in an image classification task, the attacker might add imperceptible perturbations to a subset of images, causing the model to learn a particular pattern or association. This type of poisoning can create backdoors or trojans in the trained model, which specific input patterns can trigger.\nInjecting carefully crafted malicious samples: In this mechanism, the attacker creates malicious samples designed to poison the model. These samples are crafted to have a specific impact on the model’s behavior while blending in with the legitimate training data. The attacker might use techniques such as adversarial perturbations or data synthesis to generate poisoned samples that are difficult to detect. The attacker manipulates the model’s decision boundaries by injecting these malicious samples into the training data or introducing targeted misclassifications.\nExploiting data collection and preprocessing vulnerabilities: Data poisoning attacks can also exploit the data collection and preprocessing pipeline vulnerabilities. If the data collection process is not secure or there are weaknesses in the data preprocessing steps, an attacker can manipulate the data before it reaches the training phase. For example, if data is collected from untrusted sources or issues in data cleaning or aggregation, an attacker can introduce poisoned samples or manipulate the data to their advantage.\nManipulating data at the source (e.g., sensor data): In some cases, attackers can manipulate the data at its source, such as sensor data or input devices. By tampering with the sensors or manipulating the environment in which data is collected, attackers can introduce poisoned samples or bias the data distribution. For instance, in a self-driving car scenario, an attacker might manipulate the sensors or the environment to feed misleading information into the training data, compromising the model’s ability to make safe and reliable decisions.\nPoisoning data in online learning scenarios: Data poisoning attacks can also target ML systems that employ online learning, where the model is continuously updated with new data in real time. In such scenarios, an attacker can gradually inject poisoned samples over time, slowly manipulating the model’s behavior. Online learning systems are particularly vulnerable to data poisoning because they adapt to new data without extensive validation, making it easier for attackers to introduce malicious samples, as shown in Figure 18.25.\n\n\n\n\n\n\nFigure 18.25: Data Poisoning Attack. Source: Sikandar\n\n\n\nCollaborating with insiders to manipulate data: Sometimes, data poisoning attacks can involve collaboration with insiders with access to the training data. Malicious insiders, such as employees or data providers, can manipulate the data before it is used to train the model. Insider threats are particularly challenging to detect and prevent, as the attackers have legitimate access to the data and can carefully craft the poisoning strategy to evade detection.\nThese are the key mechanisms of data poisoning in ML systems. Attackers often employ these mechanisms to make their attacks more effective and harder to detect. The risk of data poisoning attacks grows as ML systems become increasingly complex and rely on larger datasets from diverse sources. Defending against data poisoning requires a multifaceted approach. ML practitioners and system designers must be aware of the various mechanisms of data poisoning and adopt a comprehensive approach to data security and model resilience. This includes secure data collection, robust data validation, and continuous model performance monitoring. Implementing secure data collection and preprocessing practices is crucial to prevent data poisoning at the source. Data validation and anomaly detection techniques can also help identify and mitigate potential poisoning attempts. Monitoring model performance for signs of data poisoning is also essential to detect and respond to attacks promptly.\n\n\nImpact on ML Systems\nData poisoning attacks can severely affect ML systems, compromising their performance, reliability, and trustworthiness. The impact of data poisoning can manifest in various ways, depending on the attacker’s objectives and the specific mechanism used. Let’s explore each of the potential impacts in detail.\nDegradation of model performance: One of the primary impacts of data poisoning is the degradation of the model’s overall performance. By manipulating the training data, attackers can introduce noise, biases, or inconsistencies that hinder the model’s ability to learn accurate patterns and make reliable predictions. This can reduce accuracy, precision, recall, or other performance metrics. The degradation of model performance can have significant consequences, especially in critical applications such as healthcare, finance, or security, where the reliability of predictions is crucial.\nMisclassification of specific targets: Data poisoning attacks can also be designed to cause the model to misclassify specific target instances. Attackers may introduce carefully crafted poisoned samples similar to the target instances, leading the model to learn incorrect associations. This can result in the model consistently misclassifying the targeted instances, even if it performs well on other inputs. Such targeted misclassification can have severe consequences, such as causing a malware detection system to overlook specific malicious files or leading to the wrong diagnosis in a medical imaging application.\nBackdoors and trojans in trained models: Data poisoning can introduce backdoors or trojans into the trained model. Backdoors are hidden functionalities that allow attackers to trigger specific behaviors or bypass normal authentication mechanisms. On the other hand, Trojans are malicious components embedded within the model that can activate specific input patterns. By poisoning the training data, attackers can create models that appear to perform normally but contain hidden vulnerabilities that can be exploited later. Backdoors and trojans can compromise the integrity and security of the ML system, allowing attackers to gain unauthorized access, manipulate predictions, or exfiltrate sensitive information.\nBiased or unfair model outcomes: Data poisoning attacks can introduce biases or unfairness into the model’s predictions. By manipulating the training data distribution or injecting samples with specific biases, attackers can cause the model to learn and perpetuate discriminatory patterns. This can lead to unfair treatment of certain groups or individuals based on sensitive attributes such as race, gender, or age. Biased models can have severe societal implications, reinforcing existing inequalities and discriminatory practices. Ensuring fairness and mitigating biases is crucial for building trustworthy and ethical ML systems.\nCompromised system reliability and trustworthiness: Data poisoning attacks can undermine ML systems’ overall reliability and trustworthiness. When models are trained on poisoned data, their predictions become unreliable and untrustworthy. This can erode user confidence in the system and lead to a loss of trust in the decisions made by the model. In critical applications where ML systems are relied upon for decision-making, such as autonomous vehicles or medical diagnosis, compromised reliability can have severe consequences, putting lives and property at risk.\nAddressing the impact of data poisoning requires a proactive approach to data security, model testing, and monitoring. Organizations must implement robust measures to ensure the integrity and quality of training data, employ techniques to detect and mitigate poisoning attempts, and continuously monitor the performance and behavior of deployed models. Collaboration between ML practitioners, security experts, and domain specialists is essential to develop comprehensive strategies for preventing and responding to data poisoning attacks.\n\nCase Study: Protecting Art Through Data Poisoning\nInterestingly enough, data poisoning attacks are not always malicious (Shan et al. 2023). Nightshade, a tool developed by a team led by Professor Ben Zhao at the University of Chicago, utilizes data poisoning to help artists protect their art against scraping and copyright violations by generative AI models. Artists can use the tool to make subtle modifications to their images before uploading them online.\nWhile these changes are indiscernible to the human eye, they can significantly disrupt the performance of generative AI models when incorporated into the training data. Generative models can be manipulated to generate hallucinations and weird images. For example, with only 300 poisoned images, the University of Chicago researchers could trick the latest Stable Diffusion model into generating images of dogs that look like cats or images of cows when prompted for cars.\nAs the number of poisoned images on the internet increases, the performance of the models that use scraped data will deteriorate exponentially. First, the poisoned data is hard to detect and requires manual elimination. Second, the “poison” spreads quickly to other labels because generative models rely on connections between words and concepts as they generate images. So a poisoned image of a “car” could spread into generated images associated with words like “truck,” “train,” ” bus,” etc.\nOn the other hand, this tool can be used maliciously and can affect legitimate applications of the generative models. This shows the very challenging and novel nature of machine learning attacks.\nFigure 18.26 demonstrates the effects of different levels of data poisoning (50 samples, 100 samples, and 300 samples of poisoned images) on generating images in different categories. Notice how the images start deforming and deviating from the desired category. For example, after 300 poison samples, a car prompt generates a cow.\n\n\n\n\n\n\nFigure 18.26: NightShade’s poisoning effects on Stable Diffusion. Source: Shan et al. (2023)\n\n\nShan, Shawn, Wenxin Ding, Josephine Passananti, Haitao Zheng, and Ben Y Zhao. 2023. “Prompt-Specific Poisoning Attacks on Text-to-Image Generative Models.” ArXiv Preprint abs/2310.13828. https://arxiv.org/abs/2310.13828.\n\n\n\n\n\n\n\n\nExercise 18.3: Poisoning Attacks\n\n\n\n\n\nGet ready to explore the dark side of AI security! In this Colab, you’ll learn about data poisoning – how bad data can trick AI models into making wrong decisions. We’ll focus on a real-world attack against a Support Vector Machine (SVM), observing how the AI’s behavior changes under attack. This hands-on exercise will highlight why protecting AI systems is crucial, especially as they become more integrated into our lives. Think like a hacker, understand the vulnerability, and brainstorm how to defend our AI systems!\n\n\n\n\n\n\n\n\n18.4.3 Distribution Shifts\n\nDefinition and Characteristics\nDistribution shift refers to the phenomenon where the data distribution encountered by an ML model during deployment (inference) differs from the distribution it was trained on, as shown in Figure 18.27. This is not so much an attack as it is that the model’s robustness will vary over time. In other words, the data’s statistical properties, patterns, or underlying assumptions can change between the training and test phases.\n\n\n\n\n\n\nFigure 18.27: The curly brackets enclose the distribution shift between the environments. Here, z stands for the spurious feature, and y stands for label class. Source: Xin\n\n\n\nThe key characteristics of distribution shift include:\nDomain mismatch: The input data during inference comes from a different domain or distribution than the training data. When the input data during inference comes from a domain or distribution different from the training data, it can significantly affect the model’s performance. This is because the model has learned patterns and relationships specific to the training domain, and when applied to a different domain, those learned patterns may not hold. This includes scenarios like covariate shift, where the input feature distributions change while the relationship with the target variable remains consistent. For example, consider a sentiment analysis model trained on movie reviews. Suppose this model is applied to analyze sentiment in tweets. In that case, it may need help to accurately classify the sentiment because the language, grammar, and context of tweets can differ from movie reviews. This domain mismatch can result in poor performance and unreliable predictions, limiting the model’s practical utility.\nTemporal drift: The data distribution evolves, leading to a gradual or sudden shift in the input characteristics. Temporal drift occurs when the relationship between input features and the target variable changes over time, as shown in Figure 18.28. Temporal drift is important because ML models are often deployed in dynamic environments where the data distribution can change over time. If the model is not updated or adapted to these changes, its performance can gradually degrade. For instance, the patterns and behaviors associated with fraudulent activities may evolve in a fraud detection system as fraudsters adapt their techniques. If the model is not retrained or updated to capture these new patterns, it may fail to detect new types of fraud effectively. Temporal drift can lead to a decline in the model’s accuracy and reliability over time, making monitoring and addressing this type of distribution shift crucial.\nContextual changes: The ML model’s context can vary, resulting in different data distributions based on factors such as location, user behavior, or environmental conditions. Contextual changes matter because ML models are often deployed in various contexts or environments that can have different data distributions. If the model cannot generalize well to these different contexts, its performance may deteriorate. For example, consider a computer vision model trained to recognize objects in a controlled lab environment. When deployed in a real-world setting, factors such as lighting conditions, camera angles, or background clutter can vary significantly, leading to a distribution shift. If the model is robust to these contextual changes, it may be able to accurately recognize objects in the new environment, limiting its practical utility.\nUnrepresentative training data: The training data may only partially capture the variability and diversity of the real-world data encountered during deployment. This directly impacts the model’s ability to generalize to new scenarios. Suppose the training data does not capture the variability and diversity of the real-world data adequately. In that case, the model may learn patterns specific to the training set but needs to generalize better to new, unseen data. This can result in poor performance and limited model applicability. For instance, if a facial recognition model is trained primarily on images of individuals from a specific demographic group, it may struggle to accurately recognize faces from other demographic groups when deployed in a real-world setting. Ensuring that the training data is representative and diverse is crucial for building models that can generalize well to real-world scenarios.\n\n\n\n\n\n\nFigure 18.28: Concept drift refers to a change in data patterns and relationships over time. Source: Evidently AI\n\n\n\nThe presence of a distribution shift can significantly impact the performance and reliability of ML models, as the models may need help generalizing well to the new data distribution. Detecting and adapting to distribution shifts is crucial to ensure ML systems’ robustness and practical utility in real-world scenarios.\n\n\nMechanisms of Distribution Shifts\nThe mechanisms of distribution shift, such as changes in data sources, temporal evolution, domain-specific variations, selection bias, feedback loops, and adversarial manipulations, are important to understand because they help identify the underlying causes of distribution shift. By understanding these mechanisms, practitioners can develop targeted strategies to mitigate their impact and improve the model’s robustness. Here are some common mechanisms:\n\n\n\n\n\n\nFigure 18.29: Temporal evolution. Source: Białek\n\n\n\nChanges in data sources: Distribution shifts can occur when the data sources used for training and inference differ. For example, if a model is trained on data from one sensor but deployed on data from another sensor with different characteristics, it can lead to a distribution shift.\nTemporal evolution: Over time, the underlying data distribution can evolve due to changes in user behavior, market dynamics, or other temporal factors. For instance, in a recommendation system, user preferences may shift over time, leading to a distribution shift in the input data, as shown in Figure 18.29.\nDomain-specific variations: Different domains or contexts can have distinct data distributions. A model trained on data from one domain may only generalize well to another domain with appropriate adaptation techniques. For example, an image classification model trained on indoor scenes may struggle when applied to outdoor scenes.\nSelection bias: A Distribution shift can arise from selection bias during data collection or sampling. If the training data does not represent the true population or certain subgroups are over- or underrepresented, this can lead to a mismatch between the training and test distributions.\nFeedback loops: In some cases, the predictions or actions taken by an ML model can influence future data distribution. For example, in a dynamic pricing system, the prices set by the model can impact customer behavior, leading to a shift in the data distribution over time.\nAdversarial manipulations: Adversaries can intentionally manipulate the input data to create a distribution shift and deceive the ML model. By introducing carefully crafted perturbations or generating out-of-distribution samples, attackers can exploit the model’s vulnerabilities and cause it to make incorrect predictions.\nUnderstanding the mechanisms of distribution shift is important for developing effective strategies to detect and mitigate its impact on ML systems. By identifying the sources and characteristics of the shift, practitioners can design appropriate techniques, such as domain adaptation, transfer learning, or continual learning, to improve the model’s robustness and performance under distributional changes.\n\n\nImpact on ML Systems\nDistribution shifts can significantly negatively impact the performance and reliability of ML systems. Here are some key ways in which distribution shift can affect ML models:\nDegraded predictive performance: When the data distribution encountered during inference differs from the training distribution, the model’s predictive accuracy can deteriorate. The model may need help generalizing the new data well, leading to increased errors and suboptimal performance.\nReduced reliability and trustworthiness: Distribution shift can undermine the reliability and trustworthiness of ML models. If the model’s predictions become unreliable or inconsistent due to the shift, users may lose confidence in the system’s outputs, leading to potential misuse or disuse of the model.\nBiased predictions: Distribution shift can introduce biases in the model’s predictions. If the training data does not represent the real-world distribution or certain subgroups are underrepresented, the model may make biased predictions that discriminate against certain groups or perpetuate societal biases.\nIncreased uncertainty and risk: Distribution shift introduces additional uncertainty and risk into the ML system. The model’s behavior and performance may become less predictable, making it challenging to assess its reliability and suitability for critical applications. This uncertainty can lead to increased operational risks and potential failures.\nAdaptability challenges: ML models trained on a specific data distribution may need help to adapt to changing environments or new domains. The lack of adaptability can limit the model’s usefulness and applicability in dynamic real-world scenarios where the data distribution evolves.\nMaintenance and update difficulties: Distribution shift can complicate the maintenance and updating of ML models. As the data distribution changes, the model may require frequent retraining or fine-tuning to maintain its performance. This can be time-consuming and resource-intensive, especially if the shift occurs rapidly or continuously.\nVulnerability to adversarial attacks: Distribution shift can make ML models more vulnerable to adversarial attacks. Adversaries can exploit the model’s sensitivity to distributional changes by crafting adversarial examples outside the training distribution, causing the model to make incorrect predictions or behave unexpectedly.\nTo mitigate the impact of distribution shifts, it is crucial to develop robust ML systems that detect and adapt to distributional changes. Techniques such as domain adaptation, transfer learning, and continual learning can help improve the model’s generalization ability across different distributions. ML model monitoring, testing, and updating are also necessary to ensure their performance and reliability during distribution shifts.\n\n\n\n18.4.4 Detection and Mitigation\n\nAdversarial Attacks\nAs you may recall from above, adversarial attacks pose a significant threat to the robustness and reliability of ML systems. These attacks involve crafting carefully designed inputs, known as adversarial examples, to deceive ML models and cause them to make incorrect predictions. To safeguard ML systems against adversarial attacks, developing effective techniques for detecting and mitigating these threats is crucial.\n\nAdversarial Example Detection Techniques\nDetecting adversarial examples is the first line of defense against adversarial attacks. Several techniques have been proposed to identify and flag suspicious inputs that may be adversarial.\nStatistical methods aim to detect adversarial examples by analyzing the statistical properties of the input data. These methods often compare the input data distribution to a reference distribution, such as the training data distribution or a known benign distribution. Techniques like the Kolmogorov-Smirnov (Berger and Zhou 2014) test or the Anderson-Darling test can be used to measure the discrepancy between the distributions and flag inputs that deviate significantly from the expected distribution.\n\nBerger, Vance W, and YanYan Zhou. 2014. “Kolmogorovsmirnov Test: Overview.” Wiley Statsref: Statistics Reference Online.\nKernel density estimation (KDE) is a non-parametric technique used to estimate the probability density function of a dataset. In the context of adversarial example detection, KDE can be used to estimate the density of benign examples in the input space. Adversarial examples often lie in low-density regions and can be detected by comparing their estimated density to a threshold. Inputs with an estimated density below the threshold are flagged as potential adversarial examples.\nAnother technique is feature squeezing (Panda, Chakraborty, and Roy 2019), which reduces the complexity of the input space by applying dimensionality reduction or discretization. The idea behind feature squeezing is that adversarial examples often rely on small, imperceptible perturbations that can be eliminated or reduced through these transformations. Inconsistencies can be detected by comparing the model’s predictions on the original input and the squeezed input, indicating the presence of adversarial examples.\n\nPanda, Priyadarshini, Indranil Chakraborty, and Kaushik Roy. 2019. “Discretization Based Solutions for Secure Machine Learning Against Adversarial Attacks.” #IEEE_O_ACC# 7: 70157–68. https://doi.org/10.1109/access.2019.2919463.\nModel uncertainty estimation techniques aim to quantify the confidence or uncertainty associated with a model’s predictions. Adversarial examples often exploit regions of high uncertainty in the model’s decision boundary. By estimating the uncertainty using techniques like Bayesian neural networks, dropout-based uncertainty estimation, or ensemble methods, inputs with high uncertainty can be flagged as potential adversarial examples.\n\n\nAdversarial Defense Strategies\nOnce adversarial examples are detected, various defense strategies can be employed to mitigate their impact and improve the robustness of ML models.\nAdversarial training is a technique that involves augmenting the training data with adversarial examples and retraining the model on this augmented dataset. Exposing the model to adversarial examples during training teaches it to classify them correctly and becomes more robust to adversarial attacks. Adversarial training can be performed using various attack methods, such as the Fast Gradient Sign Method (FGSM) or Projected Gradient Descent (PGD) (Madry et al. 2017).\n\nMadry, Aleksander, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu. 2017. “Towards Deep Learning Models Resistant to Adversarial Attacks.” arXiv Preprint arXiv:1706.06083.\n\nPapernot, Nicolas, Patrick McDaniel, Xi Wu, Somesh Jha, and Ananthram Swami. 2016. “Distillation as a Defense to Adversarial Perturbations Against Deep Neural Networks.” In 2016 IEEE Symposium on Security and Privacy (SP), 582–97. IEEE; IEEE. https://doi.org/10.1109/sp.2016.41.\nDefensive distillation (Papernot et al. 2016) is a technique that trains a second model (the student model) to mimic the behavior of the original model (the teacher model). The student model is trained on the soft labels produced by the teacher model, which are less sensitive to small perturbations. Using the student model for inference can reduce the impact of adversarial perturbations, as the student model learns to generalize better and is less sensitive to adversarial noise.\nInput preprocessing and transformation techniques aim to remove or mitigate the effect of adversarial perturbations before feeding the input to the ML model. These techniques include image denoising, JPEG compression, random resizing, padding, or applying random transformations to the input data. By reducing the impact of adversarial perturbations, these preprocessing steps can help improve the model’s robustness to adversarial attacks.\nEnsemble methods combine multiple models to make more robust predictions. The ensemble can reduce the impact of adversarial attacks by using a diverse set of models with different architectures, training data, or hyperparameters. Adversarial examples that fool one model may not fool others in the ensemble, leading to more reliable and robust predictions. Model diversification techniques, such as using different preprocessing techniques or feature representations for each model in the ensemble, can further enhance the robustness.\n\n\nRobustness Evaluation and Testing\nConduct thorough evaluation and testing to assess the effectiveness of adversarial defense techniques and measure the robustness of ML models.\nAdversarial robustness metrics quantify the model’s resilience to adversarial attacks. These metrics can include the model’s accuracy on adversarial examples, the average distortion required to fool the model, or the model’s performance under different attack strengths. By comparing these metrics across different models or defense techniques, practitioners can assess and compare their robustness levels.\nStandardized adversarial attack benchmarks and datasets provide a common ground for evaluating and comparing the robustness of ML models. These benchmarks include datasets with pre-generated adversarial examples and tools and frameworks for generating adversarial attacks. Examples of popular adversarial attack benchmarks include the MNIST-C, CIFAR-10-C, and ImageNet-C (Hendrycks and Dietterich 2019) datasets, which contain corrupted or perturbed versions of the original datasets.\n\nHendrycks, Dan, and Thomas Dietterich. 2019. “Benchmarking Neural Network Robustness to Common Corruptions and Perturbations.” arXiv Preprint arXiv:1903.12261.\nPractitioners can develop more robust and resilient ML systems by leveraging these adversarial example detection techniques, defense strategies, and robustness evaluation methods. However, it is important to note that adversarial robustness is an ongoing research area, and no single technique provides complete protection against all types of adversarial attacks. A comprehensive approach that combines multiple defense mechanisms and regular testing is essential to maintain the security and reliability of ML systems in the face of evolving adversarial threats.\n\n\n\nData Poisoning\nRecall that data poisoning is an attack that targets the integrity of the training data used to build ML models. By manipulating or corrupting the training data, attackers can influence the model’s behavior and cause it to make incorrect predictions or perform unintended actions. Detecting and mitigating data poisoning attacks is crucial to ensure the trustworthiness and reliability of ML systems, as shown in Figure 18.30.\n\n\n\n\n\n\nFigure 18.30: Malicious data injection. Source: Li\n\n\n\n\nAnomaly Detection Techniques for Identifying Poisoned Data\nStatistical outlier detection methods identify data points that deviate significantly from most data. These methods assume that poisoned data instances are likely to be statistical outliers. Techniques such as the Z-score method, Tukey’s method, or the Mahalanobis distance can be used to measure the deviation of each data point from the central tendency of the dataset. Data points that exceed a predefined threshold are flagged as potential outliers and considered suspicious for data poisoning.\nClustering-based methods group similar data points together based on their features or attributes. The assumption is that poisoned data instances may form distinct clusters or lie far away from the normal data clusters. By applying clustering algorithms like K-means, DBSCAN, or hierarchical clustering, anomalous clusters or data points that do not belong to any cluster can be identified. These anomalous instances are then treated as potentially poisoned data.\nAutoencoders are neural networks trained to reconstruct the input data from a compressed representation, as shown in Figure 18.31. They can be used for anomaly detection by learning the normal patterns in the data and identifying instances that deviate from them. During training, the autoencoder is trained on clean, unpoisoned data. At inference time, the reconstruction error for each data point is computed. Data points with high reconstruction errors are considered abnormal and potentially poisoned, as they do not conform to the learned normal patterns.\n\n\n\n\n\n\nFigure 18.31: Autoencoder. Source: Dertat\n\n\n\n\n\nData Sanitization and Preprocessing Techniques\nData poisoning can be avoided by cleaning data, which involves identifying and removing or correcting noisy, incomplete, or inconsistent data points. Techniques such as data deduplication, missing value imputation, and outlier removal can be applied to improve the quality of the training data. By eliminating or filtering out suspicious or anomalous data points, the impact of poisoned instances can be reduced.\nData validation involves verifying the integrity and consistency of the training data. This can include checking for data type consistency, range validation, and cross-field dependencies. By defining and enforcing data validation rules, anomalous or inconsistent data points indicative of data poisoning can be identified and flagged for further investigation.\nData provenance and lineage tracking involve maintaining a record of data’s origin, transformations, and movements throughout the ML pipeline. By documenting the data sources, preprocessing steps, and any modifications made to the data, practitioners can trace anomalies or suspicious patterns back to their origin. This helps identify potential points of data poisoning and facilitates the investigation and mitigation process.\n\n\nRobust Training Techniques\nRobust optimization techniques can be used to modify the training objective to minimize the impact of outliers or poisoned instances. This can be achieved by using robust loss functions less sensitive to extreme values, such as the Huber loss or the modified Huber loss. Regularization techniques, such as L1 or L2 regularization, can also help in reducing the model’s sensitivity to poisoned data by constraining the model’s complexity and preventing overfitting.\nRobust loss functions are designed to be less sensitive to outliers or noisy data points. Examples include the modified Huber loss, the Tukey loss (Beaton and Tukey 1974), and the trimmed mean loss. These loss functions down-weight or ignore the contribution of abnormal instances during training, reducing their impact on the model’s learning process. Robust objective functions, such as the minimax or distributionally robust objective, aim to optimize the model’s performance under worst-case scenarios or in the presence of adversarial perturbations.\n\nBeaton, Albert E., and John W. Tukey. 1974. “The Fitting of Power Series, Meaning Polynomials, Illustrated on Band-Spectroscopic Data.” Technometrics 16 (2): 147. https://doi.org/10.2307/1267936.\nData augmentation techniques involve generating additional training examples by applying random transformations or perturbations to the existing data Figure 18.32. This helps in increasing the diversity and robustness of the training dataset. By introducing controlled variations in the data, the model becomes less sensitive to specific patterns or artifacts that may be present in poisoned instances. Randomization techniques, such as random subsampling or bootstrap aggregating, can also help reduce the impact of poisoned data by training multiple models on different subsets of the data and combining their predictions.\n\n\n\n\n\n\nFigure 18.32: An image of the number “3” in original form and with basic augmentations applied.\n\n\n\n\n\nSecure and Trusted Data Sourcing\nImplementing the best data collection and curation practices can help mitigate the risk of data poisoning. This includes establishing clear data collection protocols, verifying the authenticity and reliability of data sources, and conducting regular data quality assessments. Sourcing data from trusted and reputable providers and following secure data handling practices can reduce the likelihood of introducing poisoned data into the training pipeline.\nStrong data governance and access control mechanisms are essential to prevent unauthorized modifications or tampering with the training data. This involves defining clear roles and responsibilities for data access, implementing access control policies based on the principle of least privilege, and monitoring and logging data access activities. By restricting access to the training data and maintaining an audit trail, potential data poisoning attempts can be detected and investigated.\nDetecting and mitigating data poisoning attacks requires a multifaceted approach that combines anomaly detection, data sanitization, robust training techniques, and secure data sourcing practices. By implementing these measures, ML practitioners can improve the resilience of their models against data poisoning and ensure the integrity and trustworthiness of the training data. However, it is important to note that data poisoning is an active area of research, and new attack vectors and defense mechanisms continue to emerge. Staying informed about the latest developments and adopting a proactive and adaptive approach to data security is crucial for maintaining the robustness of ML systems.\n\n\n\nDistribution Shifts\n\nDetecting and Mitigating Distribution Shifts\nRecall that distribution shifts occur when the data distribution encountered by a machine learning (ML) model during deployment differs from the distribution it was trained on. These shifts can significantly impact the model’s performance and generalization ability, leading to suboptimal or incorrect predictions. Detecting and mitigating distribution shifts is crucial to ensure the robustness and reliability of ML systems in real-world scenarios.\n\n\nDetection Techniques for Distribution Shifts\nStatistical tests can be used to compare the distributions of the training and test data to identify significant differences. Techniques such as the Kolmogorov-Smirnov test or the Anderson-Darling test measure the discrepancy between two distributions and provide a quantitative assessment of the presence of distribution shift. By applying these tests to the input features or the model’s predictions, practitioners can detect if there is a statistically significant difference between the training and test distributions.\nDivergence metrics quantify the dissimilarity between two probability distributions. Commonly used divergence metrics include the Kullback-Leibler (KL) divergence and the Jensen-Shannon (JS) divergence. By calculating the divergence between the training and test data distributions, practitioners can assess the extent of the distribution shift. High divergence values indicate a significant difference between the distributions, suggesting the presence of a distribution shift.\nUncertainty quantification techniques, such as Bayesian neural networks or ensemble methods, can estimate the uncertainty associated with the model’s predictions. When a model is applied to data from a different distribution, its predictions may have higher uncertainty. By monitoring the uncertainty levels, practitioners can detect distribution shifts. If the uncertainty consistently exceeds a predetermined threshold for test samples, it suggests that the model is operating outside its trained distribution.\nIn addition, domain classifiers are trained to distinguish between different domains or distributions. Practitioners can detect distribution shifts by training a classifier to differentiate between the training and test domains. If the domain classifier achieves high accuracy in distinguishing between the two domains, it indicates a significant difference in the underlying distributions. The performance of the domain classifier serves as a measure of the distribution shift.\n\n\nMitigation Techniques for Distribution Shifts\nTransfer learning leverages knowledge gained from one domain to improve performance in another, as shown in Figure 18.33. By using pre-trained models or transferring learned features from a source domain to a target domain, transfer learning can help mitigate the impact of distribution shifts. The pre-trained model can be fine-tuned on a small amount of labeled data from the target domain, allowing it to adapt to the new distribution. Transfer learning is particularly effective when the source and target domains share similar characteristics or when labeled data in the target domain is scarce.\n\n\n\n\n\n\nFigure 18.33: Transfer learning. Source: Bhavsar\n\n\n\nContinual learning, also known as lifelong learning, enables ML models to learn continuously from new data distributions while retaining knowledge from previous distributions. Techniques such as elastic weight consolidation (EWC) (Kirkpatrick et al. 2017) or gradient episodic memory (GEM) (Lopez-Paz and Ranzato 2017) allow models to adapt to evolving data distributions over time. These techniques aim to balance the plasticity of the model (ability to learn from new data) with the stability of the model (retaining previously learned knowledge). By incrementally updating the model with new data and mitigating catastrophic forgetting, continual learning helps models stay robust to distribution shifts.\n\nKirkpatrick, James, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume Desjardins, Andrei A. Rusu, Kieran Milan, et al. 2017. “Overcoming Catastrophic Forgetting in Neural Networks.” Proc. Natl. Acad. Sci. 114 (13): 3521–26. https://doi.org/10.1073/pnas.1611835114.\n\nLopez-Paz, David, and Marc’Aurelio Ranzato. 2017. “Gradient Episodic Memory for Continual Learning.” Adv Neural Inf Process Syst 30.\nData augmentation techniques, such as those we have seen previously, involve applying transformations or perturbations to the existing training data to increase its diversity and improve the model’s robustness to distribution shifts. By introducing variations in the data, such as rotations, translations, scaling, or adding noise, data augmentation helps the model learn invariant features and generalize better to unseen distributions. Data augmentation can be performed during training and inference to improve the model’s ability to handle distribution shifts.\nEnsemble methods combine multiple models to make predictions more robust to distribution shifts. By training models on different subsets of the data, using different algorithms, or with different hyperparameters, ensemble methods can capture diverse aspects of the data distribution. When presented with a shifted distribution, the ensemble can leverage the strengths of individual models to make more accurate and stable predictions. Techniques like bagging, boosting, or stacking can create effective ensembles.\nRegularly updating models with new data from the target distribution is crucial to mitigate the impact of distribution shifts. As the data distribution evolves, models should be retrained or fine-tuned on the latest available data to adapt to the changing patterns. Monitoring model performance and data characteristics can help detect when an update is necessary. By keeping the models up to date, practitioners can ensure they remain relevant and accurate in the face of distribution shifts.\nEvaluating models using robust metrics less sensitive to distribution shifts can provide a more reliable assessment of model performance. Metrics such as the area under the precision-recall curve (AUPRC) or the F1 score are more robust to class imbalance and can better capture the model’s performance across different distributions. Additionally, using domain-specific evaluation metrics that align with the desired outcomes in the target domain can provide a more meaningful measure of the model’s effectiveness.\nDetecting and mitigating distribution shifts is an ongoing process that requires continuous monitoring, adaptation, and improvement. By employing a combination of detection techniques and mitigation strategies, ML practitioners can proactively identify and address distribution shifts, ensuring the robustness and reliability of their models in real-world deployments. It is important to note that distribution shifts can take various forms and may require domain-specific approaches depending on the nature of the data and the application. Staying informed about the latest research and best practices in handling distribution shifts is essential for building resilient ML systems.",
    "crumbs": [
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Robust AI</span>"
    ]
  },
  {
    "objectID": "contents/core/robust_ai/robust_ai.html#software-faults",
    "href": "contents/core/robust_ai/robust_ai.html#software-faults",
    "title": "18  Robust AI",
    "section": "18.5 Software Faults",
    "text": "18.5 Software Faults\n\n18.5.1 Definition and Characteristics\nSoftware faults refer to defects, errors, or bugs in the runtime software frameworks and components that support the execution and deployment of ML models (Myllyaho et al. 2022). These faults can arise from various sources, such as programming mistakes, design flaws, or compatibility issues (H. Zhang 2008), and can have significant implications for ML systems’ performance, reliability, and security. Software faults in ML frameworks exhibit several key characteristics:\n\nMyllyaho, Lalli, Mikko Raatikainen, Tomi Männistö, Jukka K. Nurminen, and Tommi Mikkonen. 2022. “On Misbehaviour and Fault Tolerance in Machine Learning Systems.” J. Syst. Software 183 (January): 111096. https://doi.org/10.1016/j.jss.2021.111096.\n\nZhang, Hongyu. 2008. “On the Distribution of Software Faults.” IEEE Trans. Software Eng. 34 (2): 301–2. https://doi.org/10.1109/tse.2007.70771.\n\nDiversity: Software faults can manifest in different forms, ranging from simple logic and syntax mistakes to more complex issues like memory leaks, race conditions, and integration problems. The variety of fault types adds to the challenge of detecting and mitigating them effectively.\nPropagation: In ML systems, software faults can propagate through the various layers and components of the framework. A fault in one module can trigger a cascade of errors or unexpected behavior in other parts of the system, making it difficult to pinpoint the root cause and assess the full impact of the fault.\nIntermittency: Some software faults may exhibit intermittent behavior, occurring sporadically or under specific conditions. These faults can be particularly challenging to reproduce and debug, as they may manifest inconsistently during testing or normal operation.\nInteraction with ML models: Software faults in ML frameworks can interact with the trained models in subtle ways. For example, a fault in the data preprocessing pipeline may introduce noise or bias into the model’s inputs, leading to degraded performance or incorrect predictions. Similarly, faults in the model serving component may cause inconsistencies between the training and inference environments.\nImpact on system properties: Software faults can compromise various desirable properties of ML systems, such as performance, scalability, reliability, and security. Faults may lead to slowdowns, crashes, incorrect outputs, or vulnerabilities that attackers can exploit.\nDependency on external factors: The occurrence and impact of software faults in ML frameworks often depend on external factors, such as the choice of hardware, operating system, libraries, and configurations. Compatibility issues and version mismatches can introduce faults that are difficult to anticipate and mitigate.\n\nUnderstanding the characteristics of software faults in ML frameworks is crucial for developing effective fault prevention, detection, and mitigation strategies. By recognizing the diversity, propagation, intermittency, and impact of software faults, ML practitioners can design more robust and reliable systems resilient to these issues.\n\n\n18.5.2 Mechanisms of Software Faults in ML Frameworks\nMachine learning frameworks, such as TensorFlow, PyTorch, and sci-kit-learn, provide powerful tools and abstractions for building and deploying ML models. However, these frameworks are not immune to software faults that can impact ML systems’ performance, reliability, and correctness. Let’s explore some of the common software faults that can occur in ML frameworks:\nMemory Leaks and Resource Management Issues: Improper memory management, such as failing to release memory or close file handles, can lead to memory leaks and resource exhaustion over time. This issue is compounded by inefficient memory usage, where creating unnecessary copies of large tensors or not leveraging memory-efficient data structures can cause excessive memory consumption and degrade system performance. Additionally, failing to manage GPU memory properly can result in out-of-memory errors or suboptimal utilization of GPU resources, further exacerbating the problem as shown in Figure 18.34.\n\n\n\n\n\n\nFigure 18.34: Example of GPU out-of-the-memory and suboptimal utilization issues\n\n\n\nSynchronization and Concurrency Problems: Incorrect synchronization between threads or processes can lead to race conditions, deadlocks, or inconsistent behavior in multi-threaded or distributed ML systems. This issue is often tied to improper handling of asynchronous operations, such as non-blocking I/O or parallel data loading, which can cause synchronization issues and impact the correctness of the ML pipeline. Moreover, proper coordination and communication between distributed nodes in a cluster can result in consistency or stale data during training or inference, compromising the reliability of the ML system.\nCompatibility Issues: Mismatches between the versions of ML frameworks, libraries, or dependencies can introduce compatibility problems and runtime errors. Upgrading or changing the versions of underlying libraries without thoroughly testing the impact on the ML system can lead to unexpected behavior or breakages. Furthermore, inconsistencies between the training and deployment environments, such as differences in hardware, operating systems, or package versions, can cause compatibility issues and affect the reproducibility of ML models, making it challenging to ensure consistent performance across different platforms.\nNumerical Instability and Precision Errors: Inadequate handling of numerical instabilities, such as division by zero, underflow, or overflow, can lead to incorrect calculations or convergence issues during training. This problem is compounded by insufficient precision or rounding errors, which can accumulate over time and impact the accuracy of the ML models, especially in deep learning architectures with many layers. Moreover, improper scaling or normalization of input data can cause numerical instabilities and affect the convergence and performance of optimization algorithms, resulting in suboptimal or unreliable model performance.\nInadequate Error Handling and Exception Management: Proper error handling and exception management can prevent ML systems from crashing or behaving unexpectedly when encountering exceptional conditions or invalid inputs. Failing to catch and handle specific exceptions or relying on generic exception handling can make it difficult to diagnose and recover from errors gracefully, leading to system instability and reduced reliability. Furthermore, incomplete or misleading error messages can hinder the ability to effectively debug and resolve software faults in ML frameworks, prolonging the time required to identify and fix issues.\n\n\n18.5.3 Impact on ML Systems\nSoftware faults in machine learning frameworks can have significant and far-reaching impacts on ML systems’ performance, reliability, and security. Let’s explore the various ways in which software faults can affect ML systems:\nPerformance Degradation and System Slowdowns: Memory leaks and inefficient resource management can lead to gradual performance degradation over time as the system becomes increasingly memory-constrained and spends more time on garbage collection or memory swapping (Maas et al. 2024). This issue is compounded by synchronization issues and concurrency bugs, which can cause delays, reduced throughput, and suboptimal utilization of computational resources, especially in multi-threaded or distributed ML systems. Furthermore, compatibility problems or inefficient code paths can introduce additional overhead and slowdowns, affecting the overall performance of the ML system.\n\nMaas, Martin, David G. Andersen, Michael Isard, Mohammad Mahdi Javanmard, Kathryn S. McKinley, and Colin Raffel. 2024. “Combining Machine Learning and Lifetime-Based Resource Management for Memory Allocation and Beyond.” Commun. ACM 67 (4): 87–96. https://doi.org/10.1145/3611018.\nIncorrect Predictions or Outputs: Software faults in data preprocessing, feature engineering, or model evaluation can introduce biases, noise, or errors propagating through the ML pipeline and resulting in incorrect predictions or outputs. Over time, numerical instabilities, precision errors, or rounding issues can accumulate and lead to degraded accuracy or convergence problems in the trained models. Moreover, faults in the model serving or inference components can cause inconsistencies between the expected and actual outputs, leading to incorrect or unreliable predictions in production.\nReliability and Stability Issues: Software faults can cause Unparalleled exceptions, crashes, or sudden terminations that can compromise the reliability and stability of ML systems, especially in production environments. Intermittent or sporadic faults can be difficult to reproduce and diagnose, leading to unpredictable behavior and reduced confidence in the ML system’s outputs. Additionally, faults in checkpointing, model serialization, or state management can cause data loss or inconsistencies, affecting the reliability and recoverability of the ML system.\nSecurity Vulnerabilities: Software faults, such as buffer overflows, injection vulnerabilities, or improper access control, can introduce security risks and expose the ML system to potential attacks or unauthorized access. Adversaries may exploit faults in the preprocessing or feature extraction stages to manipulate the input data and deceive the ML models, leading to incorrect or malicious behavior. Furthermore, inadequate protection of sensitive data, such as user information or confidential model parameters, can lead to data breaches or privacy violations (Q. Li et al. 2023).\n\nLi, Qinbin, Zeyi Wen, Zhaomin Wu, Sixu Hu, Naibo Wang, Yuan Li, Xu Liu, and Bingsheng He. 2023. “A Survey on Federated Learning Systems: Vision, Hype and Reality for Data Privacy and Protection.” IEEE Trans. Knowl. Data Eng. 35 (4): 3347–66. https://doi.org/10.1109/tkde.2021.3124599.\nDifficulty in Reproducing and Debugging: Software faults can make it challenging to reproduce and debug issues in ML systems, especially when the faults are intermittent or dependent on specific runtime conditions. Incomplete or ambiguous error messages, coupled with the complexity of ML frameworks and models, can prolong the debugging process and hinder the ability to identify and fix the underlying faults. Moreover, inconsistencies between development, testing, and production environments can make reproducing and diagnosing faults in specific contexts difficult.\nIncreased Development and Maintenance Costs Software faults can lead to increased development and maintenance costs, as teams spend more time and resources debugging, fixing, and validating the ML system. The need for extensive testing, monitoring, and fault-tolerant mechanisms to mitigate the impact of software faults can add complexity and overhead to the ML development process. Frequent patches, updates, and bug fixes to address software faults can disrupt the development workflow and require additional effort to ensure the stability and compatibility of the ML system.\nUnderstanding the potential impact of software faults on ML systems is crucial for prioritizing testing efforts, implementing fault-tolerant designs, and establishing effective monitoring and debugging practices. By proactively addressing software faults and their consequences, ML practitioners can build more robust, reliable, and secure ML systems that deliver accurate and trustworthy results.\n\n\n18.5.4 Detection and Mitigation\nDetecting and mitigating software faults in machine learning frameworks is essential to ensure ML systems’ reliability, performance, and security. Let’s explore various techniques and approaches that can be employed to identify and address software faults effectively:\nThorough Testing and Validation: Comprehensive unit testing of individual components and modules can verify their correctness and identify potential faults early in development. Integration testing validates the interaction and compatibility between different components of the ML framework, ensuring seamless integration. Systematic testing of edge cases, boundary conditions, and exceptional scenarios helps uncover hidden faults and vulnerabilities. Continuous testing and regression testing as shown in Figure 18.35 detect faults introduced by code changes or updates to the ML framework.\n\n\n\n\n\n\nFigure 18.35: Automated regression testing. Source: UTOR\n\n\n\nStatic Code Analysis and Linting: Utilizing static code analysis tools automatically identifies potential coding issues, such as syntax errors, undefined variables, or security vulnerabilities. Enforcing coding standards and best practices through linting tools maintains code quality and reduces the likelihood of common programming mistakes. Conducting regular code reviews allows manual inspection of the codebase, identification of potential faults, and ensures adherence to coding guidelines and design principles.\nRuntime Monitoring and Logging: Implementing comprehensive logging mechanisms captures relevant information during runtime, such as input data, model parameters, and system events. Monitoring key performance metrics, resource utilization, and error rates helps detect anomalies, performance bottlenecks, or unexpected behavior. Employing runtime assertion checks and invariants validates assumptions and detects violations of expected conditions during program execution. Utilizing profiling tools identifies performance bottlenecks, memory leaks, or inefficient code paths that may indicate the presence of software faults.\nFault-Tolerant Design Patterns: Implementing error handling and exception management mechanisms enables graceful handling and recovery from exceptional conditions or runtime errors. Employing redundancy and failover mechanisms, such as backup systems or redundant computations, ensures the availability and reliability of the ML system in the presence of faults. Designing modular and loosely coupled architectures minimizes the propagation and impact of faults across different components of the ML system. Utilizing checkpointing and recovery mechanisms (Eisenman et al. 2022) allows the system to resume from a known stable state in case of failures or interruptions.\n\nEisenman, Assaf, Kiran Kumar Matam, Steven Ingram, Dheevatsa Mudigere, Raghuraman Krishnamoorthi, Krishnakumar Nair, Misha Smelyanskiy, and Murali Annavaram. 2022. “Check-n-Run: A Checkpointing System for Training Deep Learning Recommendation Models.” In 19th USENIX Symposium on Networked Systems Design and Implementation (NSDI 22), 929–43.\nRegular Updates and Patches: Staying up to date with the latest versions and patches of the ML frameworks, libraries, and dependencies provides benefits from bug fixes, security updates, and performance improvements. Monitoring release notes, security advisories, and community forums inform practitioners about known issues, vulnerabilities, or compatibility problems in the ML framework. Establishing a systematic process for testing and validating updates and patches before applying them to production systems ensures stability and compatibility.\nContainerization and Isolation: Leveraging containerization technologies, such as Docker or Kubernetes, encapsulates ML components and their dependencies in isolated environments. Utilizing containerization ensures consistent and reproducible runtime environments across development, testing, and production stages, reducing the likelihood of compatibility issues or environment-specific faults. Employing isolation techniques, such as virtual environments or sandboxing, prevents faults or vulnerabilities in one component from affecting other parts of the ML system.\nAutomated Testing and Continuous Integration/Continuous Deployment (CI/CD): Implement automated testing frameworks and scripts, execute comprehensive test suites, and catch faults early in development. Integrating automated testing into the CI/CD pipeline, as shown in Figure 18.36, ensures that code changes are thoroughly tested before being merged or deployed to production. Utilizing continuous monitoring and automated alerting systems detects and notifies developers and operators about potential faults or anomalies in real-time.\n\n\n\n\n\n\nFigure 18.36: Continuous Integration/Continuous Deployment (CI/CD) procedure. Source: geeksforgeeks\n\n\n\nAdopting a proactive and systematic approach to fault detection and mitigation can significantly improve ML systems’ robustness, reliability, and maintainability. By investing in comprehensive testing, monitoring, and fault-tolerant design practices, organizations can minimize the impact of software faults and ensure their ML systems’ smooth operation in production environments.\n\n\n\n\n\n\nExercise 18.4: Fault Tolerance\n\n\n\n\n\nGet ready to become an AI fault-fighting superhero! Software glitches can derail machine learning systems, but in this Colab, you’ll learn how to make them resilient. We’ll simulate software faults to see how AI can break, then explore techniques to save your ML model’s progress, like checkpoints in a game. You’ll see how to train your AI to bounce back after a crash, ensuring it stays on track. This is crucial for building reliable, trustworthy AI, especially in critical applications. So gear up because this Colab directly connects with the Robust AI chapter—you’ll move from theory to hands-on troubleshooting and build AI systems that can handle the unexpected!",
    "crumbs": [
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Robust AI</span>"
    ]
  },
  {
    "objectID": "contents/core/robust_ai/robust_ai.html#tools-and-frameworks",
    "href": "contents/core/robust_ai/robust_ai.html#tools-and-frameworks",
    "title": "18  Robust AI",
    "section": "18.6 Tools and Frameworks",
    "text": "18.6 Tools and Frameworks\nGiven the importance of developing robust AI systems, in recent years, researchers and practitioners have developed a wide range of tools and frameworks to understand how hardware faults manifest and propagate to impact ML systems. These tools and frameworks play a crucial role in evaluating the resilience of ML systems to hardware faults by simulating various fault scenarios and analyzing their impact on the system’s performance. This enables designers to identify potential vulnerabilities and develop effective mitigation strategies, ultimately creating more robust and reliable ML systems that can operate safely despite hardware faults. This section provides an overview of widely used fault models in the literature and the tools and frameworks developed to evaluate the impact of such faults on ML systems.\n\n18.6.1 Fault Models and Error Models\nAs discussed previously, hardware faults can manifest in various ways, including transient, permanent, and intermittent faults. In addition to the type of fault under study, how the fault manifests is also important. For example, does the fault happen in a memory cell or during the computation of a functional unit? Is the impact on a single bit, or does it impact multiple bits? Does the fault propagate all the way and impact the application (causing an error), or does it get masked quickly and is considered benign? All these details impact what is known as the fault model, which plays a major role in simulating and measuring what happens to a system when a fault occurs.\nTo effectively study and understand the impact of hardware faults on ML systems, it is essential to understand the concepts of fault models and error models. A fault model describes how a hardware fault manifests itself in the system, while an error model represents how the fault propagates and affects the system’s behavior.\nFault models can be categorized based on various characteristics:\n\nDuration: Transient faults occur briefly and then disappear, while permanent faults persist indefinitely. Intermittent faults occur sporadically and may be difficult to diagnose.\nLocation: Faults can occur in hardware parts, such as memory cells, functional units, or interconnects.\nGranularity: Faults can affect a single bit (e.g., bitflip) or multiple bits (e.g., burst errors) within a hardware component.\n\nOn the other hand, error models describe how a fault propagates through the system and manifests as an error. An error may cause the system to deviate from its expected behavior, leading to incorrect results or even system failures. Error models can be defined at different levels of abstraction, from the hardware level (e.g., register-level bitflips) to the software level (e.g., corrupted weights or activations in an ML model).\nThe fault model (or error model, typically the more applicable terminology in understanding the robustness of an ML system) plays a major role in simulating and measuring what happens to a system when a fault occurs. The chosen model informs the assumptions made about the system being studied. For example, a system focusing on single-bit transient errors (Sangchoolie, Pattabiraman, and Karlsson 2017) would not be well-suited to understand the impact of permanent, multi-bit flip errors (Wilkening et al. 2014), as it is designed assuming a different model altogether.\n\nWilkening, Mark, Vilas Sridharan, Si Li, Fritz Previlon, Sudhanva Gurumurthi, and David R. Kaeli. 2014. “Calculating Architectural Vulnerability Factors for Spatial Multi-Bit Transient Faults.” In 2014 47th Annual IEEE/ACM International Symposium on Microarchitecture, 293–305. IEEE; IEEE. https://doi.org/10.1109/micro.2014.15.\nFurthermore, implementing an error model is also an important consideration, particularly regarding where an error is said to occur in the compute stack. For instance, a single-bit flip model at the architectural register level differs from a single-bit flip in the weight of a model at the PyTorch level. Although both target a similar error model, the former would usually be modeled in an architecturally accurate simulator (like gem5 [binkert2011gem5]), which captures error propagation compared to the latter, focusing on value propagation through a model.\nRecent research has shown that certain characteristics of error models may exhibit similar behaviors across different levels of abstraction (Sangchoolie, Pattabiraman, and Karlsson 2017) (Papadimitriou and Gizopoulos 2021). For example, single-bit errors are generally more problematic than multi-bit errors, regardless of whether they are modeled at the hardware or software level. However, other characteristics, such as error masking (Mohanram and Touba 2003) as shown in Figure 18.37, may not always be accurately captured by software-level models, as they can hide underlying system effects.\n\nSangchoolie, Behrooz, Karthik Pattabiraman, and Johan Karlsson. 2017. “One Bit Is (Not) Enough: An Empirical Study of the Impact of Single and Multiple Bit-Flip Errors.” In 2017 47th Annual IEEE/IFIP International Conference on Dependable Systems and Networks (DSN), 97–108. IEEE; IEEE. https://doi.org/10.1109/dsn.2017.30.\n\nPapadimitriou, George, and Dimitris Gizopoulos. 2021. “Demystifying the System Vulnerability Stack: Transient Fault Effects Across the Layers.” In 2021 ACM/IEEE 48th Annual International Symposium on Computer Architecture (ISCA), 902–15. IEEE; IEEE. https://doi.org/10.1109/isca52012.2021.00075.\n\nMohanram, K., and N. A. Touba. 2003. “Partial Error Masking to Reduce Soft Error Failure Rate in Logic Circuits.” In Proceedings. 16th IEEE Symposium on Computer Arithmetic, 433–40. IEEE; IEEE Comput. Soc. https://doi.org/10.1109/dftvs.2003.1250141.\n\n\n\n\n\n\nFigure 18.37: Example of error masking in microarchitectural components (Ko 2021)\n\n\nKo, Yohan. 2021. “Characterizing System-Level Masking Effects Against Soft Errors.” Electronics 10 (18): 2286. https://doi.org/10.3390/electronics10182286.\n\n\nSome tools, such as Fidelity (He, Balaprakash, and Li 2020), aim to bridge the gap between hardware-level and software-level error models by mapping patterns between the two levels of abstraction (Cheng et al. 2016). This allows for more accurate modeling of hardware faults in software-based tools, essential for developing robust and reliable ML systems. Lower-level tools typically represent more accurate error propagation characteristics but must be faster in simulating many errors due to the complex nature of hardware system designs. On the other hand, higher-level tools, such as those implemented in ML frameworks like PyTorch or TensorFlow, which we will discuss soon in the later sections, are often faster and more efficient for evaluating the robustness of ML systems.\n\nCheng, Eric, Shahrzad Mirkhani, Lukasz G. Szafaryn, Chen-Yong Cher, Hyungmin Cho, Kevin Skadron, Mircea R. Stan, et al. 2016. “Clear: uC/u Ross u-l/u Ayer uE/u Xploration for uA/u Rchitecting uR/u Esilience - Combining Hardware and Software Techniques to Tolerate Soft Errors in Processor Cores.” In Proceedings of the 53rd Annual Design Automation Conference, 1–6. ACM. https://doi.org/10.1145/2897937.2897996.\nIn the following subsections, we will discuss various hardware-based and software-based fault injection methods and tools, highlighting their capabilities, limitations, and the fault and error models they support.\n\n\n18.6.2 Hardware-based Fault Injection\nAn error injection tool is a tool that allows the user to implement a particular error model, such as a transient single-bit flip during inference Figure 18.38. Most error injection tools are software-based, as software-level tools are faster for ML robustness studies. However, hardware-based fault injection methods are still important for grounding the higher-level error models, as they are considered the most accurate way to study the impact of faults on ML systems by directly manipulating the hardware to introduce faults. These methods allow researchers to observe the system’s behavior under real-world fault conditions. Both software-based and hardware-based error injection tools are described in this section in more detail.\n\n\n\n\n\n\nFigure 18.38: Hardware errors can occur due to a variety of reasons and at different times and/or locations in a system, which can be explored when studying the impact of hardware-based errors on systems (Ahmadilivani et al. 2024)\n\n\nAhmadilivani, Mohammad Hasan, Mahdi Taheri, Jaan Raik, Masoud Daneshtalab, and Maksim Jenihhin. 2024. “A Systematic Literature Review on Hardware Reliability Assessment Methods for Deep Neural Networks.” ACM Comput. Surv. 56 (6): 1–39. https://doi.org/10.1145/3638242.\n\n\n\nMethods\nTwo of the most common hardware-based fault injection methods are FPGA-based fault injection and radiation or beam testing.\nFPGA-based Fault Injection: Field-Programmable Gate Arrays (FPGAs) are reconfigurable integrated circuits that can be programmed to implement various hardware designs. In the context of fault injection, FPGAs offer high precision and accuracy, as researchers can target specific bits or sets of bits within the hardware. By modifying the FPGA configuration, faults can be introduced at specific locations and times during the execution of an ML model. FPGA-based fault injection allows for fine-grained control over the fault model, enabling researchers to study the impact of different types of faults, such as single-bit flips or multi-bit errors. This level of control makes FPGA-based fault injection a valuable tool for understanding the resilience of ML systems to hardware faults.\nRadiation or Beam Testing: Radiation or beam testing (Velazco, Foucard, and Peronnard 2010) involves exposing the hardware running an ML model to high-energy particles, such as protons or neutrons as illustrated in Figure 18.39. These particles can cause bitflips or other types of faults in the hardware, mimicking the effects of real-world radiation-induced faults. Beam testing is widely regarded as a highly accurate method for measuring the error rate induced by particle strikes on a running application. It provides a realistic representation of the faults in real-world environments, particularly in applications exposed to high radiation levels, such as space systems or particle physics experiments. However, unlike FPGA-based fault injection, beam testing could be more precise in targeting specific bits or components within the hardware, as it might be difficult to aim the beam of particles to a particular bit in the hardware. Despite being quite expensive from a research standpoint, beam testing is a well-regarded industry practice for reliability.\n\nVelazco, Raoul, Gilles Foucard, and Paul Peronnard. 2010. “Combining Results of Accelerated Radiation Tests and Fault Injections to Predict the Error Rate of an Application Implemented in SRAM-Based FPGAs.” IEEE Trans. Nucl. Sci. 57 (6): 3500–3505. https://doi.org/10.1109/tns.2010.2087355.\n\n\n\n\n\n\nFigure 18.39: Radiation test setup for semiconductor components (Lee et al. 2022) Source: JD Instrument\n\n\nLee, Minwoong, Namho Lee, Huijeong Gwon, Jongyeol Kim, Younggwan Hwang, and Seongik Cho. 2022. “Design of Radiation-Tolerant High-Speed Signal Processing Circuit for Detecting Prompt Gamma Rays by Nuclear Explosion.” Electronics 11 (18): 2970. https://doi.org/10.3390/electronics11182970.\n\n\n\n\nLimitations\nDespite their high accuracy, hardware-based fault injection methods have several limitations that can hinder their widespread adoption:\nCost: FPGA-based fault injection and beam testing require specialized hardware and facilities, which can be expensive to set up and maintain. The cost of these methods can be a significant barrier for researchers and organizations with limited resources.\nScalability: Hardware-based methods are generally slower and less scalable than software-based methods. Injecting faults and collecting data on hardware can take time, limiting the number of experiments performed within a given timeframe. This can be particularly challenging when studying the resilience of large-scale ML systems or conducting statistical analyses that require many fault injection experiments.\nFlexibility: Hardware-based methods may not be as flexible as software-based methods in terms of the range of fault models and error models they can support. Modifying the hardware configuration or the experimental setup to accommodate different fault models can be more challenging and time-consuming than software-based methods.\nDespite these limitations, hardware-based fault injection methods remain essential tools for validating the accuracy of software-based methods and for studying the impact of faults on ML systems in realistic settings. By combining hardware-based and software-based methods, researchers can gain a more comprehensive understanding of ML systems’ resilience to hardware faults and develop effective mitigation strategies.\n\n\n\n18.6.3 Software-based Fault Injection Tools\nWith the rapid development of ML frameworks in recent years, software-based fault injection tools have gained popularity in studying the resilience of ML systems to hardware faults. These tools simulate the effects of hardware faults by modifying the software representation of the ML model or the underlying computational graph. The rise of ML frameworks such as TensorFlow, PyTorch, and Keras has facilitated the development of fault injection tools that are tightly integrated with these frameworks, making it easier for researchers to conduct fault injection experiments and analyze the results.\n\nAdvantages and Trade-offs\nSoftware-based fault injection tools offer several advantages over hardware-based methods:\nSpeed: Software-based tools are generally faster than hardware-based methods, as they do not require the modification of physical hardware or the setup of specialized equipment. This allows researchers to conduct more fault injection experiments in a shorter time, enabling more comprehensive analyses of the resilience of ML systems.\nFlexibility: Software-based tools are more flexible than hardware-based methods in terms of the range of fault and error models they can support. Researchers can easily modify the fault injection tool’s software implementation to accommodate different fault models or to target specific components of the ML system.\nAccessibility: Software-based tools are more accessible than hardware-based methods, as they do not require specialized hardware or facilities. This makes it easier for researchers and practitioners to conduct fault injection experiments and study the resilience of ML systems, even with limited resources.\n\n\nLimitations\nSoftware-based fault injection tools also have some limitations compared to hardware-based methods:\nAccuracy: Software-based tools may not always capture the full range of effects that hardware faults can have on the system. As these tools operate at a higher level of abstraction, they may need to catch up on some of the low-level hardware interactions and error propagation mechanisms that can impact the behavior of the ML system.\nFidelity: Software-based tools may provide a different level of Fidelity than hardware-based methods in terms of representing real-world fault conditions. The accuracy of the results obtained from software-based fault injection experiments may depend on how closely the software model approximates the actual hardware behavior.\n\n\nTypes of Fault Injection Tools\nSoftware-based fault injection tools can be categorized based on their target frameworks or use cases. Here, we will discuss some of the most popular tools in each category:\nAres (Reagen et al. 2018), a fault injection tool initially developed for the Keras framework in 2018, emerged as one of the first tools to study the impact of hardware faults on deep neural networks (DNNs) in the context of the rising popularity of ML frameworks in the mid-to-late 2010s. The tool was validated against a DNN accelerator implemented in silicon, demonstrating its effectiveness in modeling hardware faults. Ares provides a comprehensive study on the impact of hardware faults in both weights and activation values, characterizing the effects of single-bit flips and bit-error rates (BER) on hardware structures. Later, the Ares framework was extended to support the PyTorch ecosystem, enabling researchers to investigate hardware faults in a more modern setting and further extending its utility in the field.\n\nReagen, Brandon, Udit Gupta, Lillian Pentecost, Paul Whatmough, Sae Kyu Lee, Niamh Mulholland, David Brooks, and Gu-Yeon Wei. 2018. “Ares: A Framework for Quantifying the Resilience of Deep Neural Networks.” In 2018 55th ACM/ESDA/IEEE Design Automation Conference (DAC), 1–6. IEEE. https://doi.org/10.1109/dac.2018.8465834.\n\nMahmoud, Abdulrahman, Neeraj Aggarwal, Alex Nobbe, Jose Rodrigo Sanchez Vicarte, Sarita V. Adve, Christopher W. Fletcher, Iuri Frosio, and Siva Kumar Sastry Hari. 2020. “PyTorchFI: A Runtime Perturbation Tool for DNNs.” In 2020 50th Annual IEEE/IFIP International Conference on Dependable Systems and Networks Workshops (DSN-w), 25–31. IEEE; IEEE. https://doi.org/10.1109/dsn-w50199.2020.00014.\nPyTorchFI (Mahmoud et al. 2020), a fault injection tool specifically designed for the PyTorch framework, was developed in 2020 in collaboration with Nvidia Research. It enables the injection of faults into the weights, activations, and gradients of PyTorch models, supporting a wide range of fault models. By leveraging the GPU acceleration capabilities of PyTorch, PyTorchFI provides a fast and efficient implementation for conducting fault injection experiments on large-scale ML systems, as shown in Figure 18.40.\n\n\n\n\n\n\nFigure 18.40: Hardware bitflips in ML workloads can cause phantom objects and misclassifications, which can erroneously be used downstream by larger systems, such as in autonomous driving. Shown above is a correct and faulty version of the same image using the PyTorchFI injection framework.\n\n\n\nThe tool’s speed and ease of use have led to widespread adoption in the community, resulting in multiple developer-led projects, such as PyTorchALFI by Intel xColabs, which focuses on safety in automotive environments. Follow-up PyTorch-centric tools for fault injection include Dr. DNA by Meta (Ma et al. 2024) (which further facilitates the Pythonic programming model for ease of use), and the GoldenEye framework (Mahmoud et al. 2022), which incorporates novel numerical datatypes (such as AdaptivFloat (Tambe et al. 2020) and BlockFloat in the context of hardware bit flips.\n\nMa, Dongning, Fred Lin, Alban Desmaison, Joel Coburn, Daniel Moore, Sriram Sankar, and Xun Jiao. 2024. “Dr. DNA: Combating Silent Data Corruptions in Deep Learning Using Distribution of Neuron Activations.” In Proceedings of the 29th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 3, 239–52. ACM. https://doi.org/10.1145/3620666.3651349.\n\nMahmoud, Abdulrahman, Thierry Tambe, Tarek Aloui, David Brooks, and Gu-Yeon Wei. 2022. “GoldenEye: A Platform for Evaluating Emerging Numerical Data Formats in DNN Accelerators.” In 2022 52nd Annual IEEE/IFIP International Conference on Dependable Systems and Networks (DSN), 206–14. IEEE. https://doi.org/10.1109/dsn53405.2022.00031.\n\nTambe, Thierry, En-Yu Yang, Zishen Wan, Yuntian Deng, Vijay Janapa Reddi, Alexander Rush, David Brooks, and Gu-Yeon Wei. 2020. “Algorithm-Hardware Co-Design of Adaptive Floating-Point Encodings for Resilient Deep Learning Inference.” In 2020 57th ACM/IEEE Design Automation Conference (DAC), 1–6. IEEE; IEEE. https://doi.org/10.1109/dac18072.2020.9218516.\n\nChen, Zitao, Niranjhana Narayanan, Bo Fang, Guanpeng Li, Karthik Pattabiraman, and Nathan DeBardeleben. 2020. “TensorFI: A Flexible Fault Injection Framework for TensorFlow Applications.” In 2020 IEEE 31st International Symposium on Software Reliability Engineering (ISSRE), 426–35. IEEE; IEEE. https://doi.org/10.1109/issre5003.2020.00047.\n\nChen, Zitao, Guanpeng Li, Karthik Pattabiraman, and Nathan DeBardeleben. 2019. “iBinFI/i: An Efficient Fault Injector for Safety-Critical Machine Learning Systems.” In Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis. SC ’19. New York, NY, USA: ACM. https://doi.org/10.1145/3295500.3356177.\nTensorFI (Chen et al. 2020), or the TensorFlow Fault Injector, is a fault injection tool developed specifically for the TensorFlow framework. Analogous to Ares and PyTorchFI, TensorFI is considered the state-of-the-art tool for ML robustness studies in the TensorFlow ecosystem. It allows researchers to inject faults into the computational graph of TensorFlow models and study their impact on the model’s performance, supporting a wide range of fault models. One of the key benefits of TensorFI is its ability to evaluate the resilience of various ML models, not just DNNs. Further advancements, such as BinFi (Chen et al. 2019), provide a mechanism to speed up error injection experiments by focusing on the “important” bits in the system, accelerating the process of ML robustness analysis and prioritizing the critical components of a model.\nNVBitFI (T. Tsai et al. 2021), a general-purpose fault injection tool developed by Nvidia for their GPU platforms, operates at a lower level compared to framework-specific tools like Ares, PyTorchFI, and TensorFlow. While these tools focus on various deep learning platforms to implement and perform robustness analysis, NVBitFI targets the underlying hardware assembly code for fault injection. This allows researchers to inject faults into any application running on Nvidia GPUs, making it a versatile tool for studying the resilience of ML systems and other GPU-accelerated applications. By enabling users to inject errors at the architectural level, NVBitFI provides a more general-purpose fault model that is not restricted to just ML models. As Nvidia’s GPU systems are commonly used in many ML-based systems, NVBitFI is a valuable tool for comprehensive fault injection analysis across various applications.\n\nTsai, Timothy, Siva Kumar Sastry Hari, Michael Sullivan, Oreste Villa, and Stephen W. Keckler. 2021. “NVBitFI: Dynamic Fault Injection for GPUs.” In 2021 51st Annual IEEE/IFIP International Conference on Dependable Systems and Networks (DSN), 284–91. IEEE; IEEE. https://doi.org/10.1109/dsn48987.2021.00041.\n\nDomain-specific Examples\nDomain-specific fault injection tools have been developed to address various ML application domains’ unique challenges and requirements, such as autonomous vehicles and robotics. This section highlights three domain-specific fault injection tools: DriveFI and PyTorchALFI for autonomous vehicles and MAVFI for uncrewed aerial vehicles (UAVs). These tools enable researchers to inject hardware faults into these complex systems’ perception, control, and other subsystems, allowing them to study the impact of faults on system performance and safety. The development of these software-based fault injection tools has greatly expanded the capabilities of the ML community to develop more robust and reliable systems that can operate safely and effectively in the presence of hardware faults.\nDriveFI (Jha et al. 2019) is a fault injection tool designed for autonomous vehicles. It enables the injection of hardware faults into the perception and control pipelines of autonomous vehicle systems, allowing researchers to study the impact of these faults on the system’s performance and safety. DriveFI has been integrated with industry-standard autonomous driving platforms, such as Nvidia DriveAV and Baidu Apollo, making it a valuable tool for evaluating the resilience of autonomous vehicle systems.\n\nJha, Saurabh, Subho Banerjee, Timothy Tsai, Siva K. S. Hari, Michael B. Sullivan, Zbigniew T. Kalbarczyk, Stephen W. Keckler, and Ravishankar K. Iyer. 2019. “ML-Based Fault Injection for Autonomous Vehicles: A Case for Bayesian Fault Injection.” In 2019 49th Annual IEEE/IFIP International Conference on Dependable Systems and Networks (DSN), 112–24. IEEE; IEEE. https://doi.org/10.1109/dsn.2019.00025.\n\nGräfe, Ralf, Qutub Syed Sha, Florian Geissler, and Michael Paulitsch. 2023. “Large-Scale Application of Fault Injection into PyTorch Models -an Extension to PyTorchFI for Validation Efficiency.” In 2023 53rd Annual IEEE/IFIP International Conference on Dependable Systems and Networks - Supplemental Volume (DSN-s), 56–62. IEEE; IEEE. https://doi.org/10.1109/dsn-s58398.2023.00025.\nPyTorchALFI (Gräfe et al. 2023) is an extension of PyTorchFI developed by Intel xColabs for the autonomous vehicle domain. It builds upon PyTorchFI’s fault injection capabilities. It adds features specifically tailored for evaluating the resilience of autonomous vehicle systems, such as the ability to inject faults into the camera and LiDAR sensor data.\nMAVFI (Hsiao et al. 2023) is a fault injection tool designed for the robotics domain, specifically for uncrewed aerial vehicles (UAVs). MAVFI is built on top of the Robot Operating System (ROS) framework and allows researchers to inject faults into the various components of a UAV system, such as sensors, actuators, and control algorithms. By evaluating the impact of these faults on the UAV’s performance and stability, researchers can develop more resilient and fault-tolerant UAV systems.\n\nHsiao, Yu-Shun, Zishen Wan, Tianyu Jia, Radhika Ghosal, Abdulrahman Mahmoud, Arijit Raychowdhury, David Brooks, Gu-Yeon Wei, and Vijay Janapa Reddi. 2023. “MAVFI: An End-to-End Fault Analysis Framework with Anomaly Detection and Recovery for Micro Aerial Vehicles.” In 2023 Design, Automation &Amp; Test in Europe Conference &Amp; Exhibition (DATE), 1–6. IEEE; IEEE. https://doi.org/10.23919/date56975.2023.10137246.\nThe development of software-based fault injection tools has greatly expanded the capabilities of researchers and practitioners to study the resilience of ML systems to hardware faults. By leveraging the speed, flexibility, and accessibility of these tools, the ML community can develop more robust and reliable systems that can operate safely and effectively in the presence of hardware faults.\n\n\n\n\n18.6.4 Bridging the Gap between Hardware and Software Error Models\nWhile software-based fault injection tools offer many advantages in speed, flexibility, and accessibility, they may not always accurately capture the full range of effects that hardware faults can have on the system. This is because software-based tools operate at a higher level of abstraction than hardware-based methods and may miss some of the low-level hardware interactions and error propagation mechanisms that can impact the behavior of the ML system.\nAs Bolchini et al. (2023) illustrates in their work, hardware errors can manifest in complex spatial distribution patterns that are challenging to fully replicate with software-based fault injection alone. They identify four distinct patterns: (a) single point, where the fault corrupts a single value in a feature map; (b) same row, where the fault corrupts a partial or entire row in a single feature map; (c) bullet wake, where the fault corrupts the same location across multiple feature maps; and (d) shatter glass, which combines the effects of same row and bullet wake patterns, as shown in Figure 18.41. These intricate error propagation mechanisms highlight the need for hardware-aware fault injection techniques to accurately assess the resilience of ML systems.\n\n\n\n\n\n\nFigure 18.41: Hardware errors may manifest themselves in different ways at the software level, as classified by Bolchini et al. (Bolchini et al. 2023)\n\n\nBolchini, Cristiana, Luca Cassano, Antonio Miele, and Alessandro Toschi. 2023. “Fast and Accurate Error Simulation for CNNs Against Soft Errors.” IEEE Trans. Comput. 72 (4): 984–97. https://doi.org/10.1109/tc.2022.3184274.\n\n\nResearchers have developed tools to address this issue by bridging the gap between low-level hardware error models and higher-level software error models. One such tool is Fidelity, designed to map patterns between hardware-level faults and their software-level manifestations.\n\nFidelity: Bridging the Gap\nFidelity (He, Balaprakash, and Li 2020) is a tool for accurately modeling hardware faults in software-based fault injection experiments. It achieves this by carefully studying the relationship between hardware-level faults and their impact on the software representation of the ML system.\n\nHe, Yi, Prasanna Balaprakash, and Yanjing Li. 2020. “FIdelity: Efficient Resilience Analysis Framework for Deep Learning Accelerators.” In 2020 53rd Annual IEEE/ACM International Symposium on Microarchitecture (MICRO), 270–81. IEEE; IEEE. https://doi.org/10.1109/micro50266.2020.00033.\nThe key insights behind Fidelity are:\n\nFault Propagation: Fidelity models how faults propagate through the hardware and manifest as errors in the system’s state that is visible to software. By understanding these propagation patterns, Fidelity can more accurately simulate the effects of hardware faults in software-based experiments.\nFault Equivalence: Fidelity identifies equivalent classes of hardware faults that produce similar software-level errors. This allows researchers to design software-based fault models that are representative of the underlying hardware faults without the need to model every possible hardware fault individually.\nLayered Approach: Fidelity employs a layered approach to fault modeling, where the effects of hardware faults are propagated through multiple levels of abstraction, from the hardware to the software level. This approach ensures that the software-based fault models are grounded in the actual behavior of the hardware.\n\nBy incorporating these insights, Fidelity enables software-based fault injection tools to capture the effects of hardware faults on ML systems accurately. This is particularly important for safety-critical applications, where the system’s resilience to hardware faults is paramount.\n\n\nImportance of Capturing True Hardware Behavior\nCapturing true hardware behavior in software-based fault injection tools is crucial for several reasons:\n\nAccuracy: By accurately modeling the effects of hardware faults, software-based tools can provide more reliable insights into the resilience of ML systems. This is essential for designing and validating fault-tolerant systems that can operate safely and effectively in the presence of hardware faults.\nReproducibility: When software-based tools accurately capture hardware behavior, fault injection experiments become more reproducible across different platforms and environments. This is important for the scientific study of ML system resilience, as it allows researchers to compare and validate results across different studies and implementations.\nEfficiency: Software-based tools that capture true hardware behavior can be more efficient in their fault injection experiments by focusing on the most representative and impactful fault models. This allows researchers to cover a wider range of fault scenarios and system configurations with limited computational resources.\nMitigation Strategies: Understanding how hardware faults manifest at the software level is crucial for developing effective mitigation strategies. By accurately capturing hardware behavior, software-based fault injection tools can help researchers identify the most vulnerable components of the ML system and design targeted hardening techniques to improve resilience.\n\nTools like Fidelity are vital in advancing the state-of-the-art in ML system resilience research. These tools enable researchers to conduct more accurate, reproducible, and efficient fault injection experiments by bridging the gap between hardware and software error models. As the complexity and criticality of ML systems continue to grow, the importance of capturing true hardware behavior in software-based fault injection tools will only become more apparent.\nOngoing research in this area seeks to refine the mapping between hardware and software error models and develop new techniques for efficiently simulating hardware faults in software-based experiments. As these tools mature, they will provide the ML community with increasingly powerful and accessible means to study and improve the resilience of ML systems to hardware faults.",
    "crumbs": [
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Robust AI</span>"
    ]
  },
  {
    "objectID": "contents/core/robust_ai/robust_ai.html#conclusion",
    "href": "contents/core/robust_ai/robust_ai.html#conclusion",
    "title": "18  Robust AI",
    "section": "18.7 Conclusion",
    "text": "18.7 Conclusion\nDeveloping robust and resilient AI is paramount as machine learning systems become increasingly integrated into safety-critical applications and real-world environments. This chapter has explored the key challenges to AI robustness arising from hardware faults, malicious attacks, distribution shifts, and software bugs.\nSome of the key takeaways include the following:\n\nHardware Faults: Transient, permanent, and intermittent faults in hardware components can corrupt computations and degrade the performance of machine learning models if not properly detected and mitigated. Techniques such as redundancy, error correction, and fault-tolerant designs play a crucial role in building resilient ML systems that can withstand hardware faults.\nModel Robustness: Malicious actors can exploit vulnerabilities in ML models through adversarial attacks and data poisoning, aiming to induce targeted misclassifications, skew the model’s learned behavior, or compromise the system’s integrity and reliability. Also, distribution shifts can occur when the data distribution encountered during deployment differs from those seen during training, leading to performance degradation. Implementing defensive measures, including adversarial training, anomaly detection, robust model architectures, and techniques such as domain adaptation, transfer learning, and continual learning, is essential to safeguard against these challenges and ensure the model’s reliability and generalization in dynamic environments.\nSoftware Faults: Faults in ML frameworks, libraries, and software stacks can propagate errors, degrade performance, and introduce security vulnerabilities. Rigorous testing, runtime monitoring, and adopting fault-tolerant design patterns are essential for building robust software infrastructure supporting reliable ML systems.\n\nAs ML systems take on increasingly complex tasks with real-world consequences, prioritizing resilience becomes critical. The tools and frameworks discussed in this chapter, including fault injection techniques, error analysis methods, and robustness evaluation frameworks, provide practitioners with the means to thoroughly test and harden their ML systems against various failure modes and adversarial conditions.\nMoving forward, resilience must be a central focus throughout the entire AI development lifecycle, from data collection and model training to deployment and monitoring. By proactively addressing the multifaceted challenges to robustness, we can develop trustworthy, reliable ML systems that can navigate the complexities and uncertainties of real-world environments.\nFuture research in robust ML should continue to advance techniques for detecting and mitigating faults, attacks, and distributional shifts. Additionally, exploring novel paradigms for developing inherently resilient AI architectures, such as self-healing systems or fail-safe mechanisms, will be crucial in pushing the boundaries of AI robustness. By prioritizing resilience and investing in developing robust AI systems, we can unlock the full potential of machine learning technologies while ensuring their safe, reliable, and responsible deployment in real-world applications. As AI continues to shape our future, building resilient systems that can withstand the challenges of the real world will be a defining factor in the success and societal impact of this transformative technology.",
    "crumbs": [
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Robust AI</span>"
    ]
  },
  {
    "objectID": "contents/core/robust_ai/robust_ai.html#sec-robust-ai-resource",
    "href": "contents/core/robust_ai/robust_ai.html#sec-robust-ai-resource",
    "title": "18  Robust AI",
    "section": "18.8 Resources",
    "text": "18.8 Resources\nHere is a curated list of resources to support students and instructors in their learning and teaching journeys. We are continuously working on expanding this collection and will add new exercises soon.\n\n\n\n\n\n\nSlides\n\n\n\n\n\nThese slides are a valuable tool for instructors to deliver lectures and for students to review the material at their own pace. We encourage both students and instructors to leverage these slides to improve their understanding and facilitate effective knowledge transfer.\n\nComing soon.\n\n\n\n\n\n\n\n\n\n\nVideos\n\n\n\n\n\n\nComing soon.\n\n\n\n\n\n\n\n\n\n\nExercises\n\n\n\n\n\nTo reinforce the concepts covered in this chapter, we have curated a set of exercises that challenge students to apply their knowledge and deepen their understanding.\n\nExercise 18.1\nExercise 18.2\nExercise 18.3\nExercise 18.4",
    "crumbs": [
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Robust AI</span>"
    ]
  },
  {
    "objectID": "contents/core/ai_for_good/ai_for_good.html",
    "href": "contents/core/ai_for_good/ai_for_good.html",
    "title": "19  AI for Good",
    "section": "",
    "text": "Purpose\nResources: Slides, Videos, Exercises\nHow can we harness machine learning systems to address critical societal challenges, and what principles guide the development of solutions that create lasting positive impact?\nThe application of AI systems to societal challenges represents the culmination of technical capability and social responsibility. Impact-driven development reveals essential patterns for translating technological potential into meaningful change, highlighting critical relationships between system design and societal outcomes. The implementation of solutions for social good showcases pathways for addressing complex challenges while maintaining technical rigor and operational effectiveness. Understanding these impact dynamics provides insights into creating transformative systems, establishing principles for designing AI solutions that advance human welfare, and promote positive societal transformation.",
    "crumbs": [
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>AI for Good</span>"
    ]
  },
  {
    "objectID": "contents/core/ai_for_good/ai_for_good.html#purpose",
    "href": "contents/core/ai_for_good/ai_for_good.html#purpose",
    "title": "19  AI for Good",
    "section": "",
    "text": "Learning Objectives\n\n\n\n\nExplore how AI systems can address critical real-world societal challenges.\nRecognize key design patterns for ML systems in social impact.\nSelect suitable design patterns based on resource availability and adaptability needs.\nExplore how Cloud ML, Edge ML, Mobile ML, and Tiny ML integrate into these patterns.\nEvaluate the strengths and limitations of design patterns for specific deployment scenarios.",
    "crumbs": [
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>AI for Good</span>"
    ]
  },
  {
    "objectID": "contents/core/ai_for_good/ai_for_good.html#overview",
    "href": "contents/core/ai_for_good/ai_for_good.html#overview",
    "title": "19  AI for Good",
    "section": "19.1 Overview",
    "text": "19.1 Overview\nPrevious chapters examined the fundamental components of machine learning systems - from neural architectures and training methodologies to acceleration techniques and deployment strategies. These chapters established how to build, optimize, and operate ML systems at scale. The examples and techniques focused primarily on scenarios where computational resources, reliable infrastructure, and technical expertise were readily available.\nMachine learning systems, however, extend beyond commercial and industrial applications. While recommendation engines, computer vision systems, and natural language processors drive business value, ML systems also hold immense potential for addressing pressing societal challenges. This potential remains largely unrealized due to the distinct challenges of deploying ML systems in resource-constrained environments.\nEngineering ML systems for social impact differs fundamentally from commercial deployments. These systems must operate in environments with limited computing resources, intermittent connectivity, and minimal technical support infrastructure. Such constraints reshape every aspect of ML system design—from model architecture and training approaches to deployment patterns and maintenance strategies. Success requires rethinking traditional ML system design patterns to create solutions that are robust, maintainable, and effective despite these limitations.\n\nBuilding ML systems for AI for social good is an engineering challenge.\n\nThis chapter highlights some AI applications for social good and examines the unique requirements, constraints, and opportunities in engineering ML systems for social impact. We analyze how core ML system components adapt to resource-constrained environments, explore architectural patterns that enable robust deployment across the computing spectrum, and study real-world implementations in healthcare, agriculture, education, and environmental monitoring. Through these examples and the discussions involved, we develop frameworks for designing ML systems that deliver sustainable social impact.",
    "crumbs": [
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>AI for Good</span>"
    ]
  },
  {
    "objectID": "contents/core/ai_for_good/ai_for_good.html#global-challenges",
    "href": "contents/core/ai_for_good/ai_for_good.html#global-challenges",
    "title": "19  AI for Good",
    "section": "19.2 Global Challenges",
    "text": "19.2 Global Challenges\nHistory provides sobering examples of where timely interventions and coordinated responses could have dramatically altered outcomes. The 2014-2016 Ebola outbreak in West Africa, for instance, highlighted the catastrophic consequences of delayed detection and response systems (WHO). Similarly, the 2011 famine in Somalia, despite being forecasted months in advance, caused immense suffering due to inadequate mechanisms to mobilize and allocate resources effectively (ReliefWeb). In the aftermath of the 2010 Haiti earthquake, the lack of rapid and reliable damage assessment significantly hampered efforts to direct aid where it was most needed (USGS).\nToday, similar challenges persist across diverse domains, particularly in resource-constrained environments. In healthcare, remote and underserved communities often experience preventable health crises due to the absence of timely access to medical expertise. A lack of diagnostic tools and specialists means that treatable conditions can escalate into life-threatening situations, creating unnecessary suffering and loss of life. Agriculture, a sector critical to global food security, faces parallel struggles. Smallholder farmers, responsible for producing much of the world’s food, make crucial decisions with limited information. Increasingly erratic weather patterns, pest outbreaks, and soil degradation compound their difficulties, often resulting in reduced yields and heightened food insecurity, particularly in vulnerable regions. These challenges demonstrate how systemic barriers and resource constraints perpetuate inequities and undermine resilience.\nSimilar systemic barriers are evident in education, where inequity further amplifies challenges in underserved areas. Many schools lack sufficient teachers, adequate resources, and personalized support for students. This not only widens the gap between advantaged and disadvantaged learners but also creates long-term consequences for social and economic development. Without access to quality education, entire communities are left at a disadvantage, perpetuating cycles of poverty and inequality. These inequities are deeply interconnected with broader challenges, as gaps in education often exacerbate issues in other critical sectors such as healthcare and agriculture.\nThe strain on ecosystems introduces another dimension to these challenges. Environmental degradation, including deforestation, pollution, and biodiversity loss, threatens livelihoods and destabilizes the ecological balance necessary for sustaining human life. Vast stretches of forests, oceans, and wildlife habitats remain unmonitored and unprotected, particularly in regions with limited resources. This leaves ecosystems vulnerable to illegal activities such as poaching, logging, and pollution, further intensifying the pressures on communities already grappling with economic and social disparities. These interwoven challenges underscore the need for holistic solutions that address both human and environmental vulnerabilities.\nAlthough these issues vary in scope and scale, they share several critical characteristics. They disproportionately affect vulnerable populations, exacerbating existing inequalities. Resource constraints in affected regions pose significant barriers to implementing solutions. Moreover, addressing these challenges requires navigating trade-offs between competing priorities and limited resources, often under conditions of great uncertainty.\nTechnology holds the potential to play a transformative role in addressing these issues. By providing innovative tools to enhance decision-making, increase efficiency, and deliver solutions at scale, it offers hope for overcoming the barriers that have historically hindered progress. Among these technologies, machine learning systems stand out for their capacity to process vast amounts of information, uncover patterns, and generate insights that can inform action in even the most resource-constrained environments. However, realizing this potential requires deliberate and systematic approaches to ensure these tools are designed and implemented to serve the needs of all communities effectively and equitably.",
    "crumbs": [
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>AI for Good</span>"
    ]
  },
  {
    "objectID": "contents/core/ai_for_good/ai_for_good.html#spotlight-ai-applications",
    "href": "contents/core/ai_for_good/ai_for_good.html#spotlight-ai-applications",
    "title": "19  AI for Good",
    "section": "19.3 Spotlight AI Applications",
    "text": "19.3 Spotlight AI Applications\nAI technologies—including Cloud ML, Mobile ML, Edge ML, and Tiny ML—are unlocking transformative solutions to some of the world’s most pressing challenges. By adapting to diverse constraints and leveraging unique strengths, these technologies are driving innovation in agriculture, healthcare, disaster response, and environmental conservation. This section explores how these paradigms bring social good to life through real-world applications.\n\n19.3.1 Agriculture\n\n\n\n\n\n\nImportant 19.1: Plant Village Nuru\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 19.1: AI helps farmers to detect plant diseases.\n\n\n\nIn Sub-Saharan Africa, cassava farmers have long battled diseases that devastate crops and livelihoods. Now, with the help of mobile ML-powered smartphone apps, as shown in Figure 19.1, they can snap a photo of a leaf and receive instant feedback on potential diseases. This early detection system has reduced cassava losses from 40% to just 5%, offering hope to farmers in disconnected regions where access to agricultural advisors is limited (Ramcharan et al. 2017).\n\nRamcharan, Amanda, Kelsee Baranowski, Peter McCloskey, Babuali Ahmed, James Legg, and David P. Hughes. 2017. “Deep Learning for Image-Based Cassava Disease Detection.” Frontiers in Plant Science 8 (October): 1852. https://doi.org/10.3389/fpls.2017.01852.\n\nTirtalistyani, Rose, Murtiningrum Murtiningrum, and Rameshwar S. Kanwar. 2022. “Indonesia Rice Irrigation System: Time for Innovation.” Sustainability 14 (19): 12477. https://doi.org/10.3390/su141912477.\nAcross Southeast Asia, rice farmers are confronting increasingly unpredictable weather patterns. In Indonesia, Tiny ML sensors are transforming their ability to adapt by monitoring microclimates across paddies. These low-power devices process data locally to optimize water usage, enabling precision irrigation even in areas with minimal infrastructure (Tirtalistyani, Murtiningrum, and Kanwar 2022).\nOn a global scale, Microsoft’s FarmBeats is pioneering the integration of IoT sensors, drones, and Cloud ML to create actionable insights for farmers. By leveraging weather forecasts, soil conditions, and crop health data, the platform allows farmers to optimize inputs like water and fertilizer, reducing waste and improving yields. Together, these innovations illustrate how AI technologies are bringing precision agriculture to life, addressing food security, sustainability, and climate resilience.\n\n\n19.3.2 Healthcare\nFor millions in underserved communities, access to healthcare often means long waits and travel to distant clinics. Tiny ML is changing that by enabling diagnostics to occur at the patient’s side. For example, a low-cost wearable developed by Respira xColabs uses embedded machine learning to analyze cough patterns and detect pneumonia. Designed for remote areas, the device operates independently of internet connectivity and is powered by a simple microcontroller, making life-saving diagnostics accessible to those who need it most.\nTiny ML’s potential extends to tackling global health issues like vector-borne diseases that are spread by mosquitoes. Researchers have developed low-cost devices that use machine learning to identify mosquito species by their wingbeat frequencies (Altayeb, Zennaro, and Rovai 2022). This technology enables real-time monitoring of malaria-carrying mosquitoes. It offers a scalable solution for malaria control in high-risk regions.\n\nAltayeb, Moez, Marco Zennaro, and Marcelo Rovai. 2022. “Classifying Mosquito Wingbeat Sound Using TinyML.” In Proceedings of the 2022 ACM Conference on Information Technology for Social Good, 132–37. ACM. https://doi.org/10.1145/3524458.3547258.\nIn parallel, Cloud ML is advancing healthcare research and diagnostics on a broader scale. Platforms like Google Genomics analyze vast datasets to identify disease markers, accelerating breakthroughs in personalized medicine. These examples show how AI technologies—from Tiny ML’s portability to Cloud ML’s computational power—are converging to democratize healthcare access and improve outcomes worldwide.\n\n\n19.3.3 Disaster Response\nIn disaster zones, where every second counts, AI technologies are providing tools to accelerate response efforts and enhance safety. Tiny, autonomous drones equipped with Tiny ML algorithms are making their way into collapsed buildings, navigating obstacles to detect signs of life. By analyzing thermal imaging and acoustic signals locally, these drones can identify survivors and hazards without relying on cloud connectivity (Duisterhof et al. 2021). Video 19.2 and Video 19.3 show how Tiny ML algorithms can be used to enable drones to autonomously seek light and gas sources.\n\nDuisterhof, Bardienus P., Shushuai Li, Javier Burgues, Vijay Janapa Reddi, and Guido C. H. E. de Croon. 2021. “Sniffy Bug: A Fully Autonomous Swarm of Gas-Seeking Nano Quadcopters in Cluttered Environments.” In 2021 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), 9099–9106. IEEE; IEEE. https://doi.org/10.1109/iros51168.2021.9636217.\n\n\n\n\n\n\n\n\n\n\n\n\nImportant 19.2: Light Seeking\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nImportant 19.3: Gas Seeking\n\n\n\n\n\n\n\n\n\nAt a broader level, platforms like Google’s AI for Disaster Response are leveraging Cloud ML to process satellite imagery and predict flood zones. These systems provide real-time insights to help governments allocate resources more effectively and save lives during emergencies.\nMobile ML applications are also playing a critical role by delivering real-time disaster alerts directly to smartphones. Tsunami warnings and wildfire updates tailored to users’ locations enable faster evacuations and better preparedness. Whether scaling globally with Cloud ML or enabling localized insights with Edge and Mobile ML, these technologies are redefining disaster response capabilities.\n\n\n19.3.4 Environmental Conservation\nConservationists face immense challenges in monitoring and protecting biodiversity across vast and often remote landscapes. AI technologies are offering scalable solutions to these problems, combining local autonomy with global coordination.\n\n\n\n\n\n\nImportant 19.4: Elephant Edge\n\n\n\n\n\n\nEdgeML-powered collars are being used to unobtrusively track animal behavior, such as elephant movements and vocalizations (Video 19.4). By processing data on the collar itself, these devices minimize power consumption and reduce the need for frequent battery changes (Verma 2022). Meanwhile, Tiny ML systems are enabling anti-poaching efforts by detecting threats like gunshots or human activity and relaying alerts to rangers in real time (Bamoumen et al. 2022).\n\nVerma, Team Dual_Boot: Swapnil. 2022. “Elephant AI.” Hackster.io. https://www.hackster.io/dual\\_boot/elephant-ai-ba71e9.\n\nBamoumen, Hatim, Anas Temouden, Nabil Benamar, and Yousra Chtouki. 2022. “How TinyML Can Be Leveraged to Solve Environmental Problems: A Survey.” In 2022 International Conference on Innovation and Intelligence for Informatics, Computing, and Technologies (3ICT), 338–43. IEEE; IEEE. https://doi.org/10.1109/3ict56508.2022.9990661.\nAt a global scale, Cloud ML is being used to monitor illegal fishing activities. Platforms like Global Fishing Watch analyze satellite data to detect anomalies, helping governments enforce regulations and protect marine ecosystems. These examples highlight how AI technologies are enabling real-time monitoring and decision-making, advancing conservation efforts in profound ways.\n\n\n19.3.5 A Holistic View of AI’s Impact\nThe examples highlighted above demonstrate the transformative potential of AI technologies in addressing critical societal challenges. However, these successes also underscore the complexity of tackling such problems holistically. Each example addresses specific needs—optimizing agricultural resources, expanding healthcare access, or protecting ecosystems—but solving these issues sustainably requires more than isolated innovations.\nTo maximize impact and ensure equitable progress, collective efforts are essential. Large-scale challenges demand collaboration across sectors, geographies, and stakeholders. By fostering coordination between local initiatives, research institutions, and global organizations, we can align AI’s transformative potential with the infrastructure and policies needed to scale solutions effectively. Without such alignment, even the most promising innovations risk operating in silos, limiting their reach and long-term sustainability.\nTo address this, we require frameworks that help harmonize efforts and prioritize initiatives that deliver broad, lasting impact. These frameworks serve as roadmaps to bridge the gap between technological potential and meaningful global progress.",
    "crumbs": [
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>AI for Good</span>"
    ]
  },
  {
    "objectID": "contents/core/ai_for_good/ai_for_good.html#global-development-context",
    "href": "contents/core/ai_for_good/ai_for_good.html#global-development-context",
    "title": "19  AI for Good",
    "section": "19.4 Global Development Context",
    "text": "19.4 Global Development Context\nThe sheer scale and complexity of these problems demand a systematic approach to ensure that efforts are targeted, coordinated, and sustainable. This is where global frameworks such as the United Nations Sustainable Development Goals (SDGs) and guidance from institutions like the World Health Organization (WHO) play a pivotal role. These frameworks provide a structured lens for thinking about and addressing the world’s most pressing challenges. They offer a roadmap to align efforts, set priorities, and foster international collaboration to create impactful and lasting change (The Sustainable Development Goals Report 2018 2018).\n\nThe Sustainable Development Goals Report 2018. 2018. New York: United Nations. https://doi.org/10.18356/7d014b41-en.\nThe United Nations Sustainable Development Goals (SDGs), shown in Figure 19.2, are a global agenda adopted in 2015. These 17 interconnected goals form a blueprint for addressing the world’s most pressing challenges by 2030. They range from eliminating poverty and hunger to ensuring quality education, from promoting gender equality to taking climate action.\n\n\n\n\n\n\nFigure 19.2: United Nations Sustainable Development Goals (SDG). Source: United Nations.\n\n\n\nMachine learning systems can contribute to multiple SDGs simultaneously through their transformative capabilities:\n\nGoal 1 (No Poverty) & Goal 10 (Reduced Inequalities): ML systems that improve financial inclusion through mobile banking and risk assessment for microloans.\nGoals 2, 12, & 15 (Zero Hunger, Responsible Consumption, Life on Land): Systems that optimize resource distribution, reduce waste in food supply chains, and monitor biodiversity.\nGoals 3 & 5 (Good Health and Gender Equality): ML applications that improve maternal health outcomes and access to healthcare in underserved communities.\nGoals 13 & 11 (Climate Action & Sustainable Cities): Predictive systems for climate resilience and urban planning that help communities adapt to environmental changes.\n\nHowever, deploying these systems presents unique challenges. Many regions that could benefit most from machine learning applications lack reliable electricity (Goal 7: Affordable and Clean Energy) or internet infrastructure (Goal 9: Industry, Innovation and Infrastructure). This reality forces us to rethink how we design machine learning systems for social impact.\nSuccess in advancing the SDGs through machine learning requires a holistic approach that goes beyond technical solutions. Systems must operate within local resource constraints while respecting cultural contexts and existing infrastructure limitations. This reality pushes us to fundamentally rethink system design, considering not just technological capabilities but also their sustainable integration into communities that need them most.\nThe following sections explore how to navigate these technical, infrastructural, and societal factors to create ML systems that genuinely advance sustainable development goals without creating new dependencies or deepening existing inequalities.",
    "crumbs": [
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>AI for Good</span>"
    ]
  },
  {
    "objectID": "contents/core/ai_for_good/ai_for_good.html#engineering-challenges",
    "href": "contents/core/ai_for_good/ai_for_good.html#engineering-challenges",
    "title": "19  AI for Good",
    "section": "19.5 Engineering Challenges",
    "text": "19.5 Engineering Challenges\nDeploying machine learning systems in social impact contexts requires us to navigate a series of interconnected challenges spanning computational, networking, power, and data dimensions. These challenges are particularly pronounced when transitioning from development to production environments or scaling deployments in resource-constrained settings.\nTo provide an overview, Table 19.1 summarizes the key differences in resources and requirements across development, rural, and urban contexts, while also highlighting the unique constraints encountered during scaling. This comparison provides a basis for understanding the paradoxes, dilemmas, and constraints that will be explored in subsequent sections.\n\n\n\nTable 19.1: Comparison of resource constraints and challenges across rural deployments, urban deployments, and scaling in machine learning systems for social impact contexts.\n\n\n\n\n\n\n\n\n\n\n\nAspect\nRural Deployment\nUrban Deployment\nScaling Challenges\n\n\n\n\nComputational Resources\nMicrocontroller (ESP32: 240MHz, 520KB RAM)\nServer-grade systems (100-200W, 32-64GB RAM)\nAggressive model quantization (e.g., 50MB to 500KB)\n\n\nPower Infrastructure\nSolar and battery systems (10-20W, 2000-3000mAh battery)\nStable grid power\nOptimized power usage (for deployment devices)\n\n\nNetwork Bandwidth\nLoRa, NB-IoT (0.3-50 kbps, 60-250 kbps)\nHigh-bandwidth options\nProtocol adjustments (LoRa, NB-IoT, Sigfox: 100-600 bps)\n\n\nData Availability\nSparse, heterogeneous data sources (500KB/day from rural clinics)\nLarge volumes of standardized data (Gigabytes from urban hospitals)\nSpecialized pipelines (For privacy-sensitive data)\n\n\nModel Footprint\nHighly quantized models (≤1MB)\nCloud/edge systems (Supporting larger models)\nModel architecture redesign (For size, power, and bandwidth limits)\n\n\n\n\n\n\n\n19.5.1 The Resource Paradox\nDeploying machine learning systems in social impact contexts reveals a fundamental resource paradox that shapes every aspect of system design. While areas with the greatest needs could benefit most from machine learning capabilities, they often lack the basic infrastructure required for traditional deployments.\nThis paradox becomes evident in the computational and power requirements of machine learning systems, as shown in Table 19.1. A typical cloud deployment might utilize servers consuming 100-200W of power with multiple CPU cores and 32-64GB of RAM. However, rural deployments must often operate on single-board computers drawing 5W or microcontrollers consuming mere milliwatts, with RAM measured in kilobytes rather than gigabytes.\nNetwork infrastructure limitations further constrain system design. Urban environments offer high-bandwidth options like fiber (100+ Mbps) and 5G networks (1-10 Gbps). Rural deployments must instead rely on low-power wide-area network technologies such as LoRa or NB-IoT1, which achieve kilometer-range coverage with minimal power consumption.\n1 NB-IoT (Narrowband Internet of Things): NB-IoT is a low-power, wide-area wireless communication technology optimized for connecting IoT devices with minimal energy usage, often in resource-constrained environments.Power infrastructure presents additional challenges. While urban systems can rely on stable grid power, rural deployments often depend on solar charging and battery systems. A typical solar-powered system might generate 10-20W during peak sunlight hours, requiring careful power budgeting across all system components. Battery capacity limitations, often 2000-3000 mAh, mean systems must optimize every aspect of operation, from sensor sampling rates to model inference frequency.\n\n\n19.5.2 The Data Dilemma\nBeyond just computational horserpower, machine learning systems in social impact contexts face fundamental data challenges that differ significantly from commercial deployments. Where commercial systems often work with standardized datasets containing millions of examples, social impact projects must build robust systems with limited, heterogeneous data sources.\nHealthcare deployments illustrate these data constraints clearly. A typical rural clinic might generate 50-100 patient records per day, combining digital entries with handwritten notes. These records often mix structured data like vital signs with unstructured observations, requiring specialized preprocessing pipelines. The total data volume might reach only 500KB per day. This is a stark contrast to urban hospitals generating gigabytes of standardized electronic health records. Even an X-ray or MRI scan is measured in megabytes or more, underscoring the vast disparity in data scales between rural and urban healthcare facilities.\nNetwork limitations further constrain data collection and processing. Agricultural sensor networks, operating on limited power budgets, might transmit only 100-200 bytes per reading. With LoRa2 bandwidth constraints of 50kbps, these systems often limit transmission frequency to once per hour. A network of 1000 sensors thus generates only 4-5MB of data per day, requiring models to learn from sparse temporal data. For perspective, streaming a single minute of video on Netflix can consume several megabytes, highlighting the disparity in data volumes between industrial IoT networks and everyday internet usage.\n2 LoRa (Long Range): LoRA is a low-power wireless communication protocol designed for transmitting small data packets over long distances with minimal energy consumption.Privacy considerations add another layer of complexity. Protecting sensitive information while operating within hardware constraints requires careful system design. Implementing privacy-preserving techniques on devices with 512KB RAM means partitioning models and data carefully. Local processing must balance privacy requirements against hardware limitations, often restricting model sizes to under 1MB. Supporting multiple regional variants of these models can quickly consume the limited storage available on low-cost devices, typically 2-4MB total.\n\n\n19.5.3 The Scale Challenge\nScaling machine learning systems from prototype to production deployment introduces fundamental resource constraints that necessitate architectural redesign. Development environments provide computational resources that mask many real-world limitations. A typical development platform, such as a Raspberry Pi 4, offers substantial computing power with its 1.5GHz processor and 4GB RAM. These resources enable rapid prototyping and testing of machine learning models without immediate concern for optimization.\nProduction deployments reveal stark resource limitations. When scaling to thousands of devices, cost and power constraints often mandate the use of microcontroller units like the ESP32, a widely used microcontroller unit from Espressif Systems, with its 240MHz processor and mere 520KB of RAM. This dramatic reduction in computational resources demands fundamental changes in system architecture. Models must be redesigned, optimization techniques such as quantization and pruning applied, and inference strategies reconsidered.\nNetwork infrastructure constraints fundamentally influence system architecture at scale. Different deployment contexts necessitate different communication protocols, each with distinct operational parameters. This heterogeneity in network infrastructure requires systems to maintain consistent performance across varying bandwidth and latency conditions. As deployments scale across regions, system architectures must accommodate seamless transitions between network technologies while preserving functionality.\nThe transformation from development to scaled deployment presents consistent patterns across application domains. Environmental monitoring systems exemplify these scaling requirements. A typical forest monitoring system might begin with a 50MB computer vision model running on a development platform. Scaling to widespread deployment necessitates reducing the model to approximately 500KB through quantization and architectural optimization, enabling operation on distributed sensor nodes. This reduction in model footprint must preserve detection accuracy while operating within strict power constraints of 1-2W. Similar architectural transformations occur in agricultural monitoring systems and educational platforms, where models must be optimized for deployment across thousands of resource-constrained devices while maintaining system efficacy.\n\n\n19.5.4 The Sustainability Problem\nMaintaining machine learning systems in resource-constrained environments presents distinct challenges that extend beyond initial deployment considerations. These challenges encompass system longevity, environmental impact, community capacity, and financial viability—factors that ultimately determine the long-term success of social impact initiatives.\nSystem longevity requires careful consideration of hardware durability and maintainability. Environmental factors such as temperature variations (typically -20°C to 50°C in rural deployments), humidity (often 80-95% in tropical regions), and dust exposure significantly impact component lifetime. These conditions necessitate robust hardware selection and protective measures that balance durability against cost constraints. For instance, solar-powered agricultural monitoring systems must maintain consistent operation despite seasonal variations in solar irradiance3, typically ranging from 3-7 kWh/m²/day depending on geographical location and weather patterns.\n3 Solar Irradiance: The power per unit area received from the Sun in the form of electromagnetic radiation, typically measured in watts per square meter (W/m²). It varies with geographic location, time of day, and atmospheric conditions.Environmental sustainability introduces additional complexity in system design. The environmental footprint of deployed systems includes not only operational power consumption but also the impact of manufacturing, transportation, and end-of-life disposal, which we had discussed in previous chapters. A typical deployment of 1000 sensor nodes requires consideration of approximately 500kg of electronic components, including sensors, processing units, and power systems. Sustainable design principles must address both immediate operational requirements and long-term environmental impact through careful component selection and end-of-life planning.\nCommunity capacity building represents another critical dimension of sustainability. Systems must be maintainable by local technicians with varying levels of expertise. This requirement influences architectural decisions, from component selection to system modularity. Documentation must be comprehensive yet accessible, typically requiring materials in multiple languages and formats. Training programs must bridge knowledge gaps while building local technical capacity, ensuring that communities can independently maintain and adapt systems as needs evolve.\nFinancial sustainability often determines system longevity. Operating costs, including maintenance, replacement parts, and network connectivity, must align with local economic conditions. A sustainable deployment might target operational costs below 5% of local monthly income per beneficiary. This constraint influences every aspect of system design, from hardware selection to maintenance schedules, requiring careful optimization of both capital and operational expenditures.",
    "crumbs": [
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>AI for Good</span>"
    ]
  },
  {
    "objectID": "contents/core/ai_for_good/ai_for_good.html#system-design-patterns",
    "href": "contents/core/ai_for_good/ai_for_good.html#system-design-patterns",
    "title": "19  AI for Good",
    "section": "19.6 System Design Patterns",
    "text": "19.6 System Design Patterns\nThe challenges of deploying machine learning systems in resource-constrained environments reflect fundamental constraints that have shaped system architecture for decades. Computing systems across domains have developed robust solutions to operate within limited computational resources, unreliable networks, and power restrictions. These solutions, formalized as “design patterns,” represent reusable architectural approaches to common deployment challenges.\nTraditional system design patterns from distributed systems, embedded computing, and mobile applications provide valuable frameworks for machine learning deployments. The Hierarchical Processing Pattern, for instance, structures operations across system tiers to optimize resource usage. Progressive enhancement ensures graceful degradation under varying conditions, while the Distributed Knowledge Pattern sharing enables consistency across multiple data sources. These established patterns can be adapted to address the unique requirements of machine learning systems, particularly regarding model deployment, training procedures, and inference operations.\n\n19.6.1 Hierarchical Processing\nThe Hierarchical Processing Pattern organizes systems into tiers that share responsibilities based on their available resources and capabilities. Like a business with local branches, regional offices, and headquarters, this pattern segments workloads across edge, regional, and cloud tiers. Each tier is optimized for specific tasks—edge devices handle data collection and local processing, regional nodes manage aggregation and intermediate computations, and cloud infrastructure supports advanced analytics and model training.\nFigure 19.3 depicts the interaction flow across these tiers. Starting at the edge tier with data collection, information flows through regional aggregation and processing, culminating in cloud-based advanced analysis. Bidirectional feedback loops enable model updates to flow back through the hierarchy, ensuring continuous system improvement.\n\n\n\n\n\n\nFigure 19.3: Sequence diagram illustrating the Hierarchical Processing Pattern.\n\n\n\nThis architecture excels in environments with varying infrastructure quality, such as applications spanning urban and rural regions. Edge devices maintain critical functionalities during network or power disruptions by performing essential computations locally while queuing operations that require higher-tier resources. When connectivity returns, the system scales operations across available infrastructure tiers.\nIn machine learning applications, this pattern requires careful consideration of resource allocation and data flow. Edge devices must balance model inference accuracy against computational constraints, while regional nodes facilitate data aggregation and model personalization. Cloud infrastructure provides the computational power needed for comprehensive analytics and model retraining. This distribution demands thoughtful optimization of model architectures, training procedures, and update mechanisms throughout the hierarchy.\n\nCase Study: Google’s Flood Forecasting\n\n\n\n\n\n\nImportant 19.5: AI for Flood Forecasting\n\n\n\n\n\n\n\nGoogle’s Flood Forecasting Initiative demonstrates how the Hierarchical Processing Pattern supports large-scale environmental monitoring. Edge devices along river networks monitor water levels, performing basic anomaly detection even without cloud connectivity. Regional centers aggregate this data and ensure localized decision-making, while the cloud tier integrates inputs from multiple regions for advanced flood prediction and system-wide updates. This tiered approach balances local autonomy with centralized intelligence, ensuring functionality across diverse infrastructure conditions4.\n4 Google’s Flood Forecasting Initiative has been instrumental in mitigating flood risks in vulnerable regions, including parts of India and Bangladesh. By combining real-time sensor data with machine learning models, the initiative generates precise flood predictions and timely alerts, reducing disaster-related losses and enhancing community preparedness.At the edge tier, the system likely employs water-level sensors and local processing units distributed along river networks. These devices perform two critical functions: continuous monitoring of water levels at regular intervals (e.g., every 15 minutes) and preliminary time-series analysis to detect significant changes. Constrained by the tight power envelope (a few watts of power), edge devices utilize quantized models for anomaly detection, enabling low-power operation and minimizing the volume of data transmitted to higher tiers. This localized processing ensures that key monitoring tasks can continue independently of network connectivity.\nThe regional tier operates at district-level processing centers, each responsible for managing data from hundreds of sensors across its jurisdiction. At this tier, more sophisticated neural network models are employed to combine sensor data with additional contextual information, such as local terrain features and historical flood patterns. This tier reduces the data volume transmitted to the cloud by aggregating and extracting meaningful features while maintaining critical decision-making capabilities during network disruptions. By operating independently when required, the regional tier enhances system resilience and ensures localized monitoring and alerts remain functional.\nAt the cloud tier, the system integrates data from regional centers with external sources such as satellite imagery and weather data to implement the full machine learning pipeline. This includes training and running advanced flood prediction models, generating inundation maps, and distributing predictions to stakeholders. The cloud tier provides the computational resources needed for large-scale analysis and system-wide updates. However, the hierarchical structure ensures that essential monitoring and alerting functions can continue autonomously at the edge and regional tiers, even when cloud connectivity is unavailable.\nThis implementation reveals several key principles of successful Hierarchical Processing Pattern deployments. First, the careful segmentation of ML tasks across tiers enables graceful degradation. Each tier maintains critical functionality even when isolated. Secondly, the progressive enhancement of capabilities as higher tiers become available demonstrates how systems can adapt to varying resource availability. Finally, the bidirectional flow of information—sensor data moving upward and model updates flowing downward—creates a robust feedback loop that improves system performance over time. These principles extend beyond flood forecasting to inform hierarchical ML deployments across various social impact domains.\n\n\nPattern Structure\nThe Hierarchical Processing Pattern implements specific architectural components and relationships that enable its distributed operation. Understanding these structural elements is crucial for effective implementation across different deployment scenarios.\nThe edge tier’s architecture centers on resource-aware components that optimize local processing capabilities. At the hardware level, data acquisition modules implement adaptive sampling rates, typically ranging from 1Hz to 0.01Hz, adjusting dynamically based on power availability. Local storage buffers, usually 1-4MB, manage data during network interruptions through circular buffer implementations. The processing architecture incorporates lightweight inference engines specifically optimized for quantized models, working alongside state management systems that continuously track device health and resource utilization. Communication modules implement store-and-forward protocols designed for unreliable networks, ensuring data integrity during intermittent connectivity.\nThe regional tier implements aggregation and coordination structures that enable distributed decision-making. Data fusion engines are the core of this tier, combining multiple edge data streams while accounting for temporal and spatial relationships. Distributed databases, typically spanning 50-100GB, support eventual consistency models to maintain data coherence across nodes. The tier’s architecture includes load balancing systems that dynamically distribute processing tasks based on available computational resources and network conditions. Failover mechanisms ensure continuous operation during node failures, while model serving infrastructure supports multiple model versions to accommodate varying edge device capabilities. Inter-region synchronization protocols manage data consistency across geographic boundaries.\nThe cloud tier provides the architectural foundation for system-wide operations through sophisticated distributed systems. Training infrastructure supports parallel model updates across multiple compute clusters, while version control systems manage model lineage and deployment histories. High-throughput data pipelines process incoming data streams from all regional nodes, implementing automated quality control and validation mechanisms. The architecture includes robust security frameworks that manage authentication and authorization across all tiers while maintaining audit trails of system access and modifications. Global state management systems track the health and performance of the entire deployment, enabling proactive resource allocation and system optimization.\nThe Hierarchical Processing Pattern’s structure enables sophisticated management of resources and responsibilities across tiers. This architectural approach ensures that systems can maintain critical operations under varying conditions while efficiently utilizing available resources at each level of the hierarchy.\n\n\nModern Adaptations\nAdvancements in computational efficiency, model design, and distributed systems have transformed the traditional Hierarchical Processing Pattern. While maintaining its core principles, the pattern has evolved to accommodate new technologies and methodologies that enable more complex workloads and dynamic resource allocation. These innovations have particularly impacted how the different tiers interact and share responsibilities, creating more flexible and capable deployments across diverse environments.\nOne of the most notable transformations has occurred at the edge tier. Historically constrained to basic operations such as data collection and simple preprocessing, edge devices now perform sophisticated processing tasks that were previously exclusive to the cloud. This shift has been driven by two critical developments: efficient model architectures and hardware acceleration. Techniques such as model compression, pruning, and quantization have dramatically reduced the size and computational requirements of neural networks, allowing even resource-constrained devices to perform inference tasks with reasonable accuracy. Advances in specialized hardware, such as edge AI accelerators and low-power GPUs, have further enhanced the computational capabilities of edge devices. As a result, tasks like image recognition or anomaly detection that once required significant cloud resources can now be executed locally on low-power microcontrollers.\nThe regional tier has also evolved beyond its traditional role of data aggregation. Modern regional nodes leverage techniques such as federated learning, where multiple devices collaboratively improve a shared model without transferring raw data to a central location. This approach not only enhances data privacy but also reduces bandwidth requirements. Regional tiers are increasingly used to adapt global models to local conditions, enabling more accurate and context-aware decision-making for specific deployment environments. This adaptability makes the regional tier an indispensable component for systems operating in diverse or resource-variable settings.\nThe relationship between the tiers has become more fluid and dynamic with these advancements. As edge and regional capabilities have expanded, the distribution of tasks across tiers is now determined by factors such as real-time resource availability, network conditions, and application requirements. For instance, during periods of low connectivity, edge and regional tiers can temporarily take on additional responsibilities to ensure critical functionality, while seamlessly offloading tasks to the cloud when resources and connectivity improve. This dynamic allocation preserves the hierarchical structure’s inherent benefits—scalability, resilience, and efficiency—while enabling greater adaptability to changing conditions.\nThese adaptations indicate future developments in Hierarchical Processing Pattern systems. As edge computing capabilities continue to advance and new distributed learning approaches emerge, the boundaries between tiers will likely become increasingly dynamic. This evolution suggests a future where hierarchical systems can automatically optimize their structure based on deployment context, resource availability, and application requirements, while maintaining the pattern’s fundamental benefits of scalability, resilience, and efficiency.\n\n\nML System Implications\nWhile the Hierarchical Processing Pattern was originally designed for general-purpose distributed systems, its application to machine learning introduces unique considerations that significantly influence system design and operation. Machine learning systems differ from traditional systems in their heavy reliance on data flows, computationally intensive tasks, and the dynamic nature of model updates and inference processes. These additional factors introduce both challenges and opportunities in adapting the Hierarchical Processing Pattern to meet the needs of machine learning deployments.\nOne of the most significant implications for machine learning is the need to manage dynamic model behavior across tiers. Unlike static systems, ML models require regular updates to adapt to new data distributions, prevent model drift, and maintain accuracy. The hierarchical structure inherently supports this requirement by allowing the cloud tier to handle centralized training and model updates while propagating refined models to regional and edge tiers. However, this introduces challenges in synchronization, as edge and regional tiers must continue operating with older model versions when updates are delayed due to connectivity issues. Designing robust versioning systems and ensuring seamless transitions between model updates is critical to the success of such systems.\nData flows are another area where machine learning systems impose unique demands. Unlike traditional hierarchical systems, ML systems must handle large volumes of data across tiers, ranging from raw inputs at the edge to aggregated and preprocessed datasets at regional and cloud tiers. Each tier must be optimized for the specific data-processing tasks it performs. For instance, edge devices often filter or preprocess raw data to reduce transmission overhead while retaining information critical for inference. Regional tiers aggregate these inputs, performing intermediate-level analysis or feature extraction to support downstream tasks. This multistage data pipeline not only reduces bandwidth requirements but also ensures that each tier contributes meaningfully to the overall ML workflow.\nThe Hierarchical Processing Pattern also enables adaptive inference, a key consideration for deploying ML models across environments with varying computational resources. By leveraging the computational capabilities of each tier, systems can dynamically distribute inference tasks to balance latency, energy consumption, and accuracy. For example, an edge device might handle basic anomaly detection to ensure real-time responses, while more sophisticated inference tasks are offloaded to the cloud when resources and connectivity allow. This dynamic distribution is essential for resource-constrained environments, where energy efficiency and responsiveness are paramount.\nHardware advancements have further shaped the application of the Hierarchical Processing Pattern to machine learning. The proliferation of specialized edge hardware, such as AI accelerators and low-power GPUs, has enabled edge devices to handle increasingly complex ML tasks, narrowing the performance gap between tiers. Regional tiers have similarly benefited from innovations such as federated learning, where models are collaboratively improved across devices without requiring centralized data collection. These advancements enhance the autonomy of lower tiers, reducing the dependency on cloud connectivity and enabling systems to function effectively in decentralized environments.\nFinally, machine learning introduces the challenge of balancing local autonomy with global coordination. Edge and regional tiers must be able to make localized decisions based on the data available to them while remaining synchronized with the global state maintained at the cloud tier. This requires careful design of interfaces between tiers to manage not only data flows but also model updates, inference results, and feedback loops. For instance, systems employing federated learning must coordinate the aggregation of locally trained model updates without overwhelming the cloud tier or compromising privacy and security.\nBy integrating machine learning into the Hierarchical Processing Pattern, systems gain the ability to scale their capabilities across diverse environments, adapt dynamically to changing resource conditions, and balance real-time responsiveness with centralized intelligence. However, these benefits come with added complexity, requiring careful attention to model lifecycle management, data structuring, and resource allocation. The Hierarchical Processing Pattern remains a powerful framework for ML systems, enabling them to overcome the constraints of infrastructure variability while delivering high-impact solutions across a wide range of applications.\n\n\nLimitations and Challenges\nDespite its strengths, the Hierarchical Processing Pattern encounters several fundamental constraints in real-world deployments, particularly when applied to machine learning systems. These limitations arise from the distributed nature of the architecture, the variability of resource availability across tiers, and the inherent complexities of maintaining consistency and efficiency at scale.\nThe distribution of processing capabilities introduces significant complexity in resource allocation and cost management. Regional processing nodes must navigate trade-offs between local computational needs, hardware costs, and energy consumption. In battery-powered deployments, the energy efficiency of local computation versus data transmission becomes a critical factor. These constraints directly affect the scalability and operational costs of the system, as additional nodes or tiers may require significant investment in infrastructure and hardware.\nTime-critical operations present unique challenges in hierarchical systems. While edge processing reduces latency for local decisions, operations requiring cross-tier coordination introduce unavoidable delays. For instance, anomaly detection systems that require consensus across multiple regional nodes face inherent latency limitations. This coordination overhead can make hierarchical architectures unsuitable for applications requiring sub-millisecond response times or strict global consistency.\nTraining data imbalances across regions create additional complications. Different deployment environments often generate varying quantities and types of data, leading to model bias and performance disparities. For example, urban areas typically generate more training samples than rural regions, potentially causing models to underperform in less data-rich environments. This imbalance can be particularly problematic in systems where model performance directly impacts critical decision-making processes.\nSystem maintenance and debugging introduce practical challenges that grow with scale. Identifying the root cause of performance degradation becomes increasingly complex when issues can arise from hardware failures, network conditions, model drift, or interactions between tiers. Traditional debugging approaches often prove inadequate, as problems may manifest only under specific combinations of conditions across multiple tiers. This complexity increases operational costs and requires specialized expertise for system maintenance.\nThese limitations necessitate careful consideration of mitigation strategies during system design. Approaches such as asynchronous processing protocols, tiered security frameworks, and automated debugging tools can help address specific challenges. Additionally, implementing robust monitoring systems that track performance metrics across tiers enables early detection of potential issues. While these limitations don’t diminish the pattern’s overall utility, they underscore the importance of thorough planning and risk assessment in hierarchical system deployments.\n\n\n\n19.6.2 Progressive Enhancement\nThe progressive enhancement pattern applies a layered approach to system design, enabling functionality across environments with varying resource capacities. This pattern operates by establishing a baseline capability that remains operational under minimal resource conditions—typically requiring only kilobytes of memory and milliwatts of power—and incrementally incorporating advanced features as additional resources become available. While originating from web development, where applications adapted to diverse browser capabilities and network conditions, the pattern has evolved to address the complexities of distributed systems and machine learning deployments.\nThis approach fundamentally differs from the Hierarchical Processing Pattern by focusing on vertical feature enhancement rather than horizontal distribution of tasks. Systems adopting this pattern are structured to maintain operations even under severe resource constraints, such as 2G network connections (&lt; 50 kbps) or microcontroller-class devices (&lt; 1MB RAM). Additional capabilities are activated systematically as resources become available, with each enhancement layer building upon the foundation established by previous layers. This granular approach to resource utilization ensures system reliability while maximizing performance potential.\nIn machine learning applications, the progressive enhancement pattern enables sophisticated adaptation of models and workflows based on available resources. For instance, a computer vision system might deploy a 100KB quantized model capable of basic object detection under minimal conditions, progressively expanding to more sophisticated models (1-50MB) with higher accuracy and additional detection capabilities as computational resources permit. This adaptability allows systems to scale their capabilities dynamically while maintaining fundamental functionality across diverse operating environments.\n\nCase Study: PlantVillage Nuru\nPlantVillage Nuru exemplifies the progressive enhancement pattern in its approach to providing AI-powered agricultural support for smallholder farmers (Ferentinos 2018), particularly in low-resource settings. Developed to address the challenges of crop diseases and pest management, Nuru combines machine learning models with mobile technology to deliver actionable insights directly to farmers, even in remote regions with limited connectivity or computational resources.5\n\nFerentinos, Konstantinos P. 2018. “Deep Learning Models for Plant Disease Detection and Diagnosis.” Computers and Electronics in Agriculture 145 (February): 311–18. https://doi.org/10.1016/j.compag.2018.01.009.\n5 PlantVillage Nuru has significantly impacted agricultural resilience, enabling farmers in over 60 countries to diagnose crop diseases with 85-90% accuracy using entry-level smartphones. The initiative has directly contributed to improved crop yields and reduced losses in vulnerable farming communities by integrating on-device AI and cloud-based insights.PlantVillage Nuru operates with a baseline model optimized for resource-constrained environments. The system employs quantized convolutional neural networks (typically 2-5MB in size) running on entry-level smartphones, capable of processing images at 1-2 frames per second while consuming less than 100mW of power. These on-device models achieve 85-90% accuracy in identifying common crop diseases, providing essential diagnostic capabilities without requiring network connectivity.\nWhen network connectivity becomes available (even at 2G speeds of 50-100 kbps), Nuru progressively enhances its capabilities. The system uploads collected data to cloud infrastructure, where more sophisticated models (50-100MB) perform advanced analysis with 95-98% accuracy. These models integrate multiple data sources: high-resolution satellite imagery (10-30m resolution), local weather data (updated hourly), and soil sensor readings. This enhanced processing generates detailed mitigation strategies, including precise pesticide dosage recommendations and optimal timing for interventions.\nIn regions lacking widespread smartphone access, Nuru implements an intermediate enhancement layer through community digital hubs. These hubs, equipped with mid-range tablets (2GB RAM, quad-core processors), cache diagnostic models and agricultural databases (10-20GB) locally. This architecture enables offline access to enhanced capabilities while serving as data aggregation points when connectivity becomes available, typically synchronizing with cloud services during off-peak hours to optimize bandwidth usage.\nThis implementation demonstrates how progressive enhancement can scale from basic diagnostic capabilities to comprehensive agricultural support based on available resources. The system maintains functionality even under severe constraints (offline operation, basic hardware) while leveraging additional resources when available to provide increasingly sophisticated analysis and recommendations.\n\n\nPattern Structure\nThe progressive enhancement pattern organizes systems into layered functionalities, each designed to operate within specific resource conditions. This structure begins with a set of capabilities that function under minimal computational or connectivity constraints, progressively incorporating advanced features as additional resources become available.\nTable 19.2 outlines the resource specifications and capabilities across the pattern’s three primary layers:\n\n\n\nTable 19.2: Resource specifications and capabilities across progressive enhancement pattern layers\n\n\n\n\n\n\n\n\n\n\n\nResource Type\nBaseline Layer\nIntermediate Layer\nAdvanced Layer\n\n\n\n\nComputational\nMicrocontroller-class (100-200MHz CPU, &lt;1MB RAM)\nEntry-level smartphones (1-2GB RAM)\nCloud/edge servers (8GB+ RAM)\n\n\nNetwork\nOffline or 2G/GPRS\nIntermittent 3G/4G (1-10Mbps)\nReliable broadband (50Mbps+)\n\n\nStorage\nEssential models (1-5MB)\nLocal cache (10-50MB)\nDistributed systems (GB+ scale)\n\n\nPower\nBattery-operated (50-150mW)\nDaily charging cycles\nContinuous grid power\n\n\nProcessing\nBasic inference tasks\nModerate ML workloads\nFull training capabilities\n\n\nData Access\nPre-packaged datasets\nPeriodic synchronization\nReal-time data integration\n\n\n\n\n\n\nEach layer in the progressive enhancement pattern operates independently, so that systems remain functional regardless of the availability of higher tiers. The pattern’s modular structure enables seamless transitions between layers, minimizing disruptions as systems dynamically adjust to changing resource conditions. By prioritizing adaptability, the progressive enhancement pattern supports a wide range of deployment environments, from remote, resource-constrained regions to well-connected urban centers.\nFigure 19.4 illustrates these three layers, showing the functionalities at each layer. The diagram visually demonstrates how each layer scales up based on available resources and how the system can fallback to lower layers when resource constraints occur.\n\n\n\n\n\n\nFigure 19.4: Progressive enhancement pattern with specific examples of functionality at each layer.\n\n\n\n\n\nModern Adaptations\nModern implementations of the progressive enhancement pattern incorporate automated optimization techniques to create sophisticated resource-aware systems. These adaptations fundamentally reshape how systems manage varying resource constraints across deployment environments.\nAutomated architecture optimization represents a significant advancement in implementing progressive enhancement layers. Contemporary systems employ Neural Architecture Search to generate model families optimized for specific resource constraints. For example, a computer vision system might maintain multiple model variants ranging from 500KB to 50MB in size, each preserving maximum accuracy within its respective computational bounds. This automated approach ensures consistent performance scaling across enhancement layers, while setting the foundation for more sophisticated adaptation mechanisms.\nKnowledge distillation and transfer mechanisms have evolved to support progressive capability enhancement. Modern systems implement bidirectional distillation processes where simplified models operating in resource-constrained environments gradually incorporate insights from their more sophisticated counterparts. This architectural approach enables baseline models to improve their performance over time while operating within strict resource limitations, creating a dynamic learning ecosystem across enhancement layers.\nThe evolution of distributed learning frameworks further extends these enhancement capabilities through federated optimization strategies. Base layer devices participate in simple model averaging operations, while better-resourced nodes implement more sophisticated federated optimization algorithms. This tiered approach to distributed learning enables system-wide improvements while respecting the computational constraints of individual devices, effectively scaling learning capabilities across diverse deployment environments.\nThese distributed capabilities culminate in resource-aware neural architectures that exemplify recent advances in dynamic adaptation. These systems modulate their computational graphs based on available resources, automatically adjusting model depth, width, and activation functions to match current hardware capabilities. Such dynamic adaptation enables smooth transitions between enhancement layers while maintaining optimal resource utilization, representing the current state of the art in progressive enhancement implementations.\n\n\nML System Implications\nThe application of the progressive enhancement pattern to machine learning systems introduces unique architectural considerations that extend beyond traditional progressive enhancement approaches. These implications fundamentally affect model deployment strategies, inference pipelines, and system optimization techniques.\nModel architecture design requires careful consideration of computational-accuracy trade-offs across enhancement layers. At the baseline layer, models must operate within strict computational bounds (typically 100-500KB model size) while maintaining acceptable accuracy thresholds (usually 85-90% of full model performance). Each enhancement layer then incrementally incorporates more sophisticated architectural components—additional model layers, attention mechanisms, or ensemble techniques—scaling computational requirements in tandem with available resources.\nTraining pipelines present distinct challenges in progressive enhancement implementations. Systems must maintain consistent performance metrics across different model variants while enabling smooth transitions between enhancement layers. This necessitates specialized training approaches such as progressive knowledge distillation, where simpler models learn to mimic the behavior of their more complex counterparts within their computational constraints. Training objectives must balance multiple factors: baseline model efficiency, enhancement layer accuracy, and cross-layer consistency.\nInference optimization becomes particularly critical in progressive enhancement scenarios. Systems must dynamically adapt their inference strategies based on available resources, implementing techniques such as adaptive batching, dynamic quantization, and selective layer activation. These optimizations ensure efficient resource utilization while maintaining real-time performance requirements across different enhancement layers.\nModel synchronization and versioning introduce additional complexity in progressively enhanced ML systems. As models operate across different resource tiers, systems must maintain version compatibility and manage model updates without disrupting ongoing operations. This requires robust versioning protocols that track model lineage across enhancement layers while ensuring backward compatibility for baseline operations.\n\n\nLimitations and Challenges\nWhile the progressive enhancement pattern offers significant advantages for ML system deployment, it introduces several technical challenges that impact implementation feasibility and system performance. These challenges particularly affect model management, resource optimization, and system reliability.\nModel version proliferation presents a fundamental challenge. Each enhancement layer typically requires multiple model variants (often 3-5 per layer) to handle different resource scenarios, creating a combinatorial explosion in model management overhead. For example, a computer vision system supporting three enhancement layers might require up to 15 different model versions, each needing individual maintenance, testing, and validation. This complexity increases exponentially when supporting multiple tasks or domains.\nPerformance consistency across enhancement layers introduces significant technical hurdles. Models operating at the baseline layer (typically limited to 100-500KB size) must maintain at least 85-90% of the accuracy achieved by advanced models while using only 1-5% of the computational resources. Achieving this efficiency-accuracy trade-off becomes increasingly difficult as task complexity increases. Systems often struggle to maintain consistent inference behavior when transitioning between layers, particularly when handling edge cases or out-of-distribution inputs.\nResource allocation optimization presents another critical limitation. Systems must continuously monitor and predict resource availability while managing the overhead of these monitoring systems themselves. The decision-making process for switching between enhancement layers introduces additional latency (typically 50-200ms), which can impact real-time applications. This overhead becomes particularly problematic in environments with rapidly fluctuating resource availability.\nInfrastructure dependencies create fundamental constraints on system capabilities. While baseline functionality operates within minimal requirements (50-150mW power consumption, 2G network speeds), achieving full system potential requires substantial infrastructure improvements. The gap between baseline and enhanced capabilities often spans several orders of magnitude in computational requirements, creating significant disparities in system performance across deployment environments.\nUser experience continuity suffers from the inherent variability in system behavior across enhancement layers. Output quality and response times can vary significantly—from basic binary classifications at the baseline layer to detailed probabilistic predictions with confidence intervals at advanced layers. These variations can undermine user trust, particularly in critical applications where consistency is essential.\nThese limitations necessitate careful consideration during system design and deployment. Successful implementations require robust monitoring systems, graceful degradation mechanisms, and clear communication of system capabilities at each enhancement layer. While these challenges don’t negate the pattern’s utility, they emphasize the importance of thorough planning and realistic expectation setting in progressive enhancement deployments.\n\n\n\n19.6.3 Distributed Knowledge\nThe Distributed Knowledge Pattern addresses the challenges of collective learning and inference across decentralized nodes, each operating with local data and computational constraints. Unlike hierarchical processing, where tiers have distinct roles, this pattern emphasizes peer-to-peer knowledge sharing and collaborative model improvement. Each node contributes to the network’s collective intelligence while maintaining operational independence.\nThis pattern builds on established Mobile ML and Tiny ML techniques to enable autonomous local processing at each node. Devices implement quantized models (typically 1-5MB) for initial inference, while employing techniques like federated learning for collaborative model improvement. Knowledge sharing occurs through various mechanisms: model parameter updates, derived features, or processed insights, depending on bandwidth and privacy constraints. This distributed approach enables the network to leverage collective experiences while respecting local resource limitations.\nThe pattern particularly excels in environments where traditional centralized learning faces significant barriers. By distributing both data collection and model training across nodes, systems can operate effectively even with intermittent connectivity (as low as 1-2 hours of network availability per day) or severe bandwidth constraints (50-100KB/day per node). This resilience makes it especially valuable for social impact applications operating in infrastructure-limited environments.\nThe distributed approach fundamentally differs from progressive enhancement by focusing on horizontal knowledge sharing rather than vertical capability enhancement. Each node maintains similar baseline capabilities while contributing to and benefiting from the network’s collective knowledge, creating a robust system that remains functional even when significant portions of the network are temporarily inaccessible.\n\nCase Study: Wildlife Insights\nWildlife Insights demonstrates the Distributed Knowledge Pattern’s application in conservation through distributed camera trap networks. The system exemplifies how decentralized nodes can collectively build and share knowledge while operating under severe resource constraints in remote wilderness areas.\nEach camera trap functions as an independent processing node, implementing sophisticated edge computing capabilities within strict power and computational limitations. These devices employ lightweight convolutional neural networks for species identification, alongside efficient activity detection models for motion analysis. Operating within power constraints of 50-100mW, the devices utilize adaptive duty cycling to maximize battery life while maintaining continuous monitoring capabilities. This local processing approach enables each node to independently analyze and filter captured imagery, reducing raw image data from several megabytes to compact insight vectors of just a few kilobytes.\nThe system’s Distributed Knowledge Pattern sharing architecture enables effective collaboration between nodes despite connectivity limitations. Camera traps form local mesh networks using low-power radio protocols, sharing processed insights rather than raw data. This peer-to-peer communication allows the network to maintain collective awareness of wildlife movements and potential threats across the monitored area. When one node detects significant activity—such as the presence of an endangered species or signs of poaching—this information propagates through the network, enabling coordinated responses even in areas with no direct connectivity to central infrastructure.\nWhen periodic connectivity becomes available through satellite or cellular links, nodes synchronize their accumulated knowledge with cloud infrastructure. This synchronization process carefully balances the need for data sharing with bandwidth limitations, employing differential updates and compression techniques. The cloud tier then applies more sophisticated analytical models to understand population dynamics and movement patterns across the entire monitored region.\nThe Wildlife Insights implementation demonstrates how Distributed Knowledge Pattern sharing can maintain system effectiveness even in challenging environments. By distributing both processing and decision-making capabilities across the network, the system ensures continuous monitoring and rapid response capabilities while operating within the severe constraints of remote wilderness deployments. This approach has proven particularly valuable for conservation efforts, enabling real-time wildlife monitoring and threat detection across vast areas that would be impractical to monitor through centralized systems6.\n6 Camera traps have been widely used for ecological monitoring since the early 20th century. Initially reliant on physical film, they transitioned to digital and, more recently, AI-enabled systems, enhancing their ability to automate data analysis and extend deployment durations.\n\nPattern Structure\nThe Distributed Knowledge Pattern comprises specific architectural components designed to enable decentralized data collection, processing, and knowledge sharing. The pattern defines three primary structural elements: autonomous nodes, communication networks, and aggregation mechanisms.\nFigure 19.5 illustrates the key components and their interactions within the Distributed Knowledge Pattern. Individual nodes (rectangular shapes) operate autonomously while sharing insights through defined communication channels. The aggregation layer (diamond shape) combines distributed knowledge, which feeds into the analysis layer (oval shape) for processing.\n\n\n\n\n\n\nFigure 19.5: Distributed Knowledge Pattern with differentiated shapes for nodes, central aggregation, and analysis.\n\n\n\nAutonomous nodes form the foundation of the pattern’s structure. Each node implements three essential capabilities: data acquisition, local processing, and knowledge sharing. The local processing pipeline typically includes feature extraction, basic inference, and data filtering mechanisms. This architecture enables nodes to operate independently while contributing to the network’s collective intelligence.\nThe communication layer establishes pathways for knowledge exchange between nodes. This layer implements both peer-to-peer protocols for direct node communication and hierarchical protocols for aggregation. The communication architecture must balance bandwidth efficiency with information completeness, often employing techniques such as differential updates and compressed knowledge sharing.\nThe aggregation and analysis layers provide mechanisms for combining distributed insights into understanding. These layers implement more sophisticated processing capabilities while maintaining feedback channels to individual nodes. Through these channels, refined models and updated processing parameters flow back to the distributed components, creating a continuous improvement cycle.\nThis structural organization ensures system resilience while enabling scalable knowledge sharing across distributed environments. The pattern’s architecture specifically addresses the challenges of unreliable infrastructure and limited connectivity while maintaining system effectiveness through decentralized operations.\n\n\nModern Adaptations\nThe Distributed Knowledge Pattern has seen significant advancements with the rise of modern technologies like edge computing, the Internet of Things (IoT), and decentralized data networks. These innovations have enhanced the scalability, efficiency, and flexibility of systems utilizing this pattern, enabling them to handle increasingly complex data sets and to operate in more diverse and challenging environments.\nOne key adaptation has been the use of edge computing. Traditionally, distributed systems rely on transmitting data to centralized servers for analysis. However, with edge computing, nodes can perform more complex processing locally, reducing the dependency on central systems and enabling real-time data processing. This adaptation has been especially impactful in areas where network connectivity is intermittent or unreliable. For example, in remote wildlife conservation systems, camera traps can process images locally and only transmit relevant insights, such as the detection of a poacher, to a central hub when connectivity is restored. This reduces the amount of raw data sent across the network and ensures that the system remains operational even in areas with limited infrastructure.\nAnother important development is the integration of machine learning at the edge. In traditional distributed systems, machine learning models are often centralized, requiring large amounts of data to be sent to the cloud for processing. With the advent of smaller, more efficient machine learning models designed for edge devices, these models can now be deployed directly on the nodes themselves. For example, low-power devices such as smartphones or IoT sensors can run lightweight models for tasks like anomaly detection or image classification. This enables more sophisticated data analysis at the source, allowing for quicker decision-making and reducing reliance on central cloud services.\nIn terms of network communication, modern mesh networks and 5G technology have significantly improved the efficiency and speed of data sharing between nodes. Mesh networks allow nodes to communicate with each other directly, forming a self-healing and scalable network. This decentralized approach to communication ensures that even if a node or connection fails, the network can still operate seamlessly. With the advent of 5G, the bandwidth and latency issues traditionally associated with large-scale data transfer in distributed systems are mitigated, enabling faster and more reliable communication between nodes in real-time applications.\n\n\nML System Implications\nThe Distributed Knowledge Pattern fundamentally reshapes how machine learning systems handle data collection, model training, and inference across decentralized nodes. These implications extend beyond traditional distributed computing challenges to encompass ML-specific considerations in model architecture, training dynamics, and inference optimization.\nModel architecture design requires specific adaptations for distributed deployment. Models must be structured to operate effectively within node-level resource constraints while maintaining sufficient complexity for accurate inference. This often necessitates specialized architectures that support incremental learning and knowledge distillation. For instance, neural network architectures might implement modular components that can be selectively activated based on local computational resources, typically operating within 1-5MB memory constraints while maintaining 85-90% of centralized model accuracy.\nTraining dynamics become particularly complex in Distributed Knowledge Pattern systems. Unlike centralized training approaches, these systems must implement collaborative learning mechanisms that function effectively across unreliable networks. Federated averaging protocols must be adapted to handle non-IID (Independent and Identically Distributed) data distributions across nodes while maintaining convergence guarantees. Training procedures must also account for varying data qualities and quantities across nodes, implementing weighted aggregation schemes that reflect data reliability and relevance.\nInference optimization presents unique challenges in distributed environments. Models must adapt their inference strategies based on local resource availability while maintaining consistent output quality across the network. This often requires implementing dynamic batching strategies, adaptive quantization, and selective feature computation. Systems typically target sub-100ms inference latency at the node level while operating within strict power envelopes (50-150mW).\nModel lifecycle management becomes significantly more complex in Distributed Knowledge Pattern systems. Version control must handle multiple model variants operating across different nodes, managing both forward and backward compatibility. Systems must implement robust update mechanisms that can handle partial network connectivity while preventing model divergence across the network.\n\n\nLimitations and Challenges\nWhile the Distributed Knowledge Pattern offers many advantages, particularly in decentralized, resource-constrained environments, it also presents several challenges, especially when applied to machine learning systems. These challenges stem from the complexity of managing distributed nodes, ensuring data consistency, and addressing the constraints of decentralized systems.\nOne of the primary challenges is model synchronization and consistency. In distributed systems, each node may operate with its own version of a machine learning model, which is trained using local data. As these models are updated over time, ensuring consistency across all nodes becomes a difficult task. Without careful synchronization, nodes may operate using outdated models, leading to inconsistencies in the system’s overall performance. Furthermore, when nodes are intermittently connected or have limited bandwidth, synchronizing model updates across all nodes in real-time can be resource-intensive and prone to delays.\nThe issue of data fragmentation is another significant challenge. In a distributed system, data is often scattered across different nodes, and each node may have access to only a subset of the entire dataset. This fragmentation can limit the effectiveness of machine learning models, as the models may not be exposed to the full range of data needed for training. Aggregating data from multiple sources and ensuring that the data from different nodes is compatible for analysis is a complex and time-consuming process. Additionally, because some nodes may operate in offline modes or have intermittent connectivity, data may be unavailable for periods, further complicating the process.\nScalability also poses a challenge in distributed systems. As the number of nodes in the network increases, so does the volume of data generated and the complexity of managing the system. The system must be designed to handle this growth without overwhelming the infrastructure or degrading performance. The addition of new nodes often requires rebalancing data, recalibrating models, or introducing new coordination mechanisms, all of which can increase the complexity of the system.\nLatency is another issue that arises in distributed systems. While data is processed locally on each node, real-time decision-making often requires the aggregation of insights from multiple nodes. The time it takes to share data and updates between nodes, and the time needed to process that data, can introduce delays in system responsiveness. In applications like autonomous systems or disaster response, these delays can undermine the effectiveness of the system, as immediate action is often necessary.\nFinally, security and privacy concerns are magnified in distributed systems. Since data is often transmitted between nodes or stored across multiple devices, ensuring the integrity and confidentiality of the data becomes a significant challenge. The system must employ strong encryption and authentication mechanisms to prevent unauthorized access or tampering of sensitive information. This is especially important in applications involving private or protected data, such as healthcare or financial systems. Additionally, decentralized systems may be more susceptible to certain types of attacks, such as Sybil attacks, where an adversary can introduce fake nodes into the network.\nDespite these challenges, there are several strategies that can help mitigate the limitations of the Distributed Knowledge Pattern. For example, federated learning techniques can help address model synchronization issues by enabling nodes to update models locally and only share the updates, rather than raw data. Decentralized data aggregation methods can help address data fragmentation by allowing nodes to perform more localized aggregation before sending data to higher tiers. Similarly, edge computing can reduce latency by processing data closer to the source, reducing the time needed to transmit information to central servers.\n\n\n\n19.6.4 Adaptive Resource\nThe Adaptive Resource Pattern focuses on enabling systems to dynamically adjust their operations in response to varying resource availability, ensuring efficiency, scalability, and resilience in real-time. This pattern allows systems to allocate resources flexibly depending on factors like computational load, network bandwidth, and storage capacity. The key idea is that systems should be able to scale up or down based on the resources they have access to at any given time.\nRather than being a standalone pattern, Adaptive Resource Pattern management is often integrated within other system design patterns. It enhances systems by allowing them to perform efficiently even under changing conditions, ensuring that they continue to meet their objectives, regardless of resource fluctuations.\nFigure 19.6 below illustrates how systems using the Adaptive Resource Pattern adapt to different levels of resource availability. The system adjusts its operations based on the resources available at the time, optimizing its performance accordingly.\n\n\n\n\n\n\nFigure 19.6: The Adaptive Resource Pattern.\n\n\n\nIn the diagram, when the system is operating under low resources, it switches to simplified operations, ensuring basic functionality with minimal resource use. As resources become more available, the system adjusts to medium resources, enabling more moderate operations and optimized functionality. When resources are abundant, the system can leverage high resources, enabling advanced operations and full capabilities, such as processing complex data or running resource-intensive tasks.\nThe feedback loop is an essential part of this pattern, as it ensures continuous adjustment based on the system’s resource conditions. This feedback allows the system to recalibrate and adapt in real-time, scaling resources up or down to maintain optimal performance.\n\nCase Studies\nLooking at the systems we discussed earlier, it is clear that these systems could benefit from Adaptive Resource Pattern allocation in their operations. In the case of Google’s flood forecasting system, the Hierarchical Processing Pattern approach ensures that data is processed at the appropriate level, from edge sensors to cloud-based analysis. However, Adaptive Resource Pattern management would enable this system to adjust its operations dynamically depending on the resources available. In areas with limited infrastructure, the system could rely more heavily on edge processing to reduce the need for constant connectivity, while in regions with better infrastructure, the system could scale up and leverage more cloud-based processing power.\nSimilarly, PlantVillage Nuru could integrate Adaptive Resource Pattern allocation into its progressive enhancement approach. The app is designed to work in a variety of settings, from low-resource rural areas to more developed regions. The Adaptive Resource Pattern management in this context would help the system adjust the complexity of its processing based on the available device and network resources, ensuring that it provides useful insights without overwhelming the system or device.\nIn the case of Wildlife Insights, the Adaptive Resource Pattern management would complement the Distributed Knowledge Pattern. The camera traps in the field process data locally, but when network conditions improve, the system could scale up to transmit more data to central systems for deeper analysis. By using adaptive techniques, the system ensures that the camera traps can continue to function even with limited power and network connectivity, while still providing valuable insights when resources allow for greater computational effort.\nThese systems could integrate the Adaptive Resource Pattern management to dynamically adjust based on available resources, improving efficiency and ensuring continuous operation under varying conditions. By incorporating the Adaptive Resource Pattern allocation into their design, these systems can remain responsive and scalable, even as resource availability fluctuates. The Adaptive Resource Pattern, in this context, acts as an enabler, supporting the operations of these systems and helping them adapt to the demands of real-time environments.\n\n\nPattern Structure\nThe Adaptive Resource Pattern revolves around dynamically allocating resources in response to changing environmental conditions, such as network bandwidth, computational power, or storage. This requires the system to monitor available resources continuously and adjust its operations accordingly to ensure optimal performance and efficiency.\nIt is structured around several key components. First, the system needs a monitoring mechanism to constantly evaluate the availability of resources. This can involve checking network bandwidth, CPU utilization, memory usage, or other relevant metrics. Once these metrics are gathered, the system can then determine the appropriate course of action—whether it needs to scale up, down, or adjust its operations to conserve resources.\nNext, the system must include an adaptive decision-making process that interprets these metrics and decides how to allocate resources dynamically. In high-resource environments, the system might increase the complexity of tasks, using more powerful computational models or increasing the number of concurrent processes. Conversely, in low-resource environments, the system may scale back operations, reduce the complexity of models, or shift some tasks to local devices (such as edge processing) to minimize the load on the central infrastructure.\nAn important part of this structure is the feedback loop, which allows the system to adjust its resource allocation over time. After making an initial decision based on available resources, the system monitors the outcome and adapts accordingly. This process ensures that the system continues to operate effectively even as resource conditions change. The feedback loop helps the system fine-tune its resource usage, leading to more efficient operations as it learns to optimize resource allocation.\nThe system can also be organized into different tiers or layers based on the complexity and resource requirements of specific tasks. For instance, tasks requiring high computational resources, such as training machine learning models or processing large datasets, could be handled by a cloud layer, while simpler tasks, such as data collection or pre-processing, could be delegated to edge devices or local nodes. The system can then adapt the tiered structure based on available resources, allocating more tasks to the cloud or edge depending on the current conditions.\n\n\nModern Adaptations\nThe Adaptive Resource Pattern has evolved significantly with advancements in cloud computing, edge computing, and AI-driven resource management. These innovations have enhanced the flexibility and scalability of the pattern, allowing it to adapt more efficiently in increasingly complex environments.\nOne of the most notable modern adaptations is the integration of cloud computing. Cloud platforms like AWS, Microsoft Azure, and Google Cloud offer the ability to dynamically allocate resources based on demand, making it easier to scale applications in real-time. This integration allows systems to offload intensive processing tasks to the cloud when resources are available and return to more efficient, localized solutions when demand decreases or resources are constrained. The elasticity provided by cloud computing enables systems to perform heavy computational tasks, such as machine learning model training or big data processing, without requiring on-premise infrastructure.\nAt the other end of the spectrum, edge computing has emerged as a critical adaptation for the Adaptive Resource Pattern. In edge computing, data is processed locally on devices or at the edge of the network, reducing the dependency on centralized servers and improving real-time responsiveness. Edge devices, such as IoT sensors or smartphones, often operate in resource-constrained environments, and the ability to process data locally allows for more efficient use of limited resources. By offloading certain tasks to the edge, systems can maintain functionality even in low-resource areas while ensuring that computationally intensive tasks are shifted to the cloud when available.\nThe rise of AI-driven resource management has also transformed how adaptive systems function. AI can now monitor resource usage patterns in real-time and predict future resource needs, allowing systems to adjust resource allocation proactively. For example, machine learning models can be trained to identify patterns in network traffic, processing power, or storage utilization, enabling the system to predict peak usage times and prepare resources accordingly. This proactive adaptation ensures that the system can handle fluctuations in demand smoothly and without interruption, reducing latency and improving overall system performance.\nThese modern adaptations allow systems to perform complex tasks while adapting to local conditions. For example, in disaster response systems, resources such as rescue teams, medical supplies, and communication tools can be dynamically allocated based on the evolving needs of the situation. Cloud computing enables large-scale coordination, while edge computing ensures that critical decisions can be made at the local level, even when the network is down. By integrating AI-driven resource management, the system can predict resource shortages or surpluses, ensuring that resources are allocated in the most effective way.\nThese modern adaptations make the Adaptive Resource Pattern more powerful and flexible than ever. By leveraging cloud, edge computing, and AI, systems can dynamically allocate resources across distributed environments, ensuring that they remain scalable, efficient, and resilient in the face of changing conditions.\n\n\nML System Implications\nAdaptive Resource Pattern has significant implications for machine learning systems, especially when deployed in environments with fluctuating resources, such as mobile devices, edge computing platforms, and distributed systems. Machine learning workloads can be resource-intensive, requiring substantial computational power, memory, and storage. By integrating the Adaptive Resource Pattern allocation, ML systems can optimize their performance, ensure scalability, and maintain efficiency under varying resource conditions.\nIn the context of distributed machine learning (e.g., federated learning), the Adaptive Resource Pattern ensures that the system adapts to varying computational capacities across devices. For example, in federated learning, models are trained collaboratively across many edge devices (such as smartphones or IoT devices), where each device has limited resources. The Adaptive Resource Pattern management can allocate the model training tasks based on the resources available on each device. Devices with more computational power can handle heavier workloads, while devices with limited resources can participate in lighter tasks, such as local model updates or simple computations. This ensures that all devices can contribute to the learning process without overloading them.\nAnother implication of the Adaptive Resource Pattern in ML systems is its ability to optimize real-time inference. In applications like autonomous vehicles, healthcare diagnostics, and environmental monitoring, ML models need to make real-time decisions based on available data. The system must dynamically adjust its computational requirements based on the resources available at the time. For instance, an autonomous vehicle running an image recognition model may process simpler, less detailed frames when computing resources are constrained or when the vehicle is in a resource-limited area (e.g., an area with poor connectivity). When computational resources are more plentiful, such as in a connected city with high-speed internet, the system can process more detailed frames and apply more complex models.\nThe adaptive scaling of ML models also plays a significant role in cloud-based ML systems. In cloud environments, the Adaptive Resource Pattern allows the system to scale the number of resources used for tasks like model training or batch inference. When large-scale data processing or model training is required, cloud services can dynamically allocate resources to handle the increased load. When demand decreases, resources are scaled back to reduce operational costs. This dynamic scaling ensures that ML systems run efficiently and cost-effectively, without over-provisioning or underutilizing resources.\nAdditionally, AI-driven resource management is becoming an increasingly important component of adaptive ML systems. AI techniques, such as reinforcement learning or predictive modeling, can be used to optimize resource allocation in real-time. For example, reinforcement learning algorithms can be applied to predict future resource needs based on historical usage patterns, allowing systems to preemptively allocate resources before demand spikes. This proactive approach ensures that ML models are trained and inference tasks are executed with minimal latency, even as resources fluctuate.\nLastly, edge AI systems benefit greatly from the Adaptive Resource Pattern. These systems often operate in environments with highly variable resources, such as remote areas, rural regions, or environments with intermittent connectivity. The pattern allows these systems to adapt their resource allocation based on the available resources in real-time, ensuring that essential tasks, such as model inference or local data processing, can continue even in challenging conditions. For example, an environmental monitoring system deployed in a remote area may adapt by running simpler models or processing less detailed data when resources are low, while more complex analysis is offloaded to the cloud when the network is available.\n\n\nLimitations and Challenges\nThe Adaptive Resource Pattern faces several fundamental constraints in practical implementations, particularly when applied to machine learning systems in resource-variable environments. These limitations arise from the inherent complexities of real-time adaptation and the technical challenges of maintaining system performance across varying resource levels.\nPerformance predictability presents a primary challenge in adaptive systems. While adaptation enables systems to continue functioning under varying conditions, it can lead to inconsistent performance characteristics. For example, when a system transitions from high to low resource availability (e.g., from 8GB to 500MB RAM), inference latency might increase from 50ms to 200ms. Managing these performance variations while maintaining minimum quality-of-service requirements becomes increasingly complex as the range of potential resource states expands.\nState synchronization introduces significant technical hurdles in adaptive systems. As resources fluctuate, maintaining consistent system state across components becomes challenging. For instance, when adapting to reduced network bandwidth (from 50Mbps to 50Kbps), systems must manage partial updates and ensure that critical state information remains synchronized. This challenge is particularly acute in distributed ML systems, where model states and inference results must remain consistent despite varying resource conditions.\nResource transition overhead poses another fundamental limitation. Adapting to changing resource conditions incurs computational and time costs. For example, switching between different model architectures (from a 50MB full model to a 5MB quantized version) typically requires 100-200ms of transition time. During these transitions, system performance may temporarily degrade or become unpredictable. This overhead becomes particularly problematic in environments where resources fluctuate frequently.\nQuality degradation management presents ongoing challenges, especially in ML applications. As systems adapt to reduced resources, maintaining acceptable quality metrics becomes increasingly difficult. For instance, model accuracy might drop from 95% to 85% when switching to lightweight architectures, while energy consumption must stay within strict limits (typically 50-150mW for edge devices). Finding acceptable trade-offs between resource usage and output quality requires sophisticated optimization strategies.\nThese limitations necessitate careful system design and implementation strategies. Successful deployments often implement robust monitoring systems, graceful degradation mechanisms, and clear quality thresholds for different resource states. While these challenges don’t negate the pattern’s utility, they emphasize the importance of thorough planning and realistic performance expectations in adaptive system deployments.",
    "crumbs": [
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>AI for Good</span>"
    ]
  },
  {
    "objectID": "contents/core/ai_for_good/ai_for_good.html#pattern-selection-framework",
    "href": "contents/core/ai_for_good/ai_for_good.html#pattern-selection-framework",
    "title": "19  AI for Good",
    "section": "19.7 Pattern Selection Framework",
    "text": "19.7 Pattern Selection Framework\nThe selection of an appropriate design pattern for machine learning systems in social impact contexts requires careful consideration of both technical constraints and operational requirements. Rather than treating patterns as rigid templates, system architects should view them as adaptable frameworks that can be tailored to specific deployment scenarios.\nThe selection process begins with a systematic analysis of four critical dimensions: resource variability, operational scale, data distribution requirements, and adaptation needs. Resource variability encompasses both the range and predictability of available computational resources, typically spanning from severely constrained environments (50-150mW power, &lt;1MB RAM) to resource-rich deployments (multi-core servers, GB+ RAM). Operational scale considers both geographic distribution and user base size, ranging from localized deployments to systems spanning multiple regions. Data distribution requirements address how information needs to flow through the system, from centralized architectures to fully distributed networks. Adaptation needs examine how dynamically the system must respond to changing conditions, from relatively stable environments to highly variable scenarios.\n\n19.7.1 Selection Dimensions\nThese dimensions can be visualized through a quadrant analysis framework that maps patterns based on their resource requirements and adaptability needs. This approach simplifies understanding—at least for a pedagogical perspective—by providing a structured view of how systems align with varying constraints.\nFigure 19.7 provides a structured approach for pattern selection based on two key axes: resource availability and scalability/adaptability needs. The horizontal axis corresponds to the level of computational, network, and power resources available to the system. Systems designed for resource-constrained environments, such as rural or remote areas, are positioned towards the left, while those leveraging robust infrastructure, such as cloud-supported systems, are placed towards the right. The vertical axis captures the system’s ability to function across diverse settings or respond dynamically to changing conditions.\n\n\n\n\n\n\nFigure 19.7: Quadrant mapping of design patterns for AI for Social Good projects based on resource availability and scalability/adaptability needs.\n\n\n\nIn low-resource environments with high adaptability needs, the progressive enhancement pattern dominates. Projects like PlantVillage Nuru implement Tiny ML and Mobile ML paradigms for offline crop diagnostics on basic smartphones. Similarly, Medic Mobile leverages these paradigms to support community health workers, enabling offline data collection and basic diagnostics that sync when connectivity permits.\nFor environments with higher resource availability and significant scalability demands, the Hierarchical Processing Pattern prevails. Google’s Flood Forecasting Initiative exemplifies this approach, combining Edge ML for local sensor processing with Cloud ML for analytics. Global Fishing Watch similarly leverages this pattern, processing satellite data through a hierarchy of computational tiers to monitor fishing activities worldwide.\nThe Distributed Knowledge Pattern excels in low-resource environments requiring decentralized operations. Wildlife Insights demonstrates this through AI-enabled camera traps that employ Edge ML for local image processing while sharing insights across peer networks. WildEyes AI follows a similar approach, using distributed nodes for poaching detection with minimal central coordination.\nSystems requiring dynamic resource allocation in fluctuating environments benefit from the Adaptive Resource Pattern. AI for Disaster Response exemplifies this approach, combining Edge ML for immediate local processing with Cloud ML scalability during crises. The AI-powered Famine Action Mechanism similarly adapts its resource allocation dynamically, scaling analysis capabilities based on emerging conditions and available infrastructure.\n\n\n19.7.2 Implementation Guidance\nAs outlined in Table 19.3, each pattern presents distinct strengths and challenges that influence implementation decisions. The practical deployment of these patterns requires careful consideration of both the operational context and the specific requirements of machine learning systems.\n\n\n\nTable 19.3: Comparisons of design patterns.\n\n\n\n\n\n\n\n\n\n\n\n\nDesign Pattern\nCore Idea\nStrengths\nChallenges\nBest Use Case\n\n\n\n\nHierarchical Processing\nOrganizes operations into edge, regional, and cloud tiers.\nScalability, resilience, fault tolerance\nSynchronization issues, model versioning, and latency in updates.\nDistributed workloads spanning diverse infrastructures (e.g., Google’s Flood Forecasting).\n\n\nProgressive Enhancement\nProvides baseline functionality and scales up dynamically.\nAdaptability to resource variability, inclusivity\nEnsuring consistent UX and increased complexity in layered design.\nApplications serving both resource-constrained and resource-rich environments (e.g., PlantVillage Nuru).\n\n\nDistributed Knowledge\nDecentralizes data processing and sharing across nodes.\nResilient in low-bandwidth environments, scalability\nData fragmentation and challenges with synchronizing decentralized models.\nSystems requiring collaborative, decentralized insights (e.g., Wildlife Insights for conservation).\n\n\nAdaptive Resource\nDynamically adjusts operations based on resource availability.\nResource efficiency and real-time adaptability\nPredicting resource demand and managing trade-offs between performance and simplicity.\nReal-time systems operating under fluctuating resource conditions (e.g., disaster response systems).\n\n\n\n\n\n\nThe implementation approach for each pattern should align with both its position in the resource-adaptability space and its core characteristics. In low-resource, high-adaptability environments, Progressive Enhancement implementations focus on establishing reliable baseline capabilities that can scale smoothly as resources become available. This often involves careful coordination between local processing and cloud resources, ensuring that systems maintain functionality even when operating at minimal resource levels.\nHierarchical Processing Pattern implementations, suited for environments with more stable infrastructure, require careful attention to the interfaces between tiers. The key challenge lies in managing the flow of data and model updates across the hierarchy while maintaining system responsiveness. This becomes particularly critical in social impact applications where real-time response capabilities often determine intervention effectiveness.\nDistributed Knowledge Pattern implementations emphasize resilient peer-to-peer operations, particularly important in environments where centralized coordination isn’t feasible. Success depends on establishing efficient knowledge-sharing protocols that maintain system effectiveness while operating within strict resource constraints. This pattern’s implementation often requires careful balance between local autonomy and network-wide consistency.\nThe Adaptive Resource Pattern implementations focus on dynamic resource management, particularly crucial in environments with fluctuating resource availability. These systems require sophisticated monitoring and control mechanisms that can adjust operations in real-time while maintaining essential functionality. The implementation challenge lies in managing these transitions smoothly without disrupting critical operations.\n\n\n19.7.3 Comparison Analysis\nEach design pattern offers unique advantages and trade-offs in ML system implementations. Understanding these distinctions enables system architects to make informed decisions based on deployment requirements and operational constraints.\nThe Hierarchical Processing Pattern and progressive enhancement pattern represent fundamentally different approaches to resource management. While the Hierarchical Processing Pattern establishes fixed infrastructure tiers with clear boundaries and responsibilities, progressive enhancement implements a continuous spectrum of capabilities that can scale smoothly with available resources. This distinction makes the Hierarchical Processing Pattern more suitable for environments with well-defined infrastructure tiers, while progressive enhancement better serves deployments where resource availability varies unpredictably.\nThe Distributed Knowledge Pattern and Adaptive Resource Pattern address different aspects of system flexibility. The Distributed Knowledge Pattern focuses on spatial distribution and peer-to-peer collaboration, while the Adaptive Resource Pattern management emphasizes temporal adaptation to changing conditions. These patterns can be complementary. The Distributed Knowledge Pattern handles geographic scale, while the Adaptive Resource Pattern management handles temporal variations in resource availability.\nSelection between patterns often depends on the primary constraint facing the deployment. Systems primarily constrained by network reliability typically benefit from the Distributed Knowledge Pattern or Hierarchical Processing Pattern approaches. Those facing computational resource variability align better with progressive enhancement or Adaptive Resource Pattern approaches. The resource adaptability analysis presented earlier provides a structured framework for navigating these decisions based on specific deployment contexts.",
    "crumbs": [
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>AI for Good</span>"
    ]
  },
  {
    "objectID": "contents/core/ai_for_good/ai_for_good.html#conclusion",
    "href": "contents/core/ai_for_good/ai_for_good.html#conclusion",
    "title": "19  AI for Good",
    "section": "19.8 Conclusion",
    "text": "19.8 Conclusion\nThe potential of AI for addressing societal challenges is undeniable. However, the path to successful deployment is anything but straightforward. ML systems for social good are not “plug-and-play” solutions—they are complex engineering endeavors.\nThese systems must be tailored to operate under severe constraints, such as limited power, unreliable connectivity, and sparse data, all while meeting the needs of underserved communities. Designing for these environments is as rigorous and demanding as developing systems for urban deployments, often requiring even more ingenuity to overcome unique challenges. Every componen, from data collection to model deployment, must be reimagined to suit these constraints and deliver meaningful outcomes.\nMachine learning systems for social impact necessitate the systematic application of design patterns to address these unique complexities. The patterns examined in this chapter—Hierarchical Processing, Progressive Enhancement, Distributed Knowledge, and Adaptive Resource—establish frameworks for addressing these challenges while ensuring systems remain effective and sustainable across diverse deployment contexts.\nThe implementation of these patterns depends fundamentally on a comprehensive understanding of both the operational environment and system requirements. Resource availability and adaptability requirements typically determine initial pattern selection, while specific implementation decisions must account for network reliability, computational constraints, and scalability requirements. The efficacy of social impact applications depends not only on pattern selection but on implementation strategies that address local constraints while maintaining system performance.\nThese patterns will evolve as technological capabilities advance and deployment contexts transform. Developments in edge computing, federated learning, and adaptive ML architectures will expand the potential applications of these patterns, particularly in resource-constrained environments. However, the core principles—accessibility, reliability, and scalability—remain fundamental to developing ML systems that generate meaningful social impact.\nThe systematic application of these design patterns, informed by rigorous analysis of deployment contexts and constraints, enables the development of ML systems that function effectively across the computing spectrum while delivering sustainable social impact.s",
    "crumbs": [
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>AI for Good</span>"
    ]
  },
  {
    "objectID": "contents/core/conclusion/conclusion.html",
    "href": "contents/core/conclusion/conclusion.html",
    "title": "20  Conclusion",
    "section": "",
    "text": "20.1 Overview\nThis book examines the rapidly evolving field of ML systems (Chapter 2). We focused on systems because while there are many resources on ML models and algorithms, more needs to be understood about how to build the systems that run them.\nTo draw an analogy, consider the process of building a car. While many resources are available on the various components of a car, such as the engine, transmission, and suspension, there is often a need for more understanding about how to assemble these components into a functional vehicle. Just as a car requires a well-designed and properly integrated system to operate efficiently and reliably, ML models also require a robust and carefully constructed system to deliver their full potential. Moreover, there is a lot of nuance in building ML systems, given their specific use case. For example, a Formula 1 race car must be assembled differently from an everyday Prius consumer car.\nOur journey started by tracing ML’s historical trajectory, from its theoretical foundations to its current state as a transformative force across industries (Chapter 3). We explored the building blocks of machine learning models and demonstrated how their architectures, when examined through the lens of computer architecture, reveal structural similarities (Chapter 4).\nThroughout this book, we have looked into the intricacies of ML systems, examining the critical components and best practices necessary to create a seamless and efficient pipeline. From data preprocessing and model training to deployment and monitoring, we have provided insights and guidance to help readers navigate the complex landscape of ML system development.\nML systems involve complex workflows, spanning various topics from data engineering to model deployment on diverse systems (Chapter 5). By providing an overview of these ML system components, we have aimed to showcase the tremendous depth and breadth of the field and expertise that is needed. Understanding the intricacies of ML workflows is crucial for practitioners and researchers alike, as it enables them to navigate the landscape effectively and develop robust, efficient, and impactful ML solutions.\nBy focusing on the systems aspect of ML, we aim to bridge the gap between theoretical knowledge and practical implementation. Just as a healthy human body system allows the organs to function optimally, a well-designed ML system enables the models to consistently deliver accurate and reliable results. This book’s goal is to empower readers with the knowledge and tools necessary to build ML systems that showcase the underlying models’ power and ensure smooth integration and operation, much like a well-functioning human body.",
    "crumbs": [
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Conclusion</span>"
    ]
  },
  {
    "objectID": "contents/core/conclusion/conclusion.html#knowing-the-importance-of-ml-datasets",
    "href": "contents/core/conclusion/conclusion.html#knowing-the-importance-of-ml-datasets",
    "title": "20  Conclusion",
    "section": "20.2 Knowing the Importance of ML Datasets",
    "text": "20.2 Knowing the Importance of ML Datasets\nOne of the key principles we have emphasized is that data is the foundation upon which ML systems are built (Chapter 6). Data is the new code that programs deep neural networks, making data engineering the first and most critical stage of any ML pipeline. That is why we began our exploration by diving into the basics of data engineering, recognizing that quality, diversity, and ethical sourcing are key to building robust and reliable machine learning models.\nThe importance of high-quality data must be balanced. Lapses in data quality can lead to significant negative consequences, such as flawed predictions, project terminations, and even potential harm to communities. These cascading effects, highlight the need for diligent data management and governance practices. ML practitioners must prioritize data quality, ensure diversity and representativeness, and adhere to ethical data collection and usage standards. By doing so, we can mitigate the risks associated with poor data quality and build ML systems that are trustworthy, reliable, and beneficial to society.",
    "crumbs": [
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Conclusion</span>"
    ]
  },
  {
    "objectID": "contents/core/conclusion/conclusion.html#navigating-the-ai-framework-landscape",
    "href": "contents/core/conclusion/conclusion.html#navigating-the-ai-framework-landscape",
    "title": "20  Conclusion",
    "section": "20.3 Navigating the AI Framework Landscape",
    "text": "20.3 Navigating the AI Framework Landscape\nThroughout this book, we have seen how machine learning frameworks serve as the backbone of modern ML systems. We dove into the evolution of different ML frameworks, dissecting the inner workings of popular ones like TensorFlow and PyTorch, and provided insights into the core components and advanced features that define them (Chapter 7). We also looked into the specialization of frameworks tailored to specific needs, such as those designed for embedded AI. We discussed the criteria for selecting the most suitable framework for a given project.\nOur exploration also touched upon the future trends expected to shape the landscape of ML frameworks in the coming years. As the field continues to evolve, we can anticipate the emergence of more specialized and optimized frameworks that cater to the unique requirements of different domains and deployment scenarios, as we saw with TensorFlow Lite for Microcontrollers. By staying abreast of these developments and understanding the tradeoffs involved in framework selection, we can make informed decisions and leverage the most appropriate tools to build efficient ML systems.",
    "crumbs": [
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Conclusion</span>"
    ]
  },
  {
    "objectID": "contents/core/conclusion/conclusion.html#understanding-ml-training-fundamentals",
    "href": "contents/core/conclusion/conclusion.html#understanding-ml-training-fundamentals",
    "title": "20  Conclusion",
    "section": "20.4 Understanding ML Training Fundamentals",
    "text": "20.4 Understanding ML Training Fundamentals\nWe saw how the AI training process is computationally intensive, making it challenging to scale and optimize. We began by examining the fundamentals of AI training (Chapter 8), which involves feeding data into ML models and adjusting their parameters to minimize the difference between predicted and actual outputs. This process requires careful consideration of various factors, such as the choice of optimization algorithms, learning rate, batch size, and regularization techniques.\nHowever, training ML models at scale poses significant system challenges. As datasets’ size and models’ complexity grow, the computational resources required for training can become prohibitively expensive. This has led to the development of distributed training techniques, such as data and model parallelism, which allow multiple devices to collaborate in the training process. Frameworks like TensorFlow and PyTorch have evolved to support these distributed training paradigms, enabling practitioners to scale their training workloads across clusters of GPUs or TPUs.\nIn addition to distributed training, we discussed techniques for optimizing the training process, such as mixed-precision training and gradient compression. It’s important to note that while these techniques may seem algorithmic, they significantly impact system performance. The choice of training algorithms, precision, and communication strategies directly affects the ML system’s resource utilization, scalability, and efficiency. Therefore, adopting an algorithm-hardware or algorithm-system co-design approach is crucial, where the algorithmic choices are made in tandem with the system considerations. By understanding the interplay between algorithms and hardware, we can make informed decisions that optimize the model performance and the system efficiency, ultimately leading to more effective and scalable ML solutions.",
    "crumbs": [
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Conclusion</span>"
    ]
  },
  {
    "objectID": "contents/core/conclusion/conclusion.html#pursuing-efficiency-in-ai-systems",
    "href": "contents/core/conclusion/conclusion.html#pursuing-efficiency-in-ai-systems",
    "title": "20  Conclusion",
    "section": "20.5 Pursuing Efficiency in AI Systems",
    "text": "20.5 Pursuing Efficiency in AI Systems\nDeploying trained ML models is more complex than simply running the networks; efficiency is critical (Chapter 9). In this chapter on AI efficiency, we emphasized that efficiency is not merely a luxury but a necessity in artificial intelligence systems. We dug into the key concepts underpinning AI systems’ efficiency, recognizing that the computational demands on neural networks can be daunting, even for minimal systems. For AI to be seamlessly integrated into everyday devices and essential systems, it must perform optimally within the constraints of limited resources while maintaining its efficacy.\nThroughout the book, we have highlighted the importance of pursuing efficiency to ensure that AI models are streamlined, rapid, and sustainable. By optimizing models for efficiency, we can widen their applicability across various platforms and scenarios, enabling AI to be deployed in resource-constrained environments such as embedded systems and edge devices. This pursuit of efficiency is necessary for the widespread adoption and practical implementation of AI technologies in real-world applications.",
    "crumbs": [
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Conclusion</span>"
    ]
  },
  {
    "objectID": "contents/core/conclusion/conclusion.html#optimizing-ml-model-architectures",
    "href": "contents/core/conclusion/conclusion.html#optimizing-ml-model-architectures",
    "title": "20  Conclusion",
    "section": "20.6 Optimizing ML Model Architectures",
    "text": "20.6 Optimizing ML Model Architectures\nWe then explored various model architectures, from the foundational perceptron to the sophisticated transformer networks, each tailored to specific tasks and data types. This exploration has showcased machine learning models’ remarkable diversity and adaptability, enabling them to tackle various problems across domains.\nHowever, when deploying these models on systems, especially resource-constrained embedded systems, model optimization becomes a necessity. The evolution of model architectures, from the early MobileNets designed for mobile devices to the more recent TinyML models optimized for microcontrollers, is a testament to the continued innovation.\nIn the chapter on model optimization (Chapter 10), we looked into the art and science of optimizing machine learning models to ensure they are lightweight, efficient, and effective when deployed in TinyML scenarios. We explored techniques such as model compression, quantization, and architecture search, which allow us to reduce the computational footprint of models while maintaining their performance. By applying these optimization techniques, we can create models tailored to the specific constraints of embedded systems, enabling the deployment of powerful AI capabilities on edge devices. This opens many possibilities for intelligent, real-time processing and decision-making in IoT, robotics, and mobile computing applications. As we continue pushing the boundaries of AI efficiency, we expect to see even more innovative solutions for deploying machine learning models in resource-constrained environments.",
    "crumbs": [
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Conclusion</span>"
    ]
  },
  {
    "objectID": "contents/core/conclusion/conclusion.html#advancing-ai-processing-hardware",
    "href": "contents/core/conclusion/conclusion.html#advancing-ai-processing-hardware",
    "title": "20  Conclusion",
    "section": "20.7 Advancing AI Processing Hardware",
    "text": "20.7 Advancing AI Processing Hardware\nOver the years, we have witnessed remarkable strides in ML hardware, driven by the insatiable demand for computational power and the need to address the challenges of resource constraints in real-world deployments (Chapter 11). These advancements have been crucial in enabling the deployment of powerful AI capabilities on devices with limited resources, opening up new possibilities across various industries.\nSpecialized hardware acceleration is essential to overcome these constraints and enable high-performance machine learning. Hardware accelerators, such as GPUs, FPGAs, and ASICs, optimize compute-intensive operations, particularly inference, by leveraging custom silicon designed for efficient matrix multiplications. These accelerators provide substantial speedups compared to general-purpose CPUs, enabling real-time execution of advanced ML models on devices with strict size, weight, and power limitations.\nWe have also explored the various techniques and approaches for hardware acceleration in embedded machine-learning systems. We discussed the tradeoffs in selecting the appropriate hardware for specific use cases and the importance of software optimizations to harness these accelerators’ capabilities fully. By understanding these concepts, ML practitioners can make informed decisions when designing and deploying ML systems.\nGiven the plethora of ML hardware solutions available, benchmarking has become essential to developing and deploying machine learning systems (Chapter 12). Benchmarking allows developers to measure and compare the performance of different hardware platforms, model architectures, training procedures, and deployment strategies. By utilizing well-established benchmarks like MLPerf, practitioners gain valuable insights into the most effective approaches for a given problem, considering the unique constraints of the target deployment environment.\nAdvancements in ML hardware, combined with insights gained from benchmarking and optimization techniques, have paved the way for successfully deploying machine learning capabilities on various devices, from powerful edge servers to resource-constrained microcontrollers. As the field continues to evolve, we expect to see even more innovative hardware solutions and benchmarking approaches that will further push the boundaries of what is possible with embedded machine learning systems.",
    "crumbs": [
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Conclusion</span>"
    ]
  },
  {
    "objectID": "contents/core/conclusion/conclusion.html#embracing-on-device-learning",
    "href": "contents/core/conclusion/conclusion.html#embracing-on-device-learning",
    "title": "20  Conclusion",
    "section": "20.8 Embracing On-Device Learning",
    "text": "20.8 Embracing On-Device Learning\nIn addition to the advancements in ML hardware, we also explored on-device learning, where models can adapt and learn directly on the device (Chapter 14). This approach has significant implications for data privacy and security, as sensitive information can be processed locally without the need for transmission to external servers.\nOn-device learning enhances privacy by keeping data within the confines of the device, reducing the risk of unauthorized access or data breaches. It also reduces reliance on cloud connectivity, enabling ML models to function effectively even in scenarios with limited or intermittent internet access. We have discussed techniques such as transfer learning and federated learning, which have expanded the capabilities of on-device learning. Transfer learning allows models to leverage knowledge gained from one task or domain to improve performance on another, enabling more efficient and effective learning on resource-constrained devices. On the other hand, Federated learning enables collaborative model updates across distributed devices without centralized data aggregation. This approach allows multiple devices to contribute to learning while keeping their data locally, enhancing privacy and security.\nThese advancements in on-device learning have paved the way for more secure, privacy-preserving, and decentralized machine learning applications. As we prioritize data privacy and security in developing ML systems, we expect to see more innovative solutions that enable powerful AI capabilities while protecting sensitive information and ensuring user privacy.",
    "crumbs": [
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Conclusion</span>"
    ]
  },
  {
    "objectID": "contents/core/conclusion/conclusion.html#streamlining-ml-operations",
    "href": "contents/core/conclusion/conclusion.html#streamlining-ml-operations",
    "title": "20  Conclusion",
    "section": "20.9 Streamlining ML Operations",
    "text": "20.9 Streamlining ML Operations\nEven if we got the above pieces right, challenges and considerations must be addressed to ensure ML models’ successful integration and operation in production environments. In the MLOps chapter (Chapter 13), we studied the practices and architectures necessary to develop, deploy, and manage ML models throughout their entire lifecycle. We looked at the phases of ML, from data collection and model training to evaluation, deployment, and ongoing monitoring.\nWe learned about the importance of automation, collaboration, and continuous improvement in MLOps. By automating key processes, teams can streamline their workflows, reduce manual errors, and accelerate the deployment of ML models. Collaboration among diverse teams, including data scientists, engineers, and domain experts, ensures ML systems’ successful development and deployment.\nThe ultimate goal of this chapter was to provide readers with a comprehensive understanding of ML model management, equipping them with the knowledge and tools necessary to build and run ML applications that deliver sustained value successfully. By adopting best practices in MLOps, organizations can ensure their ML initiatives’ long-term success and impact, driving innovation and delivering meaningful results.",
    "crumbs": [
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Conclusion</span>"
    ]
  },
  {
    "objectID": "contents/core/conclusion/conclusion.html#ensuring-security-and-privacy",
    "href": "contents/core/conclusion/conclusion.html#ensuring-security-and-privacy",
    "title": "20  Conclusion",
    "section": "20.10 Ensuring Security and Privacy",
    "text": "20.10 Ensuring Security and Privacy\nNo ML system is ever complete without thinking about security and privacy. They are of major importance when developing real-world ML systems. As machine learning finds increasing application in sensitive domains such as healthcare, finance, and personal data, safeguarding confidentiality and preventing the misuse of data and models becomes a critical imperative, and these were the concepts we discussed previously (Chapter 15). We examined security issues from multiple perspectives, starting with threats to models themselves, such as model theft and data poisoning. We also discussed the importance of hardware security, exploring topics like hardware bugs, physical attacks, and the unique security challenges faced by embedded devices.\nIn addition to security, we addressed the critical issue of data privacy. Techniques such as differential privacy were highlighted as tools to protect sensitive information. We also discussed the growing role of legislation in enforcing privacy protections, ensuring that user data is handled responsibly and transparently.",
    "crumbs": [
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Conclusion</span>"
    ]
  },
  {
    "objectID": "contents/core/conclusion/conclusion.html#upholding-ethical-considerations",
    "href": "contents/core/conclusion/conclusion.html#upholding-ethical-considerations",
    "title": "20  Conclusion",
    "section": "20.11 Upholding Ethical Considerations",
    "text": "20.11 Upholding Ethical Considerations\nAs we embrace ML advancements in all facets of our lives, it is essential to remain mindful of the ethical considerations that will shape the future of AI (Chapter 16). Fairness, transparency, accountability, and privacy in AI systems will be paramount as they become more integrated into our lives and decision-making processes.\nAs AI systems become more pervasive and influential, it is important to ensure that they are designed and deployed in a manner that upholds ethical principles. This means actively mitigating biases, promoting fairness, and preventing discriminatory outcomes. Additionally, ethical AI design ensures transparency in how AI systems make decisions, enabling users to understand and trust their outputs.\nAccountability is another critical ethical consideration. As AI systems take on more responsibilities and make decisions that impact individuals and society, there must be clear mechanisms for holding these systems and their creators accountable. This includes establishing frameworks for auditing and monitoring AI systems and defining liability and redress mechanisms in case of harm or unintended consequences.\nEthical frameworks, regulations, and standards will be essential to address these ethical challenges. These frameworks should guide the responsible development and deployment of AI technologies, ensuring that they align with societal values and promote the well-being of individuals and communities.\nMoreover, ongoing discussions and collaborations among researchers, practitioners, policymakers, and society will be important in navigating the ethical landscape of AI. These conversations should be inclusive and diverse, bringing together different perspectives and expertise to develop comprehensive and equitable solutions. As we move forward, it is the collective responsibility of all stakeholders to prioritize ethical considerations in the development and deployment of AI systems.",
    "crumbs": [
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Conclusion</span>"
    ]
  },
  {
    "objectID": "contents/core/conclusion/conclusion.html#promoting-sustainability",
    "href": "contents/core/conclusion/conclusion.html#promoting-sustainability",
    "title": "20  Conclusion",
    "section": "20.12 Promoting Sustainability",
    "text": "20.12 Promoting Sustainability\nThe increasing computational demands of machine learning, particularly for training large models, have raised concerns about their environmental impact due to high energy consumption and carbon emissions (Chapter 17). As the scale and complexity of models continue to grow, addressing the sustainability challenges associated with AI development becomes imperative. To mitigate the environmental footprint of AI, the development of energy-efficient algorithms is necessary. This involves optimizing models and training procedures to minimize computational requirements while maintaining performance. Techniques such as model compression, quantization, and efficient neural architecture search can help reduce the energy consumption of AI systems.\nUsing renewable energy sources to power AI infrastructure is another important step towards sustainability. By transitioning to clean energy sources such as solar, wind, and hydropower, the carbon emissions associated with AI development can be significantly reduced. This requires a concerted effort from the AI community and support from policymakers and industry leaders to invest in and adopt renewable energy solutions. In addition, exploring alternative computing paradigms, such as neuromorphic and photonic computing, holds promise for developing more energy-efficient AI systems. By developing hardware and algorithms that emulate the brain’s processing mechanisms, we can potentially create AI systems that are both powerful and sustainable.\nThe AI community must prioritize sustainability as a key consideration in research and development. This involves investing in green computing initiatives, such as developing energy-efficient hardware and optimizing data centers for reduced energy consumption. It also requires collaboration across disciplines, bringing together AI, energy, and sustainability experts to develop holistic solutions.\nMoreover, it is important to acknowledge that access to AI and machine learning compute resources may not be equally distributed across organizations and regions. This disparity can lead to a widening gap between those who have the means to leverage advanced AI technologies and those who do not. Organizations like the Organisation for Economic Cooperation and Development (OECD) are actively exploring ways to address this issue and promote greater equity in AI access and adoption. By fostering international cooperation, sharing best practices, and supporting capacity-building initiatives, we can ensure that AI’s benefits are more widely accessible and that no one is left behind in the AI revolution.",
    "crumbs": [
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Conclusion</span>"
    ]
  },
  {
    "objectID": "contents/core/conclusion/conclusion.html#enhancing-robustness-and-resiliency",
    "href": "contents/core/conclusion/conclusion.html#enhancing-robustness-and-resiliency",
    "title": "20  Conclusion",
    "section": "20.13 Enhancing Robustness and Resiliency",
    "text": "20.13 Enhancing Robustness and Resiliency\nThe chapter on Robust AI dives into the fundamental concepts, techniques, and tools for building fault-tolerant and error-resilient ML systems (Chapter 18). In this chapter, we explored how, when developing machine learning systems, making them robust means accounting for hardware faults through techniques like redundant hardware, ensuring your model is resilient to issues like data poisoning and distribution shifts, and addressing software faults such as bugs, design flaws, and implementation errors.\nBy employing robust AI techniques, ML systems can maintain their reliability, safety, and performance even in adverse conditions. These techniques enable systems to detect and recover from faults, adapt to changing environments, and make decisions under uncertainty.\nThe chapter empowers researchers and practitioners to develop AI solutions that can withstand the complexities and uncertainties of real-world environments. It provides insights into the design principles, architectures, and algorithms underpinning robust AI systems and practical guidance on implementing and validating these systems.",
    "crumbs": [
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Conclusion</span>"
    ]
  },
  {
    "objectID": "contents/core/conclusion/conclusion.html#shaping-the-future-of-ml-systems",
    "href": "contents/core/conclusion/conclusion.html#shaping-the-future-of-ml-systems",
    "title": "20  Conclusion",
    "section": "20.14 Shaping the Future of ML Systems",
    "text": "20.14 Shaping the Future of ML Systems\nAs we look to the future, the trajectory of ML systems points towards a paradigm shift from a model-centric approach to a more data-centric one. This shift recognizes that the quality and diversity of data are paramount to developing robust, reliable, and fair AI models.\nWe anticipate a growing emphasis on data curation, labeling, and augmentation techniques in the coming years. These practices aim to ensure that models are trained on high-quality, representative data that accurately reflects the complexities and nuances of real-world scenarios. By focusing on data quality and diversity, we can mitigate the risks of biased or skewed models that may perpetuate unfair or discriminatory outcomes.\nThis data-centric approach will be vital in addressing the challenges of bias, fairness, and generalizability in ML systems. By actively seeking out and incorporating diverse and inclusive datasets, we can develop more robust, equitable, and applicable models for various contexts and populations. Moreover, the emphasis on data will drive advancements in techniques such as data augmentation, where existing datasets are expanded and diversified through data synthesis, translation, and generation. These techniques can help overcome the limitations of small or imbalanced datasets, enabling the development of more accurate and generalizable models.\nIn recent years, generative AI has taken the field by storm, demonstrating remarkable capabilities in creating realistic images, videos, and text. However, the rise of generative AI also brings new challenges for ML systems. Unlike traditional ML systems, generative models often demand more computational resources and pose challenges in terms of scalability and efficiency. Furthermore, evaluating and benchmarking generative models presents difficulties, as traditional metrics used for classification tasks may not be directly applicable. Developing robust evaluation frameworks for generative models is an active area of research, and something we hope to write about soon!\nUnderstanding and addressing these system challenges and ethical considerations will be important in shaping the future of generative AI and its impact on society. As ML practitioners and researchers, we are responsible for advancing the technical capabilities of generative models and developing robust systems and frameworks that can mitigate potential risks and ensure the beneficial application of this powerful technology.",
    "crumbs": [
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Conclusion</span>"
    ]
  },
  {
    "objectID": "contents/core/conclusion/conclusion.html#applying-ai-for-good",
    "href": "contents/core/conclusion/conclusion.html#applying-ai-for-good",
    "title": "20  Conclusion",
    "section": "20.15 Applying AI for Good",
    "text": "20.15 Applying AI for Good\nThe potential for AI to be used for social good is vast, provided that responsible ML systems are developed and deployed at scale across various use cases (Chapter 19). To realize this potential, it is essential for researchers and practitioners to actively engage in the process of learning, experimentation, and pushing the boundaries of what is possible.\nThroughout the development of ML systems, it is important to remember the key themes and lessons explored in this book. These include the importance of data quality and diversity, the pursuit of efficiency and robustness, the potential of TinyML and neuromorphic computing, and the imperative of security and privacy. These insights inform the work and guide the decisions of those involved in developing AI systems.\nIt is important to recognize that the development of AI is not solely a technical endeavor but also a deeply human one. It requires collaboration, empathy, and a commitment to understanding the societal implications of the systems being created. Engaging with experts from diverse fields, such as ethics, social sciences, and policy, is essential to ensure that the AI systems developed are technically sound, socially responsible, and beneficial. Embracing the opportunity to be part of this transformative field and shaping its future is a privilege and a responsibility. By working together, we can create a world where ML systems serve as tools for positive change and improving the human condition.",
    "crumbs": [
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Conclusion</span>"
    ]
  },
  {
    "objectID": "contents/core/conclusion/conclusion.html#congratulations",
    "href": "contents/core/conclusion/conclusion.html#congratulations",
    "title": "20  Conclusion",
    "section": "20.16 Congratulations",
    "text": "20.16 Congratulations\nCongratulations on coming this far, and best of luck in your future endeavors! The future of AI is bright and filled with endless possibilities. It will be exciting to see the incredible contributions you will make to this field.\nFeel free to reach out to me anytime at vj at eecs dot harvard dot edu.\n– Prof. Vijay Janapa Reddi, Harvard University",
    "crumbs": [
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Conclusion</span>"
    ]
  },
  {
    "objectID": "contents/labs/overview.html",
    "href": "contents/labs/overview.html",
    "title": "Overview",
    "section": "",
    "text": "Learning Objectives\nWelcome to the hands-on labs section where you’ll explore deploying ML models onto real embedded devices, which will offer a practical introduction to ML systems. Unlike traditional approaches with large-scale models, these labs focus on interacting directly with both hardware and software. They help us show case various sensor modalities across different application use cases. This approach provides valuable insights into the challenges and opportunities of deploying AI on real physical systems.\nBy completing these labs, we hope learners will:",
    "crumbs": [
      "Overview"
    ]
  },
  {
    "objectID": "contents/labs/overview.html#learning-objectives",
    "href": "contents/labs/overview.html#learning-objectives",
    "title": "Overview",
    "section": "",
    "text": "Tip\n\n\n\n\nGain proficiency in setting up and deploying ML models on supported devices, enabling you to tackle real-world ML deployment scenarios with confidence.\nUnderstand the steps involved in adapting and experimenting with ML models for different applications, allowing you to optimize performance and efficiency.\nLearn troubleshooting techniques specific to embedded ML deployments equipping you with the skills to overcome common pitfalls and challenges.\nAcquire practical experience in deploying TinyML models on embedded devices bridging the gap between theory and practice.\nExplore various sensor modalities and their applications expanding your understanding of how ML can be leveraged in diverse domains.\nFoster an understanding of the real-world implications and challenges associated with ML system deployments preparing you for future projects.",
    "crumbs": [
      "Overview"
    ]
  },
  {
    "objectID": "contents/labs/overview.html#target-audience",
    "href": "contents/labs/overview.html#target-audience",
    "title": "Overview",
    "section": "Target Audience",
    "text": "Target Audience\nThese labs are designed for:\n\nBeginners in the field of machine learning who have a keen interest in exploring the intersection of ML and embedded systems.\nDevelopers and engineers looking to apply ML models to real-world applications using low-power, resource-constrained devices.\nEnthusiasts and researchers who want to gain practical experience in deploying AI on edge devices and understand the unique challenges involved.",
    "crumbs": [
      "Overview"
    ]
  },
  {
    "objectID": "contents/labs/overview.html#supported-devices",
    "href": "contents/labs/overview.html#supported-devices",
    "title": "Overview",
    "section": "Supported Devices",
    "text": "Supported Devices\nWe have included laboratory materials for three key devices that represent different hardware profiles and capabilities.\n\nNicla Vision: Optimized for vision-based applications like image classification and object detection, ideal for compact, low-power use cases.\nXIAO ESP32S3: A versatile, compact board suitable for keyword spotting and motion detection tasks.\nRaspberry Pi: A flexible platform for more computationally intensive tasks, including small language models and various classification and detection applications.\n\n\n\n\nExercise\nNicla Vision\nXIAO ESP32S3\nRaspberry Pi\n\n\n\n\nInstallation & Setup\n✓\n✓\n✓\n\n\nKeyword Spotting (KWS)\n✓\n✓\n\n\n\nImage Classification\n✓\n✓\n✓\n\n\nObject Detection\n✓\n✓\n✓\n\n\nMotion Detection\n✓\n✓\n\n\n\nSmall Language Models (SLM)\n\n\n✓\n\n\nVision Language Models (VLM)\n\n\n✓",
    "crumbs": [
      "Overview"
    ]
  },
  {
    "objectID": "contents/labs/overview.html#lab-structure",
    "href": "contents/labs/overview.html#lab-structure",
    "title": "Overview",
    "section": "Lab Structure",
    "text": "Lab Structure\nEach lab follows a structured approach:\n\nIntroduction: Explore the application and its significance in real-world scenarios.\nSetup: Step-by-step instructions to configure the hardware and software environment.\nDeployment: Guidance on training and deploying the pre-trained ML models on supported devices.\nExercises: Hands-on tasks to modify and experiment with model parameters.\nDiscussion: Analysis of results, potential improvements, and practical insights.",
    "crumbs": [
      "Overview"
    ]
  },
  {
    "objectID": "contents/labs/overview.html#recommended-lab-sequence",
    "href": "contents/labs/overview.html#recommended-lab-sequence",
    "title": "Overview",
    "section": "Recommended Lab Sequence",
    "text": "Recommended Lab Sequence\nIf you’re new to embedded ML, we suggest starting with setup and keyword spotting before moving on to image classification and object detection. Raspberry Pi users can explore more advanced tasks, like small language models, after familiarizing themselves with the basics.",
    "crumbs": [
      "Overview"
    ]
  },
  {
    "objectID": "contents/labs/overview.html#troubleshooting-and-support",
    "href": "contents/labs/overview.html#troubleshooting-and-support",
    "title": "Overview",
    "section": "Troubleshooting and Support",
    "text": "Troubleshooting and Support\nIf you encounter any issues during the labs, consult the troubleshooting comments or check the FAQs within each lab. For further assistance, feel free to reach out to our support team or engage with the community forums.",
    "crumbs": [
      "Overview"
    ]
  },
  {
    "objectID": "contents/labs/overview.html#credits",
    "href": "contents/labs/overview.html#credits",
    "title": "Overview",
    "section": "Credits",
    "text": "Credits\nSpecial credit and thanks to Prof. Marcelo Rovai for his valuable contributions to the development and continuous refinement of these labs.",
    "crumbs": [
      "Overview"
    ]
  },
  {
    "objectID": "contents/labs/getting_started.html",
    "href": "contents/labs/getting_started.html",
    "title": "Getting Started",
    "section": "",
    "text": "Hardware Requirements\nWelcome to the exciting world of embedded machine learning and TinyML! In this hands-on lab series, you’ll explore various projects demonstrating the power of running machine learning models on resource-constrained devices. Before diving into the projects, ensure you have the necessary hardware and software.\nTo follow along with the hands-on labs, you’ll need the following hardware:\nThe Arduino Nicla Vision is tailored for professional-grade applications, offering advanced features and performance suitable for demanding industrial projects. On the other hand, the Seeed Studio XIAO ESP32S3 Sense is geared toward makers, hobbyists, and students who want to explore edge AI applications in a more accessible and beginner-friendly format. Both boards have their strengths and target audiences, allowing users to choose the best fit for their needs and skill level. The Raspberry Pi is aimed at more advanced engineering and machine learning projects.",
    "crumbs": [
      "Getting Started"
    ]
  },
  {
    "objectID": "contents/labs/getting_started.html#hardware-requirements",
    "href": "contents/labs/getting_started.html#hardware-requirements",
    "title": "Getting Started",
    "section": "",
    "text": "Arduino Nicla Vision board\n\nThe Arduino Nicla Vision is a powerful, compact board designed for professional-grade computer vision and audio applications. It features a high-quality camera module, a digital microphone, and an IMU, making it suitable for demanding projects in industries such as robotics, automation, and surveillance.\nArduino Nicla Vision specifications\nArduino Nicla Vision pinout diagram\n\nXIAO ESP32S3 Sense board\n\nThe Seeed Studio XIAO ESP32S3 Sense is a tiny, feature-packed board designed for makers, hobbyists, and students interested in exploring edge AI applications. It comes with a camera, microphone, and IMU, making it easy to get started with projects like image classification, keyword spotting, and motion detection.\nXIAO ESP32S3 Sense specifications\nXIAO ESP32S3 Sense pinout diagram\n\nRaspberry Pi Single Computer board\n\n\nThe Raspberry Pi is a powerful and versatile single-board computer that has become an essential tool for engineers across various disciplines. Developed by the Raspberry Pi Foundation, these compact devices offer a unique combination of affordability, computational power, and extensive GPIO (General Purpose Input/Output) capabilities, making them ideal for prototyping, embedded systems development, and advanced engineering projects.\nRaspberry Pi Hardware Documentation\nCamera Documentation\n\n\nAdditional accessories\n\nUSB-C cable for programming and powering the XIAO\nMicro-USB cable for programming and powering the Nicla\nPower Supply for the Raspberries\nBreadboard and jumper wires (optional, for connecting additional sensors)",
    "crumbs": [
      "Getting Started"
    ]
  },
  {
    "objectID": "contents/labs/getting_started.html#software-requirements",
    "href": "contents/labs/getting_started.html#software-requirements",
    "title": "Getting Started",
    "section": "Software Requirements",
    "text": "Software Requirements\nTo program the boards and develop embedded machine learning projects, you’ll need the following software:\n\nArduino IDE\n\nDownload and install\n\nInstall Arduino IDE\nFollow the installation guide for your specific OS.\nArduino CLI\nConfigure the Arduino IDE for the Arduino Nicla Vision and XIAO ESP32S3 Sense boards.\n\n\nOpenMV IDE (optional)\n\nDownload and install the OpenMV IDE for your operating system.\nConfigure the OpenMV IDE for the Arduino Nicla Vision.\n\nEdge Impulse Studio\n\nSign up for a free account on the Edge Impulse Studio.\nInstall Edge Impulse CLI\nFollow the guides to connect your Arduino Nicla Vision and XIAO ESP32S3 Sense boards to Edge Impulse Studio.\n\nRaspberry Pi OS\n\n\nDownload and install the Raspberry Pi Imager",
    "crumbs": [
      "Getting Started"
    ]
  },
  {
    "objectID": "contents/labs/getting_started.html#network-connectivity",
    "href": "contents/labs/getting_started.html#network-connectivity",
    "title": "Getting Started",
    "section": "Network Connectivity",
    "text": "Network Connectivity\nSome projects may require internet connectivity for data collection or model deployment. Ensure your development environment connection is stable through Wi-Fi or Ethernet. For the Raspberry Pi, having a Wi-Fi or Ethernet connection is necessary for remote operation without the necessity to plug in a monitor, keyboard, and mouse.\n\nFor the Arduino Nicla Vision, you can use the onboard Wi-Fi module to connect to a wireless network.\nFor the XIAO ESP32S3 Sense, you can use the onboard Wi-Fi module or connect an external Wi-Fi or Ethernet module using the available pins.\nFor the Raspberry Pi, you can use the onboard Wi-Fi module to connect an external Wi-Fi or Ethernet module using the available connector.",
    "crumbs": [
      "Getting Started"
    ]
  },
  {
    "objectID": "contents/labs/getting_started.html#conclusion",
    "href": "contents/labs/getting_started.html#conclusion",
    "title": "Getting Started",
    "section": "Conclusion",
    "text": "Conclusion\nWith your hardware and software set up, you’re ready to embark on your embedded machine learning journey. The hands-on labs will guide you through various projects, covering topics like image classification, object detection, keyword spotting, and motion classification.\nIf you encounter any issues or have questions, don’t hesitate to consult the troubleshooting guides or forums or seek support from the community.\nLet’s dive in and unlock the potential of ML on real (tiny) systems!",
    "crumbs": [
      "Getting Started"
    ]
  },
  {
    "objectID": "contents/labs/arduino/nicla_vision/nicla_vision.html",
    "href": "contents/labs/arduino/nicla_vision/nicla_vision.html",
    "title": "Nicla Vision",
    "section": "",
    "text": "Pre-requisites\nThese labs provide a unique opportunity to gain practical experience with machine learning (ML) systems. Unlike working with large models requiring data center-scale resources, these exercises allow you to directly interact with hardware and software using TinyML. This hands-on approach gives you a tangible understanding of the challenges and opportunities in deploying AI, albeit at a tiny scale. However, the principles are largely the same as what you would encounter when working with larger systems.",
    "crumbs": [
      "Nicla Vision"
    ]
  },
  {
    "objectID": "contents/labs/arduino/nicla_vision/nicla_vision.html#pre-requisites",
    "href": "contents/labs/arduino/nicla_vision/nicla_vision.html#pre-requisites",
    "title": "Nicla Vision",
    "section": "",
    "text": "Nicla Vision Board: Ensure you have the Nicla Vision board.\nUSB Cable: For connecting the board to your computer.\nNetwork: With internet access for downloading necessary software.",
    "crumbs": [
      "Nicla Vision"
    ]
  },
  {
    "objectID": "contents/labs/arduino/nicla_vision/nicla_vision.html#setup",
    "href": "contents/labs/arduino/nicla_vision/nicla_vision.html#setup",
    "title": "Nicla Vision",
    "section": "Setup",
    "text": "Setup\n\nSetup Nicla Vision",
    "crumbs": [
      "Nicla Vision"
    ]
  },
  {
    "objectID": "contents/labs/arduino/nicla_vision/nicla_vision.html#exercises",
    "href": "contents/labs/arduino/nicla_vision/nicla_vision.html#exercises",
    "title": "Nicla Vision",
    "section": "Exercises",
    "text": "Exercises\n\n\n\nModality\nTask\nDescription\nLink\n\n\n\n\nVision\nImage Classification\nLearn to classify images\nLink\n\n\nVision\nObject Detection\nImplement object detection\nLink\n\n\nSound\nKeyword Spotting\nExplore voice recognition systems\nLink\n\n\nIMU\nMotion Classification and Anomaly Detection\nClassify motion data and detect anomalies\nLink",
    "crumbs": [
      "Nicla Vision"
    ]
  },
  {
    "objectID": "contents/labs/arduino/nicla_vision/setup/setup.html",
    "href": "contents/labs/arduino/nicla_vision/setup/setup.html",
    "title": "Setup",
    "section": "",
    "text": "Overview\nThe Arduino Nicla Vision (sometimes called NiclaV) is a development board that includes two processors that can run tasks in parallel. It is part of a family of development boards with the same form factor but designed for specific tasks, such as the Nicla Sense ME and the Nicla Voice. The Niclas can efficiently run processes created with TensorFlow Lite. For example, one of the cores of the NiclaV runs a computer vision algorithm on the fly (inference), while the other executes low-level operations like controlling a motor and communicating or acting as a user interface. The onboard wireless module allows the management of WiFi and Bluetooth Low Energy (BLE) connectivity simultaneously.",
    "crumbs": [
      "Nicla Vision",
      "Setup"
    ]
  },
  {
    "objectID": "contents/labs/arduino/nicla_vision/setup/setup.html#hardware",
    "href": "contents/labs/arduino/nicla_vision/setup/setup.html#hardware",
    "title": "Setup",
    "section": "Hardware",
    "text": "Hardware\n\nTwo Parallel Cores\nThe central processor is the dual-core STM32H747, including a Cortex M7 at 480 MHz and a Cortex M4 at 240 MHz. The two cores communicate via a Remote Procedure Call mechanism that seamlessly allows calling functions on the other processor. Both processors share all the on-chip peripherals and can run:\n\nArduino sketches on top of the Arm Mbed OS\nNative Mbed applications\nMicroPython / JavaScript via an interpreter\nTensorFlow Lite\n\n\n\n\nMemory\nMemory is crucial for embedded machine learning projects. The NiclaV board can host up to 16 MB of QSPI Flash for storage. However, it is essential to consider that the MCU SRAM is the one to be used with machine learning inferences; the STM32H747 is only 1MB, shared by both processors. This MCU also has incorporated 2MB of FLASH, mainly for code storage.\n\n\nSensors\n\nCamera: A GC2145 2 MP Color CMOS Camera.\nMicrophone: The MP34DT05 is an ultra-compact, low-power, omnidirectional, digital MEMS microphone built with a capacitive sensing element and the IC interface.\n6-Axis IMU: 3D gyroscope and 3D accelerometer data from the LSM6DSOX 6-axis IMU.\nTime of Flight Sensor: The VL53L1CBV0FY Time-of-Flight sensor adds accurate and low power-ranging capabilities to the Nicla Vision. The invisible near-infrared VCSEL laser (including the analog driver) is encapsulated with receiving optics in an all-in-one small module below the camera.",
    "crumbs": [
      "Nicla Vision",
      "Setup"
    ]
  },
  {
    "objectID": "contents/labs/arduino/nicla_vision/setup/setup.html#arduino-ide-installation",
    "href": "contents/labs/arduino/nicla_vision/setup/setup.html#arduino-ide-installation",
    "title": "Setup",
    "section": "Arduino IDE Installation",
    "text": "Arduino IDE Installation\nStart connecting the board (microUSB) to your computer:\n\nInstall the Mbed OS core for Nicla boards in the Arduino IDE. Having the IDE open, navigate to Tools &gt; Board &gt; Board Manager, look for Arduino Nicla Vision on the search window, and install the board.\n\nNext, go to Tools &gt; Board &gt; Arduino Mbed OS Nicla Boards and select Arduino Nicla Vision. Having your board connected to the USB, you should see the Nicla on Port and select it.\n\nOpen the Blink sketch on Examples/Basic and run it using the IDE Upload button. You should see the Built-in LED (green RGB) blinking, which means the Nicla board is correctly installed and functional!\n\n\nTesting the Microphone\nOn Arduino IDE, go to Examples &gt; PDM &gt; PDMSerialPlotter, open and run the sketch. Open the Plotter and see the audio representation from the microphone:\n\n\nVary the frequency of the sound you generate and confirm that the mic is working correctly.\n\n\n\nTesting the IMU\nBefore testing the IMU, it will be necessary to install the LSM6DSOX library. For that, go to Library Manager and look for LSM6DSOX. Install the library provided by Arduino:\n\nNext, go to Examples &gt; Arduino_LSM6DSOX &gt; SimpleAccelerometer and run the accelerometer test (you can also run Gyro and board temperature):\n\n\n\nTesting the ToF (Time of Flight) Sensor\nAs we did with IMU, it is necessary to install the VL53L1X ToF library. For that, go to Library Manager and look for VL53L1X. Install the library provided by Pololu:\n\nNext, run the sketch proximity_detection.ino:\n\nOn the Serial Monitor, you will see the distance from the camera to an object in front of it (max of 4m).\n\n\n\nTesting the Camera\nWe can also test the camera using, for example, the code provided on Examples &gt; Camera &gt; CameraCaptureRawBytes. We cannot see the image directly, but it is possible to get the raw image data generated by the camera.\nAnyway, the best test with the camera is to see a live image. For that, we will use another IDE, the OpenMV.",
    "crumbs": [
      "Nicla Vision",
      "Setup"
    ]
  },
  {
    "objectID": "contents/labs/arduino/nicla_vision/setup/setup.html#installing-the-openmv-ide",
    "href": "contents/labs/arduino/nicla_vision/setup/setup.html#installing-the-openmv-ide",
    "title": "Setup",
    "section": "Installing the OpenMV IDE",
    "text": "Installing the OpenMV IDE\nOpenMV IDE is the premier integrated development environment with OpenMV Cameras like the one on the Nicla Vision. It features a powerful text editor, debug terminal, and frame buffer viewer with a histogram display. We will use MicroPython to program the camera.\nGo to the OpenMV IDE page, download the correct version for your Operating System, and follow the instructions for its installation on your computer.\n\nThe IDE should open, defaulting to the helloworld_1.py code on its Code Area. If not, you can open it from Files &gt; Examples &gt; HelloWord &gt; helloword.py\n\nAny messages sent through a serial connection (using print() or error messages) will be displayed on the Serial Terminal during run time. The image captured by a camera will be displayed in the Camera Viewer Area (or Frame Buffer) and in the Histogram area, immediately below the Camera Viewer.\n\nBefore connecting the Nicla to the OpenMV IDE, ensure you have the latest bootloader version. Go to your Arduino IDE, select the Nicla board, and open the sketch on Examples &gt; STM_32H747_System STM32H747_manageBootloader. Upload the code to your board. The Serial Monitor will guide you.\n\nAfter updating the bootloader, put the Nicla Vision in bootloader mode by double-pressing the reset button on the board. The built-in green LED will start fading in and out. Now return to the OpenMV IDE and click on the connect icon (Left ToolBar):\n\nA pop-up will tell you that a board in DFU mode was detected and ask how you would like to proceed. First, select Install the latest release firmware (vX.Y.Z). This action will install the latest OpenMV firmware on the Nicla Vision.\n\nYou can leave the option Erase internal file system unselected and click [OK].\nNicla’s green LED will start flashing while the OpenMV firmware is uploaded to the board, and a terminal window will then open, showing the flashing progress.\n\nWait until the green LED stops flashing and fading. When the process ends, you will see a message saying, “DFU firmware update complete!”. Press [OK].\n\nA green play button appears when the Nicla Vison connects to the Tool Bar.\n\nAlso, note that a drive named “NO NAME” will appear on your computer.:\n\nEvery time you press the [RESET] button on the board, it automatically executes the main.py script stored on it. You can load the main.py code on the IDE (File &gt; Open File...).\n\n\nThis code is the “Blink” code, confirming that the HW is OK.\n\nFor testing the camera, let’s run helloword_1.py. For that, select the script on File &gt; Examples &gt; HelloWorld &gt; helloword.py,\nWhen clicking the green play button, the MicroPython script (hellowolrd.py) on the Code Area will be uploaded and run on the Nicla Vision. On-Camera Viewer, you will start to see the video streaming. The Serial Monitor will show us the FPS (Frames per second), which should be around 14fps.\n\nHere is the helloworld.py script:\n# Hello World Example 2\n#\n# Welcome to the OpenMV IDE! Click on the green run arrow button below to run the script!\n\nimport sensor, image, time\n\nsensor.reset()                      # Reset and initialize the sensor.\nsensor.set_pixformat(sensor.RGB565) # Set pixel format to RGB565 (or GRAYSCALE)\nsensor.set_framesize(sensor.QVGA)   # Set frame size to QVGA (320x240)\nsensor.skip_frames(time = 2000)     # Wait for settings take effect.\nclock = time.clock()                # Create a clock object to track the FPS.\n\nwhile(True):\n    clock.tick()                    # Update the FPS clock.\n    img = sensor.snapshot()         # Take a picture and return the image.\n    print(clock.fps())\nIn GitHub, you can find the Python scripts used here.\nThe code can be split into two parts:\n\nSetup: Where the libraries are imported, initialized and the variables are defined and initiated.\nLoop: (while loop) part of the code that runs continually. The image (img variable) is captured (one frame). Each of those frames can be used for inference in Machine Learning Applications.\n\nTo interrupt the program execution, press the red [X] button.\n\nNote: OpenMV Cam runs about half as fast when connected to the IDE. The FPS should increase once disconnected.\n\nIn the GitHub, You can find other Python scripts. Try to test the onboard sensors.",
    "crumbs": [
      "Nicla Vision",
      "Setup"
    ]
  },
  {
    "objectID": "contents/labs/arduino/nicla_vision/setup/setup.html#connecting-the-nicla-vision-to-edge-impulse-studio",
    "href": "contents/labs/arduino/nicla_vision/setup/setup.html#connecting-the-nicla-vision-to-edge-impulse-studio",
    "title": "Setup",
    "section": "Connecting the Nicla Vision to Edge Impulse Studio",
    "text": "Connecting the Nicla Vision to Edge Impulse Studio\nWe will need the Edge Impulse Studio later in other exercises. Edge Impulse is a leading development platform for machine learning on edge devices.\nEdge Impulse officially supports the Nicla Vision. So, for starting, please create a new project on the Studio and connect the Nicla to it. For that, follow the steps:\n\nDownload the most updated EI Firmware and unzip it.\nOpen the zip file on your computer and select the uploader corresponding to your OS:\n\n\n\nPut the Nicla-Vision on Boot Mode, pressing the reset button twice.\nExecute the specific batch code for your OS for uploading the binary arduino-nicla-vision.bin to your board.\n\nGo to your project on the Studio, and on the Data Acquisition tab, select WebUSB (1). A window will pop up; choose the option that shows that the Nicla is paired (2) and press [Connect] (3).\n\nIn the Collect Data section on the Data Acquisition tab, you can choose which sensor data to pick.\n\nFor example. IMU data:\n\nOr Image (Camera):\n\nAnd so on. You can also test an external sensor connected to the ADC (Nicla pin 0) and the other onboard sensors, such as the microphone and the ToF.",
    "crumbs": [
      "Nicla Vision",
      "Setup"
    ]
  },
  {
    "objectID": "contents/labs/arduino/nicla_vision/setup/setup.html#expanding-the-nicla-vision-board-optional",
    "href": "contents/labs/arduino/nicla_vision/setup/setup.html#expanding-the-nicla-vision-board-optional",
    "title": "Setup",
    "section": "Expanding the Nicla Vision Board (optional)",
    "text": "Expanding the Nicla Vision Board (optional)\nA last item to be explored is that sometimes, during prototyping, it is essential to experiment with external sensors and devices, and an excellent expansion to the Nicla is the Arduino MKR Connector Carrier (Grove compatible).\nThe shield has 14 Grove connectors: five single analog inputs (A0-A5), one double analog input (A5/A6), five single digital I/Os (D0-D4), one double digital I/O (D5/D6), one I2C (TWI), and one UART (Serial). All connectors are 5V compatible.\n\nNote that all 17 Nicla Vision pins will be connected to the Shield Groves, but some Grove connections remain disconnected.\n\n\nThis shield is MKR compatible and can be used with the Nicla Vision and Portenta.\n\nFor example, suppose that on a TinyML project, you want to send inference results using a LoRaWAN device and add information about local luminosity. Often, with offline operations, a local low-power display such as an OLED is advised. This setup can be seen here:\n\nThe Grove Light Sensor would be connected to one of the single Analog pins (A0/PC4), the LoRaWAN device to the UART, and the OLED to the I2C connector.\nThe Nicla Pins 3 (Tx) and 4 (Rx) are connected with the Serial Shield connector. The UART communication is used with the LoRaWan device. Here is a simple code to use the UART:\n# UART Test - By: marcelo_rovai - Sat Sep 23 2023\n\nimport time\nfrom pyb import UART\nfrom pyb import LED\n\nredLED = LED(1) # built-in red LED\n\n# Init UART object.\n# Nicla Vision's UART (TX/RX pins) is on \"LP1\"\nuart = UART(\"LP1\", 9600)\n\nwhile(True):\n    uart.write(\"Hello World!\\r\\n\")\n    redLED.toggle()\n    time.sleep_ms(1000)\nTo verify that the UART is working, you should, for example, connect another device as the Arduino UNO, displaying “Hello Word” on the Serial Monitor. Here is the code.\n\nBelow is the Hello World code to be used with the I2C OLED. The MicroPython SSD1306 OLED driver (ssd1306.py), created by Adafruit, should also be uploaded to the Nicla (the ssd1306.py script can be found in GitHub).\n# Nicla_OLED_Hello_World - By: marcelo_rovai - Sat Sep 30 2023\n\n#Save on device: MicroPython SSD1306 OLED driver, I2C and SPI interfaces created by Adafruit\nimport ssd1306\n\nfrom machine import I2C\ni2c = I2C(1)\n\noled_width = 128\noled_height = 64\noled = ssd1306.SSD1306_I2C(oled_width, oled_height, i2c)\n\noled.text('Hello, World', 10, 10)\noled.show()\nFinally, here is a simple script to read the ADC value on pin “PC4” (Nicla pin A0):\n\n# Light Sensor (A0) - By: marcelo_rovai - Wed Oct 4 2023\n\nimport pyb\nfrom time import sleep\n\nadc = pyb.ADC(pyb.Pin(\"PC4\"))     # create an analog object from a pin\nval = adc.read()                  # read an analog value\n\nwhile (True):\n\n    val = adc.read()  \n    print (\"Light={}\".format (val))\n    sleep (1)\nThe ADC can be used for other sensor variables, such as Temperature.\n\nNote that the above scripts (downloaded from Github) introduce only how to connect external devices with the Nicla Vision board using MicroPython.",
    "crumbs": [
      "Nicla Vision",
      "Setup"
    ]
  },
  {
    "objectID": "contents/labs/arduino/nicla_vision/setup/setup.html#conclusion",
    "href": "contents/labs/arduino/nicla_vision/setup/setup.html#conclusion",
    "title": "Setup",
    "section": "Conclusion",
    "text": "Conclusion\nThe Arduino Nicla Vision is an excellent tiny device for industrial and professional uses! However, it is powerful, trustworthy, low power, and has suitable sensors for the most common embedded machine learning applications such as vision, movement, sensor fusion, and sound.\n\nOn the GitHub repository, you will find the last version of all the codeused or commented on in this hands-on exercise.",
    "crumbs": [
      "Nicla Vision",
      "Setup"
    ]
  },
  {
    "objectID": "contents/labs/arduino/nicla_vision/setup/setup.html#resources",
    "href": "contents/labs/arduino/nicla_vision/setup/setup.html#resources",
    "title": "Setup",
    "section": "Resources",
    "text": "Resources\n\nMicropython codes\nArduino Codes",
    "crumbs": [
      "Nicla Vision",
      "Setup"
    ]
  },
  {
    "objectID": "contents/labs/arduino/nicla_vision/image_classification/image_classification.html",
    "href": "contents/labs/arduino/nicla_vision/image_classification/image_classification.html",
    "title": "Image Classification",
    "section": "",
    "text": "Overview\nAs we initiate our studies into embedded machine learning or TinyML, it’s impossible to overlook the transformative impact of Computer Vision (CV) and Artificial Intelligence (AI) in our lives. These two intertwined disciplines redefine what machines can perceive and accomplish, from autonomous vehicles and robotics to healthcare and surveillance.\nMore and more, we are facing an artificial intelligence (AI) revolution where, as stated by Gartner, Edge AI has a very high impact potential, and it is for now!\nIn the “bullseye” of the Radar is the Edge Computer Vision, and when we talk about Machine Learning (ML) applied to vision, the first thing that comes to mind is Image Classification, a kind of ML “Hello World”!\nThis exercise will explore a computer vision project utilizing Convolutional Neural Networks (CNNs) for real-time image classification. Leveraging TensorFlow’s robust ecosystem, we’ll implement a pre-trained MobileNet model and adapt it for edge deployment. The focus will be on optimizing the model to run efficiently on resource-constrained hardware without sacrificing accuracy.\nWe’ll employ techniques like quantization and pruning to reduce the computational load. By the end of this tutorial, you’ll have a working prototype capable of classifying images in real-time, all running on a low-power embedded system based on the Arduino Nicla Vision board.",
    "crumbs": [
      "Nicla Vision",
      "Image Classification"
    ]
  },
  {
    "objectID": "contents/labs/arduino/nicla_vision/image_classification/image_classification.html#computer-vision",
    "href": "contents/labs/arduino/nicla_vision/image_classification/image_classification.html#computer-vision",
    "title": "Image Classification",
    "section": "Computer Vision",
    "text": "Computer Vision\nAt its core, computer vision enables machines to interpret and make decisions based on visual data from the world, essentially mimicking the capability of the human optical system. Conversely, AI is a broader field encompassing machine learning, natural language processing, and robotics, among other technologies. When you bring AI algorithms into computer vision projects, you supercharge the system’s ability to understand, interpret, and react to visual stimuli.\nWhen discussing Computer Vision projects applied to embedded devices, the most common applications that come to mind are Image Classification and Object Detection.\n\nBoth models can be implemented on tiny devices like the Arduino Nicla Vision and used on real projects. In this chapter, we will cover Image Classification.",
    "crumbs": [
      "Nicla Vision",
      "Image Classification"
    ]
  },
  {
    "objectID": "contents/labs/arduino/nicla_vision/image_classification/image_classification.html#image-classification-project-goal",
    "href": "contents/labs/arduino/nicla_vision/image_classification/image_classification.html#image-classification-project-goal",
    "title": "Image Classification",
    "section": "Image Classification Project Goal",
    "text": "Image Classification Project Goal\nThe first step in any ML project is to define the goal. In this case, it is to detect and classify two specific objects present in one image. For this project, we will use two small toys: a robot and a small Brazilian parrot (named Periquito). Also, we will collect images of a background where those two objects are absent.",
    "crumbs": [
      "Nicla Vision",
      "Image Classification"
    ]
  },
  {
    "objectID": "contents/labs/arduino/nicla_vision/image_classification/image_classification.html#data-collection",
    "href": "contents/labs/arduino/nicla_vision/image_classification/image_classification.html#data-collection",
    "title": "Image Classification",
    "section": "Data Collection",
    "text": "Data Collection\nOnce you have defined your Machine Learning project goal, the next and most crucial step is the dataset collection. You can use the Edge Impulse Studio, the OpenMV IDE we installed, or even your phone for the image capture. Here, we will use the OpenMV IDE for that.\n\nCollecting Dataset with OpenMV IDE\nFirst, create in your computer a folder where your data will be saved, for example, “data.” Next, on the OpenMV IDE, go to Tools &gt; Dataset Editor and select New Dataset to start the dataset collection:\n\nThe IDE will ask you to open the file where your data will be saved and choose the “data” folder that was created. Note that new icons will appear on the Left panel.\n\nUsing the upper icon (1), enter with the first class name, for example, “periquito”:\n\nRunning the dataset_capture_script.py and clicking on the camera icon (2), will start capturing images:\n\nRepeat the same procedure with the other classes\n\n\nWe suggest around 60 images from each category. Try to capture different angles, backgrounds, and light conditions.\n\nThe stored images use a QVGA frame size of 320x240 and the RGB565 (color pixel format).\nAfter capturing your dataset, close the Dataset Editor Tool on the Tools &gt; Dataset Editor.\nOn your computer, you will end with a dataset that contains three classes: periquito, robot, and background.\n\nYou should return to Edge Impulse Studio and upload the dataset to your project.",
    "crumbs": [
      "Nicla Vision",
      "Image Classification"
    ]
  },
  {
    "objectID": "contents/labs/arduino/nicla_vision/image_classification/image_classification.html#training-the-model-with-edge-impulse-studio",
    "href": "contents/labs/arduino/nicla_vision/image_classification/image_classification.html#training-the-model-with-edge-impulse-studio",
    "title": "Image Classification",
    "section": "Training the model with Edge Impulse Studio",
    "text": "Training the model with Edge Impulse Studio\nWe will use the Edge Impulse Studio for training our model. Enter your account credentials and create a new project:\n\n\nHere, you can clone a similar project: NICLA-Vision_Image_Classification.",
    "crumbs": [
      "Nicla Vision",
      "Image Classification"
    ]
  },
  {
    "objectID": "contents/labs/arduino/nicla_vision/image_classification/image_classification.html#dataset",
    "href": "contents/labs/arduino/nicla_vision/image_classification/image_classification.html#dataset",
    "title": "Image Classification",
    "section": "Dataset",
    "text": "Dataset\nUsing the EI Studio (or Studio), we will go over four main steps to have our model ready for use on the Nicla Vision board: Dataset, Impulse, Tests, and Deploy (on the Edge Device, in this case, the NiclaV).\n\nRegarding the Dataset, it is essential to point out that our Original Dataset, captured with the OpenMV IDE, will be split into Training, Validation, and Test. The Test Set will be divided from the beginning, and a part will reserved to be used only in the Test phase after training. The Validation Set will be used during training.\n\nOn Studio, go to the Data acquisition tab, and on the UPLOAD DATA section, upload the chosen categories files from your computer:\n\nLeave to the Studio the splitting of the original dataset into train and test and choose the label about that specific data:\n\nRepeat the procedure for all three classes. At the end, you should see your “raw data” in the Studio:\n\nThe Studio allows you to explore your data, showing a complete view of all the data in your project. You can clear, inspect, or change labels by clicking on individual data items. In our case, a very simple project, the data seems OK.",
    "crumbs": [
      "Nicla Vision",
      "Image Classification"
    ]
  },
  {
    "objectID": "contents/labs/arduino/nicla_vision/image_classification/image_classification.html#the-impulse-design",
    "href": "contents/labs/arduino/nicla_vision/image_classification/image_classification.html#the-impulse-design",
    "title": "Image Classification",
    "section": "The Impulse Design",
    "text": "The Impulse Design\nIn this phase, we should define how to:\n\nPre-process our data, which consists of resizing the individual images and determining the color depth to use (be it RGB or Grayscale) and\nSpecify a Model, in this case, it will be the Transfer Learning (Images) to fine-tune a pre-trained MobileNet V2 image classification model on our data. This method performs well even with relatively small image datasets (around 150 images in our case).\n\n\nTransfer Learning with MobileNet offers a streamlined approach to model training, which is especially beneficial for resource-constrained environments and projects with limited labeled data. MobileNet, known for its lightweight architecture, is a pre-trained model that has already learned valuable features from a large dataset (ImageNet).\n\nBy leveraging these learned features, you can train a new model for your specific task with fewer data and computational resources and yet achieve competitive accuracy.\n\nThis approach significantly reduces training time and computational cost, making it ideal for quick prototyping and deployment on embedded devices where efficiency is paramount.\nGo to the Impulse Design Tab and create the impulse, defining an image size of 96x96 and squashing them (squared form, without cropping). Select Image and Transfer Learning blocks. Save the Impulse.\n\n\nImage Pre-Processing\nAll the input QVGA/RGB565 images will be converted to 27,640 features (96x96x3).\n\nPress [Save parameters] and Generate all features:\n\n\n\nModel Design\nIn 2007, Google introduced MobileNetV1, a family of general-purpose computer vision neural networks designed with mobile devices in mind to support classification, detection, and more. MobileNets are small, low-latency, low-power models parameterized to meet the resource constraints of various use cases. in 2018, Google launched MobileNetV2: Inverted Residuals and Linear Bottlenecks.\nMobileNet V1 and MobileNet V2 aim at mobile efficiency and embedded vision applications but differ in architectural complexity and performance. While both use depthwise separable convolutions to reduce the computational cost, MobileNet V2 introduces Inverted Residual Blocks and Linear Bottlenecks to improve performance. These new features allow V2 to capture more complex features using fewer parameters, making it computationally more efficient and generally more accurate than its predecessor. Additionally, V2 employs a non-linear activation in the intermediate expansion layer. It still uses a linear activation for the bottleneck layer, a design choice found to preserve important information through the network. MobileNet V2 offers an optimized architecture for higher accuracy and efficiency and will be used in this project.\nAlthough the base MobileNet architecture is already tiny and has low latency, many times, a specific use case or application may require the model to be even smaller and faster. MobileNets introduces a straightforward parameter α (alpha) called width multiplier to construct these smaller, less computationally expensive models. The role of the width multiplier α is that of thinning a network uniformly at each layer.\nEdge Impulse Studio can use both MobileNetV1 (96x96 images) and V2 (96x96 or 160x160 images), with several different α values (from 0.05 to 1.0). For example, you will get the highest accuracy with V2, 160x160 images, and α=1.0. Of course, there is a trade-off. The higher the accuracy, the more memory (around 1.3MB RAM and 2.6MB ROM) will be needed to run the model, implying more latency. The smaller footprint will be obtained at the other extreme with MobileNetV1 and α=0.10 (around 53.2K RAM and 101K ROM).\n\nWe will use MobileNetV2 96x96 0.1 for this project, with an estimated memory cost of 265.3 KB in RAM. This model should be OK for the Nicla Vision with 1MB of SRAM. On the Transfer Learning Tab, select this model:",
    "crumbs": [
      "Nicla Vision",
      "Image Classification"
    ]
  },
  {
    "objectID": "contents/labs/arduino/nicla_vision/image_classification/image_classification.html#model-training",
    "href": "contents/labs/arduino/nicla_vision/image_classification/image_classification.html#model-training",
    "title": "Image Classification",
    "section": "Model Training",
    "text": "Model Training\nAnother valuable technique to be used with Deep Learning is Data Augmentation. Data augmentation is a method to improve the accuracy of machine learning models by creating additional artificial data. A data augmentation system makes small, random changes to your training data during the training process (such as flipping, cropping, or rotating the images).\nLooking under the hood, here you can see how Edge Impulse implements a data Augmentation policy on your data:\n# Implements the data augmentation policy\ndef augment_image(image, label):\n    # Flips the image randomly\n    image = tf.image.random_flip_left_right(image)\n\n    # Increase the image size, then randomly crop it down to\n    # the original dimensions\n    resize_factor = random.uniform(1, 1.2)\n    new_height = math.floor(resize_factor * INPUT_SHAPE[0])\n    new_width = math.floor(resize_factor * INPUT_SHAPE[1])\n    image = tf.image.resize_with_crop_or_pad(image, new_height, new_width)\n    image = tf.image.random_crop(image, size=INPUT_SHAPE)\n\n    # Vary the brightness of the image\n    image = tf.image.random_brightness(image, max_delta=0.2)\n\n    return image, label\nExposure to these variations during training can help prevent your model from taking shortcuts by “memorizing” superficial clues in your training data, meaning it may better reflect the deep underlying patterns in your dataset.\nThe final layer of our model will have 12 neurons with a 15% dropout for overfitting prevention. Here is the Training result:\n\nThe result is excellent, with 77ms of latency, which should result in 13fps (frames per second) during inference.",
    "crumbs": [
      "Nicla Vision",
      "Image Classification"
    ]
  },
  {
    "objectID": "contents/labs/arduino/nicla_vision/image_classification/image_classification.html#model-testing",
    "href": "contents/labs/arduino/nicla_vision/image_classification/image_classification.html#model-testing",
    "title": "Image Classification",
    "section": "Model Testing",
    "text": "Model Testing\n\nNow, you should take the data set aside at the start of the project and run the trained model using it as input:\n\nThe result is, again, excellent.",
    "crumbs": [
      "Nicla Vision",
      "Image Classification"
    ]
  },
  {
    "objectID": "contents/labs/arduino/nicla_vision/image_classification/image_classification.html#deploying-the-model",
    "href": "contents/labs/arduino/nicla_vision/image_classification/image_classification.html#deploying-the-model",
    "title": "Image Classification",
    "section": "Deploying the model",
    "text": "Deploying the model\nAt this point, we can deploy the trained model as.tflite and use the OpenMV IDE to run it using MicroPython, or we can deploy it as a C/C++ or an Arduino library.\n\n\nArduino Library\nFirst, Let’s deploy it as an Arduino Library:\n\nYou should install the library as.zip on the Arduino IDE and run the sketch nicla_vision_camera.ino available in Examples under your library name.\n\nNote that Arduino Nicla Vision has, by default, 512KB of RAM allocated for the M7 core and an additional 244KB on the M4 address space. In the code, this allocation was changed to 288 kB to guarantee that the model will run on the device (malloc_addblock((void*)0x30000000, 288 * 1024);).\n\nThe result is good, with 86ms of measured latency.\n\nHere is a short video showing the inference results: \n\n\nOpenMV\nIt is possible to deploy the trained model to be used with OpenMV in two ways: as a library and as a firmware.\nThree files are generated as a library: the trained.tflite model, a list with labels, and a simple MicroPython script that can make inferences using the model.\n\nRunning this model as a .tflite directly in the Nicla was impossible. So, we can sacrifice the accuracy using a smaller model or deploy the model as an OpenMV Firmware (FW). Choosing FW, the Edge Impulse Studio generates optimized models, libraries, and frameworks needed to make the inference. Let’s explore this option.\nSelect OpenMV Firmware on the Deploy Tab and press [Build].\n\nOn your computer, you will find a ZIP file. Open it:\n\nUse the Bootloader tool on the OpenMV IDE to load the FW on your board:\n\nSelect the appropriate file (.bin for Nicla-Vision):\n\nAfter the download is finished, press OK:\n\nIf a message says that the FW is outdated, DO NOT UPGRADE. Select [NO].\n\nNow, open the script ei_image_classification.py that was downloaded from the Studio and the.bin file for the Nicla.\n\nRun it. Pointing the camera to the objects we want to classify, the inference result will be displayed on the Serial Terminal.\n\n\nChanging the Code to add labels\nThe code provided by Edge Impulse can be modified so that we can see, for test reasons, the inference result directly on the image displayed on the OpenMV IDE.\nUpload the code from GitHub, or modify it as below:\n# Marcelo Rovai - NICLA Vision - Image Classification\n# Adapted from Edge Impulse - OpenMV Image Classification Example\n# @24Aug23\n\nimport sensor, image, time, os, tf, uos, gc\n\nsensor.reset()                         # Reset and initialize the sensor.\nsensor.set_pixformat(sensor.RGB565)    # Set pxl fmt to RGB565 (or GRAYSCALE)\nsensor.set_framesize(sensor.QVGA)      # Set frame size to QVGA (320x240)\nsensor.set_windowing((240, 240))       # Set 240x240 window.\nsensor.skip_frames(time=2000)          # Let the camera adjust.\n\nnet = None\nlabels = None\n\ntry:\n    # Load built in model\n    labels, net = tf.load_builtin_model('trained')\nexcept Exception as e:\n    raise Exception(e)\n\nclock = time.clock()\nwhile(True):\n    clock.tick()  # Starts tracking elapsed time.\n\n    img = sensor.snapshot()\n\n    # default settings just do one detection\n    for obj in net.classify(img, \n                            min_scale=1.0, \n                            scale_mul=0.8, \n                            x_overlap=0.5, \n                            y_overlap=0.5):\n        fps = clock.fps()\n        lat = clock.avg()\n\n        print(\"**********\\nPrediction:\")\n        img.draw_rectangle(obj.rect())\n        # This combines the labels and confidence values into a list of tuples\n        predictions_list = list(zip(labels, obj.output()))\n\n        max_val = predictions_list[0][1]\n        max_lbl = 'background'\n        for i in range(len(predictions_list)):\n            val = predictions_list[i][1]\n            lbl = predictions_list[i][0]\n\n            if val &gt; max_val:\n                max_val = val\n                max_lbl = lbl\n\n    # Print label with the highest probability\n    if max_val &lt; 0.5:\n        max_lbl = 'uncertain'\n    print(\"{} with a prob of {:.2f}\".format(max_lbl, max_val))\n    print(\"FPS: {:.2f} fps ==&gt; latency: {:.0f} ms\".format(fps, lat))\n\n    # Draw label with highest probability to image viewer\n    img.draw_string(\n        10, 10,\n        max_lbl + \"\\n{:.2f}\".format(max_val),\n        mono_space = False,\n        scale=2\n        )\nHere you can see the result:\n\nNote that the latency (136 ms) is almost double of what we got directly with the Arduino IDE. This is because we are using the IDE as an interface and also the time to wait for the camera to be ready. If we start the clock just before the inference:\n\nThe latency will drop to only 71 ms.\n\n\nThe NiclaV runs about half as fast when connected to the IDE. The FPS should increase once disconnected.\n\n\n\nPost-Processing with LEDs\nWhen working with embedded machine learning, we are looking for devices that can continually proceed with the inference and result, taking some action directly on the physical world and not displaying the result on a connected computer. To simulate this, we will light up a different LED for each possible inference result.\n\nTo accomplish that, we should upload the code from GitHub or change the last code to include the LEDs:\n# Marcelo Rovai - NICLA Vision - Image Classification with LEDs\n# Adapted from Edge Impulse - OpenMV Image Classification Example\n# @24Aug23\n\nimport sensor, image, time, os, tf, uos, gc, pyb\n\nledRed = pyb.LED(1)\nledGre = pyb.LED(2)\nledBlu = pyb.LED(3)\n\nsensor.reset()                         # Reset and initialize the sensor.\nsensor.set_pixformat(sensor.RGB565)    # Set pixl fmt to RGB565 (or GRAYSCALE)\nsensor.set_framesize(sensor.QVGA)      # Set frame size to QVGA (320x240)\nsensor.set_windowing((240, 240))       # Set 240x240 window.\nsensor.skip_frames(time=2000)          # Let the camera adjust.\n\nnet = None\nlabels = None\n\nledRed.off()\nledGre.off()\nledBlu.off()\n\ntry:\n    # Load built in model\n    labels, net = tf.load_builtin_model('trained')\nexcept Exception as e:\n    raise Exception(e)\n\nclock = time.clock()\n\n\ndef setLEDs(max_lbl):\n\n    if max_lbl == 'uncertain':\n        ledRed.on()\n        ledGre.off()\n        ledBlu.off()\n\n    if max_lbl == 'periquito':\n        ledRed.off()\n        ledGre.on()\n        ledBlu.off()\n\n    if max_lbl == 'robot':\n        ledRed.off()\n        ledGre.off()\n        ledBlu.on()\n\n    if max_lbl == 'background':\n        ledRed.off()\n        ledGre.off()\n        ledBlu.off()\n\n\nwhile(True):\n    img = sensor.snapshot()\n    clock.tick()  # Starts tracking elapsed time.\n\n    # default settings just do one detection.\n    for obj in net.classify(img, \n                            min_scale=1.0, \n                            scale_mul=0.8, \n                            x_overlap=0.5, \n                            y_overlap=0.5):\n        fps = clock.fps()\n        lat = clock.avg()\n\n        print(\"**********\\nPrediction:\")\n        img.draw_rectangle(obj.rect())\n        # This combines the labels and confidence values into a list of tuples\n        predictions_list = list(zip(labels, obj.output()))\n\n        max_val = predictions_list[0][1]\n        max_lbl = 'background'\n        for i in range(len(predictions_list)):\n            val = predictions_list[i][1]\n            lbl = predictions_list[i][0]\n\n            if val &gt; max_val:\n                max_val = val\n                max_lbl = lbl\n\n    # Print label and turn on LED with the highest probability\n    if max_val &lt; 0.8:\n        max_lbl = 'uncertain'\n\n    setLEDs(max_lbl)\n\n    print(\"{} with a prob of {:.2f}\".format(max_lbl, max_val))\n    print(\"FPS: {:.2f} fps ==&gt; latency: {:.0f} ms\".format(fps, lat))\n\n    # Draw label with highest probability to image viewer\n    img.draw_string(\n        10, 10,\n        max_lbl + \"\\n{:.2f}\".format(max_val),\n        mono_space = False,\n        scale=2\n        )\nNow, each time that a class scores a result greater than 0.8, the correspondent LED will be lit:\n\nLed Red 0n: Uncertain (no class is over 0.8)\nLed Green 0n: Periquito &gt; 0.8\nLed Blue 0n: Robot &gt; 0.8\nAll LEDs Off: Background &gt; 0.8\n\nHere is the result:\n\nIn more detail",
    "crumbs": [
      "Nicla Vision",
      "Image Classification"
    ]
  },
  {
    "objectID": "contents/labs/arduino/nicla_vision/image_classification/image_classification.html#image-classification-non-official-benchmark",
    "href": "contents/labs/arduino/nicla_vision/image_classification/image_classification.html#image-classification-non-official-benchmark",
    "title": "Image Classification",
    "section": "Image Classification (non-official) Benchmark",
    "text": "Image Classification (non-official) Benchmark\nSeveral development boards can be used for embedded machine learning (TinyML), and the most common ones for Computer Vision applications (consuming low energy), are the ESP32 CAM, the Seeed XIAO ESP32S3 Sense, the Arduino Nicla Vison, and the Arduino Portenta.\n\nCatching the opportunity, the same trained model was deployed on the ESP-CAM, the XIAO, and the Portenta (in this one, the model was trained again, using grayscaled images to be compatible with its camera). Here is the result, deploying the models as Arduino’s Library:",
    "crumbs": [
      "Nicla Vision",
      "Image Classification"
    ]
  },
  {
    "objectID": "contents/labs/arduino/nicla_vision/image_classification/image_classification.html#conclusion",
    "href": "contents/labs/arduino/nicla_vision/image_classification/image_classification.html#conclusion",
    "title": "Image Classification",
    "section": "Conclusion",
    "text": "Conclusion\nBefore we finish, consider that Computer Vision is more than just image classification. For example, you can develop Edge Machine Learning projects around vision in several areas, such as:\n\nAutonomous Vehicles: Use sensor fusion, lidar data, and computer vision algorithms to navigate and make decisions.\nHealthcare: Automated diagnosis of diseases through MRI, X-ray, and CT scan image analysis\nRetail: Automated checkout systems that identify products as they pass through a scanner.\nSecurity and Surveillance: Facial recognition, anomaly detection, and object tracking in real-time video feeds.\nAugmented Reality: Object detection and classification to overlay digital information in the real world.\nIndustrial Automation: Visual inspection of products, predictive maintenance, and robot and drone guidance.\nAgriculture: Drone-based crop monitoring and automated harvesting.\nNatural Language Processing: Image captioning and visual question answering.\nGesture Recognition: For gaming, sign language translation, and human-machine interaction.\nContent Recommendation: Image-based recommendation systems in e-commerce.",
    "crumbs": [
      "Nicla Vision",
      "Image Classification"
    ]
  },
  {
    "objectID": "contents/labs/arduino/nicla_vision/image_classification/image_classification.html#resources",
    "href": "contents/labs/arduino/nicla_vision/image_classification/image_classification.html#resources",
    "title": "Image Classification",
    "section": "Resources",
    "text": "Resources\n\nMicropython codes\nDataset\nEdge Impulse Project",
    "crumbs": [
      "Nicla Vision",
      "Image Classification"
    ]
  },
  {
    "objectID": "contents/labs/arduino/nicla_vision/object_detection/object_detection.html",
    "href": "contents/labs/arduino/nicla_vision/object_detection/object_detection.html",
    "title": "Object Detection",
    "section": "",
    "text": "Overview\nThis is a continuation of CV on Nicla Vision, now exploring Object Detection on microcontrollers.",
    "crumbs": [
      "Nicla Vision",
      "Object Detection"
    ]
  },
  {
    "objectID": "contents/labs/arduino/nicla_vision/object_detection/object_detection.html#overview",
    "href": "contents/labs/arduino/nicla_vision/object_detection/object_detection.html#overview",
    "title": "Object Detection",
    "section": "",
    "text": "Object Detection versus Image Classification\nThe main task with Image Classification models is to produce a list of the most probable object categories present on an image, for example, to identify a tabby cat just after his dinner:\n\nBut what happens when the cat jumps near the wine glass? The model still only recognizes the predominant category on the image, the tabby cat:\n\nAnd what happens if there is not a dominant category on the image?\n\nThe model identifies the above image completely wrong as an “ashcan,” possibly due to the color tonalities.\n\nThe model used in all previous examples is the MobileNet, trained with a large dataset, the ImageNet.\n\nTo solve this issue, we need another type of model, where not only multiple categories (or labels) can be found but also where the objects are located on a given image.\nAs we can imagine, such models are much more complicated and bigger, for example, the MobileNetV2 SSD FPN-Lite 320x320, trained with the COCO dataset. This pre-trained object detection model is designed to locate up to 10 objects within an image, outputting a bounding box for each object detected. The below image is the result of such a model running on a Raspberry Pi:\n\nThose models used for Object detection (such as the MobileNet SSD or YOLO) usually have several MB in size, which is OK for use with Raspberry Pi but unsuitable for use with embedded devices, where the RAM usually is lower than 1M Bytes.\n\n\nAn innovative solution for Object Detection: FOMO\nEdge Impulse launched in 2022, FOMO (Faster Objects, More Objects), a novel solution to perform object detection on embedded devices, not only on the Nicla Vision (Cortex M7) but also on Cortex M4F CPUs (Arduino Nano33 and OpenMV M4 series) as well the Espressif ESP32 devices (ESP-CAM and XIAO ESP32S3 Sense).\nIn this Hands-On exercise, we will explore using FOMO with Object Detection, not entering many details about the model itself. To understand more about how the model works, you can go into the official FOMO announcement by Edge Impulse, where Louis Moreau and Mat Kelcey explain in detail how it works.",
    "crumbs": [
      "Nicla Vision",
      "Object Detection"
    ]
  },
  {
    "objectID": "contents/labs/arduino/nicla_vision/object_detection/object_detection.html#the-object-detection-project-goal",
    "href": "contents/labs/arduino/nicla_vision/object_detection/object_detection.html#the-object-detection-project-goal",
    "title": "Object Detection",
    "section": "The Object Detection Project Goal",
    "text": "The Object Detection Project Goal\nAll Machine Learning projects need to start with a detailed goal. Let’s assume we are in an industrial facility and must sort and count wheels and special boxes.\n\nIn other words, we should perform a multi-label classification, where each image can have three classes:\n\nBackground (No objects)\nBox\nWheel\n\nHere are some not labeled image samples that we should use to detect the objects (wheels and boxes):\n\nWe are interested in which object is in the image, its location (centroid), and how many we can find on it. The object’s size is not detected with FOMO, as with MobileNet SSD or YOLO, where the Bounding Box is one of the model outputs.\nWe will develop the project using the Nicla Vision for image capture and model inference. The ML project will be developed using the Edge Impulse Studio. But before starting the object detection project in the Studio, let’s create a raw dataset (not labeled) with images that contain the objects to be detected.",
    "crumbs": [
      "Nicla Vision",
      "Object Detection"
    ]
  },
  {
    "objectID": "contents/labs/arduino/nicla_vision/object_detection/object_detection.html#data-collection",
    "href": "contents/labs/arduino/nicla_vision/object_detection/object_detection.html#data-collection",
    "title": "Object Detection",
    "section": "Data Collection",
    "text": "Data Collection\nWe can use the Edge Impulse Studio, the OpenMV IDE, your phone, or other devices for the image capture. Here, we will use again the OpenMV IDE for our purpose.\n\nCollecting Dataset with OpenMV IDE\nFirst, create in your computer a folder where your data will be saved, for example, “data.” Next, on the OpenMV IDE, go to Tools &gt; Dataset Editor and select New Dataset to start the dataset collection:\n\nEdge impulse suggests that the objects should be of similar size and not overlapping for better performance. This is OK in an industrial facility, where the camera should be fixed, keeping the same distance from the objects to be detected. Despite that, we will also try with mixed sizes and positions to see the result.\n\nWe will not create separate folders for our images because each contains multiple labels.\n\nConnect the Nicla Vision to the OpenMV IDE and run the dataset_capture_script.py. Clicking on the Capture Image button will start capturing images:\n\nWe suggest around 50 images mixing the objects and varying the number of each appearing on the scene. Try to capture different angles, backgrounds, and light conditions.\n\nThe stored images use a QVGA frame size 320x240 and RGB565 (color pixel format).\n\nAfter capturing your dataset, close the Dataset Editor Tool on the Tools &gt; Dataset Editor.",
    "crumbs": [
      "Nicla Vision",
      "Object Detection"
    ]
  },
  {
    "objectID": "contents/labs/arduino/nicla_vision/object_detection/object_detection.html#edge-impulse-studio",
    "href": "contents/labs/arduino/nicla_vision/object_detection/object_detection.html#edge-impulse-studio",
    "title": "Object Detection",
    "section": "Edge Impulse Studio",
    "text": "Edge Impulse Studio\n\nSetup the project\nGo to Edge Impulse Studio, enter your credentials at Login (or create an account), and start a new project.\n\n\nHere, you can clone the project developed for this hands-on: NICLA_Vision_Object_Detection.\n\nOn your Project Dashboard, go down and on Project info and select Bounding boxes (object detection) and Nicla Vision as your Target Device:\n\n\n\nUploading the unlabeled data\nOn Studio, go to the Data acquisition tab, and on the UPLOAD DATA section, upload from your computer files captured.\n\n\nYou can leave for the Studio to split your data automatically between Train and Test or do it manually.\n\n\nAll the not labeled images (51) were uploaded but they still need to be labeled appropriately before using them as a dataset in the project. The Studio has a tool for that purpose, which you can find in the link Labeling queue (51).\nThere are two ways you can use to perform AI-assisted labeling on the Edge Impulse Studio (free version):\n\nUsing yolov5\nTracking objects between frames\n\n\nEdge Impulse launched an auto-labeling feature for Enterprise customers, easing labeling tasks in object detection projects.\n\nOrdinary objects can quickly be identified and labeled using an existing library of pre-trained object detection models from YOLOv5 (trained with the COCO dataset). But since, in our case, the objects are not part of COCO datasets, we should select the option of tracking objects. With this option, once you draw bounding boxes and label the images in one frame, the objects will be tracked automatically from frame to frame, partially labeling the new ones (not all are correctly labeled).\n\nYou can use the EI uploader to import your data if you already have a labeled dataset containing bounding boxes.\n\n\n\nLabeling the Dataset\nStarting with the first image of your unlabeled data, use your mouse to drag a box around an object to add a label. Then click Save labels to advance to the next item.\n\nContinue with this process until the queue is empty. At the end, all images should have the objects labeled as those samples below:\n\nNext, review the labeled samples on the Data acquisition tab. If one of the labels was wrong, you can edit it using the three dots menu after the sample name:\n\nYou will be guided to replace the wrong label, correcting the dataset.",
    "crumbs": [
      "Nicla Vision",
      "Object Detection"
    ]
  },
  {
    "objectID": "contents/labs/arduino/nicla_vision/object_detection/object_detection.html#the-impulse-design",
    "href": "contents/labs/arduino/nicla_vision/object_detection/object_detection.html#the-impulse-design",
    "title": "Object Detection",
    "section": "The Impulse Design",
    "text": "The Impulse Design\nIn this phase, you should define how to:\n\nPre-processing consists of resizing the individual images from 320 x 240 to 96 x 96 and squashing them (squared form, without cropping). Afterwards, the images are converted from RGB to Grayscale.\nDesign a Model, in this case, “Object Detection.”\n\n\n\nPreprocessing all dataset\nIn this section, select Color depth as Grayscale, which is suitable for use with FOMO models and Save parameters.\n\nThe Studio moves automatically to the next section, Generate features, where all samples will be pre-processed, resulting in a dataset with individual 96x96x1 images or 9,216 features.\n\nThe feature explorer shows that all samples evidence a good separation after the feature generation.\n\nOne of the samples (46) apparently is in the wrong space, but clicking on it can confirm that the labeling is correct.",
    "crumbs": [
      "Nicla Vision",
      "Object Detection"
    ]
  },
  {
    "objectID": "contents/labs/arduino/nicla_vision/object_detection/object_detection.html#model-design-training-and-test",
    "href": "contents/labs/arduino/nicla_vision/object_detection/object_detection.html#model-design-training-and-test",
    "title": "Object Detection",
    "section": "Model Design, Training, and Test",
    "text": "Model Design, Training, and Test\nWe will use FOMO, an object detection model based on MobileNetV2 (alpha 0.35) designed to coarsely segment an image into a grid of background vs objects of interest (here, boxes and wheels).\nFOMO is an innovative machine learning model for object detection, which can use up to 30 times less energy and memory than traditional models like Mobilenet SSD and YOLOv5. FOMO can operate on microcontrollers with less than 200 KB of RAM. The main reason this is possible is that while other models calculate the object’s size by drawing a square around it (bounding box), FOMO ignores the size of the image, providing only the information about where the object is located in the image, by means of its centroid coordinates.\nHow FOMO works?\nFOMO takes the image in grayscale and divides it into blocks of pixels using a factor of 8. For the input of 96x96, the grid would be 12x12 (96/8=12). Next, FOMO will run a classifier through each pixel block to calculate the probability that there is a box or a wheel in each of them and, subsequently, determine the regions which have the highest probability of containing the object (If a pixel block has no objects, it will be classified as background). From the overlap of the final region, the FOMO provides the coordinates (related to the image dimensions) of the centroid of this region.\n\nFor training, we should select a pre-trained model. Let’s use the FOMO (Faster Objects, More Objects) MobileNetV2 0.35. This model uses around 250KB RAM and 80KB of ROM (Flash), which suits well with our board since it has 1MB of RAM and ROM.\n\nRegarding the training hyper-parameters, the model will be trained with:\n\nEpochs: 60,\nBatch size: 32\nLearning Rate: 0.001.\n\nFor validation during training, 20% of the dataset (validation_dataset) will be spared. For the remaining 80% (train_dataset), we will apply Data Augmentation, which will randomly flip, change the size and brightness of the image, and crop them, artificially increasing the number of samples on the dataset for training.\nAs a result, the model ends with practically 1.00 in the F1 score, with a similar result when using the Test data.\n\nNote that FOMO automatically added a 3rd label background to the two previously defined (box and wheel).\n\n\n\nIn object detection tasks, accuracy is generally not the primary evaluation metric. Object detection involves classifying objects and providing bounding boxes around them, making it a more complex problem than simple classification. The issue is that we do not have the bounding box, only the centroids. In short, using accuracy as a metric could be misleading and may not provide a complete understanding of how well the model is performing. Because of that, we will use the F1 score.\n\n\nTest model with “Live Classification”\nSince Edge Impulse officially supports the Nicla Vision, let’s connect it to the Studio. For that, follow the steps:\n\nDownload the last EI Firmware and unzip it.\nOpen the zip file on your computer and select the uploader related to your OS:\n\n\n\nPut the Nicla-Vision on Boot Mode, pressing the reset button twice.\nExecute the specific batch code for your OS for uploading the binary (arduino-nicla-vision.bin) to your board.\n\nGo to Live classification section at EI Studio, and using webUSB, connect your Nicla Vision:\n\nOnce connected, you can use the Nicla to capture actual images to be tested by the trained model on Edge Impulse Studio.\n\nOne thing to be noted is that the model can produce false positives and negatives. This can be minimized by defining a proper Confidence Threshold (use the Three dots menu for the set-up). Try with 0.8 or more.",
    "crumbs": [
      "Nicla Vision",
      "Object Detection"
    ]
  },
  {
    "objectID": "contents/labs/arduino/nicla_vision/object_detection/object_detection.html#deploying-the-model",
    "href": "contents/labs/arduino/nicla_vision/object_detection/object_detection.html#deploying-the-model",
    "title": "Object Detection",
    "section": "Deploying the Model",
    "text": "Deploying the Model\nSelect OpenMV Firmware on the Deploy Tab and press [Build].\n\nWhen you try to connect the Nicla with the OpenMV IDE again, it will try to update its FW. Choose the option Load a specific firmware instead.\n\nYou will find a ZIP file on your computer from the Studio. Open it:\n\nLoad the .bin file to your board:\n\nAfter the download is finished, a pop-up message will be displayed. Press OK, and open the script ei_object_detection.py downloaded from the Studio.\nBefore running the script, let’s change a few lines. Note that you can leave the window definition as 240 x 240 and the camera capturing images as QVGA/RGB. The captured image will be pre-processed by the FW deployed from Edge Impulse\n# Edge Impulse - OpenMV Object Detection Example\n\nimport sensor, image, time, os, tf, math, uos, gc\n\nsensor.reset()                         # Reset and initialize the sensor.\nsensor.set_pixformat(sensor.RGB565)    # Set pixel format to RGB565 (or GRAYSCALE)\nsensor.set_framesize(sensor.QVGA)      # Set frame size to QVGA (320x240)\nsensor.set_windowing((240, 240))       # Set 240x240 window.\nsensor.skip_frames(time=2000)          # Let the camera adjust.\n\nnet = None\nlabels = None\nRedefine the minimum confidence, for example, to 0.8 to minimize false positives and negatives.\nmin_confidence = 0.8\nChange if necessary, the color of the circles that will be used to display the detected object’s centroid for a better contrast.\ntry:\n    # Load built in model\n    labels, net = tf.load_builtin_model('trained')\nexcept Exception as e:\n    raise Exception(e)\n\ncolors = [ # Add more colors if you are detecting more than 7 types of classes at once.\n    (255, 255,   0), # background: yellow (not used)\n    (  0, 255,   0), # cube: green\n    (255,   0,   0), # wheel: red\n    (  0,   0, 255), # not used\n    (255,   0, 255), # not used\n    (  0, 255, 255), # not used\n    (255, 255, 255), # not used\n]\nKeep the remaining code as it is and press the green Play button to run the code:\n\nOn the camera view, we can see the objects with their centroids marked with 12 pixel-fixed circles (each circle has a distinct color, depending on its class). On the Serial Terminal, the model shows the labels detected and their position on the image window (240X240).\n\nBe ware that the coordinate origin is in the upper left corner.\n\n\nNote that the frames per second rate is around 8 fps (similar to what we got with the Image Classification project). This happens because FOMO is cleverly built over a CNN model, not with an object detection model like the SSD MobileNet. For example, when running a MobileNetV2 SSD FPN-Lite 320x320 model on a Raspberry Pi 4, the latency is around 5 times higher (around 1.5 fps)\nHere is a short video showing the inference results:",
    "crumbs": [
      "Nicla Vision",
      "Object Detection"
    ]
  },
  {
    "objectID": "contents/labs/arduino/nicla_vision/object_detection/object_detection.html#conclusion",
    "href": "contents/labs/arduino/nicla_vision/object_detection/object_detection.html#conclusion",
    "title": "Object Detection",
    "section": "Conclusion",
    "text": "Conclusion\nFOMO is a significant leap in the image processing space, as Louis Moreau and Mat Kelcey put it during its launch in 2022:\n\nFOMO is a ground-breaking algorithm that brings real-time object detection, tracking, and counting to microcontrollers for the first time.\n\nMultiple possibilities exist for exploring object detection (and, more precisely, counting them) on embedded devices, for example, to explore the Nicla doing sensor fusion (camera + microphone) and object detection. This can be very useful on projects involving bees, for example.",
    "crumbs": [
      "Nicla Vision",
      "Object Detection"
    ]
  },
  {
    "objectID": "contents/labs/arduino/nicla_vision/object_detection/object_detection.html#resources",
    "href": "contents/labs/arduino/nicla_vision/object_detection/object_detection.html#resources",
    "title": "Object Detection",
    "section": "Resources",
    "text": "Resources\n\nEdge Impulse Project",
    "crumbs": [
      "Nicla Vision",
      "Object Detection"
    ]
  },
  {
    "objectID": "contents/labs/arduino/nicla_vision/kws/kws.html",
    "href": "contents/labs/arduino/nicla_vision/kws/kws.html",
    "title": "Keyword Spotting (KWS)",
    "section": "",
    "text": "Overview\nHaving already explored the Nicla Vision board in the Image Classification and Object Detection applications, we are now shifting our focus to voice-activated applications with a project on Keyword Spotting (KWS).\nAs introduced in the Feature Engineering for Audio Classification Hands-On tutorial, Keyword Spotting (KWS) is integrated into many voice recognition systems, enabling devices to respond to specific words or phrases. While this technology underpins popular devices like Google Assistant or Amazon Alexa, it’s equally applicable and feasible on smaller, low-power devices. This tutorial will guide you through implementing a KWS system using TinyML on the Nicla Vision development board equipped with a digital microphone.\nOur model will be designed to recognize keywords that can trigger device wake-up or specific actions, bringing them to life with voice-activated commands.",
    "crumbs": [
      "Nicla Vision",
      "Keyword Spotting (KWS)"
    ]
  },
  {
    "objectID": "contents/labs/arduino/nicla_vision/kws/kws.html#how-does-a-voice-assistant-work",
    "href": "contents/labs/arduino/nicla_vision/kws/kws.html#how-does-a-voice-assistant-work",
    "title": "Keyword Spotting (KWS)",
    "section": "How does a voice assistant work?",
    "text": "How does a voice assistant work?\nAs said, voice assistants on the market, like Google Home or Amazon Echo-Dot, only react to humans when they are “waked up” by particular keywords such as ” Hey Google” on the first one and “Alexa” on the second.\n\nIn other words, recognizing voice commands is based on a multi-stage model or Cascade Detection.\n\nStage 1: A small microprocessor inside the Echo Dot or Google Home continuously listens, waiting for the keyword to be spotted, using a TinyML model at the edge (KWS application).\nStage 2: Only when triggered by the KWS application on Stage 1 is the data sent to the cloud and processed on a larger model.\nThe video below shows an example of a Google Assistant being programmed on a Raspberry Pi (Stage 2), with an Arduino Nano 33 BLE as the TinyML device (Stage 1).\n\n\nTo explore the above Google Assistant project, please see the tutorial: Building an Intelligent Voice Assistant From Scratch.\n\nIn this KWS project, we will focus on Stage 1 (KWS or Keyword Spotting), where we will use the Nicla Vision, which has a digital microphone that will be used to spot the keyword.",
    "crumbs": [
      "Nicla Vision",
      "Keyword Spotting (KWS)"
    ]
  },
  {
    "objectID": "contents/labs/arduino/nicla_vision/kws/kws.html#the-kws-hands-on-project",
    "href": "contents/labs/arduino/nicla_vision/kws/kws.html#the-kws-hands-on-project",
    "title": "Keyword Spotting (KWS)",
    "section": "The KWS Hands-On Project",
    "text": "The KWS Hands-On Project\nThe diagram below gives an idea of how the final KWS application should work (during inference):\n\nOur KWS application will recognize four classes of sound:\n\nYES (Keyword 1)\nNO (Keyword 2)\nNOISE (no words spoken; only background noise is present)\nUNKNOW (a mix of different words than YES and NO)\n\n\nFor real-world projects, it is always advisable to include other sounds besides the keywords, such as “Noise” (or Background) and “Unknown.”\n\n\nThe Machine Learning workflow\nThe main component of the KWS application is its model. So, we must train such a model with our specific keywords, noise, and other words (the “unknown”):",
    "crumbs": [
      "Nicla Vision",
      "Keyword Spotting (KWS)"
    ]
  },
  {
    "objectID": "contents/labs/arduino/nicla_vision/kws/kws.html#dataset",
    "href": "contents/labs/arduino/nicla_vision/kws/kws.html#dataset",
    "title": "Keyword Spotting (KWS)",
    "section": "Dataset",
    "text": "Dataset\nThe critical component of any Machine Learning Workflow is the dataset. Once we have decided on specific keywords, in our case (YES and NO), we can take advantage of the dataset developed by Pete Warden, “Speech Commands: A Dataset for Limited-Vocabulary Speech Recognition.” This dataset has 35 keywords (with +1,000 samples each), such as yes, no, stop, and go. In words such as yes and no, we can get 1,500 samples.\nYou can download a small portion of the dataset from Edge Studio (Keyword spotting pre-built dataset), which includes samples from the four classes we will use in this project: yes, no, noise, and background. For this, follow the steps below:\n\nDownload the keywords dataset.\nUnzip the file to a location of your choice.\n\n\nUploading the dataset to the Edge Impulse Studio\nInitiate a new project at Edge Impulse Studio (EIS) and select the Upload Existing Data tool in the Data Acquisition section. Choose the files to be uploaded:\n\nDefine the Label, select Automatically split between train and test, and Upload data to the EIS. Repeat for all classes.\n\nThe dataset will now appear in the Data acquisition section. Note that the approximately 6,000 samples (1,500 for each class) are split into Train (4,800) and Test (1,200) sets.\n\n\n\nCapturing additional Audio Data\nAlthough we have a lot of data from Pete’s dataset, collecting some words spoken by us is advised. When working with accelerometers, creating a dataset with data captured by the same type of sensor is essential. In the case of sound, this is optional because what we will classify is, in reality, audio data.\n\nThe key difference between sound and audio is the type of energy. Sound is mechanical perturbation (longitudinal sound waves) that propagate through a medium, causing variations of pressure in it. Audio is an electrical (analog or digital) signal representing sound.\n\nWhen we pronounce a keyword, the sound waves should be converted to audio data. The conversion should be done by sampling the signal generated by the microphone at a 16KHz frequency with 16-bit per sample amplitude.\nSo, any device that can generate audio data with this basic specification (16KHz/16bits) will work fine. As a device, we can use the NiclaV, a computer, or even your mobile phone.\n\n\nUsing the NiclaV and the Edge Impulse Studio\nAs we learned in the chapter Setup Nicla Vision, EIS officially supports the Nicla Vision, which simplifies the capture of the data from its sensors, including the microphone. So, please create a new project on EIS and connect the Nicla to it, following these steps:\n\nDownload the last updated EIS Firmware and unzip it.\nOpen the zip file on your computer and select the uploader corresponding to your OS:\n\n\n\nPut the NiclaV in Boot Mode by pressing the reset button twice.\nUpload the binary arduino-nicla-vision.bin to your board by running the batch code corresponding to your OS.\n\nGo to your project on EIS, and on the Data Acquisition tab, select WebUSB. A window will pop up; choose the option that shows that the Nicla is paired and press [Connect].\nYou can choose which sensor data to pick in the Collect Data section on the Data Acquisition tab. Select: Built-in microphone, define your label (for example, yes), the sampling Frequency[16000Hz], and the Sample length (in milliseconds), for example [10s]. Start sampling.\n\nData on Pete’s dataset have a length of 1s, but the recorded samples are 10s long and must be split into 1s samples. Click on three dots after the sample name and select Split sample.\nA window will pop up with the Split tool.\n\nOnce inside the tool, split the data into 1-second (1000 ms) records. If necessary, add or remove segments. This procedure should be repeated for all new samples.\n\n\nUsing a smartphone and the EI Studio\nYou can also use your PC or smartphone to capture audio data, using a sampling frequency of 16KHz and a bit depth of 16.\nGo to Devices, scan the QR Code using your phone, and click on the link. A data Collection app will appear in your browser. Select Collecting Audio, and define your Label, data capture Length, and Category.\n\nRepeat the same procedure used with the NiclaV.\n\nNote that any app, such as Audacity, can be used for audio recording, provided you use 16KHz/16-bit depth samples.",
    "crumbs": [
      "Nicla Vision",
      "Keyword Spotting (KWS)"
    ]
  },
  {
    "objectID": "contents/labs/arduino/nicla_vision/kws/kws.html#creating-impulse-pre-process-model-definition",
    "href": "contents/labs/arduino/nicla_vision/kws/kws.html#creating-impulse-pre-process-model-definition",
    "title": "Keyword Spotting (KWS)",
    "section": "Creating Impulse (Pre-Process / Model definition)",
    "text": "Creating Impulse (Pre-Process / Model definition)\nAn impulse takes raw data, uses signal processing to extract features, and then uses a learning block to classify new data.\n\nImpulse Design\n\nFirst, we will take the data points with a 1-second window, augmenting the data and sliding that window in 500ms intervals. Note that the option zero-pad data is set. It is essential to fill with ‘zeros’ samples smaller than 1 second (in some cases, some samples can result smaller than the 1000 ms window on the split tool to avoid noise and spikes).\nEach 1-second audio sample should be pre-processed and converted to an image (for example, 13 x 49 x 1). As discussed in the Feature Engineering for Audio Classification Hands-On tutorial, we will use Audio (MFCC), which extracts features from audio signals using Mel Frequency Cepstral Coefficients, which are well suited for the human voice, our case here.\nNext, we select the Classification block to build our model from scratch using a Convolution Neural Network (CNN).\n\nAlternatively, you can use the Transfer Learning (Keyword Spotting) block, which fine-tunes a pre-trained keyword spotting model on your data. This approach has good performance with relatively small keyword datasets.\n\n\n\nPre-Processing (MFCC)\nThe following step is to create the features to be trained in the next phase:\nWe could keep the default parameter values, but we will use the DSP Autotune parameters option.\n\nWe will take the Raw features (our 1-second, 16KHz sampled audio data) and use the MFCC processing block to calculate the Processed features. For every 16,000 raw features (16,000 x 1 second), we will get 637 processed features (13 x 49).\n\nThe result shows that we only used a small amount of memory to pre-process data (16KB) and a latency of 34ms, which is excellent. For example, on an Arduino Nano (Cortex-M4f @ 64MHz), the same pre-process will take around 480ms. The parameters chosen, such as the FFT length [512], will significantly impact the latency.\nNow, let’s Save parameters and move to the Generated features tab, where the actual features will be generated. Using UMAP, a dimension reduction technique, the Feature explorer shows how the features are distributed on a two-dimensional plot.\n\nThe result seems OK, with a visually clear separation between yes features (in red) and no features (in blue). The unknown features seem nearer to the no space than the yes. This suggests that the keyword no has more propensity to false positives.\n\n\nGoing under the hood\nTo understand better how the raw sound is preprocessed, look at the Feature Engineering for Audio Classification chapter. You can play with the MFCC features generation by downloading this notebook from GitHub or [Opening it In Colab]",
    "crumbs": [
      "Nicla Vision",
      "Keyword Spotting (KWS)"
    ]
  },
  {
    "objectID": "contents/labs/arduino/nicla_vision/kws/kws.html#model-design-and-training",
    "href": "contents/labs/arduino/nicla_vision/kws/kws.html#model-design-and-training",
    "title": "Keyword Spotting (KWS)",
    "section": "Model Design and Training",
    "text": "Model Design and Training\nWe will use a simple Convolution Neural Network (CNN) model, tested with 1D and 2D convolutions. The basic architecture has two blocks of Convolution + MaxPooling ([8] and [16] filters, respectively) and a Dropout of [0.25] for the 1D and [0.5] for the 2D. For the last layer, after Flattening, we have [4] neurons, one for each class:\n\nAs hyper-parameters, we will have a Learning Rate of [0.005] and a model trained by [100] epochs. We will also include a data augmentation method based on SpecAugment. We trained the 1D and the 2D models with the same hyperparameters. The 1D architecture had a better overall result (90.5% accuracy when compared with 88% of the 2D, so we will use the 1D.\n\n\nUsing 1D convolutions is more efficient because it requires fewer parameters than 2D convolutions, making them more suitable for resource-constrained environments.\n\nIt is also interesting to pay attention to the 1D Confusion Matrix. The F1 Score for yes is 95%, and for no, 91%. That was expected by what we saw with the Feature Explorer (no and unknown at close distance). In trying to improve the result, you can inspect closely the results of the samples with an error.\n\nListen to the samples that went wrong. For example, for yes, most of the mistakes were related to a yes pronounced as “yeh”. You can acquire additional samples and then retrain your model.\n\nGoing under the hood\nIf you want to understand what is happening “under the hood,” you can download the pre-processed dataset (MFCC training data) from the Dashboard tab and run this Jupyter Notebook, playing with the code or [Opening it In Colab]. For example, you can analyze the accuracy by each epoch:",
    "crumbs": [
      "Nicla Vision",
      "Keyword Spotting (KWS)"
    ]
  },
  {
    "objectID": "contents/labs/arduino/nicla_vision/kws/kws.html#testing",
    "href": "contents/labs/arduino/nicla_vision/kws/kws.html#testing",
    "title": "Keyword Spotting (KWS)",
    "section": "Testing",
    "text": "Testing\nTesting the model with the data reserved for training (Test Data), we got an accuracy of approximately 76%.\n\nInspecting the F1 score, we can see that for YES, we got 0.90, an excellent result since we expect to use this keyword as the primary “trigger” for our KWS project. The worst result (0.70) is for UNKNOWN, which is OK.\nFor NO, we got 0.72, which was expected, but to improve this result, we can move the samples that were not correctly classified to the training dataset and then repeat the training process.\n\nLive Classification\nWe can proceed to the project’s next step but also consider that it is possible to perform Live Classification using the NiclaV or a smartphone to capture live samples, testing the trained model before deployment on our device.",
    "crumbs": [
      "Nicla Vision",
      "Keyword Spotting (KWS)"
    ]
  },
  {
    "objectID": "contents/labs/arduino/nicla_vision/kws/kws.html#deploy-and-inference",
    "href": "contents/labs/arduino/nicla_vision/kws/kws.html#deploy-and-inference",
    "title": "Keyword Spotting (KWS)",
    "section": "Deploy and Inference",
    "text": "Deploy and Inference\nThe EIS will package all the needed libraries, preprocessing functions, and trained models, downloading them to your computer. Go to the Deployment section, select Arduino Library, and at the bottom, choose Quantized (Int8) and press Build.\n\nWhen the Build button is selected, a zip file will be created and downloaded to your computer. On your Arduino IDE, go to the Sketch tab, select the option Add .ZIP Library, and Choose the .zip file downloaded by EIS:\n\nNow, it is time for a real test. We will make inferences while completely disconnected from the EIS. Let’s use the NiclaV code example created when we deployed the Arduino Library.\nIn your Arduino IDE, go to the File/Examples tab, look for your project, and select nicla-vision/nicla-vision_microphone (or nicla-vision_microphone_continuous)\n\nPress the reset button twice to put the NiclaV in boot mode, upload the sketch to your board, and test some real inferences:",
    "crumbs": [
      "Nicla Vision",
      "Keyword Spotting (KWS)"
    ]
  },
  {
    "objectID": "contents/labs/arduino/nicla_vision/kws/kws.html#post-processing",
    "href": "contents/labs/arduino/nicla_vision/kws/kws.html#post-processing",
    "title": "Keyword Spotting (KWS)",
    "section": "Post-processing",
    "text": "Post-processing\nNow that we know the model is working since it detects our keywords, let’s modify the code to see the result with the NiclaV completely offline (disconnected from the PC and powered by a battery, a power bank, or an independent 5V power supply).\nThe idea is that whenever the keyword YES is detected, the Green LED will light; if a NO is heard, the Red LED will light, if it is a UNKNOW, the Blue LED will light; and in the presence of noise (No Keyword), the LEDs will be OFF.\nWe should modify one of the code examples. Let’s do it now with the nicla-vision_microphone_continuous.\nStart with initializing the LEDs:\n...\nvoid setup()\n{\n        // Once you finish debugging your code, you can comment or delete the Serial part of the code\n    Serial.begin(115200);\n    while (!Serial);\n    Serial.println(\"Inferencing - Nicla Vision KWS with LEDs\");\n    \n    // Pins for the built-in RGB LEDs on the Arduino NiclaV\n    pinMode(LEDR, OUTPUT);\n    pinMode(LEDG, OUTPUT);\n    pinMode(LEDB, OUTPUT);\n\n    // Ensure the LEDs are OFF by default.\n    // Note: The RGB LEDs on the Arduino Nicla Vision\n    // are ON when the pin is LOW, OFF when HIGH.\n    digitalWrite(LEDR, HIGH);\n    digitalWrite(LEDG, HIGH);\n    digitalWrite(LEDB, HIGH);\n...\n}\nCreate two functions, turn_off_leds() function , to turn off all RGB LEDs\n/*\n * @brief      turn_off_leds function - turn-off all RGB LEDs\n */\nvoid turn_off_leds(){\n    digitalWrite(LEDR, HIGH);\n    digitalWrite(LEDG, HIGH);\n    digitalWrite(LEDB, HIGH);\n}\nAnother turn_on_led() function is used to turn on the RGB LEDs according to the most probable result of the classifier.\n/*\n * @brief      turn_on_leds function used to turn on the RGB LEDs\n * @param[in]  pred_index     \n *             no:       [0] ==&gt; Red ON\n *             noise:    [1] ==&gt; ALL OFF \n *             unknown:  [2] ==&gt; Blue ON\n *             Yes:      [3] ==&gt; Green ON\n */\nvoid turn_on_leds(int pred_index) {\n  switch (pred_index)\n  {\n    case 0:\n      turn_off_leds();\n      digitalWrite(LEDR, LOW);\n      break;\n\n    case 1:\n      turn_off_leds();\n      break;\n    \n    case 2:\n      turn_off_leds();\n      digitalWrite(LEDB, LOW);\n      break;\n\n    case 3:\n      turn_off_leds();\n      digitalWrite(LEDG, LOW);\n      break;\n  }\n}\nAnd change the // print the predictions portion of the code on loop():\n...\n\n    if (++print_results &gt;= (EI_CLASSIFIER_SLICES_PER_MODEL_WINDOW)) {\n        // print the predictions\n        ei_printf(\"Predictions \");\n        ei_printf(\"(DSP: %d ms., Classification: %d ms., Anomaly: %d ms.)\",\n            result.timing.dsp, result.timing.classification, result.timing.anomaly);\n        ei_printf(\": \\n\");\n\n        int pred_index = 0;     // Initialize pred_index\n        float pred_value = 0;   // Initialize pred_value\n\n        for (size_t ix = 0; ix &lt; EI_CLASSIFIER_LABEL_COUNT; ix++) {\n            if (result.classification[ix].value &gt; pred_value){\n                pred_index = ix;\n                pred_value = result.classification[ix].value;\n            }\n            // ei_printf(\"    %s: \", result.classification[ix].label);\n            // ei_printf_float(result.classification[ix].value);\n            // ei_printf(\"\\n\");\n        }\n        ei_printf(\"  PREDICTION: ==&gt; %s with probability %.2f\\n\", \n                  result.classification[pred_index].label, pred_value);\n        turn_on_leds (pred_index);\n\n        \n#if EI_CLASSIFIER_HAS_ANOMALY == 1\n        ei_printf(\"    anomaly score: \");\n        ei_printf_float(result.anomaly);\n        ei_printf(\"\\n\");\n#endif\n\n        print_results = 0;\n    }\n}\n\n...\nYou can find the complete code on the project’s GitHub.\nUpload the sketch to your board and test some real inferences. The idea is that the Green LED will be ON whenever the keyword YES is detected, the Red will lit for a NO, and any other word will turn on the Blue LED. All the LEDs should be off if silence or background noise is present. Remember that the same procedure can “trigger” an external device to perform a desired action instead of turning on an LED, as we saw in the introduction.",
    "crumbs": [
      "Nicla Vision",
      "Keyword Spotting (KWS)"
    ]
  },
  {
    "objectID": "contents/labs/arduino/nicla_vision/kws/kws.html#conclusion",
    "href": "contents/labs/arduino/nicla_vision/kws/kws.html#conclusion",
    "title": "Keyword Spotting (KWS)",
    "section": "Conclusion",
    "text": "Conclusion\n\nYou will find the notebooks and codeused in this hands-on tutorial on the GitHub repository.\n\nBefore we finish, consider that Sound Classification is more than just voice. For example, you can develop TinyML projects around sound in several areas, such as:\n\nSecurity (Broken Glass detection, Gunshot)\nIndustry (Anomaly Detection)\nMedical (Snore, Cough, Pulmonary diseases)\nNature (Beehive control, insect sound, pouching mitigation)",
    "crumbs": [
      "Nicla Vision",
      "Keyword Spotting (KWS)"
    ]
  },
  {
    "objectID": "contents/labs/arduino/nicla_vision/kws/kws.html#resources",
    "href": "contents/labs/arduino/nicla_vision/kws/kws.html#resources",
    "title": "Keyword Spotting (KWS)",
    "section": "Resources",
    "text": "Resources\n\nSubset of Google Speech Commands Dataset\nKWS MFCC Analysis Colab Notebook\nKWS_CNN_training Colab Notebook\nArduino Post-processing Code\nEdge Impulse Project",
    "crumbs": [
      "Nicla Vision",
      "Keyword Spotting (KWS)"
    ]
  },
  {
    "objectID": "contents/labs/arduino/nicla_vision/motion_classification/motion_classification.html",
    "href": "contents/labs/arduino/nicla_vision/motion_classification/motion_classification.html",
    "title": "Motion Classification and Anomaly Detection",
    "section": "",
    "text": "Overview\nTransportation is the backbone of global commerce. Millions of containers are transported daily via various means, such as ships, trucks, and trains, to destinations worldwide. Ensuring these containers’ safe and efficient transit is a monumental task that requires leveraging modern technology, and TinyML is undoubtedly one of them.\nIn this hands-on tutorial, we will work to solve real-world problems related to transportation. We will develop a Motion Classification and Anomaly Detection system using the Arduino Nicla Vision board, the Arduino IDE, and the Edge Impulse Studio. This project will help us understand how containers experience different forces and motions during various phases of transportation, such as terrestrial and maritime transit, vertical movement via forklifts, and stationary periods in warehouses.\nBy the end of this tutorial, you’ll have a working prototype that can classify different types of motion and detect anomalies during the transportation of containers. This knowledge can be a stepping stone to more advanced projects in the burgeoning field of TinyML involving vibration.",
    "crumbs": [
      "Nicla Vision",
      "Motion Classification and Anomaly Detection"
    ]
  },
  {
    "objectID": "contents/labs/arduino/nicla_vision/motion_classification/motion_classification.html#overview",
    "href": "contents/labs/arduino/nicla_vision/motion_classification/motion_classification.html#overview",
    "title": "Motion Classification and Anomaly Detection",
    "section": "",
    "text": "Learning Objectives\n\n\n\n\nSetting up the Arduino Nicla Vision Board\nData Collection and Preprocessing\nBuilding the Motion Classification Model\nImplementing Anomaly Detection\nReal-world Testing and Analysis",
    "crumbs": [
      "Nicla Vision",
      "Motion Classification and Anomaly Detection"
    ]
  },
  {
    "objectID": "contents/labs/arduino/nicla_vision/motion_classification/motion_classification.html#imu-installation-and-testing",
    "href": "contents/labs/arduino/nicla_vision/motion_classification/motion_classification.html#imu-installation-and-testing",
    "title": "Motion Classification and Anomaly Detection",
    "section": "IMU Installation and testing",
    "text": "IMU Installation and testing\nFor this project, we will use an accelerometer. As discussed in the Hands-On Tutorial, Setup Nicla Vision, the Nicla Vision Board has an onboard 6-axis IMU: 3D gyroscope and 3D accelerometer, the LSM6DSOX. Let’s verify if the LSM6DSOX IMU library is installed. If not, install it.\n\nNext, go to Examples &gt; Arduino_LSM6DSOX &gt; SimpleAccelerometer and run the accelerometer test. You can check if it works by opening the IDE Serial Monitor or Plotter. The values are in g (earth gravity), with a default range of +/- 4g:\n\n\nDefining the Sampling frequency:\nChoosing an appropriate sampling frequency is crucial for capturing the motion characteristics you’re interested in studying. The Nyquist-Shannon sampling theorem states that the sampling rate should be at least twice the highest frequency component in the signal to reconstruct it properly. In the context of motion classification and anomaly detection for transportation, the choice of sampling frequency would depend on several factors:\n\nNature of the Motion: Different types of transportation (terrestrial, maritime, etc.) may involve different ranges of motion frequencies. Faster movements may require higher sampling frequencies.\nHardware Limitations: The Arduino Nicla Vision board and any associated sensors may have limitations on how fast they can sample data.\nComputational Resources: Higher sampling rates will generate more data, which might be computationally intensive, especially critical in a TinyML environment.\nBattery Life: A higher sampling rate will consume more power. If the system is battery-operated, this is an important consideration.\nData Storage: More frequent sampling will require more storage space, another crucial consideration for embedded systems with limited memory.\n\nIn many human activity recognition tasks, sampling rates of around 50 Hz to 100 Hz are commonly used. Given that we are simulating transportation scenarios, which are generally not high-frequency events, a sampling rate in that range (50-100 Hz) might be a reasonable starting point.\nLet’s define a sketch that will allow us to capture our data with a defined sampling frequency (for example, 50Hz):\n/*\n * Based on Edge Impulse Data Forwarder Example (Arduino)\n  - https://docs.edgeimpulse.com/docs/cli-data-forwarder\n * Developed by M.Rovai @11May23\n */\n\n/* Include ----------------------------------------------------------------- */\n#include &lt;Arduino_LSM6DSOX.h&gt;\n\n/* Constant defines -------------------------------------------------------- */\n#define CONVERT_G_TO_MS2 9.80665f\n#define FREQUENCY_HZ        50\n#define INTERVAL_MS         (1000 / (FREQUENCY_HZ + 1))\n\nstatic unsigned long last_interval_ms = 0;\nfloat x, y, z;\n\nvoid setup() {\n  Serial.begin(9600);\n  while (!Serial);\n\n  if (!IMU.begin()) {\n    Serial.println(\"Failed to initialize IMU!\");\n    while (1);\n  }\n}\n\nvoid loop() {\n  if (millis() &gt; last_interval_ms + INTERVAL_MS) {\n    last_interval_ms = millis();\n    \n    if (IMU.accelerationAvailable()) {\n      // Read raw acceleration measurements from the device\n      IMU.readAcceleration(x, y, z);\n\n      // converting to m/s2\n      float ax_m_s2 = x * CONVERT_G_TO_MS2;\n      float ay_m_s2 = y * CONVERT_G_TO_MS2;\n      float az_m_s2 = z * CONVERT_G_TO_MS2;\n\n      Serial.print(ax_m_s2); \n      Serial.print(\"\\t\");\n      Serial.print(ay_m_s2); \n      Serial.print(\"\\t\");\n      Serial.println(az_m_s2); \n    }\n  }\n}\nUploading the sketch and inspecting the Serial Monitor, we can see that we are capturing 50 samples per second.\n\n\nNote that with the Nicla board resting on a table (with the camera facing down), the z-axis measures around 9.8m/s\\(^2\\), the expected earth acceleration.",
    "crumbs": [
      "Nicla Vision",
      "Motion Classification and Anomaly Detection"
    ]
  },
  {
    "objectID": "contents/labs/arduino/nicla_vision/motion_classification/motion_classification.html#the-case-study-simulated-container-transportation",
    "href": "contents/labs/arduino/nicla_vision/motion_classification/motion_classification.html#the-case-study-simulated-container-transportation",
    "title": "Motion Classification and Anomaly Detection",
    "section": "The Case Study: Simulated Container Transportation",
    "text": "The Case Study: Simulated Container Transportation\nWe will simulate container (or better package) transportation through different scenarios to make this tutorial more relatable and practical. Using the built-in accelerometer of the Arduino Nicla Vision board, we’ll capture motion data by manually simulating the conditions of:\n\nTerrestrial Transportation (by road or train)\nMaritime-associated Transportation\nVertical Movement via Fork-Lift\nStationary (Idle) period in a Warehouse\n\n\nFrom the above images, we can define for our simulation that primarily horizontal movements (x or y axis) should be associated with the “Terrestrial class,” Vertical movements (z-axis) with the “Lift Class,” no activity with the “Idle class,” and movement on all three axes to Maritime class.",
    "crumbs": [
      "Nicla Vision",
      "Motion Classification and Anomaly Detection"
    ]
  },
  {
    "objectID": "contents/labs/arduino/nicla_vision/motion_classification/motion_classification.html#data-collection",
    "href": "contents/labs/arduino/nicla_vision/motion_classification/motion_classification.html#data-collection",
    "title": "Motion Classification and Anomaly Detection",
    "section": "Data Collection",
    "text": "Data Collection\nFor data collection, we can have several options. In a real case, we can have our device, for example, connected directly to one container, and the data collected on a file (for example .CSV) and stored on an SD card (Via SPI connection) or an offline repo in your computer. Data can also be sent remotely to a nearby repository, such as a mobile phone, using Bluetooth (as done in this project: Sensor DataLogger). Once your dataset is collected and stored as a .CSV file, it can be uploaded to the Studio using the CSV Wizard tool.\n\nIn this video, you can learn alternative ways to send data to the Edge Impulse Studio.\n\n\nConnecting the device to Edge Impulse\nWe will connect the Nicla directly to the Edge Impulse Studio, which will also be used for data pre-processing, model training, testing, and deployment. For that, you have two options:\n\nDownload the latest firmware and connect it directly to the Data Collection section.\nUse the CLI Data Forwarder tool to capture sensor data from the sensor and send it to the Studio.\n\nOption 1 is more straightforward, as we saw in the Setup Nicla Vision hands-on, but option 2 will give you more flexibility regarding capturing your data, such as sampling frequency definition. Let’s do it with the last one.\nPlease create a new project on the Edge Impulse Studio (EIS) and connect the Nicla to it, following these steps:\n\nInstall the Edge Impulse CLI and the Node.js into your computer.\nUpload a sketch for data capture (the one discussed previously in this tutorial).\nUse the CLI Data Forwarder to capture data from the Nicla’s accelerometer and send it to the Studio, as shown in this diagram:\n\n\nStart the CLI Data Forwarder on your terminal, entering (if it is the first time) the following command:\n$ edge-impulse-data-forwarder --clean\nNext, enter your EI credentials and choose your project, variables (for example, accX, accY, and accZ), and device name (for example, NiclaV:\n\nGo to the Devices section on your EI Project and verify if the device is connected (the dot should be green):\n\n\nYou can clone the project developed for this hands-on: NICLA Vision Movement Classification.\n\n\n\nData Collection\nOn the Data Acquisition section, you should see that your board [NiclaV] is connected. The sensor is available: [sensor with 3 axes (accX, accY, accZ)] with a sampling frequency of [50Hz]. The Studio suggests a sample length of [10000] ms (10s). The last thing left is defining the sample label. Let’s start with[terrestrial]:\n\nTerrestrial (palettes in a Truck or Train), moving horizontally. Press [Start Sample]and move your device horizontally, keeping one direction over your table. After 10 s, your data will be uploaded to the studio. Here is how the sample was collected:\n\nAs expected, the movement was captured mainly in the Y-axis (green). In the blue, we see the Z axis, around -10 m/s\\(^2\\) (the Nicla has the camera facing up).\nAs discussed before, we should capture data from all four Transportation Classes. So, imagine that you have a container with a built-in accelerometer facing the following situations:\nMaritime (pallets in boats into an angry ocean). The movement is captured on all three axes:\n\nLift (Palettes being handled vertically by a Forklift). Movement captured only in the Z-axis:\n\nIdle (Paletts in a warehouse). No movement detected by the accelerometer:\n\nYou can capture, for example, 2 minutes (twelve samples of 10 seconds) for each of the four classes (a total of 8 minutes of data). Using the three dots menu after each one of the samples, select 2 of them, reserving them for the Test set. Alternatively, you can use the automatic Train/Test Split tool on the Danger Zone of Dashboard tab. Below, you can see the resulting dataset:\n\nOnce you have captured your dataset, you can explore it in more detail using the Data Explorer, a visual tool to find outliers or mislabeled data (helping to correct them). The data explorer first tries to extract meaningful features from your data (by applying signal processing and neural network embeddings) and then uses a dimensionality reduction algorithm such as PCA or t-SNE to map these features to a 2D space. This gives you a one-look overview of your complete dataset.\n\nIn our case, the dataset seems OK (good separation). But the PCA shows we can have issues between maritime (green) and lift (orange). This is expected, once on a boat, sometimes the movement can be only “vertical”.",
    "crumbs": [
      "Nicla Vision",
      "Motion Classification and Anomaly Detection"
    ]
  },
  {
    "objectID": "contents/labs/arduino/nicla_vision/motion_classification/motion_classification.html#impulse-design",
    "href": "contents/labs/arduino/nicla_vision/motion_classification/motion_classification.html#impulse-design",
    "title": "Motion Classification and Anomaly Detection",
    "section": "Impulse Design",
    "text": "Impulse Design\nThe next step is the definition of our Impulse, which takes the raw data and uses signal processing to extract features, passing them as the input tensor of a learning block to classify new data. Go to Impulse Design and Create Impulse. The Studio will suggest the basic design. Let’s also add a second Learning Block for Anomaly Detection.\n\nThis second model uses a K-means model. If we imagine that we could have our known classes as clusters, any sample that could not fit on that could be an outlier, an anomaly such as a container rolling out of a ship on the ocean or falling from a Forklift.\n\nThe sampling frequency should be automatically captured, if not, enter it: [50]Hz. The Studio suggests a Window Size of 2 seconds ([2000] ms) with a sliding window of [20]ms. What we are defining in this step is that we will pre-process the captured data (Time-Seres data), creating a tabular dataset features) that will be the input for a Neural Networks Classifier (DNN) and an Anomaly Detection model (K-Means), as shown below:\n\nLet’s dig into those steps and parameters to understand better what we are doing here.\n\nData Pre-Processing Overview\nData pre-processing is extracting features from the dataset captured with the accelerometer, which involves processing and analyzing the raw data. Accelerometers measure the acceleration of an object along one or more axes (typically three, denoted as X, Y, and Z). These measurements can be used to understand various aspects of the object’s motion, such as movement patterns and vibrations.\nRaw accelerometer data can be noisy and contain errors or irrelevant information. Preprocessing steps, such as filtering and normalization, can clean and standardize the data, making it more suitable for feature extraction. In our case, we should divide the data into smaller segments or windows. This can help focus on specific events or activities within the dataset, making feature extraction more manageable and meaningful. The window size and overlap (window increase) choice depend on the application and the frequency of the events of interest. As a thumb rule, we should try to capture a couple of “cycles of data”.\n\nWith a sampling rate (SR) of 50Hz and a window size of 2 seconds, we will get 100 samples per axis, or 300 in total (3 axis x 2 seconds x 50 samples). We will slide this window every 200ms, creating a larger dataset where each instance has 300 raw features.\n\n\nOnce the data is preprocessed and segmented, you can extract features that describe the motion’s characteristics. Some typical features extracted from accelerometer data include:\n\nTime-domain features describe the data’s statistical properties within each segment, such as mean, median, standard deviation, skewness, kurtosis, and zero-crossing rate.\nFrequency-domain features are obtained by transforming the data into the frequency domain using techniques like the Fast Fourier Transform (FFT). Some typical frequency-domain features include the power spectrum, spectral energy, dominant frequencies (amplitude and frequency), and spectral entropy.\nTime-frequency domain features combine the time and frequency domain information, such as the Short-Time Fourier Transform (STFT) or the Discrete Wavelet Transform (DWT). They can provide a more detailed understanding of how the signal’s frequency content changes over time.\n\nIn many cases, the number of extracted features can be large, which may lead to overfitting or increased computational complexity. Feature selection techniques, such as mutual information, correlation-based methods, or principal component analysis (PCA), can help identify the most relevant features for a given application and reduce the dimensionality of the dataset. The Studio can help with such feature importance calculations.\n\n\nEI Studio Spectral Features\nData preprocessing is a challenging area for embedded machine learning, still, Edge Impulse helps overcome this with its digital signal processing (DSP) preprocessing step and, more specifically, the Spectral Features Block.\nOn the Studio, the collected raw dataset will be the input of a Spectral Analysis block, which is excellent for analyzing repetitive motion, such as data from accelerometers. This block will perform a DSP (Digital Signal Processing), extracting features such as FFT or Wavelets.\nFor our project, once the time signal is continuous, we should use FFT with, for example, a length of [32].\nThe per axis/channel Time Domain Statistical features are:\n\nRMS: 1 feature\nSkewness: 1 feature\nKurtosis: 1 feature\n\nThe per axis/channel Frequency Domain Spectral features are:\n\nSpectral Power: 16 features (FFT Length/2)\nSkewness: 1 feature\nKurtosis: 1 feature\n\nSo, for an FFT length of 32 points, the resulting output of the Spectral Analysis Block will be 21 features per axis (a total of 63 features).\n\nYou can learn more about how each feature is calculated by downloading the notebook Edge Impulse - Spectral Features Block Analysis TinyML under the hood: Spectral Analysis or opening it directly on Google CoLab.\n\n\n\nGenerating features\nOnce we understand what the pre-processing does, it is time to finish the job. So, let’s take the raw data (time-series type) and convert it to tabular data. For that, go to the Spectral Features section on the Parameters tab, define the main parameters as discussed in the previous section ([FFT] with [32] points), and select[Save Parameters]:\n\nAt the top menu, select the Generate Features option and the Generate Features button. Each 2-second window data will be converted into one data point of 63 features.\n\nThe Feature Explorer will show those data in 2D using UMAP. Uniform Manifold Approximation and Projection (UMAP) is a dimension reduction technique that can be used for visualization similarly to t-SNE but is also applicable for general non-linear dimension reduction.\n\nThe visualization makes it possible to verify that after the feature generation, the classes present keep their excellent separation, which indicates that the classifier should work well. Optionally, you can analyze how important each one of the features is for one class compared with others.",
    "crumbs": [
      "Nicla Vision",
      "Motion Classification and Anomaly Detection"
    ]
  },
  {
    "objectID": "contents/labs/arduino/nicla_vision/motion_classification/motion_classification.html#models-training",
    "href": "contents/labs/arduino/nicla_vision/motion_classification/motion_classification.html#models-training",
    "title": "Motion Classification and Anomaly Detection",
    "section": "Models Training",
    "text": "Models Training\nOur classifier will be a Dense Neural Network (DNN) that will have 63 neurons on its input layer, two hidden layers with 20 and 10 neurons, and an output layer with four neurons (one per each class), as shown here:\n\nAs hyperparameters, we will use a Learning Rate of [0.005], a Batch size of [32], and [20]% of data for validation for [30] epochs. After training, we can see that the accuracy is 98.5%. The cost of memory and latency is meager.\n\nFor Anomaly Detection, we will choose the suggested features that are precisely the most important ones in the Feature Extraction, plus the accZ RMS. The number of clusters will be [32], as suggested by the Studio:",
    "crumbs": [
      "Nicla Vision",
      "Motion Classification and Anomaly Detection"
    ]
  },
  {
    "objectID": "contents/labs/arduino/nicla_vision/motion_classification/motion_classification.html#testing",
    "href": "contents/labs/arduino/nicla_vision/motion_classification/motion_classification.html#testing",
    "title": "Motion Classification and Anomaly Detection",
    "section": "Testing",
    "text": "Testing\nWe can verify how our model will behave with unknown data using 20% of the data left behind during the data capture phase. The result was almost 95%, which is good. You can always work to improve the results, for example, to understand what went wrong with one of the wrong results. If it is a unique situation, you can add it to the training dataset and then repeat it.\nThe default minimum threshold for a considered uncertain result is [0.6] for classification and [0.3] for anomaly. Once we have four classes (their output sum should be 1.0), you can also set up a lower threshold for a class to be considered valid (for example, 0.4). You can Set confidence thresholds on the three dots menu, besides the Classify all button.\n\nYou can also perform Live Classification with your device (which should still be connected to the Studio).\n\nBe aware that here, you will capture real data with your device and upload it to the Studio, where an inference will be taken using the trained model (But the model is NOT in your device).",
    "crumbs": [
      "Nicla Vision",
      "Motion Classification and Anomaly Detection"
    ]
  },
  {
    "objectID": "contents/labs/arduino/nicla_vision/motion_classification/motion_classification.html#deploy",
    "href": "contents/labs/arduino/nicla_vision/motion_classification/motion_classification.html#deploy",
    "title": "Motion Classification and Anomaly Detection",
    "section": "Deploy",
    "text": "Deploy\nIt is time to deploy the preprocessing block and the trained model to the Nicla. The Studio will package all the needed libraries, preprocessing functions, and trained models, downloading them to your computer. You should select the option Arduino Library, and at the bottom, you can choose Quantized (Int8) or Unoptimized (float32) and [Build]. A Zip file will be created and downloaded to your computer.\n\nOn your Arduino IDE, go to the Sketch tab, select Add.ZIP Library, and Choose the.zip file downloaded by the Studio. A message will appear in the IDE Terminal: Library installed.\n\nInference\nNow, it is time for a real test. We will make inferences wholly disconnected from the Studio. Let’s change one of the code examples created when you deploy the Arduino Library.\nIn your Arduino IDE, go to the File/Examples tab and look for your project, and on examples, select Nicla_vision_fusion:\n\nNote that the code created by Edge Impulse considers a sensor fusion approach where the IMU (Accelerometer and Gyroscope) and the ToF are used. At the beginning of the code, you have the libraries related to our project, IMU and ToF:\n/* Includes ---------------------------------------------------------------- */\n#include &lt;NICLA_Vision_Movement_Classification_inferencing.h&gt; \n#include &lt;Arduino_LSM6DSOX.h&gt; //IMU\n#include \"VL53L1X.h\" // ToF\n\nYou can keep the code this way for testing because the trained model will use only features pre-processed from the accelerometer. But consider that you will write your code only with the needed libraries for a real project.\n\nAnd that is it!\nYou can now upload the code to your device and proceed with the inferences. Press the Nicla [RESET] button twice to put it on boot mode (disconnect from the Studio if it is still connected), and upload the sketch to your board.\nNow you should try different movements with your board (similar to those done during data capture), observing the inference result of each class on the Serial Monitor:\n\nIdle and lift classes:\n\n\n\nMaritime and terrestrial:\n\n\nNote that in all situations above, the value of the anomaly score was smaller than 0.0. Try a new movement that was not part of the original dataset, for example, “rolling” the Nicla, facing the camera upside-down, as a container falling from a boat or even a boat accident:\n\nAnomaly detection:\n\n\nIn this case, the anomaly is much bigger, over 1.00\n\n\nPost-processing\nNow that we know the model is working since it detects the movements, we suggest that you modify the code to see the result with the NiclaV completely offline (disconnected from the PC and powered by a battery, a power bank, or an independent 5V power supply).\nThe idea is to do the same as with the KWS project: if one specific movement is detected, a specific LED could be lit. For example, if terrestrial is detected, the Green LED will light; if maritime, the Red LED will light, if it is a lift, the Blue LED will light; and if no movement is detected (idle), the LEDs will be OFF. You can also add a condition when an anomaly is detected, in this case, for example, a white color can be used (all e LEDs light simultaneously).",
    "crumbs": [
      "Nicla Vision",
      "Motion Classification and Anomaly Detection"
    ]
  },
  {
    "objectID": "contents/labs/arduino/nicla_vision/motion_classification/motion_classification.html#conclusion",
    "href": "contents/labs/arduino/nicla_vision/motion_classification/motion_classification.html#conclusion",
    "title": "Motion Classification and Anomaly Detection",
    "section": "Conclusion",
    "text": "Conclusion\n\nThe notebooks and codeused in this hands-on tutorial will be found on the GitHub repository.\n\nBefore we finish, consider that Movement Classification and Object Detection can be utilized in many applications across various domains. Here are some of the potential applications:\n\nCase Applications\n\nIndustrial and Manufacturing\n\nPredictive Maintenance: Detecting anomalies in machinery motion to predict failures before they occur.\nQuality Control: Monitoring the motion of assembly lines or robotic arms for precision assessment and deviation detection from the standard motion pattern.\nWarehouse Logistics: Managing and tracking the movement of goods with automated systems that classify different types of motion and detect anomalies in handling.\n\n\n\nHealthcare\n\nPatient Monitoring: Detecting falls or abnormal movements in the elderly or those with mobility issues.\nRehabilitation: Monitoring the progress of patients recovering from injuries by classifying motion patterns during physical therapy sessions.\nActivity Recognition: Classifying types of physical activity for fitness applications or patient monitoring.\n\n\n\nConsumer Electronics\n\nGesture Control: Interpreting specific motions to control devices, such as turning on lights with a hand wave.\nGaming: Enhancing gaming experiences with motion-controlled inputs.\n\n\n\nTransportation and Logistics\n\nVehicle Telematics: Monitoring vehicle motion for unusual behavior such as hard braking, sharp turns, or accidents.\nCargo Monitoring: Ensuring the integrity of goods during transport by detecting unusual movements that could indicate tampering or mishandling.\n\n\n\nSmart Cities and Infrastructure\n\nStructural Health Monitoring: Detecting vibrations or movements within structures that could indicate potential failures or maintenance needs.\nTraffic Management: Analyzing the flow of pedestrians or vehicles to improve urban mobility and safety.\n\n\n\nSecurity and Surveillance\n\nIntruder Detection: Detecting motion patterns typical of unauthorized access or other security breaches.\nWildlife Monitoring: Detecting poachers or abnormal animal movements in protected areas.\n\n\n\nAgriculture\n\nEquipment Monitoring: Tracking the performance and usage of agricultural machinery.\nAnimal Behavior Analysis: Monitoring livestock movements to detect behaviors indicating health issues or stress.\n\n\n\nEnvironmental Monitoring\n\nSeismic Activity: Detecting irregular motion patterns that precede earthquakes or other geologically relevant events.\nOceanography: Studying wave patterns or marine movements for research and safety purposes.\n\n\n\n\nNicla 3D case\nFor real applications, as some described before, we can add a case to our device, and Eoin Jordan, from Edge Impulse, developed a great wearable and machine health case for the Nicla range of boards. It works with a 10mm magnet, 2M screws, and a 16mm strap for human and machine health use case scenarios. Here is the link: Arduino Nicla Voice and Vision Wearable Case.\n\nThe applications for motion classification and anomaly detection are extensive, and the Arduino Nicla Vision is well-suited for scenarios where low power consumption and edge processing are advantageous. Its small form factor and efficiency in processing make it an ideal choice for deploying portable and remote applications where real-time processing is crucial and connectivity may be limited.",
    "crumbs": [
      "Nicla Vision",
      "Motion Classification and Anomaly Detection"
    ]
  },
  {
    "objectID": "contents/labs/arduino/nicla_vision/motion_classification/motion_classification.html#resources",
    "href": "contents/labs/arduino/nicla_vision/motion_classification/motion_classification.html#resources",
    "title": "Motion Classification and Anomaly Detection",
    "section": "Resources",
    "text": "Resources\n\nArduino Code\nEdge Impulse Spectral Features Block Colab Notebook\nEdge Impulse Project",
    "crumbs": [
      "Nicla Vision",
      "Motion Classification and Anomaly Detection"
    ]
  },
  {
    "objectID": "contents/labs/seeed/xiao_esp32s3/xiao_esp32s3.html",
    "href": "contents/labs/seeed/xiao_esp32s3/xiao_esp32s3.html",
    "title": "XIAO ESP32S3",
    "section": "",
    "text": "Pre-requisites\nThese labs provide a unique opportunity to gain practical experience with machine learning (ML) systems. Unlike working with large models requiring data center-scale resources, these exercises allow you to directly interact with hardware and software using TinyML. This hands-on approach gives you a tangible understanding of the challenges and opportunities in deploying AI, albeit at a tiny scale. However, the principles are largely the same as what you would encounter when working with larger systems.",
    "crumbs": [
      "XIAO ESP32S3"
    ]
  },
  {
    "objectID": "contents/labs/seeed/xiao_esp32s3/xiao_esp32s3.html#pre-requisites",
    "href": "contents/labs/seeed/xiao_esp32s3/xiao_esp32s3.html#pre-requisites",
    "title": "XIAO ESP32S3",
    "section": "",
    "text": "XIAO ESP32S3 Sense Board: Ensure you have the XIAO ESP32S3 Sense Board.\nUSB-C Cable: This is for connecting the board to your computer.\nNetwork: With internet access for downloading necessary software.\nSD Card and an SD card Adapter: This saves audio and images (optional).",
    "crumbs": [
      "XIAO ESP32S3"
    ]
  },
  {
    "objectID": "contents/labs/seeed/xiao_esp32s3/xiao_esp32s3.html#setup",
    "href": "contents/labs/seeed/xiao_esp32s3/xiao_esp32s3.html#setup",
    "title": "XIAO ESP32S3",
    "section": "Setup",
    "text": "Setup\n\nSetup XIAO ESP32S3",
    "crumbs": [
      "XIAO ESP32S3"
    ]
  },
  {
    "objectID": "contents/labs/seeed/xiao_esp32s3/xiao_esp32s3.html#exercises",
    "href": "contents/labs/seeed/xiao_esp32s3/xiao_esp32s3.html#exercises",
    "title": "XIAO ESP32S3",
    "section": "Exercises",
    "text": "Exercises\n\n\n\nModality\nTask\nDescription\nLink\n\n\n\n\nVision\nImage Classification\nLearn to classify images\nLink\n\n\nVision\nObject Detection\nImplement object detection\nLink\n\n\nSound\nKeyword Spotting\nExplore voice recognition systems\nLink\n\n\nIMU\nMotion Classification and Anomaly Detection\nClassify motion data and detect anomalies\nLink",
    "crumbs": [
      "XIAO ESP32S3"
    ]
  },
  {
    "objectID": "contents/labs/seeed/xiao_esp32s3/setup/setup.html",
    "href": "contents/labs/seeed/xiao_esp32s3/setup/setup.html",
    "title": "Setup",
    "section": "",
    "text": "Overview\nThe XIAO ESP32S3 Sense is Seeed Studio’s affordable development board, which integrates a camera sensor, digital microphone, and SD card support. Combining embedded ML computing power and photography capability, this development board is a great tool to start with TinyML (intelligent voice and vision AI).\nXIAO ESP32S3 Sense Main Features\nBelow is the general board pinout:",
    "crumbs": [
      "XIAO ESP32S3",
      "Setup"
    ]
  },
  {
    "objectID": "contents/labs/seeed/xiao_esp32s3/setup/setup.html#overview",
    "href": "contents/labs/seeed/xiao_esp32s3/setup/setup.html#overview",
    "title": "Setup",
    "section": "",
    "text": "Powerful MCU Board: Incorporate the ESP32S3 32-bit, dual-core, Xtensa processor chip operating up to 240 MHz, mounted multiple development ports, Arduino / MicroPython supported\nAdvanced Functionality: Detachable OV2640 camera sensor for 1600 * 1200 resolution, compatible with OV5640 camera sensor, integrating an additional digital microphone\nElaborate Power Design: Lithium battery charge management capability offers four power consumption models, which allows for deep sleep mode with power consumption as low as 14μA\nGreat Memory for more Possibilities: Offer 8MB PSRAM and 8MB FLASH, supporting SD card slot for external 32GB FAT memory\nOutstanding RF performance: Support 2.4GHz Wi-Fi and BLE dual wireless communication, support 100m+ remote communication when connected with U.FL antenna\nThumb-sized Compact Design: 21 x 17.5mm, adopting the classic form factor of XIAO, suitable for space-limited projects like wearable devices\n\n\n\n\n\nFor more details, please refer to the Seeed Studio WiKi page:  https://wiki.seeedstudio.com/xiao_esp32s3_getting_started/",
    "crumbs": [
      "XIAO ESP32S3",
      "Setup"
    ]
  },
  {
    "objectID": "contents/labs/seeed/xiao_esp32s3/setup/setup.html#installing-the-xiao-esp32s3-sense-on-arduino-ide",
    "href": "contents/labs/seeed/xiao_esp32s3/setup/setup.html#installing-the-xiao-esp32s3-sense-on-arduino-ide",
    "title": "Setup",
    "section": "Installing the XIAO ESP32S3 Sense on Arduino IDE",
    "text": "Installing the XIAO ESP32S3 Sense on Arduino IDE\nOn Arduino IDE, navigate to File &gt; Preferences, and fill in the URL:\nhttps://raw.githubusercontent.com/espressif/arduino-esp32/gh-pages/package_esp32_dev_index.json\non the field ==&gt; Additional Boards Manager URLs\n\nNext, open boards manager. Go to Tools &gt; Board &gt; Boards Manager… and enter with esp32. Select and install the most updated and stable package (avoid alpha versions) :\n\n\n⚠️ Attention\nAlpha versions (for example, 3.x-alpha) do not work correctly with the XIAO and Edge Impulse. Use the last stable version (for example, 2.0.11) instead.\n\nOn Tools, select the Board (XIAO ESP32S3):\n\nLast but not least, choose the Port where the ESP32S3 is connected.\nThat is it! The device should be OK. Let’s do some tests.",
    "crumbs": [
      "XIAO ESP32S3",
      "Setup"
    ]
  },
  {
    "objectID": "contents/labs/seeed/xiao_esp32s3/setup/setup.html#testing-the-board-with-blink",
    "href": "contents/labs/seeed/xiao_esp32s3/setup/setup.html#testing-the-board-with-blink",
    "title": "Setup",
    "section": "Testing the board with BLINK",
    "text": "Testing the board with BLINK\nThe XIAO ESP32S3 Sense has a built-in LED that is connected to GPIO21. So, you can run the blink sketch as it is (using the LED_BUILTIN Arduino constant) or by changing the Blink sketch accordingly:\n#define LED_BUILT_IN 21 \n\nvoid setup() {\n  pinMode(LED_BUILT_IN, OUTPUT); // Set the pin as output\n}\n\n// Remember that the pin work with inverted logic\n// LOW to Turn on and HIGH to turn off\nvoid loop() {\n  digitalWrite(LED_BUILT_IN, LOW); //Turn on\n  delay (1000); //Wait 1 sec\n  digitalWrite(LED_BUILT_IN, HIGH); //Turn off\n  delay (1000); //Wait 1 sec\n}\n\nNote that the pins work with inverted logic: LOW to Turn on and HIGH to turn off.",
    "crumbs": [
      "XIAO ESP32S3",
      "Setup"
    ]
  },
  {
    "objectID": "contents/labs/seeed/xiao_esp32s3/setup/setup.html#connecting-sense-module-expansion-board",
    "href": "contents/labs/seeed/xiao_esp32s3/setup/setup.html#connecting-sense-module-expansion-board",
    "title": "Setup",
    "section": "Connecting Sense module (Expansion Board)",
    "text": "Connecting Sense module (Expansion Board)\nWhen purchased, the expansion board is separated from the main board, but installing the expansion board is very simple. You need to align the connector on the expansion board with the B2B connector on the XIAO ESP32S3, press it hard, and when you hear a “click,” the installation is complete.\nAs commented in the introduction, the expansion board, or the “sense” part of the device, has a 1600x1200 OV2640 camera, an SD card slot, and a digital microphone.",
    "crumbs": [
      "XIAO ESP32S3",
      "Setup"
    ]
  },
  {
    "objectID": "contents/labs/seeed/xiao_esp32s3/setup/setup.html#microphone-test",
    "href": "contents/labs/seeed/xiao_esp32s3/setup/setup.html#microphone-test",
    "title": "Setup",
    "section": "Microphone Test",
    "text": "Microphone Test\nLet’s start with sound detection. Go to the GitHub project and download the sketch: XIAOEsp2s3_Mic_Test and run it on the Arduino IDE:\n\nWhen producing sound, you can verify it on the Serial Plotter.\nSave recorded sound (.wav audio files) to a microSD card.\nNow, the onboard SD Card reader can save .wav audio files. To do that, we need to habilitate the XIAO PSRAM.\n\nESP32-S3 has only a few hundred kilobytes of internal RAM on the MCU chip. This can be insufficient for some purposes, so up to 16 MB of external PSRAM (pseudo-static RAM) can be connected with the SPI flash chip. The external memory is incorporated in the memory map and, with certain restrictions, is usable in the same way as internal data RAM.\n\nFor a start, Insert the SD Card on the XIAO as shown in the photo below (the SD Card should be formatted to FAT32).\n\n\nDownload the sketch Wav_Record, which you can find on GitHub.\nTo execute the code (Wav Record), it is necessary to use the PSRAM function of the ESP-32 chip, so turn it on before uploading: Tools&gt;PSRAM: “OPI PSRAM”&gt;OPI PSRAM\n\n\n\nRun the code Wav_Record.ino\nThis program is executed only once after the user turns on the serial monitor. It records for 20 seconds and saves the recording file to a microSD card as “arduino_rec.wav.”\nWhen the “.” is output every 1 second in the serial monitor, the program execution is finished, and you can play the recorded sound file with the help of a card reader.\n\n\nThe sound quality is excellent!\n\nThe explanation of how the code works is beyond the scope of this tutorial, but you can find an excellent description on the wiki page.",
    "crumbs": [
      "XIAO ESP32S3",
      "Setup"
    ]
  },
  {
    "objectID": "contents/labs/seeed/xiao_esp32s3/setup/setup.html#testing-the-camera",
    "href": "contents/labs/seeed/xiao_esp32s3/setup/setup.html#testing-the-camera",
    "title": "Setup",
    "section": "Testing the Camera",
    "text": "Testing the Camera\nTo test the camera, you should download the folder take_photos_command from GitHub. The folder contains the sketch (.ino) and two .h files with camera details.\n\nRun the code: take_photos_command.ino. Open the Serial Monitor and send the command capture to capture and save the image on the SD Card:\n\n\nVerify that [Both NL & CR] are selected on Serial Monitor.\n\n\nHere is an example of a taken photo:",
    "crumbs": [
      "XIAO ESP32S3",
      "Setup"
    ]
  },
  {
    "objectID": "contents/labs/seeed/xiao_esp32s3/setup/setup.html#testing-wifi",
    "href": "contents/labs/seeed/xiao_esp32s3/setup/setup.html#testing-wifi",
    "title": "Setup",
    "section": "Testing WiFi",
    "text": "Testing WiFi\nOne of the XIAO ESP32S3’s differentiators is its WiFi capability. So, let’s test its radio by scanning the Wi-Fi networks around it. You can do this by running one of the code examples on the board.\nGo to Arduino IDE Examples and look for WiFI ==&gt; WiFIScan\nYou should see the Wi-Fi networks (SSIDs and RSSIs) within your device’s range on the serial monitor. Here is what I got in the lab:\n\nSimple WiFi Server (Turning LED ON/OFF)\nLet’s test the device’s capability to behave as a WiFi Server. We will host a simple page on the device that sends commands to turn the XIAO built-in LED ON and OFF.\nLike before, go to GitHub to download the folder using the sketch SimpleWiFiServer.\nBefore running the sketch, you should enter your network credentials:\nconst char* ssid     = \"Your credentials here\";\nconst char* password = \"Your credentials here\";\nYou can monitor how your server is working with the Serial Monitor.\n\nTake the IP address and enter it on your browser:\n\nYou will see a page with links that can turn the built-in LED of your XIAO ON and OFF.\nStreaming video to Web\nNow that you know that you can send commands from the webpage to your device, let’s do the reverse. Let’s take the image captured by the camera and stream it to a webpage:\nDownload from GitHub the folder that contains the code: XIAO-ESP32S3-Streeming_Video.ino.\n\nRemember that the folder contains the.ino file and a couple of .h files necessary to handle the camera.\n\nEnter your credentials and run the sketch. On the Serial monitor, you can find the page address to enter in your browser:\n\nOpen the page on your browser (wait a few seconds to start the streaming). That’s it.\n\nStreamlining what your camera is “seen” can be important when you position it to capture a dataset for an ML project (for example, using the code “take_phots_commands.ino”.\nOf course, we can do both things simultaneously: show what the camera sees on the page and send a command to capture and save the image on the SD card. For that, you can use the code Camera_HTTP_Server_STA, which can be downloaded from GitHub.\n\nThe program will do the following tasks:\n\nSet the camera to JPEG output mode.\nCreate a web page (for example ==&gt; http://192.168.4.119//). The correct address will be displayed on the Serial Monitor.\nIf server.on (“/capture”, HTTP_GET, serverCapture), the program takes a photo and sends it to the Web.\nIt is possible to rotate the image on webPage using the button [ROTATE]\nThe command [CAPTURE] only will preview the image on the webpage, showing its size on the Serial Monitor\nThe [SAVE] command will save an image on the SD Card and show the image on the browser.\nSaved images will follow a sequential naming (image1.jpg, image2.jpg.\n\n\n\nThis program can capture an image dataset with an image classification project.\n\nInspect the code; it will be easier to understand how the camera works. This code was developed based on the great Rui Santos Tutorial ESP32-CAM Take Photo and Display in Web Server, which I invite all of you to visit.\nUsing the CameraWebServer\nIn the Arduino IDE, go to File &gt; Examples &gt; ESP32 &gt; Camera, and select CameraWebServer\nYou also should comment on all cameras’ models, except the XIAO model pins:\n#define CAMERA_MODEL_XIAO_ESP32S3 // Has PSRAM\nDo not forget the Tools to enable the PSRAM.\nEnter your wifi credentials and upload the code to the device:\n\nIf the code is executed correctly, you should see the address on the Serial Monitor:\n\nCopy the address on your browser and wait for the page to be uploaded. Select the camera resolution (for example, QVGA) and select [START STREAM]. Wait for a few seconds/minutes, depending on your connection. Using the [Save] button, you can save an image to your computer download area.\n\nThat’s it! You can save the images directly on your computer for use on projects.",
    "crumbs": [
      "XIAO ESP32S3",
      "Setup"
    ]
  },
  {
    "objectID": "contents/labs/seeed/xiao_esp32s3/setup/setup.html#conclusion",
    "href": "contents/labs/seeed/xiao_esp32s3/setup/setup.html#conclusion",
    "title": "Setup",
    "section": "Conclusion",
    "text": "Conclusion\nThe XIAO ESP32S3 Sense is flexible, inexpensive, and easy to program. With 8 MB of RAM, memory is not an issue, and the device can handle many post-processing tasks, including communication.\nYou will find the last version of the codeon the GitHub repository: XIAO-ESP32S3-Sense.",
    "crumbs": [
      "XIAO ESP32S3",
      "Setup"
    ]
  },
  {
    "objectID": "contents/labs/seeed/xiao_esp32s3/setup/setup.html#resources",
    "href": "contents/labs/seeed/xiao_esp32s3/setup/setup.html#resources",
    "title": "Setup",
    "section": "Resources",
    "text": "Resources\n\nXIAO ESP32S3 Code",
    "crumbs": [
      "XIAO ESP32S3",
      "Setup"
    ]
  },
  {
    "objectID": "contents/labs/seeed/xiao_esp32s3/image_classification/image_classification.html",
    "href": "contents/labs/seeed/xiao_esp32s3/image_classification/image_classification.html",
    "title": "Image Classification",
    "section": "",
    "text": "Overview\nMore and more, we are facing an artificial intelligence (AI) revolution where, as stated by Gartner, Edge AI has a very high impact potential, and it is for now!\nAt the forefront of the Emerging Technologies Radar is the universal language of Edge Computer Vision. When we look into Machine Learning (ML) applied to vision, the first concept that greets us is Image Classification, a kind of ML’ Hello World ’ that is both simple and profound!\nThe Seeed Studio XIAO ESP32S3 Sense is a powerful tool that combines camera and SD card support. With its embedded ML computing power and photography capability, it is an excellent starting point for exploring TinyML vision AI.",
    "crumbs": [
      "XIAO ESP32S3",
      "Image Classification"
    ]
  },
  {
    "objectID": "contents/labs/seeed/xiao_esp32s3/image_classification/image_classification.html#a-tinyml-image-classification-project---fruits-versus-veggies",
    "href": "contents/labs/seeed/xiao_esp32s3/image_classification/image_classification.html#a-tinyml-image-classification-project---fruits-versus-veggies",
    "title": "Image Classification",
    "section": "A TinyML Image Classification Project - Fruits versus Veggies",
    "text": "A TinyML Image Classification Project - Fruits versus Veggies\n\nThe whole idea of our project will be to train a model and proceed with inference on the XIAO ESP32S3 Sense. For training, we should find some data (in fact, tons of data!).\nBut first of all, we need a goal! What do we want to classify?\nWith TinyML, a set of techniques associated with machine learning inference on embedded devices, we should limit the classification to three or four categories due to limitations (mainly memory). We will differentiate apples from bananas and potatoes (you can try other categories).\nSo, let’s find a specific dataset that includes images from those categories. Kaggle is a good start:\nhttps://www.kaggle.com/kritikseth/fruit-and-vegetable-image-recognition\nThis dataset contains images of the following food items:\n\nFruits - banana, apple, pear, grapes, orange, kiwi, watermelon, pomegranate, pineapple, mango.\nVegetables - cucumber, carrot, capsicum, onion, potato, lemon, tomato, radish, beetroot, cabbage, lettuce, spinach, soybean, cauliflower, bell pepper, chili pepper, turnip, corn, sweetcorn, sweet potato, paprika, jalepeño, ginger, garlic, peas, eggplant.\n\nEach category is split into the train (100 images), test (10 images), and validation (10 images).\n\nDownload the dataset from the Kaggle website and put it on your computer.\n\n\nOptionally, you can add some fresh photos of bananas, apples, and potatoes from your home kitchen, using, for example, the code discussed in the next setup lab.",
    "crumbs": [
      "XIAO ESP32S3",
      "Image Classification"
    ]
  },
  {
    "objectID": "contents/labs/seeed/xiao_esp32s3/image_classification/image_classification.html#training-the-model-with-edge-impulse-studio",
    "href": "contents/labs/seeed/xiao_esp32s3/image_classification/image_classification.html#training-the-model-with-edge-impulse-studio",
    "title": "Image Classification",
    "section": "Training the model with Edge Impulse Studio",
    "text": "Training the model with Edge Impulse Studio\nWe will use the Edge Impulse Studio to train our model. As you may know, Edge Impulse is a leading development platform for machine learning on edge devices.\nEnter your account credentials (or create a free account) at Edge Impulse. Next, create a new project:\n\n\nData Acquisition\nNext, on the UPLOAD DATA section, upload from your computer the files from chosen categories:\n\nIt would be best if you now had your training dataset split into three classes of data:\n\n\nYou can upload extra data for further model testing or split the training data. I will leave it as it is to use the most data possible.\n\n\n\nImpulse Design\n\nAn impulse takes raw data (in this case, images), extracts features (resize pictures), and then uses a learning block to classify new data.\n\nClassifying images is the most common use of deep learning, but a lot of data should be used to accomplish this task. We have around 90 images for each category. Is this number enough? Not at all! We will need thousands of images to “teach or model” to differentiate an apple from a banana. But, we can solve this issue by re-training a previously trained model with thousands of images. We call this technique “Transfer Learning” (TL).\n\nWith TL, we can fine-tune a pre-trained image classification model on our data, performing well even with relatively small image datasets (our case).\nSo, starting from the raw images, we will resize them (96x96) pixels and feed them to our Transfer Learning block:\n\n\nPre-processing (Feature Generation)\nBesides resizing the images, we can change them to Grayscale or keep the actual RGB color depth. Let’s start selecting Grayscale. Doing that, each one of our data samples will have dimension 9,216 features (96x96x1). Keeping RGB, this dimension would be three times bigger. Working with Grayscale helps to reduce the amount of final memory needed for inference.\n\nRemember to [Save parameters]. This will generate the features to be used in training.\n\n\nModel Design\nTransfer Learning\nIn 2007, Google introduced MobileNetV1, a family of general-purpose computer vision neural networks designed with mobile devices in mind to support classification, detection, and more. MobileNets are small, low-latency, low-power models parameterized to meet the resource constraints of various use cases.\nAlthough the base MobileNet architecture is already tiny and has low latency, many times, a specific use case or application may require the model to be smaller and faster. MobileNet introduces a straightforward parameter α (alpha) called width multiplier to construct these smaller, less computationally expensive models. The role of the width multiplier α is to thin a network uniformly at each layer.\nEdge Impulse Studio has MobileNet V1 (96x96 images) and V2 (96x96 and 160x160 images) available, with several different α values (from 0.05 to 1.0). For example, you will get the highest accuracy with V2, 160x160 images, and α=1.0. Of course, there is a trade-off. The higher the accuracy, the more memory (around 1.3M RAM and 2.6M ROM) will be needed to run the model, implying more latency.\nThe smaller footprint will be obtained at another extreme with MobileNet V1 and α=0.10 (around 53.2K RAM and 101K ROM).\nFor this first pass, we will use MobileNet V1 and α=0.10.\n\n\n\nTraining\nData Augmentation\nAnother necessary technique to use with deep learning is data augmentation. Data augmentation is a method that can help improve the accuracy of machine learning models, creating additional artificial data. A data augmentation system makes small, random changes to your training data during the training process (such as flipping, cropping, or rotating the images).\nUnder the rood, here you can see how Edge Impulse implements a data Augmentation policy on your data:\n# Implements the data augmentation policy\ndef augment_image(image, label):\n    # Flips the image randomly\n    image = tf.image.random_flip_left_right(image)\n\n    # Increase the image size, then randomly crop it down to\n    # the original dimensions\n    resize_factor = random.uniform(1, 1.2)\n    new_height = math.floor(resize_factor * INPUT_SHAPE[0])\n    new_width = math.floor(resize_factor * INPUT_SHAPE[1])\n    image = tf.image.resize_with_crop_or_pad(image, new_height, new_width)\n    image = tf.image.random_crop(image, size=INPUT_SHAPE)\n\n    # Vary the brightness of the image\n    image = tf.image.random_brightness(image, max_delta=0.2)\n\n    return image, label\nExposure to these variations during training can help prevent your model from taking shortcuts by “memorizing” superficial clues in your training data, meaning it may better reflect the deep underlying patterns in your dataset.\nThe final layer of our model will have 16 neurons with a 10% dropout for overfitting prevention. Here is the Training output:\n\nThe result could be better. The model reached around 77% accuracy, but the amount of RAM expected to be used during the inference is relatively tiny (about 60 KBytes), which is very good.\n\n\nDeployment\nThe trained model will be deployed as a .zip Arduino library:\n\nOpen your Arduino IDE, and under Sketch, go to Include Library and add.ZIP Library. Please select the file you download from Edge Impulse Studio, and that’s it!\n\nUnder the Examples tab on Arduino IDE, you should find a sketch code under your project name.\n\nOpen the Static Buffer example:\n\nYou can see that the first line of code is exactly the calling of a library with all the necessary stuff for running inference on your device.\n#include &lt;XIAO-ESP32S3-CAM-Fruits-vs-Veggies_inferencing.h&gt;\nOf course, this is a generic code (a “template”) that only gets one sample of raw data (stored on the variable: features = {} and runs the classifier, doing the inference. The result is shown on the Serial Monitor.\nWe should get the sample (image) from the camera and pre-process it (resizing to 96x96, converting to grayscale, and flatting it). This will be the input tensor of our model. The output tensor will be a vector with three values (labels), showing the probabilities of each one of the classes.\n\nReturning to your project (Tab Image), copy one of the Raw Data Sample:\n\n9,216 features will be copied to the clipboard. This is the input tensor (a flattened image of 96x96x1), in this case, bananas. Past this Input tensor onfeatures[] = {0xb2d77b, 0xb5d687, 0xd8e8c0, 0xeaecba, 0xc2cf67, ...}\n\nEdge Impulse included the library ESP NN in its SDK, which contains optimized NN (Neural Network) functions for various Espressif chips, including the ESP32S3 (running at Arduino IDE).\nWhen running the inference, you should get the highest score for “banana.”\n\nGreat news! Our device handles an inference, discovering that the input image is a banana. Also, note that the inference time was around 317ms, resulting in a maximum of 3 fps if you tried to classify images from a video.\nNow, we should incorporate the camera and classify images in real time.\nGo to the Arduino IDE Examples and download from your project the sketch esp32_camera:\n\nYou should change lines 32 to 75, which define the camera model and pins, using the data related to our model. Copy and paste the below lines, replacing the lines 32-75:\n#define PWDN_GPIO_NUM     -1 \n#define RESET_GPIO_NUM    -1 \n#define XCLK_GPIO_NUM     10 \n#define SIOD_GPIO_NUM     40 \n#define SIOC_GPIO_NUM     39\n#define Y9_GPIO_NUM       48 \n#define Y8_GPIO_NUM       11 \n#define Y7_GPIO_NUM       12 \n#define Y6_GPIO_NUM       14 \n#define Y5_GPIO_NUM       16 \n#define Y4_GPIO_NUM       18 \n#define Y3_GPIO_NUM       17 \n#define Y2_GPIO_NUM       15 \n#define VSYNC_GPIO_NUM    38 \n#define HREF_GPIO_NUM     47 \n#define PCLK_GPIO_NUM     13\nHere you can see the resulting code:\n\nThe modified sketch can be downloaded from GitHub: xiao_esp32s3_camera.\n\nNote that you can optionally keep the pins as a .h file as we did in the Setup Lab.\n\nUpload the code to your XIAO ESP32S3 Sense, and you should be OK to start classifying your fruits and vegetables! You can check the result on Serial Monitor.",
    "crumbs": [
      "XIAO ESP32S3",
      "Image Classification"
    ]
  },
  {
    "objectID": "contents/labs/seeed/xiao_esp32s3/image_classification/image_classification.html#testing-the-model-inference",
    "href": "contents/labs/seeed/xiao_esp32s3/image_classification/image_classification.html#testing-the-model-inference",
    "title": "Image Classification",
    "section": "Testing the Model (Inference)",
    "text": "Testing the Model (Inference)\n\nGetting a photo with the camera, the classification result will appear on the Serial Monitor:\n\nOther tests:",
    "crumbs": [
      "XIAO ESP32S3",
      "Image Classification"
    ]
  },
  {
    "objectID": "contents/labs/seeed/xiao_esp32s3/image_classification/image_classification.html#testing-with-a-bigger-model",
    "href": "contents/labs/seeed/xiao_esp32s3/image_classification/image_classification.html#testing-with-a-bigger-model",
    "title": "Image Classification",
    "section": "Testing with a Bigger Model",
    "text": "Testing with a Bigger Model\nNow, let’s go to the other side of the model size. Let’s select a MobilinetV2 96x96 0.35, having as input RGB images.\n\nEven with a bigger model, the accuracy could be better, and the amount of memory necessary to run the model increases five times, with latency increasing seven times.\n\nNote that the performance here is estimated with a smaller device, the ESP-EYE. The actual inference with the ESP32S3 should be better.\n\nTo improve our model, we will need to train more images.\nEven though our model did not improve accuracy, let’s test whether the XIAO can handle such a bigger model. We will do a simple inference test with the Static Buffer sketch.\nLet’s redeploy the model. If the EON Compiler is enabled when you generate the library, the total memory needed for inference should be reduced, but it does not influence accuracy.\n\n⚠️ Attention - The Xiao ESP32S3 with PSRAM enable has enought memory to run the inference, even in such bigger model. Keep the EON Compiler NOT ENABLED.\n\n\nDoing an inference with MobilinetV2 96x96 0.35, having as input RGB images, the latency was 219ms, which is great for such a bigger model.\n\nFor the test, we can train the model again, using the smallest version of MobileNet V2, with an alpha of 0.05. Interesting that the result in accuracy was higher.\n\n\nNote that the estimated latency for an Arduino Portenta (or Nicla), running with a clock of 480MHz is 45ms.\n\nDeploying the model, we got an inference of only 135ms, remembering that the XIAO runs with half of the clock used by the Portenta/Nicla (240MHz):",
    "crumbs": [
      "XIAO ESP32S3",
      "Image Classification"
    ]
  },
  {
    "objectID": "contents/labs/seeed/xiao_esp32s3/image_classification/image_classification.html#running-inference-on-the-sensecraft-web-toolkit",
    "href": "contents/labs/seeed/xiao_esp32s3/image_classification/image_classification.html#running-inference-on-the-sensecraft-web-toolkit",
    "title": "Image Classification",
    "section": "Running inference on the SenseCraft-Web-Toolkit",
    "text": "Running inference on the SenseCraft-Web-Toolkit\nOne significant limitation of viewing inference on Arduino IDE is that we can not see what the camera focuses on. A good alternative is the SenseCraft-Web-Toolkit, a visual model deployment tool provided by SSCMA(Seeed SenseCraft Model Assistant). This tool allows you to deploy models to various platforms easily through simple operations. The tool offers a user-friendly interface and does not require any coding.\nFollow the following steps to start the SenseCraft-Web-Toolkit:\n\nOpen the SenseCraft-Web-Toolkit website.\nConnect the XIAO to your computer:\n\n\nHaving the XIAO connected, select it as below:\n\n\n\nSelect the device/Port and press [Connect]:\n\n\n\nYou can try several Computer Vision models previously uploaded by Seeed Studio. Try them and have fun!\n\nIn our case, we will use the blue button at the bottom of the page: [Upload Custom AI Model].\nBut first, we must download from Edge Impulse Studio our quantized.tflite model.\n\nGo to your project at Edge Impulse Studio, or clone this one:\n\n\nXIAO-ESP32S3-CAM-Fruits-vs-Veggies-v1-ESP-NN\n\n\nOn the Dashboard, download the model (“block output”): Transfer learning model - TensorFlow Lite (int8 quantized).\n\n\n\nOn SenseCraft-Web-Toolkit, use the blue button at the bottom of the page: [Upload Custom AI Model]. A window will pop up. Enter the Model file that you downloaded to your computer from Edge Impulse Studio, choose a Model Name, and enter with labels (ID: Object):\n\n\n\nNote that you should use the labels trained on EI Studio, entering them in alphabetic order (in our case: apple, banana, potato).\n\nAfter a few seconds (or minutes), the model will be uploaded to your device, and the camera image will appear in real-time on the Preview Sector:\n\nThe Classification result will be at the top of the image. You can also select the Confidence of your inference cursor Confidence.\nClicking on the top button (Device Log), you can open a Serial Monitor to follow the inference, the same that we have done with the Arduino IDE:\n\nOn Device Log, you will get information as:\n\n\nPreprocess time (image capture and Crop): 4ms,\nInference time (model latency): 106ms,\nPostprocess time (display of the image and inclusion of data): 0ms,\nOutput tensor (classes), for example: [[89,0]]; where 0 is Apple (and 1is banana and 2 is potato).\n\nHere are other screenshots:",
    "crumbs": [
      "XIAO ESP32S3",
      "Image Classification"
    ]
  },
  {
    "objectID": "contents/labs/seeed/xiao_esp32s3/image_classification/image_classification.html#conclusion",
    "href": "contents/labs/seeed/xiao_esp32s3/image_classification/image_classification.html#conclusion",
    "title": "Image Classification",
    "section": "Conclusion",
    "text": "Conclusion\nThe XIAO ESP32S3 Sense is very flexible, inexpensive, and easy to program. The project proves the potential of TinyML. Memory is not an issue; the device can handle many post-processing tasks, including communication.\nYou will find the last version of the codeon the GitHub repository: XIAO-ESP32S3-Sense.",
    "crumbs": [
      "XIAO ESP32S3",
      "Image Classification"
    ]
  },
  {
    "objectID": "contents/labs/seeed/xiao_esp32s3/image_classification/image_classification.html#resources",
    "href": "contents/labs/seeed/xiao_esp32s3/image_classification/image_classification.html#resources",
    "title": "Image Classification",
    "section": "Resources",
    "text": "Resources\n\nXIAO ESP32S3 Codes\nDataset\nEdge Impulse Project",
    "crumbs": [
      "XIAO ESP32S3",
      "Image Classification"
    ]
  },
  {
    "objectID": "contents/labs/seeed/xiao_esp32s3/object_detection/object_detection.html",
    "href": "contents/labs/seeed/xiao_esp32s3/object_detection/object_detection.html",
    "title": "Object Detection",
    "section": "",
    "text": "Overview\nIn the last section regarding Computer Vision (CV) and the XIAO ESP32S3, Image Classification, we learned how to set up and classify images with this remarkable development board. Continuing our CV journey, we will explore Object Detection on microcontrollers.",
    "crumbs": [
      "XIAO ESP32S3",
      "Object Detection"
    ]
  },
  {
    "objectID": "contents/labs/seeed/xiao_esp32s3/object_detection/object_detection.html#overview",
    "href": "contents/labs/seeed/xiao_esp32s3/object_detection/object_detection.html#overview",
    "title": "Object Detection",
    "section": "",
    "text": "Object Detection versus Image Classification\nThe main task with Image Classification models is to identify the most probable object category present on an image, for example, to classify between a cat or a dog, dominant “objects” in an image:\n\nBut what happens if there is no dominant category in the image?\n\nAn image classification model identifies the above image utterly wrong as an “ashcan,” possibly due to the color tonalities.\n\nThe model used in the previous images is MobileNet, which is trained with a large dataset, ImageNet, running on a Raspberry Pi.\n\nTo solve this issue, we need another type of model, where not only multiple categories (or labels) can be found but also where the objects are located on a given image.\nAs we can imagine, such models are much more complicated and bigger, for example, the MobileNetV2 SSD FPN-Lite 320x320, trained with the COCO dataset. This pre-trained object detection model is designed to locate up to 10 objects within an image, outputting a bounding box for each object detected. The below image is the result of such a model running on a Raspberry Pi:\n\nThose models used for object detection (such as the MobileNet SSD or YOLO) usually have several MB in size, which is OK for use with Raspberry Pi but unsuitable for use with embedded devices, where the RAM usually has, at most, a few MB as in the case of the XIAO ESP32S3.\n\n\nAn Innovative Solution for Object Detection: FOMO\nEdge Impulse launched in 2022, FOMO (Faster Objects, More Objects), a novel solution to perform object detection on embedded devices, such as the Nicla Vision and Portenta (Cortex M7), on Cortex M4F CPUs (Arduino Nano33 and OpenMV M4 series) as well the Espressif ESP32 devices (ESP-CAM, ESP-EYE and XIAO ESP32S3 Sense).\nIn this Hands-On project, we will explore Object Detection using FOMO.\n\nTo understand more about FOMO, you can go into the official FOMO announcement by Edge Impulse, where Louis Moreau and Mat Kelcey explain in detail how it works.",
    "crumbs": [
      "XIAO ESP32S3",
      "Object Detection"
    ]
  },
  {
    "objectID": "contents/labs/seeed/xiao_esp32s3/object_detection/object_detection.html#the-object-detection-project-goal",
    "href": "contents/labs/seeed/xiao_esp32s3/object_detection/object_detection.html#the-object-detection-project-goal",
    "title": "Object Detection",
    "section": "The Object Detection Project Goal",
    "text": "The Object Detection Project Goal\nAll Machine Learning projects need to start with a detailed goal. Let’s assume we are in an industrial or rural facility and must sort and count oranges (fruits) and particular frogs (bugs).\n\nIn other words, we should perform a multi-label classification, where each image can have three classes:\n\nBackground (No objects)\nFruit\nBug\n\nHere are some not labeled image samples that we should use to detect the objects (fruits and bugs):\n\nWe are interested in which object is in the image, its location (centroid), and how many we can find on it. The object’s size is not detected with FOMO, as with MobileNet SSD or YOLO, where the Bounding Box is one of the model outputs.\nWe will develop the project using the XIAO ESP32S3 for image capture and model inference. The ML project will be developed using the Edge Impulse Studio. But before starting the object detection project in the Studio, let’s create a raw dataset (not labeled) with images that contain the objects to be detected.",
    "crumbs": [
      "XIAO ESP32S3",
      "Object Detection"
    ]
  },
  {
    "objectID": "contents/labs/seeed/xiao_esp32s3/object_detection/object_detection.html#data-collection",
    "href": "contents/labs/seeed/xiao_esp32s3/object_detection/object_detection.html#data-collection",
    "title": "Object Detection",
    "section": "Data Collection",
    "text": "Data Collection\nYou can capture images using the XIAO, your phone, or other devices. Here, we will use the XIAO with code from the Arduino IDE ESP32 library.\n\nCollecting Dataset with the XIAO ESP32S3\nOpen the Arduino IDE and select the XIAO_ESP32S3 board (and the port where it is connected). On File &gt; Examples &gt; ESP32 &gt; Camera, select CameraWebServer.\nOn the BOARDS MANAGER panel, confirm that you have installed the latest “stable” package.\n\n⚠️ Attention\nAlpha versions (for example, 3.x-alpha) do not work correctly with the XIAO and Edge Impulse. Use the last stable version (for example, 2.0.11) instead.\n\nYou also should comment on all cameras’ models, except the XIAO model pins:\n#define CAMERA_MODEL_XIAO_ESP32S3 // Has PSRAM\nAnd on Tools, enable the PSRAM. Enter your wifi credentials and upload the code to the device:\n\nIf the code is executed correctly, you should see the address on the Serial Monitor:\n\nCopy the address on your browser and wait for the page to be uploaded. Select the camera resolution (for example, QVGA) and select [START STREAM]. Wait for a few seconds/minutes, depending on your connection. You can save an image on your computer download area using the [Save] button.\n\nEdge impulse suggests that the objects should be similar in size and not overlapping for better performance. This is OK in an industrial facility, where the camera should be fixed, keeping the same distance from the objects to be detected. Despite that, we will also try using mixed sizes and positions to see the result.\n\nWe do not need to create separate folders for our images because each contains multiple labels.\n\nWe suggest using around 50 images to mix the objects and vary the number of each appearing on the scene. Try to capture different angles, backgrounds, and light conditions.\n\nThe stored images use a QVGA frame size of 320x240 and RGB565 (color pixel format).\n\nAfter capturing your dataset, [Stop Stream] and move your images to a folder.",
    "crumbs": [
      "XIAO ESP32S3",
      "Object Detection"
    ]
  },
  {
    "objectID": "contents/labs/seeed/xiao_esp32s3/object_detection/object_detection.html#edge-impulse-studio",
    "href": "contents/labs/seeed/xiao_esp32s3/object_detection/object_detection.html#edge-impulse-studio",
    "title": "Object Detection",
    "section": "Edge Impulse Studio",
    "text": "Edge Impulse Studio\n\nSetup the project\nGo to Edge Impulse Studio, enter your credentials at Login (or create an account), and start a new project.\n\n\nHere, you can clone the project developed for this hands-on: XIAO-ESP32S3-Sense-Object_Detection\n\nOn your Project Dashboard, go down and on Project info and select Bounding boxes (object detection) and Espressif ESP-EYE (most similar to our board) as your Target Device:\n\n\n\nUploading the unlabeled data\nOn Studio, go to the Data acquisition tab, and on the UPLOAD DATA section, upload files captured as a folder from your computer.\n\n\nYou can leave for the Studio to split your data automatically between Train and Test or do it manually. We will upload all of them as training.\n\n\nAll the not-labeled images (47) were uploaded but must be labeled appropriately before being used as a project dataset. The Studio has a tool for that purpose, which you can find in the link Labeling queue (47).\nThere are two ways you can use to perform AI-assisted labeling on the Edge Impulse Studio (free version):\n\nUsing yolov5\nTracking objects between frames\n\n\nEdge Impulse launched an auto-labeling feature for Enterprise customers, easing labeling tasks in object detection projects.\n\nOrdinary objects can quickly be identified and labeled using an existing library of pre-trained object detection models from YOLOv5 (trained with the COCO dataset). But since, in our case, the objects are not part of COCO datasets, we should select the option of tracking objects. With this option, once you draw bounding boxes and label the images in one frame, the objects will be tracked automatically from frame to frame, partially labeling the new ones (not all are correctly labeled).\n\nYou can use the EI uploader to import your data if you already have a labeled dataset containing bounding boxes.\n\n\n\nLabeling the Dataset\nStarting with the first image of your unlabeled data, use your mouse to drag a box around an object to add a label. Then click Save labels to advance to the next item.\n\nContinue with this process until the queue is empty. At the end, all images should have the objects labeled as those samples below:\n\nNext, review the labeled samples on the Data acquisition tab. If one of the labels is wrong, you can edit it using the three dots menu after the sample name:\n\nYou will be guided to replace the wrong label and correct the dataset.\n\n\n\nBalancing the dataset and split Train/Test\nAfter labeling all data, it was realized that the class fruit had many more samples than the bug. So, 11 new and additional bug images were collected (ending with 58 images). After labeling them, it is time to select some images and move them to the test dataset. You can do it using the three-dot menu after the image name. I selected six images, representing 13% of the total dataset.",
    "crumbs": [
      "XIAO ESP32S3",
      "Object Detection"
    ]
  },
  {
    "objectID": "contents/labs/seeed/xiao_esp32s3/object_detection/object_detection.html#the-impulse-design",
    "href": "contents/labs/seeed/xiao_esp32s3/object_detection/object_detection.html#the-impulse-design",
    "title": "Object Detection",
    "section": "The Impulse Design",
    "text": "The Impulse Design\nIn this phase, you should define how to:\n\nPre-processing consists of resizing the individual images from 320 x 240 to 96 x 96 and squashing them (squared form, without cropping). Afterward, the images are converted from RGB to Grayscale.\nDesign a Model, in this case, “Object Detection.”\n\n\n\nPreprocessing all dataset\nIn this section, select Color depth as Grayscale, suitable for use with FOMO models and Save parameters.\n\nThe Studio moves automatically to the next section, Generate features, where all samples will be pre-processed, resulting in a dataset with individual 96x96x1 images or 9,216 features.\n\nThe feature explorer shows that all samples evidence a good separation after the feature generation.\n\nSome samples seem to be in the wrong space, but clicking on them confirms the correct labeling.",
    "crumbs": [
      "XIAO ESP32S3",
      "Object Detection"
    ]
  },
  {
    "objectID": "contents/labs/seeed/xiao_esp32s3/object_detection/object_detection.html#model-design-training-and-test",
    "href": "contents/labs/seeed/xiao_esp32s3/object_detection/object_detection.html#model-design-training-and-test",
    "title": "Object Detection",
    "section": "Model Design, Training, and Test",
    "text": "Model Design, Training, and Test\nWe will use FOMO, an object detection model based on MobileNetV2 (alpha 0.35) designed to coarsely segment an image into a grid of background vs objects of interest (here, boxes and wheels).\nFOMO is an innovative machine learning model for object detection, which can use up to 30 times less energy and memory than traditional models like Mobilenet SSD and YOLOv5. FOMO can operate on microcontrollers with less than 200 KB of RAM. The main reason this is possible is that while other models calculate the object’s size by drawing a square around it (bounding box), FOMO ignores the size of the image, providing only the information about where the object is located in the image through its centroid coordinates.\nHow FOMO works?\nFOMO takes the image in grayscale and divides it into blocks of pixels using a factor of 8. For the input of 96x96, the grid would be 12x12 (96/8=12). Next, FOMO will run a classifier through each pixel block to calculate the probability that there is a box or a wheel in each of them and, subsequently, determine the regions that have the highest probability of containing the object (If a pixel block has no objects, it will be classified as background). From the overlap of the final region, the FOMO provides the coordinates (related to the image dimensions) of the centroid of this region.\n\nFor training, we should select a pre-trained model. Let’s use the FOMO (Faster Objects, More Objects) MobileNetV2 0.35. This model uses around 250KB of RAM and 80KB of ROM (Flash), which suits well with our board.\n\nRegarding the training hyper-parameters, the model will be trained with:\n\nEpochs: 60\nBatch size: 32\nLearning Rate: 0.001.\n\nFor validation during training, 20% of the dataset (validation_dataset) will be spared. For the remaining 80% (train_dataset), we will apply Data Augmentation, which will randomly flip, change the size and brightness of the image, and crop them, artificially increasing the number of samples on the dataset for training.\nAs a result, the model ends with an overall F1 score of 85%, similar to the result when using the test data (83%).\n\nNote that FOMO automatically added a 3rd label background to the two previously defined (box and wheel).\n\n\n\nIn object detection tasks, accuracy is generally not the primary evaluation metric. Object detection involves classifying objects and providing bounding boxes around them, making it a more complex problem than simple classification. The issue is that we do not have the bounding box, only the centroids. In short, using accuracy as a metric could be misleading and may not provide a complete understanding of how well the model is performing. Because of that, we will use the F1 score.\n\n\nTest model with “Live Classification”\nOnce our model is trained, we can test it using the Live Classification tool. On the correspondent section, click on Connect a development board icon (a small MCU) and scan the QR code with your phone.\n\nOnce connected, you can use the smartphone to capture actual images to be tested by the trained model on Edge Impulse Studio.\n\nOne thing to be noted is that the model can produce false positives and negatives. This can be minimized by defining a proper Confidence Threshold (use the Three dots menu for the setup). Try with 0.8 or more.",
    "crumbs": [
      "XIAO ESP32S3",
      "Object Detection"
    ]
  },
  {
    "objectID": "contents/labs/seeed/xiao_esp32s3/object_detection/object_detection.html#deploying-the-model-arduino-ide",
    "href": "contents/labs/seeed/xiao_esp32s3/object_detection/object_detection.html#deploying-the-model-arduino-ide",
    "title": "Object Detection",
    "section": "Deploying the Model (Arduino IDE)",
    "text": "Deploying the Model (Arduino IDE)\nSelect the Arduino Library and Quantized (int8) model, enable the EON Compiler on the Deploy Tab, and press [Build].\n\nOpen your Arduino IDE, and under Sketch, go to Include Library and add.ZIP Library. Select the file you download from Edge Impulse Studio, and that’s it!\n\nUnder the Examples tab on Arduino IDE, you should find a sketch code (esp32 &gt; esp32_camera) under your project name.\n\nYou should change lines 32 to 75, which define the camera model and pins, using the data related to our model. Copy and paste the below lines, replacing the lines 32-75:\n#define PWDN_GPIO_NUM     -1 \n#define RESET_GPIO_NUM    -1 \n#define XCLK_GPIO_NUM     10 \n#define SIOD_GPIO_NUM     40 \n#define SIOC_GPIO_NUM     39\n#define Y9_GPIO_NUM       48 \n#define Y8_GPIO_NUM       11 \n#define Y7_GPIO_NUM       12 \n#define Y6_GPIO_NUM       14 \n#define Y5_GPIO_NUM       16 \n#define Y4_GPIO_NUM       18 \n#define Y3_GPIO_NUM       17 \n#define Y2_GPIO_NUM       15 \n#define VSYNC_GPIO_NUM    38 \n#define HREF_GPIO_NUM     47 \n#define PCLK_GPIO_NUM     13\nHere you can see the resulting code:\n\nUpload the code to your XIAO ESP32S3 Sense, and you should be OK to start detecting fruits and bugs. You can check the result on Serial Monitor.\nBackground\n\nFruits\n\nBugs\n\nNote that the model latency is 143ms, and the frame rate per second is around 7 fps (similar to what we got with the Image Classification project). This happens because FOMO is cleverly built over a CNN model, not with an object detection model like the SSD MobileNet. For example, when running a MobileNetV2 SSD FPN-Lite 320x320 model on a Raspberry Pi 4, the latency is around five times higher (around 1.5 fps).",
    "crumbs": [
      "XIAO ESP32S3",
      "Object Detection"
    ]
  },
  {
    "objectID": "contents/labs/seeed/xiao_esp32s3/object_detection/object_detection.html#deploying-the-model-sensecraft-web-toolkit",
    "href": "contents/labs/seeed/xiao_esp32s3/object_detection/object_detection.html#deploying-the-model-sensecraft-web-toolkit",
    "title": "Object Detection",
    "section": "Deploying the Model (SenseCraft-Web-Toolkit)",
    "text": "Deploying the Model (SenseCraft-Web-Toolkit)\nAs discussed in the Image Classification chapter, verifying inference with Image models on Arduino IDE is very challenging because we can not see what the camera focuses on. Again, let’s use the SenseCraft-Web Toolkit.\nFollow the following steps to start the SenseCraft-Web-Toolkit:\n\nOpen the SenseCraft-Web-Toolkit website.\nConnect the XIAO to your computer:\n\n\nHaving the XIAO connected, select it as below:\n\n\n\nSelect the device/Port and press [Connect]:\n\n\n\nYou can try several Computer Vision models previously uploaded by Seeed Studio. Try them and have fun!\n\nIn our case, we will use the blue button at the bottom of the page: [Upload Custom AI Model].\nBut first, we must download from Edge Impulse Studio our quantized .tflite model.\n\nGo to your project at Edge Impulse Studio, or clone this one:\n\n\nXIAO-ESP32S3-CAM-Fruits-vs-Veggies-v1-ESP-NN\n\n\nOn Dashboard, download the model (“block output”): Object Detection model - TensorFlow Lite (int8 quantized)\n\n\n\nOn SenseCraft-Web-Toolkit, use the blue button at the bottom of the page: [Upload Custom AI Model]. A window will pop up. Enter the Model file that you downloaded to your computer from Edge Impulse Studio, choose a Model Name, and enter with labels (ID: Object):\n\n\n\nNote that you should use the labels trained on EI Studio and enter them in alphabetic order (in our case, background, bug, fruit).\n\nAfter a few seconds (or minutes), the model will be uploaded to your device, and the camera image will appear in real-time on the Preview Sector:\n\nThe detected objects will be marked (the centroid). You can select the Confidence of your inference cursor Confidence and IoU, which is used to assess the accuracy of predicted bounding boxes compared to truth bounding boxes.\nClicking on the top button (Device Log), you can open a Serial Monitor to follow the inference, as we did with the Arduino IDE.\n\nOn Device Log, you will get information as:\n\nPreprocess time (image capture and Crop): 3 ms,\nInference time (model latency): 115 ms,\nPostprocess time (display of the image and marking objects): 1 ms.\nOutput tensor (boxes), for example, one of the boxes: [[30,150, 20, 20,97, 2]]; where 30,150, 20, 20 are the coordinates of the box (around the centroid); 97 is the inference result, and 2 is the class (in this case 2: fruit).\n\n\nNote that in the above example, we got 5 boxes because none of the fruits got 3 centroids. One solution will be post-processing, where we can aggregate close centroids in one.\n\nHere are other screenshots:",
    "crumbs": [
      "XIAO ESP32S3",
      "Object Detection"
    ]
  },
  {
    "objectID": "contents/labs/seeed/xiao_esp32s3/object_detection/object_detection.html#conclusion",
    "href": "contents/labs/seeed/xiao_esp32s3/object_detection/object_detection.html#conclusion",
    "title": "Object Detection",
    "section": "Conclusion",
    "text": "Conclusion\nFOMO is a significant leap in the image processing space, as Louis Moreau and Mat Kelcey put it during its launch in 2022:\n\nFOMO is a ground-breaking algorithm that brings real-time object detection, tracking, and counting to microcontrollers for the first time.\n\nMultiple possibilities exist for exploring object detection (and, more precisely, counting them) on embedded devices.",
    "crumbs": [
      "XIAO ESP32S3",
      "Object Detection"
    ]
  },
  {
    "objectID": "contents/labs/seeed/xiao_esp32s3/object_detection/object_detection.html#resources",
    "href": "contents/labs/seeed/xiao_esp32s3/object_detection/object_detection.html#resources",
    "title": "Object Detection",
    "section": "Resources",
    "text": "Resources\n\nEdge Impulse Project",
    "crumbs": [
      "XIAO ESP32S3",
      "Object Detection"
    ]
  },
  {
    "objectID": "contents/labs/seeed/xiao_esp32s3/kws/kws.html",
    "href": "contents/labs/seeed/xiao_esp32s3/kws/kws.html",
    "title": "Keyword Spotting (KWS)",
    "section": "",
    "text": "Overview\nKeyword Spotting (KWS) is integral to many voice recognition systems, enabling devices to respond to specific words or phrases. While this technology underpins popular devices like Google Assistant or Amazon Alexa, it’s equally applicable and achievable on smaller, low-power devices. This lab will guide you through implementing a KWS system using TinyML on the XIAO ESP32S3 microcontroller board.\nThe XIAO ESP32S3, equipped with Espressif’s ESP32-S3 chip, is a compact and potent microcontroller offering a dual-core Xtensa LX7 processor, integrated Wi-Fi, and Bluetooth. Its balance of computational power, energy efficiency, and versatile connectivity make it a fantastic platform for TinyML applications. Also, with its expansion board, we will have access to the “sense” part of the device, which has a 1600x1200 OV2640 camera, an SD card slot, and a digital microphone. The integrated microphone and the SD card will be essential in this project.\nWe will use the Edge Impulse Studio, a powerful, user-friendly platform that simplifies creating and deploying machine learning models onto edge devices. We’ll train a KWS model step-by-step, optimizing and deploying it onto the XIAO ESP32S3 Sense.\nOur model will be designed to recognize keywords that can trigger device wake-up or specific actions (in the case of “YES”), bringing your projects to life with voice-activated commands.\nLeveraging our experience with TensorFlow Lite for Microcontrollers (the engine “under the hood” on the EI Studio), we’ll create a KWS system capable of real-time machine learning on the device.\nAs we progress through the lab, we’ll break down each process stage - from data collection and preparation to model training and deployment - to provide a comprehensive understanding of implementing a KWS system on a microcontroller.",
    "crumbs": [
      "XIAO ESP32S3",
      "Keyword Spotting (KWS)"
    ]
  },
  {
    "objectID": "contents/labs/seeed/xiao_esp32s3/kws/kws.html#overview",
    "href": "contents/labs/seeed/xiao_esp32s3/kws/kws.html#overview",
    "title": "Keyword Spotting (KWS)",
    "section": "",
    "text": "How does a voice assistant work?\nKeyword Spotting (KWS) is critical to many voice assistants, enabling devices to respond to specific words or phrases. To start, it is essential to realize that Voice Assistants on the market, like Google Home or Amazon Echo-Dot, only react to humans when they are “waked up” by particular keywords such as “ Hey Google” on the first one and “Alexa” on the second.\n\nIn other words, recognizing voice commands is based on a multi-stage model or Cascade Detection.\n\nStage 1: A smaller microprocessor inside the Echo Dot or Google Home continuously listens to the sound, waiting for the keyword to be spotted. For such detection, a TinyML model at the edge is used (KWS application).\nStage 2: Only when triggered by the KWS application on Stage 1 is the data sent to the cloud and processed on a larger model.\nThe video below shows an example where I emulate a Google Assistant on a Raspberry Pi (Stage 2), having an Arduino Nano 33 BLE as the tinyML device (Stage 1).\n\n\n\nIf you want to go deeper on the full project, please see my tutorial: Building an Intelligent Voice Assistant From Scratch.\n\nIn this lab, we will focus on Stage 1 (KWS or Keyword Spotting), where we will use the XIAO ESP2S3 Sense, which has a digital microphone for spotting the keyword.\n\n\nThe KWS Project\nThe below diagram will give an idea of how the final KWS application should work (during inference):\n\nOur KWS application will recognize four classes of sound:\n\nYES (Keyword 1)\nNO (Keyword 2)\nNOISE (no keywords spoken, only background noise is present)\nUNKNOW (a mix of different words than YES and NO)\n\n\nOptionally for real-world projects, it is always advised to include different words than keywords, such as “Noise” (or Background) and “Unknow.”\n\n\n\nThe Machine Learning workflow\nThe main component of the KWS application is its model. So, we must train such a model with our specific keywords, noise, and other words (the “unknown”):",
    "crumbs": [
      "XIAO ESP32S3",
      "Keyword Spotting (KWS)"
    ]
  },
  {
    "objectID": "contents/labs/seeed/xiao_esp32s3/kws/kws.html#dataset",
    "href": "contents/labs/seeed/xiao_esp32s3/kws/kws.html#dataset",
    "title": "Keyword Spotting (KWS)",
    "section": "Dataset",
    "text": "Dataset\nThe critical component of Machine Learning Workflow is the dataset. Once we have decided on specific keywords (YES and NO), we can take advantage of the dataset developed by Pete Warden, “Speech Commands: A Dataset for Limited-Vocabulary Speech Recognition.” This dataset has 35 keywords (with +1,000 samples each), such as yes, no, stop, and go. In other words, we can get 1,500 samples of yes and no.\nYou can download a small portion of the dataset from Edge Studio (Keyword spotting pre-built dataset), which includes samples from the four classes we will use in this project: yes, no, noise, and background. For this, follow the steps below:\n\nDownload the keywords dataset.\nUnzip the file in a location of your choice.\n\nAlthough we have a lot of data from Pete’s dataset, collecting some words spoken by us is advised. When working with accelerometers, creating a dataset with data captured by the same type of sensor was essential. In the case of sound, it is different because what we will classify is, in reality, audio data.\n\nThe key difference between sound and audio is their form of energy. Sound is mechanical wave energy (longitudinal sound waves) that propagate through a medium causing variations in pressure within the medium. Audio is made of electrical energy (analog or digital signals) that represent sound electrically.\n\nThe sound waves should be converted to audio data when we speak a keyword. The conversion should be done by sampling the signal generated by the microphone in 16KHz with a 16-bit depth.\nSo, any device that can generate audio data with this basic specification (16Khz/16bits) will work fine. As a device, we can use the proper XIAO ESP32S3 Sense, a computer, or even your mobile phone.\n\nCapturing online Audio Data with Edge Impulse and a smartphone\nIn the lab Motion Classification and Anomaly Detection, we connect our device directly to Edge Impulse Studio for data capturing (having a sampling frequency of 50Hz to 100Hz). For such low frequency, we could use the EI CLI function Data Forwarder, but according to Jan Jongboom, Edge Impulse CTO, audio (16KHz) goes too fast for the data forwarder to be captured. So, once we have the digital data captured by the microphone, we can turn it into a WAV file to be sent to the Studio via Data Uploader (same as we will do with Pete’s dataset).\n\nIf we want to collect audio data directly on the Studio, we can use any smartphone connected online with it. We will not explore this option here, but you can easily follow EI documentation.\n\n\nCapturing (offline) Audio Data with the XIAO ESP32S3 Sense\nThe built-in microphone is the MSM261D3526H1CPM, a PDM digital output MEMS microphone with Multi-modes. Internally, it is connected to the ESP32S3 via an I2S bus using pins IO41 (Clock) and IO41 (Data).\n\nWhat is I2S?\nI2S, or Inter-IC Sound, is a standard protocol for transmitting digital audio from one device to another. It was initially developed by Philips Semiconductor (now NXP Semiconductors). It is commonly used in audio devices such as digital signal processors, digital audio processors, and, more recently, microcontrollers with digital audio capabilities (our case here).\nThe I2S protocol consists of at least three lines:\n\n1. Bit (or Serial) clock line (BCLK or CLK): This line toggles to indicate the start of a new bit of data (pin IO42).\n2. Word select line (WS): This line toggles to indicate the start of a new word (left channel or right channel). The Word select clock (WS) frequency defines the sample rate. In our case, L/R on the microphone is set to ground, meaning that we will use only the left channel (mono).\n3. Data line (SD): This line carries the audio data (pin IO41)\nIn an I2S data stream, the data is sent as a sequence of frames, each containing a left-channel word and a right-channel word. This makes I2S particularly suited for transmitting stereo audio data. However, it can also be used for mono or multichannel audio with additional data lines.\nLet’s start understanding how to capture raw data using the microphone. Go to the GitHub projectand download the sketch: XIAOEsp2s3_Mic_Test:\n/*\n  XIAO ESP32S3 Simple Mic Test\n*/\n\n#include &lt;I2S.h&gt;\n\nvoid setup() {\n  Serial.begin(115200);\n  while (!Serial) {\n  }\n\n  // start I2S at 16 kHz with 16-bits per sample\n  I2S.setAllPins(-1, 42, 41, -1, -1);\n  if (!I2S.begin(PDM_MONO_MODE, 16000, 16)) {\n    Serial.println(\"Failed to initialize I2S!\");\n    while (1); // do nothing\n  }\n}\n\nvoid loop() {\n  // read a sample\n  int sample = I2S.read();\n\n  if (sample && sample != -1 && sample != 1) {\n    Serial.println(sample);\n  }\n}\nThis code is a simple microphone test for the XIAO ESP32S3 using the I2S (Inter-IC Sound) interface. It sets up the I2S interface to capture audio data at a sample rate of 16 kHz with 16 bits per sample and then continuously reads samples from the microphone and prints them to the serial monitor.\nLet’s dig into the code’s main parts:\n\nInclude the I2S library: This library provides functions to configure and use the I2S interface, which is a standard for connecting digital audio devices.\nI2S.setAllPins(-1, 42, 41, -1, -1): This sets up the I2S pins. The parameters are (-1, 42, 41, -1, -1), where the second parameter (42) is the PIN for the I2S clock (CLK), and the third parameter (41) is the PIN for the I2S data (DATA) line. The other parameters are set to -1, meaning those pins are not used.\nI2S.begin(PDM_MONO_MODE, 16000, 16): This initializes the I2S interface in Pulse Density Modulation (PDM) mono mode, with a sample rate of 16 kHz and 16 bits per sample. If the initialization fails, an error message is printed, and the program halts.\nint sample = I2S.read(): This reads an audio sample from the I2S interface.\n\nIf the sample is valid, it is printed on the Serial Monitor and Plotter.\nBelow is a test “whispering” in two different tones.\n\n\n\nSave recorded sound samples (dataset) as .wav audio files to a microSD card\nLet’s use the onboard SD Card reader to save .wav audio files; we must habilitate the XIAO PSRAM first.\n\nESP32-S3 has only a few hundred kilobytes of internal RAM on the MCU chip. It can be insufficient for some purposes so that ESP32-S3 can use up to 16 MB of external PSRAM (Psuedostatic RAM) connected in parallel with the SPI flash chip. The external memory is incorporated in the memory map and, with certain restrictions, is usable in the same way as internal data RAM.\n\nFor a start, Insert the SD Card on the XIAO as shown in the photo below (the SD Card should be formatted to FAT32).\n\nTurn the PSRAM function of the ESP-32 chip on (Arduino IDE): Tools&gt;PSRAM: “OPI PSRAM”&gt;OPI PSRAM\n\n\nDownload the sketch Wav_Record_dataset, which you can find on the project’s GitHub.\n\nThis code records audio using the I2S interface of the Seeed XIAO ESP32S3 Sense board, saves the recording as a.wav file on an SD card, and allows for control of the recording process through commands sent from the serial monitor. The name of the audio file is customizable (it should be the class labels to be used with the training), and multiple recordings can be made, each saved in a new file. The code also includes functionality to increase the volume of the recordings.\nLet’s break down the most essential parts of it:\n#include &lt;I2S.h&gt;\n#include \"FS.h\"\n#include \"SD.h\"\n#include \"SPI.h\"\nThose are the necessary libraries for the program. I2S.h allows for audio input, FS.h provides file system handling capabilities, SD.h enables the program to interact with an SD card, and SPI.h handles the SPI communication with the SD card.\n#define RECORD_TIME   10  \n#define SAMPLE_RATE 16000U\n#define SAMPLE_BITS 16\n#define WAV_HEADER_SIZE 44\n#define VOLUME_GAIN 2\nHere, various constants are defined for the program.\n\nRECORD_TIME specifies the length of the audio recording in seconds.\nSAMPLE_RATE and SAMPLE_BITS define the audio quality of the recording.\nWAV_HEADER_SIZE specifies the size of the .wav file header.\nVOLUME_GAIN is used to increase the volume of the recording.\n\nint fileNumber = 1;\nString baseFileName;\nbool isRecording = false;\nThese variables keep track of the current file number (to create unique file names), the base file name, and whether the system is currently recording.\nvoid setup() {\n  Serial.begin(115200);\n  while (!Serial);\n  \n  I2S.setAllPins(-1, 42, 41, -1, -1);\n  if (!I2S.begin(PDM_MONO_MODE, SAMPLE_RATE, SAMPLE_BITS)) {\n    Serial.println(\"Failed to initialize I2S!\");\n    while (1);\n  }\n  \n  if(!SD.begin(21)){\n    Serial.println(\"Failed to mount SD Card!\");\n    while (1);\n  }\n  Serial.printf(\"Enter with the label name\\n\");\n}\nThe setup function initializes the serial communication, I2S interface for audio input, and SD card interface. If the I2S did not initialize or the SD card fails to mount, it will print an error message and halt execution.\nvoid loop() {\n  if (Serial.available() &gt; 0) {\n    String command = Serial.readStringUntil('\\n');\n    command.trim();\n    if (command == \"rec\") {\n      isRecording = true;\n    } else {\n      baseFileName = command;\n      fileNumber = 1; //reset file number each time a new basefile name is set\n      Serial.printf(\"Send rec for starting recording label \\n\");\n    }\n  }\n  if (isRecording && baseFileName != \"\") {\n    String fileName = \"/\" + baseFileName + \".\" + String(fileNumber) + \".wav\";\n    fileNumber++;\n    record_wav(fileName);\n    delay(1000); // delay to avoid recording multiple files at once\n    isRecording = false;\n  }\n}\nIn the main loop, the program waits for a command from the serial monitor. If the command is rec, the program starts recording. Otherwise, the command is assumed to be the base name for the .wav files. If it’s currently recording and a base file name is set, it records the audio and saves it as a.wav file. The file names are generated by appending the file number to the base file name.\nvoid record_wav(String fileName)\n{\n  ...\n  \n  File file = SD.open(fileName.c_str(), FILE_WRITE);\n  ...\n  rec_buffer = (uint8_t *)ps_malloc(record_size);\n  ...\n\n  esp_i2s::i2s_read(esp_i2s::I2S_NUM_0, \n                    rec_buffer, \n                    record_size, \n                    &sample_size, \n                    portMAX_DELAY);\n  ...\n}\nThis function records audio and saves it as a.wav file with the given name. It starts by initializing the sample_size and record_size variables. record_size is calculated based on the sample rate, size, and desired recording time. Let’s dig into the essential sections;\nFile file = SD.open(fileName.c_str(), FILE_WRITE);\n// Write the header to the WAV file\nuint8_t wav_header[WAV_HEADER_SIZE];\ngenerate_wav_header(wav_header, record_size, SAMPLE_RATE);\nfile.write(wav_header, WAV_HEADER_SIZE);\nThis section of the code opens the file on the SD card for writing and then generates the .wav file header using the generate_wav_header function. It then writes the header to the file.\n// PSRAM malloc for recording\nrec_buffer = (uint8_t *)ps_malloc(record_size);\nif (rec_buffer == NULL) {\n  Serial.printf(\"malloc failed!\\n\");\n  while(1) ;\n}\nSerial.printf(\"Buffer: %d bytes\\n\", ESP.getPsramSize() - ESP.getFreePsram());\nThe ps_malloc function allocates memory in the PSRAM for the recording. If the allocation fails (i.e., rec_buffer is NULL), it prints an error message and halts execution.\n// Start recording\nesp_i2s::i2s_read(esp_i2s::I2S_NUM_0, \n         rec_buffer, \n         record_size, \n         &sample_size, \n         portMAX_DELAY);\nif (sample_size == 0) {\n  Serial.printf(\"Record Failed!\\n\");\n} else {\n    Serial.printf(\"Record %d bytes\\n\", sample_size);\n  }\nThe i2s_read function reads audio data from the microphone into rec_buffer. It prints an error message if no data is read (sample_size is 0).\n// Increase volume\nfor (uint32_t i = 0; i &lt; sample_size; i += SAMPLE_BITS/8) {\n  (*(uint16_t *)(rec_buffer+i)) &lt;&lt;= VOLUME_GAIN;\n}\nThis section of the code increases the recording volume by shifting the sample values by VOLUME_GAIN.\n// Write data to the WAV file\nSerial.printf(\"Writing to the file ...\\n\");\nif (file.write(rec_buffer, record_size) != record_size)\n  Serial.printf(\"Write file Failed!\\n\");\n\nfree(rec_buffer);\nfile.close();\nSerial.printf(\"Recording complete: \\n\");\nSerial.printf(\"Send rec for a new sample or enter a new label\\n\\n\");\nFinally, the audio data is written to the .wav file. If the write operation fails, it prints an error message. After writing, the memory allocated for rec_buffer is freed, and the file is closed. The function finishes by printing a completion message and prompting the user to send a new command.\nvoid generate_wav_header(uint8_t *wav_header,  \n             uint32_t wav_size, \n             uint32_t sample_rate)\n{\n  ...\n  memcpy(wav_header, set_wav_header, sizeof(set_wav_header));\n}\nThe generate_wav_header function creates a.wav file header based on the parameters (wav_size and sample_rate). It generates an array of bytes according to the .wav file format, which includes fields for the file size, audio format, number of channels, sample rate, byte rate, block alignment, bits per sample, and data size. The generated header is then copied into the wav_header array passed to the function.\nNow, upload the code to the XIAO and get samples from the keywords (yes and no). You can also capture noise and other words.\nThe Serial monitor will prompt you to receive the label to be recorded.\n\nSend the label (for example, yes). The program will wait for another command: rec\n\nAnd the program will start recording new samples every time a command rec is sent. The files will be saved as yes.1.wav, yes.2.wav, yes.3.wav, etc., until a new label (for example, no) is sent. In this case, you should send the command rec for each new sample, which will be saved as no.1.wav, no.2.wav, no.3.wav, etc.\n\nUltimately, we will get the saved files on the SD card.\n\nThe files are ready to be uploaded to Edge Impulse Studio\n\n\nCapturing (offline) Audio Data Apps\nAlternatively, you can also use your PC or smartphone to capture audio data with a sampling frequency 16KHz and a bit depth of 16 Bits. A good app for that is Voice Recorder Pro (IOS). You should save your records as .wav files and send them to your computer.\n\n\nNote that any app, such as Audacity, can be used for audio recording or even your computer.",
    "crumbs": [
      "XIAO ESP32S3",
      "Keyword Spotting (KWS)"
    ]
  },
  {
    "objectID": "contents/labs/seeed/xiao_esp32s3/kws/kws.html#training-model-with-edge-impulse-studio",
    "href": "contents/labs/seeed/xiao_esp32s3/kws/kws.html#training-model-with-edge-impulse-studio",
    "title": "Keyword Spotting (KWS)",
    "section": "Training model with Edge Impulse Studio",
    "text": "Training model with Edge Impulse Studio\n\nUploading the Data\nWhen the raw dataset is defined and collected (Pete’s dataset + recorded keywords), we should initiate a new project at Edge Impulse Studio:\n\nOnce the project is created, select the Upload Existing Data tool in the Data Acquisition section. Choose the files to be uploaded:\n\nAnd upload them to the Studio (You can automatically split data in train/test). Repete to all classes and all raw data.\n\nThe samples will now appear in the Data acquisition section.\n\nAll data on Pete’s dataset have a 1s length, but the samples recorded in the previous section have 10s and must be split into 1s samples to be compatible.\nClick on three dots after the sample name and select Split sample.\n\nOnce inside the tool, split the data into 1-second records. If necessary, add or remove segments:\n\nThis procedure should be repeated for all samples.\n\nNote: For longer audio files (minutes), first, split into 10-second segments and after that, use the tool again to get the final 1-second splits.\n\nSuppose we do not split data automatically in train/test during upload. In that case, we can do it manually (using the three dots menu, moving samples individually) or using Perform Train / Test Split on Dashboard - Danger Zone.\n\nWe can optionally check all datasets using the tab Data Explorer.\n\n\n\nCreating Impulse (Pre-Process / Model definition)\nAn impulse takes raw data, uses signal processing to extract features, and then uses a learning block to classify new data.\n\nFirst, we will take the data points with a 1-second window, augmenting the data, sliding that window each 500ms. Note that the option zero-pad data is set. It is essential to fill with zeros samples smaller than 1 second (in some cases, I reduced the 1000 ms window on the split tool to avoid noises and spikes).\nEach 1-second audio sample should be pre-processed and converted to an image (for example, 13 x 49 x 1). We will use MFCC, which extracts features from audio signals using Mel Frequency Cepstral Coefficients, which are great for the human voice.\n\nNext, we select KERAS for classification and build our model from scratch by doing Image Classification using Convolution Neural Network).\n\n\nPre-Processing (MFCC)\nThe next step is to create the images to be trained in the next phase:\nWe can keep the default parameter values or take advantage of the DSP Autotuneparameters option, which we will do.\n\nThe result will not spend much memory to pre-process data (only 16KB). Still, the estimated processing time is high, 675 ms for an Espressif ESP-EYE (the closest reference available), with a 240KHz clock (same as our device), but with a smaller CPU ( XTensa LX6, versus the LX7 on the ESP32S). The real inference time should be smaller.\nSuppose we need to reduce the inference time later. In that case, we should return to the pre-processing stage and, for example, reduce the FFT length to 256, change the Number of coefficients, or another parameter.\nFor now, let’s keep the parameters defined by the Autotuning tool. Save parameters and generate the features.\n\n\nIf you want to go further with converting temporal serial data into images using FFT, Spectrogram, etc., you can play with this CoLab: Audio Raw Data Analysis.\n\n\n\nModel Design and Training\nWe will use a Convolution Neural Network (CNN) model. The basic architecture is defined with two blocks of Conv1D + MaxPooling (with 8 and 16 neurons, respectively) and a 0.25 Dropout. And on the last layer, after Flattening four neurons, one for each class:\n\nAs hyper-parameters, we will have a Learning Rate of 0.005 and a model that will be trained by 100 epochs. We will also include data augmentation, as some noise. The result seems OK:\n\nIf you want to understand what is happening “under the hood,” you can download the dataset and run a Jupyter Notebook playing with the code. For example, you can analyze the accuracy by each epoch:\n\nThis CoLab Notebook can explain how you can go further: KWS Classifier Project - Looking “Under the hood Training/xiao_esp32s3_keyword_spotting_project_nn_classifier.ipynb).”",
    "crumbs": [
      "XIAO ESP32S3",
      "Keyword Spotting (KWS)"
    ]
  },
  {
    "objectID": "contents/labs/seeed/xiao_esp32s3/kws/kws.html#testing",
    "href": "contents/labs/seeed/xiao_esp32s3/kws/kws.html#testing",
    "title": "Keyword Spotting (KWS)",
    "section": "Testing",
    "text": "Testing\nTesting the model with the data put apart before training (Test Data), we got an accuracy of approximately 87%.\n\nInspecting the F1 score, we can see that for YES, we got 0.95, an excellent result once we used this keyword to “trigger” our postprocessing stage (turn on the built-in LED). Even for NO, we got 0.90. The worst result is for unknown, what is OK.\nWe can proceed with the project, but it is possible to perform Live Classification using a smartphone before deployment on our device. Go to the Live Classification section and click on Connect a Development board:\n\nPoint your phone to the barcode and select the link.\n\nYour phone will be connected to the Studio. Select the option Classification on the app, and when it is running, start testing your keywords, confirming that the model is working with live and real data:",
    "crumbs": [
      "XIAO ESP32S3",
      "Keyword Spotting (KWS)"
    ]
  },
  {
    "objectID": "contents/labs/seeed/xiao_esp32s3/kws/kws.html#deploy-and-inference",
    "href": "contents/labs/seeed/xiao_esp32s3/kws/kws.html#deploy-and-inference",
    "title": "Keyword Spotting (KWS)",
    "section": "Deploy and Inference",
    "text": "Deploy and Inference\nThe Studio will package all the needed libraries, preprocessing functions, and trained models, downloading them to your computer. You should select the option Arduino Library, and at the bottom, choose Quantized (Int8) and press the button Build.\n\nNow it is time for a real test. We will make inferences wholly disconnected from the Studio. Let’s change one of the ESP32 code examples created when you deploy the Arduino Library.\nIn your Arduino IDE, go to the File/Examples tab look for your project, and select esp32/esp32_microphone:\n\nThis code was created for the ESP-EYE built-in microphone, which should be adapted for our device.\nStart changing the libraries to handle the I2S bus:\n\nBy:\n#include &lt;I2S.h&gt;\n#define SAMPLE_RATE 16000U\n#define SAMPLE_BITS 16\nInitialize the IS2 microphone at setup(), including the lines:\nvoid setup()\n{\n...\n    I2S.setAllPins(-1, 42, 41, -1, -1);\n    if (!I2S.begin(PDM_MONO_MODE, SAMPLE_RATE, SAMPLE_BITS)) {\n      Serial.println(\"Failed to initialize I2S!\");\n    while (1) ;\n...\n}\nOn the static void capture_samples(void* arg) function, replace the line 153 that reads data from I2S mic:\n\nBy:\n/* read data at once from i2s */\nesp_i2s::i2s_read(esp_i2s::I2S_NUM_0, \n                 (void*)sampleBuffer, \n                 i2s_bytes_to_read, \n                 &bytes_read, 100);\nOn function static bool microphone_inference_start(uint32_t n_samples), we should comment or delete lines 198 to 200, where the microphone initialization function is called. This is unnecessary because the I2S microphone was already initialized during the setup().\n\nFinally, on static void microphone_inference_end(void) function, replace line 243:\n\nBy:\nstatic void microphone_inference_end(void)\n{\n    free(sampleBuffer);\n    ei_free(inference.buffer);\n}\nYou can find the complete code on the project’s GitHub. Upload the sketch to your board and test some real inferences:",
    "crumbs": [
      "XIAO ESP32S3",
      "Keyword Spotting (KWS)"
    ]
  },
  {
    "objectID": "contents/labs/seeed/xiao_esp32s3/kws/kws.html#postprocessing",
    "href": "contents/labs/seeed/xiao_esp32s3/kws/kws.html#postprocessing",
    "title": "Keyword Spotting (KWS)",
    "section": "Postprocessing",
    "text": "Postprocessing\nNow that we know the model is working by detecting our keywords, let’s modify the code to see the internal LED going on every time a YES is detected.\nYou should initialize the LED:\n#define LED_BUILT_IN 21\n...\nvoid setup()\n{\n...\n  pinMode(LED_BUILT_IN, OUTPUT); // Set the pin as output\n  digitalWrite(LED_BUILT_IN, HIGH); //Turn off\n...\n}\nAnd change the // print the predictions portion of the previous code (on loop():\nint pred_index = 0;     // Initialize pred_index\nfloat pred_value = 0;   // Initialize pred_value\n\n// print the predictions\nei_printf(\"Predictions \");\nei_printf(\"(DSP: %d ms., Classification: %d ms., Anomaly: %d ms.)\",\n     result.timing.dsp, result.timing.classification, result.timing.anomaly);\nei_printf(\": \\n\");\nfor (size_t ix = 0; ix &lt; EI_CLASSIFIER_LABEL_COUNT; ix++) {\n      ei_printf(\"    %s: \", result.classification[ix].label);\n      ei_printf_float(result.classification[ix].value);\n      ei_printf(\"\\n\");\n\n      if (result.classification[ix].value &gt; pred_value){\n         pred_index = ix;\n         pred_value = result.classification[ix].value;\n      }\n}\n\n// show the inference result on LED\nif (pred_index == 3){\n    digitalWrite(LED_BUILT_IN, LOW); //Turn on\n}\nelse{\n   digitalWrite(LED_BUILT_IN, HIGH); //Turn off\n}\nYou can find the complete code on the project’s GitHub. Upload the sketch to your board and test some real inferences:\n\nThe idea is that the LED will be ON whenever the keyword YES is detected. In the same way, instead of turning on an LED, this could be a “trigger” for an external device, as we saw in the introduction.",
    "crumbs": [
      "XIAO ESP32S3",
      "Keyword Spotting (KWS)"
    ]
  },
  {
    "objectID": "contents/labs/seeed/xiao_esp32s3/kws/kws.html#conclusion",
    "href": "contents/labs/seeed/xiao_esp32s3/kws/kws.html#conclusion",
    "title": "Keyword Spotting (KWS)",
    "section": "Conclusion",
    "text": "Conclusion\nThe Seeed XIAO ESP32S3 Sense is a giant tiny device! However, it is powerful, trustworthy, not expensive, low power, and has suitable sensors to be used on the most common embedded machine learning applications such as vision and sound. Even though Edge Impulse does not officially support XIAO ESP32S3 Sense (yet!), we realized that using the Studio for training and deployment is straightforward.\n\nOn my GitHub repository, you will find the last version all the codeused on this project and the previous ones of the XIAO ESP32S3 series.\n\nBefore we finish, consider that Sound Classification is more than just voice. For example, you can develop TinyML projects around sound in several areas, such as:\n\nSecurity (Broken Glass detection)\nIndustry (Anomaly Detection)\nMedical (Snore, Toss, Pulmonary diseases)\nNature (Beehive control, insect sound)",
    "crumbs": [
      "XIAO ESP32S3",
      "Keyword Spotting (KWS)"
    ]
  },
  {
    "objectID": "contents/labs/seeed/xiao_esp32s3/kws/kws.html#resources",
    "href": "contents/labs/seeed/xiao_esp32s3/kws/kws.html#resources",
    "title": "Keyword Spotting (KWS)",
    "section": "Resources",
    "text": "Resources\n\nXIAO ESP32S3 Codes\nSubset of Google Speech Commands Dataset\nKWS MFCC Analysis Colab Notebook\nKWS CNN training Colab Notebook\nXIAO ESP32S3 Post-processing Code\nEdge Impulse Project",
    "crumbs": [
      "XIAO ESP32S3",
      "Keyword Spotting (KWS)"
    ]
  },
  {
    "objectID": "contents/labs/seeed/xiao_esp32s3/motion_classification/motion_classification.html",
    "href": "contents/labs/seeed/xiao_esp32s3/motion_classification/motion_classification.html",
    "title": "Motion Classification and Anomaly Detection",
    "section": "",
    "text": "Overview\nThe XIAO ESP32S3 Sense, with its built-in camera and mic, is a versatile device. But what if you need to add another type of sensor, such as an IMU? No problem! One of the standout features of the XIAO ESP32S3 is its multiple pins that can be used as an I2C bus (SDA/SCL pins), making it a suitable platform for sensor integration.",
    "crumbs": [
      "XIAO ESP32S3",
      "Motion Classification and Anomaly Detection"
    ]
  },
  {
    "objectID": "contents/labs/seeed/xiao_esp32s3/motion_classification/motion_classification.html#installing-the-imu",
    "href": "contents/labs/seeed/xiao_esp32s3/motion_classification/motion_classification.html#installing-the-imu",
    "title": "Motion Classification and Anomaly Detection",
    "section": "Installing the IMU",
    "text": "Installing the IMU\nWhen selecting your IMU, the market offers a wide range of devices, each with unique features and capabilities. You could choose, for example, the ADXL362 (3-axis), MAX21100 (6-axis), MPU6050 (6-axis), LIS3DHTR (3-axis), or the LCM20600Seeed Grove— (6-axis), which is part of the IMU 9DOF (lcm20600+AK09918). This variety allows you to tailor your choice to your project’s specific needs.\nFor this project, we will use an IMU, the MPU6050 (or 6500), a low-cost (less than 2.00 USD) 6-axis Accelerometer/Gyroscope unit.\n\nAt the end of the lab, we will also comment on using the LCM20600.\n\nThe MPU-6500 is a 6-axis Motion Tracking device that combines a 3-axis gyroscope, 3-axis accelerometer, and a Digital Motion ProcessorTM (DMP) in a small 3x3x0.9mm package. It also features a 4096-byte FIFO that can lower the traffic on the serial bus interface and reduce power consumption by allowing the system processor to burst read sensor data and then go into a low-power mode.\nWith its dedicated I2C sensor bus, the MPU-6500 directly accepts inputs from external I2C devices. MPU-6500, with its 6-axis integration, on-chip DMP, and run-time calibration firmware, enables manufacturers to eliminate the costly and complex selection, qualification, and system-level integration of discrete devices, guaranteeing optimal motion performance for consumers. MPU-6500 is also designed to interface with multiple non-inertial digital sensors, such as pressure sensors, on its auxiliary I2C port.\n\n\nUsually, the libraries available are for MPU6050, but they work for both devices.\n\nConnecting the HW\nConnect the IMU to the XIAO according to the below diagram:\n\nMPU6050 SCL –&gt; XIAO D5\nMPU6050 SDA –&gt; XIAO D4\nMPU6050 VCC –&gt; XIAO 3.3V\nMPU6050 GND –&gt; XIAO GND\n\n\nInstall the Library\nGo to Arduino Library Manager and type MPU6050. Install the latest version.\n\nDownload the sketch MPU6050_Acc_Data_Acquisition.in:\n/*\n * Based on I2C device class (I2Cdev) Arduino sketch for MPU6050 class \n   by Jeff Rowberg &lt;jeff@rowberg.net&gt;\n * and Edge Impulse Data Forwarder Exampe (Arduino) \n   - https://docs.edgeimpulse.com/docs/cli-data-forwarder\n * \n * Developed by M.Rovai @11May23\n */\n\n#include \"I2Cdev.h\"\n#include \"MPU6050.h\"\n#include \"Wire.h\"\n\n#define FREQUENCY_HZ        50\n#define INTERVAL_MS         (1000 / (FREQUENCY_HZ + 1))\n#define ACC_RANGE           1 // 0: -/+2G; 1: +/-4G\n\n// convert factor g to m/s^2^ ==&gt; [-32768, +32767] ==&gt; [-2g, +2g]\n#define CONVERT_G_TO_MS2    (9.81/(16384.0/(1.+ACC_RANGE))) \n\nstatic unsigned long last_interval_ms = 0;\n\nMPU6050 imu;\nint16_t ax, ay, az;\n\nvoid setup() {\n  \n    Serial.begin(115200);\n\n    \n    // initialize device\n    Serial.println(\"Initializing I2C devices...\");\n    Wire.begin();\n    imu.initialize();\n    delay(10);\n    \n//    // verify connection\n//    if (imu.testConnection()) {\n//      Serial.println(\"IMU connected\");\n//    }\n//    else {\n//      Serial.println(\"IMU Error\");\n//    }\n    delay(300);\n    \n    //Set MCU 6050 OffSet Calibration \n    imu.setXAccelOffset(-4732);\n    imu.setYAccelOffset(4703);\n    imu.setZAccelOffset(8867);\n    imu.setXGyroOffset(61);\n    imu.setYGyroOffset(-73);\n    imu.setZGyroOffset(35);\n    \n    /* Set full-scale accelerometer range.\n     * 0 = +/- 2g\n     * 1 = +/- 4g\n     * 2 = +/- 8g\n     * 3 = +/- 16g\n     */\n    imu.setFullScaleAccelRange(ACC_RANGE);\n}\n\nvoid loop() {\n\n      if (millis() &gt; last_interval_ms + INTERVAL_MS) {\n        last_interval_ms = millis();\n        \n        // read raw accel/gyro measurements from device\n        imu.getAcceleration(&ax, &ay, &az);\n\n        // converting to m/s^2^\n        float ax_m_s^2^ = ax * CONVERT_G_TO_MS2;\n        float ay_m_s^2^ = ay * CONVERT_G_TO_MS2;\n        float az_m_s^2^ = az * CONVERT_G_TO_MS2;\n\n        Serial.print(ax_m_s^2^); \n        Serial.print(\"\\t\");\n        Serial.print(ay_m_s^2^); \n        Serial.print(\"\\t\");\n        Serial.println(az_m_s^2^); \n      }\n}\nSome comments about the code:\nNote that the values generated by the accelerometer and gyroscope have a range: [-32768, +32767], so for example, if the default accelerometer range is used, the range in Gs should be: [-2g, +2g]. So, “1G” means 16384.\nFor conversion to m/s2, for example, you can define the following:\n#define CONVERT_G_TO_MS2 (9.81/16384.0)\nIn the code, I left an option (ACC_RANGE) to be set to 0 (+/-2G) or 1 (+/- 4G). We will use +/-4G; that should be enough for us. In this case.\nWe will capture the accelerometer data on a frequency of 50Hz, and the acceleration data will be sent to the Serial Port as meters per squared second (m/s2).\nWhen you ran the code with the IMU resting over your table, the accelerometer data shown on the Serial Monitor should be around 0.00, 0.00, and 9.81. If the values are a lot different, you should calibrate the IMU.\nThe MCU6050 can be calibrated using the sketch: mcu6050-calibration.ino.\nRun the code. The following will be displayed on the Serial Monitor:\n\nSend any character (in the above example, “x”), and the calibration should start.\n\nNote that a message MPU6050 connection failed. Ignore this message. For some reason, imu.testConnection() is not returning a correct result.\n\nIn the end, you will receive the offset values to be used on all your sketches:\n\nTake the values and use them on the setup:\n//Set MCU 6050 OffSet Calibration \nimu.setXAccelOffset(-4732);\nimu.setYAccelOffset(4703);\nimu.setZAccelOffset(8867);\nimu.setXGyroOffset(61);\nimu.setYGyroOffset(-73);\nimu.setZGyroOffset(35);\nNow, run the sketch MPU6050_Acc_Data_Acquisition.in:\nOnce you run the above sketch, open the Serial Monitor:\n\nOr check the Plotter:\n\nMove your device in the three axes. You should see the variation on Plotter:",
    "crumbs": [
      "XIAO ESP32S3",
      "Motion Classification and Anomaly Detection"
    ]
  },
  {
    "objectID": "contents/labs/seeed/xiao_esp32s3/motion_classification/motion_classification.html#the-tinyml-motion-classification-project",
    "href": "contents/labs/seeed/xiao_esp32s3/motion_classification/motion_classification.html#the-tinyml-motion-classification-project",
    "title": "Motion Classification and Anomaly Detection",
    "section": "The TinyML Motion Classification Project",
    "text": "The TinyML Motion Classification Project\nFor our lab, we will simulate mechanical stresses in transport. Our problem will be to classify four classes of movement:\n\nMaritime (pallets in boats)\nTerrestrial (palettes in a Truck or Train)\nLift (Palettes being handled by Fork-Lift)\nIdle (Palettes in Storage houses)\n\nSo, to start, we should collect data. Then, accelerometers will provide the data on the palette (or container).\n\nFrom the above images, we can see that primarily horizontal movements should be associated with the “Terrestrial class,” Vertical movements with the “Lift Class,” no activity with the “Idle class,” and movement on all three axes to Maritime class.",
    "crumbs": [
      "XIAO ESP32S3",
      "Motion Classification and Anomaly Detection"
    ]
  },
  {
    "objectID": "contents/labs/seeed/xiao_esp32s3/motion_classification/motion_classification.html#connecting-the-device-to-edge-impulse",
    "href": "contents/labs/seeed/xiao_esp32s3/motion_classification/motion_classification.html#connecting-the-device-to-edge-impulse",
    "title": "Motion Classification and Anomaly Detection",
    "section": "Connecting the device to Edge Impulse",
    "text": "Connecting the device to Edge Impulse\nFor data collection, we should first connect our device to the Edge Impulse Studio, which will also be used for data pre-processing, model training, testing, and deployment.\n\nFollow the instructions hereto install the Node.jsand Edge Impulse CLI on your computer.\n\nOnce the XIAO ESP32S3 is not a fully supported development board by Edge Impulse, we should, for example, use the CLI Data Forwarder to capture data from our sensor and send it to the Studio, as shown in this diagram:\n\n\nYou can alternately capture your data “offline,” store them on an SD card or send them to your computer via Bluetooth or Wi-Fi. In this video, you can learn alternative ways to send data to the Edge Impulse Studio.\n\nConnect your device to the serial port and run the previous code to capture IMU (Accelerometer) data, “printing them” on the serial. This will allow the Edge Impulse Studio to “capture” them.\nGo to the Edge Impulse page and create a project.\n\n\nThe maximum length for an Arduino library name is 63 characters. Note that the Studio will name the final library using your project name and include “_inference” to it. The name I chose initially did not work when I tried to deploy the Arduino library because it resulted in 64 characters. So, I need to change it by taking out the “anomaly detection” part.\n\nStart the CLI Data Forwarderon your terminal, entering (if it is the first time) the following command:\nedge-impulse-data-forwarder --clean\nNext, enter your EI credentials and choose your project, variables, and device names:\n\nGo to your EI Project and verify if the device is connected (the dot should be green):",
    "crumbs": [
      "XIAO ESP32S3",
      "Motion Classification and Anomaly Detection"
    ]
  },
  {
    "objectID": "contents/labs/seeed/xiao_esp32s3/motion_classification/motion_classification.html#data-collection",
    "href": "contents/labs/seeed/xiao_esp32s3/motion_classification/motion_classification.html#data-collection",
    "title": "Motion Classification and Anomaly Detection",
    "section": "Data Collection",
    "text": "Data Collection\nAs discussed before, we should capture data from all four Transportation Classes. Imagine that you have a container with a built-in accelerometer:\n\nNow imagine your container is on a boat, facing an angry ocean, on a truck, etc.:\n\nMaritime (pallets in boats)\n\nMove the XIAO in all directions, simulating an undulatory boat movement.\n\nTerrestrial (palettes in a Truck or Train)\n\nMove the XIAO over a horizontal line.\n\nLift (Palettes being handled by Fork-Lift)\n\nMove the XIAO over a vertical line.\n\nIdle (Palettes in Storage houses)\n\nLeave the XIAO over the table.\n\n\n\nBelow is one sample (raw data) of 10 seconds:\n\nYou can capture, for example, 2 minutes (twelve samples of 10 seconds each) for the four classes. Using the “3 dots” after each one of the samples, select 2, moving them for the Test set (or use the automatic Train/Test Split tool on the Danger Zone of Dashboard tab). Below, you can see the result datasets:",
    "crumbs": [
      "XIAO ESP32S3",
      "Motion Classification and Anomaly Detection"
    ]
  },
  {
    "objectID": "contents/labs/seeed/xiao_esp32s3/motion_classification/motion_classification.html#data-pre-processing",
    "href": "contents/labs/seeed/xiao_esp32s3/motion_classification/motion_classification.html#data-pre-processing",
    "title": "Motion Classification and Anomaly Detection",
    "section": "Data Pre-Processing",
    "text": "Data Pre-Processing\nThe raw data type captured by the accelerometer is a “time series” and should be converted to “tabular data”. We can do this conversion using a sliding window over the sample data. For example, in the below figure,\n\nWe can see 10 seconds of accelerometer data captured with a sample rate (SR) of 50Hz. A 2-second window will capture 300 data points (3 axis x 2 seconds x 50 samples). We will slide this window each 200ms, creating a larger dataset where each instance has 300 raw features.\n\nYou should use the best SR for your case, considering Nyquist’s theorem, which states that a periodic signal must be sampled at more than twice the signal’s highest frequency component.\n\nData preprocessing is a challenging area for embedded machine learning. Still, Edge Impulse helps overcome this with its digital signal processing (DSP) preprocessing step and, more specifically, the Spectral Features.\nOn the Studio, this dataset will be the input of a Spectral Analysis block, which is excellent for analyzing repetitive motion, such as data from accelerometers. This block will perform a DSP (Digital Signal Processing), extracting features such as “FFT” or “Wavelets”. In the most common case, FFT, the Time Domain Statistical features per axis/channel are:\n\nRMS\nSkewness\nKurtosis\n\nAnd the Frequency Domain Spectral features per axis/channel are:\n\nSpectral Power\nSkewness\nKurtosis\n\nFor example, for an FFT length of 32 points, the Spectral Analysis Block’s resulting output will be 21 features per axis (a total of 63 features).\nThose 63 features will be the Input Tensor of a Neural Network Classifier and the Anomaly Detection model (K-Means).\n\nYou can learn more by digging into the lab DSP Spectral Features",
    "crumbs": [
      "XIAO ESP32S3",
      "Motion Classification and Anomaly Detection"
    ]
  },
  {
    "objectID": "contents/labs/seeed/xiao_esp32s3/motion_classification/motion_classification.html#model-design",
    "href": "contents/labs/seeed/xiao_esp32s3/motion_classification/motion_classification.html#model-design",
    "title": "Motion Classification and Anomaly Detection",
    "section": "Model Design",
    "text": "Model Design\nOur classifier will be a Dense Neural Network (DNN) that will have 63 neurons on its input layer, two hidden layers with 20 and 10 neurons, and an output layer with four neurons (one per each class), as shown here:",
    "crumbs": [
      "XIAO ESP32S3",
      "Motion Classification and Anomaly Detection"
    ]
  },
  {
    "objectID": "contents/labs/seeed/xiao_esp32s3/motion_classification/motion_classification.html#impulse-design",
    "href": "contents/labs/seeed/xiao_esp32s3/motion_classification/motion_classification.html#impulse-design",
    "title": "Motion Classification and Anomaly Detection",
    "section": "Impulse Design",
    "text": "Impulse Design\nAn impulse takes raw data, uses signal processing to extract features, and then uses a learning block to classify new data.\nWe also take advantage of a second model, the K-means, that can be used for Anomaly Detection. If we imagine that we could have our known classes as clusters, any sample that could not fit on that could be an outlier, an anomaly (for example, a container rolling out of a ship on the ocean).\n\n\nImagine our XIAO rolling or moving upside-down, on a movement complement different from the one trained\n\n\nBelow is our final Impulse design:",
    "crumbs": [
      "XIAO ESP32S3",
      "Motion Classification and Anomaly Detection"
    ]
  },
  {
    "objectID": "contents/labs/seeed/xiao_esp32s3/motion_classification/motion_classification.html#generating-features",
    "href": "contents/labs/seeed/xiao_esp32s3/motion_classification/motion_classification.html#generating-features",
    "title": "Motion Classification and Anomaly Detection",
    "section": "Generating features",
    "text": "Generating features\nAt this point in our project, we have defined the pre-processing method and the model designed. Now, it is time to have the job done. First, let’s take the raw data (time-series type) and convert it to tabular data. Go to the Spectral Features tab and select Save Parameters:\n\nAt the top menu, select the Generate Features option and the Generate Features button. Each 2-second window data will be converted into one data point of 63 features.\n\nThe Feature Explorer will show those data in 2D using UMAP. Uniform Manifold Approximation and Projection (UMAP) is a dimension reduction technique that can be used for visualization similarly to t-SNE but also for general non-linear dimension reduction.\n\nThe visualization allows one to verify that the classes present an excellent separation, which indicates that the classifier should work well.\n\nOptionally, you can analyze the relative importance of each feature for one class compared with other classes.",
    "crumbs": [
      "XIAO ESP32S3",
      "Motion Classification and Anomaly Detection"
    ]
  },
  {
    "objectID": "contents/labs/seeed/xiao_esp32s3/motion_classification/motion_classification.html#training",
    "href": "contents/labs/seeed/xiao_esp32s3/motion_classification/motion_classification.html#training",
    "title": "Motion Classification and Anomaly Detection",
    "section": "Training",
    "text": "Training\nOur model has four layers, as shown below:\n\nAs hyperparameters, we will use a Learning Rate of 0.005 and 20% of data for validation for 30 epochs. After training, we can see that the accuracy is 97%.\n\nFor anomaly detection, we should choose the suggested features that are precisely the most important in feature extraction. The number of clusters will be 32, as suggested by the Studio:",
    "crumbs": [
      "XIAO ESP32S3",
      "Motion Classification and Anomaly Detection"
    ]
  },
  {
    "objectID": "contents/labs/seeed/xiao_esp32s3/motion_classification/motion_classification.html#testing",
    "href": "contents/labs/seeed/xiao_esp32s3/motion_classification/motion_classification.html#testing",
    "title": "Motion Classification and Anomaly Detection",
    "section": "Testing",
    "text": "Testing\nUsing 20% of the data left behind during the data capture phase, we can verify how our model will behave with unknown data; if not 100% (what is expected), the result was not that good (8%), mainly due to the terrestrial class. Once we have four classes (which output should add 1.0), we can set up a lower threshold for a class to be considered valid (for example, 0.4):\n\nNow, the Test accuracy will go up to 97%.\n\nYou should also use your device (which is still connected to the Studio) and perform some Live Classification.\n\nBe aware that here you will capture real data with your device and upload it to the Studio, where an inference will be taken using the trained model (But the model is NOT in your device).",
    "crumbs": [
      "XIAO ESP32S3",
      "Motion Classification and Anomaly Detection"
    ]
  },
  {
    "objectID": "contents/labs/seeed/xiao_esp32s3/motion_classification/motion_classification.html#deploy",
    "href": "contents/labs/seeed/xiao_esp32s3/motion_classification/motion_classification.html#deploy",
    "title": "Motion Classification and Anomaly Detection",
    "section": "Deploy",
    "text": "Deploy\nNow it is time for magic! The Studio will package all the needed libraries, preprocessing functions, and trained models, downloading them to your computer. You should select the option Arduino Library, and at the bottom, choose Quantized (Int8) and Build. A Zip file will be created and downloaded to your computer.\n\nOn your Arduino IDE, go to the Sketch tab, select the option Add.ZIP Library, and Choose the.zip file downloaded by the Studio:",
    "crumbs": [
      "XIAO ESP32S3",
      "Motion Classification and Anomaly Detection"
    ]
  },
  {
    "objectID": "contents/labs/seeed/xiao_esp32s3/motion_classification/motion_classification.html#inference",
    "href": "contents/labs/seeed/xiao_esp32s3/motion_classification/motion_classification.html#inference",
    "title": "Motion Classification and Anomaly Detection",
    "section": "Inference",
    "text": "Inference\nNow, it is time for a real test. We will make inferences that are wholly disconnected from the Studio. Let’s change one of the code examples created when you deploy the Arduino Library.\nIn your Arduino IDE, go to the File/Examples tab and look for your project, and on examples, select nano_ble_sense_accelerometer:\n\nOf course, this is not your board, but we can have the code working with only a few changes.\nFor example, at the beginning of the code, you have the library related to Arduino Sense IMU:\n/* Includes --------------------------------------------------------------- */\n#include &lt;XIAO-ESP32S3-Motion-Classification_inferencing.h&gt;\n#include &lt;Arduino_LSM9DS1.h&gt;\nChange the “includes” portion with the code related to the IMU:\n#include &lt;XIAO-ESP32S3-Motion-Classification_inferencing.h&gt;\n#include \"I2Cdev.h\"\n#include \"MPU6050.h\"\n#include \"Wire.h\"\nChange the Constant Defines\n/* Constant defines ------------------------------------------------------- */\nMPU6050 imu;\nint16_t ax, ay, az;\n\n#define ACC_RANGE           1 // 0: -/+2G; 1: +/-4G\n#define CONVERT_G_TO_MS2    (9.81/(16384/(1.+ACC_RANGE)))\n#define MAX_ACCEPTED_RANGE  (2*9.81)+(2*9.81)*ACC_RANGE\nOn the setup function, initiate the IMU set the off-set values and range:\n// initialize device\nSerial.println(\"Initializing I2C devices...\");\nWire.begin();\nimu.initialize();\ndelay(10);\n\n//Set MCU 6050 OffSet Calibration \nimu.setXAccelOffset(-4732);\nimu.setYAccelOffset(4703);\nimu.setZAccelOffset(8867);\nimu.setXGyroOffset(61);\nimu.setYGyroOffset(-73);\nimu.setZGyroOffset(35);\n\nimu.setFullScaleAccelRange(ACC_RANGE);\nAt the loop function, the buffers buffer[ix], buffer[ix + 1], and buffer[ix + 2] will receive the 3-axis data captured by the accelerometer. On the original code, you have the line:\nIMU.readAcceleration(buffer[ix], buffer[ix + 1], buffer[ix + 2]);\nChange it with this block of code:\nimu.getAcceleration(&ax, &ay, &az);       \nbuffer[ix + 0] = ax;\nbuffer[ix + 1] = ay;\nbuffer[ix + 2] = az;\nYou should change the order of the following two blocks of code. First, you make the conversion to raw data to “Meters per squared second (ms2)”, followed by the test regarding the maximum acceptance range (that here is in ms2, but on Arduino, was in Gs):\nbuffer[ix + 0] *= CONVERT_G_TO_MS2;\nbuffer[ix + 1] *= CONVERT_G_TO_MS2;\nbuffer[ix + 2] *= CONVERT_G_TO_MS2;\n\nfor (int i = 0; i &lt; 3; i++) {\n     if (fabs(buffer[ix + i]) &gt; MAX_ACCEPTED_RANGE) {\n        buffer[ix + i] = ei_get_sign(buffer[ix + i]) * MAX_ACCEPTED_RANGE;\n     }\n}\nAnd that is it! You can now upload the code to your device and proceed with the inferences. The complete code is available on the project’s GitHub.\nNow you should try your movements, seeing the result of the inference of each class on the images:\n\n\n\n\nAnd, of course, some “anomaly”, for example, putting the XIAO upside-down. The anomaly score will be over 1:",
    "crumbs": [
      "XIAO ESP32S3",
      "Motion Classification and Anomaly Detection"
    ]
  },
  {
    "objectID": "contents/labs/seeed/xiao_esp32s3/motion_classification/motion_classification.html#conclusion",
    "href": "contents/labs/seeed/xiao_esp32s3/motion_classification/motion_classification.html#conclusion",
    "title": "Motion Classification and Anomaly Detection",
    "section": "Conclusion",
    "text": "Conclusion\nRegarding the IMU, this project used the low-cost MPU6050 but could also use other IMUs, for example, the LCM20600 (6-axis), which is part of the Seeed Grove - IMU 9DOF (lcm20600+AK09918). You can take advantage of this sensor, which has integrated a Grove connector, which can be helpful in the case you use the XIAO with an extension board, as shown below:\n\nYou can follow the instructions here to connect the IMU with the MCU. Only note that for using the Grove ICM20600 Accelerometer, it is essential to update the files I2Cdev.cpp and I2Cdev.h that you will download from the library provided by Seeed Studio. For that, replace both files from this link. You can find a sketch for testing the IMU on the GitHub project: accelerometer_test.ino.\n\nOn the projet’s GitHub repository, you will find the last version of all codeand other docs: XIAO-ESP32S3 - IMU.",
    "crumbs": [
      "XIAO ESP32S3",
      "Motion Classification and Anomaly Detection"
    ]
  },
  {
    "objectID": "contents/labs/seeed/xiao_esp32s3/motion_classification/motion_classification.html#resources",
    "href": "contents/labs/seeed/xiao_esp32s3/motion_classification/motion_classification.html#resources",
    "title": "Motion Classification and Anomaly Detection",
    "section": "Resources",
    "text": "Resources\n\nXIAO ESP32S3 Codes\nEdge Impulse Spectral Features Block Colab Notebook\nEdge Impulse Project",
    "crumbs": [
      "XIAO ESP32S3",
      "Motion Classification and Anomaly Detection"
    ]
  },
  {
    "objectID": "contents/labs/raspi/raspi.html",
    "href": "contents/labs/raspi/raspi.html",
    "title": "Raspberry Pi",
    "section": "",
    "text": "Pre-requisites\nThese labs offer invaluable hands-on experience with machine learning systems, leveraging the versatility and accessibility of the Raspberry Pi platform. Unlike working with large-scale models that demand extensive cloud resources, these exercises allow you to directly interact with hardware and software in a compact yet powerful edge computing environment. You’ll gain practical insights into deploying AI at the edge by utilizing Raspberry Pi’s capabilities, from the efficient Pi Zero to the more robust Pi 4 or Pi 5 models. This approach provides a tangible understanding of the challenges and opportunities in implementing machine learning solutions in resource-constrained settings. While we’re working at a smaller scale, the principles and techniques you’ll learn are fundamentally similar to those used in larger systems. The Raspberry Pi’s ability to run a whole operating system and its extensive GPIO capabilities allow for a rich learning experience that bridges the gap between theoretical knowledge and real-world application. Through these labs, you’ll grasp the intricacies of EdgeML and develop skills applicable to a wide range of AI deployment scenarios.",
    "crumbs": [
      "Raspberry Pi"
    ]
  },
  {
    "objectID": "contents/labs/raspi/raspi.html#pre-requisites",
    "href": "contents/labs/raspi/raspi.html#pre-requisites",
    "title": "Raspberry Pi",
    "section": "",
    "text": "Raspberry Pi: Ensure you have at least one of the boards: the Raspberry Pi Zero 2W, Raspberry Pi 4 or 5 for the Vision Labs, and the Raspberry 5 for the GenAi labs.\nPower Adapter: To Power on the boards.\n\nRaspberry Pi Zero 2-W: 2.5W with a Micro-USB adapter\nRaspberry Pi 4 or 5: 3.5W with a USB-C adapter\n\nNetwork: With internet access for downloading the necessary software and controlling the boards remotely.\nSD Card (32GB minimum) and an SD card Adapter: For the Raspberry Pi OS.",
    "crumbs": [
      "Raspberry Pi"
    ]
  },
  {
    "objectID": "contents/labs/raspi/raspi.html#setup",
    "href": "contents/labs/raspi/raspi.html#setup",
    "title": "Raspberry Pi",
    "section": "Setup",
    "text": "Setup\n\nSetup Raspberry Pi",
    "crumbs": [
      "Raspberry Pi"
    ]
  },
  {
    "objectID": "contents/labs/raspi/raspi.html#exercises",
    "href": "contents/labs/raspi/raspi.html#exercises",
    "title": "Raspberry Pi",
    "section": "Exercises",
    "text": "Exercises\n\n\n\nModality\nTask\nDescription\nLink\n\n\n\n\nVision\nImage Classification\nLearn to classify images\nLink\n\n\nVision\nObject Detection\nImplement object detection\nLink\n\n\nGenAI\nSmall Language Models\nDeploy SLMs at the Edge\nLink\n\n\nGenAI\nVisual-Language Models\nDeploy VLMs at the Edge\nLink",
    "crumbs": [
      "Raspberry Pi"
    ]
  },
  {
    "objectID": "contents/labs/raspi/setup/setup.html",
    "href": "contents/labs/raspi/setup/setup.html",
    "title": "Setup",
    "section": "",
    "text": "Overview\nThis chapter will guide you through setting up Raspberry Pi Zero 2 W (Raspi-Zero) and Raspberry Pi 5 (Raspi-5) models. We’ll cover hardware setup, operating system installation, initial configuration, and tests.\nThe Raspberry Pi is a powerful and versatile single-board computer that has become an essential tool for engineers across various disciplines. Developed by the Raspberry Pi Foundation, these compact devices offer a unique combination of affordability, computational power, and extensive GPIO (General Purpose Input/Output) capabilities, making them ideal for prototyping, embedded systems development, and advanced engineering projects.",
    "crumbs": [
      "Raspberry Pi",
      "Setup"
    ]
  },
  {
    "objectID": "contents/labs/raspi/setup/setup.html#overview",
    "href": "contents/labs/raspi/setup/setup.html#overview",
    "title": "Setup",
    "section": "",
    "text": "Key Features\n\nComputational Power: Despite their small size, Raspberry Pis offers significant processing capabilities, with the latest models featuring multi-core ARM processors and up to 8GB of RAM.\nGPIO Interface: The 40-pin GPIO header allows direct interaction with sensors, actuators, and other electronic components, facilitating hardware-software integration projects.\nExtensive Connectivity: Built-in Wi-Fi, Bluetooth, Ethernet, and multiple USB ports enable diverse communication and networking projects.\nLow-Level Hardware Access: Raspberry Pis provides access to interfaces like I2C, SPI, and UART, allowing for detailed control and communication with external devices.\nReal-Time Capabilities: With proper configuration, Raspberry Pis can be used for soft real-time applications, making them suitable for control systems and signal processing tasks.\nPower Efficiency: Low power consumption enables battery-powered and energy-efficient designs, especially in models like the Pi Zero.\n\n\n\nRaspberry Pi Models (covered in this book)\n\nRaspberry Pi Zero 2 W (Raspi-Zero):\n\nIdeal for: Compact embedded systems\nKey specs: 1GHz single-core CPU (ARM Cortex-A53), 512MB RAM, minimal power consumption\n\nRaspberry Pi 5 (Raspi-5):\n\nIdeal for: More demanding applications such as edge computing, computer vision, and edgeAI applications, including LLMs.\nKey specs: 2.4GHz quad-core CPU (ARM Cortex A-76), up to 8GB RAM, PCIe interface for expansions\n\n\n\n\nEngineering Applications\n\nEmbedded Systems Design: Develop and prototype embedded systems for real-world applications.\nIoT and Networked Devices: Create interconnected devices and explore protocols like MQTT, CoAP, and HTTP/HTTPS.\nControl Systems: Implement feedback control loops, PID controllers, and interface with actuators.\nComputer Vision and AI: Utilize libraries like OpenCV and TensorFlow Lite for image processing and machine learning at the edge.\nData Acquisition and Analysis: Collect sensor data, perform real-time analysis, and create data logging systems.\nRobotics: Build robot controllers, implement motion planning algorithms, and interface with motor drivers.\nSignal Processing: Perform real-time signal analysis, filtering, and DSP applications.\nNetwork Security: Set up VPNs, firewalls, and explore network penetration testing.\n\nThis tutorial will guide you through setting up the most common Raspberry Pi models, enabling you to start on your machine learning project quickly. We’ll cover hardware setup, operating system installation, and initial configuration, focusing on preparing your Pi for Machine Learning applications.",
    "crumbs": [
      "Raspberry Pi",
      "Setup"
    ]
  },
  {
    "objectID": "contents/labs/raspi/setup/setup.html#hardware-overview",
    "href": "contents/labs/raspi/setup/setup.html#hardware-overview",
    "title": "Setup",
    "section": "Hardware Overview",
    "text": "Hardware Overview\n\nRaspberry Pi Zero 2W\n\n\nProcessor: 1GHz quad-core 64-bit Arm Cortex-A53 CPU\nRAM: 512MB SDRAM\nWireless: 2.4GHz 802.11 b/g/n wireless LAN, Bluetooth 4.2, BLE\nPorts: Mini HDMI, micro USB OTG, CSI-2 camera connector\nPower: 5V via micro USB port\n\n\n\nRaspberry Pi 5\n\n\nProcessor:\n\nPi 5: Quad-core 64-bit Arm Cortex-A76 CPU @ 2.4GHz\nPi 4: Quad-core Cortex-A72 (ARM v8) 64-bit SoC @ 1.5GHz\n\nRAM: 2GB, 4GB, or 8GB options (8GB recommended for AI tasks)\nWireless: Dual-band 802.11ac wireless, Bluetooth 5.0\nPorts: 2 × micro HDMI ports, 2 × USB 3.0 ports, 2 × USB 2.0 ports, CSI camera port, DSI display port\nPower: 5V DC via USB-C connector (3A)\n\n\nIn the labs, we will use different names to address the Raspberry: Raspi, Raspi-5, Raspi-Zero, etc. Usually, Raspi is used when the instructions or comments apply to every model.",
    "crumbs": [
      "Raspberry Pi",
      "Setup"
    ]
  },
  {
    "objectID": "contents/labs/raspi/setup/setup.html#installing-the-operating-system",
    "href": "contents/labs/raspi/setup/setup.html#installing-the-operating-system",
    "title": "Setup",
    "section": "Installing the Operating System",
    "text": "Installing the Operating System\n\nThe Operating System (OS)\nAn operating system (OS) is fundamental software that manages computer hardware and software resources, providing standard services for computer programs. It is the core software that runs on a computer, acting as an intermediary between hardware and application software. The OS manages the computer’s memory, processes, device drivers, files, and security protocols.\n\nKey functions:\n\nProcess management: Allocating CPU time to different programs\nMemory management: Allocating and freeing up memory as needed\nFile system management: Organizing and keeping track of files and directories\nDevice management: Communicating with connected hardware devices\nUser interface: Providing a way for users to interact with the computer\n\nComponents:\n\nKernel: The core of the OS that manages hardware resources\nShell: The user interface for interacting with the OS\nFile system: Organizes and manages data storage\nDevice drivers: Software that allows the OS to communicate with hardware\n\n\nThe Raspberry Pi runs a specialized version of Linux designed for embedded systems. This operating system, typically a variant of Debian called Raspberry Pi OS (formerly Raspbian), is optimized for the Pi’s ARM-based architecture and limited resources.\n\nThe latest version of Raspberry Pi OS is based on Debian Bookworm.\n\nKey features:\n\nLightweight: Tailored to run efficiently on the Pi’s hardware.\nVersatile: Supports a wide range of applications and programming languages.\nOpen-source: Allows for customization and community-driven improvements.\nGPIO support: Enables interaction with sensors and other hardware through the Pi’s pins.\nRegular updates: Continuously improved for performance and security.\n\nEmbedded Linux on the Raspberry Pi provides a full-featured operating system in a compact package, making it ideal for projects ranging from simple IoT devices to more complex edge machine-learning applications. Its compatibility with standard Linux tools and libraries makes it a powerful platform for development and experimentation.\n\n\nInstallation\nTo use the Raspberry Pi, we will need an operating system. By default, Raspberry Pi checks for an operating system on any SD card inserted in the slot, so we should install an operating system using Raspberry Pi Imager.\nRaspberry Pi Imager is a tool for downloading and writing images on macOS, Windows, and Linux. It includes many popular operating system images for Raspberry Pi. We will also use the Imager to preconfigure credentials and remote access settings.\nFollow the steps to install the OS in your Raspi.\n\nDownload and install the Raspberry Pi Imager on your computer.\nInsert a microSD card into your computer (a 32GB SD card is recommended) .\nOpen Raspberry Pi Imager and select your Raspberry Pi model.\nChoose the appropriate operating system:\n\nFor Raspi-Zero: For example, you can select: Raspberry Pi OS Lite (64-bit).\n\n\n\n\nimg\n\n\n\nDue to its reduced SDRAM (512MB), the recommended OS for the Raspi-Zero is the 32-bit version. However, to run some machine learning models, such as the YOLOv8 from Ultralitics, we should use the 64-bit version. Although Raspi-Zero can run a desktop, we will choose the LITE version (no Desktop) to reduce the RAM needed for regular operation.\n\n\nFor Raspi-5: We can select the full 64-bit version, which includes a desktop: Raspberry Pi OS (64-bit)\n\n\nSelect your microSD card as the storage device.\nClick on Next and then the gear icon to access advanced options.\nSet the hostname, the Raspi username and password, configure WiFi and enable SSH (Very important!)\n\n\n\nWrite the image to the microSD card.\n\n\nIn the examples here, we will use different hostnames depending on the device used: raspi, raspi-5, raspi-Zero, etc. It would help if you replaced it with the one you are using.\n\n\n\nInitial Configuration\n\nInsert the microSD card into your Raspberry Pi.\nConnect power to boot up the Raspberry Pi.\nPlease wait for the initial boot process to complete (it may take a few minutes).\n\n\nYou can find the most common Linux commands to be used with the Raspi here or here.",
    "crumbs": [
      "Raspberry Pi",
      "Setup"
    ]
  },
  {
    "objectID": "contents/labs/raspi/setup/setup.html#remote-access",
    "href": "contents/labs/raspi/setup/setup.html#remote-access",
    "title": "Setup",
    "section": "Remote Access",
    "text": "Remote Access\n\nSSH Access\nThe easiest way to interact with the Raspi-Zero is via SSH (“Headless”). You can use a Terminal (MAC/Linux), PuTTy (Windows), or any other.\n\nFind your Raspberry Pi’s IP address (for example, check your router).\nOn your computer, open a terminal and connect via SSH:\nssh username@[raspberry_pi_ip_address]   \nAlternatively, if you do not have the IP address, you can try the following: bash ssh username@hostname.local for example, ssh mjrovai@rpi-5.local , ssh mjrovai@raspi.local , etc.\n\n\n\nimg\n\n\nWhen you see the prompt:\nmjrovai@rpi-5:~ $\nIt means that you are interacting remotely with your Raspi. It is a good practice to update/upgrade the system regularly. For that, you should run:\nsudo apt-get update\nsudo apt upgrade\nYou should confirm the Raspi IP address. On the terminal, you can use:\nhostname -I\n\n\n\n\nTo shut down the Raspi via terminal:\nWhen you want to turn off your Raspberry Pi, there are better ideas than just pulling the power cord. This is because the Raspi may still be writing data to the SD card, in which case merely powering down may result in data loss or, even worse, a corrupted SD card.\nFor safety shut down, use the command line:\nsudo shutdown -h now\n\nTo avoid possible data loss and SD card corruption, before removing the power, you should wait a few seconds after shutdown for the Raspberry Pi’s LED to stop blinking and go dark. Once the LED goes out, it’s safe to power down.\n\n\n\nTransfer Files between the Raspi and a computer\nTransferring files between the Raspi and our main computer can be done using a pen drive, directly on the terminal (with scp), or an FTP program over the network.\n\nUsing Secure Copy Protocol (scp):\n\nCopy files to your Raspberry Pi\nLet’s create a text file on our computer, for example, test.txt.\n\n\nYou can use any text editor. In the same terminal, an option is the nano.\n\nTo copy the file named test.txt from your personal computer to a user’s home folder on your Raspberry Pi, run the following command from the directory containing test.txt, replacing the &lt;username&gt; placeholder with the username you use to log in to your Raspberry Pi and the &lt;pi_ip_address&gt; placeholder with your Raspberry Pi’s IP address:\n$ scp test.txt &lt;username&gt;@&lt;pi_ip_address&gt;:~/\n\nNote that ~/ means that we will move the file to the ROOT of our Raspi. You can choose any folder in your Raspi. But you should create the folder before you run scp, since scp won’t create folders automatically.\n\nFor example, let’s transfer the file test.txt to the ROOT of my Raspi-zero, which has an IP of 192.168.4.210:\nscp test.txt mjrovai@192.168.4.210:~/\n\nI use a different profile to differentiate the terminals. The above action happens on your computer. Now, let’s go to our Raspi (using the SSH) and check if the file is there:\n\n\n\nCopy files from your Raspberry Pi\nTo copy a file named test.txt from a user’s home directory on a Raspberry Pi to the current directory on another computer, run the following command on your Host Computer:\n$ scp &lt;username&gt;@&lt;pi_ip_address&gt;:myfile.txt .\nFor example:\nOn the Raspi, let’s create a copy of the file with another name:\ncp test.txt test_2.txt\nAnd on the Host Computer (in my case, a Mac)\nscp mjrovai@192.168.4.210:test_2.txt .\n\n\n\n\nTransferring files using FTP\nTransferring files using FTP, such as FileZilla FTP Client, is also possible. Follow the instructions, install the program for your Desktop OS, and use the Raspi IP address as the Host. For example:\nsftp://192.168.4.210\nand enter your Raspi username and password. Pressing Quickconnect will open two windows, one for your host computer desktop (right) and another for the Raspi (left).",
    "crumbs": [
      "Raspberry Pi",
      "Setup"
    ]
  },
  {
    "objectID": "contents/labs/raspi/setup/setup.html#increasing-swap-memory",
    "href": "contents/labs/raspi/setup/setup.html#increasing-swap-memory",
    "title": "Setup",
    "section": "Increasing SWAP Memory",
    "text": "Increasing SWAP Memory\nUsing htop, a cross-platform interactive process viewer, you can easily monitor the resources running on your Raspi, such as the list of processes, the running CPUs, and the memory used in real-time. To lunch hop, enter with the command on the terminal:\nhtop\n\nRegarding memory, among the devices in the Raspberry Pi family, the Raspi-Zero has the smallest amount of SRAM (500MB), compared to a selection of 2GB to 8GB on the Raspis 4 or 5. For any Raspi, it is possible to increase the memory available to the system with “Swap.” Swap memory, also known as swap space, is a technique used in computer operating systems to temporarily store data from RAM (Random Access Memory) on the SD card when the physical RAM is fully utilized. This allows the operating system (OS) to continue running even when RAM is full, which can prevent system crashes or slowdowns.\nSwap memory benefits devices with limited RAM, such as the Raspi-Zero. Increasing swap can help run more demanding applications or processes, but it’s essential to balance this with the potential performance impact of frequent disk access.\nBy default, the Rapi-Zero’s SWAP (Swp) memory is only 100MB, which is very small for running some more complex and demanding Machine Learning applications (for example, YOLO). Let’s increase it to 2MB:\nFirst, turn off swap-file:\nsudo dphys-swapfile swapoff\nNext, you should open and change the file /etc/dphys-swapfile. For that, we will use the nano:\nsudo nano /etc/dphys-swapfile\nSearch for the CONF_SWAPSIZE variable (default is 200) and update it to 2000:\nCONF_SWAPSIZE=2000\nAnd save the file.\nNext, turn on the swapfile again and reboot the Raspi-zero:\nsudo dphys-swapfile setup\nsudo dphys-swapfile swapon\nsudo reboot\nWhen your device is rebooted (you should enter with the SSH again), you will realize that the maximum swap memory value shown on top is now something near 2GB (in my case, 1.95GB).\n\nTo keep the htop running, you should open another terminal window to interact continuously with your Raspi.",
    "crumbs": [
      "Raspberry Pi",
      "Setup"
    ]
  },
  {
    "objectID": "contents/labs/raspi/setup/setup.html#installing-a-camera",
    "href": "contents/labs/raspi/setup/setup.html#installing-a-camera",
    "title": "Setup",
    "section": "Installing a Camera",
    "text": "Installing a Camera\nThe Raspi is an excellent device for computer vision applications; a camera is needed for it. We can install a standard USB webcam on the micro-USB port using a USB OTG adapter (Raspi-Zero and Raspi-5) or a camera module connected to the Raspi CSI (Camera Serial Interface) port.\n\nUSB Webcams generally have inferior quality to the camera modules that connect to the CSI port. They can also not be controlled using the raspistill and raspivid commands in the terminal or the picamera recording package in Python. Nevertheless, there may be reasons why you want to connect a USB camera to your Raspberry Pi, such as because of the benefit that it is much easier to set up multiple cameras with a single Raspberry Pi, long cables, or simply because you have such a camera on hand.\n\n\nInstalling a USB WebCam\n\nPower off the Raspi:\n\nsudo shutdown -h no\n\nConnect the USB Webcam (USB Camera Module 30fps,1280x720) to your Raspi (In this example, I am using the Raspi-Zero, but the instructions work for all Raspis).\n\n\n\nPower on again and run the SSH\nTo check if your USB camera is recognized, run:\n\nlsusb\nYou should see your camera listed in the output.\n\n\nTo take a test picture with your USB camera, use:\n\nfswebcam test_image.jpg\nThis will save an image named “test_image.jpg” in your current directory.\n\n\nSince we are using SSH to connect to our Rapsi, we must transfer the image to our main computer so we can view it. We can use FileZilla or SCP for this:\n\nOpen a terminal on your host computer and run:\nscp mjrovai@raspi-zero.local:~/test_image.jpg .\n\nReplace “mjrovai” with your username and “raspi-zero” with Pi’s hostname.\n\n\n\nIf the image quality isn’t satisfactory, you can adjust various settings; for example, define a resolution that is suitable for YOLO (640x640):\n\nfswebcam -r 640x640 --no-banner test_image_yolo.jpg\nThis captures a higher-resolution image without the default banner.\n\nAn ordinary USB Webcam can also be used:\n\nAnd verified using lsusb\n\n\nVideo Streaming\nFor stream video (which is more resource-intensive), we can install and use mjpg-streamer:\nFirst, install Git:\nsudo apt install git\nNow, we should install the necessary dependencies for mjpg-streamer, clone the repository, and proceed with the installation:\nsudo apt install cmake libjpeg62-turbo-dev\ngit clone https://github.com/jacksonliam/mjpg-streamer.git\ncd mjpg-streamer/mjpg-streamer-experimental\nmake\nsudo make install\nThen start the stream with:\nmjpg_streamer -i \"input_uvc.so\" -o \"output_http.so -w ./www\"\nWe can then access the stream by opening a web browser and navigating to:\nhttp://&lt;your_pi_ip_address&gt;:8080. In my case: http://192.168.4.210:8080\nWe should see a webpage with options to view the stream. Click on the link that says “Stream” or try accessing:\nhttp://&lt;raspberry_pi_ip_address&gt;:8080/?action=stream\n\n\n\n\nInstalling a Camera Module on the CSI port\nThere are now several Raspberry Pi camera modules. The original 5-megapixel model was released in 2013, followed by an 8-megapixel Camera Module 2 that was later released in 2016. The latest camera model is the 12-megapixel Camera Module 3, released in 2023.\nThe original 5MP camera (Arducam OV5647) is no longer available from Raspberry Pi but can be found from several alternative suppliers. Below is an example of such a camera on a Raspi-Zero.\n\nHere is another example of a v2 Camera Module, which has a Sony IMX219 8-megapixel sensor:\n\nAny camera module will work on the Raspberry Pis, but for that, the configuration.txt file must be updated:\nsudo nano /boot/firmware/config.txt\nAt the bottom of the file, for example, to use the 5MP Arducam OV5647 camera, add the line:\ndtoverlay=ov5647,cam0\nOr for the v2 module, wich has the 8MP Sony IMX219 camera:\ndtoverlay=imx219,cam0\nSave the file (CTRL+O [ENTER] CRTL+X) and reboot the Raspi:\nSudo reboot\nAfter the boot, you can see if the camera is listed:\nlibcamera-hello --list-cameras\n\n\n\nlibcamera is an open-source software library that supports camera systems directly from the Linux operating system on Arm processors. It minimizes proprietary code running on the Broadcom GPU.\n\nLet’s capture a jpeg image with a resolution of 640 x 480 for testing and save it to a file named test_cli_camera.jpg\nrpicam-jpeg --output test_cli_camera.jpg --width 640 --height 480\nif we want to see the file saved, we should use ls -f, which lists all current directory content in long format. As before, we can use scp to view the image:",
    "crumbs": [
      "Raspberry Pi",
      "Setup"
    ]
  },
  {
    "objectID": "contents/labs/raspi/setup/setup.html#running-the-raspi-desktop-remotely",
    "href": "contents/labs/raspi/setup/setup.html#running-the-raspi-desktop-remotely",
    "title": "Setup",
    "section": "Running the Raspi Desktop remotely",
    "text": "Running the Raspi Desktop remotely\nWhile we’ve primarily interacted with the Raspberry Pi using terminal commands via SSH, we can access the whole graphical desktop environment remotely if we have installed the complete Raspberry Pi OS (for example, Raspberry Pi OS (64-bit). This can be particularly useful for tasks that benefit from a visual interface. To enable this functionality, we must set up a VNC (Virtual Network Computing) server on the Raspberry Pi. Here’s how to do it:\n\nEnable the VNC Server:\n\nConnect to your Raspberry Pi via SSH.\nRun the Raspberry Pi configuration tool by entering:\nsudo raspi-config\nNavigate to Interface Options using the arrow keys.\n\n\n\nSelect VNC and Yes to enable the VNC server.\n\n\n\nExit the configuration tool, saving changes when prompted.\n\n\nInstall a VNC Viewer on Your Computer:\n\nDownload and install a VNC viewer application on your main computer. Popular options include RealVNC Viewer, TightVNC, or VNC Viewer by RealVNC. We will install VNC Viewer by RealVNC.\n\nOnce installed, you should confirm the Raspi IP address. For example, on the terminal, you can use:\nhostname -I\n\nConnect to Your Raspberry Pi:\n\nOpen your VNC viewer application.\n\n\n\nEnter your Raspberry Pi’s IP address and hostname.\nWhen prompted, enter your Raspberry Pi’s username and password.\n\n\nThe Raspberry Pi 5 Desktop should appear on your computer monitor.\n\nAdjust Display Settings (if needed):\n\nOnce connected, adjust the display resolution for optimal viewing. This can be done through the Raspberry Pi’s desktop settings or by modifying the config.txt file.\nLet’s do it using the desktop settings. Reach the menu (the Raspberry Icon at the left upper corner) and select the best screen definition for your monitor:",
    "crumbs": [
      "Raspberry Pi",
      "Setup"
    ]
  },
  {
    "objectID": "contents/labs/raspi/setup/setup.html#updating-and-installing-software",
    "href": "contents/labs/raspi/setup/setup.html#updating-and-installing-software",
    "title": "Setup",
    "section": "Updating and Installing Software",
    "text": "Updating and Installing Software\n\nUpdate your system:\nsudo apt update && sudo apt upgrade -y\nInstall essential software:\nsudo apt install python3-pip -y\nEnable pip for Python projects:\nsudo rm /usr/lib/python3.11/EXTERNALLY-MANAGED",
    "crumbs": [
      "Raspberry Pi",
      "Setup"
    ]
  },
  {
    "objectID": "contents/labs/raspi/setup/setup.html#model-specific-considerations",
    "href": "contents/labs/raspi/setup/setup.html#model-specific-considerations",
    "title": "Setup",
    "section": "Model-Specific Considerations",
    "text": "Model-Specific Considerations\n\nRaspberry Pi Zero (Raspi-Zero)\n\nLimited processing power, best for lightweight projects\nIt is better to use a headless setup (SSH) to conserve resources.\nConsider increasing swap space for memory-intensive tasks.\nIt can be used for Image Classification and Object Detection Labs but not for the LLM (SLM).\n\n\n\nRaspberry Pi 4 or 5 (Raspi-4 or Raspi-5)\n\nSuitable for more demanding projects, including AI and machine learning.\nIt can run the whole desktop environment smoothly.\nRaspi-4 can be used for Image Classification and Object Detection Labs but will not work well with LLMs (SLM).\nFor Raspi-5, consider using an active cooler for temperature management during intensive tasks, as in the LLMs (SLMs) lab.\n\nRemember to adjust your project requirements based on the specific Raspberry Pi model you’re using. The Raspi-Zero is great for low-power, space-constrained projects, while the Raspi-4 or 5 models are better suited for more computationally intensive tasks.",
    "crumbs": [
      "Raspberry Pi",
      "Setup"
    ]
  },
  {
    "objectID": "contents/labs/raspi/image_classification/image_classification.html",
    "href": "contents/labs/raspi/image_classification/image_classification.html",
    "title": "Image Classification",
    "section": "",
    "text": "Overview\nImage classification is a fundamental task in computer vision that involves categorizing an image into one of several predefined classes. It’s a cornerstone of artificial intelligence, enabling machines to interpret and understand visual information in a way that mimics human perception.\nImage classification refers to assigning a label or category to an entire image based on its visual content. This task is crucial in computer vision and has numerous applications across various industries. Image classification’s importance lies in its ability to automate visual understanding tasks that would otherwise require human intervention.",
    "crumbs": [
      "Raspberry Pi",
      "Image Classification"
    ]
  },
  {
    "objectID": "contents/labs/raspi/image_classification/image_classification.html#overview",
    "href": "contents/labs/raspi/image_classification/image_classification.html#overview",
    "title": "Image Classification",
    "section": "",
    "text": "Applications in Real-World Scenarios\nImage classification has found its way into numerous real-world applications, revolutionizing various sectors:\n\nHealthcare: Assisting in medical image analysis, such as identifying abnormalities in X-rays or MRIs.\nAgriculture: Monitoring crop health and detecting plant diseases through aerial imagery.\nAutomotive: Enabling advanced driver assistance systems and autonomous vehicles to recognize road signs, pedestrians, and other vehicles.\nRetail: Powering visual search capabilities and automated inventory management systems.\nSecurity and Surveillance: Enhancing threat detection and facial recognition systems.\nEnvironmental Monitoring: Analyzing satellite imagery for deforestation, urban planning, and climate change studies.\n\n\n\nAdvantages of Running Classification on Edge Devices like Raspberry Pi\nImplementing image classification on edge devices such as the Raspberry Pi offers several compelling advantages:\n\nLow Latency: Processing images locally eliminates the need to send data to cloud servers, significantly reducing response times.\nOffline Functionality: Classification can be performed without an internet connection, making it suitable for remote or connectivity-challenged environments.\nPrivacy and Security: Sensitive image data remains on the local device, addressing data privacy concerns and compliance requirements.\nCost-Effectiveness: Eliminates the need for expensive cloud computing resources, especially for continuous or high-volume classification tasks.\nScalability: Enables distributed computing architectures where multiple devices can work independently or in a network.\nEnergy Efficiency: Optimized models on dedicated hardware can be more energy-efficient than cloud-based solutions, which is crucial for battery-powered or remote applications.\nCustomization: Deploying specialized or frequently updated models tailored to specific use cases is more manageable.\n\nWe can create more responsive, secure, and efficient computer vision solutions by leveraging the power of edge devices like Raspberry Pi for image classification. This approach opens up new possibilities for integrating intelligent visual processing into various applications and environments.\nIn the following sections, we’ll explore how to implement and optimize image classification on the Raspberry Pi, harnessing these advantages to create powerful and efficient computer vision systems.",
    "crumbs": [
      "Raspberry Pi",
      "Image Classification"
    ]
  },
  {
    "objectID": "contents/labs/raspi/image_classification/image_classification.html#setting-up-the-environment",
    "href": "contents/labs/raspi/image_classification/image_classification.html#setting-up-the-environment",
    "title": "Image Classification",
    "section": "Setting Up the Environment",
    "text": "Setting Up the Environment\n\nUpdating the Raspberry Pi\nFirst, ensure your Raspberry Pi is up to date:\nsudo apt update\nsudo apt upgrade -y\n\n\nInstalling Required Libraries\nInstall the necessary libraries for image processing and machine learning:\nsudo apt install python3-pip\nsudo rm /usr/lib/python3.11/EXTERNALLY-MANAGED\npip3 install --upgrade pip\n\n\nSetting up a Virtual Environment (Optional but Recommended)\nCreate a virtual environment to manage dependencies:\npython3 -m venv ~/tflite\nsource ~/tflite/bin/activate\n\n\nInstalling TensorFlow Lite\nWe are interested in performing inference, which refers to executing a TensorFlow Lite model on a device to make predictions based on input data. To perform an inference with a TensorFlow Lite model, we must run it through an interpreter. The TensorFlow Lite interpreter is designed to be lean and fast. The interpreter uses a static graph ordering and a custom (less-dynamic) memory allocator to ensure minimal load, initialization, and execution latency.\nWe’ll use the TensorFlow Lite runtime for Raspberry Pi, a simplified library for running machine learning models on mobile and embedded devices, without including all TensorFlow packages.\npip install tflite_runtime --no-deps\n\nThe wheel installed: tflite_runtime-2.14.0-cp311-cp311-manylinux_2_34_aarch64.whl\n\n\n\nInstalling Additional Python Libraries\nInstall required Python libraries for use with Image Classification:\nIf you have another version of Numpy installed, first uninstall it.\npip3 uninstall numpy\nInstall version 1.23.2, which is compatible with the tflite_runtime.\n pip3 install numpy==1.23.2\npip3 install Pillow matplotlib\n\n\nCreating a working directory:\nIf you are working on the Raspi-Zero with the minimum OS (No Desktop), you may not have a user-pre-defined directory tree (you can check it with ls. So, let’s create one:\nmkdir Documents\ncd Documents/\nmkdir TFLITE\ncd TFLITE/\nmkdir IMG_CLASS\ncd IMG_CLASS\nmkdir models\ncd models\n\nOn the Raspi-5, the /Documents should be there.\n\nGet a pre-trained Image Classification model:\nAn appropriate pre-trained model is crucial for successful image classification on resource-constrained devices like the Raspberry Pi. MobileNet is designed for mobile and embedded vision applications with a good balance between accuracy and speed. Versions: MobileNetV1, MobileNetV2, MobileNetV3. Let’s download the V2:\nwget https://storage.googleapis.com/download.tensorflow.org/models/\ntflite_11_05_08/mobilenet_v2_1.0_224_quant.tgz\n\ntar xzf mobilenet_v2_1.0_224_quant.tgz\nGet its labels:\nwget https://github.com/Mjrovai/EdgeML-with-Raspberry-Pi/blob/main/IMG_CLASS/models/labels.txt\nIn the end, you should have the models in its directory:\n\n\nWe will only need the mobilenet_v2_1.0_224_quant.tflite model and the labels.txt. You can delete the other files.\n\n\n\nSetting up Jupyter Notebook (Optional)\nIf you prefer using Jupyter Notebook for development:\npip3 install jupyter\njupyter notebook --generate-config\nTo run Jupyter Notebook, run the command (change the IP address for yours):\njupyter notebook --ip=192.168.4.210 --no-browser\nOn the terminal, you can see the local URL address to open the notebook:\n\nYou can access it from another device by entering the Raspberry Pi’s IP address and the provided token in a web browser (you can copy the token from the terminal).\n\nDefine your working directory in the Raspi and create a new Python 3 notebook.\n\n\nVerifying the Setup\nTest your setup by running a simple Python script:\nimport tflite_runtime.interpreter as tflite\nimport numpy as np\nfrom PIL import Image\n\nprint(\"NumPy:\", np.__version__)\nprint(\"Pillow:\", Image.__version__)\n\n# Try to create a TFLite Interpreter\nmodel_path = \"./models/mobilenet_v2_1.0_224_quant.tflite\"\ninterpreter = tflite.Interpreter(model_path=model_path)\ninterpreter.allocate_tensors()\nprint(\"TFLite Interpreter created successfully!\")\nYou can create the Python script using nano on the terminal, saving it with CTRL+0 + ENTER + CTRL+X\n\nAnd run it with the command:\n\nOr you can run it directly on the Notebook:",
    "crumbs": [
      "Raspberry Pi",
      "Image Classification"
    ]
  },
  {
    "objectID": "contents/labs/raspi/image_classification/image_classification.html#making-inferences-with-mobilenet-v2",
    "href": "contents/labs/raspi/image_classification/image_classification.html#making-inferences-with-mobilenet-v2",
    "title": "Image Classification",
    "section": "Making inferences with Mobilenet V2",
    "text": "Making inferences with Mobilenet V2\nIn the last section, we set up the environment, including downloading a popular pre-trained model, Mobilenet V2, trained on ImageNet’s 224x224 images (1.2 million) for 1,001 classes (1,000 object categories plus 1 background). The model was converted to a compact 3.5MB TensorFlow Lite format, making it suitable for the limited storage and memory of a Raspberry Pi.\n\nLet’s start a new notebook to follow all the steps to classify one image:\nImport the needed libraries:\nimport time\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom PIL import Image\nimport tflite_runtime.interpreter as tflite\nLoad the TFLite model and allocate tensors:\nmodel_path = \"./models/mobilenet_v2_1.0_224_quant.tflite\"\ninterpreter = tflite.Interpreter(model_path=model_path)\ninterpreter.allocate_tensors()\nGet input and output tensors.\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\nInput details will give us information about how the model should be fed with an image. The shape of (1, 224, 224, 3) informs us that an image with dimensions (224x224x3) should be input one by one (Batch Dimension: 1).\n\nThe output details show that the inference will result in an array of 1,001 integer values. Those values result from the image classification, where each value is the probability of that specific label being related to the image.\n\nLet’s also inspect the dtype of input details of the model\ninput_dtype = input_details[0]['dtype']\ninput_dtype\ndtype('uint8')\nThis shows that the input image should be raw pixels (0 - 255).\nLet’s get a test image. You can transfer it from your computer or download one for testing. Let’s first create a folder under our working directory:\nmkdir images\ncd images\nwget https://upload.wikimedia.org/wikipedia/commons/3/3a/Cat03.jpg\nLet’s load and display the image:\n# Load he image\nimg_path = \"./images/Cat03.jpg\"\nimg = Image.open(img_path)\n\n# Display the image\nplt.figure(figsize=(8, 8))\nplt.imshow(img)\nplt.title(\"Original Image\")\nplt.show()\n\nWe can see the image size running the command:\nwidth, height = img.size\nThat shows us that the image is an RGB image with a width of 1600 and a height of 1600 pixels. So, to use our model, we should reshape it to (224, 224, 3) and add a batch dimension of 1, as defined in input details: (1, 224, 224, 3). The inference result, as shown in output details, will be an array with a 1001 size, as shown below:\n\nSo, let’s reshape the image, add the batch dimension, and see the result:\nimg = img.resize((input_details[0]['shape'][1], input_details[0]['shape'][2]))\ninput_data = np.expand_dims(img, axis=0)\ninput_data.shape\nThe input_data shape is as expected: (1, 224, 224, 3)\nLet’s confirm the dtype of the input data:\ninput_data.dtype\ndtype('uint8')\nThe input data dtype is ‘uint8’, which is compatible with the dtype expected for the model.\nUsing the input_data, let’s run the interpreter and get the predictions (output):\ninterpreter.set_tensor(input_details[0]['index'], input_data)\ninterpreter.invoke()\npredictions = interpreter.get_tensor(output_details[0]['index'])[0]\nThe prediction is an array with 1001 elements. Let’s get the Top-5 indices where their elements have high values:\ntop_k_results = 5\ntop_k_indices = np.argsort(predictions)[::-1][:top_k_results]\ntop_k_indices \nThe top_k_indices is an array with 5 elements: array([283, 286, 282])\nSo, 283, 286, 282, 288, and 479 are the image’s most probable classes. Having the index, we must find to what class it appoints (such as car, cat, or dog). The text file downloaded with the model has a label associated with each index from 0 to 1,000. Let’s use a function to load the .txt file as a list:\ndef load_labels(filename):\n    with open(filename, 'r') as f:\n        return [line.strip() for line in f.readlines()]\nAnd get the list, printing the labels associated with the indexes:\nlabels_path = \"./models/labels.txt\"\nlabels = load_labels(labels_path)\n\nprint(labels[286])\nprint(labels[283])\nprint(labels[282])\nprint(labels[288])\nprint(labels[479])\nAs a result, we have:\nEgyptian cat\ntiger cat\ntabby\nlynx\ncarton\nAt least the four top indices are related to felines. The prediction content is the probability associated with each one of the labels. As we saw on output details, those values are quantized and should be dequantized and apply softmax.\nscale, zero_point = output_details[0]['quantization']\ndequantized_output = (predictions.astype(np.float32) - zero_point) * scale\nexp_output = np.exp(dequantized_output - np.max(dequantized_output))\nprobabilities = exp_output / np.sum(exp_output)\nLet’s print the top-5 probabilities:\nprint (probabilities[286])\nprint (probabilities[283])\nprint (probabilities[282])\nprint (probabilities[288])\nprint (probabilities[479])\n0.27741462\n0.3732285\n0.16919471\n0.10319158\n0.023410844\nFor clarity, let’s create a function to relate the labels with the probabilities:\nfor i in range(top_k_results):\n    print(\"\\t{:20}: {}%\".format(\n        labels[top_k_indices[i]],\n        (int(probabilities[top_k_indices[i]]*100))))\ntiger cat           : 37%\nEgyptian cat        : 27%\ntabby               : 16%\nlynx                : 10%\ncarton              : 2%\n\nDefine a general Image Classification function\nLet’s create a general function to give an image as input, and we get the Top-5 possible classes:\n\ndef image_classification(img_path, model_path, labels, top_k_results=5):\n    # load the image\n    img = Image.open(img_path)\n    plt.figure(figsize=(4, 4))\n    plt.imshow(img)\n    plt.axis('off')\n\n    # Load the TFLite model\n    interpreter = tflite.Interpreter(model_path=model_path)\n    interpreter.allocate_tensors()\n    \n    # Get input and output tensors\n    input_details = interpreter.get_input_details()\n    output_details = interpreter.get_output_details()\n    \n    # Preprocess\n    img = img.resize((input_details[0]['shape'][1], \n                      input_details[0]['shape'][2]))\n    input_data = np.expand_dims(img, axis=0)\n    \n    # Inference on Raspi-Zero\n    interpreter.set_tensor(input_details[0]['index'], input_data)\n    interpreter.invoke()\n    \n    # Obtain results and map them to the classes\n    predictions = interpreter.get_tensor(output_details[0]['index'])[0]\n    \n    # Get indices of the top k results\n    top_k_indices = np.argsort(predictions)[::-1][:top_k_results]\n    \n    # Get quantization parameters\n    scale, zero_point = output_details[0]['quantization']\n    \n    # Dequantize the output and apply softmax\n    dequantized_output = (predictions.astype(np.float32) - zero_point) * scale\n    exp_output = np.exp(dequantized_output - np.max(dequantized_output))\n    probabilities = exp_output / np.sum(exp_output)\n    \n    print(\"\\n\\t[PREDICTION]        [Prob]\\n\")\n    for i in range(top_k_results):\n        print(\"\\t{:20}: {}%\".format(\n            labels[top_k_indices[i]],\n            (int(probabilities[top_k_indices[i]]*100))))\n\nAnd loading some images for testing, we have:\n\n\n\nTesting with a model trained from scratch\nLet’s get a TFLite model trained from scratch. For that, you can follow the Notebook:\nCNN to classify Cifar-10 dataset\nIn the notebook, we trained a model using the CIFAR10 dataset, which contains 60,000 images from 10 classes of CIFAR (airplane, automobile, bird, cat, deer, dog, frog, horse, ship, and truck). CIFAR has 32x32 color images (3 color channels) where the objects are not centered and can have the object with a background, such as airplanes that might have a cloudy sky behind them! In short, small but real images.\nThe CNN trained model (cifar10_model.keras) had a size of 2.0MB. Using the TFLite Converter, the model cifar10.tflite became with 674MB (around 1/3 of the original size).\n\nOn the notebook Cifar 10 - Image Classification on a Raspi with TFLite (which can be run over the Raspi), we can follow the same steps we did with the mobilenet_v2_1.0_224_quant.tflite. Below are examples of images using the General Function for Image Classification on a Raspi-Zero, as shown in the last section.\n\n\n\nInstalling Picamera2\nPicamera2, a Python library for interacting with Raspberry Pi’s camera, is based on the libcamera camera stack, and the Raspberry Pi foundation maintains it. The Picamera2 library is supported on all Raspberry Pi models, from the Pi Zero to the RPi 5. It is already installed system-wide on the Raspi, but we should make it accessible within the virtual environment.\n\nFirst, activate the virtual environment if it’s not already activated:\nsource ~/tflite/bin/activate\nNow, let’s create a .pth file in your virtual environment to add the system site-packages path:\necho \"/usr/lib/python3/dist-packages\" &gt; $VIRTUAL_ENV/lib/python3.11/\nsite-packages/system_site_packages.pth\n\nNote: If your Python version differs, replace python3.11 with the appropriate version.\n\nAfter creating this file, try importing picamera2 in Python:\npython3\n&gt;&gt;&gt; import picamera2\n&gt;&gt;&gt; print(picamera2.__file__)\n\nThe above code will show the file location of the picamera2 module itself, proving that the library can be accessed from the environment.\n/home/mjrovai/tflite/lib/python3.11/site-packages/picamera2/__init__.py\nYou can also list the available cameras in the system:\n&gt;&gt;&gt; print(Picamera2.global_camera_info())\nIn my case, with a USB installed, I got:\n\nNow that we’ve confirmed picamera2 is working in the environment with an index 0, let’s try a simple Python script to capture an image from your USB camera:\nfrom picamera2 import Picamera2\nimport time\n\n# Initialize the camera\npicam2 = Picamera2() # default is index 0\n\n# Configure the camera\nconfig = picam2.create_still_configuration(main={\"size\": (640, 480)})\npicam2.configure(config)\n\n# Start the camera\npicam2.start()\n\n# Wait for the camera to warm up\ntime.sleep(2)\n\n# Capture an image\npicam2.capture_file(\"usb_camera_image.jpg\")\nprint(\"Image captured and saved as 'usb_camera_image.jpg'\")\n\n# Stop the camera\npicam2.stop()\nUse the Nano text editor, the Jupyter Notebook, or any other editor. Save this as a Python script (e.g., capture_image.py) and run it. This should capture an image from your camera and save it as “usb_camera_image.jpg” in the same directory as your script.\n\nIf the Jupyter is open, you can see the captured image on your computer. Otherwise, transfer the file from the Raspi to your computer.\n\n\nIf you are working with a Raspi-5 with a whole desktop, you can open the file directly on the device.",
    "crumbs": [
      "Raspberry Pi",
      "Image Classification"
    ]
  },
  {
    "objectID": "contents/labs/raspi/image_classification/image_classification.html#image-classification-project",
    "href": "contents/labs/raspi/image_classification/image_classification.html#image-classification-project",
    "title": "Image Classification",
    "section": "Image Classification Project",
    "text": "Image Classification Project\nNow, we will develop a complete Image Classification project using the Edge Impulse Studio. As we did with the Movilinet V2, the trained and converted TFLite model will be used for inference.\n\nThe Goal\nThe first step in any ML project is to define its goal. In this case, it is to detect and classify two specific objects present in one image. For this project, we will use two small toys: a robot and a small Brazilian parrot (named Periquito). We will also collect images of a background where those two objects are absent.\n\n\n\nData Collection\nOnce we have defined our Machine Learning project goal, the next and most crucial step is collecting the dataset. We can use a phone for the image capture, but we will use the Raspi here. Let’s set up a simple web server on our Raspberry Pi to view the QVGA (320 x 240) captured images in a browser.\n\nFirst, let’s install Flask, a lightweight web framework for Python:\npip3 install flask\nLet’s create a new Python script combining image capture with a web server. We’ll call it get_img_data.py:\n\n\nfrom flask import Flask, Response, render_template_string, request, redirect, url_for\nfrom picamera2 import Picamera2\nimport io\nimport threading\nimport time\nimport os\nimport signal\n\napp = Flask(__name__)\n\n# Global variables\nbase_dir = \"dataset\"\npicam2 = None\nframe = None\nframe_lock = threading.Lock()\ncapture_counts = {}\ncurrent_label = None\nshutdown_event = threading.Event()\n\ndef initialize_camera():\n    global picam2\n    picam2 = Picamera2()\n    config = picam2.create_preview_configuration(main={\"size\": (320, 240)})\n    picam2.configure(config)\n    picam2.start()\n    time.sleep(2)  # Wait for camera to warm up\n\ndef get_frame():\n    global frame\n    while not shutdown_event.is_set():\n        stream = io.BytesIO()\n        picam2.capture_file(stream, format='jpeg')\n        with frame_lock:\n            frame = stream.getvalue()\n        time.sleep(0.1)  # Adjust as needed for smooth preview\n\ndef generate_frames():\n    while not shutdown_event.is_set():\n        with frame_lock:\n            if frame is not None:\n                yield (b'--frame\\r\\n'\n                       b'Content-Type: image/jpeg\\r\\n\\r\\n' + frame + b'\\r\\n')\n        time.sleep(0.1)  # Adjust as needed for smooth streaming\n\ndef shutdown_server():\n    shutdown_event.set()\n    if picam2:\n        picam2.stop()\n    # Give some time for other threads to finish\n    time.sleep(2)\n    # Send SIGINT to the main process\n    os.kill(os.getpid(), signal.SIGINT)\n\n@app.route('/', methods=['GET', 'POST'])\ndef index():\n    global current_label\n    if request.method == 'POST':\n        current_label = request.form['label']\n        if current_label not in capture_counts:\n            capture_counts[current_label] = 0\n        os.makedirs(os.path.join(base_dir, current_label), exist_ok=True)\n        return redirect(url_for('capture_page'))\n    return render_template_string('''\n        &lt;!DOCTYPE html&gt;\n        &lt;html&gt;\n        &lt;head&gt;\n            &lt;title&gt;Dataset Capture - Label Entry&lt;/title&gt;\n        &lt;/head&gt;\n        &lt;body&gt;\n            &lt;h1&gt;Enter Label for Dataset&lt;/h1&gt;\n            &lt;form method=\"post\"&gt;\n                &lt;input type=\"text\" name=\"label\" required&gt;\n                &lt;input type=\"submit\" value=\"Start Capture\"&gt;\n            &lt;/form&gt;\n        &lt;/body&gt;\n        &lt;/html&gt;\n    ''')\n\n@app.route('/capture')\ndef capture_page():\n    return render_template_string('''\n        &lt;!DOCTYPE html&gt;\n        &lt;html&gt;\n        &lt;head&gt;\n            &lt;title&gt;Dataset Capture&lt;/title&gt;\n            &lt;script&gt;\n                var shutdownInitiated = false;\n                function checkShutdown() {\n                    if (!shutdownInitiated) {\n                        fetch('/check_shutdown')\n                            .then(response =&gt; response.json())\n                            .then(data =&gt; {\n                                if (data.shutdown) {\n                                    shutdownInitiated = true;\n                                    document.getElementById('video-feed').src = '';\n                                    document.getElementById('shutdown-message')\n                                    .style.display = 'block';\n                                }\n                            });\n                    }\n                }\n                setInterval(checkShutdown, 1000);  // Check every second\n            &lt;/script&gt;\n        &lt;/head&gt;\n        &lt;body&gt;\n            &lt;h1&gt;Dataset Capture&lt;/h1&gt;\n            &lt;p&gt;Current Label: {{ label }}&lt;/p&gt;\n            &lt;p&gt;Images captured for this label: {{ capture_count }}&lt;/p&gt;\n            &lt;img id=\"video-feed\" src=\"{{ url_for('video_feed') }}\" width=\"640\" \n            height=\"480\" /&gt;\n            &lt;div id=\"shutdown-message\" style=\"display: none; color: red;\"&gt;\n                Capture process has been stopped. You can close this window.\n            &lt;/div&gt;\n            &lt;form action=\"/capture_image\" method=\"post\"&gt;\n                &lt;input type=\"submit\" value=\"Capture Image\"&gt;\n            &lt;/form&gt;\n            &lt;form action=\"/stop\" method=\"post\"&gt;\n                &lt;input type=\"submit\" value=\"Stop Capture\" \n                style=\"background-color: #ff6666;\"&gt;\n            &lt;/form&gt;\n            &lt;form action=\"/\" method=\"get\"&gt;\n                &lt;input type=\"submit\" value=\"Change Label\" \n                style=\"background-color: #ffff66;\"&gt;\n            &lt;/form&gt;\n        &lt;/body&gt;\n        &lt;/html&gt;\n    ''', label=current_label, capture_count=capture_counts.get(current_label, 0))\n\n@app.route('/video_feed')\ndef video_feed():\n    return Response(generate_frames(),\n                    mimetype='multipart/x-mixed-replace; boundary=frame')\n\n@app.route('/capture_image', methods=['POST'])\ndef capture_image():\n    global capture_counts\n    if current_label and not shutdown_event.is_set():\n        capture_counts[current_label] += 1\n        timestamp = time.strftime(\"%Y%m%d-%H%M%S\")\n        filename = f\"image_{timestamp}.jpg\"\n        full_path = os.path.join(base_dir, current_label, filename)\n        \n        picam2.capture_file(full_path)\n    \n    return redirect(url_for('capture_page'))\n\n@app.route('/stop', methods=['POST'])\ndef stop():\n    summary = render_template_string('''\n        &lt;!DOCTYPE html&gt;\n        &lt;html&gt;\n        &lt;head&gt;\n            &lt;title&gt;Dataset Capture - Stopped&lt;/title&gt;\n        &lt;/head&gt;\n        &lt;body&gt;\n            &lt;h1&gt;Dataset Capture Stopped&lt;/h1&gt;\n            &lt;p&gt;The capture process has been stopped. You can close this window.&lt;/p&gt;\n            &lt;p&gt;Summary of captures:&lt;/p&gt;\n            &lt;ul&gt;\n            {% for label, count in capture_counts.items() %}\n                &lt;li&gt;{{ label }}: {{ count }} images&lt;/li&gt;\n            {% endfor %}\n            &lt;/ul&gt;\n        &lt;/body&gt;\n        &lt;/html&gt;\n    ''', capture_counts=capture_counts)\n    \n    # Start a new thread to shutdown the server\n    threading.Thread(target=shutdown_server).start()\n    \n    return summary\n\n@app.route('/check_shutdown')\ndef check_shutdown():\n    return {'shutdown': shutdown_event.is_set()}\n\nif __name__ == '__main__':\n    initialize_camera()\n    threading.Thread(target=get_frame, daemon=True).start()\n    app.run(host='0.0.0.0', port=5000, threaded=True)\n\n\nRun this script:\npython3 get_img_data.py\nAccess the web interface:\n\nOn the Raspberry Pi itself (if you have a GUI): Open a web browser and go to http://localhost:5000\nFrom another device on the same network: Open a web browser and go to http://&lt;raspberry_pi_ip&gt;:5000 (Replace &lt;raspberry_pi_ip&gt; with your Raspberry Pi’s IP address). For example: http://192.168.4.210:5000/\n\n\nThis Python script creates a web-based interface for capturing and organizing image datasets using a Raspberry Pi and its camera. It’s handy for machine learning projects that require labeled image data.\n\nKey Features:\n\nWeb Interface: Accessible from any device on the same network as the Raspberry Pi.\nLive Camera Preview: This shows a real-time feed from the camera.\nLabeling System: Allows users to input labels for different categories of images.\nOrganized Storage: Automatically saves images in label-specific subdirectories.\nPer-Label Counters: Keeps track of how many images are captured for each label.\nSummary Statistics: Provides a summary of captured images when stopping the capture process.\n\n\n\nMain Components:\n\nFlask Web Application: Handles routing and serves the web interface.\nPicamera2 Integration: Controls the Raspberry Pi camera.\nThreaded Frame Capture: Ensures smooth live preview.\nFile Management: Organizes captured images into labeled directories.\n\n\n\nKey Functions:\n\ninitialize_camera(): Sets up the Picamera2 instance.\nget_frame(): Continuously captures frames for the live preview.\ngenerate_frames(): Yields frames for the live video feed.\nshutdown_server(): Sets the shutdown event, stops the camera, and shuts down the Flask server\nindex(): Handles the label input page.\ncapture_page(): Displays the main capture interface.\nvideo_feed(): Shows a live preview to position the camera\ncapture_image(): Saves an image with the current label.\nstop(): Stops the capture process and displays a summary.\n\n\n\nUsage Flow:\n\nStart the script on your Raspberry Pi.\nAccess the web interface from a browser.\nEnter a label for the images you want to capture and press Start Capture.\n\n\n\nUse the live preview to position the camera.\nClick Capture Image to save images under the current label.\n\n\n\nChange labels as needed for different categories, selecting Change Label.\nClick Stop Capture when finished to see a summary.\n\n\n\n\nTechnical Notes:\n\nThe script uses threading to handle concurrent frame capture and web serving.\nImages are saved with timestamps in their filenames for uniqueness.\nThe web interface is responsive and can be accessed from mobile devices.\n\n\n\nCustomization Possibilities:\n\nAdjust image resolution in the initialize_camera() function. Here we used QVGA (320X240).\nModify the HTML templates for a different look and feel.\nAdd additional image processing or analysis steps in the capture_image() function.\n\n\n\nNumber of samples on Dataset:\nGet around 60 images from each category (periquito, robot and background). Try to capture different angles, backgrounds, and light conditions. On the Raspi, we will end with a folder named dataset, witch contains 3 sub-folders periquito, robot, and background. one for each class of images.\nYou can use Filezilla to transfer the created dataset to your main computer.",
    "crumbs": [
      "Raspberry Pi",
      "Image Classification"
    ]
  },
  {
    "objectID": "contents/labs/raspi/image_classification/image_classification.html#training-the-model-with-edge-impulse-studio",
    "href": "contents/labs/raspi/image_classification/image_classification.html#training-the-model-with-edge-impulse-studio",
    "title": "Image Classification",
    "section": "Training the model with Edge Impulse Studio",
    "text": "Training the model with Edge Impulse Studio\nWe will use the Edge Impulse Studio to train our model. Go to the Edge Impulse Page, enter your account credentials, and create a new project:\n\n\nHere, you can clone a similar project: Raspi - Img Class.\n\n\nDataset\nWe will walk through four main steps using the EI Studio (or Studio). These steps are crucial in preparing our model for use on the Raspi: Dataset, Impulse, Tests, and Deploy (on the Edge Device, in this case, the Raspi).\n\nRegarding the Dataset, it is essential to point out that our Original Dataset, captured with the Raspi, will be split into Training, Validation, and Test. The Test Set will be separated from the beginning and reserved for use only in the Test phase after training. The Validation Set will be used during training.\n\nOn Studio, follow the steps to upload the captured data:\n\nGo to the Data acquisition tab, and in the UPLOAD DATA section, upload the files from your computer in the chosen categories.\nLeave to the Studio the splitting of the original dataset into train and test and choose the label about\nRepeat the procedure for all three classes. At the end, you should see your “raw data” in the Studio:\n\n\nThe Studio allows you to explore your data, showing a complete view of all the data in your project. You can clear, inspect, or change labels by clicking on individual data items. In our case, a straightforward project, the data seems OK.",
    "crumbs": [
      "Raspberry Pi",
      "Image Classification"
    ]
  },
  {
    "objectID": "contents/labs/raspi/image_classification/image_classification.html#the-impulse-design",
    "href": "contents/labs/raspi/image_classification/image_classification.html#the-impulse-design",
    "title": "Image Classification",
    "section": "The Impulse Design",
    "text": "The Impulse Design\nIn this phase, we should define how to:\n\nPre-process our data, which consists of resizing the individual images and determining the color depth to use (be it RGB or Grayscale) and\nSpecify a Model. In this case, it will be the Transfer Learning (Images) to fine-tune a pre-trained MobileNet V2 image classification model on our data. This method performs well even with relatively small image datasets (around 180 images in our case).\n\nTransfer Learning with MobileNet offers a streamlined approach to model training, which is especially beneficial for resource-constrained environments and projects with limited labeled data. MobileNet, known for its lightweight architecture, is a pre-trained model that has already learned valuable features from a large dataset (ImageNet).\n\nBy leveraging these learned features, we can train a new model for your specific task with fewer data and computational resources and achieve competitive accuracy.\n\nThis approach significantly reduces training time and computational cost, making it ideal for quick prototyping and deployment on embedded devices where efficiency is paramount.\nGo to the Impulse Design Tab and create the impulse, defining an image size of 160x160 and squashing them (squared form, without cropping). Select Image and Transfer Learning blocks. Save the Impulse.\n\n\nImage Pre-Processing\nAll the input QVGA/RGB565 images will be converted to 76,800 features (160x160x3).\n\nPress Save parameters and select Generate features in the next tab.\n\n\nModel Design\nMobileNet is a family of efficient convolutional neural networks designed for mobile and embedded vision applications. The key features of MobileNet are:\n\nLightweight: Optimized for mobile devices and embedded systems with limited computational resources.\nSpeed: Fast inference times, suitable for real-time applications.\nAccuracy: Maintains good accuracy despite its compact size.\n\nMobileNetV2, introduced in 2018, improves the original MobileNet architecture. Key features include:\n\nInverted Residuals: Inverted residual structures are used where shortcut connections are made between thin bottleneck layers.\nLinear Bottlenecks: Removes non-linearities in the narrow layers to prevent the destruction of information.\nDepth-wise Separable Convolutions: Continues to use this efficient operation from MobileNetV1.\n\nIn our project, we will do a Transfer Learning with the MobileNetV2 160x160 1.0, which means that the images used for training (and future inference) should have an input Size of 160x160 pixels and a Width Multiplier of 1.0 (full width, not reduced). This configuration balances between model size, speed, and accuracy.\n\n\nModel Training\nAnother valuable deep learning technique is Data Augmentation. Data augmentation improves the accuracy of machine learning models by creating additional artificial data. A data augmentation system makes small, random changes to the training data during the training process (such as flipping, cropping, or rotating the images).\nLooking under the hood, here you can see how Edge Impulse implements a data Augmentation policy on your data:\n# Implements the data augmentation policy\ndef augment_image(image, label):\n    # Flips the image randomly\n    image = tf.image.random_flip_left_right(image)\n\n    # Increase the image size, then randomly crop it down to\n    # the original dimensions\n    resize_factor = random.uniform(1, 1.2)\n    new_height = math.floor(resize_factor * INPUT_SHAPE[0])\n    new_width = math.floor(resize_factor * INPUT_SHAPE[1])\n    image = tf.image.resize_with_crop_or_pad(image, new_height, new_width)\n    image = tf.image.random_crop(image, size=INPUT_SHAPE)\n\n    # Vary the brightness of the image\n    image = tf.image.random_brightness(image, max_delta=0.2)\n\n    return image, label\nExposure to these variations during training can help prevent your model from taking shortcuts by “memorizing” superficial clues in your training data, meaning it may better reflect the deep underlying patterns in your dataset.\nThe final dense layer of our model will have 0 neurons with a 10% dropout for overfitting prevention. Here is the Training result:\n\nThe result is excellent, with a reasonable 35ms of latency (for a Raspi-4), which should result in around 30 fps (frames per second) during inference. A Raspi-Zero should be slower, and the Raspi-5, faster.\n\n\nTrading off: Accuracy versus speed\nIf faster inference is needed, we should train the model using smaller alphas (0.35, 0.5, and 0.75) or even reduce the image input size, trading with accuracy. However, reducing the input image size and decreasing the alpha (width multiplier) can speed up inference for MobileNet V2, but they have different trade-offs. Let’s compare:\n\nReducing Image Input Size:\n\nPros:\n\nSignificantly reduces the computational cost across all layers.\nDecreases memory usage.\nIt often provides a substantial speed boost.\n\nCons:\n\nIt may reduce the model’s ability to detect small features or fine details.\nIt can significantly impact accuracy, especially for tasks requiring fine-grained recognition.\n\n\nReducing Alpha (Width Multiplier):\n\nPros:\n\nReduces the number of parameters and computations in the model.\nMaintains the original input resolution, potentially preserving more detail.\nIt can provide a good balance between speed and accuracy.\n\nCons:\n\nIt may not speed up inference as dramatically as reducing input size.\nIt can reduce the model’s capacity to learn complex features.\n\nComparison:\n\nSpeed Impact:\n\nReducing input size often provides a more substantial speed boost because it reduces computations quadratically (halving both width and height reduces computations by about 75%).\nReducing alpha provides a more linear reduction in computations.\n\nAccuracy Impact:\n\nReducing input size can severely impact accuracy, especially when detecting small objects or fine details.\nReducing alpha tends to have a more gradual impact on accuracy.\n\nModel Architecture:\n\nChanging input size doesn’t alter the model’s architecture.\nChanging alpha modifies the model’s structure by reducing the number of channels in each layer.\n\n\nRecommendation:\n\nIf our application doesn’t require detecting tiny details and can tolerate some loss in accuracy, reducing the input size is often the most effective way to speed up inference.\nReducing alpha might be preferable if maintaining the ability to detect fine details is crucial or if you need a more balanced trade-off between speed and accuracy.\nFor best results, you might want to experiment with both:\n\nTry MobileNet V2 with input sizes like 160x160 or 92x92\nExperiment with alpha values like 1.0, 0.75, 0.5 or 0.35.\n\nAlways benchmark the different configurations on your specific hardware and with your particular dataset to find the optimal balance for your use case.\n\n\nRemember, the best choice depends on your specific requirements for accuracy, speed, and the nature of the images you’re working with. It’s often worth experimenting with combinations to find the optimal configuration for your particular use case.\n\n\n\nModel Testing\nNow, you should take the data set aside at the start of the project and run the trained model using it as input. Again, the result is excellent (92.22%).\n\n\nDeploying the model\nAs we did in the previous section, we can deploy the trained model as .tflite and use Raspi to run it using Python.\nOn the Dashboard tab, go to Transfer learning model (int8 quantized) and click on the download icon:\n\n\nLet’s also download the float32 version for comparasion\n\nTransfer the model from your computer to the Raspi (./models), for example, using FileZilla. Also, capture some images for inference (./images).\nImport the needed libraries:\nimport time\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom PIL import Image\nimport tflite_runtime.interpreter as tflite\nDefine the paths and labels:\nimg_path = \"./images/robot.jpg\"\nmodel_path = \"./models/ei-raspi-img-class-int8-quantized-model.tflite\"\nlabels = ['background', 'periquito', 'robot']\n\nNote that the models trained on the Edge Impulse Studio will output values with index 0, 1, 2, etc., where the actual labels will follow an alphabetic order.\n\nLoad the model, allocate the tensors, and get the input and output tensor details:\n# Load the TFLite model\ninterpreter = tflite.Interpreter(model_path=model_path)\ninterpreter.allocate_tensors()\n\n# Get input and output tensors\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\nOne important difference to note is that the dtype of the input details of the model is now int8, which means that the input values go from -128 to +127, while each pixel of our image goes from 0 to 255. This means that we should pre-process the image to match it. We can check here:\ninput_dtype = input_details[0]['dtype']\ninput_dtype\nnumpy.int8\nSo, let’s open the image and show it:\nimg = Image.open(img_path)\nplt.figure(figsize=(4, 4))\nplt.imshow(img)\nplt.axis('off')\nplt.show()\n\nAnd perform the pre-processing:\nscale, zero_point = input_details[0]['quantization']\nimg = img.resize((input_details[0]['shape'][1], \n                  input_details[0]['shape'][2]))\nimg_array = np.array(img, dtype=np.float32) / 255.0\nimg_array = (img_array / scale + zero_point).clip(-128, 127).astype(np.int8)\ninput_data = np.expand_dims(img_array, axis=0)\nChecking the input data, we can verify that the input tensor is compatible with what is expected by the model:\ninput_data.shape, input_data.dtype\n((1, 160, 160, 3), dtype('int8'))\nNow, it is time to perform the inference. Let’s also calculate the latency of the model:\n# Inference on Raspi-Zero\nstart_time = time.time()\ninterpreter.set_tensor(input_details[0]['index'], input_data)\ninterpreter.invoke()\nend_time = time.time()\ninference_time = (end_time - start_time) * 1000  # Convert to milliseconds\nprint (\"Inference time: {:.1f}ms\".format(inference_time))\nThe model will take around 125ms to perform the inference in the Raspi-Zero, which is 3 to 4 times longer than a Raspi-5.\nNow, we can get the output labels and probabilities. It is also important to note that the model trained on the Edge Impulse Studio has a softmax in its output (different from the original Movilenet V2), and we should use the model’s raw output as the “probabilities.”\n# Obtain results and map them to the classes\npredictions = interpreter.get_tensor(output_details[0]['index'])[0]\n\n# Get indices of the top k results\ntop_k_results=3\ntop_k_indices = np.argsort(predictions)[::-1][:top_k_results]\n\n# Get quantization parameters\nscale, zero_point = output_details[0]['quantization']\n\n# Dequantize the output\ndequantized_output = (predictions.astype(np.float32) - zero_point) * scale\nprobabilities = dequantized_output\n\nprint(\"\\n\\t[PREDICTION]        [Prob]\\n\")\nfor i in range(top_k_results):\n    print(\"\\t{:20}: {:.2f}%\".format(\n        labels[top_k_indices[i]],\n        probabilities[top_k_indices[i]] * 100))\n\nLet’s modify the function created before so that we can handle different type of models:\n\ndef image_classification(img_path, model_path, labels, top_k_results=3, \n                         apply_softmax=False):\n    # Load the image\n    img = Image.open(img_path)\n    plt.figure(figsize=(4, 4))\n    plt.imshow(img)\n    plt.axis('off')\n\n    # Load the TFLite model\n    interpreter = tflite.Interpreter(model_path=model_path)\n    interpreter.allocate_tensors()\n    \n    # Get input and output tensors\n    input_details = interpreter.get_input_details()\n    output_details = interpreter.get_output_details()\n    \n    # Preprocess\n    img = img.resize((input_details[0]['shape'][1], \n                      input_details[0]['shape'][2]))\n    \n    input_dtype = input_details[0]['dtype']\n    \n    if input_dtype == np.uint8:\n        input_data = np.expand_dims(np.array(img), axis=0)\n    elif input_dtype == np.int8:\n        scale, zero_point = input_details[0]['quantization']\n        img_array = np.array(img, dtype=np.float32) / 255.0\n        img_array = (img_array / scale + zero_point).clip(-128, 127).astype(np.int8)\n        input_data = np.expand_dims(img_array, axis=0)\n    else:  # float32\n        input_data = np.expand_dims(np.array(img, dtype=np.float32), axis=0) / 255.0\n    \n    # Inference on Raspi-Zero\n    start_time = time.time()\n    interpreter.set_tensor(input_details[0]['index'], input_data)\n    interpreter.invoke()\n    end_time = time.time()\n    inference_time = (end_time - start_time) * 1000  # Convert to milliseconds\n    \n    # Obtain results\n    predictions = interpreter.get_tensor(output_details[0]['index'])[0]\n    \n    # Get indices of the top k results\n    top_k_indices = np.argsort(predictions)[::-1][:top_k_results]\n    \n    # Handle output based on type\n    output_dtype = output_details[0]['dtype']\n    if output_dtype in [np.int8, np.uint8]:\n        # Dequantize the output\n        scale, zero_point = output_details[0]['quantization']\n        predictions = (predictions.astype(np.float32) - zero_point) * scale\n    \n    if apply_softmax:\n        # Apply softmax\n        exp_preds = np.exp(predictions - np.max(predictions))\n        probabilities = exp_preds / np.sum(exp_preds)\n    else:\n        probabilities = predictions\n    \n    print(\"\\n\\t[PREDICTION]        [Prob]\\n\")\n    for i in range(top_k_results):\n        print(\"\\t{:20}: {:.1f}%\".format(\n            labels[top_k_indices[i]],\n            probabilities[top_k_indices[i]] * 100))\n    print (\"\\n\\tInference time: {:.1f}ms\".format(inference_time))\n\nAnd test it with different images and the int8 quantized model (160x160 alpha =1.0).\n\nLet’s download a smaller model, such as the one trained for the Nicla Vision Lab (int8 quantized model, 96x96, alpha = 0.1), as a test. We can use the same function:\n\nThe model lost some accuracy, but it is still OK once our model does not look for many details. Regarding latency, we are around ten times faster on the Raspi-Zero.",
    "crumbs": [
      "Raspberry Pi",
      "Image Classification"
    ]
  },
  {
    "objectID": "contents/labs/raspi/image_classification/image_classification.html#live-image-classification",
    "href": "contents/labs/raspi/image_classification/image_classification.html#live-image-classification",
    "title": "Image Classification",
    "section": "Live Image Classification",
    "text": "Live Image Classification\nLet’s develop an app to capture images with the USB camera in real time, showing its classification.\nUsing the nano on the terminal, save the code below, such as img_class_live_infer.py.\n\nfrom flask import Flask, Response, render_template_string, request, jsonify\nfrom picamera2 import Picamera2\nimport io\nimport threading\nimport time\nimport numpy as np\nfrom PIL import Image\nimport tflite_runtime.interpreter as tflite\nfrom queue import Queue\n\napp = Flask(__name__)\n\n# Global variables\npicam2 = None\nframe = None\nframe_lock = threading.Lock()\nis_classifying = False\nconfidence_threshold = 0.8\nmodel_path = \"./models/ei-raspi-img-class-int8-quantized-model.tflite\"\nlabels = ['background', 'periquito', 'robot']\ninterpreter = None\nclassification_queue = Queue(maxsize=1)\n\ndef initialize_camera():\n    global picam2\n    picam2 = Picamera2()\n    config = picam2.create_preview_configuration(main={\"size\": (320, 240)})\n    picam2.configure(config)\n    picam2.start()\n    time.sleep(2)  # Wait for camera to warm up\n\ndef get_frame():\n    global frame\n    while True:\n        stream = io.BytesIO()\n        picam2.capture_file(stream, format='jpeg')\n        with frame_lock:\n            frame = stream.getvalue()\n        time.sleep(0.1)  # Capture frames more frequently\n\ndef generate_frames():\n    while True:\n        with frame_lock:\n            if frame is not None:\n                yield (b'--frame\\r\\n'\n                       b'Content-Type: image/jpeg\\r\\n\\r\\n' + frame + b'\\r\\n')\n        time.sleep(0.1)\n\ndef load_model():\n    global interpreter\n    if interpreter is None:\n        interpreter = tflite.Interpreter(model_path=model_path)\n        interpreter.allocate_tensors()\n    return interpreter\n\ndef classify_image(img, interpreter):\n    input_details = interpreter.get_input_details()\n    output_details = interpreter.get_output_details()\n\n    img = img.resize((input_details[0]['shape'][1], \n                      input_details[0]['shape'][2]))\n    input_data = np.expand_dims(np.array(img), axis=0)\\\n                             .astype(input_details[0]['dtype'])\n\n    interpreter.set_tensor(input_details[0]['index'], input_data)\n    interpreter.invoke()\n\n    predictions = interpreter.get_tensor(output_details[0]['index'])[0]\n    # Handle output based on type\n    output_dtype = output_details[0]['dtype']\n    if output_dtype in [np.int8, np.uint8]:\n        # Dequantize the output\n        scale, zero_point = output_details[0]['quantization']\n        predictions = (predictions.astype(np.float32) - zero_point) * scale\n    return predictions\n\ndef classification_worker():\n    interpreter = load_model()\n    while True:\n        if is_classifying:\n            with frame_lock:\n                if frame is not None:\n                    img = Image.open(io.BytesIO(frame))\n            predictions = classify_image(img, interpreter)\n            max_prob = np.max(predictions)\n            if max_prob &gt;= confidence_threshold:\n                label = labels[np.argmax(predictions)]\n            else:\n                label = 'Uncertain'\n            classification_queue.put({'label': label, \n                                      'probability': float(max_prob)})\n        time.sleep(0.1)  # Adjust based on your needs\n\n@app.route('/')\ndef index():\n    return render_template_string('''\n        &lt;!DOCTYPE html&gt;\n        &lt;html&gt;\n        &lt;head&gt;\n            &lt;title&gt;Image Classification&lt;/title&gt;\n            &lt;script \n                src=\"https://code.jquery.com/jquery-3.6.0.min.js\"&gt;\n            &lt;/script&gt;\n            &lt;script&gt;\n                function startClassification() {\n                    $.post('/start');\n                    $('#startBtn').prop('disabled', true);\n                    $('#stopBtn').prop('disabled', false);\n                }\n                function stopClassification() {\n                    $.post('/stop');\n                    $('#startBtn').prop('disabled', false);\n                    $('#stopBtn').prop('disabled', true);\n                }\n                function updateConfidence() {\n                    var confidence = $('#confidence').val();\n                    $.post('/update_confidence', {confidence: confidence});\n                }\n                function updateClassification() {\n                    $.get('/get_classification', function(data) {\n                        $('#classification').text(data.label + ': ' \n                        + data.probability.toFixed(2));\n                    });\n                }\n                $(document).ready(function() {\n                    setInterval(updateClassification, 100);  \n                    // Update every 100ms\n                });\n            &lt;/script&gt;\n        &lt;/head&gt;\n        &lt;body&gt;\n            &lt;h1&gt;Image Classification&lt;/h1&gt;\n            &lt;img src=\"{{ url_for('video_feed') }}\" width=\"640\" height=\"480\" /&gt;\n            &lt;br&gt;\n            &lt;button id=\"startBtn\" onclick=\"startClassification()\"&gt;\n            Start Classification&lt;/button&gt;\n            &lt;button id=\"stopBtn\" onclick=\"stopClassification()\" disabled&gt;\n            Stop Classification&lt;/button&gt;\n            &lt;br&gt;\n            &lt;label for=\"confidence\"&gt;Confidence Threshold:&lt;/label&gt;\n            &lt;input type=\"number\" id=\"confidence\" name=\"confidence\" min=\"0\" \n            max=\"1\" step=\"0.1\" value=\"0.8\" onchange=\"updateConfidence()\"&gt;\n            &lt;br&gt;\n            &lt;div id=\"classification\"&gt;Waiting for classification...&lt;/div&gt;\n        &lt;/body&gt;\n        &lt;/html&gt;\n    ''')\n\n@app.route('/video_feed')\ndef video_feed():\n    return Response(generate_frames(),\n                    mimetype='multipart/x-mixed-replace; boundary=frame')\n\n@app.route('/start', methods=['POST'])\ndef start_classification():\n    global is_classifying\n    is_classifying = True\n    return '', 204\n\n@app.route('/stop', methods=['POST'])\ndef stop_classification():\n    global is_classifying\n    is_classifying = False\n    return '', 204\n\n@app.route('/update_confidence', methods=['POST'])\ndef update_confidence():\n    global confidence_threshold\n    confidence_threshold = float(request.form['confidence'])\n    return '', 204\n\n@app.route('/get_classification')\ndef get_classification():\n    if not is_classifying:\n        return jsonify({'label': 'Not classifying', 'probability': 0})\n    try:\n        result = classification_queue.get_nowait()\n    except Queue.Empty:\n        result = {'label': 'Processing', 'probability': 0}\n    return jsonify(result)\n\nif __name__ == '__main__':\n    initialize_camera()\n    threading.Thread(target=get_frame, daemon=True).start()\n    threading.Thread(target=classification_worker, daemon=True).start()\n    app.run(host='0.0.0.0', port=5000, threaded=True)\n\nOn the terminal, run:\npython3 img_class_live_infer.py\nAnd access the web interface:\n\nOn the Raspberry Pi itself (if you have a GUI): Open a web browser and go to http://localhost:5000\nFrom another device on the same network: Open a web browser and go to http://&lt;raspberry_pi_ip&gt;:5000 (Replace &lt;raspberry_pi_ip&gt; with your Raspberry Pi’s IP address). For example: http://192.168.4.210:5000/\n\nHere are some screenshots of the app running on an external desktop\n\nHere, you can see the app running on the YouTube:\n\nThe code creates a web application for real-time image classification using a Raspberry Pi, its camera module, and a TensorFlow Lite model. The application uses Flask to serve a web interface where is possible to view the camera feed and see live classification results.\n\nKey Components:\n\nFlask Web Application: Serves the user interface and handles requests.\nPiCamera2: Captures images from the Raspberry Pi camera module.\nTensorFlow Lite: Runs the image classification model.\nThreading: Manages concurrent operations for smooth performance.\n\n\n\nMain Features:\n\nLive camera feed display\nReal-time image classification\nAdjustable confidence threshold\nStart/Stop classification on demand\n\n\n\nCode Structure:\n\nImports and Setup:\n\nFlask for web application\nPiCamera2 for camera control\nTensorFlow Lite for inference\nThreading and Queue for concurrent operations\n\nGlobal Variables:\n\nCamera and frame management\nClassification control\nModel and label information\n\nCamera Functions:\n\ninitialize_camera(): Sets up the PiCamera2\nget_frame(): Continuously captures frames\ngenerate_frames(): Yields frames for the web feed\n\nModel Functions:\n\nload_model(): Loads the TFLite model\nclassify_image(): Performs inference on a single image\n\nClassification Worker:\n\nRuns in a separate thread\nContinuously classifies frames when active\nUpdates a queue with the latest results\n\nFlask Routes:\n\n/: Serves the main HTML page\n/video_feed: Streams the camera feed\n/start and /stop: Controls classification\n/update_confidence: Adjusts the confidence threshold\n/get_classification: Returns the latest classification result\n\nHTML Template:\n\nDisplays camera feed and classification results\nProvides controls for starting/stopping and adjusting settings\n\nMain Execution:\n\nInitializes camera and starts necessary threads\nRuns the Flask application\n\n\n\n\nKey Concepts:\n\nConcurrent Operations: Using threads to handle camera capture and classification separately from the web server.\nReal-time Updates: Frequent updates to the classification results without page reloads.\nModel Reuse: Loading the TFLite model once and reusing it for efficiency.\nFlexible Configuration: Allowing users to adjust the confidence threshold on the fly.\n\n\n\nUsage:\n\nEnsure all dependencies are installed.\nRun the script on a Raspberry Pi with a camera module.\nAccess the web interface from a browser using the Raspberry Pi’s IP address.\nStart classification and adjust settings as needed.",
    "crumbs": [
      "Raspberry Pi",
      "Image Classification"
    ]
  },
  {
    "objectID": "contents/labs/raspi/image_classification/image_classification.html#conclusion",
    "href": "contents/labs/raspi/image_classification/image_classification.html#conclusion",
    "title": "Image Classification",
    "section": "Conclusion:",
    "text": "Conclusion:\nImage classification has emerged as a powerful and versatile application of machine learning, with significant implications for various fields, from healthcare to environmental monitoring. This chapter has demonstrated how to implement a robust image classification system on edge devices like the Raspi-Zero and Raspi-5, showcasing the potential for real-time, on-device intelligence.\nWe’ve explored the entire pipeline of an image classification project, from data collection and model training using Edge Impulse Studio to deploying and running inferences on a Raspi. The process highlighted several key points:\n\nThe importance of proper data collection and preprocessing for training effective models.\nThe power of transfer learning, allowing us to leverage pre-trained models like MobileNet V2 for efficient training with limited data.\nThe trade-offs between model accuracy and inference speed, especially crucial for edge devices.\nThe implementation of real-time classification using a web-based interface, demonstrating practical applications.\n\nThe ability to run these models on edge devices like the Raspi opens up numerous possibilities for IoT applications, autonomous systems, and real-time monitoring solutions. It allows for reduced latency, improved privacy, and operation in environments with limited connectivity.\nAs we’ve seen, even with the computational constraints of edge devices, it’s possible to achieve impressive results in terms of both accuracy and speed. The flexibility to adjust model parameters, such as input size and alpha values, allows for fine-tuning to meet specific project requirements.\nLooking forward, the field of edge AI and image classification continues to evolve rapidly. Advances in model compression techniques, hardware acceleration, and more efficient neural network architectures promise to further expand the capabilities of edge devices in computer vision tasks.\nThis project serves as a foundation for more complex computer vision applications and encourages further exploration into the exciting world of edge AI and IoT. Whether it’s for industrial automation, smart home applications, or environmental monitoring, the skills and concepts covered here provide a solid starting point for a wide range of innovative projects.",
    "crumbs": [
      "Raspberry Pi",
      "Image Classification"
    ]
  },
  {
    "objectID": "contents/labs/raspi/image_classification/image_classification.html#resources",
    "href": "contents/labs/raspi/image_classification/image_classification.html#resources",
    "title": "Image Classification",
    "section": "Resources",
    "text": "Resources\n\nDataset Example\nSetup Test Notebook on a Raspi\nImage Classification Notebook on a Raspi\nCNN to classify Cifar-10 dataset at CoLab\nCifar 10 - Image Classification on a Raspi\nPython Scripts\nEdge Impulse Project",
    "crumbs": [
      "Raspberry Pi",
      "Image Classification"
    ]
  },
  {
    "objectID": "contents/labs/raspi/object_detection/object_detection.html",
    "href": "contents/labs/raspi/object_detection/object_detection.html",
    "title": "Object Detection",
    "section": "",
    "text": "Overview\nBuilding upon our exploration of image classification, we now turn our attention to a more advanced computer vision task: object detection. While image classification assigns a single label to an entire image, object detection goes further by identifying and locating multiple objects within a single image. This capability opens up many new applications and challenges, particularly in edge computing and IoT devices like the Raspberry Pi.\nObject detection combines the tasks of classification and localization. It not only determines what objects are present in an image but also pinpoints their locations by, for example, drawing bounding boxes around them. This added complexity makes object detection a more powerful tool for understanding visual scenes, but it also requires more sophisticated models and training techniques.\nIn edge AI, where we work with constrained computational resources, implementing efficient object detection models becomes crucial. The challenges we faced with image classification—balancing model size, inference speed, and accuracy—are amplified in object detection. However, the rewards are also more significant, as object detection enables more nuanced and detailed visual data analysis.\nSome applications of object detection on edge devices include:\nAs we put our hands into object detection, we’ll build upon the concepts and techniques we explored in image classification. We’ll examine popular object detection architectures designed for efficiency, such as:\nWe will explore those object detection models using\nThroughout this lab, we’ll cover the fundamentals of object detection and how it differs from image classification. We’ll also learn how to train, fine-tune, test, optimize, and deploy popular object detection architectures using a dataset created from scratch.",
    "crumbs": [
      "Raspberry Pi",
      "Object Detection"
    ]
  },
  {
    "objectID": "contents/labs/raspi/object_detection/object_detection.html#overview",
    "href": "contents/labs/raspi/object_detection/object_detection.html#overview",
    "title": "Object Detection",
    "section": "",
    "text": "Surveillance and security systems\nAutonomous vehicles and drones\nIndustrial quality control\nWildlife monitoring\nAugmented reality applications\n\n\n\nSingle Stage Detectors, such as MobileNet and EfficientDet,\nFOMO (Faster Objects, More Objects), and\nYOLO (You Only Look Once).\n\n\nTo learn more about object detection models, follow the tutorial A Gentle Introduction to Object Recognition With Deep Learning.\n\n\n\nTensorFlow Lite Runtime (now changed to LiteRT),\nEdge Impulse Linux Python SDK and\nUltralitics\n\n\n\n\nObject Detection Fundamentals\nObject detection builds upon the foundations of image classification but extends its capabilities significantly. To understand object detection, it’s crucial first to recognize its key differences from image classification:\n\nImage Classification vs. Object Detection\nImage Classification:\n\nAssigns a single label to an entire image\nAnswers the question: “What is this image’s primary object or scene?”\nOutputs a single class prediction for the whole image\n\nObject Detection:\n\nIdentifies and locates multiple objects within an image\nAnswers the questions: “What objects are in this image, and where are they located?”\nOutputs multiple predictions, each consisting of a class label and a bounding box\n\nTo visualize this difference, let’s consider an example: \nThis diagram illustrates the critical difference: image classification provides a single label for the entire image, while object detection identifies multiple objects, their classes, and their locations within the image.\n\n\nKey Components of Object Detection\nObject detection systems typically consist of two main components:\n\nObject Localization: This component identifies where objects are located in the image. It typically outputs bounding boxes, rectangular regions encompassing each detected object.\nObject Classification: This component determines the class or category of each detected object, similar to image classification but applied to each localized region.\n\n\n\nChallenges in Object Detection\nObject detection presents several challenges beyond those of image classification:\n\nMultiple objects: An image may contain multiple objects of various classes, sizes, and positions.\nVarying scales: Objects can appear at different sizes within the image.\nOcclusion: Objects may be partially hidden or overlapping.\nBackground clutter: Distinguishing objects from complex backgrounds can be challenging.\nReal-time performance: Many applications require fast inference times, especially on edge devices.\n\n\n\nApproaches to Object Detection\nThere are two main approaches to object detection:\n\nTwo-stage detectors: These first propose regions of interest and then classify each region. Examples include R-CNN and its variants (Fast R-CNN, Faster R-CNN).\nSingle-stage detectors: These predict bounding boxes (or centroids) and class probabilities in one forward pass of the network. Examples include YOLO (You Only Look Once), EfficientDet, SSD (Single Shot Detector), and FOMO (Faster Objects, More Objects). These are often faster and more suitable for edge devices like Raspberry Pi.\n\n\n\nEvaluation Metrics\nObject detection uses different metrics compared to image classification:\n\nIntersection over Union (IoU): Measures the overlap between predicted and ground truth bounding boxes.\nMean Average Precision (mAP): Combines precision and recall across all classes and IoU thresholds.\nFrames Per Second (FPS): Measures detection speed, crucial for real-time applications on edge devices.",
    "crumbs": [
      "Raspberry Pi",
      "Object Detection"
    ]
  },
  {
    "objectID": "contents/labs/raspi/object_detection/object_detection.html#pre-trained-object-detection-models-overview",
    "href": "contents/labs/raspi/object_detection/object_detection.html#pre-trained-object-detection-models-overview",
    "title": "Object Detection",
    "section": "Pre-Trained Object Detection Models Overview",
    "text": "Pre-Trained Object Detection Models Overview\nAs we saw in the introduction, given an image or a video stream, an object detection model can identify which of a known set of objects might be present and provide information about their positions within the image.\n\nYou can test some common models online by visiting Object Detection - MediaPipe Studio\n\nOn Kaggle, we can find the most common pre-trained tflite models to use with the Raspi, ssd_mobilenet_v1, and EfficientDet. Those models were trained on the COCO (Common Objects in Context) dataset, with over 200,000 labeled images in 91 categories. Go, download the models, and upload them to the ./models folder in the Raspi.\n\nAlternatively, you can find the models and the COCO labels on GitHub.\n\nFor the first part of this lab, we will focus on a pre-trained 300x300 SSD-Mobilenet V1 model and compare it with the 320x320 EfficientDet-lite0, also trained using the COCO 2017 dataset. Both models were converted to a TensorFlow Lite format (4.2MB for the SSD Mobilenet and 4.6MB for the EfficientDet).\n\nSSD-Mobilenet V2 or V3 is recommended for transfer learning projects, but once the V1 TFLite model is publicly available, we will use it for this overview.\n\n\n\nSetting Up the TFLite Environment\nWe should confirm the steps done on the last Hands-On Lab, Image Classification, as follows:\n\nUpdating the Raspberry Pi\nInstalling Required Libraries\nSetting up a Virtual Environment (Optional but Recommended)\n\nsource ~/tflite/bin/activate\n\nInstalling TensorFlow Lite Runtime\nInstalling Additional Python Libraries (inside the environment)\n\n\n\nCreating a Working Directory:\nConsidering that we have created the Documents/TFLITE folder in the last Lab, let’s now create the specific folders for this object detection lab:\ncd Documents/TFLITE/\nmkdir OBJ_DETECT\ncd OBJ_DETECT\nmkdir images\nmkdir models\ncd models\n\n\nInference and Post-Processing\nLet’s start a new notebook to follow all the steps to detect objects on an image:\nImport the needed libraries:\nimport time\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom PIL import Image\nimport tflite_runtime.interpreter as tflite\nLoad the TFLite model and allocate tensors:\nmodel_path = \"./models/ssd-mobilenet-v1-tflite-default-v1.tflite\"\ninterpreter = tflite.Interpreter(model_path=model_path)\ninterpreter.allocate_tensors()\nGet input and output tensors.\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\nInput details will inform us how the model should be fed with an image. The shape of (1, 300, 300, 3) with a dtype of uint8 tells us that a non-normalized (pixel value range from 0 to 255) image with dimensions (300x300x3) should be input one by one (Batch Dimension: 1).\nThe output details include not only the labels (“classes”) and probabilities (“scores”) but also the relative window position of the bounding boxes (“boxes”) about where the object is located on the image and the number of detected objects (“num_detections”). The output details also tell us that the model can detect a maximum of 10 objects in the image.\n\nSo, for the above example, using the same cat image used with the Image Classification Lab looking for the output, we have a 76% probability of having found an object with a class ID of 16 on an area delimited by a bounding box of [0.028011084, 0.020121813, 0.9886069, 0.802299]. Those four numbers are related to ymin, xmin, ymax and xmax, the box coordinates.\nTaking into consideration that y goes from the top (ymin) to the bottom (ymax) and x goes from left (xmin) to the right (xmax), we have, in fact, the coordinates of the top/left corner and the bottom/right one. With both edges and knowing the shape of the picture, it is possible to draw a rectangle around the object, as shown in the figure below:\n\nNext, we should find what class ID equal to 16 means. Opening the file coco_labels.txt, as a list, each element has an associated index, and inspecting index 16, we get, as expected, cat. The probability is the value returning from the score.\nLet’s now upload some images with multiple objects on it for testing.\nimg_path = \"./images/cat_dog.jpeg\"\norig_img = Image.open(img_path)\n\n# Display the image\nplt.figure(figsize=(8, 8))\nplt.imshow(orig_img)\nplt.title(\"Original Image\")\nplt.show()\n\nBased on the input details, let’s pre-process the image, changing its shape and expanding its dimension:\nimg = orig_img.resize((input_details[0]['shape'][1], \n                  input_details[0]['shape'][2]))\ninput_data = np.expand_dims(img, axis=0)\ninput_data.shape, input_data.dtype \nThe new input_data shape is(1, 300, 300, 3) with a dtype of uint8, which is compatible with what the model expects.\nUsing the input_data, let’s run the interpreter, measure the latency, and get the output:\nstart_time = time.time()\ninterpreter.set_tensor(input_details[0]['index'], input_data)\ninterpreter.invoke()\nend_time = time.time()\ninference_time = (end_time - start_time) * 1000  # Convert to milliseconds\nprint (\"Inference time: {:.1f}ms\".format(inference_time))\nWith a latency of around 800ms, we can get 4 distinct outputs:\nboxes = interpreter.get_tensor(output_details[0]['index'])[0] \nclasses = interpreter.get_tensor(output_details[1]['index'])[0]  \nscores = interpreter.get_tensor(output_details[2]['index'])[0]   \nnum_detections = int(interpreter.get_tensor(output_details[3]['index'])[0])\nOn a quick inspection, we can see that the model detected 2 objects with a score over 0.5:\nfor i in range(num_detections):\n    if scores[i] &gt; 0.5:  # Confidence threshold\n        print(f\"Object {i}:\")\n        print(f\"  Bounding Box: {boxes[i]}\")\n        print(f\"  Confidence: {scores[i]}\")\n        print(f\"  Class: {classes[i]}\")\n\nAnd we can also visualize the results:\nplt.figure(figsize=(12, 8))\nplt.imshow(orig_img)\nfor i in range(num_detections):\n    if scores[i] &gt; 0.5:  # Adjust threshold as needed\n        ymin, xmin, ymax, xmax = boxes[i]\n        (left, right, top, bottom) = (xmin * orig_img.width, \n                                      xmax * orig_img.width, \n                                      ymin * orig_img.height, \n                                      ymax * orig_img.height)\n        rect = plt.Rectangle((left, top), right-left, bottom-top, \n                             fill=False, color='red', linewidth=2)\n        plt.gca().add_patch(rect)\n        class_id = int(classes[i])\n        class_name = labels[class_id]\n        plt.text(left, top-10, f'{class_name}: {scores[i]:.2f}', \n                 color='red', fontsize=12, backgroundcolor='white')\n\n\n\nEfficientDet\nEfficientDet is not technically an SSD (Single Shot Detector) model, but it shares some similarities and builds upon ideas from SSD and other object detection architectures:\n\nEfficientDet:\n\nDeveloped by Google researchers in 2019\nUses EfficientNet as the backbone network\nEmploys a novel bi-directional feature pyramid network (BiFPN)\nIt uses compound scaling to scale the backbone network and the object detection components efficiently.\n\nSimilarities to SSD:\n\nBoth are single-stage detectors, meaning they perform object localization and classification in a single forward pass.\nBoth use multi-scale feature maps to detect objects at different scales.\n\nKey differences:\n\nBackbone: SSD typically uses VGG or MobileNet, while EfficientDet uses EfficientNet.\nFeature fusion: SSD uses a simple feature pyramid, while EfficientDet uses the more advanced BiFPN.\nScaling method: EfficientDet introduces compound scaling for all components of the network\n\nAdvantages of EfficientDet:\n\nGenerally achieves better accuracy-efficiency trade-offs than SSD and many other object detection models.\nMore flexible scaling allows for a family of models with different size-performance trade-offs.\n\n\nWhile EfficientDet is not an SSD model, it can be seen as an evolution of single-stage detection architectures, incorporating more advanced techniques to improve efficiency and accuracy. When using EfficientDet, we can expect similar output structures to SSD (e.g., bounding boxes and class scores).\n\nOn GitHub, you can find another notebook exploring the EfficientDet model that we did with SSD MobileNet.",
    "crumbs": [
      "Raspberry Pi",
      "Object Detection"
    ]
  },
  {
    "objectID": "contents/labs/raspi/object_detection/object_detection.html#object-detection-project",
    "href": "contents/labs/raspi/object_detection/object_detection.html#object-detection-project",
    "title": "Object Detection",
    "section": "Object Detection Project",
    "text": "Object Detection Project\nNow, we will develop a complete Image Classification project from data collection to training and deployment. As we did with the Image Classification project, the trained and converted model will be used for inference.\nWe will use the same dataset to train 3 models: SSD-MobileNet V2, FOMO, and YOLO.\n\nThe Goal\nAll Machine Learning projects need to start with a goal. Let’s assume we are in an industrial facility and must sort and count wheels and special boxes.\n\nIn other words, we should perform a multi-label classification, where each image can have three classes:\n\nBackground (no objects)\nBox\nWheel\n\n\n\nRaw Data Collection\nOnce we have defined our Machine Learning project goal, the next and most crucial step is collecting the dataset. We can use a phone, the Raspi, or a mix to create the raw dataset (with no labels). Let’s use the simple web app on our Raspberry Pi to view the QVGA (320 x 240) captured images in a browser.\nFrom GitHub, get the Python script get_img_data.py and open it in the terminal:\npython3 get_img_data.py \nAccess the web interface:\n\nOn the Raspberry Pi itself (if you have a GUI): Open a web browser and go to http://localhost:5000\nFrom another device on the same network: Open a web browser and go to http://&lt;raspberry_pi_ip&gt;:5000 (Replace &lt;raspberry_pi_ip&gt; with your Raspberry Pi’s IP address). For example: http://192.168.4.210:5000/\n\nThe Python script creates a web-based interface for capturing and organizing image datasets using a Raspberry Pi and its camera. It’s handy for machine learning projects that require labeled image data or not, as in our case here.\nAccess the web interface from a browser, enter a generic label for the images you want to capture, and press Start Capture.\n\n\nNote that the images to be captured will have multiple labels that should be defined later.\n\nUse the live preview to position the camera and click Capture Image to save images under the current label (in this case, box-wheel.\n\nWhen we have enough images, we can press Stop Capture. The captured images are saved on the folder dataset/box-wheel:\n\n\nGet around 60 images. Try to capture different angles, backgrounds, and light conditions. Filezilla can transfer the created raw dataset to your main computer.\n\n\n\nLabeling Data\nThe next step in an Object Detect project is to create a labeled dataset. We should label the raw dataset images, creating bounding boxes around each picture’s objects (box and wheel). We can use labeling tools like LabelImg, CVAT, Roboflow, or even the Edge Impulse Studio. Once we have explored the Edge Impulse tool in other labs, let’s use Roboflow here.\n\nWe are using Roboflow (free version) here for two main reasons. 1) We can have auto-labeler, and 2) The annotated dataset is available in several formats and can be used both on Edge Impulse Studio (we will use it for MobileNet V2 and FOMO train) and on CoLab (YOLOv8 train), for example. Having the annotated dataset on Edge Impulse (Free account), it is not possible to use it for training on other platforms.\n\nWe should upload the raw dataset to Roboflow. Create a free account there and start a new project, for example, (“box-versus-wheel”).\n\n\nWe will not enter in deep details about the Roboflow process once many tutorials are available.\n\n\nAnnotate\nOnce the project is created and the dataset is uploaded, you should make the annotations using the “Auto-Label” Tool. Note that you can also upload images with only a background, which should be saved w/o any annotations.\n\nOnce all images are annotated, you should split them into training, validation, and testing.\n\n\n\nData Pre-Processing\nThe last step with the dataset is preprocessing to generate a final version for training. Let’s resize all images to 320x320 and generate augmented versions of each image (augmentation) to create new training examples from which our model can learn.\nFor augmentation, we will rotate the images (+/-15o), crop, and vary the brightness and exposure.\n\nAt the end of the process, we will have 153 images.\n\nNow, you should export the annotated dataset in a format that Edge Impulse, Ultralitics, and other frameworks/tools understand, for example, YOLOv8. Let’s download a zipped version of the dataset to our desktop.\n\nHere, it is possible to review how the dataset was structured\n\nThere are 3 separate folders, one for each split (train/test/valid). For each of them, there are 2 subfolders, images, and labels. The pictures are stored as image_id.jpg and images_id.txt, where “image_id” is unique for every picture.\nThe labels file format will be class_id bounding box coordinates, where in our case, class_id will be 0 for box and 1 for wheel. The numerical id (o, 1, 2…) will follow the alphabetical order of the class name.\nThe data.yaml file has info about the dataset as the classes’ names (names: ['box', 'wheel']) following the YOLO format.\nAnd that’s it! We are ready to start training using the Edge Impulse Studio (as we will do in the following step), Ultralytics (as we will when discussing YOLO), or even training from scratch on CoLab (as we did with the Cifar-10 dataset on the Image Classification lab).\n\nThe pre-processed dataset can be found at the Roboflow site, or here:",
    "crumbs": [
      "Raspberry Pi",
      "Object Detection"
    ]
  },
  {
    "objectID": "contents/labs/raspi/object_detection/object_detection.html#training-an-ssd-mobilenet-model-on-edge-impulse-studio",
    "href": "contents/labs/raspi/object_detection/object_detection.html#training-an-ssd-mobilenet-model-on-edge-impulse-studio",
    "title": "Object Detection",
    "section": "Training an SSD MobileNet Model on Edge Impulse Studio",
    "text": "Training an SSD MobileNet Model on Edge Impulse Studio\nGo to Edge Impulse Studio, enter your credentials at Login (or create an account), and start a new project.\n\nHere, you can clone the project developed for this hands-on lab: Raspi - Object Detection.\n\nOn the Project Dashboard tab, go down and on Project info, and for Labeling method select Bounding boxes (object detection)\n\nUploading the annotated data\nOn Studio, go to the Data acquisition tab, and on the UPLOAD DATA section, upload from your computer the raw dataset.\nWe can use the option Select a folder, choosing, for example, the folder train in your computer, which contains two sub-folders, images, and labels. Select the Image label format, “YOLO TXT”, upload into the caegory Training, and press Upload data.\n\nRepeat the process for the test data (upload both folders, test, and validation). At the end of the upload process, you should end with the annotated dataset of 153 images split in the train/test (84%/16%).\n\nNote that labels will be stored at the labels files 0 and 1 , which are equivalent to box and wheel.\n\n\n\n\nThe Impulse Design\nThe first thing to define when we enter the Create impulse step is to describe the target device for deployment. A pop-up window will appear. We will select Raspberry 4, an intermediary device between the Raspi-Zero and the Raspi-5.\n\nThis choice will not interfere with the training; it will only give us an idea about the latency of the model on that specific target.\n\n\nIn this phase, you should define how to:\n\nPre-processing consists of resizing the individual images. In our case, the images were pre-processed on Roboflow, to 320x320 , so let’s keep it. The resize will not matter here because the images are already squared. If you upload a rectangular image, squash it (squared form, without cropping). Afterward, you could define if the images are converted from RGB to Grayscale or not.\nDesign a Model, in this case, “Object Detection.”\n\n\n\n\nPreprocessing all dataset\nIn the section Image, select Color depth as RGB, and press Save parameters.\n\nThe Studio moves automatically to the next section, Generate features, where all samples will be pre-processed, resulting in 480 objects: 207 boxes and 273 wheels.\n\nThe feature explorer shows that all samples evidence a good separation after the feature generation.\n\n\nModel Design, Training, and Test\nFor training, we should select a pre-trained model. Let’s use the MobileNetV2 SSD FPN-Lite (320x320 only) . It is a pre-trained object detection model designed to locate up to 10 objects within an image, outputting a bounding box for each object detected. The model is around 3.7MB in size. It supports an RGB input at 320x320px.\nRegarding the training hyper-parameters, the model will be trained with:\n\nEpochs: 25\nBatch size: 32\nLearning Rate: 0.15.\n\nFor validation during training, 20% of the dataset (validation_dataset) will be spared.\n\nAs a result, the model ends with an overall precision score (based on COCO mAP) of 88.8%, higher than the result when using the test data (83.3%).\n\n\nDeploying the model\nWe have two ways to deploy our model:\n\nTFLite model, which lets deploy the trained model as .tflite for the Raspi to run it using Python.\nLinux (AARCH64), a binary for Linux (AARCH64), implements the Edge Impulse Linux protocol, which lets us run our models on any Linux-based development board, with SDKs for Python, for example. See the documentation for more information and setup instructions.\n\nLet’s deploy the TFLite model. On the Dashboard tab, go to Transfer learning model (int8 quantized) and click on the download icon:\n\nTransfer the model from your computer to the Raspi folder./models and capture or get some images for inference and save them in the folder ./images.\n\n\nInference and Post-Processing\nThe inference can be made as discussed in the Pre-Trained Object Detection Models Overview. Let’s start a new notebook to follow all the steps to detect cubes and wheels on an image.\nImport the needed libraries:\nimport time\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as patches\nfrom PIL import Image\nimport tflite_runtime.interpreter as tflite\nDefine the model path and labels:\nmodel_path = \"./models/ei-raspi-object-detection-SSD-MobileNetv2-320x0320-\\\nint8.lite\"\nlabels = ['box', 'wheel']\n\nRemember that the model will output the class ID as values (0 and 1), following an alphabetic order regarding the class names.\n\nLoad the model, allocate the tensors, and get the input and output tensor details:\n# Load the TFLite model\ninterpreter = tflite.Interpreter(model_path=model_path)\ninterpreter.allocate_tensors()\n\n# Get input and output tensors\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\nOne crucial difference to note is that the dtype of the input details of the model is now int8, which means that the input values go from -128 to +127, while each pixel of our raw image goes from 0 to 256. This means that we should pre-process the image to match it. We can check here:\ninput_dtype = input_details[0]['dtype']\ninput_dtype\nnumpy.int8\nSo, let’s open the image and show it:\n# Load the image\nimg_path = \"./images/box_2_wheel_2.jpg\"\norig_img = Image.open(img_path)\n\n# Display the image\nplt.figure(figsize=(6, 6))\nplt.imshow(orig_img)\nplt.title(\"Original Image\")\nplt.show()\n\nAnd perform the pre-processing:\nscale, zero_point = input_details[0]['quantization']\nimg = orig_img.resize((input_details[0]['shape'][1], \n                  input_details[0]['shape'][2]))\nimg_array = np.array(img, dtype=np.float32) / 255.0\nimg_array = (img_array / scale + zero_point).clip(-128, 127).astype(np.int8)\ninput_data = np.expand_dims(img_array, axis=0)\nChecking the input data, we can verify that the input tensor is compatible with what is expected by the model:\ninput_data.shape, input_data.dtype\n((1, 320, 320, 3), dtype('int8'))\nNow, it is time to perform the inference. Let’s also calculate the latency of the model:\n# Inference on Raspi-Zero\nstart_time = time.time()\ninterpreter.set_tensor(input_details[0]['index'], input_data)\ninterpreter.invoke()\nend_time = time.time()\ninference_time = (end_time - start_time) * 1000  # Convert to milliseconds\nprint (\"Inference time: {:.1f}ms\".format(inference_time))\nThe model will take around 600ms to perform the inference in the Raspi-Zero, which is around 5 times longer than a Raspi-5.\nNow, we can get the output classes of objects detected, its bounding boxes coordinates, and probabilities.\nboxes = interpreter.get_tensor(output_details[1]['index'])[0]  \nclasses = interpreter.get_tensor(output_details[3]['index'])[0]  \nscores = interpreter.get_tensor(output_details[0]['index'])[0]        \nnum_detections = int(interpreter.get_tensor(output_details[2]['index'])[0])\nfor i in range(num_detections):\n    if scores[i] &gt; 0.5:  # Confidence threshold\n        print(f\"Object {i}:\")\n        print(f\"  Bounding Box: {boxes[i]}\")\n        print(f\"  Confidence: {scores[i]}\")\n        print(f\"  Class: {classes[i]}\")\n\nFrom the results, we can see that 4 objects were detected: two with class ID 0 (box)and two with class ID 1 (wheel), what is correct!\nLet’s visualize the result for a threshold of 0.5\nthreshold = 0.5\nplt.figure(figsize=(6,6))\nplt.imshow(orig_img)\nfor i in range(num_detections):\n    if scores[i] &gt; threshold:  \n        ymin, xmin, ymax, xmax = boxes[i]\n        (left, right, top, bottom) = (xmin * orig_img.width, \n                                      xmax * orig_img.width, \n                                      ymin * orig_img.height, \n                                      ymax * orig_img.height)\n        rect = plt.Rectangle((left, top), right-left, bottom-top, \n                             fill=False, color='red', linewidth=2)\n        plt.gca().add_patch(rect)\n        class_id = int(classes[i])\n        class_name = labels[class_id]\n        plt.text(left, top-10, f'{class_name}: {scores[i]:.2f}', \n                 color='red', fontsize=12, backgroundcolor='white')\n\nBut what happens if we reduce the threshold to 0.3, for example?\n\nWe start to see false positives and multiple detections, where the model detects the same object multiple times with different confidence levels and slightly different bounding boxes.\nCommonly, sometimes, we need to adjust the threshold to smaller values to capture all objects, avoiding false negatives, which would lead to multiple detections.\nTo improve the detection results, we should implement Non-Maximum Suppression (NMS), which helps eliminate overlapping bounding boxes and keeps only the most confident detection.\nFor that, let’s create a general function named non_max_suppression(), with the role of refining object detection results by eliminating redundant and overlapping bounding boxes. It achieves this by iteratively selecting the detection with the highest confidence score and removing other significantly overlapping detections based on an Intersection over Union (IoU) threshold.\ndef non_max_suppression(boxes, scores, threshold):\n    # Convert to corner coordinates\n    x1 = boxes[:, 0]\n    y1 = boxes[:, 1]\n    x2 = boxes[:, 2]\n    y2 = boxes[:, 3]\n\n    areas = (x2 - x1 + 1) * (y2 - y1 + 1)\n    order = scores.argsort()[::-1]\n\n    keep = []\n    while order.size &gt; 0:\n        i = order[0]\n        keep.append(i)\n        xx1 = np.maximum(x1[i], x1[order[1:]])\n        yy1 = np.maximum(y1[i], y1[order[1:]])\n        xx2 = np.minimum(x2[i], x2[order[1:]])\n        yy2 = np.minimum(y2[i], y2[order[1:]])\n\n        w = np.maximum(0.0, xx2 - xx1 + 1)\n        h = np.maximum(0.0, yy2 - yy1 + 1)\n        inter = w * h\n        ovr = inter / (areas[i] + areas[order[1:]] - inter)\n\n        inds = np.where(ovr &lt;= threshold)[0]\n        order = order[inds + 1]\n\n    return keep\nHow it works:\n\nSorting: It starts by sorting all detections by their confidence scores, highest to lowest.\nSelection: It selects the highest-scoring box and adds it to the final list of detections.\nComparison: This selected box is compared with all remaining lower-scoring boxes.\nElimination: Any box that overlaps significantly (above the IoU threshold) with the selected box is eliminated.\nIteration: This process repeats with the next highest-scoring box until all boxes are processed.\n\nNow, we can define a more precise visualization function that will take into consideration an IoU threshold, detecting only the objects that were selected by the non_max_suppression function:\ndef visualize_detections(image, boxes, classes, scores, \n                         labels, threshold, iou_threshold):\n    if isinstance(image, Image.Image):\n        image_np = np.array(image)\n    else:\n        image_np = image\n\n    height, width = image_np.shape[:2]\n    \n    # Convert normalized coordinates to pixel coordinates\n    boxes_pixel = boxes * np.array([height, width, height, width])\n    \n    # Apply NMS\n    keep = non_max_suppression(boxes_pixel, scores, iou_threshold)\n    \n    # Set the figure size to 12x8 inches\n    fig, ax = plt.subplots(1, figsize=(12, 8))\n\n    ax.imshow(image_np)\n    \n    for i in keep:\n        if scores[i] &gt; threshold:\n            ymin, xmin, ymax, xmax = boxes[i]\n            rect = patches.Rectangle((xmin * width, ymin * height),\n                                     (xmax - xmin) * width,\n                                     (ymax - ymin) * height,\n                                     linewidth=2, edgecolor='r', facecolor='none')\n            ax.add_patch(rect)\n            class_name = labels[int(classes[i])]\n            ax.text(xmin * width, ymin * height - 10,\n                    f'{class_name}: {scores[i]:.2f}', color='red',\n                    fontsize=12, backgroundcolor='white')\n\n    plt.show()\nNow we can create a function that will call the others, performing inference on any image:\ndef detect_objects(img_path, conf=0.5, iou=0.5):\n    orig_img = Image.open(img_path)\n    scale, zero_point = input_details[0]['quantization']\n    img = orig_img.resize((input_details[0]['shape'][1], \n                      input_details[0]['shape'][2]))\n    img_array = np.array(img, dtype=np.float32) / 255.0\n    img_array = (img_array / scale + zero_point).clip(-128, 127).\\\n    astype(np.int8)\n    input_data = np.expand_dims(img_array, axis=0)\n    \n    # Inference on Raspi-Zero\n    start_time = time.time()\n    interpreter.set_tensor(input_details[0]['index'], input_data)\n    interpreter.invoke()\n    end_time = time.time()\n    inference_time = (end_time - start_time) * 1000  # Convert to ms\n    print (\"Inference time: {:.1f}ms\".format(inference_time))\n    \n    # Extract the outputs\n    boxes = interpreter.get_tensor(output_details[1]['index'])[0]  \n    classes = interpreter.get_tensor(output_details[3]['index'])[0]  \n    scores = interpreter.get_tensor(output_details[0]['index'])[0]        \n    num_detections = int(interpreter.get_tensor(output_details[2]['index'])[0])\n\n    visualize_detections(orig_img, boxes, classes, scores, labels, \n                         threshold=conf, \n                         iou_threshold=iou)\nNow, running the code, having the same image again with a confidence threshold of 0.3, but with a small IoU:\nimg_path = \"./images/box_2_wheel_2.jpg\"\ndetect_objects(img_path, conf=0.3,iou=0.05)",
    "crumbs": [
      "Raspberry Pi",
      "Object Detection"
    ]
  },
  {
    "objectID": "contents/labs/raspi/object_detection/object_detection.html#training-a-fomo-model-at-edge-impulse-studio",
    "href": "contents/labs/raspi/object_detection/object_detection.html#training-a-fomo-model-at-edge-impulse-studio",
    "title": "Object Detection",
    "section": "Training a FOMO Model at Edge Impulse Studio",
    "text": "Training a FOMO Model at Edge Impulse Studio\nThe inference with the SSD MobileNet model worked well, but the latency was significantly high. The inference varied from 0.5 to 1.3 seconds on a Raspi-Zero, which means around or less than 1 FPS (1 frame per second). One alternative to speed up the process is to use FOMO (Faster Objects, More Objects).\nThis novel machine learning algorithm lets us count multiple objects and find their location in an image in real-time using up to 30x less processing power and memory than MobileNet SSD or YOLO. The main reason this is possible is that while other models calculate the object’s size by drawing a square around it (bounding box), FOMO ignores the size of the image, providing only the information about where the object is located in the image through its centroid coordinates.\n\nHow FOMO works?\nIn a typical object detection pipeline, the first stage is extracting features from the input image. FOMO leverages MobileNetV2 to perform this task. MobileNetV2 processes the input image to produce a feature map that captures essential characteristics, such as textures, shapes, and object edges, in a computationally efficient way.\n\nOnce these features are extracted, FOMO’s simpler architecture, focused on center-point detection, interprets the feature map to determine where objects are located in the image. The output is a grid of cells, where each cell represents whether or not an object center is detected. The model outputs one or more confidence scores for each cell, indicating the likelihood of an object being present.\nLet’s see how it works on an image.\nFOMO divides the image into blocks of pixels using a factor of 8. For the input of 96x96, the grid would be 12x12 (96/8=12). For a 160x160, the grid will be 20x20, and so on. Next, FOMO will run a classifier through each pixel block to calculate the probability that there is a box or a wheel in each of them and, subsequently, determine the regions that have the highest probability of containing the object (If a pixel block has no objects, it will be classified as background). From the overlap of the final region, the FOMO provides the coordinates (related to the image dimensions) of the centroid of this region.\n\nTrade-off Between Speed and Precision:\n\nGrid Resolution: FOMO uses a grid of fixed resolution, meaning each cell can detect if an object is present in that part of the image. While it doesn’t provide high localization accuracy, it makes a trade-off by being fast and computationally light, which is crucial for edge devices.\nMulti-Object Detection: Since each cell is independent, FOMO can detect multiple objects simultaneously in an image by identifying multiple centers.\n\n\n\nImpulse Design, new Training and Testing\nReturn to Edge Impulse Studio, and in the Experiments tab, create another impulse. Now, the input images should be 160x160 (this is the expected input size for MobilenetV2).\n\nOn the Image tab, generate the features and go to the Object detection tab.\nWe should select a pre-trained model for training. Let’s use the FOMO (Faster Objects, More Objects) MobileNetV2 0.35.\n\nRegarding the training hyper-parameters, the model will be trained with:\n\nEpochs: 30\nBatch size: 32\nLearning Rate: 0.001.\n\nFor validation during training, 20% of the dataset (validation_dataset) will be spared. We will not apply Data Augmentation for the remaining 80% (train_dataset) because our dataset was already augmented during the labeling phase at Roboflow.\nAs a result, the model ends with an overall F1 score of 93.3% with an impressive latency of 8ms (Raspi-4), around 60X less than we got with the SSD MovileNetV2.\n\n\nNote that FOMO automatically added a third label background to the two previously defined boxes (0) and wheels (1).\n\nOn the Model testing tab, we can see that the accuracy was 94%. Here is one of the test sample results:\n\n\nIn object detection tasks, accuracy is generally not the primary evaluation metric. Object detection involves classifying objects and providing bounding boxes around them, making it a more complex problem than simple classification. The issue is that we do not have the bounding box, only the centroids. In short, using accuracy as a metric could be misleading and may not provide a complete understanding of how well the model is performing.\n\n\n\nDeploying the model\nAs we did in the previous section, we can deploy the trained model as TFLite or Linux (AARCH64). Let’s do it now as Linux (AARCH64), a binary that implements the Edge Impulse Linux protocol.\nEdge Impulse for Linux models is delivered in .eim format. This executable contains our “full impulse” created in Edge Impulse Studio. The impulse consists of the signal processing block(s) and any learning and anomaly block(s) we added and trained. It is compiled with optimizations for our processor or GPU (e.g., NEON instructions on ARM cores), plus a straightforward IPC layer (over a Unix socket).\nAt the Deploy tab, select the option Linux (AARCH64), the int8model and press Build.\n\nThe model will be automatically downloaded to your computer.\nOn our Raspi, let’s create a new working area:\ncd ~\ncd Documents\nmkdir EI_Linux\ncd EI_Linux\nmkdir models\nmkdir images\nRename the model for easy identification:\nFor example, raspi-object-detection-linux-aarch64-FOMO-int8.eim and transfer it to the new Raspi folder./models and capture or get some images for inference and save them in the folder ./images.\n\n\nInference and Post-Processing\nThe inference will be made using the Linux Python SDK. This library lets us run machine learning models and collect sensor data on Linux machines using Python. The SDK is open source and hosted on GitHub: edgeimpulse/linux-sdk-python.\nLet’s set up a Virtual Environment for working with the Linux Python SDK\npython3 -m venv ~/eilinux\nsource ~/eilinux/bin/activate\nAnd Install the all the libraries needed:\nsudo apt-get update\nsudo apt-get install libatlas-base-dev libportaudio0 libportaudio2\nsudo apt-get installlibportaudiocpp0 portaudio19-dev\n\npip3 install edge_impulse_linux -i https://pypi.python.org/simple\npip3 install Pillow matplotlib pyaudio opencv-contrib-python\n\nsudo apt-get install portaudio19-dev\npip3 install pyaudio \npip3 install opencv-contrib-python\nPermit our model to be executable.\nchmod +x raspi-object-detection-linux-aarch64-FOMO-int8.eim\nInstall the Jupiter Notebook on the new environment\npip3 install jupyter\nRun a notebook locally (on the Raspi-4 or 5 with desktop)\njupyter notebook\nor on the browser on your computer:\njupyter notebook --ip=192.168.4.210 --no-browser\nLet’s start a new notebook by following all the steps to detect cubes and wheels on an image using the FOMO model and the Edge Impulse Linux Python SDK.\nImport the needed libraries:\nimport sys, time\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as patches\nfrom PIL import Image\nimport cv2\nfrom edge_impulse_linux.image import ImageImpulseRunner\nDefine the model path and labels:\nmodel_file = \"raspi-object-detection-linux-aarch64-int8.eim\"\nmodel_path = \"models/\"+ model_file # Trained ML model from Edge Impulse\nlabels = ['box', 'wheel']\n\nRemember that the model will output the class ID as values (0 and 1), following an alphabetic order regarding the class names.\n\nLoad and initialize the model:\n# Load the model file\nrunner = ImageImpulseRunner(model_path)\n\n# Initialize model\nmodel_info = runner.init()\nThe model_info will contain critical information about our model. However, unlike the TFLite interpreter, the EI Linux Python SDK library will now prepare the model for inference.\nSo, let’s open the image and show it (Now, for compatibility, we will use OpenCV, the CV Library used internally by EI. OpenCV reads the image as BGR, so we will need to convert it to RGB :\n# Load the image\nimg_path = \"./images/1_box_1_wheel.jpg\"\norig_img = cv2.imread(img_path)\nimg_rgb = cv2.cvtColor(orig_img, cv2.COLOR_BGR2RGB)\n\n# Display the image\nplt.imshow(img_rgb)\nplt.title(\"Original Image\")\nplt.show()\n\nNow we will get the features and the preprocessed image (cropped) using the runner:\nfeatures, cropped = runner.get_features_from_image_auto_studio_setings(img_rgb)\nAnd perform the inference. Let’s also calculate the latency of the model:\nres = runner.classify(features)\nLet’s get the output classes of objects detected, their bounding boxes centroids, and probabilities.\nprint('Found %d bounding boxes (%d ms.)' % (\n  len(res[\"result\"][\"bounding_boxes\"]), \n  res['timing']['dsp'] + res['timing']['classification']))\nfor bb in res[\"result\"][\"bounding_boxes\"]:\n    print('\\t%s (%.2f): x=%d y=%d w=%d h=%d' % (\n      bb['label'], bb['value'], bb['x'], \n      bb['y'], bb['width'], bb['height']))\nFound 2 bounding boxes (29 ms.)\n    1 (0.91): x=112 y=40 w=16 h=16\n    0 (0.75): x=48 y=56 w=8 h=8\nThe results show that two objects were detected: one with class ID 0 (box) and one with class ID 1 (wheel), which is correct!\nLet’s visualize the result (The threshold is 0.5, the default value set during the model testing on the Edge Impulse Studio).\nprint('\\tFound %d bounding boxes (latency: %d ms)' % (\n  len(res[\"result\"][\"bounding_boxes\"]), \n  res['timing']['dsp'] + res['timing']['classification']))\nplt.figure(figsize=(5,5))\nplt.imshow(cropped)\n\n# Go through each of the returned bounding boxes\nbboxes = res['result']['bounding_boxes']\nfor bbox in bboxes:\n\n    # Get the corners of the bounding box\n    left = bbox['x']\n    top = bbox['y']\n    width = bbox['width']\n    height = bbox['height']\n    \n    # Draw a circle centered on the detection\n    circ = plt.Circle((left+width//2, top+height//2), 5, \n                     fill=False, color='red', linewidth=3)\n    plt.gca().add_patch(circ)\n    class_id = int(bbox['label'])\n    class_name = labels[class_id]\n    plt.text(left, top-10, f'{class_name}: {bbox[\"value\"]:.2f}', \n              color='red', fontsize=12, backgroundcolor='white')\nplt.show()",
    "crumbs": [
      "Raspberry Pi",
      "Object Detection"
    ]
  },
  {
    "objectID": "contents/labs/raspi/object_detection/object_detection.html#exploring-a-yolo-model-using-ultralitics",
    "href": "contents/labs/raspi/object_detection/object_detection.html#exploring-a-yolo-model-using-ultralitics",
    "title": "Object Detection",
    "section": "Exploring a YOLO Model using Ultralitics",
    "text": "Exploring a YOLO Model using Ultralitics\nFor this lab, we will explore YOLOv8. Ultralytics YOLOv8 is a version of the acclaimed real-time object detection and image segmentation model, YOLO. YOLOv8 is built on cutting-edge advancements in deep learning and computer vision, offering unparalleled performance in terms of speed and accuracy. Its streamlined design makes it suitable for various applications and easily adaptable to different hardware platforms, from edge devices to cloud APIs.\n\nTalking about the YOLO Model\nThe YOLO (You Only Look Once) model is a highly efficient and widely used object detection algorithm known for its real-time processing capabilities. Unlike traditional object detection systems that repurpose classifiers or localizers to perform detection, YOLO frames the detection problem as a single regression task. This innovative approach enables YOLO to simultaneously predict multiple bounding boxes and their class probabilities from full images in one evaluation, significantly boosting its speed.\n\nKey Features:\n\nSingle Network Architecture:\n\nYOLO employs a single neural network to process the entire image. This network divides the image into a grid and, for each grid cell, directly predicts bounding boxes and associated class probabilities. This end-to-end training improves speed and simplifies the model architecture.\n\nReal-Time Processing:\n\nOne of YOLO’s standout features is its ability to perform object detection in real-time. Depending on the version and hardware, YOLO can process images at high frames per second (FPS). This makes it ideal for applications requiring quick and accurate object detection, such as video surveillance, autonomous driving, and live sports analysis.\n\nEvolution of Versions:\n\nOver the years, YOLO has undergone significant improvements, from YOLOv1 to the latest YOLOv10. Each iteration has introduced enhancements in accuracy, speed, and efficiency. YOLOv8, for instance, incorporates advancements in network architecture, improved training methodologies, and better support for various hardware, ensuring a more robust performance.\nAlthough YOLOv10 is the family’s newest member with an encouraging performance based on its paper, it was just released (May 2024) and is not fully integrated with the Ultralitycs library. Conversely, the precision-recall curve analysis suggests that YOLOv8 generally outperforms YOLOv9, capturing a higher proportion of true positives while minimizing false positives more effectively (for more details, see this article). So, this lab is based on the YOLOv8n.\n\n\nAccuracy and Efficiency:\n\nWhile early versions of YOLO traded off some accuracy for speed, recent versions have made substantial strides in balancing both. The newer models are faster and more accurate, detecting small objects (such as bees) and performing well on complex datasets.\n\nWide Range of Applications:\n\nYOLO’s versatility has led to its adoption in numerous fields. It is used in traffic monitoring systems to detect and count vehicles, security applications to identify potential threats and agricultural technology to monitor crops and livestock. Its application extends to any domain requiring efficient and accurate object detection.\n\nCommunity and Development:\n\nYOLO continues to evolve and is supported by a strong community of developers and researchers (being the YOLOv8 very strong). Open-source implementations and extensive documentation have made it accessible for customization and integration into various projects. Popular deep learning frameworks like Darknet, TensorFlow, and PyTorch support YOLO, further broadening its applicability.\nUltralitics YOLOv8 can not only Detect (our case here) but also Segment and Pose models pre-trained on the COCO dataset and YOLOv8 Classify models pre-trained on the ImageNet dataset. Track mode is available for all Detect, Segment, and Pose models.\n\n\n\n\nUltralytics YOLO supported tasks\n\n\n\n\n\n\nInstallation\nOn our Raspi, let’s deactivate the current environment to create a new working area:\ndeactivate\ncd ~\ncd Documents/\nmkdir YOLO\ncd YOLO\nmkdir models\nmkdir images\nLet’s set up a Virtual Environment for working with the Ultralytics YOLOv8\npython3 -m venv ~/yolo\nsource ~/yolo/bin/activate\nAnd install the Ultralytics packages for local inference on the Raspi\n\nUpdate the packages list, install pip, and upgrade to the latest:\n\nsudo apt update\nsudo apt install python3-pip -y\npip install -U pip\n\nInstall the ultralytics pip package with optional dependencies:\n\npip install ultralytics[export]\n\nReboot the device:\n\nsudo reboot\n\n\nTesting the YOLO\nAfter the Raspi-Zero booting, let’s activate the yolo env, go to the working directory,\nsource ~/yolo/bin/activate\ncd /Documents/YOLO\nand run inference on an image that will be downloaded from the Ultralytics website, using the YOLOV8n model (the smallest in the family) at the Terminal (CLI):\nyolo predict model='yolov8n' source='https://ultralytics.com/images/bus.jpg'\n\nThe YOLO model family is pre-trained with the COCO dataset.\n\nThe inference result will appear in the terminal. In the image (bus.jpg), 4 persons, 1 bus, and 1 stop signal were detected:\n\nAlso, we got a message that Results saved to runs/detect/predict. Inspecting that directory, we can see a new image saved (bus.jpg). Let’s download it from the Raspi-Zero to our desktop for inspection:\n\nSo, the Ultrayitics YOLO is correctly installed on our Raspi. But, on the Raspi-Zero, an issue is the high latency for this inference, around 18 seconds, even with the most miniature model of the family (YOLOv8n).\n\n\nExport Model to NCNN format\nDeploying computer vision models on edge devices with limited computational power, such as the Raspi-Zero, can cause latency issues. One alternative is to use a format optimized for optimal performance. This ensures that even devices with limited processing power can handle advanced computer vision tasks well.\nOf all the model export formats supported by Ultralytics, the NCNN is a high-performance neural network inference computing framework optimized for mobile platforms. From the beginning of the design, NCNN was deeply considerate about deployment and use on mobile phones and did not have third-party dependencies. It is cross-platform and runs faster than all known open-source frameworks (such as TFLite).\nNCNN delivers the best inference performance when working with Raspberry Pi devices. NCNN is highly optimized for mobile embedded platforms (such as ARM architecture).\nSo, let’s convert our model and rerun the inference:\n\nExport a YOLOv8n PyTorch model to NCNN format, creating: ‘/yolov8n_ncnn_model’\n\nyolo export model=yolov8n.pt format=ncnn \n\nRun inference with the exported model (now the source could be the bus.jpg image that was downloaded from the website to the current directory on the last inference):\n\nyolo predict model='./yolov8n_ncnn_model' source='bus.jpg'\n\nThe first inference, when the model is loaded, usually has a high latency (around 17s), but from the 2nd, it is possible to note that the inference goes down to around 2s.\n\n\n\nExploring YOLO with Python\nTo start, let’s call the Python Interpreter so we can explore how the YOLO model works, line by line:\npython3\nNow, we should call the YOLO library from Ultralitics and load the model:\nfrom ultralytics import YOLO\nmodel = YOLO('yolov8n_ncnn_model')\nNext, run inference over an image (let’s use again bus.jpg):\nimg = 'bus.jpg'\nresult = model.predict(img, save=True, imgsz=640, conf=0.5, iou=0.3)\n\nWe can verify that the result is almost identical to the one we get running the inference at the terminal level (CLI), except that the bus stop was not detected with the reduced NCNN model. Note that the latency was reduced.\nLet’s analyze the “result” content.\nFor example, we can see result[0].boxes.data, showing us the main inference result, which is a tensor shape (4, 6). Each line is one of the objects detected, being the 4 first columns, the bounding boxes coordinates, the 5th, the confidence, and the 6th, the class (in this case, 0: person and 5: bus):\n\nWe can access several inference results separately, as the inference time, and have it printed in a better format:\ninference_time = int(result[0].speed['inference'])\nprint(f\"Inference Time: {inference_time} ms\")\nOr we can have the total number of objects detected:\nprint(f'Number of objects: {len (result[0].boxes.cls)}')\n\nWith Python, we can create a detailed output that meets our needs (See Model Prediction with Ultralytics YOLO for more details). Let’s run a Python script instead of manually entering it line by line in the interpreter, as shown below. Let’s use nano as our text editor. First, we should create an empty Python script named, for example, yolov8_tests.py:\nnano yolov8_tests.py\nEnter with the code lines:\nfrom ultralytics import YOLO\n\n# Load the YOLOv8 model\nmodel = YOLO('yolov8n_ncnn_model')\n\n# Run inference\nimg = 'bus.jpg'\nresult = model.predict(img, save=False, imgsz=640, conf=0.5, iou=0.3)\n\n# print the results\ninference_time = int(result[0].speed['inference'])\nprint(f\"Inference Time: {inference_time} ms\")\nprint(f'Number of objects: {len (result[0].boxes.cls)}')\n\nAnd enter with the commands: [CTRL+O] + [ENTER] +[CTRL+X] to save the Python script.\nRun the script:\npython yolov8_tests.py\nThe result is the same as running the inference at the terminal level (CLI) and with the built-in Python interpreter.\n\nCalling the YOLO library and loading the model for inference for the first time takes a long time, but the inferences after that will be much faster. For example, the first single inference can take several seconds, but after that, the inference time should be reduced to less than 1 second.\n\n\n\nTraining YOLOv8 on a Customized Dataset\nReturn to our “Boxe versus Wheel” dataset, labeled on Roboflow. On the Download Dataset, instead of Download a zip to computer option done for training on Edge Impulse Studio, we will opt for Show download code. This option will open a pop-up window with a code snippet that should be pasted into our training notebook.\n\nFor training, let’s adapt one of the public examples available from Ultralitytics and run it on Google Colab. Below, you can find mine to be adapted in your project:\n\nYOLOv8 Box versus Wheel Dataset Training [Open In Colab]\n\n\nCritical points on the Notebook:\n\nRun it with GPU (the NVidia T4 is free)\nInstall Ultralytics using PIP.\n\nNow, you can import the YOLO and upload your dataset to the CoLab, pasting the Download code that we get from Roboflow. Note that our dataset will be mounted under /content/datasets/:\n\n\n\nIt is essential to verify and change the file data.yaml with the correct path for the images (copy the path on each images folder).\n\nnames:\n- box\n- wheel\nnc: 2\nroboflow:\n  license: CC BY 4.0\n  project: box-versus-wheel-auto-dataset\n  url: https://universe.roboflow.com/marcelo-rovai-riila/box-versus-wheel-auto-dataset/dataset/5\n  version: 5\n  workspace: marcelo-rovai-riila\ntest: /content/datasets/Box-versus-Wheel-auto-dataset-5/test/images\ntrain: /content/datasets/Box-versus-Wheel-auto-dataset-5/train/images\nval: /content/datasets/Box-versus-Wheel-auto-dataset-5/valid/images\n\nDefine the main hyperparameters that you want to change from default, for example:\nMODEL = 'yolov8n.pt'\nIMG_SIZE = 640\nEPOCHS = 25 # For a final project, you should consider at least 100 epochs \nRun the training (using CLI):\n!yolo task=detect mode=train model={MODEL} data={dataset.location}/data.yaml epochs={EPOCHS} imgsz={IMG_SIZE} plots=True \n\n\n\nimage-20240910111319804\n\n\n\nThe model took a few minutes to be trained and has an excellent result (mAP50 of 0.995). At the end of the training, all results are saved in the folder listed, for example: /runs/detect/train/. There, you can find, for example, the confusion matrix.\n\n\nNote that the trained model (best.pt) is saved in the folder /runs/detect/train/weights/. Now, you should validate the trained model with the valid/images.\n\n!yolo task=detect mode=val model={HOME}/runs/detect/train/weights/best.pt data={dataset.location}/data.yaml\nThe results were similar to training.\n\nNow, we should perform inference on the images left aside for testing\n\n!yolo task=detect mode=predict model={HOME}/runs/detect/train/weights/best.pt conf=0.25 source={dataset.location}/test/images save=True\nThe inference results are saved in the folder runs/detect/predict. Let’s see some of them:\n\n\nIt is advised to export the train, validation, and test results for a Drive at Google. To do so, we should mount the drive.\nfrom google.colab import drive\ndrive.mount('/content/gdrive')\nand copy the content of /runs folder to a folder that you should create in your Drive, for example:\n!scp -r /content/runs '/content/gdrive/MyDrive/10_UNIFEI/Box_vs_Wheel_Project'\n\n\n\n\nInference with the trained model, using the Raspi\nDownload the trained model /runs/detect/train/weights/best.pt to your computer. Using the FileZilla FTP, let’s transfer the best.pt to the Raspi models folder (before the transfer, you may change the model name, for example, box_wheel_320_yolo.pt).\nUsing the FileZilla FTP, let’s transfer a few images from the test dataset to .\\YOLO\\images:\nLet’s return to the YOLO folder and use the Python Interpreter:\ncd ..\npython\nAs before, we will import the YOLO library and define our converted model to detect bees:\nfrom ultralytics import YOLO\nmodel = YOLO('./models/box_wheel_320_yolo.pt')\nNow, let’s define an image and call the inference (we will save the image result this time to external verification):\nimg = './images/1_box_1_wheel.jpg'\nresult = model.predict(img, save=True, imgsz=320, conf=0.5, iou=0.3)\nLet’s repeat for several images. The inference result is saved on the variable result, and the processed image on runs/detect/predict8\n\nUsing FileZilla FTP, we can send the inference result to our Desktop for verification:\n\nWe can see that the inference result is excellent! The model was trained based on the smaller base model of the YOLOv8 family (YOLOv8n). The issue is the latency, around 1 second (or 1 FPS on the Raspi-Zero). Of course, we can reduce this latency and convert the model to TFLite or NCNN.",
    "crumbs": [
      "Raspberry Pi",
      "Object Detection"
    ]
  },
  {
    "objectID": "contents/labs/raspi/object_detection/object_detection.html#object-detection-on-a-live-stream",
    "href": "contents/labs/raspi/object_detection/object_detection.html#object-detection-on-a-live-stream",
    "title": "Object Detection",
    "section": "Object Detection on a live stream",
    "text": "Object Detection on a live stream\nAll the models explored in this lab can detect objects in real-time using a camera. The captured image should be the input for the trained and converted model. For the Raspi-4 or 5 with a desktop, OpenCV can capture the frames and display the inference result.\nHowever, creating a live stream with a webcam to detect objects in real-time is also possible. For example, let’s start with the script developed for the Image Classification app and adapt it for a Real-Time Object Detection Web Application Using TensorFlow Lite and Flask.\nThis app version will work for all TFLite models. Verify if the model is in its correct folder, for example:\nmodel_path = \"./models/ssd-mobilenet-v1-tflite-default-v1.tflite\"\nDownload the Python script object_detection_app.py from GitHub.\nAnd on the terminal, run:\npython3 object_detection_app.py\nAnd access the web interface:\n\nOn the Raspberry Pi itself (if you have a GUI): Open a web browser and go to http://localhost:5000\nFrom another device on the same network: Open a web browser and go to http://&lt;raspberry_pi_ip&gt;:5000 (Replace &lt;raspberry_pi_ip&gt; with your Raspberry Pi’s IP address). For example: http://192.168.4.210:5000/\n\nHere are some screenshots of the app running on an external desktop\n\nLet’s see a technical description of the key modules used in the object detection application:\n\nTensorFlow Lite (tflite_runtime):\n\nPurpose: Efficient inference of machine learning models on edge devices.\nWhy: TFLite offers reduced model size and optimized performance compared to full TensorFlow, which is crucial for resource-constrained devices like Raspberry Pi. It supports hardware acceleration and quantization, further improving efficiency.\nKey functions: Interpreter for loading and running the model,get_input_details(), and get_output_details() for interfacing with the model.\n\nFlask:\n\nPurpose: Lightweight web framework for creating the backend server.\nWhy: Flask’s simplicity and flexibility make it ideal for rapidly developing and deploying web applications. It’s less resource-intensive than larger frameworks suitable for edge devices.\nKey components: route decorators for defining API endpoints, Response objects for streaming video, render_template_string for serving dynamic HTML.\n\nPicamera2:\n\nPurpose: Interface with the Raspberry Pi camera module.\nWhy: Picamera2 is the latest library for controlling Raspberry Pi cameras, offering improved performance and features over the original Picamera library.\nKey functions: create_preview_configuration() for setting up the camera, capture_file() for capturing frames.\n\nPIL (Python Imaging Library):\n\nPurpose: Image processing and manipulation.\nWhy: PIL provides a wide range of image processing capabilities. It’s used here to resize images, draw bounding boxes, and convert between image formats.\nKey classes: Image for loading and manipulating images, ImageDraw for drawing shapes and text on images.\n\nNumPy:\n\nPurpose: Efficient array operations and numerical computing.\nWhy: NumPy’s array operations are much faster than pure Python lists, which is crucial for efficiently processing image data and model inputs/outputs.\nKey functions: array() for creating arrays, expand_dims() for adding dimensions to arrays.\n\nThreading:\n\nPurpose: Concurrent execution of tasks.\nWhy: Threading allows simultaneous frame capture, object detection, and web server operation, crucial for maintaining real-time performance.\nKey components: Thread class creates separate execution threads, and Lock is used for thread synchronization.\n\nio.BytesIO:\n\nPurpose: In-memory binary streams.\nWhy: Allows efficient handling of image data in memory without needing temporary files, improving speed and reducing I/O operations.\n\ntime:\n\nPurpose: Time-related functions.\nWhy: Used for adding delays (time.sleep()) to control frame rate and for performance measurements.\n\njQuery (client-side):\n\nPurpose: Simplified DOM manipulation and AJAX requests.\nWhy: It makes it easy to update the web interface dynamically and communicate with the server without page reloads.\nKey functions: .get() and .post() for AJAX requests, DOM manipulation methods for updating the UI.\n\n\nRegarding the main app system architecture:\n\nMain Thread: Runs the Flask server, handling HTTP requests and serving the web interface.\nCamera Thread: Continuously captures frames from the camera.\nDetection Thread: Processes frames through the TFLite model for object detection.\nFrame Buffer: Shared memory space (protected by locks) storing the latest frame and detection results.\n\nAnd the app data flow, we can describe in short:\n\nCamera captures frame → Frame Buffer\nDetection thread reads from Frame Buffer → Processes through TFLite model → Updates detection results in Frame Buffer\nFlask routes access Frame Buffer to serve the latest frame and detection results\nWeb client receives updates via AJAX and updates UI\n\nThis architecture allows for efficient, real-time object detection while maintaining a responsive web interface running on a resource-constrained edge device like a Raspberry Pi. Threading and efficient libraries like TFLite and PIL enable the system to process video frames in real-time, while Flask and jQuery provide a user-friendly way to interact with them.\nYou can test the app with another pre-processed model, such as the EfficientDet, changing the app line:\nmodel_path = \"./models/lite-model_efficientdet_lite0_detection_metadata_1.tflite\"\n\nIf we want to use the app for the SSD-MobileNetV2 model, trained on Edge Impulse Studio with the “Box versus Wheel” dataset, the code should also be adapted depending on the input details, as we have explored on its notebook.",
    "crumbs": [
      "Raspberry Pi",
      "Object Detection"
    ]
  },
  {
    "objectID": "contents/labs/raspi/object_detection/object_detection.html#conclusion",
    "href": "contents/labs/raspi/object_detection/object_detection.html#conclusion",
    "title": "Object Detection",
    "section": "Conclusion",
    "text": "Conclusion\nThis lab has explored the implementation of object detection on edge devices like the Raspberry Pi, demonstrating the power and potential of running advanced computer vision tasks on resource-constrained hardware. We’ve covered several vital aspects:\n\nModel Comparison: We examined different object detection models, including SSD-MobileNet, EfficientDet, FOMO, and YOLO, comparing their performance and trade-offs on edge devices.\nTraining and Deployment: Using a custom dataset of boxes and wheels (labeled on Roboflow), we walked through the process of training models using Edge Impulse Studio and Ultralytics and deploying them on Raspberry Pi.\nOptimization Techniques: To improve inference speed on edge devices, we explored various optimization methods, such as model quantization (TFLite int8) and format conversion (e.g., to NCNN).\nReal-time Applications: The lab exemplified a real-time object detection web application, demonstrating how these models can be integrated into practical, interactive systems.\nPerformance Considerations: Throughout the lab, we discussed the balance between model accuracy and inference speed, a critical consideration for edge AI applications.\n\nThe ability to perform object detection on edge devices opens up numerous possibilities across various domains, from precision agriculture, industrial automation, and quality control to smart home applications and environmental monitoring. By processing data locally, these systems can offer reduced latency, improved privacy, and operation in environments with limited connectivity.\nLooking ahead, potential areas for further exploration include: - Implementing multi-model pipelines for more complex tasks - Exploring hardware acceleration options for Raspberry Pi - Integrating object detection with other sensors for more comprehensive edge AI systems - Developing edge-to-cloud solutions that leverage both local processing and cloud resources\nObject detection on edge devices can create intelligent, responsive systems that bring the power of AI directly into the physical world, opening up new frontiers in how we interact with and understand our environment.",
    "crumbs": [
      "Raspberry Pi",
      "Object Detection"
    ]
  },
  {
    "objectID": "contents/labs/raspi/object_detection/object_detection.html#resources",
    "href": "contents/labs/raspi/object_detection/object_detection.html#resources",
    "title": "Object Detection",
    "section": "Resources",
    "text": "Resources\n\nDataset (“Box versus Wheel”)\nSSD-MobileNet Notebook on a Raspi\nEfficientDet Notebook on a Raspi\nFOMO - EI Linux Notebook on a Raspi\nYOLOv8 Box versus Wheel Dataset Training on CoLab\nEdge Impulse Project - SSD MobileNet and FOMO\nPython Scripts\nModels",
    "crumbs": [
      "Raspberry Pi",
      "Object Detection"
    ]
  },
  {
    "objectID": "contents/labs/raspi/llm/llm.html",
    "href": "contents/labs/raspi/llm/llm.html",
    "title": "Small Language Models (SLM)",
    "section": "",
    "text": "Overview\nIn the fast-growing area of artificial intelligence, edge computing presents an opportunity to decentralize capabilities traditionally reserved for powerful, centralized servers. This lab explores the practical integration of small versions of traditional large language models (LLMs) into a Raspberry Pi 5, transforming this edge device into an AI hub capable of real-time, on-site data processing.\nAs large language models grow in size and complexity, Small Language Models (SLMs) offer a compelling alternative for edge devices, striking a balance between performance and resource efficiency. By running these models directly on Raspberry Pi, we can create responsive, privacy-preserving applications that operate even in environments with limited or no internet connectivity.\nThis lab will guide you through setting up, optimizing, and leveraging SLMs on Raspberry Pi. We will explore the installation and utilization of Ollama. This open-source framework allows us to run LLMs locally on our machines (our desktops or edge devices such as the Raspberry Pis or NVidia Jetsons). Ollama is designed to be efficient, scalable, and easy to use, making it a good option for deploying AI models such as Microsoft Phi, Google Gemma, Meta Llama, and LLaVa (Multimodal). We will integrate some of those models into projects using Python’s ecosystem, exploring their potential in real-world scenarios (or at least point in this direction).",
    "crumbs": [
      "Raspberry Pi",
      "Small Language Models (SLM)"
    ]
  },
  {
    "objectID": "contents/labs/raspi/llm/llm.html#setup",
    "href": "contents/labs/raspi/llm/llm.html#setup",
    "title": "Small Language Models (SLM)",
    "section": "Setup",
    "text": "Setup\nWe could use any Raspi model in the previous labs, but here, the choice must be the Raspberry Pi 5 (Raspi-5). It is a robust platform that substantially upgrades the last version 4, equipped with the Broadcom BCM2712, a 2.4GHz quad-core 64-bit Arm Cortex-A76 CPU featuring Cryptographic Extension and enhanced caching capabilities. It boasts a VideoCore VII GPU, dual 4Kp60 HDMI® outputs with HDR, and a 4Kp60 HEVC decoder. Memory options include 4GB and 8GB of high-speed LPDDR4X SDRAM, with 8GB being our choice to run SLMs. It also features expandable storage via a microSD card slot and a PCIe 2.0 interface for fast peripherals such as M.2 SSDs (Solid State Drives).\n\nFor real SSL applications, SSDs are a better option than SD cards.\n\nBy the way, as Alasdair Allan discussed, inferencing directly on the Raspberry Pi 5 CPU—with no GPU acceleration—is now on par with the performance of the Coral TPU.\n\nFor more info, please see the complete article: Benchmarking TensorFlow and TensorFlow Lite on Raspberry Pi 5.\n\nRaspberry Pi Active Cooler\nWe suggest installing an Active Cooler, a dedicated clip-on cooling solution for Raspberry Pi 5 (Raspi-5), for this lab. It combines an aluminum heatsink with a temperature-controlled blower fan to keep the Raspi-5 operating comfortably under heavy loads, such as running SLMs.\n\nThe Active Cooler has pre-applied thermal pads for heat transfer and is mounted directly to the Raspberry Pi 5 board using spring-loaded push pins. The Raspberry Pi firmware actively manages it: at 60°C, the blower’s fan will be turned on; at 67.5°C, the fan speed will be increased; and finally, at 75°C, the fan increases to full speed. The blower’s fan will spin down automatically when the temperature drops below these limits.\n\n\nTo prevent overheating, all Raspberry Pi boards begin to throttle the processor when the temperature reaches 80°Cand throttle even further when it reaches the maximum temperature of 85°C (more detail here).",
    "crumbs": [
      "Raspberry Pi",
      "Small Language Models (SLM)"
    ]
  },
  {
    "objectID": "contents/labs/raspi/llm/llm.html#generative-ai-genai",
    "href": "contents/labs/raspi/llm/llm.html#generative-ai-genai",
    "title": "Small Language Models (SLM)",
    "section": "Generative AI (GenAI)",
    "text": "Generative AI (GenAI)\nGenerative AI is an artificial intelligence system capable of creating new, original content across various mediums such as text, images, audio, and video. These systems learn patterns from existing data and use that knowledge to generate novel outputs that didn’t previously exist. Large Language Models (LLMs), Small Language Models (SLMs), and multimodal models can all be considered types of GenAI when used for generative tasks.\nGenAI provides the conceptual framework for AI-driven content creation, with LLMs serving as powerful general-purpose text generators. SLMs adapt this technology for edge computing, while multimodal models extend GenAI capabilities across different data types. Together, they represent a spectrum of generative AI technologies, each with its strengths and applications, collectively driving AI-powered content creation and understanding.\n\nLarge Language Models (LLMs)\nLarge Language Models (LLMs) are advanced artificial intelligence systems that understand, process, and generate human-like text. These models are characterized by their massive scale in terms of the amount of data they are trained on and the number of parameters they contain. Critical aspects of LLMs include:\n\nSize: LLMs typically contain billions of parameters. For example, GPT-3 has 175 billion parameters, while some newer models exceed a trillion parameters.\nTraining Data: They are trained on vast amounts of text data, often including books, websites, and other diverse sources, amounting to hundreds of gigabytes or even terabytes of text.\nArchitecture: Most LLMs use transformer-based architectures, which allow them to process and generate text by paying attention to different parts of the input simultaneously.\nCapabilities: LLMs can perform a wide range of language tasks without specific fine-tuning, including:\n\nText generation\nTranslation\nSummarization\nQuestion answering\nCode generation\nLogical reasoning\n\nFew-shot Learning: They can often understand and perform new tasks with minimal examples or instructions.\nResource-Intensive: Due to their size, LLMs typically require significant computational resources to run, often needing powerful GPUs or TPUs.\nContinual Development: The field of LLMs is rapidly evolving, with new models and techniques constantly emerging.\nEthical Considerations: The use of LLMs raises important questions about bias, misinformation, and the environmental impact of training such large models.\nApplications: LLMs are used in various fields, including content creation, customer service, research assistance, and software development.\nLimitations: Despite their power, LLMs can produce incorrect or biased information and lack true understanding or reasoning capabilities.\n\nWe must note that we use large models beyond text, calling them multi-modal models. These models integrate and process information from multiple types of input simultaneously. They are designed to understand and generate content across various forms of data, such as text, images, audio, and video.\n\n\nClosed vs Open Models:\nClosed models, also called proprietary models, are AI models whose internal workings, code, and training data are not publicly disclosed. Examples: GPT-4 (by OpenAI), Claude (by Anthropic), Gemini (by Google).\nOpen models, also known as open-source models, are AI models whose underlying code, architecture, and often training data are publicly available and accessible. Examples: Gemma (by Google), LLaMA (by Meta) and Phi (by Microsoft).\nOpen models are particularly relevant for running models on edge devices like Raspberry Pi as they can be more easily adapted, optimized, and deployed in resource-constrained environments. Still, it is crucial to verify their Licenses. Open models come with various open-source licenses that may affect their use in commercial applications, while closed models have clear, albeit restrictive, terms of service.\n\n\n\nAdapted from https://arxiv.org/pdf/2304.13712\n\n\n\n\nSmall Language Models (SLMs)\nIn the context of edge computing on devices like Raspberry Pi, full-scale LLMs are typically too large and resource-intensive to run directly. This limitation has driven the development of smaller, more efficient models, such as the Small Language Models (SLMs).\nSLMs are compact versions of LLMs designed to run efficiently on resource-constrained devices such as smartphones, IoT devices, and single-board computers like the Raspberry Pi. These models are significantly smaller in size and computational requirements than their larger counterparts while still retaining impressive language understanding and generation capabilities.\nKey characteristics of SLMs include:\n\nReduced parameter count: Typically ranging from a few hundred million to a few billion parameters, compared to two-digit billions in larger models.\nLower memory footprint: Requiring, at most, a few gigabytes of memory rather than tens or hundreds of gigabytes.\nFaster inference time: Can generate responses in milliseconds to seconds on edge devices.\nEnergy efficiency: Consuming less power, making them suitable for battery-powered devices.\nPrivacy-preserving: Enabling on-device processing without sending data to cloud servers.\nOffline functionality: Operating without an internet connection.\n\nSLMs achieve their compact size through various techniques such as knowledge distillation, model pruning, and quantization. While they may not match the broad capabilities of larger models, SLMs excel in specific tasks and domains, making them ideal for targeted applications on edge devices.\n\nWe will generally consider SLMs, language models with less than 5 billion parameters quantized to 4 bits.\n\nExamples of SLMs include compressed versions of models like Meta Llama, Microsoft PHI, and Google Gemma. These models enable a wide range of natural language processing tasks directly on edge devices, from text classification and sentiment analysis to question answering and limited text generation.\nFor more information on SLMs, the paper, LLM Pruning and Distillation in Practice: The Minitron Approach, provides an approach applying pruning and distillation to obtain SLMs from LLMs. And, SMALL LANGUAGE MODELS: SURVEY, MEASUREMENTS, AND INSIGHTS, presents a comprehensive survey and analysis of Small Language Models (SLMs), which are language models with 100 million to 5 billion parameters designed for resource-constrained devices.",
    "crumbs": [
      "Raspberry Pi",
      "Small Language Models (SLM)"
    ]
  },
  {
    "objectID": "contents/labs/raspi/llm/llm.html#ollama",
    "href": "contents/labs/raspi/llm/llm.html#ollama",
    "title": "Small Language Models (SLM)",
    "section": "Ollama",
    "text": "Ollama\n\n\n\nollama logo\n\n\nOllama is an open-source framework that allows us to run language models (LMs), large or small, locally on our machines. Here are some critical points about Ollama:\n\nLocal Model Execution: Ollama enables running LMs on personal computers or edge devices such as the Raspi-5, eliminating the need for cloud-based API calls.\nEase of Use: It provides a simple command-line interface for downloading, running, and managing different language models.\nModel Variety: Ollama supports various LLMs, including Phi, Gemma, Llama, Mistral, and other open-source models.\nCustomization: Users can create and share custom models tailored to specific needs or domains.\nLightweight: Designed to be efficient and run on consumer-grade hardware.\nAPI Integration: Offers an API that allows integration with other applications and services.\nPrivacy-Focused: By running models locally, it addresses privacy concerns associated with sending data to external servers.\nCross-Platform: Available for macOS, Windows, and Linux systems (our case, here).\nActive Development: Regularly updated with new features and model support.\nCommunity-Driven: Benefits from community contributions and model sharing.\n\nTo learn more about what Ollama is and how it works under the hood, you should see this short video from Matt Williams, one of the founders of Ollama:\n\n\nMatt has an entirely free course about Ollama that we recommend: \n\n\nInstalling Ollama\nLet’s set up and activate a Virtual Environment for working with Ollama:\npython3 -m venv ~/ollama\nsource ~/ollama/bin/activate\nAnd run the command to install Ollama:\ncurl -fsSL https://ollama.com/install.sh | sh\nAs a result, an API will run in the background on 127.0.0.1:11434. From now on, we can run Ollama via the terminal. For starting, let’s verify the Ollama version, which will also tell us that it is correctly installed:\nollama -v\n\nOn the Ollama Library page, we can find the models Ollama supports. For example, by filtering by Most popular, we can see Meta Llama, Google Gemma, Microsoft Phi, LLaVa, etc.\n\n\nMeta Llama 3.2 1B/3B\n\nLet’s install and run our first small language model, Llama 3.2 1B (and 3B). The Meta Llama 3.2 series comprises a set of multilingual generative language models available in 1 billion and 3 billion parameter sizes. These models are designed to process text input and generate text output. The instruction-tuned variants within this collection are specifically optimized for multilingual conversational applications, including tasks involving information retrieval and summarization with an agentic approach. When compared to many existing open-source and proprietary chat models, the Llama 3.2 instruction-tuned models demonstrate superior performance on widely-used industry benchmarks.\nThe 1B and 3B models were pruned from the Llama 8B, and then logits from the 8B and 70B models were used as token-level targets (token-level distillation). Knowledge distillation was used to recover performance (they were trained with 9 trillion tokens). The 1B model has 1,24B, quantized to integer (Q8_0), and the 3B, 3.12B parameters, with a Q4_0 quantization, which ends with a size of 1.3 GB and 2GB, respectively. Its context window is 131,072 tokens.\n\nInstall and run the Model\nollama run llama3.2:1b\nRunning the model with the command before, we should have the Ollama prompt available for us to input a question and start chatting with the LLM model; for example,\n&gt;&gt;&gt; What is the capital of France?\nAlmost immediately, we get the correct answer:\nThe capital of France is Paris.\nUsing the option --verbose when calling the model will generate several statistics about its performance (The model will be polling only the first time we run the command).\n\nEach metric gives insights into how the model processes inputs and generates outputs. Here’s a breakdown of what each metric means:\n\nTotal Duration (2.620170326s): This is the complete time taken from the start of the command to the completion of the response. It encompasses loading the model, processing the input prompt, and generating the response.\nLoad Duration (39.947908ms): This duration indicates the time to load the model or necessary components into memory. If this value is minimal, it can suggest that the model was preloaded or that only a minimal setup was required.\nPrompt Eval Count (32 tokens): The number of tokens in the input prompt. In NLP, tokens are typically words or subwords, so this count includes all the tokens that the model evaluated to understand and respond to the query.\nPrompt Eval Duration (1.644773s): This measures the model’s time to evaluate or process the input prompt. It accounts for the bulk of the total duration, implying that understanding the query and preparing a response is the most time-consuming part of the process.\nPrompt Eval Rate (19.46 tokens/s): This rate indicates how quickly the model processes tokens from the input prompt. It reflects the model’s speed in terms of natural language comprehension.\nEval Count (8 token(s)): This is the number of tokens in the model’s response, which in this case was, “The capital of France is Paris.”\nEval Duration (889.941ms): This is the time taken to generate the output based on the evaluated input. It’s much shorter than the prompt evaluation, suggesting that generating the response is less complex or computationally intensive than understanding the prompt.\nEval Rate (8.99 tokens/s): Similar to the prompt eval rate, this indicates the speed at which the model generates output tokens. It’s a crucial metric for understanding the model’s efficiency in output generation.\n\nThis detailed breakdown can help understand the computational demands and performance characteristics of running SLMs like Llama on edge devices like the Raspberry Pi 5. It shows that while prompt evaluation is more time-consuming, the actual generation of responses is relatively quicker. This analysis is crucial for optimizing performance and diagnosing potential bottlenecks in real-time applications.\nLoading and running the 3B model, we can see the difference in performance for the same prompt;\n\nThe eval rate is lower, 5.3 tokens/s versus 9 tokens/s with the smaller model.\nWhen question about\n&gt;&gt;&gt; What is the distance between Paris and Santiago, Chile?\nThe 1B model answered 9,841 kilometers (6,093 miles), which is inaccurate, and the 3B model answered 7,300 miles (11,700 km), which is close to the correct (11,642 km).\nLet’s ask for the Paris’s coordinates:\n&gt;&gt;&gt; what is the latitude and longitude of Paris?\nThe latitude and longitude of Paris are 48.8567° N (48°55' \n42\" N) and 2.3510° E (2°22' 8\" E), respectively.\n\nBoth 1B and 3B models gave correct answers.\n\n\nGoogle Gemma 2 2B\nLet’s install Gemma 2, a high-performing and efficient model available in three sizes: 2B, 9B, and 27B. We will install Gemma 2 2B, a lightweight model trained with 2 trillion tokens that produces outsized results by learning from larger models through distillation. The model has 2.6 billion parameters and a Q4_0 quantization, which ends with a size of 1.6 GB. Its context window is 8,192 tokens.\n\nInstall and run the Model\nollama run gemma2:2b --verbose\nRunning the model with the command before, we should have the Ollama prompt available for us to input a question and start chatting with the LLM model; for example,\n&gt;&gt;&gt; What is the capital of France?\nAlmost immediately, we get the correct answer:\nThe capital of France is **Paris**. 🗼\nAnd it’ statistics.\n\nWe can see that Gemma 2:2B has around the same performance as Lama 3.2:3B, but having less parameters.\nOther examples:\n&gt;&gt;&gt; What is the distance between Paris and Santiago, Chile?\n\nThe distance between Paris, France and Santiago, Chile is \napproximately **7,000 miles (11,267 kilometers)**. \n\nKeep in mind that this is a straight-line distance, and actual \ntravel distance can vary depending on the chosen routes and any \nstops along the way. ✈️`\nAlso, a good response but less accurate than Llama3.2:3B.\n&gt;&gt;&gt; what is the latitude and longitude of Paris?\n\nYou got it! Here are the latitudes and longitudes of Paris, \nFrance:\n\n* **Latitude:** 48.8566° N (north)\n* **Longitude:** 2.3522° E (east) \n\nLet me know if you'd like to explore more about Paris or its \nlocation! 🗼🇫🇷 \nA good and accurate answer (a little more verbose than the Llama answers).\n\n\nMicrosoft Phi3.5 3.8B\nLet’s pull a bigger (but still tiny) model, the PHI3.5, a 3.8B lightweight state-of-the-art open model by Microsoft. The model belongs to the Phi-3 model family and supports 128K token context length and the languages: Arabic, Chinese, Czech, Danish, Dutch, English, Finnish, French, German, Hebrew, Hungarian, Italian, Japanese, Korean, Norwegian, Polish, Portuguese, Russian, Spanish, Swedish, Thai, Turkish and Ukrainian.\nThe model size, in terms of bytes, will depend on the specific quantization format used. The size can go from 2-bit quantization (q2_k) of 1.4GB (higher performance/lower quality) to 16-bit quantization (fp-16) of 7.6GB (lower performance/higher quality).\nLet’s run the 4-bit quantization (Q4_0), which will need 2.2GB of RAM, with an intermediary trade-off regarding output quality and performance.\nollama run phi3.5:3.8b --verbose\n\nYou can use run or pull to download the model. What happens is that Ollama keeps note of the pulled models, and once the PHI3 does not exist, before running it, Ollama pulls it.\n\nLet’s enter with the same prompt used before:\n&gt;&gt;&gt; What is the capital of France?\n\nThe capital of France is Paris. It' extradites significant \nhistorical, cultural, and political importance to the country as \nwell as being a major European city known for its art, fashion, \ngastronomy, and culture. Its influence extends beyond national \nborders, with millions of tourists visiting each year from around \nthe globe. The Seine River flows through Paris before it reaches \nthe broader English Channel at Le Havre. Moreover, France is one \nof Europe's leading economies with its capital playing a key role \n\n...\nThe answer was very “verbose”, let’s specify a better prompt:\n\nIn this case, the answer was still longer than we expected, with an eval rate of 2.25 tokens/s, more than double that of Gemma and Llama.\n\nChoosing the most appropriate prompt is one of the most important skills to be used with LLMs, no matter its size.\n\nWhen we asked the same questions about distance and Latitude/Longitude, we did not get a good answer for a distance of 13,507 kilometers (8,429 miles), but it was OK for coordinates. Again, it could have been less verbose (more than 200 tokens for each answer).\nWe can use any model as an assistant since their speed is relatively decent, but on September 24 (2023), the Llama2:3B is a better choice. You should try other models, depending on your needs. 🤗 Open LLM Leaderboard can give you an idea about the best models in size, benchmark, license, etc.\n\nThe best model to use is the one fit for your specific necessity. Also, take into consideration that this field evolves with new models everyday.\n\n\n\nMultimodal Models\nMultimodal models are artificial intelligence (AI) systems that can process and understand information from multiple sources, such as images, text, audio, and video. In our context, multimodal LLMs can process various inputs, including text, images, and audio, as prompts and convert those prompts into various outputs, not just the source type.\nWe will work here with LLaVA-Phi-3, a fine-tuned LLaVA model from Phi 3 Mini 4k. It has strong performance benchmarks that are on par with the original LLaVA (Large Language and Vision Assistant) model.\nThe LLaVA-Phi-3 is an end-to-end trained large multimodal model designed to understand and generate content based on visual inputs (images) and textual instructions. It combines the capabilities of a visual encoder and a language model to process and respond to multimodal inputs.\nLet’s install the model:\nollama run llava-phi3:3.8b --verbose\nLet’s start with a text input:\n&gt;&gt;&gt; You are a helpful AI assistant. What is the capital of France?\n\nAs an AI language model, I can tell you that the capital of France \nis Paris. It's not only the largest city in the country but also \nserves as its political and administrative center. Paris is known \nfor its iconic landmarks such as the Eiffel Tower, Notre-Dame \nCathedral, and the Louvre Museum. The city has a rich history, \nbeautiful architecture, and is widely considered to be one of the \nmost romantic cities in the world.\nThe response took around 30s, with an eval rate of 3.93 tokens/s! Not bad!\nBut let us know to enter with an image as input. For that, let’s create a directory for working:\ncd Documents/\nmkdir OLLAMA\ncd OLLAMA\nLet’s download a 640x320 image from the internet, for example (Wikipedia: Paris, France):\n\nUsing FileZilla, for example, let’s upload the image to the OLLAMA folder at the Raspi-5 and name it image_test_1.jpg. We should have the whole image path (we can use pwd to get it).\n/home/mjrovai/Documents/OLLAMA/image_test_1.jpg\nIf you use a desktop, you can copy the image path by clicking the image with the mouse’s right button.\n\nLet’s enter with this prompt:\n&gt;&gt;&gt; Describe the image /home/mjrovai/Documents/OLLAMA/image_test_1.jpg\nThe result was great, but the overall latency was significant; almost 4 minutes to perform the inference.\n\n\n\nInspecting local resources\nUsing htop, we can monitor the resources running on our device.\nhtop\nDuring the time that the model is running, we can inspect the resources:\n\nAll four CPUs run at almost 100% of their capacity, and the memory used with the model loaded is 3.24GB. Exiting Ollama, the memory goes down to around 377MB (with no desktop).\nIt is also essential to monitor the temperature. When running the Raspberry with a desktop, you can have the temperature shown on the taskbar:\n\nIf you are “headless”, the temperature can be monitored with the command:\nvcgencmd measure_temp\nIf you are doing nothing, the temperature is around 50°C for CPUs running at 1%. During inference, with the CPUs at 100%, the temperature can rise to almost 70°C. This is OK and means the active cooler is working, keeping the temperature below 80°C / 85°C (its limit).",
    "crumbs": [
      "Raspberry Pi",
      "Small Language Models (SLM)"
    ]
  },
  {
    "objectID": "contents/labs/raspi/llm/llm.html#ollama-python-library",
    "href": "contents/labs/raspi/llm/llm.html#ollama-python-library",
    "title": "Small Language Models (SLM)",
    "section": "Ollama Python Library",
    "text": "Ollama Python Library\nSo far, we have explored SLMs’ chat capability using the command line on a terminal. However, we want to integrate those models into our projects, so Python seems to be the right path. The good news is that Ollama has such a library.\nThe Ollama Python library simplifies interaction with advanced LLM models, enabling more sophisticated responses and capabilities, besides providing the easiest way to integrate Python 3.8+ projects with Ollama.\nFor a better understanding of how to create apps using Ollama with Python, we can follow Matt Williams’s videos, as the one below:\n\nInstallation:\nIn the terminal, run the command:\npip install ollama\nWe will need a text editor or an IDE to create a Python script. If you run the Raspberry OS on a desktop, several options, such as Thonny and Geany, have already been installed by default (accessed by [Menu][Programming]). You can download other IDEs, such as Visual Studio Code, from [Menu][Recommended Software]. When the window pops up, go to [Programming], select the option of your choice, and press [Apply].\n\nIf you prefer using Jupyter Notebook for development:\npip install jupyter\njupyter notebook --generate-config\nTo run Jupyter Notebook, run the command (change the IP address for yours):\njupyter notebook --ip=192.168.4.209 --no-browser\nOn the terminal, you can see the local URL address to open the notebook:\n\nWe can access it from another computer by entering the Raspberry Pi’s IP address and the provided token in a web browser (we should copy it from the terminal).\nIn our working directory in the Raspi, we will create a new Python 3 notebook.\nLet’s enter with a very simple script to verify the installed models:\nimport ollama\nollama.list()\nAll the models will be printed as a dictionary, for example:\n  {'name': 'gemma2:2b',\n   'model': 'gemma2:2b',\n   'modified_at': '2024-09-24T19:30:40.053898094+01:00',\n   'size': 1629518495,\n   'digest': '8ccf136fdd5298f3ffe2d69862750ea7fb56555fa4d5b18c04e3fa4d82ee09d7',\n   'details': {'parent_model': '',\n    'format': 'gguf',\n    'family': 'gemma2',\n    'families': ['gemma2'],\n    'parameter_size': '2.6B',\n    'quantization_level': 'Q4_0'}}]}\nLet’s repeat one of the questions that we did before, but now using ollama.generate() from Ollama python library. This API will generate a response for the given prompt with the provided model. This is a streaming endpoint, so there will be a series of responses. The final response object will include statistics and additional data from the request.\nMODEL = 'gemma2:2b'\nPROMPT = 'What is the capital of France?'\n\nres = ollama.generate(model=MODEL, prompt=PROMPT)\nprint (res)\nIn case you are running the code as a Python script, you should save it, for example, test_ollama.py. You can use the IDE to run it or do it directly on the terminal. Also, remember that you should always call the model and define it when running a stand-alone script.\npython test_ollama.py\nAs a result, we will have the model response in a JSON format:\n{'model': 'gemma2:2b', 'created_at': '2024-09-25T14:43:31.869633807Z', \n'response': 'The capital of France is **Paris**. 🇫🇷 \\n', 'done': True, \n'done_reason': 'stop', 'context': [106, 1645, 108, 1841, 603, 573, 6037, 576,\n6081, 235336, 107, 108, 106, 2516, 108, 651, 6037, 576, 6081, 603, 5231, 29437, \n168428, 235248, 244304, 241035, 235248, 108], 'total_duration': 24259469458, \n'load_duration': 19830013859, 'prompt_eval_count': 16, 'prompt_eval_duration': \n1908757000, 'eval_count': 14, 'eval_duration': 2475410000}\nAs we can see, several pieces of information are generated, such as:\n\nresponse: the main output text generated by the model in response to our prompt.\n\nThe capital of France is **Paris**. 🇫🇷\n\ncontext: the token IDs representing the input and context used by the model. Tokens are numerical representations of text used for processing by the language model.\n\n[106, 1645, 108, 1841, 603, 573, 6037, 576, 6081, 235336, 107, 108, 106, 2516, 108, 651, 6037, 576, 6081, 603, 5231, 29437, 168428, 235248, 244304, 241035, 235248, 108]\n\n\nThe Performance Metrics:\n\ntotal_duration: The total time taken for the operation in nanoseconds. In this case, approximately 24.26 seconds.\nload_duration: The time taken to load the model or components in nanoseconds. About 19.83 seconds.\nprompt_eval_duration: The time taken to evaluate the prompt in nanoseconds. Around 16 nanoseconds.\neval_count: The number of tokens evaluated during the generation. Here, 14 tokens.\neval_duration: The time taken for the model to generate the response in nanoseconds. Approximately 2.5 seconds.\n\nBut, what we want is the plain ‘response’ and, perhaps for analysis, the total duration of the inference, so let’s change the code to extract it from the dictionary:\nprint(f\"\\n{res['response']}\")\nprint(f\"\\n [INFO] Total Duration: {(res['total_duration']/1e9):.2f} seconds\")\nNow, we got:\nThe capital of France is **Paris**. 🇫🇷 \n\n [INFO] Total Duration: 24.26 seconds\nUsing Ollama.chat()\nAnother way to get our response is to use ollama.chat(), which generates the next message in a chat with a provided model. This is a streaming endpoint, so a series of responses will occur. Streaming can be disabled using \"stream\": false. The final response object will also include statistics and additional data from the request.\nPROMPT_1 = 'What is the capital of France?'\n\nresponse = ollama.chat(model=MODEL, messages=[\n{'role': 'user','content': PROMPT_1,},])\nresp_1 = response['message']['content']\nprint(f\"\\n{resp_1}\")\nprint(f\"\\n [INFO] Total Duration: {(res['total_duration']/1e9):.2f} seconds\")\nThe answer is the same as before.\nAn important consideration is that by using ollama.generate(), the response is “clear” from the model’s “memory” after the end of inference (only used once), but If we want to keep a conversation, we must use ollama.chat(). Let’s see it in action:\nPROMPT_1 = 'What is the capital of France?'\nresponse = ollama.chat(model=MODEL, messages=[\n{'role': 'user','content': PROMPT_1,},])\nresp_1 = response['message']['content']\nprint(f\"\\n{resp_1}\")\nprint(f\"\\n [INFO] Total Duration: {(response['total_duration']/1e9):.2f} seconds\")\n\nPROMPT_2 = 'and of Italy?'\nresponse = ollama.chat(model=MODEL, messages=[\n{'role': 'user','content': PROMPT_1,},\n{'role': 'assistant','content': resp_1,},\n{'role': 'user','content': PROMPT_2,},])\nresp_2 = response['message']['content']\nprint(f\"\\n{resp_2}\")\nprint(f\"\\n [INFO] Total Duration: {(response['total_duration']/1e9):.2f} seconds\")\nIn the above code, we are running two queries, and the second prompt considers the result of the first one.\nHere is how the model responded:\nThe capital of France is **Paris**. 🇫🇷 \n\n [INFO] Total Duration: 2.82 seconds\n\nThe capital of Italy is **Rome**. 🇮🇹 \n\n [INFO] Total Duration: 4.46 seconds\nGetting an image description:\nIn the same way that we have used the LlaVa-PHI-3 model with the command line to analyze an image, the same can be done here with Python. Let’s use the same image of Paris, but now with the ollama.generate():\nMODEL = 'llava-phi3:3.8b'\nPROMPT = \"Describe this picture\"\n\nwith open('image_test_1.jpg', 'rb') as image_file:\n    img = image_file.read()\n\nresponse = ollama.generate(\n    model=MODEL,\n    prompt=PROMPT,\n    images= [img]\n)\nprint(f\"\\n{response['response']}\")\nprint(f\"\\n [INFO] Total Duration: {(res['total_duration']/1e9):.2f} seconds\")\nHere is the result:\nThis image captures the iconic cityscape of Paris, France. The vantage point \nis high, providing a panoramic view of the Seine River that meanders through \nthe heart of the city. Several bridges arch gracefully over the river, \nconnecting different parts of the city. The Eiffel Tower, an iron lattice \nstructure with a pointed top and two antennas on its summit, stands tall in the \nbackground, piercing the sky. It is painted in a light gray color, contrasting \nagainst the blue sky speckled with white clouds.\n\nThe buildings that line the river are predominantly white or beige, their uniform\ncolor palette broken occasionally by red roofs peeking through. The Seine River \nitself appears calm and wide, reflecting the city's architectural beauty in its \nsurface. On either side of the river, trees add a touch of green to the urban \nlandscape.\n\nThe image is taken from an elevated perspective, looking down on the city. This \nviewpoint allows for a comprehensive view of Paris's beautiful architecture and \nlayout. The relative positions of the buildings, bridges, and other structures \ncreate a harmonious composition that showcases the city's charm.\n\nIn summary, this image presents a serene day in Paris, with its architectural \nmarvels - from the Eiffel Tower to the river-side buildings - all bathed in soft \ncolors under a clear sky.\n\n [INFO] Total Duration: 256.45 seconds\nThe model took about 4 minutes (256.45 s) to return with a detailed image description.\n\nIn the 10-Ollama_Python_Library notebook, it is possible to find the experiments with the Ollama Python library.\n\n\nFunction Calling\nSo far, we can observe that by using the model’s response into a variable, we can effectively incorporate it into real-world projects. However, a major issue arises when the model provides varying responses to the same input. For instance, let’s assume that we only need the name of a country’s capital and its coordinates as the model’s response in the previous examples, without any additional information, even when utilizing verbose models like Microsoft Phi. To ensure consistent responses, we can employ the ‘Ollama function call,’ which is fully compatible with the OpenAI API.\n\nBut what exactly is “function calling”?\nIn modern artificial intelligence, function calling with Large Language Models (LLMs) allows these models to perform actions beyond generating text. By integrating with external functions or APIs, LLMs can access real-time data, automate tasks, and interact with various systems.\nFor instance, instead of merely responding to a query about the weather, an LLM can call a weather API to fetch the current conditions and provide accurate, up-to-date information. This capability enhances the relevance and accuracy of the model’s responses and makes it a powerful tool for driving workflows and automating processes, transforming it into an active participant in real-world applications.\nFor more details about Function Calling, please see this video made by Marvin Prison:\n\n\n\nLet’s create a project.\nWe want to create an app where the user enters a country’s name and gets, as an output, the distance in km from the capital city of such a country and the app’s location (for simplicity, We will use Santiago, Chile, as the app location).\n\nOnce the user enters a country name, the model will return the name of its capital city (as a string) and the latitude and longitude of such city (in float). Using those coordinates, we can use a simple Python library (haversine) to calculate the distance between those 2 points.\nThe idea of this project is to demonstrate a combination of language model interaction, structured data handling with Pydantic, and geospatial calculations using the Haversine formula (traditional computing).\nFirst, let us install some libraries. Besides Haversine, the main one is the OpenAI Python library, which provides convenient access to the OpenAI REST API from any Python 3.7+ application. The other one is Pydantic (and instructor), a robust data validation and settings management library engineered by Python to enhance the robustness and reliability of our codebase. In short, Pydantic will help ensure that our model’s response will always be consistent.\npip install haversine\npip install openai \npip install pydantic \npip install instructor\nNow, we should create a Python script designed to interact with our model (LLM) to determine the coordinates of a country’s capital city and calculate the distance from Santiago de Chile to that capital.\nLet’s go over the code:\n\n\n\n1. Importing Libraries\nimport sys\nfrom haversine import haversine\nfrom openai import OpenAI\nfrom pydantic import BaseModel, Field\nimport instructor\n\nsys: Provides access to system-specific parameters and functions. It’s used to get command-line arguments.\nhaversine: A function from the haversine library that calculates the distance between two geographic points using the Haversine formula.\nopenAI: A module for interacting with the OpenAI API (although it’s used in conjunction with a local setup, Ollama). Everything is off-line here.\npydantic: Provides data validation and settings management using Python-type annotations. It’s used to define the structure of expected response data.\ninstructor: A module is used to patch the OpenAI client to work in a specific mode (likely related to structured data handling).\n\n\n\n2. Defining Input and Model\ncountry = sys.argv[1]       # Get the country from command-line arguments\nMODEL = 'phi3.5:3.8b'     # The name of the model to be used\nmylat = -33.33              # Latitude of Santiago de Chile\nmylon = -70.51              # Longitude of Santiago de Chile\n\ncountry: On a Python script, getting the country name from command-line arguments is possible. On a Jupyter notebook, we can enter its name, for example,\n\ncountry = \"France\"\n\nMODEL: Specifies the model being used, which is, in this example, the phi3.5.\nmylat and mylon: Coordinates of Santiago de Chile, used as the starting point for the distance calculation.\n\n\n\n3. Defining the Response Data Structure\nclass CityCoord(BaseModel):\n    city: str = Field(..., description=\"Name of the city\")\n    lat: float = Field(..., description=\"Decimal Latitude of the city\")\n    lon: float = Field(..., description=\"Decimal Longitude of the city\")\n\nCityCoord: A Pydantic model that defines the expected structure of the response from the LLM. It expects three fields: city (name of the city), lat (latitude), and lon (longitude).\n\n\n\n4. Setting Up the OpenAI Client\nclient = instructor.patch(\n    OpenAI(\n        base_url=\"http://localhost:11434/v1\",  # Local API base URL (Ollama)\n        api_key=\"ollama\",                      # API key (not used)\n    ),\n    mode=instructor.Mode.JSON,                 # Mode for structured JSON output\n)\n\nOpenAI: This setup initializes an OpenAI client with a local base URL and an API key (ollama). It uses a local server.\ninstructor.patch: Patches the OpenAI client to work in JSON mode, enabling structured output that matches the Pydantic model.\n\n\n\n5. Generating the Response\nresp = client.chat.completions.create(\n    model=MODEL,\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": f\"return the decimal latitude and decimal longitude \\\n            of the capital of the {country}.\"\n        }\n    ],\n    response_model=CityCoord,\n    max_retries=10\n)\n\nclient.chat.completions.create: Calls the LLM to generate a response.\nmodel: Specifies the model to use (llava-phi3).\nmessages: Contains the prompt for the LLM, asking for the latitude and longitude of the capital city of the specified country.\nresponse_model: Indicates that the response should conform to the CityCoord model.\nmax_retries: The maximum number of retry attempts if the request fails.\n\n\n\n6. Calculating the Distance\ndistance = haversine((mylat, mylon), (resp.lat, resp.lon), unit='km')\nprint(f\"Santiago de Chile is about {int(round(distance, -1)):,} \\\n        kilometers away from {resp.city}.\")\n\nhaversine: Calculates the distance between Santiago de Chile and the capital city returned by the LLM using their respective coordinates.\n(mylat, mylon): Coordinates of Santiago de Chile.\nresp.city: Name of the country’s capital\n(resp.lat, resp.lon): Coordinates of the capital city are provided by the LLM response.\nunit=‘km’: Specifies that the distance should be calculated in kilometers.\nprint: Outputs the distance, rounded to the nearest 10 kilometers, with thousands of separators for readability.\n\nRunning the code\nIf we enter different countries, for example, France, Colombia, and the United States, We can note that we always receive the same structured information:\nSantiago de Chile is about 8,060 kilometers away from Washington, D.C..\nSantiago de Chile is about 4,250 kilometers away from Bogotá.\nSantiago de Chile is about 11,630 kilometers away from Paris.\nIf you run the code as a script, the result will be printed on the terminal:\n\nAnd the calculations are pretty good!\n\n\nIn the 20-Ollama_Function_Calling notebook, it is possible to find experiments with all models installed.\n\n\n\nAdding images\nNow it is time to wrap up everything so far! Let’s modify the script so that instead of entering the country name (as a text), the user enters an image, and the application (based on SLM) returns the city in the image and its geographic location. With those data, we can calculate the distance as before.\n\nFor simplicity, we will implement this new code in two steps. First, the LLM will analyze the image and create a description (text). This text will be passed on to another instance, where the model will extract the information needed to pass along.\nWe will start importing the libraries\nimport sys\nimport time\nfrom haversine import haversine\nimport ollama\nfrom openai import OpenAI\nfrom pydantic import BaseModel, Field\nimport instructor\nWe can see the image if you run the code on the Jupyter Notebook. For that we need also import:\nimport matplotlib.pyplot as plt\nfrom PIL import Image\n\nThose libraries are unnecessary if we run the code as a script.\n\nNow, we define the model and the local coordinates:\nMODEL = 'llava-phi3:3.8b'\nmylat = -33.33\nmylon = -70.51\nWe can download a new image, for example, Machu Picchu from Wikipedia. On the Notebook we can see it:\n# Load the image\nimg_path = \"image_test_3.jpg\"\nimg = Image.open(img_path)\n\n# Display the image\nplt.figure(figsize=(8, 8))\nplt.imshow(img)\nplt.axis('off')\n#plt.title(\"Image\")\nplt.show()\n\nNow, let’s define a function that will receive the image and will return the decimal latitude and decimal longitude of the city in the image, its name, and what country it is located\ndef image_description(img_path):\n    with open(img_path, 'rb') as file:\n        response = ollama.chat(\n            model=MODEL,\n            messages=[\n              {\n                'role': 'user',\n                'content': '''return the decimal latitude and decimal longitude \n                              of the city in the image, its name, and \n                              what country it is located''',\n                'images': [file.read()],\n              },\n            ],\n            options = {\n              'temperature': 0,\n              }\n      )\n    #print(response['message']['content'])\n    return response['message']['content']\n\nWe can print the entire response for debug purposes.\n\nThe image description generated for the function will be passed as a prompt for the model again.\nstart_time = time.perf_counter()  # Start timing\n\nclass CityCoord(BaseModel):\n    city: str = Field(..., description=\"Name of the city in the image\")\n    country: str = Field(..., description=\"\"\"Name of the country where\"\n                                             the city in the image is located\n                                             \"\"\")\n    lat: float = Field(..., description=\"\"\"Decimal Latitude of the city in\"\n                                            the image\"\"\")\n    lon: float = Field(..., description=\"\"\"Decimal Longitude of the city in\"\n                                           the image\"\"\")\n\n# enables `response_model` in create call\nclient = instructor.patch(\n    OpenAI(\n        base_url=\"http://localhost:11434/v1\",\n        api_key=\"ollama\"\n    ),\n    mode=instructor.Mode.JSON,\n)\n\nimage_description = image_description(img_path)\n# Send this description to the model\nresp = client.chat.completions.create(\n    model=MODEL,\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": image_description,\n        }\n    ],\n    response_model=CityCoord,\n    max_retries=10,\n    temperature=0,\n)\nIf we print the image description , we will get:\nThe image shows the ancient city of Machu Picchu, located in Peru. The city is\nperched on a steep hillside and consists of various structures made of stone. It \nis surrounded by lush greenery and towering mountains. The sky above is blue with\nscattered clouds. \n\nMachu Picchu's latitude is approximately 13.5086° S, and its longitude is around\n72.5494° W.\nAnd the second response from the model (resp) will be:\nCityCoord(city='Machu Picchu', country='Peru', lat=-13.5086, lon=-72.5494)\nNow, we can do a “Post-Processing”, calculating the distance and preparing the final answer:\ndistance = haversine((mylat, mylon), (resp.lat, resp.lon), unit='km')\n\nprint(f\"\\n The image shows {resp.city}, with lat:{round(resp.lat, 2)} and \\\n      long: {round(resp.lon, 2)}, located in {resp.country} and about \\\n            {int(round(distance, -1)):,} kilometers away from \\\n            Santiago, Chile.\\n\")\n\nend_time = time.perf_counter()  # End timing\nelapsed_time = end_time - start_time  # Calculate elapsed time\nprint(f\" [INFO] ==&gt; The code (running {MODEL}), took {elapsed_time:.1f} \\\n      seconds to execute.\\n\")\nAnd we will get:\n The image shows Machu Picchu, with lat:-13.16 and long: -72.54, located in Peru\n and about 2,250 kilometers away from Santiago, Chile.\n\n [INFO] ==&gt; The code (running llava-phi3:3.8b), took 491.3 seconds to execute.\nIn the 30-Function_Calling_with_images notebook, it is possible to find the experiments with multiple images.\nLet’s now download the script calc_distance_image.py from the GitHub and run it on the terminal with the command:\npython calc_distance_image.py /home/mjrovai/Documents/OLLAMA/image_test_3.jpg\nEnter with the Machu Picchu image full patch as an argument. We will get the same previous result.\n\nHow about Paris?\n\nOf course, there are many ways to optimize the code used here. Still, the idea is to explore the considerable potential of function calling with SLMs at the edge, allowing those models to integrate with external functions or APIs. Going beyond text generation, SLMs can access real-time data, automate tasks, and interact with various systems.",
    "crumbs": [
      "Raspberry Pi",
      "Small Language Models (SLM)"
    ]
  },
  {
    "objectID": "contents/labs/raspi/llm/llm.html#slms-optimization-techniques",
    "href": "contents/labs/raspi/llm/llm.html#slms-optimization-techniques",
    "title": "Small Language Models (SLM)",
    "section": "SLMs: Optimization Techniques",
    "text": "SLMs: Optimization Techniques\nLarge Language Models (LLMs) have revolutionized natural language processing, but their deployment and optimization come with unique challenges. One significant issue is the tendency for LLMs (and more, the SLMs) to generate plausible-sounding but factually incorrect information, a phenomenon known as hallucination. This occurs when models produce content that seems coherent but is not grounded in truth or real-world facts.\nOther challenges include the immense computational resources required for training and running these models, the difficulty in maintaining up-to-date knowledge within the model, and the need for domain-specific adaptations. Privacy concerns also arise when handling sensitive data during training or inference. Additionally, ensuring consistent performance across diverse tasks and maintaining ethical use of these powerful tools present ongoing challenges. Addressing these issues is crucial for the effective and responsible deployment of LLMs in real-world applications.\nThe fundamental techniques for enhancing LLM (and SLM) performance and efficiency are Fine-tuning, Prompt engineering, and Retrieval-Augmented Generation (RAG).\n\nFine-tuning, while more resource-intensive, offers a way to specialize LLMs for particular domains or tasks. This process involves further training the model on carefully curated datasets, allowing it to adapt its vast general knowledge to specific applications. Fine-tuning can lead to substantial improvements in performance, especially in specialized fields or for unique use cases.\nPrompt engineering is at the forefront of LLM optimization. By carefully crafting input prompts, we can guide models to produce more accurate and relevant outputs. This technique involves structuring queries that leverage the model’s pre-trained knowledge and capabilities, often incorporating examples or specific instructions to shape the desired response.\nRetrieval-Augmented Generation (RAG) represents another powerful approach to improving LLM performance. This method combines the vast knowledge embedded in pre-trained models with the ability to access and incorporate external, up-to-date information. By retrieving relevant data to supplement the model’s decision-making process, RAG can significantly enhance accuracy and reduce the likelihood of generating outdated or false information.\n\nFor edge applications, it is more beneficial to focus on techniques like RAG that can enhance model performance without needing on-device fine-tuning. Let’s explore it.",
    "crumbs": [
      "Raspberry Pi",
      "Small Language Models (SLM)"
    ]
  },
  {
    "objectID": "contents/labs/raspi/llm/llm.html#rag-implementation",
    "href": "contents/labs/raspi/llm/llm.html#rag-implementation",
    "title": "Small Language Models (SLM)",
    "section": "RAG Implementation",
    "text": "RAG Implementation\nIn a basic interaction between a user and a language model, the user asks a question, which is sent as a prompt to the model. The model generates a response based solely on its pre-trained knowledge. In a RAG process, there’s an additional step between the user’s question and the model’s response. The user’s question triggers a retrieval process from a knowledge base.\n\n\nA simple RAG project\nHere are the steps to implement a basic Retrieval Augmented Generation (RAG):\n\nDetermine the type of documents you’ll be using: The best types are documents from which we can get clean and unobscured text. PDFs can be problematic because they are designed for printing, not for extracting sensible text. To work with PDFs, we should get the source document or use tools to handle it.\nChunk the text: We can’t store the text as one long stream because of context size limitations and the potential for confusion. Chunking involves splitting the text into smaller pieces. Chunk text has many ways, such as character count, tokens, words, paragraphs, or sections. It is also possible to overlap chunks.\nCreate embeddings: Embeddings are numerical representations of text that capture semantic meaning. We create embeddings by passing each chunk of text through a particular embedding model. The model outputs a vector, the length of which depends on the embedding model used. We should pull one (or more) embedding models from Ollama, to perform this task. Here are some examples of embedding models available at Ollama.\n\n\n\nModel\nParameter Size\nEmbedding Size\n\n\n\n\nmxbai-embed-large\n334M\n1024\n\n\nnomic-embed-text\n137M\n768\n\n\nall-minilm\n23M\n384\n\n\n\n\nGenerally, larger embedding sizes capture more nuanced information about the input. Still, they also require more computational resources to process, and a higher number of parameters should increase the latency (but also the quality of the response).\n\nStore the chunks and embeddings in a vector database: We will need a way to efficiently find the most relevant chunks of text for a given prompt, which is where a vector database comes in. We will use Chromadb, an AI-native open-source vector database, which simplifies building RAGs by creating knowledge, facts, and skills pluggable for LLMs. Both the embedding and the source text for each chunk are stored.\nBuild the prompt: When we have a question, we create an embedding and query the vector database for the most similar chunks. Then, we select the top few results and include their text in the prompt.\n\nThe goal of RAG is to provide the model with the most relevant information from our documents, allowing it to generate more accurate and informative responses. So, let’s implement a simple example of an SLM incorporating a particular set of facts about bees (“Bee Facts”).\nInside the ollama env, enter the command in the terminal for Chromadb instalation:\npip install ollama chromadb\nLet’s pull an intermediary embedding model, nomic-embed-text\nollama pull nomic-embed-text\nAnd create a working directory:\ncd Documents/OLLAMA/\nmkdir RAG-simple-bee\ncd RAG-simple-bee/\nLet’s create a new Jupyter notebook, 40-RAG-simple-bee for some exploration:\nImport the needed libraries:\nimport ollama\nimport chromadb\nimport time\nAnd define aor models:\nEMB_MODEL = \"nomic-embed-text\"\nMODEL = 'llama3.2:3B'\nInitially, a knowledge base about bee facts should be created. This involves collecting relevant documents and converting them into vector embeddings. These embeddings are then stored in a vector database, allowing for efficient similarity searches later. Enter with the “document,” a base of “bee facts” as a list:\ndocuments = [\n    \"Bee-keeping, also known as apiculture, involves the maintenance of bee \\\n    colonies, typically in hives, by humans.\",\n    \"The most commonly kept species of bees is the European honey bee (Apis \\\n    mellifera).\",\n    \n    ...\n    \n    \"There are another 20,000 different bee species in the world.\",  \n    \"Brazil alone has more than 300 different bee species, and the \\\n    vast majority, unlike western honey bees, don’t sting.\", \n    \"Reports written in 1577 by Hans Staden, mention three native bees \\\n    used by indigenous people in Brazil.\",\n    \"The indigenous people in Brazil used bees for medicine and food purposes\",\n    \"From Hans Staden report: probable species: mandaçaia (Melipona \\\n    quadrifasciata), mandaguari (Scaptotrigona postica) and jataí-amarela \\\n    (Tetragonisca angustula).\"\n]\n\nWe do not need to “chunk” the document here because we will use each element of the list and a chunk.\n\nNow, we will create our vector embedding database bee_facts and store the document in it:\nclient = chromadb.Client()\ncollection = client.create_collection(name=\"bee_facts\")\n\n# store each document in a vector embedding database\nfor i, d in enumerate(documents):\n  response = ollama.embeddings(model=EMB_MODEL, prompt=d)\n  embedding = response[\"embedding\"]\n  collection.add(\n    ids=[str(i)],\n    embeddings=[embedding],\n    documents=[d]\n  )\nNow that we have our “Knowledge Base” created, we can start making queries, retrieving data from it:\n\nUser Query: The process begins when a user asks a question, such as “How many bees are in a colony? Who lays eggs, and how much? How about common pests and diseases?”\nprompt = \"How many bees are in a colony? Who lays eggs and how much? How about\\\n          common pests and diseases?\"\nQuery Embedding: The user’s question is converted into a vector embedding using the same embedding model used for the knowledge base.\nresponse = ollama.embeddings(\n  prompt=prompt,\n  model=EMB_MODEL\n)\nRelevant Document Retrieval: The system searches the knowledge base using the query embedding to find the most relevant documents (in this case, the 5 more probable). This is done using a similarity search, which compares the query embedding to the document embeddings in the database.\nresults = collection.query(\n  query_embeddings=[response[\"embedding\"]],\n  n_results=5\n)\ndata = results['documents']\nPrompt Augmentation: The retrieved relevant information is combined with the original user query to create an augmented prompt. This prompt now contains the user’s question and pertinent facts from the knowledge base.\nprompt=f\"Using this data: {data}. Respond to this prompt: {prompt}\",\nAnswer Generation: The augmented prompt is then fed into a language model, in this case, the llama3.2:3b model. The model uses this enriched context to generate a comprehensive answer. Parameters like temperature, top_k, and top_p are set to control the randomness and quality of the generated response.\noutput = ollama.generate(\n  model=MODEL,\n  prompt=f\"Using this data: {data}. Respond to this prompt: {prompt}\",\n  options={\n    \"temperature\": 0.0,\n    \"top_k\":10,\n    \"top_p\":0.5                          }\n)\nResponse Delivery: Finally, the system returns the generated answer to the user.\nprint(output['response'])\nBased on the provided data, here are the answers to your questions:\n\n1. How many bees are in a colony?\nA typical bee colony can contain between 20,000 and 80,000 bees.\n\n2. Who lays eggs and how much?\nThe queen bee lays up to 2,000 eggs per day during peak seasons.\n\n3. What about common pests and diseases?\nCommon pests and diseases that affect bees include varroa mites, hive beetles,\nand foulbrood.\nLet’s create a function to help answer new questions:\ndef rag_bees(prompt, n_results=5, temp=0.0, top_k=10, top_p=0.5):\n    start_time = time.perf_counter()  # Start timing\n    \n    # generate an embedding for the prompt and retrieve the data \n    response = ollama.embeddings(\n      prompt=prompt,\n      model=EMB_MODEL\n    )\n    \n    results = collection.query(\n      query_embeddings=[response[\"embedding\"]],\n      n_results=n_results\n    )\n    data = results['documents']\n    \n    # generate a response combining the prompt and data retrieved\n    output = ollama.generate(\n      model=MODEL,\n      prompt=f\"Using this data: {data}. Respond to this prompt: {prompt}\",\n      options={\n        \"temperature\": temp,\n        \"top_k\": top_k,\n        \"top_p\": top_p                          }\n    )\n    \n    print(output['response'])\n    \n    end_time = time.perf_counter()  # End timing\n    elapsed_time = round((end_time - start_time), 1)  # Calculate elapsed time\n    \n    print(f\"\\n [INFO] ==&gt; The code for model: {MODEL}, took {elapsed_time}s \\\n          to generate the answer.\\n\")\nWe can now create queries and call the function:\nprompt = \"Are bees in Brazil?\"\nrag_bees(prompt)\nYes, bees are found in Brazil. According to the data, Brazil has more than 300\ndifferent bee species, and indigenous people in Brazil used bees for medicine and\nfood purposes. Additionally, reports from 1577 mention three native bees used by\nindigenous people in Brazil.\n\n [INFO] ==&gt; The code for model: llama3.2:3b, took 22.7s to generate the answer.\nBy the way, if the model used supports multiple languages, we can use it (for example, Portuguese), even if the dataset was created in English:\nprompt = \"Existem abelhas no Brazil?\"\nrag_bees(prompt)\nSim, existem abelhas no Brasil! De acordo com o relato de Hans Staden, há três \nespécies de abelhas nativas do Brasil que foram mencionadas: mandaçaia (Melipona\nquadrifasciata), mandaguari (Scaptotrigona postica) e jataí-amarela (Tetragonisca\nangustula). Além disso, o Brasil é conhecido por ter mais de 300 espécies diferentes de abelhas, a maioria das quais não é agressiva e não põe veneno.\n\n [INFO] ==&gt; The code for model: llama3.2:3b, took 54.6s to generate the answer.\n\n\nGoing Further\nThe small LLM models tested worked well at the edge, both in text and with images, but of course, they had high latency regarding the last one. A combination of specific and dedicated models can lead to better results; for example, in real cases, an Object Detection model (such as YOLO) can get a general description and count of objects on an image that, once passed to an LLM, can help extract essential insights and actions.\nAccording to Avi Baum, CTO at Hailo,\n\nIn the vast landscape of artificial intelligence (AI), one of the most intriguing journeys has been the evolution of AI on the edge. This journey has taken us from classic machine vision to the realms of discriminative AI, enhancive AI, and now, the groundbreaking frontier of generative AI. Each step has brought us closer to a future where intelligent systems seamlessly integrate with our daily lives, offering an immersive experience of not just perception but also creation at the palm of our hand.",
    "crumbs": [
      "Raspberry Pi",
      "Small Language Models (SLM)"
    ]
  },
  {
    "objectID": "contents/labs/raspi/llm/llm.html#conclusion",
    "href": "contents/labs/raspi/llm/llm.html#conclusion",
    "title": "Small Language Models (SLM)",
    "section": "Conclusion",
    "text": "Conclusion\nThis lab has demonstrated how a Raspberry Pi 5 can be transformed into a potent AI hub capable of running large language models (LLMs) for real-time, on-site data analysis and insights using Ollama and Python. The Raspberry Pi’s versatility and power, coupled with the capabilities of lightweight LLMs like Llama 3.2 and LLaVa-Phi-3-mini, make it an excellent platform for edge computing applications.\nThe potential of running LLMs on the edge extends far beyond simple data processing, as in this lab’s examples. Here are some innovative suggestions for using this project:\n1. Smart Home Automation:\n\nIntegrate SLMs to interpret voice commands or analyze sensor data for intelligent home automation. This could include real-time monitoring and control of home devices, security systems, and energy management, all processed locally without relying on cloud services.\n\n2. Field Data Collection and Analysis:\n\nDeploy SLMs on Raspberry Pi in remote or mobile setups for real-time data collection and analysis. This can be used in agriculture to monitor crop health, in environmental studies for wildlife tracking, or in disaster response for situational awareness and resource management.\n\n3. Educational Tools:\n\nCreate interactive educational tools that leverage SLMs to provide instant feedback, language translation, and tutoring. This can be particularly useful in developing regions with limited access to advanced technology and internet connectivity.\n\n4. Healthcare Applications:\n\nUse SLMs for medical diagnostics and patient monitoring. They can provide real-time analysis of symptoms and suggest potential treatments. This can be integrated into telemedicine platforms or portable health devices.\n\n5. Local Business Intelligence:\n\nImplement SLMs in retail or small business environments to analyze customer behavior, manage inventory, and optimize operations. The ability to process data locally ensures privacy and reduces dependency on external services.\n\n6. Industrial IoT:\n\nIntegrate SLMs into industrial IoT systems for predictive maintenance, quality control, and process optimization. The Raspberry Pi can serve as a localized data processing unit, reducing latency and improving the reliability of automated systems.\n\n7. Autonomous Vehicles:\n\nUse SLMs to process sensory data from autonomous vehicles, enabling real-time decision-making and navigation. This can be applied to drones, robots, and self-driving cars for enhanced autonomy and safety.\n\n8. Cultural Heritage and Tourism:\n\nImplement SLMs to provide interactive and informative cultural heritage sites and museum guides. Visitors can use these systems to get real-time information and insights, enhancing their experience without internet connectivity.\n\n9. Artistic and Creative Projects:\n\nUse SLMs to analyze and generate creative content, such as music, art, and literature. This can foster innovative projects in the creative industries and allow for unique interactive experiences in exhibitions and performances.\n\n10. Customized Assistive Technologies:\n\nDevelop assistive technologies for individuals with disabilities, providing personalized and adaptive support through real-time text-to-speech, language translation, and other accessible tools.",
    "crumbs": [
      "Raspberry Pi",
      "Small Language Models (SLM)"
    ]
  },
  {
    "objectID": "contents/labs/raspi/llm/llm.html#resources",
    "href": "contents/labs/raspi/llm/llm.html#resources",
    "title": "Small Language Models (SLM)",
    "section": "Resources",
    "text": "Resources\n\n10-Ollama_Python_Library notebook\n20-Ollama_Function_Calling notebook\n30-Function_Calling_with_images notebook\n40-RAG-simple-bee notebook\ncalc_distance_image python script",
    "crumbs": [
      "Raspberry Pi",
      "Small Language Models (SLM)"
    ]
  },
  {
    "objectID": "contents/labs/raspi/vlm/vlm.html",
    "href": "contents/labs/raspi/vlm/vlm.html",
    "title": "Vision-Language Models (VLM)",
    "section": "",
    "text": "Introduction\nIn this hands-on lab, we will continuously explore AI applications at the Edge, from the basic setup of the Florence-2, Microsoft’s state-of-the-art vision foundation model, to advanced implementations on devices like the Raspberry Pi. We will learn to use Vision-Languageor Models (VLMs) for tasks such as captioning, object detection, grounding, segmentation, and OCR on a Raspberry Pi.",
    "crumbs": [
      "Raspberry Pi",
      "Vision-Language Models (VLM)"
    ]
  },
  {
    "objectID": "contents/labs/raspi/vlm/vlm.html#introduction",
    "href": "contents/labs/raspi/vlm/vlm.html#introduction",
    "title": "Vision-Language Models (VLM)",
    "section": "",
    "text": "Why Florence-2 at the Edge?\nFlorence-2 is a vision-language model open-sourced by Microsoft under the MIT license, which significantly advances vision-language models by combining a lightweight architecture with robust capabilities. Thanks to its training on the massive FLD-5B dataset, which contains 126 million images and 5.4 billion visual annotations, it achieves performance comparable to larger models. This makes Florence-2 ideal for deployment at the edge, where power and computational resources are limited.\nIn this tutorial, we will explore how to use Florence-2 for real-time computer vision applications, such as:\n\nImage captioning\nObject detection\nSegmentation\nVisual grounding\n\n\nVisual grounding involves linking textual descriptions to specific regions within an image. This enables the model to understand where particular objects or entities described in a prompt are in the image. For example, if the prompt is “a red car,” the model will identify and highlight the region where the red car is found in the image. Visual grounding is helpful for applications where precise alignment between text and visual content is needed, such as human-computer interaction, image annotation, and interactive AI systems.\n\nIn the tutorial, we will walk through:\n\nSetting up Florence-2 on the Raspberry Pi\nRunning inference tasks such as object detection and captioning\nOptimizing the model to get the best performance from the edge device\nExploring practical, real-world applications with fine-tuning.\n\n\n\nFlorence-2 Model Architecture\nFlorence-2 utilizes a unified, prompt-based representation to handle various vision-language tasks. The model architecture consists of two main components: an image encoder and a multi-modal transformer encoder-decoder.\n\n\nImage Encoder: The image encoder is based on the DaViT (Dual Attention Vision Transformers) architecture. It converts input images into a series of visual token embeddings. These embeddings serve as the foundational representations of the visual content, capturing both spatial and contextual information about the image.\nMulti-Modal Transformer Encoder-Decoder: Florence-2’s core is the multi-modal transformer encoder-decoder, which combines visual token embeddings from the image encoder with textual embeddings generated by a BERT-like model. This combination allows the model to simultaneously process visual and textual inputs, enabling a unified approach to tasks such as image captioning, object detection, and segmentation.\n\nThe model’s training on the extensive FLD-5B dataset ensures it can effectively handle diverse vision tasks without requiring task-specific modifications. Florence-2 uses textual prompts to activate specific tasks, making it highly flexible and capable of zero-shot generalization. For tasks like object detection or visual grounding, the model incorporates additional location tokens to represent regions within the image, ensuring a precise understanding of spatial relationships.\n\nFlorence-2’s compact architecture and innovative training approach allow it to perform computer vision tasks accurately, even on resource-constrained devices like the Raspberry Pi.",
    "crumbs": [
      "Raspberry Pi",
      "Vision-Language Models (VLM)"
    ]
  },
  {
    "objectID": "contents/labs/raspi/vlm/vlm.html#technical-overview",
    "href": "contents/labs/raspi/vlm/vlm.html#technical-overview",
    "title": "Vision-Language Models (VLM)",
    "section": "Technical Overview",
    "text": "Technical Overview\nFlorence-2 introduces several innovative features that set it apart:\n\nArchitecture\n\n\nLightweight Design: Two variants available\n\nFlorence-2-Base: 232 million parameters\nFlorence-2-Large: 771 million parameters\n\nUnified Representation: Handles multiple vision tasks through a single architecture\nDaViT Vision Encoder: Converts images into visual token embeddings\nTransformer-based Multi-modal Encoder-Decoder: Processes combined visual and text embeddings\n\n\n\nTraining Dataset (FLD-5B)\n\n\n126 million unique images\n5.4 billion comprehensive annotations, including:\n\n500M text annotations\n1.3B region-text annotations\n3.6B text-phrase-region annotations\n\nAutomated annotation pipeline using specialist models\nIterative refinement process for high-quality labels\n\n\n\nKey Capabilities\nFlorence-2 excels in multiple vision tasks:\n\nZero-shot Performance\n\nImage Captioning: Achieves 135.6 CIDEr score on COCO\nVisual Grounding: 84.4% recall@1 on Flickr30k\nObject Detection: 37.5 mAP on COCO val2017\nReferring Expression: 67.0% accuracy on RefCOCO\n\n\n\nFine-tuned Performance\n\nCompetitive with specialist models despite the smaller size\nOutperforms larger models in specific benchmarks\nEfficient adaptation to new tasks\n\n\n\n\nPractical Applications\nFlorence-2 can be applied across various domains:\n\nContent Understanding\n\nAutomated image captioning for accessibility\nVisual content moderation\nMedia asset management\n\nE-commerce\n\nProduct image analysis\nVisual search\nAutomated product tagging\n\nHealthcare\n\nMedical image analysis\nDiagnostic assistance\nResearch data processing\n\nSecurity & Surveillance\n\nObject detection and tracking\nAnomaly detection\nScene understanding\n\n\n\n\nComparing Florence-2 with other VLMs\nFlorence-2 stands out from other visual language models due to its impressive zero-shot capabilities. Unlike models like Google PaliGemma, which rely on extensive fine-tuning to adapt to various tasks, Florence-2 works right out of the box, as we will see in this lab. It can also compete with larger models like GPT-4V and Flamingo, which often have many more parameters but only sometimes match Florence-2’s performance. For example, Florence-2 achieves better zero-shot results than Kosmos-2 despite having over twice the parameters.\nIn benchmark tests, Florence-2 has shown remarkable performance in tasks like COCO captioning and referring expression comprehension. It outperformed models like PolyFormer and UNINEXT in object detection and segmentation tasks on the COCO dataset. It is a highly competitive choice for real-world applications where both performance and resource efficiency are crucial.",
    "crumbs": [
      "Raspberry Pi",
      "Vision-Language Models (VLM)"
    ]
  },
  {
    "objectID": "contents/labs/raspi/vlm/vlm.html#setup-and-installation",
    "href": "contents/labs/raspi/vlm/vlm.html#setup-and-installation",
    "title": "Vision-Language Models (VLM)",
    "section": "Setup and Installation",
    "text": "Setup and Installation\nOur choice of edge device is the Raspberry Pi 5 (Raspi-5). Its robust platform is equipped with the Broadcom BCM2712, a 2.4GHz quad-core 64-bit Arm Cortex-A76 CPU featuring Cryptographic Extension and enhanced caching capabilities. It boasts a VideoCore VII GPU, dual 4Kp60 HDMI® outputs with HDR, and a 4Kp60 HEVC decoder. Memory options include 4GB and 8GB of high-speed LPDDR4X SDRAM, with 8GB being our choice to run Florence-2. It also features expandable storage via a microSD card slot and a PCIe 2.0 interface for fast peripherals such as M.2 SSDs (Solid State Drives).\n\nFor real applications, SSDs are a better option than SD cards.\n\nWe suggest installing an Active Cooler, a dedicated clip-on cooling solution for Raspberry Pi 5 (Raspi-5), for this lab. It combines an aluminum heatsink with a temperature-controlled blower fan to keep the Raspi-5 operating comfortably under heavy loads, such as running Florense-2.\n\n\nEnvironment configuration\nTo run Microsoft Florense-2 on the Raspberry Pi 5, we’ll need a few libraries:\n\nTransformers:\n\nFlorence-2 uses the transformers library from Hugging Face for model loading and inference. This library provides the architecture for working with pre-trained vision-language models, making it easy to perform tasks like image captioning, object detection, and more. Essentially, transformers helps in interacting with the model, processing input prompts, and obtaining outputs.\n\nPyTorch:\n\nPyTorch is a deep learning framework that provides the infrastructure needed to run the Florence-2 model, which includes tensor operations, GPU acceleration (if a GPU is available), and model training/inference functionalities. The Florence-2 model is trained in PyTorch, and we need it to leverage its functions, layers, and computation capabilities to perform inferences on the Raspberry Pi.\n\nTimm (PyTorch Image Models):\n\nFlorence-2 uses timm to access efficient implementations of vision models and pre-trained weights. Specifically, the timm library is utilized for the image encoder part of Florence-2, particularly for managing the DaViT architecture. It provides model definitions and optimized code for common vision tasks and allows the easy integration of different backbones that are lightweight and suitable for edge devices.\n\nEinops:\n\nEinops is a library for flexible and powerful tensor operations. It makes it easy to reshape and manipulate tensor dimensions, which is especially important for the multi-modal processing done in Florence-2. Vision-language models like Florence-2 often need to rearrange image data, text embeddings, and visual embeddings to align correctly for the transformer blocks, and einops simplifies these complex operations, making the code more readable and concise.\n\n\nIn short, these libraries enable different essential components of Florence-2:\n\nTransformers and PyTorch are needed to load the model and run the inference.\nTimm is used to access and efficiently implement the vision encoder.\nEinops helps reshape data, facilitating the integration of visual and text features.\n\nAll these components work together to help Florence-2 run seamlessly on our Raspberry Pi, allowing it to perform complex vision-language tasks relatively quickly.\nConsidering that the Raspberry Pi already has its OS installed, let’s use SSH to reach it from another computer:\nssh mjrovai@raspi-5.local\nAnd check the IP allocated to it:\nhostname -I\n192.168.4.209\n\nUpdating the Raspberry Pi\nFirst, ensure your Raspberry Pi is up to date:\nsudo apt update\nsudo apt upgrade -y\nInitial setup for using PIP:\nsudo apt install python3-pip\nsudo rm /usr/lib/python3.11/EXTERNALLY-MANAGED\npip3 install --upgrade pip\nInstall Dependencies\nsudo apt-get install libjpeg-dev libopenblas-dev libopenmpi-dev libomp-dev\nLet’s set up and activate a Virtual Environment for working with Florence-2:\npython3 -m venv ~/florence\nsource ~/florence/bin/activate\nInstall PyTorch\npip3 install setuptools numpy Cython\npip3 install requests\npip3 install torch torchvision --index-url https://download.pytorch.org/whl/cpu\npip3 install torchaudio --index-url https://download.pytorch.org/whl/cpu\nLet’s verify that PyTorch is correctly installed:\n\nInstall Transformers, Timm and Einops:\npip3 install transformers\npip3 install timm einops\nInstall the model:\npip3 install autodistill-florence-2\nJupyter Notebook and Python libraries\nInstalling a Jupyter Notebook to run and test our Python scripts is possible.\npip3 install jupyter\npip3 install numpy Pillow matplotlib\njupyter notebook --generate-config\n\n\nTesting the installation\nRunning the Jupyter Notebook on the remote computer\njupyter notebook --ip=192.168.4.209 --no-browser\nRunning the above command on the SSH terminal, we can see the local URL address to open the notebook:\n\nThe notebook with the code used on this initial test can be found on the Lab GitHub:\n\n10-florence2_test.ipynb\n\nWe can access it on the remote computer by entering the Raspberry Pi’s IP address and the provided token in a web browser ( copy the entire URL from the terminal).\nFrom the Home page, create a new notebook [Python 3 (ipykernel) ] and copy and paste the example code from Hugging Face Hub.\nThe code is designed to run Florence-2 on a given image to perform object detection. It loads the model, processes an image and a prompt, and then generates a response to identify and describe the objects in the image.\n\nThe processor helps prepare text and image inputs.\nThe model takes the processed inputs to generate a meaningful response.\nThe post-processing step refines the generated output into a more interpretable form, like bounding boxes for detected objects.\n\n\nThis workflow leverages the versatility of Florence-2 to handle vision-language tasks and is implemented efficiently using PyTorch, Transformers, and related image-processing tools.\n\nimport requests\nfrom PIL import Image\nimport torch\nfrom transformers import AutoProcessor, AutoModelForCausalLM \n\ndevice = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\ntorch_dtype = torch.float16 if torch.cuda.is_available() else torch.float32\n\nmodel = AutoModelForCausalLM.from_pretrained(\"microsoft/Florence-2-base\",\n                                             torch_dtype=torch_dtype, \n                                             trust_remote_code=True).to(device)\nprocessor = AutoProcessor.from_pretrained(\"microsoft/Florence-2-base\", \n                                          trust_remote_code=True)\n\nprompt = \"&lt;OD&gt;\"\n\nurl = \"https://huggingface.co/datasets/huggingface/documentation-\nimages/resolve/main/transformers/tasks/car.jpg?download=true\"\nimage = Image.open(requests.get(url, stream=True).raw)\n\ninputs = processor(text=prompt, images=image, return_tensors=\"pt\").to(\n  device, torch_dtype)\n\ngenerated_ids = model.generate(\n    input_ids=inputs[\"input_ids\"],\n    pixel_values=inputs[\"pixel_values\"],\n    max_new_tokens=1024,\n    do_sample=False,\n    num_beams=3,\n)\ngenerated_text = processor.batch_decode(generated_ids, skip_special_tokens=False)[0]\n\nparsed_answer = processor.post_process_generation(generated_text, task=\"&lt;OD&gt;\", \n                                                  image_size=(image.width, \n                                                              image.height))\n\nprint(parsed_answer)\nLet’s break down the provided code step by step:\n\n1. Importing Required Libraries\nimport requests\nfrom PIL import Image\nimport torch\nfrom transformers import AutoProcessor, AutoModelForCausalLM \n\nrequests: Used to make HTTP requests. In this case, it downloads an image from a URL.\nPIL (Pillow): Provides tools for manipulating images. Here, it’s used to open the downloaded image.\ntorch: PyTorch is imported to handle tensor operations and determine the hardware availability (CPU or GPU).\ntransformers: This module provides easy access to Florence-2 by using AutoProcessor and AutoModelForCausalLM to load pre-trained models and process inputs.\n\n\n\n2. Determining the Device and Data Type\ndevice = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\ntorch_dtype = torch.float16 if torch.cuda.is_available() else torch.float32\n\nDevice Setup: The code checks if a CUDA-enabled GPU is available (torch.cuda.is_available()). The device is set to “cuda:0” if a GPU is available. Otherwise, it defaults to \"cpu\" (our case here).\nData Type Setup: If a GPU is available, torch.float16 is chosen, which uses half-precision floats to speed up processing and reduce memory usage. On the CPU, it defaults to torch.float32 to maintain compatibility.\n\n\n\n3. Loading the Model and Processor\nmodel = AutoModelForCausalLM.from_pretrained(\"microsoft/Florence-2-base\", \n                                             torch_dtype=torch_dtype,\n                                             trust_remote_code=True).to(device)\nprocessor = AutoProcessor.from_pretrained(\"microsoft/Florence-2-base\",\n                                          trust_remote_code=True)\n\nModel Initialization:\n\nAutoModelForCausalLM.from_pretrained() loads the pre-trained Florence-2 model from Microsoft’s repository on Hugging Face. The torch_dtype is set according to the available hardware (GPU/CPU), and trust_remote_code=True allows the use of any custom code that might be provided with the model.\n.to(device) moves the model to the appropriate device (either CPU or GPU). In our case, it will be set to CPU.\n\nProcessor Initialization:\n\nAutoProcessor.from_pretrained() loads the processor for Florence-2. The processor is responsible for transforming text and image inputs into a format the model can work with (e.g., encoding text, normalizing images, etc.).\n\n\n\n\n\n4. Defining the Prompt\nprompt = \"&lt;OD&gt;\"\n\nPrompt Definition: The string \"&lt;OD&gt;\" is used as a prompt. This refers to “Object Detection”, instructing the model to detect objects on the image.\n\n\n5. Downloading and Loading the Image\nurl = \"https://huggingface.co/datasets/huggingface/documentation-\\\nimages/resolve/main/transformers/tasks/car.jpg?download=true\"\nimage = Image.open(requests.get(url, stream=True).raw)\n\nDownloading the Image: The requests.get() function fetches the image from the specified URL. The stream=True parameter ensures the image is streamed rather than downloaded completely at once.\nOpening the Image: Image.open() opens the image so the model can process it.\n\n\n\n6. Processing Inputs\ninputs = processor(text=prompt, images=image, return_tensors=\"pt\").to(device, \n                                                                      torch_dtype)\n\nProcessing Input Data: The processor() function processes the text (prompt) and the image (image). The return_tensors=\"pt\" argument converts the processed data into PyTorch tensors, which are necessary for inputting data into the model.\nMoving Inputs to Device: .to(device, torch_dtype) moves the inputs to the correct device (CPU or GPU) and assigns the appropriate data type.\n\n\n\n\n7. Generating the Output\ngenerated_ids = model.generate(\n    input_ids=inputs[\"input_ids\"],\n    pixel_values=inputs[\"pixel_values\"],\n    max_new_tokens=1024,\n    do_sample=False,\n    num_beams=3,\n)\n\nModel Generation: model.generate() is used to generate the output based on the input data.\n\ninput_ids: Represents the tokenized form of the prompt.\npixel_values: Contains the processed image data.\nmax_new_tokens=1024: Specifies the maximum number of new tokens to be generated in the response. This limits the response length.\ndo_sample=False: Disables sampling; instead, the generation uses deterministic methods (beam search).\nnum_beams=3: Enables beam search with three beams, which improves output quality by considering multiple possibilities during generation.\n\n\n\n8. Decoding the Generated Text\ngenerated_text = processor.batch_decode(generated_ids, skip_special_tokens=False)[0]\n\nBatch Decode: processor.batch_decode() decodes the generated IDs (tokens) into readable text. The skip_special_tokens=False parameter means that the output will include any special tokens that may be part of the response.\n\n\n\n9. Post-processing the Generation\nparsed_answer = processor.post_process_generation(generated_text, task=\"&lt;OD&gt;\", \n                                                  image_size=(image.width, \n                                                              image.height))\n\nPost-Processing: processor.post_process_generation() is called to process the generated text further, interpreting it based on the task (\"&lt;OD&gt;\" for object detection) and the size of the image.\nThis function extracts specific information from the generated text, such as bounding boxes for detected objects, making the output more useful for visual tasks.\n\n\n\n10. Printing the Output\nprint(parsed_answer)\n\nFinally, print(parsed_answer) displays the output, which could include object detection results, such as bounding box coordinates and labels for the detected objects in the image.\n\n\n\nResult\nRunning the code, we get as the Parsed Answer:\n{'&lt;OD&gt;': {'bboxes': [[34.23999786376953, 160.0800018310547, 597.4400024414062, \n371.7599792480469], [272.32000732421875, 241.67999267578125, 303.67999267578125, \n247.4399871826172], [454.0799865722656, 276.7200012207031, 553.9199829101562, \n370.79998779296875], [96.31999969482422, 280.55999755859375, 198.0800018310547, \n371.2799987792969]], 'labels': ['car', 'door handle', 'wheel', 'wheel']}}\nFirst, let’s inspect the image:\nimport matplotlib.pyplot as plt\nplt.figure(figsize=(8, 8))\nplt.imshow(image)\nplt.axis('off')\nplt.show()\n\nBy the Object Detection result, we can see that:\n'labels': ['car', 'door handle', 'wheel', 'wheel']\nIt seems that at least a few objects were detected. We can also implement a code to draw the bounding boxes in the find objects:\ndef plot_bbox(image, data):\n   # Create a figure and axes\n    fig, ax = plt.subplots()\n\n    # Display the image\n    ax.imshow(image)\n\n    # Plot each bounding box\n    for bbox, label in zip(data['bboxes'], data['labels']):\n        # Unpack the bounding box coordinates\n        x1, y1, x2, y2 = bbox\n        # Create a Rectangle patch\n        rect = patches.Rectangle((x1, y1), x2-x1, y2-y1, linewidth=1, \n                                 edgecolor='r', facecolor='none')\n        # Add the rectangle to the Axes\n        ax.add_patch(rect)\n        # Annotate the label\n        plt.text(x1, y1, label, color='white', fontsize=8, \n                 bbox=dict(facecolor='red', alpha=0.5))\n\n    # Remove the axis ticks and labels\n    ax.axis('off')\n\n    # Show the plot\n    plt.show()\n\nBox (x0, y0, x1, y1): Location tokens correspond to the top-left and bottom-right corners of a box.\n\nAnd running\nplot_bbox(image, parsed_answer['&lt;OD&gt;'])\nWe get:",
    "crumbs": [
      "Raspberry Pi",
      "Vision-Language Models (VLM)"
    ]
  },
  {
    "objectID": "contents/labs/raspi/vlm/vlm.html#florence-2-tasks",
    "href": "contents/labs/raspi/vlm/vlm.html#florence-2-tasks",
    "title": "Vision-Language Models (VLM)",
    "section": "Florence-2 Tasks",
    "text": "Florence-2 Tasks\nFlorence-2 is designed to perform a variety of computer vision and vision-language tasks through prompts. These tasks can be activated by providing a specific textual prompt to the model, as we saw with &lt;OD&gt; (Object Detection).\nFlorence-2’s versatility comes from combining these prompts, allowing us to guide the model’s behavior to perform specific vision tasks. Changing the prompt allows us to adapt Florence-2 to different tasks without needing task-specific modifications in the architecture. This capability directly results from Florence-2’s unified model architecture and large-scale multi-task training on the FLD-5B dataset.\nHere are some of the key tasks that Florence-2 can perform, along with example prompts:\n\n1. Object Detection (OD)\n\nPrompt: \"&lt;OD&gt;\"\nDescription: Identifies objects in an image and provides bounding boxes for each detected object. This task is helpful for applications like visual inspection, surveillance, and general object recognition.\n\n\n\n2. Image Captioning\n\nPrompt: \"&lt;CAPTION&gt;\"\nDescription: Generates a textual description for an input image. This task helps the model describe what is happening in the image, providing a human-readable caption for content understanding.\n\n\n\n3. Detailed Captioning\n\nPrompt: \"&lt;DETAILED_CAPTION&gt;\"\nDescription: Generates a more detailed caption with more nuanced information about the scene, such as the objects present and their relationships.\n\n\n\n4. Visual Grounding\n\nPrompt: \"&lt;CAPTION_TO_PHRASE_GROUNDING&gt;\"\nDescription: Links a textual description to specific regions in an image. For example, given a prompt like “a green car,” the model highlights where the green car is in the image. This is useful for human-computer interaction, where you must find specific objects based on text.\n\n\n\n5. Segmentation\n\nPrompt: \"&lt;REFERRING_EXPRESSION_SEGMENTATION&gt;\"\nDescription: Performs segmentation based on a referring expression, such as “the blue cup.” The model identifies and segments the specific region containing the object mentioned in the prompt (all related pixels).\n\n\n\n6. Dense Region Captioning\n\nPrompt: \"&lt;DENSE_REGION_CAPTION&gt;\"\nDescription: Provides captions for multiple regions within an image, offering a detailed breakdown of all visible areas, including different objects and their relationships.\n\n\n\n7. OCR with Region\n\nPrompt: \"&lt;OCR_WITH_REGION&gt;\"\nDescription: Performs Optical Character Recognition (OCR) on an image and provides bounding boxes for the detected text. This is useful for extracting and locating textual information in images, such as reading signs, labels, or other forms of text in images.\n\n\n\n8. Phrase Grounding for Specific Expressions\n\nPrompt: \"&lt;CAPTION_TO_PHRASE_GROUNDING&gt;\" along with a specific expression, such as \"a wine glass\".\nDescription: Locates the area in the image that corresponds to a specific textual phrase. This task allows for identifying particular objects or elements when prompted with a word or keyword.\n\n\n\n9. Open Vocabulary Object Detection\n\nPrompt: \"&lt;OPEN_VOCABULARY_OD&gt;\"\nDescription: The model can detect objects without being restricted to a predefined list of classes, making it helpful in recognizing a broader range of items based on general visual understanding.",
    "crumbs": [
      "Raspberry Pi",
      "Vision-Language Models (VLM)"
    ]
  },
  {
    "objectID": "contents/labs/raspi/vlm/vlm.html#exploring-computer-vision-and-vision-language-tasks",
    "href": "contents/labs/raspi/vlm/vlm.html#exploring-computer-vision-and-vision-language-tasks",
    "title": "Vision-Language Models (VLM)",
    "section": "Exploring computer vision and vision-language tasks",
    "text": "Exploring computer vision and vision-language tasks\nFor exploration, all codes can be found on the GitHub:\n\n20-florence_2.ipynb\n\nLet’s use a couple of images created by Dall-E and upload them to the Rasp-5 (FileZilla can be used for that). The images will be saved on a sub-folder named images :\ndogs_cats = Image.open('./images/dogs-cats.jpg')\ntable = Image.open('./images/table.jpg')\n\nLet’s create a function to facilitate our exploration and to keep track of the latency of the model for different tasks:\ndef run_example(task_prompt, text_input=None, image=None):\n    start_time = time.perf_counter()  # Start timing\n    if text_input is None:\n        prompt = task_prompt\n    else:\n        prompt = task_prompt + text_input\n    inputs = processor(text=prompt, images=image, \n                       return_tensors=\"pt\").to(device)\n    generated_ids = model.generate(\n      input_ids=inputs[\"input_ids\"],\n      pixel_values=inputs[\"pixel_values\"],\n      max_new_tokens=1024,\n      early_stopping=False,\n      do_sample=False,\n      num_beams=3,\n    )\n    generated_text = processor.batch_decode(generated_ids, \n                                            skip_special_tokens=False)[0]\n    parsed_answer = processor.post_process_generation(\n        generated_text,\n        task=task_prompt,\n        image_size=(image.width, image.height)\n    )\n    \n    end_time = time.perf_counter()  # End timing\n    elapsed_time = end_time - start_time  # Calculate elapsed time\n    print(f\" \\n[INFO] ==&gt; Florence-2-base ({task_prompt}), \n          took {elapsed_time:.1f} seconds to execute.\\n\")\n    \n    return parsed_answer\n\nCaption\n1. Dogs and Cats\nrun_example(task_prompt='&lt;CAPTION&gt;',image=dogs_cats)\n[INFO] ==&gt; Florence-2-base (&lt;CAPTION&gt;), took 16.1 seconds to execute.\n\n{'&lt;CAPTION&gt;': 'A group of dogs and cats sitting in a garden.'}\n2. Table\nrun_example(task_prompt='&lt;CAPTION&gt;',image=table)\n[INFO] ==&gt; Florence-2-base (&lt;CAPTION&gt;), took 16.5 seconds to execute.\n\n{'&lt;CAPTION&gt;': 'A wooden table topped with a plate of fruit and a glass of wine.'}\n\n\nDETAILED_CAPTION\n1. Dogs and Cats\nrun_example(task_prompt='&lt;DETAILED_CAPTION&gt;',image=dogs_cats)\n[INFO] ==&gt; Florence-2-base (&lt;DETAILED_CAPTION&gt;), took 25.5 seconds to execute.\n\n{'&lt;DETAILED_CAPTION&gt;': 'The image shows a group of cats and dogs sitting on top of a\nlush green field, surrounded by plants with flowers, trees, and a house in the \nbackground. The sky is visible above them, creating a peaceful atmosphere.'}\n2. Table\nrun_example(task_prompt='&lt;DETAILED_CAPTION&gt;',image=table)\n[INFO] ==&gt; Florence-2-base (&lt;DETAILED_CAPTION&gt;), took 26.8 seconds to execute.\n\n{'&lt;DETAILED_CAPTION&gt;': 'The image shows a wooden table with a bottle of wine and a \nglass of wine on it, surrounded by a variety of fruits such as apples, oranges, and \ngrapes. In the background, there are chairs, plants, trees, and a house, all slightly \nblurred.'}\n\n\nMORE_DETAILED_CAPTION\n1. Dogs and Cats\nrun_example(task_prompt='&lt;MORE_DETAILED_CAPTION&gt;',image=dogs_cats)\n[INFO] ==&gt; Florence-2-base (&lt;MORE_DETAILED_CAPTION&gt;), took 49.8 seconds to execute.\n\n{'&lt;MORE_DETAILED_CAPTION&gt;': 'The image shows a group of four cats and a dog in a garden. \nThe garden is filled with colorful flowers and plants, and there is a pathway leading up \nto a house in the background. The main focus of the image is a large German Shepherd dog \nstanding on the left side of the garden, with its tongue hanging out and its mouth open, \nas if it is panting or panting. On the right side, there are two smaller cats, one orange \nand one gray, sitting on the grass. In the background, there is another golden retriever \ndog sitting and looking at the camera. The sky is blue and the sun is shining, creating a \nwarm and inviting atmosphere.'}\n2. Table\nrun_example(task_prompt='&lt; MORE_DETAILED_CAPTION&gt;',image=table)\nINFO] ==&gt; Florence-2-base (&lt;MORE_DETAILED_CAPTION&gt;), took 32.4 seconds to execute.\n\n{'&lt;MORE_DETAILED_CAPTION&gt;': 'The image shows a wooden table with a wooden tray on it. On \nthe tray, there are various fruits such as grapes, oranges, apples, and grapes. There is \nalso a bottle of red wine on the table. The background shows a garden with trees and a \nhouse. The overall mood of the image is peaceful and serene.'}\n\nWe can note that the more detailed the caption task, the longer the latency and the possibility of mistakes (like “The image shows a group of four cats and a dog in a garden”, instead of two dogs and three cats).\n\n\n\nOD - Object Detection\nWe can run the same previous function for object detection using the prompt &lt;OD&gt;.\ntask_prompt = '&lt;OD&gt;'\nresults = run_example(task_prompt,image=dogs_cats)\nprint(results)\nLet’s see the result:\n[INFO] ==&gt; Florence-2-base (&lt;OD&gt;), took 20.9 seconds to execute.\n\n{'&lt;OD&gt;': {'bboxes': [[737.7920532226562, 571.904052734375, 1022.4640502929688, \n980.4800415039062], [0.5120000243186951, 593.4080200195312, 211.4560089111328, \n991.7440185546875], [445.9520263671875, 721.4080200195312, 680.4480590820312, \n850.4320678710938], [39.42400360107422, 91.64800262451172, 491.0080261230469, \n933.3760375976562], [570.8800048828125, 184.83201599121094, 974.3360595703125, \n782.8480224609375]], 'labels': ['cat', 'cat', 'cat', 'dog', 'dog']}}\nOnly by the labels ['cat,' 'cat,' 'cat,' 'dog,' 'dog'] is it possible to see that the main objects in the image were captured. Let’s apply the function used before to draw the bounding boxes:\nplot_bbox(dogs_cats, results['&lt;OD&gt;'])\n\nLet’s also do it with the Table image:\ntask_prompt = '&lt;OD&gt;'\nresults = run_example(task_prompt,image=table)\nplot_bbox(table, results['&lt;OD&gt;'])\n[INFO] ==&gt; Florence-2-base (&lt;OD&gt;), took 40.8 seconds to execute.\n\n\n\nDENSE_REGION_CAPTION\nIt is possible to mix the classic Object Detection with the Caption task in specific sub-regions of the image:\ntask_prompt = '&lt;DENSE_REGION_CAPTION&gt;'\n\nresults = run_example(task_prompt,image=dogs_cats)\nplot_bbox(dogs_cats, results['&lt;DENSE_REGION_CAPTION&gt;'])\n\nresults = run_example(task_prompt,image=table)\nplot_bbox(table, results['&lt;DENSE_REGION_CAPTION&gt;'])\n\n\n\nCAPTION_TO_PHRASE_GROUNDING\nWith this task, we can enter with a caption, such as “a wine glass”, “a wine bottle,” or “a half orange,” and Florence-2 will localize the object in the image:\ntask_prompt = '&lt;CAPTION_TO_PHRASE_GROUNDING&gt;'\n\nresults = run_example(task_prompt, text_input=\"a wine bottle\",image=table)\nplot_bbox(table, results['&lt;CAPTION_TO_PHRASE_GROUNDING&gt;'])\n\nresults = run_example(task_prompt, text_input=\"a wine glass\",image=table)\nplot_bbox(table, results['&lt;CAPTION_TO_PHRASE_GROUNDING&gt;'])\n\nresults = run_example(task_prompt, text_input=\"a half orange\",image=table)\nplot_bbox(table, results['&lt;CAPTION_TO_PHRASE_GROUNDING&gt;'])\n\n[INFO] ==&gt; Florence-2-base (&lt;CAPTION_TO_PHRASE_GROUNDING&gt;), took 15.7 seconds to execute\neach task.\n\n\nCascade Tasks\nWe can also enter the image caption as the input text to push Florence-2 to find more objects:\ntask_prompt = '&lt;CAPTION&gt;'\nresults = run_example(task_prompt,image=dogs_cats)\ntext_input = results[task_prompt]\ntask_prompt = '&lt;CAPTION_TO_PHRASE_GROUNDING&gt;'\nresults = run_example(task_prompt, text_input,image=dogs_cats)\nplot_bbox(dogs_cats, results['&lt;CAPTION_TO_PHRASE_GROUNDING&gt;'])\nChanging the task_prompt among &lt;CAPTION,&gt; &lt;DETAILED_CAPTION&gt; and &lt;MORE_DETAILED_CAPTION&gt;, we will get more objects in the image.\n\n\n\nOPEN_VOCABULARY_DETECTION\n&lt;OPEN_VOCABULARY_DETECTION&gt; allows Florence-2 to detect recognizable objects in an image without relying on a predefined list of categories, making it a versatile tool for identifying various items that may not have been explicitly labeled during training. Unlike &lt;CAPTION_TO_PHRASE_GROUNDING&gt;, which requires a specific text phrase to locate and highlight a particular object in an image, &lt;OPEN_VOCABULARY_DETECTION&gt; performs a broad scan to find and classify all objects present.\nThis makes &lt;OPEN_VOCABULARY_DETECTION&gt; particularly useful for applications where you need a comprehensive overview of everything in an image without prior knowledge of what to expect. Enter with a text describing specific objects not previously detected, resulting in their detection. For example:\ntask_prompt = '&lt;OPEN_VOCABULARY_DETECTION&gt;'\ntext = [\"a house\", \"a tree\", \"a standing cat at the left\", \n        \"a sleeping cat on the ground\", \"a standing cat at the right\", \n        \"a yellow cat\"]\nfor txt in text:\n    results = run_example(task_prompt, text_input=txt,image=dogs_cats)\n    bbox_results  = convert_to_od_format(results['&lt;OPEN_VOCABULARY_DETECTION&gt;'])\n    plot_bbox(dogs_cats, bbox_results)\n\n[INFO] ==&gt; Florence-2-base (&lt;OPEN_VOCABULARY_DETECTION&gt;), took 15.1 seconds to execute \neach task.\n\nNote: Trying to use Florence-2 to find objects that were not found can leads to mistakes (see exaamples on the Notebook).\n\n\n\nReferring expression segmentation\nWe can also segment a specific object in the image and give its description (caption), such as “a wine bottle” on the table image or “a German Sheppard” on the dogs_cats.\nReferring expression segmentation results format: {'&lt;REFERRING_EXPRESSION_SEGMENTATION&gt;': {'Polygons': [[[polygon]], ...], 'labels': ['', '', ...]}}, one object is represented by a list of polygons. each polygon is [x1, y1, x2, y2, ..., xn, yn].\n\nPolygon (x1, y1, …, xn, yn): Location tokens represent the vertices of a polygon in clockwise order.\n\nSo, let’s first create a function to plot the segmentation:\nfrom PIL import Image, ImageDraw, ImageFont\nimport copy\nimport random\nimport numpy as np\ncolormap = ['blue','orange','green','purple','brown','pink','gray','olive',\n    'cyan','red','lime','indigo','violet','aqua','magenta','coral','gold',\n    'tan','skyblue']\n\ndef draw_polygons(image, prediction, fill_mask=False):\n    \"\"\"\n    Draws segmentation masks with polygons on an image.\n\n    Parameters:\n    - image_path: Path to the image file.\n    - prediction: Dictionary containing 'polygons' and 'labels' keys.\n                  'polygons' is a list of lists, each containing vertices \n                  of a polygon.\n                  'labels' is a list of labels corresponding to each polygon.\n    - fill_mask: Boolean indicating whether to fill the polygons with color.\n    \"\"\"\n    # Load the image\n\n    draw = ImageDraw.Draw(image)\n\n\n    # Set up scale factor if needed (use 1 if not scaling)\n    scale = 1\n\n    # Iterate over polygons and labels\n    for polygons, label in zip(prediction['polygons'], prediction['labels']):\n        color = random.choice(colormap)\n        fill_color = random.choice(colormap) if fill_mask else None\n\n        for _polygon in polygons:\n            _polygon = np.array(_polygon).reshape(-1, 2)\n            if len(_polygon) &lt; 3:\n                print('Invalid polygon:', _polygon)\n                continue\n\n            _polygon = (_polygon * scale).reshape(-1).tolist()\n\n            # Draw the polygon\n            if fill_mask:\n                draw.polygon(_polygon, outline=color, fill=fill_color)\n            else:\n                draw.polygon(_polygon, outline=color)\n\n            # Draw the label text\n            draw.text((_polygon[0] + 8, _polygon[1] + 2), label, fill=color)\n    \n    # Save or display the image\n    #image.show()  # Display the image\n    display(image)\nNow we can run the functions:\ntask_prompt = '&lt;REFERRING_EXPRESSION_SEGMENTATION&gt;'\n\nresults = run_example(task_prompt, text_input=\"a wine bottle\",image=table)\noutput_image = copy.deepcopy(table)\ndraw_polygons(output_image, \n              results['&lt;REFERRING_EXPRESSION_SEGMENTATION&gt;'], \n              fill_mask=True)\n\nresults = run_example(task_prompt, text_input=\"a german sheppard\",image=dogs_cats)\noutput_image = copy.deepcopy(dogs_cats)\ndraw_polygons(output_image, \n              results['&lt;REFERRING_EXPRESSION_SEGMENTATION&gt;'], \n              fill_mask=True)\n\n[INFO] ==&gt; Florence-2-base (&lt;REFERRING_EXPRESSION_SEGMENTATION&gt;), took 207.0 seconds to \nexecute each task.\n\n\nRegion to Segmentation\nWith this task, it is also possible to give the object coordinates in the image to segment it. The input format is '&lt;loc_x1&gt;&lt;loc_y1&gt;&lt;loc_x2&gt;&lt;loc_y2&gt;', [x1, y1, x2, y2] , which is the quantized coordinates in [0, 999].\nFor example, when running the code:\ntask_prompt = '&lt;CAPTION_TO_PHRASE_GROUNDING&gt;'\nresults = run_example(task_prompt, text_input=\"a half orange\",image=table)\nresults\nThe results were:\n{'&lt;CAPTION_TO_PHRASE_GROUNDING&gt;': {'bboxes': [[343.552001953125,\n    689.6640625,\n    530.9440307617188,\n    873.9840698242188]],\n  'labels': ['a half']}}\nUsing the bboxes rounded coordinates:\ntask_prompt = '&lt;REGION_TO_SEGMENTATION&gt;'\nresults = run_example(task_prompt, \n                      text_input=\"&lt;loc_343&gt;&lt;loc_690&gt;&lt;loc_531&gt;&lt;loc_874&gt;\",\n                      image=table)\noutput_image = copy.deepcopy(table)\ndraw_polygons(output_image, results['&lt;REGION_TO_SEGMENTATION&gt;'], fill_mask=True)  \nWe got the segmentation of the object on those coordinates (Latency: 83 seconds):\n\n\n\nRegion to Texts\nWe can also give the region (coordinates and ask for a caption):\ntask_prompt = '&lt;REGION_TO_CATEGORY&gt;'\nresults = run_example(task_prompt, text_input=\"&lt;loc_343&gt;&lt;loc_690&gt;&lt;loc_531&gt;\n                      &lt;loc_874&gt;\",image=table)\nresults\n[INFO] ==&gt; Florence-2-base (&lt;REGION_TO_CATEGORY&gt;), took 14.3 seconds to execute.\n{'&lt;REGION_TO_CATEGORY&gt;': 'orange&lt;loc_343&gt;&lt;loc_690&gt;&lt;loc_531&gt;&lt;loc_874&gt;'}\nThe model identified an orange in that region. Let’s ask for a description:\ntask_prompt = '&lt;REGION_TO_DESCRIPTION&gt;'\nresults = run_example(task_prompt, text_input=\"&lt;loc_343&gt;&lt;loc_690&gt;&lt;loc_531&gt;\n                      &lt;loc_874&gt;\",image=table)\nresults\n[INFO] ==&gt; Florence-2-base (&lt;REGION_TO_CATEGORY&gt;), took 14.6 seconds to execute.\n\n{'&lt;REGION_TO_CATEGORY&gt;': 'orange&lt;loc_343&gt;&lt;loc_690&gt;&lt;loc_531&gt;&lt;loc_874&gt;'}\nIn this case, the description did not provide more details, but it could. Try another example.\n\n\nOCR\nWith Florence-2, we can perform Optical Character Recognition (OCR) on an image, getting what is written on it (task_prompt = '&lt;OCR&gt;' and also get the bounding boxes (location) for the detected text (ask_prompt = '&lt;OCR_WITH_REGION&gt;'). Those tasks can help extract and locate textual information in images, such as reading signs, labels, or other forms of text in images.\nLet’s upload a flyer from a talk in Brazil to Raspi. Let’s test works in another language, here Portuguese):\nflayer = Image.open('./images/embarcados.jpg')\n# Display the image\nplt.figure(figsize=(8, 8))\nplt.imshow(flayer)\nplt.axis('off')\n#plt.title(\"Image\")\nplt.show()\n\nLet’s examine the image with '&lt;MORE_DETAILED_CAPTION&gt;' :\n[INFO] ==&gt; Florence-2-base (&lt;MORE_DETAILED_CAPTION&gt;), took 85.2 seconds to execute.\n\n{'&lt;MORE_DETAILED_CAPTION&gt;': 'The image is a promotional poster for an event called \n\"Machine Learning Embarcados\" hosted by Marcelo Roval. The poster has a black background\nwith white text. On the left side of the poster, there is a logo of a coffee cup with the\ntext \"Café Com Embarcados\" above it. Below the logo, it says \"25 de Setembro as 17th\" \nwhich translates to \"25th of September as 17\" in English. \\n\\nOn the right side, there \naretwo smaller text boxes with the names of the participants and their names. The first \ntext box reads \"Democratizando a Inteligência Artificial para Paises em Desenvolvimento\" \nand the second text box says \"Toda quarta-feira\" which is Portuguese for \"Transmissão via \nin Portuguese\".\\n\\nIn the center of the image, there has a photo of Marcelo, a man with a \nbeard and glasses, smiling at the camera. He is wearing a white hard hat and a white \nshirt. The text boxes are in orange and yellow colors.'}\nThe description is very accurate. Let’s get to the more important words with the task OCR:\ntask_prompt = '&lt;OCR&gt;'\nrun_example(task_prompt,image=flayer)\n[INFO] ==&gt; Florence-2-base (&lt;OCR&gt;), took 37.7 seconds to execute.\n{'&lt;OCR&gt;': 'Machine LearningCafécomEmbarcadoEmbarcadosDemocratizando a \nInteligênciaArtificial para Paises em25 de Setembro ás 17hDesenvolvimentoToda quarta-\nfeiraMarcelo RovalProfessor na UNIFIEI eTransmissão viainCo-Director do TinyML4D'}\nLet’s locate the words in the flyer:\ntask_prompt = '&lt;OCR_WITH_REGION&gt;'\nresults = run_example(task_prompt,image=flayer)\nLet’s also create a function to draw bounding boxes around the detected words:\ndef draw_ocr_bboxes(image, prediction):\n    scale = 1\n    draw = ImageDraw.Draw(image)\n    bboxes, labels = prediction['quad_boxes'], prediction['labels']\n    for box, label in zip(bboxes, labels):\n        color = random.choice(colormap)\n        new_box = (np.array(box) * scale).tolist()\n        draw.polygon(new_box, width=3, outline=color)\n        draw.text((new_box[0]+8, new_box[1]+2),\n                    \"{}\".format(label),\n                    align=\"right\",\n\n                    fill=color)\n    display(image)\noutput_image = copy.deepcopy(flayer)\ndraw_ocr_bboxes(output_image, results['&lt;OCR_WITH_REGION&gt;'])\n\nWe can inspect the detected words:\nresults['&lt;OCR_WITH_REGION&gt;']['labels']\n'&lt;/s&gt;Machine Learning',\n 'Café',\n 'com',\n 'Embarcado',\n 'Embarcados',\n 'Democratizando a Inteligência',\n 'Artificial para Paises em',\n '25 de Setembro ás 17h',\n 'Desenvolvimento',\n 'Toda quarta-feira',\n 'Marcelo Roval',\n 'Professor na UNIFIEI e',\n 'Transmissão via',\n 'in',\n 'Co-Director do TinyML4D']",
    "crumbs": [
      "Raspberry Pi",
      "Vision-Language Models (VLM)"
    ]
  },
  {
    "objectID": "contents/labs/raspi/vlm/vlm.html#latency-summary",
    "href": "contents/labs/raspi/vlm/vlm.html#latency-summary",
    "title": "Vision-Language Models (VLM)",
    "section": "Latency Summary",
    "text": "Latency Summary\nThe latency observed for different tasks using Florence-2 on the Raspberry Pi (Raspi-5) varied depending on the complexity of the task:\n\nImage Captioning: It took approximately 16-17 seconds to generate a caption for an image.\nDetailed Captioning: Increased latency to around 25-27 seconds, requiring generating more nuanced scene descriptions.\nMore Detailed Captioning: It took about 32-50 seconds, and the latency increased as the description grew more complex.\nObject Detection: It took approximately 20-41 seconds, depending on the image’s complexity and the number of detected objects.\nVisual Grounding: Approximately 15-16 seconds to localize specific objects based on textual prompts.\nOCR (Optical Character Recognition): Extracting text from an image took around 37-38 seconds.\nSegmentation and Region to Segmentation: Segmentation tasks took considerably longer, with a latency of around 83-207 seconds, depending on the complexity and the number of regions to be segmented.\n\nThese latency times highlight the resource constraints of edge devices like the Raspberry Pi and emphasize the need to optimize the model and the environment to achieve real-time performance.\n\n\nRunning complex tasks can use all 8GB of the Raspi-5’s memory. For example, the above screenshot during the Florence OD task shows 4 CPUs at full speed and over 5GB of memory in use. Consider increasing the SWAP memory to 2 GB.\n\nChecking the CPU temperature with vcgencmd measure_temp , showed that temperature can go up to +80oC.",
    "crumbs": [
      "Raspberry Pi",
      "Vision-Language Models (VLM)"
    ]
  },
  {
    "objectID": "contents/labs/raspi/vlm/vlm.html#fine-tunning",
    "href": "contents/labs/raspi/vlm/vlm.html#fine-tunning",
    "title": "Vision-Language Models (VLM)",
    "section": "Fine-Tunning",
    "text": "Fine-Tunning\nAs explored in this lab, Florence supports many tasks out of the box, including captioning, object detection, OCR, and more. However, like other pre-trained foundational models, Florence-2 may need domain-specific knowledge. For example, it may need to improve with medical or satellite imagery. In such cases, fine-tuning with a custom dataset is necessary. The Roboflow tutorial, How to Fine-tune Florence-2 for Object Detection Tasks, shows how to fine-tune Florence-2 on object detection datasets to improve model performance for our specific use case.\nBased on the above tutorial, it is possible to fine-tune the Florence-2 model to detect boxes and wheels used in previous labs:\n\nIt is important to note that after fine-tuning, the model can still detect classes that don’t belong to our custom dataset, like cats, dogs, grapes, etc, as seen before).\nThe complete fine-tunning project using a previously annotated dataset in Roboflow and executed on CoLab can be found in the notebook:\n\n30-Finetune_florence_2_on_detection_dataset_box_vs_wheel.ipynb\n\nIn another example, in the post, Fine-tuning Florence-2 - Microsoft’s Cutting-edge Vision Language Models, the authors show an example of fine-tuning Florence on DocVQA. The authors report that Florence 2 can perform visual question answering (VQA), but the released models don’t include VQA capability.",
    "crumbs": [
      "Raspberry Pi",
      "Vision-Language Models (VLM)"
    ]
  },
  {
    "objectID": "contents/labs/raspi/vlm/vlm.html#conclusion",
    "href": "contents/labs/raspi/vlm/vlm.html#conclusion",
    "title": "Vision-Language Models (VLM)",
    "section": "Conclusion",
    "text": "Conclusion\nFlorence-2 offers a versatile and powerful approach to vision-language tasks at the edge, providing performance that rivals larger, task-specific models, such as YOLO for object detection, BERT/RoBERTa for text analysis, and specialized OCR models.\nThanks to its multi-modal transformer architecture, Florence-2 is more flexible than YOLO in terms of the tasks it can handle. These include object detection, image captioning, and visual grounding.\nUnlike BERT, which focuses purely on language, Florence-2 integrates vision and language, allowing it to excel in applications that require both modalities, such as image captioning and visual grounding.\nMoreover, while traditional OCR models such as Tesseract and EasyOCR are designed solely for recognizing and extracting text from images, Florence-2’s OCR capabilities are part of a broader framework that includes contextual understanding and visual-text alignment. This makes it particularly useful for scenarios that require both reading text and interpreting its context within images.\nOverall, Florence-2 stands out for its ability to seamlessly integrate various vision-language tasks into a unified model that is efficient enough to run on edge devices like the Raspberry Pi. This makes it a compelling choice for developers and researchers exploring AI applications at the edge.\n\nKey Advantages of Florence-2\n\nUnified Architecture\n\nSingle model handles multiple vision tasks vs. specialized models (YOLO, BERT, Tesseract)\nEliminates the need for multiple model deployments and integrations\nConsistent API and interface across tasks\n\nPerformance Comparison\n\nObject Detection: Comparable to YOLOv8 (~37.5 mAP on COCO vs. YOLOv8’s ~39.7 mAP) despite being general-purpose\nText Recognition: Handles multiple languages effectively like specialized OCR models (Tesseract, EasyOCR)\nLanguage Understanding: Integrates BERT-like capabilities for text processing while adding visual context\n\nResource Efficiency\n\nThe Base model (232M parameters) achieves strong results despite smaller size\nRuns effectively on edge devices (Raspberry Pi)\nSingle model deployment vs. multiple specialized models\n\n\n\n\nTrade-offs\n\nPerformance vs. Specialized Models\n\nYOLO series may offer faster inference for pure object detection\nSpecialized OCR models might handle complex document layouts better\nBERT/RoBERTa provide deeper language understanding for text-only tasks\n\nResource Requirements\n\nHigher latency on edge devices (15-200s depending on task)\nRequires careful memory management on Raspberry Pi\nIt may need optimization for real-time applications\n\nDeployment Considerations\n\nInitial setup is more complex than single-purpose models\nRequires understanding of multiple task types and prompts\nThe learning curve for optimal prompt engineering\n\n\n\n\nBest Use Cases\n\nResource-Constrained Environments\n\nEdge devices requiring multiple vision capabilities\nSystems with limited storage/deployment capacity\nApplications needing flexible vision processing\n\nMulti-modal Applications\n\nContent moderation systems\nAccessibility tools\nDocument analysis workflows\n\nRapid Prototyping\n\nQuick deployment of vision capabilities\nTesting multiple vision tasks without separate models\nProof-of-concept development",
    "crumbs": [
      "Raspberry Pi",
      "Vision-Language Models (VLM)"
    ]
  },
  {
    "objectID": "contents/labs/raspi/vlm/vlm.html#future-implications",
    "href": "contents/labs/raspi/vlm/vlm.html#future-implications",
    "title": "Vision-Language Models (VLM)",
    "section": "Future Implications",
    "text": "Future Implications\nFlorence-2 represents a shift toward unified vision models that could eventually replace task-specific architectures in many applications. While specialized models maintain advantages in specific scenarios, the convenience and efficiency of unified models like Florence-2 make them increasingly attractive for real-world deployments.\nThe lab demonstrates Florence-2’s viability on edge devices, suggesting future IoT, mobile computing, and embedded systems applications where deploying multiple specialized models would be impractical.",
    "crumbs": [
      "Raspberry Pi",
      "Vision-Language Models (VLM)"
    ]
  },
  {
    "objectID": "contents/labs/raspi/vlm/vlm.html#resources",
    "href": "contents/labs/raspi/vlm/vlm.html#resources",
    "title": "Vision-Language Models (VLM)",
    "section": "Resources",
    "text": "Resources\n\n10-florence2_test.ipynb\n20-florence_2.ipynb\n30-Finetune_florence_2_on_detection_dataset_box_vs_wheel.ipynb",
    "crumbs": [
      "Raspberry Pi",
      "Vision-Language Models (VLM)"
    ]
  },
  {
    "objectID": "contents/labs/shared/shared.html",
    "href": "contents/labs/shared/shared.html",
    "title": "Shared Labs",
    "section": "",
    "text": "The labs in this section cover topics and techniques that are applicable across different hardware platforms. These labs are designed to be independent of specific boards, allowing you to focus on the fundamental concepts and algorithms used in (tiny) ML applications.\nBy exploring these shared labs, you’ll gain a deeper understanding of the common challenges and solutions in embedded machine learning. The knowledge and skills acquired here will be valuable regardless of the specific hardware you work with in the future.\n\n\n\nExercise\nNicla Vision\nXIAO ESP32S3\n\n\n\n\nKWS Feature Engineering\n✔ Link\n✔ Link\n\n\nDSP Spectral Features Block\n✔ Link\n✔ Link",
    "crumbs": [
      "Shared Labs"
    ]
  },
  {
    "objectID": "contents/labs/shared/kws_feature_eng/kws_feature_eng.html",
    "href": "contents/labs/shared/kws_feature_eng/kws_feature_eng.html",
    "title": "KWS Feature Engineering",
    "section": "",
    "text": "Overview\nIn this hands-on tutorial, the emphasis is on the critical role that feature engineering plays in optimizing the performance of machine learning models applied to audio classification tasks, such as speech recognition. It is essential to be aware that the performance of any machine learning model relies heavily on the quality of features used, and we will deal with “under-the-hood” mechanics of feature extraction, mainly focusing on Mel-frequency Cepstral Coefficients (MFCCs), a cornerstone in the field of audio signal processing.\nMachine learning models, especially traditional algorithms, don’t understand audio waves. They understand numbers arranged in some meaningful way, i.e., features. These features encapsulate the characteristics of the audio signal, making it easier for models to distinguish between different sounds.",
    "crumbs": [
      "Shared Labs",
      "KWS Feature Engineering"
    ]
  },
  {
    "objectID": "contents/labs/shared/kws_feature_eng/kws_feature_eng.html#overview",
    "href": "contents/labs/shared/kws_feature_eng/kws_feature_eng.html#overview",
    "title": "KWS Feature Engineering",
    "section": "",
    "text": "This tutorial will deal with generating features specifically for audio classification. This can be particularly interesting for applying machine learning to a variety of audio data, whether for speech recognition, music categorization, insect classification based on wingbeat sounds, or other sound analysis tasks",
    "crumbs": [
      "Shared Labs",
      "KWS Feature Engineering"
    ]
  },
  {
    "objectID": "contents/labs/shared/kws_feature_eng/kws_feature_eng.html#the-kws",
    "href": "contents/labs/shared/kws_feature_eng/kws_feature_eng.html#the-kws",
    "title": "KWS Feature Engineering",
    "section": "The KWS",
    "text": "The KWS\nThe most common TinyML application is Keyword Spotting (KWS), a subset of the broader field of speech recognition. While general speech recognition transcribes all spoken words into text, Keyword Spotting focuses on detecting specific “keywords” or “wake words” in a continuous audio stream. The system is trained to recognize these keywords as predefined phrases or words, such as yes or no. In short, KWS is a specialized form of speech recognition with its own set of challenges and requirements.\nHere a typical KWS Process using MFCC Feature Converter:\n\n\nApplications of KWS\n\nVoice Assistants: In devices like Amazon’s Alexa or Google Home, KWS is used to detect the wake word (“Alexa” or “Hey Google”) to activate the device.\nVoice-Activated Controls: In automotive or industrial settings, KWS can be used to initiate specific commands like “Start engine” or “Turn off lights.”\nSecurity Systems: Voice-activated security systems may use KWS to authenticate users based on a spoken passphrase.\nTelecommunication Services: Customer service lines may use KWS to route calls based on spoken keywords.\n\n\n\nDifferences from General Speech Recognition\n\nComputational Efficiency: KWS is usually designed to be less computationally intensive than full speech recognition, as it only needs to recognize a small set of phrases.\nReal-time Processing: KWS often operates in real-time and is optimized for low-latency detection of keywords.\nResource Constraints: KWS models are often designed to be lightweight, so they can run on devices with limited computational resources, like microcontrollers or mobile phones.\nFocused Task: While general speech recognition models are trained to handle a broad range of vocabulary and accents, KWS models are fine-tuned to recognize specific keywords, often in noisy environments accurately.",
    "crumbs": [
      "Shared Labs",
      "KWS Feature Engineering"
    ]
  },
  {
    "objectID": "contents/labs/shared/kws_feature_eng/kws_feature_eng.html#overview-to-audio-signals",
    "href": "contents/labs/shared/kws_feature_eng/kws_feature_eng.html#overview-to-audio-signals",
    "title": "KWS Feature Engineering",
    "section": "Overview to Audio Signals",
    "text": "Overview to Audio Signals\nUnderstanding the basic properties of audio signals is crucial for effective feature extraction and, ultimately, for successfully applying machine learning algorithms in audio classification tasks. Audio signals are complex waveforms that capture fluctuations in air pressure over time. These signals can be characterized by several fundamental attributes: sampling rate, frequency, and amplitude.\n\nFrequency and Amplitude: Frequency refers to the number of oscillations a waveform undergoes per unit time and is also measured in Hz. In the context of audio signals, different frequencies correspond to different pitches. Amplitude, on the other hand, measures the magnitude of the oscillations and correlates with the loudness of the sound. Both frequency and amplitude are essential features that capture audio signals’ tonal and rhythmic qualities.\nSampling Rate: The sampling rate, often denoted in Hertz (Hz), defines the number of samples taken per second when digitizing an analog signal. A higher sampling rate allows for a more accurate digital representation of the signal but also demands more computational resources for processing. Typical sampling rates include 44.1 kHz for CD-quality audio and 16 kHz or 8 kHz for speech recognition tasks. Understanding the trade-offs in selecting an appropriate sampling rate is essential for balancing accuracy and computational efficiency. In general, with TinyML projects, we work with 16KHz. Altough music tones can be heard at frequencies up to 20 kHz, voice maxes out at 8 kHz. Traditional telephone systems use an 8 kHz sampling frequency.\n\n\nFor an accurate representation of the signal, the sampling rate must be at least twice the highest frequency present in the signal.\n\n\nTime Domain vs. Frequency Domain: Audio signals can be analyzed in the time and frequency domains. In the time domain, a signal is represented as a waveform where the amplitude is plotted against time. This representation helps to observe temporal features like onset and duration but the signal’s tonal characteristics are not well evidenced. Conversely, a frequency domain representation provides a view of the signal’s constituent frequencies and their respective amplitudes, typically obtained via a Fourier Transform. This is invaluable for tasks that require understanding the signal’s spectral content, such as identifying musical notes or speech phonemes (our case).\n\nThe image below shows the words YES and NO with typical representations in the Time (Raw Audio) and Frequency domains:\n\n\nWhy Not Raw Audio?\nWhile using raw audio data directly for machine learning tasks may seem tempting, this approach presents several challenges that make it less suitable for building robust and efficient models.\nUsing raw audio data for Keyword Spotting (KWS), for example, on TinyML devices poses challenges due to its high dimensionality (using a 16 kHz sampling rate), computational complexity for capturing temporal features, susceptibility to noise, and lack of semantically meaningful features, making feature extraction techniques like MFCCs a more practical choice for resource-constrained applications.\nHere are some additional details of the critical issues associated with using raw audio:\n\nHigh Dimensionality: Audio signals, especially those sampled at high rates, result in large amounts of data. For example, a 1-second audio clip sampled at 16 kHz will have 16,000 individual data points. High-dimensional data increases computational complexity, leading to longer training times and higher computational costs, making it impractical for resource-constrained environments. Furthermore, the wide dynamic range of audio signals requires a significant amount of bits per sample, while conveying little useful information.\nTemporal Dependencies: Raw audio signals have temporal structures that simple machine learning models may find hard to capture. While recurrent neural networks like LSTMs can model such dependencies, they are computationally intensive and tricky to train on tiny devices.\nNoise and Variability: Raw audio signals often contain background noise and other non-essential elements affecting model performance. Additionally, the same sound can have different characteristics based on various factors such as distance from the microphone, the orientation of the sound source, and acoustic properties of the environment, adding to the complexity of the data.\nLack of Semantic Meaning: Raw audio doesn’t inherently contain semantically meaningful features for classification tasks. Features like pitch, tempo, and spectral characteristics, which can be crucial for speech recognition, are not directly accessible from raw waveform data.\nSignal Redundancy: Audio signals often contain redundant information, with certain portions of the signal contributing little to no value to the task at hand. This redundancy can make learning inefficient and potentially lead to overfitting.\n\nFor these reasons, feature extraction techniques such as Mel-frequency Cepstral Coefficients (MFCCs), Mel-Frequency Energies (MFEs), and simple Spectograms are commonly used to transform raw audio data into a more manageable and informative format. These features capture the essential characteristics of the audio signal while reducing dimensionality and noise, facilitating more effective machine learning.",
    "crumbs": [
      "Shared Labs",
      "KWS Feature Engineering"
    ]
  },
  {
    "objectID": "contents/labs/shared/kws_feature_eng/kws_feature_eng.html#overview-to-mfccs",
    "href": "contents/labs/shared/kws_feature_eng/kws_feature_eng.html#overview-to-mfccs",
    "title": "KWS Feature Engineering",
    "section": "Overview to MFCCs",
    "text": "Overview to MFCCs\n\nWhat are MFCCs?\nMel-frequency Cepstral Coefficients (MFCCs) are a set of features derived from the spectral content of an audio signal. They are based on human auditory perceptions and are commonly used to capture the phonetic characteristics of an audio signal. The MFCCs are computed through a multi-step process that includes pre-emphasis, framing, windowing, applying the Fast Fourier Transform (FFT) to convert the signal to the frequency domain, and finally, applying the Discrete Cosine Transform (DCT). The result is a compact representation of the original audio signal’s spectral characteristics.\nThe image below shows the words YES and NO in their MFCC representation:\n\n\nThis video explains the Mel Frequency Cepstral Coefficients (MFCC) and how to compute them.\n\n\n\nWhy are MFCCs important?\nMFCCs are crucial for several reasons, particularly in the context of Keyword Spotting (KWS) and TinyML:\n\nDimensionality Reduction: MFCCs capture essential spectral characteristics of the audio signal while significantly reducing the dimensionality of the data, making it ideal for resource-constrained TinyML applications.\nRobustness: MFCCs are less susceptible to noise and variations in pitch and amplitude, providing a more stable and robust feature set for audio classification tasks.\nHuman Auditory System Modeling: The Mel scale in MFCCs approximates the human ear’s response to different frequencies, making them practical for speech recognition where human-like perception is desired.\nComputational Efficiency: The process of calculating MFCCs is computationally efficient, making it well-suited for real-time applications on hardware with limited computational resources.\n\nIn summary, MFCCs offer a balance of information richness and computational efficiency, making them popular for audio classification tasks, particularly in constrained environments like TinyML.\n\n\nComputing MFCCs\nThe computation of Mel-frequency Cepstral Coefficients (MFCCs) involves several key steps. Let’s walk through these, which are particularly important for Keyword Spotting (KWS) tasks on TinyML devices.\n\nPre-emphasis: The first step is pre-emphasis, which is applied to accentuate the high-frequency components of the audio signal and balance the frequency spectrum. This is achieved by applying a filter that amplifies the difference between consecutive samples. The formula for pre-emphasis is: y(t) = x(t) - \\(\\alpha\\) x(t-1) , where \\(\\alpha\\) is the pre-emphasis factor, typically around 0.97.\nFraming: Audio signals are divided into short frames (the frame length), usually 20 to 40 milliseconds. This is based on the assumption that frequencies in a signal are stationary over a short period. Framing helps in analyzing the signal in such small time slots. The frame stride (or step) will displace one frame and the adjacent. Those steps could be sequential or overlapped.\nWindowing: Each frame is then windowed to minimize the discontinuities at the frame boundaries. A commonly used window function is the Hamming window. Windowing prepares the signal for a Fourier transform by minimizing the edge effects. The image below shows three frames (10, 20, and 30) and the time samples after windowing (note that the frame length and frame stride are 20 ms):\n\n\n\nFast Fourier Transform (FFT) The Fast Fourier Transform (FFT) is applied to each windowed frame to convert it from the time domain to the frequency domain. The FFT gives us a complex-valued representation that includes both magnitude and phase information. However, for MFCCs, only the magnitude is used to calculate the Power Spectrum. The power spectrum is the square of the magnitude spectrum and measures the energy present at each frequency component.\n\n\nThe power spectrum \\(P(f)\\) of a signal \\(x(t)\\) is defined as \\(P(f) = |X(f)|^2\\), where \\(X(f)\\) is the Fourier Transform of \\(x(t)\\). By squaring the magnitude of the Fourier Transform, we emphasize stronger frequencies over weaker ones, thereby capturing more relevant spectral characteristics of the audio signal. This is important in applications like audio classification, speech recognition, and Keyword Spotting (KWS), where the focus is on identifying distinct frequency patterns that characterize different classes of audio or phonemes in speech.\n\n\n\nMel Filter Banks: The frequency domain is then mapped to the Mel scale, which approximates the human ear’s response to different frequencies. The idea is to extract more features (more filter banks) in the lower frequencies and less in the high frequencies. Thus, it performs well on sounds distinguished by the human ear. Typically, 20 to 40 triangular filters extract the Mel-frequency energies. These energies are then log-transformed to convert multiplicative factors into additive ones, making them more suitable for further processing.\n\n\n\nDiscrete Cosine Transform (DCT): The last step is to apply the Discrete Cosine Transform (DCT) to the log Mel energies. The DCT helps to decorrelate the energies, effectively compressing the data and retaining only the most discriminative features. Usually, the first 12-13 DCT coefficients are retained, forming the final MFCC feature vector.",
    "crumbs": [
      "Shared Labs",
      "KWS Feature Engineering"
    ]
  },
  {
    "objectID": "contents/labs/shared/kws_feature_eng/kws_feature_eng.html#hands-on-using-python",
    "href": "contents/labs/shared/kws_feature_eng/kws_feature_eng.html#hands-on-using-python",
    "title": "KWS Feature Engineering",
    "section": "Hands-On using Python",
    "text": "Hands-On using Python\nLet’s apply what we discussed while working on an actual audio sample. Open the notebook on Google CoLab and extract the MLCC features on your audio samples: [Open In Colab]",
    "crumbs": [
      "Shared Labs",
      "KWS Feature Engineering"
    ]
  },
  {
    "objectID": "contents/labs/shared/kws_feature_eng/kws_feature_eng.html#conclusion",
    "href": "contents/labs/shared/kws_feature_eng/kws_feature_eng.html#conclusion",
    "title": "KWS Feature Engineering",
    "section": "Conclusion",
    "text": "Conclusion\nWhat Feature Extraction technique should we use?\nMel-frequency Cepstral Coefficients (MFCCs), Mel-Frequency Energies (MFEs), or Spectrogram are techniques for representing audio data, which are often helpful in different contexts.\nIn general, MFCCs are more focused on capturing the envelope of the power spectrum, which makes them less sensitive to fine-grained spectral details but more robust to noise. This is often desirable for speech-related tasks. On the other hand, spectrograms or MFEs preserve more detailed frequency information, which can be advantageous in tasks that require discrimination based on fine-grained spectral content.\n\nMFCCs are particularly strong for\n\nSpeech Recognition: MFCCs are excellent for identifying phonetic content in speech signals.\nSpeaker Identification: They can be used to distinguish between different speakers based on voice characteristics.\nEmotion Recognition: MFCCs can capture the nuanced variations in speech indicative of emotional states.\nKeyword Spotting: Especially in TinyML, where low computational complexity and small feature size are crucial.\n\n\n\nSpectrograms or MFEs are often more suitable for\n\nMusic Analysis: Spectrograms can capture harmonic and timbral structures in music, which is essential for tasks like genre classification, instrument recognition, or music transcription.\nEnvironmental Sound Classification: In recognizing non-speech, environmental sounds (e.g., rain, wind, traffic), the full spectrogram can provide more discriminative features.\nBirdsong Identification: The intricate details of bird calls are often better captured using spectrograms.\nBioacoustic Signal Processing: In applications like dolphin or bat call analysis, the fine-grained frequency information in a spectrogram can be essential.\nAudio Quality Assurance: Spectrograms are often used in professional audio analysis to identify unwanted noises, clicks, or other artifacts.",
    "crumbs": [
      "Shared Labs",
      "KWS Feature Engineering"
    ]
  },
  {
    "objectID": "contents/labs/shared/kws_feature_eng/kws_feature_eng.html#resources",
    "href": "contents/labs/shared/kws_feature_eng/kws_feature_eng.html#resources",
    "title": "KWS Feature Engineering",
    "section": "Resources",
    "text": "Resources\n\nAudio_Data_Analysis Colab Notebook",
    "crumbs": [
      "Shared Labs",
      "KWS Feature Engineering"
    ]
  },
  {
    "objectID": "contents/labs/shared/dsp_spectral_features_block/dsp_spectral_features_block.html",
    "href": "contents/labs/shared/dsp_spectral_features_block/dsp_spectral_features_block.html",
    "title": "DSP Spectral Features",
    "section": "",
    "text": "Overview\nTinyML projects related to motion (or vibration) involve data from IMUs (usually accelerometers and Gyroscopes). These time-series type datasets should be preprocessed before inputting them into a Machine Learning model training, which is a challenging area for embedded machine learning. Still, Edge Impulse helps overcome this complexity with its digital signal processing (DSP) preprocessing step and, more specifically, the Spectral Features Block for Inertial sensors.\nBut how does it work under the hood? Let’s dig into it.",
    "crumbs": [
      "Shared Labs",
      "DSP Spectral Features"
    ]
  },
  {
    "objectID": "contents/labs/shared/dsp_spectral_features_block/dsp_spectral_features_block.html#extracting-features-review",
    "href": "contents/labs/shared/dsp_spectral_features_block/dsp_spectral_features_block.html#extracting-features-review",
    "title": "DSP Spectral Features",
    "section": "Extracting Features Review",
    "text": "Extracting Features Review\nExtracting features from a dataset captured with inertial sensors, such as accelerometers, involves processing and analyzing the raw data. Accelerometers measure the acceleration of an object along one or more axes (typically three, denoted as X, Y, and Z). These measurements can be used to understand various aspects of the object’s motion, such as movement patterns and vibrations. Here’s a high-level overview of the process:\nData collection: First, we need to gather data from the accelerometers. Depending on the application, data may be collected at different sampling rates. It’s essential to ensure that the sampling rate is high enough to capture the relevant dynamics of the studied motion (the sampling rate should be at least double the maximum relevant frequency present in the signal).\nData preprocessing: Raw accelerometer data can be noisy and contain errors or irrelevant information. Preprocessing steps, such as filtering and normalization, can help clean and standardize the data, making it more suitable for feature extraction.\n\nThe Studio does not perform normalization or standardization, so sometimes, when working with Sensor Fusion, it could be necessary to perform this step before uploading data to the Studio. This is particularly crucial in sensor fusion projects, as seen in this tutorial, Sensor Data Fusion with Spresense and CommonSense.\n\nSegmentation: Depending on the nature of the data and the application, dividing the data into smaller segments or windows may be necessary. This can help focus on specific events or activities within the dataset, making feature extraction more manageable and meaningful. The window size and overlap (window span) choice depend on the application and the frequency of the events of interest. As a rule of thumb, we should try to capture a couple of “data cycles.”\nFeature extraction: Once the data is preprocessed and segmented, you can extract features that describe the motion’s characteristics. Some typical features extracted from accelerometer data include:\n\nTime-domain features describe the data’s statistical properties within each segment, such as mean, median, standard deviation, skewness, kurtosis, and zero-crossing rate.\nFrequency-domain features are obtained by transforming the data into the frequency domain using techniques like the Fast Fourier Transform (FFT). Some typical frequency-domain features include the power spectrum, spectral energy, dominant frequencies (amplitude and frequency), and spectral entropy.\nTime-frequency domain features combine the time and frequency domain information, such as the Short-Time Fourier Transform (STFT) or the Discrete Wavelet Transform (DWT). They can provide a more detailed understanding of how the signal’s frequency content changes over time.\n\nIn many cases, the number of extracted features can be large, which may lead to overfitting or increased computational complexity. Feature selection techniques, such as mutual information, correlation-based methods, or principal component analysis (PCA), can help identify the most relevant features for a given application and reduce the dimensionality of the dataset. The Studio can help with such feature-relevant calculations.\nLet’s explore in more detail a typical TinyML Motion Classification project covered in this series of Hands-Ons.",
    "crumbs": [
      "Shared Labs",
      "DSP Spectral Features"
    ]
  },
  {
    "objectID": "contents/labs/shared/dsp_spectral_features_block/dsp_spectral_features_block.html#a-tinyml-motion-classification-project",
    "href": "contents/labs/shared/dsp_spectral_features_block/dsp_spectral_features_block.html#a-tinyml-motion-classification-project",
    "title": "DSP Spectral Features",
    "section": "A TinyML Motion Classification project",
    "text": "A TinyML Motion Classification project\n\nIn the hands-on project, Motion Classification and Anomaly Detection, we simulated mechanical stresses in transport, where our problem was to classify four classes of movement:\n\nMaritime (pallets in boats)\nTerrestrial (pallets in a Truck or Train)\nLift (pallets being handled by Fork-Lift)\nIdle (pallets in Storage houses)\n\nThe accelerometers provided the data on the pallet (or container).\n\nBelow is one sample (raw data) of 10 seconds, captured with a sampling frequency of 50Hz:\n\n\nThe result is similar when this analysis is done over another dataset with the same principle, using a different sampling frequency, 62.5Hz instead of 50Hz.",
    "crumbs": [
      "Shared Labs",
      "DSP Spectral Features"
    ]
  },
  {
    "objectID": "contents/labs/shared/dsp_spectral_features_block/dsp_spectral_features_block.html#data-pre-processing",
    "href": "contents/labs/shared/dsp_spectral_features_block/dsp_spectral_features_block.html#data-pre-processing",
    "title": "DSP Spectral Features",
    "section": "Data Pre-Processing",
    "text": "Data Pre-Processing\nThe raw data captured by the accelerometer (a “time series” data) should be converted to “tabular data” using one of the typical Feature Extraction methods described in the last section.\nWe should segment the data using a sliding window over the sample data for feature extraction. The project captured accelerometer data every 10 seconds with a sample rate of 62.5 Hz. A 2-second window captures 375 data points (3 axis x 2 seconds x 62.5 samples). The window is slid every 80ms, creating a larger dataset where each instance has 375 “raw features.”\n\nOn the Studio, the previous version (V1) of the Spectral Analysis Block extracted as time-domain features only the RMS, and for the frequency-domain, the peaks and frequency (using FFT) and the power characteristics (PSD) of the signal over time resulting in a fixed tabular dataset of 33 features (11 per each axis),\n\nThose 33 features were the Input tensor of a Neural Network Classifier.\nIn 2022, Edge Impulse released version 2 of the Spectral Analysis block, which we will explore here.\n\nEdge Impulse - Spectral Analysis Block V.2 under the hood\nIn Version 2, Time Domain Statistical features per axis/channel are:\n\nRMS\nSkewness\nKurtosis\n\nAnd the Frequency Domain Spectral features per axis/channel are:\n\nSpectral Power\nSkewness (in the next version)\nKurtosis (in the next version)\n\nIn this link, we can have more details about the feature extraction.\n\nClone the public project. You can also follow the explanation, playing with the code using my Google CoLab Notebook: Edge Impulse Spectral Analysis Block Notebook.\n\nStart importing the libraries:\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport math\nfrom scipy.stats import skew, kurtosis\nfrom scipy import signal\nfrom scipy.signal import welch\nfrom scipy.stats import entropy\nfrom sklearn import preprocessing\nimport pywt\n\nplt.rcParams['figure.figsize'] = (12, 6)\nplt.rcParams['lines.linewidth'] = 3\nFrom the studied project, let’s choose a data sample from accelerometers as below:\n\nWindow size of 2 seconds: [2,000] ms\nSample frequency: [62.5] Hz\nWe will choose the [None] filter (for simplicity) and a\nFFT length: [16].\n\nf =  62.5 # Hertz\nwind_sec = 2 # seconds\nFFT_Lenght = 16\naxis = ['accX', 'accY', 'accZ']\nn_sensors = len(axis)\n\nSelecting the Raw Features on the Studio Spectral Analysis tab, we can copy all 375 data points of a particular 2-second window to the clipboard.\n\nPaste the data points to a new variable data:\ndata=[-5.6330, 0.2376, 9.8701, -5.9442, 0.4830, 9.8701, -5.4217, ...]\nNo_raw_features = len(data)\nN = int(No_raw_features/n_sensors)\nThe total raw features are 375, but we will work with each axis individually, where N= 125 (number of samples per axis).\nWe aim to understand how Edge Impulse gets the processed features.\n\nSo, you should also past the processed features on a variable (to compare the calculated features in Python with the ones provided by the Studio) :\nfeatures = [2.7322, -0.0978, -0.3813, 2.3980, 3.8924, 24.6841, 9.6303, ...]\nN_feat = len(features)\nN_feat_axis = int(N_feat/n_sensors)\nThe total number of processed features is 39, which means 13 features/axis.\nLooking at those 13 features closely, we will find 3 for the time domain (RMS, Skewness, and Kurtosis):\n\n[rms] [skew] [kurtosis]\n\nand 10 for the frequency domain (we will return to this later).\n\n[spectral skew][spectral kurtosis][Spectral Power 1] ... [Spectral Power 8]\n\nSplitting raw data per sensor\nThe data has samples from all axes; let’s split and plot them separately:\ndef plot_data(sensors, axis, title):\n    [plt.plot(x, label=y) for x,y in zip(sensors, axis)]\n    plt.legend(loc='lower right')\n    plt.title(title)\n    plt.xlabel('#Sample')\n    plt.ylabel('Value')\n    plt.box(False)\n    plt.grid()\n    plt.show()\n\naccX = data[0::3]\naccY = data[1::3]\naccZ = data[2::3]\nsensors = [accX, accY, accZ] \nplot_data(sensors, axis, 'Raw Features')\n\nSubtracting the mean\nNext, we should subtract the mean from the data. Subtracting the mean from a data set is a common data pre-processing step in statistics and machine learning. The purpose of subtracting the mean from the data is to center the data around zero. This is important because it can reveal patterns and relationships that might be hidden if the data is not centered.\nHere are some specific reasons why subtracting the mean can be helpful:\n\nIt simplifies analysis: By centering the data, the mean becomes zero, making some calculations simpler and easier to interpret.\nIt removes bias: If the data is biased, subtracting the mean can remove it and allow for a more accurate analysis.\nIt can reveal patterns: Centering the data can help uncover patterns that might be hidden if the data is not centered. For example, centering the data can help you identify trends over time if you analyze a time series dataset.\nIt can improve performance: In some machine learning algorithms, centering the data can improve performance by reducing the influence of outliers and making the data more easily comparable. Overall, subtracting the mean is a simple but powerful technique that can be used to improve the analysis and interpretation of data.\n\ndtmean = [(sum(x)/len(x)) for x in sensors]\n[print('mean_'+x+'= ', round(y, 4)) for x,y in zip(axis, dtmean)][0]\n\naccX = [(x - dtmean[0]) for x in accX]\naccY = [(x - dtmean[1]) for x in accY]\naccZ = [(x - dtmean[2]) for x in accZ]\nsensors = [accX, accY, accZ]\n\nplot_data(sensors, axis, 'Raw Features - Subctract the Mean')",
    "crumbs": [
      "Shared Labs",
      "DSP Spectral Features"
    ]
  },
  {
    "objectID": "contents/labs/shared/dsp_spectral_features_block/dsp_spectral_features_block.html#time-domain-statistical-features",
    "href": "contents/labs/shared/dsp_spectral_features_block/dsp_spectral_features_block.html#time-domain-statistical-features",
    "title": "DSP Spectral Features",
    "section": "Time Domain Statistical features",
    "text": "Time Domain Statistical features\nRMS Calculation\nThe RMS value of a set of values (or a continuous-time waveform) is the square root of the arithmetic mean of the squares of the values or the square of the function that defines the continuous waveform. In physics, the RMS value of an electrical current is defined as the “value of the direct current that dissipates the same power in a resistor.”\nIn the case of a set of n values {𝑥1, 𝑥2, …, 𝑥𝑛}, the RMS is:\n\n\nNOTE that the RMS value is different for the original raw data, and after subtracting the mean\n\n# Using numpy and standartized data (subtracting mean)\nrms = [np.sqrt(np.mean(np.square(x))) for x in sensors]\nWe can compare the calculated RMS values here with the ones presented by Edge Impulse:\n[print('rms_'+x+'= ', round(y, 4)) for x,y in zip(axis, rms)][0]\nprint(\"\\nCompare with Edge Impulse result features\")\nprint(features[0:N_feat:N_feat_axis])\nrms_accX=  2.7322\nrms_accY=  0.7833\nrms_accZ=  0.1383\nCompared with Edge Impulse result features:\n[2.7322, 0.7833, 0.1383]\nSkewness and kurtosis calculation\nIn statistics, skewness and kurtosis are two ways to measure the shape of a distribution.\nHere, we can see the sensor values distribution:\nfig, axes = plt.subplots(nrows=1, ncols=3, figsize=(13, 4))\nsns.kdeplot(accX, fill=True, ax=axes[0])\nsns.kdeplot(accY, fill=True, ax=axes[1])\nsns.kdeplot(accZ, fill=True, ax=axes[2])\naxes[0].set_title('accX')\naxes[1].set_title('accY')\naxes[2].set_title('accZ')\nplt.suptitle('IMU Sensors distribution', fontsize=16, y=1.02)\nplt.show()\n\nSkewness is a measure of the asymmetry of a distribution. This value can be positive or negative.\n\n\nA negative skew indicates that the tail is on the left side of the distribution, which extends towards more negative values.\nA positive skew indicates that the tail is on the right side of the distribution, which extends towards more positive values.\nA zero value indicates no skewness in the distribution at all, meaning the distribution is perfectly symmetrical.\n\nskew = [skew(x, bias=False) for x in sensors]\n[print('skew_'+x+'= ', round(y, 4)) for x,y in zip(axis, skew)][0]\nprint(\"\\nCompare with Edge Impulse result features\")\nfeatures[1:N_feat:N_feat_axis]\nskew_accX=  -0.099\nskew_accY=  0.1756\nskew_accZ=  6.9463\nCompared with Edge Impulse result features:\n[-0.0978, 0.1735, 6.8629]\nKurtosis is a measure of whether or not a distribution is heavy-tailed or light-tailed relative to a normal distribution.\n\n\nThe kurtosis of a normal distribution is zero.\nIf a given distribution has a negative kurtosis, it is said to be playkurtic, which means it tends to produce fewer and less extreme outliers than the normal distribution.\nIf a given distribution has a positive kurtosis , it is said to be leptokurtic, which means it tends to produce more outliers than the normal distribution.\n\nkurt = [kurtosis(x, bias=False) for x in sensors]\n[print('kurt_'+x+'= ', round(y, 4)) for x,y in zip(axis, kurt)][0]\nprint(\"\\nCompare with Edge Impulse result features\")\nfeatures[2:N_feat:N_feat_axis]\nkurt_accX=  -0.3475\nkurt_accY=  1.2673\nkurt_accZ=  68.1123\nCompared with Edge Impulse result features:\n[-0.3813, 1.1696, 65.3726]",
    "crumbs": [
      "Shared Labs",
      "DSP Spectral Features"
    ]
  },
  {
    "objectID": "contents/labs/shared/dsp_spectral_features_block/dsp_spectral_features_block.html#spectral-features",
    "href": "contents/labs/shared/dsp_spectral_features_block/dsp_spectral_features_block.html#spectral-features",
    "title": "DSP Spectral Features",
    "section": "Spectral features",
    "text": "Spectral features\nThe filtered signal is passed to the Spectral power section, which computes the FFT to generate the spectral features.\nSince the sampled window is usually larger than the FFT size, the window will be broken into frames (or “sub-windows”), and the FFT is calculated over each frame.\nFFT length - The FFT size. This determines the number of FFT bins and the resolution of frequency peaks that can be separated. A low number means more signals will average together in the same FFT bin, but it also reduces the number of features and model size. A high number will separate more signals into separate bins, generating a larger model.\n\nThe total number of Spectral Power features will vary depending on how you set the filter and FFT parameters. With No filtering, the number of features is 1/2 of the FFT Length.\n\nSpectral Power - Welch’s method\nWe should use Welch’s method to split the signal on the frequency domain in bins and calculate the power spectrum for each bin. This method divides the signal into overlapping segments, applies a window function to each segment, computes the periodogram of each segment using DFT, and averages them to obtain a smoother estimate of the power spectrum.\n# Function used by Edge Impulse instead of scipy.signal.welch().\ndef welch_max_hold(fx, sampling_freq, nfft, n_overlap):\n    n_overlap = int(n_overlap)\n    spec_powers = [0 for _ in range(nfft//2+1)]\n    ix = 0\n    while ix &lt;= len(fx):\n        # Slicing truncates if end_idx &gt; len, and rfft will auto-zero pad\n        fft_out = np.abs(np.fft.rfft(fx[ix:ix+nfft], nfft))\n        spec_powers = np.maximum(spec_powers, fft_out**2/nfft)\n        ix = ix + (nfft-n_overlap)\n    return np.fft.rfftfreq(nfft, 1/sampling_freq), spec_powers\nApplying the above function to 3 signals:\nfax,Pax = welch_max_hold(accX, fs, FFT_Lenght, 0)\nfay,Pay = welch_max_hold(accY, fs, FFT_Lenght, 0)\nfaz,Paz = welch_max_hold(accZ, fs, FFT_Lenght, 0)\nspecs = [Pax, Pay, Paz ]\nWe can plot the Power Spectrum P(f):\nplt.plot(fax,Pax, label='accX')\nplt.plot(fay,Pay, label='accY')\nplt.plot(faz,Paz, label='accZ')\nplt.legend(loc='upper right')\nplt.xlabel('Frequency (Hz)')\n#plt.ylabel('PSD [V**2/Hz]')\nplt.ylabel('Power')\nplt.title('Power spectrum P(f) using Welch's method')\nplt.grid()\nplt.box(False)\nplt.show()\n\nBesides the Power Spectrum, we can also include the skewness and kurtosis of the features in the frequency domain (should be available on a new version):\nspec_skew = [skew(x, bias=False) for x in specs]\nspec_kurtosis = [kurtosis(x, bias=False) for x in specs]\nLet’s now list all Spectral features per axis and compare them with EI:\nprint(\"EI Processed Spectral features (accX): \")\nprint(features[3:N_feat_axis][0:])\nprint(\"\\nCalculated features:\")\nprint (round(spec_skew[0],4))\nprint (round(spec_kurtosis[0],4))\n[print(round(x, 4)) for x in Pax[1:]][0]\nEI Processed Spectral features (accX):\n2.398, 3.8924, 24.6841, 9.6303, 8.4867, 7.7793, 2.9963, 5.6242, 3.4198, 4.2735\nCalculated features:\n2.9069 8.5569 24.6844 9.6304 8.4865 7.7794 2.9964 5.6242 3.4198 4.2736\nprint(\"EI Processed Spectral features (accY): \")\nprint(features[16:26][0:]) #13: 3+N_feat_axis;  26 = 2x N_feat_axis\nprint(\"\\nCalculated features:\")\nprint (round(spec_skew[1],4))\nprint (round(spec_kurtosis[1],4))\n[print(round(x, 4)) for x in Pay[1:]][0]\nEI Processed Spectral features (accY):\n0.9426, -0.8039, 5.429, 0.999, 1.0315, 0.9459, 1.8117, 0.9088, 1.3302, 3.112\nCalculated features:\n1.1426 -0.3886 5.4289 0.999 1.0315 0.9458 1.8116 0.9088 1.3301 3.1121\nprint(\"EI Processed Spectral features (accZ): \")\nprint(features[29:][0:]) #29: 3+(2*N_feat_axis);\nprint(\"\\nCalculated features:\")\nprint (round(spec_skew[2],4))\nprint (round(spec_kurtosis[2],4))\n[print(round(x, 4)) for x in Paz[1:]][0]\nEI Processed Spectral features (accZ):\n0.3117, -1.3812, 0.0606, 0.057, 0.0567, 0.0976, 0.194, 0.2574, 0.2083, 0.166\nCalculated features:\n0.3781 -1.4874 0.0606 0.057 0.0567 0.0976 0.194 0.2574 0.2083 0.166",
    "crumbs": [
      "Shared Labs",
      "DSP Spectral Features"
    ]
  },
  {
    "objectID": "contents/labs/shared/dsp_spectral_features_block/dsp_spectral_features_block.html#time-frequency-domain",
    "href": "contents/labs/shared/dsp_spectral_features_block/dsp_spectral_features_block.html#time-frequency-domain",
    "title": "DSP Spectral Features",
    "section": "Time-frequency domain",
    "text": "Time-frequency domain\n\nWavelets\nWavelet is a powerful technique for analyzing signals with transient features or abrupt changes, such as spikes or edges, which are difficult to interpret with traditional Fourier-based methods.\nWavelet transforms work by breaking down a signal into different frequency components and analyzing them individually. The transformation is achieved by convolving the signal with a wavelet function, a small waveform centered at a specific time and frequency. This process effectively decomposes the signal into different frequency bands, each of which can be analyzed separately.\nOne of the critical benefits of wavelet transforms is that they allow for time-frequency analysis, which means that they can reveal the frequency content of a signal as it changes over time. This makes them particularly useful for analyzing non-stationary signals, which vary over time.\nWavelets have many practical applications, including signal and image compression, denoising, feature extraction, and image processing.\nLet’s select Wavelet on the Spectral Features block in the same project:\n\nType: Wavelet\nWavelet Decomposition Level: 1\nWavelet: bior1.3\n\n\nThe Wavelet Function\nwavelet_name='bior1.3'\nnum_layer = 1\n\nwavelet = pywt.Wavelet(wavelet_name)\n[phi_d,psi_d,phi_r,psi_r,x] = wavelet.wavefun(level=5)\nplt.plot(x, psi_d, color='red')\nplt.title('Wavelet Function')\nplt.ylabel('Value')\nplt.xlabel('Time')\nplt.grid()\nplt.box(False)\nplt.show()\n\nAs we did before, let’s copy and past the Processed Features:\n\nfeatures = [3.6251, 0.0615, 0.0615, -7.3517, -2.7641, 2.8462, 5.0924, ...]\nN_feat = len(features)\nN_feat_axis = int(N_feat/n_sensors)\nEdge Impulse computes the Discrete Wavelet Transform (DWT) for each one of the Wavelet Decomposition levels selected. After that, the features will be extracted.\nIn the case of Wavelets, the extracted features are basic statistical values, crossing values, and entropy. There are, in total, 14 features per layer as below:\n\n[11] Statiscal Features: n5, n25, n75, n95, mean, median, standard deviation (std), variance (var) root mean square (rms), kurtosis, and skewness (skew).\n[2] Crossing Features: Zero crossing rate (zcross) and mean crossing rate (mcross) are the times that the signal passes through the baseline (y = 0) and the average level (y = u) per unit of time, respectively\n[1] Complexity Feature: Entropy is a characteristic measure of the complexity of the signal\n\nAll the above 14 values are calculated for each Layer (including L0, the original signal)\n\nThe total number of features varies depending on how you set the filter and the number of layers. For example, with [None] filtering and Level[1], the number of features per axis will be 14 x 2 (L0 and L1) = 28. For the three axes, we will have a total of 84 features.\n\n\n\nWavelet Analysis\nWavelet analysis decomposes the signal (accX, accY, and accZ) into different frequency components using a set of filters, which separate these components into low-frequency (slowly varying parts of the signal containing long-term patterns), such as accX_l1, accY_l1, accZ_l1 and, high-frequency (rapidly varying parts of the signal containing short-term patterns) components, such as accX_d1, accY_d1, accZ_d1, permitting the extraction of features for further analysis or classification.\nOnly the low-frequency components (approximation coefficients, or cA) will be used. In this example, we assume only one level (Single-level Discrete Wavelet Transform), where the function will return a tuple. With a multilevel decomposition, the “Multilevel 1D Discrete Wavelet Transform”, the result will be a list (for detail, please see: Discrete Wavelet Transform (DWT) )\n(accX_l1, accX_d1) = pywt.dwt(accX, wavelet_name)\n(accY_l1, accY_d1) = pywt.dwt(accY, wavelet_name)\n(accZ_l1, accZ_d1) = pywt.dwt(accZ, wavelet_name)\nsensors_l1 = [accX_l1, accY_l1, accZ_l1]\n\n# Plot power spectrum versus frequency\nplt.plot(accX_l1, label='accX')\nplt.plot(accY_l1, label='accY')\nplt.plot(accZ_l1, label='accZ')\nplt.legend(loc='lower right')\nplt.xlabel('Time')\nplt.ylabel('Value')\nplt.title('Wavelet Approximation')\nplt.grid()\nplt.box(False)\nplt.show()\n\n\n\nFeature Extraction\nLet’s start with the basic statistical features. Note that we apply the function for both the original signals and the resultant cAs from the DWT:\ndef calculate_statistics(signal):\n    n5 = np.percentile(signal, 5)\n    n25 = np.percentile(signal, 25)\n    n75 = np.percentile(signal, 75)\n    n95 = np.percentile(signal, 95)\n    median = np.percentile(signal, 50)\n    mean = np.mean(signal)\n    std = np.std(signal)\n    var = np.var(signal)\n    rms = np.sqrt(np.mean(np.square(signal)))\n    return [n5, n25, n75, n95, median, mean, std, var, rms]\n \nstat_feat_l0 = [calculate_statistics(x) for x in sensors]\nstat_feat_l1 = [calculate_statistics(x) for x in sensors_l1]\nThe Skelness and Kurtosis:\nskew_l0 = [skew(x, bias=False) for x in sensors]\nskew_l1 = [skew(x, bias=False) for x in sensors_l1]\nkurtosis_l0 = [kurtosis(x, bias=False) for x in sensors]\nkurtosis_l1 = [kurtosis(x, bias=False) for x in sensors_l1]\nZero crossing (zcross) is the number of times the wavelet coefficient crosses the zero axis. It can be used to measure the signal’s frequency content since high-frequency signals tend to have more zero crossings than low-frequency signals.\nMean crossing (mcross), on the other hand, is the number of times the wavelet coefficient crosses the mean of the signal. It can be used to measure the amplitude since high-amplitude signals tend to have more mean crossings than low-amplitude signals.\ndef getZeroCrossingRate(arr):\n    my_array = np.array(arr)\n    zcross = float(\"{0:.2f}\".format((((my_array[:-1] * my_array[1:]) &lt; 0).su    m())/len(arr)))\n    return zcross\n\ndef getMeanCrossingRate(arr):\n    mcross = getZeroCrossingRate(np.array(arr) - np.mean(arr))\n    return mcross\n\ndef calculate_crossings(list):\n    zcross=[]\n    mcross=[]\n    for i in range(len(list)):\n        zcross_i = getZeroCrossingRate(list[i])\n        zcross.append(zcross_i)\n        mcross_i = getMeanCrossingRate(list[i])\n        mcross.append(mcross_i)\n    return zcross, mcross\n\ncross_l0 = calculate_crossings(sensors)\ncross_l1 = calculate_crossings(sensors_l1)\nIn wavelet analysis, entropy refers to the degree of disorder or randomness in the distribution of wavelet coefficients. Here, we used Shannon entropy, which measures a signal’s uncertainty or randomness. It is calculated as the negative sum of the probabilities of the different possible outcomes of the signal multiplied by their base 2 logarithm. In the context of wavelet analysis, Shannon entropy can be used to measure the complexity of the signal, with higher values indicating greater complexity.\ndef calculate_entropy(signal, base=None):\n    value, counts = np.unique(signal, return_counts=True)\n    return entropy(counts, base=base)\n\nentropy_l0 = [calculate_entropy(x) for x in sensors]\nentropy_l1 = [calculate_entropy(x) for x in sensors_l1]\nLet’s now list all the wavelet features and create a list by layers.\nL1_features_names = [\"L1-n5\", \"L1-n25\", \"L1-n75\", \"L1-n95\", \"L1-median\", \"L1-mean\", \"L1-std\", \"L1-var\", \"L1-rms\", \"L1-skew\", \"L1-Kurtosis\", \"L1-zcross\", \"L1-mcross\", \"L1-entropy\"]\n\nL0_features_names = [\"L0-n5\", \"L0-n25\", \"L0-n75\", \"L0-n95\", \"L0-median\", \"L0-mean\", \"L0-std\", \"L0-var\", \"L0-rms\", \"L0-skew\", \"L0-Kurtosis\", \"L0-zcross\", \"L0-mcross\", \"L0-entropy\"]\n\nall_feat_l0 = []\nfor i in range(len(axis)):\n    feat_l0 = stat_feat_l0[i]+[skew_l0[i]]+[kurtosis_l0[i]]+[cross_l0[0][i]]+[cross_l0[1][i]]+[entropy_l0[i]]\n    [print(axis[i]+' '+x+'= ', round(y, 4)) for x,y in zip(L0_features_names, feat_l0)][0]\n    all_feat_l0.append(feat_l0)\nall_feat_l0 = [item for sublist in all_feat_l0 for item in sublist]\nprint(f\"\\nAll L0 Features = {len(all_feat_l0)}\")\n\nall_feat_l1 = []\nfor i in range(len(axis)):\nfeat_l1 = stat_feat_l1[i]+[skew_l1[i]]+[kurtosis_l1[i]]+[cross_l1[0][i]]+[cross_l1[1][i]]+[entropy_l1[i]]\n[print(axis[i]+' '+x+'= ', round(y, 4)) for x,y in zip(L1_features_names, feat_l1)][0]\nall_feat_l1.append(feat_l1)\nall_feat_l1 = [item for sublist in all_feat_l1 for item in sublist]\nprint(f\"\\nAll L1 Features = {len(all_feat_l1)}\")",
    "crumbs": [
      "Shared Labs",
      "DSP Spectral Features"
    ]
  },
  {
    "objectID": "contents/labs/shared/dsp_spectral_features_block/dsp_spectral_features_block.html#conclusion",
    "href": "contents/labs/shared/dsp_spectral_features_block/dsp_spectral_features_block.html#conclusion",
    "title": "DSP Spectral Features",
    "section": "Conclusion",
    "text": "Conclusion\nEdge Impulse Studio is a powerful online platform that can handle the pre-processing task for us. Still, given our engineering perspective, we want to understand what is happening under the hood. This knowledge will help us find the best options and hyper-parameters for tuning our projects.\nDaniel Situnayake wrote in his blog: “Raw sensor data is highly dimensional and noisy. Digital signal processing algorithms help us sift the signal from the noise. DSP is an essential part of embedded engineering, and many edge processors have on-board acceleration for DSP. As an ML engineer, learning basic DSP gives you superpowers for handling high-frequency time series data in your models.” I recommend you read Dan’s excellent post in its totality: nn to cpp: What you need to know about porting deep learning models to the edge.",
    "crumbs": [
      "Shared Labs",
      "DSP Spectral Features"
    ]
  },
  {
    "objectID": "contents/appendix/phd_survival_guide.html",
    "href": "contents/appendix/phd_survival_guide.html",
    "title": "PhD Survival Guide",
    "section": "",
    "text": "Career Advice\nTechnical knowledge in machine learning systems or be it in any other field, while essential, is only one dimension of successful research and scholarship. The journey through (graduate) school and beyond demands a broader set of skills: the ability to read and synthesize complex literature, communicate ideas effectively, manage time, and navigate academic careers thoughtfully.\nThis appendix is a small set of resources that address these important but often underdiscussed aspects of academic life. The curated materials span from seminal works that have guided multiple generations of researchers to contemporary discussions of productivity and scientific communication.\nMany of these resources originated in computer science and engineering contexts, with each section focusing on a distinct aspect of academic life and presenting authoritative sources that have proven particularly valuable for graduate students and early-career researchers.\nIf you have suggestions or recommendations, please feel free to contact me vj[@]eecs harvard edu or issue a GitHub PR with your suggestion!",
    "crumbs": [
      "Appendix",
      "PhD Survival Guide"
    ]
  },
  {
    "objectID": "contents/appendix/phd_survival_guide.html#on-research-careers-and-productivity",
    "href": "contents/appendix/phd_survival_guide.html#on-research-careers-and-productivity",
    "title": "PhD Survival Guide",
    "section": "On Research Careers and Productivity",
    "text": "On Research Careers and Productivity\n\nHow to Have a Bad Career in Research/Academia\nA humorous and insightful guide by Turing Award winner David Patterson on common pitfalls to avoid in academic research.\nYou and Your Research\nA famous lecture by Richard Hamming on how to do impactful research and why some researchers excel.\nTen Simple Rules for Doing Your Best Research, According to Hamming\nA summary and expansion on Richard Hamming’s principles, providing practical and motivational guidance for researchers at all stages.\nThe Importance of Stupidity in Scientific Research\nA short essay by Martin A. Schwartz on embracing the challenges of research and learning to thrive in the unknown.\nAdvice to a Young Scientist\nA classic book by Peter Medawar offering practical and philosophical advice on scientific research careers.",
    "crumbs": [
      "Appendix",
      "PhD Survival Guide"
    ]
  },
  {
    "objectID": "contents/appendix/phd_survival_guide.html#on-reading-and-learning",
    "href": "contents/appendix/phd_survival_guide.html#on-reading-and-learning",
    "title": "PhD Survival Guide",
    "section": "On Reading and Learning",
    "text": "On Reading and Learning\n\nHow to Read a Paper\nA guide by S. Keshav on how to efficiently read and understand research papers.\nEfficient Reading of Papers in Science and Technology Practical advice by Michael J. Hanson for handling the large volume of research papers in technical fields.",
    "crumbs": [
      "Appendix",
      "PhD Survival Guide"
    ]
  },
  {
    "objectID": "contents/appendix/phd_survival_guide.html#on-time-management-and-productivity",
    "href": "contents/appendix/phd_survival_guide.html#on-time-management-and-productivity",
    "title": "PhD Survival Guide",
    "section": "On Time Management and Productivity",
    "text": "On Time Management and Productivity\n\nDeep Work\nBy Cal Newport, this book provides strategies for focusing deeply and maximizing productivity in cognitively demanding tasks.\nApplying to Ph.D. Programs in Computer Science)\nA guide by Mor Harchol-Balter on time management, research strategies, and thriving during a Ph.D.\nThe Unwritten Laws of Engineering\nThough focused on engineering, W. J. King offers timeless advice on professionalism and effectiveness in technical work.",
    "crumbs": [
      "Appendix",
      "PhD Survival Guide"
    ]
  },
  {
    "objectID": "contents/appendix/phd_survival_guide.html#on-oral-presentation-advice",
    "href": "contents/appendix/phd_survival_guide.html#on-oral-presentation-advice",
    "title": "PhD Survival Guide",
    "section": "On Oral Presentation Advice",
    "text": "On Oral Presentation Advice\n\nOral Presentation Advice\nA concise guide by Mark Hill on delivering clear and engaging oral presentations in academic and technical contexts.\nHow to Give a Good Research Talk\nA guide by Simon Peyton Jones, John Hughes, and John Launchbury on crafting and delivering effective research presentations.\nTen Simple Rules for Making Good Oral Presentations\nA practical set of tips published by PLOS Computational Biology for delivering impactful oral presentations.",
    "crumbs": [
      "Appendix",
      "PhD Survival Guide"
    ]
  },
  {
    "objectID": "contents/appendix/phd_survival_guide.html#on-writing-and-communicating-science",
    "href": "contents/appendix/phd_survival_guide.html#on-writing-and-communicating-science",
    "title": "PhD Survival Guide",
    "section": "On Writing and Communicating Science",
    "text": "On Writing and Communicating Science\nAny suggestions?",
    "crumbs": [
      "Appendix",
      "PhD Survival Guide"
    ]
  },
  {
    "objectID": "contents/appendix/phd_survival_guide.html#video-resources",
    "href": "contents/appendix/phd_survival_guide.html#video-resources",
    "title": "PhD Survival Guide",
    "section": "Video Resources",
    "text": "Video Resources\n\nYou and Your Research by Richard Hamming\nA video lecture of Richard Hamming’s talk on achieving significant research contributions.\nHow to Write a Great Research Paper\nSimon Peyton Jones shares tips on writing research papers and presenting ideas effectively.",
    "crumbs": [
      "Appendix",
      "PhD Survival Guide"
    ]
  },
  {
    "objectID": "contents/core/references.html",
    "href": "contents/core/references.html",
    "title": "References",
    "section": "",
    "text": "0003, Mu Li, David G. Andersen, Alexander J. Smola, and Kai Yu. 2014.\n“Communication Efficient Distributed Machine Learning with the\nParameter Server.” In Advances in Neural Information\nProcessing Systems 27: Annual Conference on Neural Information\nProcessing Systems 2014, December 8-13 2014, Montreal, Quebec,\nCanada, edited by Zoubin Ghahramani, Max Welling, Corinna Cortes,\nNeil D. Lawrence, and Kilian Q. Weinberger, 19–27. https://proceedings.neurips.cc/paper/2014/hash/1ff1de774005f8da13f42943881c655f-Abstract.html.\n\n\nAbadi, Martín, Ashish Agarwal, Paul Barham, et al. 2015.\n“TensorFlow: Large-Scale Machine Learning on Heterogeneous\nSystems.” Google Brain.\n\n\nAbadi, Martín, Ashish Agarwal, Paul Barham, Eugene Brevdo, Zhifeng Chen,\nCraig Citro, Greg S. Corrado, et al. 2016. “TensorFlow:\nLarge-Scale Machine Learning on Heterogeneous Distributed\nSystems.” arXiv Preprint arXiv:1603.04467, March. http://arxiv.org/abs/1603.04467v2.\n\n\nAbadi, Martín, Paul Barham, Jianmin Chen, Zhifeng Chen, Andy Davis,\nJeffrey Dean, Matthieu Devin, et al. 2016. “TensorFlow: A System\nfor Large-Scale Machine Learning.” In 12th USENIX Symposium\non Operating Systems Design and Implementation (OSDI 16), 265–83.\nUSENIX Association. https://www.usenix.org/conference/osdi16/technical-sessions/presentation/abadi.\n\n\nAbadi, Martin, Andy Chu, Ian Goodfellow, H. Brendan McMahan, Ilya\nMironov, Kunal Talwar, and Li Zhang. 2016. “Deep Learning with\nDifferential Privacy.” In Proceedings of the 2016 ACM SIGSAC\nConference on Computer and Communications Security, 308–18. CCS\n’16. New York, NY, USA: ACM. https://doi.org/10.1145/2976749.2978318.\n\n\nAbdelkader, Ahmed, Michael J. Curry, Liam Fowl, Tom Goldstein, Avi\nSchwarzschild, Manli Shu, Christoph Studer, and Chen Zhu. 2020.\n“Headless Horseman: Adversarial Attacks on Transfer\nLearning Models.” In ICASSP 2020 - 2020 IEEE International\nConference on Acoustics, Speech and Signal Processing (ICASSP),\n3087–91. IEEE. https://doi.org/10.1109/icassp40776.2020.9053181.\n\n\nAddepalli, Sravanti, B. S. Vivek, Arya Baburaj, Gaurang Sriramanan, and\nR. Venkatesh Babu. 2020. “Towards Achieving Adversarial Robustness\nby Enforcing Feature Consistency Across Bit Planes.” In 2020\nIEEE/CVF Conference on Computer Vision and Pattern Recognition\n(CVPR), 1020–29. IEEE. https://doi.org/10.1109/cvpr42600.2020.00110.\n\n\nAdolf, Robert, Saketh Rama, Brandon Reagen, Gu-yeon Wei, and David\nBrooks. 2016. “Fathom: Reference Workloads for Modern\nDeep Learning Methods.” In 2016 IEEE International Symposium\non Workload Characterization (IISWC), 1–10. IEEE; IEEE. https://doi.org/10.1109/iiswc.2016.7581275.\n\n\nAgarwal, Alekh, Alina Beygelzimer, Miroslav Dudı́k, John Langford, and\nHanna M. Wallach. 2018. “A Reductions Approach to Fair\nClassification.” In Proceedings of the 35th International\nConference on Machine Learning, ICML 2018, Stockholmsmässan, Stockholm,\nSweden, July 10-15, 2018, edited by Jennifer G. Dy and Andreas\nKrause, 80:60–69. Proceedings of Machine Learning Research. PMLR. http://proceedings.mlr.press/v80/agarwal18a.html.\n\n\nAgnesina, Anthony, Puranjay Rajvanshi, Tian Yang, Geraldo Pradipta,\nAustin Jiao, Ben Keller, Brucek Khailany, and Haoxing Ren. 2023.\n“AutoDMP: Automated DREAMPlace-Based Macro\nPlacement.” In Proceedings of the 2023 International\nSymposium on Physical Design, 149–57. ACM. https://doi.org/10.1145/3569052.3578923.\n\n\nAgrawal, Dakshi, Selcuk Baktir, Deniz Karakoyunlu, Pankaj Rohatgi, and\nBerk Sunar. 2007. “Trojan Detection Using IC\nFingerprinting.” In 2007 IEEE Symposium on Security and\nPrivacy (SP ’07), 296–310. Springer; IEEE. https://doi.org/10.1109/sp.2007.36.\n\n\nAhmadilivani, Mohammad Hasan, Mahdi Taheri, Jaan Raik, Masoud\nDaneshtalab, and Maksim Jenihhin. 2024. “A Systematic Literature\nReview on Hardware Reliability Assessment Methods for Deep Neural\nNetworks.” ACM Comput. Surv. 56 (6): 1–39. https://doi.org/10.1145/3638242.\n\n\n“AI and Compute | OpenAI.” https://openai.com/index/ai-and-compute/.\n\n\n“AI and Efficiency | OpenAI.” https://openai.com/index/ai-and-efficiency/.\n\n\nAkidau, Tyler, Robert Bradshaw, Craig Chambers, Slava Chernyak, Rafael\nJ. Fernández-Moctezuma, Reuven Lax, Sam McVeety, et al. 2015. “The\nDataflow Model: A Practical Approach to Balancing Correctness, Latency,\nand Cost in Massive-Scale, Unbounded, Out-of-Order Data\nProcessing.” Proceedings of the VLDB Endowment 8 (12):\n1792–1803. https://doi.org/10.14778/2824032.2824076.\n\n\nAlghamdi, Wael, Hsiang Hsu, Haewon Jeong, Hao Wang, Peter Michalak,\nShahab Asoodeh, and Flavio Calmon. 2022. “Beyond Adult and\nCOMPAS: Fair Multi-Class Prediction via\nInformation Projection.” Adv. Neur. In. 35: 38747–60.\n\n\nAltayeb, Moez, Marco Zennaro, and Marcelo Rovai. 2022.\n“Classifying Mosquito Wingbeat Sound Using TinyML.” In\nProceedings of the 2022 ACM Conference on Information Technology for\nSocial Good, 132–37. ACM. https://doi.org/10.1145/3524458.3547258.\n\n\nAmershi, Saleema, Andrew Begel, Christian Bird, Rob DeLine, Harald Gall,\nEce Kamar, Nachiappan Nagappan, Besmira Nushi, and Thomas Zimmermann.\n2019. “Software Engineering for Machine Learning: A Case\nStudy.” Proceedings of the 41st International Conference on\nSoftware Engineering: Software Engineering in Practice, 291–300.\n\n\nAmiel, Frederic, Christophe Clavier, and Michael Tunstall. 2006.\n“Fault Analysis of DPA-Resistant Algorithms.” In Fault\nDiagnosis and Tolerance in Cryptography, 223–36. Springer; Springer\nBerlin Heidelberg. https://doi.org/10.1007/11889700\\_20.\n\n\nAmodei, Dario, Danny Hernandez, et al. 2018. “AI and\nCompute.” OpenAI Blog. https://openai.com/research/ai-and-compute.\n\n\nAnthony, Lasse F. Wolff, Benjamin Kanding, and Raghavendra Selvan. 2020.\nICML Workshop on Challenges in Deploying and monitoring Machine Learning\nSystems.\n\n\nAntol, Stanislaw, Aishwarya Agrawal, Jiasen Lu, Margaret Mitchell, Dhruv\nBatra, C. Lawrence Zitnick, and Devi Parikh. 2015. “VQA: Visual\nQuestion Answering.” In 2015 IEEE International Conference on\nComputer Vision (ICCV), 2425–33. IEEE. https://doi.org/10.1109/iccv.2015.279.\n\n\nAntonakakis, Manos, Tim April, Michael Bailey, Matt Bernhard, Elie\nBursztein, Jaime Cochran, Zakir Durumeric, et al. 2017.\n“Understanding the Mirai Botnet.” In 26th USENIX\nSecurity Symposium (USENIX Security 17), 1093–1110.\n\n\nArdila, Rosana, Megan Branson, Kelly Davis, Michael Kohler, Josh Meyer,\nMichael Henretty, Reuben Morais, Lindsay Saunders, Francis Tyers, and\nGregor Weber. 2020. “Common Voice: A Massively-Multilingual Speech\nCorpus.” In Proceedings of the Twelfth Language Resources and\nEvaluation Conference, 4218–22. Marseille, France: European\nLanguage Resources Association. https://aclanthology.org/2020.lrec-1.520.\n\n\nArifeen, Tooba, Abdus Sami Hassan, and Jeong-A Lee. 2020.\n“Approximate Triple Modular Redundancy: A\nSurvey.” #IEEE_O_ACC# 8: 139851–67. https://doi.org/10.1109/access.2020.3012673.\n\n\nAsonov, D., and R. Agrawal. n.d. “Keyboard Acoustic\nEmanations.” In IEEE Symposium on Security and Privacy, 2004.\nProceedings. 2004, 3–11. IEEE; IEEE. https://doi.org/10.1109/secpri.2004.1301311.\n\n\nAteniese, Giuseppe, Luigi V. Mancini, Angelo Spognardi, Antonio Villani,\nDomenico Vitali, and Giovanni Felici. 2015. “Hacking Smart\nMachines with Smarter Ones: How to Extract Meaningful Data from Machine\nLearning Classifiers.” International Journal of Security and\nNetworks 10 (3): 137. https://doi.org/10.1504/ijsn.2015.071829.\n\n\nAttia, Zachi I., Alan Sugrue, Samuel J. Asirvatham, Michael J. Ackerman,\nSuraj Kapa, Paul A. Friedman, and Peter A. Noseworthy. 2018.\n“Noninvasive Assessment of Dofetilide Plasma Concentration Using a\nDeep Learning (Neural Network) Analysis of the Surface\nElectrocardiogram: A Proof of Concept Study.” PLOS ONE\n13 (8): e0201059. https://doi.org/10.1371/journal.pone.0201059.\n\n\nAygun, Sercan, Ece Olcay Gunes, and Christophe De Vleeschouwer. 2021.\n“Efficient and Robust Bitstream Processing in Binarised Neural\nNetworks.” Electron. Lett. 57 (5): 219–22. https://doi.org/10.1049/ell2.12045.\n\n\nBa, Jimmy Lei, Jamie Ryan Kiros, and Geoffrey E. Hinton. 2016.\n“Layer Normalization.” arXiv Preprint\narXiv:1607.06450, July. http://arxiv.org/abs/1607.06450v1.\n\n\nBahdanau, Dzmitry, Kyunghyun Cho, and Yoshua Bengio. 2014. “Neural\nMachine Translation by Jointly Learning to Align and Translate.”\narXiv Preprint arXiv:1409.0473, September. http://arxiv.org/abs/1409.0473v7.\n\n\nBai, Tao, Jinqi Luo, Jun Zhao, Bihan Wen, and Qian Wang. 2021.\n“Recent Advances in Adversarial Training for Adversarial\nRobustness.” arXiv Preprint arXiv:2102.01356.\n\n\nBains, Sunny. 2020. “The Business of Building Brains.”\nNature Electronics 3 (7): 348–51. https://doi.org/10.1038/s41928-020-0449-1.\n\n\nBamoumen, Hatim, Anas Temouden, Nabil Benamar, and Yousra Chtouki. 2022.\n“How TinyML Can Be Leveraged to Solve Environmental Problems: A\nSurvey.” In 2022 International Conference on Innovation and\nIntelligence for Informatics, Computing, and Technologies (3ICT),\n338–43. IEEE; IEEE. https://doi.org/10.1109/3ict56508.2022.9990661.\n\n\nBanbury, Colby, Vijay Janapa Reddi, Peter Torelli, Jeremy Holleman, Nat\nJeffries, Csaba Kiraly, Pietro Montino, et al. 2021. “MLPerf Tiny\nBenchmark.” arXiv Preprint arXiv:2106.07597, June. http://arxiv.org/abs/2106.07597v4.\n\n\nBannon, Pete, Ganesh Venkataramanan, Debjit Das Sarma, and Emil Talpes.\n2019. “Computer and Redundancy Solution for the Full Self-Driving\nComputer.” In 2019 IEEE Hot Chips 31 Symposium (HCS),\n1–22. IEEE Computer Society; IEEE. https://doi.org/10.1109/hotchips.2019.8875645.\n\n\nBarenghi, Alessandro, Guido M. Bertoni, Luca Breveglieri, Mauro\nPellicioli, and Gerardo Pelosi. 2010. “Low Voltage Fault Attacks\nto AES.” In 2010 IEEE International Symposium on\nHardware-Oriented Security and Trust (HOST), 7–12. IEEE; IEEE. https://doi.org/10.1109/hst.2010.5513121.\n\n\nBarroso, Luiz André, and Urs Hölzle. 2007. “The Case for\nEnergy-Proportional Computing.” Computer 40 (12): 33–37.\nhttps://doi.org/10.1109/mc.2007.443.\n\n\nBarroso, Luiz André, Urs Hölzle, and Parthasarathy Ranganathan. 2019.\nThe Datacenter as a Computer: Designing Warehouse-Scale\nMachines. Springer International Publishing. https://doi.org/10.1007/978-3-031-01761-2.\n\n\nBau, David, Bolei Zhou, Aditya Khosla, Aude Oliva, and Antonio Torralba.\n2017. “Network Dissection: Quantifying\nInterpretability of Deep Visual Representations.” In 2017\nIEEE Conference on Computer Vision and Pattern Recognition (CVPR),\n3319–27. IEEE. https://doi.org/10.1109/cvpr.2017.354.\n\n\nBaydin, Atilim Gunes, Barak A. Pearlmutter, Alexey Andreyevich Radul,\nand Jeffrey Mark Siskind. 2017a. “Automatic Differentiation in\nMachine Learning: A Survey.” J. Mach. Learn. Res. 18:\n153:1–43. https://jmlr.org/papers/v18/17-468.html.\n\n\n———. 2017b. “Automatic Differentiation in Machine Learning: A\nSurvey.” J. Mach. Learn. Res. 18 (153): 153:1–43. https://jmlr.org/papers/v18/17-468.html.\n\n\nBeaton, Albert E., and John W. Tukey. 1974. “The Fitting of Power\nSeries, Meaning Polynomials, Illustrated on Band-Spectroscopic\nData.” Technometrics 16 (2): 147. https://doi.org/10.2307/1267936.\n\n\nBeck, Nathaniel, and Simon Jackman. 1998. “Beyond Linearity by\nDefault: Generalized Additive Models.” Am. J.\nPolit. Sci. 42 (2): 596. https://doi.org/10.2307/2991772.\n\n\nBerger, Vance W, and YanYan Zhou. 2014.\n“Kolmogorovsmirnov Test:\nOverview.” Wiley Statsref: Statistics Reference\nOnline.\n\n\nBergstra, James, Olivier Breuleux, Frédéric Bastien, Pascal Lamblin,\nRazvan Pascanu, Guillaume Desjardins, Joseph Turian, David Warde-Farley,\nand Yoshua Bengio. 2010. “Theano: A CPU and GPU Math Compiler in\nPython.” In Proceedings of the 9th Python in Science\nConference, 4:18–24. 1. SciPy. https://doi.org/10.25080/majora-92bf1922-003.\n\n\nBeyer, Lucas, Olivier J. Hénaff, Alexander Kolesnikov, Xiaohua Zhai, and\nAäron van den Oord. 2020. “Are We Done with ImageNet?”\narXiv Preprint arXiv:2006.07159, June. http://arxiv.org/abs/2006.07159v1.\n\n\nBhagoji, Arjun Nitin, Warren He, Bo Li, and Dawn Song. 2018.\n“Practical Black-Box Attacks on Deep Neural Networks Using\nEfficient Query Mechanisms.” In Proceedings of the European\nConference on Computer Vision (ECCV), 154–69.\n\n\nBhardwaj, Kshitij, Marton Havasi, Yuan Yao, David M. Brooks, José Miguel\nHernández-Lobato, and Gu-Yeon Wei. 2020. “A Comprehensive\nMethodology to Determine Optimal Coherence Interfaces for\nMany-Accelerator SoCs.” In Proceedings of the\nACM/IEEE International Symposium on Low Power Electronics and\nDesign, 145–50. ACM. https://doi.org/10.1145/3370748.3406564.\n\n\nBianco, Simone, Remi Cadene, Luigi Celona, and Paolo Napoletano. 2018.\n“Benchmark Analysis of Representative Deep Neural Network\nArchitectures.” IEEE Access 6: 64270–77. https://doi.org/10.1109/access.2018.2877890.\n\n\nBiega, Asia J., Peter Potash, Hal Daumé, Fernando Diaz, and Michèle\nFinck. 2020. “Operationalizing the Legal Principle of Data\nMinimization for Personalization.” In Proceedings of the 43rd\nInternational ACM SIGIR Conference on Research and Development in\nInformation Retrieval, edited by Jimmy Huang, Yi Chang, Xueqi\nCheng, Jaap Kamps, Vanessa Murdock, Ji-Rong Wen, and Yiqun Liu, 399–408.\nACM. https://doi.org/10.1145/3397271.3401034.\n\n\nBiggio, Battista, Blaine Nelson, and Pavel Laskov. 2012.\n“Poisoning Attacks Against Support Vector Machines.” In\nProceedings of the 29th International Conference on Machine\nLearning, ICML 2012, Edinburgh, Scotland, UK, June 26 - July 1,\n2012. icml.cc / Omnipress. http://icml.cc/2012/papers/880.pdf.\n\n\nBiggs, John, James Myers, Jedrzej Kufel, Emre Ozer, Simon Craske, Antony\nSou, Catherine Ramsdale, Ken Williamson, Richard Price, and Scott White.\n2021. “A Natively Flexible 32-Bit Arm Microprocessor.”\nNature 595 (7868): 532–36. https://doi.org/10.1038/s41586-021-03625-w.\n\n\nBinkert, Nathan, Bradford Beckmann, Gabriel Black, Steven K. Reinhardt,\nAli Saidi, Arkaprava Basu, Joel Hestness, et al. 2011. “The Gem5\nSimulator.” ACM SIGARCH Computer Architecture News 39\n(2): 1–7. https://doi.org/10.1145/2024716.2024718.\n\n\nBishop, Christopher M. 2006. Pattern Recognition and Machine\nLearning. Springer.\n\n\nBlackwood, J, FC Wright, NJL Hong, and AR Gagliardi. n.d. “Quality\nof DCIS Information on the Internet: A Content Analysis.”\nBreast Cancer Research and Treatment 177 (2): 295–305. https://doi.org/10.1007/s10549-019-05315-8.\n\n\nBohr, Adam, and Kaveh Memarzadeh. 2020. “The Rise of Artificial\nIntelligence in Healthcare Applications.” In Artificial\nIntelligence in Healthcare, 25–60. Elsevier. https://doi.org/10.1016/b978-0-12-818438-7.00002-2.\n\n\nBolchini, Cristiana, Luca Cassano, Antonio Miele, and Alessandro Toschi.\n2023. “Fast and Accurate Error Simulation for CNNs\nAgainst Soft Errors.” IEEE Trans. Comput. 72 (4):\n984–97. https://doi.org/10.1109/tc.2022.3184274.\n\n\nBondi, Elizabeth, Ashish Kapoor, Debadeepta Dey, James Piavis, Shital\nShah, Robert Hannaford, Arvind Iyer, Lucas Joppa, and Milind Tambe.\n2018. “Near Real-Time Detection of Poachers from Drones in\nAirSim.” In Proceedings of the Twenty-Seventh\nInternational Joint Conference on Artificial Intelligence, edited\nby Jérôme Lang, 5814–16. International Joint Conferences on Artificial\nIntelligence Organization. https://doi.org/10.24963/ijcai.2018/847.\n\n\nBourtoule, Lucas, Varun Chandrasekaran, Christopher A. Choquette-Choo,\nHengrui Jia, Adelin Travers, Baiwu Zhang, David Lie, and Nicolas\nPapernot. 2021. “Machine Unlearning.” In 2021 IEEE\nSymposium on Security and Privacy (SP), 141–59. IEEE; IEEE. https://doi.org/10.1109/sp40001.2021.00019.\n\n\nBradbury, James, Roy Frostig, Peter Hawkins, Matthew James Johnson,\nChris Leary, Dougal Maclaurin, George Necula, et al. 2018. “JAX:\nComposable Transformations of Python+NumPy Programs.” http://github.com/google/jax.\n\n\nBreier, Jakub, Xiaolu Hou, Dirmanto Jap, Lei Ma, Shivam Bhasin, and Yang\nLiu. 2018. “DeepLaser: Practical Fault Attack on Deep Neural\nNetworks.” ArXiv Preprint abs/1806.05859 (June). http://arxiv.org/abs/1806.05859v2.\n\n\nBrown, Tom B., Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan,\nand et al. 2020. “Language Models Are Few-Shot Learners.”\nAdvances in Neural Information Processing Systems (NeurIPS) 33:\n1877–1901.\n\n\nBrown, Tom B., Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan,\nPrafulla Dhariwal, Arvind Neelakantan, et al. 2020. “Language\nModels Are Few-Shot Learners.” arXiv Preprint\narXiv:2005.14165, May. http://arxiv.org/abs/2005.14165v4.\n\n\nBrynjolfsson, Erik, and Andrew McAfee. 2014. The Second Machine Age:\nWork, Progress, and Prosperity in a Time of Brilliant Technologies, 1st\nEdition. W. W. Norton Company.\n\n\nBuolamwini, Joy, and Timnit Gebru. 2018. “Gender Shades:\nIntersectional Accuracy Disparities in Commercial Gender\nClassification.” In Conference on Fairness, Accountability\nand Transparency, 77–91. PMLR.\n\n\nBurnet, David, and Richard Thomas. 1989. “Spycatcher: The\nCommodification of Truth.” Journal of Law and Society 16\n(2): 210. https://doi.org/10.2307/1410360.\n\n\nBurr, Geoffrey W., Matthew J. BrightSky, Abu Sebastian, Huai-Yu Cheng,\nJau-Yi Wu, Sangbum Kim, Norma E. Sosa, et al. 2016. “Recent\nProgress in Phase-Change?Pub _Newline ?Memory\nTechnology.” IEEE Journal on Emerging and Selected Topics in\nCircuits and Systems 6 (2): 146–62. https://doi.org/10.1109/jetcas.2016.2547718.\n\n\nBushnell, Michael L, and Vishwani D Agrawal. 2002. “Built-in\nSelf-Test.” Essentials of Electronic Testing for Digital,\nMemory and Mixed-Signal VLSI Circuits, 489–548.\n\n\nBuyya, Rajkumar, Anton Beloglazov, and Jemal Abawajy. 2010.\n“Energy-Efficient Management of Data Center Resources for Cloud\nComputing: A Vision, Architectural Elements, and Open\nChallenges.” https://arxiv.org/abs/1006.0308.\n\n\nCai, Carrie J., Emily Reif, Narayan Hegde, Jason Hipp, Been Kim, Daniel\nSmilkov, Martin Wattenberg, et al. 2019. “Human-Centered Tools for\nCoping with Imperfect Algorithms During Medical Decision-Making.”\nIn Proceedings of the 2019 CHI Conference on Human Factors in\nComputing Systems, edited by Jennifer G. Dy and Andreas Krause,\n80:2673–82. Proceedings of Machine Learning Research. ACM. https://doi.org/10.1145/3290605.3300234.\n\n\nCai, Han, Chuang Gan, Ligeng Zhu, and Song Han 0003. 2020.\n“TinyTL: Reduce Memory, Not Parameters for Efficient on-Device\nLearning.” In Advances in Neural Information Processing\nSystems 33: Annual Conference on Neural Information Processing Systems\n2020, NeurIPS 2020, December 6-12, 2020, Virtual, edited by Hugo\nLarochelle, Marc’Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan,\nand Hsuan-Tien Lin. https://proceedings.neurips.cc/paper/2020/hash/81f7acabd411274fcf65ce2070ed568a-Abstract.html.\n\n\nCai, Han, Ligeng Zhu, and Song Han. 2019.\n“ProxylessNAS: Direct Neural\nArchitecture Search on Target Task and Hardware.” In 7th\nInternational Conference on Learning Representations, ICLR 2019, New\nOrleans, LA, USA, May 6-9, 2019. OpenReview.net. https://openreview.net/forum?id=HylVB3AqYm.\n\n\nCalvo, Rafael A, Dorian Peters, Karina Vold, and Richard M Ryan. 2020.\n“Supporting Human Autonomy in AI Systems:\nA Framework for Ethical Enquiry.” Ethics of\nDigital Well-Being: A Multidisciplinary Approach, 31–54.\n\n\nCarlini, Nicholas, Pratyush Mishra, Tavish Vaidya, Yuankai Zhang, Micah\nSherr, Clay Shields, David Wagner, and Wenchao Zhou. 2016. “Hidden\nVoice Commands.” In 25th USENIX Security Symposium (USENIX\nSecurity 16), 513–30.\n\n\nCarlini, Nicolas, Jamie Hayes, Milad Nasr, Matthew Jagielski, Vikash\nSehwag, Florian Tramer, Borja Balle, Daphne Ippolito, and Eric Wallace.\n2023. “Extracting Training Data from Diffusion Models.” In\n32nd USENIX Security Symposium (USENIX Security 23), 5253–70.\n\n\nCarta, Salvatore, Alessandro Sebastian Podda, Diego Reforgiato Recupero,\nand Roberto Saia. 2020. “A Local Feature Engineering Strategy to\nImprove Network Anomaly Detection.” Future Internet 12\n(10): 177. https://doi.org/10.3390/fi12100177.\n\n\nCavoukian, Ann. 2009. “Privacy by Design.” Office of\nthe Information and Privacy Commissioner.\n\n\nCenci, Marcelo Pilotto, Tatiana Scarazzato, Daniel Dotto Munchen, Paula\nCristina Dartora, Hugo Marcelo Veit, Andrea Moura Bernardes, and Pablo\nR. Dias. 2021. “Eco-Friendly\nElectronicsA Comprehensive Review.”\nAdv. Mater. Technol. 7 (2): 2001263. https://doi.org/10.1002/admt.202001263.\n\n\nChallenge, WEF Net-Zero. 2021. “The Supply Chain\nOpportunity.” In World Economic Forum: Geneva,\nSwitzerland.\n\n\nChandola, Varun, Arindam Banerjee, and Vipin Kumar. 2009. “Anomaly\nDetection: A Survey.” ACM Comput. Surv. 41 (3): 1–58. https://doi.org/10.1145/1541880.1541882.\n\n\nChapelle, O., B. Scholkopf, and A. Zien Eds. 2009.\n“Semi-Supervised Learning (Chapelle, o. Et Al., Eds.; 2006) [Book\nReviews].” IEEE Transactions on Neural Networks 20 (3):\n542–42. https://doi.org/10.1109/tnn.2009.2015974.\n\n\nChen, Chaofan, Oscar Li, Daniel Tao, Alina Barnett, Cynthia Rudin, and\nJonathan Su. 2019. “This Looks Like That: Deep\nLearning for Interpretable Image Recognition.” In Advances in\nNeural Information Processing Systems 32: Annual Conference on Neural\nInformation Processing Systems 2019, NeurIPS 2019, December 8-14, 2019,\nVancouver, BC, Canada, edited by Hanna M. Wallach, Hugo Larochelle,\nAlina Beygelzimer, Florence d’Alché-Buc, Emily B. Fox, and Roman\nGarnett, 8928–39. https://proceedings.neurips.cc/paper/2019/hash/adf7ee2dcf142b0e11888e72b43fcb75-Abstract.html.\n\n\nChen, Emma, Shvetank Prakash, Vijay Janapa Reddi, David Kim, and Pranav\nRajpurkar. 2023. “A Framework for Integrating Artificial\nIntelligence for Clinical Care with Continuous Therapeutic\nMonitoring.” Nature Biomedical Engineering, November. https://doi.org/10.1038/s41551-023-01115-0.\n\n\nChen, H.-W. 2006. “Gallium, Indium, and Arsenic Pollution of\nGroundwater from a Semiconductor Manufacturing Area of\nTaiwan.” B. Environ. Contam. Tox. 77 (2):\n289–96. https://doi.org/10.1007/s00128-006-1062-3.\n\n\nChen, Mark, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de\nOliveira Pinto, Jared Kaplan, Harri Edwards, et al. 2021.\n“Evaluating Large Language Models Trained on Code.”\narXiv Preprint arXiv:2107.03374, July. http://arxiv.org/abs/2107.03374v2.\n\n\nChen, Mia Xu, Orhan Firat, Ankur Bapna, Melvin Johnson, Wolfgang\nMacherey, George Foster, Llion Jones, et al. 2018. “The Best of\nBoth Worlds: Combining Recent Advances in Neural Machine\nTranslation.” In Proceedings of the 56th Annual Meeting of\nthe Association for Computational Linguistics (Volume 1: Long\nPapers), 30:5998–6008. Association for Computational Linguistics.\nhttps://doi.org/10.18653/v1/p18-1008.\n\n\nChen, Tianqi, Mu Li, Yutian Li, Min Lin, Naiyan Wang, Minjie Wang,\nTianjun Xiao, Bing Xu, Chiyuan Zhang, and Zheng Zhang. 2015.\n“MXNet: A Flexible and Efficient Machine Learning Library for\nHeterogeneous Distributed Systems.” arXiv Preprint\narXiv:1512.01274.\n\n\nChen, Tianqi, Thierry Moreau, Ziheng Jiang, Lianmin Zheng, Eddie Yan,\nHaichen Shen, Meghan Cowan, et al. 2018. “TVM: An Automated\nEnd-to-End Optimizing Compiler for Deep Learning.” In 13th\nUSENIX Symposium on Operating Systems Design and Implementation (OSDI\n18), 578–94.\n\n\nChen, Tianqi, Bing Xu, Chiyuan Zhang, and Carlos Guestrin. 2016.\n“Training Deep Nets with Sublinear Memory Cost.”\nCoRR abs/1604.06174 (April). http://arxiv.org/abs/1604.06174v2.\n\n\nChen, Zhiyong, and Shugong Xu. 2023. “Learning\nDomain-Heterogeneous Speaker Recognition Systems with Personalized\nContinual Federated Learning.” EURASIP Journal on Audio,\nSpeech, and Music Processing 2023 (1): 33. https://doi.org/10.1186/s13636-023-00299-2.\n\n\nChen, Zitao, Guanpeng Li, Karthik Pattabiraman, and Nathan DeBardeleben.\n2019. “iBinFI/i: An Efficient Fault\nInjector for Safety-Critical Machine Learning Systems.” In\nProceedings of the International Conference for High Performance\nComputing, Networking, Storage and Analysis. SC ’19. New York, NY,\nUSA: ACM. https://doi.org/10.1145/3295500.3356177.\n\n\nChen, Zitao, Niranjhana Narayanan, Bo Fang, Guanpeng Li, Karthik\nPattabiraman, and Nathan DeBardeleben. 2020.\n“TensorFI: A Flexible Fault Injection\nFramework for TensorFlow Applications.” In 2020\nIEEE 31st International Symposium on Software Reliability Engineering\n(ISSRE), 426–35. IEEE; IEEE. https://doi.org/10.1109/issre5003.2020.00047.\n\n\nCheng, Eric, Shahrzad Mirkhani, Lukasz G. Szafaryn, Chen-Yong Cher,\nHyungmin Cho, Kevin Skadron, Mircea R. Stan, et al. 2016. “Clear:\nuC/u Ross u-l/u Ayer uE/u Xploration for uA/u Rchitecting uR/u Esilience\n- Combining Hardware and Software Techniques to Tolerate Soft Errors in\nProcessor Cores.” In Proceedings of the 53rd Annual Design\nAutomation Conference, 1–6. ACM. https://doi.org/10.1145/2897937.2897996.\n\n\nCheng, Yu, Duo Wang, Pan Zhou, and Tao Zhang. 2018. “Model\nCompression and Acceleration for Deep Neural Networks: The\nPrinciples, Progress, and Challenges.” IEEE Signal Process\nMag. 35 (1): 126–36. https://doi.org/10.1109/msp.2017.2765695.\n\n\nChetlur, Sharan, Cliff Woolley, Philippe Vandermersch, Jonathan Cohen,\nJohn Tran, Bryan Catanzaro, and Evan Shelhamer. 2014. “Cudnn:\nEfficient Primitives for Deep Learning.” arXiv Preprint\narXiv:1410.0759.\n\n\nChi, Ping, Shuangchen Li, Cong Xu, Tao Zhang, Jishen Zhao, Yongpan Liu,\nYu Wang, and Yuan Xie. 2016. “Prime: A Novel Processing-in-Memory\nArchitecture for Neural Network Computation in ReRAM-Based Main\nMemory.” ACM SIGARCH Computer Architecture News 44 (3):\n27–39. https://doi.org/10.1145/3007787.3001140.\n\n\nCho, Kyunghyun, Bart van Merrienboer, Dzmitry Bahdanau, and Yoshua\nBengio. 2014. “On the Properties of Neural Machine Translation:\nEncoder-Decoder Approaches.” In Eighth Workshop on Syntax,\nSemantics and Structure in Statistical Translation (SSST-8),\n103–11. Association for Computational Linguistics.\n\n\nChollet, François et al. 2015. “Keras.” GitHub\nRepository. https://github.com/fchollet/keras.\n\n\nChollet, François. 2018. “Introduction to Keras.” March\n9th.\n\n\nChristiano, Paul F., Jan Leike, Tom B. Brown, Miljan Martic, Shane Legg,\nand Dario Amodei. 2017. “Deep Reinforcement Learning from Human\nPreferences.” In Advances in Neural Information Processing\nSystems 30: Annual Conference on Neural Information Processing Systems\n2017, December 4-9, 2017, Long Beach, CA, USA, edited by Isabelle\nGuyon, Ulrike von Luxburg, Samy Bengio, Hanna M. Wallach, Rob Fergus, S.\nV. N. Vishwanathan, and Roman Garnett, 4299–4307. https://proceedings.neurips.cc/paper/2017/hash/d5e2c0adad503c91f91df240d0cd4e49-Abstract.html.\n\n\nChu, Grace, Okan Arikan, Gabriel Bender, Weijun Wang, Achille Brighton,\nPieter-Jan Kindermans, Hanxiao Liu, Berkin Akin, Suyog Gupta, and Andrew\nHoward. 2021. “Discovering Multi-Hardware Mobile Models via\nArchitecture Search.” In 2021 IEEE/CVF Conference on Computer\nVision and Pattern Recognition Workshops (CVPRW), 3022–31. IEEE. https://doi.org/10.1109/cvprw53098.2021.00337.\n\n\nChua, L. 1971. “Memristor-the Missing Circuit Element.”\n#IEEE_J_CT# 18 (5): 507–19. https://doi.org/10.1109/tct.1971.1083337.\n\n\nChung, Jae-Won, Yile Gu, Insu Jang, Luoxi Meng, Nikhil Bansal, and\nMosharaf Chowdhury. 2023. “Perseus: Removing Energy\nBloat from Large Model Training.” ArXiv Preprint\nabs/2312.06902. https://arxiv.org/abs/2312.06902.\n\n\nCohen, Maxime C., Ruben Lobel, and Georgia Perakis. 2016. “The\nImpact of Demand Uncertainty on Consumer Subsidies for Green Technology\nAdoption.” Manage. Sci. 62 (5): 1235–58. https://doi.org/10.1287/mnsc.2015.2173.\n\n\nColeman, Cody, Edward Chou, Julian Katz-Samuels, Sean Culatana, Peter\nBailis, Alexander C. Berg, Robert Nowak, Roshan Sumbaly, Matei Zaharia,\nand I. Zeki Yalniz. 2022. “Similarity Search for Efficient Active\nLearning and Search of Rare Concepts.” Proceedings of the\nAAAI Conference on Artificial Intelligence 36 (6): 6402–10. https://doi.org/10.1609/aaai.v36i6.20591.\n\n\nColeman, Cody, Daniel Kang, Deepak Narayanan, Luigi Nardi, Tian Zhao,\nJian Zhang, Peter Bailis, Kunle Olukotun, Chris Ré, and Matei Zaharia.\n2019. “Analysis of DAWNBench, a Time-to-Accuracy Machine Learning\nPerformance Benchmark.” ACM SIGOPS Operating Systems\nReview 53 (1): 14–25. https://doi.org/10.1145/3352020.3352024.\n\n\nConstantinescu, Cristian. 2008. “Intermittent Faults and Effects\non Reliability of Integrated Circuits.” In 2008 Annual\nReliability and Maintainability Symposium, 370–74. IEEE; IEEE. https://doi.org/10.1109/rams.2008.4925824.\n\n\nCooper, Tom, Suzanne Fallender, Joyann Pafumi, Jon Dettling, Sebastien\nHumbert, and Lindsay Lessard. 2011. “A Semiconductor Company’s\nExamination of Its Water Footprint Approach.” In Proceedings\nof the 2011 IEEE International Symposium on Sustainable Systems and\nTechnology, 1–6. IEEE; IEEE. https://doi.org/10.1109/issst.2011.5936865.\n\n\nCope, Gord. 2009. “Pure Water, Semiconductors and the\nRecession.” Global Water Intelligence 10 (10).\n\n\nCorporation, Thinking Machines. 1992. CM-5 Technical Summary.\nThinking Machines Corporation.\n\n\nCourbariaux, Matthieu, Itay Hubara, Daniel Soudry, Ran El-Yaniv, and\nYoshua Bengio. 2016. “Binarized Neural Networks:\nTraining Deep Neural Networks with Weights and Activations\nConstrained to+ 1 or-1.” arXiv Preprint\narXiv:1602.02830.\n\n\nCrankshaw, Daniel, Xin Wang, Guilio Zhou, Michael J Franklin, Joseph E\nGonzalez, and Ion Stoica. 2017. “Clipper: A {Low-Latency} Online Prediction Serving System.”\nIn 14th USENIX Symposium on Networked Systems Design and\nImplementation (NSDI 17), 613–27.\n\n\nCybenko, G. 1992. “Approximation by Superpositions of a Sigmoidal\nFunction.” Mathematics of Control, Signals, and Systems\n5 (4): 455–55. https://doi.org/10.1007/bf02134016.\n\n\nD’ignazio, Catherine, and Lauren F Klein. 2023. Data Feminism.\nMIT press.\n\n\nDally, William J, Stephen W Keckler, and David B Kirk. 2021.\n“Evolution of the Graphics Processing Unit (GPU).” IEEE\nMicro 41 (6): 42–51.\n\n\nDarvish Rouhani, Bita, Azalia Mirhoseini, and Farinaz Koushanfar. 2017.\n“TinyDL: Just-in-Time Deep Learning Solution for Constrained\nEmbedded Systems.” In 2017 IEEE International Symposium on\nCircuits and Systems (ISCAS), 1–4. IEEE. https://doi.org/10.1109/iscas.2017.8050343.\n\n\nDavarzani, Samaneh, David Saucier, Purva Talegaonkar, Erin Parker, Alana\nTurner, Carver Middleton, Will Carroll, et al. 2023. “Closing the\nWearable Gap: Footankle\nKinematic Modeling via Deep Learning Models Based on a Smart Sock\nWearable.” Wearable Technologies 4. https://doi.org/10.1017/wtc.2023.3.\n\n\nDavid, Robert, Jared Duke, Advait Jain, Vijay Janapa Reddi, Nat\nJeffries, Jian Li, Nick Kreeger, et al. 2021. “Tensorflow Lite\nMicro: Embedded Machine Learning for Tinyml Systems.”\nProceedings of Machine Learning and Systems 3: 800–811.\n\n\nDavies, Emma. 2011. “Endangered Elements: Critical\nThinking.” https://www.rsc.org/images/Endangered\\%20Elements\\%20-\\%20Critical\\%20Thinking\\_tcm18-196054.pdf.\n\n\nDavies, Mike, Narayan Srinivasa, Tsung-Han Lin, Gautham Chinya,\nYongqiang Cao, Sri Harsha Choday, Georgios Dimou, et al. 2018.\n“Loihi: A Neuromorphic Manycore Processor with\non-Chip Learning.” IEEE Micro 38 (1): 82–99. https://doi.org/10.1109/mm.2018.112130359.\n\n\nDavies, Mike, Andreas Wild, Garrick Orchard, Yulia Sandamirskaya,\nGabriel A. Fonseca Guerra, Prasad Joshi, Philipp Plank, and Sumedh R.\nRisbud. 2021. “Advancing Neuromorphic Computing with Loihi:\nA Survey of Results and Outlook.” Proc.\nIEEE 109 (5): 911–34. https://doi.org/10.1109/jproc.2021.3067593.\n\n\nDavis, Jacqueline, Daniel Bizo, Andy Lawrence, Owen Rogers, and Max\nSmolaks. 2022. “Uptime Institute Global Data Center Survey\n2022.” Uptime Institute.\n\n\nDayarathna, Miyuru, Yonggang Wen, and Rui Fan. 2016. “Data Center\nEnergy Consumption Modeling: A Survey.” IEEE\nCommunications Surveys &Amp; Tutorials 18 (1): 732–94. https://doi.org/10.1109/comst.2015.2481183.\n\n\nDean, Jeffrey, and Sanjay Ghemawat. 2008. “MapReduce: Simplified\nData Processing on Large Clusters.” Communications of the\nACM 51 (1): 107–13. https://doi.org/10.1145/1327452.1327492.\n\n\nDesai, Tanvi, Felix Ritchie, Richard Welpton, et al. 2016. “Five\nSafes: Designing Data Access for Research.” Economics Working\nPaper Series 1601: 28.\n\n\nDevlin, Jacob, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018.\n“BERT: Pre-Training of Deep Bidirectional Transformers for\nLanguage Understanding,” October, 4171–86. http://arxiv.org/abs/1810.04805v2.\n\n\n———. 2019. “None.” In Proceedings of the 2019\nConference of the North, 4171–86. Minneapolis, Minnesota:\nAssociation for Computational Linguistics. https://doi.org/10.18653/v1/n19-1423.\n\n\nDhar, Sauptik, Junyao Guo, Jiayi (Jason) Liu, Samarth Tripathi, Unmesh\nKurup, and Mohak Shah. 2021. “A Survey of on-Device Machine\nLearning: An Algorithms and Learning Theory Perspective.” ACM\nTransactions on Internet of Things 2 (3): 1–49. https://doi.org/10.1145/3450494.\n\n\nDomingos, Pedro. 2016. “The Master Algorithm: How the Quest for\nthe Ultimate Learning Machine Will Remake Our World.” Choice\nReviews Online 53 (07): 53–3100. https://doi.org/10.5860/choice.194685.\n\n\nDong, Xin, Barbara De Salvo, Meng Li, Chiao Liu, Zhongnan Qu, H. T.\nKung, and Ziyun Li. 2022. “SplitNets:\nDesigning Neural Architectures for Efficient Distributed\nComputing on Head-Mounted Systems.” In 2022 IEEE/CVF\nConference on Computer Vision and Pattern Recognition (CVPR),\n12549–59. IEEE. https://doi.org/10.1109/cvpr52688.2022.01223.\n\n\nDongarra, Jack J. 2009. “The Evolution of High Performance\nComputing on System z.” IBM J. Res. Dev. 53: 3–4.\n\n\nDongarra, Jack J., Jeremy Du Croz, Sven Hammarling, and Richard J.\nHanson. 1988. “An Extended Set of FORTRAN Basic Linear Algebra\nSubprograms.” ACM Transactions on Mathematical Software\n14 (1): 1–17. https://doi.org/10.1145/42288.42291.\n\n\nDosovitskiy, Alexey, Lucas Beyer, Alexander Kolesnikov, Dirk\nWeissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, et al.\n2021. “An Image Is Worth 16x16 Words: Transformers for Image\nRecognition at Scale.” International Conference on Learning\nRepresentations.\n\n\nDuarte, Javier, Nhan Tran, Ben Hawks, Christian Herwig, Jules Muhizi,\nShvetank Prakash, and Vijay Janapa Reddi. 2022.\n“FastML Science Benchmarks: Accelerating\nReal-Time Scientific Edge Machine Learning.” ArXiv\nPreprint abs/2207.07958. https://arxiv.org/abs/2207.07958.\n\n\nDuisterhof, Bardienus P., Shushuai Li, Javier Burgues, Vijay Janapa\nReddi, and Guido C. H. E. de Croon. 2021. “Sniffy Bug: A Fully\nAutonomous Swarm of Gas-Seeking Nano Quadcopters in Cluttered\nEnvironments.” In 2021 IEEE/RSJ International Conference on\nIntelligent Robots and Systems (IROS), 9099–9106. IEEE; IEEE. https://doi.org/10.1109/iros51168.2021.9636217.\n\n\nDwork, Cynthia. n.d. “Differential Privacy: A Survey of\nResults.” In Theory and Applications of Models of\nComputation, 1–19. Springer Berlin Heidelberg. https://doi.org/10.1007/978-3-540-79228-4\\_1.\n\n\nDwork, Cynthia, Frank McSherry, Kobbi Nissim, and Adam Smith. 2006.\n“Calibrating Noise to Sensitivity in Private Data\nAnalysis.” In Theory of Cryptography, edited by Shai\nHalevi and Tal Rabin, 265–84. Berlin, Heidelberg: Springer Berlin\nHeidelberg. https://doi.org/10.1007/11681878\\_14.\n\n\nDwork, Cynthia, and Aaron Roth. 2013. “The Algorithmic Foundations\nof Differential Privacy.” Foundations and Trends® in\nTheoretical Computer Science 9 (3-4): 211–407. https://doi.org/10.1561/0400000042.\n\n\nEbrahimi, Khosrow, Gerard F. Jones, and Amy S. Fleischer. 2014. “A\nReview of Data Center Cooling Technology, Operating Conditions and the\nCorresponding Low-Grade Waste Heat Recovery Opportunities.”\nRenewable Sustainable Energy Rev. 31 (March): 622–38. https://doi.org/10.1016/j.rser.2013.12.007.\n\n\nEgwutuoha, Ifeanyi P., David Levy, Bran Selic, and Shiping Chen. 2013.\n“A Survey of Fault Tolerance Mechanisms and Checkpoint/Restart\nImplementations for High Performance Computing Systems.” The\nJournal of Supercomputing 65 (3): 1302–26. https://doi.org/10.1007/s11227-013-0884-0.\n\n\nEisenman, Assaf, Kiran Kumar Matam, Steven Ingram, Dheevatsa Mudigere,\nRaghuraman Krishnamoorthi, Krishnakumar Nair, Misha Smelyanskiy, and\nMurali Annavaram. 2022. “Check-n-Run: A Checkpointing\nSystem for Training Deep Learning Recommendation Models.” In\n19th USENIX Symposium on Networked Systems Design and Implementation\n(NSDI 22), 929–43.\n\n\nEldan, Ronen, and Mark Russinovich. 2023. “Who’s Harry Potter?\nApproximate Unlearning in LLMs.” ArXiv Preprint\nabs/2310.02238 (October). http://arxiv.org/abs/2310.02238v2.\n\n\nElman, Jeffrey L. 2002. “Finding Structure in Time.” In\nCognitive Modeling, 14:257–88. 2. The MIT Press. https://doi.org/10.7551/mitpress/1888.003.0015.\n\n\nEl-Rayis, A. O. 2014. “Reconfigurable Architectures for the Next\nGeneration of Mobile Device Telecommunications Systems.” :\nhttps://www.researchgate.net/publication/292608967.\n\n\nElsken, Thomas, Jan Hendrik Metzen, and Frank Hutter. 2019.\n“Neural Architecture Search: A Survey.” Journal of\nMachine Learning Research 20 (55): 1–21.\n\n\nEshraghian, Jason K., Max Ward, Emre O. Neftci, Xinxin Wang, Gregor\nLenz, Girish Dwivedi, Mohammed Bennamoun, Doo Seok Jeong, and Wei D. Lu.\n2023. “Training Spiking Neural Networks Using Lessons from Deep\nLearning.” Proc. IEEE 111 (9): 1016–54. https://doi.org/10.1109/jproc.2023.3308088.\n\n\nEsteva, Andre, Brett Kuprel, Roberto A. Novoa, Justin Ko, Susan M.\nSwetter, Helen M. Blau, and Sebastian Thrun. 2017.\n“Dermatologist-Level Classification of Skin Cancer with Deep\nNeural Networks.” Nature 542 (7639): 115–18. https://doi.org/10.1038/nature21056.\n\n\nEykholt, Kevin, Ivan Evtimov, Earlence Fernandes, Bo Li, Amir Rahmati,\nChaowei Xiao, Atul Prakash, Tadayoshi Kohno, and Dawn Song. 2017.\n“Robust Physical-World Attacks on Deep Learning Models.”\nArXiv Preprint abs/1707.08945. https://arxiv.org/abs/1707.08945.\n\n\nFahim, Farah, Benjamin Hawks, Christian Herwig, James Hirschauer, Sergo\nJindariani, Nhan Tran, Luca P. Carloni, et al. 2021. “Hls4ml:\nAn Open-Source Codesign Workflow to Empower Scientific\nLow-Power Machine Learning Devices.” https://arxiv.org/abs/2103.05579.\n\n\nFarah, Martha J. 2005. “Neuroethics: The Practical\nand the Philosophical.” Trends Cogn. Sci. 9 (1): 34–40.\nhttps://doi.org/10.1016/j.tics.2004.12.001.\n\n\nFarwell, James P., and Rafal Rohozinski. 2011. “Stuxnet and the\nFuture of Cyber War.” Survival 53 (1): 23–40. https://doi.org/10.1080/00396338.2011.555586.\n\n\nFeldman, Andrew, Sean Lie, Michael James, et al. 2020. “The\nCerebras Wafer-Scale Engine: Opportunities and Challenges of Building an\nAccelerator at Wafer Scale.” IEEE Micro 40 (2): 20–29.\nhttps://doi.org/10.1109/MM.2020.2975796.\n\n\nFerentinos, Konstantinos P. 2018. “Deep Learning Models for Plant\nDisease Detection and Diagnosis.” Computers and Electronics\nin Agriculture 145 (February): 311–18. https://doi.org/10.1016/j.compag.2018.01.009.\n\n\nFowers, Jeremy, Kalin Ovtcharov, Michael Papamichael, Todd Massengill,\nMing Liu, Daniel Lo, Shlomi Alkalay, et al. 2018. “A Configurable\nCloud-Scale DNN Processor for Real-Time\nAI.” In 2018 ACM/IEEE 45th Annual International\nSymposium on Computer Architecture (ISCA), 1–14. IEEE; IEEE. https://doi.org/10.1109/isca.2018.00012.\n\n\nFrancalanza, Adrian, Luca Aceto, Antonis Achilleos, Duncan Paul Attard,\nIan Cassar, Dario Della Monica, and Anna Ingólfsdóttir. 2017. “A\nFoundation for Runtime Monitoring.” In International\nConference on Runtime Verification, 8–29. Springer.\n\n\nFrankle, Jonathan, and Michael Carbin. 2019. “The Lottery Ticket\nHypothesis: Finding Sparse, Trainable Neural\nNetworks.” In 7th International Conference on Learning\nRepresentations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019.\nOpenReview.net. https://openreview.net/forum?id=rJl-b3RcF7.\n\n\nFriedman, Batya. 1996. “Value-Sensitive Design.”\nInteractions 3 (6): 16–23. https://doi.org/10.1145/242485.242493.\n\n\nFurber, Steve. 2016. “Large-Scale Neuromorphic Computing\nSystems.” J. Neural Eng. 13 (5): 051001. https://doi.org/10.1088/1741-2560/13/5/051001.\n\n\nFursov, Ivan, Matvey Morozov, Nina Kaploukhaya, Elizaveta Kovtun,\nRodrigo Rivera-Castro, Gleb Gusev, Dmitry Babaev, Ivan Kireev, Alexey\nZaytsev, and Evgeny Burnaev. 2021. “Adversarial Attacks on Deep\nModels for Financial Transaction Records.” In Proceedings of\nthe 27th ACM SIGKDD Conference on Knowledge Discovery &Amp; Data\nMining, 2868–78. ACM. https://doi.org/10.1145/3447548.3467145.\n\n\nGale, Trevor, Erich Elsen, and Sara Hooker. 2019. “The State of\nSparsity in Deep Neural Networks.” ArXiv Preprint\nabs/1902.09574. https://arxiv.org/abs/1902.09574.\n\n\nGandolfi, Karine, Christophe Mourtel, and Francis Olivier. 2001.\n“Electromagnetic Analysis: Concrete Results.” In\nCryptographic Hardware and Embedded Systems — CHES 2001,\n251–61. Springer; Springer Berlin Heidelberg. https://doi.org/10.1007/3-540-44709-1\\_21.\n\n\nGannot, G., and M. Ligthart. 1994. “Verilog HDL Based\nFPGA Design.” In International Verilog HDL\nConference, 86–92. IEEE. https://doi.org/10.1109/ivc.1994.323743.\n\n\nGao, Yansong, Said F. Al-Sarawi, and Derek Abbott. 2020. “Physical\nUnclonable Functions.” Nature Electronics 3 (2): 81–91.\nhttps://doi.org/10.1038/s41928-020-0372-5.\n\n\nGates, Byron D. 2009. “Flexible Electronics.”\nScience 323 (5921): 1566–67. https://doi.org/10.1126/science.1171230.\n\n\nGebru, Timnit, Jamie Morgenstern, Briana Vecchione, Jennifer Wortman\nVaughan, Hanna Wallach, Hal Daumé III, and Kate Crawford. 2021.\n“Datasheets for Datasets.” Communications of the\nACM 64 (12): 86–92. https://doi.org/10.1145/3458723.\n\n\nGeiger, Atticus, Hanson Lu, Thomas Icard, and Christopher Potts. 2021.\n“Causal Abstractions of Neural Networks.” In Advances\nin Neural Information Processing Systems 34: Annual Conference on Neural\nInformation Processing Systems 2021, NeurIPS 2021, December 6-14, 2021,\nVirtual, edited by Marc’Aurelio Ranzato, Alina Beygelzimer, Yann N.\nDauphin, Percy Liang, and Jennifer Wortman Vaughan, 9574–86. https://proceedings.neurips.cc/paper/2021/hash/4f5c422f4d49a5a807eda27434231040-Abstract.html.\n\n\nGholami, Dong Kim, Mahoney Yao, and Keutzer. 2021. “A Survey of\nQuantization Methods for Efficient Neural Network Inference).”\nArXiv Preprint. https://arxiv.org/abs/2103.13630.\n\n\nGnad, Dennis R. E., Fabian Oboril, and Mehdi B. Tahoori. 2017.\n“Voltage Drop-Based Fault Attacks on FPGAs Using Valid\nBitstreams.” In 2017 27th International Conference on Field\nProgrammable Logic and Applications (FPL), 1–7. IEEE; IEEE. https://doi.org/10.23919/fpl.2017.8056840.\n\n\nGoodfellow, Ian J., Aaron Courville, and Yoshua Bengio. 2013.\n“Scaling up Spike-and-Slab Models for Unsupervised Feature\nLearning.” IEEE Transactions on Pattern Analysis and Machine\nIntelligence 35 (8): 1902–14. https://doi.org/10.1109/tpami.2012.273.\n\n\nGoodfellow, Ian, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David\nWarde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. 2020.\n“Generative Adversarial Networks.” Commun. ACM 63\n(11): 139–44. https://doi.org/10.1145/3422622.\n\n\nGoodyear, Victoria A. 2017. “Social Media, Apps and Wearable\nTechnologies: Navigating Ethical Dilemmas and\nProcedures.” Qualitative Research in Sport, Exercise and\nHealth 9 (3): 285–302. https://doi.org/10.1080/2159676x.2017.1303790.\n\n\nGordon, Ariel, Elad Eban, Ofir Nachum, Bo Chen, Hao Wu, Tien-Ju Yang,\nand Edward Choi. 2018. “MorphNet: Fast\n&Amp; Simple Resource-Constrained Structure Learning of Deep\nNetworks.” In 2018 IEEE/CVF Conference on Computer Vision and\nPattern Recognition, 1586–95. IEEE. https://doi.org/10.1109/cvpr.2018.00171.\n\n\nGräfe, Ralf, Qutub Syed Sha, Florian Geissler, and Michael Paulitsch.\n2023. “Large-Scale Application of Fault Injection into\nPyTorch Models -an Extension to PyTorchFI for\nValidation Efficiency.” In 2023 53rd Annual IEEE/IFIP\nInternational Conference on Dependable Systems and Networks -\nSupplemental Volume (DSN-s), 56–62. IEEE; IEEE. https://doi.org/10.1109/dsn-s58398.2023.00025.\n\n\nGreengard, Samuel. 2021. The Internet of Things. The MIT Press.\nhttps://doi.org/10.7551/mitpress/13937.001.0001.\n\n\nGroeneveld, Dirk, Iz Beltagy, Pete Walsh, Akshita Bhagia, Rodney Kinney,\nOyvind Tafjord, Ananya Harsh Jha, et al. 2024. “OLMo: Accelerating\nthe Science of Language Models.” arXiv Preprint\narXiv:2402.00838, February. http://arxiv.org/abs/2402.00838v4.\n\n\nGrossman, Elizabeth. 2007. High Tech Trash: Digital\nDevices, Hidden Toxics, and Human Health. Island press.\n\n\nGruslys, Audrunas, Rémi Munos, Ivo Danihelka, Marc Lanctot, and Alex\nGraves. 2016. “Memory-Efficient Backpropagation Through\nTime.” In Advances in Neural Information Processing Systems\n29: Annual Conference on Neural Information Processing Systems 2016,\nDecember 5-10, 2016, Barcelona, Spain, edited by Daniel D. Lee,\nMasashi Sugiyama, Ulrike von Luxburg, Isabelle Guyon, and Roman Garnett,\n4125–33. https://proceedings.neurips.cc/paper/2016/hash/a501bebf79d570651ff601788ea9d16d-Abstract.html.\n\n\nGu, Ivy. 2023. “Deep Learning Model Compression (Ii) by Ivy Gu\nMedium.” https://ivygdy.medium.com/deep-learning-model-compression-ii-546352ea9453.\n\n\nGudivada, Venkat N., Dhana Rao Rao, et al. 2017. “Data Quality\nConsiderations for Big Data and Machine Learning: Going Beyond Data\nCleaning and Transformations.” IEEE Transactions on Knowledge\nand Data Engineering.\n\n\nGujarati, Arpan, Reza Karimi, Safya Alzayat, Wei Hao, Antoine Kaufmann,\nYmir Vigfusson, and Jonathan Mace. 2020. “Serving DNNs Like\nClockwork: Performance Predictability from the Bottom Up.” In\n14th USENIX Symposium on Operating Systems Design and Implementation\n(OSDI 20), 443–62. https://www.usenix.org/conference/osdi20/presentation/gujarati.\n\n\nGulshan, Varun, Lily Peng, Marc Coram, Markus C Stumpe, Derek Wu,\nArunachalam Narayanaswamy, Subhashini Venugopalan, et al. 2016.\n“Development and Validation of a Deep Learning Algorithm for\nDetection of Diabetic Retinopathy in Retinal Fundus Photographs.”\nJAMA 316 (22): 2402–10. https://doi.org/10.1001/jama.2016.17216.\n\n\nGuo, Yutao, Hao Wang, Hui Zhang, Tong Liu, Zhaoguang Liang, Yunlong Xia,\nLi Yan, et al. 2019. “Mobile Photoplethysmographic Technology to\nDetect Atrial Fibrillation.” Journal of the American College\nof Cardiology 74 (19): 2365–75. https://doi.org/10.1016/j.jacc.2019.08.019.\n\n\nGupta, Maanak, Charankumar Akiri, Kshitiz Aryal, Eli Parker, and\nLopamudra Praharaj. 2023. “From ChatGPT to ThreatGPT: Impact of\nGenerative AI in Cybersecurity and Privacy.” IEEE Access\n11: 80218–45. https://doi.org/10.1109/access.2023.3300381.\n\n\nGupta, Maya, Andrew Cotter, Jan Pfeifer, Konstantin Voevodski, Kevin\nCanini, Alexander Mangylov, Wojciech Moczydlowski, and Alexander Van\nEsbroeck. 2016. “Monotonic Calibrated Interpolated Look-up\nTables.” The Journal of Machine Learning Research 17\n(1): 3790–3836.\n\n\nGupta, Udit, Mariam Elgamal, Gage Hills, Gu-Yeon Wei, Hsien-Hsin S. Lee,\nDavid Brooks, and Carole-Jean Wu. 2022. “Act: Designing\nSustainable Computer Systems with an Architectural Carbon Modeling\nTool.” In Proceedings of the 49th Annual International\nSymposium on Computer Architecture, 784–99. ACM. https://doi.org/10.1145/3470496.3527408.\n\n\nGwennap, Linley. n.d. “Certus-NX Innovates\nGeneral-Purpose FPGAs.”\n\n\nHaensch, Wilfried, Tayfun Gokmen, and Ruchir Puri. 2019. “The Next\nGeneration of Deep Learning Hardware: Analog\nComputing.” Proc. IEEE 107 (1): 108–22. https://doi.org/10.1109/jproc.2018.2871057.\n\n\nHamming, R. W. 1950. “Error Detecting and Error Correcting\nCodes.” Bell Syst. Tech. J. 29 (2): 147–60. https://doi.org/10.1002/j.1538-7305.1950.tb00463.x.\n\n\nHan, Song, Huizi Mao, and William J Dally. 2015. “Deep\nCompression: Compressing Deep Neural Networks with Pruning,\nTrained Quantization and Huffman Coding.” arXiv Preprint\narXiv:1510.00149.\n\n\nHandlin, Oscar. 1965. “Science and Technology in Popular\nCulture.” Daedalus-Us., 156–70.\n\n\nHardt, Moritz, Eric Price, and Nati Srebro. 2016. “Equality of\nOpportunity in Supervised Learning.” In Advances in Neural\nInformation Processing Systems 29: Annual Conference on Neural\nInformation Processing Systems 2016, December 5-10, 2016, Barcelona,\nSpain, edited by Daniel D. Lee, Masashi Sugiyama, Ulrike von\nLuxburg, Isabelle Guyon, and Roman Garnett, 3315–23. https://proceedings.neurips.cc/paper/2016/hash/9d2682367c3935defcb1f9e247a97c0d-Abstract.html.\n\n\nHawks, Benjamin, Javier Duarte, Nicholas J. Fraser, Alessandro\nPappalardo, Nhan Tran, and Yaman Umuroglu. 2021. “Ps and Qs: Quantization-aware Pruning for Efficient Low\nLatency Neural Network Inference.” Frontiers in Artificial\nIntelligence 4 (July). https://doi.org/10.3389/frai.2021.676564.\n\n\nHazan, Avi, and Elishai Ezra Tsur. 2021. “Neuromorphic Analog\nImplementation of Neural Engineering Framework-Inspired Spiking Neuron\nfor High-Dimensional Representation.” Front. Neurosci.\n15 (February): 627221. https://doi.org/10.3389/fnins.2021.627221.\n\n\nHe, Kaiming, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016a.\n“Deep Residual Learning for Image Recognition.” In 2016\nIEEE Conference on Computer Vision and Pattern Recognition (CVPR),\n770–78. IEEE. https://doi.org/10.1109/cvpr.2016.90.\n\n\n———. 2016b. “Deep Residual Learning for Image Recognition.”\nIn 2016 IEEE Conference on Computer Vision and Pattern Recognition\n(CVPR), 770–78. IEEE. https://doi.org/10.1109/cvpr.2016.90.\n\n\nHe, Yi, Prasanna Balaprakash, and Yanjing Li. 2020.\n“FIdelity: Efficient Resilience Analysis\nFramework for Deep Learning Accelerators.” In 2020 53rd\nAnnual IEEE/ACM International Symposium on Microarchitecture\n(MICRO), 270–81. IEEE; IEEE. https://doi.org/10.1109/micro50266.2020.00033.\n\n\nHe, Yi, Mike Hutton, Steven Chan, Robert De Gruijl, Rama Govindaraju,\nNishant Patil, and Yanjing Li. 2023. “Understanding and Mitigating\nHardware Failures in Deep Learning Training Systems.” In\nProceedings of the 50th Annual International Symposium on Computer\nArchitecture, 1–16. IEEE; ACM. https://doi.org/10.1145/3579371.3589105.\n\n\nHébert-Johnson, Úrsula, Michael P. Kim, Omer Reingold, and Guy N.\nRothblum. 2018. “Multicalibration: Calibration for\nthe (Computationally-Identifiable) Masses.” In\nProceedings of the 35th International Conference on Machine\nLearning, ICML 2018, Stockholmsmässan, Stockholm, Sweden, July 10-15,\n2018, edited by Jennifer G. Dy and Andreas Krause, 80:1944–53.\nProceedings of Machine Learning Research. PMLR. http://proceedings.mlr.press/v80/hebert-johnson18a.html.\n\n\nHegde, Sumant. 2023. “An Introduction to Separable Convolutions -\nAnalytics Vidhya.” https://www.analyticsvidhya.com/blog/2021/11/an-introduction-to-separable-convolutions/.\n\n\nHenderson, Peter, Jieru Hu, Joshua Romoff, Emma Brunskill, Dan Jurafsky,\nand Joelle Pineau. 2020. “Towards the Systematic Reporting of the\nEnergy and Carbon Footprints of Machine Learning.” The\nJournal of Machine Learning Research 21 (1): 10039–81.\n\n\nHendrycks, Dan, and Thomas Dietterich. 2019. “Benchmarking Neural\nNetwork Robustness to Common Corruptions and Perturbations.”\narXiv Preprint arXiv:1903.12261.\n\n\nHendrycks, Dan, Kevin Zhao, Steven Basart, Jacob Steinhardt, and Dawn\nSong. 2021. “Natural Adversarial Examples.” In 2021\nIEEE/CVF Conference on Computer Vision and Pattern Recognition\n(CVPR), 15257–66. IEEE. https://doi.org/10.1109/cvpr46437.2021.01501.\n\n\nHennessy, John L., and David A. Patterson. 2019. “A New Golden Age\nfor Computer Architecture.” Commun. ACM 62 (2): 48–60.\nhttps://doi.org/10.1145/3282307.\n\n\nHernandez, Danny, Tom B. Brown, et al. 2020. “Measuring the\nAlgorithmic Efficiency of Neural Networks.” OpenAI Blog.\nhttps://openai.com/research/ai-and-efficiency.\n\n\nHeyndrickx, Wouter, Lewis Mervin, Tobias Morawietz, Noé Sturm, Lukas\nFriedrich, Adam Zalewski, Anastasia Pentina, et al. 2023.\n“Melloddy: Cross-Pharma Federated Learning at Unprecedented Scale\nUnlocks Benefits in Qsar Without Compromising Proprietary\nInformation.” Journal of Chemical Information and\nModeling 64 (7): 2331–44. https://pubs.acs.org/doi/10.1021/acs.jcim.3c00799.\n\n\nHimmelstein, Gracie, David Bates, and Li Zhou. 2022. “Examination\nof Stigmatizing Language in the Electronic Health Record.”\nJAMA Network Open 5 (1): e2144967. https://doi.org/10.1001/jamanetworkopen.2021.44967.\n\n\nHinton, Geoffrey. 2005. “Van Nostrand’s Scientific Encyclopedia.” Wiley.\nhttps://doi.org/10.1002/0471743984.vse0673.\n\n\nHinton, Geoffrey, Oriol Vinyals, and Jeff Dean. 2015. “Distilling\nthe Knowledge in a Neural Network.” arXiv Preprint\narXiv:1503.02531. https://arxiv.org/abs/1503.02531.\n\n\nHo Yoon, Jung, Hyung-Suk Jung, Min Hwan Lee, Gun Hwan Kim, Seul Ji Song,\nJun Yeong Seok, Kyung Jean Yoon, et al. 2012. “Frontiers in\nElectronic Materials.” Wiley. https://doi.org/10.1002/9783527667703.ch67.\n\n\nHochreiter, Sepp. 1998. “The Vanishing Gradient Problem During\nLearning Recurrent Neural Nets and Problem Solutions.”\nInternational Journal of Uncertainty, Fuzziness and Knowledge-Based\nSystems 06 (02): 107–16. https://doi.org/10.1142/s0218488598000094.\n\n\nHochreiter, Sepp, and Jürgen Schmidhuber. 1997. “Long Short-Term\nMemory.” Neural Computation 9 (8): 1735–80. https://doi.org/10.1162/neco.1997.9.8.1735.\n\n\nHong, Sanghyun, Nicholas Carlini, and Alexey Kurakin. 2023.\n“Publishing Efficient on-Device Models Increases Adversarial\nVulnerability.” In 2023 IEEE Conference on Secure and\nTrustworthy Machine Learning (SaTML), abs 1603 5279:271–90. IEEE;\nIEEE. https://doi.org/10.1109/satml54575.2023.00026.\n\n\nHooker, Sara. 2021a. “The Hardware Lottery.”\nCommunications of the ACM 64 (12): 58–65. https://doi.org/10.1145/3467017.\n\n\n———. 2021b. “The Hardware Lottery.” Communications of\nthe ACM 64 (12): 58–65. https://doi.org/10.1145/3467017.\n\n\nHornik, Kurt, Maxwell Stinchcombe, and Halbert White. 1989.\n“Multilayer Feedforward Networks Are Universal\nApproximators.” Neural Networks 2 (5): 359–66. https://doi.org/10.1016/0893-6080(89)90020-8.\n\n\nHosseini, Hossein, Sreeram Kannan, Baosen Zhang, and Radha Poovendran.\n2017. “Deceiving Google’s Perspective Api Built for Detecting\nToxic Comments.” ArXiv Preprint abs/1702.08138. https://arxiv.org/abs/1702.08138.\n\n\nHoward, Andrew G., Menglong Zhu, Bo Chen, Dmitry Kalenichenko, Weijun\nWang, Tobias Weyand, Marco Andreetto, and Hartwig Adam. 2017a.\n“MobileNets: Efficient Convolutional Neural Networks for Mobile\nVision Applications.” In arXiv Preprint\narXiv:1704.04861. https://arxiv.org/abs/1704.04861.\n\n\n———. 2017b. “MobileNets: Efficient\nConvolutional Neural Networks for Mobile Vision Applications.”\nArXiv Preprint. https://arxiv.org/abs/1704.04861.\n\n\nHoward, Jeremy, and Sylvain Gugger. 2020. “Fastai: A Layered API\nfor Deep Learning.” Information 11 (2): 108. https://doi.org/10.3390/info11020108.\n\n\nHsiao, Yu-Shun, Zishen Wan, Tianyu Jia, Radhika Ghosal, Abdulrahman\nMahmoud, Arijit Raychowdhury, David Brooks, Gu-Yeon Wei, and Vijay\nJanapa Reddi. 2023. “MAVFI: An\nEnd-to-End Fault Analysis Framework with Anomaly Detection and Recovery\nfor Micro Aerial Vehicles.” In 2023 Design, Automation\n&Amp; Test in Europe Conference &Amp; Exhibition (DATE),\n1–6. IEEE; IEEE. https://doi.org/10.23919/date56975.2023.10137246.\n\n\nHsu, Liang-Ching, Ching-Yi Huang, Yen-Hsun Chuang, Ho-Wen Chen, Ya-Ting\nChan, Heng Yi Teah, Tsan-Yao Chen, Chiung-Fen Chang, Yu-Ting Liu, and\nYu-Min Tzou. 2016. “Accumulation of Heavy Metals and Trace\nElements in Fluvial Sediments Received Effluents from Traditional and\nSemiconductor Industries.” Scientific Reports 6 (1):\n34250. https://doi.org/10.1038/srep34250.\n\n\nHu, Yang, Jie Jiang, Lifu Zhang, Yunfeng Shi, and Jian Shi. 2023.\n“Halide Perovskite Semiconductors.” Wiley. https://doi.org/10.1002/9783527829026.ch13.\n\n\nHuang, Tsung-Ching, Kenjiro Fukuda, Chun-Ming Lo, Yung-Hui Yeh, Tsuyoshi\nSekitani, Takao Someya, and Kwang-Ting Cheng. 2011.\n“Pseudo-CMOS: A Design Style for\nLow-Cost and Robust Flexible Electronics.” IEEE Trans.\nElectron Devices 58 (1): 141–50. https://doi.org/10.1109/ted.2010.2088127.\n\n\nHutter, Frank, Lars Kotthoff, and Joaquin Vanschoren. 2019.\n“Automated Machine Learning: Methods, Systems, Challenges.”\nAutomated Machine Learning, 5–33.\n\n\nHutter, Michael, Jorn-Marc Schmidt, and Thomas Plos. 2009.\n“Contact-Based Fault Injections and Power Analysis on RFID\nTags.” In 2009 European Conference on Circuit Theory and\nDesign, 409–12. IEEE; IEEE. https://doi.org/10.1109/ecctd.2009.5275012.\n\n\nIandola, Forrest N., Song Han, Matthew W. Moskewicz, Khalid Ashraf,\nWilliam J. Dally, and Kurt Keutzer. 2016a. “SqueezeNet:\nAlexNet-Level Accuracy with 50x Fewer Parameters and &lt;0.5MB Model\nSize.” In arXiv Preprint arXiv:1602.07360. https://arxiv.org/abs/1602.07360.\n\n\nIandola, Forrest N, Song Han, Matthew W Moskewicz, Khalid Ashraf,\nWilliam J Dally, and Kurt Keutzer. 2016b.\n“SqueezeNet: Alexnet-level Accuracy with 50x Fewer Parameters\nand 0.5 MB Model Size.” ArXiv Preprint\nabs/1602.07360. https://arxiv.org/abs/1602.07360.\n\n\nIgnatov, Andrey, Radu Timofte, William Chou, Ke Wang, Max Wu, Tim\nHartley, and Luc Van Gool. 2018. “AI Benchmark:\nRunning Deep Neural Networks on Android\nSmartphones,” 0–0.\n\n\nImani, Mohsen, Abbas Rahimi, and Tajana S. Rosing. 2016.\n“Resistive Configurable Associative Memory for Approximate\nComputing.” In Proceedings of the 2016 Design, Automation\n&Amp; Test in Europe Conference &Amp; Exhibition (DATE),\n1327–32. IEEE; Research Publishing Services. https://doi.org/10.3850/9783981537079_0454.\n\n\nInmon, W. H. 2005. Building the Data Warehouse. John Wiley\n& Sons.\n\n\nIntelLabs. 2023. “Knowledge Distillation - Neural Network\nDistiller.” https://intellabs.github.io/distiller/knowledge_distillation.html.\n\n\nIoffe, Sergey, and Christian Szegedy. 2015. “Batch Normalization:\nAccelerating Deep Network Training by Reducing Internal Covariate\nShift.” International Conference on Machine Learning,\n448–56.\n\n\nIppolito, Daphne, Florian Tramer, Milad Nasr, Chiyuan Zhang, Matthew\nJagielski, Katherine Lee, Christopher Choquette Choo, and Nicholas\nCarlini. 2023. “Preventing Generation of Verbatim Memorization in\nLanguage Models Gives a False Sense of Privacy.” In\nProceedings of the 16th International Natural Language Generation\nConference, 5253–70. Association for Computational Linguistics. https://doi.org/10.18653/v1/2023.inlg-main.3.\n\n\nIrimia-Vladu, Mihai. 2014.\n““Green” Electronics:\nBiodegradable and Biocompatible Materials and Devices for\nSustainable Future.” Chem. Soc. Rev. 43 (2): 588–610. https://doi.org/10.1039/c3cs60235d.\n\n\nIsscc. 2014. “Computing’s Energy Problem (and What We Can Do about\nIt).” https://ieeexplore.ieee.org/document/6757323.\n\n\nJacob, Benoit, Skirmantas Kligys, Bo Chen, Menglong Zhu, Matthew Tang,\nAndrew Howard, Hartwig Adam, and Dmitry Kalenichenko. 2018a.\n“Quantization and Training of Neural Networks for Efficient\nInteger-Arithmetic-Only Inference.” In Proceedings of the\nIEEE Conference on Computer Vision and Pattern Recognition (CVPR),\n2704–13.\n\n\n———. 2018b. “Quantization and Training of Neural Networks for\nEfficient Integer-Arithmetic-Only Inference.” In Proceedings\nof the IEEE Conference on Computer Vision and Pattern Recognition,\n2704–13.\n\n\nJanapa Reddi, Vijay, Alexander Elium, Shawn Hymel, David Tischler,\nDaniel Situnayake, Carl Ward, Louis Moreau, et al. 2023. “Edge\nImpulse: An MLOps Platform for Tiny Machine Learning.”\nProceedings of Machine Learning and Systems 5.\n\n\nJeffrey, Dave, and Lisa Benares. 2022. “AI Efficiency: Balancing\nPerformance with Energy Sustainability.” Journal of AI\nResearch 75: 345–60.\n\n\nJha, A. R. 2014. Rare Earth Materials: Properties and\nApplications. CRC Press. https://doi.org/10.1201/b17045.\n\n\nJha, Saurabh, Subho Banerjee, Timothy Tsai, Siva K. S. Hari, Michael B.\nSullivan, Zbigniew T. Kalbarczyk, Stephen W. Keckler, and Ravishankar K.\nIyer. 2019. “ML-Based Fault Injection for Autonomous\nVehicles: A Case for Bayesian Fault\nInjection.” In 2019 49th Annual IEEE/IFIP International\nConference on Dependable Systems and Networks (DSN), 112–24. IEEE;\nIEEE. https://doi.org/10.1109/dsn.2019.00025.\n\n\nJia, Xianyan, Shutao Song, Wei He, Yangzihao Wang, Haidong Rong, Feihu\nZhou, Liqiang Xie, et al. 2018. “Highly Scalable Deep Learning\nTraining System with Mixed-Precision: Training ImageNet in Four\nMinutes.” arXiv Preprint arXiv:1807.11205, July. http://arxiv.org/abs/1807.11205v1.\n\n\nJia, Yangqing, Evan Shelhamer, Jeff Donahue, Sergey Karayev, Jonathan\nLong, Ross Girshick, Sergio Guadarrama, and Trevor Darrell. 2014.\n“Caffe: Convolutional Architecture for Fast Feature\nEmbedding.” In Proceedings of the 22nd ACM International\nConference on Multimedia, 675–78. ACM. https://doi.org/10.1145/2647868.2654889.\n\n\nJia, Zhe, Marco Maggioni, Benjamin Staiger, and Daniele P. Scarpazza.\n2018. “Dissecting the NVIDIA Volta\nGPU Architecture via Microbenchmarking.” ArXiv\nPreprint. https://arxiv.org/abs/1804.06826.\n\n\nJia, Zhihao, Matei Zaharia, and Alex Aiken. 2019. “Beyond Data and\nModel Parallelism for Deep Neural Networks.” In Proceedings\nof Machine Learning and Systems 2019, MLSys 2019, Stanford, CA, USA,\nMarch 31 - April 2, 2019, edited by Ameet Talwalkar, Virginia\nSmith, and Matei Zaharia. mlsys.org. https://proceedings.mlsys.org/book/265.pdf.\n\n\nJin, Yilun, Xiguang Wei, Yang Liu, and Qiang Yang. 2020. “Towards\nUtilizing Unlabeled Data in Federated Learning: A Survey and\nProspective.” arXiv Preprint arXiv:2002.11545, February.\nhttp://arxiv.org/abs/2002.11545v2.\n\n\nJohnson-Roberson, Matthew, Charles Barto, Rounak Mehta, Sharath Nittur\nSridhar, Karl Rosaen, and Ram Vasudevan. 2017. “Driving in the\nMatrix: Can Virtual Worlds Replace Human-Generated Annotations for Real\nWorld Tasks?” In 2017 IEEE International Conference on\nRobotics and Automation (ICRA), 746–53. Singapore, Singapore: IEEE.\nhttps://doi.org/10.1109/icra.2017.7989092.\n\n\nJouppi, Norman P., Cliff Young, Nishant Patil, David Patterson, Gaurav\nAgrawal, Raminder Bajwa, Sarah Bates, et al. 2017a. “In-Datacenter\nPerformance Analysis of a Tensor Processing Unit.” In\nProceedings of the 44th Annual International Symposium on Computer\nArchitecture, 1–12. ISCA ’17. New York, NY, USA: ACM. https://doi.org/10.1145/3079856.3080246.\n\n\n———, et al. 2017b. “In-Datacenter Performance Analysis of a Tensor\nProcessing Unit.” In Proceedings of the 44th Annual\nInternational Symposium on Computer Architecture, 1–12. ISCA ’17.\nNew York, NY, USA: ACM. https://doi.org/10.1145/3079856.3080246.\n\n\n———, et al. 2017c. “In-Datacenter Performance Analysis of a Tensor\nProcessing Unit.” In Proceedings of the 44th Annual\nInternational Symposium on Computer Architecture, 1–12. ACM. https://doi.org/10.1145/3079856.3080246.\n\n\nJouppi, Norman P., Cliff Young, Nishant Patil, David Patterson, and et\nal. 2017. “In-Datacenter Performance Analysis of a Tensor\nProcessing Unit.” Proceedings of the 44th Annual\nInternational Symposium on Computer Architecture (ISCA), 1–12.\n\n\nJouppi, Norm, George Kurian, Sheng Li, Peter Ma, Rahul Nagarajan, Lifeng\nNai, Nishant Patil, et al. 2023. “TPU V4:\nAn Optically Reconfigurable Supercomputer for Machine\nLearning with Hardware Support for Embeddings.” In\nProceedings of the 50th Annual International Symposium on Computer\nArchitecture. ISCA ’23. New York, NY, USA: ACM. https://doi.org/10.1145/3579371.3589350.\n\n\nJoye, Marc, and Michael Tunstall. 2012. Fault Analysis in\nCryptography. Springer Berlin Heidelberg. https://doi.org/10.1007/978-3-642-29656-7.\n\n\nKairouz, Peter, Sewoong Oh, and Pramod Viswanath. 2015. “Secure\nMulti-Party Differential Privacy.” In Advances in Neural\nInformation Processing Systems 28: Annual Conference on Neural\nInformation Processing Systems 2015, December 7-12, 2015, Montreal,\nQuebec, Canada, edited by Corinna Cortes, Neil D. Lawrence, Daniel\nD. Lee, Masashi Sugiyama, and Roman Garnett, 2008–16. https://proceedings.neurips.cc/paper/2015/hash/a01610228fe998f515a72dd730294d87-Abstract.html.\n\n\nKao, Sheng-Chun, Geonhwa Jeong, and Tushar Krishna. 2020.\n“ConfuciuX: Autonomous Hardware Resource\nAssignment for DNN Accelerators Using Reinforcement\nLearning.” In 2020 53rd Annual IEEE/ACM International\nSymposium on Microarchitecture (MICRO), 622–36. IEEE; IEEE. https://doi.org/10.1109/micro50266.2020.00058.\n\n\nKao, Sheng-Chun, and Tushar Krishna. 2020. “Gamma: Automating the\nHW Mapping of DNN Models on Accelerators via Genetic Algorithm.”\nIn Proceedings of the 39th International Conference on\nComputer-Aided Design, 1–9. ACM. https://doi.org/10.1145/3400302.3415639.\n\n\nKaplan, Jared, Sam McCandlish, Tom Henighan, Tom B. Brown, Benjamin\nChess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario\nAmodei. 2020. “Scaling Laws for Neural Language Models.”\nArXiv Preprint abs/2001.08361. https://arxiv.org/abs/2001.08361.\n\n\nKarargyris, Alexandros, Renato Umeton, Micah J. Sheller, Alejandro\nAristizabal, Johnu George, Anna Wuest, Sarthak Pati, et al. 2023.\n“Federated Benchmarking of Medical Artificial Intelligence with\nMedPerf.” Nature Machine Intelligence 5 (7): 799–810. https://doi.org/10.1038/s42256-023-00652-2.\n\n\nKaur, Harmanpreet, Harsha Nori, Samuel Jenkins, Rich Caruana, Hanna\nWallach, and Jennifer Wortman Vaughan. 2020. “Interpreting\nInterpretability: Understanding Data Scientists’ Use of\nInterpretability Tools for Machine Learning.” In Proceedings\nof the 2020 CHI Conference on Human Factors in Computing Systems,\nedited by Regina Bernhaupt, Florian ’Floyd’Mueller, David Verweij, Josh\nAndres, Joanna McGrenere, Andy Cockburn, Ignacio Avellino, et al., 1–14.\nACM. https://doi.org/10.1145/3313831.3376219.\n\n\nKawazoe Aguilera, Marcos, Wei Chen, and Sam Toueg. 1997.\n“Heartbeat: A Timeout-Free Failure Detector for\nQuiescent Reliable Communication.” In Distributed Algorithms:\n11th International Workshop, WDAG’97 Saarbrücken, Germany, September\n2426, 1997 Proceedings 11, 126–40. Springer.\n\n\nKhan, Mohammad Emtiyaz, and Siddharth Swaroop. 2021.\n“Knowledge-Adaptation Priors.” In Advances in Neural\nInformation Processing Systems 34: Annual Conference on Neural\nInformation Processing Systems 2021, NeurIPS 2021, December 6-14, 2021,\nVirtual, edited by Marc’Aurelio Ranzato, Alina Beygelzimer, Yann N.\nDauphin, Percy Liang, and Jennifer Wortman Vaughan, 19757–70. https://proceedings.neurips.cc/paper/2021/hash/a4380923dd651c195b1631af7c829187-Abstract.html.\n\n\nKiela, Douwe, Max Bartolo, Yixin Nie, Divyansh Kaushik, Atticus Geiger,\nZhengxuan Wu, Bertie Vidgen, et al. 2021. “Dynabench: Rethinking\nBenchmarking in NLP.” In Proceedings of the 2021 Conference\nof the North American Chapter of the Association for Computational\nLinguistics: Human Language Technologies, 4110–24. Online:\nAssociation for Computational Linguistics. https://doi.org/10.18653/v1/2021.naacl-main.324.\n\n\nKim, Jungrae, Michael Sullivan, and Mattan Erez. 2015. “Bamboo\nECC: Strong, Safe, and Flexible Codes for\nReliable Computer Memory.” In 2015 IEEE 21st International\nSymposium on High Performance Computer Architecture (HPCA), 101–12.\nIEEE; IEEE. https://doi.org/10.1109/hpca.2015.7056025.\n\n\nKim, Sunju, Chungsik Yoon, Seunghon Ham, Jihoon Park, Ohun Kwon, Donguk\nPark, Sangjun Choi, Seungwon Kim, Kwonchul Ha, and Won Kim. 2018.\n“Chemical Use in the Semiconductor Manufacturing Industry.”\nInt. J. Occup. Env. Heal. 24 (3-4): 109–18. https://doi.org/10.1080/10773525.2018.1519957.\n\n\nKingma, Diederik P., and Jimmy Ba. 2014. “Adam: A Method for\nStochastic Optimization.” ICLR, December. http://arxiv.org/abs/1412.6980v9.\n\n\nKirkpatrick, James, Razvan Pascanu, Neil Rabinowitz, Joel Veness,\nGuillaume Desjardins, Andrei A. Rusu, Kieran Milan, et al. 2017.\n“Overcoming Catastrophic Forgetting in Neural Networks.”\nProc. Natl. Acad. Sci. 114 (13): 3521–26. https://doi.org/10.1073/pnas.1611835114.\n\n\nKleppmann, Martin. 2016. Designing Data-Intensive Applications: The\nBig Ideas Behind Reliable, Scalable, and Maintainable Systems.\nO’Reilly Media. http://shop.oreilly.com/product/0636920032175.do.\n\n\nKo, Yohan. 2021. “Characterizing System-Level Masking Effects\nAgainst Soft Errors.” Electronics 10 (18): 2286. https://doi.org/10.3390/electronics10182286.\n\n\nKocher, Paul, Jann Horn, Anders Fogh, Daniel Genkin, Daniel Gruss,\nWerner Haas, Mike Hamburg, et al. 2019a. “Spectre Attacks:\nExploiting Speculative Execution.” In 2019 IEEE Symposium on\nSecurity and Privacy (SP), 1–19. IEEE. https://doi.org/10.1109/sp.2019.00002.\n\n\n———, et al. 2019b. “Spectre Attacks: Exploiting Speculative\nExecution.” In 2019 IEEE Symposium on Security and Privacy\n(SP), 1–19. IEEE. https://doi.org/10.1109/sp.2019.00002.\n\n\nKocher, Paul, Joshua Jaffe, and Benjamin Jun. 1999. “Differential\nPower Analysis.” In Advances in Cryptology — CRYPTO’ 99,\n388–97. Springer; Springer Berlin Heidelberg. https://doi.org/10.1007/3-540-48405-1\\_25.\n\n\nKocher, Paul, Joshua Jaffe, Benjamin Jun, and Pankaj Rohatgi. 2011.\n“Introduction to Differential Power Analysis.” Journal\nof Cryptographic Engineering 1 (1): 5–27. https://doi.org/10.1007/s13389-011-0006-y.\n\n\nKoh, Pang Wei, Thao Nguyen, Yew Siang Tang, Stephen Mussmann, Emma\nPierson, Been Kim, and Percy Liang. 2020. “Concept Bottleneck\nModels.” In Proceedings of the 37th International Conference\non Machine Learning, ICML 2020, 13-18 July 2020, Virtual Event,\n119:5338–48. Proceedings of Machine Learning Research. PMLR. http://proceedings.mlr.press/v119/koh20a.html.\n\n\nKoh, Pang Wei, Shiori Sagawa, Henrik Marklund, Sang Michael Xie, Marvin\nZhang, Akshay Balsubramani, Weihua Hu, et al. 2021. “WILDS: A\nBenchmark of in-the-Wild Distribution Shifts.” In Proceedings\nof the 38th International Conference on Machine Learning, ICML 2021,\n18-24 July 2021, Virtual Event, edited by Marina Meila and Tong\nZhang, 139:5637–64. Proceedings of Machine Learning Research. PMLR. http://proceedings.mlr.press/v139/koh21a.html.\n\n\nKoren, Yehuda, Robert Bell, and Chris Volinsky. 2009. “Matrix\nFactorization Techniques for Recommender Systems.”\nComputer 42 (8): 30–37. https://doi.org/10.1109/mc.2009.263.\n\n\nKrishna, Adithya, Srikanth Rohit Nudurupati, Chandana D G, Pritesh\nDwivedi, André van Schaik, Mahesh Mehendale, and Chetan Singh Thakur.\n2023. “RAMAN: A Re-Configurable and\nSparse TinyML Accelerator for Inference on Edge.” https://arxiv.org/abs/2306.06493.\n\n\nKrishnamoorthi, Raghuraman. 2018. “Quantizing Deep Convolutional\nNetworks for Efficient Inference: A Whitepaper.” arXiv\nPreprint arXiv:1806.08342.\n\n\nKrishnan, Rayan, Pranav Rajpurkar, and Eric J. Topol. 2022.\n“Self-Supervised Learning in Medicine and Healthcare.”\nNature Biomedical Engineering 6 (12): 1346–52. https://doi.org/10.1038/s41551-022-00914-1.\n\n\nKrishnan, Srivatsan, Natasha Jaques, Shayegan Omidshafiei, Dan Zhang,\nIzzeddin Gur, Vijay Janapa Reddi, and Aleksandra Faust. 2022.\n“Multi-Agent Reinforcement Learning for Microprocessor Design\nSpace Exploration.” https://arxiv.org/abs/2211.16385.\n\n\nKrishnan, Srivatsan, Amir Yazdanbakhsh, Shvetank Prakash, Jason Jabbour,\nIkechukwu Uchendu, Susobhan Ghosh, Behzad Boroujerdian, et al. 2023.\n“ArchGym: An Open-Source Gymnasium for\nMachine Learning Assisted Architecture Design.” In\nProceedings of the 50th Annual International Symposium on Computer\nArchitecture, 1–16. ACM. https://doi.org/10.1145/3579371.3589049.\n\n\nKrizhevsky, Alex, Ilya Sutskever, and Geoffrey E. Hinton. 2012.\n“ImageNet Classification with Deep Convolutional Neural\nNetworks.” Communications of the ACM 60 (6): 84–90. https://doi.org/10.1145/3065386.\n\n\n———. 2017a. “ImageNet Classification with Deep\nConvolutional Neural Networks.” Edited by F. Pereira, C. J.\nBurges, L. Bottou, and K. Q. Weinberger. Commun. ACM 60 (6):\n84–90. https://doi.org/10.1145/3065386.\n\n\n———. 2017b. “ImageNet Classification with Deep Convolutional\nNeural Networks.” Communications of the ACM 60 (6):\n84–90. https://doi.org/10.1145/3065386.\n\n\nKuchaiev, Oleksii, Boris Ginsburg, Igor Gitman, Vitaly Lavrukhin, Carl\nCase, and Paulius Micikevicius. 2018. “OpenSeq2Seq: Extensible\nToolkit for Distributed and Mixed Precision Training of\nSequence-to-Sequence Models.” In Proceedings of Workshop for\nNLP Open Source Software (NLP-OSS), 41–46. Association for\nComputational Linguistics. https://doi.org/10.18653/v1/w18-2507.\n\n\nKuhn, Max, and Kjell Johnson. 2013. Applied Predictive\nModeling. Springer New York. https://doi.org/10.1007/978-1-4614-6849-3.\n\n\nKung, Hsiang Tsung, and Charles E Leiserson. 1979. “Systolic\nArrays (for VLSI).” In Sparse Matrix Proceedings 1978,\n1:256–82. Society for industrial; applied mathematics Philadelphia, PA,\nUSA.\n\n\nKurth, Thorsten, Shashank Subramanian, Peter Harrington, Jaideep Pathak,\nMorteza Mardani, David Hall, Andrea Miele, Karthik Kashinath, and Anima\nAnandkumar. 2023. “FourCastNet:\nAccelerating Global High-Resolution Weather Forecasting\nUsing Adaptive Fourier Neural Operators.” In\nProceedings of the Platform for Advanced Scientific Computing\nConference, 1–11. ACM. https://doi.org/10.1145/3592979.3593412.\n\n\nKuzmin, Andrey, Mart Van Baalen, Yuwei Ren, Markus Nagel, Jorn Peters,\nand Tijmen Blankevoort. 2022. “FP8 Quantization:\nThe Power of the Exponent.” https://arxiv.org/abs/2208.09225.\n\n\nKwon, Jisu, and Daejin Park. 2021. “Hardware/Software\nCo-Design for TinyML Voice-Recognition Application on\nResource Frugal Edge Devices.” Applied Sciences 11 (22):\n11073. https://doi.org/10.3390/app112211073.\n\n\nKwon, Sun Hwa, and Lin Dong. 2022. “Flexible Sensors and Machine\nLearning for Heart Monitoring.” Nano Energy 102\n(November): 107632. https://doi.org/10.1016/j.nanoen.2022.107632.\n\n\nKwon, Young D., Rui Li, Stylianos I. Venieris, Jagmohan Chauhan,\nNicholas D. Lane, and Cecilia Mascolo. 2023. “TinyTrain:\nResource-Aware Task-Adaptive Sparse Training of DNNs at the Data-Scarce\nEdge.” ArXiv Preprint abs/2307.09988 (July). http://arxiv.org/abs/2307.09988v2.\n\n\nLai, Liangzhen, Naveen Suda, and Vikas Chandra. 2018a.\n“CMSIS-NN: Efficient Neural\nNetwork Kernels for Arm Cortex-m CPUs.” https://arxiv.org/abs/1801.06601.\n\n\n———. 2018b. “CMSIS-NN: Efficient Neural Network Kernels for Arm\nCortex-m CPUs.” ArXiv Preprint abs/1801.06601 (January).\nhttp://arxiv.org/abs/1801.06601v1.\n\n\nLakkaraju, Himabindu, and Osbert Bastani. 2020.\n“”How Do i Fool You?”:\nManipulating User Trust via Misleading Black Box Explanations.”\nIn Proceedings of the AAAI/ACM Conference on AI, Ethics, and\nSociety, 79–85. ACM. https://doi.org/10.1145/3375627.3375833.\n\n\nLam, Remi, Alvaro Sanchez-Gonzalez, Matthew Willson, Peter Wirnsberger,\nMeire Fortunato, Ferran Alet, Suman Ravuri, et al. 2023. “Learning\nSkillful Medium-Range Global Weather Forecasting.”\nScience 382 (6677): 1416–21. https://doi.org/10.1126/science.adi2336.\n\n\nLannelongue, Loı̈c, Jason Grealey, and Michael Inouye. 2021. “Green\nAlgorithms: Quantifying the Carbon Footprint of\nComputation.” Adv. Sci. 8 (12): 2100707. https://doi.org/10.1002/advs.202100707.\n\n\nLeCun, Yann, Yoshua Bengio, and Geoffrey Hinton. 2015. “Deep\nLearning.” Nature 521 (7553): 436–44. https://doi.org/10.1038/nature14539.\n\n\nLeCun, Yann, Leon Bottou, Genevieve B. Orr, and Klaus -Robert Müller.\n1998. “Efficient BackProp.” In Neural Networks: Tricks\nof the Trade, 1524:9–50. Springer Berlin Heidelberg. https://doi.org/10.1007/3-540-49430-8\\_2.\n\n\nLeCun, Yann, John S. Denker, and Sara A. Solla. 1990. “Optimal\nBrain Damage.” In Advances in Neural Information Processing\nSystems, 2:598–605. Morgan-Kaufmann.\n\n\nLeCun, Y., B. Boser, J. S. Denker, D. Henderson, R. E. Howard, W.\nHubbard, and L. D. Jackel. 1989. “Backpropagation Applied to\nHandwritten Zip Code Recognition.” Neural Computation 1\n(4): 541–51. https://doi.org/10.1162/neco.1989.1.4.541.\n\n\nLecun, Y., L. Bottou, Y. Bengio, and P. Haffner. 1998.\n“Gradient-Based Learning Applied to Document Recognition.”\nProceedings of the IEEE 86 (11): 2278–2324. https://doi.org/10.1109/5.726791.\n\n\nLee, Minwoong, Namho Lee, Huijeong Gwon, Jongyeol Kim, Younggwan Hwang,\nand Seongik Cho. 2022. “Design of Radiation-Tolerant High-Speed\nSignal Processing Circuit for Detecting Prompt Gamma Rays by Nuclear\nExplosion.” Electronics 11 (18): 2970. https://doi.org/10.3390/electronics11182970.\n\n\nLeRoy Poff, N, MM Brinson, and JW Day. 2002. “Aquatic Ecosystems\n& Global Climate Change.” Pew Center on Global Climate\nChange.\n\n\nLi, Guanpeng, Siva Kumar Sastry Hari, Michael Sullivan, Timothy Tsai,\nKarthik Pattabiraman, Joel Emer, and Stephen W. Keckler. 2017.\n“Understanding Error Propagation in Deep Learning Neural Network\n(DNN) Accelerators and Applications.” In\nProceedings of the International Conference for High Performance\nComputing, Networking, Storage and Analysis, 1–12. ACM. https://doi.org/10.1145/3126908.3126964.\n\n\nLi, Jingzhen, Igbe Tobore, Yuhang Liu, Abhishek Kandwal, Lei Wang, and\nZedong Nie. 2021. “Non-Invasive Monitoring of Three Glucose Ranges\nBased on ECG by Using DBSCAN-CNN.” IEEE Journal of Biomedical\nand Health Informatics 25 (9): 3340–50. https://doi.org/10.1109/jbhi.2021.3072628.\n\n\nLi, Qinbin, Zeyi Wen, Zhaomin Wu, Sixu Hu, Naibo Wang, Yuan Li, Xu Liu,\nand Bingsheng He. 2023. “A Survey on Federated Learning Systems:\nVision, Hype and Reality for Data Privacy and\nProtection.” IEEE Trans. Knowl. Data Eng. 35 (4):\n3347–66. https://doi.org/10.1109/tkde.2021.3124599.\n\n\nLi, Tian, Anit Kumar Sahu, Ameet Talwalkar, and Virginia Smith. 2020.\n“Federated Learning: Challenges, Methods, and Future\nDirections.” IEEE Signal Processing Magazine 37 (3):\n50–60. https://doi.org/10.1109/msp.2020.2975749.\n\n\nLi, Xiang, Tao Qin, Jian Yang, and Tie-Yan Liu. 2016. “LightRNN:\nMemory and Computation-Efficient Recurrent Neural Networks.” In\nAdvances in Neural Information Processing Systems 29: Annual\nConference on Neural Information Processing Systems 2016, December 5-10,\n2016, Barcelona, Spain, edited by Daniel D. Lee, Masashi Sugiyama,\nUlrike von Luxburg, Isabelle Guyon, and Roman Garnett, 4385–93. https://proceedings.neurips.cc/paper/2016/hash/c3e4035af2a1cde9f21e1ae1951ac80b-Abstract.html.\n\n\nLi, Yuhang, Xin Dong, and Wei Wang. 2020. “Additive Powers-of-Two\nQuantization: An Efficient Non-Uniform Discretization for\nNeural Networks.” In 8th International Conference on Learning\nRepresentations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30,\n2020. OpenReview.net. https://openreview.net/forum?id=BkgXT24tDS.\n\n\nLi, Zhuohan, Lianmin Zheng, Yinmin Zhong, Vincent Liu, Ying Sheng, Xin\nJin, Yanping Huang, et al. 2023. “{AlpaServe}:\nStatistical Multiplexing with Model Parallelism for Deep Learning\nServing.” In 17th USENIX Symposium on Operating Systems\nDesign and Implementation (OSDI 23), 663–79.\n\n\nLin, Ji, Wei-Ming Chen, Yujun Lin, John Cohn, Chuang Gan, and Song Han.\n2020. “MCUNet: Tiny Deep Learning on\nIoT Devices.” In Advances in Neural Information\nProcessing Systems 33: Annual Conference on Neural Information\nProcessing Systems 2020, NeurIPS 2020, December 6-12, 2020,\nVirtual, edited by Hugo Larochelle, Marc’Aurelio Ranzato, Raia\nHadsell, Maria-Florina Balcan, and Hsuan-Tien Lin. https://proceedings.neurips.cc/paper/2020/hash/86c51678350f656dcc7f490a43946ee5-Abstract.html.\n\n\nLin, Ji, Ligeng Zhu, Wei-Ming Chen, Wei-Chen Wang, Chuang Gan, and Song\nHan. 2022. “On-Device Training Under 256kb Memory.”\nAdv. Neur. In. 35: 22941–54.\n\n\nLin, Ji, Ligeng Zhu, Wei-Ming Chen, Wei-Chen Wang, and Song Han. 2023.\n“Tiny Machine Learning: Progress and Futures [Feature].”\nIEEE Circuits and Systems Magazine 23 (3): 8–34. https://doi.org/10.1109/mcas.2023.3302182.\n\n\nLin, Tsung-Yi, Michael Maire, Serge Belongie, James Hays, Pietro Perona,\nDeva Ramanan, Piotr Dollár, and C Lawrence Zitnick. 2014.\n“Microsoft Coco: Common Objects in Context.”\nIn Computer VisionECCV 2014: 13th European Conference,\nZurich, Switzerland, September 6-12, 2014, Proceedings, Part v 13,\n740–55. Springer.\n\n\nLindgren, Simon. 2023. Handbook of Critical Studies of Artificial\nIntelligence. Edward Elgar Publishing.\n\n\nLindholm, Andreas, Dave Zachariah, Petre Stoica, and Thomas B. Schon.\n2019. “Data Consistency Approach to Model Validation.”\n#IEEE_O_ACC# 7: 59788–96. https://doi.org/10.1109/access.2019.2915109.\n\n\nLindholm, Erik, John Nickolls, Stuart Oberman, and John Montrym. 2008.\n“NVIDIA Tesla: A Unified Graphics and\nComputing Architecture.” IEEE Micro 28 (2): 39–55. https://doi.org/10.1109/mm.2008.31.\n\n\nLin, Tang Tang, Dang Yang, and Han Gan. 2023. “AWQ:\nActivation-aware Weight Quantization for\nLLM Compression and Acceleration.” ArXiv\nPreprint. https://arxiv.org/abs/2306.00978.\n\n\nLiu, Yanan, Xiaoxia Wei, Jinyu Xiao, Zhijie Liu, Yang Xu, and Yun Tian.\n2020. “Energy Consumption and Emission Mitigation Prediction Based\non Data Center Traffic and PUE for Global Data\nCenters.” Global Energy Interconnection 3 (3): 272–82.\nhttps://doi.org/10.1016/j.gloei.2020.07.008.\n\n\nLiu, Yingcheng, Guo Zhang, Christopher G. Tarolli, Rumen Hristov, Stella\nJensen-Roberts, Emma M. Waddell, Taylor L. Myers, et al. 2022.\n“Monitoring Gait at Home with Radio Waves in Parkinson’s Disease:\nA Marker of Severity, Progression, and Medication Response.”\nScience Translational Medicine 14 (663): eadc9669. https://doi.org/10.1126/scitranslmed.adc9669.\n\n\nLoh, Gabriel H. 2008. “3D-Stacked Memory\nArchitectures for Multi-Core Processors.” ACM SIGARCH\nComputer Architecture News 36 (3): 453–64. https://doi.org/10.1145/1394608.1382159.\n\n\nLopez-Paz, David, and Marc’Aurelio Ranzato. 2017. “Gradient\nEpisodic Memory for Continual Learning.” Adv Neural Inf\nProcess Syst 30.\n\n\nLou, Yin, Rich Caruana, Johannes Gehrke, and Giles Hooker. 2013.\n“Accurate Intelligible Models with Pairwise Interactions.”\nIn Proceedings of the 19th ACM SIGKDD International Conference on\nKnowledge Discovery and Data Mining, edited by Inderjit S. Dhillon,\nYehuda Koren, Rayid Ghani, Ted E. Senator, Paul Bradley, Rajesh Parekh,\nJingrui He, Robert L. Grossman, and Ramasamy Uthurusamy, 623–31. ACM. https://doi.org/10.1145/2487575.2487579.\n\n\nLowy, Andrew, Rakesh Pavan, Sina Baharlouei, Meisam Razaviyayn, and\nAhmad Beirami. 2021. “Fermi: Fair Empirical Risk\nMinimization via Exponential Rényi Mutual Information.”\n\n\nLubana, Ekdeep Singh, and Robert P Dick. 2020. “A Gradient Flow\nFramework for Analyzing Network Pruning.” arXiv Preprint\narXiv:2009.11839.\n\n\nLuebke, David. 2008. “CUDA: Scalable\nParallel Programming for High-Performance Scientific Computing.”\nIn 2008 5th IEEE International Symposium on Biomedical Imaging: From\nNano to Macro, 836–38. IEEE. https://doi.org/10.1109/isbi.2008.4541126.\n\n\nLundberg, Scott M., and Su-In Lee. 2017. “A Unified Approach to\nInterpreting Model Predictions.” In Advances in Neural\nInformation Processing Systems 30: Annual Conference on Neural\nInformation Processing Systems 2017, December 4-9, 2017, Long Beach, CA,\nUSA, edited by Isabelle Guyon, Ulrike von Luxburg, Samy Bengio,\nHanna M. Wallach, Rob Fergus, S. V. N. Vishwanathan, and Roman Garnett,\n4765–74. https://proceedings.neurips.cc/paper/2017/hash/8a20a8621978632d76c43dfd28b67767-Abstract.html.\n\n\nMa, Dongning, Fred Lin, Alban Desmaison, Joel Coburn, Daniel Moore,\nSriram Sankar, and Xun Jiao. 2024. “Dr.\nDNA: Combating Silent Data Corruptions in Deep\nLearning Using Distribution of Neuron Activations.” In\nProceedings of the 29th ACM International Conference on\nArchitectural Support for Programming Languages and Operating Systems,\nVolume 3, 239–52. ACM. https://doi.org/10.1145/3620666.3651349.\n\n\nMaas, Martin, David G. Andersen, Michael Isard, Mohammad Mahdi\nJavanmard, Kathryn S. McKinley, and Colin Raffel. 2024. “Combining\nMachine Learning and Lifetime-Based Resource Management for Memory\nAllocation and Beyond.” Commun. ACM 67 (4): 87–96. https://doi.org/10.1145/3611018.\n\n\nMaass, Wolfgang. 1997. “Networks of Spiking Neurons:\nThe Third Generation of Neural Network Models.”\nNeural Networks 10 (9): 1659–71. https://doi.org/10.1016/s0893-6080(97)00011-7.\n\n\nMadry, Aleksander, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras,\nand Adrian Vladu. 2017. “Towards Deep Learning Models Resistant to\nAdversarial Attacks.” arXiv Preprint arXiv:1706.06083.\n\n\nMahmoud, Abdulrahman, Neeraj Aggarwal, Alex Nobbe, Jose Rodrigo Sanchez\nVicarte, Sarita V. Adve, Christopher W. Fletcher, Iuri Frosio, and Siva\nKumar Sastry Hari. 2020. “PyTorchFI: A\nRuntime Perturbation Tool for DNNs.” In 2020\n50th Annual IEEE/IFIP International Conference on Dependable Systems and\nNetworks Workshops (DSN-w), 25–31. IEEE; IEEE. https://doi.org/10.1109/dsn-w50199.2020.00014.\n\n\nMahmoud, Abdulrahman, Siva Kumar Sastry Hari, Christopher W. Fletcher,\nSarita V. Adve, Charbel Sakr, Naresh Shanbhag, Pavlo Molchanov, Michael\nB. Sullivan, Timothy Tsai, and Stephen W. Keckler. 2021.\n“Optimizing Selective Protection for CNN\nResilience.” In 2021 IEEE 32nd International Symposium on\nSoftware Reliability Engineering (ISSRE), 127–38. IEEE. https://doi.org/10.1109/issre52982.2021.00025.\n\n\nMahmoud, Abdulrahman, Thierry Tambe, Tarek Aloui, David Brooks, and\nGu-Yeon Wei. 2022. “GoldenEye: A\nPlatform for Evaluating Emerging Numerical Data Formats in\nDNN Accelerators.” In 2022 52nd Annual IEEE/IFIP\nInternational Conference on Dependable Systems and Networks (DSN),\n206–14. IEEE. https://doi.org/10.1109/dsn53405.2022.00031.\n\n\nMarković, Danijela, Alice Mizrahi, Damien Querlioz, and Julie Grollier.\n2020. “Physics for Neuromorphic Computing.” Nature\nReviews Physics 2 (9): 499–510. https://doi.org/10.1038/s42254-020-0208-2.\n\n\nMartin, C. Dianne. 1993. “The Myth of the Awesome Thinking\nMachine.” Commun. ACM 36 (4): 120–33. https://doi.org/10.1145/255950.153587.\n\n\nMarulli, Fiammetta, Stefano Marrone, and Laura Verde. 2022.\n“Sensitivity of Machine Learning Approaches to Fake and Untrusted\nData in Healthcare Domain.” Journal of Sensor and Actuator\nNetworks 11 (2): 21. https://doi.org/10.3390/jsan11020021.\n\n\nMaslej, Nestor, Loredana Fattorini, Erik Brynjolfsson, John Etchemendy,\nKatrina Ligett, Terah Lyons, James Manyika, et al. 2023.\n“Artificial Intelligence Index Report 2023.” ArXiv\nPreprint abs/2310.03715. https://arxiv.org/abs/2310.03715.\n\n\nMattson, Peter, Vijay Janapa Reddi, Christine Cheng, Cody Coleman, Greg\nDiamos, David Kanter, Paulius Micikevicius, et al. 2020a. “MLPerf:\nAn Industry Standard Benchmark Suite for Machine Learning\nPerformance.” IEEE Micro 40 (2): 8–16. https://doi.org/10.1109/mm.2020.2974843.\n\n\n———, et al. 2020b. “MLPerf: An Industry\nStandard Benchmark Suite for Machine Learning Performance.”\nIEEE Micro 40 (2): 8–16. https://doi.org/10.1109/mm.2020.2974843.\n\n\nMazumder, Mark, Sharad Chitlangia, Colby Banbury, Yiping Kang, Juan\nManuel Ciro, Keith Achorn, Daniel Galvez, et al. 2021.\n“Multilingual Spoken Words Corpus.” In Thirty-Fifth\nConference on Neural Information Processing Systems Datasets and\nBenchmarks Track (Round 2).\n\n\nMcAuliffe, Michael, Michaela Socolof, Sarah Mihuc, Michael Wagner, and\nMorgan Sonderegger. 2017. “Montreal Forced Aligner: Trainable\nText-Speech Alignment Using Kaldi.” In Interspeech 2017,\n498–502. ISCA. https://doi.org/10.21437/interspeech.2017-1386.\n\n\nMcCarthy, John. 1981. “Epistemological Problems of Artificial\nIntelligence.” In Readings in Artificial Intelligence,\n459–65. Elsevier. https://doi.org/10.1016/b978-0-934613-03-3.50035-0.\n\n\nMcMahan, Brendan, Eider Moore, Daniel Ramage, Seth Hampson, and Blaise\nAgüera y Arcas. 2017b. “Communication-Efficient Learning of Deep\nNetworks from Decentralized Data.” In Proceedings of the 20th\nInternational Conference on Artificial Intelligence and Statistics,\nAISTATS 2017, 20-22 April 2017, Fort Lauderdale, FL, USA, edited by\nAarti Singh and Xiaojin (Jerry) Zhu, 54:1273–82. Proceedings of Machine\nLearning Research. PMLR. http://proceedings.mlr.press/v54/mcmahan17a.html.\n\n\n———. 2017a. “Communication-Efficient Learning of Deep Networks\nfrom Decentralized Data.” In Artificial Intelligence and\nStatistics, 1273–82. PMLR. http://proceedings.mlr.press/v54/mcmahan17a.html.\n\n\nMicikevicius, Paulius, Sharan Narang, Jonah Alben, Gregory Diamos, Erich\nElsen, David Garcia, Boris Ginsburg, et al. 2017b. “Mixed\nPrecision Training.” arXiv Preprint arXiv:1710.03740,\nOctober. http://arxiv.org/abs/1710.03740v3.\n\n\n———, et al. 2017a. “Mixed Precision Training.” arXiv\nPreprint arXiv:1710.03740, October. http://arxiv.org/abs/1710.03740v3.\n\n\nMiller, Charlie. 2019. “Lessons Learned from Hacking a\nCar.” IEEE Design &Amp; Test 36 (6): 7–9. https://doi.org/10.1109/mdat.2018.2863106.\n\n\nMiller, Charlie, and Chris Valasek. 2015. “Remote Exploitation of\nan Unaltered Passenger Vehicle.” Black Hat USA 2015 (S\n91): 1–91.\n\n\nMiller, D. A. B. 2000. “Optical Interconnects to Silicon.”\n#IEEE_J_JSTQE# 6 (6): 1312–17. https://doi.org/10.1109/2944.902184.\n\n\nMills, Andrew, and Stephen Le Hunte. 1997. “An Overview of\nSemiconductor Photocatalysis.” J. Photochem. Photobiol.,\nA 108 (1): 1–35. https://doi.org/10.1016/s1010-6030(97)00118-4.\n\n\nMirhoseini, Azalia, Anna Goldie, Mustafa Yazgan, Joe Wenjie Jiang,\nEbrahim Songhori, Shen Wang, Young-Joon Lee, et al. 2021. “A Graph\nPlacement Methodology for Fast Chip Design.” Nature 594\n(7862): 207–12. https://doi.org/10.1038/s41586-021-03544-w.\n\n\nMishra, Asit K., Jorge Albericio Latorre, Jeff Pool, Darko Stosic, Dusan\nStosic, Ganesh Venkatesh, Chong Yu, and Paulius Micikevicius. 2021.\n“Accelerating Sparse Deep Neural Networks.” CoRR\nabs/2104.08378. https://arxiv.org/abs/2104.08378.\n\n\nMittal, Sparsh, Gaurav Verma, Brajesh Kaushik, and Farooq A. Khanday.\n2021. “A Survey of SRAM-Based in-Memory Computing\nTechniques and Applications.” J. Syst. Architect. 119\n(October): 102276. https://doi.org/10.1016/j.sysarc.2021.102276.\n\n\nModha, Dharmendra S., Filipp Akopyan, Alexander Andreopoulos,\nRathinakumar Appuswamy, John V. Arthur, Andrew S. Cassidy, Pallab Datta,\net al. 2023. “Neural Inference at the Frontier of Energy, Space,\nand Time.” Science 382 (6668): 329–35. https://doi.org/10.1126/science.adh1174.\n\n\nMohanram, K., and N. A. Touba. 2003. “Partial Error Masking to\nReduce Soft Error Failure Rate in Logic Circuits.” In\nProceedings. 16th IEEE Symposium on Computer Arithmetic,\n433–40. IEEE; IEEE Comput. Soc. https://doi.org/10.1109/dftvs.2003.1250141.\n\n\nMonyei, Chukwuka G., and Kirsten E. H. Jenkins. 2018. “Electrons\nHave No Identity: Setting Right Misrepresentations in\nGoogle and Apple’s Clean Energy Purchasing.”\nEnergy Research &Amp; Social Science 46 (December): 48–51.\nhttps://doi.org/10.1016/j.erss.2018.06.015.\n\n\nMoore, Gordon. 2021. “Cramming More Components onto Integrated\nCircuits (1965).”\n\n\nMoshawrab, Mohammad, Mehdi Adda, Abdenour Bouzouane, Hussein Ibrahim,\nand Ali Raad. 2023. “Reviewing Federated Learning Aggregation\nAlgorithms; Strategies, Contributions, Limitations and Future\nPerspectives.” Electronics 12 (10): 2287. https://doi.org/10.3390/electronics12102287.\n\n\nMukherjee, S. S., J. Emer, and S. K. Reinhardt. 2005. “The Soft\nError Problem: An Architectural Perspective.” In\n11th International Symposium on High-Performance Computer\nArchitecture, 243–47. IEEE; IEEE. https://doi.org/10.1109/hpca.2005.37.\n\n\nMunshi, Aaftab. 2009. “The OpenCL\nSpecification.” In 2009 IEEE Hot Chips 21 Symposium\n(HCS), 1–314. IEEE. https://doi.org/10.1109/hotchips.2009.7478342.\n\n\nMusk, Elon et al. 2019. “An Integrated Brain-Machine Interface\nPlatform with Thousands of Channels.” J. Med. Internet\nRes. 21 (10): e16194. https://doi.org/10.2196/16194.\n\n\nMyllyaho, Lalli, Mikko Raatikainen, Tomi Männistö, Jukka K. Nurminen,\nand Tommi Mikkonen. 2022. “On Misbehaviour and Fault Tolerance in\nMachine Learning Systems.” J. Syst. Software 183\n(January): 111096. https://doi.org/10.1016/j.jss.2021.111096.\n\n\nNakano, Jane. 2021. The Geopolitics of Critical Minerals Supply\nChains. JSTOR.\n\n\nNarayanan, Arvind, and Vitaly Shmatikov. 2006. “How to Break\nAnonymity of the Netflix Prize Dataset.” CoRR. http://arxiv.org/abs/cs/0610105.\n\n\nNarayanan, Deepak, Mohammad Shoeybi, Jared Casper, Patrick LeGresley,\nMostofa Patwary, Vijay Korthikanti, Dmitri Vainbrand, et al. 2021.\n“Efficient Large-Scale Language Model Training on GPU Clusters\nUsing Megatron-LM.” In Proceedings of the International\nConference for High Performance Computing, Networking, Storage and\nAnalysis, 1–15. ACM. https://doi.org/10.1145/3458817.3476209.\n\n\nNayak, Prateeth, Takuya Higuchi, Anmol Gupta, Shivesh Ranjan, Stephen\nShum, Siddharth Sigtia, Erik Marchi, et al. 2022. “Improving Voice\nTrigger Detection with Metric Learning.” arXiv Preprint\narXiv:2204.02455, April. http://arxiv.org/abs/2204.02455v2.\n\n\nNg, Davy Tsz Kit, Jac Ka Lok Leung, Kai Wah Samuel Chu, and Maggie Shen\nQiao. 2021. “AI Literacy: Definition,\nTeaching, Evaluation and Ethical Issues.” Proceedings of the\nAssociation for Information Science and Technology 58 (1): 504–9.\n\n\nNgo, Richard, Lawrence Chan, and Sören Mindermann. 2022. “The\nAlignment Problem from a Deep Learning Perspective.” ArXiv\nPreprint abs/2209.00626. https://arxiv.org/abs/2209.00626.\n\n\nNguyen, Ngoc-Bao, Keshigeyan Chandrasegaran, Milad Abdollahzadeh, and\nNgai-Man Cheung. 2023. “Re-Thinking Model Inversion Attacks\nAgainst Deep Neural Networks.” In 2023 IEEE/CVF Conference on\nComputer Vision and Pattern Recognition (CVPR), 16384–93. IEEE. https://doi.org/10.1109/cvpr52729.2023.01572.\n\n\nNorrie, Thomas, Nishant Patil, Doe Hyun Yoon, George Kurian, Sheng Li,\nJames Laudon, Cliff Young, Norman Jouppi, and David Patterson. 2021.\n“The Design Process for Google’s Training Chips:\nTpuv2 and TPUv3.” IEEE Micro\n41 (2): 56–63. https://doi.org/10.1109/mm.2021.3058217.\n\n\nNorthcutt, Curtis G, Anish Athalye, and Jonas Mueller. 2021.\n“Pervasive Label Errors in Test Sets Destabilize Machine Learning\nBenchmarks.” arXiv. https://doi.org/https://doi.org/10.48550/arXiv.2103.14749\narXiv-issued DOI via DataCite.\n\n\nOakden-Rayner, Luke, Jared Dunnmon, Gustavo Carneiro, and Christopher\nRe. 2020. “Hidden Stratification Causes Clinically Meaningful\nFailures in Machine Learning for Medical Imaging.” In\nProceedings of the ACM Conference on Health, Inference, and\nLearning, 151–59. ACM. https://doi.org/10.1145/3368555.3384468.\n\n\nObermeyer, Ziad, Brian Powers, Christine Vogeli, and Sendhil\nMullainathan. 2019. “Dissecting Racial Bias in an Algorithm Used\nto Manage the Health of Populations.” Science 366\n(6464): 447–53. https://doi.org/10.1126/science.aax2342.\n\n\nOecd. 2023. “A Blueprint for Building National Compute Capacity\nfor Artificial Intelligence.” 350. Organisation for Economic\nCo-Operation; Development (OECD). https://doi.org/10.1787/876367e3-en.\n\n\nOECD.AI. 2021. “Measuring the Geographic Distribution of AI\nComputing Capacity.” https://oecd.ai/en/policy-circle/computing-capacity.\n\n\nOlah, Chris, Nick Cammarata, Ludwig Schubert, Gabriel Goh, Michael\nPetrov, and Shan Carter. 2020. “Zoom in: An\nIntroduction to Circuits.” Distill 5 (3): e00024–001. https://doi.org/10.23915/distill.00024.001.\n\n\nOliynyk, Daryna, Rudolf Mayer, and Andreas Rauber. 2023. “I Know\nWhat You Trained Last Summer: A Survey on Stealing Machine Learning\nModels and Defences.” ACM Computing Surveys 55 (14s):\n1–41. https://doi.org/10.1145/3595292.\n\n\nOprea, Alina, Anoop Singhal, and Apostol Vassilev. 2022.\n“Poisoning Attacks Against Machine Learning: Can\nMachine Learning Be Trustworthy?” Computer 55 (11):\n94–99. https://doi.org/10.1109/mc.2022.3190787.\n\n\nPan, Sinno Jialin, and Qiang Yang. 2010. “A Survey on Transfer\nLearning.” IEEE Transactions on Knowledge and Data\nEngineering 22 (10): 1345–59. https://doi.org/10.1109/tkde.2009.191.\n\n\nPanda, Priyadarshini, Indranil Chakraborty, and Kaushik Roy. 2019.\n“Discretization Based Solutions for Secure Machine Learning\nAgainst Adversarial Attacks.” #IEEE_O_ACC# 7: 70157–68.\nhttps://doi.org/10.1109/access.2019.2919463.\n\n\nPapadimitriou, George, and Dimitris Gizopoulos. 2021.\n“Demystifying the System Vulnerability Stack:\nTransient Fault Effects Across the Layers.” In\n2021 ACM/IEEE 48th Annual International Symposium on Computer\nArchitecture (ISCA), 902–15. IEEE; IEEE. https://doi.org/10.1109/isca52012.2021.00075.\n\n\nPapernot, Nicolas, Patrick McDaniel, Xi Wu, Somesh Jha, and Ananthram\nSwami. 2016. “Distillation as a Defense to Adversarial\nPerturbations Against Deep Neural Networks.” In 2016 IEEE\nSymposium on Security and Privacy (SP), 582–97. IEEE; IEEE. https://doi.org/10.1109/sp.2016.41.\n\n\nPark, Daniel S., William Chan, Yu Zhang, Chung-Cheng Chiu, Barret Zoph,\nEkin D. Cubuk, and Quoc V. Le. 2019. “SpecAugment: A Simple Data\nAugmentation Method for Automatic Speech Recognition.” arXiv\nPreprint arXiv:1904.08779, April. http://arxiv.org/abs/1904.08779v3.\n\n\nParrish, Alicia, Hannah Rose Kirk, Jessica Quaye, Charvi Rastogi, Max\nBartolo, Oana Inel, Juan Ciro, et al. 2023. “Adversarial Nibbler:\nA Data-Centric Challenge for Improving the Safety of\nText-to-Image Models.” ArXiv Preprint abs/2305.14384. https://arxiv.org/abs/2305.14384.\n\n\nPaszke, Adam, Sam Gross, Francisco Massa, and et al. 2019.\n“PyTorch: An Imperative Style, High-Performance Deep Learning\nLibrary.” Advances in Neural Information Processing Systems\n(NeurIPS) 32: 8026–37.\n\n\nPaszke, Adam, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury,\nGregory Chanan, Trevor Killeen, et al. 2019. “PyTorch: An\nImperative Style, High-Performance Deep Learning Library.” In\nAdvances in Neural Information Processing Systems, 8026–37.\n\n\nPatterson, David A., and John L. Hennessy. 2021a. Computer\nOrganization and Design RISC-v Edition: The Hardware Software\nInterface. 2nd ed. San Francisco, CA: Morgan Kaufmann.\n\n\nPatterson, David A, and John L Hennessy. 2016. Computer Organization\nand Design ARM Edition: The Hardware Software\nInterface. Morgan kaufmann.\n\n\n———. 2021b. “Carbon Emissions and Large Neural Network\nOptimization.” Communications of the ACM 64 (7): 54–61.\n\n\nPatterson, David, Joseph Gonzalez, Urs Holzle, Quoc Le, Chen Liang,\nLluis-Miquel Munguia, Daniel Rothchild, David R. So, Maud Texier, and\nJeff Dean. 2022. “The Carbon Footprint of Machine Learning\nTraining Will Plateau, Then Shrink.” Computer 55 (7):\n18–28. https://doi.org/10.1109/mc.2022.3148714.\n\n\nPatterson, David, Joseph Gonzalez, Quoc V. Le, Chen Liang, Lluis-Miquel\nMunguia, Daniel Rothchild, David So, Morgan Texier, and Jeff Dean. 2021.\n“Carbon Emissions and Large Neural Network Training.”\narXiv Preprint arXiv:2104.10350. https://arxiv.org/abs/2104.10350.\n\n\nPerrault, Ray, and Jack Clark. 2024. “Artificial Intelligence\nIndex Report 2024.”\n\n\nPeters, Dorian, Rafael A. Calvo, and Richard M. Ryan. 2018.\n“Designing for Motivation, Engagement and Wellbeing in Digital\nExperience.” Front. Psychol. 9 (May): 797. https://doi.org/10.3389/fpsyg.2018.00797.\n\n\nPhillips, P Jonathon, Carina A Hahn, Peter C Fontana, David A\nBroniatowski, and Mark A Przybocki. 2020. “Four Principles of\nExplainable Artificial Intelligence.” Gaithersburg,\nMaryland 18.\n\n\nPineau, Joelle, Philippe Vincent-Lamarre, Koustuv Sinha, Vincent\nLarivière, Alina Beygelzimer, Florence d’Alché-Buc, Emily Fox, and Hugo\nLarochelle. 2021. “Improving Reproducibility in Machine Learning\nResearch (a Report from the Neurips 2019 Reproducibility\nProgram).” Journal of Machine Learning Research 22\n(164): 1–20.\n\n\nPlank, James S. 1997. “A Tutorial on\nReedSolomon Coding for Fault-Tolerance in\nRAID-Like Systems.” Software: Practice and\nExperience 27 (9): 995–1012.\n\n\nPont, Michael J, and Royan HL Ong. 2002. “Using Watchdog Timers to\nImprove the Reliability of Single-Processor Embedded Systems:\nSeven New Patterns and a Case Study.” In\nProceedings of the First Nordic Conference on Pattern Languages of\nPrograms, 159–200. Citeseer.\n\n\nPrakash, Shvetank, Tim Callahan, Joseph Bushagour, Colby Banbury, Alan\nV. Green, Pete Warden, Tim Ansell, and Vijay Janapa Reddi. 2023.\n“CFU Playground: Full-stack Open-Source Framework for Tiny Machine\nLearning (TinyML) Acceleration on\nFPGAs.” In 2023 IEEE International Symposium on\nPerformance Analysis of Systems and Software (ISPASS). Vol.\nabs/2201.01863. IEEE. https://doi.org/10.1109/ispass57527.2023.00024.\n\n\nPrakash, Shvetank, Matthew Stewart, Colby Banbury, Mark Mazumder, Pete\nWarden, Brian Plancher, and Vijay Janapa Reddi. 2023. “Is\nTinyML Sustainable? Assessing the Environmental Impacts of\nMachine Learning on Microcontrollers.” ArXiv Preprint.\nhttps://arxiv.org/abs/2301.11899.\n\n\nPsoma, Sotiria D., and Chryso Kanthou. 2023. “Wearable Insulin\nBiosensors for Diabetes Management: Advances and Challenges.”\nBiosensors 13 (7): 719. https://doi.org/10.3390/bios13070719.\n\n\nPushkarna, Mahima, Andrew Zaldivar, and Oddur Kjartansson. 2022.\n“Data Cards: Purposeful and Transparent Dataset Documentation for\nResponsible AI.” In 2022 ACM Conference on Fairness,\nAccountability, and Transparency, 1776–826. ACM. https://doi.org/10.1145/3531146.3533231.\n\n\nPutnam, Andrew, Adrian M. Caulfield, Eric S. Chung, Derek Chiou, Kypros\nConstantinides, John Demme, Hadi Esmaeilzadeh, et al. 2014a. “A\nReconfigurable Fabric for Accelerating Large-Scale Datacenter\nServices.” ACM SIGARCH Computer Architecture News 42\n(3): 13–24. https://doi.org/10.1145/2678373.2665678.\n\n\n———, et al. 2014b. “A Reconfigurable Fabric for Accelerating\nLarge-Scale Datacenter Services.” ACM SIGARCH Computer\nArchitecture News 42 (3): 13–24. https://doi.org/10.1145/2678373.2665678.\n\n\nQi, Chen, Shibo Shen, Rongpeng Li, Zhifeng Zhao, Qing Liu, Jing Liang,\nand Honggang Zhang. 2021. “An Efficient Pruning Scheme of Deep\nNeural Networks for Internet of Things Applications.” EURASIP\nJournal on Advances in Signal Processing 2021 (1): 31. https://doi.org/10.1186/s13634-021-00744-4.\n\n\nQian, Yu, Xuegong Zhou, Hao Zhou, and Lingli Wang. 2024. “An\nEfficient Reinforcement Learning Based Framework for Exploring Logic\nSynthesis.” ACM Trans. Des. Autom. Electron. Syst. 29\n(2): 1–33. https://doi.org/10.1145/3632174.\n\n\nR. V., Rashmi, and Karthikeyan A. 2018. “Secure Boot of Embedded\nApplications - a Review.” In 2018 Second International\nConference on Electronics, Communication and Aerospace Technology\n(ICECA), 291–98. IEEE. https://doi.org/10.1109/iceca.2018.8474730.\n\n\nRachwan, John, Daniel Zügner, Bertrand Charpentier, Simon Geisler,\nMorgane Ayle, and Stephan Günnemann. 2022. “Winning the Lottery\nAhead of Time: Efficient Early Network Pruning.” In\nInternational Conference on Machine Learning, 18293–309. PMLR.\n\n\nRaina, Rajat, Anand Madhavan, and Andrew Y. Ng. 2009. “Large-Scale\nDeep Unsupervised Learning Using Graphics Processors.” In\nProceedings of the 26th Annual International Conference on Machine\nLearning, edited by Andrea Pohoreckyj Danyluk, Léon Bottou, and\nMichael L. Littman, 382:873–80. ACM International Conference Proceeding\nSeries. ACM. https://doi.org/10.1145/1553374.1553486.\n\n\nRamaswamy, Vikram V., Sunnie S. Y. Kim, Ruth Fong, and Olga Russakovsky.\n2023a. “Overlooked Factors in Concept-Based Explanations:\nDataset Choice, Concept Learnability, and Human\nCapability.” In 2023 IEEE/CVF Conference on Computer Vision\nand Pattern Recognition (CVPR), 10932–41. IEEE. https://doi.org/10.1109/cvpr52729.2023.01052.\n\n\nRamaswamy, Vikram V, Sunnie SY Kim, Ruth Fong, and Olga Russakovsky.\n2023b. “UFO: A Unified Method for\nControlling Understandability and Faithfulness Objectives in\nConcept-Based Explanations for CNNs.” ArXiv\nPreprint abs/2303.15632. https://arxiv.org/abs/2303.15632.\n\n\nRamcharan, Amanda, Kelsee Baranowski, Peter McCloskey, Babuali Ahmed,\nJames Legg, and David P. Hughes. 2017. “Deep Learning for\nImage-Based Cassava Disease Detection.” Frontiers in Plant\nScience 8 (October): 1852. https://doi.org/10.3389/fpls.2017.01852.\n\n\nRamesh, Aditya, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss,\nAlec Radford, Mark Chen, and Ilya Sutskever. 2021. “Zero-Shot\nText-to-Image Generation.” In Proceedings of the 38th\nInternational Conference on Machine Learning, ICML 2021, 18-24 July\n2021, Virtual Event, edited by Marina Meila and Tong Zhang,\n139:8821–31. Proceedings of Machine Learning Research. PMLR. http://proceedings.mlr.press/v139/ramesh21a.html.\n\n\nRanganathan, Parthasarathy. 2011. “From Microprocessors to\nNanostores: Rethinking Data-Centric Systems.”\nComputer 44 (1): 39–48. https://doi.org/10.1109/mc.2011.18.\n\n\nRashid, Layali, Karthik Pattabiraman, and Sathish Gopalakrishnan. 2012.\n“Intermittent Hardware Errors Recovery: Modeling and\nEvaluation.” In 2012 Ninth International Conference on\nQuantitative Evaluation of Systems, 220–29. IEEE; IEEE. https://doi.org/10.1109/qest.2012.37.\n\n\n———. 2015. “Characterizing the Impact of Intermittent Hardware\nFaults on Programs.” IEEE Trans. Reliab. 64 (1):\n297–310. https://doi.org/10.1109/tr.2014.2363152.\n\n\nRatner, Alex, Braden Hancock, Jared Dunnmon, Roger Goldman, and\nChristopher Ré. 2018. “Snorkel MeTaL: Weak Supervision for\nMulti-Task Learning.” In Proceedings of the Second Workshop\non Data Management for End-to-End Machine Learning. ACM. https://doi.org/10.1145/3209889.3209898.\n\n\nReagen, Brandon, Robert Adolf, Paul Whatmough, Gu-Yeon Wei, and David\nBrooks. 2017. Deep Learning for Computer Architects. Springer\nInternational Publishing. https://doi.org/10.1007/978-3-031-01756-8.\n\n\nReagen, Brandon, Udit Gupta, Lillian Pentecost, Paul Whatmough, Sae Kyu\nLee, Niamh Mulholland, David Brooks, and Gu-Yeon Wei. 2018. “Ares:\nA Framework for Quantifying the Resilience of Deep Neural\nNetworks.” In 2018 55th ACM/ESDA/IEEE Design Automation\nConference (DAC), 1–6. IEEE. https://doi.org/10.1109/dac.2018.8465834.\n\n\nReagen, Brandon, Jose Miguel Hernandez-Lobato, Robert Adolf, Michael\nGelbart, Paul Whatmough, Gu-Yeon Wei, and David Brooks. 2017. “A\nCase for Efficient Accelerator Design Space Exploration via\nBayesian Optimization.” In 2017 IEEE/ACM\nInternational Symposium on Low Power Electronics and Design\n(ISLPED), 1–6. IEEE; IEEE. https://doi.org/10.1109/islped.2017.8009208.\n\n\nReddi, Vijay Janapa, Christine Cheng, David Kanter, Peter Mattson,\nGuenther Schmuelling, Carole-Jean Wu, Brian Anderson, et al. 2020.\n“MLPerf Inference Benchmark.” In 2020\nACM/IEEE 47th Annual International Symposium on Computer Architecture\n(ISCA), 446–59. IEEE; IEEE. https://doi.org/10.1109/isca45697.2020.00045.\n\n\nReddi, Vijay Janapa, and Meeta Sharma Gupta. 2013. Resilient\nArchitecture Design for Voltage Variation. Springer International\nPublishing. https://doi.org/10.1007/978-3-031-01739-1.\n\n\nReis, G. A., J. Chang, N. Vachharajani, R. Rangan, and D. I. August.\n2005. “SWIFT: Software Implemented Fault\nTolerance.” In International Symposium on Code Generation and\nOptimization, 243–54. IEEE; IEEE. https://doi.org/10.1109/cgo.2005.34.\n\n\nResearch, Microsoft. 2021. DeepSpeed: Extreme-Scale Model Training\nfor Everyone.\n\n\nRibeiro, Marco Tulio, Sameer Singh, and Carlos Guestrin. 2016.\n“” Why Should i Trust You?” Explaining\nthe Predictions of Any Classifier.” In Proceedings of the\n22nd ACM SIGKDD International Conference on Knowledge Discovery and Data\nMining, 1135–44.\n\n\nRombach, Robin, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and\nBjorn Ommer. 2022. “High-Resolution Image Synthesis with Latent\nDiffusion Models.” In 2022 IEEE/CVF Conference on Computer\nVision and Pattern Recognition (CVPR). IEEE. https://doi.org/10.1109/cvpr52688.2022.01042.\n\n\nRomero, Francisco, Qian Li 0027, Neeraja J. Yadwadkar, and Christos\nKozyrakis. 2021. “INFaaS: Automated Model-Less Inference\nServing.” In 2021 USENIX Annual Technical Conference (USENIX\nATC 21), 397–411. https://www.usenix.org/conference/atc21/presentation/romero.\n\n\nRosenblatt, F. 1958. “The Perceptron: A Probabilistic Model for\nInformation Storage and Organization in the Brain.”\nPsychological Review 65 (6): 386–408. https://doi.org/10.1037/h0042519.\n\n\nRoskies, Adina. 2002. “Neuroethics for the New Millenium.”\nNeuron 35 (1): 21–23. https://doi.org/10.1016/s0896-6273(02)00763-8.\n\n\nRudin, Cynthia. 2019. “Stop Explaining Black Box Machine Learning\nModels for High Stakes Decisions and Use Interpretable Models\nInstead.” Nature Machine Intelligence 1 (5): 206–15. https://doi.org/10.1038/s42256-019-0048-x.\n\n\nRumelhart, David E., Geoffrey E. Hinton, and Ronald J. Williams. 1986.\n“Learning Representations by Back-Propagating Errors.”\nNature 323 (6088): 533–36. https://doi.org/10.1038/323533a0.\n\n\nRussell, Stuart. 2021. “Human-Compatible Artificial\nIntelligence.” Human-Like Machine Intelligence, 3–23.\n\n\nRyan, Richard M., and Edward L. Deci. 2000. “Self-Determination\nTheory and the Facilitation of Intrinsic Motivation, Social Development,\nand Well-Being.” Am. Psychol. 55 (1): 68–78. https://doi.org/10.1037/0003-066x.55.1.68.\n\n\nSamajdar, Ananda, Yuhao Zhu, Paul Whatmough, Matthew Mattina, and Tushar\nKrishna. 2018. “Scale-Sim: Systolic Cnn Accelerator\nSimulator.” ArXiv Preprint abs/1811.02883. https://arxiv.org/abs/1811.02883.\n\n\nSambasivan, Nithya, Shivani Kapania, Hannah Highfill, Diana Akrong,\nPraveen Paritosh, and Lora M Aroyo. 2021. “‘Everyone Wants\nto Do the Model Work, Not the Data Work’: Data Cascades in\nHigh-Stakes AI.” In Proceedings of the 2021 CHI Conference on\nHuman Factors in Computing Systems, 1–15. ACM. https://doi.org/10.1145/3411764.3445518.\n\n\nSangchoolie, Behrooz, Karthik Pattabiraman, and Johan Karlsson. 2017.\n“One Bit Is (Not) Enough: An Empirical\nStudy of the Impact of Single and Multiple Bit-Flip Errors.” In\n2017 47th Annual IEEE/IFIP International Conference on Dependable\nSystems and Networks (DSN), 97–108. IEEE; IEEE. https://doi.org/10.1109/dsn.2017.30.\n\n\nSchäfer, Mike S. 2023. “The Notorious GPT:\nScience Communication in the Age of Artificial\nIntelligence.” Journal of Science Communication 22 (02):\nY02. https://doi.org/10.22323/2.22020402.\n\n\nSchuman, Catherine D., Shruti R. Kulkarni, Maryam Parsa, J. Parker\nMitchell, Prasanna Date, and Bill Kay. 2022. “Opportunities for\nNeuromorphic Computing Algorithms and Applications.” Nature\nComputational Science 2 (1): 10–19. https://doi.org/10.1038/s43588-021-00184-y.\n\n\nSchwartz, Daniel, Jonathan Michael Gomes Selman, Peter Wrege, and\nAndreas Paepcke. 2021. “Deployment of Embedded\nEdge-AI for Wildlife Monitoring in Remote Regions.”\nIn 2021 20th IEEE International Conference on Machine Learning and\nApplications (ICMLA), 1035–42. IEEE; IEEE. https://doi.org/10.1109/icmla52953.2021.00170.\n\n\nSchwartz, Roy, Jesse Dodge, Noah A. Smith, and Oren Etzioni. 2020.\n“Green AI.” Commun. ACM 63 (12):\n54–63. https://doi.org/10.1145/3381831.\n\n\nSculley, David, Gary Holt, Daniel Golovin, Eugene Davydov, Todd\nPhillips, Dietmar Ebner, Vinay Chaudhary, Michael Young, Jean-Francois\nCrespo, and Dan Dennison. 2015. “Hidden Technical Debt in Machine\nLearning Systems.” Advances in Neural Information Processing\nSystems 28.\n\n\nSegal, Mark, and Kurt Akeley. 1999. “The OpenGL\nGraphics System: A Specification (Version 1.1).”\n\n\nSegura Anaya, L. H., Abeer Alsadoon, N. Costadopoulos, and P. W. C.\nPrasad. 2017. “Ethical Implications of User Perceptions of\nWearable Devices.” Sci. Eng. Ethics 24 (1): 1–28. https://doi.org/10.1007/s11948-017-9872-8.\n\n\nSeide, Frank, and Amit Agarwal. 2016. “CNTK: Microsoft’s\nOpen-Source Deep-Learning Toolkit.” In Proceedings of the\n22nd ACM SIGKDD International Conference on Knowledge Discovery and Data\nMining, 2135–35. ACM. https://doi.org/10.1145/2939672.2945397.\n\n\nSelvaraju, Ramprasaath R., Michael Cogswell, Abhishek Das, Ramakrishna\nVedantam, Devi Parikh, and Dhruv Batra. 2017.\n“Grad-CAM: Visual Explanations from Deep\nNetworks via Gradient-Based Localization.” In 2017 IEEE\nInternational Conference on Computer Vision (ICCV), 618–26. IEEE.\nhttps://doi.org/10.1109/iccv.2017.74.\n\n\nSeong, Nak Hee, Dong Hyuk Woo, Vijayalakshmi Srinivasan, Jude A. Rivers,\nand Hsien-Hsin S. Lee. 2010. “SAFER: Stuck-at-fault Error Recovery for\nMemories.” In 2010 43rd Annual IEEE/ACM International\nSymposium on Microarchitecture, 115–24. IEEE; IEEE. https://doi.org/10.1109/micro.2010.46.\n\n\nSettles, Burr. 2009b. “Active Learning Literature Survey.”\nUniversity of Wisconsin-Madison Department of Computer Sciences\n1648: 1–67.\n\n\n———. 2009a. “Active Learning Literature Survey.”\nComputer Sciences Technical Report. http://burrsettles.com/pub/settles.activelearning.pdf.\n\n\nShalev-Shwartz, Shai, Shaked Shammah, and Amnon Shashua. 2017. “On\na Formal Model of Safe and Scalable Self-Driving Cars.” ArXiv\nPreprint abs/1708.06374. https://arxiv.org/abs/1708.06374.\n\n\nShan, Shawn, Wenxin Ding, Josephine Passananti, Haitao Zheng, and Ben Y\nZhao. 2023. “Prompt-Specific Poisoning Attacks on Text-to-Image\nGenerative Models.” ArXiv Preprint abs/2310.13828. https://arxiv.org/abs/2310.13828.\n\n\nShastri, Bhavin J., Alexander N. Tait, T. Ferreira de Lima, Wolfram H.\nP. Pernice, Harish Bhaskaran, C. D. Wright, and Paul R. Prucnal. 2021.\n“Photonics for Artificial Intelligence and Neuromorphic\nComputing.” Nat. Photonics 15 (2): 102–14. https://doi.org/10.1038/s41566-020-00754-y.\n\n\nShazeer, Noam, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc\nLe, Geoffrey Hinton, and Jeff Dean. 2017. “Outrageously Large\nNeural Networks: The Sparsely-Gated Mixture-of-Experts Layer.”\narXiv Preprint arXiv:1701.06538, January. http://arxiv.org/abs/1701.06538v1.\n\n\nSheaffer, Jeremy W, David P Luebke, and Kevin Skadron. 2007. “A\nHardware Redundancy and Recovery Mechanism for Reliable Scientific\nComputation on Graphics Processors.” In Graphics\nHardware, 2007:55–64. Citeseer.\n\n\nShehabi, Arman, Sarah Smith, Dale Sartor, Richard Brown, Magnus Herrlin,\nJonathan Koomey, Eric Masanet, Nathaniel Horner, Inês Azevedo, and\nWilliam Lintner. 2016. “United States Data Center Energy Usage\nReport.”\n\n\nShen, Sheng, Zhen Dong, Jiayu Ye, Linjian Ma, Zhewei Yao, Amir Gholami,\nMichael W. Mahoney, and Kurt Keutzer. 2020. “Q-BERT:\nHessian Based Ultra Low Precision Quantization of\nBERT.” Proceedings of the AAAI Conference on\nArtificial Intelligence 34 (05): 8815–21. https://doi.org/10.1609/aaai.v34i05.6409.\n\n\nSheng, Victor S., and Jing Zhang. 2019. “Machine Learning with\nCrowdsourcing: A Brief Summary of the Past Research and Future\nDirections.” Proceedings of the AAAI Conference on Artificial\nIntelligence 33 (01): 9837–43. https://doi.org/10.1609/aaai.v33i01.33019837.\n\n\nShi, Hongrui, and Valentin Radu. 2022. “Data Selection for\nEfficient Model Update in Federated Learning.” In Proceedings\nof the 2nd European Workshop on Machine Learning and Systems,\n72–78. ACM. https://doi.org/10.1145/3517207.3526980.\n\n\nShneiderman, Ben. 2020. “Bridging the Gap Between Ethics and\nPractice: Guidelines for Reliable, Safe, and Trustworthy Human-Centered\nAI Systems.” ACM Trans. Interact. Intell. Syst. 10 (4):\n1–31. https://doi.org/10.1145/3419764.\n\n\n———. 2022. Human-Centered AI. Oxford University\nPress.\n\n\nShokri, Reza, Marco Stronati, Congzheng Song, and Vitaly Shmatikov.\n2017. “Membership Inference Attacks Against Machine Learning\nModels.” In 2017 IEEE Symposium on Security and Privacy\n(SP), 3–18. IEEE; IEEE. https://doi.org/10.1109/sp.2017.41.\n\n\nSiddik, Md Abu Bakar, Arman Shehabi, and Landon Marston. 2021.\n“The Environmental Footprint of Data Centers in the United\nStates.” Environ. Res. Lett. 16 (6): 064017. https://doi.org/10.1088/1748-9326/abfba1.\n\n\nSilvestro, Daniele, Stefano Goria, Thomas Sterner, and Alexandre\nAntonelli. 2022. “Improving Biodiversity Protection Through\nArtificial Intelligence.” Nature Sustainability 5 (5):\n415–24. https://doi.org/10.1038/s41893-022-00851-6.\n\n\nSingh, Narendra, and Oladele A. Ogunseitan. 2022. “Disentangling\nthe Worldwide Web of e-Waste and Climate Change Co-Benefits.”\nCircular Economy 1 (2): 100011. https://doi.org/10.1016/j.cec.2022.100011.\n\n\nSkorobogatov, Sergei. 2009. “Local Heating Attacks on Flash Memory\nDevices.” In 2009 IEEE International Workshop on\nHardware-Oriented Security and Trust, 1–6. IEEE; IEEE. https://doi.org/10.1109/hst.2009.5225028.\n\n\nSkorobogatov, Sergei P., and Ross J. Anderson. 2002. “Optical\nFault Induction Attacks.” In Cryptographic Hardware and\nEmbedded Systems-CHES 2002: 4th International Workshop Redwood Shores,\nCA, USA, August 13–15, 2002 Revised Papers 4, 2–12. Springer. https://doi.org/10.1007/3-540-36400-5\\_2.\n\n\nSmilkov, Daniel, Nikhil Thorat, Been Kim, Fernanda Viégas, and Martin\nWattenberg. 2017. “Smoothgrad: Removing Noise by\nAdding Noise.” ArXiv Preprint abs/1706.03825. https://arxiv.org/abs/1706.03825.\n\n\nStrassen, Volker. 1969. “Gaussian Elimination Is Not\nOptimal.” Numerische Mathematik 13 (4): 354–56. https://doi.org/10.1007/bf02165411.\n\n\nStrickland, Eliza. 2019. “IBM Watson, Heal Thyself: How IBM\nOverpromised and Underdelivered on AI Health Care.” IEEE\nSpectrum 56 (4): 24–31. https://doi.org/10.1109/mspec.2019.8678513.\n\n\nStrubell, Emma, Ananya Ganesh, and Andrew McCallum. 2019. “Energy\nand Policy Considerations for Deep Learning in NLP.”\nIn Proceedings of the 57th Annual Meeting of the Association for\nComputational Linguistics, 3645–50. Florence, Italy: Association\nfor Computational Linguistics. https://doi.org/10.18653/v1/p19-1355.\n\n\nSuda, Naveen, Vikas Chandra, Ganesh Dasika, Abinash Mohanty, Yufei Ma,\nSarma Vrudhula, Jae-sun Seo, and Yu Cao. 2016.\n“Throughput-Optimized OpenCL-Based FPGA\nAccelerator for Large-Scale Convolutional Neural Networks.” In\nProceedings of the 2016 ACM/SIGDA International Symposium on\nField-Programmable Gate Arrays, 16–25. ACM. https://doi.org/10.1145/2847263.2847276.\n\n\nSudhakar, Soumya, Vivienne Sze, and Sertac Karaman. 2023. “Data\nCenters on Wheels: Emissions from Computing Onboard\nAutonomous Vehicles.” IEEE Micro 43 (1): 29–39. https://doi.org/10.1109/mm.2022.3219803.\n\n\nSze, Vivienne, Yu-Hsin Chen, Tien-Ju Yang, and Joel S. Emer. 2017.\n“Efficient Processing of Deep Neural Networks: A\nTutorial and Survey.” Proc. IEEE 105 (12): 2295–2329. https://doi.org/10.1109/jproc.2017.2761740.\n\n\nSzegedy, Christian, Wojciech Zaremba, Ilya Sutskever, Joan Bruna,\nDumitru Erhan, Ian J. Goodfellow, and Rob Fergus. 2014.\n“Intriguing Properties of Neural Networks.” In 2nd\nInternational Conference on Learning Representations, ICLR 2014, Banff,\nAB, Canada, April 14-16, 2014, Conference Track Proceedings, edited\nby Yoshua Bengio and Yann LeCun. http://arxiv.org/abs/1312.6199.\n\n\nTambe, Thierry, En-Yu Yang, Zishen Wan, Yuntian Deng, Vijay Janapa\nReddi, Alexander Rush, David Brooks, and Gu-Yeon Wei. 2020.\n“Algorithm-Hardware Co-Design of Adaptive Floating-Point Encodings\nfor Resilient Deep Learning Inference.” In 2020 57th ACM/IEEE\nDesign Automation Conference (DAC), 1–6. IEEE; IEEE. https://doi.org/10.1109/dac18072.2020.9218516.\n\n\nTan, Mingxing, Bo Chen, Ruoming Pang, Vijay Vasudevan, Mark Sandler,\nAndrew Howard, and Quoc V. Le. 2019. “MnasNet: Platform-aware Neural Architecture Search for\nMobile.” In 2019 IEEE/CVF Conference on Computer Vision and\nPattern Recognition (CVPR), 2820–28. IEEE. https://doi.org/10.1109/cvpr.2019.00293.\n\n\nTan, Mingxing, and Quoc V. Le. 2019. “EfficientNet: Rethinking\nModel Scaling for Convolutional Neural Networks.” In\nProceedings of the International Conference on Machine Learning\n(ICML), 6105–14.\n\n\n———. 2023. “Demystifying Deep Learning.” Wiley. https://doi.org/10.1002/9781394205639.ch6.\n\n\nTang, Xin, Yichun He, and Jia Liu. 2022. “Soft Bioelectronics for\nCardiac Interfaces.” Biophysics Reviews 3 (1). https://doi.org/10.1063/5.0069516.\n\n\nTang, Xin, Hao Shen, Siyuan Zhao, Na Li, and Jia Liu. 2023.\n“Flexible Braincomputer Interfaces.”\nNature Electronics 6 (2): 109–18. https://doi.org/10.1038/s41928-022-00913-9.\n\n\nTarun, Ayush K, Vikram S Chundawat, Murari Mandal, and Mohan\nKankanhalli. 2022. “Deep Regression Unlearning.” ArXiv\nPreprint abs/2210.08196 (October). http://arxiv.org/abs/2210.08196v2.\n\n\nTeam, The Theano Development, Rami Al-Rfou, Guillaume Alain, Amjad\nAlmahairi, Christof Angermueller, Dzmitry Bahdanau, Nicolas Ballas, et\nal. 2016. “Theano: A Python Framework for Fast Computation of\nMathematical Expressions,” May. http://arxiv.org/abs/1605.02688v1.\n\n\nThe Sustainable Development Goals Report 2018. 2018. New York:\nUnited Nations. https://doi.org/10.18356/7d014b41-en.\n\n\n“The Ultimate Guide to Deep Learning Model Quantization and\nQuantization-Aware Training.” n.d. https://deci.ai/quantization-and-quantization-aware-training/.\n\n\nThompson, Neil C., Kristjan Greenewald, Keeheon Lee, and Gabriel F.\nManso. 2021. “Deep Learning’s Diminishing Returns:\nThe Cost of Improvement Is Becoming Unsustainable.”\nIEEE Spectr. 58 (10): 50–55. https://doi.org/10.1109/mspec.2021.9563954.\n\n\nThornton, James E. 1965. “Design of a Computer: The Control Data\n6600.” Communications of the ACM 8 (6): 330–35.\n\n\nTill, Aaron, Andrew L. Rypel, Andrew Bray, and Samuel B. Fey. 2019.\n“Fish Die-Offs Are Concurrent with Thermal Extremes in North\nTemperate Lakes.” Nat. Clim. Change 9 (8): 637–41. https://doi.org/10.1038/s41558-019-0520-y.\n\n\nTirtalistyani, Rose, Murtiningrum Murtiningrum, and Rameshwar S. Kanwar.\n2022. “Indonesia Rice Irrigation System: Time for\nInnovation.” Sustainability 14 (19): 12477. https://doi.org/10.3390/su141912477.\n\n\nTramèr, Florian, Pascal Dupré, Gili Rusak, Giancarlo Pellegrino, and Dan\nBoneh. 2019. “AdVersarial: Perceptual Ad Blocking\nMeets Adversarial Machine Learning.” In Proceedings of the\n2019 ACM SIGSAC Conference on Computer and Communications Security,\n2005–21. ACM. https://doi.org/10.1145/3319535.3354222.\n\n\nTran, Cuong, Ferdinando Fioretto, Jung-Eun Kim, and Rakshit Naidu. 2022.\n“Pruning Has a Disparate Impact on Model Accuracy.” Adv\nNeural Inf Process Syst 35: 17652–64.\n\n\nTsai, Min-Jen, Ping-Yi Lin, and Ming-En Lee. 2023. “Adversarial\nAttacks on Medical Image Classification.” Cancers 15\n(17): 4228. https://doi.org/10.3390/cancers15174228.\n\n\nTsai, Timothy, Siva Kumar Sastry Hari, Michael Sullivan, Oreste Villa,\nand Stephen W. Keckler. 2021. “NVBitFI:\nDynamic Fault Injection for GPUs.” In\n2021 51st Annual IEEE/IFIP International Conference on Dependable\nSystems and Networks (DSN), 284–91. IEEE; IEEE. https://doi.org/10.1109/dsn48987.2021.00041.\n\n\nTschand, Arya, Arun Tejusve Raghunath Rajan, Sachin Idgunji, Anirban\nGhosh, Jeremy Holleman, Csaba Kiraly, Pawan Ambalkar, et al. 2024.\n“MLPerf Power: Benchmarking the Energy Efficiency of Machine\nLearning Systems from Microwatts to Megawatts for Sustainable\nAI.” arXiv Preprint arXiv:2410.12032, October. http://arxiv.org/abs/2410.12032v1.\n\n\nUddin, Mueen, and Azizah Abdul Rahman. 2012. “Energy Efficiency\nand Low Carbon Enabler Green IT Framework for Data Centers\nConsidering Green Metrics.” Renewable Sustainable Energy\nRev. 16 (6): 4078–94. https://doi.org/10.1016/j.rser.2012.03.014.\n\n\nUn, and World Economic Forum. 2019. A New Circular Vision for\nElectronics, Time for a Global Reboot. PACE - Platform for\nAccelerating the Circular Economy. https://www3.weforum.org/docs/WEF\\_A\\_New\\_Circular\\_Vision\\_for\\_Electronics.pdf.\n\n\nValenzuela, Christine L, and Pearl Y Wang. 2000. “A Genetic\nAlgorithm for VLSI Floorplanning.” In Parallel\nProblem Solving from Nature PPSN VI: 6th International Conference Paris,\nFrance, September 1820, 2000 Proceedings 6, 671–80.\nSpringer.\n\n\nVan Noorden, Richard. 2016. “ArXiv Preprint Server\nPlans Multimillion-Dollar Overhaul.” Nature 534 (7609):\n602–2. https://doi.org/10.1038/534602a.\n\n\nVangal, Sriram, Somnath Paul, Steven Hsu, Amit Agarwal, Saurabh Kumar,\nRam Krishnamurthy, Harish Krishnamurthy, James Tschanz, Vivek De, and\nChris H. Kim. 2021. “Wide-Range Many-Core SoC Design\nin Scaled CMOS: Challenges and\nOpportunities.” IEEE Trans. Very Large Scale Integr. VLSI\nSyst. 29 (5): 843–56. https://doi.org/10.1109/tvlsi.2021.3061649.\n\n\nVelazco, Raoul, Gilles Foucard, and Paul Peronnard. 2010.\n“Combining Results of Accelerated Radiation Tests and Fault\nInjections to Predict the Error Rate of an Application Implemented in\nSRAM-Based FPGAs.” IEEE Trans.\nNucl. Sci. 57 (6): 3500–3505. https://doi.org/10.1109/tns.2010.2087355.\n\n\nVerma, Naveen, Hongyang Jia, Hossein Valavi, Yinqi Tang, Murat Ozatay,\nLung-Yen Chen, Bonan Zhang, and Peter Deaville. 2019. “In-Memory\nComputing: Advances and Prospects.” IEEE\nSolid-State Circuits Mag. 11 (3): 43–55. https://doi.org/10.1109/mssc.2019.2922889.\n\n\nVerma, Team Dual_Boot: Swapnil. 2022. “Elephant AI.”\nHackster.io. https://www.hackster.io/dual\\_boot/elephant-ai-ba71e9.\n\n\nVivet, Pascal, Eric Guthmuller, Yvain Thonnart, Gael Pillonnet, Cesar\nFuguet, Ivan Miro-Panades, Guillaume Moritz, et al. 2021.\n“IntAct: A 96-Core Processor with Six\nChiplets 3D-Stacked on an Active Interposer with\nDistributed Interconnects and Integrated Power Management.”\nIEEE J. Solid-State Circuits 56 (1): 79–97. https://doi.org/10.1109/jssc.2020.3036341.\n\n\nWachter, Sandra, Brent Mittelstadt, and Chris Russell. 2017.\n“Counterfactual Explanations Without Opening the Black Box:\nAutomated Decisions and the GDPR.”\nSSRN Electronic Journal 31: 841. https://doi.org/10.2139/ssrn.3063289.\n\n\nWald, Peter H., and Jeffrey R. Jones. 1987. “Semiconductor\nManufacturing: An Introduction to Processes and\nHazards.” Am. J. Ind. Med. 11 (2): 203–21. https://doi.org/10.1002/ajim.4700110209.\n\n\nWan, Zishen, Aqeel Anwar, Yu-Shun Hsiao, Tianyu Jia, Vijay Janapa Reddi,\nand Arijit Raychowdhury. 2021. “Analyzing and Improving Fault\nTolerance of Learning-Based Navigation Systems.” In 2021 58th\nACM/IEEE Design Automation Conference (DAC), 841–46. IEEE; IEEE. https://doi.org/10.1109/dac18074.2021.9586116.\n\n\nWan, Zishen, Yiming Gan, Bo Yu, S Liu, A Raychowdhury, and Y Zhu. 2023.\n“Vpp: The Vulnerability-Proportional Protection\nParadigm Towards Reliable Autonomous Machines.” In\nProceedings of the 5th International Workshop on Domain Specific\nSystem Architecture (DOSSA), 1–6.\n\n\nWang, LingFeng, and YaQing Zhan. 2019. “A Conceptual Peer Review\nModel for arXiv and Other Preprint\nDatabases.” Learn. Publ. 32 (3): 213–19. https://doi.org/10.1002/leap.1229.\n\n\nWang, Tianlu, Jieyu Zhao, Mark Yatskar, Kai-Wei Chang, and Vicente\nOrdonez. 2019. “Balanced Datasets Are Not Enough: Estimating and\nMitigating Gender Bias in Deep Image Representations.” In\n2019 IEEE/CVF International Conference on Computer Vision\n(ICCV), 5309–18. IEEE. https://doi.org/10.1109/iccv.2019.00541.\n\n\nWang, Tianzhe, Kuan Wang, Han Cai, Ji Lin, Zhijian Liu, Hanrui Wang,\nYujun Lin, and Song Han. 2020. “APQ:\nJoint Search for Network Architecture, Pruning and\nQuantization Policy.” In 2020 IEEE/CVF Conference on Computer\nVision and Pattern Recognition (CVPR), 2075–84. IEEE. https://doi.org/10.1109/cvpr42600.2020.00215.\n\n\nWang, Y., and P. Kanwar. 2019. “BFloat16: The Secret to High\nPerformance on Cloud TPUs.” Google Cloud Blog.\n\n\nWarden, Pete. 2018. “Speech Commands: A Dataset for\nLimited-Vocabulary Speech Recognition.” arXiv Preprint\narXiv:1804.03209.\n\n\nWeik, Martin H. 1955. A Survey of Domestic Electronic Digital\nComputing Systems. Ballistic Research Laboratories.\n\n\nWerchniak, Andrew, Roberto Barra Chicote, Yuriy Mishchenko, Jasha\nDroppo, Jeff Condal, Peng Liu, and Anish Shah. 2021. “Exploring\nthe Application of Synthetic Audio in Training Keyword Spotters.”\nIn ICASSP 2021 - 2021 IEEE International Conference on Acoustics,\nSpeech and Signal Processing (ICASSP), 7993–96. IEEE; IEEE. https://doi.org/10.1109/icassp39728.2021.9413448.\n\n\nWess, Matthias, Matvey Ivanov, Christoph Unger, and Anvesh Nookala.\n2020. “ANNETTE: Accurate Neural Network\nExecution Time Estimation with Stacked Models.” IEEE. https://doi.org/10.1109/ACCESS.2020.3047259.\n\n\nWiener, Norbert. 1960. “Some Moral and Technical Consequences of\nAutomation: As Machines Learn They May Develop Unforeseen Strategies at\nRates That Baffle Their Programmers.” Science 131\n(3410): 1355–58. https://doi.org/10.1126/science.131.3410.1355.\n\n\nWilkening, Mark, Vilas Sridharan, Si Li, Fritz Previlon, Sudhanva\nGurumurthi, and David R. Kaeli. 2014. “Calculating Architectural\nVulnerability Factors for Spatial Multi-Bit Transient Faults.” In\n2014 47th Annual IEEE/ACM International Symposium on\nMicroarchitecture, 293–305. IEEE; IEEE. https://doi.org/10.1109/micro.2014.15.\n\n\nWinkler, Harald, Franck Lecocq, Hans Lofgren, Maria Virginia Vilariño,\nSivan Kartha, and Joana Portugal-Pereira. 2022. “Examples of\nShifting Development Pathways: Lessons on How to Enable\nBroader, Deeper, and Faster Climate Action.” Climate\nAction 1 (1). https://doi.org/10.1007/s44168-022-00026-1.\n\n\nWitten, Ian H., and Eibe Frank. 2002. “Data Mining: Practical\nMachine Learning Tools and Techniques with Java Implementations.”\nACM SIGMOD Record 31 (1): 76–77. https://doi.org/10.1145/507338.507355.\n\n\nWolpert, David H, and William G Macready. 1997. “No Free Lunch\nTheorems for Optimization.” IEEE Transactions on Evolutionary\nComputation 1 (1): 67–82.\n\n\nWong, H.-S. Philip, Heng-Yuan Lee, Shimeng Yu, Yu-Sheng Chen, Yi Wu,\nPang-Shiu Chen, Byoungil Lee, Frederick T. Chen, and Ming-Jinn Tsai.\n2012. “MetalOxide\nRRAM.” Proc. IEEE 100 (6): 1951–70. https://doi.org/10.1109/jproc.2012.2190369.\n\n\nWu, Bichen, Kurt Keutzer, Xiaoliang Dai, Peizhao Zhang, Yanghan Wang,\nFei Sun, Yiming Wu, Yuandong Tian, Peter Vajda, and Yangqing Jia. 2019.\n“FBNet: Hardware-aware\nEfficient ConvNet Design via Differentiable Neural\nArchitecture Search.” In 2019 IEEE/CVF Conference on Computer\nVision and Pattern Recognition (CVPR), 10734–42. IEEE. https://doi.org/10.1109/cvpr.2019.01099.\n\n\nWu, Carole-Jean, David Brooks, Kevin Chen, Douglas Chen, Sy Choudhury,\nMarat Dukhan, Kim Hazelwood, et al. 2019. “Machine Learning at\nFacebook: Understanding Inference at the Edge.” In 2019 IEEE\nInternational Symposium on High Performance Computer Architecture\n(HPCA), 331–44. IEEE; IEEE. https://doi.org/10.1109/hpca.2019.00048.\n\n\nWu, Carole-Jean, Ramya Raghavendra, Udit Gupta, Bilge Acun, Newsha\nArdalani, Kiwan Maeng, Gloria Chang, et al. 2022. “Sustainable Ai:\nEnvironmental Implications, Challenges and\nOpportunities.” Proceedings of Machine Learning and\nSystems 4: 795–813.\n\n\nWu, Zhang Judd, and Micikevicius Isaev. 2020. “Integer\nQuantization for Deep Learning Inference: Principles and\nEmpirical Evaluation).” ArXiv Preprint. https://arxiv.org/abs/2004.09602.\n\n\nXiao, Seznec Lin, Demouth Wu, and Han. 2022.\n“SmoothQuant: Accurate and Efficient\nPost-Training Quantization for Large Language Models.” ArXiv\nPreprint. https://arxiv.org/abs/2211.10438.\n\n\nXie, Cihang, Mingxing Tan, Boqing Gong, Jiang Wang, Alan L. Yuille, and\nQuoc V. Le. 2020. “Adversarial Examples Improve Image\nRecognition.” In 2020 IEEE/CVF Conference on Computer Vision\nand Pattern Recognition (CVPR), 816–25. IEEE. https://doi.org/10.1109/cvpr42600.2020.00090.\n\n\nXinyu, Chen. n.d.\n\n\nXiong, Siyu, Guoqing Wu, Xitian Fan, Xuan Feng, Zhongcheng Huang, Wei\nCao, Xuegong Zhou, et al. 2021. “MRI-Based Brain\nTumor Segmentation Using FPGA-Accelerated Neural\nNetwork.” BMC Bioinf. 22 (1): 421. https://doi.org/10.1186/s12859-021-04347-6.\n\n\nXiu, Liming. 2019. “Time Moore: Exploiting Moore’s Law from the Perspective of Time.”\nIEEE Solid-State Circuits Mag. 11 (1): 39–55. https://doi.org/10.1109/mssc.2018.2882285.\n\n\nXu, Chen, Jianqiang Yao, Zhouchen Lin, Wenwu Ou, Yuanbin Cao, Zhirong\nWang, and Hongbin Zha. 2018. “Alternating Multi-Bit Quantization\nfor Recurrent Neural Networks.” In 6th International\nConference on Learning Representations, ICLR 2018, Vancouver, BC,\nCanada, April 30 - May 3, 2018, Conference Track Proceedings.\nOpenReview.net. https://openreview.net/forum?id=S19dR9x0b.\n\n\nXu, Hu, Saining Xie, Xiaoqing Ellen Tan, Po-Yao Huang, Russell Howes,\nVasu Sharma, Shang-Wen Li, Gargi Ghosh, Luke Zettlemoyer, and Christoph\nFeichtenhofer. 2023. “Demystifying CLIP Data.” ArXiv\nPreprint abs/2309.16671 (September). http://arxiv.org/abs/2309.16671v4.\n\n\nXu, Zheng, Yanxiang Zhang, Galen Andrew, Christopher A. Choquette-Choo,\nPeter Kairouz, H. Brendan McMahan, Jesse Rosenstock, and Yuanbo Zhang.\n2023. “Federated Learning of Gboard Language Models with\nDifferential Privacy.” ArXiv Preprint abs/2305.18465\n(May). http://arxiv.org/abs/2305.18465v2.\n\n\nYang, Tien-Ju, Yonghui Xiao, Giovanni Motta, Françoise Beaufays, Rajiv\nMathews, and Mingqing Chen. 2023. “Online Model Compression for\nFederated Learning with Large Models.” In ICASSP 2023 - 2023\nIEEE International Conference on Acoustics, Speech and Signal Processing\n(ICASSP), 1–5. IEEE; IEEE. https://doi.org/10.1109/icassp49357.2023.10097124.\n\n\nYao, Zhewei, Zhen Dong, Zhangcheng Zheng, Amir Gholami, Jiali Yu, Eric\nTan, Leyuan Wang, et al. 2021. “Hawq-V3: Dyadic\nNeural Network Quantization.” In International Conference on\nMachine Learning, 11875–86. PMLR.\n\n\nYeh, Y. C. 1996. “Triple-Triple Redundant 777 Primary Flight\nComputer.” In 1996 IEEE Aerospace Applications Conference.\nProceedings, 1:293–307. IEEE; IEEE. https://doi.org/10.1109/aero.1996.495891.\n\n\nYik, Jason, Korneel Van den Berghe, Douwe den Blanken, Younes Bouhadjar,\nMaxime Fabre, Paul Hueber, Denis Kleyko, et al. 2023. “NeuroBench:\nA Framework for Benchmarking Neuromorphic Computing Algorithms and\nSystems,” April. http://arxiv.org/abs/2304.04640v3.\n\n\nYosinski, Jason, Jeff Clune, Yoshua Bengio, and Hod Lipson. 2014.\n“How Transferable Are Features in Deep Neural Networks?”\n27: 3320–28. https://proceedings.neurips.cc/paper/2014/file/375c71349b295fbe2dcdca9206f20a06-Paper.pdf.\n\n\nYou, Jie, Jae-Won Chung, and Mosharaf Chowdhury. 2023. “Zeus:\nUnderstanding and Optimizing GPU Energy\nConsumption of DNN Training.” In 20th USENIX\nSymposium on Networked Systems Design and Implementation (NSDI 23),\n119–39. Boston, MA: USENIX Association. https://www.usenix.org/conference/nsdi23/presentation/you.\n\n\nYoung, Tom, Devamanyu Hazarika, Soujanya Poria, and Erik Cambria. 2018.\n“Recent Trends in Deep Learning Based Natural Language Processing\n[Review Article].” IEEE Comput. Intell.\nMag. 13 (3): 55–75. https://doi.org/10.1109/mci.2018.2840738.\n\n\nZafrir, Ofir, Guy Boudoukh, Peter Izsak, and Moshe Wasserblat. 2019.\n“Q8BERT: Quantized 8Bit\nBERT.” In 2019 Fifth Workshop on Energy\nEfficient Machine Learning and Cognitive Computing - NeurIPS Edition\n(EMC2-NIPS), 36–39. IEEE; IEEE. https://doi.org/10.1109/emc2-nips53020.2019.00016.\n\n\nZeghidour, Neil, Olivier Teboul, Félix de Chaumont Quitry, and Marco\nTagliasacchi. 2021. “LEAF: A Learnable Frontend for Audio\nClassification.” arXiv Preprint arXiv:2101.08596,\nJanuary. http://arxiv.org/abs/2101.08596v1.\n\n\nZhang, Chengliang, Minchen Yu, Wei Wang 0030, and Feng Yan 0001. 2019.\n“MArk: Exploiting Cloud Services for Cost-Effective, SLO-Aware\nMachine Learning Inference Serving.” In 2019 USENIX Annual\nTechnical Conference (USENIX ATC 19), 1049–62. https://www.usenix.org/conference/atc19/presentation/zhang-chengliang.\n\n\nZhang, Chen, Peng Li, Guangyu Sun, Yijin Guan, Bingjun Xiao, and Jason\nOptimizing Cong. 2015. “FPGA-Based Accelerator Design\nfor Deep Convolutional Neural Networks Proceedings of the 2015\nACM.” In SIGDA International Symposium on\nField-Programmable Gate Arrays-FPGA, 15:161–70.\n\n\nZhang, Dan, Safeen Huda, Ebrahim Songhori, Kartik Prabhu, Quoc Le, Anna\nGoldie, and Azalia Mirhoseini. 2022. “A Full-Stack Search\nTechnique for Domain Optimized Deep Learning Accelerators.” In\nProceedings of the 27th ACM International Conference on\nArchitectural Support for Programming Languages and Operating\nSystems, 27–42. ASPLOS ’22. New York, NY, USA: ACM. https://doi.org/10.1145/3503222.3507767.\n\n\nZhang, Dongxia, Xiaoqing Han, and Chunyu Deng. 2018. “Review on\nthe Research and Practice of Deep Learning and Reinforcement Learning in\nSmart Grids.” CSEE Journal of Power and Energy Systems 4\n(3): 362–70. https://doi.org/10.17775/cseejpes.2018.00520.\n\n\nZhang, Hongyu. 2008. “On the Distribution of Software\nFaults.” IEEE Trans. Software Eng. 34 (2): 301–2. https://doi.org/10.1109/tse.2007.70771.\n\n\nZhang, Jeff Jun, Tianyu Gu, Kanad Basu, and Siddharth Garg. 2018.\n“Analyzing and Mitigating the Impact of Permanent Faults on a\nSystolic Array Based Neural Network Accelerator.” In 2018\nIEEE 36th VLSI Test Symposium (VTS), 1–6. IEEE; IEEE. https://doi.org/10.1109/vts.2018.8368656.\n\n\nZhang, Jeff, Kartheek Rangineni, Zahra Ghodsi, and Siddharth Garg. 2018.\n“ThUnderVolt: Enabling Aggressive\nVoltage Underscaling and Timing Error Resilience for Energy Efficient\nDeep Learning Accelerators.” In 2018 55th ACM/ESDA/IEEE\nDesign Automation Conference (DAC), 1–6. IEEE. https://doi.org/10.1109/dac.2018.8465918.\n\n\nZhang, Li Lyna, Yuqing Yang, Yuhang Jiang, Wenwu Zhu, and Yunxin Liu.\n2020. “Fast Hardware-Aware Neural Architecture Search.” In\n2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition\nWorkshops (CVPRW). IEEE. https://doi.org/10.1109/cvprw50498.2020.00354.\n\n\nZhang, Qingxue, Dian Zhou, and Xuan Zeng. 2017. “Highly Wearable\nCuff-Less Blood Pressure and Heart Rate Monitoring with Single-Arm\nElectrocardiogram and Photoplethysmogram Signals.” BioMedical\nEngineering OnLine 16 (1): 23. https://doi.org/10.1186/s12938-017-0317-z.\n\n\nZhang, Tunhou, Hsin-Pai Cheng, Zhenwen Li, Feng Yan, Chengyu Huang, Hai\nHelen Li, and Yiran Chen. 2020. “AutoShrink:\nA Topology-Aware NAS for Discovering Efficient\nNeural Architecture.” In The Thirty-Fourth AAAI Conference on\nArtificial Intelligence, AAAI 2020, the Thirty-Second Innovative\nApplications of Artificial Intelligence Conference, IAAI 2020, the Tenth\nAAAI Symposium on Educational Advances in Artificial Intelligence, EAAI\n2020, New York, NY, USA, February 7-12, 2020, 6829–36. AAAI Press.\nhttps://aaai.org/ojs/index.php/AAAI/article/view/6163.\n\n\nZhao, Mark, and G. Edward Suh. 2018. “FPGA-Based Remote Power\nSide-Channel Attacks.” In 2018 IEEE Symposium on Security and\nPrivacy (SP), 229–44. IEEE; IEEE. https://doi.org/10.1109/sp.2018.00049.\n\n\nZhao, Yue, Meng Li, Liangzhen Lai, Naveen Suda, Damon Civin, and Vikas\nChandra. 2018. “Federated Learning with Non-IID Data.”\nArXiv Preprint abs/1806.00582 (June). http://arxiv.org/abs/1806.00582v2.\n\n\nZhou, Bolei, Yiyou Sun, David Bau, and Antonio Torralba. 2018.\n“Interpretable Basis Decomposition for Visual Explanation.”\nIn Proceedings of the European Conference on Computer Vision\n(ECCV), 119–34.\n\n\nZhou, Chuteng, Fernando Garcia Redondo, Julian Büchel, Irem Boybat,\nXavier Timoneda Comas, S. R. Nandakumar, Shidhartha Das, Abu Sebastian,\nManuel Le Gallo, and Paul N. Whatmough. 2021.\n“AnalogNets: Ml-hw\nCo-Design of Noise-Robust TinyML Models and Always-on\nAnalog Compute-in-Memory Accelerator.” https://arxiv.org/abs/2111.06503.\n\n\nZhou, Peng, Xintong Han, Vlad I. Morariu, and Larry S. Davis. 2018.\n“Learning Rich Features for Image Manipulation Detection.”\nIn 2018 IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, 1053–61. IEEE. https://doi.org/10.1109/cvpr.2018.00116.\n\n\nZhu, Hongyu, Mohamed Akrout, Bojian Zheng, Andrew Pelegris, Anand\nJayarajan, Amar Phanishayee, Bianca Schroeder, and Gennady Pekhimenko.\n2018. “Benchmarking and Analyzing Deep Neural Network\nTraining.” In 2018 IEEE International Symposium on Workload\nCharacterization (IISWC), 88–100. IEEE; IEEE. https://doi.org/10.1109/iiswc.2018.8573476.\n\n\nZhu, Ligeng, Lanxiang Hu, Ji Lin, Wei-Ming Chen, Wei-Chen Wang, Chuang\nGan, and Song Han. 2023. “PockEngine:\nSparse and Efficient Fine-Tuning in a Pocket.” In\n56th Annual IEEE/ACM International Symposium on\nMicroarchitecture. ACM. https://doi.org/10.1145/3613424.3614307.\n\n\nZhuang, Fuzhen, Zhiyuan Qi, Keyu Duan, Dongbo Xi, Yongchun Zhu, Hengshu\nZhu, Hui Xiong, and Qing He. 2021. “A Comprehensive Survey on\nTransfer Learning.” Proceedings of the IEEE 109 (1):\n43–76. https://doi.org/10.1109/jproc.2020.3004555.",
    "crumbs": [
      "References"
    ]
  }
]