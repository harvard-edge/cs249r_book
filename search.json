[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "MACHINE LEARNING SYSTEMS",
    "section": "",
    "text": "Preface\nWelcome to ‚ÄúMachine Learning Systems for TinyML‚Äù This book is your gateway to the fast-paced world of artificial intelligence within embedded systems. It as an extension of the foundational course, tinyML from CS249r at Harvard University.\nOur aim? To make this book a collaborative effort that brings together insights from students, professionals, and the broader community. We want to create a one-stop guide that dives deep into the nuts and bolts of embedded AI and its many uses.\n\n‚ÄúIf you want to go fast, go alone. If you want to go far, go together.‚Äù ‚Äì African Proverb\n\nThis isn‚Äôt just a static textbook; it‚Äôs a living, breathing document. We‚Äôre making it open-source and continually updated to meet the ever-changing needs of this dynamic field. Expect a rich blend of expert knowledge that guides you through the complex interplay between cutting-edge algorithms and the foundational principles that make them work. We‚Äôre setting the stage for the next big leap in tech innovation.\n\n\nWhy We Wrote This Book\nWe‚Äôre in an age where technology is always evolving. Open collaboration and sharing knowledge are the building blocks of true innovation. That‚Äôs the spirit behind ‚ÄúMachine Learning Systems for TinyML.‚Äù We‚Äôre going beyond the traditional textbook model to create a living knowledge hub.\nThe book covers principles, algorithms, and real-world application case studies, aiming to give you a deep understanding that will help you navigate the ever-changing landscape of embedded AI. By keeping it open, we‚Äôre not just making learning accessible; we‚Äôre inviting new ideas and ongoing improvements. In short, we‚Äôre building a community where knowledge is free to grow and light the way forward in global embedded AI tech.\n\n\nWhat You‚Äôll Need to Know\nDon‚Äôt worry, you don‚Äôt need to be a machine learning whiz to dive into this book. All you really need is a basic understanding of systems and a curiosity to explore how embedded hardware, AI, and software come together. This is where innovation happens, and a basic grasp of how systems work will be your compass.\nWe‚Äôre also focusing on the exciting overlaps between these fields, aiming to create a learning environment where traditional boundaries fade away, making room for a more holistic, integrated view of modern tech. Your interest in embedded AI and low-level software will guide you through a rich and rewarding learning experience.\n\n\nBook Conventions\nFor details on the conventions used in this book, check out the Conventions section.\n\n\nWant to Help Out?\nIf you‚Äôre interested in contributing, you can find the guidelines here.\n\n\nGet in Touch\nGot questions or feedback? Feel free to e-mail us.\n\n\nContributors\nA big thanks to everyone who‚Äôs helped make this book what it is! You can see the full list of contributors here."
  },
  {
    "objectID": "dedication.html",
    "href": "dedication.html",
    "title": "Dedication",
    "section": "",
    "text": "This book is a testament to the idea that, in the vast expanse of technology and innovation, it‚Äôs not always the largest systems, but the smallest ones, that can change the world."
  },
  {
    "objectID": "acknowledgements.html",
    "href": "acknowledgements.html",
    "title": "Acknowledgements",
    "section": "",
    "text": "Assembling this book has been an incredible journey, spanning several years of hard work. The initial idea for this book sprang from the tinyML edX course, and its realization would not have been possible without the invaluable contributions of countless individuals. We are deeply indebted to the researchers whose groundbreaking work laid the foundation for this book.\nWe extend our heartfelt gratitude to the GitHub community. Whether you contributed an entire section, a single sentence, or merely corrected a typo, your efforts have significantly enhanced this book. We deeply appreciate everyone‚Äôs time, expertise, and commitment. This book is as much yours as it is ours.\nSpecial thanks go to Professor Vijay Janapa Reddi, whose belief in the transformative power of open-source communities and invaluable guidance have been our guiding light from the outset.\nWe also owe a great deal to the team at GitHub. You‚Äôve revolutionized the way people collaborate, and this book stands as a testament to what can be achieved when barriers to global cooperation are removed.\nTo all who pick up this book‚Äîthank you! We wrote it with you in mind, hoping to provoke thought, inspire questions, and perhaps even ignite a spark of inspiration. After all, what is the point of writing if no one is reading?\nLast but certainly not least, our deepest thanks go to our friends, families, mentors, and all the kind souls who have supported us emotionally and intellectually as this book came to fruition."
  },
  {
    "objectID": "contributors.html",
    "href": "contributors.html",
    "title": "Contributors",
    "section": "",
    "text": "We extend our sincere thanks to the diverse group of individuals who have generously contributed their expertise, insights, and time to enhance both the content and codebase of this project. Below you will find a list of all contributors. If you would like to contribute to this project, please see our GitHub page.\n\n\n\n\n\n\n\n\nnaeemkhüìñ\n\n\nsophiacho1üìñ\n\n\nShvetank Prakashüìñ\n\n\nJayson Linüìñ\n\n\nHenry Baeüìñ\n\n\n\n\nishapiraüìñ\n\n\nsjohri20üìñ\n\n\nMarco Zennaroüìñ\n\n\noishibüìñ\n\n\nMatthew Stewartüìñ\n\n\n\n\nJessica Quayeüìñ\n\n\nColby Banburyüìñ\n\n\nIkechukwu Uchenduüìñ\n\n\naptl26üìñ\n\n\nMark Mazumderüìñ\n\n\n\n\nJeffrey Maüìñ\n\n\nMarcelo Rovaiüìñ\n\n\nDivyaüìñ\n\n\nVijay Janapa Reddiüìñ"
  },
  {
    "objectID": "copyright.html",
    "href": "copyright.html",
    "title": "Copyright",
    "section": "",
    "text": "This book is open-source and developed collaboratively through GitHub. Unless otherwise stated, this work is licensed under the Creative Commons Attribution-ShareAlike 4.0 International License (CC BY-SA 4.0). You can find the full text of the license here.\nContributors to this project have dedicated their contributions to the public domain or under the same open license as the original project. While the contributions are collaborative, each contributor retains copyright in their respective contributions.\nFor details on authorship, contributions, and how to contribute, please see the project repository on GitHub.\nAll trademarks and registered trademarks mentioned in this book are the property of their respective owners.\nThe information provided in this book is believed to be accurate and reliable. However, the authors, editors, and publishers cannot be held liable for any damages caused or alleged to be caused either directly or indirectly by the information contained in this book."
  },
  {
    "objectID": "about.html#overview",
    "href": "about.html#overview",
    "title": "About the Book",
    "section": "Overview",
    "text": "Overview\nWelcome to this collaborative project initiated by the CS249r Tiny Machine Learning class at Harvard University. Our goal is to make this book a community resource that assists educators and learners in understanding TinyML. The book will be regularly updated to reflect new insights into TinyML and effective teaching methods."
  },
  {
    "objectID": "about.html#topics-explored",
    "href": "about.html#topics-explored",
    "title": "About the Book",
    "section": "Topics Explored",
    "text": "Topics Explored\nThis book offers a comprehensive look at various aspects of embedded machine learning. The topics we delve into include:\n\nIntroduction and Overview of Embedded Machine Learning\nData Engineering Techniques\nFrameworks for Embedded Machine Learning\nEfficient Representation and Compression of Models\nPerformance Metrics and Benchmarking for Machine Learning Systems\nEdge Learning\nHardware Acceleration Options: GPUs, TPUs, and FPGAs\nOperational Aspects of Embedded Machine Learning\nSecurity and Privacy in On-Device Machine Learning\nEthical Considerations in AI\nSustainability Concerns in Edge Computing\nGenerative AI in Edge Computing\n\nBy the time you finish this book, you‚Äôll have a foundational understanding of machine learning and the Internet of Things. You‚Äôll also learn about real-world applications of embedded machine learning systems and gain practical experience through project-based assignments."
  },
  {
    "objectID": "about.html#who-should-read-this",
    "href": "about.html#who-should-read-this",
    "title": "About the Book",
    "section": "Who Should Read This",
    "text": "Who Should Read This\nThis book is tailored for those new to the exciting field of tiny machine learning (TinyML). It starts with the basics of machine learning and embedded systems and progresses to more advanced topics relevant to the TinyML community and broader research areas. The book is particularly beneficial for:\n\nEmbedded Systems Engineers: For engineers in the embedded systems domain, this book serves as an excellent guide to TinyML, helping them create intelligent applications on resource-limited platforms.\nStudents in Computer Science and Electrical Engineering: This book is a useful resource for students studying computer science and electrical engineering. It introduces them to the methods, algorithms, and techniques used in TinyML, preparing them for real-world challenges in embedded machine learning.\nResearchers and Academics: Those involved in machine learning, computer vision, and signal processing research will find this book insightful. It sheds light on the unique challenges of running machine learning algorithms on low-power, low-memory devices.\nIndustry Professionals: If you‚Äôre working in areas like IoT, robotics, wearable tech, or smart devices, this book will equip you with the knowledge you need to add machine learning features to your products."
  },
  {
    "objectID": "about.html#key-learning-outcomes",
    "href": "about.html#key-learning-outcomes",
    "title": "About the Book",
    "section": "Key Learning Outcomes",
    "text": "Key Learning Outcomes\nReaders will acquire skills in training and deploying deep neural network models on resource-limited microcontrollers, along with understanding the broader challenges involved in their design, development, and deployment. Specifically, you‚Äôll learn about:\n\nFoundational Concepts in Machine Learning\nFundamentals of Embedded AI\nHardware Platforms Suitable for Embedded AI\nTechniques for Training Models for Embedded Systems\nStrategies for Model Optimization\nReal-world Applications of Embedded AI\nCurrent Challenges and Future Trends in Embedded AI\n\nOur aim is to make this book a comprehensive resource for anyone interested in developing intelligent applications on embedded systems. Upon completing the book, you‚Äôll be well-equipped to design and implement your own machine learning-enabled projects."
  },
  {
    "objectID": "about.html#prerequisites-for-readers",
    "href": "about.html#prerequisites-for-readers",
    "title": "About the Book",
    "section": "Prerequisites for Readers",
    "text": "Prerequisites for Readers\n\nBasic Programming Skills: We recommend that you have some prior programming experience, ideally in Python. A grasp of variables, data types, and control structures will make it easier to engage with the book.\nSome Machine Learning Knowledge: While not mandatory, a basic understanding of machine learning concepts will help you absorb the material more readily. If you‚Äôre new to the field, the book provides enough background information to get you up to speed.\nPython Programming (Optional): If you‚Äôre familiar with Python, you‚Äôll find it easier to engage with the coding sections of the book. Knowing libraries like NumPy, scikit-learn, and TensorFlow will be particularly helpful.\nWillingness to Learn: The book is designed to be accessible to a broad audience, with varying levels of technical expertise. A willingness to challenge yourself and engage in practical exercises will help you get the most out of it.\nResource Availability: For the hands-on aspects, you‚Äôll need a computer with Python and the relevant libraries installed. Optional access to an embedded development board or microcontroller will also be beneficial for experimenting with machine learning model deployment.\n\nBy meeting these prerequisites, you‚Äôll be well-positioned to deepen your understanding of TinyML, engage in coding exercises, and even implement practical applications on embedded devices."
  },
  {
    "objectID": "introduction.html#overview",
    "href": "introduction.html#overview",
    "title": "1¬† Introduction",
    "section": "1.1 Overview",
    "text": "1.1 Overview\nWelcome to this comprehensive exploration of Tiny Machine Learning (TinyML). This book aims to bridge the gap between intricate machine learning theories and their practical applications on small devices. Whether you‚Äôre a newcomer, an industry professional, or an academic researcher, this book offers a balanced mix of essential theory and hands-on insights into TinyML."
  },
  {
    "objectID": "introduction.html#whats-inside",
    "href": "introduction.html#whats-inside",
    "title": "1¬† Introduction",
    "section": "1.2 What‚Äôs Inside",
    "text": "1.2 What‚Äôs Inside\nThe book starts with a foundational look at embedded systems and machine learning, focusing on deep learning methods due to their effectiveness across various tasks. We then guide you through the entire machine learning workflow, from data engineering to advanced model training.\nWe also delve into TinyML model optimization and deployment, with a special emphasis on on-device learning. You‚Äôll find comprehensive discussions on current hardware acceleration techniques and model lifecycle management. Additionally, we explore the sustainability and ecological impact of AI, and how TinyML fits into this larger conversation.\nThe book concludes with a look at the exciting possibilities of generative AI within the TinyML context."
  },
  {
    "objectID": "introduction.html#chapter-breakdown",
    "href": "introduction.html#chapter-breakdown",
    "title": "1¬† Introduction",
    "section": "1.3 Chapter Breakdown",
    "text": "1.3 Chapter Breakdown\nHere‚Äôs a closer look at what each chapter covers:\nChapter 1: Introduction This chapter sets the stage, providing an overview of embedded AI and laying the groundwork for the chapters that follow.\nChapter 2: Embedded Systems We introduce the basics of embedded systems, the platforms where AI algorithms are widely applied.\nChapter 3: Deep Learning Primer This chapter offers a comprehensive introduction to the algorithms and principles that underpin AI applications in embedded systems.\nChapter 4: Embedded AI Here, we explore how machine learning techniques can be integrated into embedded systems, enabling intelligent functionalities.\nChapter 5: AI Workflow This chapter breaks down the machine learning workflow, offering insights into the steps leading to proficient AI applications.\nChapter 6: Data Engineering We focus on the importance of data in AI systems, discussing how to effectively manage and organize data.\nChapter 7: AI Training This chapter delves into model training, exploring techniques for developing efficient and reliable models.\nChapter 8: On-Device AI Here, we discuss strategies for achieving efficiency in AI applications, from computational resource optimization to performance enhancement.\nChapter 9: Model Optimizations We explore various avenues for optimizing AI models for seamless integration into embedded systems.\nChapter 10: AI Frameworks This chapter reviews different frameworks for developing machine learning models, guiding you in choosing the most suitable one for your projects.\nChapter 11: AI Acceleration We discuss the role of specialized hardware in enhancing the performance of embedded AI systems.\nChapter 12: Benchmarking AI This chapter focuses on how to evaluate AI systems through systematic benchmarking methods.\nChapter 13: On-Device Learning We explore techniques for localized learning, which enhances both efficiency and privacy.\nChapter 14: Embedded AIOps This chapter looks at the processes involved in the seamless integration, monitoring, and maintenance of AI functionalities in embedded systems.\nChapter 15: Privacy and Security As AI becomes more ubiquitous, this chapter addresses the crucial aspects of privacy and security in embedded AI systems.\nChapter 16: Responsible AI We discuss the ethical principles guiding the responsible use of AI, focusing on fairness, accountability, and transparency.\nChapter 17: AI Sustainability This chapter explores practices and strategies for sustainable AI, ensuring long-term viability and reduced environmental impact.\nChapter 18: Generative AI We explore the algorithms and techniques behind generative AI, opening avenues for innovation and creativity.\nChapter 19: AI for Good\nWe highlight positive applications of TinyML in areas like healthcare, agriculture, and conservation."
  },
  {
    "objectID": "introduction.html#how-to-navigate-this-book",
    "href": "introduction.html#how-to-navigate-this-book",
    "title": "1¬† Introduction",
    "section": "1.4 How to Navigate This Book",
    "text": "1.4 How to Navigate This Book\nTo get the most out of this book, consider the following structured approach:\n\nFoundational Knowledge (Chapters 1-4): Start by building a strong foundation with the initial chapters, which provide the context and groundwork for more advanced topics.\nPractical Insights (Chapters 5-14): With a solid foundation, move on to the chapters that offer practical insights into machine learning workflows, data engineering, and optimizations. Engage in hands-on exercises and case studies to solidify your understanding.\nEthics and Sustainability (Chapters 15-17): These chapters offer a critical perspective on the ethical and sustainable practices in AI, encouraging responsible AI deployment.\nFuture Trends (Chapter 18): Conclude your journey by exploring the exciting domain of generative AI, which offers a glimpse into the future of the field.\nInterconnected Learning: While the chapters are designed for a progressive learning curve, feel free to navigate non-linearly based on your interests and needs.\nPractical Applications: Throughout the book, try to relate theoretical knowledge to real-world applications. Engage with practical exercises and case studies to bridge the gap between theory and practice.\nDiscussion and Networking: Engage in discussions, forums, or study groups to share insights and debate concepts, which can deepen your understanding.\nRevisit and Reflect: Given the dynamic nature of AI, don‚Äôt hesitate to revisit chapters. A second reading can offer new insights and foster continuous learning.\n\nBy adopting this structured yet flexible approach, you‚Äôre setting the stage for a fulfilling and enriching learning experience."
  },
  {
    "objectID": "introduction.html#the-road-ahead",
    "href": "introduction.html#the-road-ahead",
    "title": "1¬† Introduction",
    "section": "1.5 The Road Ahead",
    "text": "1.5 The Road Ahead\nAs we navigate the multifaceted world of embedded AI, we‚Äôll cover a broad range of topics, from computational theories and engineering principles to ethical considerations and innovative applications. Each chapter unveils a piece of this expansive puzzle, inviting you to forge new connections, ignite discussions, and fuel a perpetual curiosity about embedded AI. Join us as we explore this fascinating field, which is not only reshaping embedded systems but also redrawing the contours of our technological future."
  },
  {
    "objectID": "introduction.html#contribute-back",
    "href": "introduction.html#contribute-back",
    "title": "1¬† Introduction",
    "section": "1.6 Contribute Back",
    "text": "1.6 Contribute Back\nLearning in the fast-paced world of embedded AI is a collaborative journey. This book aims to nurture a vibrant community of learners, innovators, and contributors. As you explore the concepts and engage with the exercises, we encourage you to share your insights and experiences. Whether it‚Äôs a novel approach, an interesting application, or a thought-provoking question, your contributions can enrich the learning ecosystem. Engage in discussions, offer and seek guidance, and collaborate on projects to foster a culture of mutual growth and learning. By sharing knowledge, you play a pivotal role in fostering a globally connected, informed, and empowered community."
  },
  {
    "objectID": "embedded_sys.html#basics-and-components",
    "href": "embedded_sys.html#basics-and-components",
    "title": "2¬† Embedded Systems",
    "section": "2.1 Basics and Components",
    "text": "2.1 Basics and Components\n\n2.1.1 Definition and Characteristics\nEmbedded systems are specialized forms of computing that do not resemble traditional computers. These systems are dedicated to particular tasks and integrate as components within larger devices. Unlike general-purpose computers capable of running a multitude of applications, embedded systems are designed to execute predefined tasks, often with stringent requirements. Due to their task-specific nature, their architecture is optimized for performance and reliability. The defining traits of these systems include:\n\nDedicated Functionality: These systems are engineered to carry out a specific function or a cluster of closely related functions. This specialization allows for optimization, resulting in enhanced performance and reliability.\nReal-Time Operation: A large number of embedded systems function in real-time, necessitating immediate responses to environmental inputs or changes within a set time frame.\nIntegration with Physical Hardware: Unlike general-purpose computing systems, embedded systems are tightly integrated with physical components, making them more mechanically oriented.\nLong Lifecycle: Typically, these systems have an extended lifecycle, continuing to operate for many years post their initial deployment.\nResource Constraints: Often operating under resource limitations, embedded systems require efficient algorithms and software due to restricted computational power and memory.\n\n\n\n2.1.2 Historical Background\nThe lineage of embedded systems dates back to the 1960s, marked by the introduction of the first microprocessor, labeled as Figure¬†2.1. This groundbreaking development led to the creation of the inaugural embedded system used in the Apollo Guidance Computer, the primary navigational system for the Apollo spacecraft. Over subsequent years, the domain has expanded remarkably, finding utility in diverse sectors such as automotive electronics, consumer electronics, telecommunications, and healthcare.\n\n\n\nFigure¬†2.1: Intel 4004.\n\n\n\n\n2.1.3 Importance in tinyML\nWithin the tinyML framework, embedded systems constitute a vital frontier. The direct integration of machine learning algorithms into these systems enables intelligent, edge-based decision-making, thereby minimizing latency and bolstering security. Here are several factors that underscore the importance of embedded systems in the tinyML ecosystem:\n\nEdge Computing: By localizing computation near the data source, embedded systems amplify efficiency and diminish the need for continuous interaction with centralized data repositories.\nLow Power Consumption: Designed for minimal energy usage, embedded systems in tinyML are particularly suited for battery-dependent devices and Internet of Things applications.\nReal-Time Analysis and Decision Making: These systems can conduct instantaneous data analysis, facilitating immediate decisions based on the generated insights.\nSecurity and Privacy: Local data processing on embedded systems enhances security and privacy by reducing the likelihood of data interception during transmission.\nCost-Effective: The deployment of machine learning models on embedded systems can be economically advantageous, particularly when data transmission and cloud storage could incur substantial costs.\n\nAs we progress further into this chapter, we will uncover the complexities that dictate the operations of embedded systems and examine how they serve as the foundational layer upon which tinyML is built, heralding a future filled with integrated, intelligent, and efficient devices and systems."
  },
  {
    "objectID": "embedded_sys.html#embedded-system-architecture",
    "href": "embedded_sys.html#embedded-system-architecture",
    "title": "2¬† Embedded Systems",
    "section": "2.2 Embedded System Architecture",
    "text": "2.2 Embedded System Architecture\nThe architectural layout of embedded systems serves as the schematic that outlines the structure and operations of these specialized entities. It sheds light on the interactions and collaborations among various components within an embedded system. This section will dissect the key elements of the architecture, including microcontrollers, microprocessors, diverse types of memory and their management, as well as the complexities of System on Chip (SoC).\n\n2.2.1 Microcontrollers vs Microprocessors\nComprehending the distinctions between microcontrollers and microprocessors is essential for understanding the basics of embedded system architecture. In this section, we will explore the unique attributes of each:\n\nMicrocontrollers\nMicrocontrollers are compact, integrated circuits engineered to control specific functions within an embedded system. They incorporate a processor, memory, and input/output peripherals within a single unit, as depicted in Figure¬†2.2, simplifying the overall system design. Microcontrollers are generally employed in applications where computational demands are moderate and cost-effectiveness is a primary consideration.\nCharacteristics:\n\nSingle-chip solution\nOn-chip memory and peripherals\nMinimal energy consumption\nWell-suited for control-oriented tasks\n\n\n\n\n\nFigure¬†2.2: Microcontrollers\n\n\n\nMicroprocessors\nIn contrast, microprocessors are more intricate and serve as the central processing unit within a system. They lack the integrated memory and input/output peripherals commonly found in microcontrollers. These processors are typically present in systems requiring elevated computational power and adaptability. They are suitable for devices where high processing power is a necessity and the tasks are data-intensive.\nCharacteristics:\n\nNecessitates external components like memory and input/output peripherals\nElevated processing power in comparison to microcontrollers\nGreater flexibility for connectivity with diverse components\nWell-suited for data-intensive tasks\n\n\n\n\n2.2.2 Memory Types and Management\nEmbedded systems utilize a variety of memory types, each fulfilling specific roles. Efficient memory management is vital for optimizing both performance and resource utilization. The following section elaborates on different types of memory and their management within the context of embedded systems:\n\nROM (Read-Only Memory): This non-volatile memory retains data written during the manufacturing process and remains unaltered throughout the lifespan of the device. It houses firmware and boot-up instructions.\nRAM (Random Access Memory): This volatile memory stores transient data generated during system operation. It is faster and permits read-write operations, but data is lost when power is disconnected.\nFlash Memory: This is a type of non-volatile memory that can be electrically erased and reprogrammed. It is commonly used for storing firmware or data that must be retained between system reboots.\n\nMemory Management:\n\nStatic Memory Allocation: In this approach, memory is allocated prior to runtime and remains fixed throughout system operation.\nDynamic Memory Allocation: Here, memory is allocated during runtime, offering flexibility but introducing the risk of increased complexity and potential memory leaks.\n\n\n\n2.2.3 System on Chip (SoC)\nThe majority of embedded systems are Systems on Chip (SoCs). An SoC embodies an advanced level of integration technology, incorporating most components required to construct a complete system onto a single chip. It often includes a microprocessor or microcontroller, blocks of memory, peripheral interfaces, and other requisite components for a fully operational system. Below is a detailed examination of its characteristics and applications:\n\nIntegration of Multiple Components: SoCs consolidate multiple components like CPUs, memory, and peripherals onto a single chip, facilitating higher levels of integration and reducing the need for external components.\nPower Efficiency: The high degree of integration often results in SoCs being more power-efficient compared to systems assembled from separate chips.\nCost-Effectiveness: The integrated nature leads to reduced manufacturing expenses, as fewer individual components are needed.\nApplications: SoCs are employed in a diverse range of sectors including mobile computing, automotive electronics, and Internet of Things devices where compact form factors and energy efficiency are highly valued.\n\nHere is a list of widely recognized SoCs that have found substantial applications across various domains:\n\nQualcomm Snapdragon: Predominantly used in smartphones and tablets, these SoCs offer a blend of processing power, graphics, and connectivity features.\nApple A-series: Custom-developed SoCs by Apple, used in their lineup of iPhones, iPads, and in certain versions of Apple TV and HomePod. Notable examples include the A14 Bionic and A15 Bionic chips.\nSamsung Exynos: Developed by Samsung, these SoCs are extensively used in their range of smartphones, tablets, and other electronic devices.\nNVIDIA Tegra: Initially intended for mobile devices, these SoCs have found significant applications in automotive and gaming consoles, such as the Nintendo Switch. A visual representation can be seen below in Figure¬†2.3.\nIntel Atom: Employed in a wide array of systems including netbooks, smartphones, and even embedded systems, these SoCs are known for their power efficiency.\nMediaTek Helio: Commonly found in budget to mid-range smartphones, these chips offer a balanced mix of power efficiency and performance.\nBroadcom SoCs: Extensively used in networking equipment, Broadcom provides a variety of SoCs with diverse functionalities, including those optimized for wireless communications and data processing.\nTexas Instruments (TI) OMAP: Previously popular in smartphones and tablets, these SoCs offered a range of functionalities including multimedia processing and connectivity.\nXilinx Zynq: Mainly used in embedded systems for industrial automation and in applications requiring high levels of data processing, such as advanced driver-assistance systems (ADAS).\nAltera SoC FPGA: Now a part of Intel, these SoCs combine FPGA technology with ARM cores, offering flexibility and performance for a range of applications including automotive and industrial systems.\n\n\n\n\nFigure¬†2.3: NVIDIA‚Äôs Tegra 2 combines two ARM Cortex-A9 cores with an ARM7 for SoC management tasks.\n\n\nEach of these Systems on Chip (SoCs) offers a unique array of features and capabilities, tailored to meet the diverse demands of an ever-evolving technological landscape. They consolidate multiple components onto a single chip, delivering power efficiency, cost-effectiveness, and compact solutions suitable for contemporary electronic devices."
  },
  {
    "objectID": "embedded_sys.html#embedded-system-programming",
    "href": "embedded_sys.html#embedded-system-programming",
    "title": "2¬† Embedded Systems",
    "section": "2.3 Embedded System Programming",
    "text": "2.3 Embedded System Programming\nProgramming for embedded systems differs significantly from traditional software development, being specifically designed to navigate the constraints of limited resources and real-time requirements commonly associated with embedded hardware. This section aims to shed light on the distinct programming languages employed, delve into the subtleties of firmware development, and explore the pivotal role of Real-time Operating Systems (RTOS) in this specialized domain.\n\n2.3.1 Programming Languages: C, C++, Python, etc.\nChoosing the right programming languages is essential in embedded systems, often emphasizing direct hardware interaction and memory usage optimization. Here, we will examine the unique attributes of these languages and how they differ from those commonly used in more conventional computing systems:\n\nC: Often considered the bedrock of embedded systems programming, the C language enables direct engagement with hardware, providing capabilities for bit-wise operations and memory address manipulation. Its procedural nature and low-level functionalities make it the preferred choice for resource-constrained environments, particularly for firmware development.\nC++: Building upon the foundational principles of C, C++ incorporates object-oriented features, promoting organized and modular code development. Despite its inherent complexity, it is employed in scenarios where higher-level abstractions do not undermine the detailed control offered by C.\nPython: Although not a traditional choice for embedded systems due to its higher memory consumption and runtime delays, Python is gradually gaining traction in the embedded sphere, particularly in systems with less stringent resource limitations. A specialized variant known as MicroPython has been developed, optimized for microcontrollers and retaining the simplicity and ease of Python. This flexible programming paradigm facilitates quick prototyping and development, as illustrated by the code snippet below that interfaces with pins on a PyBoard.\n\nimport pyb # Package from PyBoard\n\n# turn on an LED\npyb.LED(1).on()\n\n# print some text to the serial console\nprint('Hello MicroPython!')\nComparison with Traditional Systems: In contrast to mainstream computing systems, where languages like Java, Python, or JavaScript are lauded for their ease of development and extensive libraries, embedded systems favor languages that provide fine-grained control over hardware and opportunities for optimization, all while carefully navigating resource constraints.\n\n\n2.3.2 Firmware Development\nFirmware development in embedded systems involves creating programs that are permanently stored in the device‚Äôs non-volatile memory, ensuring consistent operation. This section outlines how firmware development diverges from software development in traditional computing systems:\n\nResource Optimization: The imperative for continual optimization is paramount, enabling the code to operate within the limitations of restricted memory and processing capabilities.\nHardware Interaction: Firmware often maintains a close relationship with hardware, requiring an in-depth understanding of hardware components and their functionalities.\nLifecycle Management: Firmware updates are less frequent than software updates in traditional systems, necessitating rigorous testing to prevent failures that could lead to hardware malfunctions.\nSecurity Concerns: Given its integral role, firmware is a potential target for security breaches, necessitating meticulous attention to security aspects, including secure coding practices and encryption protocols.\n\n\n\n2.3.3 Real-time Operating Systems (RTOS)\nRTOSs serve as the backbone for real-time embedded systems, managing task execution in a predictable and deterministic manner. This is a marked departure from operating systems in general-purpose computing, as outlined below:\n\nDeterministic Timing: RTOSs are designed to respond to inputs or events within a well-defined time frame, fulfilling the stringent time-sensitive requirements of many embedded systems.\nTask Prioritization: These systems enable task prioritization, allowing critical tasks to receive preferential processing time over less crucial tasks.\nMicrokernel Architecture: Many RTOSs employ a microkernel architecture, epitomizing efficiency and minimalism by focusing solely on essential functionalities.\nMemory Management: Memory management in RTOSs is often more streamlined compared to their counterparts in traditional operating systems, contributing to quick response times and operational efficiency.\n\nExamples of RTOS: Notable instances in this category include FreeRTOS, RTEMS, and VxWorks, each providing unique features tailored to meet the varied needs of different embedded systems applications."
  },
  {
    "objectID": "embedded_sys.html#interfaces-and-peripherals",
    "href": "embedded_sys.html#interfaces-and-peripherals",
    "title": "2¬† Embedded Systems",
    "section": "2.4 Interfaces and Peripherals",
    "text": "2.4 Interfaces and Peripherals\nEmbedded systems engage with the external environment through a range of interfaces and peripherals, which are often more specialized and streamlined than those in general-purpose systems. Let us explore these in detail:\n\n2.4.1 Digital I/O\nDigital Input/Output (I/O) interfaces are fundamental to embedded systems, enabling interaction with other devices and components. For instance, a digital I/O pin may be used to read a binary signal (0 or 1) from sensors or to control actuators. In embedded systems, these I/O ports often operate under strict timing constraints, a\nrequirement less common in general-purpose computing systems. Moreover, these systems are usually programmed for specific, optimized operations on digital signals, sometimes needing to function in real-time or near-real-time settings.\n\n\n2.4.2 Analog Interfaces\nAnalog interfaces in embedded systems are vital for interacting with a predominantly analog world. These interfaces may include components like Analog-to-Digital Converters (ADCs) and Digital-to-Analog Converters (DACs). For example, ADCs can be employed to read sensor data from environmental sensors such as temperature or humidity sensors, converting real-world analog data into a digital format that the microcontroller can process.\nIn contrast to general-purpose systems, embedded systems often utilize analog interfaces more directly and frequently, especially in sensor-integrated applications that require the conversion of a broad range of analog signals into digital data for further processing and analysis.\nIf you examine Figure¬†2.4 closely, you will notice indications of I/O pinouts for analog, digital, and communication layouts.\n\n\n\nFigure¬†2.4: Nicla Vision pinout\n\n\n\n\n2.4.3 Communication Protocols (SPI, I2C, UART, etc.)\nCommunication protocols act as the channels that enable communication between various components within or connected to an embedded system. Let us examine some commonly used ones:\n\nSPI (Serial Peripheral Interface): This synchronous serial communication protocol is primarily used for short-distance communication in embedded systems. For instance, it is frequently employed in communications with SD cards and TFT displays.\nI2C (Inter-Integrated Circuit): This multi-master, multi-slave, packet-switched, single-ended, serial communication bus is widely used in embedded systems to connect low-speed peripherals to motherboards, embedded systems, or cell phones. It is valued for its simplicity and low pin count.\nUART (Universal Asynchronous Receiver-Transmitter): This protocol enables asynchronous serial communication between devices. It is commonly used in embedded systems to transmit data between devices over a serial port, such as sending data logs from a sensor node to a computer.\n\nCompared to general-purpose systems, communication protocols in embedded systems are often more finely tuned for speed and reliability, especially in critical applications where data transmission integrity is crucial. Additionally, these protocols may be directly integrated into the microcontroller, facilitating more cohesive and seamless interactions between components, a feature less commonly observed in general-purpose systems."
  },
  {
    "objectID": "embedded_sys.html#power-management-in-embedded-systems",
    "href": "embedded_sys.html#power-management-in-embedded-systems",
    "title": "2¬† Embedded Systems",
    "section": "2.5 Power Management in Embedded Systems",
    "text": "2.5 Power Management in Embedded Systems\nPower management is a critical focus area in the design of embedded systems, influencing both the system‚Äôs efficiency and its applicability in real-world scenarios. The wide range of applications for embedded systems, from handheld devices to industrial equipment, highlights the need for meticulous power management. Let us explore this essential aspect of embedded systems:\n\n2.5.1 Power Consumption Considerations\nIn embedded systems, power consumption is a key factor that dictates both performance and longevity. Microcontrollers in these systems usually operate within a voltage range of 1.8V to 5V, with current consumption varying from microamperes (ŒºA) to milliamperes (mA) during active states. In sleep or standby modes, the current consumption can drop to nanoamperes (nA), extending battery life.\nIn contrast, general-purpose computing systems like desktop computers consume power on the scale of tens to hundreds of watts, several orders of magnitude higher than embedded systems. This significant difference underscores the need for careful power management in embedded systems, where the power budget is often much more limited.\nManaging power consumption involves a complex interplay of factors such as operating voltage, clock frequency, and the specific tasks the system performs. Engineers often find themselves balancing power consumption against performance and responsiveness, navigating a complex landscape of trade-offs.\n\n\n2.5.2 Energy-Efficient Design\nIncorporating energy efficiency into the design phase is crucial for the successful deployment of embedded systems. Techniques like dynamic voltage and frequency scaling (DVFS) are often employed, allowing the system to adjust voltage and frequency dynamically based on processing needs, thereby optimizing power consumption.\nAdditionally, the use of low-power modes, where non-essential peripherals are deactivated or clock frequencies are reduced, can significantly conserve energy. For example, deep sleep modes that consume as little as 100 nA can dramatically extend battery life, particularly in battery-operated embedded systems.\nThe architecture of the microcontroller, especially its instruction set architecture (ISA), is often highly specialized to eliminate unnecessary complexities that could increase power consumption. This specialization allows operations to be executed in fewer cycles compared to general-purpose processors, reducing the power consumed per operation. Moreover, these specialized ISAs are designed to efficiently execute the specific tasks that the embedded system is intended to perform, optimizing the execution path and thereby saving energy.\n\n\n2.5.3 Battery Management\nManaging batteries is an integral component of power management strategies in embedded systems. The goal is to maximize battery life without sacrificing performance. Battery-powered embedded systems often use lithium-ion or lithium-polymer batteries due to their high energy density and rechargeable features. These batteries typically have a voltage range of 3.7V to 4.2V per cell. For example, the Nicla Vision utilizes a 3.7V battery, as shown in Figure¬†2.5.\n\n\n\nFigure¬†2.5: Nicla Vision battery\n\n\nBy focusing on these elements, engineers can create systems that not only meet functional requirements but do so in a manner that reflects a deep understanding of the broader impacts of technology on society and the environment.\nEngineers are tasked with implementing methods such as effective charge regulation, protection against voltage spikes, and thermal monitoring to ensure the longevity and health of the battery. Additionally, the incorporation of systems that can tap into renewable energy sources like solar or kinetic energy can augment battery reserves, leading to enduring and eco-friendly solutions.\nThe emphasis on power management is driven by the imperative to make the most of available resources, prolong battery longevity, and minimize operational expenditures. In scenarios where the embedded systems are situated in remote or hard-to-reach locations, adept power management can substantially cut down on the frequency of maintenance visits, thereby guaranteeing continuous and seamless functionality.\nIt‚Äôs fair to assert that power management goes beyond being a mere technical specification in embedded systems; it serves as a pivotal factor that can either make or break the success of a project. Significant engineering effort is channeled into fine-tuning power management approaches, aiming to develop systems that are not just operationally efficient but also environmentally sustainable. This reflects a profound dedication to both technological innovation and excellence within the realm of embedded systems."
  },
  {
    "objectID": "embedded_sys.html#real-time-characteristics",
    "href": "embedded_sys.html#real-time-characteristics",
    "title": "2¬† Embedded Systems",
    "section": "2.6 Real-Time Characteristics",
    "text": "2.6 Real-Time Characteristics\nWithin the complex tapestry of embedded systems, real-time attributes serve as essential threads, interlacing various components and tasks into a unified, responsive whole. This element, often specific to embedded systems, occupies a vital role in both their architecture and functionality, endowing them with the nimbleness and accuracy needed for timely interaction with their surroundings. Let‚Äôs examine the nuances that underscore the real-time attributes of embedded systems:\n\n2.6.1 Real-time Clocks\nReal-time clocks (RTCs) hold a central position in embedded systems, offering an accurate time benchmark that directs the system‚Äôs activities. These clocks frequently come with battery backups to maintain reliable timekeeping, even when the primary power source is compromised. The role of RTCs is more critical and widespread in embedded systems compared to general-purpose computing, where timekeeping, while important, usually doesn‚Äôt govern the core operations of the system.\nFor example, in the realm of industrial automation, RTCs facilitate the precise coordination of tasks, ensuring synchronized and timely processes. They are particularly crucial in scenarios requiring time-stamped data, such as environmental monitoring systems where the accuracy and time relevance of data are imperative.\n\n\n2.6.2 Timing and Synchronization\nTiming and synchronization stand as defining features of embedded systems, requiring various components and processes to operate in concert. The essence of a real-time embedded system is shaped by its capability to execute tasks within a specified time window. Such systems often have rigorous timing constraints, necessitating synchronization methods that are both sturdy and exact.\nIn the context of automotive control systems, the synchronized and timely operation of diverse sensors and actuators is imperative for both safety and peak performance. This sharply contrasts with general-purpose systems, where timing, though managed, usually lacks immediate and critical consequences.\n\n\n2.6.3 Task Management and Scheduling\nIn the world of embedded systems, the management and scheduling of tasks are crucial for effective real-time responses. Task schedulers in these systems often use techniques like priority scheduling, where tasks are ranked by importance, allowing higher-priority tasks to interrupt those of lower priority. This is especially critical in systems where some functions have greater urgency.\nFor example, in medical devices such as pacemakers, the punctual delivery of electrical impulses is a high-priority task, and the scheduler must give it precedence over all other activities to ensure patient safety. This level of refined scheduling and task management sets embedded systems apart from the more adaptable but less deterministic scheduling seen in general-purpose systems.\n\n\n2.6.4 Error Handling and Fault Tolerance\nTo enhance their real-time features, embedded systems frequently incorporate mechanisms for error detection and fault resilience. These are engineered to swiftly identify and rectify errors or to sustain system functionality even when faults occur. In aviation control systems, for instance, real-time fault tolerance is essential for maintaining the stability and safety of drones. This meticulous approach to error management is somewhat unique to embedded systems, accentuating the critical nature of many such applications.\nThe real-time attributes of embedded systems distinguish them, creating an environment where accuracy, synchrony, and prompt responses are not optional but obligatory. These attributes resonate across a wide range of applications, from automotive systems to industrial automation and healthcare devices, highlighting the role of embedded systems as quiet yet potent conductors of a technologically synchronized world. Through their real-time features, embedded systems offer solutions that not only satisfy functional needs but do so with a degree of precision and dependability that is both extraordinary and essential in today‚Äôs world."
  },
  {
    "objectID": "embedded_sys.html#security-and-reliability",
    "href": "embedded_sys.html#security-and-reliability",
    "title": "2¬† Embedded Systems",
    "section": "2.7 Security and Reliability",
    "text": "2.7 Security and Reliability\nIn an increasingly interconnected and tech-dependent world, the issues of security and reliability have risen to become primary considerations in system engineering. This is especially true for embedded systems, which often serve as key components in critical infrastructures and applications, thereby raising the stakes considerably. Let‚Äôs explore the crucial elements that fortify the bastion of security and reliability in embedded systems:\n\n2.7.1 Secure Boot and Root of Trust\nEmbedded systems are increasingly central to a variety of critical applications, making it imperative to assure their authenticity and integrity from the moment they boot up. The secure boot sequence serves as a foundational element in this security framework, permitting the system to run only code that has been authenticated and deemed trustworthy. This is often augmented by a ‚ÄúRoot of Trust,‚Äù a stable and secure environment, typically hardware-based, that validates the initial firmware and each subsequent layer of software during the boot-up sequence.\nFor example, in financial settings involving Point-of-Sale (POS) terminals, a secure boot mechanism guarantees that the firmware remains intact and secure, thereby preventing any malicious alterations that could lead to significant data breaches. Likewise, in the realm of home automation, a strong secure boot process acts as a barrier to unauthorized access, thereby protecting user data and privacy.\n\n\n2.7.2 Fault Tolerance\nFault tolerance is an essential quality in embedded systems, granting them the ability to maintain functionality even when faced with faults or system failures. This resilience is achieved through various means such as redundancy, where vital components are replicated to assume control in the event of a failure, or via sophisticated error detection and correction methods.\nIn sectors like aerospace and aviation, fault tolerance is not merely an advantageous feature but an obligatory specification. For instance, aircraft control systems utilize multiple redundant configurations that operate in parallel to assure uninterrupted functionality, even if a component fails. This degree of fault tolerance provides a heightened level of reliability, enabling the system to endure failures without disastrous outcomes, a feature that distinguishes it from conventional computing systems.\n\n\n2.7.3 Safety-Critical Systems\nSafety-critical systems are defined as those where a malfunction could lead to loss of life, substantial property damage, or environmental degradation. Such systems demand rigorous design protocols to guarantee the highest levels of reliability and safety. Embedded systems falling under this classification often comply with stringent development guidelines and are subject to exhaustive testing to confirm their safety and reliability metrics.\nFor instance, in automotive safety features like Anti-lock Braking Systems (ABS) and Electronic Stability Control (ESC), embedded controllers are crucial. These controllers are engineered in accordance with rigorous standards like ISO 26262, ensuring they meet the elevated safety and reliability criteria essential for safeguarding lives. In the healthcare sector, devices such as pacemakers and infusion pumps are categorized as safety-critical, where the dependability of embedded systems can quite literally be life-altering.\nThe focus on security and reliability in embedded systems is of paramount importance, a point that is often underestimated by many. As these systems become increasingly woven into the fabric of our everyday lives and critical infrastructure, the principles of security and reliability serve as guiding lights in their development and deployment. Through features like secure booting and fault tolerance, these systems offer not just operational efficiency but also a layer of trust and security, providing a steadfast and secure anchor in a rapidly evolving technological landscape. These foundational tenets shape today‚Äôs embedded systems, molding them into dependable stewards and proficient operators in various critical domains of contemporary society."
  },
  {
    "objectID": "embedded_sys.html#future-trends-and-challenges",
    "href": "embedded_sys.html#future-trends-and-challenges",
    "title": "2¬† Embedded Systems",
    "section": "2.8 Future Trends and Challenges",
    "text": "2.8 Future Trends and Challenges\nArm, the leading producer of microcontrollers, has reached a milestone by shipping an unprecedented 8.0 billion chips, either directly or through its partners. This takes the total number of chips shipped to date to an astounding quarter of a trillion, or 250 billion (ARM.com)!\n\nARM.com. ‚ÄúThe Future Is Being Built on Arm: Market Diversification Continues to Drive Strong Royalty and Licensing Growth as Ecosystem Reaches Quarter of a Trillion Chips Milestone ‚Äì Arm¬Æ.‚Äù https://www.arm.com/company/news/2023/02/arm-announces-q3-fy22-results.\nAs we find ourselves at the threshold of a new era marked by extraordinary growth in the embedded systems sector, it becomes both exhilarating and imperative to scrutinize the emerging trends and challenges that lie ahead. From the expanding horizons of edge computing to the imperatives of scalability, the landscape is poised for transformation, unveiling new realms of both opportunities and challenges. Let‚Äôs explore the evolving frontier that awaits embedded systems:\n\n2.8.1 Edge Computing and IoT\nWith the rapid expansion of the Internet of Things (IoT), edge computing is gaining increasing prominence. Essentially, edge computing enables data to be processed closer to its source, thereby reducing latency and alleviating the burden on centralized data centers. This shift in computing paradigms is anticipated to revolutionize embedded systems, endowing them with enhanced processing power and the intelligence to perform intricate tasks on-site.\nAdditionally, as the IoT is projected to include billions of interconnected devices worldwide, embedded systems are slated to be the linchpin in ensuring smooth connectivity and interoperability among a diverse set of devices. This interconnected ecosystem is expected to enable real-time analytics and decision-making, laying the groundwork for more intelligent cities, industries, and households. The challenge resides in crafting systems that are secure, energy-efficient, and adept at managing the anticipated data deluge effectively.\n\n\n2.8.2 Scalability and Upgradation\nAs the landscape of embedded systems continues its evolutionary trajectory, the focus will increasingly turn towards scalability and ease of upgradation. Systems will be required to adapt to evolving technologies and user needs without undergoing extensive modifications. This necessitates modular architectures and adherence to open standards, facilitating the effortless incorporation of new functionalities and features.\nMoreover, in light of rapid technological advancements, embedded systems will need to incorporate capabilities for remote updates and maintenance to ensure their continued relevance and longevity. The responsibility will fall on the shoulders of developers and manufacturers to engineer systems that not only satisfy current needs but are also prepared for future enhancements, thereby securing a path for sustainable and progressive development.\n\n\n2.8.3 Market Opportunities\nThe market landscape for embedded systems is on the cusp of dynamic changes. As various industries accelerate their adoption of automation and digital transformation, the demand for advanced embedded systems is set to skyrocket. The integration of Artificial Intelligence (AI) and Machine Learning (ML) into embedded systems is expected to offer unparalleled levels of intelligence and automation.\nAt the same time, burgeoning opportunities are emerging in sectors like consumer electronics, automotive, healthcare, and industrial applications. While this growth presents enormous potential for innovation, it also introduces challenges such as heightened competition and the necessity for adherence to evolving regulatory frameworks. Companies entering this arena will need to exhibit agility, innovation, and adaptability to the shifting market conditions in order to establish a competitive edge."
  },
  {
    "objectID": "embedded_sys.html#conclusion",
    "href": "embedded_sys.html#conclusion",
    "title": "2¬† Embedded Systems",
    "section": "2.9 Conclusion",
    "text": "2.9 Conclusion\nThe table provides a side-by-side comparison between these two distinct types of computing systems, covering a range of categories including processing power, memory capabilities, user interface, and real-time functionalities, among others. The aim of this comparative analysis is to offer readers a concise yet thorough understanding of the unique attributes and specificities of both conventional and embedded computing systems. This, in turn, enables a more nuanced and informed grasp of their respective roles in today‚Äôs computing landscape.\n\n\n\n\n\n\n\n\nCategory\nTraditional Computing System\nEmbedded System Architecture\n\n\n\n\nHardware Characteristics\n\n\n\n\nProcessing Power\nHigh (Multi-core processors)\nModerate to Low (Single/Multi-core, optimized for specific tasks)\n\n\nMemory\nHigh (Upgradable)\nLimited (Fixed)\n\n\nStorage\nHigh (Upgradable)\nLimited (Fixed or expandable to a certain extent)\n\n\nHardware Scalability\nHigh (Can upgrade various components)\nLow (Hardware is often fixed and focused)\n\n\nSoftware Characteristics\n\n\n\n\nOperating System\nGeneral Purpose (Windows, Linux, macOS)\nReal-Time Operating System (RTOS) or No OS\n\n\nDevelopment Flexibility\nHigh (Supports multiple programming languages and frameworks)\nModerate (Focused on specific programming languages and tools)\n\n\nPerformance & Efficiency\n\n\n\n\nPower Consumption\nHigh\nLow (Optimized for energy efficiency)\n\n\nReal-Time Capabilities\nLimited (Not optimized for real-time tasks)\nHigh (Designed for real-time tasks)\n\n\nUser Interaction\n\n\n\n\nUser Interface\nComplex (GUI-Based)\nSimple or None (Can be GUI, command-line, or none)\n\n\nConnectivity\nExtensive (Multiple ports and connectivity options)\nLimited (Focused on necessary connectivity options)\n\n\nLifecycle & Maintenance\n\n\n\n\nMaintenance\nRegular Maintenance Required\nLow Maintenance (Set up to run specific tasks consistently)\n\n\nLifecycle\nShorter (Due to rapid technological advancements)\nLonger (Designed to perform specific tasks over a long period)\n\n\nCost and Use Cases\n\n\n\n\nCost\nVariable (Can be high depending on specifications)\nGenerally Lower (Due to focused functionalities)\n\n\nUse Cases\nGeneral (Various applications across sectors)\nSpecific (Dedicated to particular tasks or applications)\n\n\n\nAs we gaze into the future, it‚Äôs clear that the realm of embedded systems stands on the cusp of a transformative era, characterized by groundbreaking innovations, abundant opportunities, and formidable challenges. The horizon is replete with the promise of enhanced connectivity, heightened intelligence, and superior efficiency, carving out a trajectory where embedded systems will serve as the guiding force behind society‚Äôs technological progress. The path forward is one of discovery and adaptability, where the confluence of technological prowess and creative ingenuity will sculpt a future that is not only rich in technological advancements but also attuned to the intricate and continually shifting needs of a dynamic global landscape. It‚Äôs a field teeming with possibilities, inviting trailblazers to embark on a journey to define the parameters of a bright and flourishing future."
  },
  {
    "objectID": "embedded_sys.html#exercises",
    "href": "embedded_sys.html#exercises",
    "title": "2¬† Embedded Systems",
    "section": "2.10 Exercises",
    "text": "2.10 Exercises\nNow would be a great time for you to get your hands on a real embedded device, and get it setup.\n\n\n\n\n\n\nNicla Vision\n\n\n\nIf you want to play with an embedded system, try out the Nicla Vision\nSetup Nicla Vision"
  },
  {
    "objectID": "dl_primer.html#introduction",
    "href": "dl_primer.html#introduction",
    "title": "3¬† Deep Learning Primer",
    "section": "3.1 Introduction",
    "text": "3.1 Introduction\n\n3.1.1 Definition and Importance\nDeep learning, a specialized area within machine learning and artificial intelligence (AI), utilizes algorithms modeled after the structure and function of the human brain, known as artificial neural networks. This field is a foundational element in AI, driving progress in diverse sectors such as computer vision, natural language processing, and self-driving vehicles. Its significance in embedded AI systems is highlighted by its capability to handle intricate calculations and predictions, optimizing the limited resources in embedded settings.\n\n\n\n3.1.2 Brief History of Deep Learning\nThe idea of deep learning has origins in early artificial neural networks. It has experienced several cycles of interest, starting with the introduction of the Perceptron in the 1950s (Rosenblatt 1957), followed by the invention of backpropagation algorithms in the 1980s (Rumelhart, Hinton, and Williams 1986).\n\nRosenblatt, Frank. 1957. The Perceptron, a Perceiving and Recognizing Automaton Project Para. Cornell Aeronautical Laboratory.\n\nRumelhart, David E, Geoffrey E Hinton, and Ronald J Williams. 1986. ‚ÄúLearning Representations by Back-Propagating Errors.‚Äù Nature 323 (6088): 533‚Äì36.\n\nKrizhevsky, Alex, Ilya Sutskever, and Geoffrey E Hinton. 2012. ‚ÄúImagenet Classification with Deep Convolutional Neural Networks.‚Äù Advances in Neural Information Processing Systems 25.\nThe term ‚Äúdeep learning‚Äù became prominent in the 2000s, characterized by advances in computational power and data accessibility. Important milestones include the successful training of deep networks like AlexNet (Krizhevsky, Sutskever, and Hinton 2012) by Geoffrey Hinton, a leading figure in AI, and the renewed focus on neural networks as effective tools for data analysis and modeling.\nIn recent times, deep learning has seen exponential growth, transforming various industries. Computational growth followed an 18-month doubling pattern from 1952 to 2010, which then accelerated to a 6-month cycle from 2010 to 2022, as shown in Figure¬†3.1. Concurrently, we saw the emergence of large-scale models between 2015 and 2022, appearing 2 to 3 orders of magnitude faster and following a 10-month doubling cycle.\n\n\n\nFigure¬†3.1: Growth of deep learning models.\n\n\nMultiple factors have contributed to this surge, including advancements in computational power, the abundance of big data, and improvements in algorithmic designs. First, the growth of computational capabilities, especially the arrival of Graphics Processing Units (GPUs) and Tensor Processing Units (TPUs) (Jouppi et al. 2017), has significantly sped up the training and inference times of deep learning models. These hardware improvements have enabled the construction and training of more complex, deeper networks than what was possible in earlier years.\n\nJouppi, Norman P, Cliff Young, Nishant Patil, David Patterson, Gaurav Agrawal, Raminder Bajwa, Sarah Bates, et al. 2017. ‚ÄúIn-Datacenter Performance Analysis of a Tensor Processing Unit.‚Äù In Proceedings of the 44th Annual International Symposium on Computer Architecture, 1‚Äì12.\nSecond, the digital revolution has yielded a wealth of big data, offering rich material for deep learning models to learn from and excel in tasks such as image and speech recognition, language translation, and game playing. The presence of large, labeled datasets has been key in refining and successfully deploying deep learning applications in real-world settings.\nAdditionally, collaborations and open-source efforts have nurtured a dynamic community of researchers and practitioners, accelerating advancements in deep learning techniques. Innovations like deep reinforcement learning, transfer learning, and generative adversarial networks have broadened the scope of what is achievable with deep learning, opening new possibilities in various sectors including healthcare, finance, transportation, and entertainment.\nOrganizations around the world recognize the transformative potential of deep learning and are investing heavily in research and development to leverage its capabilities in providing innovative solutions, optimizing operations, and creating new business opportunities. As deep learning continues its upward trajectory, it is set to redefine how we interact with technology, enhancing convenience, safety, and connectivity in our lives.\n\n\n3.1.3 Applications of Deep Learning\nDeep learning finds extensive use across numerous industries today. In finance, it is employed for stock market prediction, risk assessment, and fraud detection. In marketing, it is used for customer segmentation, personalization, and content optimization. In healthcare, machine learning aids in diagnosis, treatment planning, and patient monitoring. The transformative impact on society is evident.\nFor instance, deep learning algorithms can predict stock market trends, guiding investment strategies and enhancing financial decisions. Similarly, in healthcare, deep learning can make medical predictions that improve patient diagnosis and save lives. The benefits are clear: machine learning not only predicts with greater accuracy than humans but also does so much more quickly.\nIn manufacturing, deep learning has had a significant impact. By continuously learning from vast amounts of data collected during the manufacturing process, companies can boost productivity while minimizing waste through improved efficiency. This financial benefit for companies translates to better quality products at lower prices for customers. Machine learning enables manufacturers to continually refine their processes, producing higher quality goods more efficiently than ever before.\nDeep learning also enhances everyday products like Netflix recommendations and Google Translate text translations. Moreover, it helps companies like Amazon and Uber reduce customer service costs by swiftly identifying dissatisfied customers.\n\n\n3.1.4 Relevance to Embedded AI\nEmbedded AI, the integration of AI algorithms directly into hardware devices, naturally gains from the capabilities of deep learning. The combination of deep learning algorithms and embedded systems has laid the groundwork for intelligent, autonomous devices capable of advanced on-device data processing and analysis. Deep learning aids in extracting complex patterns and information from input data, serving as an essential tool in the development of smart embedded systems, from household appliances to industrial machinery. This collaboration aims to usher in a new era of intelligent, interconnected devices that can learn and adapt to user behavior and environmental conditions, optimizing performance and offering unprecedented levels of convenience and efficiency."
  },
  {
    "objectID": "dl_primer.html#neural-networks",
    "href": "dl_primer.html#neural-networks",
    "title": "3¬† Deep Learning Primer",
    "section": "3.2 Neural Networks",
    "text": "3.2 Neural Networks\nDeep learning draws inspiration from the neural networks of the human brain to create patterns used in decision-making. This section delves into the foundational concepts that make up deep learning, providing insights into the more complex topics discussed later in this primer.\nNeural networks serve as the foundation of deep learning, inspired by the biological neural networks in the human brain to process and analyze data hierarchically. Below, we examine the primary components and structures commonly found in neural networks.\n\n3.2.1 Perceptrons\nThe perceptron is the basic unit or node that serves as the foundation for more complex structures. A perceptron takes various inputs, applies weights and a bias to these inputs, and then uses an activation function to produce an output.\n\n\n\nFigure¬†3.2: Perceptron\n\n\nConceived in the 1950s, perceptrons paved the way for the development of more intricate neural networks and have been a fundamental building block in the field of deep learning.\n\n\n3.2.2 Multi-layer Perceptrons\nMulti-layer perceptrons (MLPs) are an evolution of the single-layer perceptron model, featuring multiple layers of nodes connected in a feedforward manner. These layers include an input layer for data reception, several hidden layers for data processing, and an output layer for final result generation. MLPs are skilled at identifying non-linear relationships and use a backpropagation technique for training, where weights are optimized through a gradient descent algorithm.\n\n\n\nMultilayer Perceptron\n\n\n\n\n3.2.3 Activation Functions\nActivation functions are crucial components in neural networks, providing the mathematical equations that determine a network‚Äôs output. These functions introduce non-linearity into the network, enabling the learning of complex patterns. Popular activation functions include the sigmoid, tanh, and ReLU (Rectified Linear Unit) functions.\n\n\n\nActivation Function\n\n\n\n\n3.2.4 Computational Graphs\nDeep learning uses computational graphs to represent the various operations and their interactions within a neural network. This subsection explores the key phases of computational graph processing.\n\n\n\nTensorFlow Computational Graph\n\n\n\nForward Pass\nThe forward pass is the initial phase where data moves through the network from the input to the output layer. During this phase, each layer performs specific computations on the input data, using weights and biases before passing the resulting values to subsequent layers. The final output of this phase is used to compute the loss, indicating the difference between the predicted output and actual target values.\n\n\nBackward Pass (Backpropagation)\nBackpropagation is a key algorithm in training deep neural networks. This phase involves calculating the gradient of the loss function concerning each weight by using the chain rule, effectively moving backward through the network. The gradients calculated in this step guide the adjustment of weights with the objective of minimizing the loss function, thereby enhancing the network‚Äôs performance with each iteration of training.\nGrasping these foundational concepts paves the way to understanding more intricate deep learning architectures and techniques, fostering the development of more sophisticated and efficacious applications, especially within the realm of embedded AI systems.\n\n\n\n\n3.2.5 Model Architectures\nDeep learning architectures refer to the various structured approaches that dictate how neurons and layers are organized and interact in neural networks. These architectures have evolved to tackle different problems and data types effectively. This section offers an overview of some well-known deep learning architectures and their characteristics.\n\nMulti-Layer Perceptrons (MLPs)\nMLPs are basic deep learning architectures, comprising three or more layers: an input layer, one or more hidden layers, and an output layer. These layers are fully connected, meaning each neuron in a layer is linked to every neuron in the preceding and following layers. MLPs can model intricate functions and are used in a broad array of tasks, such as regression, classification, and pattern recognition. Their capacity to learn non-linear relationships through backpropagation makes them a versatile instrument in the deep learning toolkit.\nIn embedded AI systems, MLPs can function as compact models for simpler tasks like sensor data analysis or basic pattern recognition, where computational resources are limited. Their ability to learn non-linear relationships with relatively less complexity makes them a suitable choice for embedded systems.\n\n\nConvolutional Neural Networks (CNNs)\nCNNs are mainly used in image and video recognition tasks. This architecture employs convolutional layers that apply a series of filters to the input data to identify features like edges, corners, and textures. A typical CNN also includes pooling layers to reduce the spatial dimensions of the data, and fully connected layers for classification. CNNs have proven highly effective in tasks such as image recognition, object detection, and computer vision applications.\nIn embedded AI, CNNs are crucial for image and video recognition tasks, where real-time processing is often needed. They can be optimized for embedded systems by using techniques like quantization and pruning to minimize memory usage and computational demands, enabling efficient object detection and facial recognition functionalities in devices with limited computational resources.\n\n\nRecurrent Neural Networks (RNNs)\nRNNs are suitable for sequential data analysis, like time series forecasting and natural language processing. In this architecture, connections between nodes form a directed graph along a temporal sequence, allowing information to be carried across sequences through hidden state vectors. Variants of RNNs include Long Short-Term Memory (LSTM) and Gated Recurrent Units (GRU), designed to capture longer dependencies in sequence data.\nIn embedded systems, these networks can be used in voice recognition systems, predictive maintenance, or in IoT devices where sequential data patterns are common. Optimizations specific to embedded platforms can assist in managing their typically high computational and memory requirements.\n\n\nGenerative Adversarial Networks (GANs)\nGANs consist of two networks, a generator and a discriminator, trained simultaneously through adversarial training (Goodfellow et al. 2020). The generator produces data that tries to mimic the real data distribution, while the discriminator aims to distinguish between real and generated data. GANs are widely used in image generation, style transfer, and data augmentation.\n\nGoodfellow, Ian, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. 2020. ‚ÄúGenerative Adversarial Networks.‚Äù Communications of the ACM 63 (11): 139‚Äì44.\nIn embedded settings, GANs could be used for on-device data augmentation to enhance the training of models directly on the embedded device, enabling continual learning and adaptation to new data without the need for cloud computing resources.\n\n\nAutoencoders\nAutoencoders are neural networks used for data compression and noise reduction (Bank, Koenigstein, and Giryes 2023). They are structured to encode input data into a lower-dimensional representation and then decode it back to its original form. Variants like Variational Autoencoders (VAEs) introduce probabilistic layers that allow for generative properties, finding applications in image generation and anomaly detection.\n\nBank, Dor, Noam Koenigstein, and Raja Giryes. 2023. ‚ÄúAutoencoders.‚Äù Machine Learning for Data Science Handbook: Data Mining and Knowledge Discovery Handbook, 353‚Äì74.\nUsing autoencoders can help in efficient data transmission and storage, improving the overall performance of embedded systems with limited computational and memory resources.\n\n\nTransformer Networks\nTransformer networks have emerged as a powerful architecture, especially in natural language processing (Vaswani et al. 2017). These networks use self-attention mechanisms to weigh the influence of different input words on each output word, enabling parallel computation and capturing intricate patterns in data. Transformer networks have led to state-of-the-art results in tasks like language translation, summarization, and text generation.\n\nVaswani, Ashish, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, ≈Åukasz Kaiser, and Illia Polosukhin. 2017. ‚ÄúAttention Is All You Need.‚Äù Advances in Neural Information Processing Systems 30.\nThese networks can be optimized to perform language-related tasks directly on-device. For example, transformers can be used in embedded systems for real-time translation services or voice-assisted interfaces, where latency and computational efficiency are crucial. Techniques such as model distillation can be employed to deploy these networks on embedded devices with limited resources.\nEach of these architectures serves specific purposes and excels in different domains, offering a rich toolkit for addressing diverse problems in the realm of embedded AI systems. Understanding the nuances of these architectures is crucial in designing effective and efficient deep learning models for various applications.\n\n\n\n3.2.6 Traditional ML vs Deep Learning\nTo succinctly highlight the differences, a comparative table illustrates the contrasting characteristics between traditional ML and deep learning:\n\n\n\n\n\n\n\n\nAspect\nTraditional ML\nDeep Learning\n\n\n\n\nData Requirements\nLow to Moderate (efficient with smaller datasets)\nHigh (requires large datasets for nuanced learning)\n\n\nModel Complexity\nModerate (suitable for well-defined problems)\nHigh (detects intricate patterns, suited for complex tasks)\n\n\nComputational Resources\nLow to Moderate (cost-effective, less resource-intensive)\nHigh (demands substantial computational power and resources)\n\n\nDeployment Speed\nFast (quicker training and deployment cycles)\nSlow (prolonged training times, especially with larger datasets)\n\n\nInterpretability\nHigh (clear insights into decision pathways)\nLow (complex layered structures, ‚Äúblack box‚Äù nature)\n\n\nMaintenance\nEasier (simple to update and maintain)\nComplex (requires more efforts in maintenance and updates)\n\n\n\n\n\n3.2.7 Choosing Traditional ML vs.¬†DL\n\nData Availability and Volume\n\nAmount of Data: Traditional machine learning algorithms, such as decision trees or Naive Bayes, are often more suitable when data availability is limited, offering robust predictions even with smaller datasets. This is particularly true in cases like medical diagnostics for disease prediction and customer segmentation in marketing.\nData Diversity and Quality: Traditional machine learning algorithms are flexible in handling various data types and often require less preprocessing compared to deep learning models. They may also be more robust in situations with noisy data.\n\n\n\nComplexity of the Problem\n\nProblem Granularity: Problems that are simple to moderately complex, which may involve linear or polynomial relationships between variables, often find a better fit with traditional machine learning methods.\nHierarchical Feature Representation: Deep learning models are excellent in tasks that require hierarchical feature representation, such as image and speech recognition. However, not all problems require this level of complexity, and traditional machine learning algorithms may sometimes offer simpler and equally effective solutions.\n\n\n\nHardware and Computational Resources\n\nResource Constraints: The availability of computational resources often influences the choice between traditional ML and deep learning. The former is generally less resource-intensive and thus preferable in environments with hardware limitations or budget constraints.\nScalability and Speed: Traditional machine learning algorithms, like support vector machines (SVM), often allow for faster training times and easier scalability, particularly beneficial in projects with tight timelines and growing data volumes.\n\n\n\nRegulatory Compliance\nRegulatory compliance is crucial in various industries, requiring adherence to guidelines and best practices such as the GDPR in the EU. Traditional ML models, due to their inherent interpretability, often align better with these regulations, especially in sectors like finance and healthcare.\n\n\nInterpretability\nUnderstanding the decision-making process is easier with traditional machine learning techniques compared to deep learning models, which function as ‚Äúblack boxes,‚Äù making it challenging to trace decision pathways.\n\n\n\n3.2.8 Making an Informed Choice\nGiven the constraints of embedded AI systems, understanding the differences between traditional ML techniques and deep learning becomes essential. Both avenues offer unique advantages, and their distinct characteristics often dictate the choice of one over the other in different scenarios.\nDespite this, deep learning has been steadily outperforming traditional machine learning methods in several key areas due to a combination of abundant data, computational advancements, and proven effectiveness in complex tasks.\nHere are some specific reasons why we focus on deep learning in this text:\n\nSuperior Performance in Complex Tasks: Deep learning models, particularly deep neural networks, excel in tasks where the relationships between data points are incredibly intricate. Tasks like image and speech recognition, language translation, and playing complex games like Go and Chess have seen significant advancements primarily through deep learning algorithms.\nEfficient Handling of Unstructured Data: Unlike traditional machine learning methods, deep learning can process unstructured data more effectively. This is crucial in today‚Äôs data landscape, where a large majority of data is unstructured, such as text, images, and videos.\nLeveraging Big Data: With the availability of big data, deep learning models have the capacity to continually learn and improve. These models excel at utilizing large datasets to enhance their predictive accuracy, a limitation in traditional machine learning approaches.\nHardware Advancements and Parallel Computing: The advent of powerful GPUs and the availability of cloud computing platforms have enabled the rapid training of deep learning models. These advancements have addressed one of the significant challenges of deep learning‚Äîthe need for substantial computational resources.\nDynamic Adaptability and Continuous Learning: Deep learning models can adapt to new information or data dynamically. They can be trained to generalize their learning to new, unseen data, which is crucial in rapidly evolving fields like autonomous driving or real-time language translation.\n\nWhile deep learning has gained significant traction, it‚Äôs essential to understand that traditional machine learning is far from obsolete. As we delve deeper into the intricacies of deep learning, we will also highlight situations where traditional machine learning methods may be more appropriate due to their simplicity, efficiency, and interpretability. By focusing on deep learning in this text, we aim to equip readers with the knowledge and tools needed to tackle modern, complex problems across various domains, while also providing insights into the comparative advantages and appropriate application scenarios for both deep learning and traditional machine learning techniques."
  },
  {
    "objectID": "dl_primer.html#conclusion",
    "href": "dl_primer.html#conclusion",
    "title": "3¬† Deep Learning Primer",
    "section": "3.3 Conclusion",
    "text": "3.3 Conclusion\nDeep learning has risen as a potent set of techniques for addressing intricate pattern recognition and prediction challenges. Starting with an overview, we outlined the fundamental concepts and principles governing deep learning, laying the groundwork for more advanced studies.\nCentral to deep learning, we explored the basic ideas of neural networks, the powerful computational models inspired by the human brain‚Äôs interconnected neuron structure. This exploration allowed us to appreciate the capabilities and potential of neural networks in creating sophisticated algorithms capable of learning and adapting from data.\nUnderstanding the role of libraries and frameworks was a key part of our discussion, offering insights into the tools that can facilitate the development and deployment of deep learning models. These resources not only ease the implementation of neural networks but also open avenues for innovation and optimization.\nNext, we tackled the challenges one might face when embedding deep learning algorithms within embedded systems, providing a critical perspective on the complexities and considerations that come with bringing AI to edge devices.\nFurthermore, we delved into an examination of the limitations of deep learning. Through a series of discussions, we unraveled the challenges faced in deep learning applications and outlined scenarios where traditional machine learning might outperform deep learning. These sections are crucial for fostering a balanced view of the capabilities and limitations of deep learning.\nIn this primer, we have equipped you with the knowledge to make informed choices between deploying traditional machine learning or deep learning techniques, depending on the unique demands and constraints of a specific problem.\nAs we conclude this chapter, we hope you are now well-equipped with the basic ‚Äúlanguage‚Äù of deep learning, prepared to delve deeper into the subsequent chapters with a solid understanding and critical perspective. The journey ahead is filled with exciting opportunities and challenges in embedding AI within systems."
  },
  {
    "objectID": "embedded_ml.html#introduction",
    "href": "embedded_ml.html#introduction",
    "title": "4¬† Embedded AI",
    "section": "4.1 Introduction",
    "text": "4.1 Introduction\nML is rapidly evolving, with new paradigms emerging that are reshaping how these algorithms are developed, trained, and deployed. In particular, the area of embedded machine learning is experiencing significant innovation, driven by the proliferation of smart sensors, edge devices, and microcontrollers. This chapter explores the landscape of embedded machine learning, covering the key approaches of Cloud ML, Edge ML, and TinyML.\n\n\n\nCloud vs.¬†Edge vs.¬†TinyML: The Spectrum of Distributed Intelligence\n\n\nWe begin by outlining the features or characteristics, benefits, challenges, and use cases for each embedded ML variant. This provides context on where these technologies do well and where they face limitations. We then bring all three approaches together into a comparative analysis, evaluating them across critical parameters like latency, privacy, computational demands, and more. This side-by-side perspective highlights the unique strengths and tradeoffs involved in selecting among these strategies.\nNext, we trace the evolution timeline of embedded systems and machine learning, from the origins of wireless sensor networks to the integration of ML algorithms into microcontrollers. This historical lens enriches our understanding of the rapid pace of advancement in this domain. Finally, practical hands-on exercises offer an opportunity to experiment first-hand with embedded computer vision applications.\nBy the end of this multipronged exploration of embedded ML, you will possess the conceptual and practical knowledge to determine the appropriate ML implementation for your specific use case constraints. The chapter aims to equip you with the contextual clarity and technical skills to navigate this quickly shifting landscape, empowering impactful innovations."
  },
  {
    "objectID": "embedded_ml.html#cloud-ml",
    "href": "embedded_ml.html#cloud-ml",
    "title": "4¬† Embedded AI",
    "section": "4.2 Cloud ML",
    "text": "4.2 Cloud ML\n\n4.2.1 Characteristics\nCloud ML is a specialized branch of the broader machine learning field that operates within cloud computing environments. It offers a virtual platform for the development, training, and deployment of machine learning models, providing both flexibility and scalability.\nAt its foundation, Cloud ML utilizes a powerful blend of high-capacity servers, expansive storage solutions, and robust networking architectures, all located in data centers around the world. This setup centralizes computational resources, simplifying the management and scaling of machine learning projects.\nThe cloud environment excels in data processing and model training, designed to manage large data volumes and complex computations. Models crafted in Cloud ML can leverage vast amounts of data, processed and analyzed centrally, thereby enhancing the model‚Äôs learning and predictive performance.\n\n\n4.2.2 Benefits\nCloud ML is synonymous with immense computational power, adept at handling complex algorithms and large datasets. This is particularly advantageous for machine learning models that demand significant computational resources, effectively circumventing the constraints of local setups.\nA key advantage of Cloud ML is its dynamic scalability. As data volumes or computational needs grow, the infrastructure can adapt seamlessly, ensuring consistent performance.\nCloud ML platforms often offer a wide array of advanced tools and algorithms. Developers can utilize these resources to accelerate the building, training, and deployment of sophisticated models, thereby fostering innovation.\n\n\n4.2.3 Challenges\nDespite its capabilities, Cloud ML can face latency issues, especially in applications that require real-time responses. The time taken to send data to centralized servers and back can introduce delays, a significant drawback in time-sensitive scenarios.\nCentralizing data processing and storage can also create vulnerabilities in data privacy and security. Data centers become attractive targets for cyber-attacks, requiring substantial investments in security measures to protect sensitive data.\nAdditionally, as data processing needs escalate, so do the costs of using cloud services. Organizations dealing with large data volumes may encounter rising costs, potentially affecting the long-term scalability and feasibility of their operations.\n\n\n4.2.4 Example Use Cases\nCloud ML plays a pivotal role in powering virtual assistants like Siri and Alexa. These systems harness the cloud‚Äôs computational prowess to analyze and process voice inputs, delivering intelligent and personalized responses to users.\nIt also serves as the foundation for advanced recommendation systems in platforms like Netflix and Amazon. These systems sift through extensive datasets to identify patterns and preferences, offering personalized content or product suggestions to boost user engagement.\nIn the financial realm, Cloud ML has been instrumental in creating robust fraud detection systems. These systems scrutinize vast amounts of transactional data to flag potential fraudulent activities, enabling timely interventions and reducing financial risks.\nIn summary, it‚Äôs virtually impossible to navigate the internet today without encountering some form of Cloud ML, either directly or indirectly. From the personalized ads that appear on your social media feed to the predictive text features in email services, Cloud ML is deeply integrated into our online experiences. It powers smart algorithms that recommend products on e-commerce sites, fine-tunes search engines to deliver accurate results, and even automates the tagging and categorization of photos on platforms like Facebook.\nFurthermore, Cloud ML bolsters user security through anomaly detection systems that monitor for unusual activities, potentially shielding users from cyber threats. Essentially, it acts as the unseen powerhouse, continuously operating behind the scenes to refine, secure, and personalize our digital interactions, making the modern internet a more intuitive and user-friendly environment."
  },
  {
    "objectID": "embedded_ml.html#edge-ml",
    "href": "embedded_ml.html#edge-ml",
    "title": "4¬† Embedded AI",
    "section": "4.3 Edge ML",
    "text": "4.3 Edge ML\n\n4.3.1 Characteristics\nDefinition of Edge ML\nEdge Machine Learning (Edge ML) is the practice of running machine learning algorithms directly on endpoint devices or closer to where the data is generated, rather than relying on centralized cloud servers. This approach aims to bring computation closer to the data source, reducing the need to send large volumes of data over networks, which often results in lower latency and improved data privacy.\nDecentralized Data Processing\nIn Edge ML, data processing happens in a decentralized fashion. Instead of sending data to remote servers, the data is processed locally on devices like smartphones, tablets, or IoT devices. This local processing allows devices to make quick decisions based on the data they collect, without having to rely heavily on a central server‚Äôs resources. This decentralization is particularly important in real-time applications where even a slight delay can have significant consequences.\nLocal Data Storage and Computation\nLocal data storage and computation are key features of Edge ML. This setup ensures that data can be stored and analyzed directly on the devices, thereby maintaining the privacy of the data and reducing the need for constant internet connectivity. Moreover, this often leads to more efficient computation, as data doesn‚Äôt have to travel long distances, and computations are performed with a more nuanced understanding of the local context, which can sometimes result in more insightful analyses.\n\n\n4.3.2 Benefits\nReduced Latency\nOne of the main advantages of Edge ML is the significant reduction in latency compared to Cloud ML. In situations where milliseconds count, such as in autonomous vehicles where quick decision-making can mean the difference between safety and an accident, this reduced latency can be a critical benefit.\nEnhanced Data Privacy\nEdge ML also offers improved data privacy, as data is primarily stored and processed locally. This minimizes the risk of data breaches that are more common in centralized data storage solutions. This means sensitive information can be kept more secure, as it‚Äôs not sent over networks where it could potentially be intercepted.\nLower Bandwidth Usage\nOperating closer to the data source means that less data needs to be sent over networks, reducing bandwidth usage. This can result in cost savings and efficiency gains, especially in environments where bandwidth is limited or costly.\n\n\n4.3.3 Challenges\nLimited Computational Resources Compared to Cloud ML\nHowever, Edge ML is not without its challenges. One of the main concerns is the limited computational resources compared to cloud-based solutions. Endpoint devices may not have the same processing power or storage capacity as cloud servers, which can limit the complexity of the machine learning models that can be deployed.\nComplexity in Managing Edge Nodes\nManaging a network of edge nodes can introduce complexity, especially when it comes to coordination, updates, and maintenance. Ensuring that all nodes are operating seamlessly and are up-to-date with the latest algorithms and security protocols can be a logistical challenge.\nSecurity Concerns at the Edge Nodes\nWhile Edge ML offers enhanced data privacy, edge nodes can sometimes be more vulnerable to physical and cyber-attacks. Developing robust security protocols that protect data at each node, without compromising the system‚Äôs efficiency, remains a significant challenge in deploying Edge ML solutions.\n\n\n4.3.4 Example Use Cases\nEdge ML has a wide range of applications, from autonomous vehicles and smart homes to industrial IoT. These examples were chosen to highlight scenarios where real-time data processing, reduced latency, and enhanced privacy are not just beneficial but often critical to the operation and success of these technologies. They serve to demonstrate the pivotal role that Edge ML can play in driving advancements in various sectors, fostering innovation, and paving the way for more intelligent, responsive, and adaptive systems.\nAutonomous Vehicles\nAutonomous vehicles stand as a prime example of Edge ML‚Äôs potential. These vehicles rely heavily on real-time data processing to navigate and make decisions. Localized machine learning models assist in quickly analyzing data from various sensors to make immediate driving decisions, essentially ensuring safety and smooth operation.\nSmart Homes and Buildings\nIn smart homes and buildings, Edge ML plays a crucial role in efficiently managing various systems, from lighting and heating to security. By processing data locally, these systems can operate more responsively and in harmony with the occupants‚Äô habits and preferences, creating a more comfortable living environment.\nIndustrial IoT\nThe Industrial Internet of Things (IoT) leverages Edge ML to monitor and control complex industrial processes. Here, machine learning models can analyze data from numerous sensors in real-time, enabling predictive maintenance, optimizing operations, and enhancing safety measures. This brings about a revolution in industrial automation and efficiency.\nThe applicability of Edge ML is vast and not limited to these examples. Various other sectors, including healthcare, agriculture, and urban planning, are exploring and integrating Edge ML to develop solutions that are both innovative and responsive to real-world needs and challenges, heralding a new era of smart, interconnected systems."
  },
  {
    "objectID": "embedded_ml.html#tiny-ml",
    "href": "embedded_ml.html#tiny-ml",
    "title": "4¬† Embedded AI",
    "section": "4.4 Tiny ML",
    "text": "4.4 Tiny ML\n\n4.4.1 Characteristics\nDefinition of TinyML\nTinyML sits at the crossroads of embedded systems and machine learning, representing a burgeoning field that brings smart algorithms directly to tiny microcontrollers and sensors. These microcontrollers operate under severe resource constraints, particularly in terms of memory, storage, and computational power.\nOn-Device Machine Learning\nIn TinyML, the focus is on on-device machine learning. This means that machine learning models are not just deployed but also trained right on the device, eliminating the need for external servers or cloud infrastructures. This allows TinyML to enable intelligent decision-making right where the data is generated, making real-time insights and actions possible, even in settings where connectivity is limited or unavailable.\nLow Power and Resource-Constrained Environments\nTinyML excels in low-power and resource-constrained settings. These environments require solutions that are highly optimized to function within the available resources. TinyML meets this need through specialized algorithms and models designed to deliver decent performance while consuming minimal energy, thus ensuring extended operational periods, even in battery-powered devices.\n\n\n4.4.2 Benefits\nExtremely Low Latency\nOne of the standout benefits of TinyML is its ability to offer ultra-low latency. Since computation occurs directly on the device, the time required to send data to external servers and receive a response is eliminated. This is crucial in applications requiring immediate decision-making, enabling quick responses to changing conditions.\nHigh Data Security\nTinyML inherently enhances data security. Because data processing and analysis happen on the device itself, the risk of data interception during transmission is virtually eliminated. This localized approach to data management ensures that sensitive information stays on the device, thereby strengthening user data security.\nEnergy Efficiency\nTinyML operates within an energy-efficient framework, a necessity given the resource-constrained environments in which it functions. By employing lean algorithms and optimized computational methods, TinyML ensures that devices can execute complex tasks without rapidly depleting battery life, making it a sustainable option for long-term deployments.\n\n\n4.4.3 Challenges\nLimited Computational Capabilities\nHowever, the shift to TinyML comes with its set of hurdles. The primary limitation is the constrained computational capabilities of the devices. The need to operate within such limits means that deployed models must be simplified, which could affect the accuracy and sophistication of the solutions.\nComplex Development Cycle\nTinyML also introduces a complicated development cycle. Crafting models that are both lightweight and effective demands a deep understanding of machine learning principles, along with expertise in embedded systems. This complexity calls for a collaborative development approach, where multi-domain expertise is essential for success.\nModel Optimization and Compression\nA central challenge in TinyML is model optimization and compression. Creating machine learning models that can operate effectively within the limited memory and computational power of microcontrollers requires innovative approaches to model design. Developers often face the challenge of striking a delicate balance, optimizing models to maintain effectiveness while fitting within stringent resource constraints.\n\n\n4.4.4 Example Use Cases\nWearable Devices\nIn wearables, TinyML opens the door to smarter, more responsive gadgets. From fitness trackers offering real-time workout feedback to smart glasses processing visual data on the fly, TinyML is transforming how we engage with wearable tech, delivering personalized experiences directly from the device.\nPredictive Maintenance\nIn industrial settings, TinyML plays a significant role in predictive maintenance. By deploying TinyML algorithms on sensors that monitor equipment health, companies can preemptively identify potential issues, reducing downtime and preventing costly breakdowns. On-site data analysis ensures quick responses, potentially stopping minor issues from becoming major problems.\nAnomaly Detection\nTinyML can be employed to create anomaly detection models that identify unusual data patterns. For instance, a smart factory could use TinyML to monitor industrial processes and spot anomalies, helping prevent accidents and improve product quality. Similarly, a security company could use TinyML to monitor network traffic for unusual patterns, aiding in the detection and prevention of cyber attacks. In healthcare, TinyML could monitor patient data for anomalies, aiding early disease detection and better patient treatment.\nEnvironmental Monitoring\nIn the field of environmental monitoring, TinyML enables real-time data analysis from various field-deployed sensors. These could range from air quality monitoring in cities to wildlife tracking in protected areas. Through TinyML, data can be processed locally, allowing for quick responses to changing conditions and providing a nuanced understanding of environmental patterns, crucial for informed decision-making.\nIn summary, TinyML serves as a trailblazer in the evolution of machine learning, fostering innovation across various fields by bringing intelligence directly to the edge. Its potential to transform our interaction with technology and the world is immense, promising a future where devices are not just connected but also intelligent, capable of making real-time decisions and responses."
  },
  {
    "objectID": "embedded_ml.html#comparison",
    "href": "embedded_ml.html#comparison",
    "title": "4¬† Embedded AI",
    "section": "4.5 Comparison",
    "text": "4.5 Comparison\nUp to this point, we‚Äôve explored each of the different ML variants individually. Now, let‚Äôs bring them all together for a comprehensive view. Below is a table offering a comparative analysis of Cloud ML, Edge ML, and TinyML based on various features and aspects. This comparison aims to provide a clear perspective on the unique advantages and distinguishing factors of each, aiding in making informed decisions based on the specific needs and constraints of a given application or project.\n\n\n\n\n\n\n\n\n\nFeature/Aspect\nCloud ML\nEdge ML\nTinyML\n\n\n\n\nProcessing Location\nCentralized servers (Data Centers)\nLocal devices (closer to data sources)\nOn-device (microcontrollers, embedded systems)\n\n\nLatency\nHigh (Depends on internet connectivity)\nModerate (Reduced latency compared to Cloud ML)\nLow (Immediate processing without network delay)\n\n\nData Privacy\nModerate (Data transmitted over networks)\nHigh (Data remains on local networks)\nVery High (Data processed on-device, not transmitted)\n\n\nComputational Power\nHigh (Utilizes powerful data center infrastructure)\nModerate (Utilizes local device capabilities)\nLow (Limited to the power of the embedded system)\n\n\nEnergy Consumption\nHigh (Data centers consume significant energy)\nModerate (Less than data centers, more than TinyML)\nLow (Highly energy-efficient, designed for low power)\n\n\nScalability\nHigh (Easy to scale with additional server resources)\nModerate (Depends on local device capabilities)\nLow (Limited by the hardware resources of the device)\n\n\nCost\nHigh (Recurring costs for server usage, maintenance)\nVariable (Depends on the complexity of local setup)\nLow (Primarily upfront costs for hardware components)\n\n\nConnectivity Dependence\nHigh (Requires stable internet connectivity)\nLow (Can operate with intermittent connectivity)\nVery Low (Can operate without any network connectivity)\n\n\nReal-time Processing\nModerate (Can be affected by network latency)\nHigh (Capable of real-time processing locally)\nVery High (Immediate processing with minimal latency)\n\n\nApplication Examples\nBig Data Analysis, Virtual Assistants\nAutonomous Vehicles, Smart Homes\nWearables, Sensor Networks\n\n\nDevelopment Complexity\nModerate to High (Requires knowledge in cloud computing)\nModerate (Requires knowledge in local network setup)\nModerate to High (Requires expertise in embedded systems)"
  },
  {
    "objectID": "embedded_ml.html#evolution-timeline",
    "href": "embedded_ml.html#evolution-timeline",
    "title": "4¬† Embedded AI",
    "section": "4.6 Evolution Timeline",
    "text": "4.6 Evolution Timeline\n\n4.6.1 Late 1990s - Early 2000s: The Dawn of Wireless Sensor Networks\nDuring the late 1990s and early 2000s, wireless sensor networks (WSNs) marked a significant milestone in information technology. These networks consisted of sensor nodes that could collect and wirelessly transmit data. With capabilities to monitor various environmental conditions like temperature and humidity, WSNs found applications across diverse sectors, including industrial automation, healthcare, and environmental monitoring. This era also saw the development of standardized protocols like Zigbee, which facilitated secure and reliable data transmission.\n\n\n4.6.2 Mid-2000s: The Rise of the Internet of Things (IoT)\nMoving into the mid-2000s, the Internet of Things (IoT) began to take shape. IoT expanded upon the principles of WSNs, connecting a variety of devices and enabling them to communicate and share data over the internet. The incorporation of embedded systems in IoT devices led to smarter operations, as these devices could now not only collect but also process data for intelligent decision-making. This era witnessed the widespread adoption of smart homes and industrial IoT, transforming our interaction with devices and systems.\n\n\n4.6.3 Late 2000s - Early 2010s: The Smartphone Revolution and Mobile Computing\nThe late 2000s ushered in the smartphone revolution, significantly impacting the evolution of embedded systems. Smartphones evolved into powerful computing devices, equipped with various sensors and embedded systems capable of executing complex tasks. This integration laid the foundation for mobile computing, with applications ranging from gaming and navigation to health monitoring.\n\n\n4.6.4 Mid-2010s: The Era of Big Data and Edge Computing\nBy the mid-2010s, the enormous volume of data generated by interconnected devices necessitated new data processing strategies. Big Data technologies emerged to manage this data influx, and alongside, the concept of edge computing gained prominence. Edge computing brought data processing closer to the data source, reducing latency and bandwidth usage. Embedded systems adapted to support edge computing, enabling substantial local data processing and lessening the reliance on centralized data centers.\n\n\n4.6.5 Late 2010s - Early 2020s: Integration of Machine Learning and AI\nAs we approached the late 2010s and early 2020s, machine learning and AI became integral to embedded systems. This integration led to the development of smart devices with enhanced decision-making and predictive capabilities. Advances in natural language processing, computer vision, and predictive analytics were notable, as embedded systems became capable of supporting complex AI algorithms.\n\n\n4.6.6 Early 2020s: The Advent of TinyML\nEntering the 2020s, the field saw the emergence of TinyML, bringing machine learning capabilities to ultra-low-power microcontrollers. This development enabled the deployment of ML models directly onto small embedded devices, allowing for intelligent edge data processing even on devices with limited computational resources. This has expanded the possibilities for IoT devices, making them smarter and more autonomous.\n\n\n4.6.7 2023 and Beyond: Towards a Future of Ubiquitous Embedded AI\nAs we move further into this decade, we foresee a transformative phase where embedded AI and TinyML transition from being innovative concepts to pervasive forces integral to our technological landscape. This promises a future where the lines between artificial intelligence and daily functionalities increasingly blur, heralding a new era of innovation and efficiency."
  },
  {
    "objectID": "embedded_ml.html#conclusion",
    "href": "embedded_ml.html#conclusion",
    "title": "4¬† Embedded AI",
    "section": "4.7 Conclusion",
    "text": "4.7 Conclusion\nIn this chapter, we‚Äôve offered a panoramic view of the evolving landscape of embedded machine learning, covering cloud, edge, and tiny ML paradigms. Cloud-based machine learning leverages the immense computational resources of cloud platforms to enable powerful and accurate models but comes with its own set of limitations, including latency and privacy concerns. Edge ML mitigates these limitations by bringing ML inference directly to edge devices, offering lower latency and reduced connectivity needs. TinyML takes this a step further by miniaturizing ML models to run directly on highly resource-constrained devices, opening up a new category of intelligent applications.\nEach approach comes with its own set of trade-offs, including model complexity, latency, privacy, and hardware costs. Over time, we anticipate a convergence of these embedded ML approaches, with cloud pre-training facilitating more sophisticated edge and tiny ML implementations. Advances like federated learning and on-device learning will also enable embedded devices to refine their models by learning from real-world data.\nThe embedded ML landscape is in a state of rapid evolution, poised to enable intelligent applications across a broad spectrum of devices and use cases. This chapter serves as a snapshot of the current state of embedded ML, and as algorithms, hardware, and connectivity continue to improve, we can expect embedded devices of all sizes to become increasingly capable, unlocking transformative new applications for artificial intelligence."
  },
  {
    "objectID": "embedded_ml.html#exercises",
    "href": "embedded_ml.html#exercises",
    "title": "4¬† Embedded AI",
    "section": "4.8 Exercises",
    "text": "4.8 Exercises\nNow would be a great time for you to try out a small computer vision model out of the box.\n\n\n\n\n\n\nNicla Vision\n\n\n\nIf you want to play with an embedded system, try out the Nicla Vision\nComputer Vision"
  },
  {
    "objectID": "workflow.html#overview",
    "href": "workflow.html#overview",
    "title": "5¬† AI Workflow",
    "section": "5.1 Overview",
    "text": "5.1 Overview\nAn ML workflow is a systematic process that encompasses the development, deployment, and maintenance of ML models. The typical steps involved are:\n\nProblem Definition: Clearly define the problem you aim to solve with your ML model, whether it‚Äôs image classification, customer churn prediction, or text generation. This clarity sets the stage for data collection and algorithm selection.\nData Collection and Preparation: Gather a high-quality dataset that accurately represents the problem at hand. Data cleaning and preparation are essential steps, which may include outlier removal, missing value imputation, and feature scaling.\nAlgorithm Selection: Choose an ML algorithm that aligns with your data type and problem. Various algorithms have their own pros and cons, making the selection critical.\nModel Training: Train your chosen ML algorithm on the prepared dataset. The duration of this process can vary based on dataset size and complexity.\nModel Evaluation: Assess the model‚Äôs performance using a separate test set to gauge its generalization capabilities.\nModel Deployment: Integrate the model into production once its performance meets your criteria. This could involve embedding it into a software application or offering it as a web service.\nMonitoring and Maintenance: Keep track of the model‚Äôs performance post-deployment and update it as necessary to adapt to changing real-world conditions.\n\nThe ML workflow is iterative, requiring ongoing monitoring and potential adjustments. Additional considerations include:\n\nVersion Control: Keep track of code and data changes to reproduce results and revert to earlier versions if needed.\nDocumentation: Maintain detailed documentation to allow for workflow understanding and reproduction.\nTesting: Rigorously test the workflow to ensure its functionality.\nSecurity: Safeguard your workflow and data, particularly when deploying models in production settings."
  },
  {
    "objectID": "workflow.html#general-vs.-embedded-ai",
    "href": "workflow.html#general-vs.-embedded-ai",
    "title": "5¬† AI Workflow",
    "section": "5.2 General vs.¬†Embedded AI",
    "text": "5.2 General vs.¬†Embedded AI\nThe ML workflow serves as a universal guide, applicable across various platforms including cloud-based solutions, edge computing, and tinyML. However, the workflow for Embedded AI introduces unique complexities and challenges, which not only make it a captivating domain but also pave the way for remarkable innovations.\n\n5.2.1 Resource Optimization\n\nGeneral ML Workflow: Prioritizes model accuracy and performance, often leveraging abundant computational resources in cloud or data center environments.\nEmbedded AI Workflow: Requires careful planning to optimize model size and computational demands, given the resource constraints of embedded systems. Techniques like model quantization and pruning are crucial.\n\n\n\n5.2.2 Real-time Processing\n\nGeneral ML Workflow: Less emphasis on real-time processing, often relying on batch data processing.\nEmbedded AI Workflow: Prioritizes real-time data processing, making low latency and quick execution essential, especially in applications like autonomous vehicles and industrial automation.\n\n\n\n5.2.3 Data Management and Privacy\n\nGeneral ML Workflow: Processes data in centralized locations, often necessitating extensive data transfer and focusing on data security during transit and storage.\nEmbedded AI Workflow: Leverages edge computing to process data closer to its source, reducing data transmission and enhancing privacy through data localization.\n\n\n\n5.2.4 Hardware-Software Integration\n\nGeneral ML Workflow: Typically operates on general-purpose hardware, with software development occurring somewhat independently.\nEmbedded AI Workflow: Involves a more integrated approach to hardware and software development, often incorporating custom chips or hardware accelerators to achieve optimal performance."
  },
  {
    "objectID": "workflow.html#roles-responsibilities",
    "href": "workflow.html#roles-responsibilities",
    "title": "5¬† AI Workflow",
    "section": "5.3 Roles & Responsibilities",
    "text": "5.3 Roles & Responsibilities\nCreating an ML solution, especially for embedded AI, is a multidisciplinary effort involving various specialists.\nHere‚Äôs a rundown of the typical roles involved:\n\n\n\n\n\n\n\nRole\nResponsibilities\n\n\n\n\nProject Manager\nOversees the project, ensuring timelines and milestones are met.\n\n\nDomain Experts\nOffer domain-specific insights to define project requirements.\n\n\nData Scientists\nSpecialize in data analysis and model development.\n\n\nMachine Learning Engineers\nFocus on model development and deployment.\n\n\nData Engineers\nManage data pipelines.\n\n\nEmbedded Systems Engineers\nIntegrate ML models into embedded systems.\n\n\nSoftware Developers\nDevelop software components for AI system integration.\n\n\nHardware Engineers\nDesign and optimize hardware for the embedded AI system.\n\n\nUI/UX Designers\nFocus on user-centric design.\n\n\nQA Engineers\nEnsure the system meets quality standards.\n\n\nEthicists and Legal Advisors\nConsult on ethical and legal compliance.\n\n\nOperations and Maintenance Personnel\nMonitor and maintain the deployed system.\n\n\nSecurity Specialists\nEnsure system security.\n\n\n\nUnderstanding these roles is crucial for the successful completion of an ML project. As we proceed through the upcoming chapters, we‚Äôll delve into each role‚Äôs essence and expertise, fostering a comprehensive understanding of the complexities involved in embedded AI projects. This holistic view not only facilitates seamless collaboration but also nurtures an environment ripe for innovation and breakthroughs."
  },
  {
    "objectID": "data_engineering.html#introduction",
    "href": "data_engineering.html#introduction",
    "title": "6¬† Data Engineering",
    "section": "6.1 Introduction",
    "text": "6.1 Introduction\nDataset creators face complex privacy and representation challenges when building high-quality training data, especially for sensitive domains like healthcare. Legally, creators may need to remove direct identifiers like names and ages. Even without legal obligations, removing such information can help build user trust. However, excessive anonymization can compromise dataset utility. Techniques like differential privacy\\(^{1}\\), aggregation, and reducing detail provide alternatives to balance privacy and utility, but have downsides. Creators must strike a thoughtful balance based on use case.\nLooking beyond privacy, creators need to proactively assess and address representation gaps that could introduce model biases. It is crucial yet insufficient to ensure diversity across individual variables like gender, race, and accent. Combinations of characteristics also require assessment, as models can struggle when certain intersections are absent. For example, a medical dataset could have balanced gender, age, and diagnosis data individually, but lack enough cases capturing elderly women with a specific condition. Such higher-order gaps are not immediately obvious but can critically impact model performance.\nCreating useful, ethical training data requires holistic consideration of privacy risks and representation gaps. Perfect solutions are elusive. However, conscientious data engineering practices like anonymization, aggregation, undersampling overrepresented groups, and synthesized data generation can help balance competing needs. This facilitates models that are both accurate and socially responsible. Cross-functional collaboration and external audits can also strengthen training data. The challenges are multifaceted, but surmountable with thoughtful effort.\nWe begin by discussing data collection: Where do we source data, and how do we gather it? Options range from scraping the web, accessing APIs, utilizing sensors and IoT devices, to conducting surveys and gathering user input. These methods reflect real-world practices. Next, we delve into data labeling, including considerations for human involvement. We‚Äôll discuss the trade-offs and limitations of human labeling and explore emerging methods for automated labeling. Following that, we‚Äôll address data cleaning and preprocessing, a crucial yet frequently undervalued step in preparing raw data for AI model training. Data augmentation comes next, a strategy for enhancing limited datasets by generating synthetic samples. This is particularly pertinent for embedded systems, as many use cases don‚Äôt have extensive data repositories readily available for curation. Synthetic data generation emerges as a viable alternative, though it comes with its own set of advantages and disadvantages. We‚Äôll also touch upon dataset versioning, emphasizing the importance of tracking data modifications over time. Data is ever-evolving; hence, it‚Äôs imperative to devise strategies for managing and storing expansive datasets. By the end of this section, you‚Äôll possess a comprehensive understanding of the entire data pipeline, from collection to storage, essential for operationalizing AI systems. Let‚Äôs embark on this journey!"
  },
  {
    "objectID": "data_engineering.html#problem-definition",
    "href": "data_engineering.html#problem-definition",
    "title": "6¬† Data Engineering",
    "section": "6.2 Problem Definition",
    "text": "6.2 Problem Definition\nIn many domains of machine learning, while sophisticated algorithms take center stage, the fundamental importance of data quality is often overlooked. This neglect gives rise to ‚ÄúData Cascades‚Äù ‚Äî events where lapses in data quality compound, leading to negative downstream consequences such as flawed predictions, project terminations, and even potential harm to communities.\n\n\n\nA visual representation of the stages in the machine learning pipeline and the potential pitfalls, illustrating how data quality lapses can lead to cascading negative consequences throughout the process.\n\n\nDespite many ML professionals recognizing the importance of data, numerous practitioners report facing these cascades. This highlights a systemic issue: while the allure of developing advanced models remains, data is often underappreciated.\nTake, for example, Keyword Spotting (KWS). KWS serves as a prime example of TinyML in action and is a critical technology behind voice-enabled interfaces on endpoint devices such as smartphones. Typically functioning as lightweight wake-word engines, these systems are consistently active, listening for a specific phrase to trigger further actions. When we say the phrases ‚ÄúOk Google‚Äù or ‚ÄúAlexa,‚Äù this initiates a process on a microcontroller embedded within the device. Despite their limited resources, these microcontrollers play a pivotal role in enabling seamless voice interactions with devices, often operating in environments with high levels of ambient noise. The uniqueness of the wake-word helps minimize false positives, ensuring that the system is not triggered inadvertently.\nIt is important to appreciate that these keyword spotting technologies are not isolated; they integrate seamlessly into larger systems, processing signals continuously while managing low power consumption. These systems extend beyond simple keyword recognition, evolving to facilitate diverse sound detections, such as the breaking of glass. This evolution is geared towards creating intelligent devices capable of understanding and responding to a myriad of vocal commands, heralding a future where even household appliances can be controlled through voice interactions.\n\n\n\nThe seamless integration of Keyword Spotting technology allows users to command their devices with simple voice prompts, even in ambient noise environments.\n\n\nBuilding a reliable KWS model is not a straightforward task. It demands a deep understanding of the deployment scenario, encompassing where and how these devices will operate. For instance, a KWS model‚Äôs effectiveness is not just about recognizing a word; it‚Äôs about discerning it among various accents and background noises, whether in a bustling cafe or amid the blaring sound of a television in a living room or a kitchen where these devices are commonly found. It‚Äôs about ensuring that a whispered ‚ÄúAlexa‚Äù in the dead of night or a shouted ‚ÄúOk Google‚Äù in a noisy marketplace are both recognized with equal precision.\nMoreover, many of the current KWS voice assistants support a limited number of languages, leaving a substantial portion of the world‚Äôs linguistic diversity unrepresented. This limitation is partly due to the difficulty in gathering and monetizing data for languages spoken by smaller populations. The long-tail distribution of languages implies that many languages have limited data available, making the development of supportive technologies challenging.\nThis level of accuracy and robustness hinges on the availability of data, quality of data, ability to label the data correctly, and ensuring transparency of the data for the end user‚Äîall before the data is used to train the model. But it all begins with a clear understanding of the problem statement or definition.\nGenerally, in ML, problem definition has a few key steps:\n\nIdentifying the problem definition clearly\nSetting clear objectives\nEstablishing success benchmark\nUnderstanding end-user engagement/use\nUnderstanding the constraints and limitations of deployment\nFollowed by finally doing the data collection.\n\nLaying a solid foundation for a project is essential for its trajectory and eventual success. Central to this foundation is first identifying a clear problem, such as ensuring that voice commands in voice assistance systems are recognized consistently across varying environments. Clear objectives, like creating representative datasets for diverse scenarios, provide a unified direction. Benchmarks, such as system accuracy in keyword detection, offer measurable outcomes to gauge progress. Engaging with stakeholders, from end-users to investors, provides invaluable insights and ensures alignment with market needs. Additionally, when delving into areas like voice assistance, understanding platform constraints is pivotal. Embedded systems, such as microcontrollers, come with inherent limitations in processing power, memory, and energy efficiency. Recognizing these limitations ensures that functionalities, like keyword detection, are tailored to operate optimally, balancing performance with resource conservation.\nIn this context, using KWS as an example, we can break each of the steps out as follows:\n\nIdentifying the Problem: At its core, KWS aims to detect specific keywords amidst a sea of ambient sounds and other spoken words. The primary problem is to design a system that can recognize these keywords with high accuracy, low latency, and minimal false positives or negatives, especially when deployed on devices with limited computational resources.\nSetting Clear Objectives: The objectives for a KWS system might include:\n\nAchieving a specific accuracy rate (e.g., 98% accuracy in keyword detection).\nEnsuring low latency (e.g., keyword detection and response within 200 milliseconds).\nMinimizing power consumption to extend battery life on embedded devices.\nEnsuring the model‚Äôs size is optimized for the available memory on the device.\n\nBenchmarks for Success: Establish clear metrics to measure the success of the KWS system. This could include:\n\nTrue Positive Rate: The percentage of correctly identified keywords.\nFalse Positive Rate: The percentage of non-keywords incorrectly identified as keywords.\nResponse Time: The time taken from keyword utterance to system response.\nPower Consumption: Average power used during keyword detection.\n\nStakeholder Engagement and Understanding: Engage with stakeholders, which might include device manufacturers, hardware and software developers, and end-users. Understand their needs, capabilities, and constraints. For instance:\n\nDevice manufacturers might prioritize low power consumption.\nSoftware developers might emphasize ease of integration.\nEnd-users would prioritize accuracy and responsiveness.\n\nUnderstanding the Constraints and Limitations of Embedded Systems: Embedded devices come with their own set of challenges:\n\nMemory Limitations: KWS models need to be lightweight to fit within the memory constraints of embedded devices. Typically, KWS models might need to be as small as 16KB to fit in the always-on island of the SoC. Moreover, this is just the model size. Additional application code for pre-processing may also need to fit within the memory constraints.\nProcessing Power: The computational capabilities of embedded devices are limited (few hundred MHz of clock speed), so the KWS model must be optimized for efficiency.\nPower Consumption: Since many embedded devices are battery-powered, the KWS system must be power-efficient.\nEnvironmental Challenges: Devices might be deployed in various environments, from quiet bedrooms to noisy industrial settings. The KWS system must be robust enough to function effectively across these scenarios.\n\nData Collection and Analysis: For a KWS system, the quality and diversity of data are paramount. Considerations might include:\n\nVariety of Accents: Collect data from speakers with various accents to ensure wide-ranging recognition.\nBackground Noises: Include data samples with different ambient noises to train the model for real-world scenarios.\nKeyword Variations: People might either pronounce keywords differently or have slight variations in the wake word itself. Ensure the dataset captures these nuances.\n\nIterative Feedback and Refinement: Once a prototype KWS system is developed, it‚Äôs crucial to test it in real-world scenarios, gather feedback, and iteratively refine the model. This ensures that the system remains aligned with the defined problem and objectives. This is important because the deployment scenarios change over time as things evolve."
  },
  {
    "objectID": "data_engineering.html#data-sourcing",
    "href": "data_engineering.html#data-sourcing",
    "title": "6¬† Data Engineering",
    "section": "6.3 Data Sourcing",
    "text": "6.3 Data Sourcing\nThe quality and diversity of data gathered is important for developing accurate and robust AI systems. Sourcing high-quality training data requires careful consideration of the objectives, resources, and ethical implications. Data can be obtained from various sources depending on the needs of the project:\n\n6.3.1 Pre-existing datasets\nPlatforms like Kaggle and UCI Machine Learning Repository provide a convenient starting point. Pre-existing datasets are a valuable resource for researchers, developers, and businesses alike. One of their primary advantages is cost-efficiency. Creating a dataset from scratch can be both time-consuming and expensive, so having access to ready-made data can save significant resources. Moreover, many of these datasets, like ImageNet, have become standard benchmarks in the machine learning community, allowing for consistent performance comparisons across different models and algorithms. This availability of data means that experiments can be started immediately without any delays associated with data collection and preprocessing. In a fast moving field like ML, this expediency is important.\nThe quality assurance that comes with popular pre-existing datasets is important to consider because several datasets have errors in them. For instance, the ImageNet dataset was found to have over 6.4% errors. Given their widespread use, any errors or biases in these datasets are often identified and rectified by the community. This assurance is especially beneficial for students and newcomers to the field, as they can focus on learning and experimentation without worrying about data integrity. Supporting documentation that often accompanies existing datasets is invaluable, though this generally applies only to widely used datasets. Good documentation provides insights into the data collection process, variable definitions, and sometimes even offers baseline model performances. This information not only aids understanding but also promotes reproducibility in research, a cornerstone of scientific integrity; currently there is a crisis around improving reproducibility in machine learning systems. When other researchers have access to the same data, they can validate findings, test new hypotheses, or apply different methodologies, thus allowing us to build on each other‚Äôs work more rapidly.\nWhile platforms like Kaggle and UCI Machine Learning Repository are invaluable resources, it‚Äôs essential to understand the context in which the data was collected. Researchers should be wary of potential overfitting when using popular datasets, as multiple models might have been trained on them, leading to inflated performance metrics. Sometimes these datasets do not reflect the real-world data.\nIn addition, bias, validity, and reproducibility issues may exist in these datasets and in recent years there is a growing awareness of these issues.\n\n\n6.3.2 Web Scraping\nWeb scraping refers to automated techniques for extracting data from websites. It typically involves sending HTTP requests to web servers, retrieving HTML content, and parsing that content to extract relevant information. Popular tools and frameworks for web scraping include Beautiful Soup, Scrapy, and Selenium. These tools offer different functionalities, from parsing HTML content to automating web browser interactions, especially for websites that load content dynamically using JavaScript.\nWeb scraping can be an effective way to gather large datasets for training machine learning models, particularly when human-labeled data is scarce. For computer vision research, web scraping enables the collection of massive volumes of images and videos. Researchers have used this technique to build influential datasets like ImageNet and OpenImages. For example, one could scrape e-commerce sites to amass product photos for object recognition, or social media platforms to collect user uploads for facial analysis. Even before ImageNet, Stanford‚Äôs LabelMe project scraped Flickr for over 63,000 annotated images covering hundreds of object categories.\nBeyond computer vision, web scraping supports the gathering of textual data for natural language tasks. Researchers can scrape news sites for sentiment analysis data, forums, and review sites for dialogue systems research, or social media for topic modeling. For example, the training data for chatbot ChatGPT was obtained by scraping much of the public internet. GitHub repositories were scraped to train GitHub‚Äôs Copilot AI coding assistant.\nWeb scraping can also collect structured data like stock prices, weather data, or product information for analytical applications. Once data is scraped, it is essential to store it in a structured manner, often using databases or data warehouses. Proper data management ensures the usability of the scraped data for future analysis and applications.\nHowever, while web scraping offers numerous advantages, there are significant limitations and ethical considerations to bear in mind. Not all websites permit scraping, and violating these restrictions can lead to legal repercussions. It is also unethical and potentially illegal to scrape copyrighted material or private communications. Ethical web scraping mandates adherence to a website‚Äôs ‚Äòrobots.txt‚Äô file, which outlines the sections of the site that can be accessed and scraped by automated bots. To deter automated scraping, many websites implement rate limits. If a bot sends too many requests in a short period, it might be temporarily blocked, restricting the speed of data access. Additionally, the dynamic nature of web content means that data scraped at different intervals might lack consistency, posing challenges for longitudinal studies. Though there are emerging trends like Web Navigation where machine learning algorithms can automatically navigate the website to access the dynamic content.\nFor niche subjects, the volume of pertinent data available for scraping might be limited. For example, while scraping for common topics like images of cats and dogs might yield abundant data, searching for rare medical conditions might not be as fruitful. Moreover, the data obtained through scraping is often unstructured and noisy, necessitating thorough preprocessing and cleaning. It is crucial to understand that not all scraped data will be of high quality or accuracy. Employing verification methods, such as cross-referencing with alternate data sources, can enhance data reliability.\nPrivacy concerns arise when scraping personal data, emphasizing the need for anonymization. Therefore, it is paramount to adhere to a website‚Äôs Terms of Service, confine data collection to public domains, and ensure the anonymity of any personal data acquired.\nWhile web scraping can be a scalable method to amass large training datasets for AI systems, its applicability is confined to specific data types. For example, sourcing data for Inertial Measurement Units (IMU) for gesture recognition is not straightforward through web scraping. At most, one might be able to scrape an existing dataset.\n\n\n6.3.3 Crowdsourcing\nCrowdsourcing for datasets is the practice of obtaining data by using the services of a large number of people, either from a specific community or the general public, typically via the internet. Instead of relying on a small team or specific organization to collect or label data, crowdsourcing leverages the collective effort of a vast, distributed group of participants. Services like Amazon Mechanical Turk enable the distribution of annotation tasks to a large, diverse workforce. This facilitates the collection of labels for complex tasks like sentiment analysis or image recognition that specifically require human judgment.\nCrowdsourcing has emerged as an effective approach for many data collection and problem-solving needs. One major advantage of crowdsourcing is scalability‚Äîby distributing tasks to a large, global pool of contributors on digital platforms, projects can process huge volumes of data in a short timeframe. This makes crowdsourcing ideal for large-scale data labeling, collection, and analysis.\nIn addition, crowdsourcing taps into a diverse group of participants, bringing a wide range of perspectives, cultural insights, and language abilities that can enrich data and enhance creative problem-solving in ways that a more homogenous group may not. Because crowdsourcing draws from a large audience beyond traditional channels, it also tends to be more cost-effective than conventional methods, especially for simpler microtasks.\nCrowdsourcing platforms also allow for great flexibility, as task parameters can be adjusted in real-time based on initial results. This creates a feedback loop for iterative improvements to the data collection process. Complex jobs can be broken down into microtasks and distributed to multiple people, with cross-validation of results by assigning redundant versions of the same task. Ultimately, when thoughtfully managed, crowdsourcing enables community engagement around a collaborative project, where participants find reward in contributing.\nHowever, while crowdsourcing offers numerous advantages, it‚Äôs essential to approach it with a clear strategy. While it provides access to a diverse set of annotators, it also introduces variability in the quality of annotations. Additionally, platforms like Mechanical Turk might not always capture a complete demographic spectrum; often tech-savvy individuals are overrepresented, while children and the elderly may be underrepresented. It‚Äôs crucial to provide clear instructions and possibly even training for the annotators. Periodic checks and validations of the labeled data can help maintain quality. This ties back to the topic of clear Problem Definition that we discussed earlier. Crowdsourcing for datasets also requires careful attention to ethical considerations. It‚Äôs crucial to ensure that participants are informed about how their data will be used and that their privacy is protected. Quality control through detailed protocols, transparency in sourcing, and auditing is essential to ensure reliable outcomes.\nFor TinyML, crowdsourcing can pose some unique challenges. TinyML devices are highly specialized for particular tasks within tight constraints. As a result, the data they require tends to be very specific. It may be difficult to obtain such specialized data from a general audience through crowdsourcing. For example, TinyML applications often rely on data collected from certain sensors or hardware. Crowdsourcing would require participants to have access to very specific and consistent devices - like microphones with the same sampling rates. Even for simple audio tasks like keyword spotting, these hardware nuances present obstacles.\nBeyond hardware, the data itself needs high granularity and quality given the limitations of TinyML. It can be hard to ensure this when crowdsourcing from those unfamiliar with the application‚Äôs context and requirements. There are also potential issues around privacy, real-time collection, standardization, and technical expertise. Moreover, the narrow nature of many TinyML tasks makes accurate data labeling difficult without the proper understanding. Participants may struggle to provide reliable annotations without full context.\nThus, while crowdsourcing can work well in many cases, the specialized needs of TinyML introduce unique data challenges. Careful planning is required for guidelines, targeting, and quality control. For some applications, crowdsourcing may be feasible, but others may require more focused data collection efforts to obtain relevant, high-quality training data.\n\n\n6.3.4 Synthetic Data\nSynthetic data generation can be useful for addressing some of the limitations of data collection. It involves creating data that wasn‚Äôt originally captured or observed, but is generated using algorithms, simulations, or other techniques to resemble real-world data. It has become a valuable tool in various fields, particularly in scenarios where real-world data is scarce, expensive, or ethically challenging to obtain (e.g., TinyML). Various techniques, such as Generative Adversarial Networks (GANs), can produce high-quality synthetic data that is almost indistinguishable from real data. These techniques have advanced significantly, making synthetic data generation increasingly realistic and reliable.\nIn many domains, especially emerging ones, there may not be enough real-world data available for analysis or training machine learning models. Synthetic data can fill this gap by producing large volumes of data that mimic real-world scenarios. For instance, detecting the sound of breaking glass might be challenging in security applications where a TinyML device is trying to identify break-ins. Collecting real-world data would require breaking numerous windows, which is impractical and costly.\nMoreover, in machine learning, especially in deep learning, having a diverse dataset is crucial. Synthetic data can augment existing datasets by introducing variations, thereby enhancing the robustness of models. For example, SpecAugment is an excellent data augmentation technique for Automatic Speech Recognition (ASR) systems.\nPivacy and confidentiality is also a big issue. Datasets containing sensitive or personal information pose privacy concerns when shared or used. Synthetic data, being artificially generated, doesn‚Äôt have these direct ties to real individuals, allowing for safer use while preserving essential statistical properties.\nGenerating synthetic data, especially once the generation mechanisms have been established, can be a more cost-effective alternative. In the aforementioned security application scenario, synthetic data eliminates the need for breaking multiple windows to gather relevant data.\nMany embedded use-cases deal with unique situations, such as manufacturing plants, that are difficult to simulate. Synthetic data allows researchers complete control over the data generation process, enabling the creation of specific scenarios or conditions that are challenging to capture in real life.\nWhile synthetic data offers numerous advantages, it is essential to use it judiciously. Care must be taken to ensure that the generated data accurately represents the underlying real-world distributions and does not introduce unintended biases."
  },
  {
    "objectID": "data_engineering.html#data-storage",
    "href": "data_engineering.html#data-storage",
    "title": "6¬† Data Engineering",
    "section": "6.4 Data Storage",
    "text": "6.4 Data Storage\nData sourcing and data storage go hand-in-hand and it is necessary to store data in a format that facilitates easy access and processing. Depending on the use case, there are various kinds of data storage systems that can be used to store your datasets.\n\n\n\n\n\n\n\n\n\n\nDatabase\nData Warehouse\nData Lake\n\n\n\n\nPurpose\nOperational and transactional\nAnalytical\nAnalytical\n\n\nData type\nStructured\nStructured\nStructured, semi-structured and/or unstructured\n\n\nScale\nSmall to large volumes of data\nLarge volumes of integrated data\nLarge volumes of diverse data\n\n\nExamples\nMySQL\nGoogle BigQuery, Amazon Redshift, Microsoft Azure Synapse.\nGoogle Cloud Storage, AWS S3, Azure Data Lake Storage\n\n\n\nThe stored data is often accompanied by metadata, which is defined as ‚Äòdata about data‚Äô. It provides detailed contextual information about the data, such as means of data creation, time of creation, attached data use license etc. For example, Hugging Face has Dataset Cards. To promote responsible data use, dataset creators should disclose potential biases through the dataset cards. These cards can educate users about a dataset's contents and limitations. The cards also give vital context on appropriate dataset usage by highlighting biases and other important details. Having this type of metadata can also allow fast retrieval if structured properly. Once the model is developed and deployed to edge devices, the storage systems can continue to store incoming data, model updates or analytical results.\nData Governance: With a large amount of data storage, it is also imperative to have policies and practices (i.e., data governance) that helps manage data during its life cycle, from acquisition to disposal. Data governance frames the way data is managed and includes making pivotal decisions about data access and control. It involves exercising authority and making decisions concerning data, with the aim to uphold its quality, ensure compliance, maintain security, and derive value. Data governance is operationalized through the development of policies, incentives, and penalties, cultivating a culture that perceives data as a valuable asset. Specific procedures and assigned authorities are implemented to safeguard data quality and monitor its utilization and the related risks.\nData governance utilizes three integrative approaches: planning and control, organizational, and risk-based. The planning and control approach, common in IT, aligns business and technology through annual cycles and continuous adjustments, focusing on policy-driven, auditable governance. The organizational approach emphasizes structure, establishing authoritative roles like Chief Data Officers, ensuring responsibility and accountability in governance. The risk-based approach, intensified by AI advancements, focuses on identifying and managing inherent risks in data and algorithms, especially addressing AI-specific issues through regular assessments and proactive risk management strategies, allowing for incidental and preventive actions to mitigate undesired algorithm impacts.\n\n\n\nComprehensive overview of the data governance framework.\n\n\nSome examples of data governance across different sectors include:\n\nMedicine: Health Information Exchanges(HIEs) enable the sharing of health information across different healthcare providers to improve patient care. They implement strict data governance practices to maintain data accuracy, integrity, privacy, and security, complying with regulations such as the Health Insurance Portability and Accountability Act (HIPAA). Governance policies ensure that patient data is only shared with authorized entities and that patients can control access to their information.\nFinance: Basel III Framework is an international regulatory framework for banks. It ensures that banks establish clear policies, practices, and responsibilities for data management, ensuring data accuracy, completeness, and timeliness. Not only does it enable banks to meet regulatory compliance, it also prevents financial crises by more effective management of risks.\nGovernment: Governments agencies managing citizen data, public records, and administrative information implement data governance to manage data transparently and securely. Social Security System in the US, and Aadhar system in India are good examples of such governance systems.\n\nSpecial data storage considerations for tinyML\nEfficient Audio Storage Formats: Keyword spotting systems need specialized audio storage formats to enable quick keyword searching in audio data. Traditional formats like WAV and MP3 store full audio waveforms, which require extensive processing to search through. Keyword spotting uses compressed storage optimized for snippet-based search. One approach is to store compact acoustic features instead of raw audio. Such a workflow would involve:\n\nExtracting acoustic features - Mel-frequency cepstral coefficients (MFCCs) are commonly used to represent important audio characteristics.\nCreating Embeddings- Embeddings transform extracted acoustic features into continuous vector spaces, enabling more compact and representative data storage. This representation is essential in converting high-dimensional data, like audio, into a format that‚Äôs more manageable and efficient for computation and storage.\nVector quantization - This technique is used to represent high-dimensional data, like embeddings, with lower-dimensional vectors, reducing storage needs. Initially, a codebook is generated from the training data to define a set of code vectors representing the original data vectors. Subsequently, each data vector is matched to the nearest codeword according to the codebook, ensuring minimal loss of information.\nSequential storage - The audio is fragmented into short frames, and the quantized features (or embeddings) for each frame are stored sequentially to maintain the temporal order, preserving the coherence and context of the audio data.\n\nThis format enables decoding the features frame-by-frame for keyword matching. Searching the features is faster than decompressing the full audio.\nSelective Network Output Storage: Another technique for reducing storage is to discard the intermediate audio features stored during training, but not required during inference. The network is run on the full audio during training, however, only the final outputs are stored during inference. In a recent study (Rybakov et al.¬†2018), the authors discuss adaptation of the model‚Äôs intermediate data storage structure to incorporate the nature of streaming models that are prevalent in tinyML applications."
  },
  {
    "objectID": "data_engineering.html#data-processing",
    "href": "data_engineering.html#data-processing",
    "title": "6¬† Data Engineering",
    "section": "6.5 Data Processing",
    "text": "6.5 Data Processing\nData processing refers to the steps involved in transforming raw data into a format that is suitable for feeding into machine learning algorithms. It is a crucial stage in any machine learning workflow, yet often overlooked. Without proper data processing, machine learning models are unlikely to achieve optimal performance. ‚ÄúData preparation accounts for about 60-80% of the work of a data scientist.‚Äù\n\n\n\nA breakdown of tasks that data scientists allocate their time to, highlighting the significant portion spent on data cleaning and organizing.\n\n\nProper data cleaning is a crucial step that directly impacts model performance. Real-world data is often dirty - it contains errors, missing values, noise, anomalies, and inconsistencies. Data cleaning involves detecting and fixing these issues to prepare high-quality data for modeling. By carefully selecting appropriate techniques, data scientists can improve model accuracy, reduce overfitting, and enable algorithms to learn more robust patterns. Overall, thoughtful data processing allows machine learning systems to better uncover insights and make predictions from real-world data.\nData often comes from diverse sources and can be unstructured or semi-structured. Thus, it‚Äôs essential to process and standardize it, ensuring it adheres to a uniform format. Such transformations may include:\n\nNormalizing numerical variables\nEncoding categorical variables\nUsing techniques like dimensionality reduction\n\nData validation serves a broader role than just ensuring adherence to certain standards like preventing temperature values from falling below absolute zero. These types of issues arise in TinyML because sensors may malfunction or temporarily produce incorrect readings, such transients are not uncommon. Therefore, it is imperative to catch data errors early before they propagate through the data pipeline. Rigorous validation processes, including verifying the initial annotation practices, detecting outliers, and handling missing values through techniques like mean imputation, contribute directly to the quality of datasets. This, in turn, impacts the performance, fairness, and safety of the models trained on them.\n\n\n\nA detailed overview of the Multilingual Spoken Words Corpus (MSWC) data processing pipeline: from raw audio and text data input, through forced alignment for word boundary estimation, to keyword extraction and model training.\n\n\nLet‚Äôs take a look at an example of a data processing pipeline. In the context of tinyML, the Multilingual Spoken Words Corpus (MSWC) is an example of data processing pipelines‚Äîsystematic and automated workflows for data transformation, storage, and processing. By streamlining the data flow, from raw data to usable datasets, data pipelines enhance productivity and facilitate the rapid development of machine learning models. The MSWC is an expansive and expanding collection of audio recordings of spoken words in 50 different languages, which are collectively used by over 5 billion people. This dataset is intended for academic study and business uses in areas like keyword identification and speech-based search. It is openly licensed under Creative Commons Attribution 4.0 for broad usage.\nThe MSWC used a forced alignment method to automatically extract individual word recordings to train keyword-spotting models from the Common Voice project, which features crowdsourced sentence-level recordings. Forced alignment refers to a group of long-standing methods in speech processing that are used to predict when speech phenomena like syllables, words, or sentences start and end within an audio recording. In the MSWC data, crowd-sourced recordings often feature background noises, such as static and wind. Depending on the model‚Äôs requirements, these noises can be removed or intentionally retained.\nMaintaining the integrity of the data infrastructure is a continuous endeavor. This encompasses data storage, security, error handling, and stringent version control. Periodic updates are crucial, especially in dynamic realms like keyword spotting, to adjust to evolving linguistic trends and device integrations.\nThere is a boom of data processing pipelines, these are commonly found in ML operations toolchains, which we will discuss in the MLOps chapter. Briefly, these include frameworks like MLOps by Google Cloud. It provides methods for automation and monitoring at all steps of ML system construction, including integration, testing, releasing, deployment, and infrastructure management, and there are several mechanisms that specifically focus on data processing which is an integral part of these systems."
  },
  {
    "objectID": "data_engineering.html#data-labeling",
    "href": "data_engineering.html#data-labeling",
    "title": "6¬† Data Engineering",
    "section": "6.6 Data Labeling",
    "text": "6.6 Data Labeling\nData labeling is an important step in creating high-quality training datasets for machine learning models. Labels provide the ground truth information that allows models to learn relationships between inputs and desired outputs. This section covers key considerations around selecting label types, formats, and content to capture the necessary information for given tasks. It discusses common annotation approaches, from manual labeling to crowdsourcing to AI-assisted methods, and best practices for ensuring label quality through training, guidelines, and quality checks. Ethical treatment of human annotators is also something we emphasize. The integration of AI to accelerate and augment human annotation is also explored. Understanding labeling needs, challenges, and strategies is essential for constructing reliable, useful datasets that can train performant, trustworthy machine learning systems.\nLabel Types Labels capture information about key tasks or concepts. Common label types include binary classification, bounding boxes, segmentation masks, transcripts, captions, etc. The choice of label format depends on the use case and resource constraints, as more detailed labels require greater effort to collect (Johnson-Roberson et al. (2017)).\n\nJohnson-Roberson, Matthew, Charles Barto, Rounak Mehta, Sharath Nittur Sridhar, Karl Rosaen, and Ram Vasudevan. 2017. ‚ÄúDriving in the Matrix: Can Virtual Worlds Replace Human-Generated Annotations for Real World Tasks?‚Äù 2017 IEEE International Conference on Robotics and Automation (ICRA). https://doi.org/10.1109/icra.2017.7989092.\n\n\n\nAn overview of common label types.\n\n\nUnless focused on self-supervised learning, a dataset will likely provide labels addressing one or more tasks of interest. Dataset creators must consider what information labels should capture and how they can practically obtain the necessary labels, given their unique resource constraints. Creators must first decide what type(s) of content labels should capture. For example, a creator interested in car detection would want to label cars in their dataset. Still, they might also consider whether to simultaneously collect labels for other tasks that the dataset could potentially be used for in the future, such as pedestrian detection.\nAdditionally, annotators can potentially provide metadata that provides insight into how the dataset represents different characteristics of interest (see: Data Transparency). The Common Voice dataset, for example, includes various types of metadata that provide information about the speakers, recordings, and dataset quality for each language represented (Ardila et al. (2020)). They include demographic splits showing the number of recordings by speaker age range and gender. This allows us to see the breakdown of who contributed recordings for each language. They also include statistics like average recording duration and total hours of validated recordings. These give insights into the nature and size of the datasets for each language. Additionally, quality control metrics like the percentage of recordings that have been validated are useful to know how complete and clean the datasets are. The metadata also includes normalized demographic splits scaled to 100% for comparison across languages. This highlights representation differences between higher and lower resource languages.\n\nArdila, Rosana, Megan Branson, Kelly Davis, Michael Henretty, Michael Kohler, Josh Meyer, Reuben Morais, Lindsay Saunders, Francis M. Tyers, and Gregor Weber. 2020. ‚ÄúCommon Voice: A Massively-Multilingual Speech Corpus.‚Äù Proceedings of the 12th Conference on Language Resources and Evaluation, May, 4218‚Äì22.\nNext, creators must determine the format of those labels. For example, a creator interested in car detection might choose between binary classification labels that say whether a car is present, bounding boxes that show the general locations of any cars, or pixel-wise segmentation labels that show the exact location of each car. Their choice of label format may depend both on their use case and their resource constraints, as finer-grained labels are typically more expensive and time-consuming to acquire.\nAnnotation Methods: Common annotation approaches include manual labeling, crowdsourcing, and semi-automated techniques. Manual labeling by experts yields high quality but lacks scalability. Crowdsourcing enables distributed annotation by non-experts, often through dedicated platforms (Sheng and Zhang (2019)). Weakly supervised and programmatic methods can reduce manual effort by heuristically or automatically generating labels (Ratner et al. (2018)).\n\nSheng, Victor S., and Jing Zhang. 2019. ‚ÄúMachine Learning with Crowdsourcing: A Brief Summary of the Past Research and Future Directions.‚Äù Proceedings of the AAAI Conference on Artificial Intelligence 33 (01): 9837‚Äì43. https://doi.org/10.1609/aaai.v33i01.33019837.\n\nRatner, Alex, Braden Hancock, Jared Dunnmon, Roger Goldman, and Christopher R√©. 2018. ‚ÄúSnorkel Metal: Weak Supervision for Multi-Task Learning.‚Äù Proceedings of the Second Workshop on Data Management for End-To-End Machine Learning. https://doi.org/10.1145/3209889.3209898.\nAfter deciding on their labels‚Äô desired content and format, creators begin the annotation process. To collect large numbers of labels from human annotators, creators frequently rely on dedicated annotation platforms, which can connect them to teams of human annotators. When using these platforms, creators may have little insight to annotators‚Äô backgrounds and levels of experience with topics of interest. However, some platforms offer access to annotators with specific expertise (e.g.¬†doctors).\nEnsuring Label Quality: There is no guarantee that the data labels are actually correct. It is possible that despite the best instructions being given to labelers, they still mislabel some images (Northcutt, Athalye, and Mueller (2021)). Strategies like quality checks, training annotators, and collecting multiple labels per datapoint can help ensure label quality. For ambiguous tasks, multiple annotators can help identify controversial datapoints and quantify disagreement levels.\n\nNorthcutt, Curtis G, Anish Athalye, and Jonas Mueller. 2021. ‚ÄúPervasive Label Errors in Test Sets Destabilize Machine Learning Benchmarks.‚Äù arXiv, March. https://doi.org/&nbsp; https://doi.org/10.48550/arXiv.2103.14749 arXiv-issued DOI via DataCite.\n\n\n\nSome examples of hard labeling cases.\n\n\nWhen working with human annotators, it is important to offer fair compensation and otherwise prioritize ethical treatment, as annotators can be exploited or otherwise harmed during the labeling process (Perrigo, 2023). For example, if a dataset is likely to contain disturbing content, annotators may benefit from having the option to view images in grayscale (Google (n.d.)).\n\nGoogle. n.d. ‚ÄúInformation Quality & Content Moderation.‚Äù https://blog.google/documents/83/.\nAI-Assisted Annotation: ML has an insatiable demand for data. Therefore, no amount of data is sufficient data. This raises the question of how we can get more labeled data. Rather than always generating and curating data manually, we can rely on existing AI models to help label datasets more quickly and cheaply, though often with lower quality than human annotation. This can be done in various ways, such as the following:\n\nPre-annotation: AI models can generate preliminary labels for a dataset using methods such as semi-supervised learning (Chapelle, Scholkopf, and Zien (2009)), which humans can then review and correct. This can save a significant amount of time, especially for large datasets.\nActive learning: AI models can identify the most informative data points in a dataset, which can then be prioritized for human annotation. This can help improve the labeled dataset‚Äôs quality while reducing the overall annotation time.\nQuality control: AI models can be used to identify and flag potential errors in human annotations. This can help to ensure the accuracy and consistency of the labeled dataset.\n\n\nChapelle, O., B. Scholkopf, and A. Zien Eds. 2009. ‚ÄúSemi-Supervised Learning (Chapelle, o. Et Al., Eds.; 2006) [Book Reviews].‚Äù IEEE Transactions on Neural Networks 20 (3): 542‚Äì42. https://doi.org/10.1109/tnn.2009.2015974.\nHere are some examples of how AI-assisted annotation has been proposed to be useful:\n\nMedical imaging: AI-assisted annotation is being used to label medical images, such as MRI scans and X-rays (Krishnan, Rajpurkar, and Topol (2022)). Carefully annotating medical datasets is extremely challenging, especially at scale, since domain experts are both scarce and it becomes a costly effort. This can help to train AI models to diagnose diseases and other medical conditions more accurately and efficiently.\n\nSelf-driving cars: AI-assisted annotation is being used to label images and videos from self-driving cars. This can help to train AI models to identify objects on the road, such as other vehicles, pedestrians, and traffic signs.\nSocial media: AI-assisted annotation is being used to label social media posts, such as images and videos. This can help to train AI models to identify and classify different types of content, such as news, advertising, and personal posts.\n\n\nKrishnan, Rayan, Pranav Rajpurkar, and Eric J. Topol. 2022. ‚ÄúSelf-Supervised Learning in Medicine and Healthcare.‚Äù Nature Biomedical Engineering 6 (12): 1346‚Äì52. https://doi.org/10.1038/s41551-022-00914-1.\n\n\n\nStrategies for acquiring additional labeled training data in machine learning."
  },
  {
    "objectID": "data_engineering.html#data-version-control",
    "href": "data_engineering.html#data-version-control",
    "title": "6¬† Data Engineering",
    "section": "6.7 Data Version Control",
    "text": "6.7 Data Version Control\nProduction systems are perpetually inundated with fluctuating and escalating volumes of data, prompting the rapid emergence of numerous data replicas. This proliferating data serves as the foundation for training machine learning models. For instance, a global sales company engaged in sales forecasting continuously receives consumer behavior data. Similarly, healthcare systems formulating predictive models for disease diagnosis are consistently acquiring new patient data. TinyML applications, such as keyword spotting, are highly data hungry in terms of the amount of data generated. Consequently, meticulous tracking of data versions and the corresponding model performance is imperative.\nData Version Control offers a structured methodology to handle alterations and versions of datasets efficiently. It facilitates the monitoring of modifications, preserves multiple versions, and guarantees reproducibility and traceability in data-centric projects. Furthermore, data version control provides the versatility to review and utilize specific versions as needed, ensuring that each stage of the data processing and model development can be revisited and audited with precision and ease. It has a variety of practical uses -\nRisk Management: Data version control allows transparency and accountability by tracking versions of the dataset.\nCollaboration and Efficiency: Easy access to different versions of the dataset in one place can improve data sharing of specific checkpoints, and enable efficient collaboration.\nReproducibility: Data version control allows for tracking the performance of models with respect to different versions of the data, and therefore enabling reproducibility.\nKey Concepts\n\nCommits: It is an immutable snapshot of the data at a specific point in time, representing a unique version. Every commit is associated with a unique identifier to allow\nBranches: Branching allows developers and data scientists to diverge from the main line of development and continue to work independently without affecting other branches. This is especially useful when experimenting with new features or models, enabling parallel development and experimentation without the risk of corrupting the stable, main branch.\nMerges: Merges help to integrate changes from different branches while maintaining the integrity of the data.\n\nPopular Data Version Control Systems\nDVC: It stands for Data Version Control in short, and is an open-source, lightweight tool that works on top of github and supports all kinds of data format. It can seamlessly integrate into the Git workflow, if Git is being used for managing code. It captures the versions of data and models in the Git commits, while storing them on premises or on cloud (e.g.¬†AWS, Google Cloud, Azure). These data and models (e.g.¬†ML artifacts) are defined in the metadata files, which get updated in every commit. It can allow metrics tracking of models on different versions of the data.\nlakeFS: It is an open-source tool that supports the data version control on data lakes. It supports many git-like operations such as branching and merging of data, as well as reverting to previous versions of the data. It also has a unique UI feature which allows exploration and management of data much easier.\nGit LFS: It is useful for data version control on smaller sized datasets. It uses Git‚Äôs inbuilt branching and merging features, but is limited in terms of tracking metrics, reverting to previous versions or integration with data lakes."
  },
  {
    "objectID": "data_engineering.html#optimizing-data-for-embedded-ai",
    "href": "data_engineering.html#optimizing-data-for-embedded-ai",
    "title": "6¬† Data Engineering",
    "section": "6.8 Optimizing Data for Embedded AI",
    "text": "6.8 Optimizing Data for Embedded AI\nCreators working on embedded systems may have unusual priorities when cleaning their datasets. On the one hand, models may be developed for unusually specific use cases, requiring heavy filtering of datasets. While other natural language models may be capable of turning any speech to text, a model for an embedded system may be focused on a single limited task, such as detecting a keyword. As a result, creators may aggressively filter out large amounts of data because they do not address the task of interest. Additionally, an embedded AI system may be tied to specific hardware devices or environments. For example, a video model may need to process images from a single type of camera, which will only be mounted on doorbells in residential neighborhoods. In this scenario, creators may discard images if they came from a different kind of camera, show the wrong type of scenery, or were taken from the wrong height or angle.\nOn the other hand, embedded AI systems are often expected to provide especially accurate performance in unpredictable real-world settings. As a result, creators may design datasets specifically to represent variations in potential inputs and promote model robustness. As a result, they may define a narrow scope for their project but then aim for deep coverage within those bounds. For example, creators of the doorbell model mentioned above might try to cover variations in data arising from:\n\nGeographically, socially and architecturally diverse neighborhoods\nDifferent types of artificial and natural lighting\nDifferent seasons and weather conditions\nObstructions (e.g.¬†raindrops or delivery boxes obscuring the camera‚Äôs view)\n\nAs described above, creators may consider crowdsourcing or synthetically generating data to include these different kinds of variations."
  },
  {
    "objectID": "data_engineering.html#data-transparency",
    "href": "data_engineering.html#data-transparency",
    "title": "6¬† Data Engineering",
    "section": "6.9 Data Transparency",
    "text": "6.9 Data Transparency\nBy providing clear, detailed documentation, creators can help developers understand how best to use their datasets. Several groups have suggested standardized documentation formats for datasets, such as Data Cards (Pushkarna, Zaldivar, and Kjartansson (2022)), datasheets (Gebru et al. (2021)), data statements (Bender and Friedman (2018)), or Data Nutrition Labels (Holland et al. (2020)). When releasing a dataset, creators may describe what kinds of data they collected, how they collected and labeled it, and what kinds of use cases may be a good or poor fit for the dataset. Quantitatively, it may be appropriate to provide a breakdown of how well the dataset represents different groups (e.g.¬†different gender groups, different cameras).\n\nPushkarna, Mahima, Andrew Zaldivar, and Oddur Kjartansson. 2022. ‚ÄúData Cards: Purposeful and Transparent Dataset Documentation for Responsible Ai.‚Äù 2022 ACM Conference on Fairness, Accountability, and Transparency. https://doi.org/10.1145/3531146.3533231.\n\nGebru, Timnit, Jamie Morgenstern, Briana Vecchione, Jennifer Wortman Vaughan, Hanna Wallach, Hal Daum√© III, and Kate Crawford. 2021. ‚ÄúDatasheets for Datasets.‚Äù Communications of the ACM 64 (12): 86‚Äì92. https://doi.org/10.1145/3458723.\n\nBender, Emily M., and Batya Friedman. 2018. ‚ÄúData Statements for Natural Language Processing: Toward Mitigating System Bias and Enabling Better Science.‚Äù Transactions of the Association for Computational Linguistics 6: 587‚Äì604. https://doi.org/10.1162/tacl_a_00041.\n\nHolland, Sarah, Ahmed Hosny, Sarah Newman, Joshua Joseph, and Kasia Chmielinski. 2020. ‚ÄúThe Dataset Nutrition Label.‚Äù Data Protection and Privacy. https://doi.org/10.5040/9781509932771.ch-001.\nKeeping track of data provenance‚Äîessentially the origins and the journey of each data point through the data pipeline‚Äîis not merely a good practice but an essential requirement for data quality. Data provenance contributes significantly to the transparency of machine learning systems. Transparent systems make it easier to scrutinize data points, enabling better identification and rectification of errors, biases, or inconsistencies. For instance, if a ML model trained on medical data is underperforming in particular areas, tracing back the data provenance can help identify whether the issue is with the data collection methods, the demographic groups represented in the data, or other factors. This level of transparency doesn‚Äôt just help in debugging the system but also plays a crucial role in enhancing the overall data quality. By improving the reliability and credibility of the dataset, data provenance also enhances the model‚Äôs performance and its acceptability among end-users.\nWhen producing documentation, creators should also clearly specify how users can access the dataset and how the dataset will be maintained over time. For example, users may need to undergo training or receive special permission from the creators before accessing a dataset containing protected information, as is the case with many medical datasets. In some cases, users may not be permitted to directly access the data and must instead submit their model to be trained on the dataset creators‚Äô hardware, following a federated learning setup (Aledhari et al. (2020)). Creators may also describe how long the dataset will remain accessible, how the users can submit feedback on any errors that they discover, and whether there are plans to update the dataset.\n\nAledhari, Mohammed, Rehma Razzak, Reza M. Parizi, and Fahad Saeed. 2020. ‚ÄúFederated Learning: A Survey on Enabling Technologies, Protocols, and Applications.‚Äù IEEE Access 8: 140699‚Äì725. https://doi.org/10.1109/access.2020.3013541.\nSome laws and regulations promote also data transparency through new requirements for organizations:\n\nGeneral Data Protection Regulation (GDPR) in European Union: It establishes strict requirements for processing and protecting personal data of EU citizens. It mandates plain language privacy policies that clearly explain what data is collected, why it is used, how long it is stored, and with whom it is shared. GDPR also mandates that privacy notices must include details on legal basis for processing, data transfers, retention periods, rights to access and deletion, and contact info for data controllers.\nCalifornia‚Äôs Consumer Privacy Act (CCPA): CCPA requires clear privacy policies and opt-out rights for the sale of personal data. Significantly, it also establishes rights for consumers to request their specific data be disclosed. Businesses must provide copies of collected personal information along with details on what it is used for, what categories are collected, and what third parties receive it. Consumers can identify data points they believe are inaccurate. The law represents a major step forward in empowering personal data access.\n\nThere are several current challenges in ensuring data transparency, especially because it requires significant time and financial resources. Data systems are also quite complex, and full transparency can be difficult to achieve in these cases. Full transparency may also overwhelm the consumers with too much detail. And finally, it is also important to balance the tradeoff between transparency and privacy."
  },
  {
    "objectID": "data_engineering.html#licensing",
    "href": "data_engineering.html#licensing",
    "title": "6¬† Data Engineering",
    "section": "6.10 Licensing",
    "text": "6.10 Licensing\nMany high-quality datasets either come from proprietary sources or contain copyrighted information. This introduces licensing as a challenging legal domain. Companies eager to train ML systems must engage in negotiations to obtain licenses that grant legal access to these datasets. Furthermore, licensing terms can impose restrictions on data applications and sharing methods. Failure to comply with these licenses can have severe consequences.\nFor instance, ImageNet, one of the most extensively utilized datasets for computer vision research, is a case in point. A majority of its images were procured from public online sources without obtaining explicit permissions, sparking ethical concerns (Prabhu and Birhane, 2020). Accessing the ImageNet dataset for corporations requires registration and adherence to its terms of use, which restricts commercial usage (ImageNet, 2021). Major players like Google and Microsoft invest significantly in licensing datasets to enhance their ML vision systems. However, the cost factor restricts accessibility for researchers from smaller companies with constrained budgets.\nThe legal domain of data licensing has seen major cases that help define parameters of fair use. A prominent example is Authors Guild, Inc.¬†v. Google, Inc. This 2005 lawsuit alleged that Google's book scanning project infringed copyrights by displaying snippets without permission. However, the courts ultimately ruled in Google's favor, upholding fair use based on the transformative nature of creating a searchable index and showing limited text excerpts. This precedent provides some legal grounds for arguing fair use protections apply to indexing datasets and generating representative samples for machine learning. However, restrictions specified in licenses remain binding, so comprehensive analysis of licensing terms is critical. The case demonstrates why negotiations with data providers are important to enable legal usage within acceptable bounds.\nNew Data Regulations and Their Implications\nNew data regulations also impact licensing practices. The legislative landscape is evolving with regulations like the EU‚Äôs Artificial Intelligence Act, which is poised to regulate AI system development and use within the European Union (EU). This legislation:\n\nClassifies AI systems by risk.\nMandates development and usage prerequisites.\nEmphasizes data quality, transparency, human oversight, and accountability.\n\nAdditionally, the EU Act addresses the ethical dimensions and operational challenges in sectors such as healthcare and finance. Key elements include the prohibition of AI systems posing \"unacceptable\" risks, stringent conditions for high-risk systems, and minimal obligations for \"limited risk\" AI systems. The proposed European AI Board will oversee and ensure efficient regulation implementation.\nChallenges in Assembling ML Training Datasets\nComplex licensing issues around proprietary data, copyright law, and privacy regulations all constrain options for assembling ML training datasets. But expanding accessibility through more open licensing or public-private data collaborations could greatly accelerate industry progress and ethical standards.\nIn some cases, certain portions of a dataset may need to be removed or obscured in order to comply with data usage agreements or protect sensitive information. For example, a dataset of user information may have names, contact details, and other identifying data that may need to be removed from the dataset, this is well after the dataset has already been actively sourced and used for training models. Similarly, a dataset that includes copyrighted content or trade secrets may need to have those portions filtered out before being distributed. Laws such as the General Data Protection Regulation (GDPR), the California Consumer Privacy Act (CCPA), and the Amended Act on the Protection of Personal Information (APPI) have been passed to guarantee the right to be forgotten. These regulations legally require model providers to erase user data upon request.\nData collectors and providers need to be able to take appropriate measures to de-identify or filter out any proprietary, licensed, confidential, or regulated information as needed. In some cases, the users may explicitly request that their data be removed.\nHaving the ability to update the dataset by removing data from the dataset will enable the dataset creators to uphold legal and ethical obligations around data usage and privacy. However, the ability to remove data has some important limitations. We need to think about the fact that some models may have already been trained on the dataset and there is no clear or known way to eliminate a particular data sample's effect from the trained network. There is no erase mechanism. Thus, this begs the question, should the model be re-trained from scratch each time a sample is removed? That's a costly option. Once data has been used to train a model, simply removing it from the original dataset may not fully eliminate its impact on the model's behavior. New research is needed around the effects of data removal on already-trained models and whether full retraining is necessary to avoid retaining artifacts of deleted data. This presents an important consideration when balancing data licensing obligations with efficiency and practicality in an evolving, deployed ML system.\nDataset licensing is a multifaceted domain intersecting technology, ethics, and law. As the world around us evolves, understanding these intricacies becomes paramount for anyone building datasets during data engineering."
  },
  {
    "objectID": "data_engineering.html#conclusion",
    "href": "data_engineering.html#conclusion",
    "title": "6¬† Data Engineering",
    "section": "6.11 Conclusion",
    "text": "6.11 Conclusion\nData is the fundamental building block of AI systems. Without quality data, even the most advanced machine learning algorithms will fail. Data engineering encompasses the end-to-end process of collecting, storing, processing and managing data to fuel the development of machine learning models. It begins with clearly defining the core problem and objectives, which guides effective data collection. Data can be sourced from diverse means including existing datasets, web scraping, crowdsourcing and synthetic data generation. Each approach involves tradeoffs between factors like cost, speed, privacy and specificity. Once data is collected, thoughtful labeling through manual or AI-assisted annotation enables the creation of high-quality training datasets. Proper storage in databases, warehouses or lakes facilitates easy access and analysis. Metadata provides contextual details about the data. Data processing transforms raw data into a clean, consistent format ready for machine learning model development. Throughout this pipeline, transparency through documentation and provenance tracking is crucial for ethics, auditability and reproducibility. Data licensing protocols also govern legal data access and use. Key challenges in data engineering include privacy risks, representation gaps, legal restrictions around proprietary data, and the need to balance competing constraints like speed versus quality. By thoughtfully engineering high-quality training data, machine learning practitioners can develop accurate, robust and responsible AI systems, including for embedded and tinyML applications."
  },
  {
    "objectID": "frameworks.html#introduction",
    "href": "frameworks.html#introduction",
    "title": "7¬† AI Frameworks",
    "section": "7.1 Introduction",
    "text": "7.1 Introduction\nMachine learning frameworks provide the tools and infrastructure to efficiently build, train, and deploy machine learning models. In this chapter, we will explore the evolution and key capabilities of major frameworks like TensorFlow (TF), PyTorch, and specialized frameworks for embedded devices. We will dive into the components like computational graphs, optimization algorithms, hardware acceleration, and more that enable developers to quickly construct performant models. Understanding these frameworks is essential to leverage the power of deep learning across the spectrum from cloud to edge devices.\nML frameworks handle much of the complexity of model development through high-level APIs and domain-specific languages that allow practitioners to quickly construct models by combining pre-made components and abstractions. For example, frameworks like TensorFlow and PyTorch provide Python APIs to define neural network architectures using layers, optimizers, datasets, and more. This enables rapid iteration compared to coding every model detail from scratch.\nA key capability offered by frameworks is distributed training engines that can scale model training across clusters of GPUs and TPUs. This makes it feasible to train state-of-the-art models with billions or trillions of parameters on vast datasets. Frameworks also integrate with specialized hardware like NVIDIA GPUs to further accelerate training via optimizations like parallelization and efficient matrix operations.\nIn addition, frameworks simplify deploying finished models into production through tools like TensorFlow Serving for scalable model serving and TensorFlow Lite for optimization on mobile and edge devices. Other valuable capabilities include visualization, model optimization techniques like quantization and pruning, and monitoring metrics during training.\nLeading open source frameworks like TensorFlow, PyTorch, and MXNet power much of AI research and development today. Commercial offerings like Amazon SageMaker and Microsoft Azure Machine Learning integrate these open source frameworks with proprietary capabilities and enterprise tools.\nMachine learning engineers and practitioners leverage these robust frameworks to focus on high-value tasks like model architecture, feature engineering, and hyperparameter tuning instead of infrastructure. The goal is to efficiently build and deploy performant models that solve real-world problems.\nIn this chapter, we will explore today's leading cloud frameworks and how they have adapted models and tools specifically for embedded and edge deployment. We will compare programming models, supported hardware, optimization capabilities, and more to fully understand how frameworks enable scalable machine learning from the cloud to the edge."
  },
  {
    "objectID": "frameworks.html#framework-evolution",
    "href": "frameworks.html#framework-evolution",
    "title": "7¬† AI Frameworks",
    "section": "7.2 Framework Evolution",
    "text": "7.2 Framework Evolution\nMachine learning frameworks have evolved significantly over time to meet the diverse needs of machine learning practitioners and advancements in AI techniques. A few decades ago, building and training machine learning models required extensive low-level coding and infrastructure. Machine learning frameworks have evolved considerably over the past decade to meet the expanding needs of practitioners and rapid advances in deep learning techniques. Early neural network research was constrained by insufficient data and compute power. Building and training machine learning models required extensive low-level coding and infrastructure. But the release of large datasets like ImageNet (Deng et al. 2009) and advancements in parallel GPU computing unlocked the potential for far deeper neural networks.\n\nDeng, Jia, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. 2009. ‚ÄúImagenet: A Large-Scale Hierarchical Image Database.‚Äù In 2009 IEEE Conference on Computer Vision and Pattern Recognition, 248‚Äì55. Ieee.\n\nAl-Rfou, Rami, Guillaume Alain, Amjad Almahairi, Christof Angermueller, Dzmitry Bahdanau, Nicolas Ballas, Fr√©d√©ric Bastien, et al. 2016. ‚ÄúTheano: A Python Framework for Fast Computation of Mathematical Expressions.‚Äù arXiv e-Prints, arXiv‚Äì1605.\n\nJia, Yangqing, Evan Shelhamer, Jeff Donahue, Sergey Karayev, Jonathan Long, Ross Girshick, Sergio Guadarrama, and Trevor Darrell. 2014. ‚ÄúCaffe: Convolutional Architecture for Fast Feature Embedding.‚Äù In Proceedings of the 22nd ACM International Conference on Multimedia, 675‚Äì78.\n\nKrizhevsky, Alex, Ilya Sutskever, and Geoffrey E Hinton. 2012. ‚ÄúImagenet Classification with Deep Convolutional Neural Networks.‚Äù Advances in Neural Information Processing Systems 25.\n\nChollet, Fran√ßois et al. 2018. ‚ÄúKeras: The Python Deep Learning Library.‚Äù Astrophysics Source Code Library, ascl‚Äì1806.\n\nTokui, Seiya, Kenta Oono, Shohei Hido, and Justin Clayton. 2015. ‚ÄúChainer: A Next-Generation Open Source Framework for Deep Learning.‚Äù In Proceedings of Workshop on Machine Learning Systems (LearningSys) in the Twenty-Ninth Annual Conference on Neural Information Processing Systems (NIPS), 5:1‚Äì6.\n\nSeide, Frank, and Amit Agarwal. 2016. ‚ÄúCNTK: Microsoft‚Äôs Open-Source Deep-Learning Toolkit.‚Äù In Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, 2135‚Äì35.\n\nPaszke, Adam, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, et al. 2019. ‚ÄúPytorch: An Imperative Style, High-Performance Deep Learning Library.‚Äù Advances in Neural Information Processing Systems 32.\nThe first ML frameworks, Theano by Al-Rfou et al. (2016) and Caffe by Jia et al. (2014), were developed by academic institutions (Montreal Institute for Learning Algorithms, Berkeley Vision and Learning Center). Amid a growing interest in deep learning due to state-of-the-art performance of AlexNet Krizhevsky, Sutskever, and Hinton (2012) on the ImageNet dataset, private companies and individuals began developing ML frameworks, resulting in frameworks such as Keras by Chollet et al. (2018), Chainer by Tokui et al. (2015), TensorFlow from Google (Abadi et al. 2016), CNTK by Microsoft (Seide and Agarwal 2016), and PyTorch by Facebook (Paszke et al. 2019).\nMany of these ML frameworks can be divided into categories, namely high-level vs.¬†low-level frameworks and static vs.¬†dynamic computational graph frameworks. High-level frameworks provide a higher level of abstraction than low-level frameworks. That is, high-level frameworks have pre-built functions and modules for common ML tasks, such as creating, training, and evaluating common ML models as well as preprocessing data, engineering features, and visualizing data, which low-level frameworks do not have. Thus, high-level frameworks may be easier to use, but are not as customizable as low-level frameworks (i.e. users of low-level frameworks can define custom layers, loss functions, optimization algorithms, etc.). Examples of high-level frameworks include TensorFlow/Keras and PyTorch. Examples of low-level ML frameworks include TensorFlow with low-level APIs, Theano, Caffe, Chainer, and CNTK.\nFrameworks like Theano and Caffe used static computational graphs which required rigidly defining the full model architecture upfront. Static graphs require upfront declaration and limit flexibility. Dynamic graphs construct on-the-fly for more iterative development. But around 2016, frameworks began adopting dynamic graphs like PyTorch and TensorFlow 2.0 which can construct graphs on-the-fly. This provides greater flexibility for model development. We will discuss these concepts and details later on in the AI Training section.\nThe development of these frameworks facilitated an explosion in model size and complexity over time‚Äîfrom early multilayer perceptrons and convolutional networks to modern transformers with billions or trillions of parameters. In 2016, ResNet models by He et al. (2016) achieved record ImageNet accuracy with over 150 layers and 25 million parameters. Then in 2020, the GPT-3 language model from OpenAI (Brown et al. 2020) pushed parameters to an astonishing 175 billion using model parallelism in frameworks to train across thousands of GPUs and TPUs.\n\nHe, Kaiming, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016. ‚ÄúDeep Residual Learning for Image Recognition.‚Äù In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 770‚Äì78.\n\nBrown, Tom, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, et al. 2020. ‚ÄúLanguage Models Are Few-Shot Learners.‚Äù Advances in Neural Information Processing Systems 33: 1877‚Äì1901.\nEach generation of frameworks unlocked new capabilities that powered advancement:\n\nTheano and TensorFlow (2015) introduced computational graphs and automatic differentiation to simplify model building.\nCNTK (2016) pioneered efficient distributed training by combining model and data parallelism.\nPyTorch (2016) provided imperative programming and dynamic graphs for flexible experimentation.\nTensorFlow 2.0 (2019) made eager execution default for intuitiveness and debugging.\nTensorFlow Graphics (2020) added 3D data structures to handle point clouds and meshes.\n\nIn recent years, there has been a convergence on the frameworks. TensorFlow and PyTorch have become the overwhelmingly dominant ML frameworks, representing more than 95% of ML frameworks used in research and production. Keras was integrated into TensorFlow in 2019; Preferred Networks transitioned Chainer to PyTorch in 2019; and Microsoft stopped actively developing CNTK in 2022 in favor of supporting PyTorch on Windows.\n\n\n\nPopularity of ML frameworks in the United States as measured by Google web searches\n\n\nHowever, a one-size-fits-all approach does not work well across the spectrum from cloud to tiny edge devices. Different frameworks represent various philosophies around graph execution, declarative versus imperative APIs, and more. Declarative defines what the program should do while imperative focuses on how it should do it step-by-step. For instance, TensorFlow uses graph execution and declarative-style modeling while PyTorch adopts eager execution and imperative modeling for more Pythonic flexibility. Each approach carries tradeoffs that we will discuss later in the Basic Components section.\nToday's advanced frameworks enable practitioners to develop and deploy increasingly complex models - a key driver of innovation in the AI field. But they continue to evolve and expand their capabilities for the next generation of machine learning. To understand how these systems continue to evolve, we will dive deeper into TensorFlow as an example of how the framework grew in complexity over time."
  },
  {
    "objectID": "frameworks.html#deepdive-into-tensorflow",
    "href": "frameworks.html#deepdive-into-tensorflow",
    "title": "7¬† AI Frameworks",
    "section": "7.3 DeepDive into TensorFlow",
    "text": "7.3 DeepDive into TensorFlow\nTensorFlow was developed by the Google Brain team and was released as an open-source software library on November 9, 2015. It was designed for numerical computation using data flow graphs and has since become popular for a wide range of machine learning and deep learning applications.\nTensorFlow is both a training and inference framework and provides built-in functionality to handle everything from model creation and training, to deployment. Since its initial development, the TensorFlow ecosystem has grown to include many different ‚Äúvarieties‚Äù of TensorFlow that are each intended to allow users to support ML on different platforms. In this section, we will mainly discuss only the core package.\n\n7.3.1 TF Ecosystem\n\nTensorFlow Core: primary package that most developers engage with. It provides a comprehensive, flexible platform for defining, training, and deploying machine learning models. It includes tf.keras as its high-level API.\nTensorFlow Lite: designed for deploying lightweight models on mobile, embedded, and edge devices. It offers tools to convert TensorFlow models to a more compact format suitable for limited-resource devices and provides optimized pre-trained models for mobile.\nTensorFlow.js: JavaScript library that allows training and deployment of machine learning models directly in the browser or on Node.js. It also provides tools for porting pre-trained TensorFlow models to the browser-friendly format.\nTensorFlow on Edge Devices (Coral): platform of hardware components and software tools from Google that allows the execution of TensorFlow models on edge devices, leveraging Edge TPUs for acceleration.\nTensorFlow Federated (TFF): framework for machine learning and other computations on decentralized data. TFF facilitates federated learning, allowing model training across many devices without centralizing the data.\nTensorFlow Graphics: library for using TensorFlow to carry out graphics-related tasks, including 3D shapes and point clouds processing, using deep learning.\nTensorFlow Hub: repository of reusable machine learning model components to allow developers to reuse pre-trained model components, facilitating transfer learning and model composition\nTensorFlow Serving: framework designed for serving and deploying machine learning models for inference in production environments. It provides tools for versioning and dynamically updating deployed models without service interruption.\nTensorFlow Extended (TFX): end-to-end platform designed to deploy and manage machine learning pipelines in production settings. TFX encompasses components for data validation, preprocessing, model training, validation, and serving.\n\nTensorFlow was developed to address the limitations of DistBelief (Abadi et al. 2016)‚Äîthe framework in use at Google from 2011 to 2015‚Äîby providing flexibility along three axes: 1) defining new layers, 2) refining training algorithms, and 3) defining new training algorithms. To understand what limitations in DistBelief led to the development of TensorFlow, we will first give a brief overview of the Parameter Server Architecture that DistBelief employed (Dean et al. 2012).\n\nAbadi, Martƒ±ÃÅn, Paul Barham, Jianmin Chen, Zhifeng Chen, Andy Davis, Jeffrey Dean, Matthieu Devin, et al. 2016. ‚Äú\\(\\{\\)TensorFlow\\(\\}\\): A System for \\(\\{\\)Large-Scale\\(\\}\\) Machine Learning.‚Äù In 12th USENIX Symposium on Operating Systems Design and Implementation (OSDI 16), 265‚Äì83.\n\nDean, Jeffrey, Greg Corrado, Rajat Monga, Kai Chen, Matthieu Devin, Mark Mao, Marc‚Äôaurelio Ranzato, et al. 2012. ‚ÄúLarge Scale Distributed Deep Networks.‚Äù Advances in Neural Information Processing Systems 25.\nThe Parameter Server (PS) architecture is a popular design for distributing the training of machine learning models, especially deep neural networks, across multiple machines. The fundamental idea is to separate the storage and management of model parameters from the computation used to update these parameters:\nStorage: The storage and management of model parameters were handled by the stateful parameter server processes. Given the large scale of models and the distributed nature of the system, these parameters were sharded across multiple parameter servers. Each server maintained a portion of the model parameters, making it \"stateful\" as it had to maintain and manage this state across the training process.\nComputation: The worker processes, which could be run in parallel, were stateless and purely computational, processing data and computing gradients without maintaining any state or long-term memory (M. Li et al. 2014).\n\nLi, Mu, David G Andersen, Alexander J Smola, and Kai Yu. 2014. ‚ÄúCommunication Efficient Distributed Machine Learning with the Parameter Server.‚Äù Advances in Neural Information Processing Systems 27.\nDistBelief and its architecture defined above were crucial in enabling distributed deep learning at Google but also introduced limitations that motivated the development of TensorFlow:\n\n\n7.3.2 Static Computation Graph\nIn the parameter server architecture, model parameters are distributed across various parameter servers. Since DistBelief was primarily designed for the neural network paradigm, parameters corresponded to a fixed structure of the neural network. If the computation graph were dynamic, the distribution and coordination of parameters would become significantly more complicated. For example, a change in the graph might require the initialization of new parameters or the removal of existing ones, complicating the management and synchronization tasks of the parameter servers. This made it harder to implement models outside the neural framework or models that required dynamic computation graphs.\nTensorFlow was designed to be a more general computation framework where the computation is expressed as a data flow graph. This allows for a wider variety of machine learning models and algorithms outside of just neural networks, and provides flexibility in refining models.\n\n\n7.3.3 Usability & Deployment\nThe parameter server model involves a clear delineation of roles (worker nodes and parameter servers), and is optimized for data center deployments which might not be optimal for all use cases. For instance, on edge devices or in other non-data center environments, this division introduces overheads or complexities.\nTensorFlow was built to run on multiple platforms, from mobile devices and edge devices, to cloud infrastructure. It also aimed to provide ease of use between local and distributed training, and to be more lightweight, and developer friendly.\n\n\n7.3.4 Architecture Design\nRather than using the parameter server architecture, TensorFlow instead deploys tasks across a cluster. These tasks are named processes that can communicate over a network, and each can execute TensorFlow's core construct: the dataflow graph, and interface with various computing devices (like CPUs or GPUs). This graph is a directed representation where nodes symbolize computational operations, and edges depict the tensors (data) flowing between these operations.\nDespite the absence of traditional parameter servers, some tasks, called ‚ÄúPS tasks‚Äù, still perform the role of storing and managing parameters, reminiscent of parameter servers in other systems. The remaining tasks, which usually handle computation, data processing, and gradient calculations, are referred to as \"worker tasks.\" TensorFlow's PS tasks can execute any computation representable by the dataflow graph, meaning they aren't just limited to parameter storage, and the computation can be distributed. This capability makes them significantly more versatile and gives users the power to program the PS tasks using the standard TensorFlow interface, the same one they'd use to define their models. As mentioned above, dataflow graphs‚Äô structure also makes it inherently good for parallelism allowing for processing of large datasets.\n\n\n7.3.5 Built-in Functionality & Keras\nTensorFlow includes libraries to help users develop and deploy more use-case specific models, and since this framework is open-source, this list continues to grow. These libraries address the entire ML development life-cycle: data preparation, model building, deployment, as well as responsible AI.\nAdditionally, one of TensorFlow‚Äôs biggest advantages is its integration with Keras, though as we will cover in the next section, Pytorch recently also added a Keras integration. Keras is another ML framework that was built to be extremely user-friendly and as a result has a high level of abstraction. We will cover Keras in more depth later in this chapter, but when discussing its integration with TensorFlow, the most important thing to note is that it was originally built to be backend agnostic. This means users could abstract away these complexities, offering a cleaner, more intuitive way to define and train models without worrying about compatibility issues with different backends. TensorFlow users had some complaints about the usability and readability of TensorFlow‚Äôs API, so as TF gained prominence it integrated Keras as its high-level API. This integration offered major benefits to TensorFlow users since it introduced more intuitive readability, and portability of models while still taking advantage of powerful backend features, Google support, and infrastructure to deploy models on various platforms.\n\n\n7.3.6 Limitations and Challenges\nTensorFlow is one of the most popular deep learning frameworks but does have criticisms and weaknesses‚Äì mostly focusing on usability, and resource usage. The rapid pace of updates through its support from Google, while advantageous, has sometimes led to issues of backward compatibility, deprecated functions, and shifting documentation. Additionally, even with the Keras implementation, the syntax and learning curve of TensorFlow can be difficult for new users. One major critique of TensorFlow is its high overhead and memory consumption due to the range of built in libraries and support. Some of these concerns can be addressed by using pared down versions, but can still be limiting in resource-constrained environments.\n\n\n7.3.7 PyTorch vs.¬†TensorFlow\nPyTorch and TensorFlow have established themselves as frontrunners in the industry. Both frameworks offer robust functionalities, but they differ in terms of their design philosophies, ease of use, ecosystem, and deployment capabilities.\nDesign Philosophy and Programming Paradigm: PyTorch uses a dynamic computational graph, termed as eager execution. This makes it intuitive and facilitates debugging since operations are executed immediately and can be inspected on-the-fly. In comparison, earlier versions of TensorFlow were centered around a static computational graph, which required the graph's complete definition before execution. However, TensorFlow 2.0 introduced eager execution by default, making it more aligned with PyTorch in this regard. PyTorch's dynamic nature and Python based approach has enabled its simplicity and flexibility, particularly for rapid prototyping. TensorFlow's static graph approach in its earlier versions had a steeper learning curve; the introduction of TensorFlow 2.0, with its Keras integration as the high-level API, has significantly simplified the development process.\nDeployment: PyTorch is heavily favored in research environments, deploying PyTorch models in production settings was traditionally challenging. However, with the introduction of TorchScript and the TorchServe tool, deployment has become more feasible. One of TensorFlow's strengths lies in its scalability and deployment capabilities, especially on embedded and mobile platforms with TensorFlow Lite. TensorFlow Serving and TensorFlow.js further facilitate deployment in various environments, thus giving it a broader reach in the ecosystem.\nPerformance: Both frameworks offer efficient hardware acceleration for their operations. However, TensorFlow has a slightly more robust optimization workflow, such as the XLA (Accelerated Linear Algebra) compiler, which can further boost performance. Its static computational graph, in the early versions, was also advantageous for certain optimizations.\nEcosystem: PyTorch has a growing ecosystem with tools like TorchServe for serving models and libraries like TorchVision, TorchText, and TorchAudio for specific domains. As we mentioned earlier, TensorFlow has a broad and mature ecosystem. TensorFlow Extended (TFX) provides an end-to-end platform for deploying production machine learning pipelines. Other tools and libraries include TensorFlow Lite, TensorFlow.js, TensorFlow Hub, and TensorFlow Serving.\nHere‚Äôs a summarizing comparative analysis:\n\n\n\n\n\n\n\n\nFeature/Aspect\nPyTorch\nTensorFlow\n\n\n\n\nDesign Philosophy\nDynamic computational graph (eager execution)\nStatic computational graph (early versions); Eager execution in TensorFlow 2.0\n\n\nDeployment\nTraditionally challenging; Improved with TorchScript & TorchServe\nScalable, especially on embedded platforms with TensorFlow Lite\n\n\nPerformance & Optimization\nEfficient GPU acceleration\nRobust optimization with XLA compiler\n\n\nEcosystem\nTorchServe, TorchVision, TorchText, TorchAudio\nTensorFlow Extended (TFX), TensorFlow Lite, TensorFlow.js, TensorFlow Hub, TensorFlow Serving\n\n\nEase of Use\nPreferred for its Pythonic approach and rapid prototyping\nInitially steep learning curve; Simplified with Keras in TensorFlow 2.0"
  },
  {
    "objectID": "frameworks.html#basic-framework-components",
    "href": "frameworks.html#basic-framework-components",
    "title": "7¬† AI Frameworks",
    "section": "7.4 Basic Framework Components",
    "text": "7.4 Basic Framework Components\n\n7.4.1 Tensor data structures\nTo understand tensors, let us start from the familiar concepts in linear algebra. Vectors can be represented as a stack of numbers in a 1-dimensional array. Matrices follow the same idea, and one can think of them as many vectors being stacked on each other, making it 2 dimensional. Higher dimensional tensors work the same way. A 3-dimensional tensor is simply a set of matrices stacked on top of each other in another direction. The figure below demonstrates this step. Therefore, vectors and matrices can be considered special cases of tensors, with 1D and 2D dimensions respectively.\n\n\n\nVisualization of Tensor Data Structure\n\n\nDefining formally, in machine learning, tensors are a multi-dimensional array of numbers. The number of dimensions defines the rank of the tensor. As a generalization of linear algebra, the study of tensors is called multilinear algebra. There are noticeable similarities between matrices and higher ranked tensors. First, it is possible to extend the definitions given in linear algebra to tensors, such as with eigenvalues, eigenvectors, and rank (in the linear algebra sense) . Furthermore, with the way that we have defined tensors, it is possible to turn higher dimensional tensors into matrices. This turns out to be very critical in practice, as multiplication of abstract representations of higher dimensional tensors are often completed by first converting them into matrices for multiplication.\nTensors offer a flexible data structure with its ability to represent data in higher dimensions. For example, to represent color image data, for each of the pixel values (in 2 dimensions), one needs the color values for red, green and blue. With tensors, it is easy to contain image data in a single 3-dimensional tensor with each of the numbers within it representing a certain color value in the certain location of the image. Extending even further, if we wanted to store a series of images, we can simply extend the dimensions such that the new dimension (to create a 4-dimensional tensor) represents the different images that we have. This is exactly what the famous MNIST dataset does, loading a single 4-dimensional tensor when one calls to load the dataset, allowing a compact representation of all the data in one place.\n\n\n7.4.2 Computational graphs\n\nGraph Definition\nComputational graphs are a key component of deep learning frameworks like TensorFlow and PyTorch. They allow us to express complex neural network architectures in a way that can be efficiently executed and differentiated. A computational graph consists of a directed acyclic graph (DAG) where each node represents an operation or variable, and edges represent data dependencies between them.\nFor example, a node might represent a matrix multiplication operation, taking two input matrices (or tensors) and producing an output matrix (or tensor). To visualize this, consider the simple example below. The directed acyclic graph above computes \\(z = x \\times y\\), where each of the variables are just numbers.\n\n\n\nBasic Example of Computational Graph\n\n\nUnderneath the hood, the computational graphs represent abstractions for common layers like convolutional, pooling, recurrent, and dense layers, with data including activations, weights, biases, are represented in tensors. Convolutional layers form the backbone of CNN models for computer vision. They detect spatial patterns in input data through learned filters. Recurrent layers like LSTMs and GRUs enable processing sequential data for tasks like language translation. Attention layers are used in transformers to draw global context from the entire input.\nBroadly speaking, layers are higher level abstractions that define computations on top of those tensors. For example, a Dense layer performs a matrix multiplication and addition between input/weight/bias tensors. Note that a layer operates on tensors as inputs and outputs and the layer itself is not a tensor. Some key differences:\n\nLayers contain states like weights and biases. Tensors are stateless, just holding data.\nLayers can modify internal state during training. Tensors are immutable/read-only.\nLayers are higher level abstractions. Tensors are lower level, directly representing data and math operations.\nLayers define fixed computation patterns. Tensors flow between layers during execution.\nLayers are used indirectly when building models. Tensors flow &gt; between layers during execution.\n\nSo while tensors are a core data structure that layers consume and produce, layers have additional functionality for defining parameterized operations and training. While a layer configures tensor operations under the hood, the layer itself remains distinct from the tensor objects. The layer abstraction makes building and training neural networks much more intuitive. This sort of abstraction enables developers to build models by stacking these layers together, without having to implement the layer logic themselves. For example, calling tf.keras.layers.Conv2D in TensorFlow creates a convolutional layer. The framework handles computing the convolutions, managing parameters, etc. This simplifies model development, allowing developers to focus on architecture rather than low-level implementations. Layer abstractions utilize highly optimized implementations for performance. They also enable portability, as the same architecture can run on different hardware backends like GPUs and TPUs.\nIn addition, computational graphs include activation functions like ReLU, sigmoid, and tanh that are essential to neural networks and many frameworks provide these as standard abstractions. These functions introduce non-linearities that enable models to approximate complex functions. Frameworks provide these as simple, pre-defined operations that can be used when constructing models. For example, tf.nn.relu in TensorFlow. This abstraction enables flexibility, as developers can easily swap activation functions for tuning performance. Pre-defined activations are also optimized by the framework for faster execution.\nIn recent years, models like ResNets and MobileNets have emerged as popular architectures, with current frameworks pre-packaging these as computational graphs. Rather than worrying about the fine details, developers can utilize them as a starting point, customizing as needed by substituting layers. This simplifies and speeds up model development, avoiding reinventing architectures from scratch. Pre-defined models include well-tested, optimized implementations that ensure good performance. Their modular design also enables transferring learned features to new tasks via transfer learning. In essence, these pre-defined architectures provide high-performance building blocks to quickly create robust models.\nThese layer abstractions, activation functions, and predefined architectures provided by the frameworks are what constitute a computational graph. When a user defines a layer in a framework (e.g. tf.keras.layers.Dense()), the framework is configuring computational graph nodes and edges to represent that layer. The layer parameters like weights and biases become variables in the graph. The layer computations become operation nodes (such as the x and y in the figure above). When you call an activation function like tf.nn.relu(), the framework adds a ReLU operation node to the graph. Predefined architectures are just pre-configured subgraphs that can be inserted into your model's graph. Thus, model definition via high-level abstractions creates a computational graph. The layers, activations, and architectures we use become graph nodes and edges.\nWhen we define a neural network architecture in a framework, we are implicitly constructing a computational graph. The framework uses this graph to determine operations to run during training and inference. Computational graphs bring several advantages over raw code and that‚Äôs one of the core functionalities that is offered by a good ML framework:\n\nExplicit representation of data flow and operations\nAbility to optimize graph before execution\nAutomatic differentiation for training\nLanguage agnosticism - graph can be translated to run on GPUs, TPUs, etc.\nPortability - graph can be serialized, saved, and restored later\n\nComputational graphs are the fundamental building blocks of ML frameworks. Model definition via high-level abstractions creates a computational graph. The layers, activations, and architectures we use become graph nodes and edges. The framework compilers and optimizers operate on this graph to generate executable code. Essentially, the abstractions provide a developer-friendly API for building computational graphs. Under the hood, it's still graphs all the way down! So while you may not directly manipulate graphs as a framework user, they enable your high-level model specifications to be efficiently executed. The abstractions simplify model-building while computational graphs make it possible.\n\n\nStatic vs.¬†Dynamic Graphs\nDeep learning frameworks have traditionally followed one of two approaches for expressing computational graphs.\nStatic graphs (declare-then-execute): With this model, the entire computational graph must be defined upfront before it can be run. All operations and data dependencies must be specified during the declaration phase. TensorFlow originally followed this static approach - models were defined in a separate context, then a session was created to run them. The benefit of static graphs is they allow more aggressive optimization, since the framework can see the full graph. But it also tends to be less flexible for research and interactivity. Changes to the graph require re-declaring the full model.\nFor example:\n```{python}\nx = tf.placeholder(tf.float32)\ny = tf.matmul(x, weights) + biases\n```\nThe model is defined separately from execution, like building a blueprint. For TensorFlow 1.x, this is done using tf.Graph(). All ops and variables must be declared upfront. Subsequently, the graph is compiled and optimized before running. Execution is done later by feeding in tensor values.\nDynamic graphs (define-by-run): In contrast to declare (all) first and then execute, the graph is built dynamically as execution happens. There is no separate declaration phase - operations execute immediately as they are defined. This style is more imperative and flexible, facilitating experimentation.\nPyTorch uses dynamic graphs, building the graph on-the-fly as execution happens. For example, consider the following code snippet, where the graph is built as the execution is taking place:\n```{python}\nx = torch.randn(4,784)\ny = torch.matmul(x, weights) + biases\n```\nIn the above example, there are no separate compile/build/run phases. Ops define and execute immediately. With dynamic graphs, definition is intertwined with execution. This provides a more intuitive, interactive workflow. But the downside is less potential for optimizations, since the framework only sees the graph as it is built.\nRecently, however, the distinction has blurred as frameworks adopt both modes. TensorFlow 2.0 defaults to dynamic graph mode, while still letting users work with static graphs when needed. Dynamic declaration makes frameworks easier to use, while static models provide optimization benefits. The ideal framework offers both options.\nStatic graph declaration provides optimization opportunities but less interactivity. While dynamic execution offers flexibility and ease of use, it may have performance overhead. Here is a table comparing the pros and cons of static vs dynamic execution graphs:\n\n\n\n\n\n\n\n\nExecution Graph\nPros\nCons\n\n\n\n\nStatic (Declare-then-execute)\nEnable graph optimizations by seeing full model ahead of timeCan export and deploy frozen graphsGraph is packaged independently of code\nLess flexible for research and iterationChanges require rebuilding graphExecution has separate compile and run phases\n\n\nDynamic (Define-by-run)\nIntuitive imperative style like Python codeInterleave graph build with executionEasy to modify graphsDebugging seamlessly fits workflow\nHarder to optimize without full graphPossible slowdowns from graph building during executionCan require more memory\n\n\n\n\n\n\n7.4.3 Data Pipeline Tools\nComputational graphs can only be as good as the data they learn from and work on. Therefore, feeding training data efficiently is crucial for optimizing deep neural networks performance, though it is often overlooked as one of the core functionalities. Many modern AI frameworks provide specialized pipelines to ingest, process, and augment datasets for model training.\n\nData Loaders\nAt the core of these pipelines are data loaders, which handle reading examples from storage formats like CSV files or image folders. Reading training examples from sources like files, databases, object storage, etc. is the job of the data loaders. Deep learning models require diverse data formats depending on the application. Among the popular formats are CSV: A versatile, simple format often used for tabular data. TFRecord: TensorFlow's proprietary format, optimized for performance. Parquet: Columnar storage, offering efficient data compression and retrieval. JPEG/PNG: Commonly used for image data. WAV/MP3: Prevalent formats for audio data. For instance, tf.data is TensorFlows‚Äôs dataloading pipeline: https://www.tensorflow.org/guide/data.\nData loaders batch examples to leverage vectorization support in hardware. Batching refers to grouping multiple data points for simultaneous processing, leveraging the vectorized computation capabilities of hardware like GPUs. While typical batch sizes range from 32-512 examples, the optimal size often depends on the memory footprint of the data and the specific hardware constraints. Advanced loaders can stream virtually unlimited datasets from disk and cloud storage. Streaming large datasets from disk or networks instead of loading fully into memory. This enables virtually unlimited dataset sizes.\nData loaders can also shuffle data across epochs for randomization, and preprocess features in parallel with model training to expedite the training process. Randomly shuffling the order of examples between training epochs reduces bias and improves generalization.\nData loaders also support caching and prefetching strategies to optimize data delivery for fast, smooth model training. Caching preprocessed batches in memory so they can be reused efficiently during multiple training steps. Caching these batches in memory eliminates redundant processing. Prefetching, on the other hand, involves preloading subsequent batches, ensuring that the model never idles waiting for data.\n\n\n\n7.4.4 Data Augmentation\nBesides loading, data augmentation expands datasets synthetically. Augmentations apply random transformations like flipping, cropping, rotating, altering color, adding noise etc. for images. For audio, common augmentations involve mixing clips with background noise, or modulating speed/pitch/volume.\nAugmentations increase variation in the training data. Frameworks like TensorFlow and PyTorch simplify applying random augmentations each epoch by integrating into the data pipeline.By programmatically increasing variation in the training data distribution, augmentations reduce overfitting and improve model generalization.\nMany frameworks make it easy to integrate augmentations into the data pipeline so they are applied on-the-fly each epoch. Together, performant data loaders and extensive augmentations enable practitioners to feed massive, varied datasets to neural networks efficiently. Hands-off data pipelines represent a significant improvement in usability and productivity. They allow developers to focus more on model architecture and less on data wrangling when training deep learning models.\n\n\n7.4.5 Optimization Algorithms\nTraining a neural network is fundamentally an iterative process that seeks to minimize a loss function. At its core, the goal is to fine-tune the model weights and parameters to produce predictions as close as possible to the true target labels. Machine learning frameworks have greatly streamlined this process by offering extensive support in three critical areas: loss functions, optimization algorithms, and regularization techniques.\nLoss Functions are useful to quantify the difference between the model's predictions and the true values. Different datasets require a different loss function to perform properly, as the loss function tells the computer the ‚Äúobjective‚Äù for it to aim to. Commonly used loss functions are Mean Squared Error (MSE) for regression tasks and Cross-Entropy Loss for classification tasks.\nTo demonstrate some of the loss functions, imagine that you have a set of inputs and the corresponding outputs, \\(Y_n\\) that denotes the output of \\(n\\)‚Äôth value. The inputs are fed into the model, and the model outputs a prediction, which we can call \\(\\hat{Y_n}\\). With the predicted value and the real value, we can for example use the MSE to calculate the loss function:\n\\[MSE = \\frac{1}{N}\\sum_{n=1}^{N}(Y_n - \\hat{Y_n})^2\\]\nIf the problem is a classification problem, we do not want to use the MSE, since the distance between the predicted value and the real value does not have significant meaning. For example, if one wants to recognize handwritten models, while 9 is further away from 2, it does not mean that the model is more wrong by making the prediction. Therefore, we use the cross-entropy loss function, which is defined as:\n\\[Cross-Entropy = -\\sum_{n=1}^{N}Y_n\\log(\\hat{Y_n})\\]\nOnce the loss like above is computed, we need methods to adjust the model's parameters to reduce this loss or error during the training process. To do so, current frameworks use a gradient based approach, where it computes how much changes tuning the weights in a certain way changes the value of the loss function. Knowing this gradient, the model moves in the direction that reduces the gradient. There are many challenges associated with this, however, primarily stemming from the fact that the optimization problem is not convex, making it very easy to solve, and more details about this will come in the AI Training section. Modern frameworks come equipped with efficient implementations of several optimization algorithms, many of which are variants of gradient descent algorithms with stochastic methods and adaptive learning rates. More information with clear examples can be found in the AI Training section.\nLast but not least, overly complex models tend to overfit, meaning they perform well on the training data but fail to generalize to new, unseen data (see Overfitting). To counteract this, regularization methods are employed to penalize model complexity and encourage it to learn simpler patterns. Dropout for instance randomly sets a fraction of input units to 0 at each update during training, which helps prevent overfitting.\nHowever, there are cases where the problem is more complex than what the model can represent, and this may result in underfitting. Therefore, choosing the right model architecture is also a critical step in the training process. Further heuristics and techniques are discussed in the AI Training section.\nFrameworks also provide efficient implementations of gradient descent, Adagrad, Adadelta, and Adam. Adding regularization like dropout and L1/L2 penalties prevents overfitting during training. Batch normalization accelerates training by normalizing inputs to layers.\n\n\n7.4.6 Model Training Support\nBefore training a defined neural network model, a compilation step is required. During this step, the high-level architecture of the neural network is transformed into an optimized, executable format. This process comprises several steps. The construction of the computational graph is the first step. It represents all the mathematical operations and data flow within the model. We discussed this earlier.\nDuring training, the focus is on executing the computational graph. Every parameter within the graph, such as weights and biases, is assigned an initial value. This value might be random or based on a predefined logic, depending on the chosen initialization method.\nThe next critical step is memory allocation. Essential memory is reserved for the model's operations on both CPUs and GPUs, ensuring efficient data processing. The model's operations are then mapped to the available hardware resources, particularly GPUs or TPUs, to expedite computation. Once compilation is finalized, the model is prepared for training.\nThe training process employs various tools to enhance efficiency. Batch processing is commonly used to maximize computational throughput. Techniques like vectorization enable operations on entire data arrays, rather than proceeding element-wise, which bolsters speed. Optimizations such as kernel fusion (refer to the Optimizations chapter) amalgamate multiple operations into a single action, minimizing computational overhead. Operations can also be segmented into phases, facilitating the concurrent processing of different mini-batches at various stages.\nFrameworks consistently checkpoint the state, preserving intermediate model versions during training. This ensures that if an interruption occurs, the progress isn't wholly lost, and training can recommence from the last checkpoint. Additionally, the system vigilantly monitors the model's performance against a validation data set. Should the model begin to overfit (that is, if its performance on the validation set declines), training is automatically halted, conserving computational resources and time.\nML frameworks incorporate a blend of model compilation, enhanced batch processing methods, and utilities such as checkpointing and early stopping. These resources manage the complex aspects of performance, enabling practitioners to zero in on model development and training. As a result, developers experience both speed and ease when utilizing the capabilities of neural networks.\n\n\n7.4.7 Validation and Analysis\nAfter training deep learning models, frameworks provide utilities to evaluate performance and gain insights into the models' workings. These tools enable disciplined experimentation and debugging.\n\nEvaluation Metrics\nFrameworks include implementations of common evaluation metrics for validation:\n\nAccuracy - Fraction of correct predictions overall. Widely used for classification.\nPrecision - Of positive predictions, how many were actually positive. Useful for imbalanced datasets.\nRecall - Of actual positives, how many did we predict correctly. Measures completeness.\nF1-score - Harmonic mean of precision and recall. Combines both metrics.\nAUC-ROC - Area under ROC curve. Used for classification threshold analysis.\nMAP - Mean Average Precision. Evaluates ranked predictions in retrieval/detection.\nConfusion Matrix - Matrix that shows the true positives, true negatives, false positives, and false negatives. Provides a more detailed view of classification performance.\n\nThese metrics quantify model performance on validation data for comparison.\n\n\nVisualization\nVisualization tools provide insight into models:\n\nLoss curves - Plot training and validation loss over time to spot overfitting.\nActivation grids - Illustrate features learned by convolutional filters.\nProjection - Reduce dimensionality for intuitive visualization.\nPrecision-recall curves - Assess classification tradeoffs.\n\nTools like TensorBoard for TensorFlow and TensorWatch for PyTorch enable real-time metrics and visualization during training.\n\n\n\n7.4.8 Differentiable programming\nWith the machine learning training methods such as backpropagation relying on the change in the loss function with respect to the change in weights (which essentially is the definition of derivatives), the ability to quickly and efficiently train large machine learning models rely on the computer‚Äôs ability to take derivatives. This makes differentiable programming one of the most important elements of a machine learning framework.\nThere are primarily four methods that we can use to make computers take derivatives. First, we can manually figure out the derivatives by hand and input them to the computer. One can see that this would quickly become a nightmare with many layers of neural networks, if we had to compute all the derivatives in the backpropagation steps by hand. Another method is symbolic differentiation using computer algebra systems such as Mathematica, but this can introduce a layer of inefficiency, as there needs to be a level of abstraction to take derivatives. Numerical derivatives, the practice of approximating gradients using finite difference methods, suffer from many problems including high computational costs, and larger grid size can lead to a significant amount of errors. This leads to automatic differentiation, which exploits the primitive functions that computers use to represent operations to obtain an exact derivative. With automatic differentiation, computational complexity of computing the gradient is proportional to computing the function itself. Intricacies of automatic differentiation are not dealt with by end users now, but resources to learn more can be found widely, such as from here. Automatic differentiation and differentiable programming today is ubiquitous and is done efficiently and automatically by modern machine learning frameworks.\n\n\n7.4.9 Hardware Acceleration\nThe trend to continuously train and deploy larger machine learning models has essentially made hardware acceleration support a necessity for machine learning platforms. Deep layers of neural networks require many matrix multiplications, which attracts hardware that can compute matrix operations fast and in parallel. In this landscape, two types of hardware architectures, the GPU and TPU, have emerged as leading choices for training machine learning models.\nThe use of hardware accelerators began with AlexNet, which paved the way for future works to utilize GPUs as hardware accelerators for training computer vision models. GPUs, or Graphics Processing Units, excel in handling a large number of computations at once, making them ideal for the matrix operations that are central to neural network training. Their architecture, designed for rendering graphics, turns out to be perfect for the kind of mathematical operations required in machine learning. While they are very useful for machine learning tasks and have been implemented in many hardware platforms, GPU‚Äôs are still general purpose in that they can be used for other applications.\nOn the other hand, Tensor Processing Units (TPU) are hardware units designed specifically for neural networks. They focus on the multiply and accumulate (MAC) operation, and their hardware essentially consists of a large hardware matrix that contains elements efficiently computing the MAC operation. This concept called the systolic array architecture, was pioneered by Kung and Leiserson (1979), but has proven to be a useful structure to efficiently compute matrix products and other operations within neural networks (such as convolutions).\n\nKung, Hsiang Tsung, and Charles E Leiserson. 1979. ‚ÄúSystolic Arrays (for VLSI).‚Äù In Sparse Matrix Proceedings 1978, 1:256‚Äì82. Society for industrial; applied mathematics Philadelphia, PA, USA.\nWhile TPU‚Äôs can drastically reduce training times, it also has disadvantages. For example, many operations within the machine learning frameworks (primarily TensorFlow here since the TPU directly integrates with it) are not supported with the TPU‚Äôs. It also cannot support custom custom operations from the machine learning frameworks, and the network design must closely align to the hardware capabilities.\nToday, NVIDIA GPUs dominate training, aided by software libraries like CUDA, cuDNN, and TensorRT. Frameworks also tend to include optimizations to maximize performance on these hardware types, like pruning unimportant connections and fusing layers. Combining these techniques with hardware acceleration provides greater efficiency. For inference, hardware is increasingly moving towards optimized ASICs and SoCs. Google's TPUs accelerate models in data centers. Apple, Qualcomm, and others now produce AI-focused mobile chips. The NVIDIA Jetson family targets autonomous robots."
  },
  {
    "objectID": "frameworks.html#sec-ai_frameworks-advanced",
    "href": "frameworks.html#sec-ai_frameworks-advanced",
    "title": "7¬† AI Frameworks",
    "section": "7.5 Advanced Features",
    "text": "7.5 Advanced Features\n\n7.5.1 Distributed training\nAs machine learning models have become larger over the years, it has become essential for large models to utilize multiple computing nodes in the training process. This process, called distributed learning, has allowed for higher training capabilities, but has also imposed challenges in implementation.\nWe can consider three different ways to spread the work of training machine learning models to multiple computing nodes. Input data partitioning, referring to multiple processors running the same model on different input partitions. This is the easiest to implement that is available for many machine learning frameworks. The more challenging distribution of work comes with model parallelism, which refers to multiple computing nodes working on different parts of the model, and pipelined model parallelism, which refers to multiple computing nodes working on different layers of the model on the same input. The latter two mentioned here are active research areas.\nML frameworks that support distributed learning include TensorFlow (through its tf.distribute module), PyTorch (through its torch.nn.DataParallel and torch.nn.DistributedDataParallel modules), and MXNet (through its gluon API).\n\n\n7.5.2 Model Conversion\nMachine learning models have various methods to be represented in order to be used within different frameworks and for different device types. For example, a model can be converted to be compatible with inference frameworks within the mobile device. The default format for TensorFlow models is checkpoint files containing weights and architectures, which are needed in case we have to retrain the models. But for mobile deployment, models are typically converted to TensorFlow Lite format. TensorFlow Lite uses a compact flatbuffer representation and optimizations for fast inference on mobile hardware, discarding all the unnecessary baggage associated with training metadata such as checkpoint file structures.\nThe default format for TensorFlow models is checkpoint files containing weights and architectures. For mobile deployment, models are typically converted to TensorFlow Lite format. TensorFlow Lite uses a compact flatbuffer representation and optimizations for fast inference on mobile hardware.\nModel optimizations like quantization (see Optimizations chapter) can further optimize models for target architectures like mobile. This reduces precision of weights and activations to uint8 or int8 for a smaller footprint and faster execution with supported hardware accelerators. For post-training quantization, TensorFlow's converter handles analysis and conversion automatically.\nFrameworks like TensorFlow simplify deploying trained models to mobile and embedded IoT devices through easy conversion APIs for TFLite format and quantization. Ready-to-use conversion enables high performance inference on mobile without manual optimization burden. Besides TFLite, other common targets include TensorFlow.js for web deployment, TensorFlow Serving for cloud services, and TensorFlow Hub for transfer learning. TensorFlow's conversion utilities handle these scenarios to streamline end-to-end workflows.\nMore information about model conversion in TensorFlow is linked here.\n\n\n7.5.3 AutoML, No-Code/Low-Code ML\nIn many cases, machine learning can have a relatively high barrier of entry compared to other fields. To successfully train and deploy models, one needs to have a critical understanding of a variety of disciplines, from data science (data processing, data cleaning), model structures (hyperparameter tuning, neural network architecture), hardware (acceleration, parallel processing), and more depending on the problem at hand. The complexity of these problems have led to the introduction to frameworks such as AutoML, which aims to make ‚ÄúMachine learning available for non-Machine Learning exports‚Äù and to ‚Äúautomate research in machine learning‚Äù. They have constructed AutoWEKA, which aids in the complex process of hyperparameter selection, as well as Auto-sklearn and Auto-pytorch, an extension of AutoWEKA into the popular sklearn and PyTorch Libraries.\nWhile these works of automating parts of machine learning tasks are underway, others have focused on constructing machine learning models easier by deploying no-code/low code machine learning, utilizing a drag and drop interface with an easy to navigate user interface. Companies such as Apple, Google, and Amazon have already created these easy to use platforms to allow users to construct machine learning models that can integrate to their ecosystem.\nThese steps to remove barrier to entry continue to democratize machine learning and make it easier to access for beginners and simplify workflow for experts.\n\n\n7.5.4 Advanced Learning Methods\n\nTransfer Learning\nTransfer learning is the practice of using knowledge gained from a pretrained model to train and improve performance of a model that is for a different task. For example, datasets that have been trained on ImageNet datasets such as MobileNet and ResNet can help classify other image datasets. To do so, one may freeze the pretrained model, utilizing it as a feature extractor to train a much smaller model that is built on top of the feature extraction. One can also fine tune the entire model to fit the new task.\nTransfer learning has a series of challenges, in that the modified model may not be able to conduct its original tasks after transfer learning. Papers such as ‚ÄúLearning without Forgetting‚Äù by Z. Li and Hoiem (2017) aims to address these challenges and have been implemented in modern machine learning platforms.\n\nLi, Zhizhong, and Derek Hoiem. 2017. ‚ÄúLearning Without Forgetting.‚Äù IEEE Transactions on Pattern Analysis and Machine Intelligence 40 (12): 2935‚Äì47.\n\n\nFederated Learning\nConsider the problem of labeling items that are present in a photo from personal devices. One may consider moving the image data from the devices to a central server, where a single model will train Using these image data provided by the devices. However, this presents many potential challenges. First, with many devices one needs a massive network infrastructure to move and store data from these devices to a central location. With the number of devices that are present today this is often not feasible, and very costly. Furthermore, there are privacy challenges associated with moving personal data, such as Photos central servers.\nFederated learning by McMahan et al. (2023) is a form of distributed computing that resolves these issues by distributing the models into personal devices for them to be trained on device. At the beginning, a base global model is trained on a central server to be distributed to all devices. Using this base model, the devices individually compute the gradients and send them back to the central hub. Intuitively this is the transfer of model parameters instead of the data itself. This innovative approach allows the model to be trained with many different datasets (which, in our example, would be the set of images that are on personal devices), without the need to transfer a large amount of potentially sensitive data. However, federated learning also comes with a series of challenges.\n\nMcMahan, H. Brendan, Eider Moore, Daniel Ramage, Seth Hampson, and Blaise Ag√ºera y Arcas. 2023. ‚ÄúCommunication-Efficient Learning of Deep Networks from Decentralized Data.‚Äù https://arxiv.org/abs/1602.05629.\nIn many real-world situations, data collected from devices may not come with suitable labels. This issue is compounded by the fact that users, who are often the primary source of data, can be unreliable. This unreliability means that even when data is labeled, there‚Äôs no guarantee of its accuracy or relevance. Furthermore, each user‚Äôs data is unique, resulting in a significant variance in the data generated by different users. This non-IID nature of data, coupled with the unbalanced data production where some users generate more data than others, can adversely impact the performance of the global model. Researchers have worked to compensate for this, such as by adding a proximal term to achieve a balance between the local and global model, and adding a frozen global hypersphere classifier.\nThere are additional challenges associated with federated learning. The number of mobile device owners can far exceed the average number of training samples on each device, leading to substantial communication overhead. This issue is particularly pronounced in the context of mobile networks, which are often used for such communication and can be unstable. This instability can result in delayed or failed transmission of model updates, thereby affecting the overall training process.\nThe heterogeneity of device resources is another hurdle. Devices participating in Federated Learning can have varying computational powers and memory capacities. This diversity makes it challenging to design algorithms that are efficient across all devices. Privacy and security issues are not a guarantee for federated learning. Techniques such as inversion gradient attacks can be used to extract information about the training data from the model parameters. Despite these challenges, the large amount of potential benefits continue to make it a popular research area. Open source programs such as Flower have been developed to make it simpler to implement federated learning with a variety of machine learning frameworks."
  },
  {
    "objectID": "frameworks.html#framework-specialization",
    "href": "frameworks.html#framework-specialization",
    "title": "7¬† AI Frameworks",
    "section": "7.6 Framework Specialization",
    "text": "7.6 Framework Specialization\nThus far, we have talked about ML frameworks generally. However, typically frameworks are optimized based on the target environment's computational capabilities and application requirements, ranging from the cloud to the edge to tiny devices. Choosing the right framework is crucial based on the target environment for deployment. This section provides an overview of the major types of AI frameworks tailored for cloud, edge, and tinyML environments to help understand the similarities and differences between these different ecosystems.\n\n7.6.1 Cloud\nCloud-based AI frameworks assume access to ample computational power, memory, and storage resources in the cloud. They generally support both training and inference. Cloud-based AI frameworks are suited for applications where data can be sent to the cloud for processing, such as cloud-based AI services, large-scale data analytics, and web applications. Popular cloud AI frameworks include the ones we mentioned earlier such as TensorFlow, PyTorch, MXNet, Keras, and others. These frameworks utilize technologies like GPUs, TPUs, distributed training, and AutoML to deliver scalable AI. Concepts like model serving, MLOps, and AIOps relate to the operationalization of AI in the cloud. Cloud AI powers services like Google Cloud AI and enables transfer learning using pre-trained models.\n\n\n7.6.2 Edge\nEdge AI frameworks are tailored for deploying AI models on edge devices, such as IoT devices, smartphones, and edge servers. Edge AI frameworks are optimized for devices with moderate computational resources, offering a balance between power and performance. Edge AI frameworks are ideal for applications requiring real-time or near-real-time processing, including robotics, autonomous vehicles, and smart devices. Key edge AI frameworks include TensorFlow Lite, PyTorch Mobile, CoreML, and others. They employ optimizations like model compression, quantization, and efficient neural network architectures. Hardware support includes CPUs, GPUs, NPUs and accelerators like the Edge TPU. Edge AI enables use cases like mobile vision, speech recognition, and real-time anomaly detection.\n\n\n7.6.3 Embedded\nTinyML frameworks are specialized for deploying AI models on extremely resource-constrained devices, specifically microcontrollers and sensors within the IoT ecosystem. TinyML frameworks are designed for devices with severely limited resources, emphasizing minimal memory and power consumption. TinyML frameworks are specialized for use cases on resource-constrained IoT devices for applications such as predictive maintenance, gesture recognition, and environmental monitoring. Major tinyML frameworks include TensorFlow Lite Micro, uTensor, and ARM NN. They optimize complex models to fit within kilobytes of memory through techniques like quantization-aware training and reduced precision. TinyML allows intelligent sensing across battery-powered devices, enabling collaborative learning via federated learning. The choice of framework involves balancing model performance and computational constraints of the target platform, whether cloud, edge or tinyML. Here is a summary table comparing the major AI frameworks across cloud, edge, and tinyML environments:\n\n\n\n\n\n\n\n\n\nFramework Type\nExamples\nKey Technologies\nUse Cases\n\n\n\n\nCloud AI\nTensorFlow, PyTorch, MXNet, Keras\nGPUs, TPUs, distributed training, AutoML, MLOps\nCloud services, web apps, big data analytics\n\n\nEdge AI\nTensorFlow Lite, PyTorch Mobile, Core ML\nModel optimization, compression, quantization, efficient NN architectures\nMobile apps, robots, autonomous systems, real-time processing\n\n\nTinyML\nTensorFlow Lite Micro, uTensor, ARM NN\nQuantization-aware training, reduced precision, neural architecture search\nIoT sensors, wearables, predictive maintenance, gesture recognition\n\n\n\nKey differences:\n\nCloud AI leverages massive computational power for complex models &gt; using GPUs/TPUs and distributed training\nEdge AI optimizes models to run locally on resource-constrained edge &gt; devices.\nTinyML fits models into extremely low memory and compute &gt; environments like microcontrollers"
  },
  {
    "objectID": "frameworks.html#sec-ai_frameworks_embedded",
    "href": "frameworks.html#sec-ai_frameworks_embedded",
    "title": "7¬† AI Frameworks",
    "section": "7.7 Embedded AI Frameworks",
    "text": "7.7 Embedded AI Frameworks\n\n7.7.1 Resource Constraints\nEmbedded systems face severe resource constraints that pose unique challenges for deploying machine learning models compared to traditional computing platforms. For example, microcontroller units (MCUs) commonly used in IoT devices often have:\n\nRAM in the range of tens of kilobytes to a few megabytes. The popular ESP8266 MCU has around 80KB RAM available to developers. This contrasts with 8GB or more on typical laptops and desktops today.\nFlash storage ranging from hundreds of kilobytes to a few megabytes. The Arduino Uno microcontroller provides just 32KB of storage for code. Standard computers today have disk storage in the order of terabytes.\nProcessing power from just a few MHz to approximately 200MHz. The ESP8266 operates at 80MHz. This is several orders of magnitude slower than multi-GHz multi-core CPUs in servers and high-end laptops.\n\nThese tight constraints make training machine learning models directly on microcontrollers infeasible in most cases. The limited RAM precludes handling large datasets for training. Energy usage for training would also quickly deplete battery-powered devices. Instead, models are trained on resource-rich systems and deployed on microcontrollers for optimized inference. But even inference poses challenges:\n\nModel Size: AI models are too large to fit on embedded and IoT devices. This necessitates the need for model compression techniques, such as quantization, pruning, and knowledge distillation. Additionally, as we will see, many of the frameworks used by developers for AI development have large amounts of overhead, and built in libraries that embedded systems can‚Äôt support.\nComplexity of Tasks: With only tens of KBs to a few MBs of RAM, IoT devices and embedded systems are constrained in the complexity of tasks they can handle. Tasks that require large datasets or sophisticated algorithms‚Äì for example LLMs‚Äì which would run smoothly on traditional computing platforms, might be infeasible on embedded systems without compression or other optimization techniques due to memory limitations.\nData Storage and Processing: Embedded systems often process data in real-time and might not store large amounts of data locally. Conversely, traditional computing systems can hold and process large datasets in memory, enabling faster data operations and analysis as well as real-time updates.\nSecurity and Privacy: Limited memory also restricts the complexity of security algorithms and protocols, data encryption, reverse engineering protections, and more that can be implemented on the device. This can potentially make some IoT devices more vulnerable to attacks.\n\nConsequently, specialized software optimizations and ML frameworks tailored for microcontrollers are necessary to work within these tight resource bounds. Clever optimization techniques like quantization, pruning and knowledge distillation compress models to fit within limited memory (see Optimizations section). Learnings from neural architecture search help guide model designs.\nHardware improvements like dedicated ML accelerators on microcontrollers also help alleviate constraints. For instance, Qualcomm‚Äôs Hexagon DSP provides acceleration for TensorFlow Lite models on Snapdragon mobile chips. Google‚Äôs Edge TPU packs ML performance into a tiny ASIC for edge devices. ARM Ethos-U55 offers efficient inference on Cortex-M class microcontrollers. These customized ML chips unlock advanced capabilities for resource-constrained applications.\nGenerally, due to the limited processing power, it‚Äôs almost always infeasible to train AI models on IoT or embedded systems. Instead, models are trained on powerful traditional computers (often with GPUs) and then deployed on the embedded device for inference. TinyML specifically deals with this, ensuring models are lightweight enough for real-time inference on these constrained devices.\n\n\n7.7.2 Frameworks & Libraries\nEmbedded AI frameworks are software tools and libraries designed to enable AI and ML capabilities on embedded systems. These frameworks are essential for bringing AI to IoT devices, robotics, and other edge computing platforms and they are designed to work where computational resources, memory, and power consumption are limited.\n\n\n7.7.3 Challenges\nWhile embedded systems present an enormous opportunity for deploying machine learning to enable intelligent capabilities at the edge, these resource-constrained environments also pose significant challenges. Unlike typical cloud or desktop environments rich with computational resources, embedded devices introduce severe constraints around memory, processing power, energy efficiency, and specialized hardware. As a result, existing machine learning techniques and frameworks designed for server clusters with abundant resources do not directly translate to embedded systems. This section uncovers some of the challenges and opportunities for embedded systems and ML frameworks.\nFragmented Ecosystem\nThe lack of a unified ML framework led to a highly fragmented ecosystem. Engineers at companies like STMicroelectronics, NXP Semiconductors, and Renesas had to develop custom solutions tailored to their specific microcontroller and DSP architectures. These ad-hoc frameworks required extensive manual optimization for each low-level hardware platform. This made porting models extremely difficult, requiring redevelopment for new Arm, RISC-V or proprietary architectures.\nDisparate Hardware Needs \nWithout a shared framework, there was no standard way to assess hardware‚Äôs capabilities. Vendors like Intel, Qualcomm and NVIDIA created integrated solutions blending model, software and hardware improvements. This made it hard to discern the sources of performance gains - whether new chip designs like Intel‚Äôs low-power x86 cores or software optimizations were responsible. A standard framework was needed so vendors could evaluate their hardware‚Äôs capabilities in a fair, reproducible way.\nLack of Portability\nAdapting models trained in common frameworks like TensorFlow or PyTorch to run efficiently on microcontrollers was very challenging without standardized tools. It required time-consuming manual translation of models to run on specialized DSPs from companies like CEVA or low-power Arm M-series cores. There were no turnkey tools enabling portable deployment across different architectures.\nIncomplete Infrastructure \nThe infrastructure to support key model development workflows was lacking. There was minimal support for compression techniques to fit large models within constrained memory budgets. Tools for quantization to lower precision for faster inference were missing. Standardized APIs for integration into applications were incomplete. Essential functionality like on-device debugging, metrics, and performance profiling was absent. These gaps increased the cost and difficulty of embedded ML development.\nNo Standard Benchmark\nWithout unified benchmarks, there was no standard way to assess and compare the capabilities of different hardware platforms from vendors like NVIDIA, Arm and Ambiq Micro. Existing evaluations relied on proprietary benchmarks tailored to showcased strengths of particular chips. This made it impossible to objectively measure hardware improvements in a fair, neutral manner. This topic is discussed in more detail in the Benchmarking AI chapter.\nMinimal Real-World Testing\nMuch of the benchmarks relied on synthetic data. Rigorously testing models on real-world embedded applications was difficult without standardized datasets and benchmarks. This raised questions on how performance claims would translate to real-world usage. More extensive testing was needed to validate chips in actual use cases.\nThe lack of shared frameworks and infrastructure slowed TinyML adoption, hampering the integration of ML into embedded products. Recent standardized frameworks have begun addressing these issues through improved portability, performance profiling, and benchmarking support. But ongoing innovation is still needed to enable seamless, cost-effective deployment of AI to edge devices.\nSummary\nThe absence of standardized frameworks, benchmarks, and infrastructure for embedded ML has traditionally hampered adoption. However, recent progress has been made in developing shared frameworks like TensorFlow Lite Micro and benchmark suites like MLPerf Tiny that aim to accelerate the proliferation of TinyML solutions. But overcoming the fragmentation and difficulty of embedded deployment remains an ongoing process."
  },
  {
    "objectID": "frameworks.html#examples",
    "href": "frameworks.html#examples",
    "title": "7¬† AI Frameworks",
    "section": "7.8 Examples",
    "text": "7.8 Examples\nMachine learning deployment on microcontrollers and other embedded devices often requires specially optimized software libraries and frameworks to work within the tight constraints of memory, compute, and power. Several options exist for performing inference on such resource-limited hardware, each with their own approach to optimizing model execution. This section will explore the key characteristics and design principles behind TFLite Micro, TinyEngine, and CMSIS-NN, providing insight into how each framework tackles the complex problem of high-accuracy yet efficient neural network execution on microcontrollers. They showcase different approaches for implementing efficient TinyML frameworks.\nThe table summarizes the key differences and similarities between these three specialized machine learning inference frameworks for embedded systems and microcontrollers.\n\n\n\n\n\n\n\n\n\nFramework\nTensorFlow Lite Micro\nTinyEngine\nCMSIS-NN\n\n\n\n\nApproach\nInterpreter-based\nStatic compilation\nOptimized neural network kernels\n\n\nHardware Focus\nGeneral embedded devices\nMicrocontrollers\nARM Cortex-M processors\n\n\nArithmetic Support\nFloating point\nFloating point, fixed point\nFloating point, fixed point\n\n\nModel Support\nGeneral neural network models\nModels co-designed with TinyNAS\nCommon neural network layer types\n\n\nCode Footprint\nLarger due to inclusion of interpreter and ops\nSmall, includes only ops needed for model\nLightweight by design\n\n\nLatency\nHigher due to interpretation overhead\nVery low due to compiled model\nLow latency focus\n\n\nMemory Management\nDynamically managed by interpreter\nModel-level optimization\nTools for efficient allocation\n\n\nOptimization Approach\nSome code generation features\nSpecialized kernels, operator fusion\nArchitecture-specific assembly optimizations\n\n\nKey Benefits\nFlexibility, portability, ease of updating models\nMaximizes performance, optimized memory usage\nHardware acceleration, standardized API, portability\n\n\n\nIn the following sections, we will dive into understanding each of these in greater detail.\n\n7.8.1 Interpreter\nTensorFlow Lite Micro (TFLM) is a machine learning inference framework designed for embedded devices with limited resources. It uses an interpreter to load and execute machine learning models, which provides flexibility and ease of updating models in the field (David et al. 2021).\n\nDavid, Robert, Jared Duke, Advait Jain, Vijay Janapa Reddi, Nat Jeffries, Jian Li, Nick Kreeger, et al. 2021. ‚ÄúTensorflow Lite Micro: Embedded Machine Learning for Tinyml Systems.‚Äù Proceedings of Machine Learning and Systems 3: 800‚Äì811.\nTraditional interpreters often have significant branching overhead, which can reduce performance. However, machine learning model interpretation benefits from the efficiency of long-running kernels, where each kernel runtime is relatively large and helps mitigate interpreter overhead.\nAn alternative to an interpreter-based inference engine is to generate native code from a model during export. This can improve performance, but it sacrifices portability and flexibility, as the generated code needs recompilation for each target platform and must be replaced entirely to modify a model.\nTFLM strikes a balance between the simplicity of code compilation and the flexibility of an interpreter-based approach by incorporating certain code-generation features. For example, the library can be constructed solely from source files, offering much of the compilation simplicity associated with code generation while retaining the benefits of an interpreter-based model execution framework.\nAn interpreter-based approach offers several benefits over code generation for machine learning inference on embedded devices:\n\nFlexibility: Models can be updated in the field without recompiling the entire application.\nPortability: The interpreter can be used to execute models on different target platforms without porting the code.\nMemory efficiency: The interpreter can share code across multiple models, reducing memory usage.\nEase of development: Interpreters are easier to develop and maintain than code generators.\n\nTensorFlow Lite Micro is a powerful and flexible framework for machine learning inference on embedded devices. Its interpreter-based approach offers several benefits over code generation, including flexibility, portability, memory efficiency, and ease of development.\n\n\n7.8.2 Compiler-based\nTinyEngine by is an ML inference framework designed specifically for resource-constrained microcontrollers. It employs several optimizations to enable high-accuracy neural network execution within the tight constraints of memory, compute, and storage on microcontrollers (Lin et al. 2020).\n\nLin, Ji, Wei-Ming Chen, Yujun Lin, Chuang Gan, Song Han, et al. 2020. ‚ÄúMcunet: Tiny Deep Learning on Iot Devices.‚Äù Advances in Neural Information Processing Systems 33: 11711‚Äì22.\nWhile inference frameworks like TFLite Micro use interpreters to execute the neural network graph dynamically at runtime, this adds significant overhead in terms of memory usage to store metadata, interpretation latency, and lack of optimizations, although TFLite argues that the overhead is small. TinyEngine eliminates this overhead by employing a code generation approach. During compilation, it analyzes the network graph and generates specialized code to execute just that model. This code is natively compiled into the application binary, avoiding runtime interpretation costs.\nConventional ML frameworks schedule memory per layer, trying to minimize usage for each layer separately. TinyEngine does model-level scheduling instead, analyzing memory usage across layers. It allocates a common buffer size based on the max memory needs of all layers. This buffer is then shared efficiently across layers to increase data reuse.\nTinyEngine also specializes the kernels for each layer through techniques like tiling, unrolling, and fusing operators. For example, it will generate unrolled compute kernels with the exact number of loops needed for a 3x3 or 5x5 convolution. These specialized kernels extract maximum performance from the microcontroller hardware. It uses depthwise convolutions that are optimized to minimize memory allocations by computing each channel's output in-place over the input channel data. This technique exploits the channel-separable nature of depthwise convolutions to reduce peak memory size.\nSimilar to TFLite Micro, the compiled TinyEngine binary only includes ops needed for a specific model rather than all possible operations. This results in a very small binary footprint, keeping code size low for memory-constrained devices.\nOne difference between TFLite Micro and TinyEngine is that the latter is co-designed with ‚ÄúTinyNAS,‚Äù an architecture search method for microcontroller models, similar to differential NAS for microcontrollers. The efficiency of TinyEngine allows exploring larger and more accurate models through NAS. It also provides feedback to TinyNAS on which models can fit within the hardware constraints.\nThrough all these various custom techniques like static compilation, model-based scheduling, specialized kernels, and co-design with NAS, TinyEngine enables high-accuracy deep learning inference within the tight resource constraints of microcontrollers.\n\n\n7.8.3 Library\nCMSIS-NN, standing for Cortex Microcontroller Software Interface Standard for Neural Networks, is a software library devised by ARM. It offers a standardized interface for deploying neural network inference on microcontrollers and embedded systems, with a particular focus on optimization for ARM Cortex-M processors (Lai, Suda, and Chandra 2018).\n\nLai, Liangzhen, Naveen Suda, and Vikas Chandra. 2018. ‚ÄúCmsis-Nn: Efficient Neural Network Kernels for Arm Cortex-m Cpus.‚Äù arXiv Preprint arXiv:1801.06601.\nNeural Network Kernels: CMSIS-NN is equipped with highly efficient kernels that handle fundamental neural network operations such as convolution, pooling, fully connected layers, and activation functions. It caters to a broad range of neural network models by supporting both floating-point and fixed-point arithmetic. The latter is especially beneficial for resource-constrained devices as it curtails memory and computational requirements (Quantization).\nHardware Acceleration: CMSIS-NN harnesses the power of Single Instruction, Multiple Data (SIMD) instructions available on many Cortex-M processors. This allows for parallel processing of multiple data elements within a single instruction, thereby boosting computational efficiency. Certain Cortex-M processors feature Digital Signal Processing (DSP) extensions that CMSIS-NN can exploit for accelerated neural network execution. The library also incorporates assembly-level optimizations tailored to specific microcontroller architectures to further enhance performance.\nStandardized API: CMSIS-NN offers a consistent and abstracted API that protects developers from the complexities of low-level hardware details. This makes the integration of neural network models into applications simpler. It may also encompass tools or utilities for converting popular neural network model formats into a format that is compatible with CMSIS-NN.\nMemory Management: CMSIS-NN provides functions for efficient memory allocation and management, which is vital in embedded systems where memory resources are scarce. It ensures optimal memory usage during inference and in some instances, allows for in-place operations to further decrease memory overhead.\nPortability: CMSIS-NN is designed with portability in mind across various Cortex-M processors. This enables developers to write code that can operate on different microcontrollers without significant modifications.\nLow Latency: CMSIS-NN minimizes inference latency, making it an ideal choice for real-time applications where swift decision-making is paramount.\nEnergy Efficiency: The library is designed with a focus on energy efficiency, making it suitable for battery-powered and energy-constrained devices."
  },
  {
    "objectID": "frameworks.html#choosing-the-right-framework",
    "href": "frameworks.html#choosing-the-right-framework",
    "title": "7¬† AI Frameworks",
    "section": "7.9 Choosing the Right Framework",
    "text": "7.9 Choosing the Right Framework\nChoosing the right machine learning framework for a given application requires carefully evaluating models, hardware, and software considerations. By analyzing these three aspects - models, hardware, and software - ML engineers can select the optimal framework and customize as needed for efficient and performant on-device ML applications. The goal is to balance model complexity, hardware limitations, and software integration to design a tailored ML pipeline for embedded and edge devices.\n\n\n\nTensorFlow Framework Comparison - General\n\n\n\n7.9.1 Model\nTensorFlow supports significantly more ops than TensorFlow Lite and TensorFlow Lite Micro as it is typically used for research or cloud deployment, which require a large number of and more flexibility with operators (ops),. TensorFlow Lite supports select ops for on-device training, whereas TensorFlow Micro does not. TensorFlow Lite also supports dynamic shapes and quantization aware training, but TensorFlow Micro does not. In contrast, TensorFlow Lite and TensorFlow Micro offer native quantization tooling and support, where quantization refers to the process of transforming an ML program into an approximated representation with available lower precision operations.\n\n\n7.9.2 Software\n\n\n\nTensorFlow Framework Comparison - Software\n\n\nTensorFlow Lite Micro does not have OS support, while TensorFlow and TensorFlow Lite do, in order to reduce memory overhead, make startup times faster, and consume less energy. TensorFlow Lite Micro can be used in conjunction with real-time operating systems (RTOS) like FreeRTOS, Zephyr, and Mbed OS. TensorFlow Lite and TensorFlow Lite Micro support model memory mapping, allowing models to be directly accessed from flash storage rather than loaded into RAM, whereas TensorFlow does not. TensorFlow and TensorFlow Lite support accelerator delegation to schedule code to different accelerators, whereas TensorFlow Lite Micro does not, as embedded systems tend not to have a rich array of specialized accelerators.\n\n\n7.9.3 Hardware\n\n\n\nTensorFlow Framework Comparison - Hardware\n\n\nTensorFlow Lite and TensorFlow Lite Micro have significantly smaller base binary sizes and base memory footprints compared to TensorFlow. For example, a typical TensorFlow Lite Micro binary is less than 200KB, whereas TensorFlow is much larger. This is due to the resource-constrained environments of embedded systems. TensorFlow provides support for x86, TPUs, and GPUs like NVIDIA, AMD, and Intel. TensorFlow Lite provides support for Arm Cortex A and x86 processors commonly used in mobile and tablets. The latter is stripped out of all the training logic that is not necessary for ondevice deployment. TensorFlow Lite Micro provides support for microcontroller-focused Arm Cortex M cores like M0, M3, M4, and M7, as well as DSPs like Hexagon and SHARC and MCUs like STM32, NXP Kinetis, Microchip AVR.\nSelecting the appropriate AI framework is essential to ensure that embedded systems can efficiently execute AI models. There are key factors to consider when choosing a machine learning framework, with a focus on ease of use, community support, performance, scalability, integration with data engineering tools, and integration with model optimization tools. By understanding these factors, you can make informed decisions and maximize the potential of your machine learning initiatives.\n\n\n7.9.4 Other Factors\nWhen evaluating AI frameworks for embedded systems, several other key factors beyond models, hardware, and software should be considered.\n\nPerformance\nPerformance is critical in embedded systems where computational resources are limited. Evaluate the framework's ability to optimize model inference for embedded hardware. Factors such as model quantization and hardware acceleration support play a crucial role in achieving efficient inference.\n\n\nScalability\nScalability is essential when considering the potential growth of an embedded AI project. The framework should support the deployment of models on a variety of embedded devices, from microcontrollers to more powerful processors. It should also handle both small-scale and large-scale deployments seamlessly.\n\n\nIntegration with Data Engineering Tools\nData engineering tools are essential for data preprocessing and pipeline management. An ideal AI framework for embedded systems should seamlessly integrate with these tools, allowing for efficient data ingestion, transformation, and model training.\n\n\nIntegration with Model Optimization Tools\nModel optimization is crucial to ensure that AI models are well-suited for embedded deployment. Evaluate whether the framework integrates with model optimization tools, such as TensorFlow Lite Converter or ONNX Runtime, to facilitate model quantization and size reduction.\n\n\nEase of Use\nThe ease of use of an AI framework significantly impacts development efficiency. A framework with a user-friendly interface and clear documentation reduces the learning curve for developers. Consideration should be given to whether the framework supports high-level APIs, allowing developers to focus on model design rather than low-level implementation details. This factor is incredibly important for embedded systems, which have less features that typical developers might be accustomed to.\n\n\nCommunity Support\nCommunity support plays another essential factor. Frameworks with active and engaged communities often have well-maintained codebases, receive regular updates, and provide valuable forums for problem-solving. As a result, community support plays into Ease of Use as well because it ensures that developers have access to a wealth of resources, including tutorials and example projects. Community support provides some assurance that the framework will continue to be supported for future updates. There are only a handful of frameworks that cater to TinyML needs. Of that, TensorFlow Lite Micro is the most popular and has the most community support."
  },
  {
    "objectID": "frameworks.html#future-trends-in-ml-frameworks",
    "href": "frameworks.html#future-trends-in-ml-frameworks",
    "title": "7¬† AI Frameworks",
    "section": "7.10 Future Trends in ML Frameworks",
    "text": "7.10 Future Trends in ML Frameworks\n\n7.10.1 Decomposition\nCurrently, the ML system stack consists of four abstractions, namely (1) computational graphs, (2) tensor programs, (3) libraries and runtimes, and (4) hardware primitives.\n\n\n\n\n\nThis has led to vertical (i.e.¬†between abstraction levels) and horizontal (i.e.¬†library-driven vs.¬†compilation-driven approaches to tensor computation) boundaries, which hinder innovation for ML. Future work in ML frameworks can look toward breaking these boundaries. In December 2021, Apache TVM Unity was proposed, which aimed to facilitate interactions between the different abstraction levels (as well as the people behind them, such as ML scientists, ML engineers, and hardware engineers) and co-optimize decisions in all four abstraction levels.\n\n\n7.10.2 High-Performance Compilers & Libraries\nAs ML frameworks further develop, high-performance compilers and libraries will continue to emerge. Some current examples include TensorFlow XLA and Nvidia‚Äôs CUTLASS, which accelerate linear algebra operations in computational graphs, and Nvidia‚Äôs TensorRT, which accelerates and optimizes inference.\n\n\n7.10.3 ML for ML Frameworks\nWe can also use ML to improve ML frameworks in the future. Some current uses of ML for ML frameworks include:\n\nhyperparameter optimization using techniques such as Bayesian optimization, random search, and grid search\nneural architecture search (NAS) to automatically search for optimal network architectures\nAutoML, which as described in the Advanced FeaturesSection¬†7.5 section, automates the ML pipeline."
  },
  {
    "objectID": "frameworks.html#conclusion",
    "href": "frameworks.html#conclusion",
    "title": "7¬† AI Frameworks",
    "section": "7.11 Conclusion",
    "text": "7.11 Conclusion\nIn summary, selecting the optimal framework requires thoroughly evaluating options against criteria like usability, community support, performance, hardware compatibility, and model conversion abilities. There is no universal best solution, as the right framework depends on the specific constraints and use case.\nFor extremely resource constrained microcontroller-based platforms, TensorFlow Lite Micro currently provides a strong starting point. Its comprehensive optimization tooling like quantization mapping and kernel optimizations enables high performance on devices like Arm Cortex-M and RISC-V processors. The active developer community ensures accessible technical support. Seamless integration with TensorFlow for training and converting models makes the workflow cohesive.\nFor platforms with more capable CPUs like Cortex-A, TensorFlow Lite for Microcontrollers expand possibilities. They provide greater flexibility for custom and advanced models beyond the core operators in TFLite Micro. However, this comes at the cost of a larger memory footprint. These frameworks are ideal for automotive systems, drones, and more powerful edge devices that can benefit from greater model sophistication.\nFrameworks specifically built for specialized hardware like CMSIS-NN on Cortex-M processors can further maximize performance, but sacrifice portability. Integrated frameworks from processor vendors tailor the stack to their architectures. This can unlock the full potential of their chips but lock you into their ecosystem.\nUltimately, choosing the right framework involves finding the best match between its capabilities and the requirements of the target platform. This requires balancing tradeoffs between performance needs, hardware constraints, model complexity, and other factors. Thoroughly assessing intended models, use cases, and evaluating options against key metrics will guide developers towards picking the ideal framework for their embedded ML application."
  },
  {
    "objectID": "training.html#introduction",
    "href": "training.html#introduction",
    "title": "8¬† AI Training",
    "section": "8.1 Introduction",
    "text": "8.1 Introduction\nExplanation: An introductory section sets the stage for the reader, explaining what AI training is and why it‚Äôs crucial, especially in the context of embedded systems. It helps to align the reader‚Äôs expectations and prepares them for the upcoming content.\n\nBrief overview of what AI training entails\nImportance of training in the context of embedded AI"
  },
  {
    "objectID": "training.html#types-of-training",
    "href": "training.html#types-of-training",
    "title": "8¬† AI Training",
    "section": "8.2 Types of Training",
    "text": "8.2 Types of Training\nExplanation: Understanding the different types of training methods is foundational. It allows the reader to appreciate the diversity of approaches and to select the most appropriate one for their specific embedded AI application.\n\nSupervised Learning\nUnsupervised Learning\nReinforcement Learning\nSemi-supervised Learning"
  },
  {
    "objectID": "training.html#data-preparation",
    "href": "training.html#data-preparation",
    "title": "8¬† AI Training",
    "section": "8.3 Data Preparation",
    "text": "8.3 Data Preparation\nExplanation: Data is the fuel for AI. This section is essential because it guides the reader through the initial steps of gathering and preparing data, which is a prerequisite for effective training.\n\nData Collection\nData Annotation\nData Augmentation\nData Preprocessing"
  },
  {
    "objectID": "training.html#training-algorithms",
    "href": "training.html#training-algorithms",
    "title": "8¬† AI Training",
    "section": "8.4 Training Algorithms",
    "text": "8.4 Training Algorithms\nExplanation: This section delves into the algorithms that power the training process. It‚Äôs crucial for understanding how models learn from data and how to implement these algorithms efficiently in embedded systems.\n\nGradient Descent\nBackpropagation\nOptimizers (SGD, Adam, RMSprop, etc.)"
  },
  {
    "objectID": "training.html#training-environments",
    "href": "training.html#training-environments",
    "title": "8¬† AI Training",
    "section": "8.5 Training Environments",
    "text": "8.5 Training Environments\nExplanation: Different training environments have their own pros and cons. This section helps the reader make informed decisions about where to train their models, considering factors like computational resources and latency.\n\nLocal vs.¬†Cloud\nSpecialized Hardware (GPUs, TPUs, etc.)"
  },
  {
    "objectID": "training.html#hyperparameter-tuning",
    "href": "training.html#hyperparameter-tuning",
    "title": "8¬† AI Training",
    "section": "8.6 Hyperparameter Tuning",
    "text": "8.6 Hyperparameter Tuning\nExplanation: Hyperparameters can significantly impact the performance of a trained model. This section educates the reader on how to fine-tune these settings for optimal results, which is especially important for resource-constrained embedded systems.\n\nLearning Rate\nBatch Size\nNumber of Epochs\nRegularization Techniques"
  },
  {
    "objectID": "training.html#evaluation-metrics",
    "href": "training.html#evaluation-metrics",
    "title": "8¬† AI Training",
    "section": "8.7 Evaluation Metrics",
    "text": "8.7 Evaluation Metrics\nExplanation: Knowing how to evaluate a model‚Äôs performance is crucial. This section introduces metrics that help in assessing how well the model will perform in real-world embedded applications.\n\nAccuracy\nPrecision and Recall\nF1 Score\nROC and AUC"
  },
  {
    "objectID": "training.html#overfitting-and-underfitting",
    "href": "training.html#overfitting-and-underfitting",
    "title": "8¬† AI Training",
    "section": "8.8 Overfitting and Underfitting",
    "text": "8.8 Overfitting and Underfitting\nExplanation: Overfitting and underfitting are common pitfalls in AI training. This section is vital for teaching strategies to avoid these issues, ensuring that the model generalizes well to new, unseen data.\n\nTechniques to Avoid Overfitting (Dropout, Early Stopping, etc.)\nUnderstanding Underfitting and How to Address It"
  },
  {
    "objectID": "training.html#transfer-learning",
    "href": "training.html#transfer-learning",
    "title": "8¬† AI Training",
    "section": "8.9 Transfer Learning",
    "text": "8.9 Transfer Learning\nExplanation: Transfer learning can save time and computational resources, which is particularly beneficial for embedded systems. This section explains how to leverage pre-trained models for new tasks.\n\nBasics of Transfer Learning\nApplications in Embedded AI"
  },
  {
    "objectID": "training.html#challenges-and-best-practices",
    "href": "training.html#challenges-and-best-practices",
    "title": "8¬† AI Training",
    "section": "8.10 Challenges and Best Practices",
    "text": "8.10 Challenges and Best Practices\nExplanation: Every technology comes with its own set of challenges. This section prepares the reader for potential hurdles in AI training, offering best practices to navigate them effectively.\n\nComputational Constraints\nData Privacy\nEthical Considerations"
  },
  {
    "objectID": "training.html#conclusion",
    "href": "training.html#conclusion",
    "title": "8¬† AI Training",
    "section": "8.11 Conclusion",
    "text": "8.11 Conclusion\nExplanation: A summary helps to consolidate the key points of the chapter, aiding in better retention and understanding of the material.\n\nKey Takeaways\nFuture Trends in AI Training for Embedded Systems"
  },
  {
    "objectID": "efficient_ai.html#introduction",
    "href": "efficient_ai.html#introduction",
    "title": "9¬† Efficient AI",
    "section": "9.1 Introduction",
    "text": "9.1 Introduction\nTraining models can consume a significant amount of energy, sometimes equivalent to the carbon footprint of sizable industrial processes. We will cover some of these sustainability details in the AI Sustainability chapter. On the deployment side, if these models are not optimized for efficiency, they can quickly drain device batteries, demand excessive memory, or fall short of real-time processing needs. Through this introduction, our objective is to elucidate the nuances of efficiency, setting the groundwork for a comprehensive exploration in the subsequent chapters."
  },
  {
    "objectID": "efficient_ai.html#the-need-for-efficient-ai",
    "href": "efficient_ai.html#the-need-for-efficient-ai",
    "title": "9¬† Efficient AI",
    "section": "9.2 The Need for Efficient AI",
    "text": "9.2 The Need for Efficient AI\nEfficiency takes on different connotations based on where AI computations occur. Let‚Äôs take a brief moment to revisit and differentiate between Cloud, Edge, and TinyML in terms of efficiency.\n\n\n\nCloud, Mobile and TinyML.\n\n\nFor cloud AI, traditional AI models often ran in the large‚Äîscale data centers equipped with powerful GPUs and TPUs (Barroso, H√∂lzle, and Ranganathan 2019). Here, efficiency pertains to optimizing computational resources, reducing costs, and ensuring timely data processing and return. However, relying on the cloud introduced latency, especially when dealing with large data streams that needed to be uploaded, processed, and then downloaded.\n\nBarroso, Luiz Andr√©, Urs H√∂lzle, and Parthasarathy Ranganathan. 2019. The Datacenter as a Computer: Designing Warehouse-Scale Machines. Springer Nature.\n\nLi, En, Liekang Zeng, Zhi Zhou, and Xu Chen. 2019. ‚ÄúEdge AI: On-Demand Accelerating Deep Neural Network Inference via Edge Computing.‚Äù IEEE Transactions on Wireless Communications 19 (1): 447‚Äì57.\nFor edge AI, edge computing brought AI closer to the data source, processing information directly on local devices like smartphones, cameras, or industrial machines (Li et al. 2019). Here, efficiency encompasses quick real-time responses and reduced data transmission needs. The constraints, however, are tighter‚Äîthese devices, while more powerful than microcontrollers, have limited computational power compared to cloud setups.\nPushing the frontier even further is TinyML, where AI models run on microcontrollers or extremely resource-constrained environments. The difference in performance for processors and memory between TinyML and cloud or mobile systems can be several orders of magnitude (Warden and Situnayake 2019). Efficiency in TinyML is about ensuring models are lightweight enough to fit on these devices, use minimal energy (critical for battery-powered devices), and still perform their tasks effectively.\n\nWarden, Pete, and Daniel Situnayake. 2019. Tinyml: Machine Learning with Tensorflow Lite on Arduino and Ultra-Low-Power Microcontrollers. O‚ÄôReilly Media.\nThe spectrum from Cloud to TinyML represents a shift from vast, centralized computational resources to distributed, localized, and constrained environments. As we transition from one to the other, the challenges and strategies related to efficiency evolve, underlining the need for specialized approaches tailored to each scenario. Having underscored the need for efficient AI, especially within the context of TinyML, we will transition to exploring the methodologies devised to meet these challenges. The following sections outline at a high level the main concepts that we will dwelve into deeper at a later point. As we delve into these strategies, we will demonstrate the breadth and depth of innovation needed to achieve efficient AI."
  },
  {
    "objectID": "efficient_ai.html#efficient-model-architectures",
    "href": "efficient_ai.html#efficient-model-architectures",
    "title": "9¬† Efficient AI",
    "section": "9.3 Efficient Model Architectures",
    "text": "9.3 Efficient Model Architectures\nChoosing the right model architecture is as crucial as optimizing it. In recent years, researchers have explored some novel architectures that can have inherently fewer parameters while maintaining strong performance.\nMobileNets: MobileNets are a class of efficient models for mobile and embedded vision applications (Howard et al. 2017). The key idea that led to the success of MobileNets is the use of depth-wise separable convolutions which significantly reduce the number of parameters and computations in the network. MobileNetV2 and V3 further enhance this design with the introduction of inverted residuals and linear bottlenecks.\n\nHoward, Andrew G, Menglong Zhu, Bo Chen, Dmitry Kalenichenko, Weijun Wang, Tobias Weyand, Marco Andreetto, and Hartwig Adam. 2017. ‚ÄúMobilenets: Efficient Convolutional Neural Networks for Mobile Vision Applications.‚Äù arXiv Preprint arXiv:1704.04861.\n\nIandola, Forrest N, Song Han, Matthew W Moskewicz, Khalid Ashraf, William J Dally, and Kurt Keutzer. 2016. ‚ÄúSqueezeNet: AlexNet-Level Accuracy with 50x Fewer Parameters and&lt; 0.5 MB Model Size.‚Äù arXiv Preprint arXiv:1602.07360.\nSqueezeNet: SqueezeNet is a class of ML models known for its smaller size without sacrificing accuracy. It achieves this by using a ‚Äúfire module‚Äù that reduces the number of input channels to 3x3 filters, thus reducing the parameters (Iandola et al. 2016). Moreover, it employs delayed downsampling to increase the accuracy by maintaining a larger feature map.\nResNet variants: The Residual Network (ResNet) architecture allows introduced skip connections, or shortcuts (He et al. 2016). Some variants of ResNet are designed to be more efficient. For instance, ResNet-SE incorporates the ‚Äúsqueeze and excitation‚Äù mechanism to recalibrate feature maps, while ResNeXt offers grouped convolutions for efficiency.\n\nHe, Kaiming, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016. ‚ÄúDeep Residual Learning for Image Recognition.‚Äù In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 770‚Äì78."
  },
  {
    "objectID": "efficient_ai.html#efficient-model-compression",
    "href": "efficient_ai.html#efficient-model-compression",
    "title": "9¬† Efficient AI",
    "section": "9.4 Efficient Model Compression",
    "text": "9.4 Efficient Model Compression\nModel compression methods are very important for bringing deep learning models to devices with limited resources. These techniques reduce the size, energy consumption, and computational demands of models without a significant loss in accuracy. At a high level, the methods can briefly be binned into the following fundamental methods:\nPruning: This is akin to trimming the branches of a tree. This was first thought of in the Optimal Brain Damage paper (LeCun, Denker, and Solla 1989). This was later popularized in the context of deep learning by Han, Mao, and Dally (2016). In pruning, certain weights or even entire neurons are removed from the network, based on specific criteria. This can significantly reduce the model size. There are various strategies, like weight pruning, neuron pruning, and structured pruning. We will explore these in more detail in Section¬†10.2.1.\n\nLeCun, Yann, John Denker, and Sara Solla. 1989. ‚ÄúOptimal Brain Damage.‚Äù Advances in Neural Information Processing Systems 2.\n\nHan, Song, Huizi Mao, and William J. Dally. 2016. ‚ÄúDeep Compression: Compressing Deep Neural Networks with Pruning, Trained Quantization and Huffman Coding.‚Äù https://arxiv.org/abs/1510.00149.\nQuantization: Quantization is the process of constraining an input from a large set to output in a smaller set, primarily in deep learning, this means reducing the number of bits that represent the weights and biases of the model. For example, using 16-bit or 8-bit representations instead of 32-bit can reduce model size and speed up computations, with a minor trade-off in accuracy. We will explore these in more detail in Section¬†10.3.5.\nKnowledge Distillation: Knowledge distillation involves training a smaller model (student) to replicate the behavior of a larger model (teacher). The idea is to transfer the knowledge from the cumbersome model to the lightweight one, so the smaller model attains performance close to its larger counterpart but with significantly fewer parameters. We will explore knowledge distillation in more detail in the Section¬†10.2.2.1."
  },
  {
    "objectID": "efficient_ai.html#efficient-inference-hardware",
    "href": "efficient_ai.html#efficient-inference-hardware",
    "title": "9¬† Efficient AI",
    "section": "9.5 Efficient Inference Hardware",
    "text": "9.5 Efficient Inference Hardware\nTraining an AI model is an intensive task that requires powerful hardware and can take hours to weeks, but inference needs to be as fast as possible, especially in real-time applications. This is where efficient inference hardware comes into play. By optimizing the hardware specifically for inference tasks, we can achieve rapid response times and power-efficient operation, especially crucial for edge devices and embedded systems.\nTPUs (Tensor Processing Units): TPUs are custom-built ASICs (Application-Specific Integrated Circuits) by Google to accelerate machine learning workloads (Jouppi et al. 2017). They are optimized for tensor operations, offering high throughput for low-precision arithmetic, and are designed specifically for neural network machine learning. TPUs deliver a significant acceleration in model training and inference as compared to general-purpose GPU/CPUs. This boost means faster model training and real-time or near-real-time inference capabilities, crucial for applications like voice search and augmented reality.\n\nJouppi, Norman P, Cliff Young, Nishant Patil, David Patterson, Gaurav Agrawal, Raminder Bajwa, Sarah Bates, et al. 2017. ‚ÄúIn-Datacenter Performance Analysis of a Tensor Processing Unit.‚Äù In Proceedings of the 44th Annual International Symposium on Computer Architecture, 1‚Äì12.\nEdge TPUs are a smaller, power-efficient version of Google‚Äôs TPUs, tailored for edge devices. They provide fast on-device ML inferencing for TensorFlow Lite models. Edge TPUs allow for low-latency, high-efficiency inference on edge devices like smartphones, IoT devices, and embedded systems. This means AI capabilities can be deployed in real-time applications without needing to communicate with a central server, thus saving bandwidth and reducing latency.\nNN Accelerators: Fixed function neural network accelerators are hardware accelerators designed explicitly for neural network computations. These can be standalone chips or part of a larger system-on-chip (SoC) solution. By optimizing the hardware for the specific operations that neural networks require, such as matrix multiplications and convolutions, NN accelerators can achieve faster inference times and lower power consumption compared to general-purpose CPUs and GPUs. They are especially beneficial in TinyML devices with power or thermal constraints, such as smartwatches, micro-drones, or robotics.\nBut these are all but the most common place examples, there are a number of other types of hardware that are emerging that have the potential to offer signficiant advantages for inference. These include but are not limited to neuromorphic hardware, photonic computing, and so forth. In Section¬†11.3 we will explore these in greater detail.\nEfficient hardware for inference not only speeds up the process but also saves energy, extends battery life, and can operate in real-time conditions. As AI continues to be integrated into a myriad of applications ‚Äì from smart cameras to voice assistants ‚Äì the role of optimized hardware will only become more prominent. By leveraging these specialized hardware components, developers and engineers can bring the power of AI to devices and situations that were previously unthinkable."
  },
  {
    "objectID": "efficient_ai.html#efficient-numerics",
    "href": "efficient_ai.html#efficient-numerics",
    "title": "9¬† Efficient AI",
    "section": "9.6 Efficient Numerics",
    "text": "9.6 Efficient Numerics\nMachine learning, and especially deep learning, involves enormous amounts of computation. Models can have millions to billions of parameters, and these are often trained on vast datasets. Every operation, every multiplication or addition, demands computational resources. Therefore, the precision of the numbers used in these operations can have a significant impact on the computational speed, energy consumption, and memory requirements. This is where the concept of efficient numerics comes into play.\n\n9.6.1 Numerical Formats\nThere are many different types of numerics. Numerics have a long history in computing systems.\nFloating point: Known as single-precision floating-point, FP32 utilizes 32 bits to represent a number, incorporating its sign, exponent, and fraction. FP32 is widely adopted in many deep learning frameworks and offers a balance between accuracy and computational requirements. It‚Äôs prevalent in the training phase for many neural networks due to its sufficient precision in capturing minute details during weight updates.\nAlso known as half-precision floating point, FP16 uses 16 bits to represent a number, including its sign, exponent, and fraction. FP16 offers a good balance between precision and memory savings. It‚Äôs particularly popular in deep learning training on GPUs that support mixed-precision arithmetic, combining the speed benefits of FP16 with the precision of FP32 where needed.\nThere are also several other numerical formats that fall into an exotic calss. An exotic example is BF16, or Brain Floating Point. It is a 16-bit numerical format that is designed explicitly for deep learning applications. It‚Äôs a compromise between FP32 and FP16, retaining the 8-bit exponent from FP32 while reducing the mantissa to 7 bits (as compared to FP32‚Äôs 23-bit mantissa). This structure prioritizes range over precision. BF16 has been shown to achieve training results that are comparable in accuracy to FP32 while using significantly less memory and computational resources. This makes it suitable not just for inference but also for training deep neural networks.\nBy retaining the 8-bit exponent of FP32, BF16 offers a similar range, which is crucial for deep learning tasks where certain operations can result in very large or very small numbers. At the same time, by truncating precision, BF16 allows for reduced memory and computational requirements compared to FP32. BF16 has emerged as a promising middle ground in the landscape of numerical formats for deep learning, providing an efficient and effective alternative to the more traditional FP32 and FP16 formats.\n\n\n\nThree floating-point formats. Source: Google blog\n\n\nInteger: These are integer representations using 8, 4, and 2 bits. They are often used during the inference phase of neural networks, where the weights and activations of the model are quantized to these lower precisions. Integer representations are deterministic and offer significant speed and memory advantages over floating-point representations. For many inference tasks, especially on edge devices, the slight loss in accuracy due to quantization is often acceptable given the efficiency gains. An extreme form of integer numerics is for binary neural networks (BNNs), where weights and activations are constrained to one of two values: either +1 or -1.\nVariable bit widths: Beyond the standard widths, research is ongoing into extremely low bit-width numerics, even down to binary or ternary representations. Extremely low bit-width operations can offer significant speedups and reduce power consumption even further. While challenges remain in maintaining model accuracy with such drastic quantization, advances continue to be made in this area.\nEfficient numerics is not just about reducing the bit-width of numbers but understanding the trade-offs between accuracy and efficiency. As machine learning models become more pervasive, especially in real-world, resource-constrained environments, the focus on efficient numerics will continue to grow. By thoughtfully selecting and leveraging the appropriate numeric precision, one can achieve robust model performance while optimizing for speed, memory, and energy. The table below summarizes them.\n\n\n\n\n\n\n\n\nPrecision\nPros\nCons\n\n\n\n\nFP32 (Floating Point 32-bit)\n- Standard precision used in most deep learning frameworks. - High accuracy due to ample representational capacity. - Well-suited for training.\n- High memory usage. - Slower inference times compared to quantized models. - Higher energy consumption.\n\n\nFP16 (Floating Point 16-bit)\n- Reduces memory usage compared to FP32. - Speeds up computations on hardware that supports FP16. - Often used in mixed-precision training to balance speed and accuracy.\n- Lower representational capacity compared to FP32. - Risk of numerical instability in some models or layers.\n\n\nINT8 (8-bit Integer)\n- Significantly reduced memory footprint compared to floating-point representations. - Faster inference if hardware supports INT8 computations. - Suitable for many post-training quantization scenarios.\n- Quantization can lead to some accuracy loss. - Requires careful calibration during quantization to minimize accuracy degradation.\n\n\nINT4 (4-bit Integer)\n- Even lower memory usage than INT8. - Further speed-up potential for inference.\n- Higher risk of accuracy loss compared to INT8. - Calibration during quantization becomes more critical.\n\n\nBinary\n- Minimal memory footprint (only 1 bit per parameter). - Extremely fast inference due to bitwise operations. - Power efficient.\n- Significant accuracy drop for many tasks. - Complex training dynamics due to extreme quantization.\n\n\nTernary\n- Low memory usage but slightly more than binary. - Offers a middle ground between representation and efficiency.\n- Accuracy might still be lower than higher precision models. - Training dynamics can be complex.\n\n\n\n\n\n9.6.2 Efficiency Benefits\nNumerical efficiency matters for machine learning workloads for a number of reasons:\nComputational Efficiency: High-precision computations (like FP32 or FP64) can be slow and resource-intensive. By reducing numeric precision, one can achieve faster computation times, especially on specialized hardware that supports lower precision.\nMemory Efficiency: Storage requirements decrease with reduced numeric precision. For instance, FP16 requires half the memory of FP32. This is crucial when deploying models to edge devices with limited memory or when working with very large models.\nPower Efficiency: Lower precision computations often consume less power, which is especially important for battery-operated devices.\nNoise Introduction: Interestingly, the noise introduced by using lower precision can sometimes act as a regularizer, helping to prevent overfitting in some models.\nHardware Acceleration: Many modern AI accelerators and GPUs are optimized for lower precision operations, leveraging the efficiency benefits of such numerics."
  },
  {
    "objectID": "efficient_ai.html#evaluating-models",
    "href": "efficient_ai.html#evaluating-models",
    "title": "9¬† Efficient AI",
    "section": "9.7 Evaluating Models",
    "text": "9.7 Evaluating Models\nIt‚Äôs worth noting that the actual benefits and trade-offs can vary based on the specific architecture of the neural network, the dataset, the task, and the hardware being used. Before deciding on a numeric precision, it‚Äôs advisable to perform experiments to evaluate the impact on the desired application.\n\n9.7.1 Efficiency Metrics\nTo guide this process systematically, it is important to have a deep understanding of model evaluation methods. When assessing AI models‚Äô effectiveness and suitability for various applications, efficiency metrics come to the forefront.\nFLOPs (Floating Point Operations) gauge the computational demands of a model. For instance, a modern neural network like BERT has billions of FLOPs, which might be manageable on a powerful cloud server but would be taxing on a smartphone. Higher FLOPs can lead to more prolonged inference times and more significant power drain, especially on devices without specialized hardware accelerators. Hence, for real-time applications such as video streaming or gaming, models with lower FLOPs might be more desirable.\nMemory Usage pertains to how much storage the model requires, which affects both the storage and RAM of the deploying device. Consider deploying a model onto a smartphone: a model that occupies several gigabytes of space not only consumes precious storage but might also be slower due to the need to load large weights into memory. This becomes especially crucial for edge devices like security cameras or drones, where minimal memory footprints are vital for both storage and rapid data processing.\nPower Consumption becomes especially crucial for devices that rely on batteries. For instance, a wearable health monitor using a power-hungry model could drain its battery in hours, rendering it impractical for continuous health monitoring. As we move toward an era dominated by IoT devices, where many devices operate on battery power, optimizing models for low power consumption becomes essential.\nInference Time is about how swiftly a model can produce results. In applications like autonomous driving, where split-second decisions are the difference between safety and calamity, models must operate rapidly. If a self-driving car‚Äôs model takes even a few seconds too long to recognize an obstacle, the consequences could be dire. Hence, ensuring a model‚Äôs inference time aligns with the real-time demands of its application is paramount.\nIn essence, these efficiency metrics are more than mere numbers‚Äîthey dictate where and how a model can be effectively deployed. A model might boast high accuracy, but if its FLOPs, memory usage, power consumption, or inference time make it unsuitable for its intended platform or real-world scenarios, its practical utility becomes limited.\n\n\n9.7.2 Efficiency Comparisons\nThere is an abundance of models in the ecosystem, each boasting its unique strengths and idiosyncrasies. However, pure model accuracy figures or training and inference speeds don‚Äôt paint the complete picture. When we dive deeper into comparative analyses, several critical nuances emerge.\nOften, we encounter the delicate balance between accuracy and efficiency. For instance, while a dense deep learning model and a lightweight MobileNet variant might both excel in image classification, their computational demands could be at two extremes. This differentiation is especially pronounced when comparing deployments on resource-abundant cloud servers versus constrained TinyML devices. In many real-world scenarios, the marginal gains in accuracy could be overshadowed by the inefficiencies of a resource-intensive model.\nMoreover, the optimal model choice isn‚Äôt always universal but often depends on the specifics of an application. Consider object detection: a model that excels in general scenarios might falter in niche environments like detecting manufacturing defects on a factory floor. This adaptability‚Äîor the lack of it‚Äîcan dictate a model‚Äôs real-world utility.\nAnother important consideration is the relationship between model complexity and its practical benefits. Take voice-activated assistants as an example such as ‚ÄúAlexa‚Äù or ‚ÄúOK Google.‚Äù While a complex model might demonstrate a marginally superior understanding of user speech, if it‚Äôs slower to respond than a simpler counterpart, the user experience could be compromised. Thus, adding layers or parameters doesn‚Äôt always equate to better real-world outcomes.\nFurthermore, while benchmark datasets, such as ImageNet, COCO, Visual Wake Words, Google Speech Commands, etc. provide a standardized performance metric, they might not capture the diversity and unpredictability of real-world data. Two facial recognition models with similar benchmark scores might exhibit varied competencies when faced with diverse ethnic backgrounds or challenging lighting conditions. Such disparities underscore the importance of robustness and consistency across varied data. For example, the image below from the Dollar Street dataset shows stove images across extreme monthly incomes.\n\n\n\nDollar Street stove images.\n\n\nIn essence, a thorough comparative analysis transcends numerical metrics. It‚Äôs a holistic assessment, intertwined with real-world applications, costs, and the intricate subtleties that each model brings to the table. This is why it becomes important to have standard benchmarks and metrics that are widely estabslished and adopted by the community."
  },
  {
    "objectID": "efficient_ai.html#conclusion",
    "href": "efficient_ai.html#conclusion",
    "title": "9¬† Efficient AI",
    "section": "9.8 Conclusion",
    "text": "9.8 Conclusion\nEfficient AI is extremely important as we push towards broader and more diverse real-world deployment of machine learning. This chapter provided an overview, exploring the various methodologies and considerations behind achieving efficient AI, starting with the fundamental need, similarities and differences across cloud, edge, and TinyML systems.\nWe saw that efficient model architectures can be useful for optimizations. Model compression techniques such as pruning, quantization, and knowledge distillation exist to help reduce computational demands and memory footprint without significantly impacting accuracy. Specialized hardware like TPUs and NN accelerators offer optimized silicon for the operations and data flow of neural networks. And efficient numerics strike a balance between precision and efficiency, enabling models to attain robust performance using minimal resources. In the subsequent chapters, we will dive deeper into each of these different topics and explore them in great depth and detail.\nTogether, these form a holistic framework for efficient AI. But the journey doesn‚Äôt end here. Achieving optimally efficient intelligence requires continued research and innovation. As models become more sophisticated, datasets grow larger, and applications diversify into specialized domains, efficiency must evolve in lockstep. Measuring real-world impact would need nuanced benchmarks and standardized metrics beyond simplistic accuracy figures.\nMoreover, efficient AI expands beyond technological optimization but also encompasses costs, environmental impact, and ethical considerations for the broader societal good. As AI permeates across industries and daily lives, a comprehensive outlook on efficiency underpins its sustainable and responsible progress. The subsequent chapters will build upon these foundational concepts, providing actionable insights and hands-on best practices for developing and deploying efficient AI solutions."
  },
  {
    "objectID": "optimizations.html#introduction",
    "href": "optimizations.html#introduction",
    "title": "10¬† Model Optimizations",
    "section": "10.1 Introduction",
    "text": "10.1 Introduction\nWhen machine learning models are deployed on systems, especially on resource-constrained embedded systems, the optimization of models is a necessity. While machine learning inherently often demands substantial computational resources, the systems are inherently limited in memory, processing power, and energy. This chapter will dive into the art and science of optimizing machine learning models to ensure they are lightweight, efficient, and effective when deployed in TinyML scenarios.\nWe have structured this chapter in three tiers. First, in Section¬†10.2 we examine the significance and methodologies of reducing the parameter complexity of models without compromising their inference capabilities. Techniques such as pruning and knowledge distillation are discussed, offering insights into how models can be compressed and simplified while maintaining, or even enhancing, their performance.\nGoing one level lower, in Section¬†10.3, we study the role of numerical precision in model computations and how altering it impacts model size, speed, and accuracy. We will examine the various numerical formats and how reduced-precision arithmetic can be leveraged to optimize models for embedded deployment.\nFinally, as we go lower closer to the hardware, in Section¬†10.4, we will navigate through the landscape of hardware-software co-design, exploring how models can be optimized by tailoring them to the specific characteristics and capabilities of the target hardware. We will discuss how models can be adapted to exploit the available hardware resources effectively.\n\n\n\nA visualization showing each of the three sections to be covered on the hardware-software gradient."
  },
  {
    "objectID": "optimizations.html#sec-model_ops_representation",
    "href": "optimizations.html#sec-model_ops_representation",
    "title": "10¬† Model Optimizations",
    "section": "10.2 Efficient Model Representation",
    "text": "10.2 Efficient Model Representation\nThe first avenue of attack for model optimization starts in familiar territory for most ML practitioners: efficient model representation is often first tackled at the highest level of parametrization abstraction: the model‚Äôs architecture itself.\nMost traditional ML practitioners design models with a general high-level objective in mind, whether it be image classification, person detection, or keyword spotting as mentioned previously in this textbook. Their designs generally end up naturally fitting into some soft constraints due to limited compute resources during development, but generally these designs are not aware of later constraints, such as those required if the model is to be deployed on a more constrained device instead of the cloud.\nIn this section, we‚Äôll discuss how practitioners can harness principles of hardware-software co-design even at a model‚Äôs high level architecture to make their models compatible with edge devices. From most to least hardware aware at this level of modification, we discuss several of the most common strategies for efficient model parametrization: pruning, model compression, and edge-friendly model architectures.\n\n10.2.1 Pruning\n\nOverview\nModel pruning is a technique in machine learning that aims to reduce the size and complexity of a neural network model while maintaining its predictive capabilities as much as possible. The goal of model pruning is to remove redundant or non-essential components of the model, including connections between neurons, individual neurons, or even entire layers of the network.\nThis process typically involves analyzing the machine learning model to identify and remove weights, nodes, or layers that have little impact on the model‚Äôs outputs. By selectively pruning a model in this way, the total number of parameters can be reduced significantly without substantial declines in model accuracy. The resulting compressed model requires less memory and computational resources to train and run while enabling faster inference times.\nModel pruning is especially useful when deploying machine learning models to devices with limited compute resources, such as mobile phones or TinyML systems. The technique facilitates the deployment of larger, more complex models on these devices by reducing their resource demands. Additionally, smaller models require less data to generalize well and are less prone to overfitting. By providing an efficient way to simplify models, model pruning has become a vital technique for optimizing neural networks in machine learning.\nThere are several common pruning techniques used in machine learning, these include structured pruning, unstructured pruning, iterative pruning, bayesian pruning, and even random pruning. In addition to pruning the weights, one can also prune the activations. Activation pruning specifically targets neurons or filters that activate rarely or have overall low activation. There are numerous other methods, such as sensitivity and movement pruning. For a comprehensive list of methods, the reader is encouraged to read the following paper: ‚ÄúA Survey on Deep Neural Network Pruning: Taxonomy, Comparison, Analysis, and Recommendations‚Äù (2023).\nSo how does one choose the type of pruning methods? Many variations of pruning techniques exist where each varies the heuristic of what should be kept and pruned from the model as well the number of times pruning occurs. Traditionally, pruning happens after the model is fully trained, where the pruned model may experience mild accuracy loss. However, as we will discuss further, recent discoveries have found that pruning can be used during training (i.e., iteratively) to identify more efficient and accurate model representations.\n\n\nStructured Pruning\nWe start with structured pruning, a technique that reduces the size of a neural network by eliminating entire model-specific substructures while maintaining the overall model structure. It removes entire neurons/filters or layers based on importance criteria. For example, for a convolutional neural network (CNN), this could be certain filter instances or channels. For fully connected networks, this could be neurons themselves while maintaining full connectivity or even be elimination of entire model layers that are deemed to be insignificant. This type of pruning often leads to regular, structured sparse networks that are hardware friendly.\n\nComponents\nBest practices have started to emerge on how to think about structured pruning. There are three main components:\n\nStructures to target for pruning\nEstablishing a criteria for pruning\nSelecting a pruning strategy\n\n\n\nStructures to target for pruning\nGiven that there are different strategies, each of these structures (i.e., neurons, channels and layers) is pruned based on specific criteria and strategies, ensuring that the reduced model maintains as much of the predictive prowess of the original model as possible while gaining in computational efficiency and reduction in size.\nThe primary structures targeted for pruning include neurons , channels, and sometimes, entire layers, each having its unique implications and methodologies. When neurons are pruned, we are removing entire neurons along with their associated weights and biases, thereby reducing the width of the layer. This type of pruning is often utilized in fully connected layers.\nWith channel pruning, which is predominantly applied in convolutional neural networks (CNNs), it involves eliminating entire channels or filters, which in turn reduces the depth of the feature maps and impacts the network‚Äôs ability to extract certain features from the input data. This is particularly crucial in image processing tasks where computational efficiency is paramount.\nFinally, layer pruning takes a more aggressive approach by removing entire layers of the network. This significantly reduces the network‚Äôs depth and thereby its capacity to model complex patterns and hierarchies in the data. This approach necessitates a careful balance to ensure that the model‚Äôs predictive capability is not unduly compromised.\n\n\nEstablishing a criteria for pruning\nEstablishing well-defined criteria for determining which specific structures to prune from a neural network model is a crucial component of the model pruning process. The core goal here is to identify and remove components that contribute the least to the model‚Äôs predictive capabilities, while retaining structures integral to preserving the model‚Äôs accuracy.\nA widely adopted and effective strategy for systematically pruning structures relies on computing importance scores for individual components like neurons, filters, channels or layers. These scores serve as quantitative metrics to gauge the significance of each structure and its effect on the model‚Äôs output.\nThere are several techniques for assigning these importance scores:\n\nWeight magnitude-based pruning assigns scores based on the absolute values of the weights. Components with very small weights contribute minimally to activations and can be removed.\nGradient-based pruning utilizes the gradients of the loss function with respect to each weight to determine sensitivity. Weights with low gradient magnitudes when altered have little effect on the loss and can be pruned.\nActivation-based pruning tracks activation values for neurons/filters over a validation dataset. Consistently low activation values suggest less relevance, warranting removal.\nTaylor expansion approximates the change in loss function from removing a given weight. Weights with negligible impact on loss are prime candidates for pruning.\n\nThe idea is to measure, either directly or indirectly, the contribution of each component to the model‚Äôs output. Structures with minimal influence according to the defined criteria are pruned first. This enables selective, optimized pruning that maximally compresses models while preserving predictive capacity. In general, it is important to evaluate the impact of removing particular structures on the model‚Äôs output.\n\n\nSelecting a pruning strategy\nThe pruning strategy orchestrates how structures are removed and integrates with subsequent model fine-tuning to recover predictive performance. Two main structured pruning strategies exist: iterative pruning and one-shot pruning.\nIterative pruning gradually removes structures across multiple cycles of pruning followed by fine-tuning. In each cycle, a small set of structures are pruned based on importance criteria. The model is then fine-tuned, allowing it to adjust smoothly to the structural changes before the next pruning iteration. This gradual, cyclic approach prevents abrupt accuracy drops. It allows the model to slowly adapt as structures are reduced across iterations.\nOne-shot pruning takes a more aggressive approach by pruning a large portion of structures simultaneously in one shot based on predefined importance criteria. This is followed by extensive fine-tuning to recover model accuracy. While faster, this aggressive strategy can degrade accuracy if the model cannot recover during fine-tuning.\nThe choice between these strategies involves weighing factors like model size, target sparsity level, available compute and acceptable accuracy losses. One-shot pruning can rapidly compress models, but iterative pruning may enable better accuracy retention for a target level of pruning. In practice, the strategy is tailored based on use case constraints. The overarching aim is to generate an optimal strategy that removes redundancy, achieves efficiency gains through pruning, and finely tunes the model to stabilize accuracy at an acceptable level for deployment.\n\n\n\nAdvantages of Structured Pruning\nStructured pruning brings forth a myriad of advantages that cater to various facets of model deployment and utilization, especially in environments where computational resources are constrained.\n\nComputational Efficiency\nBy eliminating entire structures, such as neurons or channels, structured pruning significantly diminishes the computational load during both training and inference phases, thereby enabling faster model predictions and training convergence. Moreover, the removal of structures inherently reduces the model‚Äôs memory footprint, ensuring that it demands less storage and memory during operation, which is particularly beneficial in memory-constrained environments like TinyML systems.\n\n\nHardware Efficiency\nStructured pruning often results in models that are more amenable to deployment on specialized hardware, such as Field-Programmable Gate Arrays (FPGAs) or Application-Specific Integrated Circuits (ASICs), due to the regularity and simplicity of the pruned architecture. With reduced computational requirements, it translates to lower energy consumption, which is crucial for battery-powered devices and sustainable computing practices.\n\n\nMaintenance and Deployment\nThe pruned model, while smaller, retains its original architectural form, which can simplify the deployment pipeline and ensure compatibility with existing systems and frameworks. Also, with fewer parameters and simpler structures, the pruned model becomes easier to manage and monitor in production environments, potentially reducing the overhead associated with model maintenance and updates. Later on, when we dive into MLOps, this need will become apparent.\n\n\n\nUnstructured Pruning\nUnstructured pruning is, as its name suggests, pruning the model without regard to model-specific substructure. As mentioned above, it offers a greater aggression in pruning and can achieve higher model sparsities while maintaining accuracy given less constraints on what can and can‚Äôt be pruned. Generally, post-training unstructured pruning consists of an importance criterion for individual model parameters/weights, pruning/removal of weights that fall below the criteria, and optional fine-tuning after to try and recover the accuracy lost during weight removal.\nUnstructured pruning has some advantages over structured pruning: removing individual weights instead of entire model substructures often leads in practice to lower model accuracy hits. Furthermore, generally determining the criterion of importance for an individual weight is much simpler than for an entire substructure of parameters in structured pruning, making the former preferable for cases where that overhead is hard or unclear to compute. Similarly, the actual process of structured pruning is generally less flexible, as removing individual weights is generally simpler than removing entire substructures and ensuring the model still works.\nUnstructured pruning, while offering the potential for significant model size reduction and enhanced deployability, brings with it challenges related to managing sparse representations and ensuring computational efficiency. It is particularly useful in scenarios where achieving the highest possible model compression is paramount and where the deployment environment can handle sparse computations efficiently.\nThe following compact table provides a concise comparison between structured and unstructured pruning. In this table, aspects related to the nature and architecture of the pruned model (Definition, Model Regularity, and Compression Level) are grouped together, followed by aspects related to computational considerations (Computational Efficiency and Hardware Compatibility), and ending with aspects related to the implementation and adaptation of the pruned model (Implementation Complexity and Fine-Tuning Complexity). Both pruning strategies offer unique advantages and challenges, and the selection between them should be influenced by specific project and deployment requirements.\n\n\n\n\n\n\n\n\nAspect\nStructured Pruning\nUnstructured Pruning\n\n\n\n\nDefinition\nPruning entire structures (e.g., neurons, channels, layers) within the network.\nPruning individual weights or neurons, resulting in sparse matrices or non-regular network structures.\n\n\nModel Regularity\nMaintains a regular, structured network architecture.\nResults in irregular, sparse network architectures.\n\n\nCompression Level\nMay offer limited model compression compared to unstructured pruning.\nCan achieve higher model compression due to fine-grained pruning.\n\n\nComputational Efficiency\nTypically more computationally efficient due to maintaining regular structures.\nCan be computationally inefficient due to sparse weight matrices, unless specialized hardware/software is used.\n\n\nHardware Compatibility\nGenerally better compatible with various hardware due to regular structures.\nMay require hardware that efficiently handles sparse computations to realize benefits.\n\n\nImplementation Complexity\nOften simpler to implement and manage due to maintaining network structure.\nCan be complex to manage and compute due to sparse representations.\n\n\nFine-Tuning Complexity\nMay require less complex fine-tuning strategies post-pruning.\nMight necessitate more complex retraining or fine-tuning strategies post-pruning.\n\n\n\n\n\n\nA visualization showing the differences and examples between unstructured and structured pruning. Observe that unstructured pruning can lead to models that no longer obey high-level structural guaruntees of their original unpruned counterparts: the left network is no longer a fully connected network after pruning. Structured pruning on the other hand maintains those invariants: in the middle, the fully connected network is pruned in a way that the pruned network is still fully connected; likewise, the CNN maintains its convolutional structure, albeit with fewer filters (Credit: EURASIP).\n\n\n\n\nLottery Ticket Hypothesis\nPruning has evolved from a purely post-training technique that came at the cost of some accuracy, to a powerful meta-learning approach applied during training to reduce model complexity. This advancement in turn improves compute, memory, and latency efficiency at both training and inference.\nA breakthrough finding that catalyzed this evolution was the lottery ticket hypothesis empirically discovered by Jonathan Frankle and Michael Carbin. Their work states that within dense neural networks, there exist sparse subnetworks, referred to as ‚Äúwinning tickets,‚Äù that can match or even exceed the performance of the original model when trained in isolation. Specifically, these winning tickets, when initialized using the same weights as the original network, can achieve similarly high training convergence and accuracy on a given task. It is worthwhile pointing out that they empirically discovered the lottery ticket hypothesis, which was later formalized.\nMore formally, the lottery ticket hypothesis is a concept in deep learning that suggests that within a neural network, there exist sparse subnetworks (or ‚Äúwinning tickets‚Äù) that, when initialized with the right weights, are capable of achieving high training convergence and inference performance on a given task. The intuition behind this hypothesis is that, during the training process of a neural network, many neurons and connections become redundant or unimportant, particularly with the inclusion of training techniques encouraging redundancy like dropout. Identifying, pruning out, and initializing these ‚Äúwinning tickets‚Äô‚Äô allows for faster training and more efficient models, as they contain the essential model decision information for the task. Furthermore, as generally known with the bias-variance tradeoff theory, these tickets suffer less from overparameterization and thus generalize better rather than overfitting to the task. \n\n\n\nAn example experiment from the lottery ticket hypothesis showing pruning and training experiments on a fully connected LeNet over a variety of pruning ratios: note the first plot showing how pruning is able to reveal a subnetwork nearly one-fifth the size that trains to a higher test accuracy faster than the unpruned network. However, further note how in the second plot that further pruned models in models that both train slower and are not able to achieve that same maximal test accuracy due to the lower number of parameters (Credit: ICLR).\n\n\n\n\nChallenges & Limitations\nThere is no free lunch with pruning optimizations.\n\nQuality vs.¬†Size Reduction\nA key challenge in both structured and unstructured pruning is balancing size reduction with maintaining or improving predictive performance. This trade-off becomes more complex with unstructured pruning, where individual weight removal can create sparse weight matrices. Ensuring the pruned model retains generalization capacity while becoming more computationally efficient is critical, often requiring extensive experimentation and validation.\n\n\nDetermining Pruning Criteria\nEstablishing a robust pruning criteria, whether for removing entire structures (structured pruning) or individual weights (unstructured pruning), is challenging. The criteria must accurately identify elements whose removal minimally impacts performance. For unstructured pruning, this might involve additional complexities due to the potential for generating sparse weight matrices, which can be computationally inefficient on certain hardware.\n\n\nFine-Tuning and Retraining\nPost-pruning fine-tuning is imperative in both structured and unstructured pruning to recover lost performance and stabilize the model. The challenge encompasses determining the extent, duration, and nature of the fine-tuning process, which can be influenced by the pruning method and the degree of pruning applied.\n\n\nScalability of Pruning Strategies\nEnsuring that pruning strategies, whether structured or unstructured, are scalable and applicable across various models and domains is challenging. Unstructured pruning might introduce additional challenges related to managing and deploying models with sparse weight matrices, especially in hardware that is not optimized for sparse computations.\n\n\nHardware Compatibility and Efficiency\nEspecially pertinent to unstructured pruning, hardware compatibility and efficiency become critical. Unstructured pruning often results in sparse weight matrices, which may not be efficiently handled by certain hardware, potentially negating the computational benefits of pruning. Ensuring that pruned models, particularly those resulting from unstructured pruning, are compatible and efficient on the target hardware is a significant consideration.\n\n\nComplexity in Implementing Pruning Algorithms\nUnstructured pruning might introduce additional complexity in implementing pruning algorithms due to the need to manage sparse representations of weights. Developing or adapting algorithms that can efficiently handle, store, and compute sparse weight matrices is an additional challenge and consideration in unstructured pruning.\n\n\nLegal and Ethical Considerations\nLast but not least, adherence to legal and ethical guidelines is paramount, especially in domains with significant consequences. Both pruning methods must undergo rigorous validation, testing, and potentially certification processes to ensure compliance with relevant regulations and standards. This is especially important in use cases like medical AI applications or autonomous driving where quality drops due to pruning like optimizationscan be life threatening.\n\n\n\n\n10.2.2 Model Compression\nModel compression techniques are crucial for deploying deep learning models on resource-constrained devices. These techniques aim to create smaller, more efficient models that preserve the predictive performance of the original models.\n\nKnowledge Distillation\nOne popular technique is knowledge distillation (KD), which transfers knowledge from a large, complex ‚Äúteacher‚Äù model to a smaller ‚Äústudent‚Äù model. The key idea is to train the student model to mimic the teacher‚Äôs outputs.The concept of KD was first popularized by the work of Geoffrey Hinton, Oriol Vinyals, and Jeff Dean in their paper ‚ÄúDistilling the Knowledge in a Neural Network‚Äù (2015).\n\nOverview and Benefits\nAt its core, KD strategically leverages the refined outputs of a pre-trained teacher model to transfer knowledge to a smaller student model. The key technique is using ‚Äúsoft targets‚Äù derived from the teacher‚Äôs probabilistic predictions. Specifically, the teacher‚Äôs outputs are passed through a temperature-scaled softmax function, yielding softened probability distributions over classes. This softening provides richer supervision signals for the student model compared to hard target labels.\nThe loss function is another critical component that typically amalgamates a distillation loss, which measures the divergence between the teacher and student outputs, and a classification loss, which ensures the student model adheres to the true data labels. The Kullback-Leibler (KL) divergence is commonly employed to quantify the distillation loss, providing a measure of the discrepancy between the probability distributions output by the teacher and student models.\nAnother core concept is ‚Äútemperature scaling‚Äù in the softmax function. It plays the role in controlling the granularity of the information distilled from the teacher model. A higher temperature parameter produces softer, more informative distributions, thereby facilitating the transfer of more nuanced knowledge to the student model. However, it also introduces the challenge of effectively balancing the trade-off between the informativeness of the soft targets and the stability of the training process.\nThese components, when adeptly configured and harmonized, enable the student model to assimilate the teacher model‚Äôs knowledge, crafting a pathway towards efficient and robust smaller models that retain the predictive prowess of their larger counterparts.\n\n\n\nA visualization of the training procedure of knowledge distillation. Note how the logits or soft labels of the teacher model are used to provide a distillation loss for the student model to learn from (Credit: IntelLabs).\n\n\n\n\nChallenges\nHowever, KD has a unique set of challenges and considerations that researchers and practitioners must attentively address. One of the challenges is in the meticulous tuning of hyperparameters, such as the temperature parameter in the softmax function and the weighting between the distillation and classification loss in the objective function. Striking a balance that effectively leverages the softened outputs of the teacher model while maintaining fidelity to the true data labels is non-trivial and can significantly impact the student model‚Äôs performance and generalization capabilities.\nFurthermore, the architecture of the student model itself poses a considerable challenge. Designing a model that is compact to meet computational and memory constraints, while still being capable of assimilating the essential knowledge from the teacher model, demands a nuanced understanding of model capacity and the inherent trade-offs involved in compression. The student model must be carefully architected to navigate the dichotomy of size and performance, ensuring that the distilled knowledge is meaningfully captured and utilized. Moreover, the choice of teacher model, which inherently influences the quality and nature of the knowledge to be transferred, is important and it introduces an added layer of complexity to the KD process.\nThese challenges underscore the necessity for a thorough and nuanced approach to implementing KD, ensuring that the resultant student models are both efficient and effective in their operational contexts.\n\n\n\nLow-rank Matrix Factorization\nSimilar in approximation theme, low-rank matrix factorization (LRFM) is a mathematical technique used in linear algebra and data analysis to approximate a given matrix by decomposing it into two or more lower-dimensional matrices. The fundamental idea is to express a high-dimensional matrix as a product of lower-rank matrices, which can help reduce the complexity of data while preserving its essential structure. Mathematically, given a matrix \\(A \\in \\mathbb{R}^{m \\times n}\\), LRMF seeks matrices \\(U \\in \\mathbb{R}^{m \\times k}\\) and \\(V \\in \\mathbb{R}^{k \\times n}\\) such that \\(A \\approx UV\\), where \\(k\\) is the rank and is typically much smaller than \\(m\\) and \\(n\\).\n\nBackground and Benefits\nOne of the seminal works in the realm of matrix factorization, particularly in the context of recommendation systems, is the paper by Yehuda Koren, Robert Bell, and Chris Volinsky, ‚ÄúMatrix Factorization Techniques for Recommender Systems‚Äù (2009). The authors delve into various factorization models, providing insights into their efficacy in capturing the underlying patterns in the data and enhancing predictive accuracy in collaborative filtering. LRFM has been widely applied in recommendation systems (such as Netflix, Facebook, etc.), where the user-item interaction matrix is factorized to capture latent factors corresponding to user preferences and item attributes.\nThe main advantage of low-rank matrix factorization lies in its ability to reduce data dimensionality as shown in the image below where there are fewer parameters to store, making it computationally more efficient and reducing storage requirements at the cost of some additional compute. This can lead to faster computations and more compact data representations, which is especially valuable when dealing with large datasets. Additionally, it may aid in noise reduction and can reveal underlying patterns and relationships in the data.\n\n\n\nA visualization showing the decrease in parameterization enabled by low-rank matrix factorization. Observe how the matrix \\(M\\) can be approximated by the product of matrices \\(L_k\\) and \\(R_k^T\\). For intuition, most fully connected layers in networks are stored as a projection matrix \\(M\\), which requires \\(m \\times n\\) parameter to be loaded on computation. However, by decomposing and approximating it as the product of two lower rank matrices, we thus only need to store \\(m \\times k + k\\times n\\) parameters in terms of storage while incurring an additional compute cost of the matrix multiplication.__So long as \\(k \\&lt; n/2\\), this factorization has fewer parameters total to store while adding a computation of runtime \\(O(mkn)\\) (Credit: Medium).\n\n\n\n\nChallenges\nBut practitioners and researchers encounter a spectrum of challenges and considerations that necessitate careful attention and strategic approaches. As with any lossy compression technique, we may lose information during this approximation process: choosing the correct rank that balances the information lost and the computational costs is tricky as well and adds an additional hyper-parameter to tune for.\nLow-rank matrix factorization is a valuable tool for dimensionality reduction and making compute fit onto edge devices but, like other techniques, needs to be carefully tuned to the model and task at hand. A key challenge resides in managing the computational complexity inherent to LRMF, especially when grappling with high-dimensional and large-scale data. The computational burden, particularly in the context of real-time applications and massive datasets, remains a significant hurdle for effectively using LRFM.\nMoreover, the conundrum of choosing the optimal rank, (k), for the factorization introduces another layer of complexity. The selection of (k) inherently involves a trade-off between approximation accuracy and model simplicity, and identifying a rank that adeptly balances these conflicting objectives often demands a combination of domain expertise, empirical validation, and sometimes, heuristic approaches. The challenge is further amplified when the data encompasses noise or when the inherent low-rank structure is not pronounced, making the determination of a suitable (k) even more elusive.\nHandling missing or sparse data, a common occurrence in applications like recommendation systems, poses another substantial challenge. Traditional matrix factorization techniques, such as Singular Value Decomposition (SVD), are not directly applicable to matrices with missing entries, necessitating the development and application of specialized algorithms that can factorize incomplete matrices while mitigating the risks of overfitting to the observed entries. This often involves incorporating regularization terms or constraining the factorization in specific ways, which in turn introduces additional hyperparameters that need to be judiciously selected.\nFurthermore, in scenarios where data evolves or grows over time, developing LRMF models that can adapt to new data without necessitating a complete re-factorization is a critical yet challenging endeavor. Online and incremental matrix factorization algorithms seek to address this by enabling the update of factorized matrices as new data arrives, yet ensuring stability, accuracy, and computational efficiency in these dynamic settings remains an intricate task. This is particularly challenging in the space of TinyML, where edge redeployment for refreshed models can be quite challenging.\n\n\n\nTensor Decomposition\nSimilar to low-rank matrix factorization, more complex models may store weights in higher dimensions, such as tensors: tensor decomposition is the higher-dimensional analogue of matrix factorization, where a model tensor is decomposed into lower rank components, which again are easier to compute on and store but may suffer from the same issues as mentioned above of information loss and nuanced hyperparameter tuning. Mathematically, given a tensor \\(\\mathcal{A}\\), tensor decomposition seeks to represent \\(\\mathcal{A}\\) as a combination of simpler tensors, facilitating a compressed representation that approximates the original data while minimizing the loss of information.\nThe work of Tamara G. Kolda and Brett W. Bader, ‚ÄúTensor Decompositions and Applications‚Äù (2009), stands out as a seminal paper in the field of tensor decompositions. The authors provide a comprehensive overview of various tensor decomposition methods, exploring their mathematical underpinnings, algorithms, and a wide array of applications, ranging from signal processing to data mining. Of course, the reason we are discussing it is because it has huge potential for system performance improvements, particularly in the space of TinyML, where throughput and memory footprint savings are crucial to feasibility of deployments .\n\n\n\nA visualization showing the decrease in parameterization enabled by tensor decomposition. Observe how the shown three-dimensional tensor \\(y\\) can be approximated by three matrices, all of lower rank, greatly reducing the number of parameters to be held in memory. (Credit: Medium).\n\n\n\n\n\n10.2.3 Edge-Aware Model Design\nFinally, we reach the other end of the gradient, where we specifically make model architecture decisions directly given knowledge of the edge devices we wish to deploy on.\nAs covered in previous sections, edge devices are constrained specifically with limitations on memory and parallelizable computations: as such, if there are critical inference speed requirements, computations must be flexible enough to satisfy hardware constraints, something that can be designed at the model architecture level. Furthermore, trying to cram SOTA large ML models onto edge devices even after pruning and compression is generally infeasible purely due to size: the model complexity itself must be chosen with more nuance as to more feasibly fit the device. Edge ML developers have approached this architectural challenge both through designing bespoke edge ML model architectures and through device-aware neural architecture search (NAS), which can more systematically generate feasible on-device model architectures.\n\nModel Design Techniques\nOne edge friendly architecture design is depthwise separable convolutions. Commonly used in deep learning for image processing, it consists of two distinct steps: the first is the depthwise convolution, where each input channel is convolved independently with its own set of learnable filters. This step reduces computational complexity by a significant margin compared to standard convolutions, as it drastically reduces the number of parameters and computations involved. The second step is the pointwise convolution, which combines the output of the depthwise convolution channels through a 1x1 convolution, creating inter-channel interactions. This approach offers several advantages. Pros include reduced model size, faster inference times, and often better generalization due to fewer parameters, making it suitable for mobile and embedded applications. However, depthwise separable convolutions may not capture complex spatial interactions as effectively as standard convolutions and might require more depth (layers) to achieve the same level of representational power, potentially leading to longer training times. Nonetheless, their efficiency in terms of parameters and computation makes them a popular choice in modern convolutional neural network architectures.\n\n\n\nA visualization showing each of the individual operations behind a single depthwise separable convolution: first, we give the input image a convolution without modifying the depth. Once those convolutions are completed, we then do a pointwise 1-by-1 convolution to get to the desired number of channels. This reduces the number of parameters, making it a key TinyML technique (Credit: AnalyticsVidhya).\n\n\n\n\nExample Model Architectures\nIn this vein, a number of recent architectures have been, from inception, specifically designed for maximizing accuracy on an edge deployment, notably SqueezeNet, MobileNet, and EfficientNet. SqueezeNet, for instance, utilizes a compact architecture with 1x1 convolutions and fire modules to minimize the number of parameters while maintaining strong accuracy. MobileNet, on the other hand, employs the aforementioned depthwise separable convolutions to reduce both computation and model size. EfficientNet takes a different approach by optimizing network scaling (i.e.¬†varying the depth, width and resolution of a network) and compound scaling, a more nuanced variation network scaling, to achieve superior performance with fewer parameters. These models are essential in the context of edge computing where limited processing power and memory require lightweight yet effective models that can efficiently perform tasks such as image recognition, object detection, and more. Their design principles showcase the importance of intentionally tailored model architecture for edge computing, where performance and efficiency must fit within constraints.\n\n\nStreamlining Model Architecture Search\nFinally, systematized pipelines for searching for performant edge-compatible model architectures are possible through frameworks like TinyNAS and MorphNet.\nTinyNAS is an innovative neural architecture search framework introduced in the MCUNet paper, designed to efficiently discover lightweight neural network architectures for edge devices with limited computational resources. Leveraging reinforcement learning and a compact search space of micro neural modules, TinyNAS optimizes for both accuracy and latency, enabling the deployment of deep learning models on microcontrollers, IoT devices, and other resource-constrained platforms. Specifically, TinyNAS, in conjunction with a network optimizer TinyEngine, generates different search spaces by scaling the input resolution and the model width of a model, then collects the computation FLOPs distribution of satisfying networks within the search space to evaluate its priority. TinyNAS relies on the assumption that a search space that accommodates higher FLOPs under memory constraint can produce higher accuracy models, something that the authors verified in practice in their work. In empirical performance, TinyEngine reduced models the peak memory usage by around 3.4 times and accelerated inference by 1.7 to 3.3 times compared to TFLite and CMSIS-NN..\nSimilarly, MorphNet is a neural network optimization framework designed to automatically reshape and morph the architecture of deep neural networks, optimizing them for specific deployment requirements. It achieves this through two steps: first, it leverages a set of customizable network morphing operations, such as widening or deepening layers, to dynamically adjust the network‚Äôs structure. These operations enable the network to adapt to various computational constraints, including model size, latency, and accuracy targets, which are extremely prevalent in edge computing usage. In the second step, MorphNet uses a reinforcement learning-based approach to search for the optimal permutation of morphing operations, effectively balancing the trade-off between model size and performance. This innovative method allows deep learning practitioners to automatically tailor neural network architectures to specific application and hardware requirements, ensuring efficient and effective deployment across various platforms.\nTinyNAS and MorphNet represent a few of the many significant advancements in the field of systematic neural network optimization, allowing architectures to be systematically chosen and generated to fit perfectly within problem constraints."
  },
  {
    "objectID": "optimizations.html#sec-model_ops_numerics",
    "href": "optimizations.html#sec-model_ops_numerics",
    "title": "10¬† Model Optimizations",
    "section": "10.3 Efficient Numerics Representation",
    "text": "10.3 Efficient Numerics Representation\nNumerics representation involves a myriad of considerations, including but not limited to, the precision of numbers, their encoding formats, and the arithmetic operations facilitated. It invariably involves a rich array of different trade-offs, where practitioners are tasked with navigating between numerical accuracy and computational efficiency. For instance, while lower-precision numerics may offer the allure of reduced memory usage and expedited computations, they concurrently present challenges pertaining to numerical stability and potential degradation of model accuracy.\n\n10.3.1 Motivation\nThe imperative for efficient numerics representation arises, particularly as efficient model optimization alone falls short when adapting models for deployment on low-powered edge devices operating under stringent constraints.\nBeyond minimizing memory demands, the tremendous potential of efficient numerics representation lies in but is not limited to these fundamental ways. By diminishing computational intensity, efficient numerics can thereby amplify computational speed, allowing more complex models to compute on low-powered devices. Reducing the bit precision of weights and activations on heavily over-parameterized models enables condensation of model size for edge devices without significantly harming the model‚Äôs predictive accuracy. With the omnipresence of neural networks in models, efficient numerics has a unique advantage in leveraging the layered structure of NNs to vary numeric precision across layers, minimizing precision in resistant layers while preserving higher precision in sensitive layers.\nIn this segment, we‚Äôll delve into how practitioners can harness the principles of hardware-software co-design at the lowest levels of a model to facilitate compatibility with edge devices. Kicking off with an introduction to the numerics, we will examine its implications for device memory and computational complexity. Subsequently, we will embark on a discussion regarding the trade-offs entailed in adopting this strategy, followed by a deep dive into a paramount method of efficient numerics: quantization.\n\n\n10.3.2 The Basics\n\nTypes\nNumerical data, the bedrock upon which machine learning models stand, manifest in two primary forms. These are integers and floating point numbers.\nIntegers : Whole numbers, devoid of fractional components, integers (e.g., -3, 0, 42) are key in scenarios demanding discrete values. For instance, in ML, class labels in a classification task might be represented as integers, where ‚Äúcat‚Äù, ‚Äúdog‚Äù, and ‚Äúbird‚Äù could be encoded as 0, 1, and 2 respectively.\nFloating-Point Numbers: Encompassing real numbers, floating-point numbers (e.g., -3.14, 0.01, 2.71828) afford the representation of values with fractional components. In ML model parameters, weights might be initialized with small floating-point values, such as 0.001 or -0.045, to commence the training process. Currently, there are 4 popular precision formats discussed below.\nVariable bit widths: Beyond the standard widths, research is ongoing into extremely low bit-width numerics, even down to binary or ternary representations. Extremely low bit-width operations can offer significant speedups and reduce power consumption even further. While challenges remain in maintaining model accuracy with such drastic quantization, advances continue to be made in this area.\n\n\nPrecision\nPrecision, delineating the exactness with which a number is represented, bifurcates typically into single, double, half and in recent years there have been a number of other precisions that have emerged to better support machine learning tasks efficiently on the underlying hardware.\nDouble Precision (Float64): Allocating 64 bits, double precision (e.g., 3.141592653589793) provides heightened accuracy, albeit demanding augmented memory and computational resources. In scientific computations, where precision is paramount, variables like œÄ might be represented with Float64.\nSingle Precision (Float32): With 32 bits at its disposal, single precision (e.g., 3.1415927) strikes a balance between numerical accuracy and memory conservation. In ML, Float32 might be employed to store weights during training to maintain a reasonable level of precision.\nHalf Precision (Float16): Constrained to 16 bits, half precision (e.g., 3.14) curtails memory usage and can expedite computations, albeit sacrificing numerical accuracy and range. In ML, especially during inference on resource-constrained devices, Float16 might be utilized to reduce the model‚Äôs memory footprint.\nBfloat16: Brain Floating-Point Format or Bfloat16, also employs 16 bits but allocates them differently compared to FP16: 1 bit for the sign, 8 bits for the exponent, and 7 bits for the fraction. This format, developed by Google, prioritizes a larger exponent range over precision, making it particularly useful in deep learning applications where the dynamic range is crucial.\n\n\n\nThree floating-point formats. Source: Google blog\n\n\nInteger: Integer representations are made using 8, 4, and 2 bits. They are often used during the inference phase of neural networks, where the weights and activations of the model are quantized to these lower precisions. Integer representations are deterministic and offer significant speed and memory advantages over floating-point representations. For many inference tasks, especially on edge devices, the slight loss in accuracy due to quantization is often acceptable given the efficiency gains. An extreme form of integer numerics is for binary neural networks (BNNs), where weights and activations are constrained to one of two values: either +1 or -1.\nPrecision | Pros | Cons |\n|‚Äî‚Äî‚Äî‚Äî|‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äì|‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äì|\nFP32 (Floating Point 32-bit) | - Standard precision used in most deep learning frameworks.&lt;br&gt; - High accuracy due to ample representational capacity.&lt;br&gt; - Well-suited for training. | - High memory usage.&lt;br&gt; - Slower inference times compared to quantized models.&lt;br&gt; - Higher energy consumption. |\nFP16 (Floating Point 16-bit) | - Reduces memory usage compared to FP32.&lt;br&gt; - Speeds up computations on hardware that supports FP16.&lt;br&gt; - Often used in mixed-precision training to balance speed and accuracy. | - Lower representational capacity compared to FP32.&lt;br&gt; - Risk of numerical instability in some models or layers. |\nINT8 (8-bit Integer) | - Significantly reduced memory footprint compared to floating-point representations.&lt;br&gt; - Faster inference if hardware supports INT8 computations.&lt;br&gt; - Suitable for many post-training quantization scenarios. | - Quantization can lead to some accuracy loss.&lt;br&gt; - Requires careful calibration during quantization to minimize accuracy degradation. |\nINT4 (4-bit Integer) | - Even lower memory usage than INT8.&lt;br&gt; - Further speed-up potential for inference. | - Higher risk of accuracy loss compared to INT8.&lt;br&gt; - Calibration during quantization becomes more critical. |\nBinary | - Minimal memory footprint (only 1 bit per parameter).&lt;br&gt; - Extremely fast inference due to bitwise operations.&lt;br&gt; - Power efficient. | - Significant accuracy drop for many tasks.&lt;br&gt; - Complex training dynamics due to extreme quantization. |\nTernary | - Low memory usage but slightly more than binary.&lt;br&gt; - Offers a middle ground between representation and efficiency. | - Accuracy might still be lower than higher precision models.&lt;br&gt; - Training dynamics can be complex. |\n\n\nNumeric Encoding and Storage\nNumeric encoding, the art of transmuting numbers into a computer-amenable format, and their subsequent storage are critical for computational efficiency. For instance, floating-point numbers might be encoded using the IEEE 754 standard, which apportions bits among sign, exponent, and fraction components, thereby enabling the representation of a vast array of values with a single format. There are a few new IEEE floating point formats that have been defined specifically for AI workloads:\n\nbfloat16- A 16-bit floating point format introduced by Google. It has 8 bits for exponent, 7 bits for mantissa and 1 bit for sign. Offers a reduced precision compromise between 32-bit float and 8-bit integers. Supported on many hardware accelerators.\nposit - A configurable format that can represent different levels of precision based on exponent bits. Aims to be more efficient than IEEE 754 binary floats. Has adjustable dynamic range and precision.\nFlexpoint - A format introduced by Intel that can dynamically adjust precision across layers or within a layer. Allows tuning precision to accuracy and hardware requirements.\nBF16ALT - A proposed 16-bit format by ARM as an alternative to bfloat16. Uses additional bit in exponent to prevent overflow/underflow.\nTF32 - Introduced by Nvidia for Ampere GPUs. Uses 10 bits for exponent instead of 8 bits like FP32. Improves model training performance while maintaining accuracy.\nFP8 - 8-bit floating point format that keeps 6 bits for mantissa and 2 bits for exponent. Enables better dynamic range than integers.\n\nThe key goals of these new formats are to provide lower precision alternatives to 32-bit floats for better computational efficiency and performance on AI accelerators while maintaining model accuracy. They offer different tradeoffs in terms of precision, range and implementation cost/complexity.\n\n\n\n10.3.3 Efficiency Benefits\nNumerical efficiency matters for machine learning workloads for a number of reasons:\nComputational Efficiency: High-precision computations (like FP32 or FP64) can be slow and resource-intensive. By reducing numeric precision, one can achieve faster computation times, especially on specialized hardware that supports lower precision.\nMemory Efficiency: Storage requirements decrease with reduced numeric precision. For instance, FP16 requires half the memory of FP32. This is crucial when deploying models to edge devices with limited memory or when working with very large models.\nPower Efficiency: Lower precision computations often consume less power, which is especially important for battery-operated devices.\nNoise Introduction: Interestingly, the noise introduced by using lower precision can sometimes act as a regularizer, helping to prevent overfitting in some models.\nHardware Acceleration: Many modern AI accelerators and GPUs are optimized for lower precision operations, leveraging the efficiency benefits of such numerics.\nEfficient numerics is not just about reducing the bit-width of numbers but understanding the trade-offs between accuracy and efficiency. As machine learning models become more pervasive, especially in real-world, resource-constrained environments, the focus on efficient numerics will continue to grow. By thoughtfully selecting and leveraging the appropriate numeric precision, one can achieve robust model performance while optimizing for speed, memory, and energy.\n\n\n10.3.4 Numeric Representation Nuances\nThere are a number of nuances with numerical representations for ML that require us to have an understanding of both the theoretical and practical aspects of numerics representation, as well as a keen awareness of the specific requirements and constraints of the application domain.\n\nMemory Usage\nThe memory footprint of ML models, particularly those of considerable complexity and depth, can be substantial, thereby posing a significant challenge in both training and deployment phases. For instance, a deep neural network with 100 million parameters, represented using Float32 (32 bits or 4 bytes per parameter), would necessitate approximately 400 MB of memory just for storing the model weights. This does not account for additional memory requirements during training for storing gradients, optimizer states, and forward pass caches, which can further amplify the memory usage, potentially straining the resources on certain hardware, especially edge devices with limited memory capacity.\n\n\nImpact on Model Parameters and Weights\nThe numeric representation casts a significant impact on the storage and computational requisites of ML model parameters and weights. For instance, a model utilizing Float64 for weights will demand double the memory and potentially increased computational time compared to a counterpart employing Float32. A weight matrix, for instance, with dimensions [1000, 1000] using Float64 would consume approximately 8MB of memory, whereas using Float32 would halve this to approximately 4MB.\n\n\nComputational Complexity\nNumerical precision directly impacts computational complexity, influencing the time and resources required to perform arithmetic operations. For example, operations using Float64 generally consume more computational resources than their Float32 or Float16 counterparts. In the realm of ML, where models might need to process millions of operations (e.g., multiplications and additions in matrix operations during forward and backward passes), even minor differences in the computational complexity per operation can aggregate into a substantial impact on training and inference times.\nIn addition to pure runtimes, there is also a concern over energy efficiency. Not all numerical computations are created equal from the underlying hardware standpoint. Some numerical operations are more energy efficient than others. For example, the figure below shows that integer addition is much more energy efficient than integer multiplication.\n  Source: https://ieeexplore.ieee.org/document/6757323 \n\n\nHardware Compatibility\nEnsuring compatibility and optimized performance across diverse hardware platforms is another challenge in numerics representation. Different hardware, such as CPUs, GPUs, TPUs, and FPGAs, have varying capabilities and optimizations for handling different numeric precisions. For example, certain GPUs might be optimized for Float32 computations, while others might provide accelerations for Float16. Developing and optimizing ML models that can leverage the specific numerical capabilities of different hardware, while ensuring that the model maintains its accuracy and robustness, requires careful consideration and potentially additional development and testing efforts.\nPrecision and Accuracy Trade-offs\nThe trade-off between numerical precision and model accuracy is a nuanced challenge in numerics representation. Utilizing lower-precision numerics, such as Float16, might conserve memory and expedite computations but can also introduce issues like quantization error and reduced numerical range. For instance, training a model with Float16 might introduce challenges in representing very small gradient values, potentially impacting the convergence and stability of the training process. Furthermore, in certain applications, such as scientific simulations or financial computations, where high precision is paramount, the use of lower-precision numerics might not be permissible due to the risk of accruing significant errors.\n\n\nTrade-off Examples\nTo understand and appreciate the nuances let‚Äôs consider some use case examples. Through these we will realize that the choice of numeric representation is not merely a technical decision but a strategic one, influencing the model‚Äôs predictive acumen, its computational demands, and its deployability across diverse computational environments. In this section we will look at a couple of examples to better understand the trade-offs with numerics and how they tie to the real world.\n\nAutonomous Vehicles\nIn the domain of autonomous vehicles, ML models are employed to interpret sensor data and make real-time decisions. The models must process high-dimensional data from various sensors (e.g., LiDAR, cameras, radar) and execute numerous computations within a constrained time frame to ensure safe and responsive vehicle operation. So the trade-offs here would include:\n\nMemory Usage: Storing and processing high-resolution sensor data, especially in floating-point formats, can consume substantial memory.\nComputational Complexity: Real-time processing demands efficient computations, where higher-precision numerics might impede the timely execution of control actions.\n\n\n\nMobile Health Applications\nMobile health applications often utilize ML models for tasks like activity recognition, health monitoring, or predictive analytics, operating within the resource-constrained environment of mobile devices. The trade-offs here would include:\n\nPrecision and Accuracy Trade-offs: Employing lower-precision numerics to conserve resources might impact the accuracy of health predictions or anomaly detections, which could have significant implications for user health and safety.\nHardware Compatibility: Models need to be optimized for diverse mobile hardware, ensuring efficient operation across a wide range of devices with varying numerical computation capabilities.\n\n\n\nHigh-Frequency Trading (HFT) Systems\nHFT systems leverage ML models to make rapid trading decisions based on real-time market data. These systems demand extremely low-latency responses to capitalize on short-lived trading opportunities.\n\nComputational Complexity: The models must process and analyze vast streams of market data with minimal latency, where even slight delays, potentially introduced by higher-precision numerics, can result in missed opportunities.\nPrecision and Accuracy Trade-offs: Financial computations often demand high numerical precision to ensure accurate pricing and risk assessments, posing challenges in balancing computational efficiency and numerical accuracy.\n\n\n\nEdge-Based Surveillance Systems\nSurveillance systems deployed on edge devices, like security cameras, utilize ML models for tasks like object detection, activity recognition, and anomaly detection, often operating under stringent resource constraints.\n\nMemory Usage: Storing pre-trained models and processing video feeds in real-time demands efficient memory usage, which can be challenging with high-precision numerics.\nHardware Compatibility: Ensuring that models can operate efficiently on edge devices with varying hardware capabilities and optimizations for different numeric precisions is crucial for widespread deployment.\n\n\n\nScientific Simulations\nML models are increasingly being utilized in scientific simulations, such as climate modeling or molecular dynamics simulations, to enhance predictive capabilities and reduce computational demands.\n\nPrecision and Accuracy Trade-offs: Scientific simulations often require high numerical precision to ensure accurate and reliable results, which can conflict with the desire to reduce computational demands via lower-precision numerics.\nComputational Complexity: The models must manage and process complex, high-dimensional simulation data efficiently to ensure timely results and enable large-scale or long-duration simulations.\n\nThese examples illustrate diverse scenarios where the challenges of numerics representation in ML models are prominently manifested. Each system presents a unique set of requirements and constraints, necessitating tailored strategies and solutions to navigate the challenges of memory usage, computational complexity, precision-accuracy trade-offs, and hardware compatibility.\n\n\n\n\n10.3.5 Quantization\nQuantization is prevalent in various scientific and technological domains, essentially involves the mapping or constraining of a continuous set or range into a discrete counterpart to minimize the number of bits required.\n\nHistory\nHistorically, the idea of quantization is not novel and can be traced back to ancient times, particularly in the realm of music and astronomy. In music, the Greeks utilized a system of tetrachords, segmenting the continuous range of pitches into discrete notes, thereby quantizing musical sounds. In astronomy and physics, the concept of quantization was present in the discretized models of planetary orbits, as seen in the Ptolemaic and Copernican systems.\nDuring the 1800s, quantization-based discretization was used to approximate the calculation of integrals, and further used to investigate the impact of rounding errors on the integration result. However, the term ‚Äúquantization‚Äù was firmly embedded in scientific literature with the advent of quantum mechanics in the early 20th century, where it was used to describe the phenomenon that certain physical properties, such as energy, exist only in discrete, quantized states. This principle was pivotal in explaining phenomena at the atomic and subatomic levels. In the digital age, quantization found its application in signal processing, where continuous signals are converted into a discrete digital form, and in numerical algorithms, where computations on real-valued numbers are performed with finite-precision arithmetic.\nExtending upon this second application and relevant to this section, it is used in computer science to optimize neural networks by reducing the precision of the network weights. Thus, quantization, as a concept, has been subtly woven into the tapestry of scientific and technological development, evolving and adapting to the needs and discoveries of various epochs.\n\n\nInitial Breakdown\nWe begin our foray into quantization with a brief analysis of one important use for quantization.\nIn signal processing, the continuous sine wave can be quantized into discrete values through a process known as sampling. This is a fundamental concept in digital signal processing and is crucial for converting analog signals (like the continuous sine wave) into a digital form that can be processed by computers. The sine wave is a prevalent example due to its periodic and smooth nature, making it a useful tool for explaining concepts like frequency, amplitude, phase, and, of course, quantization.\n\n\n\nSine Wave\n\n\nIn the quantized version shown below, the continuous sine wave is sampled at regular intervals (in this case, every () radians), and only these sampled values are represented in the digital version of the signal. The step-wise lines between the points show one way to represent the quantized signal in a piecewise-constant form. This is a simplified example of how analog-to-digital conversion works, where a continuous signal is mapped to a discrete set of values, enabling it to be represented and processed digitally.\n\n\n\nQuantized Sine Wave\n\n\nReturning to the context of Machine Learning (ML), quantization refers to the process of constraining the possible values that numerical parameters (such as weights and biases) can take to a discrete set, thereby reducing the precision of the parameters and consequently, the model‚Äôs memory footprint. When properly implemented, quantization can reduce model size by up to 4x and improve inference latency and throughput by up to 2-3x. For example, an Image Classification model like ResNet-50 can be compressed from 96MB down to 24MB with 8-bit quantization.There is typically less than 1% loss in model accuracy from well tuned quantization. Accuracy can often be recovered by re-training the quantized model with quantization aware training techniques. Therefore, this technique has emerged to be very important in deploying ML models to resource-constrained environments, such as mobile devices, IoT devices, and edge computing platforms, where computational resources (memory and processing power) are limited.\n\nQuantization figure - Example figure showing reduced model size from quantization\nThere are several dimensions to quantization such as uniformity, stochasticity (or determinism), symmetry, granularity (across layers/channels/groups or even within channels), range calibration considerations (static vs dynamic), and fine-tuning methods (QAT, PTQ, ZSQ). We examine these below.\n\n\n\n10.3.6 Types\n\nUniform Quantization\nUniform quantization involves mapping continuous or high-precision values to a lower-precision representation using a uniform scale. This means that the interval between each possible quantized value is consistent. For example, if weights of a neural network layer are quantized to 8-bit integers (values between 0 and 255), a weight with a floating-point value of 0.56 might be mapped to an integer value of 143, assuming a linear mapping between the original and quantized scales. Due to its use of integer or fixed-point math pipelines, this form of quantization allows computation on the quantized domain without the need to dequantize beforehand.\nThe process for implementing uniform quantization starts with choosing a range of real numbers to be quantized. The next step is to select a quantization function and map the real values to the integers representable by the bit-width of the quantized representation. For instance, a popular choice for a quantization function is:\nQ(r)=Int(r/S) - Z\nwhere Q is the quantization operator, r is a real valued input (in our case, an activation or weight), S is a real valued scaling factor, and Z is an integer zero point. The Int function maps a real value to an integer value through a rounding operation. Through this function, we have effectively mapped real values r to some integer values, resulting in quantized levels which are uniformly spaced.\nWhen the need arises for practitioners to retrieve the original higher precision values, real values r can be recovered from quantized values through an operation known as dequantization. In the example above, this would mean performing the following operation on our quantized value:\nr ÃÉ = S(Q(r) + Z) (~ should be on top, ignore)\nAs discussed, some precision in the real value is lost by quantization. In this case, the recovered value r ÃÉ will not exactly match r due to the rounding operation. This is an important tradeoff to note; however, in many successful uses of quantization, the loss of precision can be negligible and the test accuracy remains high. Despite this, uniform quantization continues to be the current de-facto choice due to its simplicity and efficient mapping to hardware.\n\n\nNon-uniform Quantization\nNon-uniform quantization, on the other hand, does not maintain a consistent interval between quantized values. This approach might be used to allocate more possible discrete values in regions where the parameter values are more densely populated, thereby preserving more detail where it is most needed. For instance, in bell-shaped distributions of weights with long tails, a set of weights in a model predominantly lies within a certain range; thus, more quantization levels might be allocated to that range to preserve finer details, enabling us to better capture information. However, one major weakness of non-uniform quantization is that it requires dequantization before higher precision computations due to its non-uniformity, restricting its ability to accelerate computation compared to uniform quantization.\nTypically, a rule-based non-uniform quantization uses a logarithmic distribution of exponentially increasing steps and levels as opposed to linearly. Another popular branch lies in binary-code-based quantization where real number vectors are quantized into binary vectors with a scaling factor. Notably, there is no closed form solution for minimizing errors between the real value and non-uniformly quantized value, so most quantizations in this field rely on heuristic solutions. For instance, recent work formulates non-uniform quantization as an optimization problem where the quantization steps/levels in quantizer Q are adjusted to minimize the difference between the original tensor and quantized counterpart.\n_Q ||Q(r)-r||^2\nFurthermore, learnable quantizers can be jointly trained with model parameters, and the quantization steps/levels are generally trained with iterative optimization or gradient descent. Additionally, clustering has been used to alleviate information loss from quantization. While capable of capturing higher levels of detail, non-uniform quantization schemes can be difficult to deploy efficiently on general computation hardware, making it less-preferred to methods which use uniform quantization.\n\nComparison between uniform quantization (left) and non-uniform quantization (right) (Credit: A Survey of Quantization Methods for Efficient Neural Network Inference ).\n\n\nStochastic Quantization\nUnlike the two previous approaches which generate deterministic mappings, there is some work exploring the idea of stochastic quantization for quantization aware training and reduced precision training. This approach maps floating numbers up or down with a probability associated to the magnitude of the weight update. The hope generated by high level intuition is that such a probabilistic approach may allow a neural network to explore more, as compared to deterministic quantization. Supposedly, enabling a stochastic rounding may allow neural networks to escape local optimums, thereby updating its parameters. Below are two example stochastic mapping functions:\n\n\n\n\nZero Shot Quantization\nZero-shot quantization refers to the process of converting a full-precision deep learning model directly into a low-precision, quantized model without the need for any retraining or fine-tuning on the quantized model. The primary advantage of this approach is its efficiency, as it eliminates the often time-consuming and resource-intensive process of retraining a model post-quantization. By leveraging techniques that anticipate and minimize quantization errors, zero-shot quantization aims to maintain the model‚Äôs original accuracy even after reducing its numerical precision. It is particularly useful for Machine Learning as a Service (MLaaS) providers aiming to expedite the deployment of their customer‚Äôs workloads without having to access their datasets.\n\n\n\n10.3.7 Calibration\nCalibration is the process of selecting the most effective clipping range [, ] for weights and activations to be quantized to. For example, consider quantizing activations that originally have a floating-point range between -6 and 6 to 8-bit integers. If you just take the minimum and maximum possible 8-bit integer values (-128 to 127) as your quantization range, it might not be the most effective. Instead, calibration would involve passing a representative dataset then use this observed range for quantization.\nThere are many calibration methods but a few commonly used include:\nMax: Use the maximum absolute value seen during calibration. However, this method is susceptible to outlier data.\nEntropy: Use KL divergence to minimize information loss between the original floating-point values and values that could be represented by the quantized format. This is the default method used by TensorRT.\nPercentile: Set the range to a percentile of the distribution of absolute values seen during calibration. For example, 99% calibration would clip 1% of the largest magnitude values.\n\nSrc: Integer quantization for deep learning inference\nImportantly, the quality of calibration can make a difference between a quantized model that retains most of its accuracy and one that degrades significantly. Hence, it‚Äôs an essential step in the quantization process. When choosing a calibration range, there are two types: symmetric and asymmetric.\n\nSymmetric Quantization\nSymmetric quantization maps real values to a symmetrical clipping range centered around 0. This involves choosing a range [, ] where = -. For example, one symmetrical range would be based on the min/max values of the real values such that: -= = max(abs(r_max), abs(r_min)).\nSymmetric clipping ranges are the most widely adopted in practice as they have the advantage of easier implementation. In particular, the zeroing out of the zero point can lead to reduction in computational cost during inference ‚ÄúInteger Quantization for Deep Learning Inference: Principles and Empirical Evaluation‚Äù (2023) .\n\n\nAsymmetric Quantization\nAsymmetric quantization maps real values to an asymmetrical clipping range that isn‚Äôt necessarily centered around 0. It involves choosing a range [, ] where -. For example, selecting a range based on the minimum and maximum real values, or where = r_min and = r_max, creates an asymmetric range. Typically, asymmetric quantization produces tighter clipping ranges compared to symmetric quantization, which is important when target weights and activations are imbalanced, e.g., the activation after the ReLU always has non-negative values. Despite producing tighter clipping ranges, asymmetric quantization is less preferred to symmetric quantization as it doesn‚Äôt always zero out the real value zero.\n\nIllustration of symmetric quantization (left) and asymmetric quantization (right). Symmetric quantization maps real values to [-127, 127], and asymmetric maps to [-128, 127]. (Credit: A Survey of Quantization Methods for Efficient Neural Network Inference ).\n\n\n\n10.3.8 Granularity\nUpon deciding the type of clipping range, it is essential to tighten the range to allow a model to retain as much of its accuracy as possible. We‚Äôll be taking a look at convolutional neural networks as our way of exploring methods that fine tune the granularity of clipping ranges for quantization. The input activation of a layer in our CNN undergoes convolution with multiple convolutional filters. Every convolutional filter can possess a unique range of values. Consequently, one distinguishing feature of quantization approaches is the precision with which the clipping range [Œ±,Œ≤] is determined for the weights. \nIllustration of the main forms of quantization granularities. In layerwise quantization, the same clipping range is applied to all filters which belong to the same layer. Notice how this can result in lower quantization resolutions for channels with narrow distributions, e.g.¬†Filter 1, Filter 2, and Filter C. A higher quantization resolution can be achieved using channelwise quantization which dedicates different clipping ranges to different channels. (Credit: A Survey of Quantization Methods for Efficient Neural Network Inference ).\n\nLayerwise Quantization: This approach determines the clipping range by considering all of the weights in the convolutional filters of a layer. Then, the same clipping range is used for all convolutional filters. It‚Äôs the simplest to implement, and, as such, it often results in sub-optimal accuracy due the wide variety of differing ranges between filters. For example, a convolutional kernel with a narrower range of parameters loses its quantization resolution due to another kernel in the same layer having a wider range. .\nGroupwise Quantization: This approach groups different channels inside a layer to calculate the clipping range. This method can be helpful when the distribution of parameters across a single convolution/activation varies a lot. In practice, this method was useful in Q-BERT [Q-BERT: Hessian based ultra low precision quantization of bert] for quantizing Transformer [‚Äã‚ÄãAttention Is All You Need] models that consist of fully-connected attention layers. The downside with this approach comes with the extra cost of accounting for different scaling factors.\nChannelwise Quantization: This popular method uses a fixed range for each convolutional filter that is independent of other channels. Because each channel is assigned a dedicated scaling factor, this method ensures a higher quantization resolution and often results in higher accuracy.\nSub-channelwise Quantization: Taking channelwise quantization to the extreme, this method determines the clipping range with respect to any groups of parameters in a convolution or fully-connected layer. It may result in considerable overhead since different scaling factors need to be taken into account when processing a single convolution or fully-connected layer.\n\nOf these, channelwise quantization is the current standard used for quantizing convolutional kernels, since it enables the adjustment of clipping ranges for each individual kernel with negligible overhead.\n\n\n10.3.9 Static and Dynamic Quantization\nAfter determining the type and granularity of the clipping range, practitioners must decide when ranges are determined in their range calibration algorithms. There are two approaches to quantizing activations: static quantization and dynamic quantization.\nStatic quantization is the most frequently used approach. In this, the clipping range is pre-calculated and static during inference. It does not add any computational overhead, but, consequently, results in lower accuracy as compared to dynamic quantization. A popular method of implementing this is to run a series of calibration inputs to compute the typical range of activations [Quantization and training of neural networks for efficient integer-arithmetic-only inference, Dyadic neural network quantization].\nDynamic quantization is an alternative approach which dynamically calculates the range for each activation map during runtime. The approach requires real-time computations which might have a very high overhead. By doing this, dynamic quantization often achieves the highest accuracy as the range is calculated specifically for each input.\nBetween the two, calculating the range dynamically usually is very costly, so most practitioners will often use static quantization instead.\n\n\n10.3.10 Techniques\nThe two prevailing techniques for quantizing models are Post Training Quantization and Quantization Aware Training.\nPost Training Quantization - Post-training quantization (PTQ) is a quantization technique where the model is quantized after it has been trained.The model is trained in floating point and then weights and activations are quantized as a post-processing step. This is the simplest approach and does not require access to the training data. Unlike Quantization-Aware Training (QAT), PTQ sets weight and activation quantization parameters directly, making it low-overhead and suitable for limited or unlabeled data situations. However, not readjusting the weights after quantizing, especially in low-precision quantization can lead to very different behavior and thus lower accuracy. To tackle this, techniques like bias correction, equalizing weight ranges, and adaptive rounding methods have been developed. PTQ can also be applied in zero-shot scenarios, where no training or testing data are available. This method has been made even more efficient to benefit compute- and memory- intensive large language models. Recently, SmoothQuant, a training-free, accuracy-preserving, and general-purpose PTQ solution which enables 8-bit weight, 8-bit activation quantization for LLMs, has been developed, demonstrating up to 1.56x speedup and 2x memory reduction for LLMs with negligible loss in accuracy SmoothQuant: Accurate and Efficient Post-Training Quantization for Large Language Models(https://arxiv.org/abs/2211.10438).\n \nIn PTQ, a pretrained model is calibrated using calibration data (e.g., a small subset of training data) to compute the clipping ranges and scaling factors. (Credit: A Survey of Quantization Methods for Efficient Neural Network Inference )\nQuantization Aware Training - Quantization-aware training (QAT) is a fine-tuning of the PTQ model. The model is trained aware of quantization, allowing it to adjust for quantization effects. This produces better accuracy with quantized inference. Quantizing a trained neural network model with methods such as PTQ introduces perturbations that can deviate the model from its original convergence point. For instance, Krishnamoorthi showed that even with per-channel quantization, networks like MobileNet do not reach baseline accuracy with int8 Post Training Quantization (PTQ) and require Quantization Aware Training (QAT) Quantizing deep convolutional networks for efficient inference(https://arxiv.org/abs/1806.08342).To address this, QAT retrains the model with quantized parameters, employing forward and backward passes in floating point but quantizing parameters after each gradient update. Handling the non-differentiable quantization operator is crucial; a widely used method is the Straight Through Estimator (STE), approximating the rounding operation as an identity function. While other methods and variations exist, STE remains the most commonly used due to its practical effectiveness.\n\nIn QAT, a pretrained model is quantized and then finetuned using training data to adjust parameters and recover accuracy degradation. Note: the calibration process is often conducted in parallel with the finetuning process for QAT. (Credit: A Survey of Quantization Methods for Efficient Neural Network Inference ).\nSrc: Integer quantization for deep learning inference \nNote that QAT is an extension of PTQ. It receives the model quantized by PTQ and retrains it to finetune quantized parameters. Src: https://deci.ai/quantization-and-quantization-aware-training/\n\nSrc: integer quantization for deep learning Inference: principles and empirical evaluations\n\n\n\n\n\n\n\n\n\nFeature/Technique\nPost Training Quantization\nQuantization Aware Training\nDynamic Quantization\n\n\n\n\nPros\n\n\n\n\n\nSimplicity\n‚úì\n‚úó\n‚úó\n\n\nAccuracy Preservation\n‚úó\n‚úì\n‚úì\n\n\nAdaptability\n‚úó\n‚úó\n‚úì\n\n\nOptimized Performance\n‚úó\n‚úì\nPotentially\n\n\nCons\n\n\n\n\n\nAccuracy Degradation\n‚úì\n‚úó\nPotentially\n\n\nComputational Overhead\n‚úó\n‚úì\n‚úì\n\n\nImplementation Complexity\n‚úó\n‚úì\n‚úì\n\n\nTradeoffs\n\n\n\n\n\nSpeed vs.¬†Accuracy\n‚úì\n‚úó\n‚úó\n\n\nAccuracy vs.¬†Cost\n‚úó\n‚úì\n‚úó\n\n\nAdaptability vs.¬†Overhead\n‚úó\n‚úó\n‚úì\n\n\n\n\n\n10.3.11 Weights vs.¬†Activations\nWeight Quantization: Involves converting the continuous or high-precision weights of a model to lower-precision, such as converting Float32 weights to quantized INT8 (integer) weights. This reduces the model size, thereby reducing the memory required to store the model and the computational resources needed to perform inference. For example, consider a weight matrix in a neural network layer with Float32 weights as [0.215, -1.432, 0.902, ‚Ä¶]. Through weight quantization, these might be mapped to INT8 values like [27, -183, 115, ‚Ä¶], significantly reducing the memory required to store them.\n[Figure X.2 - Diagram of quantizing weights and activations] \nActivation Quantization: Involves quantizing the activation values (outputs of layers) during model inference. This can reduce the computational resources required during inference, but it introduces additional challenges in maintaining model accuracy due to the reduced precision of intermediate computations. For example, in a convolutional neural network (CNN), the activation maps (feature maps) produced by convolutional layers, originally in Float32, might be quantized to INT8 during inference to accelerate computation, especially on hardware optimized for integer arithmetic. Additionally, recent work has explored the use of Activation-aware Weight Quantization for LLM compression and acceleration, which involves protecting only 1% of the most important salient weights by observing the activations not weights AWQ: Activation-aware Weight Quantization for LLM Compression and Acceleration(https://arxiv.org/pdf/2306.00978.pdf).\n\n\n10.3.12 Trade-offs\nQuantization invariably introduces a trade-off between model size/performance and accuracy. While it significantly reduces the memory footprint and can accelerate inference, especially on hardware optimized for low-precision arithmetic, the reduced precision can degrade model accuracy.\nModel Size: A model with weights represented as Float32 being quantized to INT8 can theoretically reduce the model size by a factor of 4, enabling it to be deployed on devices with limited memory. \nSrc: https://arxiv.org/abs/2211.10438\nInference Speed: Quantization can also accelerate inference, as lower-precision arithmetic is computationally less expensive. For example, certain hardware accelerators, like Google‚Äôs Edge TPU, are optimized for INT8 arithmetic and can perform inference significantly faster with INT8 quantized models compared to their floating-point counterparts.\n\nSrc: Integer quantization for deep learning inference\nAccuracy: The reduction in numerical precision post-quantization can lead to a degradation in model accuracy, which might be acceptable in certain applications (e.g., image classification) but not in others (e.g., medical diagnosis). Therefore, post-quantization, the model typically requires re-calibration or fine-tuning to mitigate accuracy loss. Furthermore, recent work has explored the use of Activation-aware Weight Quantization which is based on the observation that protecting only 1% of salient weights can greatly reduce quantization error AWQ: Activation-aware Weight Quantization for LLM Compression and Acceleration(https://arxiv.org/pdf/2306.00978.pdf).\n\nSrc: https://arxiv.org/abs/1510.00149\nFigure\n\n\n10.3.13 Quantization and Pruning\nPruning and quantization work well together, and it‚Äôs been found that pruning doesn‚Äôt hinder quantization. In fact, pruning can help reduce quantization error. Intuitively, this is due to pruning reducing the number of weights to quantize, thereby reducing the accumulated error from quantization. For example, an unpruned AlexNet has 60 million weights to quantize whereas a pruned AlexNet only has 6.7 million weights to quantize. This significant drop in weights helps reduce the error between quantizing the unpruned AlexNet vs.¬†the pruned AlexNet. Furthermore, recent work has found that quantization-aware pruning generates more computationally efficient models than either pruning or quantization alone; It typically performs similar to or better in terms of computational efficiency compared to other neural architecture search techniques like Bayesian optimization [Ps and Qs: Quantization-Aware Pruning for Efficient Low Latency Neural Network Inference][2021](https://arxiv.org/pdf/2102.11289.pdf).\n\nSrc: https://arxiv.org/abs/1510.00149\n\n\n\n10.3.14 Edge-aware Quantization\nQuantization not only reduces model size but also enables faster computations and draws less power, making it vital to edge development. Edge devices typically have tight resource constraints with compute, memory, and power, which are impossible to meet for many of the deep NN models of today. Furthermore, edge processors do not support floating point operations, making integer quantization particularly important for chips like GAP-8, a RISC-=V SoC for edge inference with a dedicated CNN accelerator, which only support integer arithmetic..\nOne hardware platform utilizing quantization is the ARM Cortex-M group of 32=bit RISC ARM processor cores. They leverage fixed-point quantization with power of two scaling factors so that quantization and dequantization can be efficiently done by bit shifting. Additionally, Google Edge TPUs, Google‚Äôs emerging solution for running inference at the edge, is designed for small, low-powered devices and can only support 8-bit arithmetic. Recently, there has been significant strides in the computing power of edge processors, enabling the deployment and inference of costly NN models previously limited to servers.\n\nIn addition to being an indispensable technique for many edge processors, quantization has also brought noteworthy improvements to non-edge processors such as encouraging such processors to meet the Service Level Agreement (SLA) requirements such as 99th percentile latency.\nThus, quantization combined with efficient low-precision logic and dedicated deep learning accelerators, has been one crucial driving force for the evolution of such edge processors."
  },
  {
    "objectID": "optimizations.html#sec-model_ops_hw",
    "href": "optimizations.html#sec-model_ops_hw",
    "title": "10¬† Model Optimizations",
    "section": "10.4 Efficient Hardware Implementation",
    "text": "10.4 Efficient Hardware Implementation\nEfficient hardware implementation transcends the selection of suitable components; it requires a holistic understanding of how software will interact with underlying architectures. The essence of achieving peak performance in TinyML applications lies not only in refining algorithms to hardware but also in ensuring that the hardware is strategically tailored to support these algorithms. This synergy between hardware and software is crucial. As we delve deeper into the intricacies of efficient hardware implementation, the significance of a co-design approach, where hardware and software are developed in tandem, becomes increasingly evident. This section provides an overview of the techniques of how hardware and the interactions between hardware and software can be optimized to improve models performance.\n\n10.4.1 Hardware-Aware Neural Architecture Search\nFocusing only on the accuracy when performing Neural Architecture Search leads to models that are exponentially complex and require increasing memory and compute. This has lead to hardware constraints limiting the exploitation of the deep learning models at their full potential. Manually designing the architecture of the model is even harder when considering the hardware variety and limitations. This has lead to the creation of Hardware-aware Neural Architecture Search that incorporate the hardware contractions into their search and optimize the search space for a specific hardware and accuracy. HW-NAS can be catogrized based how it optimizes for hardware. We will briefly explore these categories and leave links to related papers for the interested reader.\n\n\n\nTaxonomy of HW-NAS 1\n\n\n\nSingle Target, Fixed Platfrom Configuration\nThe goal here is to find the best architecture in terms of accuracy and hardware efficiency for one fixed target hardware. For a specific hardware, the Arduino Nicla Vision for example, this category of HW-NAS will look for the architecture that optimizes accuracy, latency, energy consumption, ‚Ä¶\nTwo approaches fall under this category\n\nHardware-aware Search Strategy\nHere, the search is a multi-objective optimization problem, where both the accuracy and hardware cost guide the searching algorithm to find the most efficient architecture. 123\n\n\nHardware-aware Search Space\nHere, the search space is restricted to the architectures that perform well on the specific hardware. This can be achieved by either measuring the operators (Conv operator, Pool operator, ‚Ä¶) performance, or define a set of rules that limit the search space. 1\n\n\n\nSingle Target, Multiple Platform Configurations\nSome hardwares may have different configurations. For example, FPGAs have Configurable Logic Blocks (CLBs) that can be configured by the firmware. This method allows for the HW-NAS to explore different configurations. 12\n\n\nMultiple Targets\nThis category aims at optimizing a single model for multiple hardwares. This can be helpful for mobile devices development as it can optimize to different phones models. 12\n\n\nExamples of Hardware-Aware Neural Architecture Search\n\nTinyNAS\nTinyNAS adopts a two stage approach to finding an optimal architecture for model with the constraints of the specific microcontroller in mind.\nFirst, TinyNAS generate multiple search spaces by varying the input resolution of the model, and the number of channels of the layers of the model. Then, TinyNAS chooses a search space based on the FLOPs (Floating Point Operations Per Second) of each search space\nThen, TinyNAS performs a search operation on the chosen space to find the optimal architecture for the specific constraints of the microcontroller. 1\n\n\n\nA diagram showing how search spaces with high probability of finding an architecture with large number of FLOPs provide models with higher accuracy 1\n\n\n\n\n\nTopology-Aware NAS\nFocuses on creating and optimizing a search space that aligns with the hardware topology of the device. 1\n\n\n\n10.4.2 Challenges of Hardware-Aware Neural Architecture Search\nWhile HW-NAS carries high potential for finding optimal architectures for TinyML, it comes with some challenges. Hardware Metrics like latency, energy consumption and hardware utilization are harder to evaluate than the metrics of accuracy or loss. They often require specilized tools for precise measurements. Moreover, adding all these metrics leads to a much bigger search space. This leads to HW-NAS being time-consuming and expensive. It has to be applied to every hardware for optimal results, moreover, meaning that if one needs to deploy the model on multiple devices, the search has to be conducted multiple times and will result in different models, unless optimizing for all of them which means less accuracy. Finally, hardware changes frequently, and HW-NAS may need to be conducted on each version.\n\n\n10.4.3 Kernel Optimizations\nKernel Optimizations are modifications made to the kernel to enhance the performance of machine learning models onf resource-constrained devices. We will separate kernel optimizations into two types.\n\nGeneral Kernel Optimizations\nThese are kernel optimizations that all devices can benefit from. They provide technics to convert the code to more efficient instructions.\n\nLoop unrolling\nInstead of having a loop with loop control (incrementing the loop counter, checking the loop termination condition) the loop can be unrolled and the overhead of loop control can be omitted. This may also provide additional opportunities for parallelism that may not be possible with the loop structure. This can be particularly beneficial for tight loops, where the boy of the loop is a small number of instructions with a lot of iterations.\n\n\nBlocking\nBlocking is used to make memory access patterns more efficient. If we have three computations the first and the last need to access cache A and the second needs to access cache B, blocking blocks the first two computations together to reduce the number of memory reads needed.\n\n\nTiling\nSimilarly to blocking, tiling divides data and computation into chunks, but extends beyond cache improvements. Tiling creates independent partitions of computation that can be run in parallel, which can result in significant performance improvements.:\n\n\nOptimized Kernel Libraries\nThis comprises developing optimized kernels that take full advantage of a specific hardware. One example is the CMSIS-NN library, which is a collection of efficient neural network kernels developed to optimize the performance and minimize the memory footprint of models on Arm Cortex-M processors, which are common on IoT edge devices. The kernel leverage multiple hardware capabilities of Cortex-M processors like Single Instruction Multple Data (SIMD), Floating Point Units (FPUs) and M-Profile Vector Extensions (MVE). These optimization make common operations like matrix multiplications more efficient, boosting the performance of model operations on Cortex-M processors. 1\n\n\n\n\n10.4.4 Compute-in-Memory (CiM)\nThis is one example of Algorithm-Hardware Co-design. CiM is a computing paradigm that performs computation within memory. Therefore, CiM architectures allow for operations to be performed directly on the stored data, without the need to shuttle data back and forth between separate processing and memory units. This design paradigm is particularly beneficial in scenarios where data movement is a primary source of energy consumption and latency, such as in TinyML applications on edge devices. Through algorithm-hardware co-design, the algorithms can be optimized to leverage the unique characteristics of CiM architectures, and conversely, the CiM hardware can be customized or configured to better support the computational requirements and characteristics of the algorithms. This is achieved by using the analog properties of memory cells, such as addition and multiplication in DRAM. 1\n\n\n\nA figure showing how Computing in Memory can be used for always-on tasks to offload tasks of the power consuming processing unit 1"
  },
  {
    "objectID": "optimizations.html#memory-access-optimization",
    "href": "optimizations.html#memory-access-optimization",
    "title": "10¬† Model Optimizations",
    "section": "10.5 Memory Access Optimization",
    "text": "10.5 Memory Access Optimization\nDifferent devices may have different memory hierarchies. Optimizing for the specific memory hierarchy in the specific hardware can lead to great performance improvements by reducing the costly operations of reading and writing to memory. Dataflow optimization can be achieved by optimizing for reusing data within a single layer and across multiple layers. This dataflow optimization can be tailored to the specific memory hierarchy of the hardware, which can lead to greater benefits than general optimizations for different hardwares.\n\n10.5.1 Leveraging Sparsity\nPruning is a fundamental approach to compress models to make them compatible with resource constrained devices. This results in sparse models where a lot of weights are 0‚Äôs. Therefore, leveraging this sparsity can lead to significant improvements in performance. Tools were created to achieve exactly this. RAMAN, is a sparseTinyML accelerator designed for inference on edge devices. RAMAN overlap input and output activations on the same memory space, reducing storage requirements by up to 50%. 1\n\n\n\nA figure showing the sparse columns of the filter matrix of a CNN that are aggregated to create a dense matrix that, leading to smaller dimensions in the matrix and more efficient computations1\n\n\n\n\n10.5.2 Optimization Frameworks\nOptimization Frameworks have been introduced to exploit the specific capabilities of the hardware to accelerate the software. One example of such a framework is hls4ml. This open-source software-hardware co-design workflow aids in interpreting and translating machine learning algorithms for implementation with both FPGA and ASIC technologies, enhancing their. Features such as network optimization, new Python APIs, quantization-aware pruning, and end-to-end FPGA workflows are embedded into the hls4ml framework, leveraging parallel processing units, memory hierarchies, and specialized instruction sets to optimize models for edge hardware. Moreover, hls4ml is capable of translating machine learning algorithms directly into FPGA firmware.\n\n\n\nA Diagram showing the workflow with the hls4ml framework 1\n\n\nOne other framework for FPGAs that focuses on a holistic approach is CFU Playground 1\n\n\n10.5.3 Hardware Built Around Software\nIn a contrasting approach, hardware can be custom-designed around software requirements to optimize the performance for a specific application. This paradigm creates specialized hardware to better adapt to the specifics of the software, thus reducing computational overhead and improving operational efficiency. One example of this approach is a voice-recognition application by 1. The paper proposes a structure wherein preprocessing operations, traditionally handled by software, are allocated to custom-designed hardware. This technique was achieved by introducing resistor‚Äìtransistor logic to an inter-integrated circuit sound module for windowing and audio raw data acquisition in the voice-recognition application. Consequently, this offloading of preprocessing operations led to a reduction in computational load on the software, showcasing a practical application of building hardware around software to enhance the efficiency and performance. 1\n\n\n\nA diagram showing how an FPGA was used to offload data preprocessing of the general purpose computation unit. 1\n\n\n\n\n10.5.4 SplitNets\nSplitNets were introduced in the context of Head-Mounted systems. They distribute the Deep Neural Networks (DNNs) workload among camera sensors and an aggregator. This is particularly compelling the in context of TinyML. The SplitNet framework is a split-aware NAS to find the optimal neural network architecture to achieve good accuracy, split the model among the sensors and the aggregator, and minimize the communication between the sensors and the aggregator. Minimal communication is important in TinyML where memory is highly constrained, this way the sensors conduct some of the processing on their chips and then they send only the necessary information to the aggregator. When testing on ImageNet, SplitNets were able to reduce the latency by one order of magnitude on head-mounted devices. This can be helpful when the sensor has it‚Äôs own chip. 1\n\n\n\nA chart showing a comparison between the performance of SplitNets vs all on sensor and all on aggregator approaches. 1\n\n\n\n\n10.5.5 Hardware Specific Data Augmentation\nEach edge device may possess unique sensor characteristics, leading to specific noise patterns that can impact model performance. One example is audio data, where variations stemming from the choice of microphone are prevalent. Applications such as Keyword Spotting can experience substantial enhancements by incorporating data recorded from devices similar to those intended for deployment. Fine-tuning of existing models can be employed to adapt the data precisely to the sensor‚Äôs distinctive characteristics.\n\n\n10.5.6 Software and Framework Support\nWhile all of the aforementioned techniques like pruning, quantization, and efficient numerics are well-known, they would remain impractical and inaccessible without extensive software support. For example, directly quantizing weights and activations in a model would require manually modifying the model definition and inserting quantization operations throughout. Similarly, directly pruning model weights requires manipulating weight tensors. Such tedious approaches become infeasible at scale.\nWithout the extensive software innovation across frameworks, optimization tools and hardware integration, most of these techniques would remain theoretical or only viable to experts. Without framework APIs and automation to simplify applying these optimizations, they would not see adoption. Software support makes them accessible to general practitioners and unlocks real-world benefits. In addition, issues such as hyperparameter tuning for pruning, managing the trade-off between model size and accuracy, and ensuring compatibility with target devices pose hurdles that developers must navigate.\n\nBuilt-in Optimization APIs\nMajor machine learning frameworks like TensorFlow, PyTorch, and MXNet provide libraries and APIs to allow common model optimization techniques to be applied without requiring custom implementations. For example, TensorFlow offers the TensorFlow Model Optimization Toolkit which contains modules like:\n\nquantization - Applies quantization-aware training to convert floating point models to lower precision like int8 with minimal accuracy loss. Handles weight and activation quantization.\nsparsity - Provides pruning APIs to induce sparsity and remove unnecessary connections in models like neural networks. Can prune weights, layers, etc.\nclustering - Supports model compression by clustering weights into groups for higher compression rates.\n\nThese APIs allow users to enable optimization techniques like quantization and pruning without directly modifying model code. Parameters like target sparsity rates, quantization bit-widths etc. can be configured. Similarly, PyTorch provides torch.quantization for converting models to lower precision representations. TorchTensor and TorchModule form the base classes for quantization support. It also offers torch.nn.utils.prune for built-in pruning of models. MXNet offers gluon.contrib layers that add quantization capabilities like fixed point rounding and stochastic rounding of weights/activations during training. This allows quantization to be readily included in gluon models.\nThe core benefit of built-in optimizations is that users can apply them without re-implementing complex techniques. This makes optimized models accessible to a broad range of practitioners. It also ensures best practices are followed by building on research and experience implementing the methods. As new optimizations emerge, frameworks strive to provide native support and APIs where possible to further lower the barrier to efficient ML. The availability of these tools is key to widespread adoption.\n\n\nAutomated Optimization Tools\nAutomated optimization tools provided by frameworks can analyze models and automatically apply optimizations like quantization, pruning, and operator fusion to make the process easier and accessible without excessive manual tuning. In effect, this builds on top of the previous section. For example, TensorFlow provides the TensorFlow Model Optimization Toolkit which contains modules like:\n\nQuantizationAwareTraining - Automatically quantizes weights and activations in a model to lower precision like UINT8 or INT8 with minimal accuracy loss. It inserts fake quantization nodes during training so that the model can learn to be quantization-friendly.\nPruning - Automatically removes unnecessary connections in a model based on analysis of weight importance. Can prune entire filters in convolutional layers or attention heads in transformers. Handles iterative re-training to recover any accuracy loss.\nGraphOptimizer - Applies graph optimizations like operator fusion to consolidate operations and reduce execution latency, especially for inference.\n\n\n\n\nBefore/after diagram showing GraphOptimizer fusing operators in a sample graph\n\n\nThese automated modules only require the user to provide the original floating point model, and handle the end-to-end optimization pipeline including any re-training to regain accuracy. Other frameworks like PyTorch also offer increasing automation support, for example through torch.quantization.quantize_dynamic. Automated optimization makes efficient ML accessible to practitioners without optimization expertise.\n\n\nHardware Optimization Libraries\nHardware libraries like TensorRT and TensorFlow XLA allow models to be highly optimized for target hardware through techniques that we discussed earlier.\nQuantization: For example, TensorRT and TensorFlow Lite both support quantization of models during conversion to their format. This provides speedups on mobile SoCs with INT8/INT4 support.\nKernel Optimization: For instance, TensorRT does auto-tuning to optimize CUDA kernels based on the GPU architecture for each layer in the model graph. This extracts maximum throughput.\nOperator Fusion: TensorFlow XLA does aggressive fusion to create optimized binary for TPUs. On mobile, frameworks like NCNN also support fused operators.\nHardware-Specific Code: Libraries are used to generate optimized binary code specialized for the target hardware. For example, TensorRT uses Nvidia CUDA/cuDNN libraries which are hand-tuned for each GPU architecture. This hardware-specific coding is key for performance. On tinyML devices, this can mean assembly code optimized for a Cortex M4 CPU for example. Vendors provide CMSIS-NN and other libraries.\nData Layout Optimizations - We can efficiently leverage memory hierarchy of hardware like cache and registers through techniques like tensor/weight rearrangement, tiling, and reuse. For example, TensorFlow XLA optimizes buffer layouts to maximize TPU utilization. This helps any memory constrained systems.\nProfiling-based Tuning - We can use profiling tools to identify bottlenecks. For example, adjust kernel fusion levels based on latency profiling. On mobile SoCs, vendors like Qualcomm provide profilers in SNPE to find optimization opportunities in CNNs. This data-driven approach is important for performance.\nBy integrating framework models with these hardware libraries through conversion and execution pipelines, ML developers can achieve significant speedups and efficiency gains from low-level optimizations tailored to the target hardware. The tight integration between software and hardware is key to enabling performant deployment of ML applications, especially on mobile and tinyML devices.\n\n\nVisualizing Optimizations\nImplementing model optimization techniques without visibility into the effects on the model can be challenging. Dedicated tooling or visualization tools can provide critical and useful insight into model changes and helps track the optimization process. Let‚Äôs consider the optimizations we considered earlier, such as pruning for sparsity and quantization.\n\nSparsity (ADD SOME LINKS INTO HERE)\nFor example, consider sparsity optimizations. Sparsity visualization tools can provide critical insights into pruned models by mapping out exactly which weights have been removed. For example, sparsity heat maps can use color gradients to indicate the percentage of weights pruned in each layer of a neural network. Layers with higher percentages pruned appear darker. This identifies which layers have been simplified the most by pruning.\n[Figure: maybe consider including an example from Wolfram]\nTrend plots can also track sparsity over successive pruning rounds - they may show initial rapid pruning followed by more gradual incremental increases. Tracking the current global sparsity along with statistics like average, minimum, and maximum sparsity per-layer in tables or plots provides an overview of the model composition. For a sample convolutional network, these tools could reveal that the first convolution layer is pruned 20% while the final classifier layer is pruned 70% given its redundancy. The global model sparsity may increase from 10% after initial pruning to 40% after five rounds.\n[Figure: Line graph with one line per layer, showing sparsity % over multiple pruning rounds or something to that effet]\nBy making sparsity data visually accessible, practitioners can better understand exactly how their model is being optimized and which areas are being impacted. The visibility enables them to fine-tune and control the pruning process for a given architecture.\nSparsity visualization turns pruning into a transparent technique instead of a black-box operation.\n\n\nQuantization\nConverting models to lower numeric precisions through quantization introduces errors that can impact model accuracy if not properly tracked and addressed. Visualizing quantization error distributions provides valuable insights into the effects of reduced precision numerics applied to different parts of a model. For this, histograms of the quantization errors for weights and activations can be generated. These histograms can reveal the shape of the error distribution - whether they resemble a Gaussian distribution or contain significant outliers and spikes. Large outliers may indicate issues with particular layers handling the quantization. Comparing the histograms across layers highlights any problem areas standing out with abnormally high errors.\n[Figure: include the example of the histograms, this stuff exists in papers]\nActivation visualizations are also important to detect overflow issues. By color mapping the activations before and after quantization, any values pushed outside the intended ranges become visible. This reveals saturation and truncation issues that could skew the information flowing through the model. Detecting these errors allows recalibrating activations to prevent loss of information.\n[Figure: include a color mapping example]\nOther techniques, such as tracking the overall mean square quantization error at each step of the quantization-aware training process identifies fluctuations and divergences. Sudden spikes in the tracking plot may indicate points where quantization is disrupting the model training. Monitoring this metric builds intuition on model behavior under quantization. Together these techniques turn quantization into a transparent process. The empirical insights enable practitioners to properly assess quantization effects. They pinpoint areas of the model architecture or training process to recalibrate based on observed quantization issues. This helps achieve numerically stable and accurate quantized models.\nProviding this data enables practitioners to properly assess the impact of quantization and identify potential problem areas of the model to recalibrate or redesign to be more quantization friendly. This empirical analysis builds intuition on achieving optimal quantization.\nVisualization tools can provide insights that help practitioners better understand the effects of optimizations on their models. The visibility enables correcting issues early before accuracy or performance is impacted significantly. It also aids applying optimizations more effectively for specific models. These optimization analytics help build intuition when transitioning models to more efficient representations.\n\n\n\n\n10.5.7 Model Conversion and Deployment\nOnce models have been successfully optimized in frameworks like TensorFlow and PyTorch, specialized model conversion and deployment platforms are needed to bridge the gap to running them on target devices.\nTensorFlow Lite - TensorFlow‚Äôs platform to convert models to a lightweight format optimized for mobile, embedded and edge devices. Supports optimizations like quantization, kernel fusion, and stripping away unused ops. Models can be executed using optimized TensorFlow Lite kernels on device hardware. Critical for mobile and tinyML deployment.\nONNX Runtime - Performs model conversion and inference for models in the open ONNX model format. Provides optimized kernels, supports hardware accelerators like GPUs, and cross-platform deployment from cloud to edge. Allows framework-agnostic deployment.\n[add figure of ONNX being an interoperable framework]\nPyTorch Mobile - Enables PyTorch models to be run on iOS and Android by converting to mobile-optimized representations. Provides efficient mobile implementations of ops like convolution and special functions optimized for mobile hardware.\nThese platforms integrate with hardware drivers, operating systems, and accelerator libraries on devices to execute models efficiently using hardware optimization. They also offload operations to dedicated ML accelerators where present. The availability of these proven, robust deployment platforms bridges the gap between optimizing models in frameworks and actual deployment to billions of devices. They allow users to focus on model development rather than building custom mobile runtimes. Continued innovation to support new hardware and optimizations in these platforms is key to widespread ML optimizations.\nBy providing these optimized deployment pipelines, the entire workflow from training to device deployment can leverage model optimizations to deliver performant ML applications. This end-to-end software infrastructure has helped drive the adoption of on-device ML."
  },
  {
    "objectID": "optimizations.html#conclusion",
    "href": "optimizations.html#conclusion",
    "title": "10¬† Model Optimizations",
    "section": "10.6 Conclusion",
    "text": "10.6 Conclusion\nIn this chapter we‚Äôve discussed model optimization across the software-hardware span. We dove deep into efficient model representation, where we covered the nuances of structured and unstructured pruning and other techniques for model compression such as knowledge distillation and matrix and tensor decomposition. We also dove briefly into edge-specific model design at the parameter and model architecture level, exploring topics like edge-specific models and hardware-aware NAS.\nWe then explored efficient numerics representations, where we covered the basics of numerics, numeric encodings and storage, benefits of efficient numerics, and the nuances of numeric representation with memory usage, computational complexity, hardware compatibility, and tradeoff scenarios. We finished by honing in on an efficient numerics staple: quantization, where we examined its history, calibration, techniques, and interaction with pruning.\nFinally, we looked at how we can make optimizations specific to the hardware we have. We explored how we can find model architectures tailored to the hardware, make optimizations in the kernel to better handle the model, and frameworks built to make the most use out of the hardware. We also looked at how we can go the other way around and build hardware around our specific software and talked about splitting networks to run on multiple processor available on the edge device.\nBy understanding the full picture of the degrees of freedom within model optimization both away and close to the hardware and the tradeoffs to consider when implementing these methods, practitioners can develop a more thoughtful pipeline for compressing their workloads onto edge devices."
  },
  {
    "objectID": "hw_acceleration.html#introduction",
    "href": "hw_acceleration.html#introduction",
    "title": "11¬† AI Acceleration",
    "section": "11.1 Introduction",
    "text": "11.1 Introduction\nExplanation: This section lays the groundwork for the chapter, introducing readers to the fundamental concepts of hardware acceleration and its role in enhancing the performance of AI systems, particularly embedded AI. This context is essential because hardware acceleration is a pivotal topic in the domain of embedded AI."
  },
  {
    "objectID": "hw_acceleration.html#background-and-basics",
    "href": "hw_acceleration.html#background-and-basics",
    "title": "11¬† AI Acceleration",
    "section": "11.2 Background and Basics",
    "text": "11.2 Background and Basics\nExplanation: Here, readers are provided with a foundational understanding of the historical and theoretical aspects of hardware acceleration technologies. This section is essential to give readers a historical perspective and a base to aid them in understanding the current state of hardware acceleration technologies.\n\nHistorical Background\nThe Need for Hardware Acceleration\nGeneral Principles of Hardware Acceleration"
  },
  {
    "objectID": "hw_acceleration.html#sec-aihw",
    "href": "hw_acceleration.html#sec-aihw",
    "title": "11¬† AI Acceleration",
    "section": "11.3 Types of Hardware Accelerators",
    "text": "11.3 Types of Hardware Accelerators\nExplanation: This section offers an overview of the hardware options available for accelerating AI tasks, discussing each type in detail, and comparing their advantages and disadvantages. It is key for readers to comprehend the various hardware solutions available for specific AI tasks, and to make informed decisions when selecting hardware solutions.\n\nCentral Processing Units (CPUs) with AI Capabilities\nGraphics Processing Units (GPUs)\nDigital Signal Processors (DSPs)\nField-Programmable Gate Arrays (FPGAs)\nApplication-Specific Integrated Circuits (ASICs)\nTensor Processing Units (TPUs)\nComparative Analysis of Different Hardware Accelerators"
  },
  {
    "objectID": "hw_acceleration.html#hardware-software-co-design",
    "href": "hw_acceleration.html#hardware-software-co-design",
    "title": "11¬† AI Acceleration",
    "section": "11.4 Hardware-Software Co-Design",
    "text": "11.4 Hardware-Software Co-Design\nExplanation: Focusing on the synergies between hardware and software components, this section discusses the principles and techniques of hardware-software co-design to achieve optimized performance in AI systems. This information is crucial to understanding how to design powerful and efficient AI systems that leverage both hardware and software components effectively.\n\nPrinciples of Hardware-Software Co-Design\nOptimization Techniques\nIntegration with Embedded Systems"
  },
  {
    "objectID": "hw_acceleration.html#acceleration-techniques",
    "href": "hw_acceleration.html#acceleration-techniques",
    "title": "11¬† AI Acceleration",
    "section": "11.5 Acceleration Techniques",
    "text": "11.5 Acceleration Techniques\nExplanation: In this section, various techniques to enhance computational efficiency and reduce latency through hardware acceleration are discussed. This information is fundamental for readers to understand how to maximize the benefits of hardware acceleration in AI systems, focusing on achieving superior computational performance.\n\nParallel Computing\nPipeline Computing\nMemory Hierarchy Optimization\nInstruction Set Optimization"
  },
  {
    "objectID": "hw_acceleration.html#tools-and-frameworks",
    "href": "hw_acceleration.html#tools-and-frameworks",
    "title": "11¬† AI Acceleration",
    "section": "11.6 Tools and Frameworks",
    "text": "11.6 Tools and Frameworks\nExplanation: This section introduces readers to an array of tools and frameworks available for facilitating work with hardware accelerators. It is essential for practical applications to help readers understand the resources they have at their disposal for implementing and optimizing hardware-accelerated AI systems.\n\nSoftware Tools for Hardware Acceleration\nDevelopment Environments\nLibraries and APIs"
  },
  {
    "objectID": "hw_acceleration.html#case-studies",
    "href": "hw_acceleration.html#case-studies",
    "title": "11¬† AI Acceleration",
    "section": "11.7 Case Studies",
    "text": "11.7 Case Studies\nExplanation: Providing real-world case studies offers practical insights and lessons from actual hardware-accelerated AI implementations. This section helps readers bridge theory with practice by demonstrating potential benefits and challenges in real-world scenarios, and offers a practical perspective on the topics discussed.\n\nReal-world Applications\nCase Study 1: Implementing Neural Networks on FPGAs\nCase Study 2: Optimizing Performance with GPUs\nLessons Learned from Case Studies"
  },
  {
    "objectID": "hw_acceleration.html#challenges-and-solutions",
    "href": "hw_acceleration.html#challenges-and-solutions",
    "title": "11¬† AI Acceleration",
    "section": "11.8 Challenges and Solutions",
    "text": "11.8 Challenges and Solutions\nExplanation: This segment discusses the prevalent challenges encountered in implementing hardware acceleration in AI systems and proposes potential solutions. It equips readers with a realistic view of the complexities involved and guides them in overcoming common hurdles.\n\nPortability/Compatibility Issues\nPower Consumption Concerns\nLatency Reduction\nOvercoming Resource Constraints"
  },
  {
    "objectID": "hw_acceleration.html#emerging-hardware-technologies-and-future-trends",
    "href": "hw_acceleration.html#emerging-hardware-technologies-and-future-trends",
    "title": "11¬† AI Acceleration",
    "section": "11.9 Emerging Hardware Technologies and Future Trends",
    "text": "11.9 Emerging Hardware Technologies and Future Trends\nExplanation: Discussing emerging technologies and trends, this section offers readers a glimpse into the future developments in the field of embedded hardware. This is vital to help readers stay abreast of the evolving landscape and potentially guide research and development efforts in the sector.\n\nOptimization Techniques for New Hardware\nFlexible Electronics\nNeuromorphic Computing\nIn-Memory Computing\n‚Ä¶\nChallenges with Scalability and Hardware-Software Integration\nNext-Generation Hardware Trends and Innovations"
  },
  {
    "objectID": "hw_acceleration.html#conclusion",
    "href": "hw_acceleration.html#conclusion",
    "title": "11¬† AI Acceleration",
    "section": "11.10 Conclusion",
    "text": "11.10 Conclusion\nExplanation: This section consolidates the key learnings from the chapter, providing a summary and a future outlook on hardware acceleration in embedded AI systems. This offers insight into where the field might be headed, helping to inspire future projects or study.\n\nSummary of Key Points\nThe Future Outlook for Hardware Acceleration in Embedded AI Systems"
  },
  {
    "objectID": "benchmarking.html#introduction",
    "href": "benchmarking.html#introduction",
    "title": "12¬† Benchmarking AI",
    "section": "12.1 Introduction",
    "text": "12.1 Introduction\nBenchmarking provides the essential measurements needed to drive progress in machine learning and to truly understand system performance. As the physicist Lord Kelvin famously said, ‚ÄúTo measure is to know.‚Äù Benchmarks give us the ability to know the capabilities of different models, software, and hardware quantitatively. They allow ML developers to measure the inference time, memory usage, power consumption, and other metrics that characterize a system. Moreover, benchmarks create standardized processes for measurement, enabling fair comparisons across different solutions.\nWhen benchmarks are maintained over time, they become instrumental in capturing progress across generations of algorithms, datasets, and hardware. The models and techniques that set new records on ML benchmarks from one year to the next demonstrate tangible improvements in what‚Äôs possible for on-device machine learning. By using benchmarks to measure, ML practitioners can know the real-world capabilities of their systems and have confidence that each step reflects genuine progress towards the state-of-the-art.\nBenchmarking has several important goals and objectives that guide its implementation for machine learning systems.\n\nPerformance assessment. This involves evaluating key metrics like the speed, accuracy, and efficiency of a given model. For instance, in a TinyML context, it is crucial to benchmark how quickly a voice assistant can recognize commands, as this evaluates real-time performance.\nResource evaluation. This means assessing the model‚Äôs impact on critical system resources including battery life, memory usage, and computational overhead. A relevant example is comparing the battery drain of two different image recognition algorithms running on a wearable device.\nValidation and verification. Benchmarking helps ensure the system functions correctly and meets specified requirements. One way is by checking the accuracy of an algorithm, like a heart rate monitor on a smartwatch, against readings from medical-grade equipment as a form of clinical validation.\nCompetitive analysis. This enables comparing solutions against competing offerings in the market. For example, benchmarking a custom object detection model versus common tinyML benchmarks like MobileNet and Tiny-YOLO.\nCredibility. Accurate benchmarks uphold the credibility of AI solutions and the organizations that develop them. They demonstrate a commitment to transparency, honesty, and quality, which is essential in building trust with users and stakeholders.\nRegulation and Standardization. As the AI industry continues to grow, there is an increasing need for regulation and standardization to ensure that AI solutions are safe, ethical, and effective. Accurate and reliable benchmarks are an essential component of this regulatory framework, as they provide the data and evidence needed to assess compliance with industry standards and legal requirements.\n\nThis chapter will cover the 3 types of benchmarks in AI, the standard metrics, tools, and techniques designers use to optimize their systems, and the challenges and trends in benchmarking."
  },
  {
    "objectID": "benchmarking.html#historical-context",
    "href": "benchmarking.html#historical-context",
    "title": "12¬† Benchmarking AI",
    "section": "12.2 Historical Context",
    "text": "12.2 Historical Context\n\n12.2.1 Standard Benchmarks\nThe evolution of benchmarks in computing vividly illustrates the industry‚Äôs relentless pursuit of excellence and innovation. In the early days of computing during the 1960s and 1970s, benchmarks were rudimentary and designed for mainframe computers. For example, the Whetstone benchmark, named after the Whetstone ALGOL compiler, was one of the first standardized tests to measure floating-point arithmetic performance of a CPU. These pioneering benchmarks prompted manufacturers to refine their architectures and algorithms to achieve better benchmark scores.\nThe 1980s marked a significant shift with the rise of personal computers. As companies like IBM, Apple, and Commodore competed for market share, and so benchmarks became critical tools to enable fair competition. The SPEC CPU benchmarks, introduced by the System Performance Evaluation Cooperative (SPEC), established standardized tests allowing objective comparisons between different machines. This standardization created a competitive environment, pushing silicon manufacturers and system creators to enhance their hardware and software offerings continually.\nWith the 1990s came the era of graphics-intensive applications and video games. The need for benchmarks to evaluate graphics card performance led to the creation of 3DMark by Futuremark. As gamers and professionals sought high-performance graphics cards, companies like NVIDIA and AMD were driven to rapid innovation, leading to major advancements in GPU technology like programmable shaders.\nThe 2000s saw a surge in mobile phones and portable devices like tablets. With portability came the challenge of balancing performance and power consumption. Benchmarks like MobileMark by BAPCo evaluated not just speed but also battery life. This drove companies to develop more energy-efficient System-on-Chips (SOCs), leading to the emergence of architectures like ARM that prioritized power efficiency.\nThe recent decade‚Äôs focus has shifted towards cloud computing, big data, and artificial intelligence. Cloud services providers like Amazon Web Services and Google Cloud compete on performance, scalability, and cost-effectiveness. Tailored cloud benchmarks like CloudSuite have become essential, driving providers to optimize their infrastructure for better services.\n\n\n12.2.2 Custom Benchmarks\nIn addition to industry-standard benchmarks, there are custom benchmarks that are specifically designed to meet the unique requirements of a particular application or task. They are tailored to the specific needs of the user or developer, ensuring that the performance metrics are directly relevant to the intended use of the AI model or system. Custom benchmarks can be created by individual organizations, researchers, or developers, and are often used in conjunction with industry standard benchmarks to provide a comprehensive evaluation of AI performance.\nFor example, a hospital could develop a benchmark to assess an AI model for predicting patient readmission. This benchmark would incorporate metrics relevant to the hospital‚Äôs patient population like demographics, medical history, and social factors. Similarly, a financial institution‚Äôs fraud detection benchmark could focus on identifying fraudulent transactions accurately while minimizing false positives. In automotive, an autonomous vehicle benchmark may prioritize performance in diverse conditions, responding to obstacles, and safety. Retailers could benchmark recommendation systems using click-through rate, conversion rate, and customer satisfaction. Manufacturing companies might benchmark quality control systems on defect identification, efficiency, and waste reduction. In each industry, custom benchmarks provide organizations with evaluation criteria tailored to their unique needs and context. This allows for more meaningful assessment of how well AI systems meet requirements.\nThe advantage of custom benchmarks lies in their flexibility and relevance. They can be designed to test specific aspects of performance that are critical to the success of the AI solution in its intended application. This allows for a more targeted and accurate assessment of the AI model or system‚Äôs capabilities. Custom benchmarks also provide valuable insights into the performance of AI solutions in real-world scenarios, which can be crucial for identifying potential issues and areas for improvement.\nIn AI, benchmarks play a crucial role in driving progress and innovation. While benchmarks have long been used in computing, their application to machine learning is relatively recent. AI-focused benchmarks aim to provide standardized metrics to evaluate and compare the performance of different algorithms, model architectures, and hardware platforms.\n\n\n12.2.3 Community Concensus\nA key prepragoative for any benchmark to be impactful is that it must reflect the shared priorities and values of the broader research community. Benchmarks designed in isolation risk failing to gain acceptance if they overlook key metrics considered important by leading groups. Through collaborative development with open participation from academic labs, companies, and other stakeholders, benchmarks can incorporate collective input on critical capabilities worth measuring. This helps ensure the benchmarks evaluate aspects the community agrees are essential to advance the field. The process of reaching alignment on tasks and metrics itself supports converging on what matters most.\nFurthermore, benchmarks published with broad co-authorship from respected institutions carry authority and validity that convinces the community to adopt them as trusted standards. Benchmarks perceived as biased by particular corporate or institutional interests breed skepticism. Ongoing community engagement through workshops and challenges is also key after initial release, and that is what, for instance, led to the success of ImageNet. As research rapidly progresses, collective participation enables continual refinement and expansion of benchmarks over time.\nFinally, community-developed benchmarks released with open access accelerate adoption and consistent implementation. Shared open source code, documentation, models and infrastructure lower barriers for groups to benchmark solutions on an equal footing using standardized implementations. This consistency is critical for fair comparisons. Without coordination, labs and companies may implement benchmarks differently, reducing result reproducibility.\nCommunity consensus brings benchmarks lasting relevance while fragmentation causes confusion. Through collaborative development and transparent operation, benchmarks can become authoritative standards for tracking progress. Several of the benchmarks that we discuss in this chapter were developed and built by the community, for the community, and that is what ultimately led to their success."
  },
  {
    "objectID": "benchmarking.html#ai-benchmarks-system-model-and-data",
    "href": "benchmarking.html#ai-benchmarks-system-model-and-data",
    "title": "12¬† Benchmarking AI",
    "section": "12.3 AI Benchmarks: System, Model, and Data",
    "text": "12.3 AI Benchmarks: System, Model, and Data\nAs AI systems grow in complexity and ubiquity, the need for comprehensive benchmarking becomes paramount. Within this context, benchmarks are often classified into three primary categories: Hardware, Model, and Data. Let‚Äôs delve into why each of these buckets is essential and the significance of evaluating AI from these three distinct dimensions:\n\nSystem Benchmarks\nAI computations, especially those in deep learning, are resource-intensive. The hardware on which these computations run plays a pivotal role in determining the speed, efficiency, and scalability of AI solutions. Consequently, hardware benchmarks help evaluate the performance of CPUs, GPUs, TPUs, and other accelerators in the context of AI tasks. By understanding hardware performance, developers can make informed choices about which hardware platforms are best suited for specific AI applications. Furthermore, hardware manufacturers use these benchmarks to identify areas for improvement, driving innovation in AI-specific chip designs.\n\n\nModel Benchmarks\nThe architecture, size, and complexity of AI models vary widely. Different models have different computational demands and offer varying levels of accuracy and efficiency. Model benchmarks help us assess the performance of various AI architectures on standardized tasks. They provide insights into the speed, accuracy, and resource demands of different models. By benchmarking models, researchers can identify best-performing architectures for specific tasks, guiding the AI community towards more efficient and effective solutions. Additionally, these benchmarks aid in tracking the progress of AI research, showcasing advancements in model design and optimization.\n\n\nData Benchmarks\nAI, particularly machine learning, is inherently data-driven. The quality, size, and diversity of data influence the training efficacy and generalization capability of AI models. Data benchmarks focus on the datasets used in AI training and evaluation. They provide standardized datasets that the community can use to train and test models, ensuring a level playing field for comparisons. Moreover, these benchmarks highlight challenges in data quality, diversity, and representation, pushing the community to address biases and gaps in AI training data. By understanding data benchmarks, researchers can also gauge how models might perform in real-world scenarios, ensuring robustness and reliability.\nIn the remainder of the sections, we will go through each of these benchmark types. The focus will be an in-depth exploration of system benchmarks, as these are critical to understanding and advancing machine learning system performance. We will cover model and data benchmarks briefly for a comprehensive perspective, but the emphasis and majority of the content will be devoted to system benchmarks."
  },
  {
    "objectID": "benchmarking.html#system-benchmarking",
    "href": "benchmarking.html#system-benchmarking",
    "title": "12¬† Benchmarking AI",
    "section": "12.4 System Benchmarking",
    "text": "12.4 System Benchmarking\n\n12.4.1 Granularity\nMachine learning system benchmarking provides a structured and systematic approach to assess how well a system is performing across various dimensions. Given the complexity of ML systems, we can dissect their performance through different levels of granularity and obtain a comprehensive view of the system‚Äôs efficiency, identify potential bottlenecks, and pinpoint areas for improvement. To this end, there are various types of benchmarks that have evolved over the years and continue to persist.\n\n\nMicro Benchmarks\nMicro-benchmarks in AI are specialized, focusing on the evaluation of distinct components or specific operations within a broader machine learning process. These benchmarks zero in on individual tasks, offering insights into the computational demands of a particular neural network layer, the efficiency of a unique optimization technique, or the throughput of a specific activation function. For instance, practitioners might use micro-benchmarks to measure the computational time required by a convolutional layer in a deep learning model or to evaluate the speed of data preprocessing that feeds data into the model. Such granular assessments are instrumental in fine-tuning and optimizing discrete aspects of AI models, ensuring that each component operates at its peak potential.\nThese types of microbenchmarks include that zoom into very specific operations or components of the AI pipeline, such as the following:\n\nTensor Operations: Libraries like cuDNN (by NVIDIA) often have benchmarks to measure the performance of individual tensor operations, such as convolutions or matrix multiplications, which are foundational to deep learning computations.\nActivation Functions: Benchmarks that measure the speed and efficiency of various activation functions like ReLU, Sigmoid, or Tanh in isolation.\nLayer Benchmarks: Evaluations of the computational efficiency of distinct neural network layers, such as a LSTM layer or a Transformer block, when operating on standardized input sizes.\n\nExample: DeepBench, introduced by Baidu, is a good example of something that asseses the above. DeepBench assesses the performance of basic operations in deep learning models, providing insights into how different hardware platforms handle neural network training and inference.\n\n\nMacro Benchmarks\nMacro-benchmarks provide a holistic view, assessing the end-to-end performance of entire machine learning models or comprehensive AI systems. Rather than focusing on individual operations, macro-benchmarks evaluate the collective efficacy of models under real-world scenarios or tasks. For example, a macro-benchmark might assess the complete performance of a deep learning model undertaking image classification on a dataset like ImageNet. This includes gauging accuracy, computational speed, and resource consumption. Similarly, one might measure the cumulative time and resources needed to train a natural language processing model on extensive text corpora or evaluate the performance of an entire recommendation system, from data ingestion to final user-specific outputs.\nExamples: These benchmarks evaluate the AI model:\n\nMLPerf Inference(Reddi et al. 2020): An industry-standard set of benchmarks for measuring the performance of machine learning software and hardware. MLPerf has a suite of dedicated benchmarks for specific scales, such as MLPerf Mobile for mobile class devices and MLPerf Tiny, which focuses on microcontrollers and other resource-constrained devices.\nEEMBC‚Äôs MLMark: A benchmarking suite for evaluating the performance and power efficiency of embedded devices running machine learning workloads. This benchmark provides insights into how different hardware platforms handle tasks like image recognition or audio processing.\nAI-Benchmark(Ignatov et al. 2018): A benchmarking tool designed for Android devices, it valuates the performance of AI tasks on mobile devices, encompassing various real-world scenarios like image recognition, face parsing, and optical character recognition.\n\n\nReddi, Vijay Janapa, Christine Cheng, David Kanter, Peter Mattson, Guenther Schmuelling, Carole-Jean Wu, Brian Anderson, et al. 2020. ‚ÄúMlperf Inference Benchmark.‚Äù In 2020 ACM/IEEE 47th Annual International Symposium on Computer Architecture (ISCA), 446‚Äì59. IEEE.\n\nIgnatov, Andrey, Radu Timofte, William Chou, Ke Wang, Max Wu, Tim Hartley, and Luc Van Gool. 2018. ‚ÄúAi Benchmark: Running Deep Neural Networks on Android Smartphones.‚Äù In Proceedings of the European Conference on Computer Vision (ECCV) Workshops, 0‚Äì0.\n\n\nEnd-to-end Benchmarks\nEnd-to-End Benchmarks provide an all-inclusive evaluation that extends beyond the boundaries of the AI model itself. Instead of focusing solely on the computational efficiency or accuracy of a machine learning model, these benchmarks encompass the entire pipeline of an AI system. This includes initial data pre-processing, the core model‚Äôs performance, post-processing of the model‚Äôs outputs, and even other integral components like storage and network interactions.\nData pre-processing is the first stage in many AI systems, transforming raw data into a format suitable for model training or inference. The efficiency, scalability, and accuracy of these pre-processing steps are vital for the overall system‚Äôs performance. End-to-end benchmarks assess this phase, ensuring that data cleaning, normalization, augmentation, or any other transformation process doesn‚Äôt become a bottleneck.\nThe post-processing phase also takes center stage. This involves interpreting the model‚Äôs raw outputs, possibly converting scores into meaningful categories, filtering results, or even integrating with other systems. In real-world applications, this phase is crucial for delivering actionable insights, and end-to-end benchmarks ensure it‚Äôs both efficient and effective.\nBeyond the core AI operations, other system components play a pivotal role in the overall performance and user experience. Storage solutions, be it cloud-based, on-premises, or hybrid, can significantly impact data retrieval and storage times, especially with vast AI datasets. Similarly, network interactions, vital for cloud-based AI solutions or distributed systems, can become performance bottlenecks if not optimized. End-to-end benchmarks holistically evaluate these components, ensuring that the entire system, from data retrieval to final output delivery, operates seamlessly.\nTo date, there are no public, end to end benchmarks that take into account the role of data storage, network and compute performance. Arguably, MLPerf Training and Inference, come close to the idea of an end to end benchmark but they are exclusively focused on ML model performance and do not represent real world deployment scenarios of how models are used in the field. Nonetheless, they provide a very useful signal that helps assess AI system performance.\nGiven the inherent specificity of end-to-end benchmarking, it is typically performed internally at a company by instrumenting real production deployments of AI. This allows engineers to have a realistic understanding and breakdown of the performance, but given the sensitivity and specificity of the information, it is rarely reported outside of the company.\n\n\nUnderstanding the Trade-offs\nDifferent issues arise at different stages of an AI system. Micro-benchmarks help in fine-tuning individual components, macro-benchmarks aid in refining model architectures or algorithms, and end-to-end benchmarks guide the optimization of the entire workflow. By understanding where a problem lies, developers can apply targeted optimizations.\nMoreover, while individual components of an AI system might perform optimally in isolation, bottlenecks can emerge when they interact. End-to-end benchmarks, in particular, are crucial to ensure that the entire system, when operating collectively, meets desired performance and efficiency standards.\nFinally, by discerning where performance bottlenecks or inefficiencies lie, organizations can make informed decisions on where to allocate resources. For instance, if micro-benchmarks reveal inefficiencies in specific tensor operations, investments can be directed towards specialized hardware accelerators. Conversely, if end-to-end benchmarks indicate data retrieval issues, investments might be channeled towards better storage solutions.\n\n\n\n12.4.2 Benchmark Components\nAt its core, an AI benchmark is more than just a test or a score; it‚Äôs a comprehensive evaluation framework. To understand this in-depth, let‚Äôs break down the typical components that go into an AI benchmark.\n\nStandardized Datasets\nDatasets serve as the foundation for most AI benchmarks. They provide a consistent set of data on which models are trained and evaluated, ensuring a level playing field for comparisons.\nExample: ImageNet, a large-scale dataset containing millions of labeled images spanning thousands of categories, is a popular benchmarking standard for image classification tasks.\n\n\nPre-defined Tasks\nA benchmark should have a clear objective or task that models aim to achieve. This task defines the problem the AI system is trying to solve.\nExample: For natural language processing benchmarks, tasks might include sentiment analysis, named entity recognition, or machine translation.\n\n\nEvaluation Metrics\nOnce a task is defined, benchmarks require metrics to quantify performance. These metrics offer objective measures to compare different models or systems.\nIn classification tasks, metrics like accuracy, precision, recall, and F1 score are commonly used. For regression tasks, mean squared error or mean absolute error might be employed.\n\n\nBaseline Models\nBenchmarks often include baseline models or reference implementations. These serve as starting points or minimum performance standards against which new models or techniques can be compared.\nExample: In many benchmark suites, simple models like linear regression or basic neural networks serve as baselines to provide context for more complex model evaluations.\n\n\nHardware and Software Specifications\nGiven the variability introduced by different hardware and software configurations, benchmarks often specify or document the hardware and software environments in which tests are conducted.\nExample: An AI benchmark might note that evaluations were conducted on an NVIDIA Tesla V100 GPU using TensorFlow v2.4.\n\n\nEnvironmental Conditions\nAs external factors can influence benchmark results, it‚Äôs essential to either control or document conditions like temperature, power source, or system background processes.\nExample: Mobile AI benchmarks might specify that tests were conducted at room temperature with devices plugged into a power source to eliminate battery-level variances.\n\n\nReproducibility Rules\nTo ensure benchmarks are credible and can be replicated by others in the community, they often include detailed protocols, covering everything from random seeds used to exact hyperparameters.\nExample: A benchmark for a reinforcement learning task might detail the exact training episodes, exploration-exploitation ratios, and reward structures used.\n\n\nResult Interpretation Guidelines\nBeyond raw scores or metrics, benchmarks often provide guidelines or context to interpret results, helping practitioners understand the broader implications.\nExample: A benchmark might highlight that while Model A scored higher than Model B in accuracy, Model B offers better real-time performance, making it more suitable for time-sensitive applications.\n\n\n\n12.4.3 Training vs.¬†Inference\nThe development life cycle of a machine learning model involves two critical phases - training and inference. Training is the process of learning patterns from data to create the model. Inference refers to the model making predictions on new unlabeled data. Both phases play indispensable yet distinct roles. Consequently, each phase warrants rigorous benchmarking to evaluate performance metrics like speed, accuracy, and computational efficiency.\nBenchmarking the training phase provides insights into how different model architectures, hyperparameter values, and optimization algorithms impact the time and resources needed to train the model. For instance, benchmarking shows how neural network depth affects training time on a given dataset. Benchmarking also reveals how hardware accelerators like GPUs and TPUs can speed up training.\nOn the other hand, benchmarking inference evaluates model performance in real-world conditions after deployment. Key metrics include latency, throughput, memory footprint, and power consumption. Inference benchmarking determines if an model meets the requirements of its target application regarding response time and device constraints, which is typically the focus of tinyML but we will discsuss these broadly to make sure we have a general understanding.\n\n\n12.4.4 Training Benchmarks\nTraining represents the phase where raw data is processed and ingested by the system to adjust and refine its parameters. Therefore, it is not just an algorithmic activity but also involves system-level considerations, including data pipelines, storage, computing resources, and orchestration mechanisms. The goal is to ensure that the ML system can efficiently learn from data, optimizing both the model‚Äôs performance and the system‚Äôs resource utilization.\n\nPurpose\nFrom an ML systems perspective, training benchmarks evaluate how well the system scales with increasing data volumes and computational demands. It‚Äôs about understanding the interplay between hardware, software, and the data pipeline in the training process.\nConsider a distributed ML system designed to train on vast datasets, like those used in large-scale e-commerce product recommendations. A training benchmark would assess how efficiently the system scales across multiple nodes, how it manages data sharding, and how it handles failures or node drop-offs during the training process.\nTraining benchmarks evaluate CPU, GPU, memory, and network utilization during the training phase, guiding system optimizations. When training a model in a cloud-based ML system, it‚Äôs crucial to understand how resources are being utilized. Are GPUs being fully leveraged? Is there unnecessary memory overhead? Benchmarks can highlight bottlenecks or inefficiencies in resource utilization, leading to cost savings and performance improvements.\nTraining an ML model is contingent on the timely and efficient delivery of data. Benchmarks in this context would also assess the efficiency of data pipelines, data preprocessing speed, and storage retrieval times. For real-time analytics systems, like those used in fraud detection, the speed at which training data is ingested, preprocessed, and fed into the model can be critical. Benchmarks would evaluate the latency of data pipelines, the efficiency of storage systems (like SSDs vs.¬†HDDs), and the speed of data augmentation or transformation tasks.\n\n\nMetrics\nTraining metrics, when viewed from a systems perspective, offer insights that transcend the conventional algorithmic performance indicators. These metrics not only measure the model‚Äôs learning efficacy but also gauge the efficiency, scalability, and robustness of the entire ML system during the training phase. Let‚Äôs delve deeper into these metrics and their significance.\nThe following metrics are often considered important:\n\nTraining Time: The time taken to train a model from scratch until it reaches a satisfactory performance level. It is a direct measure of the computational resources required to train a model. For example, Google‚Äôs BERT(Devlin et al. 2018) model is a natural language processing model that requires several days to train on a massive corpus of text data using multiple GPUs. The long training time is a significant challenge in terms of resource consumption and cost.\nScalability: How well the training process can handle increases in data size or model complexity. Scalability can be assessed by measuring training time, memory usage, and other resource consumption as data size or model complexity increases. OpenAI‚Äôs GPT-3(Brown et al. 2020) model has 175 billion parameters, making it one of the largest language models in existence. Training GPT-3 required extensive engineering efforts to scale up the training process to handle the massive model size. This involved the use of specialized hardware, distributed training, and other techniques to ensure that the model could be trained efficiently.\nResource Utilization: The extent to which the training process utilizes available computational resources such as CPU, GPU, memory, and disk I/O. High resource utilization can indicate an efficient training process, while low utilization can suggest bottlenecks or inefficiencies. For instance, training a convolutional neural network (CNN) for image classification requires significant GPU resources. Utilizing multi-GPU setups and optimizing the training code for GPU acceleration can greatly improve resource utilization and training efficiency.\nMemory Consumption: The amount of memory used by the training process. Memory consumption can be a limiting factor for training large models or datasets. As an example, Google researchers faced significant memory consumption challenges when training BERT. The model has hundreds of millions of parameters, which require large amounts of memory to store. The researchers had to develop techniques to reduce memory consumption, such as gradient checkpointing and model parallelism.\nEnergy Consumption: The amount of energy consumed during the training process. As machine learning models become larger and more complex, energy consumption has become an important consideration. Training large machine learning models can consume significant amounts of energy, leading to a large carbon footprint. For instance, the training of OpenAI‚Äôs GPT-3 was estimated to have a carbon footprint equivalent to traveling by car for 700,000 kilometers.\nThroughput: The number of training samples processed per unit time. Higher throughput generally indicates a more efficient training process. When training a recommendation system for an e-commerce platform, the throughput is an important metric to consider. A high throughput ensures that the model can process large volumes of user interaction data in a timely manner, which is crucial for maintaining the relevance and accuracy of the recommendations. But it‚Äôs also important to understand how to balance throughput with latency bounds. Therefore, often there is a latency-bounded throughput constraint that‚Äôs imposed on service-level agreements for datacenter application deployments.\nCost: The cost of training a model, which can include both computational and human resources. Cost is an important factor when considering the practicality and feasibility of training large or complex models. The cost of training large language models like GPT-3 is estimated to be in the range of millions of dollars. This cost includes computational resources, electricity, and human resources required for model development and training.\nFault Tolerance and Robustness: The ability of the training process to handle failures or errors without crashing or producing incorrect results. This is important for ensuring the reliability of the training process. In a real-world scenario, where a machine learning model is being trained on a distributed system, network failures or hardware malfunctions can occur. In recent years, for instance, it has become abundantly clear that faults that arise from silent data corruption have emerged as a major issue. A fault-tolerant and robust training process can recover from such failures without compromising the integrity of the model.\nEase of Use and Flexibility: The ease with which the training process can be set up and used, as well as its flexibility in handling different types of data and models. In companies like Google, efficiency can sometimes be measured in terms of the number of Software Engineer (SWE) years saved since that translates directly to impact. Ease of use and flexibility can reduce the time and effort required to train a model. TensorFlow and PyTorch are popular machine learning frameworks that provide user-friendly interfaces and flexible APIs for building and training machine learning models. These frameworks support a wide range of model architectures and are equipped with tools that simplify the training process.\nReproducibility: The ability to reproduce the results of the training process. Reproducibility is important for verifying the correctness and validity of a model. However, there are often variations due to stochastic network characteristics and this makes it hard to reproduce the precise behavior of applications being trained, and this can present a challenge for benchmarking.\n\nBy benchmarking for these types of metrics, we can obtain a comprehensive view of the performance and efficiency of the training process from a systems perspective, which can help identify areas for improvement and ensure that resources are used effectively.\n\n\nTasks\nSelecting a handful of representative tasks for benchmarking machine learning systems is challenging because machine learning is applied to a diverse range of domains, each with its own unique characteristics and requirements. Here are some of the challenges faced in selecting representative tasks:\n\nDiversity of Applications: Machine learning is used in numerous fields such as healthcare, finance, natural language processing, computer vision, and many more. Each field has specific tasks that may not be representative of other fields. For example, image classification tasks in computer vision may not be relevant to financial fraud detection.\nVariability in Data Types and Quality: Different tasks require different types of data, such as text, images, videos, or numerical data. The quality and availability of data can vary greatly between tasks, making it difficult to select tasks that are representative of the general challenges faced in machine learning.\nTask Complexity and Difficulty: The complexity of tasks varies greatly, with some tasks being relatively straightforward, while others are highly complex and require sophisticated models and techniques. Selecting representative tasks that cover the range of complexities encountered in machine learning is a challenge.\nEthical and Privacy Concerns: Some tasks may involve sensitive or private data, such as medical records or personal information. These tasks may have ethical and privacy concerns that need to be addressed, which can make them less suitable as representative tasks for benchmarking.\nScalability and Resource Requirements: Different tasks may have different scalability and resource requirements. Some tasks may require extensive computational resources, while others can be performed with minimal resources. Selecting tasks that are representative of the general resource requirements in machine learning is difficult.\nEvaluation Metrics: The metrics used to evaluate the performance of machine learning models vary between tasks. Some tasks may have well-established evaluation metrics, while others may lack clear or standardized metrics. This can make it challenging to compare performance across different tasks.\nGeneralizability of Results: The results obtained from benchmarking on a specific task may not be generalizable to other tasks. This means that the performance of a machine learning system on a selected task may not be indicative of its performance on other tasks.\n\nIt is important to carefully consider these factors when designing benchmarks to ensure that they are meaningful and relevant to the diverse range of tasks encountered in machine learning.\n\n\nBenchmarks\nHere are some original works that laid the fundamental groundwork for developing systematic benchmarks for training machine learning systems.\nMLPerf Training Benchmark\nMLPerf is a suite of benchmarks designed to measure the performance of machine learning hardware, software, and services. The MLPerf Training benchmark(Mattson et al. 2020) focuses on the time it takes to train models to a target quality metric. It includes a diverse set of workloads, such as image classification, object detection, translation, and reinforcement learning.\n\nMattson, Peter, Christine Cheng, Gregory Diamos, Cody Coleman, Paulius Micikevicius, David Patterson, Hanlin Tang, et al. 2020. ‚ÄúMlperf Training Benchmark.‚Äù Proceedings of Machine Learning and Systems 2: 336‚Äì49.\nMetrics:\n\nTraining time to target quality\nThroughput (examples per second)\nResource utilization (CPU, GPU, memory, disk I/O)\n\nDAWNBench\nDAWNBench(Coleman et al. 2017) is a benchmark suite that focuses on end-to-end deep learning training time and inference performance. It includes common tasks such as image classification and question answering.\n\nColeman, Cody, Deepak Narayanan, Daniel Kang, Tian Zhao, Jian Zhang, Luigi Nardi, Peter Bailis, Kunle Olukotun, Chris R√©, and Matei Zaharia. 2017. ‚ÄúDawnbench: An End-to-End Deep Learning Benchmark and Competition.‚Äù Training 100 (101): 102.\nMetrics:\n\nTime to train to target accuracy\nInference latency\nCost (in terms of cloud compute and storage resources)\n\nFathom\nFathom(Adolf et al. 2016) is a benchmark from Harvard University that includes a diverse set of workloads to evaluate the performance of deep learning models. It includes common tasks such as image classification, speech recognition, and language modeling.\n\nAdolf, Robert, Saketh Rama, Brandon Reagen, Gu-Yeon Wei, and David Brooks. 2016. ‚ÄúFathom: Reference Workloads for Modern Deep Learning Methods.‚Äù In 2016 IEEE International Symposium on Workload Characterization (IISWC), 1‚Äì10. IEEE.\nMetrics:\n\nOperations per second (to measure computational efficiency)\nTime to completion for each workload\nMemory bandwidth\n\nExample Use Case\nConsider a scenario where we want to benchmark the training of an image classification model on a specific hardware platform.\n\nTask: The task is to train a convolutional neural network (CNN) for image classification on the CIFAR-10 dataset.\nBenchmark: We can use the MLPerf Training benchmark for this task. It includes an image classification workload that is relevant to our task.\nMetrics: We will measure the following metrics:\n\n\nTraining time to reach a target accuracy of 90%.\nThroughput in terms of images processed per second.\nGPU and CPU utilization during training.\n\nBy measuring these metrics, we can assess the performance and efficiency of the training process on the selected hardware platform. This information can then be used to identify potential bottlenecks or areas for improvement.\n\n\n\n12.4.5 Inference Benchmarks\nInference in machine learning refers to the process of using a trained model to make predictions on new, unseen data. It is the phase where the model applies its learned knowledge to solve the problem it was designed for, such as classifying images, recognizing speech, or translating text.\n\nPurpose\nWhen we build machine learning models, our ultimate goal is to deploy them in real-world applications where they can provide accurate and reliable predictions on new, unseen data. This process of using a trained model to make predictions is known as inference. The real-world performance of a machine learning model can differ significantly from its performance on training or validation datasets, which makes benchmarking inference a crucial step in the development and deployment of machine learning models.\nBenchmarking inference allows us to evaluate how well a machine learning model performs in real-world scenarios. This evaluation ensures that the model is practical and reliable when deployed in applications, providing a more comprehensive understanding of the model‚Äôs behavior with real data. Additionally, benchmarking can help identify potential bottlenecks or limitations in the model‚Äôs performance. For example, if a model takes too long to make a prediction, it may be impractical for real-time applications such as autonomous driving or voice assistants.\nResource efficiency is another critical aspect of inference, as it can be computationally intensive and require significant memory and processing power. Benchmarking helps ensure that the model is efficient in terms of resource usage, which is particularly important for edge devices with limited computational capabilities, such as smartphones or IoT devices. Moreover, benchmarking allows us to compare the performance of our model with competing models or previous versions of the same model. This comparison is essential for making informed decisions about which model to deploy in a specific application.\nFinally, ensuring that the model‚Äôs predictions are not only accurate but also consistent across different data points is vital. Benchmarking helps verify the model‚Äôs accuracy and consistency, ensuring that it meets the application‚Äôs requirements. It also assesses the robustness of the model, ensuring that it can handle real-world data variability and still make accurate predictions.\n\n\nMetrics\n\nAccuracy: Accuracy is one of the most vital metrics when benchmarking machine learning models, quantifying the proportion of correct predictions made by the model compared to the true values or labels. For example, in the case of a spam detection model that can correctly classify 95 out of 100 email messages as spam or not spam, the accuracy of this model would be calculated as 95%.\nLatency: Latency is a performance metric that calculates the time lag or delay occurring between the receipt of an input and the production of the corresponding output by the machine learning system. An example that clearly depicts latency is a real-time translation application; if there exists a half-second delay from the moment a user inputs a sentence to the time the translated text is displayed by the app, then the system‚Äôs latency is 0.5 seconds.\nLatency-Bounded Throughput: Latency-bounded throughput is a valuable metric that combines the aspects of latency and throughput, measuring the maximum throughput of a system while still meeting a specified latency constraint. For example, in a video streaming application that utilizes a machine learning model to automatically generate and display subtitles, latency-bounded throughput would measure how many video frames the system can process per second (throughput) while ensuring that the subtitles are displayed with no more than a 1-second delay (latency). This metric is particularly important in real-time applications where meeting latency requirements is crucial to the user experience.\nThroughput: Throughput assesses the system‚Äôs capacity by measuring the total number of inferences or predictions a machine learning model can handle within a specific unit of time. Consider a speech recognition system that employs a Recurrent Neural Network (RNN) as its underlying model; if this system is capable of processing and understanding 50 different audio clips in a minute, then its throughput rate stands at 50 clips per minute.\nInference Time: Inference time is a crucial metric that measures the duration a machine learning system, such as a Convolutional Neural Network (CNN) used in image recognition tasks, takes to process an input and generate a prediction or output. For instance, if a CNN takes approximately 2 milliseconds to accurately identify and label a cat within a given photo, then its inference time is said to be 2 milliseconds.\nEnergy Efficiency: Energy efficiency is a metric that determines the amount of energy consumed by the machine learning model to perform a single inference. A prime example of this would be a natural language processing model built on a Transformer network architecture; if it utilizes 0.1 Joules of energy to translate a sentence from English to French, its energy efficiency is measured at 0.1 Joules per inference.\nMemory Usage: Memory usage quantifies the volume of RAM needed by a machine learning model to carry out inference tasks. A relevant example to illustrate this would be a face recognition system that is based on a CNN; if such a system requires 150 MB of RAM to process and recognize faces within an image, then its memory usage is 150 MB.\n\n\n\nTasks\nBy and large, the challenges in picking representative tasks for benchmarking inference machine learning systems are somewhat of the same taxonomy as what we have provided for training. Nevertheless, to be pedantic, let‚Äôs discuss those in the context of inference machine learning systems.\n\nDiversity of Applications: Inference machine learning is employed across numerous domains such as healthcare, finance, entertainment, security, and more. Each domain has its unique tasks, and what‚Äôs representative in one domain might not be in another. For example, an inference task for predicting stock prices in the financial domain might not be representative of image recognition tasks in the medical domain.\nVariability in Data Types: Different inference tasks require different types of data ‚Äì text, images, videos, numerical data, etc. Ensuring that benchmarks address the wide variety of data types used in real-world applications is challenging. For example, voice recognition systems process audio data, which is vastly different from the visual data processed by facial recognition systems.\nTask Complexity: The complexity of inference tasks can differ immensely, from basic classification tasks to intricate tasks requiring state-of-the-art models. For example, differentiating between two categories (binary classification) is typically simpler than detecting hundreds of object types in a crowded scene.\nReal-time Requirements: Some applications demand immediate or real-time responses, while others may allow for some delay. In autonomous driving, real-time object detection and decision-making are paramount, whereas a recommendation engine for a shopping website might tolerate slight delays.\nScalability Concerns: Given the varied scale of applications, from edge devices to cloud-based servers, tasks must represent the diverse computational environments where inference occurs. For example, an inference task running on a smartphone‚Äôs limited resources is quite different from one running on a powerful cloud server.\nEvaluation Metrics Diversity: Depending on the task, the metrics to evaluate performance can differ significantly. Finding a common ground or universally accepted metric for diverse tasks is a challenge. For example, precision and recall might be vital for a medical diagnosis task, whereas throughput (inferences per second) might be more crucial for video processing tasks.\nEthical and Privacy Concerns: Especially in sensitive areas like facial recognition or personal data processing, there are concerns related to ethics and privacy. These concerns can impact the selection and nature of tasks used for benchmarking. For example, using real-world facial data for benchmarking can raise privacy issues, whereas synthetic data might not replicate real-world challenges.\nHardware Diversity: With a wide range of devices from GPUs, CPUs, TPUs, to custom ASICs used for inference, ensuring that tasks are representative across varied hardware is challenging. For example, a task optimized for inference on a GPU might perform sub-optimally on an edge device.\n\n\n\nBenchmarks\nHere are some original works that laid the fundamental groundwork for developing systematic benchmarks for inference machine learning systems.\nMLPerf Inference Benchmark\nMLPerf Inference is a comprehensive suite of benchmarks that assess the performance of machine learning models during the inference phase. It encompasses a variety of workloads including image classification, object detection, and natural language processing, aiming to provide standardized and insightful metrics for evaluating different inference systems.\nMetrics:\n\nInference time\nLatency\nThroughput\nAccuracy\nEnergy consumption\n\nAI Benchmark\nAI Benchmark is a benchmarking tool that evaluates the performance of AI and machine learning models on mobile devices and edge computing platforms. It includes tests for image classification, object detection, and natural language processing tasks, providing a detailed analysis of the inference performance on different hardware platforms.\nMetrics:\n\nInference time\nLatency\nEnergy consumption\nMemory usage\nThroughput\n\nOpenVINO‚Ñ¢ toolkit\nOpenVINO‚Ñ¢ toolkit provides a benchmark tool to measure the performance of deep learning models for a variety of tasks such as image classification, object detection, and facial recognition on Intel hardware. It offers detailed insights into the inference performance of the models on different hardware configurations.\nMetrics:\n\nInference time\nThroughput\nLatency\nCPU and GPU utilization\n\nExample Use Case\nConsider a scenario where we want to evaluate the inference performance of an object detection model on a specific edge device.\nTask: The task is to perform real-time object detection on video streams, detecting and identifying objects such as vehicles, pedestrians, and traffic signs.\nBenchmark: We can use the AI Benchmark for this task as it focuses on evaluating inference performance on edge devices, which is suitable for our scenario.\nMetrics: We will measure the following metrics:\n\nInference time to process each video frame\nLatency to generate the bounding boxes for detected objects\nEnergy consumption during the inference process\nThroughput in terms of video frames processed per second\n\nBy measuring these metrics, we can assess the performance of the object detection model on the edge device and identify any potential bottlenecks or areas for optimization to enhance real-time processing capabilities.\n\n\n\n12.4.6 Benchmark Example\nIn order to properly illustrate the components of a systems benchmark, we can look at the keyword spotting benchmark in MLPerf Tiny and explain the motivation behind each decision.\n\nTask\nKeyword spotting was selected as a task because it is a common usecase in TinyML that has been well established for years. Additionally the typical hardware used for keyword spotting differs substantially from the offerings of other benchmarks such as MLPerf Inference‚Äôs speech recognition task.\n\n\nDataset\nGoogle Speech Commands(Warden 2018) was selected as the best dataset to represent the task. The dataset is well established in the research community and has permissive licensing which allows it to be easily used in a benchmark.\n\nWarden, Pete. 2018. ‚ÄúSpeech Commands: A Dataset for Limited-Vocabulary Speech Recognition.‚Äù arXiv Preprint arXiv:1804.03209.\n\n\nModel\nThe next core component is the model which will act as the primary workload for the benchmark. The model should be well established as a solution to the selected task and not necessarily the state of the art solution. The model selected is a simple depthwise seperable convolution model. This architecture is not the state of the art solution to the task, but it is well established and not designed for a specific hardware platform like many of the state of the art solutions. The benchmark also establishes a reference training recipe, despite being an inference benchmark, in order to be fully reproducible and transparent.\n\n\nMetrics\nLatency was selected as the primary metric for the benchmark, as keyword spotting systems need to react quickly to maintain user satisfaction. Additionally, given that TinyML systems are often battery powered, energy consumption is measured to ensure the hardware platform is efficient. The accuracy of the model is also measure to ensure that the optimizations applied by a submitter, such as quantization, don‚Äôt degrade the accuracy beyond a threshold.\n\n\nBenchmark Harness\nMLPerf Tiny uses EEMBCs EnergyRunner‚Ñ¢ benchmark harness to load the inputs to the model and to isolate and measure the energy consumption of the device. When measuring energy consumption it‚Äôs critical to select a harness that is accurate at the expected power levels of the devices under test, and simple enough to not become a burden for participants of the benchmark.\n\n\nBaseline Submission\nBaseline submissions are critical for contextualizing results and acting as a reference point to help participants get started. The baseline submission should prioritise simplicity and readability over state of the art performance. The keyword spotting baseline uses a standard STM microcontroller as it‚Äôs hardware and TensorFlow Lite for Microcontrollers(David et al. 2021) as it‚Äôs inference framework.\n\nDavid, Robert, Jared Duke, Advait Jain, Vijay Janapa Reddi, Nat Jeffries, Jian Li, Nick Kreeger, et al. 2021. ‚ÄúTensorflow Lite Micro: Embedded Machine Learning for Tinyml Systems.‚Äù Proceedings of Machine Learning and Systems 3: 800‚Äì811.\n\n\n\n12.4.7 Challenges and Limitations\nWhile benchmarking provides a structured methodology for performance evaluation in complex domains like artificial intelligence and computing, the process also poses several challenges. If not properly addressed, these challenges can undermine the credibility and accuracy of benchmarking results. Some of the predominant difficulties faced in benchmarking include the following:\n\nIncomplete problem coverage - Benchmark tasks may not fully represent the problem space. For instance, common image classification datasets like CIFAR-10 have limited diversity in image types. Algorithms tuned for such benchmarks may fail to generalize well to real-world datasets.\nStatistical insignificance - Benchmarks must have enough trials and data samples to produce statistically significant results. For example, benchmarking an OCR model on only a few text scans may not adequately capture its true error rates.\nLimited reproducibility - Varying hardware, software versions, codebases and other factors can reduce reproducibility of benchmark results. MLPerf addresses this by providing reference implementations and environment specification.\nMisalignment with end goals - Benchmarks focusing only on speed or accuracy metrics may misalign with real-world objectives like cost and power efficiency. Benchmarks must reflect all critical performance axes.\nRapid staleness - Due to the fast pace of advancements in AI and computing, benchmarks and their datasets can become outdated quickly. Maintaining up-to-date benchmarks is thus a persistent challenge.\n\nBut of all these, perhaps the most important challenge is dealing with benchmark engineering.\n\nHardware Lottery\nThe ‚Äúhardware lottery‚Äù in benchmarking machine learning systems refers to the situation where the success or efficiency of a machine learning model is significantly influenced by the compatibility of the model with the underlying hardware(Chu et al. 2021). In other words, some models perform exceptionally well because they are a good fit for the particular characteristics or capabilities of the hardware on which they are run, rather than because they are intrinsically superior models. Unfortunately, the hardware used is often omitted from papers or given only brief mentions, making reproducing results difficult if not impossible.\n\nChu, Grace, Okan Arikan, Gabriel Bender, Weijun Wang, Achille Brighton, Pieter-Jan Kindermans, Hanxiao Liu, Berkin Akin, Suyog Gupta, and Andrew Howard. 2021. ‚ÄúDiscovering Multi-Hardware Mobile Models via Architecture Search.‚Äù In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 3022‚Äì31.\nFor instance, certain machine learning models may be designed and optimized to take advantage of parallel processing capabilities of specific hardware accelerators, such as Graphics Processing Units (GPUs) or Tensor Processing Units (TPUs). As a result, these models might show superior performance when benchmarked on such hardware, compared to other models that are not optimized for the hardware.\nFor example, a 2018 paper introduced a new convolutional neural network architecture for image classification that achieved state-of-the-art accuracy on ImageNet. However, the paper only mentioned that the model was trained on 8 GPUs, without specifying the model, memory size, or other relevant details. A follow-up study tried to reproduce the results but found that training the same model on commonly available GPUs achieved 10% lower accuracy, even after hyperparameter tuning. The original hardware likely had far higher memory bandwidth and compute power. As another example, training times for large language models can vary drastically based on the GPUs used.\nThe ‚Äúhardware lottery‚Äù can introduce challenges and biases in benchmarking machine learning systems, as the performance of the model is not solely dependent on the model‚Äôs architecture or algorithm, but also on the compatibility and synergies with the underlying hardware. This can make it difficult to fairly compare different models and to identify the best model based on its intrinsic merits. It can also lead to a situation where the community converges on models that are a good fit for the popular hardware of the day, potentially overlooking other models that might be superior but are not compatible with the current hardware trends.\nHardware Lottery\n\n\nBenchmark Engineering\nHardware lottery occurs when a machine learning model unintentionally performs exceptionally well or poorly on a specific hardware setup due to unforeseen compatibility or incompatibility. The model is not explicitly designed or optimized for that particular hardware by the developers or engineers; rather, it happens to align or (mis)align with the hardware‚Äôs capabilities or limitations. In this case, the performance of the model on the hardware is a byproduct of coincidence rather than design.\nIn contrast to the accidental hardware lottery, benchmark engineering involves deliberately optimizing or designing a machine learning model to perform exceptionally well on specific hardware, often to win benchmarks or competitions. This intentional optimization might include tweaking the model‚Äôs architecture, algorithms, or parameters to take full advantage of the hardware‚Äôs features and capabilities.\n\n\nProblem\nBenchmark engineering refers to the process of tweaking or modifying an AI system to optimize its performance on specific benchmark tests, often at the expense of generalizability or real-world performance. This can include adjusting hyperparameters, training data, or other aspects of the system specifically to achieve high scores on benchmark metrics, without necessarily improving the overall functionality or utility of the system.\nThe motivation behind benchmark engineering often stems from the desire to achieve high performance scores for marketing or competitive purposes. High benchmark scores can be used to demonstrate the superiority of an AI system compared to competitors, and can be a key selling point for potential users or investors. This pressure to perform well on benchmarks can sometimes lead to the prioritization of benchmark-specific optimizations over more holistic improvements to the system.\nIt can lead to a number of risks and challenges. One of the primary risks is that the AI system may not perform as well in real-world applications as the benchmark scores suggest. This can lead to user dissatisfaction, reputational damage, and potential safety or ethical concerns. Furthermore, benchmark engineering can contribute to a lack of transparency and accountability in the AI community, as it can be difficult to discern how much of an AI system‚Äôs performance is due to genuine improvements versus benchmark-specific optimizations.\nTo mitigate the risks associated with benchmark engineering, it is important for the AI community to prioritize transparency and accountability. This can include clearly disclosing any optimizations or adjustments made specifically for benchmark tests, as well as providing more comprehensive evaluations of AI systems that include real-world performance metrics in addition to benchmark scores. Additionally, it is important for researchers and developers to prioritize holistic improvements to AI systems that improve their generalizability and functionality across a range of applications, rather than focusing solely on benchmark-specific optimizations.\n\n\nIssues\nOne of the primary problems with benchmark engineering is that it can compromise the real-world performance of AI systems. When developers focus on optimizing their systems to achieve high scores on specific benchmark tests, they may neglect other important aspects of system performance that are crucial in real-world applications. For example, an AI system designed for image recognition might be engineered to perform exceptionally well on a benchmark test that includes a specific set of images, but struggle to accurately recognize images that are slightly different from those in the test set.\nAnother issue with benchmark engineering is that it can result in AI systems that lack generalizability. In other words, while the system may perform well on the benchmark test, it may not be able to handle a diverse range of inputs or scenarios. For instance, an AI model developed for natural language processing might be engineered to achieve high scores on a benchmark test that includes a specific type of text, but fail to accurately process text that falls outside of that specific type.\nIt can also lead to misleading results. When AI systems are engineered to perform well on benchmark tests, the results may not accurately reflect the true capabilities of the system. This can be problematic for users or investors who rely on benchmark scores to make informed decisions about which AI systems to use or invest in. For example, an AI system that has been engineered to achieve high scores on a benchmark test for speech recognition might not actually be capable of accurately recognizing speech in real-world situations, leading users or investors to make decisions based on inaccurate information.\n\n\nMitigation\nThere are several ways to mitigate benchmark engineering. Transparency in the benchmarking process is crucial to maintaining the accuracy and reliability of benchmarks. This involves clearly disclosing the methodologies, data sets, and evaluation criteria used in benchmark tests, as well as any optimizations or adjustments made to the AI system for the purpose of the benchmark.\nOne way to achieve transparency is through the use of open-source benchmarks. Open-source benchmarks are made publicly available, allowing researchers, developers, and other stakeholders to review, critique, and contribute to the benchmark, thereby ensuring its accuracy and reliability. This collaborative approach also facilitates the sharing of best practices and the development of more robust and comprehensive benchmarks.\nAnother method for achieving transparency is through peer review of benchmarks. This involves having independent experts review and validate the benchmark‚Äôs methodology, data sets, and results to ensure their credibility and reliability. Peer review can provide a valuable means of verifying the accuracy of benchmark tests and can help to build confidence in the results.\nStandardization of benchmarks is another important solution to mitigate benchmark engineering. Standardized benchmarks provide a common framework for evaluating AI systems, ensuring consistency and comparability across different systems and applications. This can be achieved through the development of industry-wide standards and best practices for benchmarking, as well as through the use of common metrics and evaluation criteria.\nThird-party verification of results can also be a valuable tool in mitigating benchmark engineering. This involves having an independent third party verify the results of a benchmark test to ensure their credibility and reliability. Third-party verification can help to build confidence in the results and can provide a valuable means of validating the performance and capabilities of AI systems.\nResource: Benchmarking TinyML Systems: Challenges and Directions(Banbury et al. 2020)\n\nBanbury, Colby R, Vijay Janapa Reddi, Max Lam, William Fu, Amin Fazel, Jeremy Holleman, Xinyuan Huang, et al. 2020. ‚ÄúBenchmarking Tinyml Systems: Challenges and Direction.‚Äù arXiv Preprint arXiv:2003.04821.\n\nFigure 1: The modular design of MLPerf Tiny enables both the direct comparison of solutions and the demonstration of an improvement over the reference. The reference implementations are fully implemented solutions that allow individual components to be swapped out. The components in green can be modified in either division, and the orange components can only be modified in the open division. The reference implementations also act as the baseline for the results.\nSource: MLPerf Tiny Benchmark (https://arxiv.org/pdf/2106.07597.pdf)"
  },
  {
    "objectID": "benchmarking.html#model-benchmarking",
    "href": "benchmarking.html#model-benchmarking",
    "title": "12¬† Benchmarking AI",
    "section": "12.5 Model Benchmarking",
    "text": "12.5 Model Benchmarking\nBenchmarking machine learning models is important for determining the effectiveness and efficiency of various machine learning algorithms in solving specific tasks or problems. By analyzing the results obtained from benchmarking, developers and researchers can identify the strengths and weaknesses of their models, leading to more informed decisions on model selection and further optimization.\nThe evolution and progress of machine learning models are intrinsically linked to the availability and quality of data sets. In the world of machine learning, data acts as the raw material that powers the algorithms, allowing them to learn, adapt, and ultimately perform tasks that were traditionally the domain of humans. Therefore, it is important to understand this history.\n\n12.5.1 Historical Context\nMachine learning datasets have a rich history and have evolved significantly over the years, growing in size, complexity, and diversity to meet the ever-increasing demands of the field. Let‚Äôs take a closer look at this evolution, starting from one of the earliest and most iconic datasets ‚Äì MNIST.\n\nMNIST (1998)\nThe MNIST dataset, created by Yann LeCun, Corinna Cortes, and Christopher J.C. Burges in 1998, can be considered a cornerstone in the history of machine learning datasets. It consists of 70,000 labeled 28x28 pixel grayscale images of handwritten digits (0-9). MNIST has been widely used for benchmarking algorithms in image processing and machine learning, serving as a starting point for many researchers and practitioners in the field.\n\nSource: https://en.wikipedia.org/wiki/File:MnistExamplesModified.png\n\n\nImageNet (2009)\nFast forward to 2009, and we see the introduction of the ImageNet dataset, which marked a significant leap in the scale and complexity of datasets. ImageNet consists of over 14 million labeled images spanning more than 20,000 categories. It was developed by Fei-Fei Li and her team with the goal of advancing research in object recognition and computer vision. The dataset became synonymous with the ImageNet Large Scale Visual Recognition Challenge (ILSVRC), an annual competition that played a crucial role in the development of deep learning models, including the famous AlexNet in 2012.\n\nSource: https://cv.gluon.ai/_images/imagenet_banner.jpeg\n\n\nCOCO (2014)\nThe Common Objects in Context (COCO) dataset(Lin et al. 2014), released in 2014, further expanded the landscape of machine learning datasets by introducing a richer set of annotations. COCO consists of images containing complex scenes with multiple objects, and each image is annotated with object bounding boxes, segmentation masks, and captions. This dataset has been instrumental in advancing research in object detection, segmentation, and image captioning.\n\nLin, Tsung-Yi, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Doll√°r, and C Lawrence Zitnick. 2014. ‚ÄúMicrosoft Coco: Common Objects in Context.‚Äù In Computer Vision‚ÄìECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part v 13, 740‚Äì55. Springer.\n ‚Äã‚Äãhttps://cocodataset.org/images/coco-examples.jpg\n\n\nGPT-3 (2020)\nWhile the above examples primarily focus on image datasets, there have been significant developments in text datasets as well. One notable example is GPT-3(Brown et al. 2020), developed by OpenAI. GPT-3 is a language model trained on a diverse range of internet text. Although the dataset used to train GPT-3 is not publicly available, the model itself, consisting of 175 billion parameters, is a testament to the scale and complexity of modern machine learning datasets and models.\n\nBrown, Tom, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, et al. 2020. ‚ÄúLanguage Models Are Few-Shot Learners.‚Äù Advances in Neural Information Processing Systems 33: 1877‚Äì1901.\n\n\nPresent and Future\nToday, we have a plethora of datasets spanning various domains, including healthcare, finance, social sciences, and more. The following characteristics are how we can taxonomiize the space and growth of machine learning datasets that fuel model development.\n\nDiversity of Data Sets: The variety of data sets available to researchers and engineers has expanded dramatically over the years, covering a wide range of fields, including natural language processing, image recognition, and more. This diversity has fueled the development of specialized machine learning models tailored to specific tasks, such as translation, speech recognition, and facial recognition.\nVolume of Data: The sheer volume of data that has become available in the digital age has also played a crucial role in advancing machine learning models. Large data sets enable models to capture the complexity and nuances of real-world phenomena, leading to more accurate and reliable predictions.\nQuality and Cleanliness of Data: The quality of data is another critical factor that influences the performance of machine learning models. Clean, well-labeled, and unbiased data sets are essential for training models that are robust and fair.\nOpen Access to Data: The availability of open-access data sets has also contributed significantly to the progress in machine learning. Open data allows researchers from around the world to collaborate, share insights, and build upon each other‚Äôs work, leading to faster innovation and development of more advanced models.\nEthics and Privacy Concerns: As data sets continue to grow in size and complexity, ethical considerations and privacy concerns become increasingly important. There is an ongoing debate about the balance between leveraging data for machine learning advancements and protecting individuals‚Äô privacy rights.\n\nThe development of machine learning models is heavily reliant on the availability of diverse, large, high-quality, and open-access data sets. As we move forward, addressing the ethical considerations and privacy concerns associated with the use of large data sets is crucial to ensure that machine learning technologies benefit society as a whole. There is a growing awareness that data acts as the rocket fuel for machine learning, driving and fueling the development of machine learning models. Consequently, an increasing amount of focus is being placed on the development of the data sets themselves. We will explore this in further detail in the data benchmarking section.\n\n\n\n12.5.2 Model Metrics\nThe evolution of machine learning model evaluation has witnessed a transition from a narrow focus on accuracy to a more comprehensive approach that takes into account a range of factors, from ethical considerations and real-world applicability to practical constraints like model size and efficiency. This shift reflects the maturation of the field as machine learning models are increasingly applied in diverse and complex real-world scenarios.\n\nAccuracy\nAccuracy is one of the most intuitive and commonly used metrics for evaluating machine learning models. At its core, accuracy measures the proportion of correct predictions made by the model out of all predictions. As an example, imagine we have developed a machine learning model to classify images as either containing a cat or not. If we test this model on a dataset of 100 images, and it correctly identifies 90 of them, we would calculate its accuracy as 90%.\nIn the initial stages of machine learning, accuracy was often the primary, if not the only, metric considered when evaluating model performance. This is perhaps understandable, given its straightforward nature and ease of interpretation. However, as the field has progressed, the limitations of relying solely on accuracy have become more apparent.\nConsider the example of a medical diagnosis model that has an accuracy of 95%. While at first glance this may seem impressive, we must delve deeper to fully assess the model‚Äôs performance. If the model fails to accurately diagnose severe conditions that, while rare, can have severe consequences, its high accuracy may not be as meaningful. A pertinent example of this is Google‚Äôs retinopathy machine learning model, which was designed to diagnose diabetic retinopathy and diabetic macular edema from retinal photographs.\nThe Google model demonstrated impressive accuracy levels in lab settings, but when deployed in real-world clinical environments in Thailand, it faced significant challenges. In the real-world setting, the model encountered diverse patient populations, varying image quality, and a range of different medical conditions that it had not been exposed to during its training. Consequently, its performance was compromised, and it struggled to maintain the same levels of accuracy that had been observed in lab settings. This example serves as a clear reminder that while high accuracy is an important and desirable attribute for a medical diagnosis model, it must be evaluated in conjunction with other factors, such as the model‚Äôs ability to generalize to different populations and handle diverse and unpredictable real-world conditions, to truly understand its value and potential impact on patient care.\nSimilarly, if the model performs well on average but exhibits significant disparities in performance across different demographic groups, this too would be a cause for concern.\nThe evolution of machine learning has thus seen a shift towards a more holistic approach to model evaluation, taking into account not just accuracy, but also other crucial factors such as fairness, transparency, and real-world applicability. A prime example of this is the Gender Shades project at MIT Media Lab, led by Joy Buolamwini, which highlighted significant racial and gender biases in commercial facial recognition systems. The project evaluated the performance of three facial recognition technologies developed by IBM, Microsoft, and Face++ and found that they all exhibited biases, performing better on lighter-skinned and male faces compared to darker-skinned and female faces.\nWhile accuracy remains a fundamental and valuable metric for evaluating machine learning models, it is clear that a more comprehensive approach is required to fully assess a model‚Äôs performance. This means considering additional metrics that account for fairness, transparency, and real-world applicability, as well as conducting rigorous testing across diverse datasets to uncover and mitigate any potential biases. The move towards a more holistic approach to model evaluation reflects the maturation of the field and its increasing recognition of the real-world implications and ethical considerations associated with deploying machine learning models.\n\n\nFairness\nFairness in machine learning models is a multifaceted and critical aspect that requires careful attention, particularly in high-stakes applications that significantly affect people‚Äôs lives, such as in loan approval processes, hiring, and criminal justice. It refers to the equitable treatment of all individuals, irrespective of their demographic or social attributes such as race, gender, age, or socioeconomic status.\nWhen evaluating models, simply relying on accuracy can be insufficient and potentially misleading. For instance, consider a loan approval model that boasts a 95% accuracy rate. While this figure may appear impressive at first glance, it does not reveal how the model performs across different demographic groups. If this model consistently discriminates against a particular group, its accuracy is less commendable, and its fairness comes into question.\nDiscrimination can manifest in various forms, such as direct discrimination, where a model explicitly uses sensitive attributes like race or gender in its decision-making process, or indirect discrimination, where seemingly neutral variables correlate with sensitive attributes, indirectly influencing the model‚Äôs outcomes. An infamous example of the latter is the COMPAS tool used in the US criminal justice system, which exhibited racial biases in predicting recidivism rates, despite not explicitly using race as a variable.\nAddressing fairness involves careful examination of the model‚Äôs performance across diverse groups, identification of potential biases, and rectification of disparities through corrective measures such as re-balancing datasets, adjusting model parameters, and implementing fairness-aware algorithms. Researchers and practitioners are continuously developing metrics and methodologies tailored to specific use cases to evaluate fairness in real-world scenarios. For example, disparate impact analysis, demographic parity, and equal opportunity are some of the metrics employed to assess fairness.\nAdditionally, transparency and interpretability of models are fundamental to achieving fairness. Understanding how a model makes decisions can reveal potential biases and enable stakeholders to hold developers accountable. Open-source tools like AI Fairness 360 by IBM and Fairness Indicators by TensorFlow are being developed to facilitate fairness assessments and mitigation of biases in machine learning models.\nEnsuring fairness in machine learning models particularly in applications that significantly impact people‚Äôs lives. It requires rigorous evaluation of the model‚Äôs performance across diverse groups, careful identification and mitigation of biases, and implementation of transparency and interpretability measures. By addressing fairness in a comprehensive manner, we can work towards developing machine learning models that are equitable, just, and beneficial for society as a whole.\n\n\nComplexity\n\nParameters*\nIn the initial stages of machine learning, model benchmarking often relied on parameter counts as a proxy for model complexity. The rationale was that more parameters typically lead to a more complex model, which should, in turn, deliver better performance. However, this approach has proven to be inadequate as it doesn‚Äôt account for the computational cost associated with processing a large number of parameters.\nFor example, GPT-3, developed by OpenAI, is a language model that boasts an astounding 175 billion parameters. While it achieves state-of-the-art performance on a variety of natural language processing tasks, its size and the computational resources required to run it make it impractical for deployment in many real-world scenarios, especially those with limited computational capabilities.\nThe reliance on parameter counts as a proxy for model complexity also fails to consider the efficiency of the model. A model with fewer parameters might be just as effective, if not more so, than a model with a higher parameter count if it is optimized for efficiency. For instance, MobileNets, developed by Google, are a family of models designed specifically for mobile and edge devices. They utilize depth-wise separable convolutions to reduce the number of parameters and computational cost, while still achieving competitive performance.\nIn light of these limitations, the field has moved towards a more holistic approach to model benchmarking that considers not just parameter counts, but also other crucial factors such as floating-point operations per second (FLOPs), memory consumption, and latency. FLOPs, in particular, have emerged as an important metric as they provide a more accurate representation of the computational load a model imposes. This shift towards a more comprehensive approach to model benchmarking reflects a recognition of the need to balance performance with practicality, ensuring that models are not just effective, but also efficient and deployable in real-world scenarios.\n\n\nFLOPS\nThe size of a machine learning model is an essential aspect that directly impacts its usability in practical scenarios, especially when computational resources are limited. Traditionally, the number of parameters in a model was often used as a proxy for its size, with the underlying assumption being that more parameters would translate to better performance. However, this simplistic view does not consider the computational cost associated with processing these parameters. This is where the concept of floating-point operations per second (FLOPs) comes into play, providing a more accurate representation of the computational load a model imposes.\nFLOPs measure the number of floating-point operations a model performs to generate a prediction. For example, a model with a high number of FLOPs requires substantial computational resources to process the vast number of operations, which may render it impractical for certain applications. Conversely, a model with a lower FLOP count is more lightweight and can be easily deployed in scenarios where computational resources are limited.\nLet‚Äôs consider an example. BERT (Bidirectional Encoder Representations from Transformers)(Devlin et al. 2018), a popular natural language processing model, has over 340 million parameters, making it a large model with high accuracy and impressive performance across a range of tasks. However, the sheer size of BERT, coupled with its high FLOP count, makes it a computationally intensive model that may not be suitable for real-time applications or deployment on edge devices with limited computational capabilities.\n\nDevlin, Jacob, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018. ‚ÄúBert: Pre-Training of Deep Bidirectional Transformers for Language Understanding.‚Äù arXiv Preprint arXiv:1810.04805.\nIn light of this, there has been a growing interest in developing smaller models that can achieve similar performance levels as their larger counterparts while being more efficient in terms of computational load. DistilBERT, for instance, is a smaller version of BERT that retains 97% of its performance while being 40% smaller in terms of parameter count. The reduction in size also translates to a lower FLOP count, making DistilBERT a more practical choice for resource-constrained scenarios.\nTo sum up, while parameter count provides a useful indication of model size, it is not a comprehensive metric as it does not consider the computational cost associated with processing these parameters. FLOPs, on the other hand, offer a more accurate representation of a model‚Äôs computational load and are thus an essential consideration when deploying machine learning models in real-world scenarios, particularly when computational resources are limited. The evolution from relying solely on parameter count to also considering FLOPs signifies a maturation in the field, reflecting a greater awareness of the practical constraints and challenges associated with deploying machine learning models in diverse settings.\n\n\nEfficiency\nEfficiency metrics, such as memory consumption and latency/throughput, have also gained prominence. These metrics are particularly crucial when deploying models on edge devices or in real-time applications, as they measure how quickly a model can process data and how much memory it requires. In this context, Pareto curves are often used to visualize the trade-off between different metrics, helping stakeholders make informed decisions about which model is best suited to their needs.\n\n\n\n\n12.5.3 Lessons Learned\nModel benchmarking has offered us several valuable insights that can be leveraged to drive innovation in system benchmarks. The progression of machine learning models has been profoundly influenced by the advent of leaderboards and the open-source availability of models and datasets. These elements have served as significant catalysts, propelling innovation and accelerating the integration of cutting-edge models into production environments. However, these are not the only contributors to the development of machine learning benchmarks, as we will explore further.\nLeaderboards play a vital role in providing an objective and transparent method for researchers and practitioners to evaluate the efficacy of different models, ranking them based on their performance in benchmarks. This system fosters a competitive environment, encouraging the development of models that are not only accurate but also efficient. The ImageNet Large Scale Visual Recognition Challenge (ILSVRC) is a prime example of this, with its annual leaderboard significantly contributing to the development of groundbreaking models such as AlexNet.\nOpen-source access to state-of-the-art models and datasets further democratizes the field of machine learning, facilitating collaboration among researchers and practitioners worldwide. This open access accelerates the process of testing, validation, and deployment of new models in production environments, as evidenced by the widespread adoption of models like BERT and GPT-3 in various applications, from natural language processing to more complex, multi-modal tasks.\nCommunity collaboration platforms like Kaggle have revolutionized the field by hosting competitions that unite data scientists from across the globe to solve intricate problems, with specific benchmarks serving as the goalposts for innovation and model development.\nMoreover, the availability of diverse and high-quality datasets is paramount in training and testing machine learning models. Datasets such as ImageNet have played an instrumental role in the evolution of image recognition models, while extensive text datasets have facilitated advancements in natural language processing models.\nLastly, the contributions of academic and research institutions cannot be overstated. Their role in publishing research papers, sharing findings at conferences, and fostering collaboration between various institutions has significantly contributed to the advancement of machine learning models and benchmarks.\n\nEmerging Trends\nAs machine learning models become more sophisticated, so do the benchmarks required to accurately assess them. There are several emerging benchmarks and datasets that are gaining popularity due to their ability to evaluate models in more complex and realistic scenarios:\nMultimodal Datasets: These datasets contain multiple types of data, such as text, images, and audio, to better represent real-world situations. An example is the VQA (Visual Question Answering) dataset(Antol et al. 2015), where models are tested on their ability to answer text-based questions about images.\n\nAntol, Stanislaw, Aishwarya Agrawal, Jiasen Lu, Margaret Mitchell, Dhruv Batra, C Lawrence Zitnick, and Devi Parikh. 2015. ‚ÄúVqa: Visual Question Answering.‚Äù In Proceedings of the IEEE International Conference on Computer Vision, 2425‚Äì33.\nFairness and Bias Evaluation: There is an increasing focus on creating benchmarks that assess the fairness and bias of machine learning models. Examples include the AI Fairness 360 toolkit, which offers a comprehensive set of metrics and datasets for evaluating bias in models.\nOut-of-Distribution Generalization: Testing how well models perform on data that is different from the original training distribution. This evaluates the model‚Äôs ability to generalize to new, unseen data. Example benchmarks are Wilds(Koh et al. 2021), RxRx, and ANC-Bench.\n\nKoh, Pang Wei, Shiori Sagawa, Henrik Marklund, Sang Michael Xie, Marvin Zhang, Akshay Balsubramani, Weihua Hu, et al. 2021. ‚ÄúWilds: A Benchmark of in-the-Wild Distribution Shifts.‚Äù In International Conference on Machine Learning, 5637‚Äì64. PMLR.\n\nHendrycks, Dan, Kevin Zhao, Steven Basart, Jacob Steinhardt, and Dawn Song. 2021. ‚ÄúNatural Adversarial Examples.‚Äù In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 15262‚Äì71.\n\nXie, Cihang, Mingxing Tan, Boqing Gong, Jiang Wang, Alan L Yuille, and Quoc V Le. 2020. ‚ÄúAdversarial Examples Improve Image Recognition.‚Äù In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 819‚Äì28.\nAdversarial Robustness: Evaluating model performance under adversarial attacks or perturbations to the input data. This tests the model‚Äôs robustness. Example benchmarks are ImageNet-A(Hendrycks et al. 2021), ImageNet-C(Xie et al. 2020), and CIFAR-10.1.\nReal-World Performance: Testing models on real-world datasets that closely match end tasks, rather than just canned benchmark datasets. Examples are medical imaging datasets for healthcare tasks or actual customer support chat logs for dialogue systems.\nEnergy and Compute Efficiency: Benchmarks that measure the computational resources required to achieve a particular accuracy. This evaluates the model‚Äôs efficiency. Examples are MLPerf and Greenbench, and these were already discussed in the Systems benchmarking section.\nInterpretability and Explainability: Benchmarks that assess how easy it is to understand and explain a model‚Äôs internal logic and predictions. Example metrics are faithfulness to input gradients and coherence of explanations.\n\n\n\n12.5.4 Limitations and Challenges\nWhile model benchmarks are an essential tool in the assessment of machine learning models, there are several limitations and challenges that should be addressed to ensure that they accurately reflect a model‚Äôs performance in real-world scenarios.\nDataset does not Correspond to Real-World Scenarios: Often, the data used in model benchmarks is cleaned and preprocessed to such an extent that it may not accurately represent the data that a model would encounter in real-world applications. This idealized version of the data can lead to overestimations of a model‚Äôs performance. In the case of the ImageNet dataset, the images are well-labeled and categorized, but in a real-world scenario, a model may need to deal with images that are blurry, poorly lit, or taken from awkward angles. This discrepancy can significantly affect the model‚Äôs performance.\nSim2Real Gap: The Sim2Real gap refers to the difference in performance of a model when transitioning from a simulated environment to a real-world environment. This gap is often observed in robotics, where a robot trained in a simulated environment struggles to perform tasks in the real world due to the complexity and unpredictability of real-world environments. A robot trained to pick up objects in a simulated environment may struggle to perform the same task in the real world because the simulated environment does not accurately represent the complexities of real-world physics, lighting, and object variability.\nChallenges in Creating Datasets: Creating a dataset for model benchmarking is a challenging task that requires careful consideration of various factors such as data quality, diversity, and representation. As discussed in the data engineering section, ensuring that the data is clean, unbiased, and representative of the real-world scenario is crucial for the accuracy and reliability of the benchmark. For example, when creating a dataset for a healthcare-related task, it is important to ensure that the data is representative of the entire population and not biased towards a particular demographic. This ensures that the model performs well across diverse patient populations.\nModel benchmarks are essential in measuring the capability of a model architecture in solving a fixed task, but it is important to address the limitations and challenges associated with them. This includes ensuring that the dataset accurately represents real-world scenarios, addressing the Sim2Real gap, and overcoming the challenges associated with creating unbiased and representative datasets. By addressing these challenges, and many others, we can ensure that model benchmarks provide a more accurate and reliable assessment of a model‚Äôs performance in real-world applications.\nThe Speech Commands dataset, and its successor MSWC, are common benchmarks for one of the quintessential TinyML applications, keyword spotting. Speech Commands establish streaming error metrics beyond the standard top-1 classification accuracy that are more relevant to the keyword spotting use case. Use case relevant metrics are what elevates a dataset to a model benchmark."
  },
  {
    "objectID": "benchmarking.html#data-benchmarking",
    "href": "benchmarking.html#data-benchmarking",
    "title": "12¬† Benchmarking AI",
    "section": "12.6 Data Benchmarking",
    "text": "12.6 Data Benchmarking\nFor the past several years, the field of AI has been focused on developing increasingly sophisticated machine learning models like large language models. The goal has been to create models capable of human-level or superhuman performance on a wide range of tasks by training them on massive datasets. This model-centric approach produced rapid progress, with models attaining state-of-the-art results on many established benchmarks.\nHowever, there are growing concerns about issues like bias, safety, and robustness that persist even in models that achieve high accuracy on standard benchmarks. Additionally, some popular datasets used for evaluating models are beginning to saturate, with models reaching near perfect performance on existing test splits (Kiela et al. 2021). As a simple example, there are test images in the classic MNIST handwritten digit dataset which may look indecipherable to most human evaluators, but nonetheless were assigned a label when the dataset was created - models which happen to agree with those labels may appear to exhibit superhuman performance but instead may only be capturing idiosyncrasies of the labeling and acquisition process from the dataset‚Äôs creation in 1994. In the same spirit, computer vision researchers now ask ‚ÄúAre we done with ImageNet?‚Äù (Beyer et al. 2020). This highlights limitations in the conventional model-centric approach of optimizing accuracy on fixed datasets through architectural innovations.\n\nKiela, Douwe, Max Bartolo, Yixin Nie, Divyansh Kaushik, Atticus Geiger, Zhengxuan Wu, Bertie Vidgen, et al. 2021. ‚ÄúDynabench: Rethinking Benchmarking in NLP.‚Äù arXiv Preprint arXiv:2104.14337.\n\nBeyer, Lucas, Olivier J H√©naff, Alexander Kolesnikov, Xiaohua Zhai, and A√§ron van den Oord. 2020. ‚ÄúAre We Done with Imagenet?‚Äù arXiv Preprint arXiv:2006.07159.\nAn alternative paradigm is emerging called data-centric AI. Rather than treating data as static and focusing narrowly on model performance, this approach recognizes that models are only as good as their training data. So the emphasis shifts to curating high-quality datasets that better reflect real-world complexity, developing more informative evaluation benchmarks, and carefully considering how data is sampled, preprocessed, and augmented. The goal is to optimize model behavior by improving the data, rather than just optimizing metrics on flawed datasets. Data-centric AI critically examines and enhances the data itself to produce beneficial AI. This reflects an important evolution in mindset as the field addresses the shortcomings of narrow benchmarking.\nIn this section, we will explore the key differences between model-centric and data-centric approaches to AI. This distinction has important implications for how we benchmark AI systems. Specifically, we will see how a focus on data quality and efficiency can directly improve machine learning performance, as an alternative to solely optimizing model architectures. The data-centric approach recognizes that models are only as good as their training data. So enhancing data curation, evaluation benchmarks, and data handling processes can produce AI systems that are safer, fairer, and more robust. Rethinking benchmarking to prioritize data alongside models represents an important evolution as the field aims to deliver trustworthy real-world impact.\n\n\n12.6.1 Limitations of Model-Centric AI\nIn the model-centric AI era, a prominent characteristic was the development of complex model architectures. Researchers and practitioners dedicated substantial effort to devise sophisticated and intricate models in the quest for superior performance. This frequently involved the incorporation of additional layers and the fine-tuning of a multitude of hyperparameters to achieve incremental improvements in accuracy. Concurrently, there was a significant emphasis on leveraging advanced algorithms. These algorithms, often at the forefront of the latest research, were employed to enhance the performance of AI models. The primary aim of these algorithms was to optimize the learning process of models, thereby extracting maximal information from the training data.\nWhile the model-centric approach has been central to many advancements in AI, it has several shortcomings. First, the development of complex model architectures can often lead to overfitting. This is where the model performs well on the training data but fails to generalize to new, unseen data. The additional layers and complexity can capture noise in the training data as if it were a real pattern, which harms the model‚Äôs performance on new data.\nSecond, the reliance on advanced algorithms can sometimes obscure the real understanding of a model‚Äôs functioning. These algorithms often act as a black box, making it difficult to interpret how the model is making decisions. This lack of transparency can be a significant hurdle, especially in critical applications such as healthcare and finance, where understanding the model‚Äôs decision-making process is crucial.\nThird, the emphasis on achieving state-of-the-art results on benchmark datasets can sometimes be misleading. These datasets are often not fully representative of the complexities and variability found in real-world data. A model that performs well on a benchmark dataset may not necessarily generalize well to new, unseen data in a real-world application. This discrepancy can lead to a false sense of confidence in the model‚Äôs capabilities and hinder its practical applicability.\nLastly, the model-centric approach often relies on large labeled datasets for training. However, in many real-world scenarios, obtaining such datasets is difficult and costly. This reliance on large datasets also limits the applicability of AI in domains where data is scarce or expensive to label.\nAs a result of the above reasons, and many more, the AI community is shifting to a more data-centric approach. Rather than focusing just on model architecture, researchers are now prioritizing curating high-quality datasets, developing better evaluation benchmarks, and considering how data is sampled and preprocessed. The key idea is that models are only as good as their training data. So focusing on getting the right data will allow us to develop AI systems that are more fair, safe, and aligned with human values. This data-centric shift represents an important change in mindset as AI progresses.\n\n\n12.6.2 The Shift Toward Data-centric AI\nData-centric AI is a paradigm that emphasizes the importance of high-quality, well-labeled, and diverse datasets in the development of AI models. In contrast to the model-centric approach, which focuses on refining and iterating on the model architecture and algorithm to improve performance, data-centric AI prioritizes the quality of the input data as the primary driver of improved model performance. High-quality data is clean, well-labeled, and representative of the real-world scenarios the model will encounter. In contrast, low-quality data can lead to poor model performance, regardless of the complexity or sophistication of the model architecture.\nData-centric AI puts a strong emphasis on the cleaning and labeling of data. Cleaning involves the removal of outliers, handling missing values, and addressing other data inconsistencies. Labeling, on the other hand, involves assigning meaningful and accurate labels to the data. Both these processes are crucial in ensuring that the AI model is trained on accurate and relevant data. Another important aspect of the data-centric approach is data augmentation. This involves artificially increasing the size and diversity of the dataset by applying various transformations to the data, such as rotation, scaling, and flipping training images. Data augmentation helps in improving the model‚Äôs robustness and generalization capabilities.\nThere are several benefits to adopting a data-centric approach to AI development. First and foremost, it leads to improved model performance and generalization capabilities. By ensuring that the model is trained on high-quality, diverse data, the model is better able to generalize to new, unseen data (Gaviria Rojas et al. 2022).\nAdditionally, a data-centric approach can often lead to simpler models that are easier to interpret and maintain. This is because the emphasis is on the data, rather than the model architecture, meaning that simpler models can achieve high performance when trained on high-quality data.\nThe shift towards data-centric AI represents a significant paradigm shift. By prioritizing the quality of the input data, this approach aims to improve model performance and generalization capabilities, ultimately leading to more robust and reliable AI systems. As we continue to advance in our understanding and application of AI, the data-centric approach is likely to play a pivotal role in shaping the future of this field.\n\n\n12.6.3 Benchmarking Data\nData benchmarking aims to evaluate common issues in datasets, such as identifying label errors, noisy features, representation imbalance (for example, out of the 1000 classes in Imagenet-1K, there are over 100 categories which are just types of dogs), class imbalance (where some classes have many more samples than others), whether models trained on a given dataset can generalize to out-of-distribution features, or what types of biases might exist in a given dataset(Gaviria Rojas et al. 2022). In its simplest form, data benchmarking aims to improve accuracy on a test set by removing noisy or mislabeled training samples while keeping the model architecture fixed. Recent competitions in data benchmarking have invited participants to submit novel augmentation strategies and active learning techniques.\n\nGaviria Rojas, William, Sudnya Diamos, Keertan Kini, David Kanter, Vijay Janapa Reddi, and Cody Coleman. 2022. ‚ÄúThe Dollar Street Dataset: Images Representing the Geographic and Socioeconomic Diversity of the World.‚Äù Advances in Neural Information Processing Systems 35: 12979‚Äì90.\nData-centric techniques continue to gain attention in benchmarking, especially as foundation models are increasingly trained on self-supervised objectives. Compared to smaller datasets like Imagenet-1K, massive datasets commonly used in self-supervised learning such as Common Crawl, OpenImages, and LAION-5B contain an order of magnitude higher amounts of noise, duplicates, bias, and potentially offensive data.\nDataComp is a recently-launched dataset competition which targets evaluation of large corpora. DataComp focuses on language-image pairs used to train CLIP models. The introductory whitepaper finds that, when the total compute budget for training is held constant, the best-performing CLIP models on downstream tasks such as ImageNet classification are trained on just 30% of the available training sample pool. This suggests that proper filtering of large corpora is critical to improving the accuracy of foundation models. Similarly, Demystifying CLIP Data (Xu et al. 2023) asks whether the success of CLIP is attributable to the architecture or the dataset.\n\nXu, Hu, Saining Xie, Xiaoqing Ellen Tan, Po-Yao Huang, Russell Howes, Vasu Sharma, Shang-Wen Li, Gargi Ghosh, Luke Zettlemoyer, and Christoph Feichtenhofer. 2023. ‚ÄúDemystifying CLIP Data.‚Äù arXiv Preprint arXiv:2309.16671.\nDataPerf is another recent effort which focuses on benchmarking data in a wide range of modalities. DataPerf provides rounds of online competition to spur improvement in datasets. The inaugural offering launched with challenges in vision, speech, acquisition, debugging, and text prompting for image generation.\n\n\n12.6.4 Data Efficiency\nAs machine learning models grow larger and more complex and compute resources more scarce in the face of rising demand, it becomes challenging to meet the requirements for computation even with the largest machine learning fleets. To overcome these challenges and ensure machine learning system scalability, it is necessary to explore novel opportunities that augment conventional approaches to resource scaling.\nImproving data quality can be a useful method to significantly impact machine learning system performance. One of the primary benefits of enhancing data quality is the potential to reduce the size of the training dataset while still maintaining, or even improving, model performance. This reduction in data size has a direct relationship to the amount of training time required, thereby allowing models to converge more quickly and efficiently. But achieving this balance between data quality and dataset size is a challenging task that requires the development of sophisticated methods, algorithms, and techniques.\nThere are several approaches that can be taken to improve data quality. These methods include and are not limited to the following:\n\nData Cleaning: This involves handling missing values, correcting errors, and removing outliers. Clean data ensures that the model is not learning from noise or inaccuracies.\nData Interpretability and Explainability: Common techniques include LIME (Ribeiro, Singh, and Guestrin 2016) which provides insight into the decision boundaries of classifiers, and Shapley values (Lundberg and Lee 2017) which estimate the importance of individual samples in contributing to a model‚Äôs predictions.\nFeature Engineering: Transforming or creating new features can significantly improve model performance by providing more relevant information for learning.\nData Augmentation: Augmenting data by creating new samples through various transformations can help improve model robustness and generalization.\nActive Learning: This is a semi-supervised learning approach where the model actively queries a human oracle to label the most informative samples (Coleman et al. 2022). This ensures that the model is trained on the most relevant data.\nDimensionality Reduction: Techniques like PCA can be used to reduce the number of features in a dataset, thereby reducing complexity and training time.\n\n\nRibeiro, Marco Tulio, Sameer Singh, and Carlos Guestrin. 2016. ‚Äú\" Why Should i Trust You?\" Explaining the Predictions of Any Classifier.‚Äù In Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, 1135‚Äì44.\n\nLundberg, Scott M, and Su-In Lee. 2017. ‚ÄúA Unified Approach to Interpreting Model Predictions.‚Äù Advances in Neural Information Processing Systems 30.\n\nColeman, Cody, Edward Chou, Julian Katz-Samuels, Sean Culatana, Peter Bailis, Alexander C Berg, Robert Nowak, Roshan Sumbaly, Matei Zaharia, and I Zeki Yalniz. 2022. ‚ÄúSimilarity Search for Efficient Active Learning and Search of Rare Concepts.‚Äù In Proceedings of the AAAI Conference on Artificial Intelligence, 36:6402‚Äì10. 6.\nThere are many other methods in the wild. But the goal is the same. By refining the dataset and ensuring it is of the highest quality, we can directly reduce the training time required for models to converge. However, achieving this requires the development and implementation of sophisticated methods, algorithms, and techniques that can clean, preprocess, and augment data while retaining the most informative samples. This is an ongoing challenge that will require continued research and innovation in the field of machine learning."
  },
  {
    "objectID": "benchmarking.html#the-trifecta",
    "href": "benchmarking.html#the-trifecta",
    "title": "12¬† Benchmarking AI",
    "section": "12.7 The Trifecta",
    "text": "12.7 The Trifecta\nWhile system, model, and data benchmarks have traditionally been studied in isolation, there is a growing recognition that to fully understand and advance AI we must take a more holistic view. By iterating between benchmarking systems, models, and datasets together, novel insights may emerge that are not apparent when these components are analyzed separately. System performance impacts model accuracy, model capabilities drive data needs, and data characteristics shape system requirements.\nBenchmarking the triad of system, model, and data in an integrated fashion will likely lead to new discoveries about the co-design of AI systems, the generalization properties of models, and the role of data curation and quality in enabling performance. Rather than narrow benchmarks of individual components, the future of AI requires benchmarks that evaluate the symbiotic relationship between computing platforms, algorithms, and training data. This systems-level perspective will be critical to overcoming current limitations and unlocking the next level of AI capabilities.\n\nThe figure illustrates the many potential ways to interlace and interplay data benchmarking, model benchmarking, and system infrastructure benchmarking together. Through exploring these intricate interactions, we are likely to uncover new optimization opportunities and capabilities for enhancement. The triad of data, model, and system benchmarks offers a rich space for co-design and co-optimization.\nWhile this integrated perspective represents an emerging trend, the field has much more to discover about the synergies and trade-offs between these different components. As we iteratively benchmark combinations of data, models, and systems, entirely new insights will emerge that remain hidden when these elements are studied in isolation. This multi-faceted benchmarking approach charting the intersections of data, algorithms, and hardware promises to be a fruitful avenue for major progress in AI, even though it is still in its early stages."
  },
  {
    "objectID": "benchmarking.html#conclusion",
    "href": "benchmarking.html#conclusion",
    "title": "12¬† Benchmarking AI",
    "section": "12.8 Conclusion",
    "text": "12.8 Conclusion\nWhat gets measured gets improved. This chapter has explored the multifaceted nature of benchmarking spanning systems, models, and data. Benchmarking is important to advancing AI by providing the essential measurements to track progress.\nML system benchmarks enable optimization across metrics like speed, efficiency, and scalability. Model benchmarks drive innovation through standardized tasks and metrics beyond just accuracy. And data benchmarks highlight issues of quality, balance and representation.\nImportantly, evaluating these components in isolation has limitations. The future will likely see more integrated benchmarking that explores the interplay between system benchmarks, model benchmarks and data benchmarks. This view promises new insights into the co-design of data, algorithms and infrastructure.\nAs AI grows more complex, comprehensive benchmarking becomes even more critical. Standards must continuously evolve to measure new capabilities and reveal limitations. Close collaboration between industry, academics and national labls etc. is essential to develop benchmarks that are rigorous, transparent and socially beneficial.\nBenchmarking provides the compass to guide progress in AI. By persistently measuring and openly sharing results, we can navigate towards systems that are performant, robust and trustworthy. If AI is to properly serve societail and human needs, it must be benchmarked with humanity‚Äôs best interests in mind. To this end, there are emerging areas such as benchmarking the safety of AI systems but that‚Äôs for another day and perhaps something we can discuss further in Generative AI!\nBenchmarking is a continuously evolving topic. The article The Olympics of AI: Benchmarking Machine Learning Systems covers several emerging subfields in AI benchmarking, including robotics, extended reality, and neuromorphic computing that we encourage the reader to pursue."
  },
  {
    "objectID": "ondevice_learning.html#introduction",
    "href": "ondevice_learning.html#introduction",
    "title": "13¬† On-Device Learning",
    "section": "13.1 Introduction",
    "text": "13.1 Introduction\nExplanation: This section sets the stage for the reader, explaining why on-device learning is a critical aspect of embedded AI systems.\n\nImportance in Embedded AI\nWhy is On-device Learning Needed"
  },
  {
    "objectID": "ondevice_learning.html#advantages-and-limitations",
    "href": "ondevice_learning.html#advantages-and-limitations",
    "title": "13¬† On-Device Learning",
    "section": "13.2 Advantages and Limitations",
    "text": "13.2 Advantages and Limitations\nExplanation: Understanding the pros and cons of on-device learning helps to identify the scenarios where it is most effective and the challenges that need to be addressed.\n\nBenefits\nConstraints"
  },
  {
    "objectID": "ondevice_learning.html#continuous-learning",
    "href": "ondevice_learning.html#continuous-learning",
    "title": "13¬† On-Device Learning",
    "section": "13.3 Continuous Learning",
    "text": "13.3 Continuous Learning\nExplanation: Continuous learning is essential for embedded systems to adapt to new data and situations without requiring frequent updates from a central server.\n\nIncremental Algorithms\nAdaptability"
  },
  {
    "objectID": "ondevice_learning.html#federated-machine-learning",
    "href": "ondevice_learning.html#federated-machine-learning",
    "title": "13¬† On-Device Learning",
    "section": "13.4 Federated Machine Learning",
    "text": "13.4 Federated Machine Learning\nExplanation: Federated learning allows multiple devices to collaborate in model training without sharing raw data, which is highly relevant for embedded systems concerned with data privacy.\n\nArchitecture\nOptimization"
  },
  {
    "objectID": "ondevice_learning.html#transfer-learning",
    "href": "ondevice_learning.html#transfer-learning",
    "title": "13¬† On-Device Learning",
    "section": "13.5 Transfer Learning",
    "text": "13.5 Transfer Learning\nExplanation: Transfer learning enables a pre-trained model to adapt to new tasks with less data, which is beneficial for embedded systems where data might be scarce.\n\nUse Cases\nBenefits"
  },
  {
    "objectID": "ondevice_learning.html#data-augmentation",
    "href": "ondevice_learning.html#data-augmentation",
    "title": "13¬† On-Device Learning",
    "section": "13.6 Data Augmentation",
    "text": "13.6 Data Augmentation\nExplanation: Data augmentation can enrich the training set, improving model performance, which is particularly useful when data is limited in embedded systems.\n\nTechniques\nRole in On-Device Learning"
  },
  {
    "objectID": "ondevice_learning.html#security-concerns",
    "href": "ondevice_learning.html#security-concerns",
    "title": "13¬† On-Device Learning",
    "section": "13.7 Security Concerns",
    "text": "13.7 Security Concerns\nExplanation: Security is a significant concern for any system that performs learning on-device, as it may expose vulnerabilities.\n\nRisks\nMitigation"
  },
  {
    "objectID": "ondevice_learning.html#conclusion",
    "href": "ondevice_learning.html#conclusion",
    "title": "13¬† On-Device Learning",
    "section": "13.8 Conclusion",
    "text": "13.8 Conclusion\n\nKey Takeaways"
  },
  {
    "objectID": "ops.html#introduction",
    "href": "ops.html#introduction",
    "title": "14¬† Embedded AIOps",
    "section": "14.1 Introduction",
    "text": "14.1 Introduction\nExplanation: This subsection sets the groundwork for the discussions to follow, elucidating the fundamental concept of MLOps and its critical role in enhancing the efficiency, reliability, and scalability of embedded AI systems. It outlines the unique characteristics of implementing MLOps in an embedded context, emphasizing its significance in the streamlined deployment and management of machine learning models.\n\nOverview of MLOps\nThe importance of MLOps in the embedded domain\nUnique challenges and opportunities in embedded MLOps"
  },
  {
    "objectID": "ops.html#deployment-environments",
    "href": "ops.html#deployment-environments",
    "title": "14¬† Embedded AIOps",
    "section": "14.2 Deployment Environments",
    "text": "14.2 Deployment Environments\nExplanation: This section focuses on different environments where embedded AI systems can be deployed. It will delve into aspects like edge devices, cloud platforms, and hybrid environments, offering insights into the unique characteristics and considerations of each.\n\nCloud-based deployment: Features and benefits\nEdge computing: Characteristics and applications\nHybrid environments: Combining the best of edge and cloud computing\nConsiderations for selecting an appropriate deployment environment"
  },
  {
    "objectID": "ops.html#deployment-strategies",
    "href": "ops.html#deployment-strategies",
    "title": "14¬† Embedded AIOps",
    "section": "14.3 Deployment Strategies",
    "text": "14.3 Deployment Strategies\nExplanation: Here, readers will be introduced to various deployment strategies that facilitate a smooth transition from development to production. It discusses approaches such as blue-green deployments, canary releases, and rolling deployments, which can help in maintaining system stability and minimizing downtime during updates.\n\nOverview of different deployment strategies\nBlue-green deployments: Definition and benefits\nCanary releases: Phased rollouts and monitoring\nRolling deployments: Ensuring continuous service availability\nStrategy selection: Factors to consider"
  },
  {
    "objectID": "ops.html#workflow-automation",
    "href": "ops.html#workflow-automation",
    "title": "14¬† Embedded AIOps",
    "section": "14.4 Workflow Automation",
    "text": "14.4 Workflow Automation\nExplanation: Automation is at the heart of MLOps, helping to streamline workflows and enhance efficiency. This subsection highlights the significance of workflow automation in embedded MLOps, discussing various strategies and techniques for automating tasks such as testing, deployment, and monitoring, fostering a faster and error-free development lifecycle.\n\nAutomated testing: unit tests, integration tests\nAutomated deployment: scripting, configuration management\nContinuous monitoring: setting up automated alerts and dashboards\nBenefits of workflow automation: speed, reliability, repeatability"
  },
  {
    "objectID": "ops.html#model-versioning",
    "href": "ops.html#model-versioning",
    "title": "14¬† Embedded AIOps",
    "section": "14.5 Model Versioning",
    "text": "14.5 Model Versioning\nExplanation: Model versioning is a pivotal aspect of MLOps, facilitating the tracking and management of different versions of machine learning models throughout their lifecycle. This subsection emphasizes the importance of model versioning in embedded systems, where memory and computational resources are limited, offering strategies for effective version management and rollback.\n\nImportance of versioning in machine learning pipelines\nTools for model versioning: DVC, MLflow\nStrategies for version control: naming conventions, metadata tagging\nRollback strategies: handling model regressions and rollbacks"
  },
  {
    "objectID": "ops.html#model-monitoring-and-maintenance",
    "href": "ops.html#model-monitoring-and-maintenance",
    "title": "14¬† Embedded AIOps",
    "section": "14.6 Model Monitoring and Maintenance",
    "text": "14.6 Model Monitoring and Maintenance\nExplanation: The process of monitoring and maintaining deployed models is crucial to ensure their long-term performance and reliability. This subsection underscores the significance of proactive monitoring and maintenance in embedded systems, discussing methodologies for monitoring model health, performance metrics, and implementing routine maintenance tasks to ensure optimal functionality.\n\nThe importance of monitoring deployed AI models\nSetting up monitoring systems: tools and techniques\nTracking model performance: accuracy, latency, resource usage\nMaintenance strategies: periodic updates, fine-tuning\nAlerts and notifications: Setting up mechanisms for timely responses to issues\nOver the air updates\nResponding to anomalies: troubleshooting and resolution strategies"
  },
  {
    "objectID": "ops.html#security-and-compliance",
    "href": "ops.html#security-and-compliance",
    "title": "14¬† Embedded AIOps",
    "section": "14.7 Security and Compliance",
    "text": "14.7 Security and Compliance\nExplanation: Security and compliance are paramount in MLOps, safeguarding sensitive data and ensuring adherence to regulatory requirements. This subsection illuminates the critical role of implementing security measures and ensuring compliance in embedded MLOps, offering insights into best practices for data protection, access control, and regulatory adherence.\n\nSecurity considerations in embedded MLOps: data encryption, secure communications\nCompliance requirements: GDPR, HIPAA, and other regulations\nStrategies for ensuring compliance: documentation, audits, training\nTools for security and compliance management: SIEM systems, compliance management platforms"
  },
  {
    "objectID": "ops.html#conclusion",
    "href": "ops.html#conclusion",
    "title": "14¬† Embedded AIOps",
    "section": "14.8 Conclusion",
    "text": "14.8 Conclusion\nExplanation: As we wrap up this chapter, we consolidate the key takeaways regarding the implementation of MLOps in the embedded domain. This final section seeks to furnish readers with a holistic view of the principles and practices of embedded MLOps, encouraging a thoughtful approach to adopting MLOps strategies in their projects, with a glimpse into the potential future trends in this dynamic field.\n\nRecap of key concepts and best practices in embedded MLOps\nChallenges and opportunities in implementing MLOps in embedded systems\nFuture directions: emerging trends and technologies in embedded MLOps"
  },
  {
    "objectID": "privacy_security.html#introduction",
    "href": "privacy_security.html#introduction",
    "title": "15¬† Privacy and Security",
    "section": "15.1 Introduction",
    "text": "15.1 Introduction\nExplanation: In this section, we will set the stage for the readers by introducing the critical role of privacy and security in embedded AI systems. Understanding the foundational concepts is essential to appreciate the various nuances and strategies that will be discussed in the subsequent sections.\n\nImportance of privacy and security in AI\nOverview of privacy and security challenges in embedded AI\nSignificance of user trust and data protection"
  },
  {
    "objectID": "privacy_security.html#data-privacy-in-ai-systems",
    "href": "privacy_security.html#data-privacy-in-ai-systems",
    "title": "15¬† Privacy and Security",
    "section": "15.2 Data Privacy in AI Systems",
    "text": "15.2 Data Privacy in AI Systems\nExplanation: This section is of utmost importance as it delves into the various ways to protect sensitive data during collection, storage, and processing. Given that AI systems often handle a large amount of personal data, implementing data privacy measures is critical to prevent unauthorized access and misuse.\n\nData anonymization techniques\nPrinciples of data minimization\nLegal frameworks governing data privacy"
  },
  {
    "objectID": "privacy_security.html#encryption-techniques",
    "href": "privacy_security.html#encryption-techniques",
    "title": "15¬† Privacy and Security",
    "section": "15.3 Encryption Techniques",
    "text": "15.3 Encryption Techniques\nExplanation: Encryption techniques are pivotal in safeguarding data at rest and during transmission. In this section, we will explore various encryption methodologies and how they can be used effectively in embedded AI systems to ensure data confidentiality and security.\n\nSymmetric and asymmetric encryption\nEnd-to-end encryption\nEncryption protocols and standards"
  },
  {
    "objectID": "privacy_security.html#secure-multi-party-computation",
    "href": "privacy_security.html#secure-multi-party-computation",
    "title": "15¬† Privacy and Security",
    "section": "15.4 Secure Multi-Party Computation",
    "text": "15.4 Secure Multi-Party Computation\nExplanation: Secure Multi-Party Computation (SMPC) is a cryptographic protocol that allows for the secure sharing of data between multiple parties. This section is vital as it discusses how SMPC can be used to perform computations on encrypted data without revealing the underlying information, which is a significant stride in preserving privacy in AI systems.\n\nBasics of SMPC\nUse cases for SMPC in AI\nChallenges and solutions in implementing SMPC"
  },
  {
    "objectID": "privacy_security.html#privacy-preserving-machine-learning",
    "href": "privacy_security.html#privacy-preserving-machine-learning",
    "title": "15¬† Privacy and Security",
    "section": "15.5 Privacy-Preserving Machine Learning",
    "text": "15.5 Privacy-Preserving Machine Learning\nExplanation: This section explores the innovative approaches to developing machine learning models that can operate on encrypted data or provide results without revealing sensitive information. Understanding these concepts is fundamental in designing AI systems that respect user privacy and prevent data exploitation.\n\nDifferential privacy\nHomomorphic encryption\nFederated learning"
  },
  {
    "objectID": "privacy_security.html#authentication-and-authorization",
    "href": "privacy_security.html#authentication-and-authorization",
    "title": "15¬† Privacy and Security",
    "section": "15.6 Authentication and Authorization",
    "text": "15.6 Authentication and Authorization\nExplanation: Authentication and authorization mechanisms are essential to control access to sensitive resources within an AI system. This section will highlight various strategies to securely manage and restrict access to various components in an embedded AI environment, ensuring that only authorized entities can interact with the system.\n\nRole-based access control\nMulti-factor authentication\nSecure tokens and API keys"
  },
  {
    "objectID": "privacy_security.html#secure-hardware-enclaves",
    "href": "privacy_security.html#secure-hardware-enclaves",
    "title": "15¬† Privacy and Security",
    "section": "15.7 Secure Hardware Enclaves",
    "text": "15.7 Secure Hardware Enclaves\nExplanation: This section will dissect how secure hardware enclaves can provide a protected execution environment for critical operations in an embedded AI system. Understanding the role and implementation of hardware enclaves is crucial for building AI systems resistant to both physical and software attacks.\n\nConcepts of hardware enclaves\nHardware security modules (HSMs)\nTrusted execution environments (TEEs)"
  },
  {
    "objectID": "privacy_security.html#security-audits-and-compliance",
    "href": "privacy_security.html#security-audits-and-compliance",
    "title": "15¬† Privacy and Security",
    "section": "15.8 Security Audits and Compliance",
    "text": "15.8 Security Audits and Compliance\nExplanation: Security audits and compliance are vital components to ensure the continual adherence to privacy and security standards. This section is crucial as it discusses the various methods of conducting security audits and the importance of maintaining compliance with established regulatory frameworks.\n\nSecurity audit methodologies\nRegulatory compliance standards\nRisk assessment and management"
  },
  {
    "objectID": "privacy_security.html#conclusion",
    "href": "privacy_security.html#conclusion",
    "title": "15¬† Privacy and Security",
    "section": "15.9 Conclusion",
    "text": "15.9 Conclusion\nExplanation: This final section will encapsulate the key takeaways from the chapter, providing readers with a consolidated view of the critical aspects of privacy and security in embedded AI systems. It aims to reinforce the importance of implementing robust security measures to protect data and preserve user trust.\n\nRecap of privacy and security principles\nImportance of an integrated approach to privacy and security\nFuture directions and areas for further study s"
  },
  {
    "objectID": "responsible_ai.html#introduction",
    "href": "responsible_ai.html#introduction",
    "title": "16¬† Responsible AI",
    "section": "16.1 Introduction",
    "text": "16.1 Introduction\nExplanation: In this introduction, we lay the groundwork by explicating the pivotal role of responsibility in AI, focusing on the integration of ethical considerations and accountability in the development and deployment of embedded AI systems.\n\nDefining responsible AI in the context of embedded systems\nImportance of ethical considerations in AI\nThe alignment of responsibility and sustainability in embedded AI\nOverview of challenges and opportunities in responsible AI development"
  },
  {
    "objectID": "responsible_ai.html#ethical-considerations-in-ai-design",
    "href": "responsible_ai.html#ethical-considerations-in-ai-design",
    "title": "16¬† Responsible AI",
    "section": "16.2 Ethical Considerations in AI Design",
    "text": "16.2 Ethical Considerations in AI Design\nExplanation: This section probes the ethical dimensions to consider during the design phase of embedded AI systems, emphasizing responsible data handling, inclusive design practices, and avoidance of bias.\n\nEthical data acquisition and handling in embedded AI\nInclusive design and diversity in AI model development\nAddressing bias in embedded AI design\nCase studies: Implementations of ethical considerations in AI design"
  },
  {
    "objectID": "responsible_ai.html#transparency-and-explainability",
    "href": "responsible_ai.html#transparency-and-explainability",
    "title": "16¬† Responsible AI",
    "section": "16.3 Transparency and Explainability",
    "text": "16.3 Transparency and Explainability\nExplanation: In this portion, we delve into the critical components of transparency and explainability in embedded AI, discussing how these aspects facilitate trust and reliability in AI applications.\n\nImportance of transparency in embedded AI systems (maybe use ML sensors)\nTechniques to enhance explainability in embedded AI\nTools and frameworks for improving transparency\nCase studies: Transparent and explainable embedded AI implementations"
  },
  {
    "objectID": "responsible_ai.html#privacy-and-data-security",
    "href": "responsible_ai.html#privacy-and-data-security",
    "title": "16¬† Responsible AI",
    "section": "16.4 Privacy and Data Security",
    "text": "16.4 Privacy and Data Security\nExplanation: Here, we focus on the paramount importance of ensuring data privacy and security in embedded AI systems, delineating techniques and best practices to safeguard sensitive information.\n\nPrivacy-preserving techniques in embedded AI\nEnsuring data security in embedded AI systems\nRegulatory landscapes governing privacy and data security in AI"
  },
  {
    "objectID": "responsible_ai.html#accountability-and-oversight",
    "href": "responsible_ai.html#accountability-and-oversight",
    "title": "16¬† Responsible AI",
    "section": "16.5 Accountability and Oversight",
    "text": "16.5 Accountability and Oversight\nExplanation: This section underscores the necessity of incorporating mechanisms for accountability and oversight in embedded AI systems, ensuring that these systems are developed and deployed responsibly.\n\nImplementing accountability measures in embedded AI development\nOversight mechanisms for monitoring embedded AI systems\nBuilding accountable AI through community and stakeholder engagement\nCase studies: Implementations of accountability and oversight in AI"
  },
  {
    "objectID": "responsible_ai.html#social-and-cultural-impacts",
    "href": "responsible_ai.html#social-and-cultural-impacts",
    "title": "16¬† Responsible AI",
    "section": "16.6 Social and Cultural Impacts",
    "text": "16.6 Social and Cultural Impacts\nExplanation: Here, we explore the broader social and cultural impacts of embedded AI systems, analyzing both the positive influences and potential pitfalls, and discussing strategies to mitigate adverse effects.\n\nAssessing the social implications of embedded AI systems\nCultural considerations in embedded AI deployment\nStrategies for mitigating adverse social and cultural impacts\nCase studies: Socially and culturally responsible AI implementations"
  },
  {
    "objectID": "responsible_ai.html#inclusive-and-accessible-ai",
    "href": "responsible_ai.html#inclusive-and-accessible-ai",
    "title": "16¬† Responsible AI",
    "section": "16.7 Inclusive and Accessible AI",
    "text": "16.7 Inclusive and Accessible AI\nExplanation: This segment explores the principles of inclusivity and accessibility in embedded AI, offering guidance on building systems that are accessible to a diverse user base and cater to different needs and abilities.\n\nDesigning inclusive AI: Guidelines and best practices\nEnsuring accessibility in embedded AI applications\nTools and frameworks for developing inclusive AI"
  },
  {
    "objectID": "responsible_ai.html#policy-frameworks-and-global-initiatives",
    "href": "responsible_ai.html#policy-frameworks-and-global-initiatives",
    "title": "16¬† Responsible AI",
    "section": "16.8 Policy Frameworks and Global Initiatives",
    "text": "16.8 Policy Frameworks and Global Initiatives\nExplanation: In this section, we highlight the evolving policy frameworks and global initiatives that govern responsible AI, discussing their implications for embedded AI development and deployment.\n\nOverview of policy frameworks governing responsible AI\nGlobal initiatives fostering responsible AI development\nImplications of policy frameworks on embedded AI systems\nFuture directions in policy and regulation"
  },
  {
    "objectID": "responsible_ai.html#conclusion",
    "href": "responsible_ai.html#conclusion",
    "title": "16¬† Responsible AI",
    "section": "16.9 Conclusion",
    "text": "16.9 Conclusion\nExplanation: This concluding section synthesizes the essential discussions and insights throughout the chapter, fostering a deeper comprehension of the importance and approaches to responsible AI in the context of embedded systems.\n\nRecap of key insights and discussions\nThe path forward: fostering responsible embedded AI development\nEncouraging research and innovation in responsible AI"
  },
  {
    "objectID": "generative_ai.html#introduction",
    "href": "generative_ai.html#introduction",
    "title": "17¬† Generative AI",
    "section": "17.1 Introduction",
    "text": "17.1 Introduction\nExplanation: This section will introduce readers to the basics of generative AI, emphasizing its importance and role in the modern technology landscape, particularly within the domain of embedded systems. This sets the stage for a deeper exploration of the specific aspects and applications of generative AI in the following sections.\n\nDefinition and Overview\nImportance in Embedded AI\nOverview of Generative AI Models"
  },
  {
    "objectID": "generative_ai.html#generative-models",
    "href": "generative_ai.html#generative-models",
    "title": "17¬† Generative AI",
    "section": "17.2 Generative Models",
    "text": "17.2 Generative Models\nExplanation: In this section, readers will build a basic foundation by learning about different generative models. Understanding the general working principles and characteristics of these models may help set the stage to think about the applications and issues down the road.\n\nVariational Autoencoders (VAEs)\nGenerative Adversarial Networks (GANs)\nRestricted Boltzmann Machines (RBMs)"
  },
  {
    "objectID": "generative_ai.html#applications-of-generative-models-for-embedded-systems",
    "href": "generative_ai.html#applications-of-generative-models-for-embedded-systems",
    "title": "17¬† Generative AI",
    "section": "17.3 Applications of Generative Models for Embedded Systems",
    "text": "17.3 Applications of Generative Models for Embedded Systems\nExplanation: This section delves into the practical applications of generative models in embedded systems, highlighting their versatility and potential for innovation. Readers will explore how generative AI can foster creativity, enhance data augmentation, and personalize user experiences on embedded devices.\n\n17.3.1 Creative Applications\n\nGenerating realistic images and videos\nProducing text and music compositions\nOther innovative content creations\n\n\n\n17.3.2 Data Augmentation\n\nAugmenting existing datasets for sensors\nEnhancing machine learning model training on embedded devices\nTackling data limitations on embedded systems\n\n\n\n17.3.3 Personalization\n\nGenerating custom recommendations\nFacilitating multi-language text translations\nEnhancing user experiences through tailored content"
  },
  {
    "objectID": "generative_ai.html#challenges-and-opportunities",
    "href": "generative_ai.html#challenges-and-opportunities",
    "title": "17¬† Generative AI",
    "section": "17.4 Challenges and Opportunities",
    "text": "17.4 Challenges and Opportunities\nExplanation: This critical section directly ties generative AI to embedded systems, offering a balanced view of the challenges and opportunities this integration brings about. Through this discussion, readers will gain insights into the synergies between generative AI and embedded systems, paving the way for future developments and practical applications.\n\nChallenges of implementing generative AI models on embedded systems\n\nResource constraints\nPower limitations\n\nStrategies for optimizing generative AI models for embedded systems\n\nModel quantization\nPruning\nHardware acceleration\n\nCan likely refer back to the previous chapters for these details."
  },
  {
    "objectID": "generative_ai.html#conclusion",
    "href": "generative_ai.html#conclusion",
    "title": "17¬† Generative AI",
    "section": "17.5 Conclusion",
    "text": "17.5 Conclusion\nExplanation: This section serves as a summation of the chapter, revisiting the important points discussed and emphasizing the potential impacts of generative AI in the industry. It aims to reinforce the knowledge acquired and inspire readers to further explore or initiate projects in the field of generative AI and embedded systems.\n\nRecap of key takeaways\nEncouragement for deeper exploration and practical engagement in the field"
  },
  {
    "objectID": "ai_for_good.html#introduction",
    "href": "ai_for_good.html#introduction",
    "title": "18¬† AI for Good",
    "section": "18.1 Introduction",
    "text": "18.1 Introduction\nTo give ourselves a framework around which to think about AI for social good, we will be following the UN Sustainable Development Goals (SDGs). The UN SDGs are a collection of 17 global goals adopted by the United Nations in 2015 as part of the 2030 Agenda for Sustainable Development. The SDGs address global challenges related to poverty, inequality, climate change, environmental degradation, prosperity, and peace and justice.\nWhat is special about SDGs is that they are a collection of interlinked objectives designed to serve as a ‚Äúshared blueprint for peace and prosperity for people and the planet, now and into the future.‚Äù. The SDGs emphasize the interconnected environmental, social and economic aspects of sustainable development by putting sustainability at their center.\nA recent study (Vinuesa et al. 2020) highlights the influence of AI on all aspects of sustainable development, in particular on the 17 Sustainable Development Goals (SDGs) and 169 targets internationally defined in the 2030 Agenda for Sustainable Development. The study shows that AI can act as an enabler for 134 targets through technological improvements, but it also highlights the challenges of AI on some of the targets. When considering AI and societal outcomes, the study shows that AI can benefit 67 targets, but it also warns about the issues related to the implementation of AI in countries with different cultural values and wealth.\n\nVinuesa, Ricardo, Hossein Azizpour, Iolanda Leite, Madeline Balaam, Virginia Dignum, Sami Domisch, Anna Fell√§nder, Simone Daniela Langhans, Max Tegmark, and Francesco Fuso Nerini. 2020. ‚ÄúThe Role of Artificial Intelligence in Achieving the Sustainable Development Goals.‚Äù Nature Communications 11 (1): 1‚Äì10.\n\n\n\nUnited Nations Sustainable Developemnt Goals (SDG)\n\n\nIn the context of our book, here is how TinyML could potentially help advance at least some of these SDG goals.\n\nGoal 1 - No Poverty: TinyML could help provide low-cost solutions for tasks like crop monitoring to improve agricultural yields in developing countries.\nGoal 2 - Zero Hunger: TinyML could enable localized and precise crop health monitoring and disease detection to reduce crop losses.\nGoal 3 - Good Health and Wellbeing: TinyML could help enable low-cost medical diagnosis tools for early detection and prevention of diseases in remote areas.\nGoal 6 - Clean Water and Sanitation: TinyML could monitor water quality and detect contaminants to ensure access to clean drinking water.\nGoal 7 - Affordable and Clean Energy: TinyML could optimize energy consumption and enable predictive maintenance for renewable energy infrastructure.\nGoal 11 - Sustainable Cities and Communities: TinyML could enable intelligent traffic management, air quality monitoring, and optimized resource management in smart cities.\nGoal 13 - Climate Action: TinyML could monitor deforestation and track reforestation efforts. It could also help predict extreme weather events.\n\nThe portability, lower power requirements, and real-time analytics enabled by TinyML make it well-suited for addressing several sustainability challenges faced by developing regions. Widespread deployment of power solutions has the potential to provide localized and cost-effective monitoring to help achieve some of the UN SDGs. In the rest of the sections, we will dive into the details of how TinyML is useful across many of the sectors that have the potential to address the UN SDGs."
  },
  {
    "objectID": "ai_for_good.html#agriculture",
    "href": "ai_for_good.html#agriculture",
    "title": "18¬† AI for Good",
    "section": "18.2 Agriculture",
    "text": "18.2 Agriculture\nAgriculture is essential to achieving many of the UN Sustainable Development Goals, including eradicating hunger and malnutrition, promoting economic growth, and using natural resources sustainably. TinyML can be a valuable tool to help advance sustainable agriculture, especially for smallholder farmers in developing regions.\nTinyML solutions can provide real-time monitoring and data analytics for crop health and growing conditions - all without reliance on connectivity infrastructure. For example, low-cost camera modules connected to microcontrollers can monitor for disease, pests, and nutritional deficiencies. TinyML algorithms can analyze the images to detect issues early before they spread and damage yields. This kind of precision monitoring can optimize inputs like water, fertilizer, and pesticides - improving efficiency and sustainability.\nOther sensors like GPS units and accelerometers can track microclimate conditions, soil humidity, and livestock wellbeing. Local real-time data helps farmers respond and adapt better to changes in the field. TinyML analytics at the edge avoids lag, network disruptions, and high data costs of cloud-based systems. And localized systems allow for customization to specific crops, diseases, and regional issues.\nWidespread TinyML applications can help digitize smallholder farms to increase productivity, incomes, and resilience. The low cost of hardware and minimal connectivity requirements make solutions accessible. Projects across the developing world have shown the benefits:\n\nMicrosoft‚Äôs FarmBeats project is an end-to-end approach to enable data-driven farming by using low-cost sensors, drones, and vision and machine learning algorithms ¬π. The project aims to solve the problem of limited adoption of technology in farming due to the lack of power and internet connectivity in farms and the farmers‚Äô limited technology savviness. The project‚Äôs goal is to increase farm productivity and reduce costs by coupling data with the farmer‚Äôs knowledge and intuition about their farm. The project has been successful in enabling actionable insights from data by building artificial intelligence (AI) or machine learning (ML) models based on fused data sets.\nIn Sub-Saharan Africa, off-the-shelf cameras and edge AI cut cassava losses to disease from 40% down to 5%, protecting a staple crop (Ramcharan et al. 2017).\nIn Indonesia, sensors monitor microclimates across rice paddies, optimizing water usage even with erratic rains (Tirtalistyani, Murtiningrum, and Kanwar 2022).\n\n\nRamcharan, Amanda, Kelsee Baranowski, Peter McCloskey, Babuali Ahmed, James Legg, and David P Hughes. 2017. ‚ÄúDeep Learning for Image-Based Cassava Disease Detection.‚Äù Frontiers in Plant Science 8: 1852.\n\nTirtalistyani, Rose, Murtiningrum Murtiningrum, and Rameshwar S Kanwar. 2022. ‚ÄúIndonesia Rice Irrigation System: Time for Innovation.‚Äù Sustainability 14 (19): 12477.\nWith greater investment and integration into rural advisory services, TinyML could transform small-scale agriculture and improve livelihoods for farmers worldwide. The technology effectively brings the benefits of precision agriculture to disconnected regions most in need."
  },
  {
    "objectID": "ai_for_good.html#healthcare",
    "href": "ai_for_good.html#healthcare",
    "title": "18¬† AI for Good",
    "section": "18.3 Healthcare",
    "text": "18.3 Healthcare\n\n18.3.1 Expanding Access\nUniversal health coverage and quality care remain out of reach for millions worldwide. A shortage of medical professionals severely limits access to even basic diagnosis and treatment in many regions. Additionally, healthcare infrastructure like clinics, hospitals, and utilities to power complex equipment are lacking. These gaps disproportionately impact marginalized communities, exacerbating health disparities.\nTinyML offers a promising technological solution to help expand access to quality healthcare globally. TinyML refers to the ability to deploy machine learning algorithms on microcontrollers, tiny chips with processing power, memory, and connectivity. TinyML enables real-time data analysis and intelligence in low-powered, compact devices.\nThis creates opportunities for transformative medical tools that are portable, affordable, and accessible. TinyML software and hardware can be optimized to run even in resource-constrained environments. For example, a TinyML system could analyze symptoms or make diagnostic predictions using minimal computing power, no continuous internet connectivity, and a battery or solar power source. These capabilities can bring medical-grade screening and monitoring directly to underserved patients.\n\n\n18.3.2 Early Diagnosis\nEarly detection of diseases is one major application. Small sensors paired with TinyML software can identify symptoms before conditions escalate or visible signs appear. For instance, cough monitors with embedded machine learning can pick up on acoustic patterns indicative of respiratory illness, malaria, or tuberculosis. Detecting diseases at onset improves outcomes and reduces healthcare costs.\nA detailed example could be given for using TinyML to monitor pneumonia in children. Pneumonia is a leading cause of death for children under 5, and detecting it early is critical. A startup called Respira Labs has developed a low-cost wearable audio sensor that uses TinyML algorithms to analyze coughs and identify symptoms of respiratory illnesses like pneumonia. The device contains a microphone sensor and microcontroller that runs a neural network model trained to classify respiratory sounds. It can identify features like wheezing, crackling, and stridor that may indicate pneumonia. The device is designed to be highly accessible - it has a simple strap, requires no battery or charging, and results are provided through LED lights and audio cues.\nAnother example involves researchers at UNIFEI in Brazil who have developed a low-cost device that leverages TinyML to monitor heart rhythms. Their innovative solution addresses a critical need - atrial fibrillation and other heart rhythm abnormalities often go undiagnosed due to the prohibitive cost and limited availability of screening tools. The device overcomes these barriers through its ingenious design. It uses an off-the-shelf microcontroller that costs only a few dollars, along with a basic pulse sensor. By minimizing complexity, the device becomes accessible to under-resourced populations. The TinyML algorithm running locally on the microcontroller analyzes pulse data in real time to detect irregular heart rhythms. This life-saving heart monitoring device demonstrates how TinyML enables powerful AI capabilities to be deployed in cost-effective, user-friendly designs.\nTinyML‚Äôs versatility also shows promise for tackling infectious diseases. Researchers have proposed applying TinyML to identify malaria-spreading mosquitoes by their wingbeat sounds. When equipped with microphones, small microcontrollers can run advanced audio classification models to determine mosquito species. This compact, low-power solution produces results in real time, suitable for remote field use. By making entomology analytics affordable and accessible, TinyML could revolutionize monitoring of insects that endanger human health. From heart disease to malaria, TinyML is expanding healthcare access for vulnerable communities.\n\n\n18.3.3 Infectious Disease Control\nMosquitoes remain the most deadly disease vector worldwide, transmitting illnesses that infect over one billion people annually (‚ÄúVector-Borne Diseases‚Äù). Diseases like malaria, dengue, and Zika are especially prevalent in resource-limited regions lacking robust infrastructure for mosquito control. Monitoring local mosquito populations is essential to prevent outbreaks and properly target interventions.\n\n‚ÄúVector-Borne Diseases.‚Äù https://www.who.int/news-room/fact-sheets/detail/vector-borne-diseases.\nTraditional monitoring methods are expensive, labor-intensive, and difficult to deploy remotely. The proposed TinyML solution aims to overcome these barriers. Small microphones coupled with machine learning algorithms can classify mosquitoes by species based on minute differences in wing oscillations. The TinyML software runs efficiently on low-cost microcontrollers, eliminating the need for continuous connectivity.\nA collaborative research team from the University of Khartoum and the ICTP is exploring an innovative solution using TinyML. In a recent paper, they presented a low-cost device that can identify disease-spreading mosquito species through their wingbeat sounds (Altayeb, Zennaro, and Rovai 2022).\n\nAltayeb, Moez, Marco Zennaro, and Marcelo Rovai. 2022. ‚ÄúClassifying Mosquito Wingbeat Sound Using TinyML.‚Äù In Proceedings of the 2022 ACM Conference on Information Technology for Social Good, 132‚Äì37.\nThis portable, self-contained system shows great promise for entomology. The researchers suggest it could revolutionize insect monitoring and vector control strategies in remote areas. By providing cheaper, easier mosquito analytics, TinyML could significantly bolster malaria eradication efforts. Its versatility and minimal power needs make it ideal for field use in isolated, off-grid regions with scarce resources but high disease burden."
  },
  {
    "objectID": "ai_for_good.html#science",
    "href": "ai_for_good.html#science",
    "title": "18¬† AI for Good",
    "section": "18.4 Science",
    "text": "18.4 Science\nIn many scientific fields, researchers are limited by the quality and resolution of data they can collect. They often must infer the true parameters of interest indirectly, using approximate correlations and models built on sparse data points. This constrains the accuracy of scientific understanding and predictions.\nThe emergence of TinyML opens new possibilities for gathering high-fidelity scientific measurements. With embedded machine learning, tiny low-cost sensors can automatically process and analyze data locally in real time. This creates intelligent sensor networks that capture nuanced data at much greater scales and frequencies.\nFor example, monitoring environmental conditions to model climate change remains a challenge due to the lack of widespread, continuous data. The Ribbit Project from UC Berkeley is pioneering a crowdsourced TinyML solution (Rao 2021). They developed an open-source CO2 sensor that uses an onboard microcontroller to process the gas measurements. By distributing hundreds of these low-cost sensors, an extensive dataset can be aggregated. The TinyML devices compensate for environmental factors and provide granular, accurate readings not possible previously.\n\nRao, Ravi. 2021. Www.wevolver.com. https://www.wevolver.com/article/tinyml-unlocks-new-possibilities-for-sustainable-development-technologies.\nThe potential to massively scale out intelligent sensing via TinyML has profound scientific implications. From ecology to cosmology, higher resolution data can lead to new discoveries and predictive capabilities. Other applications could include seismic sensors for earthquake early warning systems, distributed weather monitors to track microclimate changes, and acoustic sensors to study animal populations.\nAs sensors and algorithms continue improving, TinyML networks may generate more detailed maps of natural systems than ever before. Democratizing the collection of scientific data can accelerate research and understanding across disciplines. But it also raises new challenges around data quality, privacy, and modeling unknowns. Overall, TinyML signifies a growing convergence of AI and the natural sciences to answer fundamental questions."
  },
  {
    "objectID": "ai_for_good.html#conservation-and-environment",
    "href": "ai_for_good.html#conservation-and-environment",
    "title": "18¬† AI for Good",
    "section": "18.5 Conservation and Environment",
    "text": "18.5 Conservation and Environment\nTinyML is emerging as a powerful tool for environmental conservation and sustainability efforts. Recent research has highlighted numerous applications of tiny machine learning across domains like wildlife monitoring, natural resource management, and tracking climate change.\nOne example is using TinyML for real-time wildlife tracking and protection. Researchers have developed Smart Wildlife Tracker devices that leverage TinyML algorithms to detect poaching activities. The collars contain sensors like cameras, microphones, and GPS to continuously monitor the surrounding environment. Embedded machine learning models analyze the audio and visual data to identify threats like nearby humans or gunshots. Early poaching detection gives wildlife rangers critical information to intervene and take action.\nOther projects apply TinyML to study animal behavior through sensors. The smart wildlife collar uses accelerometers and acoustic monitoring to track elephant movements, communication, and moods (Verma 2022). The low-power TinyML collar devices transmit rich data on elephant activities while avoiding burdensome Battery changes. This helps researchers unobtrusively observe elephant populations to inform conservation strategies.\n\nVerma, Team Dual_Boot: Swapnil. 2022. ‚ÄúElephant AI.‚Äù Hackster.io. https://www.hackster.io/dual_boot/elephant-ai-ba71e9.\nOn a broader scale, distributed TinyML devices are envisioned to create dense sensor networks for environmental modeling. Hundreds of low-cost air quality monitors could map pollution across cities. Underwater sensors may detect toxins and give early warning of algal blooms. Such applications underscore TinyML‚Äôs versatility in ecology, climatology, and sustainability.\nA survey on how TinyML can be used to solve environmental issues has been published by researchers from Moulay Ismail University of Meknes in Morocco (Bamoumen et al. 2022). However, thoughtfully assessing benefits, risks, and equitable access will be vital as TinyML expands environmental research and conservation. With ethical consideration of impacts, TinyML offers data-driven solutions to protect biodiversity, natural resources, and our planet as a whole.\n\nBamoumen, Hatim, Anas Temouden, Nabil Benamar, and Yousra Chtouki. 2022. ‚ÄúHow TinyML Can Be Leveraged to Solve Environmental Problems: A Survey.‚Äù In 2022 International Conference on Innovation and Intelligence for Informatics, Computing, and Technologies (3ICT), 338‚Äì43. IEEE."
  },
  {
    "objectID": "ai_for_good.html#disaster-response",
    "href": "ai_for_good.html#disaster-response",
    "title": "18¬† AI for Good",
    "section": "18.6 Disaster Response",
    "text": "18.6 Disaster Response\nIn disaster response, speed and safety are paramount. But rubble and wreckage create hazardous, confined environments that impede human search efforts. TinyML enables nimble drones to assist rescue teams in these dangerous scenarios.\nWhen buildings collapse after earthquakes, small drones can prove invaluable. Equipped with TinyML navigation algorithms, micro-sized drones like the CrazyFlie can traverse cramped voids and map pathways beyond human reach (Duisterhof et al. 2019). Obstacle avoidance allows the drones to weave through unstable debris. This autonomous mobility lets them rapidly sweep areas humans cannot access.\n\nDuisterhof, Bardienus P, Srivatsan Krishnan, Jonathan J Cruz, Colby R Banbury, William Fu, Aleksandra Faust, Guido CHE de Croon, and Vijay Janapa Reddi. 2019. ‚ÄúLearning to Seek: Autonomous Source Seeking with Deep Reinforcement Learning Onboard a Nano Drone Microcontroller.‚Äù arXiv Preprint arXiv:1909.11236.\n\nCrucially, onboard sensors and TinyML processors analyze real-time data to identify signs of survivors. Thermal cameras detect body heat, microphones pick up calls for help, and gas sensors warn of leaks (Duisterhof et al. 2021). Processing data locally using TinyML allows for quick interpretation to guide rescue efforts. As conditions evolve, the drones can adapt by adjusting their search patterns and priorities.\n\nAdditionally, coordinated swarms of drones unlock new capabilities. By collaborating and sharing insights, drone teams achieve a comprehensive view of the situation. Blanketing disaster sites allows TinyML algorithms to fuse and analyze data from multiple vantage points. This amplifies situational awareness beyond individual drones (Duisterhof et al. 2021).\n\nDuisterhof, Bardienus P, Shushuai Li, Javier Burgu√©s, Vijay Janapa Reddi, and Guido CHE de Croon. 2021. ‚ÄúSniffy Bug: A Fully Autonomous Swarm of Gas-Seeking Nano Quadcopters in Cluttered Environments.‚Äù In 2021 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), 9099‚Äì9106. IEEE.\nMost importantly, initial drone reconnaissance enhances safety for human responders. Keeping rescue teams at a safe distance until drone surveys assess hazards saves lives. Once secured, drones can guide precise placement of personnel.\nBy combining agile mobility, real-time data, and swarm coordination, TinyML-enabled drones promise to transform disaster response. Their versatility, speed, and safety make them a vital asset for rescue efforts in dangerous, inaccessible environments. Integrating autonomous drones with traditional methods can accelerate responses when it matters most."
  },
  {
    "objectID": "ai_for_good.html#education-and-outreach",
    "href": "ai_for_good.html#education-and-outreach",
    "title": "18¬† AI for Good",
    "section": "18.7 Education and Outreach",
    "text": "18.7 Education and Outreach\nTinyML holds immense potential to help address challenges in developing regions, but realizing its benefits requires focused education and capacity building. Recognizing this need, academic researchers have spearheaded outreach initiatives to spread TinyML education globally.\nIn 2020, Harvard University, Columbia University, the International Centre for Theoretical Physics (ICTP), and UNIFEI jointly founded the TinyML for Developing Communities (TinyML4D) network (Zennaro, Plancher, and Reddi 2022). This network aims to empower universities and researchers in developing countries to harness TinyML for local impact.\n\nZennaro, Marco, Brian Plancher, and V Janapa Reddi. 2022. ‚ÄúTinyML: Applied AI for Development.‚Äù In The UN 7th Multi-Stakeholder Forum on Science, Technology and Innovation for the Sustainable Development Goals, 2022‚Äì05.\nA core focus is expanding access to applied machine learning education. The TinyML4D network provides training, curricula, and lab resources to members. Hands-on workshops and data collection projects give students practical experience. Through conferences and academic collaborations, members can share best practices and build a community.\nThe network prioritizes enabling locally-relevant TinyML solutions. Projects address challenges like agriculture, health, and environmental monitoring based on community needs. For example, a member university in Rwanda developed a low-cost flood monitoring system using TinyML and sensors.\nTo date, TinyML4D includes over 50 member institutions across Africa, Asia, and Latin America. But greater investments and industry partnerships are needed to reach all underserved regions. The ultimate vision is training new generations to ethically apply TinyML for sustainable development. Outreach efforts today lay the foundation to democratize transformative technology for the future."
  },
  {
    "objectID": "ai_for_good.html#accessibility",
    "href": "ai_for_good.html#accessibility",
    "title": "18¬† AI for Good",
    "section": "18.8 Accessibility",
    "text": "18.8 Accessibility\nTechnology has immense potential to break down barriers faced by people with disabilities and bridge gaps in accessibility. TinyML specifically opens new possibilities for developing intelligent, personalized assistive devices.\nWith machine learning algorithms running locally on microcontrollers, compact accessibility tools can operate in real-time without reliance on connectivity. The National Institute on Deafness and Other Communication Disorders (NIDCD) states that 20% of the world‚Äôs population has some form of hearing loss. Hearing aids leveraging TinyML could recognize multiple speakers and amplify the voice of a chosen target in crowded rooms. This allows people with hearing impairments to focus on specific conversations.\nSimilarly, mobility devices could use on-device vision processing to identify obstacles and terrain characteristics. This enables enhanced navigation and safety for the visually impaired. Companies like Envision are developing smart glasses, converting visual information into speech, with embedded TinyML to guide the blind by detecting objects, text, and traffic signals.\n\nTinyML could even power responsive prosthetic limbs. By analyzing nerve signals and sensory data like muscle tension, prosthetics and exoskeletons with embedded ML can move and adjust grip dynamically. This makes control more natural and intuitive. Companies are creating affordable, everyday bionic hands using TinyML. And for those with speech difficulties, voice-enabled devices with TinyML can generate personalized vocal outputs from non-verbal inputs. Pairs by Anthropic translates gestures into natural speech tailored for individual users.\nBy enabling more customizable assistive tech, TinyML makes services more accessible and tailored to individual needs. And through translation and interpretation applications, TinyML can break down communication barriers. Apps like Microsoft Translator offer real-time translation powered by TinyML algorithms.\nWith thoughtful and inclusive design, TinyML promises more autonomy and dignity for people with disabilities. But developers should engage communities directly, avoid compromising privacy, and consider affordability to maximize benefit. Overall, TinyML has huge potential to contribute to a more just, equitable world."
  },
  {
    "objectID": "ai_for_good.html#infrastructure-and-urban-planning",
    "href": "ai_for_good.html#infrastructure-and-urban-planning",
    "title": "18¬† AI for Good",
    "section": "18.9 Infrastructure and Urban Planning",
    "text": "18.9 Infrastructure and Urban Planning\nAs urban populations swell, cities face immense challenges in efficiently managing resources and infrastructure. TinyML presents a powerful tool for developing intelligent systems to optimize city operations and sustainability. It could revolutionize energy efficiency in smart buildings.\nMachine learning models can learn to predict and regulate energy usage based on occupancy patterns. Miniaturized sensors placed throughout buildings can provide granular, real-time data on space utilization, temperature, and more (Seyedzadeh et al. 2018). This visibility allows TinyML systems to minimize waste by optimizing heating, cooling, lighting, etc.\n\nSeyedzadeh, Saleh, Farzad Pour Rahimian, Ivan Glesk, and Marc Roper. 2018. ‚ÄúMachine Learning for Estimation of Building Energy Consumption and Performance: A Review.‚Äù Visualization in Engineering 6: 1‚Äì20.\nThese examples demonstrate TinyML‚Äôs huge potential for efficient, sustainable city infrastructure. But urban planners must consider privacy, security, and accessibility to ensure responsible adoption. With careful implementation, TinyML could profoundly modernize urban life."
  },
  {
    "objectID": "ai_for_good.html#challenges-and-considerations",
    "href": "ai_for_good.html#challenges-and-considerations",
    "title": "18¬† AI for Good",
    "section": "18.10 Challenges and Considerations",
    "text": "18.10 Challenges and Considerations\nWhile TinyML presents immense opportunities, thoughtful consideration of challenges and ethical implications will be critical as adoption spreads globally. Researchers have highlighted key factors to address, especially in deploying TinyML in developing regions.\nA foremost challenge is limited access to training and hardware (Ooko et al. 2021). Few educational programs exist tailored to TinyML, and emerging economies often lack a robust electronics supply chain. Thorough training and partnerships will be needed to nurture expertise and avail devices to underserved communities. Initiatives like the TinyML4D network help provide structured learning pathways.\n\nOoko, Samson Otieno, Marvin Muyonga Ogore, Jimmy Nsenga, and Marco Zennaro. 2021. ‚ÄúTinyML in Africa: Opportunities and Challenges.‚Äù In 2021 IEEE Globecom Workshops (GC Wkshps), 1‚Äì6. IEEE.\nData limitations also pose hurdles. TinyML models require quality localized datasets, but these are scarce in under-resourced environments. Creating frameworks to ethically crowdsource data could address this. But data collection should benefit local communities directly, not just extract value.\nOptimizing power usage and connectivity will be vital for sustainability. TinyML‚Äôs low power needs make it ideal for off-grid use cases. Integrating battery or solar can enable continuous operation. Adapting devices for low-bandwidth transmission where internet is limited also maximizes impact.\nCultural and language barriers further complicate adoption. User interfaces and devices should account for all literacy levels and avoid excluding subgroups. Voice-controllable solutions in local dialects can enhance accessibility.\nAddressing these challenges requires holistic partnerships, funding, and policy support. But inclusively and ethically scaling TinyML has monumental potential to uplift disadvantaged populations worldwide. With thoughtful implementation, the technology could profoundly democratize opportunity."
  },
  {
    "objectID": "ai_for_good.html#conclusion",
    "href": "ai_for_good.html#conclusion",
    "title": "18¬† AI for Good",
    "section": "18.11 Conclusion",
    "text": "18.11 Conclusion\nTinyML presents a tremendous opportunity to harness the power of artificial intelligence to advance the UN Sustainable Development Goals and drive social impact globally. As highlighted through the examples across sectors like healthcare, agriculture, conservation and more, embedded machine learning unlocks new capabilities for low-cost, accessible solutions tailored to local contexts. TinyML circumvents barriers like poor infrastructure, limited connectivity, and high costs that often exclude developing communities from emerging technology.\nHowever, realizing TinyML‚Äôs full potential requires holistic collaboration. Researchers, policymakers, companies and local stakeholders must work together to provide training, establish ethical frameworks, co-design solutions, and adapt them to community needs. Only through inclusive development and deployment can TinyML deliver on its promise to bridge inequities and uplift vulnerable populations without leaving any behind.\nIf cultivated responsibly, TinyML could democratize opportunity and accelerate progress on global priorities from poverty alleviation to climate resilience. The technology represents a new wave of applied AI to empower societies, promote sustainability, and propel all of humanity collectively towards greater justice, prosperity and peace. TinyML provides a glimpse into an AI-enabled future that is accessible to all."
  },
  {
    "objectID": "sustainable_ai.html#introduction",
    "href": "sustainable_ai.html#introduction",
    "title": "19¬† Sustainable AI",
    "section": "19.1 Introduction",
    "text": "19.1 Introduction\nExplanation: In this introductory section, we elucidate the significance of sustainability in the context of AI, emphasizing the necessity to address environmental, economic, and social dimensions to build resilient and sustainable AI systems.\n\nImportance of sustainability in AI\nSustainability dimensions: environmental, economic, and social\nOverview of challenges and opportunities"
  },
  {
    "objectID": "sustainable_ai.html#energy-efficiency-of-ai-models",
    "href": "sustainable_ai.html#energy-efficiency-of-ai-models",
    "title": "19¬† Sustainable AI",
    "section": "19.2 Energy Efficiency of AI Models",
    "text": "19.2 Energy Efficiency of AI Models\nExplanation: This section addresses the pressing issue of high energy consumption associated with AI models, offering insights into techniques for creating energy-efficient AI models which are not only economical but also environmentally friendly.\n\nEnergy consumption patterns of AI models\nTechniques for improving energy efficiency\nCase studies of energy-efficient AI deployments"
  },
  {
    "objectID": "sustainable_ai.html#responsible-resource-utilization",
    "href": "sustainable_ai.html#responsible-resource-utilization",
    "title": "19¬† Sustainable AI",
    "section": "19.3 Responsible Resource Utilization",
    "text": "19.3 Responsible Resource Utilization\nExplanation: Here, we delve into strategies for responsible resource utilization in AI, discussing how optimizing resource allocation can lead to more sustainable and cost-effective AI systems.\n\nResource allocation and management in AI\nReducing resource wastage\nResource optimization techniques and tools\nExplain resource difference between big / small systems"
  },
  {
    "objectID": "sustainable_ai.html#e-waste-management",
    "href": "sustainable_ai.html#e-waste-management",
    "title": "19¬† Sustainable AI",
    "section": "19.4 E-Waste Management",
    "text": "19.4 E-Waste Management\nExplanation: This segment explores the problem of electronic waste generated by AI components, suggesting guidelines and best practices for reducing e-waste and promoting recycling and reusing initiatives.\n\nOverview of e-waste generated by AI components\nBest practices for e-waste management\nPromoting recycling and reuse in AI systems\nDiscuss tinyML e-waste from CACM"
  },
  {
    "objectID": "sustainable_ai.html#carbon-footprint-reduction",
    "href": "sustainable_ai.html#carbon-footprint-reduction",
    "title": "19¬† Sustainable AI",
    "section": "19.5 Carbon Footprint Reduction",
    "text": "19.5 Carbon Footprint Reduction\nExplanation: In this section, readers will learn about the carbon footprint associated with AI operations and the methods to mitigate it, contributing to a greener and more sustainable AI ecosystem.\n\nAssessing the carbon footprint of AI operations\nStrategies for carbon footprint reduction\nDiscuss how edge/tinyML might help address issues\nCarbon offset initiatives in AI"
  },
  {
    "objectID": "sustainable_ai.html#sustainable-embedded-ml",
    "href": "sustainable_ai.html#sustainable-embedded-ml",
    "title": "19¬† Sustainable AI",
    "section": "19.6 Sustainable Embedded ML",
    "text": "19.6 Sustainable Embedded ML\nExplanation: The focus here is on the full footprint, embodied and carbon footprint, which are the backbone of sustainability, providing insights into how the devices can be designed or modified to be more sustainable\n\nRead through the tinyML sustainability paper"
  },
  {
    "objectID": "sustainable_ai.html#community-engagement-and-collaboration",
    "href": "sustainable_ai.html#community-engagement-and-collaboration",
    "title": "19¬† Sustainable AI",
    "section": "19.7 Community Engagement and Collaboration",
    "text": "19.7 Community Engagement and Collaboration\nExplanation: This section accentuates the role of community engagement and collaboration in fostering AI sustainability, presenting ways in which a collaborative approach can help in sharing knowledge and resources for sustainable AI development.\n\nCommunity-driven sustainability initiatives\nCollaborative research and development\nPublic-private partnerships for sustainable AI"
  },
  {
    "objectID": "sustainable_ai.html#policy-frameworks-and-regulations",
    "href": "sustainable_ai.html#policy-frameworks-and-regulations",
    "title": "19¬† Sustainable AI",
    "section": "19.8 Policy Frameworks and Regulations",
    "text": "19.8 Policy Frameworks and Regulations\nExplanation: This segment emphasizes the necessity for robust policy frameworks and regulations to govern AI sustainability, highlighting global efforts and initiatives that are steering the path towards a sustainable AI future.\n\nExisting policy frameworks for AI sustainability\nInternational initiatives and collaborations\nFuture directions in policy and regulation"
  },
  {
    "objectID": "sustainable_ai.html#future-trends-in-ai-sustainability",
    "href": "sustainable_ai.html#future-trends-in-ai-sustainability",
    "title": "19¬† Sustainable AI",
    "section": "19.9 Future Trends in AI Sustainability",
    "text": "19.9 Future Trends in AI Sustainability\nExplanation: Here, we discuss anticipated trends in AI sustainability, projecting how evolving technologies and methodologies might shape the sustainability landscape of AI in the coming years.\n\nAnticipated technological advancements\nRole of AI in promoting global sustainability\nChallenges and opportunities ahead"
  },
  {
    "objectID": "sustainable_ai.html#conclusion",
    "href": "sustainable_ai.html#conclusion",
    "title": "19¬† Sustainable AI",
    "section": "19.10 Conclusion",
    "text": "19.10 Conclusion\nExplanation: The closing section encapsulates the key discussions and insights presented throughout the chapter, fostering a deep-seated understanding of the necessity and approaches for AI sustainability.\n\nRecap of key insights and discussions\nThe road ahead: fostering sustainability in AI\nEncouraging innovation and research in AI sustainability"
  },
  {
    "objectID": "robust_ai.html#hardware-resilience",
    "href": "robust_ai.html#hardware-resilience",
    "title": "20¬† Robust AI",
    "section": "20.1 Hardware Resilience",
    "text": "20.1 Hardware Resilience\nExplanation: With the proliferation of TinyML on edge devices, the hardware on which these models run can be exposed to various environmental factors and wear-and-tear. Ensuring hardware resilience is crucial to maintain consistent AI performance.\n\n20.1.1 Compute Faults\nDescription: Discusses issues related to faults in the computation units, such as CPUs, GPUs, and custom accelerators. This can include issues like overheating, transistor failures, or other malfunctions.\n\n\n20.1.2 Memory Faults\nDescription: Addresses faults in the memory components of a system, including RAM, cache, and storage. Topics can include bit flips, wear-out, and other memory-related issues."
  },
  {
    "objectID": "robust_ai.html#software-resilience",
    "href": "robust_ai.html#software-resilience",
    "title": "20¬† Robust AI",
    "section": "20.2 Software Resilience",
    "text": "20.2 Software Resilience\nExplanation: Software forms the backbone of any AI system. Ensuring its resilience means that the system can handle unexpected inputs, software bugs, or other issues without catastrophic failure.\n\n20.2.1 Framework Faults\nDescription: Discusses potential issues in the software stack, from the OS to the AI framework. This can include bugs, version incompatibilities, or other software-related problems.\n\n\n20.2.2 Faulty Inputs\nDescription: Explores how AI systems can handle unexpected or corrupted inputs. This is especially important for systems in the real world where input data can be noisy or unreliable."
  },
  {
    "objectID": "robust_ai.html#model-resilience",
    "href": "robust_ai.html#model-resilience",
    "title": "20¬† Robust AI",
    "section": "20.3 Model Resilience",
    "text": "20.3 Model Resilience\nAs the core of any AI system, the model‚Äôs resilience to various challenges, from adversarial attacks to real-world data shifts, is paramount for reliable operation.\n\n20.3.1 Worst-case Faults\nDescription: Investigates the model‚Äôs behavior under worst-case scenarios, such as extreme data values or conditions outside the training distribution.\n\n\n20.3.2 Adversarial Attacks\nDescription: Discusses potential threats where malicious actors intentionally manipulate inputs to deceive the AI model, and strategies to defend against these attacks."
  },
  {
    "objectID": "robust_ai.html#conclusion",
    "href": "robust_ai.html#conclusion",
    "title": "20¬† Robust AI",
    "section": "20.4 Conclusion",
    "text": "20.4 Conclusion\nExplanation: Conclude with the key highlights."
  },
  {
    "objectID": "niclav_sys.html#introduction",
    "href": "niclav_sys.html#introduction",
    "title": "Setup Nicla Vision",
    "section": "Introduction",
    "text": "Introduction\nThe Arduino Nicla Vision (sometimes called NiclaV) is a development board that includes two processors that can run tasks in parallel. It is part of a family of development boards with the same form factor but designed for specific tasks, such as the Nicla Sense ME and the Nicla Voice. The Niclas can efficiently run processes created with TensorFlow‚Ñ¢ Lite. For example, one of the cores of the NiclaV runs a computer vision algorithm on the fly (inference), while the other executes low-level operations like controlling a motor and communicating or acting as a user interface. The onboard wireless module allows the management of WiFi and Bluetooth Low Energy (BLE) connectivity simultaneously."
  },
  {
    "objectID": "niclav_sys.html#hardware",
    "href": "niclav_sys.html#hardware",
    "title": "Setup Nicla Vision",
    "section": "Hardware",
    "text": "Hardware\n\nTwo Parallel Cores\nThe central processor is the dual-core STM32H747, including a Cortex¬Æ M7 at 480 MHz and a Cortex¬Æ M4 at 240 MHz. The two cores communicate via a Remote Procedure Call mechanism that seamlessly allows calling functions on the other processor. Both processors share all the on-chip peripherals and can run:\n\nArduino sketches on top of the Arm¬Æ Mbed‚Ñ¢ OS\nNative Mbed‚Ñ¢ applications\nMicroPython / JavaScript via an interpreter\nTensorFlow‚Ñ¢ Lite\n\n\n\n\n\n\n\n\nMemory\nMemory is crucial for embedded machine learning projects. The NiclaV board can host up to 16 MB of QSPI Flash for storage. However, it is essential to consider that the MCU SRAM is the one to be used with machine learning inferences; the STM32H747 is only 1MB, shared by both processors. This MCU also has incorporated 2MB of FLASH, mainly for code storage.\n\n\nSensors\n\nCamera: A GC2145 2 MP Color CMOS Camera.\nMicrophone: The MP34DT05 is an ultra-compact, low-power, omnidirectional, digital MEMS microphone built with a capacitive sensing element and the IC interface.\n6-Axis IMU: 3D gyroscope and 3D accelerometer data from the LSM6DSOX 6-axis IMU.\nTime of Flight Sensor: The VL53L1CBV0FY Time-of-Flight sensor adds accurate and low power-ranging capabilities to the Nicla Vision. The invisible near-infrared VCSEL laser (including the analog driver) is encapsulated with receiving optics in an all-in-one small module below the camera."
  },
  {
    "objectID": "niclav_sys.html#arduino-ide-installation",
    "href": "niclav_sys.html#arduino-ide-installation",
    "title": "Setup Nicla Vision",
    "section": "Arduino IDE Installation",
    "text": "Arduino IDE Installation\nStart connecting the board (microUSB) to your computer:\n\n\n\n\n\nInstall the Mbed OS core for Nicla boards in the Arduino IDE. Having the IDE open, navigate to Tools &gt; Board &gt; Board Manager, look for Arduino Nicla Vision on the search window, and install the board.\n\n\n\n\n\nNext, go to Tools &gt; Board &gt; Arduino Mbed OS Nicla Boards and select Arduino Nicla Vision. Having your board connected to the USB, you should see the Nicla on Port and select it.\n\nOpen the Blink sketch on Examples/Basic and run it using the IDE Upload button. You should see the Built-in LED (green RGB) blinking, which means the Nicla board is correctly installed and functional!\n\n\nTesting the Microphone\nOn Arduino IDE, go to Examples &gt; PDM &gt; PDMSerialPlotter, open and run the sketch. Open the Plotter and see the audio representation from the microphone:\n\n\n\n\n\n\nVary the frequency of the sound you generate and confirm that the mic is working correctly.\n\n\n\nTesting the IMU\nBefore testing the IMU, it will be necessary to install the LSM6DSOX library. For that, go to Library Manager and look for LSM6DSOX. Install the library provided by Arduino:\n\n\n\n\n\nNext, go to Examples &gt; Arduino_LSM6DSOX &gt; SimpleAccelerometer and run the accelerometer test (you can also run Gyro and board temperature):\n\n\n\n\n\n\n\nTesting the ToF (Time of Flight) Sensor\nAs we did with IMU, it is necessary to install the VL53L1X ToF library. For that, go to Library Manager and look for VL53L1X. Install the library provided by Pololu:\n\n\n\n\n\nNext, run the sketch proximity_detection.ino:\n\n\n\n\n\nOn the Serial Monitor, you will see the distance from the camera to an object in front of it (max of 4m).\n\n\n\n\n\n\n\nTesting the Camera\nWe can also test the camera using, for example, the code provided on Examples &gt; Camera &gt; CameraCaptureRawBytes. We cannot see the image directly, but it is possible to get the raw image data generated by the camera.\nAnyway, the best test with the camera is to see a live image. For that, we will use another IDE, the OpenMV."
  },
  {
    "objectID": "niclav_sys.html#installing-the-openmv-ide",
    "href": "niclav_sys.html#installing-the-openmv-ide",
    "title": "Setup Nicla Vision",
    "section": "Installing the OpenMV IDE",
    "text": "Installing the OpenMV IDE\nOpenMV IDE is the premier integrated development environment with OpenMV Cameras like the one on the Nicla Vision. It features a powerful text editor, debug terminal, and frame buffer viewer with a histogram display. We will use MicroPython to program the camera.\nGo to the OpenMV IDE page, download the correct version for your Operating System, and follow the instructions for its installation on your computer.\n\n\n\n\n\nThe IDE should open, defaulting to the helloworld_1.py code on its Code Area. If not, you can open it from Files &gt; Examples &gt; HelloWord &gt; helloword.py\n\n\n\n\n\nAny messages sent through a serial connection (using print() or error messages) will be displayed on the Serial Terminal during run time. The image captured by a camera will be displayed in the Camera Viewer Area (or Frame Buffer) and in the Histogram area, immediately below the Camera Viewer.\nOpenMV IDE is the premier integrated development environment with OpenMV Cameras and the Arduino Pro boards. It features a powerful text editor, debug terminal, and frame buffer viewer with a histogram display. We will use MicroPython to program the Nicla Vision.\n\nBefore connecting the Nicla to the OpenMV IDE, ensure you have the latest bootloader version. Go to your Arduino IDE, select the Nicla board, and open the sketch on Examples &gt; STM_32H747_System STM_32H747_updateBootloader. Upload the code to your board. The Serial Monitor will guide you.\n\nAfter updating the bootloader, put the Nicla Vision in bootloader mode by double-pressing the reset button on the board. The built-in green LED will start fading in and out. Now return to the OpenMV IDE and click on the connect icon (Left ToolBar):\n\n\n\n\n\nA pop-up will tell you that a board in DFU mode was detected and ask how you would like to proceed. First, select Install the latest release firmware (vX.Y.Z). This action will install the latest OpenMV firmware on the Nicla Vision.\n\n\n\n\n\nYou can leave the option Erase internal file system unselected and click [OK].\nNicla‚Äôs green LED will start flashing while the OpenMV firmware is uploaded to the board, and a terminal window will then open, showing the flashing progress.\n\n\n\n\n\nWait until the green LED stops flashing and fading. When the process ends, you will see a message saying, ‚ÄúDFU firmware update complete!‚Äù. Press [OK].\n\n\n\n\n\nA green play button appears when the Nicla Vison connects to the Tool Bar.\n\n\n\n\n\nAlso, note that a drive named ‚ÄúNO NAME‚Äù will appear on your computer.:\n\n\n\n\n\nEvery time you press the [RESET] button on the board, it automatically executes the main.py script stored on it. You can load the main.py code on the IDE (File &gt; Open File...).\n\n\n\n\n\n\nThis code is the ‚ÄúBlink‚Äù code, confirming that the HW is OK.\n\nFor testing the camera, let‚Äôs run helloword_1.py. For that, select the script on File &gt; Examples &gt; HelloWorld &gt; helloword.py,\nWhen clicking the green play button, the MicroPython script (hellowolrd.py) on the Code Area will be uploaded and run on the Nicla Vision. On-Camera Viewer, you will start to see the video streaming. The Serial Monitor will show us the FPS (Frames per second), which should be around 14fps.\n\n\n\n\n\nHere is the helloworld.py script:\n# Hello World Example 2\n#\n# Welcome to the OpenMV IDE! Click on the green run arrow button below to run the script!\n\nimport sensor, image, time\n\nsensor.reset()                      # Reset and initialize the sensor.\nsensor.set_pixformat(sensor.RGB565) # Set pixel format to RGB565 (or GRAYSCALE)\nsensor.set_framesize(sensor.QVGA)   # Set frame size to QVGA (320x240)\nsensor.skip_frames(time = 2000)     # Wait for settings take effect.\nclock = time.clock()                # Create a clock object to track the FPS.\n\nwhile(True):\n    clock.tick()                    # Update the FPS clock.\n    img = sensor.snapshot()         # Take a picture and return the image.\n    print(clock.fps())\nIn GitHub, you can find the Python scripts used here.\nThe code can be split into two parts:\n\nSetup: Where the libraries are imported, initialized and the variables are defined and initiated.\nLoop: (while loop) part of the code that runs continually. The image (img variable) is captured (one frame). Each of those frames can be used for inference in Machine Learning Applications.\n\nTo interrupt the program execution, press the red [X] button.\n\nNote: OpenMV Cam runs about half as fast when connected to the IDE. The FPS should increase once disconnected.\n\nIn the GitHub, You can find other Python scripts. Try to test the onboard sensors."
  },
  {
    "objectID": "niclav_sys.html#connecting-the-nicla-vision-to-edge-impulse-studio",
    "href": "niclav_sys.html#connecting-the-nicla-vision-to-edge-impulse-studio",
    "title": "Setup Nicla Vision",
    "section": "Connecting the Nicla Vision to Edge Impulse Studio",
    "text": "Connecting the Nicla Vision to Edge Impulse Studio\nWe will need the Edge Impulse Studio later in other exercises. Edge Impulse is a leading development platform for machine learning on edge devices.\nEdge Impulse officially supports the Nicla Vision. So, for starting, please create a new project on the Studio and connect the Nicla to it. For that, follow the steps:\n\nDownload the most updated EI Firmware and unzip it.\nOpen the zip file on your computer and select the uploader corresponding to your OS:\n\n\n\n\n\n\n\nPut the Nicla-Vision on Boot Mode, pressing the reset button twice.\nExecute the specific batch code for your OS for uploading the binary arduino-nicla-vision.bin to your board.\n\nGo to your project on the Studio, and on the Data Acquisition tab, select WebUSB (1). A window will pop up; choose the option that shows that the Nicla is paired (2) and press [Connect] (3).\n\n\n\n\n\nIn the Collect Data section on the Data Acquisition tab, you can choose which sensor data to pick.\n\n\n\n\n\nFor example. IMU data:\n\n\n\n\n\nOr Image (Camera):\n\n\n\n\n\nAnd so on. You can also test an external sensor connected to the ADC (Nicla pin 0) and the other onboard sensors, such as the microphone and the ToF."
  },
  {
    "objectID": "niclav_sys.html#expanding-the-nicla-vision-board-optional",
    "href": "niclav_sys.html#expanding-the-nicla-vision-board-optional",
    "title": "Setup Nicla Vision",
    "section": "Expanding the Nicla Vision Board (optional)",
    "text": "Expanding the Nicla Vision Board (optional)\nA last item to be explored is that sometimes, during prototyping, it is essential to experiment with external sensors and devices, and an excellent expansion to the Nicla is the Arduino MKR Connector Carrier (Grove compatible).\nThe shield has 14 Grove connectors: five single analog inputs (A0-A5), one double analog input (A5/A6), five single digital I/Os (D0-D4), one double digital I/O (D5/D6), one I2C (TWI), and one UART (Serial). All connectors are 5V compatible.\n\nNote that all 17 Nicla Vision pins will be connected to the Shield Groves, but some Grove connections remain disconnected.\n\n\n\n\n\n\nThis shield is MKR compatible and can be used with the Nicla Vision and Portenta.\n\n\n\n\n\nFor example, suppose that on a TinyML project, you want to send inference results using a LoRaWAN device and add information about local luminosity. Often, with offline operations, a local low-power display such as an OLED is advised. This setup can be seen here:\n\n\n\n\n\nThe Grove Light Sensor would be connected to one of the single Analog pins (A0/PC4), the LoRaWAN device to the UART, and the OLED to the I2C connector.\nThe Nicla Pins 3 (Tx) and 4 (Rx) are connected with the Serial Shield connector. The UART communication is used with the LoRaWan device. Here is a simple code to use the UART:\n# UART Test - By: marcelo_rovai - Sat Sep 23 2023\n\nimport time\nfrom pyb import UART\nfrom pyb import LED\n\nredLED = LED(1) # built-in red LED\n\n# Init UART object.\n# Nicla Vision's UART (TX/RX pins) is on \"LP1\"\nuart = UART(\"LP1\", 9600)\n\nwhile(True):\n    uart.write(\"Hello World!\\r\\n\")\n    redLED.toggle()\n    time.sleep_ms(1000)\nTo verify that the UART is working, you should, for example, connect another device as the Arduino UNO, displaying ‚ÄúHello Word‚Äù on the Serial Monitor. Here is the code.\n\n\n\n\n\nBelow is the Hello World code to be used with the I2C OLED. The MicroPython SSD1306 OLED driver (ssd1306.py), created by Adafruit, should also be uploaded to the Nicla (the ssd1306.py script can be found in GitHub).\n# Nicla_OLED_Hello_World - By: marcelo_rovai - Sat Sep 30 2023\n\n#Save on device: MicroPython SSD1306 OLED driver, I2C and SPI interfaces created by Adafruit\nimport ssd1306\n\nfrom machine import I2C\ni2c = I2C(1)\n\noled_width = 128\noled_height = 64\noled = ssd1306.SSD1306_I2C(oled_width, oled_height, i2c)\n\noled.text('Hello, World', 10, 10)\noled.show()\nFinally, here is a simple script to read the ADC value on pin ‚ÄúPC4‚Äù (Nicla pin A0):\n\n# Light Sensor (A0) - By: marcelo_rovai - Wed Oct 4 2023\n\nimport pyb\nfrom time import sleep\n\nadc = pyb.ADC(pyb.Pin(\"PC4\"))     # create an analog object from a pin\nval = adc.read()                  # read an analog value\n\nwhile (True):\n\n    val = adc.read()  \n    print (\"Light={}\".format (val))\n    sleep (1)\nThe ADC can be used for other sensor variables, such as Temperature.\n\nNote that the above scripts (downloaded from Github) introduce only how to connect external devices with the Nicla Vision board using MicroPython."
  },
  {
    "objectID": "niclav_sys.html#conclusion",
    "href": "niclav_sys.html#conclusion",
    "title": "Setup Nicla Vision",
    "section": "Conclusion",
    "text": "Conclusion\nThe Arduino Nicla Vision is an excellent tiny device for industrial and professional uses! However, it is powerful, trustworthy, low power, and has suitable sensors for the most common embedded machine learning applications such as vision, movement, sensor fusion, and sound.\n\nOn the GitHub repository, you will find the last version of all the codes used or commented on in this hands-on exercise."
  },
  {
    "objectID": "image_classification.html#introduction",
    "href": "image_classification.html#introduction",
    "title": "CV on Nicla Vision",
    "section": "Introduction",
    "text": "Introduction\nAs we initiate our studies into embedded machine learning or tinyML, it‚Äôs impossible to overlook the transformative impact of Computer Vision (CV) and Artificial Intelligence (AI) in our lives. These two intertwined disciplines redefine what machines can perceive and accomplish, from autonomous vehicles and robotics to healthcare and surveillance.\nMore and more, we are facing an artificial intelligence (AI) revolution where, as stated by Gartner, Edge AI has a very high impact potential, and it is for now!\n\n\n\n\n\nIn the ‚Äúbullseye‚Äù of the Radar is the Edge Computer Vision, and when we talk about Machine Learning (ML) applied to vision, the first thing that comes to mind is Image Classification, a kind of ML ‚ÄúHello World‚Äù!\nThis exercise will explore a computer vision project utilizing Convolutional Neural Networks (CNNs) for real-time image classification. Leveraging TensorFlow‚Äôs robust ecosystem, we‚Äôll implement a pre-trained MobileNet model and adapt it for edge deployment. The focus will be on optimizing the model to run efficiently on resource-constrained hardware without sacrificing accuracy.\nWe‚Äôll employ techniques like quantization and pruning to reduce the computational load. By the end of this tutorial, you‚Äôll have a working prototype capable of classifying images in real-time, all running on a low-power embedded system based on the Arduino Nicla Vision board."
  },
  {
    "objectID": "image_classification.html#computer-vision",
    "href": "image_classification.html#computer-vision",
    "title": "CV on Nicla Vision",
    "section": "Computer Vision",
    "text": "Computer Vision\nAt its core, computer vision aims to enable machines to interpret and make decisions based on visual data from the world, essentially mimicking the capability of the human optical system. Conversely, AI is a broader field encompassing machine learning, natural language processing, and robotics, among other technologies. When you bring AI algorithms into computer vision projects, you supercharge the system‚Äôs ability to understand, interpret, and react to visual stimuli.\nWhen discussing Computer Vision projects applied to embedded devices, the most common applications that come to mind are Image Classification and Object Detection.\n\n\n\n\n\nBoth models can be implemented on tiny devices like the Arduino Nicla Vision and used on real projects. In this chapter, we will cover Image Classification."
  },
  {
    "objectID": "image_classification.html#image-classification-project-goal",
    "href": "image_classification.html#image-classification-project-goal",
    "title": "CV on Nicla Vision",
    "section": "Image Classification Project Goal",
    "text": "Image Classification Project Goal\nThe first step in any ML project is to define the goal. In this case, it is to detect and classify two specific objects present in one image. For this project, we will use two small toys: a robot and a small Brazilian parrot (named Periquito). Also, we will collect images of a background where those two objects are absent."
  },
  {
    "objectID": "image_classification.html#data-collection",
    "href": "image_classification.html#data-collection",
    "title": "CV on Nicla Vision",
    "section": "Data Collection",
    "text": "Data Collection\nOnce you have defined your Machine Learning project goal, the next and most crucial step is the dataset collection. You can use the Edge Impulse Studio, the OpenMV IDE we installed, or even your phone for the image capture. Here, we will use the OpenMV IDE for that.\n\nCollecting Dataset with OpenMV IDE\nFirst, create in your computer a folder where your data will be saved, for example, ‚Äúdata.‚Äù Next, on the OpenMV IDE, go to Tools &gt; Dataset Editor and select New Dataset to start the dataset collection:\n\n\n\n\n\nThe IDE will ask you to open the file where your data will be saved and choose the ‚Äúdata‚Äù folder that was created. Note that new icons will appear on the Left panel.\n\n\n\n\n\nUsing the upper icon (1), enter with the first class name, for example, ‚Äúperiquito‚Äù:\n\n\n\n\n\nRunning the dataset_capture_script.py and clicking on the camera icon (2), will start capturing images:\n\n\n\n\n\nRepeat the same procedure with the other classes\n\n\n\n\n\n\nWe suggest around 60 images from each category. Try to capture different angles, backgrounds, and light conditions.\n\nThe stored images use a QVGA frame size of 320x240 and the RGB565 (color pixel format).\nAfter capturing your dataset, close the Dataset Editor Tool on the Tools &gt; Dataset Editor.\nOn your computer, you will end with a dataset that contains three classes: periquito, robot, and background.\n\n\n\n\n\nYou should return to Edge Impulse Studio and upload the dataset to your project."
  },
  {
    "objectID": "image_classification.html#training-the-model-with-edge-impulse-studio",
    "href": "image_classification.html#training-the-model-with-edge-impulse-studio",
    "title": "CV on Nicla Vision",
    "section": "Training the model with Edge Impulse Studio",
    "text": "Training the model with Edge Impulse Studio\nWe will use the Edge Impulse Studio for training our model. Enter your account credentials and create a new project:\n\n\n\n\n\n\nHere, you can clone a similar project: NICLA-Vision_Image_Classification."
  },
  {
    "objectID": "image_classification.html#dataset",
    "href": "image_classification.html#dataset",
    "title": "CV on Nicla Vision",
    "section": "Dataset",
    "text": "Dataset\nUsing the EI Studio (or Studio), we will go over four main steps to have our model ready for use on the Nicla Vision board: Dataset, Impulse, Tests, and Deploy (on the Edge Device, in this case, the NiclaV).\n\n\n\n\n\nRegarding the Dataset, it is essential to point out that our Original Dataset, captured with the OpenMV IDE, will be split into Training, Validation, and Test. The Test Set will be divided from the beginning, and a part will reserved to be used only in the Test phase after training. The Validation Set will be used during training.\n\n\n\n\n\nOn Studio, go to the Data acquisition tab, and on the UPLOAD DATA section, upload the chosen categories files from your computer:\n\n\n\n\n\nLeave to the Studio the splitting of the original dataset into train and test and choose the label about that specific data:\n\n\n\n\n\nRepeat the procedure for all three classes. At the end, you should see your ‚Äúraw data‚Äù in the Studio:\n\n\n\n\n\nThe Studio allows you to explore your data, showing a complete view of all the data in your project. You can clear, inspect, or change labels by clicking on individual data items. In our case, a very simple project, the data seems OK."
  },
  {
    "objectID": "image_classification.html#the-impulse-design",
    "href": "image_classification.html#the-impulse-design",
    "title": "CV on Nicla Vision",
    "section": "The Impulse Design",
    "text": "The Impulse Design\nIn this phase, we should define how to:\n\nPre-process our data, which consists of resizing the individual images and determining the color depth to use (be it RGB or Grayscale) and\nSpecify a Model, in this case, it will be the Transfer Learning (Images) to fine-tune a pre-trained MobileNet V2 image classification model on our data. This method performs well even with relatively small image datasets (around 150 images in our case).\n\n\n\n\n\n\nTransfer Learning with MobileNet offers a streamlined approach to model training, which is especially beneficial for resource-constrained environments and projects with limited labeled data. MobileNet, known for its lightweight architecture, is a pre-trained model that has already learned valuable features from a large dataset (ImageNet).\n\n\n\n\n\nBy leveraging these learned features, you can train a new model for your specific task with fewer data and computational resources and yet achieve competitive accuracy.\n\n\n\n\n\nThis approach significantly reduces training time and computational cost, making it ideal for quick prototyping and deployment on embedded devices where efficiency is paramount.\nGo to the Impulse Design Tab and create the impulse, defining an image size of 96x96 and squashing them (squared form, without cropping). Select Image and Transfer Learning blocks. Save the Impulse.\n\n\n\n\n\n\nImage Pre-Processing\nAll the input QVGA/RGB565 images will be converted to 27,640 features (96x96x3).\n\n\n\n\n\nPress [Save parameters] and Generate all features:\n\n\n\n\n\n\n\nModel Design\nIn 2007, Google introduced MobileNetV1, a family of general-purpose computer vision neural networks designed with mobile devices in mind to support classification, detection, and more. MobileNets are small, low-latency, low-power models parameterized to meet the resource constraints of various use cases. in 2018, Google launched MobileNetV2: Inverted Residuals and Linear Bottlenecks.\nMobileNet V1 and MobileNet V2 aim at mobile efficiency and embedded vision applications but differ in architectural complexity and performance. While both use depthwise separable convolutions to reduce the computational cost, MobileNet V2 introduces Inverted Residual Blocks and Linear Bottlenecks to enhance performance. These new features allow V2 to capture more complex features using fewer parameters, making it computationally more efficient and generally more accurate than its predecessor. Additionally, V2 employs a non-linear activation in the intermediate expansion layer. It still uses a linear activation for the bottleneck layer, a design choice found to preserve important information through the network. MobileNet V2 offers an optimized architecture for higher accuracy and efficiency and will be used in this project.\nAlthough the base MobileNet architecture is already tiny and has low latency, many times, a specific use case or application may require the model to be even smaller and faster. MobileNets introduces a straightforward parameter Œ± (alpha) called width multiplier to construct these smaller, less computationally expensive models. The role of the width multiplier Œ± is that of thinning a network uniformly at each layer.\nEdge Impulse Studio can use both MobileNetV1 (96x96 images) and V2 (96x96 or 160x160 images), with several different Œ± values (from 0.05 to 1.0). For example, you will get the highest accuracy with V2, 160x160 images, and Œ±=1.0. Of course, there is a trade-off. The higher the accuracy, the more memory (around 1.3MB RAM and 2.6MB ROM) will be needed to run the model, implying more latency. The smaller footprint will be obtained at the other extreme with MobileNetV1 and Œ±=0.10 (around 53.2K RAM and 101K ROM).\n\n\n\n\n\nWe will use MobileNetV2 96x96 0.1 for this project, with an estimated memory cost of 265.3 KB in RAM. This model should be OK for the Nicla Vision with 1MB of SRAM. On the Transfer Learning Tab, select this model:"
  },
  {
    "objectID": "image_classification.html#model-training",
    "href": "image_classification.html#model-training",
    "title": "CV on Nicla Vision",
    "section": "Model Training",
    "text": "Model Training\nAnother valuable technique to be used with Deep Learning is Data Augmentation. Data augmentation is a method to improve the accuracy of machine learning models by creating additional artificial data. A data augmentation system makes small, random changes to your training data during the training process (such as flipping, cropping, or rotating the images).\nLooking under the hood, here you can see how Edge Impulse implements a data Augmentation policy on your data:\n# Implements the data augmentation policy\ndef augment_image(image, label):\n    # Flips the image randomly\n    image = tf.image.random_flip_left_right(image)\n\n    # Increase the image size, then randomly crop it down to\n    # the original dimensions\n    resize_factor = random.uniform(1, 1.2)\n    new_height = math.floor(resize_factor * INPUT_SHAPE[0])\n    new_width = math.floor(resize_factor * INPUT_SHAPE[1])\n    image = tf.image.resize_with_crop_or_pad(image, new_height, new_width)\n    image = tf.image.random_crop(image, size=INPUT_SHAPE)\n\n    # Vary the brightness of the image\n    image = tf.image.random_brightness(image, max_delta=0.2)\n\n    return image, label\nExposure to these variations during training can help prevent your model from taking shortcuts by ‚Äúmemorizing‚Äù superficial clues in your training data, meaning it may better reflect the deep underlying patterns in your dataset.\nThe final layer of our model will have 12 neurons with a 15% dropout for overfitting prevention. Here is the Training result:\n\n\n\n\n\nThe result is excellent, with 77ms of latency, which should result in 13fps (frames per second) during inference."
  },
  {
    "objectID": "image_classification.html#model-testing",
    "href": "image_classification.html#model-testing",
    "title": "CV on Nicla Vision",
    "section": "Model Testing",
    "text": "Model Testing\n\n\n\n\n\nNow, you should take the data set aside at the start of the project and run the trained model using it as input:\n\n\n\n\n\nThe result is, again, excellent."
  },
  {
    "objectID": "image_classification.html#deploying-the-model",
    "href": "image_classification.html#deploying-the-model",
    "title": "CV on Nicla Vision",
    "section": "Deploying the model",
    "text": "Deploying the model\nAt this point, we can deploy the trained model as.tflite and use the OpenMV IDE to run it using MicroPython, or we can deploy it as a C/C++ or an Arduino library.\n\n\n\n\n\n\nArduino Library\nFirst, Let‚Äôs deploy it as an Arduino Library:\n\n\n\n\n\nYou should install the library as.zip on the Arduino IDE and run the sketch nicla_vision_camera.ino available in Examples under your library name.\n\nNote that Arduino Nicla Vision has, by default, 512KB of RAM allocated for the M7 core and an additional 244KB on the M4 address space. In the code, this allocation was changed to 288 kB to guarantee that the model will run on the device (malloc_addblock((void*)0x30000000, 288 * 1024);).\n\nThe result is good, with 86ms of measured latency.\n\n\n\n\n\nHere is a short video showing the inference results: \n\n\nOpenMV\nIt is possible to deploy the trained model to be used with OpenMV in two ways: as a library and as a firmware.\nThree files are generated as a library: the trained.tflite model, a list with labels, and a simple MicroPython script that can make inferences using the model.\n\n\n\n\n\nRunning this model as a .tflite directly in the Nicla was impossible. So, we can sacrifice the accuracy using a smaller model or deploy the model as an OpenMV Firmware (FW). Choosing FW, the Edge Impulse Studio generates optimized models, libraries, and frameworks needed to make the inference. Let‚Äôs explore this option.\nSelect OpenMV Firmware on the Deploy Tab and press [Build].\n\n\n\n\n\nOn your computer, you will find a ZIP file. Open it:\n\n\n\n\n\nUse the Bootloader tool on the OpenMV IDE to load the FW on your board:\n\n\n\n\n\nSelect the appropriate file (.bin for Nicla-Vision):\n\n\n\n\n\nAfter the download is finished, press OK:\n\n\n\n\n\nIf a message says that the FW is outdated, DO NOT UPGRADE. Select [NO].\n\n\n\n\n\nNow, open the script ei_image_classification.py that was downloaded from the Studio and the.bin file for the Nicla.\n\n\n\n\n\nRun it. Pointing the camera to the objects we want to classify, the inference result will be displayed on the Serial Terminal.\n\n\n\n\n\n\nChanging the Code to add labels\nThe code provided by Edge Impulse can be modified so that we can see, for test reasons, the inference result directly on the image displayed on the OpenMV IDE.\nUpload the code from GitHub, or modify it as below:\n# Marcelo Rovai - NICLA Vision - Image Classification\n# Adapted from Edge Impulse - OpenMV Image Classification Example\n# @24Aug23\n\nimport sensor, image, time, os, tf, uos, gc\n\nsensor.reset()                         # Reset and initialize the sensor.\nsensor.set_pixformat(sensor.RGB565)    # Set pxl fmt to RGB565 (or GRAYSCALE)\nsensor.set_framesize(sensor.QVGA)      # Set frame size to QVGA (320x240)\nsensor.set_windowing((240, 240))       # Set 240x240 window.\nsensor.skip_frames(time=2000)          # Let the camera adjust.\n\nnet = None\nlabels = None\n\ntry:\n    # Load built in model\n    labels, net = tf.load_builtin_model('trained')\nexcept Exception as e:\n    raise Exception(e)\n\nclock = time.clock()\nwhile(True):\n    clock.tick()  # Starts tracking elapsed time.\n\n    img = sensor.snapshot()\n\n    # default settings just do one detection\n    for obj in net.classify(img, \n                            min_scale=1.0, \n                            scale_mul=0.8, \n                            x_overlap=0.5, \n                            y_overlap=0.5):\n        fps = clock.fps()\n        lat = clock.avg()\n\n        print(\"**********\\nPrediction:\")\n        img.draw_rectangle(obj.rect())\n        # This combines the labels and confidence values into a list of tuples\n        predictions_list = list(zip(labels, obj.output()))\n\n        max_val = predictions_list[0][1]\n        max_lbl = 'background'\n        for i in range(len(predictions_list)):\n            val = predictions_list[i][1]\n            lbl = predictions_list[i][0]\n\n            if val &gt; max_val:\n                max_val = val\n                max_lbl = lbl\n\n    # Print label with the highest probability\n    if max_val &lt; 0.5:\n        max_lbl = 'uncertain'\n    print(\"{} with a prob of {:.2f}\".format(max_lbl, max_val))\n    print(\"FPS: {:.2f} fps ==&gt; latency: {:.0f} ms\".format(fps, lat))\n\n    # Draw label with highest probability to image viewer\n    img.draw_string(\n        10, 10,\n        max_lbl + \"\\n{:.2f}\".format(max_val),\n        mono_space = False,\n        scale=2\n        )\nHere you can see the result:\n\n\n\n\n\nNote that the latency (136 ms) is almost double of what we got directly with the Arduino IDE. This is because we are using the IDE as an interface and also the time to wait for the camera to be ready. If we start the clock just before the inference:\n\n\n\n\n\nThe latency will drop to only 71 ms.\n\n\n\n\n\n\nThe NiclaV runs about half as fast when connected to the IDE. The FPS should increase once disconnected.\n\n\n\nPost-Processing with LEDs\nWhen working with embedded machine learning, we are looking for devices that can continually proceed with the inference and result, taking some action directly on the physical world and not displaying the result on a connected computer. To simulate this, we will light up a different LED for each possible inference result.\n\n\n\n\n\nTo accomplish that, we should upload the code from GitHub or change the last code to include the LEDs:\n# Marcelo Rovai - NICLA Vision - Image Classification with LEDs\n# Adapted from Edge Impulse - OpenMV Image Classification Example\n# @24Aug23\n\nimport sensor, image, time, os, tf, uos, gc, pyb\n\nledRed = pyb.LED(1)\nledGre = pyb.LED(2)\nledBlu = pyb.LED(3)\n\nsensor.reset()                         # Reset and initialize the sensor.\nsensor.set_pixformat(sensor.RGB565)    # Set pixl fmt to RGB565 (or GRAYSCALE)\nsensor.set_framesize(sensor.QVGA)      # Set frame size to QVGA (320x240)\nsensor.set_windowing((240, 240))       # Set 240x240 window.\nsensor.skip_frames(time=2000)          # Let the camera adjust.\n\nnet = None\nlabels = None\n\nledRed.off()\nledGre.off()\nledBlu.off()\n\ntry:\n    # Load built in model\n    labels, net = tf.load_builtin_model('trained')\nexcept Exception as e:\n    raise Exception(e)\n\nclock = time.clock()\n\n\ndef setLEDs(max_lbl):\n\n    if max_lbl == 'uncertain':\n        ledRed.on()\n        ledGre.off()\n        ledBlu.off()\n\n    if max_lbl == 'periquito':\n        ledRed.off()\n        ledGre.on()\n        ledBlu.off()\n\n    if max_lbl == 'robot':\n        ledRed.off()\n        ledGre.off()\n        ledBlu.on()\n\n    if max_lbl == 'background':\n        ledRed.off()\n        ledGre.off()\n        ledBlu.off()\n\n\nwhile(True):\n    img = sensor.snapshot()\n    clock.tick()  # Starts tracking elapsed time.\n\n    # default settings just do one detection.\n    for obj in net.classify(img, \n                            min_scale=1.0, \n                            scale_mul=0.8, \n                            x_overlap=0.5, \n                            y_overlap=0.5):\n        fps = clock.fps()\n        lat = clock.avg()\n\n        print(\"**********\\nPrediction:\")\n        img.draw_rectangle(obj.rect())\n        # This combines the labels and confidence values into a list of tuples\n        predictions_list = list(zip(labels, obj.output()))\n\n        max_val = predictions_list[0][1]\n        max_lbl = 'background'\n        for i in range(len(predictions_list)):\n            val = predictions_list[i][1]\n            lbl = predictions_list[i][0]\n\n            if val &gt; max_val:\n                max_val = val\n                max_lbl = lbl\n\n    # Print label and turn on LED with the highest probability\n    if max_val &lt; 0.8:\n        max_lbl = 'uncertain'\n\n    setLEDs(max_lbl)\n\n    print(\"{} with a prob of {:.2f}\".format(max_lbl, max_val))\n    print(\"FPS: {:.2f} fps ==&gt; latency: {:.0f} ms\".format(fps, lat))\n\n    # Draw label with highest probability to image viewer\n    img.draw_string(\n        10, 10,\n        max_lbl + \"\\n{:.2f}\".format(max_val),\n        mono_space = False,\n        scale=2\n        )\nNow, each time that a class scores a result greater than 0.8, the correspondent LED will be lit:\n\nLed Red 0n: Uncertain (no class is over 0.8)\nLed Green 0n: Periquito &gt; 0.8\nLed Blue 0n: Robot &gt; 0.8\nAll LEDs Off: Background &gt; 0.8\n\nHere is the result:\n\n\n\n\n\nIn more detail"
  },
  {
    "objectID": "image_classification.html#image-classification-non-official-benchmark",
    "href": "image_classification.html#image-classification-non-official-benchmark",
    "title": "CV on Nicla Vision",
    "section": "Image Classification (non-official) Benchmark",
    "text": "Image Classification (non-official) Benchmark\nSeveral development boards can be used for embedded machine learning (tinyML), and the most common ones for Computer Vision applications (consuming low energy), are the ESP32 CAM, the Seeed XIAO ESP32S3 Sense, the Arduino Nicla Vison, and the Arduino Portenta.\n\n\n\n\n\nCatching the opportunity, the same trained model was deployed on the ESP-CAM, the XIAO, and the Portenta (in this one, the model was trained again, using grayscaled images to be compatible with its camera). Here is the result, deploying the models as Arduino‚Äôs Library:"
  },
  {
    "objectID": "image_classification.html#conclusion",
    "href": "image_classification.html#conclusion",
    "title": "CV on Nicla Vision",
    "section": "Conclusion",
    "text": "Conclusion\nBefore we finish, consider that Computer Vision is more than just image classification. For example, you can develop Edge Machine Learning projects around vision in several areas, such as:\n\nAutonomous Vehicles: Use sensor fusion, lidar data, and computer vision algorithms to navigate and make decisions.\nHealthcare: Automated diagnosis of diseases through MRI, X-ray, and CT scan image analysis\nRetail: Automated checkout systems that identify products as they pass through a scanner.\nSecurity and Surveillance: Facial recognition, anomaly detection, and object tracking in real-time video feeds.\nAugmented Reality: Object detection and classification to overlay digital information in the real world.\nIndustrial Automation: Visual inspection of products, predictive maintenance, and robot and drone guidance.\nAgriculture: Drone-based crop monitoring and automated harvesting.\nNatural Language Processing: Image captioning and visual question answering.\nGesture Recognition: For gaming, sign language translation, and human-machine interaction.\nContent Recommendation: Image-based recommendation systems in e-commerce."
  },
  {
    "objectID": "object_detection_fomo.html#introduction",
    "href": "object_detection_fomo.html#introduction",
    "title": "Object Detection",
    "section": "Introduction",
    "text": "Introduction\nThis is a continuation of CV on Nicla Vision, now exploring Object Detection on microcontrollers.\n\n\n\n\n\n\nObject Detection versus Image Classification\nThe main task with Image Classification models is to produce a list of the most probable object categories present on an image, for example, to identify a tabby cat just after his dinner:\n\n\n\n\n\nBut what happens when the cat jumps near the wine glass? The model still only recognizes the predominant category on the image, the tabby cat:\n\n\n\n\n\nAnd what happens if there is not a dominant category on the image?\n\n\n\n\n\nThe model identifies the above image completely wrong as an ‚Äúashcan,‚Äù possibly due to the color tonalities.\n\nThe model used in all previous examples is the MobileNet, trained with a large dataset, the ImageNet.\n\nTo solve this issue, we need another type of model, where not only multiple categories (or labels) can be found but also where the objects are located on a given image.\nAs we can imagine, such models are much more complicated and bigger, for example, the MobileNetV2 SSD FPN-Lite 320x320, trained with the COCO dataset. This pre-trained object detection model is designed to locate up to 10 objects within an image, outputting a bounding box for each object detected. The below image is the result of such a model running on a Raspberry Pi:\n\n\n\n\n\nThose models used for Object detection (such as the MobileNet SSD or YOLO) usually have several MB in size, which is OK for use with Raspberry Pi but unsuitable for use with embedded devices, where the RAM usually is lower than 1M Bytes.\n\n\nAn innovative solution for Object Detection: FOMO\nEdge Impulse launched in 2022, FOMO (Faster Objects, More Objects), a novel solution to perform object detection on embedded devices, not only on the Nicla Vision (Cortex M7) but also on Cortex M4F CPUs (Arduino Nano33 and OpenMV M4 series) as well the Espressif ESP32 devices (ESP-CAM and XIAO ESP32S3 Sense).\nIn this Hands-On exercise, we will explore using FOMO with Object Detection, not entering many details about the model itself. To understand more about how the model works, you can go into the official FOMO announcement by Edge Impulse, where Louis Moreau and Mat Kelcey explain in detail how it works."
  },
  {
    "objectID": "object_detection_fomo.html#the-object-detection-project-goal",
    "href": "object_detection_fomo.html#the-object-detection-project-goal",
    "title": "Object Detection",
    "section": "The Object Detection Project Goal",
    "text": "The Object Detection Project Goal\nAll Machine Learning projects need to start with a detailed goal. Let‚Äôs assume we are in an industrial facility and must sort and count wheels and special boxes.\n\n\n\n\n\nIn other words, we should perform a multi-label classification, where each image can have three classes:\n\nBackground (No objects)\nBox\nWheel\n\nHere are some not labeled image samples that we should use to detect the objects (wheels and boxes):\n\n\n\n\n\nWe are interested in which object is in the image, its location (centroid), and how many we can find on it. The object‚Äôs size is not detected with FOMO, as with MobileNet SSD or YOLO, where the Bounding Box is one of the model outputs.\nWe will develop the project using the Nicla Vision for image capture and model inference. The ML project will be developed using the Edge Impulse Studio. But before starting the object detection project in the Studio, let‚Äôs create a raw dataset (not labeled) with images that contain the objects to be detected."
  },
  {
    "objectID": "object_detection_fomo.html#data-collection",
    "href": "object_detection_fomo.html#data-collection",
    "title": "Object Detection",
    "section": "Data Collection",
    "text": "Data Collection\nWe can use the Edge Impulse Studio, the OpenMV IDE, your phone, or other devices for the image capture. Here, we will use again the OpenMV IDE for our purpose.\n\nCollecting Dataset with OpenMV IDE\nFirst, create in your computer a folder where your data will be saved, for example, ‚Äúdata.‚Äù Next, on the OpenMV IDE, go to Tools &gt; Dataset Editor and select New Dataset to start the dataset collection:\n\n\n\n\n\nEdge impulse suggests that the objects should be of similar size and not overlapping for better performance. This is OK in an industrial facility, where the camera should be fixed, keeping the same distance from the objects to be detected. Despite that, we will also try with mixed sizes and positions to see the result.\n\nWe will not create separate folders for our images because each contains multiple labels.\n\nConnect the Nicla Vision to the OpenMV IDE and run the dataset_capture_script.py. Clicking on the Capture Image button will start capturing images:\n\n\n\n\n\nWe suggest around 50 images mixing the objects and varying the number of each appearing on the scene. Try to capture different angles, backgrounds, and light conditions.\n\nThe stored images use a QVGA frame size 320x240 and RGB565 (color pixel format).\n\nAfter capturing your dataset, close the Dataset Editor Tool on the Tools &gt; Dataset Editor."
  },
  {
    "objectID": "object_detection_fomo.html#edge-impulse-studio",
    "href": "object_detection_fomo.html#edge-impulse-studio",
    "title": "Object Detection",
    "section": "Edge Impulse Studio",
    "text": "Edge Impulse Studio\n\nSetup the project\nGo to Edge Impulse Studio, enter your credentials at Login (or create an account), and start a new project.\n\n\n\n\n\n\nHere, you can clone the project developed for this hands-on: NICLA_Vision_Object_Detection.\n\nOn your Project Dashboard, go down and on Project info and select Bounding boxes (object detection) and Nicla Vision as your Target Device:\n\n\n\n\n\n\n\nUploading the unlabeled data\nOn Studio, go to the Data acquisition tab, and on the UPLOAD DATA section, upload from your computer files captured.\n\n\n\n\n\n\nYou can leave for the Studio to split your data automatically between Train and Test or do it manually.\n\n\n\n\n\n\nAll the not labeled images (51) were uploaded but they still need to be labeled appropriately before using them as a dataset in the project. The Studio has a tool for that purpose, which you can find in the link Labeling queue (51).\nThere are two ways you can use to perform AI-assisted labeling on the Edge Impulse Studio (free version):\n\nUsing yolov5\nTracking objects between frames\n\n\nEdge Impulse launched an auto-labeling feature for Enterprise customers, easing labeling tasks in object detection projects.\n\nOrdinary objects can quickly be identified and labeled using an existing library of pre-trained object detection models from YOLOv5 (trained with the COCO dataset). But since, in our case, the objects are not part of COCO datasets, we should select the option of tracking objects. With this option, once you draw bounding boxes and label the images in one frame, the objects will be tracked automatically from frame to frame, partially labeling the new ones (not all are correctly labeled).\n\nYou can use the EI uploader to import your data if you already have a labeled dataset containing bounding boxes.\n\n\n\nLabeling the Dataset\nStarting with the first image of your unlabeled data, use your mouse to drag a box around an object to add a label. Then click Save labels to advance to the next item.\n\n\n\n\n\nContinue with this process until the queue is empty. At the end, all images should have the objects labeled as those samples below:\n\n\n\n\n\nNext, review the labeled samples on the Data acquisition tab. If one of the labels was wrong, you can edit it using the three dots menu after the sample name:\n\n\n\n\n\nYou will be guided to replace the wrong label, correcting the dataset."
  },
  {
    "objectID": "object_detection_fomo.html#the-impulse-design",
    "href": "object_detection_fomo.html#the-impulse-design",
    "title": "Object Detection",
    "section": "The Impulse Design",
    "text": "The Impulse Design\nIn this phase, you should define how to:\n\nPre-processing consists of resizing the individual images from 320 x 240 to 96 x 96 and squashing them (squared form, without cropping). Afterwards, the images are converted from RGB to Grayscale.\nDesign a Model, in this case, ‚ÄúObject Detection.‚Äù\n\n\n\n\n\n\n\nPreprocessing all dataset\nIn this section, select Color depth as Grayscale, which is suitable for use with FOMO models and Save parameters.\n\n\n\n\n\nThe Studio moves automatically to the next section, Generate features, where all samples will be pre-processed, resulting in a dataset with individual 96x96x1 images or 9,216 features.\n\n\n\n\n\nThe feature explorer shows that all samples evidence a good separation after the feature generation.\n\nOne of the samples (46) apparently is in the wrong space, but clicking on it can confirm that the labeling is correct."
  },
  {
    "objectID": "object_detection_fomo.html#model-design-training-and-test",
    "href": "object_detection_fomo.html#model-design-training-and-test",
    "title": "Object Detection",
    "section": "Model Design, Training, and Test",
    "text": "Model Design, Training, and Test\nWe will use FOMO, an object detection model based on MobileNetV2 (alpha 0.35) designed to coarsely segment an image into a grid of background vs objects of interest (here, boxes and wheels).\nFOMO is an innovative machine learning model for object detection, which can use up to 30 times less energy and memory than traditional models like Mobilenet SSD and YOLOv5. FOMO can operate on microcontrollers with less than 200 KB of RAM. The main reason this is possible is that while other models calculate the object‚Äôs size by drawing a square around it (bounding box), FOMO ignores the size of the image, providing only the information about where the object is located in the image, by means of its centroid coordinates.\nHow FOMO works?\nFOMO takes the image in grayscale and divides it into blocks of pixels using a factor of 8. For the input of 96x96, the grid would be 12x12 (96/8=12). Next, FOMO will run a classifier through each pixel block to calculate the probability that there is a box or a wheel in each of them and, subsequently, determine the regions which have the highest probability of containing the object (If a pixel block has no objects, it will be classified as background). From the overlap of the final region, the FOMO provides the coordinates (related to the image dimensions) of the centroid of this region.\n\n\n\n\n\nFor training, we should select a pre-trained model. Let‚Äôs use the FOMO (Faster Objects, More Objects) MobileNetV2 0.35`. This model uses around 250KB RAM and 80KB of ROM (Flash), which suits well with our board since it has 1MB of RAM and ROM.\n\n\n\n\n\nRegarding the training hyper-parameters, the model will be trained with:\n\nEpochs: 60,\nBatch size: 32\nLearning Rate: 0.001.\n\nFor validation during training, 20% of the dataset (validation_dataset) will be spared. For the remaining 80% (train_dataset), we will apply Data Augmentation, which will randomly flip, change the size and brightness of the image, and crop them, artificially increasing the number of samples on the dataset for training.\nAs a result, the model ends with practically 1.00 in the F1 score, with a similar result when using the Test data.\n\nNote that FOMO automatically added a 3rd label background to the two previously defined (box and wheel).\n\n\n\n\n\n\n\nIn object detection tasks, accuracy is generally not the primary evaluation metric. Object detection involves classifying objects and providing bounding boxes around them, making it a more complex problem than simple classification. The issue is that we do not have the bounding box, only the centroids. In short, using accuracy as a metric could be misleading and may not provide a complete understanding of how well the model is performing. Because of that, we will use the F1 score.\n\n\nTest model with ‚ÄúLive Classification‚Äù\nSince Edge Impulse officially supports the Nicla Vision, let‚Äôs connect it to the Studio. For that, follow the steps:\n\nDownload the last EI Firmware and unzip it.\nOpen the zip file on your computer and select the uploader related to your OS:\n\n\n\n\n\n\n\nPut the Nicla-Vision on Boot Mode, pressing the reset button twice.\nExecute the specific batch code for your OS for uploading the binary (arduino-nicla-vision.bin) to your board.\n\nGo to Live classification section at EI Studio, and using webUSB, connect your Nicla Vision:\n\n\n\n\n\nOnce connected, you can use the Nicla to capture actual images to be tested by the trained model on Edge Impulse Studio.\n\n\n\n\n\nOne thing to be noted is that the model can produce false positives and negatives. This can be minimized by defining a proper Confidence Threshold (use the Three dots menu for the set-up). Try with 0.8 or more."
  },
  {
    "objectID": "object_detection_fomo.html#deploying-the-model",
    "href": "object_detection_fomo.html#deploying-the-model",
    "title": "Object Detection",
    "section": "Deploying the Model",
    "text": "Deploying the Model\nSelect OpenMV Firmware on the Deploy Tab and press [Build].\n\n\n\n\n\nWhen you try to connect the Nicla with the OpenMV IDE again, it will try to update its FW. Choose the option Load a specific firmware instead.\n\n\n\n\n\nYou will find a ZIP file on your computer from the Studio. Open it:\n\n\n\n\n\nLoad the .bin file to your board:\n\n\n\n\n\nAfter the download is finished, a pop-up message will be displayed. Press OK, and open the script ei_object_detection.py downloaded from the Studio.\nBefore running the script, let‚Äôs change a few lines. Note that you can leave the window definition as 240 x 240 and the camera capturing images as QVGA/RGB. The captured image will be pre-processed by the FW deployed from Edge Impulse\n# Edge Impulse - OpenMV Object Detection Example\n\nimport sensor, image, time, os, tf, math, uos, gc\n\nsensor.reset()                         # Reset and initialize the sensor.\nsensor.set_pixformat(sensor.RGB565)    # Set pixel format to RGB565 (or GRAYSCALE)\nsensor.set_framesize(sensor.QVGA)      # Set frame size to QVGA (320x240)\nsensor.set_windowing((240, 240))       # Set 240x240 window.\nsensor.skip_frames(time=2000)          # Let the camera adjust.\n\nnet = None\nlabels = None\nRedefine the minimum confidence, for example, to 0.8 to minimize false positives and negatives.\nmin_confidence = 0.8\nChange if necessary, the color of the circles that will be used to display the detected object‚Äôs centroid for a better contrast.\ntry:\n    # Load built in model\n    labels, net = tf.load_builtin_model('trained')\nexcept Exception as e:\n    raise Exception(e)\n\ncolors = [ # Add more colors if you are detecting more than 7 types of classes at once.\n    (255, 255,   0), # background: yellow (not used)\n    (  0, 255,   0), # cube: green\n    (255,   0,   0), # wheel: red\n    (  0,   0, 255), # not used\n    (255,   0, 255), # not used\n    (  0, 255, 255), # not used\n    (255, 255, 255), # not used\n]\nKeep the remaining code as it is and press the green Play button to run the code:\n\n\n\n\n\nOn the camera view, we can see the objects with their centroids marked with 12 pixel-fixed circles (each circle has a distinct color, depending on its class). On the Serial Terminal, the model shows the labels detected and their position on the image window (240X240).\n\nBe ware that the coordinate origin is in the upper left corner.\n\n\n\n\n\n\nNote that the frames per second rate is around 8 fps (similar to what we got with the Image Classification project). This happens because FOMO is cleverly built over a CNN model, not with an object detection model like the SSD MobileNet. For example, when running a MobileNetV2 SSD FPN-Lite 320x320 model on a Raspberry Pi 4, the latency is around 5 times higher (around 1.5 fps)\nHere is a short video showing the inference results:"
  },
  {
    "objectID": "object_detection_fomo.html#conclusion",
    "href": "object_detection_fomo.html#conclusion",
    "title": "Object Detection",
    "section": "Conclusion",
    "text": "Conclusion\nFOMO is a significant leap in the image processing space, as Louis Moreau and Mat Kelcey put it during its launch in 2022:\n\nFOMO is a ground-breaking algorithm that brings real-time object detection, tracking, and counting to microcontrollers for the first time.\n\nMultiple possibilities exist for exploring object detection (and, more precisely, counting them) on embedded devices, for example, to explore the Nicla doing sensor fusion (camera + microphone) and object detection. This can be very useful on projects involving bees, for example."
  },
  {
    "objectID": "kws_feature_eng.html#introduction",
    "href": "kws_feature_eng.html#introduction",
    "title": "Audio Feature Engineering",
    "section": "Introduction",
    "text": "Introduction\nIn this hands-on tutorial, the emphasis is on the critical role that feature engineering plays in optimizing the performance of machine learning models applied to audio classification tasks, such as speech recognition. It is essential to be aware that the performance of any machine learning model relies heavily on the quality of features used, and we will deal with ‚Äúunder-the-hood‚Äù mechanics of feature extraction, mainly focusing on Mel-frequency Cepstral Coefficients (MFCCs), a cornerstone in the field of audio signal processing.\nMachine learning models, especially traditional algorithms, don‚Äôt understand audio waves. They understand numbers arranged in some meaningful way, i.e., features. These features encapsulate the characteristics of the audio signal, making it easier for models to distinguish between different sounds.\n\nThis tutorial will deal with generating features specifically for audio classification. This can be particularly interesting for applying machine learning to a variety of audio data, whether for speech recognition, music categorization, insect classification based on wingbeat sounds, or other sound analysis tasks"
  },
  {
    "objectID": "kws_feature_eng.html#the-kws",
    "href": "kws_feature_eng.html#the-kws",
    "title": "Audio Feature Engineering",
    "section": "The KWS",
    "text": "The KWS\nThe most common TinyML application is Keyword Spotting (KWS), a subset of the broader field of speech recognition. While general speech recognition aims to transcribe all spoken words into text, Keyword Spotting focuses on detecting specific ‚Äúkeywords‚Äù or ‚Äúwake words‚Äù in a continuous audio stream. The system is trained to recognize these keywords as predefined phrases or words, such as yes or no. In short, KWS is a specialized form of speech recognition with its own set of challenges and requirements.\nHere a typical KWS Process using MFCC Feature Converter:\n\n\n\n\n\n\nApplications of KWS:\n\nVoice Assistants: In devices like Amazon‚Äôs Alexa or Google Home, KWS is used to detect the wake word (‚ÄúAlexa‚Äù or ‚ÄúHey Google‚Äù) to activate the device.\nVoice-Activated Controls: In automotive or industrial settings, KWS can be used to initiate specific commands like ‚ÄúStart engine‚Äù or ‚ÄúTurn off lights.‚Äù\nSecurity Systems: Voice-activated security systems may use KWS to authenticate users based on a spoken passphrase.\nTelecommunication Services: Customer service lines may use KWS to route calls based on spoken keywords.\n\n\n\nDifferences from General Speech Recognition:\n\nComputational Efficiency: KWS is usually designed to be less computationally intensive than full speech recognition, as it only needs to recognize a small set of phrases.\nReal-time Processing: KWS often operates in real-time and is optimized for low-latency detection of keywords.\nResource Constraints: KWS models are often designed to be lightweight, so they can run on devices with limited computational resources, like microcontrollers or mobile phones.\nFocused Task: While general speech recognition models are trained to handle a broad range of vocabulary and accents, KWS models are fine-tuned to recognize specific keywords, often in noisy environments accurately."
  },
  {
    "objectID": "kws_feature_eng.html#introduction-to-audio-signals",
    "href": "kws_feature_eng.html#introduction-to-audio-signals",
    "title": "Audio Feature Engineering",
    "section": "Introduction to Audio Signals",
    "text": "Introduction to Audio Signals\nUnderstanding the basic properties of audio signals is crucial for effective feature extraction and, ultimately, for successfully applying machine learning algorithms in audio classification tasks. Audio signals are complex waveforms that capture fluctuations in air pressure over time. These signals can be characterized by several fundamental attributes: sampling rate, frequency, and amplitude.\n\nFrequency and Amplitude: Frequency refers to the number of oscillations a waveform undergoes per unit time and is also measured in Hz. In the context of audio signals, different frequencies correspond to different pitches. Amplitude, on the other hand, measures the magnitude of the oscillations and correlates with the loudness of the sound. Both frequency and amplitude are essential features that capture audio signals‚Äô tonal and rhythmic qualities.\nSampling Rate: The sampling rate, often denoted in Hertz (Hz), defines the number of samples taken per second when digitizing an analog signal. A higher sampling rate allows for a more accurate digital representation of the signal but also demands more computational resources for processing. Typical sampling rates include 44.1 kHz for CD-quality audio and 16 kHz or 8 kHz for speech recognition tasks. Understanding the trade-offs in selecting an appropriate sampling rate is essential for balancing accuracy and computational efficiency. In general, with TinyML projects, we work with 16KHz. Altough music tones can be heard at frequencies up to 20 kHz, voice maxes out at 8 kHz. Traditional telephone systems use an 8 kHz sampling frequency.\n\n\nFor an accurate representation of the signal, the sampling rate must be at least twice the highest frequency present in the signal.\n\n\nTime Domain vs.¬†Frequency Domain: Audio signals can be analyzed in the time and frequency domains. In the time domain, a signal is represented as a waveform where the amplitude is plotted against time. This representation helps to observe temporal features like onset and duration but the signal‚Äôs tonal characteristics are not well evidenced. Conversely, a frequency domain representation provides a view of the signal‚Äôs constituent frequencies and their respective amplitudes, typically obtained via a Fourier Transform. This is invaluable for tasks that require understanding the signal‚Äôs spectral content, such as identifying musical notes or speech phonemes (our case).\n\nThe image below shows the words YES and NO with typical representations in the Time (Raw Audio) and Frequency domains:\n\n\n\n\n\n\nWhy Not Raw Audio?\nWhile using raw audio data directly for machine learning tasks may seem tempting, this approach presents several challenges that make it less suitable for building robust and efficient models.\nUsing raw audio data for Keyword Spotting (KWS), for example, on TinyML devices poses challenges due to its high dimensionality (using a 16 kHz sampling rate), computational complexity for capturing temporal features, susceptibility to noise, and lack of semantically meaningful features, making feature extraction techniques like MFCCs a more practical choice for resource-constrained applications.\nHere are some additional details of the critical issues associated with using raw audio:\n\nHigh Dimensionality: Audio signals, especially those sampled at high rates, result in large amounts of data. For example, a 1-second audio clip sampled at 16 kHz will have 16,000 individual data points. High-dimensional data increases computational complexity, leading to longer training times and higher computational costs, making it impractical for resource-constrained environments. Furthermore, the wide dynamic range of audio signals requires a significant amount of bits per sample, while conveying little useful information.\nTemporal Dependencies: Raw audio signals have temporal structures that simple machine learning models may find hard to capture. While recurrent neural networks like LSTMs can model such dependencies, they are computationally intensive and tricky to train on tiny devices.\nNoise and Variability: Raw audio signals often contain background noise and other non-essential elements affecting model performance. Additionally, the same sound can have different characteristics based on various factors such as distance from the microphone, the orientation of the sound source, and acoustic properties of the environment, adding to the complexity of the data.\nLack of Semantic Meaning: Raw audio doesn‚Äôt inherently contain semantically meaningful features for classification tasks. Features like pitch, tempo, and spectral characteristics, which can be crucial for speech recognition, are not directly accessible from raw waveform data.\nSignal Redundancy: Audio signals often contain redundant information, with certain portions of the signal contributing little to no value to the task at hand. This redundancy can make learning inefficient and potentially lead to overfitting.\n\nFor these reasons, feature extraction techniques such as Mel-frequency Cepstral Coefficients (MFCCs), Mel-Frequency Energies (MFEs), and simple Spectograms are commonly used to transform raw audio data into a more manageable and informative format. These features capture the essential characteristics of the audio signal while reducing dimensionality and noise, facilitating more effective machine learning."
  },
  {
    "objectID": "kws_feature_eng.html#introduction-to-mfccs",
    "href": "kws_feature_eng.html#introduction-to-mfccs",
    "title": "Audio Feature Engineering",
    "section": "Introduction to MFCCs",
    "text": "Introduction to MFCCs\n\nWhat are MFCCs?\nMel-frequency Cepstral Coefficients (MFCCs) are a set of features derived from the spectral content of an audio signal. They are based on human auditory perceptions and are commonly used to capture the phonetic characteristics of an audio signal. The MFCCs are computed through a multi-step process that includes pre-emphasis, framing, windowing, applying the Fast Fourier Transform (FFT) to convert the signal to the frequency domain, and finally, applying the Discrete Cosine Transform (DCT). The result is a compact representation of the original audio signal‚Äôs spectral characteristics.\nThe image below shows the words YES and NO in their MFCC representation:\n\n\n\n\n\n\nThis video explains the Mel Frequency Cepstral Coefficients (MFCC) and how to compute them.\n\n\n\nWhy are MFCCs important?\nMFCCs are crucial for several reasons, particularly in the context of Keyword Spotting (KWS) and TinyML:\n\nDimensionality Reduction: MFCCs capture essential spectral characteristics of the audio signal while significantly reducing the dimensionality of the data, making it ideal for resource-constrained TinyML applications.\nRobustness: MFCCs are less susceptible to noise and variations in pitch and amplitude, providing a more stable and robust feature set for audio classification tasks.\nHuman Auditory System Modeling: The Mel scale in MFCCs approximates the human ear‚Äôs response to different frequencies, making them practical for speech recognition where human-like perception is desired.\nComputational Efficiency: The process of calculating MFCCs is computationally efficient, making it well-suited for real-time applications on hardware with limited computational resources.\n\nIn summary, MFCCs offer a balance of information richness and computational efficiency, making them popular for audio classification tasks, particularly in constrained environments like TinyML.\n\n\nComputing MFCCs\nThe computation of Mel-frequency Cepstral Coefficients (MFCCs) involves several key steps. Let‚Äôs walk through these, which are particularly important for Keyword Spotting (KWS) tasks on TinyML devices.\n\nPre-emphasis: The first step is pre-emphasis, which is applied to accentuate the high-frequency components of the audio signal and balance the frequency spectrum. This is achieved by applying a filter that amplifies the difference between consecutive samples. The formula for pre-emphasis is: y(t) = x(t) - \\(\\alpha\\) x(t-1) , where \\(\\alpha\\) is the pre-emphasis factor, typically around 0.97.\nFraming: Audio signals are divided into short frames (the frame length), usually 20 to 40 milliseconds. This is based on the assumption that frequencies in a signal are stationary over a short period. Framing helps in analyzing the signal in such small time slots. The frame stride (or step) will displace one frame and the adjacent. Those steps could be sequential or overlapped.\nWindowing: Each frame is then windowed to minimize the discontinuities at the frame boundaries. A commonly used window function is the Hamming window. Windowing prepares the signal for a Fourier transform by minimizing the edge effects. The image below shows three frames (10, 20, and 30) and the time samples after windowing (note that the frame length and frame stride are 20 ms):\n\n\n\n\n\n\n\nFast Fourier Transform (FFT) The Fast Fourier Transform (FFT) is applied to each windowed frame to convert it from the time domain to the frequency domain. The FFT gives us a complex-valued representation that includes both magnitude and phase information. However, for MFCCs, only the magnitude is used to calculate the Power Spectrum. The power spectrum is the square of the magnitude spectrum and measures the energy present at each frequency component.\n\n\nThe power spectrum \\(P(f)\\) of a signal \\(x(t)\\) is defined as \\(P(f) = |X(f)|^2\\), where \\(X(f)\\) is the Fourier Transform of \\(x(t)\\). By squaring the magnitude of the Fourier Transform, we emphasize stronger frequencies over weaker ones, thereby capturing more relevant spectral characteristics of the audio signal. This is important in applications like audio classification, speech recognition, and Keyword Spotting (KWS), where the focus is on identifying distinct frequency patterns that characterize different classes of audio or phonemes in speech.\n\n\n\n\n\n\n\nMel Filter Banks: The frequency domain is then mapped to the Mel scale, which approximates the human ear‚Äôs response to different frequencies. The idea is to extract more features (more filter banks) in the lower frequencies and less in the high frequencies. Thus, it performs well on sounds distinguished by the human ear. Typically, 20 to 40 triangular filters extract the Mel-frequency energies. These energies are then log-transformed to convert multiplicative factors into additive ones, making them more suitable for further processing.\n\n\n\n\n\n\n\nDiscrete Cosine Transform (DCT): The last step is to apply the Discrete Cosine Transform (DCT) to the log Mel energies. The DCT helps to decorrelate the energies, effectively compressing the data and retaining only the most discriminative features. Usually, the first 12-13 DCT coefficients are retained, forming the final MFCC feature vector."
  },
  {
    "objectID": "kws_feature_eng.html#hands-on-using-python",
    "href": "kws_feature_eng.html#hands-on-using-python",
    "title": "Audio Feature Engineering",
    "section": "Hands-On using Python",
    "text": "Hands-On using Python\nLet‚Äôs apply what we discussed while working on an actual audio sample. Open the notebook on Google CoLab and extract the MLCC features on your audio samples: [Open In Colab]"
  },
  {
    "objectID": "kws_feature_eng.html#conclusion",
    "href": "kws_feature_eng.html#conclusion",
    "title": "Audio Feature Engineering",
    "section": "Conclusion",
    "text": "Conclusion\n\nWhat Feature Extraction technique should we use?\nMel-frequency Cepstral Coefficients (MFCCs), Mel-Frequency Energies (MFEs), or Spectrogram are techniques for representing audio data, which are often helpful in different contexts.\nIn general, MFCCs are more focused on capturing the envelope of the power spectrum, which makes them less sensitive to fine-grained spectral details but more robust to noise. This is often desirable for speech-related tasks. On the other hand, spectrograms or MFEs preserve more detailed frequency information, which can be advantageous in tasks that require discrimination based on fine-grained spectral content.\n\nMFCCs are particularly strong for:\n\nSpeech Recognition: MFCCs are excellent for identifying phonetic content in speech signals.\nSpeaker Identification: They can be used to distinguish between different speakers based on voice characteristics.\nEmotion Recognition: MFCCs can capture the nuanced variations in speech indicative of emotional states.\nKeyword Spotting: Especially in TinyML, where low computational complexity and small feature size are crucial.\n\n\n\nSpectrograms or MFEs are often more suitable for:\n\nMusic Analysis: Spectrograms can capture harmonic and timbral structures in music, which is essential for tasks like genre classification, instrument recognition, or music transcription.\nEnvironmental Sound Classification: In recognizing non-speech, environmental sounds (e.g., rain, wind, traffic), the full spectrogram can provide more discriminative features.\nBirdsong Identification: The intricate details of bird calls are often better captured using spectrograms.\nBioacoustic Signal Processing: In applications like dolphin or bat call analysis, the fine-grained frequency information in a spectrogram can be essential.\nAudio Quality Assurance: Spectrograms are often used in professional audio analysis to identify unwanted noises, clicks, or other artifacts."
  },
  {
    "objectID": "kws_nicla.html#introduction",
    "href": "kws_nicla.html#introduction",
    "title": "Keyword Spotting (KWS)",
    "section": "Introduction",
    "text": "Introduction\nHaving already explored the Nicla Vision board in the Image Classification and Object Detection applications, we are now shifting our focus to voice-activated applications with a project on Keyword Spotting (KWS).\nAs introduced in the Feature Engineering for Audio Classification Hands-On tutorial, Keyword Spotting (KWS) is integrated into many voice recognition systems, enabling devices to respond to specific words or phrases. While this technology underpins popular devices like Google Assistant or Amazon Alexa, it‚Äôs equally applicable and feasible on smaller, low-power devices. This tutorial will guide you through implementing a KWS system using TinyML on the Nicla Vision development board equipped with a digital microphone.\nOur model will be designed to recognize keywords that can trigger device wake-up or specific actions, bringing them to life with voice-activated commands."
  },
  {
    "objectID": "kws_nicla.html#how-does-a-voice-assistant-work",
    "href": "kws_nicla.html#how-does-a-voice-assistant-work",
    "title": "Keyword Spotting (KWS)",
    "section": "How does a voice assistant work?",
    "text": "How does a voice assistant work?\nAs said, voice assistants on the market, like Google Home or Amazon Echo-Dot, only react to humans when they are ‚Äúwaked up‚Äù by particular keywords such as ‚Äù Hey Google‚Äù on the first one and ‚ÄúAlexa‚Äù on the second.\n\n\n\n\n\nIn other words, recognizing voice commands is based on a multi-stage model or Cascade Detection.\n\n\n\n\n\nStage 1: A small microprocessor inside the Echo Dot or Google Home continuously listens, waiting for the keyword to be spotted, using a TinyML model at the edge (KWS application).\nStage 2: Only when triggered by the KWS application on Stage 1 is the data sent to the cloud and processed on a larger model.\nThe video below shows an example of a Google Assistant being programmed on a Raspberry Pi (Stage 2), with an Arduino Nano 33 BLE as the tinyML device (Stage 1).\n\n\nTo explore the above Google Assistant project, please see the tutorial: Building an Intelligent Voice Assistant From Scratch.\n\nIn this KWS project, we will focus on Stage 1 (KWS or Keyword Spotting), where we will use the Nicla Vision, which has a digital microphone that will be used to spot the keyword."
  },
  {
    "objectID": "kws_nicla.html#the-kws-hands-on-project",
    "href": "kws_nicla.html#the-kws-hands-on-project",
    "title": "Keyword Spotting (KWS)",
    "section": "The KWS Hands-On Project",
    "text": "The KWS Hands-On Project\nThe diagram below gives an idea of how the final KWS application should work (during inference):\n\n\n\n\n\nOur KWS application will recognize four classes of sound:\n\nYES (Keyword 1)\nNO (Keyword 2)\nNOISE (no words spoken; only background noise is present)\nUNKNOW (a mix of different words than YES and NO)\n\n\nFor real-world projects, it is always advisable to include other sounds besides the keywords, such as ‚ÄúNoise‚Äù (or Background) and ‚ÄúUnknown.‚Äù\n\n\nThe Machine Learning workflow\nThe main component of the KWS application is its model. So, we must train such a model with our specific keywords, noise, and other words (the ‚Äúunknown‚Äù):"
  },
  {
    "objectID": "kws_nicla.html#dataset",
    "href": "kws_nicla.html#dataset",
    "title": "Keyword Spotting (KWS)",
    "section": "Dataset",
    "text": "Dataset\nThe critical component of any Machine Learning Workflow is the dataset. Once we have decided on specific keywords, in our case (YES and NO), we can take advantage of the dataset developed by Pete Warden, ‚ÄúSpeech Commands: A Dataset for Limited-Vocabulary Speech Recognition.‚Äù This dataset has 35 keywords (with +1,000 samples each), such as yes, no, stop, and go. In words such as yes and no, we can get 1,500 samples.\nYou can download a small portion of the dataset from Edge Studio (Keyword spotting pre-built dataset), which includes samples from the four classes we will use in this project: yes, no, noise, and background. For this, follow the steps below:\n\nDownload the keywords dataset.\nUnzip the file to a location of your choice.\n\n\nUploading the dataset to the Edge Impulse Studio\nInitiate a new project at Edge Impulse Studio (EIS) and select the Upload Existing Data tool in the Data Acquisition section. Choose the files to be uploaded:\n\n\n\n\n\nDefine the Label, select Automatically split between train and test, and Upload data to the EIS. Repeat for all classes.\n\n\n\n\n\nThe dataset will now appear in the Data acquisition section. Note that the approximately 6,000 samples (1,500 for each class) are split into Train (4,800) and Test (1,200) sets.\n\n\n\n\n\n\n\nCapturing additional Audio Data\nAlthough we have a lot of data from Pete‚Äôs dataset, collecting some words spoken by us is advised. When working with accelerometers, creating a dataset with data captured by the same type of sensor is essential. In the case of sound, this is optional because what we will classify is, in reality, audio data.\n\nThe key difference between sound and audio is the type of energy. Sound is mechanical perturbation (longitudinal sound waves) that propagate through a medium, causing variations of pressure in it. Audio is an electrical (analog or digital) signal representing sound.\n\nWhen we pronounce a keyword, the sound waves should be converted to audio data. The conversion should be done by sampling the signal generated by the microphone at a 16KHz frequency with 16-bit per sample amplitude.\nSo, any device that can generate audio data with this basic specification (16KHz/16bits) will work fine. As a device, we can use the NiclaV, a computer, or even your mobile phone.\n\n\n\n\n\n\nUsing the NiclaV and the Edge Impulse Studio\nAs we learned in the chapter Setup Nicla Vision, EIS officially supports the Nicla Vision, which simplifies the capture of the data from its sensors, including the microphone. So, please create a new project on EIS and connect the Nicla to it, following these steps:\n\nDownload the last updated EIS Firmware and unzip it.\nOpen the zip file on your computer and select the uploader corresponding to your OS:\n\n\n\n\n\n\n\nPut the NiclaV in Boot Mode by pressing the reset button twice.\n\n\n\n\n\n\n\nUpload the binary arduino-nicla-vision.bin to your board by running the batch code corresponding to your OS.\n\nGo to your project on EIS, and on the Data Acquisition tab, select WebUSB. A window will pop up; choose the option that shows that the Nicla is paired and press [Connect].\nYou can choose which sensor data to pick in the Collect Data section on the Data Acquisition tab. Select: Built-in microphone, define your label (for example, yes), the sampling Frequency[16000Hz], and the Sample length (in milliseconds), for example [10s]. Start sampling.\n\n\n\n\n\nData on Pete‚Äôs dataset have a length of 1s, but the recorded samples are 10s long and must be split into 1s samples. Click on three dots after the sample name and select Split sample.\nA window will pop up with the Split tool.\n\n\n\n\n\nOnce inside the tool, split the data into 1-second (1000 ms) records. If necessary, add or remove segments. This procedure should be repeated for all new samples.\n\n\nUsing a smartphone and the EI Studio\nYou can also use your PC or smartphone to capture audio data, using a sampling frequency of 16KHz and a bit depth of 16.\nGo to Devices, scan the QR Code using your phone, and click on the link. A data Collection app will appear in your browser. Select Collecting Audio, and define your Label, data capture Length, and Category.\n\n\n\n\n\nRepeat the same procedure used with the NiclaV.\n\nNote that any app, such as Audacity, can be used for audio recording, provided you use 16KHz/16-bit depth samples."
  },
  {
    "objectID": "kws_nicla.html#creating-impulse-pre-process-model-definition",
    "href": "kws_nicla.html#creating-impulse-pre-process-model-definition",
    "title": "Keyword Spotting (KWS)",
    "section": "Creating Impulse (Pre-Process / Model definition)",
    "text": "Creating Impulse (Pre-Process / Model definition)\nAn impulse takes raw data, uses signal processing to extract features, and then uses a learning block to classify new data.\n\nImpulse Design\n\n\n\n\n\nFirst, we will take the data points with a 1-second window, augmenting the data and sliding that window in 500ms intervals. Note that the option zero-pad data is set. It is essential to fill with ‚Äòzeros‚Äô samples smaller than 1 second (in some cases, some samples can result smaller than the 1000 ms window on the split tool to avoid noise and spikes).\nEach 1-second audio sample should be pre-processed and converted to an image (for example, 13 x 49 x 1). As discussed in the Feature Engineering for Audio Classification Hands-On tutorial, we will use Audio (MFCC), which extracts features from audio signals using Mel Frequency Cepstral Coefficients, which are well suited for the human voice, our case here.\nNext, we select the Classification block to build our model from scratch using a Convolution Neural Network (CNN).\n\nAlternatively, you can use the Transfer Learning (Keyword Spotting) block, which fine-tunes a pre-trained keyword spotting model on your data. This approach has good performance with relatively small keyword datasets.\n\n\n\nPre-Processing (MFCC)\nThe following step is to create the features to be trained in the next phase:\nWe could keep the default parameter values, but we will use the DSP Autotune parameters option.\n\n\n\n\n\nWe will take the Raw features (our 1-second, 16KHz sampled audio data) and use the MFCC processing block to calculate the Processed features. For every 16,000 raw features (16,000 x 1 second), we will get 637 processed features (13 x 49).\n\n\n\n\n\nThe result shows that we only used a small amount of memory to pre-process data (16KB) and a latency of 34ms, which is excellent. For example, on an Arduino Nano (Cortex-M4f @ 64MHz), the same pre-process will take around 480ms. The parameters chosen, such as the FFT length [512], will significantly impact the latency.\nNow, let‚Äôs Save parameters and move to the Generated features tab, where the actual features will be generated. Using UMAP, a dimension reduction technique, the Feature explorer shows how the features are distributed on a two-dimensional plot.\n\n\n\n\n\nThe result seems OK, with a visually clear separation between yes features (in red) and no features (in blue). The unknown features seem nearer to the no space than the yes. This suggests that the keyword no has more propensity to false positives.\n\n\nGoing under the hood\nTo understand better how the raw sound is preprocessed, look at the Feature Engineering for Audio Classification chapter. You can play with the MFCC features generation by downloading this notebook from GitHub or [Opening it In Colab]"
  },
  {
    "objectID": "kws_nicla.html#model-design-and-training",
    "href": "kws_nicla.html#model-design-and-training",
    "title": "Keyword Spotting (KWS)",
    "section": "Model Design and Training",
    "text": "Model Design and Training\nWe will use a simple Convolution Neural Network (CNN) model, tested with 1D and 2D convolutions. The basic architecture has two blocks of Convolution + MaxPooling ([8] and [16] filters, respectively) and a Dropout of [0.25] for the 1D and [0.5] for the 2D. For the last layer, after Flattening, we have [4] neurons, one for each class:\n\n\n\n\n\nAs hyper-parameters, we will have a Learning Rate of [0.005] and a model trained by [100] epochs. We will also include a data augmentation method based on SpecAugment. We trained the 1D and the 2D models with the same hyperparameters. The 1D architecture had a better overall result (90.5% accuracy when compared with 88% of the 2D, so we will use the 1D.\n\n\n\n\n\n\nUsing 1D convolutions is more efficient because it requires fewer parameters than 2D convolutions, making them more suitable for resource-constrained environments.\n\nIt is also interesting to pay attention to the 1D Confusion Matrix. The F1 Score for yes is 95%, and for no, 91%. That was expected by what we saw with the Feature Explorer (no and unknown at close distance). In trying to improve the result, you can inspect closely the results of the samples with an error.\n\n\n\n\n\nListen to the samples that went wrong. For example, for yes, most of the mistakes were related to a yes pronounced as ‚Äúyeh‚Äù. You can acquire additional samples and then retrain your model.\n\nGoing under the hood\nIf you want to understand what is happening ‚Äúunder the hood,‚Äù you can download the pre-processed dataset (MFCC training data) from the Dashboard tab and run this Jupyter Notebook, playing with the code or [Opening it In Colab]. For example, you can analyze the accuracy by each epoch:"
  },
  {
    "objectID": "kws_nicla.html#testing",
    "href": "kws_nicla.html#testing",
    "title": "Keyword Spotting (KWS)",
    "section": "Testing",
    "text": "Testing\nTesting the model with the data reserved for training (Test Data), we got an accuracy of approximately 76%.\n\n\n\n\n\nInspecting the F1 score, we can see that for YES, we got 0.90, an excellent result since we expect to use this keyword as the primary ‚Äútrigger‚Äù for our KWS project. The worst result (0.70) is for UNKNOWN, which is OK.\nFor NO, we got 0.72, which was expected, but to improve this result, we can move the samples that were not correctly classified to the training dataset and then repeat the training process.\n\nLive Classification\nWe can proceed to the project‚Äôs next step but also consider that it is possible to perform Live Classification using the NiclaV or a smartphone to capture live samples, testing the trained model before deployment on our device."
  },
  {
    "objectID": "kws_nicla.html#deploy-and-inference",
    "href": "kws_nicla.html#deploy-and-inference",
    "title": "Keyword Spotting (KWS)",
    "section": "Deploy and Inference",
    "text": "Deploy and Inference\nThe EIS will package all the needed libraries, preprocessing functions, and trained models, downloading them to your computer. Go to the Deployment section, select Arduino Library, and at the bottom, choose Quantized (Int8) and press Build.\n\n\n\n\n\nWhen the Build button is selected, a zip file will be created and downloaded to your computer. On your Arduino IDE, go to the Sketch tab, select the option Add .ZIP Library, and Choose the .zip file downloaded by EIS:\n\n\n\n\n\nNow, it is time for a real test. We will make inferences while completely disconnected from the EIS. Let‚Äôs use the NiclaV code example created when we deployed the Arduino Library.\nIn your Arduino IDE, go to the File/Examples tab, look for your project, and select nicla-vision/nicla-vision_microphone (or nicla-vision_microphone_continuous)\n\n\n\n\n\nPress the reset button twice to put the NiclaV in boot mode, upload the sketch to your board, and test some real inferences:"
  },
  {
    "objectID": "kws_nicla.html#post-processing",
    "href": "kws_nicla.html#post-processing",
    "title": "Keyword Spotting (KWS)",
    "section": "Post-processing",
    "text": "Post-processing\nNow that we know the model is working since it detects our keywords, let‚Äôs modify the code to see the result with the NiclaV completely offline (disconnected from the PC and powered by a battery, a power bank, or an independent 5V power supply).\nThe idea is that whenever the keyword YES is detected, the Green LED will light; if a NO is heard, the Red LED will light, if it is a UNKNOW, the Blue LED will light; and in the presence of noise (No Keyword), the LEDs will be OFF.\nWe should modify one of the code examples. Let‚Äôs do it now with the nicla-vision_microphone_continuous.\nStart with initializing the LEDs:\n...\nvoid setup()\n{\n        // Once you finish debugging your code, you can comment or delete the Serial part of the code\n    Serial.begin(115200);\n    while (!Serial);\n    Serial.println(\"Inferencing - Nicla Vision KWS with LEDs\");\n    \n    // Pins for the built-in RGB LEDs on the Arduino NiclaV\n    pinMode(LEDR, OUTPUT);\n    pinMode(LEDG, OUTPUT);\n    pinMode(LEDB, OUTPUT);\n\n    // Ensure the LEDs are OFF by default.\n    // Note: The RGB LEDs on the Arduino Nicla Vision\n    // are ON when the pin is LOW, OFF when HIGH.\n    digitalWrite(LEDR, HIGH);\n    digitalWrite(LEDG, HIGH);\n    digitalWrite(LEDB, HIGH);\n...\n}\nCreate two functions, turn_off_leds() function , to turn off all RGB LEDs\n**\n * @brief      turn_off_leds function - turn-off all RGB LEDs\n */\nvoid turn_off_leds(){\n    digitalWrite(LEDR, HIGH);\n    digitalWrite(LEDG, HIGH);\n    digitalWrite(LEDB, HIGH);\n}\nAnother turn_on_led() function is used to turn on the RGB LEDs according to the most probable result of the classifier.\n/**\n * @brief      turn_on_leds function used to turn on the RGB LEDs\n * @param[in]  pred_index     \n *             no:       [0] ==&gt; Red ON\n *             noise:    [1] ==&gt; ALL OFF \n *             unknown:  [2] ==&gt; Blue ON\n *             Yes:      [3] ==&gt; Green ON\n */\nvoid turn_on_leds(int pred_index) {\n  switch (pred_index)\n  {\n    case 0:\n      turn_off_leds();\n      digitalWrite(LEDR, LOW);\n      break;\n\n    case 1:\n      turn_off_leds();\n      break;\n    \n    case 2:\n      turn_off_leds();\n      digitalWrite(LEDB, LOW);\n      break;\n\n    case 3:\n      turn_off_leds();\n      digitalWrite(LEDG, LOW);\n      break;\n  }\n}\nAnd change the // print the predictions portion of the code on loop():\n...\n\n    if (++print_results &gt;= (EI_CLASSIFIER_SLICES_PER_MODEL_WINDOW)) {\n        // print the predictions\n        ei_printf(\"Predictions \");\n        ei_printf(\"(DSP: %d ms., Classification: %d ms., Anomaly: %d ms.)\",\n            result.timing.dsp, result.timing.classification, result.timing.anomaly);\n        ei_printf(\": \\n\");\n\n        int pred_index = 0;     // Initialize pred_index\n        float pred_value = 0;   // Initialize pred_value\n\n        for (size_t ix = 0; ix &lt; EI_CLASSIFIER_LABEL_COUNT; ix++) {\n            if (result.classification[ix].value &gt; pred_value){\n                pred_index = ix;\n                pred_value = result.classification[ix].value;\n            }\n            // ei_printf(\"    %s: \", result.classification[ix].label);\n            // ei_printf_float(result.classification[ix].value);\n            // ei_printf(\"\\n\");\n        }\n        ei_printf(\"  PREDICTION: ==&gt; %s with probability %.2f\\n\", \n                  result.classification[pred_index].label, pred_value);\n        turn_on_leds (pred_index);\n\n        \n#if EI_CLASSIFIER_HAS_ANOMALY == 1\n        ei_printf(\"    anomaly score: \");\n        ei_printf_float(result.anomaly);\n        ei_printf(\"\\n\");\n#endif\n\n        print_results = 0;\n    }\n}\n\n...\nYou can find the complete code on the project‚Äôs GitHub.\nUpload the sketch to your board and test some real inferences. The idea is that the Green LED will be ON whenever the keyword YES is detected, the Red will lit for a NO, and any other word will turn on the Blue LED. All the LEDs should be off if silence or background noise is present. Remember that the same procedure can ‚Äútrigger‚Äù an external device to perform a desired action instead of turning on an LED, as we saw in the introduction."
  },
  {
    "objectID": "kws_nicla.html#conclusion",
    "href": "kws_nicla.html#conclusion",
    "title": "Keyword Spotting (KWS)",
    "section": "Conclusion",
    "text": "Conclusion\n\nYou will find the notebooks and codes used in this hands-on tutorial on the GitHub repository.\n\nBefore we finish, consider that Sound Classification is more than just voice. For example, you can develop TinyML projects around sound in several areas, such as:\n\nSecurity (Broken Glass detection, Gunshot)\nIndustry (Anomaly Detection)\nMedical (Snore, Cough, Pulmonary diseases)\nNature (Beehive control, insect sound, pouching mitigation)"
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Abadi, Martƒ±ÃÅn, Paul Barham, Jianmin Chen, Zhifeng Chen, Andy Davis,\nJeffrey Dean, Matthieu Devin, et al. 2016. ‚Äú{TensorFlow}: A System for {Large-Scale} Machine Learning.‚Äù In 12th\nUSENIX Symposium on Operating Systems Design and Implementation (OSDI\n16), 265‚Äì83.\n\n\nAdolf, Robert, Saketh Rama, Brandon Reagen, Gu-Yeon Wei, and David\nBrooks. 2016. ‚ÄúFathom: Reference Workloads for Modern Deep\nLearning Methods.‚Äù In 2016 IEEE International Symposium on\nWorkload Characterization (IISWC), 1‚Äì10. IEEE.\n\n\nAledhari, Mohammed, Rehma Razzak, Reza M. Parizi, and Fahad Saeed. 2020.\n‚ÄúFederated Learning: A Survey on Enabling Technologies, Protocols,\nand Applications.‚Äù IEEE Access 8: 140699‚Äì725. https://doi.org/10.1109/access.2020.3013541.\n\n\nAl-Rfou, Rami, Guillaume Alain, Amjad Almahairi, Christof Angermueller,\nDzmitry Bahdanau, Nicolas Ballas, Fr√©d√©ric Bastien, et al. 2016.\n‚ÄúTheano: A Python Framework for Fast Computation of Mathematical\nExpressions.‚Äù arXiv e-Prints, arXiv‚Äì1605.\n\n\nAltayeb, Moez, Marco Zennaro, and Marcelo Rovai. 2022.\n‚ÄúClassifying Mosquito Wingbeat Sound Using TinyML.‚Äù In\nProceedings of the 2022 ACM Conference on Information Technology for\nSocial Good, 132‚Äì37.\n\n\nAntol, Stanislaw, Aishwarya Agrawal, Jiasen Lu, Margaret Mitchell, Dhruv\nBatra, C Lawrence Zitnick, and Devi Parikh. 2015. ‚ÄúVqa: Visual\nQuestion Answering.‚Äù In Proceedings of the IEEE International\nConference on Computer Vision, 2425‚Äì33.\n\n\nArdila, Rosana, Megan Branson, Kelly Davis, Michael Henretty, Michael\nKohler, Josh Meyer, Reuben Morais, Lindsay Saunders, Francis M. Tyers,\nand Gregor Weber. 2020. ‚ÄúCommon Voice: A Massively-Multilingual\nSpeech Corpus.‚Äù Proceedings of the 12th Conference on\nLanguage Resources and Evaluation, May, 4218‚Äì22.\n\n\nARM.com. ‚ÄúThe Future Is Being Built on Arm: Market Diversification\nContinues to Drive Strong Royalty and Licensing Growth as Ecosystem\nReaches Quarter of a Trillion Chips Milestone ‚Äì Arm¬Æ.‚Äù https://www.arm.com/company/news/2023/02/arm-announces-q3-fy22-results.\n\n\nBamoumen, Hatim, Anas Temouden, Nabil Benamar, and Yousra Chtouki. 2022.\n‚ÄúHow TinyML Can Be Leveraged to Solve Environmental Problems: A\nSurvey.‚Äù In 2022 International Conference on Innovation and\nIntelligence for Informatics, Computing, and Technologies (3ICT),\n338‚Äì43. IEEE.\n\n\nBanbury, Colby R, Vijay Janapa Reddi, Max Lam, William Fu, Amin Fazel,\nJeremy Holleman, Xinyuan Huang, et al. 2020. ‚ÄúBenchmarking Tinyml\nSystems: Challenges and Direction.‚Äù arXiv Preprint\narXiv:2003.04821.\n\n\nBank, Dor, Noam Koenigstein, and Raja Giryes. 2023.\n‚ÄúAutoencoders.‚Äù Machine Learning for Data Science\nHandbook: Data Mining and Knowledge Discovery Handbook, 353‚Äì74.\n\n\nBarroso, Luiz Andr√©, Urs H√∂lzle, and Parthasarathy Ranganathan. 2019.\nThe Datacenter as a Computer: Designing Warehouse-Scale\nMachines. Springer Nature.\n\n\nBender, Emily M., and Batya Friedman. 2018. ‚ÄúData Statements for\nNatural Language Processing: Toward Mitigating System Bias and Enabling\nBetter Science.‚Äù Transactions of the Association for\nComputational Linguistics 6: 587‚Äì604. https://doi.org/10.1162/tacl_a_00041.\n\n\nBeyer, Lucas, Olivier J H√©naff, Alexander Kolesnikov, Xiaohua Zhai, and\nA√§ron van den Oord. 2020. ‚ÄúAre We Done with Imagenet?‚Äù\narXiv Preprint arXiv:2006.07159.\n\n\nBrown, Tom, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan,\nPrafulla Dhariwal, Arvind Neelakantan, et al. 2020. ‚ÄúLanguage\nModels Are Few-Shot Learners.‚Äù Advances in Neural Information\nProcessing Systems 33: 1877‚Äì1901.\n\n\nChapelle, O., B. Scholkopf, and A. Zien Eds. 2009.\n‚ÄúSemi-Supervised Learning (Chapelle, o. Et Al., Eds.; 2006) [Book\nReviews].‚Äù IEEE Transactions on Neural Networks 20 (3):\n542‚Äì42. https://doi.org/10.1109/tnn.2009.2015974.\n\n\nChollet, Fran√ßois et al. 2018. ‚ÄúKeras: The Python Deep Learning\nLibrary.‚Äù Astrophysics Source Code Library, ascl‚Äì1806.\n\n\nChu, Grace, Okan Arikan, Gabriel Bender, Weijun Wang, Achille Brighton,\nPieter-Jan Kindermans, Hanxiao Liu, Berkin Akin, Suyog Gupta, and Andrew\nHoward. 2021. ‚ÄúDiscovering Multi-Hardware Mobile Models via\nArchitecture Search.‚Äù In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition, 3022‚Äì31.\n\n\nColeman, Cody, Edward Chou, Julian Katz-Samuels, Sean Culatana, Peter\nBailis, Alexander C Berg, Robert Nowak, Roshan Sumbaly, Matei Zaharia,\nand I Zeki Yalniz. 2022. ‚ÄúSimilarity Search for Efficient Active\nLearning and Search of Rare Concepts.‚Äù In Proceedings of the\nAAAI Conference on Artificial Intelligence, 36:6402‚Äì10. 6.\n\n\nColeman, Cody, Deepak Narayanan, Daniel Kang, Tian Zhao, Jian Zhang,\nLuigi Nardi, Peter Bailis, Kunle Olukotun, Chris R√©, and Matei Zaharia.\n2017. ‚ÄúDawnbench: An End-to-End Deep Learning Benchmark and\nCompetition.‚Äù Training 100 (101): 102.\n\n\nDavid, Robert, Jared Duke, Advait Jain, Vijay Janapa Reddi, Nat\nJeffries, Jian Li, Nick Kreeger, et al. 2021. ‚ÄúTensorflow Lite\nMicro: Embedded Machine Learning for Tinyml Systems.‚Äù\nProceedings of Machine Learning and Systems 3: 800‚Äì811.\n\n\nDean, Jeffrey, Greg Corrado, Rajat Monga, Kai Chen, Matthieu Devin, Mark\nMao, Marc‚Äôaurelio Ranzato, et al. 2012. ‚ÄúLarge Scale Distributed\nDeep Networks.‚Äù Advances in Neural Information Processing\nSystems 25.\n\n\nDeng, Jia, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei.\n2009. ‚ÄúImagenet: A Large-Scale Hierarchical Image\nDatabase.‚Äù In 2009 IEEE Conference on Computer Vision and\nPattern Recognition, 248‚Äì55. Ieee.\n\n\nDevlin, Jacob, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018.\n‚ÄúBert: Pre-Training of Deep Bidirectional Transformers for\nLanguage Understanding.‚Äù arXiv Preprint\narXiv:1810.04805.\n\n\nDuisterhof, Bardienus P, Srivatsan Krishnan, Jonathan J Cruz, Colby R\nBanbury, William Fu, Aleksandra Faust, Guido CHE de Croon, and Vijay\nJanapa Reddi. 2019. ‚ÄúLearning to Seek: Autonomous Source Seeking\nwith Deep Reinforcement Learning Onboard a Nano Drone\nMicrocontroller.‚Äù arXiv Preprint arXiv:1909.11236.\n\n\nDuisterhof, Bardienus P, Shushuai Li, Javier Burgu√©s, Vijay Janapa\nReddi, and Guido CHE de Croon. 2021. ‚ÄúSniffy Bug: A Fully\nAutonomous Swarm of Gas-Seeking Nano Quadcopters in Cluttered\nEnvironments.‚Äù In 2021 IEEE/RSJ International Conference on\nIntelligent Robots and Systems (IROS), 9099‚Äì9106. IEEE.\n\n\nGaviria Rojas, William, Sudnya Diamos, Keertan Kini, David Kanter, Vijay\nJanapa Reddi, and Cody Coleman. 2022. ‚ÄúThe Dollar Street Dataset:\nImages Representing the Geographic and Socioeconomic Diversity of the\nWorld.‚Äù Advances in Neural Information Processing\nSystems 35: 12979‚Äì90.\n\n\nGebru, Timnit, Jamie Morgenstern, Briana Vecchione, Jennifer Wortman\nVaughan, Hanna Wallach, Hal Daum√© III, and Kate Crawford. 2021.\n‚ÄúDatasheets for Datasets.‚Äù Communications of the\nACM 64 (12): 86‚Äì92. https://doi.org/10.1145/3458723.\n\n\nGoodfellow, Ian, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David\nWarde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. 2020.\n‚ÄúGenerative Adversarial Networks.‚Äù Communications of\nthe ACM 63 (11): 139‚Äì44.\n\n\nGoogle. n.d. ‚ÄúInformation Quality & Content\nModeration.‚Äù https://blog.google/documents/83/.\n\n\nHan, Song, Huizi Mao, and William J. Dally. 2016. ‚ÄúDeep\nCompression: Compressing Deep Neural Networks with Pruning, Trained\nQuantization and Huffman Coding.‚Äù https://arxiv.org/abs/1510.00149.\n\n\nHe, Kaiming, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016.\n‚ÄúDeep Residual Learning for Image Recognition.‚Äù In\nProceedings of the IEEE Conference on Computer Vision and Pattern\nRecognition, 770‚Äì78.\n\n\nHendrycks, Dan, Kevin Zhao, Steven Basart, Jacob Steinhardt, and Dawn\nSong. 2021. ‚ÄúNatural Adversarial Examples.‚Äù In\nProceedings of the IEEE/CVF Conference on Computer Vision and\nPattern Recognition, 15262‚Äì71.\n\n\nHolland, Sarah, Ahmed Hosny, Sarah Newman, Joshua Joseph, and Kasia\nChmielinski. 2020. ‚ÄúThe Dataset Nutrition Label.‚Äù Data\nProtection and Privacy. https://doi.org/10.5040/9781509932771.ch-001.\n\n\nHoward, Andrew G, Menglong Zhu, Bo Chen, Dmitry Kalenichenko, Weijun\nWang, Tobias Weyand, Marco Andreetto, and Hartwig Adam. 2017.\n‚ÄúMobilenets: Efficient Convolutional Neural Networks for Mobile\nVision Applications.‚Äù arXiv Preprint arXiv:1704.04861.\n\n\nIandola, Forrest N, Song Han, Matthew W Moskewicz, Khalid Ashraf,\nWilliam J Dally, and Kurt Keutzer. 2016. ‚ÄúSqueezeNet:\nAlexNet-Level Accuracy with 50x Fewer Parameters and&lt; 0.5 MB Model\nSize.‚Äù arXiv Preprint arXiv:1602.07360.\n\n\nIgnatov, Andrey, Radu Timofte, William Chou, Ke Wang, Max Wu, Tim\nHartley, and Luc Van Gool. 2018. ‚ÄúAi Benchmark: Running Deep\nNeural Networks on Android Smartphones.‚Äù In Proceedings of\nthe European Conference on Computer Vision (ECCV) Workshops, 0‚Äì0.\n\n\nJia, Yangqing, Evan Shelhamer, Jeff Donahue, Sergey Karayev, Jonathan\nLong, Ross Girshick, Sergio Guadarrama, and Trevor Darrell. 2014.\n‚ÄúCaffe: Convolutional Architecture for Fast Feature\nEmbedding.‚Äù In Proceedings of the 22nd ACM International\nConference on Multimedia, 675‚Äì78.\n\n\nJohnson-Roberson, Matthew, Charles Barto, Rounak Mehta, Sharath Nittur\nSridhar, Karl Rosaen, and Ram Vasudevan. 2017. ‚ÄúDriving in the\nMatrix: Can Virtual Worlds Replace Human-Generated Annotations for Real\nWorld Tasks?‚Äù 2017 IEEE International Conference on Robotics\nand Automation (ICRA). https://doi.org/10.1109/icra.2017.7989092.\n\n\nJouppi, Norman P, Cliff Young, Nishant Patil, David Patterson, Gaurav\nAgrawal, Raminder Bajwa, Sarah Bates, et al. 2017. ‚ÄúIn-Datacenter\nPerformance Analysis of a Tensor Processing Unit.‚Äù In\nProceedings of the 44th Annual International Symposium on Computer\nArchitecture, 1‚Äì12.\n\n\nKiela, Douwe, Max Bartolo, Yixin Nie, Divyansh Kaushik, Atticus Geiger,\nZhengxuan Wu, Bertie Vidgen, et al. 2021. ‚ÄúDynabench: Rethinking\nBenchmarking in NLP.‚Äù arXiv Preprint arXiv:2104.14337.\n\n\nKoh, Pang Wei, Shiori Sagawa, Henrik Marklund, Sang Michael Xie, Marvin\nZhang, Akshay Balsubramani, Weihua Hu, et al. 2021. ‚ÄúWilds: A\nBenchmark of in-the-Wild Distribution Shifts.‚Äù In\nInternational Conference on Machine Learning, 5637‚Äì64. PMLR.\n\n\nKrishnan, Rayan, Pranav Rajpurkar, and Eric J. Topol. 2022.\n‚ÄúSelf-Supervised Learning in Medicine and Healthcare.‚Äù\nNature Biomedical Engineering 6 (12): 1346‚Äì52. https://doi.org/10.1038/s41551-022-00914-1.\n\n\nKrizhevsky, Alex, Ilya Sutskever, and Geoffrey E Hinton. 2012.\n‚ÄúImagenet Classification with Deep Convolutional Neural\nNetworks.‚Äù Advances in Neural Information Processing\nSystems 25.\n\n\nKung, Hsiang Tsung, and Charles E Leiserson. 1979. ‚ÄúSystolic\nArrays (for VLSI).‚Äù In Sparse Matrix Proceedings 1978,\n1:256‚Äì82. Society for industrial; applied mathematics Philadelphia, PA,\nUSA.\n\n\nLai, Liangzhen, Naveen Suda, and Vikas Chandra. 2018. ‚ÄúCmsis-Nn:\nEfficient Neural Network Kernels for Arm Cortex-m Cpus.‚Äù\narXiv Preprint arXiv:1801.06601.\n\n\nLeCun, Yann, John Denker, and Sara Solla. 1989. ‚ÄúOptimal Brain\nDamage.‚Äù Advances in Neural Information Processing\nSystems 2.\n\n\nLi, En, Liekang Zeng, Zhi Zhou, and Xu Chen. 2019. ‚ÄúEdge AI:\nOn-Demand Accelerating Deep Neural Network Inference via Edge\nComputing.‚Äù IEEE Transactions on Wireless Communications\n19 (1): 447‚Äì57.\n\n\nLi, Mu, David G Andersen, Alexander J Smola, and Kai Yu. 2014.\n‚ÄúCommunication Efficient Distributed Machine Learning with the\nParameter Server.‚Äù Advances in Neural Information Processing\nSystems 27.\n\n\nLi, Zhizhong, and Derek Hoiem. 2017. ‚ÄúLearning Without\nForgetting.‚Äù IEEE Transactions on Pattern Analysis and\nMachine Intelligence 40 (12): 2935‚Äì47.\n\n\nLin, Ji, Wei-Ming Chen, Yujun Lin, Chuang Gan, Song Han, et al. 2020.\n‚ÄúMcunet: Tiny Deep Learning on Iot Devices.‚Äù Advances\nin Neural Information Processing Systems 33: 11711‚Äì22.\n\n\nLin, Tsung-Yi, Michael Maire, Serge Belongie, James Hays, Pietro Perona,\nDeva Ramanan, Piotr Doll√°r, and C Lawrence Zitnick. 2014.\n‚ÄúMicrosoft Coco: Common Objects in Context.‚Äù In\nComputer Vision‚ÄìECCV 2014: 13th European Conference, Zurich,\nSwitzerland, September 6-12, 2014, Proceedings, Part v 13, 740‚Äì55.\nSpringer.\n\n\nLundberg, Scott M, and Su-In Lee. 2017. ‚ÄúA Unified Approach to\nInterpreting Model Predictions.‚Äù Advances in Neural\nInformation Processing Systems 30.\n\n\nMattson, Peter, Christine Cheng, Gregory Diamos, Cody Coleman, Paulius\nMicikevicius, David Patterson, Hanlin Tang, et al. 2020. ‚ÄúMlperf\nTraining Benchmark.‚Äù Proceedings of Machine Learning and\nSystems 2: 336‚Äì49.\n\n\nMcMahan, H. Brendan, Eider Moore, Daniel Ramage, Seth Hampson, and\nBlaise Ag√ºera y Arcas. 2023. ‚ÄúCommunication-Efficient Learning of\nDeep Networks from Decentralized Data.‚Äù https://arxiv.org/abs/1602.05629.\n\n\nNorthcutt, Curtis G, Anish Athalye, and Jonas Mueller. 2021.\n‚ÄúPervasive Label Errors in Test Sets Destabilize Machine Learning\nBenchmarks.‚Äù arXiv, March. https://doi.org/&nbsp;\nhttps://doi.org/10.48550/arXiv.2103.14749 arXiv-issued DOI via\nDataCite.\n\n\nOoko, Samson Otieno, Marvin Muyonga Ogore, Jimmy Nsenga, and Marco\nZennaro. 2021. ‚ÄúTinyML in Africa: Opportunities and\nChallenges.‚Äù In 2021 IEEE Globecom Workshops (GC\nWkshps), 1‚Äì6. IEEE.\n\n\nPaszke, Adam, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury,\nGregory Chanan, Trevor Killeen, et al. 2019. ‚ÄúPytorch: An\nImperative Style, High-Performance Deep Learning Library.‚Äù\nAdvances in Neural Information Processing Systems 32.\n\n\nPushkarna, Mahima, Andrew Zaldivar, and Oddur Kjartansson. 2022.\n‚ÄúData Cards: Purposeful and Transparent Dataset Documentation for\nResponsible Ai.‚Äù 2022 ACM Conference on Fairness,\nAccountability, and Transparency. https://doi.org/10.1145/3531146.3533231.\n\n\nRamcharan, Amanda, Kelsee Baranowski, Peter McCloskey, Babuali Ahmed,\nJames Legg, and David P Hughes. 2017. ‚ÄúDeep Learning for\nImage-Based Cassava Disease Detection.‚Äù Frontiers in Plant\nScience 8: 1852.\n\n\nRao, Ravi. 2021. Www.wevolver.com. https://www.wevolver.com/article/tinyml-unlocks-new-possibilities-for-sustainable-development-technologies.\n\n\nRatner, Alex, Braden Hancock, Jared Dunnmon, Roger Goldman, and\nChristopher R√©. 2018. ‚ÄúSnorkel Metal: Weak Supervision for\nMulti-Task Learning.‚Äù Proceedings of the Second Workshop on\nData Management for End-To-End Machine Learning. https://doi.org/10.1145/3209889.3209898.\n\n\nReddi, Vijay Janapa, Christine Cheng, David Kanter, Peter Mattson,\nGuenther Schmuelling, Carole-Jean Wu, Brian Anderson, et al. 2020.\n‚ÄúMlperf Inference Benchmark.‚Äù In 2020 ACM/IEEE 47th\nAnnual International Symposium on Computer Architecture (ISCA),\n446‚Äì59. IEEE.\n\n\nRibeiro, Marco Tulio, Sameer Singh, and Carlos Guestrin. 2016. ‚Äú\"\nWhy Should i Trust You?\" Explaining the Predictions of Any\nClassifier.‚Äù In Proceedings of the 22nd ACM SIGKDD\nInternational Conference on Knowledge Discovery and Data Mining,\n1135‚Äì44.\n\n\nRosenblatt, Frank. 1957. The Perceptron, a Perceiving and\nRecognizing Automaton Project Para. Cornell Aeronautical\nLaboratory.\n\n\nRumelhart, David E, Geoffrey E Hinton, and Ronald J Williams. 1986.\n‚ÄúLearning Representations by Back-Propagating Errors.‚Äù\nNature 323 (6088): 533‚Äì36.\n\n\nSeide, Frank, and Amit Agarwal. 2016. ‚ÄúCNTK: Microsoft‚Äôs\nOpen-Source Deep-Learning Toolkit.‚Äù In Proceedings of the\n22nd ACM SIGKDD International Conference on Knowledge Discovery and Data\nMining, 2135‚Äì35.\n\n\nSeyedzadeh, Saleh, Farzad Pour Rahimian, Ivan Glesk, and Marc Roper.\n2018. ‚ÄúMachine Learning for Estimation of Building Energy\nConsumption and Performance: A Review.‚Äù Visualization in\nEngineering 6: 1‚Äì20.\n\n\nSheng, Victor S., and Jing Zhang. 2019. ‚ÄúMachine Learning with\nCrowdsourcing: A Brief Summary of the Past Research and Future\nDirections.‚Äù Proceedings of the AAAI Conference on Artificial\nIntelligence 33 (01): 9837‚Äì43. https://doi.org/10.1609/aaai.v33i01.33019837.\n\n\nTirtalistyani, Rose, Murtiningrum Murtiningrum, and Rameshwar S Kanwar.\n2022. ‚ÄúIndonesia Rice Irrigation System: Time for\nInnovation.‚Äù Sustainability 14 (19): 12477.\n\n\nTokui, Seiya, Kenta Oono, Shohei Hido, and Justin Clayton. 2015.\n‚ÄúChainer: A Next-Generation Open Source Framework for Deep\nLearning.‚Äù In Proceedings of Workshop on Machine Learning\nSystems (LearningSys) in the Twenty-Ninth Annual Conference on Neural\nInformation Processing Systems (NIPS), 5:1‚Äì6.\n\n\nVaswani, Ashish, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion\nJones, Aidan N Gomez, ≈Åukasz Kaiser, and Illia Polosukhin. 2017.\n‚ÄúAttention Is All You Need.‚Äù Advances in Neural\nInformation Processing Systems 30.\n\n\n‚ÄúVector-Borne Diseases.‚Äù https://www.who.int/news-room/fact-sheets/detail/vector-borne-diseases.\n\n\nVerma, Team Dual_Boot: Swapnil. 2022. ‚ÄúElephant AI.‚Äù\nHackster.io. https://www.hackster.io/dual_boot/elephant-ai-ba71e9.\n\n\nVinuesa, Ricardo, Hossein Azizpour, Iolanda Leite, Madeline Balaam,\nVirginia Dignum, Sami Domisch, Anna Fell√§nder, Simone Daniela Langhans,\nMax Tegmark, and Francesco Fuso Nerini. 2020. ‚ÄúThe Role of\nArtificial Intelligence in Achieving the Sustainable Development\nGoals.‚Äù Nature Communications 11 (1): 1‚Äì10.\n\n\nWarden, Pete. 2018. ‚ÄúSpeech Commands: A Dataset for\nLimited-Vocabulary Speech Recognition.‚Äù arXiv Preprint\narXiv:1804.03209.\n\n\nWarden, Pete, and Daniel Situnayake. 2019. Tinyml: Machine Learning\nwith Tensorflow Lite on Arduino and Ultra-Low-Power\nMicrocontrollers. O‚ÄôReilly Media.\n\n\nXie, Cihang, Mingxing Tan, Boqing Gong, Jiang Wang, Alan L Yuille, and\nQuoc V Le. 2020. ‚ÄúAdversarial Examples Improve Image\nRecognition.‚Äù In Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition, 819‚Äì28.\n\n\nXu, Hu, Saining Xie, Xiaoqing Ellen Tan, Po-Yao Huang, Russell Howes,\nVasu Sharma, Shang-Wen Li, Gargi Ghosh, Luke Zettlemoyer, and Christoph\nFeichtenhofer. 2023. ‚ÄúDemystifying CLIP Data.‚Äù arXiv\nPreprint arXiv:2309.16671.\n\n\nZennaro, Marco, Brian Plancher, and V Janapa Reddi. 2022. ‚ÄúTinyML:\nApplied AI for Development.‚Äù In The UN 7th Multi-Stakeholder\nForum on Science, Technology and Innovation for the Sustainable\nDevelopment Goals, 2022‚Äì05."
  },
  {
    "objectID": "tools.html#hardware-kits",
    "href": "tools.html#hardware-kits",
    "title": "Appendix A: Tools",
    "section": "A.1 Hardware Kits",
    "text": "A.1 Hardware Kits\n\nA.1.1 Microcontrollers and Development Boards\n\n\n\n\n\n\n\n\n\n\nNo\nHardware\nProcessor\nFeatures\ntinyML Compatibility\n\n\n\n\n1\nArduino Nano 33 BLE Sense\nARM Cortex-M4\nOnboard sensors, Bluetooth connectivity\nTensorFlow Lite Micro\n\n\n2\nRaspberry Pi Pico\nDual-core Arm Cortex-M0+\nLow-cost, large community support\nTensorFlow Lite Micro\n\n\n3\nSparkFun Edge\nAmbiq Apollo3 Blue\nUltra-low power consumption, onboard microphone\nTensorFlow Lite Micro\n\n\n4\nAdafruit EdgeBadge\nATSAMD51 32-bit Cortex M4\nCompact size, integrated display and microphone\nTensorFlow Lite Micro\n\n\n5\nGoogle Coral Development Board\nNXP i.MX 8M SOC (quad Cortex-A53, Cortex-M4F)\nEdge TPU, Wi-Fi, Bluetooth\nTensorFlow Lite for Coral\n\n\n6\nSTM32 Discovery Kits\nVarious (e.g., STM32F7, STM32H7)\nDifferent configurations, Cube.AI software support\nSTM32Cube.AI\n\n\n7\nArduino Nicla Vision\nSTM32H747AII6 Dual Arm¬Æ Cortex¬Æ M7/M4\nIntegrated camera, low power, compact design\nTensorFlow Lite Micro\n\n\n8\nArduino Nicla Sense ME\n64 MHz Arm¬Æ Cortex M4 (nRF52832)\nMulti-sensor platform, environment sensing, BLE, Wi-Fi\nTensorFlow Lite Micro"
  },
  {
    "objectID": "tools.html#software-tools",
    "href": "tools.html#software-tools",
    "title": "Appendix A: Tools",
    "section": "A.2 Software Tools",
    "text": "A.2 Software Tools\n\nA.2.1 Machine Learning Frameworks\n\n\n\n\n\n\n\n\n\nNo\nMachine Learning Framework\nDescription\nUse Cases\n\n\n\n\n1\nTensorFlow Lite\nLightweight library for running machine learning models on constrained devices\nImage recognition, voice commands, anomaly detection\n\n\n2\nEdge Impulse\nA platform providing tools for creating machine learning models optimized for edge devices\nData collection, model training, deployment on tiny devices\n\n\n3\nONNX Runtime\nA performance-optimized engine for running ONNX models, fine-tuned for edge devices\nCross-platform deployment of machine learning models\n\n\n\n\n\nA.2.2 Libraries and APIs\n\n\n\n\n\n\n\n\n\nNo\nLibrary/API\nDescription\nUse Cases\n\n\n\n\n1\nCMSIS-NN\nA collection of efficient neural network kernels optimized for Cortex-M processors\nEmbedded vision and AI applications\n\n\n2\nARM NN\nAn inference engine for CPUs, GPUs, and NPUs, enabling the translation of neural network frameworks\nAccelerating machine learning model inference on ARM-based devices"
  },
  {
    "objectID": "tools.html#ides-and-development-environments",
    "href": "tools.html#ides-and-development-environments",
    "title": "Appendix A: Tools",
    "section": "A.3 IDEs and Development Environments",
    "text": "A.3 IDEs and Development Environments\n\n\n\n\n\n\n\n\n\nNo\nIDE/Development Environment\nDescription\nFeatures\n\n\n\n\n1\nPlatformIO\nAn open-source ecosystem for IoT development catering to various boards & platforms\nCross-platform build system, continuous testing, firmware updates\n\n\n2\nEclipse Embedded CDT\nA plugin for Eclipse facilitating embedded systems development\nSupports various compilers and debuggers, integrates with popular build tools\n\n\n3\nArduino IDE\nOfficial development environment for Arduino supporting various boards & languages\nUser-friendly interface, large community support, extensive library collection\n\n\n4\nMbed Studio\nARM‚Äôs IDE for developing robust embedded software with Mbed OS\nIntegrated debugger, Mbed OS integration, version control support\n\n\n5\nSegger Embedded Studio\nA powerful IDE for ARM microcontrollers supporting a wide range of development boards\nAdvanced code editor, project management, debugging capabilities"
  },
  {
    "objectID": "zoo_datasets.html",
    "href": "zoo_datasets.html",
    "title": "Appendix B: Datasets",
    "section": "",
    "text": "Google Speech Commands Dataset\n\nDescription: A set of one-second .wav audio files, each containing a single spoken English word.\nLink to the Dataset\n\nVisualWakeWords Dataset\n\nDescription: A dataset tailored for tinyML vision applications, consisting of binary labeled images indicating whether a person is in the image or not.\nLink to the Dataset\n\nEMNIST Dataset\n\nDescription: A dataset containing 28x28 pixel images of handwritten characters and digits, which is an extension of the MNIST dataset but includes letters.\nLink to the Dataset\n\nUCI Machine Learning Repository: Human Activity Recognition Using Smartphones\n\nDescription: A dataset with the recordings of 30 study participants performing activities of daily living (ADL) while carrying a waist-mounted smartphone with embedded inertial sensors.\nLink to the Dataset\n\nPlantVillage Dataset\n\nDescription: A dataset comprising of images of healthy and diseased crop leaves categorized based on the crop type and disease type, which could be used in a tinyML agricultural project.\nLink to the Dataset\n\nGesture Recognition using 3D Motion Sensing (3D Gesture Database)\n\nDescription: This dataset contains 3D gesture data recorded using a Leap Motion Controller, which might be useful for gesture recognition projects.\nLink to the Dataset\n\nMultilingual Spoken Words Corpus\n\nDescription: A dataset containing recordings of common spoken words in various languages, useful for speech recognition projects targeting multiple languages.\nLink to the Dataset\n\n\nRemember to verify the dataset‚Äôs license or terms of use to ensure it can be used for your intended purpose."
  },
  {
    "objectID": "learning_resources.html#books",
    "href": "learning_resources.html#books",
    "title": "Appendix D: Resources",
    "section": "D.1 Books",
    "text": "D.1 Books\nHere is a list of recommended books for learning about TinyML or embedded AI:\n\nTinyML: Machine Learning with TensorFlow Lite on Arduino and Ultra-Low-Power Microcontrollers by Pete Warden and Daniel Situnayake\nAI at the Edge: Solving Real-World Problems with Embedded Machine Learning by Daniel Situnayake\nTinyML Cookbook: Combine artificial intelligence and ultra-low-power embedded devices to make the world smarter by Gian Marco Iodice\nDeep Learning on Microcontrollers: Learn how to develop embedded AI applications using TinyML by Ashish Vaswani\nIntroduction to TinyML by Rohit Sharma\n\nThese books cover a range of topics related to TinyML and embedded AI, including:\n\nThe fundamentals of machine learning and TinyML\nHow to choose the right hardware and software for your project\nHow to train and deploy TinyML models on embedded devices\nReal-world examples of TinyML applications\n\nIn addition to the above books, there are a number of other resources available for learning about TinyML and embedded AI, including online courses, tutorials, and blog posts. Some of these are listed below. Another great way to learn is join the community of embedded AI developers."
  },
  {
    "objectID": "learning_resources.html#tutorials",
    "href": "learning_resources.html#tutorials",
    "title": "Appendix D: Resources",
    "section": "D.2 Tutorials",
    "text": "D.2 Tutorials"
  },
  {
    "objectID": "learning_resources.html#frameworks",
    "href": "learning_resources.html#frameworks",
    "title": "Appendix D: Resources",
    "section": "D.3 Frameworks",
    "text": "D.3 Frameworks\n\nGitHub Description: There are various GitHub repositories dedicated to TinyML where you can contribute or learn from existing projects. Some popular organizations/repos to check out are:\n\nTensorFlow Lite Micro: GitHub Repository\nTinyML4D: GitHub Repository\n\nStack Overflow Tags: tinyml Description: Use the ‚Äútinyml‚Äù tag on Stack Overflow to ask technical questions and find answers from the community."
  },
  {
    "objectID": "learning_resources.html#courses-and-learning-platforms",
    "href": "learning_resources.html#courses-and-learning-platforms",
    "title": "Appendix D: Resources",
    "section": "D.4 Courses and Learning Platforms",
    "text": "D.4 Courses and Learning Platforms\n\nCoursera Course: Introduction to Embedded Machine Learning Description: A dedicated course on Coursera to learn the basics and advances of TinyML.\nEdX Course: Intro to TinyML Description: Learn about TinyML with this HarvardX course."
  },
  {
    "objectID": "community.html#online-forums",
    "href": "community.html#online-forums",
    "title": "Appendix E: Communities",
    "section": "E.1 Online Forums",
    "text": "E.1 Online Forums\n\nTinyML Forum Website: TinyML Forum Description: A dedicated forum for discussions, news, and updates on TinyML.\nReddit Subreddits: r/TinyML Description: Reddit community discussing various topics related to TinyML."
  },
  {
    "objectID": "community.html#blogs-and-websites",
    "href": "community.html#blogs-and-websites",
    "title": "Appendix E: Communities",
    "section": "E.2 Blogs and Websites",
    "text": "E.2 Blogs and Websites\n\nTinyML Foundation Website: TinyML Foundation Description: The official website offers a wealth of information including research, news, and events.\nEdge Impulse Blog Website: Blog Description: Contains several articles, tutorials, and resources on TinyML.\nTiny Machine Learning Open Education Initiative (TinyMLedu) Website: TinyML Open Education Initiative Description: The website offers links to educational materials on TinyML, training events and research papers. ## Social Media Groups\nLinkedIn Groups Description: Join TinyML groups on LinkedIn to connect with professionals and enthusiasts in the field.\nTwitter Description: Follow TinyML enthusiasts, organizations, and experts on Twitter for the latest news and updates. Example handles to follow:\n\nTwitter\nEdgeImpulse"
  },
  {
    "objectID": "community.html#conferences-and-meetups",
    "href": "community.html#conferences-and-meetups",
    "title": "Appendix E: Communities",
    "section": "E.3 Conferences and Meetups",
    "text": "E.3 Conferences and Meetups\n\nTinyML Summit Website: TinyML Summit Description: Annual event where professionals and enthusiasts gather to discuss the latest developments in TinyML.\nMeetup Website: Meetup Description: Search for TinyML groups on Meetup to find local or virtual gatherings.\n\nRemember to always check the credibility and activity level of the platforms and groups before diving in to ensure a productive experience."
  },
  {
    "objectID": "case_studies.html",
    "href": "case_studies.html",
    "title": "Appendix F: Case Studies",
    "section": "",
    "text": "Learning Objectives\n\n\n\n\ncoming soon."
  }
]