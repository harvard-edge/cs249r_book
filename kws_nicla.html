<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.3.450">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="keywords" content="open-source, embedded systems, machine learning, tinyML">

<title>MACHINE LEARNING SYSTEMS - Keyword Spotting (KWS)</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./references.html" rel="next">
<link href="./kws_feature_eng.html" rel="prev">
<link href="./cover.png" rel="icon" type="image/png">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script src="https://hypothes.is/embed.js" async=""></script>


</head>

<body class="nav-sidebar floating nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg navbar-dark ">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container">
    <a class="navbar-brand" href="./index.html">
    <span class="navbar-title">MACHINE LEARNING SYSTEMS</span>
    </a>
  </div>
        <div class="quarto-navbar-tools ms-auto tools-wide">
    <a href="https://github.com/harvard-edge/cs249r_book" rel="" title="Source Code" class="quarto-navigation-tool px-1" aria-label="Source Code"><i class="bi bi-github"></i></a>
    <div class="dropdown">
      <a href="" title="Download" id="quarto-navigation-tool-dropdown-0" class="quarto-navigation-tool dropdown-toggle px-1" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download"><i class="bi bi-download"></i></a>
      <ul class="dropdown-menu dropdown-menu-end" aria-labelledby="quarto-navigation-tool-dropdown-0">
          <li>
            <a class="dropdown-item quarto-navbar-tools ms-auto-item" href="./MACHINE-LEARNING-SYSTEMS.pdf">
              <i class="bi bi-bi-file-pdf pe-1"></i>
            Download PDF
            </a>
          </li>
          <li>
            <a class="dropdown-item quarto-navbar-tools ms-auto-item" href="./MACHINE-LEARNING-SYSTEMS.epub">
              <i class="bi bi-bi-journal pe-1"></i>
            Download ePub
            </a>
          </li>
      </ul>
    </div>
    <div class="dropdown">
      <a href="" title="Share" id="quarto-navigation-tool-dropdown-1" class="quarto-navigation-tool dropdown-toggle px-1" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Share"><i class="bi bi-share"></i></a>
      <ul class="dropdown-menu dropdown-menu-end" aria-labelledby="quarto-navigation-tool-dropdown-1">
          <li>
            <a class="dropdown-item quarto-navbar-tools ms-auto-item" href="https://twitter.com/intent/tweet?url=|url|">
              <i class="bi bi-bi-twitter pe-1"></i>
            Twitter
            </a>
          </li>
          <li>
            <a class="dropdown-item quarto-navbar-tools ms-auto-item" href="https://www.facebook.com/sharer/sharer.php?u=|url|">
              <i class="bi bi-bi-facebook pe-1"></i>
            Facebook
            </a>
          </li>
      </ul>
    </div>
  <a href="" class="quarto-reader-toggle quarto-navigation-tool px-1" onclick="window.quartoToggleReader(); return false;" title="Toggle reader mode">
  <div class="quarto-reader-toggle-btn">
  <i class="bi"></i>
  </div>
</a>
</div>
          <div id="quarto-search" class="" title="Search"></div>
      </div> <!-- /container-fluid -->
    </nav>
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
      <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./niclav_sys.html">EXERCISES</a></li><li class="breadcrumb-item"><a href="./kws_nicla.html">Keyword Spotting (KWS)</a></li></ol></nav>
      <a class="flex-grow-1" role="button" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
      </a>
      <button type="button" class="btn quarto-search-button" aria-label="" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal sidebar-navigation floating overflow-auto">
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="./front.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">FRONT MATTER</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Preface</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./dedication.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Dedication</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./acknowledgements.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Acknowledgements</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./contributors.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Contributors</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./copyright.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Copyright</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./about.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">About the Book</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <span class="sidebar-item-text sidebar-link text-start">
 <span class="menu-text">MAIN</span></span>
  </li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./introduction.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Introduction</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./embedded_sys.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Embedded Systems</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./dl_primer.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Deep Learning Primer</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./embedded_ml.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Embedded AI</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./workflow.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">AI Workflow</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./data_engineering.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Data Engineering</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./frameworks.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">AI Frameworks</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./training.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">AI Training</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./efficient_ai.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Efficient AI</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./optimizations.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Model Optimizations</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./hw_acceleration.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">AI Acceleration</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./benchmarking.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">Benchmarking AI</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./ondevice_learning.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">On-Device Learning</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./ops.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">14</span>&nbsp; <span class="chapter-title">Embedded AIOps</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./privacy_security.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">15</span>&nbsp; <span class="chapter-title">Privacy and Security</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./responsible_ai.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">16</span>&nbsp; <span class="chapter-title">Responsible AI</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./generative_ai.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">17</span>&nbsp; <span class="chapter-title">Generative AI</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./ai_for_good.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">18</span>&nbsp; <span class="chapter-title">AI for Good</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./sustainable_ai.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">19</span>&nbsp; <span class="chapter-title">Sustainable AI</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./robust_ai.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">20</span>&nbsp; <span class="chapter-title">Robust AI</span></span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" aria-expanded="true">
 <span class="menu-text">EXERCISES</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./niclav_sys.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Setup Nicla Vision</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./image_classification.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">CV on Nicla Vision</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./object_detection_fomo.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Object Detection</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./kws_feature_eng.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Audio Feature Engineering</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./kws_nicla.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text">Keyword Spotting (KWS)</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./references.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">References</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" aria-expanded="true">
 <span class="menu-text">Appendices</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-4" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./tools.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">A</span>&nbsp; <span class="chapter-title">Tools</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./zoo_datasets.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">B</span>&nbsp; <span class="chapter-title">Datasets</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./zoo_models.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">C</span>&nbsp; <span class="chapter-title">Model Zoo</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./learning_resources.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">D</span>&nbsp; <span class="chapter-title">Resources</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./community.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">E</span>&nbsp; <span class="chapter-title">Communities</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./case_studies.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">F</span>&nbsp; <span class="chapter-title">Case Studies</span></span></a>
  </div>
</li>
      </ul>
  </li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#introduction" id="toc-introduction" class="nav-link active" data-scroll-target="#introduction">Introduction</a></li>
  <li><a href="#as-introduced-in-the-feature-engineering-for-audio-classification-hands-on-tutorial-keyword-spotting-kws-is-integral-to-many-voice-recognition-systems-enabling-devices-to-respond-to-specific-words-or-phrases.-while-this-technology-underpins-popular-devices-like-google-assistant-or-amazon-alexa-its-equally-applicable-and-achievable-on-smaller-low-power-devices.-this-tutorial-will-guide-you-through-implementing-a-kws-system-using-tinyml-on-the-nicla-vision-development-board-equipped-with-a-digital-microphone." id="toc-as-introduced-in-the-feature-engineering-for-audio-classification-hands-on-tutorial-keyword-spotting-kws-is-integral-to-many-voice-recognition-systems-enabling-devices-to-respond-to-specific-words-or-phrases.-while-this-technology-underpins-popular-devices-like-google-assistant-or-amazon-alexa-its-equally-applicable-and-achievable-on-smaller-low-power-devices.-this-tutorial-will-guide-you-through-implementing-a-kws-system-using-tinyml-on-the-nicla-vision-development-board-equipped-with-a-digital-microphone." class="nav-link" data-scroll-target="#as-introduced-in-the-feature-engineering-for-audio-classification-hands-on-tutorial-keyword-spotting-kws-is-integral-to-many-voice-recognition-systems-enabling-devices-to-respond-to-specific-words-or-phrases.-while-this-technology-underpins-popular-devices-like-google-assistant-or-amazon-alexa-its-equally-applicable-and-achievable-on-smaller-low-power-devices.-this-tutorial-will-guide-you-through-implementing-a-kws-system-using-tinyml-on-the-nicla-vision-development-board-equipped-with-a-digital-microphone.">As introduced in the <em>Feature Engineering for Audio Classification,</em> Hands-On tutorial, Keyword Spotting (KWS) is integral to many voice recognition systems, enabling devices to respond to specific words or phrases. While this technology underpins popular devices like Google Assistant or Amazon Alexa, it’s equally applicable and achievable on smaller, low-power devices. This tutorial will guide you through implementing a KWS system using TinyML on the Nicla Vision development board equipped with a digital microphone.</a>
  <ul class="collapse">
  <li><a href="#how-does-a-voice-assistant-work" id="toc-how-does-a-voice-assistant-work" class="nav-link" data-scroll-target="#how-does-a-voice-assistant-work">How does a voice assistant work?</a></li>
  <li><a href="#the-kws-hands-on-project" id="toc-the-kws-hands-on-project" class="nav-link" data-scroll-target="#the-kws-hands-on-project">The KWS Hands-On Project</a></li>
  </ul></li>
  <li><a href="#optionally-for-real-world-projects-it-is-always-advised-to-include-different-words-than-keywords-such-as-noise-or-background-and-unknow." id="toc-optionally-for-real-world-projects-it-is-always-advised-to-include-different-words-than-keywords-such-as-noise-or-background-and-unknow." class="nav-link" data-scroll-target="#optionally-for-real-world-projects-it-is-always-advised-to-include-different-words-than-keywords-such-as-noise-or-background-and-unknow.">&gt; Optionally for real-world projects, it is always advised to include different words than keywords, such as “Noise” (or Background) and “Unknow.”</a>
  <ul class="collapse">
  <li><a href="#the-machine-learning-workflow" id="toc-the-machine-learning-workflow" class="nav-link" data-scroll-target="#the-machine-learning-workflow">The Machine Learning workflow</a></li>
  <li><a href="#dataset" id="toc-dataset" class="nav-link" data-scroll-target="#dataset">Dataset</a>
  <ul class="collapse">
  <li><a href="#uploading-the-dataset-to-the-edge-impulse-studio" id="toc-uploading-the-dataset-to-the-edge-impulse-studio" class="nav-link" data-scroll-target="#uploading-the-dataset-to-the-edge-impulse-studio">Uploading the dataset to the Edge Impulse Studio</a></li>
  </ul></li>
  </ul></li>
  <li><a href="#the-dataset-will-now-appear-in-the-data-acquisition-section.-note-that-the-approximately-6000-samples-1500-for-each-class-are-split-between-train-4800-and-test-1200." id="toc-the-dataset-will-now-appear-in-the-data-acquisition-section.-note-that-the-approximately-6000-samples-1500-for-each-class-are-split-between-train-4800-and-test-1200." class="nav-link" data-scroll-target="#the-dataset-will-now-appear-in-the-data-acquisition-section.-note-that-the-approximately-6000-samples-1500-for-each-class-are-split-between-train-4800-and-test-1200.">The dataset will now appear in the <code>Data acquisition</code> section. Note that the approximately 6,000 samples (1,500 for each class) are split between Train (4,800) and Test (1,200).</a>
  <ul class="collapse">
  <li><a href="#uploading-the-dataset-to-the-edge-impulse-studio-1" id="toc-uploading-the-dataset-to-the-edge-impulse-studio-1" class="nav-link" data-scroll-target="#uploading-the-dataset-to-the-edge-impulse-studio-1">Uploading the dataset to the Edge Impulse Studio</a></li>
  <li><a href="#capturing-additional-audio-data" id="toc-capturing-additional-audio-data" class="nav-link" data-scroll-target="#capturing-additional-audio-data">Capturing additional Audio Data</a></li>
  </ul></li>
  <li><a href="#so-any-device-that-can-generate-audio-data-with-this-basic-specification-16khz16bits-will-work-fine.-as-a-device-we-can-use-the-proper-niclav-a-computer-or-even-your-mobile-phone." id="toc-so-any-device-that-can-generate-audio-data-with-this-basic-specification-16khz16bits-will-work-fine.-as-a-device-we-can-use-the-proper-niclav-a-computer-or-even-your-mobile-phone." class="nav-link" data-scroll-target="#so-any-device-that-can-generate-audio-data-with-this-basic-specification-16khz16bits-will-work-fine.-as-a-device-we-can-use-the-proper-niclav-a-computer-or-even-your-mobile-phone.">So, any device that can generate audio data with this basic specification (16Khz/16bits) will work fine. As a <em>device</em>, we can use the proper NiclaV, a computer, or even your mobile phone.</a></li>
  <li><a href="#all-data-on-petes-dataset-have-a-1s-length-but-the-samples-recorded-have-10s-and-must-be-split-into-1s-samples-to-be-compatible.-click-on-three-dots-after-the-sample-name-and-select-split-sample." id="toc-all-data-on-petes-dataset-have-a-1s-length-but-the-samples-recorded-have-10s-and-must-be-split-into-1s-samples-to-be-compatible.-click-on-three-dots-after-the-sample-name-and-select-split-sample." class="nav-link" data-scroll-target="#all-data-on-petes-dataset-have-a-1s-length-but-the-samples-recorded-have-10s-and-must-be-split-into-1s-samples-to-be-compatible.-click-on-three-dots-after-the-sample-name-and-select-split-sample.">All data on Pete’s dataset have a 1s length, but the samples recorded have 10s and must be split into 1s samples to be compatible. Click on <code>three dots</code> after the sample name and select <code>Split sample</code>.</a></li>
  <li><a href="#go-to-devices-and-using-your-phone-scan-the-qr-code-and-click-on-the-link.-a-data-collection-app-will-appear-in-your-browser.-select-collecting-audio-and-define-your-label-data-capture-length-and-category." id="toc-go-to-devices-and-using-your-phone-scan-the-qr-code-and-click-on-the-link.-a-data-collection-app-will-appear-in-your-browser.-select-collecting-audio-and-define-your-label-data-capture-length-and-category." class="nav-link" data-scroll-target="#go-to-devices-and-using-your-phone-scan-the-qr-code-and-click-on-the-link.-a-data-collection-app-will-appear-in-your-browser.-select-collecting-audio-and-define-your-label-data-capture-length-and-category.">Go to <code>Devices</code>, and using your phone scan the <code>QR Code</code>, and click on the link. A data Collection app will appear in your browser. Select <code>Collecting Audio</code>, and define your <code>Label</code>, data capture <code>Length,</code> and <code>Category</code>.</a>
  <ul class="collapse">
  <li><a href="#creating-impulse-pre-process-model-definition" id="toc-creating-impulse-pre-process-model-definition" class="nav-link" data-scroll-target="#creating-impulse-pre-process-model-definition">Creating Impulse (Pre-Process / Model definition)</a>
  <ul class="collapse">
  <li><a href="#impulse-design" id="toc-impulse-design" class="nav-link" data-scroll-target="#impulse-design">Impulse Design</a></li>
  </ul></li>
  </ul></li>
  <li><a href="#each-1-second-audio-sample-should-be-pre-processed-and-converted-to-an-image-for-example-13-x-49-x-1.-as-discussed-in-the-feature-engineering-for-audio-classification-hands-on-tutorial-we-will-use-audio-mfcc-which-extracts-features-from-audio-signals-using-mel-frequency-cepstral-coefficients-which-are-applicable-to-the-human-voice-that-is-our-case-here." id="toc-each-1-second-audio-sample-should-be-pre-processed-and-converted-to-an-image-for-example-13-x-49-x-1.-as-discussed-in-the-feature-engineering-for-audio-classification-hands-on-tutorial-we-will-use-audio-mfcc-which-extracts-features-from-audio-signals-using-mel-frequency-cepstral-coefficients-which-are-applicable-to-the-human-voice-that-is-our-case-here." class="nav-link" data-scroll-target="#each-1-second-audio-sample-should-be-pre-processed-and-converted-to-an-image-for-example-13-x-49-x-1.-as-discussed-in-the-feature-engineering-for-audio-classification-hands-on-tutorial-we-will-use-audio-mfcc-which-extracts-features-from-audio-signals-using-mel-frequency-cepstral-coefficients-which-are-applicable-to-the-human-voice-that-is-our-case-here.">Each 1-second audio sample should be pre-processed and converted to an image (for example, 13 x 49 x 1). As discussed in the <em>Feature Engineering for Audio Classification,</em> Hands-On tutorial, We will use <code>Audio (MFCC)</code>, which extracts features from audio signals using Mel Frequency Cepstral Coefficients, which are applicable to the human voice that is our case here.</a>
  <ul class="collapse">
  <li><a href="#pre-processing-mfcc" id="toc-pre-processing-mfcc" class="nav-link" data-scroll-target="#pre-processing-mfcc">Pre-Processing (MFCC)</a></li>
  </ul></li>
  <li><a href="#the-result-shows-that-we-will-only-spend-a-little-memory-to-pre-process-data-16kb-with-a-latency-of-34ms-which-is-excellent.-for-example-on-an-arduino-nano-cortex-m4f-64mhz-this-same-pre-process-will-take-around-480ms.-the-chosen-parameters-such-as-the-fft-length-512-will-significantly-impact-the-latency." id="toc-the-result-shows-that-we-will-only-spend-a-little-memory-to-pre-process-data-16kb-with-a-latency-of-34ms-which-is-excellent.-for-example-on-an-arduino-nano-cortex-m4f-64mhz-this-same-pre-process-will-take-around-480ms.-the-chosen-parameters-such-as-the-fft-length-512-will-significantly-impact-the-latency." class="nav-link" data-scroll-target="#the-result-shows-that-we-will-only-spend-a-little-memory-to-pre-process-data-16kb-with-a-latency-of-34ms-which-is-excellent.-for-example-on-an-arduino-nano-cortex-m4f-64mhz-this-same-pre-process-will-take-around-480ms.-the-chosen-parameters-such-as-the-fft-length-512-will-significantly-impact-the-latency.">The result shows that we will only spend a little memory to pre-process data (16KB), with a latency of 34ms, which is excellent. For example, on an Arduino Nano (Cortex-M4f @ 64MHz), this same pre-process will take around 480ms. The chosen parameters, such as the <code>FFT length</code> [512], will significantly impact the latency.</a>
  <ul class="collapse">
  <li><a href="#going-under-the-hood" id="toc-going-under-the-hood" class="nav-link" data-scroll-target="#going-under-the-hood">Going under the hood</a></li>
  <li><a href="#model-design-and-training" id="toc-model-design-and-training" class="nav-link" data-scroll-target="#model-design-and-training">Model Design and Training</a></li>
  </ul></li>
  <li><a href="#for-better-visualization-open-the-image-on-another-tab-use-the-right-button" id="toc-for-better-visualization-open-the-image-on-another-tab-use-the-right-button" class="nav-link" data-scroll-target="#for-better-visualization-open-the-image-on-another-tab-use-the-right-button"><img src="images/imgs_kws_nicla/models_1d-2d.jpg" class="img-fluid" style="width:6.5in" data-fig-align="center" alt="For better visualization, open the image on another tab (use the right button)"></a>
  <ul class="collapse">
  <li><a href="#going-under-the-hood-1" id="toc-going-under-the-hood-1" class="nav-link" data-scroll-target="#going-under-the-hood-1">Going under the hood</a></li>
  <li><a href="#model-design-and-training-1" id="toc-model-design-and-training-1" class="nav-link" data-scroll-target="#model-design-and-training-1">Model Design and Training</a>
  <ul class="collapse">
  <li><a href="#going-under-the-hood-2" id="toc-going-under-the-hood-2" class="nav-link" data-scroll-target="#going-under-the-hood-2">Going under the hood</a></li>
  </ul></li>
  </ul></li>
  <li><a href="#if-you-want-to-understand-what-is-happening-under-the-hood-you-can-download-the-pre-processed-dataset-mfcc-training-data-from-the-dashboard-tab-and-run-this-jupyter-notebook-playing-with-the-code-open-in-colab.-for-example-you-can-analyze-the-accuracy-by-each-epoch" id="toc-if-you-want-to-understand-what-is-happening-under-the-hood-you-can-download-the-pre-processed-dataset-mfcc-training-data-from-the-dashboard-tab-and-run-this-jupyter-notebook-playing-with-the-code-open-in-colab.-for-example-you-can-analyze-the-accuracy-by-each-epoch" class="nav-link" data-scroll-target="#if-you-want-to-understand-what-is-happening-under-the-hood-you-can-download-the-pre-processed-dataset-mfcc-training-data-from-the-dashboard-tab-and-run-this-jupyter-notebook-playing-with-the-code-open-in-colab.-for-example-you-can-analyze-the-accuracy-by-each-epoch">If you want to understand what is happening “under the hood,” you can download the pre-processed dataset (<code>MFCC training data</code>) from the <code>Dashboard</code> tab and run this Jupyter Notebook, playing with the code [Open In Colab]. For example, you can analyze the accuracy by each epoch:</a>
  <ul class="collapse">
  <li><a href="#going-under-the-hood-3" id="toc-going-under-the-hood-3" class="nav-link" data-scroll-target="#going-under-the-hood-3">Going under the hood</a></li>
  <li><a href="#testing" id="toc-testing" class="nav-link" data-scroll-target="#testing">Testing</a>
  <ul class="collapse">
  <li><a href="#live-classification" id="toc-live-classification" class="nav-link" data-scroll-target="#live-classification">Live Classification</a></li>
  </ul></li>
  <li><a href="#deploy-and-inference" id="toc-deploy-and-inference" class="nav-link" data-scroll-target="#deploy-and-inference">Deploy and Inference</a></li>
  </ul></li>
  <li><a href="#now-it-is-time-for-a-real-test.-we-will-make-inferences-wholly-disconnected-from-the-studio.-lets-use-the-nicla-vision-code-example-created-when-you-deploy-the-arduino-library." id="toc-now-it-is-time-for-a-real-test.-we-will-make-inferences-wholly-disconnected-from-the-studio.-lets-use-the-nicla-vision-code-example-created-when-you-deploy-the-arduino-library." class="nav-link" data-scroll-target="#now-it-is-time-for-a-real-test.-we-will-make-inferences-wholly-disconnected-from-the-studio.-lets-use-the-nicla-vision-code-example-created-when-you-deploy-the-arduino-library.">Now, it is time for a real test. We will make inferences wholly disconnected from the Studio. Let’s use the Nicla Vision code example created when you deploy the Arduino Library.</a>
  <ul class="collapse">
  <li><a href="#live-classification-1" id="toc-live-classification-1" class="nav-link" data-scroll-target="#live-classification-1">Live Classification</a></li>
  <li><a href="#deploy-and-inference-1" id="toc-deploy-and-inference-1" class="nav-link" data-scroll-target="#deploy-and-inference-1">Deploy and Inference</a></li>
  <li><a href="#post-processing" id="toc-post-processing" class="nav-link" data-scroll-target="#post-processing">Post-processing</a></li>
  </ul></li>
  <li><a href="#the-idea-is-that-whenever-the-keyword-yes-is-detected-the-led-green-will-be-on-if-it-is-no-led-red-will-be-on-if-it-is-a-unknow-led-blue-will-be-on-and-if-it-is-noise-no-keyword-the-leds-will-be-off." id="toc-the-idea-is-that-whenever-the-keyword-yes-is-detected-the-led-green-will-be-on-if-it-is-no-led-red-will-be-on-if-it-is-a-unknow-led-blue-will-be-on-and-if-it-is-noise-no-keyword-the-leds-will-be-off." class="nav-link" data-scroll-target="#the-idea-is-that-whenever-the-keyword-yes-is-detected-the-led-green-will-be-on-if-it-is-no-led-red-will-be-on-if-it-is-a-unknow-led-blue-will-be-on-and-if-it-is-noise-no-keyword-the-leds-will-be-off.">The idea is that whenever the keyword YES is detected, the LED Green will be ON; if it is NO, LED Red will be ON, if it is a UNKNOW, LED Blue will be ON; and if it is noise (No Keyword), the LEDs will be OFF.</a>
  <ul class="collapse">
  <li><a href="#conclusion" id="toc-conclusion" class="nav-link" data-scroll-target="#conclusion">Conclusion</a></li>
  </ul></li>
  </ul>
<div class="toc-actions"><div><i class="bi bi-github"></i></div><div class="action-links"><p><a href="https://github.com/harvard-edge/cs249r_book/edit/main/kws_nicla.qmd" class="toc-action">Edit this page</a></p><p><a href="https://github.com/harvard-edge/cs249r_book/issues/new" class="toc-action">Report an issue</a></p><p><a href="https://github.com/harvard-edge/cs249r_book/blob/main/kws_nicla.qmd" class="toc-action">View source</a></p></div></div></nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Keyword Spotting (KWS)</h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  

</header>

<section id="introduction" class="level2">
<h2 class="anchored" data-anchor-id="introduction">Introduction</h2>
<p>&lt;&lt;&lt;&lt;&lt;&lt;&lt; Updated upstream Having already explored the Nicla Vision board in Computer Vision applications, such as <em>Image Classification</em> and <em>Object Detection</em>, we are now shifting our focus to voice-activated applications with a project on Keyword Spotting (KWS).</p>
</section>
<section id="as-introduced-in-the-feature-engineering-for-audio-classification-hands-on-tutorial-keyword-spotting-kws-is-integral-to-many-voice-recognition-systems-enabling-devices-to-respond-to-specific-words-or-phrases.-while-this-technology-underpins-popular-devices-like-google-assistant-or-amazon-alexa-its-equally-applicable-and-achievable-on-smaller-low-power-devices.-this-tutorial-will-guide-you-through-implementing-a-kws-system-using-tinyml-on-the-nicla-vision-development-board-equipped-with-a-digital-microphone." class="level1">
<h1>As introduced in the <em>Feature Engineering for Audio Classification,</em> Hands-On tutorial, Keyword Spotting (KWS) is integral to many voice recognition systems, enabling devices to respond to specific words or phrases. While this technology underpins popular devices like Google Assistant or Amazon Alexa, it’s equally applicable and achievable on smaller, low-power devices. This tutorial will guide you through implementing a KWS system using TinyML on the Nicla Vision development board equipped with a digital microphone.</h1>
<p>Having already explored the Nicla Vision board in the <em>Image Classification</em> and <em>Object Detection</em> applications, we are now shifting our focus to voice-activated applications with a project on Keyword Spotting (KWS).</p>
<p>As introduced in the <em>Feature Engineering for Audio Classification</em> Hands-On tutorial, Keyword Spotting (KWS) is integrated into many voice recognition systems, enabling devices to respond to specific words or phrases. While this technology underpins popular devices like Google Assistant or Amazon Alexa, it’s equally applicable and feasible on smaller, low-power devices. This tutorial will guide you through implementing a KWS system using TinyML on the Nicla Vision development board equipped with a digital microphone. &gt;&gt;&gt;&gt;&gt;&gt;&gt; Stashed changes</p>
<p>Our model will be designed to recognize keywords that can trigger device wake-up or specific actions, bringing them to life with voice-activated commands.</p>
<section id="how-does-a-voice-assistant-work" class="level2">
<h2 class="anchored" data-anchor-id="how-does-a-voice-assistant-work">How does a voice assistant work?</h2>
<p>As said, <em>voice assistants</em> on the market, like Google Home or Amazon Echo-Dot, only react to humans when they are “waked up” by particular keywords such as ” Hey Google” on the first one and “Alexa” on the second.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/imgs_kws_nicla/hey_google.png" class="img-fluid figure-img" style="width:6.5in"></p>
</figure>
</div>
<p>In other words, recognizing voice commands is based on a multi-stage model or Cascade Detection.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/imgs_kws_nicla/pa_block.jpg" class="img-fluid figure-img" style="width:6.5in"></p>
</figure>
</div>
<p>&lt;&lt;&lt;&lt;&lt;&lt;&lt; Updated upstream <strong>Stage 1:</strong> A smaller microprocessor inside the Echo Dot or Google Home <strong>continuously</strong> listens to the sound, waiting for the keyword to be spotted. For such detection, a TinyML model at the edge is used (KWS application). ======= <strong>Stage 1:</strong> A small microprocessor inside the Echo Dot or Google Home continuously listens, waiting for the keyword to be spotted, using a TinyML model at the edge (KWS application). &gt;&gt;&gt;&gt;&gt;&gt;&gt; Stashed changes</p>
<p><strong>Stage 2:</strong> Only when triggered by the KWS application on Stage 1 is the data sent to the cloud and processed on a larger model.</p>
<p>The video below shows an example of a Google Assistant being programmed on a Raspberry Pi (Stage 2), with an Arduino Nano 33 BLE as the tinyML device (Stage 1).</p>
<div class="quarto-video"><iframe data-external="1" src="https://www.youtube.com/embed/e_OPgcnsyvM" width="480" height="270" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div>
<blockquote class="blockquote">
<p>To explore the above Google Assistant project, please see the tutorial: <a href="https://www.hackster.io/mjrobot/building-an-intelligent-voice-assistant-from-scratch-2199c3">Building an Intelligent Voice Assistant From Scratch</a>.</p>
</blockquote>
<p>&lt;&lt;&lt;&lt;&lt;&lt;&lt; Updated upstream In this KWS project, we will focus on Stage 1 ( KWS or Keyword Spotting), where we will use the XIAO ESP2S3 Sense, which has a digital microphone that will be used to spot the keyword. ======= In this KWS project, we will focus on Stage 1 (KWS or Keyword Spotting), where we will use the XIAO ESP2S3 Sense, which has a digital microphone that will be used to spot the keyword. &gt;&gt;&gt;&gt;&gt;&gt;&gt; Stashed changes</p>
</section>
<section id="the-kws-hands-on-project" class="level2">
<h2 class="anchored" data-anchor-id="the-kws-hands-on-project">The KWS Hands-On Project</h2>
<p>The diagram below gives an idea of how the final KWS application should work (during inference):</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/imgs_kws_nicla/KWS_PROJ_INF_BLK.jpg" class="img-fluid figure-img" style="width:6.5in"></p>
</figure>
</div>
<p>Our KWS application will recognize four classes of sound:</p>
<ul>
<li><strong>YES</strong> (Keyword 1)</li>
<li><strong>NO</strong> (Keyword 2) &lt;&lt;&lt;&lt;&lt;&lt;&lt; Updated upstream</li>
<li><strong>NOISE</strong> (no keywords spoken, only background noise is present)</li>
<li><strong>UNKNOW</strong> (a mix of different words than YES and NO)</li>
</ul>
</section>
</section>
<section id="optionally-for-real-world-projects-it-is-always-advised-to-include-different-words-than-keywords-such-as-noise-or-background-and-unknow." class="level1">
<h1>&gt; Optionally for real-world projects, it is always advised to include different words than keywords, such as “Noise” (or Background) and “Unknow.”</h1>
<ul>
<li><strong>NOISE</strong> (no words spoken; only background noise is present)</li>
<li><strong>UNKNOW</strong> (a mix of different words than YES and NO)</li>
</ul>
<blockquote class="blockquote">
<p>For real-world projects, it is always advisable to include other sounds besides the keywords, such as “Noise” (or Background) and “Unknown.” &gt;&gt;&gt;&gt;&gt;&gt; Stashed changes</p>
</blockquote>
<section id="the-machine-learning-workflow" class="level3">
<h3 class="anchored" data-anchor-id="the-machine-learning-workflow">The Machine Learning workflow</h3>
<p>The main component of the KWS application is its model. So, we must train such a model with our specific keywords, noise, and other words (the “unknown”):</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/imgs_kws_nicla/KWS_PROJ_TRAIN_BLK.jpg" class="img-fluid figure-img" style="width:6.5in"></p>
</figure>
</div>
</section>
<section id="dataset" class="level2">
<h2 class="anchored" data-anchor-id="dataset">Dataset</h2>
<p>&lt;&lt;&lt;&lt;&lt;&lt;&lt; Updated upstream The critical component of any Machine Learning Workflow is the <strong>dataset</strong>. Once we have decided on specific keywords (<em>YES</em> and NO), we can take advantage of the dataset developed by Pete Warden, <a href="https://arxiv.org/pdf/1804.03209.pdf">“Speech Commands: A Dataset for Limited-Vocabulary Speech Recognition</a>.” This dataset has 35 keywords (with +1,000 samples each), such as yes, no, stop, and go. In some words, we can get 1,500 samples, such as <em>yes</em> and <em>no</em>. ======= The critical component of any Machine Learning Workflow is the <strong>dataset</strong>. Once we have decided on specific keywords, in our case (<em>YES</em> and NO), we can take advantage of the dataset developed by Pete Warden, <a href="https://arxiv.org/pdf/1804.03209.pdf">“Speech Commands: A Dataset for Limited-Vocabulary Speech Recognition</a>.” This dataset has 35 keywords (with +1,000 samples each), such as yes, no, stop, and go. In words such as <em>yes</em> and <em>no,</em> we can get 1,500 samples. &gt;&gt;&gt;&gt;&gt;&gt;&gt; Stashed changes</p>
<p>You can download a small portion of the dataset from Edge Studio (<a href="https://docs.edgeimpulse.com/docs/pre-built-datasets/keyword-spotting">Keyword spotting pre-built dataset</a>), which includes samples from the four classes we will use in this project: yes, no, noise, and background. For this, follow the steps below:</p>
<ul>
<li>Download the <a href="https://cdn.edgeimpulse.com/datasets/keywords2.zip">keywords dataset.</a> &lt;&lt;&lt;&lt;&lt;&lt;&lt; Updated upstream</li>
<li>Unzip the file in a location of your choice.</li>
</ul>
<section id="uploading-the-dataset-to-the-edge-impulse-studio" class="level3">
<h3 class="anchored" data-anchor-id="uploading-the-dataset-to-the-edge-impulse-studio">Uploading the dataset to the Edge Impulse Studio</h3>
<p>Initiate a new project at Edge Impulse Studio.</p>
<blockquote class="blockquote">
<p>Here, you can clone the project developed for this hands-on: <a href="https://studio.edgeimpulse.com/public/292418/latest">Nicla-Vision-KWS</a>.</p>
</blockquote>
<p>Select the <code>Upload Existing Data</code> tool in the <code>Data Acquisition</code> section. Choose the files to be uploaded:</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/imgs_kws_nicla/files.jpg" class="img-fluid figure-img" style="width:6.5in"></p>
</figure>
</div>
<p>Define the Label, select <code>Automatically split between train and test,</code> and <code>Upload data</code> to the Studio. Repete to all classes.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/imgs_kws_nicla/upload.jpg" class="img-fluid figure-img" style="width:6.5in"></p>
</figure>
</div>
</section>
</section>
</section>
<section id="the-dataset-will-now-appear-in-the-data-acquisition-section.-note-that-the-approximately-6000-samples-1500-for-each-class-are-split-between-train-4800-and-test-1200." class="level1">
<h1>The dataset will now appear in the <code>Data acquisition</code> section. Note that the approximately 6,000 samples (1,500 for each class) are split between Train (4,800) and Test (1,200).</h1>
<ul>
<li>Unzip the file to a location of your choice.</li>
</ul>
<section id="uploading-the-dataset-to-the-edge-impulse-studio-1" class="level3">
<h3 class="anchored" data-anchor-id="uploading-the-dataset-to-the-edge-impulse-studio-1">Uploading the dataset to the Edge Impulse Studio</h3>
<p>Initiate a new project at Edge Impulse Studio (EIS) and select the <code>Upload Existing Data</code> tool in the <code>Data Acquisition</code> section. Choose the files to be uploaded:</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/imgs_kws_nicla/files.jpg" class="img-fluid figure-img" style="width:6.5in"></p>
</figure>
</div>
<p>Define the Label, select <code>Automatically split between train and test,</code> and <code>Upload data</code> to the EIS. Repeat for all classes.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/imgs_kws_nicla/upload.jpg" class="img-fluid figure-img" style="width:6.5in"></p>
</figure>
</div>
<p>The dataset will now appear in the <code>Data acquisition</code> section. Note that the approximately 6,000 samples (1,500 for each class) are split into Train (4,800) and Test (1,200) sets. &gt;&gt;&gt;&gt;&gt;&gt;&gt; Stashed changes</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/imgs_kws_nicla/dataset.jpg" class="img-fluid figure-img" style="width:6.5in"></p>
</figure>
</div>
</section>
<section id="capturing-additional-audio-data" class="level3">
<h3 class="anchored" data-anchor-id="capturing-additional-audio-data">Capturing additional Audio Data</h3>
<p>&lt;&lt;&lt;&lt;&lt;&lt;&lt; Updated upstream Although we have a lot of data from Pete’s dataset, collecting some words spoken by us is advised. When working with accelerometers, creating a dataset with data captured by the same type of sensor is essential. Still, In the case of <em>sound</em>, it is different because what we will classify is, in reality, <em>audio</em> data.</p>
<blockquote class="blockquote">
<p>The key difference between sound and audio is their form of energy. Sound is mechanical wave energy (longitudinal sound waves) that propagate through a medium causing variations in pressure within the medium. Audio is made of electrical energy (analog or digital signals) that represent sound electrically.</p>
</blockquote>
<p>The sound waves should be converted to audio data when we speak a keyword. The conversion should be done by sampling the signal generated by the microphone in 16KHz with a 16-bit depth.</p>
</section>
</section>
<section id="so-any-device-that-can-generate-audio-data-with-this-basic-specification-16khz16bits-will-work-fine.-as-a-device-we-can-use-the-proper-niclav-a-computer-or-even-your-mobile-phone." class="level1">
<h1>So, any device that can generate audio data with this basic specification (16Khz/16bits) will work fine. As a <em>device</em>, we can use the proper NiclaV, a computer, or even your mobile phone.</h1>
<p>Although we have a lot of data from Pete’s dataset, collecting some words spoken by us is advised. When working with accelerometers, creating a dataset with data captured by the same type of sensor is essential. In the case of <em>sound</em>, this is optional because what we will classify is, in reality, <em>audio</em> data.</p>
<blockquote class="blockquote">
<p>The key difference between sound and audio is the type of energy. Sound is mechanical perturbation (longitudinal sound waves) that propagate through a medium, causing variations of pressure in it. Audio is an electrical (analog or digital) signal representing sound.</p>
</blockquote>
<p>When we pronounce a keyword, the sound waves should be converted to audio data. The conversion should be done by sampling the signal generated by the microphone at a 16KHz frequency with 16-bit per sample amplitude.</p>
<p>So, any device that can generate audio data with this basic specification (16KHz/16bits) will work fine. As a <em>device</em>, we can use the NiclaV, a computer, or even your mobile phone. &gt;&gt;&gt;&gt;&gt;&gt;&gt; Stashed changes</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/imgs_kws_nicla/audio_capt.jpg" class="img-fluid figure-img" style="width:6.5in"></p>
</figure>
</div>
<section id="using-the-niclav-and-the-edge-impulse-studio" class="level4">
<h4 class="anchored" data-anchor-id="using-the-niclav-and-the-edge-impulse-studio">Using the NiclaV and the Edge Impulse Studio</h4>
<p>&lt;&lt;&lt;&lt;&lt;&lt;&lt; Updated upstream As we learned in the chapter <em>Setup Nicla Vision</em>, Edge Impulse officially supports the Nicla Vision, which simplifies the capture of the data from its sensors, such as the microphone. So, please create a new project on the Studio and connect the Nicla to it. For that, follow the steps:</p>
<ul>
<li><p>Download the most updated <a href="https://cdn.edgeimpulse.com/firmware/arduino-nicla-vision.zip">EI Firmware</a> and unzip it.</p></li>
<li><p>Open the zip file on your computer and select the uploader corresponding to your OS:</p></li>
</ul>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images_2/media/image17.png" class="img-fluid figure-img" style="width:4.41667in"></p>
</figure>
</div>
<ul>
<li>Put the Nicla-Vision on Boot Mode, pressing the reset button twice.</li>
</ul>
<p><img src="https://84771188-files.gitbook.io/~/files/v0/b/gitbook-x-prod.appspot.com/o/spaces%2FGEgcCk4PkS5Pa6uBabld%2Fuploads%2Fgit-blob-111b26f413cd411b29594c377868bba901863233%2Fnicla_bootloader.gif?alt=media" class="img-fluid"></p>
<ul>
<li>Execute the specific batch code for your OS for uploading the binary <em>arduino-nicla-vision.bin</em> to your board.</li>
</ul>
<p>Go to your project on the Studio, and on the <code>Data Acquisition tab</code>, select <code>WebUSB</code>. A window will pop up; choose the option that shows that the <code>Nicla is paired</code> and press <code>[Connect]</code>.</p>
<p>You can choose which sensor data to pick in the <code>Collect Data</code> section on the <code>Data Acquisition</code> tab. Select: <code>Built-in microphone</code>, define your <code>label</code> (for example, <em>yes</em>), the sampling <code>Frequency</code>[16000Hz], and the <code>Sample length (ms.)</code>, for example [10s]. <code>Start sampling</code>.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/imgs_kws_nicla/ei_data_collection.jpg" class="img-fluid figure-img" style="width:6.5in"></p>
</figure>
</div>
</section>
</section>
<section id="all-data-on-petes-dataset-have-a-1s-length-but-the-samples-recorded-have-10s-and-must-be-split-into-1s-samples-to-be-compatible.-click-on-three-dots-after-the-sample-name-and-select-split-sample." class="level1">
<h1>All data on Pete’s dataset have a 1s length, but the samples recorded have 10s and must be split into 1s samples to be compatible. Click on <code>three dots</code> after the sample name and select <code>Split sample</code>.</h1>
<p>As we learned in the chapter <em>Setup Nicla Vision</em>, EIS officially supports the Nicla Vision, which simplifies the capture of the data from its sensors, including the microphone. So, please create a new project on EIS and connect the Nicla to it, following these steps:</p>
<ul>
<li><p>Download the last updated <a href="https://cdn.edgeimpulse.com/firmware/arduino-nicla-vision.zip">EIS Firmware</a> and unzip it.</p></li>
<li><p>Open the zip file on your computer and select the uploader corresponding to your OS:</p></li>
</ul>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/imgs_niclav_sys/image17.png" class="img-fluid figure-img" style="width:4.41667in"></p>
</figure>
</div>
<ul>
<li>Put the NiclaV in Boot Mode by pressing the reset button twice.</li>
</ul>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://84771188-files.gitbook.io/~/files/v0/b/gitbook-x-prod.appspot.com/o/spaces%2FGEgcCk4PkS5Pa6uBabld%2Fuploads%2Fgit-blob-111b26f413cd411b29594c377868bba901863233%2Fnicla_bootloader.gif?alt=media" class="img-fluid figure-img" style="width:6.5in"></p>
</figure>
</div>
<ul>
<li>Upload the binary <em>arduino-nicla-vision.bin</em> to your board by running the batch code corresponding to your OS.</li>
</ul>
<p>Go to your project on EIS, and on the <code>Data Acquisition tab</code>, select <code>WebUSB</code>. A window will pop up; choose the option that shows that the <code>Nicla is paired</code> and press <code>[Connect]</code>.</p>
<p>You can choose which sensor data to pick in the <code>Collect Data</code> section on the <code>Data Acquisition</code> tab. Select: <code>Built-in microphone</code>, define your <code>label</code> (for example, <em>yes</em>), the sampling <code>Frequency</code>[16000Hz], and the <code>Sample length (in milliseconds)</code>, for example [10s]. <code>Start sampling</code>.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/imgs_kws_nicla/ei_data_collection.jpg" class="img-fluid figure-img" style="width:6.5in"></p>
</figure>
</div>
<p>Data on Pete’s dataset have a length of 1s, but the recorded samples are 10s long and must be split into 1s samples. Click on <code>three dots</code> after the sample name and select <code>Split sample</code>. &gt;&gt;&gt;&gt;&gt;&gt;&gt; Stashed changes</p>
<p>A window will pop up with the Split tool.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/imgs_kws_nicla/split.jpg" class="img-fluid figure-img" style="width:6.5in"></p>
</figure>
</div>
<p>Once inside the tool, split the data into 1-second (1000 ms) records. If necessary, add or remove segments. This procedure should be repeated for all new samples.</p>
<p>&lt;&lt;&lt;&lt;&lt;&lt;&lt; Updated upstream #### Using a smartphone and the Studio</p>
<p>Alternatively, you can use your PC or smartphone to capture audio data with a sampling frequency of 16KHz and a bit depth of 16 Bits.</p>
</section>
<section id="go-to-devices-and-using-your-phone-scan-the-qr-code-and-click-on-the-link.-a-data-collection-app-will-appear-in-your-browser.-select-collecting-audio-and-define-your-label-data-capture-length-and-category." class="level1">
<h1>Go to <code>Devices</code>, and using your phone scan the <code>QR Code</code>, and click on the link. A data Collection app will appear in your browser. Select <code>Collecting Audio</code>, and define your <code>Label</code>, data capture <code>Length,</code> and <code>Category</code>.</h1>
<section id="using-a-smartphone-and-the-ei-studio" class="level4">
<h4 class="anchored" data-anchor-id="using-a-smartphone-and-the-ei-studio">Using a smartphone and the EI Studio</h4>
<p>You can also use your PC or smartphone to capture audio data, using a sampling frequency of 16KHz and a bit depth of 16.</p>
<p>Go to <code>Devices</code>, scan the <code>QR Code</code> using your phone, and click on the link. A data Collection app will appear in your browser. Select <code>Collecting Audio</code>, and define your <code>Label</code>, data capture <code>Length,</code> and <code>Category</code>. &gt;&gt;&gt;&gt;&gt;&gt;&gt; Stashed changes</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/imgs_kws_nicla/phone.jpg" class="img-fluid figure-img" style="width:6.5in"></p>
</figure>
</div>
<p>Repeat the same procedure used with the NiclaV.</p>
<p>&lt;&lt;&lt;&lt;&lt;&lt;&lt; Updated upstream &gt; Note that any app, such as <a href="https://www.audacityteam.org/">Audacity</a>, can be used for audio recording since you have 16KHz/16-bit depth samples. ======= &gt; Note that any app, such as <a href="https://www.audacityteam.org/">Audacity</a>, can be used for audio recording, provided you use 16KHz/16-bit depth samples. &gt;&gt;&gt;&gt;&gt;&gt;&gt; Stashed changes</p>
</section>
<section id="creating-impulse-pre-process-model-definition" class="level2">
<h2 class="anchored" data-anchor-id="creating-impulse-pre-process-model-definition">Creating Impulse (Pre-Process / Model definition)</h2>
<p><em>An</em> <strong>impulse</strong> <em>takes raw data, uses signal processing to extract features, and then uses a learning block to classify new data.</em></p>
<section id="impulse-design" class="level3">
<h3 class="anchored" data-anchor-id="impulse-design">Impulse Design</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/imgs_kws_nicla/impulse.jpg" class="img-fluid figure-img" style="width:6.5in"></p>
</figure>
</div>
<p>&lt;&lt;&lt;&lt;&lt;&lt;&lt; Updated upstream First, we will take the data points with a 1-second window, augmenting the data, sliding that window each 500ms. Note that the option zero-pad data is set. It is essential to fill with zeros samples smaller than 1 second (in some cases, some samples can result smaller than the 1000 ms window on the split tool to avoid noises and spikes).</p>
</section>
</section>
</section>
<section id="each-1-second-audio-sample-should-be-pre-processed-and-converted-to-an-image-for-example-13-x-49-x-1.-as-discussed-in-the-feature-engineering-for-audio-classification-hands-on-tutorial-we-will-use-audio-mfcc-which-extracts-features-from-audio-signals-using-mel-frequency-cepstral-coefficients-which-are-applicable-to-the-human-voice-that-is-our-case-here." class="level1">
<h1>Each 1-second audio sample should be pre-processed and converted to an image (for example, 13 x 49 x 1). As discussed in the <em>Feature Engineering for Audio Classification,</em> Hands-On tutorial, We will use <code>Audio (MFCC)</code>, which extracts features from audio signals using <a href="https://en.wikipedia.org/wiki/Mel-frequency_cepstrum">Mel Frequency Cepstral Coefficients</a>, which are applicable to the human voice that is our case here.</h1>
<p>First, we will take the data points with a 1-second window, augmenting the data and sliding that window in 500ms intervals. Note that the option zero-pad data is set. It is essential to fill with ‘zeros’ samples smaller than 1 second (in some cases, some samples can result smaller than the 1000 ms window on the split tool to avoid noise and spikes).</p>
<p>Each 1-second audio sample should be pre-processed and converted to an image (for example, 13 x 49 x 1). As discussed in the <em>Feature Engineering for Audio Classification</em> Hands-On tutorial, we will use <code>Audio (MFCC)</code>, which extracts features from audio signals using <a href="https://en.wikipedia.org/wiki/Mel-frequency_cepstrum">Mel Frequency Cepstral Coefficients</a>, which are well suited for the human voice, our case here. &gt;&gt;&gt;&gt;&gt;&gt;&gt; Stashed changes</p>
<p>Next, we select the <code>Classification</code> block to build our model from scratch using a Convolution Neural Network (CNN).</p>
<blockquote class="blockquote">
<p>Alternatively, you can use the <code>Transfer Learning (Keyword Spotting)</code> block, which fine-tunes a pre-trained keyword spotting model on your data. This approach has good performance with relatively small keyword datasets.</p>
</blockquote>
<section id="pre-processing-mfcc" class="level3">
<h3 class="anchored" data-anchor-id="pre-processing-mfcc">Pre-Processing (MFCC)</h3>
<p>&lt;&lt;&lt;&lt;&lt;&lt;&lt; Updated upstream The next step is to create the features to be trained in the next phase:</p>
<p>We can keep the default parameter values or take advantage of the DSP <code>Autotune parameters</code> option, which we will do.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/imgs_kws_nicla/ei_MFCC.jpg" class="img-fluid figure-img" style="width:6.5in"></p>
</figure>
</div>
<p>We will take the <code>Raw features</code> (our 1-second, 16Khz sampled audio data) and use the MFCC processing block to calculate the <code>Processed features</code>. For every 16,000 raw features (16,000 x 1 second), we will get 637 processed features (13 x 49).</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/imgs_kws_nicla/MFCC.jpg" class="img-fluid figure-img" style="width:6.5in"></p>
</figure>
</div>
</section>
</section>
<section id="the-result-shows-that-we-will-only-spend-a-little-memory-to-pre-process-data-16kb-with-a-latency-of-34ms-which-is-excellent.-for-example-on-an-arduino-nano-cortex-m4f-64mhz-this-same-pre-process-will-take-around-480ms.-the-chosen-parameters-such-as-the-fft-length-512-will-significantly-impact-the-latency." class="level1">
<h1>The result shows that we will only spend a little memory to pre-process data (16KB), with a latency of 34ms, which is excellent. For example, on an Arduino Nano (Cortex-M4f @ 64MHz), this same pre-process will take around 480ms. The chosen parameters, such as the <code>FFT length</code> [512], will significantly impact the latency.</h1>
<p>The following step is to create the features to be trained in the next phase:</p>
<p>We could keep the default parameter values, but we will use the DSP <code>Autotune parameters</code> option.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/imgs_kws_nicla/ei_MFCC.jpg" class="img-fluid figure-img" style="width:6.5in"></p>
</figure>
</div>
<p>We will take the <code>Raw features</code> (our 1-second, 16KHz sampled audio data) and use the MFCC processing block to calculate the <code>Processed features</code>. For every 16,000 raw features (16,000 x 1 second), we will get 637 processed features (13 x 49).</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/imgs_kws_nicla/MFCC.jpg" class="img-fluid figure-img" style="width:6.5in"></p>
</figure>
</div>
<p>The result shows that we only used a small amount of memory to pre-process data (16KB) and a latency of 34ms, which is excellent. For example, on an Arduino Nano (Cortex-M4f @ 64MHz), the same pre-process will take around 480ms. The parameters chosen, such as the <code>FFT length</code> [512], will significantly impact the latency. &gt;&gt;&gt;&gt;&gt;&gt;&gt; Stashed changes</p>
<p>Now, let’s <code>Save parameters</code> and move to the <code>Generated features</code> tab, where the actual features will be generated. Using <a href="https://umap-learn.readthedocs.io/en/latest/">UMAP</a>, a dimension reduction technique, the <code>Feature explorer</code> shows how the features are distributed on a two-dimensional plot.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/imgs_kws_nicla/feat_expl.jpg" class="img-fluid figure-img" style="width:5.9in"></p>
</figure>
</div>
<p>&lt;&lt;&lt;&lt;&lt;&lt;&lt; Updated upstream The result seems OK, with a visual and clear separation from <em>yes</em> features (in red) and <em>no</em> features (in blue). The <em>unknown</em> features seem nearer to the <em>no space</em> than the <em>yes</em>. This suggests that the keyword <em>no</em> has more propensity to false positives.</p>
<section id="going-under-the-hood" class="level3">
<h3 class="anchored" data-anchor-id="going-under-the-hood">Going under the hood</h3>
<p>To better understand how the raw sound is preprocessed, take a look at the <em>Feature Engineering for Audio Classification</em> chapter. You can play with the MFCC features generation in this <a href="https://github.com/Mjrovai/Arduino_Nicla_Vision/blob/main/KWS/KWS_MFCC_Analysis.ipynb">notebook</a> or <a href="https://colab.research.google.com/github/Mjrovai/Arduino_Nicla_Vision/blob/main/KWS/KWS_MFCC_Analysis.ipynb">[Open In Colab]</a></p>
</section>
<section id="model-design-and-training" class="level2">
<h2 class="anchored" data-anchor-id="model-design-and-training">Model Design and Training</h2>
<p>We will use a simple Convolution Neural Network (CNN) model, tested with 1D and 2D convolutions. The basic architecture has two blocks of Convolution + MaxPooling (8 and 16 filters, respectively) and a Dropout of 0.25 for the 1D and 0.5 for the 2D. For the last layer, after Flattening 4 neurons, one for each class:</p>
</section>
</section>
<section id="for-better-visualization-open-the-image-on-another-tab-use-the-right-button" class="level1">
<h1><img src="images/imgs_kws_nicla/models_1d-2d.jpg" class="img-fluid" style="width:6.5in" data-fig-align="center" alt="For better visualization, open the image on another tab (use the right button)"></h1>
<p>The result seems OK, with a visually clear separation between <em>yes</em> features (in red) and <em>no</em> features (in blue). The <em>unknown</em> features seem nearer to the <em>no space</em> than the <em>yes</em>. This suggests that the keyword <em>no</em> has more propensity to false positives.</p>
<section id="going-under-the-hood-1" class="level3">
<h3 class="anchored" data-anchor-id="going-under-the-hood-1">Going under the hood</h3>
<p>To understand better how the raw sound is preprocessed, look at the <em>Feature Engineering for Audio Classification</em> chapter. You can play with the MFCC features generation by downloading this <a href="https://github.com/Mjrovai/Arduino_Nicla_Vision/blob/main/KWS/KWS_MFCC_Analysis.ipynb">notebook</a> from GitHub or <a href="https://colab.research.google.com/github/Mjrovai/Arduino_Nicla_Vision/blob/main/KWS/KWS_MFCC_Analysis.ipynb">[Opening it In Colab]</a></p>
</section>
<section id="model-design-and-training-1" class="level2">
<h2 class="anchored" data-anchor-id="model-design-and-training-1">Model Design and Training</h2>
<p>We will use a simple Convolution Neural Network (CNN) model, tested with 1D and 2D convolutions. The basic architecture has two blocks of Convolution + MaxPooling ([8] and [16] filters, respectively) and a Dropout of [0.25] for the 1D and [0.5] for the 2D. For the last layer, after Flattening, we have [4] neurons, one for each class:</p>
<p><img src="images/imgs_kws_nicla/models_1d-2d.jpg" class="img-fluid" style="width:6.5in" data-fig-align="center"> &gt;&gt;&gt;&gt;&gt;&gt;&gt; Stashed changes</p>
<p>As hyper-parameters, we will have a <code>Learning Rate</code> of [0.005] and a model trained by [100] epochs. We will also include a data augmentation method based on <a href="https://arxiv.org/abs/1904.08779">SpecAugment</a>. We trained the 1D and the 2D models with the same hyperparameters. The 1D architecture had a better overall result (90.5% accuracy when compared with 88% of the 2D, so we will use the 1D.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/imgs_kws_nicla/train_result.jpg" class="img-fluid figure-img" style="width:6.5in"></p>
</figure>
</div>
<blockquote class="blockquote">
<p>Using 1D convolutions is more efficient because it requires fewer parameters than 2D convolutions, making them more suitable for resource-constrained environments.</p>
</blockquote>
<p>&lt;&lt;&lt;&lt;&lt;&lt;&lt; Updated upstream It is also interesting to pay attention to the 1D Confusion Matrix. The F1 Score for <code>yes</code> is 95%, and for <code>no</code>, 91%. That was expected by what we saw with the Feature Explorer (<code>no</code> and <code>unknown</code> at near spaces). In trying to improve the result, you can closely examine the results by the samples with an error.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/imgs_kws_nicla/train_errors.jpg" class="img-fluid figure-img" style="width:6.5in"></p>
</figure>
</div>
<p>Try to listen to the samples that went wrong. For example, for <code>yes</code>, most of the wrongs were related to a yes pronunciation as “yeh”. You can get more samples like that and retrain your model.</p>
<section id="going-under-the-hood-2" class="level3">
<h3 class="anchored" data-anchor-id="going-under-the-hood-2">Going under the hood</h3>
</section>
</section>
</section>
<section id="if-you-want-to-understand-what-is-happening-under-the-hood-you-can-download-the-pre-processed-dataset-mfcc-training-data-from-the-dashboard-tab-and-run-this-jupyter-notebook-playing-with-the-code-open-in-colab.-for-example-you-can-analyze-the-accuracy-by-each-epoch" class="level1">
<h1>If you want to understand what is happening “under the hood,” you can download the pre-processed dataset (<code>MFCC training data</code>) from the <code>Dashboard</code> tab and run this <a href="https://github.com/Mjrovai/Arduino_Nicla_Vision/blob/main/KWS/KWS_CNN_training.ipynb">Jupyter Notebook</a>, playing with the code <a href="https://colab.research.google.com/github/Mjrovai/Arduino_Nicla_Vision/blob/main/KWS/KWS_CNN_training.ipynb">[Open In Colab]</a>. For example, you can analyze the accuracy by each epoch:</h1>
<p>It is also interesting to pay attention to the 1D Confusion Matrix. The F1 Score for <code>yes</code> is 95%, and for <code>no</code>, 91%. That was expected by what we saw with the Feature Explorer (<code>no</code> and <code>unknown</code> at close distance). In trying to improve the result, you can inspect closely the results of the samples with an error.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/imgs_kws_nicla/train_errors.jpg" class="img-fluid figure-img" style="width:6.5in"></p>
</figure>
</div>
<p>Listen to the samples that went wrong. For example, for <code>yes</code>, most of the mistakes were related to a yes pronounced as “yeh”. You can acquire additional samples and then retrain your model.</p>
<section id="going-under-the-hood-3" class="level3">
<h3 class="anchored" data-anchor-id="going-under-the-hood-3">Going under the hood</h3>
<p>If you want to understand what is happening “under the hood,” you can download the pre-processed dataset (<code>MFCC training data</code>) from the <code>Dashboard</code> tab and run this <a href="https://github.com/Mjrovai/Arduino_Nicla_Vision/blob/main/KWS/KWS_CNN_training.ipynb">Jupyter Notebook</a>, playing with the code or <a href="https://colab.research.google.com/github/Mjrovai/Arduino_Nicla_Vision/blob/main/KWS/KWS_CNN_training.ipynb">[Opening it In Colab]</a>. For example, you can analyze the accuracy by each epoch: &gt;&gt;&gt;&gt;&gt;&gt;&gt; Stashed changes</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/imgs_kws_nicla/train_graphs.jpg" class="img-fluid figure-img" style="width:6.5in"></p>
</figure>
</div>
</section>
<section id="testing" class="level2">
<h2 class="anchored" data-anchor-id="testing">Testing</h2>
<p>&lt;&lt;&lt;&lt;&lt;&lt;&lt; Updated upstream Testing the model with the data put apart before training (Test Data), we got an accuracy of approximately 76%.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/imgs_kws_nicla/test.jpg" class="img-fluid figure-img" style="width:6.5in"></p>
</figure>
</div>
<p>Inspecting the F1 score, we can see that for YES. We got 0.90, an excellent result once we expect to use this keyword to primarily “trigger” our KWS project. The worst result (0.70) is for UNKNOWN, what is OK.</p>
<p>For NO, we got 0.72, which was expected, but one action that can be done here is to take those samples that were not correctly classified, move them to the training dataset, and repeat the training process.</p>
<section id="live-classification" class="level3">
<h3 class="anchored" data-anchor-id="live-classification">Live Classification</h3>
<p>We can proceed with the project but consider that it is possible to perform <code>Live Classification</code> using the NiclaV or a smartphone to capture live samples, testing the trained model before deployment on our device.</p>
</section>
</section>
<section id="deploy-and-inference" class="level2">
<h2 class="anchored" data-anchor-id="deploy-and-inference">Deploy and Inference</h2>
<p>The Studio will package all the needed libraries, preprocessing functions, and trained models, downloading them to your computer. Go to the <code>Deployment</code> section, select the option <code>Arduino Library</code>, and at the bottom, choose <code>Quantized (Int8)</code> and press the button <code>Build</code>.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/imgs_kws_nicla/deploy.jpg" class="img-fluid figure-img" style="width:5.29in"></p>
</figure>
</div>
<p>When the <code>Build</code> button is selected, a Zip file will be created and downloaded to your computer. On your Arduino IDE, go to the <code>Sketch</code> tab, select the option <code>Add .ZIP Library</code>, and Choose the.zip file downloaded by the Studio:</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/imgs_kws_nicla/install_zip.jpg" class="img-fluid figure-img" style="width:6.5in"></p>
</figure>
</div>
</section>
</section>
<section id="now-it-is-time-for-a-real-test.-we-will-make-inferences-wholly-disconnected-from-the-studio.-lets-use-the-nicla-vision-code-example-created-when-you-deploy-the-arduino-library." class="level1">
<h1>Now, it is time for a real test. We will make inferences wholly disconnected from the Studio. Let’s use the Nicla Vision code example created when you deploy the Arduino Library.</h1>
<p>Testing the model with the data reserved for training (Test Data), we got an accuracy of approximately 76%.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/imgs_kws_nicla/test.jpg" class="img-fluid figure-img" style="width:6.5in"></p>
</figure>
</div>
<p>Inspecting the F1 score, we can see that for YES, we got 0.90, an excellent result since we expect to use this keyword as the primary “trigger” of our KWS project. The worst result (0.70) is for UNKNOWN, which is OK.</p>
<p>For NO, we got 0.72, which was expected, but to improve this result, we can move the samples that were not correctly classified to the training dataset and then repeat the training process.</p>
<section id="live-classification-1" class="level3">
<h3 class="anchored" data-anchor-id="live-classification-1">Live Classification</h3>
<p>We can proceed to the project’s next step but also consider that it is possible to perform <code>Live Classification</code> using the NiclaV or a smartphone to capture live samples, testing the trained model before deployment on our device.</p>
</section>
<section id="deploy-and-inference-1" class="level2">
<h2 class="anchored" data-anchor-id="deploy-and-inference-1">Deploy and Inference</h2>
<p>The EIS will package all the needed libraries, preprocessing functions, and trained models, downloading them to your computer. Go to the <code>Deployment</code> section, select <code>Arduino Library</code>, and at the bottom, choose <code>Quantized (Int8)</code> and press <code>Build</code>.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/imgs_kws_nicla/deploy.jpg" class="img-fluid figure-img" style="width:5.29in"></p>
</figure>
</div>
<p>When the <code>Build</code> button is selected, a zip file will be created and downloaded to your computer. On your Arduino IDE, go to the <code>Sketch</code> tab, select the option <code>Add .ZIP Library</code>, and Choose the .zip file downloaded by EIS:</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/imgs_kws_nicla/install_zip.jpg" class="img-fluid figure-img" style="width:6.5in"></p>
</figure>
</div>
<p>Now, it is time for a real test. We will make inferences while completely disconnected from the EIS. Let’s use the NiclaV code example created when we deployed the Arduino Library. &gt;&gt;&gt;&gt;&gt;&gt;&gt; Stashed changes</p>
<p>In your Arduino IDE, go to the <code>File/Examples</code> tab, look for your project, and select <code>nicla-vision/nicla-vision_microphone</code> (or <code>nicla-vision_microphone_continuous</code>)</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/imgs_kws_nicla/code_ide.jpg" class="img-fluid figure-img" style="width:6.5in"></p>
</figure>
</div>
<p>Press the reset button twice to put the NiclaV in boot mode, upload the sketch to your board, and test some real inferences:</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/imgs_kws_nicla/yes_no.jpg" class="img-fluid figure-img" style="width:6.5in"></p>
</figure>
</div>
</section>
<section id="post-processing" class="level2">
<h2 class="anchored" data-anchor-id="post-processing">Post-processing</h2>
<p>&lt;&lt;&lt;&lt;&lt;&lt;&lt; Updated upstream Now that we know that the model is working by detecting our keywords, let’s modify the code so we can see the result with the NiclaV completely offline (disconnected from the PC and powered by a battery).</p>
</section>
</section>
<section id="the-idea-is-that-whenever-the-keyword-yes-is-detected-the-led-green-will-be-on-if-it-is-no-led-red-will-be-on-if-it-is-a-unknow-led-blue-will-be-on-and-if-it-is-noise-no-keyword-the-leds-will-be-off." class="level1">
<h1>The idea is that whenever the keyword YES is detected, the LED Green will be ON; if it is NO, LED Red will be ON, if it is a UNKNOW, LED Blue will be ON; and if it is noise (No Keyword), the LEDs will be OFF.</h1>
<p>Now that we know the model is working since it detects our keywords, let’s modify the code to see the result with the NiclaV completely offline (disconnected from the PC and powered by a battery, a power bank, or an independent 5V power supply).</p>
<p>The idea is that whenever the keyword YES is detected, the Green LED will light; if a NO is heard, the Red LED will light, if it is a UNKNOW, the Blue LED will light; and in the presence of noise (No Keyword), the LEDs will be OFF. &gt;&gt;&gt;&gt;&gt;&gt;&gt; Stashed changes</p>
<p>We should modify one of the code examples. Let’s do it now with the <code>nicla-vision_microphone_continuous</code>.</p>
<p>Start with initializing the LEDs:</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode cpp code-with-copy"><code class="sourceCode cpp"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="op">...</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="dt">void</span> setup<span class="op">()</span></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="op">{</span></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a>        <span class="co">// Once you finish debugging your code, you can comment or delete the Serial part of the code</span></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a>    Serial<span class="op">.</span>begin<span class="op">(</span><span class="dv">115200</span><span class="op">);</span></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a>    <span class="cf">while</span> <span class="op">(!</span>Serial<span class="op">);</span></span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a>    Serial<span class="op">.</span>println<span class="op">(</span><span class="st">"Inferencing - Nicla Vision KWS with LEDs"</span><span class="op">);</span></span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a>    <span class="co">// Pins for the built-in RGB LEDs on the Arduino NiclaV</span></span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a>    pinMode<span class="op">(</span>LEDR<span class="op">,</span> OUTPUT<span class="op">);</span></span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a>    pinMode<span class="op">(</span>LEDG<span class="op">,</span> OUTPUT<span class="op">);</span></span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a>    pinMode<span class="op">(</span>LEDB<span class="op">,</span> OUTPUT<span class="op">);</span></span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a>    <span class="co">// Ensure the LEDs are OFF by default.</span></span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a>    <span class="co">// Note: The RGB LEDs on the Arduino Nicla Vision</span></span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a>    <span class="co">// are ON when the pin is LOW, OFF when HIGH.</span></span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a>    digitalWrite<span class="op">(</span>LEDR<span class="op">,</span> HIGH<span class="op">);</span></span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a>    digitalWrite<span class="op">(</span>LEDG<span class="op">,</span> HIGH<span class="op">);</span></span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a>    digitalWrite<span class="op">(</span>LEDB<span class="op">,</span> HIGH<span class="op">);</span></span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a><span class="op">...</span></span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a><span class="op">}</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>&lt;&lt;&lt;&lt;&lt;&lt;&lt; Updated upstream Create two functions, <code>turn_off_leds()</code> function , to turn off all RGB LEDs ======= Create two functions, <code>turn_off_leds()</code> function, to turn off all RGB LEDs &gt;&gt;&gt;&gt;&gt;&gt;&gt; Stashed changes</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode cpp code-with-copy"><code class="sourceCode cpp"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="op">**</span></span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a> <span class="op">*</span> <span class="er">@</span>brief      turn_off_leds function <span class="op">-</span> turn<span class="op">-</span>off all RGB LEDs</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a> <span class="op">*/</span></span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a><span class="dt">void</span> turn_off_leds<span class="op">(){</span></span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a>    digitalWrite<span class="op">(</span>LEDR<span class="op">,</span> HIGH<span class="op">);</span></span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a>    digitalWrite<span class="op">(</span>LEDG<span class="op">,</span> HIGH<span class="op">);</span></span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a>    digitalWrite<span class="op">(</span>LEDB<span class="op">,</span> HIGH<span class="op">);</span></span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a><span class="op">}</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>&lt;&lt;&lt;&lt;&lt;&lt;&lt; Updated upstream and another <code>turn_on_led()</code> function, which is used to turn on the RGB LEDs depending on the index of the most probable result of the classifier. ======= Another <code>turn_on_led()</code> function is used to turn on the RGB LEDs according to the most probable result of the classifier. &gt;&gt;&gt;&gt;&gt;&gt;&gt; Stashed changes</p>
<div class="sourceCode" id="cb3"><pre class="sourceCode cpp code-with-copy"><code class="sourceCode cpp"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="co">/**</span></span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a><span class="co"> * </span><span class="an">@brief</span><span class="co">      turn_on_leds function used to turn on the RGB LEDs</span></span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a><span class="co"> * </span><span class="an">@param[in]</span><span class="co">  </span><span class="cv">pred_index</span><span class="co">     </span></span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a><span class="co"> *             no:       [0] ==&gt; Red ON</span></span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a><span class="co"> *             noise:    [1] ==&gt; ALL OFF </span></span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a><span class="co"> *             unknown:  [2] ==&gt; Blue ON</span></span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a><span class="co"> *             Yes:      [3] ==&gt; Green ON</span></span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a><span class="co"> */</span></span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a><span class="dt">void</span> turn_on_leds<span class="op">(</span><span class="dt">int</span> pred_index<span class="op">)</span> <span class="op">{</span></span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a>  <span class="cf">switch</span> <span class="op">(</span>pred_index<span class="op">)</span></span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a>  <span class="op">{</span></span>
<span id="cb3-12"><a href="#cb3-12" aria-hidden="true" tabindex="-1"></a>    <span class="cf">case</span> <span class="dv">0</span><span class="op">:</span></span>
<span id="cb3-13"><a href="#cb3-13" aria-hidden="true" tabindex="-1"></a>      turn_off_leds<span class="op">();</span></span>
<span id="cb3-14"><a href="#cb3-14" aria-hidden="true" tabindex="-1"></a>      digitalWrite<span class="op">(</span>LEDR<span class="op">,</span> LOW<span class="op">);</span></span>
<span id="cb3-15"><a href="#cb3-15" aria-hidden="true" tabindex="-1"></a>      <span class="cf">break</span><span class="op">;</span></span>
<span id="cb3-16"><a href="#cb3-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-17"><a href="#cb3-17" aria-hidden="true" tabindex="-1"></a>    <span class="cf">case</span> <span class="dv">1</span><span class="op">:</span></span>
<span id="cb3-18"><a href="#cb3-18" aria-hidden="true" tabindex="-1"></a>      turn_off_leds<span class="op">();</span></span>
<span id="cb3-19"><a href="#cb3-19" aria-hidden="true" tabindex="-1"></a>      <span class="cf">break</span><span class="op">;</span></span>
<span id="cb3-20"><a href="#cb3-20" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb3-21"><a href="#cb3-21" aria-hidden="true" tabindex="-1"></a>    <span class="cf">case</span> <span class="dv">2</span><span class="op">:</span></span>
<span id="cb3-22"><a href="#cb3-22" aria-hidden="true" tabindex="-1"></a>      turn_off_leds<span class="op">();</span></span>
<span id="cb3-23"><a href="#cb3-23" aria-hidden="true" tabindex="-1"></a>      digitalWrite<span class="op">(</span>LEDB<span class="op">,</span> LOW<span class="op">);</span></span>
<span id="cb3-24"><a href="#cb3-24" aria-hidden="true" tabindex="-1"></a>      <span class="cf">break</span><span class="op">;</span></span>
<span id="cb3-25"><a href="#cb3-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-26"><a href="#cb3-26" aria-hidden="true" tabindex="-1"></a>    <span class="cf">case</span> <span class="dv">3</span><span class="op">:</span></span>
<span id="cb3-27"><a href="#cb3-27" aria-hidden="true" tabindex="-1"></a>      turn_off_leds<span class="op">();</span></span>
<span id="cb3-28"><a href="#cb3-28" aria-hidden="true" tabindex="-1"></a>      digitalWrite<span class="op">(</span>LEDG<span class="op">,</span> LOW<span class="op">);</span></span>
<span id="cb3-29"><a href="#cb3-29" aria-hidden="true" tabindex="-1"></a>      <span class="cf">break</span><span class="op">;</span></span>
<span id="cb3-30"><a href="#cb3-30" aria-hidden="true" tabindex="-1"></a>  <span class="op">}</span></span>
<span id="cb3-31"><a href="#cb3-31" aria-hidden="true" tabindex="-1"></a><span class="op">}</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>And change the <code>// print the predictions</code> portion of the code on <code>loop()</code>:</p>
<div class="sourceCode" id="cb4"><pre class="sourceCode cpp code-with-copy"><code class="sourceCode cpp"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="op">...</span></span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> <span class="op">(++</span>print_results <span class="op">&gt;=</span> <span class="op">(</span>EI_CLASSIFIER_SLICES_PER_MODEL_WINDOW<span class="op">))</span> <span class="op">{</span></span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a>        <span class="co">// print the predictions</span></span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a>        ei_printf<span class="op">(</span><span class="st">"Predictions "</span><span class="op">);</span></span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a>        ei_printf<span class="op">(</span><span class="st">"(DSP: </span><span class="sc">%d</span><span class="st"> ms., Classification: </span><span class="sc">%d</span><span class="st"> ms., Anomaly: </span><span class="sc">%d</span><span class="st"> ms.)"</span><span class="op">,</span></span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a>            result<span class="op">.</span>timing<span class="op">.</span>dsp<span class="op">,</span> result<span class="op">.</span>timing<span class="op">.</span>classification<span class="op">,</span> result<span class="op">.</span>timing<span class="op">.</span>anomaly<span class="op">);</span></span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a>        ei_printf<span class="op">(</span><span class="st">": </span><span class="sc">\n</span><span class="st">"</span><span class="op">);</span></span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-10"><a href="#cb4-10" aria-hidden="true" tabindex="-1"></a>        <span class="dt">int</span> pred_index <span class="op">=</span> <span class="dv">0</span><span class="op">;</span>     <span class="co">// Initialize pred_index</span></span>
<span id="cb4-11"><a href="#cb4-11" aria-hidden="true" tabindex="-1"></a>        <span class="dt">float</span> pred_value <span class="op">=</span> <span class="dv">0</span><span class="op">;</span>   <span class="co">// Initialize pred_value</span></span>
<span id="cb4-12"><a href="#cb4-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-13"><a href="#cb4-13" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> <span class="op">(</span><span class="dt">size_t</span> ix <span class="op">=</span> <span class="dv">0</span><span class="op">;</span> ix <span class="op">&lt;</span> EI_CLASSIFIER_LABEL_COUNT<span class="op">;</span> ix<span class="op">++)</span> <span class="op">{</span></span>
<span id="cb4-14"><a href="#cb4-14" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> <span class="op">(</span>result<span class="op">.</span>classification<span class="op">[</span>ix<span class="op">].</span>value <span class="op">&gt;</span> pred_value<span class="op">){</span></span>
<span id="cb4-15"><a href="#cb4-15" aria-hidden="true" tabindex="-1"></a>                pred_index <span class="op">=</span> ix<span class="op">;</span></span>
<span id="cb4-16"><a href="#cb4-16" aria-hidden="true" tabindex="-1"></a>                pred_value <span class="op">=</span> result<span class="op">.</span>classification<span class="op">[</span>ix<span class="op">].</span>value<span class="op">;</span></span>
<span id="cb4-17"><a href="#cb4-17" aria-hidden="true" tabindex="-1"></a>            <span class="op">}</span></span>
<span id="cb4-18"><a href="#cb4-18" aria-hidden="true" tabindex="-1"></a>            <span class="co">// ei_printf("    %s: ", result.classification[ix].label);</span></span>
<span id="cb4-19"><a href="#cb4-19" aria-hidden="true" tabindex="-1"></a>            <span class="co">// ei_printf_float(result.classification[ix].value);</span></span>
<span id="cb4-20"><a href="#cb4-20" aria-hidden="true" tabindex="-1"></a>            <span class="co">// ei_printf("\n");</span></span>
<span id="cb4-21"><a href="#cb4-21" aria-hidden="true" tabindex="-1"></a>        <span class="op">}</span></span>
<span id="cb4-22"><a href="#cb4-22" aria-hidden="true" tabindex="-1"></a>        ei_printf<span class="op">(</span><span class="st">"  PREDICTION: ==&gt; </span><span class="sc">%s</span><span class="st"> with probability </span><span class="sc">%.2f\n</span><span class="st">"</span><span class="op">,</span> </span>
<span id="cb4-23"><a href="#cb4-23" aria-hidden="true" tabindex="-1"></a>                  result<span class="op">.</span>classification<span class="op">[</span>pred_index<span class="op">].</span>label<span class="op">,</span> pred_value<span class="op">);</span></span>
<span id="cb4-24"><a href="#cb4-24" aria-hidden="true" tabindex="-1"></a>        turn_on_leds <span class="op">(</span>pred_index<span class="op">);</span></span>
<span id="cb4-25"><a href="#cb4-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-26"><a href="#cb4-26" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb4-27"><a href="#cb4-27" aria-hidden="true" tabindex="-1"></a><span class="pp">#if EI_CLASSIFIER_HAS_ANOMALY == 1</span></span>
<span id="cb4-28"><a href="#cb4-28" aria-hidden="true" tabindex="-1"></a>        ei_printf<span class="op">(</span><span class="st">"    anomaly score: "</span><span class="op">);</span></span>
<span id="cb4-29"><a href="#cb4-29" aria-hidden="true" tabindex="-1"></a>        ei_printf_float<span class="op">(</span>result<span class="op">.</span>anomaly<span class="op">);</span></span>
<span id="cb4-30"><a href="#cb4-30" aria-hidden="true" tabindex="-1"></a>        ei_printf<span class="op">(</span><span class="st">"</span><span class="sc">\n</span><span class="st">"</span><span class="op">);</span></span>
<span id="cb4-31"><a href="#cb4-31" aria-hidden="true" tabindex="-1"></a><span class="pp">#endif</span></span>
<span id="cb4-32"><a href="#cb4-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-33"><a href="#cb4-33" aria-hidden="true" tabindex="-1"></a>        print_results <span class="op">=</span> <span class="dv">0</span><span class="op">;</span></span>
<span id="cb4-34"><a href="#cb4-34" aria-hidden="true" tabindex="-1"></a>    <span class="op">}</span></span>
<span id="cb4-35"><a href="#cb4-35" aria-hidden="true" tabindex="-1"></a><span class="op">}</span></span>
<span id="cb4-36"><a href="#cb4-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-37"><a href="#cb4-37" aria-hidden="true" tabindex="-1"></a><span class="op">...</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>You can find the complete code on the <a href="https://github.com/Mjrovai/Arduino_Nicla_Vision/tree/main/KWS/nicla_vision_microphone_continuous_LED">project’s GitHub</a>.</p>
<p>&lt;&lt;&lt;&lt;&lt;&lt;&lt; Updated upstream Upload the sketch to your board and test some real inferences. The idea is that the RGB Green LED will be ON whenever the keyword YES is detected, the same for NO (Red) and any other word that turns on the Blue LED. If silence or background noise is present, the LEDs should be off. In the same way, instead of turning on an LED, this could be a “trigger” for an external device, as we saw in the introduction. ======= Upload the sketch to your board and test some real inferences. The idea is that the Green LED will be ON whenever the keyword YES is detected, the Red will lit for a NO, and any other word will turn on the Blue LED. All the LEDs should be off if silence or background noise is present. Remember that the same procedure can “trigger” an external device to perform a desired action instead of turning on an LED, as we saw in the introduction. &gt;&gt;&gt;&gt;&gt;&gt;&gt; Stashed changes</p>
<div class="quarto-video"><iframe data-external="1" src="https://www.youtube.com/embed/25Rd76OTXLY" width="480" height="270" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div>
<section id="conclusion" class="level2">
<h2 class="anchored" data-anchor-id="conclusion">Conclusion</h2>
<p>&lt;&lt;&lt;&lt;&lt;&lt;&lt; Updated upstream &gt; On the <a href="https://github.com/Mjrovai/Arduino_Nicla_Vision/tree/main/KWS">GitHub repository</a>, you will find the notebooks and codes used on this hands-on tutorial.</p>
<p>Before we finish, consider that Sound Classification is more than just voice. For example, you can develop TinyML projects around sound in several areas, such as:</p>
<ul>
<li><strong>Security</strong> (Broken Glass detection)</li>
<li><strong>Industry</strong> (Anomaly Detection)</li>
<li><strong>Medical</strong> (Snore, Toss, Pulmonary diseases)</li>
<li><h1 id="nature-beehive-control-insect-sound"><strong>Nature</strong> (Beehive control, insect sound)</h1>
<blockquote class="blockquote">
<p>You will find the notebooks and codes used in this hands-on tutorial on the <a href="https://github.com/Mjrovai/Arduino_Nicla_Vision/tree/main/KWS">GitHub</a> repository.</p>
</blockquote></li>
</ul>
<p>Before we finish, consider that Sound Classification is more than just voice. For example, you can develop TinyML projects around sound in several areas, such as:</p>
<ul>
<li><strong>Security</strong> (Broken Glass detection, Gunshot)</li>
<li><strong>Industry</strong> (Anomaly Detection)</li>
<li><strong>Medical</strong> (Snore, Cough, Pulmonary diseases)</li>
<li><strong>Nature</strong> (Beehive control, insect sound, pouching mitigation) &gt;&gt;&gt;&gt;&gt;&gt;&gt; Stashed changes</li>
</ul>


</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
          // target, if specified
          link.setAttribute("target", "_blank");
          // default icon
          link.classList.add("external");
      }
    }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./kws_feature_eng.html" class="pagination-link">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text">Audio Feature Engineering</span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./references.html" class="pagination-link">
        <span class="nav-page-text">References</span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">Edited by Prof.&nbsp;Vijay Janapa Reddi (Harvard University)</div>   
    <div class="nav-footer-center">
      &nbsp;
    </div>
    <div class="nav-footer-right">This book was built with <a href="https://quarto.org/">Quarto</a>.</div>
  </div>
</footer>



</body></html>