{
  "figures": {
    "fig-algo-efficiency": {
      "original_caption": "**Algorithmic Efficiency Trajectory.** Training efficiency factor relative to AlexNet (2012 baseline) for ImageNet classification. Each point represents a model architecture that achieves comparable accuracy with fewer computational resources. The trajectory from AlexNet (1x) through VGG, ResNet, MobileNet, and ShuffleNet to EfficientNet (44x) demonstrates that algorithmic innovation has delivered a 44-fold reduction in required compute over eight years, independent of hardware improvements.",
      "current_caption": "**Algorithmic Efficiency Trajectory.** Training efficiency factor relative to AlexNet (2012 baseline) for ImageNet classification. Each point represents a model architecture that achieves comparable accuracy with fewer computational resources. The trajectory from AlexNet (1x) through VGG, ResNet, MobileNet, and ShuffleNet to EfficientNet (44x) demonstrates that algorithmic innovation has delivered a 44-fold reduction in required compute over eight years, independent of hardware improvements.",
      "new_caption": "",
      "type": "code",
      "source_file": "contents/vol1/introduction/introduction.qmd"
    },
    "fig-evolution-efficiency": {
      "original_caption": "",
      "current_caption": "",
      "new_caption": "",
      "type": "tikz",
      "source_file": "contents/vol1/introduction/introduction.qmd"
    },
    "fig-ml_lifecycle_overview": {
      "original_caption": "",
      "current_caption": "",
      "new_caption": "",
      "type": "tikz",
      "source_file": "contents/vol1/introduction/introduction.qmd"
    },
    "fig-ai-training-compute-growth": {
      "original_caption": "**The Era of Scale.** Training Compute (FLOPs) vs. Year (Log Scale). While early Deep Learning (blue) showed rapid growth, the Transformer Era (red) accelerated this trend significantly. From AlexNet (2012) to GPT-4 (2023), compute requirements increased by $10^8$ (100 million times), far outpacing Moore's Law. This exponential demand drives the specialized infrastructure described in this book.",
      "current_caption": "**The Era of Scale.** Training Compute (FLOPs) vs. Year (Log Scale). While early Deep Learning (blue) showed rapid growth, the Transformer Era (red) accelerated this trend significantly. From AlexNet (2012) to GPT-4 (2023), compute requirements increased by $10^8$ (100 million times), far outpacing Moore's Law. This exponential demand drives the specialized infrastructure described in this book.",
      "new_caption": "",
      "type": "code",
      "source_file": "contents/vol1/introduction/introduction.qmd"
    },
    "fig-alexnet": {
      "original_caption": "",
      "current_caption": "",
      "new_caption": "",
      "type": "tikz",
      "source_file": "contents/vol1/introduction/introduction.qmd"
    },
    "fig-pillars": {
      "original_caption": "**Five-Pillar Framework.** Five labeled columns represent Data Engineering, Training Systems, Deployment Infrastructure, Operations and Monitoring, and Ethics and Governance. The pillars rest on a shared foundation labeled Performance Optimization and Hardware Acceleration, indicating the technical imperatives that support all five disciplines.",
      "current_caption": "**Five-Pillar Framework.** Five labeled columns represent Data Engineering, Training Systems, Deployment Infrastructure, Operations and Monitoring, and Ethics and Governance. The pillars rest on a shared foundation labeled Performance Optimization and Hardware Acceleration, indicating the technical imperatives that support all five disciplines.",
      "new_caption": "",
      "type": "markdown",
      "source_file": "contents/vol1/introduction/introduction.qmd"
    },
    "fig-ai-triad": {
      "original_caption": "",
      "current_caption": "",
      "new_caption": "",
      "type": "tikz",
      "source_file": "contents/vol1/introduction/introduction.qmd"
    },
    "fig-ai-timeline": {
      "original_caption": "",
      "current_caption": "",
      "new_caption": "",
      "type": "tikz",
      "source_file": "contents/vol1/introduction/introduction.qmd"
    },
    "fig-TinyML-example": {
      "original_caption": "**TinyML System Scale**: Small development boards, including Arduino Nano BLE Sense and similar microcontroller kits approximately 2 to 5 cm in length, with visible processor chips and pin connectors that enable sensor integration for always-on ML inference at milliwatt power budgets. Source: [@warden2018speech]",
      "current_caption": "**TinyML System Scale**: Small development boards, including Arduino Nano BLE Sense and similar microcontroller kits approximately 2 to 5 cm in length, with visible processor chips and pin connectors that enable sensor integration for always-on ML inference at milliwatt power budgets. Source: [@warden2018speech]",
      "new_caption": "",
      "type": "markdown",
      "source_file": "contents/vol1/ml_systems/ml_systems.qmd"
    },
    "fig-op_char": {
      "original_caption": "",
      "current_caption": "",
      "new_caption": "",
      "type": "tikz",
      "source_file": "contents/vol1/ml_systems/ml_systems.qmd"
    },
    "fig-tiny-ml": {
      "original_caption": "",
      "current_caption": "",
      "new_caption": "",
      "type": "tikz",
      "source_file": "contents/vol1/ml_systems/ml_systems.qmd"
    },
    "fig-cloud-ml": {
      "original_caption": "",
      "current_caption": "",
      "new_caption": "",
      "type": "tikz",
      "source_file": "contents/vol1/ml_systems/ml_systems.qmd"
    },
    "fig-edge-ml": {
      "original_caption": "",
      "current_caption": "",
      "new_caption": "",
      "type": "tikz",
      "source_file": "contents/vol1/ml_systems/ml_systems.qmd"
    },
    "fig-cloud-edge-TinyML-comparison": {
      "original_caption": "",
      "current_caption": "",
      "new_caption": "",
      "type": "tikz",
      "source_file": "contents/vol1/ml_systems/ml_systems.qmd"
    },
    "fig-mobile-ml": {
      "original_caption": "",
      "current_caption": "",
      "new_caption": "",
      "type": "tikz",
      "source_file": "contents/vol1/ml_systems/ml_systems.qmd"
    },
    "fig-mlsys-playbook-flowchart": {
      "original_caption": "",
      "current_caption": "",
      "new_caption": "",
      "type": "tikz",
      "source_file": "contents/vol1/ml_systems/ml_systems.qmd"
    },
    "fig-ml-systems-convergence": {
      "original_caption": "",
      "current_caption": "",
      "new_caption": "",
      "type": "tikz",
      "source_file": "contents/vol1/ml_systems/ml_systems.qmd"
    },
    "fig-cloudml-example": {
      "original_caption": "**Cloud Data Center Scale**: Rows of server racks illuminated by blue LEDs extend across a Google Cloud TPU data center floor, housing thousands of specialized AI accelerator chips that collectively deliver petaflop-scale training throughput. Source: [@google2024gemini].",
      "current_caption": "**Cloud Data Center Scale**: Rows of server racks illuminated by blue LEDs extend across a Google Cloud TPU data center floor, housing thousands of specialized AI accelerator chips that collectively deliver petaflop-scale training throughput. Source: [@google2024gemini].",
      "new_caption": "",
      "type": "markdown",
      "source_file": "contents/vol1/ml_systems/ml_systems.qmd"
    },
    "fig-edgeml-example": {
      "original_caption": "**Edge Device Deployment**: Diverse IoT devices, from wearables to home appliances, enable decentralized machine learning by performing inference locally, reducing reliance on cloud connectivity and improving response times. Source: Edge Impulse.",
      "current_caption": "**Edge Device Deployment**: Diverse IoT devices, from wearables to home appliances, enable decentralized machine learning by performing inference locally, reducing reliance on cloud connectivity and improving response times. Source: Edge Impulse.",
      "new_caption": "",
      "type": "markdown",
      "source_file": "contents/vol1/ml_systems/ml_systems.qmd"
    },
    "fig-hybrid": {
      "original_caption": "",
      "current_caption": "",
      "new_caption": "",
      "type": "tikz",
      "source_file": "contents/vol1/ml_systems/ml_systems.qmd"
    },
    "fig-lifecycle-overview": {
      "original_caption": "",
      "current_caption": "",
      "new_caption": "",
      "type": "tikz",
      "source_file": "contents/vol1/workflow/workflow.qmd"
    },
    "fig-eye-dr": {
      "original_caption": "**Retinal Hemorrhages**: Diabetic retinopathy causes visible hemorrhages in retinal images. While this appears to be straightforward image classification, the path from laboratory success to clinical deployment illustrates every aspect of AI lifecycle complexity. Source: Google.",
      "current_caption": "**Retinal Hemorrhages**: Diabetic retinopathy causes visible hemorrhages in retinal images. While this appears to be straightforward image classification, the path from laboratory success to clinical deployment illustrates every aspect of AI lifecycle complexity. Source: Google.",
      "new_caption": "",
      "type": "markdown",
      "source_file": "contents/vol1/workflow/workflow.qmd"
    },
    "fig-ml-lifecycle-feedback": {
      "original_caption": "",
      "current_caption": "",
      "new_caption": "",
      "type": "tikz",
      "source_file": "contents/vol1/workflow/workflow.qmd"
    },
    "fig-ds-time": {
      "original_caption": "",
      "current_caption": "",
      "new_caption": "",
      "type": "tikz",
      "source_file": "contents/vol1/workflow/workflow.qmd"
    },
    "fig-mlops-leverage": {
      "original_caption": "**The MLOps Leverage**: Why infrastructure investment yields exponential returns. Manual workflows (red) scale linearly with team size but eventually saturate due to the *Coordination Tax*—the overhead of managing conflicting experiments and untracked artifacts. In contrast, an automated MLOps platform (blue) enables the *Flywheel Effect*, where shared components (feature stores, pipelines) allow experimentation velocity to scale super-linearly with team size.",
      "current_caption": "**The MLOps Leverage**: Why infrastructure investment yields exponential returns. Manual workflows (red) scale linearly with team size but eventually saturate due to the *Coordination Tax*—the overhead of managing conflicting experiments and untracked artifacts. In contrast, an automated MLOps platform (blue) enables the *Flywheel Effect*, where shared components (feature stores, pipelines) allow experimentation velocity to scale super-linearly with team size.",
      "new_caption": "",
      "type": "code",
      "source_file": "contents/vol1/workflow/workflow.qmd"
    },
    "fig-ml-lifecycle": {
      "original_caption": "",
      "current_caption": "",
      "new_caption": "",
      "type": "tikz",
      "source_file": "contents/vol1/workflow/workflow.qmd"
    },
    "fig-four-pillars": {
      "original_caption": "",
      "current_caption": "",
      "new_caption": "",
      "type": "tikz",
      "source_file": "contents/vol1/data_engineering/data_engineering.qmd"
    },
    "fig-weak-supervision": {
      "original_caption": "",
      "current_caption": "",
      "new_caption": "",
      "type": "tikz",
      "source_file": "contents/vol1/data_engineering/data_engineering.qmd"
    },
    "fig-tfx-pipeline-example": {
      "original_caption": "",
      "current_caption": "",
      "new_caption": "",
      "type": "tikz",
      "source_file": "contents/vol1/data_engineering/data_engineering.qmd"
    },
    "fig-keywords": {
      "original_caption": "**Keyword Spotting System**: A voice-activated device uses a lightweight, always-on wake word detector that listens continuously and triggers the main voice assistant upon keyword detection.",
      "current_caption": "**Keyword Spotting System**: A voice-activated device uses a lightweight, always-on wake word detector that listens continuously and triggers the main voice assistant upon keyword detection.",
      "new_caption": "",
      "type": "markdown",
      "source_file": "contents/vol1/data_engineering/data_engineering.qmd"
    },
    "fig-traffic-light": {
      "original_caption": "**Data Source Noise**: A black-and-white photograph from 1914 showing early manual semaphore traffic signals, illustrating how historical images can appear in modern web scraping results for contemporary queries. Such anachronistic content requires systematic validation and filtering to prevent spurious correlations in training data. Source: Vox.",
      "current_caption": "**Data Source Noise**: A black-and-white photograph from 1914 showing early manual semaphore traffic signals, illustrating how historical images can appear in modern web scraping results for contemporary queries. Such anachronistic content requires systematic validation and filtering to prevent spurious correlations in training data. Source: Vox.",
      "new_caption": "",
      "type": "markdown",
      "source_file": "contents/vol1/data_engineering/data_engineering.qmd"
    },
    "fig-spectrogram-example": {
      "original_caption": "**Audio Feature Transformation**: Advanced audio features compress raw audio waveforms into representations that emphasize perceptually relevant characteristics for machine learning tasks. This transformation reduces noise and data dimensionality while preserving essential speech information, improving model performance in applications like keyword spotting.",
      "current_caption": "**Audio Feature Transformation**: Advanced audio features compress raw audio waveforms into representations that emphasize perceptually relevant characteristics for machine learning tasks. This transformation reduces noise and data dimensionality while preserving essential speech information, improving model performance in applications like keyword spotting.",
      "new_caption": "",
      "type": "markdown",
      "source_file": "contents/vol1/data_engineering/data_engineering.qmd"
    },
    "fig-labels": {
      "original_caption": "**Data Annotation Granularity**: Three versions of the same street scene show increasing annotation detail: a simple classification label, bounding boxes around vehicles and pedestrians, and pixel-level semantic segmentation with distinct colors. Each level increases labeling cost and storage requirements while providing richer training signal.",
      "current_caption": "**Data Annotation Granularity**: Three versions of the same street scene show increasing annotation detail: a simple classification label, bounding boxes around vehicles and pedestrians, and pixel-level semantic segmentation with distinct colors. Each level increases labeling cost and storage requirements while providing richer training signal.",
      "new_caption": "",
      "type": "markdown",
      "source_file": "contents/vol1/data_engineering/data_engineering.qmd"
    },
    "fig-hard-labels": {
      "original_caption": "**Labeling Ambiguity**: How subjective or difficult examples, such as blurry images or rare species, can introduce errors during data labeling, highlighting the need for careful quality control and potentially expert annotation. Source: [@northcutt2021pervasive].",
      "current_caption": "**Labeling Ambiguity**: How subjective or difficult examples, such as blurry images or rare species, can introduce errors during data labeling, highlighting the need for careful quality control and potentially expert annotation. Source: [@northcutt2021pervasive].",
      "new_caption": "",
      "type": "markdown",
      "source_file": "contents/vol1/data_engineering/data_engineering.qmd"
    },
    "fig-debug-flowchart": {
      "original_caption": "",
      "current_caption": "",
      "new_caption": "",
      "type": "tikz",
      "source_file": "contents/vol1/data_engineering/data_engineering.qmd"
    },
    "fig-misalignment": {
      "original_caption": "",
      "current_caption": "",
      "new_caption": "",
      "type": "tikz",
      "source_file": "contents/vol1/data_engineering/data_engineering.qmd"
    },
    "fig-synthetic-data": {
      "original_caption": "",
      "current_caption": "",
      "new_caption": "",
      "type": "tikz",
      "source_file": "contents/vol1/data_engineering/data_engineering.qmd"
    },
    "fig-pipeline-flow": {
      "original_caption": "",
      "current_caption": "",
      "new_caption": "",
      "type": "tikz",
      "source_file": "contents/vol1/data_engineering/data_engineering.qmd"
    },
    "fig-cascades": {
      "original_caption": "",
      "current_caption": "",
      "new_caption": "",
      "type": "tikz",
      "source_file": "contents/vol1/data_engineering/data_engineering.qmd"
    },
    "fig-dataloader-choke-point": {
      "original_caption": "**The Dataloader Choke Point.** Training Throughput (img/s) vs. Number of DataLoader Workers. The blue curve shows CPU throughput scaling linearly with workers until hitting disk limits. The red dashed line is the GPU's consumption capacity (e.g., ResNet-50 consuming 3,000 img/s). The system is bottlenecked by whichever is lower. In the 'Starvation Region' (left), the GPU is idle waiting for data. In the 'Saturated Region' (right), the GPU is fully utilized, and adding more workers wastes CPU memory.",
      "current_caption": "**The Dataloader Choke Point.** Training Throughput (img/s) vs. Number of DataLoader Workers. The blue curve shows CPU throughput scaling linearly with workers until hitting disk limits. The red dashed line is the GPU's consumption capacity (e.g., ResNet-50 consuming 3,000 img/s). The system is bottlenecked by whichever is lower. In the 'Starvation Region' (left), the GPU is idle waiting for data. In the 'Saturated Region' (right), the GPU is fully utilized, and adding more workers wastes CPU memory.",
      "new_caption": "",
      "type": "code",
      "source_file": "contents/vol1/data_engineering/data_engineering.qmd"
    },
    "fig-etl-vs-elt": {
      "original_caption": "",
      "current_caption": "",
      "new_caption": "",
      "type": "tikz",
      "source_file": "contents/vol1/data_engineering/data_engineering.qmd"
    },
    "fig-data-quality-multiplier": {
      "original_caption": "**The Data Quality Multiplier**: Model Accuracy vs. Dataset Size (Log Scale) for Clean vs. Noisy Data. High-quality data (Green) follows a steeper power law, reaching higher accuracy with fewer samples. Low-quality data (Red) hits a 'Statistical Ceiling' earlier, where adding more data yields diminishing returns due to irreducible label noise. This gap illustrates why data cleaning is often a higher-leverage optimization than model scaling.",
      "current_caption": "**The Data Quality Multiplier**: Model Accuracy vs. Dataset Size (Log Scale) for Clean vs. Noisy Data. High-quality data (Green) follows a steeper power law, reaching higher accuracy with fewer samples. Low-quality data (Red) hits a 'Statistical Ceiling' earlier, where adding more data yields diminishing returns due to irreducible label noise. This gap illustrates why data cleaning is often a higher-leverage optimization than model scaling.",
      "new_caption": "",
      "type": "code",
      "source_file": "contents/vol1/data_engineering/data_engineering.qmd"
    },
    "fig-mswc": {
      "original_caption": "**Multilingual Data Preparation**: Forced alignment and segmentation transform paired audio-text data into labeled one-second segments, creating a large-scale corpus for training keyword spotting models across 50+ languages. This automated process enables scalable development of KWS systems by efficiently generating training examples from readily available speech resources like common voice and multilingual captioned content.",
      "current_caption": "**Multilingual Data Preparation**: Forced alignment and segmentation transform paired audio-text data into labeled one-second segments, creating a large-scale corpus for training keyword spotting models across 50+ languages. This automated process enables scalable development of KWS systems by efficiently generating training examples from readily available speech resources like common voice and multilingual captioned content.",
      "new_caption": "",
      "type": "markdown",
      "source_file": "contents/vol1/data_engineering/data_engineering.qmd"
    },
    "fig-virtuous-cycle": {
      "original_caption": "",
      "current_caption": "",
      "new_caption": "",
      "type": "tikz",
      "source_file": "contents/vol1/dl_primer/dl_primer.qmd"
    },
    "fig-forward-propagation": {
      "original_caption": "",
      "current_caption": "",
      "new_caption": "",
      "type": "tikz",
      "source_file": "contents/vol1/dl_primer/dl_primer.qmd"
    },
    "fig-hog": {
      "original_caption": "**HOG Method**: Identifies edges in images to create a histogram of gradients, transforming pixel values into shape descriptors that are invariant to lighting changes.",
      "current_caption": "**HOG Method**: Identifies edges in images to create a histogram of gradients, transforming pixel values into shape descriptors that are invariant to lighting changes.",
      "new_caption": "",
      "type": "markdown",
      "source_file": "contents/vol1/dl_primer/dl_primer.qmd"
    },
    "fig-usps-inference-pipeline": {
      "original_caption": "",
      "current_caption": "",
      "new_caption": "",
      "type": "tikz",
      "source_file": "contents/vol1/dl_primer/dl_primer.qmd"
    },
    "fig-trends": {
      "original_caption": "**Computational Growth**: Log-scale scatter plot showing training compute in FLOPS from 1952 to 2022. Computational power grew at a 1.4x rate from 1952 to 2010, then accelerated to a doubling every 3.4 months from 2012 to 2022. Large-scale models after 2015 followed an even faster 10-month doubling cycle, addressing the historical bottleneck of training complex neural networks.",
      "current_caption": "**Computational Growth**: Log-scale scatter plot showing training compute in FLOPS from 1952 to 2022. Computational power grew at a 1.4x rate from 1952 to 2010, then accelerated to a doubling every 3.4 months from 2012 to 2022. Large-scale models after 2015 followed an even faster 10-month doubling cycle, addressing the historical bottleneck of training complex neural networks.",
      "new_caption": "",
      "type": "markdown",
      "source_file": "contents/vol1/dl_primer/dl_primer.qmd"
    },
    "fig-nonlinear": {
      "original_caption": "",
      "current_caption": "",
      "new_caption": "",
      "type": "tikz",
      "source_file": "contents/vol1/dl_primer/dl_primer.qmd"
    },
    "fig-traditional": {
      "original_caption": "",
      "current_caption": "",
      "new_caption": "",
      "type": "tikz",
      "source_file": "contents/vol1/dl_primer/dl_primer.qmd"
    },
    "fig-mnist-topology-1": {
      "original_caption": "",
      "current_caption": "",
      "new_caption": "",
      "type": "tikz",
      "source_file": "contents/vol1/dl_primer/dl_primer.qmd"
    },
    "fig-bio_nn2ai_nn": {
      "original_caption": "**Biological-to-Artificial Neuron Mapping**: Side-by-side comparison showing how biological neuron structures map to artificial neuron components. Dendrites correspond to inputs, synapses to weights, the cell body to the summation function, and the axon to the activation output. This mapping established the \"Compute-Aggregate-Activate\" pattern central to neural network design.",
      "current_caption": "**Biological-to-Artificial Neuron Mapping**: Side-by-side comparison showing how biological neuron structures map to artificial neuron components. Dendrites correspond to inputs, synapses to weights, the cell body to the summation function, and the axon to the activation output. This mapping established the \"Compute-Aggregate-Activate\" pattern central to neural network design.",
      "new_caption": "",
      "type": "markdown",
      "source_file": "contents/vol1/dl_primer/dl_primer.qmd"
    },
    "fig-training-vs-inference": {
      "original_caption": "",
      "current_caption": "",
      "new_caption": "",
      "type": "tikz",
      "source_file": "contents/vol1/dl_primer/dl_primer.qmd"
    },
    "fig-activity-rules": {
      "original_caption": "**Activity Classification Decision Tree**: A rule-based decision tree classifies human activity by branching on speed thresholds, with values below 4 mph mapped to walking, 4 to 15 mph to running, and above 15 mph to biking. Real-world edge cases and transitions between activities demand increasingly complex branching logic.",
      "current_caption": "**Activity Classification Decision Tree**: A rule-based decision tree classifies human activity by branching on speed thresholds, with values below 4 mph mapped to walking, 4 to 15 mph to running, and above 15 mph to biking. Real-world edge cases and transitions between activities demand increasingly complex branching logic.",
      "new_caption": "",
      "type": "markdown",
      "source_file": "contents/vol1/dl_primer/dl_primer.qmd"
    },
    "fig-connections": {
      "original_caption": "",
      "current_caption": "",
      "new_caption": "",
      "type": "tikz",
      "source_file": "contents/vol1/dl_primer/dl_primer.qmd"
    },
    "fig-ai-ml-dl": {
      "original_caption": "**AI Hierarchy**: Neural networks form a core component of deep learning within machine learning and artificial intelligence by modeling patterns in large datasets. Machine learning algorithms enable systems to learn from data as a subset of the broader AI field.",
      "current_caption": "**AI Hierarchy**: Neural networks form a core component of deep learning within machine learning and artificial intelligence by modeling patterns in large datasets. Machine learning algorithms enable systems to learn from data as a subset of the broader AI field.",
      "new_caption": "",
      "type": "markdown",
      "source_file": "contents/vol1/dl_primer/dl_primer.qmd"
    },
    "fig-deeplearning": {
      "original_caption": "",
      "current_caption": "",
      "new_caption": "",
      "type": "tikz",
      "source_file": "contents/vol1/dl_primer/dl_primer.qmd"
    },
    "fig-double-descent": {
      "original_caption": "\"**The Double Descent Phenomenon**: Why modern deep learning defies classical statistics. In the *Classical Regime* (left), increasing model complexity eventually leads to overfitting (the \\\"U\\\" curve). However, past the *Interpolation Threshold* (middle), where the model perfectly fits the training data, test error drops again in the *Modern Regime* (right). Massive over-parameterization acts as an implicit regularizer, allowing larger models to generalize better.\"",
      "current_caption": "\"**The Double Descent Phenomenon**: Why modern deep learning defies classical statistics. In the *Classical Regime* (left), increasing model complexity eventually leads to overfitting (the \\\"U\\\" curve). However, past the *Interpolation Threshold* (middle), where the model perfectly fits the training data, test error drops again in the *Modern Regime* (right). Massive over-parameterization acts as an implicit regularizer, allowing larger models to generalize better.\"",
      "new_caption": "",
      "type": "code",
      "source_file": "contents/vol1/dl_primer/dl_primer.qmd"
    },
    "fig-usps-digit-examples": {
      "original_caption": "**Handwritten Digit Variability**: Real-world handwritten digits exhibit significant variations in stroke width, slant, and character formation, posing challenges for automated recognition systems like those used by the USPS. These examples demonstrate the need for effective feature extraction and model generalization to achieve high accuracy in optical character recognition (OCR) tasks.",
      "current_caption": "**Handwritten Digit Variability**: Real-world handwritten digits exhibit significant variations in stroke width, slant, and character formation, posing challenges for automated recognition systems like those used by the USPS. These examples demonstrate the need for effective feature extraction and model generalization to achieve high accuracy in optical character recognition (OCR) tasks.",
      "new_caption": "",
      "type": "markdown",
      "source_file": "contents/vol1/dl_primer/dl_primer.qmd"
    },
    "fig-breakout": {
      "original_caption": "",
      "current_caption": "",
      "new_caption": "",
      "type": "tikz",
      "source_file": "contents/vol1/dl_primer/dl_primer.qmd"
    },
    "fig-perceptron": {
      "original_caption": "",
      "current_caption": "",
      "new_caption": "",
      "type": "tikz",
      "source_file": "contents/vol1/dl_primer/dl_primer.qmd"
    },
    "fig-activation-functions": {
      "original_caption": "",
      "current_caption": "",
      "new_caption": "",
      "type": "tikz",
      "source_file": "contents/vol1/dl_primer/dl_primer.qmd"
    },
    "fig-layers": {
      "original_caption": "",
      "current_caption": "",
      "new_caption": "",
      "type": "tikz",
      "source_file": "contents/vol1/dl_primer/dl_primer.qmd"
    },
    "fig-efficiency-frontier": {
      "original_caption": "**The Efficiency Frontier**: ImageNet Top-1 Accuracy vs. Computational Cost (GFLOPs). The dashed 'Pareto Frontier' represents the optimal trade-off between representational power and computational efficiency. Notice the progression from dense CNNs (blue) to efficient Mobile architectures (green) that minimized compute, and finally to Transformers (red) that push the accuracy boundary at significantly higher computational costs.",
      "current_caption": "**The Efficiency Frontier**: ImageNet Top-1 Accuracy vs. Computational Cost (GFLOPs). The dashed 'Pareto Frontier' represents the optimal trade-off between representational power and computational efficiency. Notice the progression from dense CNNs (blue) to efficient Mobile architectures (green) that minimized compute, and finally to Transformers (red) that push the accuracy boundary at significantly higher computational costs.",
      "new_caption": "",
      "type": "code",
      "source_file": "contents/vol1/dnn_architectures/dnn_architectures.qmd"
    },
    "fig-attention-weightcalc": {
      "original_caption": "",
      "current_caption": "",
      "new_caption": "",
      "type": "tikz",
      "source_file": "contents/vol1/dnn_architectures/dnn_architectures.qmd"
    },
    "fig-transformer": {
      "original_caption": "",
      "current_caption": "",
      "new_caption": "",
      "type": "tikz",
      "source_file": "contents/vol1/dnn_architectures/dnn_architectures.qmd"
    },
    "fig-attention": {
      "original_caption": "",
      "current_caption": "",
      "new_caption": "",
      "type": "tikz",
      "source_file": "contents/vol1/dnn_architectures/dnn_architectures.qmd"
    },
    "fig-im2col-diagram": {
      "original_caption": "",
      "current_caption": "",
      "new_caption": "",
      "type": "tikz",
      "source_file": "contents/vol1/dnn_architectures/dnn_architectures.qmd"
    },
    "fig-dnn-fm-framework": {
      "original_caption": "",
      "current_caption": "",
      "new_caption": "",
      "type": "tikz",
      "source_file": "contents/vol1/dnn_architectures/dnn_architectures.qmd"
    },
    "fig-cnn-spatial-processing": {
      "original_caption": "",
      "current_caption": "",
      "new_caption": "",
      "type": "tikz",
      "source_file": "contents/vol1/dnn_architectures/dnn_architectures.qmd"
    },
    "fig-transformer-attention-visualized": {
      "original_caption": "",
      "current_caption": "",
      "new_caption": "",
      "type": "tikz",
      "source_file": "contents/vol1/dnn_architectures/dnn_architectures.qmd"
    },
    "fig-collective-comm": {
      "original_caption": "",
      "current_caption": "",
      "new_caption": "",
      "type": "tikz",
      "source_file": "contents/vol1/dnn_architectures/dnn_architectures.qmd"
    },
    "fig-context-explosion": {
      "original_caption": "**The Context Explosion**: Maximum supported context window (tokens) over time (Log Scale). The transition from 'Standard' windows (512–2k tokens) to 'Massive' windows (1M+ tokens) represents a fundamental shift in how ML systems handle long-range dependencies, increasingly favoring in-context reasoning over traditional retrieval-based approaches.",
      "current_caption": "**The Context Explosion**: Maximum supported context window (tokens) over time (Log Scale). The transition from 'Standard' windows (512–2k tokens) to 'Massive' windows (1M+ tokens) represents a fundamental shift in how ML systems handle long-range dependencies, increasingly favoring in-context reasoning over traditional retrieval-based approaches.",
      "new_caption": "",
      "type": "code",
      "source_file": "contents/vol1/dnn_architectures/dnn_architectures.qmd"
    },
    "fig-mlp": {
      "original_caption": "",
      "current_caption": "",
      "new_caption": "",
      "type": "tikz",
      "source_file": "contents/vol1/dnn_architectures/dnn_architectures.qmd"
    },
    "fig-tensor-data-structure-a": {
      "original_caption": "",
      "current_caption": "",
      "new_caption": "",
      "type": "tikz",
      "source_file": "contents/vol1/frameworks/frameworks.qmd"
    },
    "fig-3d-parallelism": {
      "original_caption": "",
      "current_caption": "",
      "new_caption": "",
      "type": "tikz",
      "source_file": "contents/vol1/frameworks/frameworks.qmd"
    },
    "fig-compilation-continuum": {
      "original_caption": "",
      "current_caption": "",
      "new_caption": "",
      "type": "tikz",
      "source_file": "contents/vol1/frameworks/frameworks.qmd"
    },
    "fig-mlfm-core-ops": {
      "original_caption": "",
      "current_caption": "",
      "new_caption": "",
      "type": "tikz",
      "source_file": "contents/vol1/frameworks/frameworks.qmd"
    },
    "fig-comp-graph": {
      "original_caption": "",
      "current_caption": "",
      "new_caption": "",
      "type": "tikz",
      "source_file": "contents/vol1/frameworks/frameworks.qmd"
    },
    "fig-mlfm-comp-graph": {
      "original_caption": "",
      "current_caption": "",
      "new_caption": "",
      "type": "tikz",
      "source_file": "contents/vol1/frameworks/frameworks.qmd"
    },
    "fig-mlfm-static-graph": {
      "original_caption": "",
      "current_caption": "",
      "new_caption": "",
      "type": "tikz",
      "source_file": "contents/vol1/frameworks/frameworks.qmd"
    },
    "fig-tensor-data-structure-b": {
      "original_caption": "",
      "current_caption": "",
      "new_caption": "",
      "type": "tikz",
      "source_file": "contents/vol1/frameworks/frameworks.qmd"
    },
    "fig-tensor-memory-layout": {
      "original_caption": "",
      "current_caption": "",
      "new_caption": "",
      "type": "tikz",
      "source_file": "contents/vol1/frameworks/frameworks.qmd"
    },
    "fig-python-tax": {
      "original_caption": "**The Python Tax**: Execution timeline for a sequence of small operations (e.g., LayerNorm). In Eager Mode (top), the GPU (blue) finishes processing each op in microseconds but must sit idle while the Python interpreter (red) dispatches the next kernel launch. Compilation (bottom) fuses these operations into a single kernel, effectively hiding the dispatch latency and maximizing GPU utilization.",
      "current_caption": "**The Python Tax**: Execution timeline for a sequence of small operations (e.g., LayerNorm). In Eager Mode (top), the GPU (blue) finishes processing each op in microseconds but must sit idle while the Python interpreter (red) dispatches the next kernel launch. Compilation (bottom) fuses these operations into a single kernel, effectively hiding the dispatch latency and maximizing GPU utilization.",
      "new_caption": "",
      "type": "code",
      "source_file": "contents/vol1/frameworks/frameworks.qmd"
    },
    "fig-tensorflow-architecture": {
      "original_caption": "",
      "current_caption": "",
      "new_caption": "",
      "type": "tikz",
      "source_file": "contents/vol1/frameworks/frameworks.qmd"
    },
    "fig-mlfm-dynamic-graph-flow": {
      "original_caption": "",
      "current_caption": "",
      "new_caption": "",
      "type": "tikz",
      "source_file": "contents/vol1/frameworks/frameworks.qmd"
    },
    "fig-onnx": {
      "original_caption": "**Framework Interoperability**: ONNX enables model portability across frameworks, allowing training in one framework and deployment in another.",
      "current_caption": "**Framework Interoperability**: ONNX enables model portability across frameworks, allowing training in one framework and deployment in another.",
      "new_caption": "",
      "type": "markdown",
      "source_file": "contents/vol1/frameworks/frameworks.qmd"
    },
    "fig-mlfm-timeline": {
      "original_caption": "",
      "current_caption": "",
      "new_caption": "",
      "type": "tikz",
      "source_file": "contents/vol1/frameworks/frameworks.qmd"
    },
    "fig-data-pipeline": {
      "original_caption": "",
      "current_caption": "",
      "new_caption": "",
      "type": "tikz",
      "source_file": "contents/vol1/training/training.qmd"
    },
    "fig-training-pipeline": {
      "original_caption": "",
      "current_caption": "",
      "new_caption": "",
      "type": "tikz",
      "source_file": "contents/vol1/training/training.qmd"
    },
    "fig-model-parallelism": {
      "original_caption": "",
      "current_caption": "",
      "new_caption": "",
      "type": "tikz",
      "source_file": "contents/vol1/training/training.qmd"
    },
    "fig-optimization-flowchart": {
      "original_caption": "",
      "current_caption": "",
      "new_caption": "",
      "type": "tikz",
      "source_file": "contents/vol1/training/training.qmd"
    },
    "fig-galore-llm-memory-breakdown": {
      "original_caption": "",
      "current_caption": "",
      "new_caption": "",
      "type": "tikz",
      "source_file": "contents/vol1/training/training.qmd"
    },
    "fig-fetching-naive": {
      "original_caption": "",
      "current_caption": "",
      "new_caption": "",
      "type": "tikz",
      "source_file": "contents/vol1/training/training.qmd"
    },
    "fig-training-loop": {
      "original_caption": "",
      "current_caption": "",
      "new_caption": "",
      "type": "tikz",
      "source_file": "contents/vol1/training/training.qmd"
    },
    "fig-grad-accumulation": {
      "original_caption": "",
      "current_caption": "",
      "new_caption": "",
      "type": "tikz",
      "source_file": "contents/vol1/training/training.qmd"
    },
    "fig-layers-blocks": {
      "original_caption": "",
      "current_caption": "",
      "new_caption": "",
      "type": "tikz",
      "source_file": "contents/vol1/training/training.qmd"
    },
    "fig-mixed-precision": {
      "original_caption": "",
      "current_caption": "",
      "new_caption": "",
      "type": "tikz",
      "source_file": "contents/vol1/training/training.qmd"
    },
    "fig-fetching-optimized": {
      "original_caption": "",
      "current_caption": "",
      "new_caption": "",
      "type": "tikz",
      "source_file": "contents/vol1/training/training.qmd"
    },
    "fig-activation-checkpointing": {
      "original_caption": "",
      "current_caption": "",
      "new_caption": "",
      "type": "tikz",
      "source_file": "contents/vol1/training/training.qmd"
    },
    "fig-training-roofline": {
      "original_caption": "",
      "current_caption": "",
      "new_caption": "",
      "type": "tikz",
      "source_file": "contents/vol1/training/training.qmd"
    },
    "fig-evolution-systems": {
      "original_caption": "",
      "current_caption": "",
      "new_caption": "",
      "type": "tikz",
      "source_file": "contents/vol1/training/training.qmd"
    },
    "fig-communication-tax": {
      "original_caption": "**The Communication Tax**: Effective Throughput vs. GPU Count (Log-Log Scale). Ideal scaling (dashed gray) represents the linear ceiling. Compute-bound workloads like ResNet (Blue) maintain high efficiency. Balanced workloads like LLMs with high-speed interconnects (Green) show slight degradation, while bandwidth-bound workloads (Red) suffer from the full 'Communication Tax' (shaded region). This divergence reveals why network topology and bandwidth become the dominant constraints when scaling to massive clusters.",
      "current_caption": "**The Communication Tax**: Effective Throughput vs. GPU Count (Log-Log Scale). Ideal scaling (dashed gray) represents the linear ceiling. Compute-bound workloads like ResNet (Blue) maintain high efficiency. Balanced workloads like LLMs with high-speed interconnects (Green) show slight degradation, while bandwidth-bound workloads (Red) suffer from the full 'Communication Tax' (shaded region). This divergence reveals why network topology and bandwidth become the dominant constraints when scaling to massive clusters.",
      "new_caption": "",
      "type": "code",
      "source_file": "contents/vol1/training/training.qmd"
    },
    "fig-tf-bottleneck-trace": {
      "original_caption": "**Data-Bound Profiler Trace**: TensorFlow profiler output capturing a data loading bottleneck during training. The gaps in GPU activity (white regions between compute blocks) indicate periods where the device idles while waiting for input data, with utilization dropping to zero during data loading phases.",
      "current_caption": "**Data-Bound Profiler Trace**: TensorFlow profiler output capturing a data loading bottleneck during training. The gaps in GPU activity (white regions between compute blocks) indicate periods where the device idles while waiting for input data, with utilization dropping to zero during data loading phases.",
      "new_caption": "",
      "type": "markdown",
      "source_file": "contents/vol1/training/training.qmd"
    },
    "fig-linear-scaling-failure": {
      "original_caption": "**The Linear Scaling Failure.** Training Loss vs. Steps. Curve A (Blue) represents a standard baseline batch size. Curve B (Gray) shows what happens when batch size is increased 8x without tuning: convergence slows dramatically because weight updates are too infrequent. Curve C (Green) restores convergence by scaling the learning rate linearly (8x LR), allowing the model to take larger steps to compensate for fewer updates.",
      "current_caption": "**The Linear Scaling Failure.** Training Loss vs. Steps. Curve A (Blue) represents a standard baseline batch size. Curve B (Gray) shows what happens when batch size is increased 8x without tuning: convergence slows dramatically because weight updates are too infrequent. Curve C (Green) restores convergence by scaling the learning rate linearly (8x LR), allowing the model to take larger steps to compensate for fewer updates.",
      "new_caption": "",
      "type": "code",
      "source_file": "contents/vol1/training/training.qmd"
    },
    "fig-activation-perf": {
      "original_caption": "",
      "current_caption": "",
      "new_caption": "",
      "type": "tikz",
      "source_file": "contents/vol1/training/training.qmd"
    },
    "fig-train-data-parallelism": {
      "original_caption": "",
      "current_caption": "",
      "new_caption": "",
      "type": "tikz",
      "source_file": "contents/vol1/training/training.qmd"
    },
    "fig-active-learning-loop": {
      "original_caption": "",
      "current_caption": "",
      "new_caption": "",
      "type": "tikz",
      "source_file": "contents/vol1/data_selection/data_selection.qmd"
    },
    "fig-domain-gap": {
      "original_caption": "",
      "current_caption": "",
      "new_caption": "",
      "type": "tikz",
      "source_file": "contents/vol1/data_selection/data_selection.qmd"
    },
    "fig-coreset-selection": {
      "original_caption": "",
      "current_caption": "",
      "new_caption": "",
      "type": "tikz",
      "source_file": "contents/vol1/data_selection/data_selection.qmd"
    },
    "fig-compute-optimal-frontier": {
      "original_caption": "",
      "current_caption": "",
      "new_caption": "",
      "type": "tikz",
      "source_file": "contents/vol1/data_selection/data_selection.qmd"
    },
    "fig-amortization-comparison": {
      "original_caption": "",
      "current_caption": "",
      "new_caption": "",
      "type": "tikz",
      "source_file": "contents/vol1/data_selection/data_selection.qmd"
    },
    "fig-running-out-of-human-data": {
      "original_caption": "**Dataset Growth Approaching Limits**: Foundation models are increasingly trained on vast datasets, approaching the total stock of human-generated text. Current projections suggest that high-quality public text data faces exhaustion on a near-term horizon, forcing a shift toward data selection, synthetic generation, and multimodal learning. Source: @villalobos_ho_sevilla_besiroglu_heim_hobbhahn_2024.",
      "current_caption": "**Dataset Growth Approaching Limits**: Foundation models are increasingly trained on vast datasets, approaching the total stock of human-generated text. Current projections suggest that high-quality public text data faces exhaustion on a near-term horizon, forcing a shift toward data selection, synthetic generation, and multimodal learning. Source: @villalobos_ho_sevilla_besiroglu_heim_hobbhahn_2024.",
      "new_caption": "",
      "type": "markdown",
      "source_file": "contents/vol1/data_selection/data_selection.qmd"
    },
    "fig-optimization-stack": {
      "original_caption": "",
      "current_caption": "",
      "new_caption": "",
      "type": "tikz",
      "source_file": "contents/vol1/data_selection/data_selection.qmd"
    },
    "fig-ppd-curve": {
      "original_caption": "**Diminishing Returns of Data**: Random sampling (gray) versus data-efficient selection (blue). The efficient strategy achieves higher performance with less data, reaching the convergence plateau much earlier. The red arrow shows the efficiency gap at a fixed dataset size.",
      "current_caption": "**Diminishing Returns of Data**: Random sampling (gray) versus data-efficient selection (blue). The efficient strategy achieves higher performance with less data, reaching the convergence plateau much earlier. The red arrow shows the efficiency gap at a fixed dataset size.",
      "new_caption": "",
      "type": "code",
      "source_file": "contents/vol1/data_selection/data_selection.qmd"
    },
    "fig-active-learning-multiplier": {
      "original_caption": "**The Active Learning Multiplier**: Model Accuracy vs. Number of Labeled Samples (Log Scale). Random sampling (gray dashed) yields linear improvements, often requiring massive datasets to capture rare edge cases. Active Learning (green solid) specifically targets informative samples, achieving the same 90% accuracy with 4x fewer labels. The green shaded region represents the direct economic value (labeling cost saved) of intelligent data selection.",
      "current_caption": "**The Active Learning Multiplier**: Model Accuracy vs. Number of Labeled Samples (Log Scale). Random sampling (gray dashed) yields linear improvements, often requiring massive datasets to capture rare edge cases. Active Learning (green solid) specifically targets informative samples, achieving the same 90% accuracy with 4x fewer labels. The green shaded region represents the direct economic value (labeling cost saved) of intelligent data selection.",
      "new_caption": "",
      "type": "code",
      "source_file": "contents/vol1/data_selection/data_selection.qmd"
    },
    "fig-technique-decision-tree": {
      "original_caption": "",
      "current_caption": "",
      "new_caption": "",
      "type": "tikz",
      "source_file": "contents/vol1/data_selection/data_selection.qmd"
    },
    "fig-selection-inequality": {
      "original_caption": "**The Selection Inequality**: Data selection only improves end-to-end efficiency if the overhead of selection plus training on the subset is less than training on the full dataset. A lightweight selection function (proxy model, cached embeddings) keeps selection overhead low; an expensive selection function (full model forward pass) can negate the savings.",
      "current_caption": "**The Selection Inequality**: Data selection only improves end-to-end efficiency if the overhead of selection plus training on the subset is less than training on the full dataset. A lightweight selection function (proxy model, cached embeddings) keeps selection overhead low; an expensive selection function (full model forward pass) can negate the savings.",
      "new_caption": "",
      "type": "code",
      "source_file": "contents/vol1/data_selection/data_selection.qmd"
    },
    "fig-data-selection-pipeline": {
      "original_caption": "",
      "current_caption": "",
      "new_caption": "",
      "type": "tikz",
      "source_file": "contents/vol1/data_selection/data_selection.qmd"
    },
    "fig-optimization-triad": {
      "original_caption": "",
      "current_caption": "",
      "new_caption": "",
      "type": "tikz",
      "source_file": "contents/vol1/data_selection/data_selection.qmd"
    },
    "fig-tensor-decomposition": {
      "original_caption": "",
      "current_caption": "",
      "new_caption": "",
      "type": "tikz",
      "source_file": "contents/vol1/optimizations/model_compression.qmd"
    },
    "fig-early-exit-transformers": {
      "original_caption": "",
      "current_caption": "",
      "new_caption": "",
      "type": "tikz",
      "source_file": "contents/vol1/optimizations/model_compression.qmd"
    },
    "fig-resnet-activations-histogram": {
      "original_caption": "**Activation Distribution**: Resnet50 layer activations exhibit a long tail, with outlier values that can lead to inefficient precision use if not handled carefully. Source: [@wu2020integer].",
      "current_caption": "**Activation Distribution**: Resnet50 layer activations exhibit a long tail, with outlier values that can lead to inefficient precision use if not handled carefully. Source: [@wu2020integer].",
      "new_caption": "",
      "type": "markdown",
      "source_file": "contents/vol1/optimizations/model_compression.qmd"
    },
    "fig-quantization-free-lunch": {
      "original_caption": "**The Quantization Free Lunch.** Model accuracy vs. Bit-width. Most models exhibit a 'Free Lunch' plateau where reducing precision from FP32 to INT8 yields <1% accuracy loss. This robustness collapses at the 'Quantization Cliff' (typically 3-4 bits), where the noise overwhelms the signal. Transformers (red) are generally more sensitive to quantization outliers than CNNs (blue).",
      "current_caption": "**The Quantization Free Lunch.** Model accuracy vs. Bit-width. Most models exhibit a 'Free Lunch' plateau where reducing precision from FP32 to INT8 yields <1% accuracy loss. This robustness collapses at the 'Quantization Cliff' (typically 3-4 bits), where the noise overwhelms the signal. Transformers (red) are generally more sensitive to quantization outliers than CNNs (blue).",
      "new_caption": "",
      "type": "code",
      "source_file": "contents/vol1/optimizations/model_compression.qmd"
    },
    "fig-quantization-roadmap": {
      "original_caption": "",
      "current_caption": "",
      "new_caption": "",
      "type": "tikz",
      "source_file": "contents/vol1/optimizations/model_compression.qmd"
    },
    "fig-sparse-heat-map": {
      "original_caption": "**Sparsity Distribution**: Darker shades indicate higher sparsity where more weights were removed. Source: Numenta [@numenta_sparsity]",
      "current_caption": "**Sparsity Distribution**: Darker shades indicate higher sparsity where more weights were removed. Source: Numenta [@numenta_sparsity]",
      "new_caption": "",
      "type": "markdown",
      "source_file": "contents/vol1/optimizations/model_compression.qmd"
    },
    "fig-oneshot-pruning": {
      "original_caption": "",
      "current_caption": "",
      "new_caption": "",
      "type": "tikz",
      "source_file": "contents/vol1/optimizations/model_compression.qmd"
    },
    "fig-switch-transformer": {
      "original_caption": "",
      "current_caption": "",
      "new_caption": "",
      "type": "tikz",
      "source_file": "contents/vol1/optimizations/model_compression.qmd"
    },
    "fig-2-4-gemm": {
      "original_caption": "",
      "current_caption": "",
      "new_caption": "",
      "type": "tikz",
      "source_file": "contents/vol1/optimizations/model_compression.qmd"
    },
    "fig-winning-ticket": {
      "original_caption": "",
      "current_caption": "",
      "new_caption": "",
      "type": "tikz",
      "source_file": "contents/vol1/optimizations/model_compression.qmd"
    },
    "fig-structured-unstructured": {
      "original_caption": "",
      "current_caption": "",
      "new_caption": "",
      "type": "tikz",
      "source_file": "contents/vol1/optimizations/model_compression.qmd"
    },
    "fig-3-sections": {
      "original_caption": "",
      "current_caption": "",
      "new_caption": "",
      "type": "tikz",
      "source_file": "contents/vol1/optimizations/model_compression.qmd"
    },
    "fig-quantization_impact": {
      "original_caption": "",
      "current_caption": "",
      "new_caption": "",
      "type": "tikz",
      "source_file": "contents/vol1/optimizations/model_compression.qmd"
    },
    "fig-sparse-matrix": {
      "original_caption": "",
      "current_caption": "",
      "new_caption": "",
      "type": "tikz",
      "source_file": "contents/vol1/optimizations/model_compression.qmd"
    },
    "fig-calibration-ranges": {
      "original_caption": "",
      "current_caption": "",
      "new_caption": "",
      "type": "tikz",
      "source_file": "contents/vol1/optimizations/model_compression.qmd"
    },
    "fig-ptq-calibration": {
      "original_caption": "",
      "current_caption": "",
      "new_caption": "",
      "type": "tikz",
      "source_file": "contents/vol1/optimizations/model_compression.qmd"
    },
    "fig-3float": {
      "original_caption": "",
      "current_caption": "",
      "new_caption": "",
      "type": "tikz",
      "source_file": "contents/vol1/optimizations/model_compression.qmd"
    },
    "fig-compression-methods": {
      "original_caption": "",
      "current_caption": "",
      "new_caption": "",
      "type": "tikz",
      "source_file": "contents/vol1/optimizations/model_compression.qmd"
    },
    "fig-block-sparse-gemm": {
      "original_caption": "",
      "current_caption": "",
      "new_caption": "",
      "type": "tikz",
      "source_file": "contents/vol1/optimizations/model_compression.qmd"
    },
    "fig-quantization": {
      "original_caption": "**The Quantization Free Lunch.** Model accuracy vs. Bit-width. Most models exhibit a 'Free Lunch' plateau where reducing precision from FP32 to INT8 yields <1% accuracy loss. This robustness collapses at the 'Quantization Cliff' (typically 3-4 bits), where the noise overwhelms the signal. Transformers (red) are generally more sensitive to quantization outliers than CNNs (blue).",
      "current_caption": "**The Quantization Free Lunch.** Model accuracy vs. Bit-width. Most models exhibit a 'Free Lunch' plateau where reducing precision from FP32 to INT8 yields <1% accuracy loss. This robustness collapses at the 'Quantization Cliff' (typically 3-4 bits), where the noise overwhelms the signal. Transformers (red) are generally more sensitive to quantization outliers than CNNs (blue).",
      "new_caption": "",
      "type": "code",
      "source_file": "contents/vol1/optimizations/model_compression.qmd"
    },
    "fig-kd-targets": {
      "original_caption": "",
      "current_caption": "",
      "new_caption": "",
      "type": "tikz",
      "source_file": "contents/vol1/optimizations/model_compression.qmd"
    },
    "fig-nas-flow": {
      "original_caption": "",
      "current_caption": "",
      "new_caption": "",
      "type": "tikz",
      "source_file": "contents/vol1/optimizations/model_compression.qmd"
    },
    "fig-quantization-granularity": {
      "original_caption": "",
      "current_caption": "",
      "new_caption": "",
      "type": "tikz",
      "source_file": "contents/vol1/optimizations/model_compression.qmd"
    },
    "fig-matrix-factorization": {
      "original_caption": "",
      "current_caption": "",
      "new_caption": "",
      "type": "tikz",
      "source_file": "contents/vol1/optimizations/model_compression.qmd"
    },
    "fig-automl-comparison": {
      "original_caption": "",
      "current_caption": "",
      "new_caption": "",
      "type": "tikz",
      "source_file": "contents/vol1/optimizations/model_compression.qmd"
    },
    "fig-kd-overview": {
      "original_caption": "",
      "current_caption": "",
      "new_caption": "",
      "type": "tikz",
      "source_file": "contents/vol1/optimizations/model_compression.qmd"
    },
    "fig-weight-activations-quantization": {
      "original_caption": "",
      "current_caption": "",
      "new_caption": "",
      "type": "tikz",
      "source_file": "contents/vol1/optimizations/model_compression.qmd"
    },
    "fig-channel-layer-pruning": {
      "original_caption": "",
      "current_caption": "",
      "new_caption": "",
      "type": "tikz",
      "source_file": "contents/vol1/optimizations/model_compression.qmd"
    },
    "fig-quantized-energy": {
      "original_caption": "",
      "current_caption": "",
      "new_caption": "",
      "type": "tikz",
      "source_file": "contents/vol1/optimizations/model_compression.qmd"
    },
    "fig-color-mapping": {
      "original_caption": "**Convolutional Kernel Weights**: Color mapping reveals learned feature patterns in convolutional filters. Analyzing weight distributions helps diagnose issues like dead or saturated filters. Source: [@alexnet2012].",
      "current_caption": "**Convolutional Kernel Weights**: Color mapping reveals learned feature patterns in convolutional filters. Analyzing weight distributions helps diagnose issues like dead or saturated filters. Source: [@alexnet2012].",
      "new_caption": "",
      "type": "markdown",
      "source_file": "contents/vol1/optimizations/model_compression.qmd"
    },
    "fig-qat": {
      "original_caption": "",
      "current_caption": "",
      "new_caption": "",
      "type": "tikz",
      "source_file": "contents/vol1/optimizations/model_compression.qmd"
    },
    "fig-ptq-qat": {
      "original_caption": "",
      "current_caption": "",
      "new_caption": "",
      "type": "tikz",
      "source_file": "contents/vol1/optimizations/model_compression.qmd"
    },
    "fig-iterative-pruning": {
      "original_caption": "",
      "current_caption": "",
      "new_caption": "",
      "type": "tikz",
      "source_file": "contents/vol1/optimizations/model_compression.qmd"
    },
    "fig-sparse-formats": {
      "original_caption": "",
      "current_caption": "",
      "new_caption": "",
      "type": "tikz",
      "source_file": "contents/vol1/hw_acceleration/hw_acceleration.qmd"
    },
    "fig-ai-performance": {
      "original_caption": "**GPU Performance Scaling**: NVIDIA GPUs experienced approximately a ~1,000$\\times$ increase in integer 8-bit TOPS (tera operations per second) over a decade, from 4 TOPS on the K20X to 4,000 TOPS on the H100. This three-orders-of-magnitude gain was driven by architectural innovations transitioning from floating-point to tensor core acceleration.",
      "current_caption": "**GPU Performance Scaling**: NVIDIA GPUs experienced approximately a ~1,000$\\times$ increase in integer 8-bit TOPS (tera operations per second) over a decade, from 4 TOPS on the K20X to 4,000 TOPS on the H100. This three-orders-of-magnitude gain was driven by architectural innovations transitioning from floating-point to tensor core acceleration.",
      "new_caption": "",
      "type": "markdown",
      "source_file": "contents/vol1/hw_acceleration/hw_acceleration.qmd"
    },
    "fig-rising-ridge": {
      "original_caption": "**The Rising Ridge**: Hardware Arithmetic Intensity (FLOPs/Byte) over time. As compute capability (FLOPs) grows faster than memory bandwidth (Bytes/s), the 'Ridge Point'—the intensity required to saturate the chip—skyrockets. This trend explains why architectures with high data reuse (like Transformers) have flourished while sparse or low-reuse architectures (like RNNs) face a growing 'Hardware Tax' that makes them increasingly inefficient on modern silicon.",
      "current_caption": "**The Rising Ridge**: Hardware Arithmetic Intensity (FLOPs/Byte) over time. As compute capability (FLOPs) grows faster than memory bandwidth (Bytes/s), the 'Ridge Point'—the intensity required to saturate the chip—skyrockets. This trend explains why architectures with high data reuse (like Transformers) have flourished while sparse or low-reuse architectures (like RNNs) face a growing 'Hardware Tax' that makes them increasingly inefficient on modern silicon.",
      "new_caption": "",
      "type": "code",
      "source_file": "contents/vol1/hw_acceleration/hw_acceleration.qmd"
    },
    "fig-iron-law-heatmap": {
      "original_caption": "**The Iron Law Heatmap**: Total system speedup as a function of Accelerator Speed ($S$) and Parallel Fraction ($P$). The 'Accelerator Wall' at the top reveals that if a workload is even slightly serial ($P < 0.9$), increasing hardware speed yields almost no benefit. Realizing the potential of 1000x accelerators requires engineering workloads with near-perfect ($P > 0.999$) parallelism.",
      "current_caption": "**The Iron Law Heatmap**: Total system speedup as a function of Accelerator Speed ($S$) and Parallel Fraction ($P$). The 'Accelerator Wall' at the top reveals that if a workload is even slightly serial ($P < 0.9$), increasing hardware speed yields almost no benefit. Realizing the potential of 1000x accelerators requires engineering workloads with near-perfect ($P > 0.999$) parallelism.",
      "new_caption": "",
      "type": "code",
      "source_file": "contents/vol1/hw_acceleration/hw_acceleration.qmd"
    },
    "fig-compute-memory-imbalance": {
      "original_caption": "**The Compute-Bandwidth Divergence**: Computational capability and memory bandwidth plotted on a log scale (2000–2025). While arithmetic throughput (FLOPs) has grown exponentially, memory bandwidth has improved at a significantly slower linear rate. This widening 'Systems Gap' defines the AI Memory Wall, forcing architects to design systems that minimize data movement to avoid idling powerful compute units.",
      "current_caption": "**The Compute-Bandwidth Divergence**: Computational capability and memory bandwidth plotted on a log scale (2000–2025). While arithmetic throughput (FLOPs) has grown exponentially, memory bandwidth has improved at a significantly slower linear rate. This widening 'Systems Gap' defines the AI Memory Wall, forcing architects to design systems that minimize data movement to avoid idling powerful compute units.",
      "new_caption": "",
      "type": "code",
      "source_file": "contents/vol1/hw_acceleration/hw_acceleration.qmd"
    },
    "fig-tech-s-curve": {
      "original_caption": "**The Twin S-Curves of Modern Computing**. General-purpose CPUs (gray) enjoyed decades of exponential growth driven by Moore's Law and Dennard Scaling. As physics constrained this curve around 2010 (Saturation), the industry was forced to jump to a new curve: Domain Specific Architectures (blue). We are currently in the **Take-off** phase of this new paradigm, where massive efficiency gains are unlocked by specializing hardware for linear algebra, albeit at the cost of general programmability.",
      "current_caption": "**The Twin S-Curves of Modern Computing**. General-purpose CPUs (gray) enjoyed decades of exponential growth driven by Moore's Law and Dennard Scaling. As physics constrained this curve around 2010 (Saturation), the industry was forced to jump to a new curve: Domain Specific Architectures (blue). We are currently in the **Take-off** phase of this new paradigm, where massive efficiency gains are unlocked by specializing hardware for linear algebra, albeit at the cost of general programmability.",
      "new_caption": "",
      "type": "code",
      "source_file": "contents/vol1/hw_acceleration/hw_acceleration.qmd"
    },
    "fig-systems-gap": {
      "original_caption": "**The Systems Gap**: Relative Compute Growth (Log Scale) comparing Model Demand to Hardware Supply. The gray dotted line (CPU) and blue dashed line (GPU) reflect hardware progress, which significantly lags behind the exponential red solid line (Model Demand). The massive purple shaded region represents the 'Systems Gap': a physical and economic deficit that cannot be closed by faster chips alone, but must be bridged through parallelism, architectural innovation, and hardware-software co-design.",
      "current_caption": "**The Systems Gap**: Relative Compute Growth (Log Scale) comparing Model Demand to Hardware Supply. The gray dotted line (CPU) and blue dashed line (GPU) reflect hardware progress, which significantly lags behind the exponential red solid line (Model Demand). The massive purple shaded region represents the 'Systems Gap': a physical and economic deficit that cannot be closed by faster chips alone, but must be bridged through parallelism, architectural innovation, and hardware-software co-design.",
      "new_caption": "",
      "type": "code",
      "source_file": "contents/vol1/hw_acceleration/hw_acceleration.qmd"
    },
    "fig-memory-wall": {
      "original_caption": "**Model Size vs. Hardware Bandwidth.** Model parameter counts and hardware memory bandwidth plotted from 2012 to 2024, showing how model growth from AlexNet to trillion-parameter models has far outpaced bandwidth improvements across GPU and TPU generations.",
      "current_caption": "**Model Size vs. Hardware Bandwidth.** Model parameter counts and hardware memory bandwidth plotted from 2012 to 2024, showing how model growth from AlexNet to trillion-parameter models has far outpaced bandwidth improvements across GPU and TPU generations.",
      "new_caption": "",
      "type": "code",
      "source_file": "contents/vol1/hw_acceleration/hw_acceleration.qmd"
    },
    "fig-tiling-diagram": {
      "original_caption": "",
      "current_caption": "",
      "new_caption": "",
      "type": "tikz",
      "source_file": "contents/vol1/hw_acceleration/hw_acceleration.qmd"
    },
    "fig-timeline": {
      "original_caption": "",
      "current_caption": "",
      "new_caption": "",
      "type": "tikz",
      "source_file": "contents/vol1/hw_acceleration/hw_acceleration.qmd"
    },
    "fig-host-accelerator-data-movement": {
      "original_caption": "",
      "current_caption": "",
      "new_caption": "",
      "type": "tikz",
      "source_file": "contents/vol1/hw_acceleration/hw_acceleration.qmd"
    },
    "fig-accelerator-anatomy": {
      "original_caption": "",
      "current_caption": "",
      "new_caption": "",
      "type": "tikz",
      "source_file": "contents/vol1/hw_acceleration/hw_acceleration.qmd"
    },
    "fig-energy-hierarchy": {
      "original_caption": "**The Energy Hierarchy**: Energy cost per operation (Log Scale) based on the 'Horowitz Numbers.' Fetching data from off-chip DRAM costs ~128x more energy than an SRAM access and ~20,000x more than an INT8 addition. This fundamental physical disparity dictates that AI accelerators must prioritize data locality (keeping weights in SRAM/Registers) over raw arithmetic throughput to remain within power budgets.",
      "current_caption": "**The Energy Hierarchy**: Energy cost per operation (Log Scale) based on the 'Horowitz Numbers.' Fetching data from off-chip DRAM costs ~128x more energy than an SRAM access and ~20,000x more than an INT8 addition. This fundamental physical disparity dictates that AI accelerators must prioritize data locality (keeping weights in SRAM/Registers) over raw arithmetic throughput to remain within power budgets.",
      "new_caption": "",
      "type": "code",
      "source_file": "contents/vol1/hw_acceleration/hw_acceleration.qmd"
    },
    "fig-systolic-array": {
      "original_caption": "",
      "current_caption": "",
      "new_caption": "",
      "type": "tikz",
      "source_file": "contents/vol1/hw_acceleration/hw_acceleration.qmd"
    },
    "fig-power-differentials": {
      "original_caption": "**Power Consumption Differentials**: Power usage spans six orders of magnitude across ML system types, from milliwatts in tinyML devices through watts at the edge to kilowatts in datacenter inference and hundreds of kilowatts for training clusters. These differentials shape deployment trade-offs between latency, cost, and energy efficiency.",
      "current_caption": "**Power Consumption Differentials**: Power usage spans six orders of magnitude across ML system types, from milliwatts in tinyML devices through watts at the edge to kilowatts in datacenter inference and hundreds of kilowatts for training clusters. These differentials shape deployment trade-offs between latency, cost, and energy efficiency.",
      "new_caption": "",
      "type": "code",
      "source_file": "contents/vol1/benchmarking/benchmarking.qmd"
    },
    "fig-benchmark-tradeoffs": {
      "original_caption": "",
      "current_caption": "",
      "new_caption": "",
      "type": "tikz",
      "source_file": "contents/vol1/benchmarking/benchmarking.qmd"
    },
    "fig-dataset-saturation": {
      "original_caption": "",
      "current_caption": "",
      "new_caption": "",
      "type": "tikz",
      "source_file": "contents/vol1/benchmarking/benchmarking.qmd"
    },
    "fig-granularity": {
      "original_caption": "",
      "current_caption": "",
      "new_caption": "",
      "type": "tikz",
      "source_file": "contents/vol1/benchmarking/benchmarking.qmd"
    },
    "fig-power-trends": {
      "original_caption": "",
      "current_caption": "",
      "new_caption": "",
      "type": "tikz",
      "source_file": "contents/vol1/benchmarking/benchmarking.qmd"
    },
    "fig-hw-lottery": {
      "original_caption": "",
      "current_caption": "",
      "new_caption": "",
      "type": "tikz",
      "source_file": "contents/vol1/benchmarking/benchmarking.qmd"
    },
    "fig-power-diagram": {
      "original_caption": "",
      "current_caption": "",
      "new_caption": "",
      "type": "tikz",
      "source_file": "contents/vol1/benchmarking/benchmarking.qmd"
    },
    "fig-benchmark-components": {
      "original_caption": "",
      "current_caption": "",
      "new_caption": "",
      "type": "tikz",
      "source_file": "contents/vol1/benchmarking/benchmarking.qmd"
    },
    "fig-imagenet-gpus": {
      "original_caption": "",
      "current_caption": "",
      "new_caption": "",
      "type": "tikz",
      "source_file": "contents/vol1/benchmarking/benchmarking.qmd"
    },
    "fig-sciml-graph": {
      "original_caption": "",
      "current_caption": "",
      "new_caption": "",
      "type": "tikz",
      "source_file": "contents/vol1/benchmarking/benchmarking.qmd"
    },
    "fig-model-vs-data": {
      "original_caption": "",
      "current_caption": "",
      "new_caption": "",
      "type": "tikz",
      "source_file": "contents/vol1/benchmarking/benchmarking.qmd"
    },
    "fig-mlperf-training-improve": {
      "original_caption": "",
      "current_caption": "",
      "new_caption": "",
      "type": "tikz",
      "source_file": "contents/vol1/benchmarking/benchmarking.qmd"
    },
    "fig-imagenet-challenge": {
      "original_caption": "**ImageNet Challenge Progression**: Neural networks have reduced error rates from 28.2% in 2010 to 3.57% by 2015, highlighting the impact of architectural advancements on classification accuracy. These milestones establish the baselines against which compression techniques are evaluated.",
      "current_caption": "**ImageNet Challenge Progression**: Neural networks have reduced error rates from 28.2% in 2010 to 3.57% by 2015, highlighting the impact of architectural advancements on classification accuracy. These milestones establish the baselines against which compression techniques are evaluated.",
      "new_caption": "",
      "type": "code",
      "source_file": "contents/vol1/benchmarking/benchmarking.qmd"
    },
    "fig-kv-cache-growth": {
      "original_caption": "**The KV-Cache Explosion**: Memory usage vs. Context Length for a 70B parameter model. The linear growth of the Key-Value cache (storing attention history) quickly consumes available GPU memory (red dashed line). For batch size 32 (purple), the system hits the 'OOM Zone' at just 8k context length, forcing a trade-off between batch size (throughput) and context window (capability).",
      "current_caption": "**The KV-Cache Explosion**: Memory usage vs. Context Length for a 70B parameter model. The linear growth of the Key-Value cache (storing attention history) quickly consumes available GPU memory (red dashed line). For batch size 32 (purple), the system hits the 'OOM Zone' at just 8k context length, forcing a trade-off between batch size (throughput) and context window (capability).",
      "new_caption": "",
      "type": "code",
      "source_file": "contents/vol1/serving/serving.qmd"
    },
    "fig-server-anatomy": {
      "original_caption": "",
      "current_caption": "",
      "new_caption": "",
      "type": "tikz",
      "source_file": "contents/vol1/serving/serving.qmd"
    },
    "fig-throughput-latency-knee": {
      "original_caption": "**The Throughput-Latency Knee.** Batch Size vs. Throughput (Blue) and Latency (Orange). Throughput increases with batch size as hardware utilization improves, but eventually saturates. Latency remains relatively flat (hidden by parallel resources) until the 'Knee,' after which it spikes linearly due to queuing. The optimal operating point lies just before this spike.",
      "current_caption": "**The Throughput-Latency Knee.** Batch Size vs. Throughput (Blue) and Latency (Orange). Throughput increases with batch size as hardware utilization improves, but eventually saturates. Latency remains relatively flat (hidden by parallel resources) until the 'Knee,' after which it spikes linearly due to queuing. The optimal operating point lies just before this spike.",
      "new_caption": "",
      "type": "code",
      "source_file": "contents/vol1/serving/serving.qmd"
    },
    "fig-serving-pipeline-timing": {
      "original_caption": "",
      "current_caption": "",
      "new_caption": "",
      "type": "tikz",
      "source_file": "contents/vol1/serving/serving.qmd"
    },
    "fig-serving-inference-pipeline": {
      "original_caption": "",
      "current_caption": "",
      "new_caption": "",
      "type": "tikz",
      "source_file": "contents/vol1/serving/serving.qmd"
    },
    "fig-tail-latency-explosion": {
      "original_caption": "**The Tail Latency Explosion**: Request Latency vs. System Utilization ($\\\\rho$). While median latency (Blue) remains stable, tail latency (Red, p99) explodes exponentially once utilization passes the 'Knee' at ~70%. This physics-driven behavior dictates that production serving clusters must maintain headroom (40-60% utilization) to guarantee stable response times and avoid queue collapse under stochastic load.",
      "current_caption": "**The Tail Latency Explosion**: Request Latency vs. System Utilization ($\\\\rho$). While median latency (Blue) remains stable, tail latency (Red, p99) explodes exponentially once utilization passes the 'Knee' at ~70%. This physics-driven behavior dictates that production serving clusters must maintain headroom (40-60% utilization) to guarantee stable response times and avoid queue collapse under stochastic load.",
      "new_caption": "",
      "type": "code",
      "source_file": "contents/vol1/serving/serving.qmd"
    },
    "fig-intelligence-deflation": {
      "original_caption": "**Intelligence Deflation**: Cost per 1M tokens (USD) over time (Log Scale). The cost of token generation has collapsed by multiple orders of magnitude (2020–2025). Initially driven by OpenAI's GPT series, the market has entered a phase of intense price competition with entrants like Anthropic (Claude), Google (Gemini), and DeepSeek pushing costs below $0.10/million tokens. This hyper-deflation transforms the economics of automated AI workflows.",
      "current_caption": "**Intelligence Deflation**: Cost per 1M tokens (USD) over time (Log Scale). The cost of token generation has collapsed by multiple orders of magnitude (2020–2025). Initially driven by OpenAI's GPT series, the market has entered a phase of intense price competition with entrants like Anthropic (Claude), Google (Gemini), and DeepSeek pushing costs below $0.10/million tokens. This hyper-deflation transforms the economics of automated AI workflows.",
      "new_caption": "",
      "type": "code",
      "source_file": "contents/vol1/serving/serving.qmd"
    },
    "fig-interactive-loop": {
      "original_caption": "",
      "current_caption": "",
      "new_caption": "",
      "type": "tikz",
      "source_file": "contents/vol1/ops/ops.qmd"
    },
    "fig-uptime-iceberg": {
      "original_caption": "",
      "current_caption": "",
      "new_caption": "",
      "type": "tikz",
      "source_file": "contents/vol1/ops/ops.qmd"
    },
    "fig-mlops-diagram": {
      "original_caption": "",
      "current_caption": "",
      "new_caption": "",
      "type": "tikz",
      "source_file": "contents/vol1/ops/ops.qmd"
    },
    "fig-technical-debt": {
      "original_caption": "",
      "current_caption": "",
      "new_caption": "",
      "type": "tikz",
      "source_file": "contents/vol1/ops/ops.qmd"
    },
    "fig-clinaiops": {
      "original_caption": "",
      "current_caption": "",
      "new_caption": "",
      "type": "tikz",
      "source_file": "contents/vol1/ops/ops.qmd"
    },
    "fig-rotting-asset-curve": {
      "original_caption": "**The Rotting Asset Curve**: Model Accuracy vs. Time (Days) showing the impact of statistical drift. Unlike traditional software that remains static unless modified, ML models decay as the world changes. Periodic retraining (sawtooth) and triggered retraining (green) are the primary engineering responses to prevent this silent failure. Monitoring is the mechanism that transforms model 'decay' into a manageable maintenance schedule.",
      "current_caption": "**The Rotting Asset Curve**: Model Accuracy vs. Time (Days) showing the impact of statistical drift. Unlike traditional software that remains static unless modified, ML models decay as the world changes. Periodic retraining (sawtooth) and triggered retraining (green) are the primary engineering responses to prevent this silent failure. Monitoring is the mechanism that transforms model 'decay' into a manageable maintenance schedule.",
      "new_caption": "",
      "type": "code",
      "source_file": "contents/vol1/ops/ops.qmd"
    },
    "fig-correction-cascades-flowchart": {
      "original_caption": "",
      "current_caption": "",
      "new_caption": "",
      "type": "tikz",
      "source_file": "contents/vol1/ops/ops.qmd"
    },
    "fig-business-cost-curve": {
      "original_caption": "**The Business Cost Curve.** Expected Cost vs. Classification Threshold. Technical metrics like ROC curves hide the economic reality: errors have different costs. In this fraud detection scenario, a False Negative (missed fraud) costs $1000, while a False Positive (blocked user) costs $10. The optimal threshold ($T=0.85$) is shifted far to the right to minimize total cost, even if it reduces aggregate accuracy. MLOps is the discipline of tuning this threshold dynamically as costs change.",
      "current_caption": "**The Business Cost Curve.** Expected Cost vs. Classification Threshold. Technical metrics like ROC curves hide the economic reality: errors have different costs. In this fraud detection scenario, a False Negative (missed fraud) costs $1000, while a False Positive (blocked user) costs $10. The optimal threshold ($T=0.85$) is shifted far to the right to minimize total cost, even if it reduces aggregate accuracy. MLOps is the discipline of tuning this threshold dynamically as costs change.",
      "new_caption": "",
      "type": "code",
      "source_file": "contents/vol1/ops/ops.qmd"
    },
    "fig-technical-debt-taxonomy": {
      "original_caption": "",
      "current_caption": "",
      "new_caption": "",
      "type": "tikz",
      "source_file": "contents/vol1/ops/ops.qmd"
    },
    "fig-data-drift": {
      "original_caption": "",
      "current_caption": "",
      "new_caption": "",
      "type": "tikz",
      "source_file": "contents/vol1/ops/ops.qmd"
    },
    "fig-ops-layers": {
      "original_caption": "",
      "current_caption": "",
      "new_caption": "",
      "type": "tikz",
      "source_file": "contents/vol1/ops/ops.qmd"
    },
    "fig-ops-cicd": {
      "original_caption": "",
      "current_caption": "",
      "new_caption": "",
      "type": "tikz",
      "source_file": "contents/vol1/ops/ops.qmd"
    },
    "fig-data-card": {
      "original_caption": "",
      "current_caption": "",
      "new_caption": "",
      "type": "tikz",
      "source_file": "contents/vol1/responsible_engr/responsible_engr.qmd"
    },
    "fig-interpretability-spectrum": {
      "original_caption": "**Model Interpretability Spectrum**. A horizontal spectrum arranges model architectures from most interpretable on the left (decision trees, linear regression, logistic regression) to least interpretable on the right (random forests, neural networks, convolutional neural networks). Models on the left allow direct inspection of decision logic, while those on the right require post-hoc explanation techniques such as LIME or SHAP. High-stakes regulatory requirements may constrain model selection toward the interpretable end of this spectrum.",
      "current_caption": "**Model Interpretability Spectrum**. A horizontal spectrum arranges model architectures from most interpretable on the left (decision trees, linear regression, logistic regression) to least interpretable on the right (random forests, neural networks, convolutional neural networks). Models on the left allow direct inspection of decision logic, while those on the right require post-hoc explanation techniques such as LIME or SHAP. High-stakes regulatory requirements may constrain model selection toward the interpretable end of this spectrum.",
      "new_caption": "",
      "type": "markdown",
      "source_file": "contents/vol1/responsible_engr/responsible_engr.qmd"
    },
    "fig-data-governance-pillars": {
      "original_caption": "",
      "current_caption": "",
      "new_caption": "",
      "type": "tikz",
      "source_file": "contents/vol1/responsible_engr/responsible_engr.qmd"
    },
    "fig-governance-layers": {
      "original_caption": "**Responsible AI Governance Layers**. Nested governance structures surround engineering practice. At the center, engineering teams implement technical safeguards. Successive layers represent organizational safety culture, industry certification and external review, and government regulation. Technical excellence at the center enables compliance with requirements flowing inward from outer layers.",
      "current_caption": "**Responsible AI Governance Layers**. Nested governance structures surround engineering practice. At the center, engineering teams implement technical safeguards. Successive layers represent organizational safety culture, industry certification and external review, and government regulation. Technical excellence at the center enables compliance with requirements flowing inward from outer layers.",
      "new_caption": "",
      "type": "markdown",
      "source_file": "contents/vol1/responsible_engr/responsible_engr.qmd"
    },
    "fig-fairness-frontier": {
      "original_caption": "**The Fairness-Accuracy Pareto Frontier.** Model Accuracy vs. Demographic Disparity. Point A represents unconstrained optimization (maximum accuracy, high disparity). Point C represents strict equality constraints (zero disparity, significant accuracy drop). Point B is the 'Sweet Spot' where engineers can often reduce disparity by 80% while sacrificing less than 1% of aggregate accuracy. Responsible engineering is the practice of finding and implementing Point B.",
      "current_caption": "**The Fairness-Accuracy Pareto Frontier.** Model Accuracy vs. Demographic Disparity. Point A represents unconstrained optimization (maximum accuracy, high disparity). Point C represents strict equality constraints (zero disparity, significant accuracy drop). Point B is the 'Sweet Spot' where engineers can often reduce disparity by 80% while sacrificing less than 1% of aggregate accuracy. Responsible engineering is the practice of finding and implementing Point B.",
      "new_caption": "",
      "type": "code",
      "source_file": "contents/vol1/responsible_engr/responsible_engr.qmd"
    },
    "fig-fairness-threshold": {
      "original_caption": "**Threshold Effects on Subgroup Outcomes**. A single classification threshold (vertical lines) applied to two subgroups with different score distributions produces disparate outcomes. Circles represent positive outcomes (loan repayment), crosses represent negative outcomes (default). The 75% threshold approves most of Subgroup A but rejects most of Subgroup B, even when qualified individuals exist in both groups. The 81.25% threshold shows how threshold adjustment changes the fairness-accuracy tradeoff. This visualization explains why aggregate accuracy can mask severe subgroup disparities.",
      "current_caption": "**Threshold Effects on Subgroup Outcomes**. A single classification threshold (vertical lines) applied to two subgroups with different score distributions produces disparate outcomes. Circles represent positive outcomes (loan repayment), crosses represent negative outcomes (default). The 75% threshold approves most of Subgroup A but rejects most of Subgroup B, even when qualified individuals exist in both groups. The 81.25% threshold shows how threshold adjustment changes the fairness-accuracy tradeoff. This visualization explains why aggregate accuracy can mask severe subgroup disparities.",
      "new_caption": "",
      "type": "markdown",
      "source_file": "contents/vol1/responsible_engr/responsible_engr.qmd"
    },
    "fig-invariants-cycle": {
      "original_caption": "",
      "current_caption": "",
      "new_caption": "",
      "type": "tikz",
      "source_file": "contents/vol1/conclusion/conclusion.qmd"
    }
  },
  "tables": {
    "tbl-ai-evolution-strengths": {
      "original_caption": "**AI Paradigm Evolution**: Each era is defined by the systems bottleneck that constrained it. Deep learning (far right) overcame the Feature Engineering bottleneck but introduced new infrastructure challenges, necessitating modern ML systems engineering.",
      "current_caption": "**AI Paradigm Evolution**: Each era is defined by the systems bottleneck that constrained it. Deep learning (far right) overcame the Feature Engineering bottleneck but introduced new infrastructure challenges, necessitating modern ML systems engineering.",
      "new_caption": "",
      "type": "table",
      "source_file": "contents/vol1/introduction/introduction.qmd"
    },
    "tbl-book-structure": {
      "original_caption": "**Book Organization**: The four parts follow a pedagogical progression from context (Foundations) through theory (Build) to practice (Optimize, Deploy).",
      "current_caption": "**Book Organization**: The four parts follow a pedagogical progression from context (Foundations) through theory (Build) to practice (Optimize, Deploy).",
      "new_caption": "",
      "type": "table",
      "source_file": "contents/vol1/introduction/introduction.qmd"
    },
    "tbl-software-1-vs-2": {
      "original_caption": "**The Paradigm Shift from Software 1.0 to Software 2.0**: In Software 2.0, the \"programmer\" does not write the logic; they curate the dataset that the optimization process uses to write the logic. Debugging therefore moves upstream from code to data.",
      "current_caption": "**The Paradigm Shift from Software 1.0 to Software 2.0**: In Software 2.0, the \"programmer\" does not write the logic; they curate the dataset that the optimization process uses to write the logic. Debugging therefore moves upstream from code to data.",
      "new_caption": "",
      "type": "table",
      "source_file": "contents/vol1/introduction/introduction.qmd"
    },
    "tbl-lighthouse-examples": {
      "original_caption": "**Lighthouse Models as Systems Detectives**: Each workload isolates a distinct bottleneck, enabling systematic investigation of how system constraints affect different architectural patterns. Quantitative specifications and architectural details appear in @sec-dnn-architectures.",
      "current_caption": "**Lighthouse Models as Systems Detectives**: Each workload isolates a distinct bottleneck, enabling systematic investigation of how system constraints affect different architectural patterns. Quantitative specifications and architectural details appear in @sec-dnn-architectures.",
      "new_caption": "",
      "type": "table",
      "source_file": "contents/vol1/introduction/introduction.qmd"
    },
    "tbl-efficiency-priorities": {
      "original_caption": "**Efficiency Priorities by Deployment Context**: Each deployment environment creates distinct bottlenecks, requiring tailored optimization strategies. Cloud systems optimize for throughput and cost; edge systems optimize for memory and power; TinyML systems require extreme efficiency across all dimensions.",
      "current_caption": "**Efficiency Priorities by Deployment Context**: Each deployment environment creates distinct bottlenecks, requiring tailored optimization strategies. Cloud systems optimize for throughput and cost; edge systems optimize for memory and power; TinyML systems require extreme efficiency across all dimensions.",
      "new_caption": "",
      "type": "table",
      "source_file": "contents/vol1/introduction/introduction.qmd"
    },
    "tbl-dam-taxonomy": {
      "original_caption": "**The DAM Taxonomy**: Every ML system comprises these three interdependent components. When performance stalls, ask: *\"Where is the flow blocked? Check the DAM.\"*",
      "current_caption": "**The DAM Taxonomy**: Every ML system comprises these three interdependent components. When performance stalls, ask: *\"Where is the flow blocked? Check the DAM.\"*",
      "new_caption": "",
      "type": "table",
      "source_file": "contents/vol1/introduction/introduction.qmd"
    },
    "tbl-big_vs_tiny": {
      "original_caption": "**Deployment Locations**: Machine learning systems vary in where computation occurs, from centralized cloud servers to local edge devices and ultra-low-power TinyML chips, each impacting latency, bandwidth, and energy consumption. Deployments are categorized by processing location and associated characteristics, enabling informed decisions about system architecture and resource allocation.",
      "current_caption": "**Deployment Locations**: Machine learning systems vary in where computation occurs, from centralized cloud servers to local edge devices and ultra-low-power TinyML chips, each impacting latency, bandwidth, and energy consumption. Deployments are categorized by processing location and associated characteristics, enabling informed decisions about system architecture and resource allocation.",
      "new_caption": "",
      "type": "table",
      "source_file": "contents/vol1/ml_systems/ml_systems.qmd"
    },
    "tbl-representative-systems": {
      "original_caption": "**Hardware Spectrum (Concrete Platforms)**: Representative devices that instantiate each deployment paradigm from @tbl-deployment-paradigms-overview. Where the conceptual table defines operating regimes, this table provides the specific processors, memory capacities, power envelopes, and quantitative thresholds that practitioners use to match workloads to hardware.",
      "current_caption": "**Hardware Spectrum (Concrete Platforms)**: Representative devices that instantiate each deployment paradigm from @tbl-deployment-paradigms-overview. Where the conceptual table defines operating regimes, this table provides the specific processors, memory capacities, power envelopes, and quantitative thresholds that practitioners use to match workloads to hardware.",
      "new_caption": "",
      "type": "table",
      "source_file": "contents/vol1/ml_systems/ml_systems.qmd"
    },
    "tbl-latency-numbers": {
      "original_caption": "**Latency Numbers for ML System Design**: Order-of-magnitude latencies across compute, memory, network, and ML operations that determine deployment feasibility. Spanning eight orders of magnitude, from nanosecond compute operations to hundreds of milliseconds for cross-region network calls, these physical constraints shape architectural decisions.",
      "current_caption": "**Latency Numbers for ML System Design**: Order-of-magnitude latencies across compute, memory, network, and ML operations that determine deployment feasibility. Spanning eight orders of magnitude, from nanosecond compute operations to hundreds of milliseconds for cross-region network calls, these physical constraints shape architectural decisions.",
      "new_caption": "",
      "type": "table",
      "source_file": "contents/vol1/ml_systems/ml_systems.qmd"
    },
    "tbl-deployment-paradigms-overview": {
      "original_caption": "**The Deployment Spectrum (Conceptual)**: Four paradigms span nine orders of magnitude in power (MW to mW) and memory (TB to KB). This conceptual overview defines each paradigm by its operating regime; @tbl-representative-systems later grounds these categories in specific hardware platforms and quantitative decision thresholds.",
      "current_caption": "**The Deployment Spectrum (Conceptual)**: Four paradigms span nine orders of magnitude in power (MW to mW) and memory (TB to KB). This conceptual overview defines each paradigm by its operating regime; @tbl-representative-systems later grounds these categories in specific hardware platforms and quantitative decision thresholds.",
      "new_caption": "",
      "type": "table",
      "source_file": "contents/vol1/ml_systems/ml_systems.qmd"
    },
    "tbl-dam-phase": {
      "original_caption": "**DAM × Phase**: The same model imposes fundamentally different demands on Data, Algorithm, and Machine depending on whether the system is training or serving. When bottlenecks shift unexpectedly, check which phase you're optimizing for.",
      "current_caption": "**DAM × Phase**: The same model imposes fundamentally different demands on Data, Algorithm, and Machine depending on whether the system is training or serving. When bottlenecks shift unexpectedly, check which phase you're optimizing for.",
      "new_caption": "",
      "type": "table",
      "source_file": "contents/vol1/ml_systems/ml_systems.qmd"
    },
    "tbl-stage-interface": {
      "original_caption": "**Stage Interface Specification**: Each lifecycle stage has explicit input requirements, output deliverables, and quality invariants that must hold for the stage to be considered complete. Violations of these contracts create technical debt that compounds through subsequent stages. The deployment paradigm selection in Problem Definition (Cloud, Edge, Mobile, or TinyML from @sec-ml-system-architecture) constrains all downstream stages, as a TinyML target imposes different data, model, and monitoring requirements than a Cloud target.",
      "current_caption": "**Stage Interface Specification**: Each lifecycle stage has explicit input requirements, output deliverables, and quality invariants that must hold for the stage to be considered complete. Violations of these contracts create technical debt that compounds through subsequent stages. The deployment paradigm selection in Problem Definition (Cloud, Edge, Mobile, or TinyML from @sec-ml-system-architecture) constrains all downstream stages, as a TinyML target imposes different data, model, and monitoring requirements than a Cloud target.",
      "new_caption": "",
      "type": "table",
      "source_file": "contents/vol1/workflow/workflow.qmd"
    },
    "tbl-sw-ml-cycles": {
      "original_caption": "**Traditional Software vs ML Development Lifecycles**: Six dimensions where ML development diverges fundamentally from traditional software engineering. The most critical difference appears in the final row: while traditional software rarely sees later stages influence earlier phases, ML systems require continuous feedback loops where deployment insights reshape data collection, monitoring drives model updates, and production experiences inform architectural decisions. These differences explain why traditional project management approaches fail when applied to ML projects without modification.",
      "current_caption": "**Traditional Software vs ML Development Lifecycles**: Six dimensions where ML development diverges fundamentally from traditional software engineering. The most critical difference appears in the final row: while traditional software rarely sees later stages influence earlier phases, ML systems require continuous feedback loops where deployment insights reshape data collection, monitoring drives model updates, and production experiences inform architectural decisions. These differences explain why traditional project management approaches fail when applied to ML projects without modification.",
      "new_caption": "",
      "type": "table",
      "source_file": "contents/vol1/workflow/workflow.qmd"
    },
    "tbl-four-pillars-diagnostic": {
      "original_caption": "**Four Pillars Diagnostic Guide**: When ML systems exhibit failures, this mapping helps teams identify which pillar to investigate first based on observed symptoms.",
      "current_caption": "**Four Pillars Diagnostic Guide**: When ML systems exhibit failures, this mapping helps teams identify which pillar to investigate first based on observed symptoms.",
      "new_caption": "",
      "type": "table",
      "source_file": "contents/vol1/data_engineering/data_engineering.qmd"
    },
    "tbl-data-debt-metrics": {
      "original_caption": "**Data Debt Metrics**: Quantitative thresholds for detecting data debt accumulation. Warning thresholds indicate debt requiring attention in upcoming planning cycles; critical thresholds indicate debt requiring immediate remediation to prevent system degradation.",
      "current_caption": "**Data Debt Metrics**: Quantitative thresholds for detecting data debt accumulation. Warning thresholds indicate debt requiring attention in upcoming planning cycles; critical thresholds indicate debt requiring immediate remediation to prevent system degradation.",
      "new_caption": "",
      "type": "table",
      "source_file": "contents/vol1/data_engineering/data_engineering.qmd"
    },
    "tbl-model-vs-data-centric": {
      "original_caption": "**Model-Centric vs. Data-Centric Development**: The shift in ML development philosophy. Production experience shows that data improvements often outperform algorithmic innovation: a dataset with 10,000 consistently labeled examples often outperforms 100,000 examples with noisy labels.",
      "current_caption": "**Model-Centric vs. Data-Centric Development**: The shift in ML development philosophy. Production experience shows that data improvements often outperform algorithmic innovation: a dataset with 10,000 consistently labeled examples often outperforms 100,000 examples with noisy labels.",
      "new_caption": "",
      "type": "table",
      "source_file": "contents/vol1/data_engineering/data_engineering.qmd"
    },
    "tbl-storage": {
      "original_caption": "**Storage System Characteristics**: Different storage systems suit distinct stages of machine learning workflows based on data structure and purpose; databases manage transactional data, data warehouses support analytical reporting, and data lakes accommodate diverse, raw data for future processing. Understanding these characteristics enables efficient data management and supports the scalability of machine learning applications.",
      "current_caption": "**Storage System Characteristics**: Different storage systems suit distinct stages of machine learning workflows based on data structure and purpose; databases manage transactional data, data warehouses support analytical reporting, and data lakes accommodate diverse, raw data for future processing. Understanding these characteristics enables efficient data management and supports the scalability of machine learning applications.",
      "new_caption": "",
      "type": "table",
      "source_file": "contents/vol1/data_engineering/data_engineering.qmd"
    },
    "tbl-ml-latencies": {
      "original_caption": "**Latency Numbers Every ML Systems Engineer Should Know**: Understanding the quantitative disparities in the storage hierarchy is essential for diagnosing bottlenecks. If a CPU cycle were 1 second, fetching data from local SSD would be like waiting 2 days, while a cross-country network request would take 6 years. Source: Adapted from Jeff Dean's \"Numbers Every Programmer Should Know\".",
      "current_caption": "**Latency Numbers Every ML Systems Engineer Should Know**: Understanding the quantitative disparities in the storage hierarchy is essential for diagnosing bottlenecks. If a CPU cycle were 1 second, fetching data from local SSD would be like waiting 2 days, while a cross-country network request would take 6 years. Source: Adapted from Jeff Dean's \"Numbers Every Programmer Should Know\".",
      "new_caption": "",
      "type": "table",
      "source_file": "contents/vol1/data_engineering/data_engineering.qmd"
    },
    "tbl-storage-performance": {
      "original_caption": "**Storage Cost-Performance Trade-offs**: Different storage tiers provide distinct cost-performance characteristics that determine their suitability for specific ML workloads. Training data loading requires high-throughput sequential access—an I/O pattern that must align with the **Accelerator Memory Hierarchy** (@sec-ai-acceleration)—while online serving needs low-latency random reads, while archival storage prioritizes cost over access speed for compliance and historical data.",
      "current_caption": "**Storage Cost-Performance Trade-offs**: Different storage tiers provide distinct cost-performance characteristics that determine their suitability for specific ML workloads. Training data loading requires high-throughput sequential access—an I/O pattern that must align with the **Accelerator Memory Hierarchy** (@sec-ai-acceleration)—while online serving needs low-latency random reads, while archival storage prioritizes cost over access speed for compliance and historical data.",
      "new_caption": "",
      "type": "table",
      "source_file": "contents/vol1/data_engineering/data_engineering.qmd"
    },
    "tbl-kws-design-space": {
      "original_caption": "**KWS Data Engineering Design Space**: Each design choice creates quantifiable trade-offs across the four pillars. Higher sampling rates improve quality but double storage and processing (scalability impact). More training data improves accuracy but multiplies labeling costs (governance/cost impact). Local inference eliminates latency but requires aggressive quantization (quality/reliability trade-off). This design space analysis guides systematic optimization rather than intuition-based decisions.",
      "current_caption": "**KWS Data Engineering Design Space**: Each design choice creates quantifiable trade-offs across the four pillars. Higher sampling rates improve quality but double storage and processing (scalability impact). More training data improves accuracy but multiplies labeling costs (governance/cost impact). Local inference eliminates latency but requires aggressive quantization (quality/reliability trade-off). This design space analysis guides systematic optimization rather than intuition-based decisions.",
      "new_caption": "",
      "type": "table",
      "source_file": "contents/vol1/data_engineering/data_engineering.qmd"
    },
    "tbl-anonymization-comparison": {
      "original_caption": "**Anonymization Techniques Comparison**: Privacy-utility trade-offs across anonymization methods. Masking preserves utility for display but offers minimal protection; differential privacy provides mathematical guarantees but reduces data accuracy. Practitioners must select techniques based on their specific regulatory requirements, data sensitivity, and analytical needs.",
      "current_caption": "**Anonymization Techniques Comparison**: Privacy-utility trade-offs across anonymization methods. Masking preserves utility for display but offers minimal protection; differential privacy provides mathematical guarantees but reduces data accuracy. Practitioners must select techniques based on their specific regulatory requirements, data sensitivity, and analytical needs.",
      "new_caption": "",
      "type": "table",
      "source_file": "contents/vol1/data_engineering/data_engineering.qmd"
    },
    "tbl-four-pillars-matrix": {
      "original_caption": "**Four Pillars Applied Across Data Pipeline Stages**: Quality, Reliability, Scalability, and Governance principles manifest differently at each major stage of the data engineering pipeline. Specific techniques and practices implement each pillar at every stage, providing a comprehensive framework for systematic decision-making and troubleshooting.",
      "current_caption": "**Four Pillars Applied Across Data Pipeline Stages**: Quality, Reliability, Scalability, and Governance principles manifest differently at each major stage of the data engineering pipeline. Specific techniques and practices implement each pillar at every stage, providing a comprehensive framework for systematic decision-making and troubleshooting.",
      "new_caption": "",
      "type": "table",
      "source_file": "contents/vol1/data_engineering/data_engineering.qmd"
    },
    "tbl-forward-pass-comparison": {
      "original_caption": "**Training vs. Inference Forward Pass**: While both phases execute the same mathematical operations layer-by-layer, they differ fundamentally in memory management and computational organization. Training must preserve intermediate values for gradient computation; inference can discard them immediately, enabling significant memory optimization.",
      "current_caption": "**Training vs. Inference Forward Pass**: While both phases execute the same mathematical operations layer-by-layer, they differ fundamentally in memory management and computational organization. Training must preserve intermediate values for gradient computation; inference can discard them immediately, enabling significant memory optimization.",
      "new_caption": "",
      "type": "table",
      "source_file": "contents/vol1/dl_primer/dl_primer.qmd"
    },
    "tbl-usps-numbers": {
      "original_caption": "**USPS LeNet Deployment Results**: Performance comparison of LeNet neural network against human operators for ZIP code recognition (1989), including accuracy, throughput, and training specifications.",
      "current_caption": "**USPS LeNet Deployment Results**: Performance comparison of LeNet neural network against human operators for ZIP code recognition (1989), including accuracy, throughput, and training specifications.",
      "new_caption": "",
      "type": "table",
      "source_file": "contents/vol1/dl_primer/dl_primer.qmd"
    },
    "tbl-energy-costs": {
      "original_caption": "**Energy Consumption Across Neural Network Scales**: CO₂ estimates assume US average grid (~0.5 kg CO₂/kWh). Inference energy assumes single request; training energy covers full model development. *BERT-base figure includes hyperparameter tuning; base training alone is lower.",
      "current_caption": "**Energy Consumption Across Neural Network Scales**: CO₂ estimates assume US average grid (~0.5 kg CO₂/kWh). Inference energy assumes single request; training energy covers full model development. *BERT-base figure includes hyperparameter tuning; base training alone is lower.",
      "new_caption": "",
      "type": "table",
      "source_file": "contents/vol1/dl_primer/dl_primer.qmd"
    },
    "tbl-then-vs-now": {
      "original_caption": "**Hardware Progress for Neural Network Computation**: Four decades of efficiency improvement for equivalent neural network computation, comparing the original 1990s USPS deployment infrastructure against modern 2025 hardware.",
      "current_caption": "**Hardware Progress for Neural Network Computation**: Four decades of efficiency improvement for equivalent neural network computation, comparing the original 1990s USPS deployment infrastructure against modern 2025 hardware.",
      "new_caption": "",
      "type": "table",
      "source_file": "contents/vol1/dl_primer/dl_primer.qmd"
    },
    "tbl-comp2sys": {
      "original_caption": "**Computational Demands**: Artificial neural network design directly translates into specific system requirements; for example, efficient activation functions necessitate fast nonlinear operation units and large-scale weight storage demands high-bandwidth memory access. Understanding this mapping guides hardware and system architecture choices for effective implementation of artificial intelligence.",
      "current_caption": "**Computational Demands**: Artificial neural network design directly translates into specific system requirements; for example, efficient activation functions necessitate fast nonlinear operation units and large-scale weight storage demands high-bandwidth memory access. Understanding this mapping guides hardware and system architecture choices for effective implementation of artificial intelligence.",
      "new_caption": "",
      "type": "table",
      "source_file": "contents/vol1/dl_primer/dl_primer.qmd"
    },
    "tbl-evolution": {
      "original_caption": "**System Resource Evolution**: Programming paradigms shift system demands from sequential computation to structured parallelism with feature engineering, and finally to massive matrix operations and complex memory hierarchies in deep learning. Deep learning fundamentally alters system requirements compared to traditional programming and classical machine learning, impacting both computation and memory access patterns.",
      "current_caption": "**System Resource Evolution**: Programming paradigms shift system demands from sequential computation to structured parallelism with feature engineering, and finally to massive matrix operations and complex memory hierarchies in deep learning. Deep learning fundamentally alters system requirements compared to traditional programming and classical machine learning, impacting both computation and memory access patterns.",
      "new_caption": "",
      "type": "table",
      "source_file": "contents/vol1/dl_primer/dl_primer.qmd"
    },
    "tbl-comp_mapping": {
      "original_caption": "**Conceptual to Computational Mapping**: Deep learning systems abstract pattern recognition into a series of matrix operations and nonlinear transformations. This mapping enables efficient digital implementation while preserving the system's ability to learn complex hierarchical features.",
      "current_caption": "**Conceptual to Computational Mapping**: Deep learning systems abstract pattern recognition into a series of matrix operations and nonlinear transformations. This mapping enables efficient digital implementation while preserving the system's ability to learn complex hierarchical features.",
      "new_caption": "",
      "type": "table",
      "source_file": "contents/vol1/dl_primer/dl_primer.qmd"
    },
    "tbl-neuron_structure": {
      "original_caption": "**Neuron Structure and Function**: The artificial neuron functions as a processing pipeline where inputs are modulated by learnable weights, aggregated into a single signal, and transformed through a nonlinear activation function. This abstraction enables the construction of massive networks capable of sophisticated pattern recognition.",
      "current_caption": "**Neuron Structure and Function**: The artificial neuron functions as a processing pipeline where inputs are modulated by learnable weights, aggregated into a single signal, and transformed through a nonlinear activation function. This abstraction enables the construction of massive networks capable of sophisticated pattern recognition.",
      "new_caption": "",
      "type": "table",
      "source_file": "contents/vol1/dl_primer/dl_primer.qmd"
    },
    "tbl-lighthouse-comparison": {
      "original_caption": "**Lighthouse Model Comparison**: Quantitative characteristics and pedagogical roles of the five canonical workloads. Parameters and memory represent model weights at FP32 precision. FLOPs measured per single inference. The bottleneck column indicates the primary system constraint each model reveals: compute-bound models like ResNet stress arithmetic throughput, while bandwidth-bound models like GPT-2 stress memory transfer rates.",
      "current_caption": "**Lighthouse Model Comparison**: Quantitative characteristics and pedagogical roles of the five canonical workloads. Parameters and memory represent model weights at FP32 precision. FLOPs measured per single inference. The bottleneck column indicates the primary system constraint each model reveals: compute-bound models like ResNet stress arithmetic throughput, while bandwidth-bound models like GPT-2 stress memory transfer rates.",
      "new_caption": "",
      "type": "table",
      "source_file": "contents/vol1/dnn_architectures/dnn_architectures.qmd"
    },
    "tbl-sys-design-implications": {
      "original_caption": "**Primitive-Hardware Co-Design**: Efficient machine learning systems require tight integration between algorithmic primitives and underlying hardware. Common primitives map to specific hardware accelerations and software optimizations, each presenting distinct implementation challenges. Specialized hardware such as tensor cores addresses computational demands of matrix multiplication and sliding windows, while software techniques like batching and dynamic graph execution further enhance performance.",
      "current_caption": "**Primitive-Hardware Co-Design**: Efficient machine learning systems require tight integration between algorithmic primitives and underlying hardware. Common primitives map to specific hardware accelerations and software optimizations, each presenting distinct implementation challenges. Specialized hardware such as tensor cores addresses computational demands of matrix multiplication and sliding windows, while software techniques like batching and dynamic graph execution further enhance performance.",
      "new_caption": "",
      "type": "table",
      "source_file": "contents/vol1/dnn_architectures/dnn_architectures.qmd"
    },
    "tbl-computational-complexity": {
      "original_caption": "**Computational Complexity Comparison**: Scaling behaviors and resource requirements for major neural network architectures. Variables: $d$ = dimension, $h$ = hidden size, $k$ = kernel size, $c$&nbsp;=&nbsp;channels, $H,W$ = spatial dimensions, $T$ = time steps, $n$ = sequence length, $b$ = batch size. The bottleneck column identifies the primary performance-limiting factor in typical deployments.",
      "current_caption": "**Computational Complexity Comparison**: Scaling behaviors and resource requirements for major neural network architectures. Variables: $d$ = dimension, $h$ = hidden size, $k$ = kernel size, $c$&nbsp;=&nbsp;channels, $H,W$ = spatial dimensions, $T$ = time steps, $n$ = sequence length, $b$ = batch size. The bottleneck column identifies the primary performance-limiting factor in typical deployments.",
      "new_caption": "",
      "type": "table",
      "source_file": "contents/vol1/dnn_architectures/dnn_architectures.qmd"
    },
    "tbl-arch-complexity": {
      "original_caption": "**Memory Access Complexity**: Different neural network architectures exhibit varying memory access patterns and storage requirements, impacting system performance and scalability. Parameter storage scales with input dependency and model size, while activation storage represents a significant runtime cost, particularly for sequence-based models where RNNs offer a parameter efficiency advantage when sequence length exceeds hidden state size ($n > h$).",
      "current_caption": "**Memory Access Complexity**: Different neural network architectures exhibit varying memory access patterns and storage requirements, impacting system performance and scalability. Parameter storage scales with input dependency and model size, while activation storage represents a significant runtime cost, particularly for sequence-based models where RNNs offer a parameter efficiency advantage when sequence length exceeds hidden state size ($n > h$).",
      "new_caption": "",
      "type": "table",
      "source_file": "contents/vol1/dnn_architectures/dnn_architectures.qmd"
    },
    "tbl-primitive-comparison": {
      "original_caption": "**Primitive Utilization Across Architectures**: Computational primitives vary across architectures, with Transformers uniquely combining matrix multiplication with attention mechanisms. Memory access patterns range from sequential (MLPs) to strided (CNNs) to random (attention). Data movement patterns, including broadcast, scatter, gather, and reduction, define information flow and often dominate performance.",
      "current_caption": "**Primitive Utilization Across Architectures**: Computational primitives vary across architectures, with Transformers uniquely combining matrix multiplication with attention mechanisms. Memory access patterns range from sequential (MLPs) to strided (CNNs) to random (attention). Data movement patterns, including broadcast, scatter, gather, and reduction, define information flow and often dominate performance.",
      "new_caption": "",
      "type": "table",
      "source_file": "contents/vol1/dnn_architectures/dnn_architectures.qmd"
    },
    "tbl-dl-evolution": {
      "original_caption": "**Deep Learning Evolution**: Neural network architectures have progressed from simple, fully connected layers to complex models leveraging specialized hardware and addressing sequential data dependencies. Architectural eras map to key computational primitives and corresponding system-level optimizations, revealing a historical trend toward increased parallelism and memory bandwidth requirements.",
      "current_caption": "**Deep Learning Evolution**: Neural network architectures have progressed from simple, fully connected layers to complex models leveraging specialized hardware and addressing sequential data dependencies. Architectural eras map to key computational primitives and corresponding system-level optimizations, revealing a historical trend toward increased parallelism and memory bandwidth requirements.",
      "new_caption": "",
      "type": "table",
      "source_file": "contents/vol1/dnn_architectures/dnn_architectures.qmd"
    },
    "tbl-framework-execution-models": {
      "original_caption": "**Execution Model Trade-Offs.** Comparison of static graph, eager execution, and hybrid JIT compilation across six dimensions including performance, debugging, and deployment flexibility.",
      "current_caption": "**Execution Model Trade-Offs.** Comparison of static graph, eager execution, and hybrid JIT compilation across six dimensions including performance, debugging, and deployment flexibility.",
      "new_caption": "",
      "type": "table",
      "source_file": "contents/vol1/frameworks/frameworks.qmd"
    },
    "tbl-framework-archetype-strategy": {
      "original_caption": "**Framework Execution Strategy by Workload.** Recommended execution strategy for each workload archetype, aligned to the dominant Iron Law term. Compute-bound workloads benefit most from compilation, while irregular access patterns favor eager execution.",
      "current_caption": "**Framework Execution Strategy by Workload.** Recommended execution strategy for each workload archetype, aligned to the dominant Iron Law term. Compute-bound workloads benefit most from compilation, while irregular access patterns favor eager execution.",
      "new_caption": "",
      "type": "table",
      "source_file": "contents/vol1/frameworks/frameworks.qmd"
    },
    "tbl-mlfm-graphs": {
      "original_caption": "**Graph Computation Modes.** Static graphs define the entire computation upfront for optimization, while dynamic graphs construct the computation on the fly for flexibility with variable-length inputs and control flow. The choice affects both execution efficiency and the ease of model development and debugging.",
      "current_caption": "**Graph Computation Modes.** Static graphs define the entire computation upfront for optimization, while dynamic graphs construct the computation on the fly for flexibility with variable-length inputs and control flow. The choice affects both execution efficiency and the ease of model development and debugging.",
      "new_caption": "",
      "type": "table",
      "source_file": "contents/vol1/frameworks/frameworks.qmd"
    },
    "tbl-tf-sw-comparison": {
      "original_caption": "**TensorFlow Variant Capability Comparison.** Capabilities of TensorFlow, TensorFlow Lite, and TensorFlow Lite Micro regarding operating system dependence, memory management, and hardware acceleration. Progressive constraint across variants enables selection by deployment context, from full-scale servers to resource-constrained edge devices.",
      "current_caption": "**TensorFlow Variant Capability Comparison.** Capabilities of TensorFlow, TensorFlow Lite, and TensorFlow Lite Micro regarding operating system dependence, memory management, and hardware acceleration. Progressive constraint across variants enables selection by deployment context, from full-scale servers to resource-constrained edge devices.",
      "new_caption": "",
      "type": "table",
      "source_file": "contents/vol1/frameworks/frameworks.qmd"
    },
    "tbl-tf-comparison": {
      "original_caption": "**TensorFlow Variant Software Comparison.** Design trade-offs across TensorFlow, TensorFlow Lite, and TensorFlow Lite Micro, balancing model expressiveness, binary size, and resource constraints. Supported operations decrease from approximately 1,400 in full TensorFlow to approximately 50 in TensorFlow Lite Micro, reflecting a shift from training capability to efficient edge inference. Native quantization tooling enables further optimization for constrained environments.",
      "current_caption": "**TensorFlow Variant Software Comparison.** Design trade-offs across TensorFlow, TensorFlow Lite, and TensorFlow Lite Micro, balancing model expressiveness, binary size, and resource constraints. Supported operations decrease from approximately 1,400 in full TensorFlow to approximately 50 in TensorFlow Lite Micro, reflecting a shift from training capability to efficient edge inference. Native quantization tooling enables further optimization for constrained environments.",
      "new_caption": "",
      "type": "table",
      "source_file": "contents/vol1/frameworks/frameworks.qmd"
    },
    "tbl-tf-hw-comparison": {
      "original_caption": "**TensorFlow Hardware Optimization.** Resource requirements (binary size and memory footprint) decrease across TensorFlow variants as they target increasingly constrained hardware, from servers to microcontrollers. Optimized architectures shift from general-purpose CPUs and GPUs to ARM Cortex-M processors and digital signal processors for resource-limited environments.",
      "current_caption": "**TensorFlow Hardware Optimization.** Resource requirements (binary size and memory footprint) decrease across TensorFlow variants as they target increasingly constrained hardware, from servers to microcontrollers. Optimized architectures shift from general-purpose CPUs and GPUs to ARM Cortex-M processors and digital signal processors for resource-limited environments.",
      "new_caption": "",
      "type": "table",
      "source_file": "contents/vol1/frameworks/frameworks.qmd"
    },
    "tbl-device-transfer-overhead": {
      "original_caption": "**Device Transfer Overhead.** Transfer time for a 4 MB tensor across different interconnects. PCIe bandwidth shown is unidirectional (typical for GPU transfers), with full-duplex operation providing 2x total bandwidth. NVLink bandwidth is bidirectional (300 GB/s per direction). Transfer times dominate for small operations, making device placement critical for performance.",
      "current_caption": "**Device Transfer Overhead.** Transfer time for a 4 MB tensor across different interconnects. PCIe bandwidth shown is unidirectional (typical for GPU transfers), with full-duplex operation providing 2x total bandwidth. NVLink bandwidth is bidirectional (300 GB/s per direction). Transfer times dominate for small operations, making device placement critical for performance.",
      "new_caption": "",
      "type": "table",
      "source_file": "contents/vol1/frameworks/frameworks.qmd"
    },
    "tbl-training-benchmark": {
      "original_caption": "**Training and Inference Throughput.** Representative throughput comparison across execution modes for common model architectures on NVIDIA A100 GPU with batch size 32. torch.compile typically provides 1.4 to 1.5$\\times$ speedup over eager mode, while TensorRT provides 2 to 3$\\times$ speedup but requires longer compilation and is inference only. Compile times vary based on model complexity and optimization level.",
      "current_caption": "**Training and Inference Throughput.** Representative throughput comparison across execution modes for common model architectures on NVIDIA A100 GPU with batch size 32. torch.compile typically provides 1.4 to 1.5$\\times$ speedup over eager mode, while TensorRT provides 2 to 3$\\times$ speedup but requires longer compilation and is inference only. Compile times vary based on model complexity and optimization level.",
      "new_caption": "",
      "type": "table",
      "source_file": "contents/vol1/frameworks/frameworks.qmd"
    },
    "tbl-nsight-metrics": {
      "original_caption": "**Nsight Compute Metrics.** Key metrics for ML kernel optimization. Low values indicate specific optimization opportunities. Nsight Systems identifies which kernels dominate runtime, and Nsight Compute reveals why those kernels underperform.",
      "current_caption": "**Nsight Compute Metrics.** Key metrics for ML kernel optimization. Low values indicate specific optimization opportunities. Nsight Systems identifies which kernels dominate runtime, and Nsight Compute reveals why those kernels underperform.",
      "new_caption": "",
      "type": "table",
      "source_file": "contents/vol1/frameworks/frameworks.qmd"
    },
    "tbl-framework-efficiency-matrix": {
      "original_caption": "**Framework Efficiency Comparison.** Quantitative comparison of major ML frameworks across efficiency dimensions using ResNet-50 inference on representative hardware (NVIDIA A100 GPU for server frameworks, ARM Cortex-A78 for mobile). Metrics reflect production workloads with accuracy maintained within 1% of baseline. Hardware utilization represents percentage of theoretical peak performance on typical operations.",
      "current_caption": "**Framework Efficiency Comparison.** Quantitative comparison of major ML frameworks across efficiency dimensions using ResNet-50 inference on representative hardware (NVIDIA A100 GPU for server frameworks, ARM Cortex-A78 for mobile). Metrics reflect production workloads with accuracy maintained within 1% of baseline. Hardware utilization represents percentage of theoretical peak performance on typical operations.",
      "new_caption": "",
      "type": "table",
      "source_file": "contents/vol1/frameworks/frameworks.qmd"
    },
    "tbl-mlfm-comparison": {
      "original_caption": "**Framework Characteristics.** Comparison of TensorFlow, PyTorch, and JAX across graph construction, data mutability, automatic differentiation, GPU utilization, and distributed scalability. GPU utilization varies by model architecture, batch size, and operation mix. JAX/XLA achieves higher utilization for TPU workloads through aggressive fusion, while PyTorch and TensorFlow perform similarly for most deep learning workloads. Students should profile their specific workloads rather than relying on framework-level generalizations.",
      "current_caption": "**Framework Characteristics.** Comparison of TensorFlow, PyTorch, and JAX across graph construction, data mutability, automatic differentiation, GPU utilization, and distributed scalability. GPU utilization varies by model architecture, batch size, and operation mix. JAX/XLA achieves higher utilization for TPU workloads through aggressive fusion, while PyTorch and TensorFlow perform similarly for most deep learning workloads. Students should profile their specific workloads rather than relying on framework-level generalizations.",
      "new_caption": "",
      "type": "table",
      "source_file": "contents/vol1/frameworks/frameworks.qmd"
    },
    "tbl-deployment-frameworks": {
      "original_caption": "**Framework Selection by Deployment Target.** Recommended frameworks, optimization techniques, and key constraints for each deployment tier, from cloud servers to microcontrollers.",
      "current_caption": "**Framework Selection by Deployment Target.** Recommended frameworks, optimization techniques, and key constraints for each deployment tier, from cloud servers to microcontrollers.",
      "new_caption": "",
      "type": "table",
      "source_file": "contents/vol1/frameworks/frameworks.qmd"
    },
    "tbl-prefetching": {
      "original_caption": "**Pipeline Optimization.** Prefetching and overlapping maximize hardware utilization and reduce training time by enabling parallel data loading and computation, overcoming bottlenecks inherent in sequential pipelines.",
      "current_caption": "**Pipeline Optimization.** Prefetching and overlapping maximize hardware utilization and reduce training time by enabling parallel data loading and computation, overcoming bottlenecks inherent in sequential pipelines.",
      "new_caption": "",
      "type": "table",
      "source_file": "contents/vol1/training/training.qmd"
    },
    "tbl-checkpoint-tradeoffs": {
      "original_caption": "**Checkpointing Memory-Compute Tradeoffs.** Different checkpoint strategies trade memory savings against recomputation overhead. The optimal number of checkpoints balances these factors.",
      "current_caption": "**Checkpointing Memory-Compute Tradeoffs.** Different checkpoint strategies trade memory savings against recomputation overhead. The optimal number of checkpoints balances these factors.",
      "new_caption": "",
      "type": "table",
      "source_file": "contents/vol1/training/training.qmd"
    },
    "tbl-optimization-roadmap": {
      "original_caption": "**Optimization Technique Roadmap.** Each primary bottleneck category has targeted solutions that address specific performance constraints, matching techniques to profiling results for systematic optimization.",
      "current_caption": "**Optimization Technique Roadmap.** Each primary bottleneck category has targeted solutions that address specific performance constraints, matching techniques to profiling results for systematic optimization.",
      "new_caption": "",
      "type": "table",
      "source_file": "contents/vol1/training/training.qmd"
    },
    "tbl-computing-eras": {
      "original_caption": "**Computing Era Characteristics.** Each computing era optimized for different workload patterns. AI hypercomputing uniquely combines HPC's parallel numerical computation with warehouse-scale's distributed processing, while adding specialized support for gradient-based optimization central to neural network training.",
      "current_caption": "**Computing Era Characteristics.** Each computing era optimized for different workload patterns. AI hypercomputing uniquely combines HPC's parallel numerical computation with warehouse-scale's distributed processing, while adding specialized support for gradient-based optimization central to neural network training.",
      "new_caption": "",
      "type": "table",
      "source_file": "contents/vol1/training/training.qmd"
    },
    "tbl-scaling-decision": {
      "original_caption": "**Scaling Decision Guidelines.** Model size, dataset scale, and available hardware determine when distributed training complexity is justified. Single-machine optimization provides better cost-efficiency below these thresholds.",
      "current_caption": "**Scaling Decision Guidelines.** Model size, dataset scale, and available hardware determine when distributed training complexity is justified. Single-machine optimization provides better cost-efficiency below these thresholds.",
      "new_caption": "",
      "type": "table",
      "source_file": "contents/vol1/training/training.qmd"
    },
    "tbl-precision-comparison": {
      "original_caption": "**Precision Format Comparison.** The choice between FP16 and BF16 depends on whether dynamic range (BF16's strength) or precision (FP16's advantage) matters more for the specific workload. Minimum normal values shown are the practical thresholds for training, as subnormal values may flush to zero on many GPUs.",
      "current_caption": "**Precision Format Comparison.** The choice between FP16 and BF16 depends on whether dynamic range (BF16's strength) or precision (FP16's advantage) matters more for the specific workload. Minimum normal values shown are the practical thresholds for training, as subnormal values may flush to zero on many GPUs.",
      "new_caption": "",
      "type": "table",
      "source_file": "contents/vol1/training/training.qmd"
    },
    "tbl-optimizer-properties": {
      "original_caption": "**Optimizer Memory Footprint.** Different optimization algorithms impose varying memory costs due to the storage of intermediate values like gradients, velocities, and squared gradients. Understanding these trade-offs is important for resource-constrained deployments and large-scale model training.",
      "current_caption": "**Optimizer Memory Footprint.** Different optimization algorithms impose varying memory costs due to the storage of intermediate values like gradients, velocities, and squared gradients. Understanding these trade-offs is important for resource-constrained deployments and large-scale model training.",
      "new_caption": "",
      "type": "table",
      "source_file": "contents/vol1/training/training.qmd"
    },
    "tbl-iron-law-mapping": {
      "original_caption": "**Iron Law Optimization Mapping.** Optimization techniques mapped to Iron Law terms. Understanding which term a technique affects guides optimization strategy selection.",
      "current_caption": "**Iron Law Optimization Mapping.** Optimization techniques mapped to Iron Law terms. Understanding which term a technique affects guides optimization strategy selection.",
      "new_caption": "",
      "type": "table",
      "source_file": "contents/vol1/training/training.qmd"
    },
    "tbl-gpt2-summary": {
      "original_caption": "**GPT-2 Training Optimization Summary.** Applying mixed-precision training and gradient checkpointing reduces memory from 89 GB to 32 GB, training time by 40%, energy consumption by 58%, and carbon footprint proportionally.",
      "current_caption": "**GPT-2 Training Optimization Summary.** Applying mixed-precision training and gradient checkpointing reduces memory from 89 GB to 32 GB, training time by 40%, energy consumption by 58%, and carbon footprint proportionally.",
      "new_caption": "",
      "type": "table",
      "source_file": "contents/vol1/training/training.qmd"
    },
    "tbl-compare-activations": {
      "original_caption": "**Activation Function Systems Comparison.** While activation functions contribute only a fraction of total training time, their implementation characteristics (computational complexity, hardware utilization, and memory patterns) significantly impact the efficiency of modern learning pipelines.",
      "current_caption": "**Activation Function Systems Comparison.** While activation functions contribute only a fraction of total training time, their implementation characteristics (computational complexity, hardware utilization, and memory patterns) significantly impact the efficiency of modern learning pipelines.",
      "new_caption": "",
      "type": "table",
      "source_file": "contents/vol1/training/training.qmd"
    },
    "tbl-optimization": {
      "original_caption": "**Optimization Technique Roadmap.** Each primary bottleneck category has targeted solutions that address specific performance constraints, matching techniques to profiling results for systematic optimization.",
      "current_caption": "**Optimization Technique Roadmap.** Each primary bottleneck category has targeted solutions that address specific performance constraints, matching techniques to profiling results for systematic optimization.",
      "new_caption": "",
      "type": "table",
      "source_file": "contents/vol1/training/training.qmd"
    },
    "tbl-dam-training-bottlenecks": {
      "original_caption": "**DAM Taxonomy Applied to Training Bottlenecks.** Each component of the DAM Taxonomy (Data, Algorithm, Machine) maps to a distinct training bottleneck with characteristic symptoms. Profiling reveals which component is the limiting factor, guiding practitioners to the appropriate optimization technique.",
      "current_caption": "**DAM Taxonomy Applied to Training Bottlenecks.** Each component of the DAM Taxonomy (Data, Algorithm, Machine) maps to a distinct training bottleneck with characteristic symptoms. Profiling reveals which component is the limiting factor, guiding practitioners to the appropriate optimization technique.",
      "new_caption": "",
      "type": "table",
      "source_file": "contents/vol1/training/training.qmd"
    },
    "tbl-training-arithmetic-intensity": {
      "original_caption": "**Training Operation Classifications.** Different operations in the training pipeline have vastly different arithmetic intensities, determining whether they are limited by compute throughput or memory bandwidth.",
      "current_caption": "**Training Operation Classifications.** Different operations in the training pipeline have vastly different arithmetic intensities, determining whether they are limited by compute throughput or memory bandwidth.",
      "new_caption": "",
      "type": "table",
      "source_file": "contents/vol1/training/training.qmd"
    },
    "tbl-hw-precision-strategy": {
      "original_caption": "**Precision Strategy by GPU Architecture.** Each generation introduces wider precision support, reducing the engineering burden of loss scaling while increasing throughput.",
      "current_caption": "**Precision Strategy by GPU Architecture.** Each generation introduces wider precision support, reducing the engineering burden of loss scaling while increasing throughput.",
      "new_caption": "",
      "type": "table",
      "source_file": "contents/vol1/training/training.qmd"
    },
    "tbl-self-supervised-tasks": {
      "original_caption": "**Self-Supervised Pretext Tasks by Modality.** Each task extracts supervision from data structure rather than human labels, enabling pre-training on unlimited unlabeled corpora.",
      "current_caption": "**Self-Supervised Pretext Tasks by Modality.** Each task extracts supervision from data structure rather than human labels, enabling pre-training on unlimited unlabeled corpora.",
      "new_caption": "",
      "type": "table",
      "source_file": "contents/vol1/data_selection/data_selection.qmd"
    },
    "tbl-ssl-tradeoffs": {
      "original_caption": "**Self-Supervised Learning Method Trade-Offs.** Contrastive methods excel at downstream data efficiency but require massive batches; masked modeling balances cost and efficiency; generative methods scale best with unlimited data.",
      "current_caption": "**Self-Supervised Learning Method Trade-Offs.** Contrastive methods excel at downstream data efficiency but require massive batches; masked modeling balances cost and efficiency; generative methods scale best with unlimited data.",
      "new_caption": "",
      "type": "table",
      "source_file": "contents/vol1/data_selection/data_selection.qmd"
    },
    "tbl-data-selection": {
      "original_caption": "**Three-Stage Data Selection Pipeline.** Each stage increases ICR by different mechanisms: pruning removes low-value samples, dynamic selection focuses compute on high-value samples, and synthesis creates new high-value samples.",
      "current_caption": "**Three-Stage Data Selection Pipeline.** Each stage increases ICR by different mechanisms: pruning removes low-value samples, dynamic selection focuses compute on high-value samples, and synthesis creates new high-value samples.",
      "new_caption": "",
      "type": "table",
      "source_file": "contents/vol1/data_selection/data_selection.qmd"
    },
    "tbl-technique-prerequisites": {
      "original_caption": "**Technique Prerequisites.** Required resources and capabilities for each data selection technique. Verify these requirements before committing to implementation.",
      "current_caption": "**Technique Prerequisites.** Required resources and capabilities for each data selection technique. Verify these requirements before committing to implementation.",
      "new_caption": "",
      "type": "table",
      "source_file": "contents/vol1/data_selection/data_selection.qmd"
    },
    "tbl-io-performance": {
      "original_caption": "::",
      "current_caption": "::",
      "new_caption": "",
      "type": "table",
      "source_file": "contents/vol1/data_selection/data_selection.qmd"
    },
    "tbl-coreset-comparison": {
      "original_caption": "**Coreset Selection Algorithm Comparison.** N = dataset size, K = coreset size. Gradient-based methods generally outperform geometry-based methods but require proxy model training.",
      "current_caption": "**Coreset Selection Algorithm Comparison.** N = dataset size, K = coreset size. Gradient-based methods generally outperform geometry-based methods but require proxy model training.",
      "new_caption": "",
      "type": "table",
      "source_file": "contents/vol1/data_selection/data_selection.qmd"
    },
    "tbl-scaling-asymmetry": {
      "original_caption": "**Scaling Asymmetry in ML Resources.** Compute grows exponentially while high-quality data grows linearly or sub-linearly, creating an increasing compute-to-data imbalance that makes data selection essential.",
      "current_caption": "**Scaling Asymmetry in ML Resources.** Compute grows exponentially while high-quality data grows linearly or sub-linearly, creating an increasing compute-to-data imbalance that makes data selection essential.",
      "new_caption": "",
      "type": "table",
      "source_file": "contents/vol1/data_selection/data_selection.qmd"
    },
    "tbl-cost-amortization": {
      "original_caption": "**Cost Amortization in Foundation Model Fine-Tuning.** Pre-training costs are paid once; fine-tuning costs scale with task count but remain small per task.",
      "current_caption": "**Cost Amortization in Foundation Model Fine-Tuning.** Pre-training costs are paid once; fine-tuning costs scale with task count but remain small per task.",
      "new_caption": "",
      "type": "table",
      "source_file": "contents/vol1/data_selection/data_selection.qmd"
    },
    "tbl-difficulty-scoring": {
      "original_caption": "**Difficulty Scoring Strategies for Curriculum Learning.** Loss-based and confidence-based methods require additional model inference; domain heuristics are free but require expertise; self-paced methods adapt dynamically during training.",
      "current_caption": "**Difficulty Scoring Strategies for Curriculum Learning.** Loss-based and confidence-based methods require additional model inference; domain heuristics are free but require expertise; self-paced methods adapt dynamically during training.",
      "new_caption": "",
      "type": "table",
      "source_file": "contents/vol1/data_selection/data_selection.qmd"
    },
    "tbl-curriculum-benchmarks": {
      "original_caption": "**Curriculum Learning Convergence Speedups.** Target accuracy is 95% of final baseline performance. Gains are larger on redundant datasets (CIFAR-10) and noisy datasets (MentorNet removes approximately 40% noise). ImageNet shows smaller gains because the dataset is less redundant.",
      "current_caption": "**Curriculum Learning Convergence Speedups.** Target accuracy is 95% of final baseline performance. Gains are larger on redundant datasets (CIFAR-10) and noisy datasets (MentorNet removes approximately 40% noise). ImageNet shows smaller gains because the dataset is less redundant.",
      "new_caption": "",
      "type": "table",
      "source_file": "contents/vol1/data_selection/data_selection.qmd"
    },
    "tbl-synthetic-mix": {
      "original_caption": "**Synthetic-to-Real Data Mixing Ratios.** Pure synthetic data suffers from distribution shift; pure real data is expensive. The optimal ratio varies by domain but typically falls in the 50–80% synthetic range when simulation fidelity is high.",
      "current_caption": "**Synthetic-to-Real Data Mixing Ratios.** Pure synthetic data suffers from distribution shift; pure real data is expensive. The optimal ratio varies by domain but typically falls in the 50–80% synthetic range when simulation fidelity is high.",
      "new_caption": "",
      "type": "table",
      "source_file": "contents/vol1/data_selection/data_selection.qmd"
    },
    "tbl-technique-selection": {
      "original_caption": "**Technique Selection Guide by Primary Constraint.** Recommended data selection techniques mapped to the dominant resource constraint in the ML pipeline.",
      "current_caption": "**Technique Selection Guide by Primary Constraint.** Recommended data selection techniques mapped to the dominant resource constraint in the ML pipeline.",
      "new_caption": "",
      "type": "table",
      "source_file": "contents/vol1/data_selection/data_selection.qmd"
    },
    "tbl-ml-vs-systems-framing": {
      "original_caption": "**ML vs. Systems Perspectives on Data Selection.** The ML framing optimizes sample complexity; the systems framing optimizes total resource cost across the pipeline.",
      "current_caption": "**ML vs. Systems Perspectives on Data Selection.** The ML framing optimizes sample complexity; the systems framing optimizes total resource cost across the pipeline.",
      "new_caption": "",
      "type": "table",
      "source_file": "contents/vol1/data_selection/data_selection.qmd"
    },
    "tbl-fixmatch-cifar10": {
      "original_caption": "**FixMatch Label Efficiency on CIFAR-10.** With 250 labels (0.5% of the dataset), FixMatch achieves within 1.2 points of full supervision, demonstrating 200$\\times$ label efficiency.",
      "current_caption": "**FixMatch Label Efficiency on CIFAR-10.** With 250 labels (0.5% of the dataset), FixMatch achieves within 1.2 points of full supervision, demonstrating 200$\\times$ label efficiency.",
      "new_caption": "",
      "type": "table",
      "source_file": "contents/vol1/data_selection/data_selection.qmd"
    },
    "tbl-hardware-efficient-design": {
      "original_caption": "**Hardware-Aware Design Principles**: Categorizing model design choices by their impact on computational cost, memory usage, and inference latency enables structured optimization for diverse hardware platforms and deployment scenarios. MobileNet exemplifies computation reduction through depthwise separable convolutions, while DenseNet and SqueezeNet demonstrate memory optimization strategies.",
      "current_caption": "**Hardware-Aware Design Principles**: Categorizing model design choices by their impact on computational cost, memory usage, and inference latency enables structured optimization for diverse hardware platforms and deployment scenarios. MobileNet exemplifies computation reduction through depthwise separable convolutions, while DenseNet and SqueezeNet demonstrate memory optimization strategies.",
      "new_caption": "",
      "type": "table",
      "source_file": "contents/vol1/optimizations/model_compression.qmd"
    },
    "tbl-numerics": {
      "original_caption": "**Numerical Precision Formats**: Comparison of precision formats by bit width, memory reduction, computational efficiency, accuracy retention, and typical use cases across deployment contexts.",
      "current_caption": "**Numerical Precision Formats**: Comparison of precision formats by bit width, memory reduction, computational efficiency, accuracy retention, and typical use cases across deployment contexts.",
      "new_caption": "",
      "type": "table",
      "source_file": "contents/vol1/optimizations/model_compression.qmd"
    },
    "tbl-pruning": {
      "original_caption": "**Pruning Strategies**: Unstructured, structured, and dynamic pruning each modify model weights differently, impacting both model size and computational efficiency. Unstructured pruning offers the greatest compression but requires specialized hardware, while dynamic pruning adapts to input data for a balance between accuracy and resource usage.",
      "current_caption": "**Pruning Strategies**: Unstructured, structured, and dynamic pruning each modify model weights differently, impacting both model size and computational efficiency. Unstructured pruning offers the greatest compression but requires specialized hardware, while dynamic pruning adapts to input data for a balance between accuracy and resource usage.",
      "new_caption": "",
      "type": "table",
      "source_file": "contents/vol1/optimizations/model_compression.qmd"
    },
    "tbl-model-vs-device": {
      "original_caption": "**The Deployment Gap**: Model memory requirements compared against typical device capacities. Even MobileNetV2 quantized to INT8 exceeds TinyML constraints by 7×, while the purpose-built DS-CNN keyword spotter fits comfortably. Numbers in parentheses show how many times the model exceeds device memory.",
      "current_caption": "**The Deployment Gap**: Model memory requirements compared against typical device capacities. Even MobileNetV2 quantized to INT8 exceeds TinyML constraints by 7×, while the purpose-built DS-CNN keyword spotter fits comfortably. Numbers in parentheses show how many times the model exceeds device memory.",
      "new_caption": "",
      "type": "table",
      "source_file": "contents/vol1/optimizations/model_compression.qmd"
    },
    "tbl-kd-pruning": {
      "original_caption": "**Model Compression Trade-Offs**: Knowledge distillation and pruning represent distinct approaches to reducing model size and improving efficiency, each with unique strengths and weaknesses regarding accuracy, computational cost, and implementation complexity. Distillation prioritizes preserving accuracy through knowledge transfer, while pruning directly reduces computational demands by eliminating redundant parameters, making their combined use a common strategy for optimal performance.",
      "current_caption": "**Model Compression Trade-Offs**: Knowledge distillation and pruning represent distinct approaches to reducing model size and improving efficiency, each with unique strengths and weaknesses regarding accuracy, computational cost, and implementation complexity. Distillation prioritizes preserving accuracy through knowledge transfer, while pruning directly reduces computational demands by eliminating redundant parameters, making their combined use a common strategy for optimal performance.",
      "new_caption": "",
      "type": "table",
      "source_file": "contents/vol1/optimizations/model_compression.qmd"
    },
    "tbl-qat": {
      "original_caption": "**QAT versus PTQ**: Quantization-aware training (QAT) minimizes accuracy loss by incorporating quantization into the training process, while post-training quantization (PTQ) offers faster deployment but may require calibration to mitigate accuracy degradation. QAT increases training complexity compared to the simplicity of applying PTQ to a pre-trained model.",
      "current_caption": "**QAT versus PTQ**: Quantization-aware training (QAT) minimizes accuracy loss by incorporating quantization into the training process, while post-training quantization (PTQ) offers faster deployment but may require calibration to mitigate accuracy degradation. QAT increases training complexity compared to the simplicity of applying PTQ to a pre-trained model.",
      "new_caption": "",
      "type": "table",
      "source_file": "contents/vol1/optimizations/model_compression.qmd"
    },
    "tbl-optimization-comparison": {
      "original_caption": "**Optimization Technique Trade-offs**: Comparison of the three major optimization approaches across key performance dimensions, highlighting how each technique addresses different constraints and deployment scenarios. Pruning excels for computational reduction but requires sparse hardware support, quantization provides balanced size and speed improvements with wide hardware compatibility, while distillation produces high-quality compressed models at higher training cost.",
      "current_caption": "**Optimization Technique Trade-offs**: Comparison of the three major optimization approaches across key performance dimensions, highlighting how each technique addresses different constraints and deployment scenarios. Pruning excels for computational reduction but requires sparse hardware support, quantization provides balanced size and speed improvements with wide hardware compatibility, while distillation produces high-quality compressed models at higher training cost.",
      "new_caption": "",
      "type": "table",
      "source_file": "contents/vol1/optimizations/model_compression.qmd"
    },
    "tbl-lrmf-tensor": {
      "original_caption": "**Dimensionality & Factorization**: Low-rank matrix factorization (LRMF) and tensor decomposition reduce model storage requirements by representing data with fewer parameters, but introduce computational trade-offs during inference; LRMF applies to two-dimensional matrices, while tensor decomposition extends this approach to multi-dimensional tensors for greater compression potential.",
      "current_caption": "**Dimensionality & Factorization**: Low-rank matrix factorization (LRMF) and tensor decomposition reduce model storage requirements by representing data with fewer parameters, but introduce computational trade-offs during inference; LRMF applies to two-dimensional matrices, while tensor decomposition extends this approach to multi-dimensional tensors for greater compression potential.",
      "new_caption": "",
      "type": "table",
      "source_file": "contents/vol1/optimizations/model_compression.qmd"
    },
    "tbl-constraint-opt-mapping": {
      "original_caption": "**Optimization Dimensions**: System constraints drive optimization along three core dimensions: model representation, numerical precision, and architectural efficiency, each addressing different resource limitations and performance goals.",
      "current_caption": "**Optimization Dimensions**: System constraints drive optimization along three core dimensions: model representation, numerical precision, and architectural efficiency, each addressing different resource limitations and performance goals.",
      "new_caption": "",
      "type": "table",
      "source_file": "contents/vol1/optimizations/model_compression.qmd"
    },
    "tbl-quantization_methods": {
      "original_caption": "**Quantization Method Comparison**: Post-training quantization, quantization-aware training, and dynamic quantization represent distinct approaches to model compression. Legend: ✓ = present, ✗ = absent, △ = input-dependent.",
      "current_caption": "**Quantization Method Comparison**: Post-training quantization, quantization-aware training, and dynamic quantization represent distinct approaches to model compression. Legend: ✓ = present, ✗ = absent, △ = input-dependent.",
      "new_caption": "",
      "type": "table",
      "source_file": "contents/vol1/optimizations/model_compression.qmd"
    },
    "tbl-sparsity-optimization": {
      "original_caption": "**Sparsity Optimization Challenges**: Unstructured sparsity, while reducing model size, hinders hardware acceleration due to irregular memory access patterns, limiting potential computational savings and requiring specialized hardware or software to realize efficiency gains.",
      "current_caption": "**Sparsity Optimization Challenges**: Unstructured sparsity, while reducing model size, hinders hardware acceleration due to irregular memory access patterns, limiting potential computational savings and requiring specialized hardware or software to realize efficiency gains.",
      "new_caption": "",
      "type": "table",
      "source_file": "contents/vol1/optimizations/model_compression.qmd"
    },
    "tbl-deployment-scenarios": {
      "original_caption": "**Deployment Constraints**: Each deployment context imposes different optimization priorities.",
      "current_caption": "**Deployment Constraints**: Each deployment context imposes different optimization priorities.",
      "new_caption": "",
      "type": "table",
      "source_file": "contents/vol1/optimizations/model_compression.qmd"
    },
    "tbl-nas-strategies": {
      "original_caption": "**NAS Search Strategy Comparison**: Trade-offs between search efficiency, use cases, and limitations for different NAS approaches. Reinforcement learning offers unconstrained exploration at high cost, evolutionary methods leverage parallelism, and gradient-based approaches achieve dramatic speedups with potential optimality trade-offs.",
      "current_caption": "**NAS Search Strategy Comparison**: Trade-offs between search efficiency, use cases, and limitations for different NAS approaches. Reinforcement learning offers unconstrained exploration at high cost, evolutionary methods leverage parallelism, and gradient-based approaches achieve dramatic speedups with potential optimality trade-offs.",
      "new_caption": "",
      "type": "table",
      "source_file": "contents/vol1/optimizations/model_compression.qmd"
    },
    "tbl-memory-allocation": {
      "original_caption": "**Memory Allocation Challenges.** Efficient memory management in AI accelerators balances data access speed with hardware constraints, mitigating performance bottlenecks caused by latency, bandwidth limitations, and irregular data patterns. Complex models such as transformers and graph networks impose variable and demanding memory requirements that amplify these challenges.",
      "current_caption": "**Memory Allocation Challenges.** Efficient memory management in AI accelerators balances data access speed with hardware constraints, mitigating performance bottlenecks caused by latency, bandwidth limitations, and irregular data patterns. Complex models such as transformers and graph networks impose variable and demanding memory requirements that amplify these challenges.",
      "new_caption": "",
      "type": "table",
      "source_file": "contents/vol1/hw_acceleration/hw_acceleration.qmd"
    },
    "tbl-nvidia-numerics": {
      "original_caption": "**Precision Support Evolution.** GPU architectures progressively expanded support for lower-precision data types, enabling performance gains and efficiency improvements in AI workloads. Early architectures primarily utilized FP32, while later generations incorporated FP16, BF16, INT8, and INT4 to accelerate both training and inference tasks.",
      "current_caption": "**Precision Support Evolution.** GPU architectures progressively expanded support for lower-precision data types, enabling performance gains and efficiency improvements in AI workloads. Early architectures primarily utilized FP32, while later generations incorporated FP16, BF16, INT8, and INT4 to accelerate both training and inference tasks.",
      "new_caption": "",
      "type": "table",
      "source_file": "contents/vol1/hw_acceleration/hw_acceleration.qmd"
    },
    "tbl-tiling-strategies": {
      "original_caption": "**Tiling Strategies.** Spatial, temporal, and hybrid tiling optimize memory access patterns for improved performance. Spatial tiling maximizes data reuse within fast memory, temporal tiling exploits loop structure for reduced accesses, and hybrid tiling combines both approaches. AI compilers and runtime systems use these techniques to automatically optimize model execution on diverse hardware.",
      "current_caption": "**Tiling Strategies.** Spatial, temporal, and hybrid tiling optimize memory access patterns for improved performance. Spatial tiling maximizes data reuse within fast memory, temporal tiling exploits loop structure for reduced accesses, and hybrid tiling combines both approaches. AI compilers and runtime systems use these techniques to automatically optimize model execution on diverse hardware.",
      "new_caption": "",
      "type": "table",
      "source_file": "contents/vol1/hw_acceleration/hw_acceleration.qmd"
    },
    "tbl-hw-evolution": {
      "original_caption": "**Hardware Specialization Trends.** Successive computing eras progressively integrate specialized hardware to accelerate prevalent workloads, moving from general-purpose CPUs to domain-specific architectures and ultimately to customizable AI accelerators. Tailoring hardware to computational patterns improves performance and energy efficiency, driving innovation in machine learning systems.",
      "current_caption": "**Hardware Specialization Trends.** Successive computing eras progressively integrate specialized hardware to accelerate prevalent workloads, moving from general-purpose CPUs to domain-specific architectures and ultimately to customizable AI accelerators. Tailoring hardware to computational patterns improves performance and energy efficiency, driving innovation in machine learning systems.",
      "new_caption": "",
      "type": "table",
      "source_file": "contents/vol1/hw_acceleration/hw_acceleration.qmd"
    },
    "tbl-ml-vs-traditional-compilers": {
      "original_caption": "**Compiler Optimization Priorities.** Traditional and machine learning compilers diverge in their optimization targets: traditional compilers prioritize efficient execution of sequential code, while ML compilers focus on optimizing tensor operations within computation graphs for specialized hardware. ML compilers incorporate domain-specific transformations such as kernel fusion and memory-aware scheduling, unlike the instruction scheduling and register allocation techniques used in conventional compilation.",
      "current_caption": "**Compiler Optimization Priorities.** Traditional and machine learning compilers diverge in their optimization targets: traditional compilers prioritize efficient execution of sequential code, while ML compilers focus on optimizing tensor operations within computation graphs for specialized hardware. ML compilers incorporate domain-specific transformations such as kernel fusion and memory-aware scheduling, unlike the instruction scheduling and register allocation techniques used in conventional compilation.",
      "new_caption": "",
      "type": "table",
      "source_file": "contents/vol1/hw_acceleration/hw_acceleration.qmd"
    },
    "tbl-roofline-operations": {
      "original_caption": "**Operations on the Roofline.** Neural network layers span a wide range of arithmetic intensities. By mapping these operations to the **Lighthouse Models**, ResNet-50 emerges as compute-bound (high AI) while MobileNet and DLRM are memory-bound (low AI).",
      "current_caption": "**Operations on the Roofline.** Neural network layers span a wide range of arithmetic intensities. By mapping these operations to the **Lighthouse Models**, ResNet-50 emerges as compute-bound (high AI) while MobileNet and DLRM are memory-bound (low AI).",
      "new_caption": "",
      "type": "table",
      "source_file": "contents/vol1/hw_acceleration/hw_acceleration.qmd"
    },
    "tbl-sfu": {
      "original_caption": "**Special Function Units.** Dedicated hardware implementations of common mathematical functions (like relu, sigmoid, and reciprocal square root) accelerate machine learning computations by eliminating software overhead and enabling parallel processing of vector data. Typical latencies of 1 to 2 cycles per function demonstrate the performance gains achieved through specialized circuitry.",
      "current_caption": "**Special Function Units.** Dedicated hardware implementations of common mathematical functions (like relu, sigmoid, and reciprocal square root) accelerate machine learning computations by eliminating software overhead and enabling parallel processing of vector data. Typical latencies of 1 to 2 cycles per function demonstrate the performance gains achieved through specialized circuitry.",
      "new_caption": "",
      "type": "table",
      "source_file": "contents/vol1/hw_acceleration/hw_acceleration.qmd"
    },
    "tbl-matrix": {
      "original_caption": "**Operation Characteristics.** Matrix operations excel at many-to-many transformations common in neural network layers, while vector operations efficiently handle one-to-one transformations like activation functions and normalization. These distinctions guide the selection of appropriate computational primitives for different machine learning tasks.",
      "current_caption": "**Operation Characteristics.** Matrix operations excel at many-to-many transformations common in neural network layers, while vector operations efficiently handle one-to-one transformations like activation functions and normalization. These distinctions guide the selection of appropriate computational primitives for different machine learning tasks.",
      "new_caption": "",
      "type": "table",
      "source_file": "contents/vol1/hw_acceleration/hw_acceleration.qmd"
    },
    "tbl-traditional-vs-ml-mem": {
      "original_caption": "**Memory Access Characteristics.** Traditional workloads exhibit predictable, sequential memory access benefiting from standard caching, while machine learning workloads introduce irregular and dynamic patterns due to sparsity and data dependencies. These differences inform the design of memory systems that efficiently support modern AI applications.",
      "current_caption": "**Memory Access Characteristics.** Traditional workloads exhibit predictable, sequential memory access benefiting from standard caching, while machine learning workloads introduce irregular and dynamic patterns due to sparsity and data dependencies. These differences inform the design of memory systems that efficiently support modern AI applications.",
      "new_caption": "",
      "type": "table",
      "source_file": "contents/vol1/hw_acceleration/hw_acceleration.qmd"
    },
    "tbl-major": {
      "original_caption": "**Data Layout Strategies.** Row-major (NHWC) and channel-major (NCHW) layouts optimize memory access patterns for different hardware architectures; NHWC suits CPUs and element-wise operations, while NCHW accelerates GPU and TPU-based convolution operations. Choosing the appropriate layout significantly impacts performance by maximizing cache utilization and memory bandwidth efficiency.",
      "current_caption": "**Data Layout Strategies.** Row-major (NHWC) and channel-major (NCHW) layouts optimize memory access patterns for different hardware architectures; NHWC suits CPUs and element-wise operations, while NCHW accelerates GPU and TPU-based convolution operations. Choosing the appropriate layout significantly impacts performance by maximizing cache utilization and memory bandwidth efficiency.",
      "new_caption": "",
      "type": "table",
      "source_file": "contents/vol1/hw_acceleration/hw_acceleration.qmd"
    },
    "tbl-combinatorial-complexity": {
      "original_caption": "**Placement-Allocation Trade-Offs.** AI accelerator performance depends on strategically mapping computations to hardware and allocating resources over time, balancing parallelism, memory access, and execution efficiency. Careful consideration of these interdependent factors is essential for maximizing throughput and minimizing energy consumption.",
      "current_caption": "**Placement-Allocation Trade-Offs.** AI accelerator performance depends on strategically mapping computations to hardware and allocating resources over time, balancing parallelism, memory access, and execution efficiency. Careful consideration of these interdependent factors is essential for maximizing throughput and minimizing energy consumption.",
      "new_caption": "",
      "type": "table",
      "source_file": "contents/vol1/hw_acceleration/hw_acceleration.qmd"
    },
    "tbl-runtime-comparison": {
      "original_caption": "**Runtime Execution Models.** Traditional runtimes prioritize sequential or multi-threaded instruction processing, while AI runtimes use massively parallel tensor operations for accelerated computation on machine learning workloads. This divergence necessitates specialized AI runtime architectures designed for efficient parallelization and memory management of large-scale tensor data.",
      "current_caption": "**Runtime Execution Models.** Traditional runtimes prioritize sequential or multi-threaded instruction processing, while AI runtimes use massively parallel tensor operations for accelerated computation on machine learning workloads. This divergence necessitates specialized AI runtime architectures designed for efficient parallelization and memory management of large-scale tensor data.",
      "new_caption": "",
      "type": "table",
      "source_file": "contents/vol1/hw_acceleration/hw_acceleration.qmd"
    },
    "tbl-placement-challenges": {
      "original_caption": "**Computation Placement Challenges.** Effective neural network deployment requires strategic allocation of computations to processing elements, balancing workload distribution, data movement costs, and hardware constraints to maximize execution efficiency. These challenges guide the design of mapping strategies that optimize resource utilization and minimize communication overhead.",
      "current_caption": "**Computation Placement Challenges.** Effective neural network deployment requires strategic allocation of computations to processing elements, balancing workload distribution, data movement costs, and hardware constraints to maximize execution efficiency. These challenges guide the design of mapping strategies that optimize resource utilization and minimize communication overhead.",
      "new_caption": "",
      "type": "table",
      "source_file": "contents/vol1/hw_acceleration/hw_acceleration.qmd"
    },
    "tbl-mapping-strategies": {
      "original_caption": "**Architecture-Specific Mapping Strategies.** Each neural network architecture benefits from different optimization priorities based on its computational and memory characteristics.",
      "current_caption": "**Architecture-Specific Mapping Strategies.** Each neural network architecture benefits from different optimization priorities based on its computational and memory characteristics.",
      "new_caption": "",
      "type": "table",
      "source_file": "contents/vol1/hw_acceleration/hw_acceleration.qmd"
    },
    "tbl-execution-units": {
      "original_caption": "**AI Processor Configurations.** Modern AI processors prioritize different execution unit characteristics for specific workloads: NVIDIA A100 leverages wide SIMD and tensor cores for training, Google TPUv4 emphasizes high-throughput BF16 matrix multiplication, Intel Sapphire Rapids focuses on INT8-optimized inference, and Apple M1 prioritizes low-power FP16 execution. These variations in SIMD width, tensor core size, and processing element count reflect the growing diversity in AI hardware architectures.",
      "current_caption": "**AI Processor Configurations.** Modern AI processors prioritize different execution unit characteristics for specific workloads: NVIDIA A100 leverages wide SIMD and tensor cores for training, Google TPUv4 emphasizes high-throughput BF16 matrix multiplication, Intel Sapphire Rapids focuses on INT8-optimized inference, and Apple M1 prioritizes low-power FP16 execution. These variations in SIMD width, tensor core size, and processing element count reflect the growing diversity in AI hardware architectures.",
      "new_caption": "",
      "type": "table",
      "source_file": "contents/vol1/hw_acceleration/hw_acceleration.qmd"
    },
    "tbl-memory-hierarchy": {
      "original_caption": "**Memory Hierarchy Trade-Offs.** AI accelerators use a multi-level memory hierarchy to balance performance and capacity. Each level provides distinct latency, bandwidth, and capacity characteristics that dictate how neural network components (weights, activations, and intermediate results) should be allocated to minimize bottlenecks and maximize throughput.",
      "current_caption": "**Memory Hierarchy Trade-Offs.** AI accelerators use a multi-level memory hierarchy to balance performance and capacity. Each level provides distinct latency, bandwidth, and capacity characteristics that dictate how neural network components (weights, activations, and intermediate results) should be allocated to minimize bottlenecks and maximize throughput.",
      "new_caption": "",
      "type": "table",
      "source_file": "contents/vol1/hw_acceleration/hw_acceleration.qmd"
    },
    "tbl-model-mem-compare": {
      "original_caption": "**ML Model Memory Access.** Different machine learning models exhibit distinct memory access patterns and bottlenecks due to variations in weight size, activation reuse, and data sparsity. Transformers demand high bandwidth and capacity due to their massive, sparsely accessed weights, while CNNs benefit from spatial locality and high activation reuse, reducing memory pressure.",
      "current_caption": "**ML Model Memory Access.** Different machine learning models exhibit distinct memory access patterns and bottlenecks due to variations in weight size, activation reuse, and data sparsity. Transformers demand high bandwidth and capacity due to their massive, sparsely accessed weights, while CNNs benefit from spatial locality and high activation reuse, reducing memory pressure.",
      "new_caption": "",
      "type": "table",
      "source_file": "contents/vol1/hw_acceleration/hw_acceleration.qmd"
    },
    "tbl-ridge-points": {
      "original_caption": "**Hardware Ridge Points.** Representative, order-of-magnitude ridge points for different accelerators, determined by their compute-to-bandwidth ratios. Higher ridge points require more operations per byte to achieve peak utilization.",
      "current_caption": "**Hardware Ridge Points.** Representative, order-of-magnitude ridge points for different accelerators, determined by their compute-to-bandwidth ratios. Higher ridge points require more operations per byte to achieve peak utilization.",
      "new_caption": "",
      "type": "table",
      "source_file": "contents/vol1/hw_acceleration/hw_acceleration.qmd"
    },
    "tbl-vector": {
      "original_caption": "**Vector Operations.** Core vector operations, including reduction, gather, and scatter, accelerate computation and efficiently process data in parallel. These operations enable efficient implementation of common layers like pooling, embedding lookups, and attention mechanisms within deep learning models.",
      "current_caption": "**Vector Operations.** Core vector operations, including reduction, gather, and scatter, accelerate computation and efficiently process data in parallel. These operations enable efficient implementation of common layers like pooling, embedding lookups, and attention mechanisms within deep learning models.",
      "new_caption": "",
      "type": "table",
      "source_file": "contents/vol1/hw_acceleration/hw_acceleration.qmd"
    },
    "tbl-accelerator-economics": {
      "original_caption": "**Accelerator Cost-Performance Comparison.** Hardware costs evaluated against computational capabilities for optimal deployment strategy selection. Newer accelerators offer better price-performance ratios, though total cost of ownership includes power consumption, cooling requirements, and infrastructure costs. Prices are approximate list prices and vary by region and volume; TPU pricing estimated from cloud rates.",
      "current_caption": "**Accelerator Cost-Performance Comparison.** Hardware costs evaluated against computational capabilities for optimal deployment strategy selection. Newer accelerators offer better price-performance ratios, though total cost of ownership includes power consumption, cooling requirements, and infrastructure costs. Prices are approximate list prices and vary by region and volume; TPU pricing estimated from cloud rates.",
      "new_caption": "",
      "type": "table",
      "source_file": "contents/vol1/hw_acceleration/hw_acceleration.qmd"
    },
    "tbl-mlperf-scenarios": {
      "original_caption": "**MLPerf Execution Scenarios.** The four MLPerf inference scenarios map to distinct deployment contexts, each requiring different optimization strategies. SingleStream and MultiStream prioritize latency, Server balances throughput and latency, and Offline maximizes throughput. Matching the scenario to deployment context determines which benchmark results are relevant.",
      "current_caption": "**MLPerf Execution Scenarios.** The four MLPerf inference scenarios map to distinct deployment contexts, each requiring different optimization strategies. SingleStream and MultiStream prioritize latency, Server balances throughput and latency, and Offline maximizes throughput. Matching the scenario to deployment context determines which benchmark results are relevant.",
      "new_caption": "",
      "type": "table",
      "source_file": "contents/vol1/benchmarking/benchmarking.qmd"
    },
    "tbl-dam-bottleneck": {
      "original_caption": "**DAM x Bottleneck Diagnostic Matrix.** Each cell describes a performance constraint symptom, and the row identifies which AI Triad component to address. When performance stalls, ask: *\"Where is the flow blocked? Check the DAM.\"*",
      "current_caption": "**DAM x Bottleneck Diagnostic Matrix.** Each cell describes a performance constraint symptom, and the row identifies which AI Triad component to address. When performance stalls, ask: *\"Where is the flow blocked? Check the DAM.\"*",
      "new_caption": "",
      "type": "table",
      "source_file": "contents/vol1/benchmarking/benchmarking.qmd"
    },
    "tbl-benchmark-evolution": {
      "original_caption": "**Benchmark Evolution.** Evolution of computing benchmarks from synthetic operations to ML-specific evaluation. Each generation addressed limitations of its predecessors, culminating in MLPerf's synthesis of representative workloads, multi-objective metrics, and integrated system measurement.",
      "current_caption": "**Benchmark Evolution.** Evolution of computing benchmarks from synthetic operations to ML-specific evaluation. Each generation addressed limitations of its predecessors, culminating in MLPerf's synthesis of representative workloads, multi-objective metrics, and integrated system measurement.",
      "new_caption": "",
      "type": "table",
      "source_file": "contents/vol1/benchmarking/benchmarking.qmd"
    },
    "tbl-metric-taxonomy": {
      "original_caption": "**ML Benchmarking Metric Taxonomy.** Metrics organized by category, showing the appropriate unit and primary use case for each.",
      "current_caption": "**ML Benchmarking Metric Taxonomy.** Metrics organized by category, showing the appropriate unit and primary use case for each.",
      "new_caption": "",
      "type": "table",
      "source_file": "contents/vol1/benchmarking/benchmarking.qmd"
    },
    "tbl-benchmark-comparison": {
      "original_caption": "**Benchmarking Granularity Levels.** Different benchmark scopes target distinct stages of ML system development. Micro-benchmarks isolate individual operations for low-level optimization, macro-benchmarks evaluate complete models to guide architectural choices, and end-to-end benchmarks assess full system performance in production environments.",
      "current_caption": "**Benchmarking Granularity Levels.** Different benchmark scopes target distinct stages of ML system development. Micro-benchmarks isolate individual operations for low-level optimization, macro-benchmarks evaluate complete models to guide architectural choices, and end-to-end benchmarks assess full system performance in production environments.",
      "new_caption": "",
      "type": "table",
      "source_file": "contents/vol1/benchmarking/benchmarking.qmd"
    },
    "tbl-inference-metrics": {
      "original_caption": "**Inference Performance Metrics.** Latency, throughput, and resource usage metrics provide a quantitative basis for optimizing deployed machine learning systems and selecting appropriate hardware configurations, balancing speed, cost, and accuracy in production applications.",
      "current_caption": "**Inference Performance Metrics.** Latency, throughput, and resource usage metrics provide a quantitative basis for optimizing deployed machine learning systems and selecting appropriate hardware configurations, balancing speed, cost, and accuracy in production applications.",
      "new_caption": "",
      "type": "table",
      "source_file": "contents/vol1/benchmarking/benchmarking.qmd"
    },
    "tbl-training-metrics": {
      "original_caption": "**Training Benchmark Dimensions.** Key categories and metrics for evaluating machine learning training systems beyond simple speed, covering resource efficiency, reproducibility, and overall performance tradeoffs across different training approaches and infrastructure configurations.",
      "current_caption": "**Training Benchmark Dimensions.** Key categories and metrics for evaluating machine learning training systems beyond simple speed, covering resource efficiency, reproducibility, and overall performance tradeoffs across different training approaches and infrastructure configurations.",
      "new_caption": "",
      "type": "table",
      "source_file": "contents/vol1/benchmarking/benchmarking.qmd"
    },
    "tbl-metric-priorities": {
      "original_caption": "**Performance Metric Priorities by Deployment Context.** Different operational environments demand distinct optimization focuses, reflecting varying constraints and success criteria. These priorities guide both benchmark selection and result interpretation.",
      "current_caption": "**Performance Metric Priorities by Deployment Context.** Different operational environments demand distinct optimization focuses, reflecting varying constraints and success criteria. These priorities guide both benchmark selection and result interpretation.",
      "new_caption": "",
      "type": "table",
      "source_file": "contents/vol1/benchmarking/benchmarking.qmd"
    },
    "tbl-latency-breakdown": {
      "original_caption": "**Inference Latency Breakdown.** Different components contribute to end-to-end latency, with model inference often representing only a fraction of total request time.",
      "current_caption": "**Inference Latency Breakdown.** Different components contribute to end-to-end latency, with model inference often representing only a fraction of total request time.",
      "new_caption": "",
      "type": "table",
      "source_file": "contents/vol1/benchmarking/benchmarking.qmd"
    },
    "tbl-power": {
      "original_caption": "**Power Consumption Spectrum.** Machine learning deployments span a wide range of power demands, from microwatt-scale TinyML devices to kilowatt-scale server racks. This variability challenges standardized energy efficiency benchmarking and requires scale-appropriate measurement techniques.",
      "current_caption": "**Power Consumption Spectrum.** Machine learning deployments span a wide range of power demands, from microwatt-scale TinyML devices to kilowatt-scale server racks. This variability challenges standardized energy efficiency benchmarking and requires scale-appropriate measurement techniques.",
      "new_caption": "",
      "type": "table",
      "source_file": "contents/vol1/benchmarking/benchmarking.qmd"
    },
    "tbl-mlperf-suites": {
      "original_caption": "**MLPerf Benchmark Suite Variants.** Each variant addresses a different deployment context, from datacenter-scale training to ultra-constrained microcontroller inference, targeting specific operational constraints and measuring metrics relevant to its deployment scenario.",
      "current_caption": "**MLPerf Benchmark Suite Variants.** Each variant addresses a different deployment context, from datacenter-scale training to ultra-constrained microcontroller inference, targeting specific operational constraints and measuring metrics relevant to its deployment scenario.",
      "new_caption": "",
      "type": "table",
      "source_file": "contents/vol1/benchmarking/benchmarking.qmd"
    },
    "tbl-serving-tax": {
      "original_caption": "**The Serving Tax Bill**: A breakdown of non-inference latency sources. While individual components like serialization seem small ($<1$ ms), they compound. In a 5ms inference service, this \"tax\" can easily consume 50% of the latency budget. The primary engineering goal is to drive these costs to zero through architectural choices like gRPC and Zero-Copy data paths.",
      "current_caption": "**The Serving Tax Bill**: A breakdown of non-inference latency sources. While individual components like serialization seem small ($<1$ ms), they compound. In a 5ms inference service, this \"tax\" can easily consume 50% of the latency budget. The primary engineering goal is to drive these costs to zero through architectural choices like gRPC and Zero-Copy data paths.",
      "new_caption": "",
      "type": "table",
      "source_file": "contents/vol1/serving/serving.qmd"
    },
    "tbl-batch-variability": {
      "original_caption": "**Batch Size Variability**: At low traffic, batching windows frequently contain zero requests (wasted GPU cycles). At moderate traffic, batch sizes fluctuate significantly around the mean. High traffic provides more stable batching but still sees 13% of batches exceeding twice the mean size.",
      "current_caption": "**Batch Size Variability**: At low traffic, batching windows frequently contain zero requests (wasted GPU cycles). At moderate traffic, batch sizes fluctuate significantly around the mean. High traffic provides more stable batching but still sees 13% of batches exceeding twice the mean size.",
      "new_caption": "",
      "type": "table",
      "source_file": "contents/vol1/serving/serving.qmd"
    },
    "tbl-traffic-patterns-summary": {
      "original_caption": "**Traffic Patterns and Batching Strategies**: The four MLPerf inference scenarios map to distinct deployment contexts. Server traffic (cloud APIs) uses dynamic batching with timeout; MultiStream (autonomous driving) uses synchronized sensor fusion; SingleStream (mobile) processes requests individually; Offline (batch processing) maximizes batch size for throughput.",
      "current_caption": "**Traffic Patterns and Batching Strategies**: The four MLPerf inference scenarios map to distinct deployment contexts. Server traffic (cloud APIs) uses dynamic batching with timeout; MultiStream (autonomous driving) uses synchronized sensor fusion; SingleStream (mobile) processes requests individually; Offline (batch processing) maximizes batch size for throughput.",
      "new_caption": "",
      "type": "table",
      "source_file": "contents/vol1/serving/serving.qmd"
    },
    "tbl-serving-spectrum": {
      "original_caption": "**Serving Architecture Spectrum**: The deployment context fundamentally shapes every aspect of serving system design. Cloud systems optimize for throughput with dynamic batching; mobile systems optimize for energy with fixed batch-1; TinyML systems operate under extreme memory and power constraints with no dynamic allocation.",
      "current_caption": "**Serving Architecture Spectrum**: The deployment context fundamentally shapes every aspect of serving system design. Cloud systems optimize for throughput with dynamic batching; mobile systems optimize for energy with fixed batch-1; TinyML systems operate under extreme memory and power constraints with no dynamic allocation.",
      "new_caption": "",
      "type": "table",
      "source_file": "contents/vol1/serving/serving.qmd"
    },
    "tbl-pareto-batching": {
      "original_caption": "**Batching Pareto Frontier**: Each configuration represents a different point on the throughput-latency trade-off curve. Moving from 2ms to 50ms windows improves throughput by only 52% while increasing p99 latency by 5.4×. Diminishing returns make aggressive batching costly for latency-sensitive applications.",
      "current_caption": "**Batching Pareto Frontier**: Each configuration represents a different point on the throughput-latency trade-off curve. Moving from 2ms to 50ms windows improves throughput by only 52% while increasing p99 latency by 5.4×. Diminishing returns make aggressive batching costly for latency-sensitive applications.",
      "new_caption": "",
      "type": "table",
      "source_file": "contents/vol1/serving/serving.qmd"
    },
    "tbl-traffic-adaptive": {
      "original_caption": "**Traffic-Adaptive Batching**: Higher traffic enables shorter windows while still achieving larger batches. The optimal window decreases even as batch sizes grow because more requests arrive per unit time.",
      "current_caption": "**Traffic-Adaptive Batching**: Higher traffic enables shorter windows while still achieving larger batches. The optimal window decreases even as batch sizes grow because more requests arrive per unit time.",
      "new_caption": "",
      "type": "table",
      "source_file": "contents/vol1/serving/serving.qmd"
    },
    "tbl-optimization-impact": {
      "original_caption": "**Node-Level Optimization Impact**: A decision matrix for selecting optimization techniques. High-impact techniques like quantization often carry higher implementation costs (calibration data requirements), while architectural changes like zero-copy loading offer dramatic gains for specific metrics (startup time) with low effort.",
      "current_caption": "**Node-Level Optimization Impact**: A decision matrix for selecting optimization techniques. High-impact techniques like quantization often carry higher implementation costs (calibration data requirements), while architectural changes like zero-copy loading offer dramatic gains for specific metrics (startup time) with low effort.",
      "new_caption": "",
      "type": "table",
      "source_file": "contents/vol1/serving/serving.qmd"
    },
    "tbl-utilization-latency": {
      "original_caption": "**Utilization-Latency Relationship**: Average **latency** as a multiple of service time for an M/M/1 queue. At 50% utilization, latency is 2x service time; at 90%, it reaches 10x. This nonlinear growth explains why systems that perform well at moderate load suddenly violate SLOs when traffic increases: moving from 80% to 90% utilization doubles latency.",
      "current_caption": "**Utilization-Latency Relationship**: Average **latency** as a multiple of service time for an M/M/1 queue. At 50% utilization, latency is 2x service time; at 90%, it reaches 10x. This nonlinear growth explains why systems that perform well at moderate load suddenly violate SLOs when traffic increases: moving from 80% to 90% utilization doubles latency.",
      "new_caption": "",
      "type": "table",
      "source_file": "contents/vol1/serving/serving.qmd"
    },
    "tbl-gpu-memory-hierarchy": {
      "original_caption": "GPU Memory Hierarchy and Bandwidth Characteristics",
      "current_caption": "GPU Memory Hierarchy and Bandwidth Characteristics",
      "new_caption": "",
      "type": "table",
      "source_file": "contents/vol1/ops/ops.qmd"
    },
    "tbl-mlops": {
      "original_caption": "**MLOps Separation of Concerns.** Each layer addresses a distinct responsibility and evolves at different rates, from stable hardware foundations through model-level components that change with each experiment. This separation enables independent scaling and updates, reducing blast radius when changes are required.",
      "current_caption": "**MLOps Separation of Concerns.** Each layer addresses a distinct responsibility and evolves at different rates, from stable hardware foundations through model-level components that change with each experiment. This separation enables independent scaling and updates, reducing blast radius when changes are required.",
      "new_caption": "",
      "type": "table",
      "source_file": "contents/vol1/ops/ops.qmd"
    },
    "tbl-slice-analysis-example": {
      "original_caption": "**Slice Analysis Example.** Overall accuracy of 91% appears acceptable, but tablet users (5% of traffic) experience 62% accuracy, a severe degradation masked by aggregation. Effective debugging requires systematic slice analysis across key dimensions.",
      "current_caption": "**Slice Analysis Example.** Overall accuracy of 91% appears acceptable, but tablet users (5% of traffic) experience 62% accuracy, a severe degradation masked by aggregation. Effective debugging requires systematic slice analysis across key dimensions.",
      "new_caption": "",
      "type": "table",
      "source_file": "contents/vol1/ops/ops.qmd"
    },
    "tbl-training-serving-skew": {
      "original_caption": "**Training-Serving Skew Categories.** Each category requires different detection and prevention strategies. Schema and preprocessing skew emerge from code divergence and require feature store unification, while data distribution skew requires statistical monitoring against training baselines. Timing skew demands careful analysis of feature freshness between training and serving contexts.",
      "current_caption": "**Training-Serving Skew Categories.** Each category requires different detection and prevention strategies. Schema and preprocessing skew emerge from code divergence and require feature store unification, while data distribution skew requires statistical monitoring against training baselines. Timing skew demands careful analysis of feature freshness between training and serving contexts.",
      "new_caption": "",
      "type": "table",
      "source_file": "contents/vol1/ops/ops.qmd"
    },
    "tbl-ml-roles-matrix": {
      "original_caption": "**ML Team Roles Matrix.** Clear role boundaries prevent gaps and overlaps. Data Scientists focus on model quality while ML Engineers handle productionization. Data Engineers own data pipelines while Platform Engineers own MLOps tooling. SREs ensure overall system reliability.",
      "current_caption": "**ML Team Roles Matrix.** Clear role boundaries prevent gaps and overlaps. Data Scientists focus on model quality while ML Engineers handle productionization. Data Engineers own data pipelines while Platform Engineers own MLOps tooling. SREs ensure overall system reliability.",
      "new_caption": "",
      "type": "table",
      "source_file": "contents/vol1/ops/ops.qmd"
    },
    "tbl-ml-test-score": {
      "original_caption": "**ML Test Score Checklist.** A practical rubric for assessing ML system production readiness. Each test scores 0 (not implemented), 0.5 (partially implemented), or 1 (fully automated). Systems scoring below 5 require significant investment before production deployment; scores above 10 indicate mature operational practices. Based on Breck et al. [@breck2020ml].",
      "current_caption": "**ML Test Score Checklist.** A practical rubric for assessing ML system production readiness. Each test scores 0 (not implemented), 0.5 (partially implemented), or 1 (fully automated). Systems scoring below 5 require significant investment before production deployment; scores above 10 indicate mature operational practices. Based on Breck et al. [@breck2020ml].",
      "new_caption": "",
      "type": "table",
      "source_file": "contents/vol1/ops/ops.qmd"
    },
    "tbl-ab-test-decisions": {
      "original_caption": "**A/B Test Decision Matrix.** Deployment decisions should consider both primary metrics and guardrails. Improvements that come at the cost of guardrail violations require careful tradeoff analysis rather than automatic deployment.",
      "current_caption": "**A/B Test Decision Matrix.** Deployment decisions should consider both primary metrics and guardrails. Improvements that come at the cost of guardrail violations require careful tradeoff analysis rather than automatic deployment.",
      "new_caption": "",
      "type": "table",
      "source_file": "contents/vol1/ops/ops.qmd"
    },
    "tbl-clinical_ops": {
      "original_caption": "**Clinical AI Operations.** Traditional MLOps focuses on model performance, while ClinAIOps integrates technical systems with clinical workflows, ethical considerations, and ongoing feedback loops to ensure safe, trustworthy AI assistance in healthcare settings. ClinAIOps prioritizes human oversight and accountability alongside automation, addressing unique challenges in clinical decision-making that standard MLOps pipelines often overlook.",
      "current_caption": "**Clinical AI Operations.** Traditional MLOps focuses on model performance, while ClinAIOps integrates technical systems with clinical workflows, ethical considerations, and ongoing feedback loops to ensure safe, trustworthy AI assistance in healthcare settings. ClinAIOps prioritizes human oversight and accountability alongside automation, addressing unique challenges in clinical decision-making that standard MLOps pipelines often overlook.",
      "new_caption": "",
      "type": "table",
      "source_file": "contents/vol1/ops/ops.qmd"
    },
    "tbl-oncall-structure": {
      "original_caption": "**ML On-Call Structure.** Tiered escalation with parallel data on-call enables efficient incident response. Tier 1 handles routine issues using runbooks; Tier 2 addresses complex debugging; Tier 3 manages critical incidents requiring architectural decisions.",
      "current_caption": "**ML On-Call Structure.** Tiered escalation with parallel data on-call enables efficient incident response. Tier 1 handles routine issues using runbooks; Tier 2 addresses complex debugging; Tier 3 manages critical incidents requiring architectural decisions.",
      "new_caption": "",
      "type": "table",
      "source_file": "contents/vol1/ops/ops.qmd"
    },
    "tbl-serving-techniques": {
      "original_caption": "**Serving System Techniques.** Scalable ML-as-a-service infrastructure relies on techniques like request scheduling and instance selection to optimize resource utilization and reduce latency under high load. Key strategies and representative systems illustrate approaches for efficient deployment of machine learning models.",
      "current_caption": "**Serving System Techniques.** Scalable ML-as-a-service infrastructure relies on techniques like request scheduling and instance selection to optimize resource utilization and reduce latency under high load. Key strategies and representative systems illustrate approaches for efficient deployment of machine learning models.",
      "new_caption": "",
      "type": "table",
      "source_file": "contents/vol1/ops/ops.qmd"
    },
    "tbl-mlops-layers": {
      "original_caption": "**MLOps Separation of Concerns.** Each layer addresses a distinct responsibility and evolves at different rates, from stable hardware foundations through model-level components that change with each experiment. This separation enables independent scaling and updates, reducing blast radius when changes are required.",
      "current_caption": "**MLOps Separation of Concerns.** Each layer addresses a distinct responsibility and evolves at different rates, from stable hardware foundations through model-level components that change with each experiment. This separation enables independent scaling and updates, reducing blast radius when changes are required.",
      "new_caption": "",
      "type": "table",
      "source_file": "contents/vol1/ops/ops.qmd"
    },
    "tbl-rollback-patterns": {
      "original_caption": "**Rollback Patterns by Scenario.** Each rollback type requires different infrastructure support and state handling strategies. Immediate rollback demands always-warm standbys; delayed rollback may require data migration procedures.",
      "current_caption": "**Rollback Patterns by Scenario.** Each rollback type requires different infrastructure support and state handling strategies. Immediate rollback demands always-warm standbys; delayed rollback may require data migration procedures.",
      "new_caption": "",
      "type": "table",
      "source_file": "contents/vol1/ops/ops.qmd"
    },
    "tbl-technical-debt-summary": {
      "original_caption": "**Technical Debt Patterns.** Machine learning systems accumulate distinct forms of technical debt from data dependencies, model interactions, and evolving operational contexts. Primary debt patterns, their causes, symptoms, and recommended mitigation strategies guide practitioners in recognizing and addressing these challenges systematically.",
      "current_caption": "**Technical Debt Patterns.** Machine learning systems accumulate distinct forms of technical debt from data dependencies, model interactions, and evolving operational contexts. Primary debt patterns, their causes, symptoms, and recommended mitigation strategies guide practitioners in recognizing and addressing these challenges systematically.",
      "new_caption": "",
      "type": "table",
      "source_file": "contents/vol1/ops/ops.qmd"
    },
    "tbl-retraining-schedules": {
      "original_caption": "**Typical Retraining Schedules by Domain.** These represent starting points; teams should calibrate based on observed drift rates and business impact.",
      "current_caption": "**Typical Retraining Schedules by Domain.** These represent starting points; teams should calibrate based on observed drift rates and business impact.",
      "new_caption": "",
      "type": "table",
      "source_file": "contents/vol1/ops/ops.qmd"
    },
    "tbl-mlops-principles-summary": {
      "original_caption": "**MLOps Principles Summary.** Quick reference for the five foundational principles that guide all MLOps tooling and practice decisions.",
      "current_caption": "**MLOps Principles Summary.** Quick reference for the five foundational principles that guide all MLOps tooling and practice decisions.",
      "new_caption": "",
      "type": "table",
      "source_file": "contents/vol1/ops/ops.qmd"
    },
    "tbl-maturity-levels": {
      "original_caption": "**Maturity Progression.** Machine learning operational practices evolve from manual, fragile workflows toward fully integrated, automated systems, impacting reproducibility and scalability. Key characteristics and outcomes at each maturity level emphasize architectural cohesion and lifecycle integration for building maintainable learning systems.",
      "current_caption": "**Maturity Progression.** Machine learning operational practices evolve from manual, fragile workflows toward fully integrated, automated systems, impacting reproducibility and scalability. Key characteristics and outcomes at each maturity level emphasize architectural cohesion and lifecycle integration for building maintainable learning systems.",
      "new_caption": "",
      "type": "table",
      "source_file": "contents/vol1/ops/ops.qmd"
    },
    "tbl-retraining-sensitivity": {
      "original_caption": "**Retraining Interval Sensitivity.** How parameter changes affect optimal retraining frequency. Doubling query volume halves the optimal interval because degradation costs scale linearly with traffic. Halving retraining costs similarly reduces the interval, while lower drift rates extend it. Systems with high traffic and high per-query value benefit most from frequent retraining automation.",
      "current_caption": "**Retraining Interval Sensitivity.** How parameter changes affect optimal retraining frequency. Doubling query volume halves the optimal interval because degradation costs scale linearly with traffic. Halving retraining costs similarly reduces the interval, while lower drift rates extend it. Systems with high traffic and high per-query value benefit most from frequent retraining automation.",
      "new_caption": "",
      "type": "table",
      "source_file": "contents/vol1/ops/ops.qmd"
    },
    "tbl-incident-severity": {
      "original_caption": "**Incident Severity Classification for ML Systems.** Response times reflect the urgency and potential business impact of each severity level.",
      "current_caption": "**Incident Severity Classification for ML Systems.** Response times reflect the urgency and potential business impact of each severity level.",
      "new_caption": "",
      "type": "table",
      "source_file": "contents/vol1/ops/ops.qmd"
    },
    "tbl-model-optimization-frameworks": {
      "original_caption": "**Model Optimization Frameworks.** Different optimization tools target different deployment scenarios. TensorRT provides maximum performance on NVIDIA GPUs but locks you into that hardware. ONNX Runtime offers broader compatibility across hardware targets. OpenVINO optimizes for Intel hardware ecosystems.",
      "current_caption": "**Model Optimization Frameworks.** Different optimization tools target different deployment scenarios. TensorRT provides maximum performance on NVIDIA GPUs but locks you into that hardware. ONNX Runtime offers broader compatibility across hardware targets. OpenVINO optimizes for Intel hardware ecosystems.",
      "new_caption": "",
      "type": "table",
      "source_file": "contents/vol1/ops/ops.qmd"
    },
    "tbl-degradation-types": {
      "original_caption": "**Degradation Detection Strategies.** Different failure modes require different monitoring approaches and response strategies. Statistical tests detect distribution shifts before performance degrades visibly, while performance monitoring catches issues that evade statistical detection. Adaptive thresholds prevent false alarms while maintaining sensitivity to genuine degradation.",
      "current_caption": "**Degradation Detection Strategies.** Different failure modes require different monitoring approaches and response strategies. Statistical tests detect distribution shifts before performance degrades visibly, while performance monitoring catches issues that evade statistical detection. Adaptive thresholds prevent false alarms while maintaining sensitivity to genuine degradation.",
      "new_caption": "",
      "type": "table",
      "source_file": "contents/vol1/ops/ops.qmd"
    },
    "tbl-monitoring-archetype-strategy": {
      "original_caption": "**Monitoring Strategy by Workload Archetype.** Monitoring strategy varies by workload archetype's dominant failure mode, requiring tailored metrics and response thresholds for each deployment context.",
      "current_caption": "**Monitoring Strategy by Workload Archetype.** Monitoring strategy varies by workload archetype's dominant failure mode, requiring tailored metrics and response thresholds for each deployment context.",
      "new_caption": "",
      "type": "table",
      "source_file": "contents/vol1/ops/ops.qmd"
    },
    "tbl-feature-distribution-thresholds": {
      "original_caption": "**Feature Distribution Thresholds.** Starting points for drift detection; teams should calibrate based on feature sensitivity and business impact. PSI values above 0.2 indicate significant distribution shift, while KS statistics exceeding 0.1 suggest statistically significant divergence from training distributions. Higher thresholds reduce alert fatigue but risk missing gradual drift.",
      "current_caption": "**Feature Distribution Thresholds.** Starting points for drift detection; teams should calibrate based on feature sensitivity and business impact. PSI values above 0.2 indicate significant distribution shift, while KS statistics exceeding 0.1 suggest statistically significant divergence from training distributions. Higher thresholds reduce alert fatigue but risk missing gradual drift.",
      "new_caption": "",
      "type": "table",
      "source_file": "contents/vol1/ops/ops.qmd"
    },
    "tbl-retraining-parameters": {
      "original_caption": "**Retraining Decision Parameters.** Example values for a fraud detection system processing 50,000 queries daily. The 5% per week drift rate reflects observed fraud evolution in financial services, while the $50 per point accuracy value captures the cost of missed fraud cases. Actual parameters vary by domain and should be calibrated from production observations.",
      "current_caption": "**Retraining Decision Parameters.** Example values for a fraud detection system processing 50,000 queries daily. The 5% per week drift rate reflects observed fraud evolution in financial services, while the $50 per point accuracy value captures the cost of missed fraud cases. Actual parameters vary by domain and should be calibrated from production observations.",
      "new_caption": "",
      "type": "table",
      "source_file": "contents/vol1/ops/ops.qmd"
    },
    "tbl-gpu-utilization-patterns": {
      "original_caption": "**GPU Utilization Patterns.** Different utilization signatures require different optimizations. High GPU utilization with low memory bandwidth suggests compute-bound workloads that benefit from parallelism. High memory bandwidth with moderate GPU utilization indicates memory-bound workloads requiring model optimization.",
      "current_caption": "**GPU Utilization Patterns.** Different utilization signatures require different optimizations. High GPU utilization with low memory bandwidth suggests compute-bound workloads that benefit from parallelism. High memory bandwidth with moderate GPU utilization indicates memory-bound workloads requiring model optimization.",
      "new_caption": "",
      "type": "table",
      "source_file": "contents/vol1/ops/ops.qmd"
    },
    "tbl-monitoring-cost-components": {
      "original_caption": "**Monitoring Cost Components.** Costs scale differently across components. Metric ingestion scales with cardinality (number of unique metric series), while storage scales with retention. Query costs scale with dashboard usage patterns.",
      "current_caption": "**Monitoring Cost Components.** Costs scale differently across components. Metric ingestion scales with cardinality (number of unique metric series), while storage scales with retention. Query costs scale with dashboard usage patterns.",
      "new_caption": "",
      "type": "table",
      "source_file": "contents/vol1/ops/ops.qmd"
    },
    "tbl-fairness-metrics-summary": {
      "original_caption": "**Fairness Metrics Summary**: Comparison of fairness metrics across demographic groups reveals substantial disparities in how the model treats qualified applicants from each group.",
      "current_caption": "**Fairness Metrics Summary**: Comparison of fairness metrics across demographic groups reveals substantial disparities in how the model treats qualified applicants from each group.",
      "new_caption": "",
      "type": "table",
      "source_file": "contents/vol1/responsible_engr/responsible_engr.qmd"
    },
    "tbl-incident-response": {
      "original_caption": "**Incident Response Framework**: Systematic preparation for ML system failures requires five distinct components. Detection identifies anomalies through specialized monitoring; assessment evaluates scope using severity classifications; mitigation reduces harm through tested rollback procedures; communication notifies stakeholders through pre-approved channels; remediation implements permanent fixes through root cause analysis. Each component requires both operational requirements and pre-deployment verification.",
      "current_caption": "**Incident Response Framework**: Systematic preparation for ML system failures requires five distinct components. Detection identifies anomalies through specialized monitoring; assessment evaluates scope using severity classifications; mitigation reduces harm through tested rollback procedures; communication notifies stakeholders through pre-approved channels; remediation implements permanent fixes through root cause analysis. Each component requires both operational requirements and pre-deployment verification.",
      "new_caption": "",
      "type": "table",
      "source_file": "contents/vol1/responsible_engr/responsible_engr.qmd"
    },
    "tbl-model-card-example": {
      "original_caption": "**Example Model Card: MobileNetV2 for Edge Deployment**: Abstract model card categories translate to practical documentation that guides responsible deployment decisions.",
      "current_caption": "**Example Model Card: MobileNetV2 for Edge Deployment**: Abstract model card categories translate to practical documentation that guides responsible deployment decisions.",
      "new_caption": "",
      "type": "table",
      "source_file": "contents/vol1/responsible_engr/responsible_engr.qmd"
    },
    "tbl-model-efficiency-comparison": {
      "original_caption": "**Model Efficiency Comparison**: Model selection must account for deployment constraints. Larger models provide better accuracy but require more power and time. The smallest model that meets accuracy requirements minimizes both cost and environmental impact.",
      "current_caption": "**Model Efficiency Comparison**: Model selection must account for deployment constraints. Larger models provide better accuracy but require more power and time. The smallest model that meets accuracy requirements minimizes both cost and environmental impact.",
      "new_caption": "",
      "type": "table",
      "source_file": "contents/vol1/responsible_engr/responsible_engr.qmd"
    },
    "tbl-confusion-group-a": {
      "original_caption": "**Confusion Matrix for Group A (Majority)**: Loan approval outcomes for 10,000 applicants from the majority demographic group. The 90% true positive rate (4,500 approved of 5,000 qualified) and 20% false positive rate establish the baseline for fairness comparison.",
      "current_caption": "**Confusion Matrix for Group A (Majority)**: Loan approval outcomes for 10,000 applicants from the majority demographic group. The 90% true positive rate (4,500 approved of 5,000 qualified) and 20% false positive rate establish the baseline for fairness comparison.",
      "new_caption": "",
      "type": "table",
      "source_file": "contents/vol1/responsible_engr/responsible_engr.qmd"
    },
    "tbl-explainability-requirements": {
      "original_caption": "**Explainability Requirements by Domain**: Different applications require different levels of decision transparency. Credit and medical applications face regulatory requirements for individual explanations. Fraud detection may intentionally limit explainability to prevent gaming. The engineering challenge is matching explainability mechanisms to domain requirements.",
      "current_caption": "**Explainability Requirements by Domain**: Different applications require different levels of decision transparency. Credit and medical applications face regulatory requirements for individual explanations. Fraud detection may intentionally limit explainability to prevent gaming. The engineering challenge is matching explainability mechanisms to domain requirements.",
      "new_caption": "",
      "type": "table",
      "source_file": "contents/vol1/responsible_engr/responsible_engr.qmd"
    },
    "tbl-gender-shades-results": {
      "original_caption": "**Gender Shades Facial Recognition Error Rates**: Disaggregated evaluation reveals that aggregate accuracy metrics conceal severe performance disparities. Systems that appear highly accurate overall show error rates varying by more than 40× across demographic groups. Source: @buolamwini2018gender.",
      "current_caption": "**Gender Shades Facial Recognition Error Rates**: Disaggregated evaluation reveals that aggregate accuracy metrics conceal severe performance disparities. Systems that appear highly accurate overall show error rates varying by more than 40× across demographic groups. Source: @buolamwini2018gender.",
      "new_caption": "",
      "type": "table",
      "source_file": "contents/vol1/responsible_engr/responsible_engr.qmd"
    },
    "tbl-fairness-archetype": {
      "original_caption": "**Fairness Risk by ML Archetype**: Fairness risks vary by archetype's data source and deployment context.",
      "current_caption": "**Fairness Risk by ML Archetype**: Fairness risks vary by archetype's data source and deployment context.",
      "new_caption": "",
      "type": "table",
      "source_file": "contents/vol1/responsible_engr/responsible_engr.qmd"
    },
    "tbl-failure-modes": {
      "original_caption": "**ML System Failure Mode Taxonomy**: Different failure modes require different detection strategies and remediation approaches. Silent failures such as data quality issues, distribution shift, and fairness violations demand proactive monitoring because they do not trigger traditional alerts.",
      "current_caption": "**ML System Failure Mode Taxonomy**: Different failure modes require different detection strategies and remediation approaches. Silent failures such as data quality issues, distribution shift, and fairness violations demand proactive monitoring because they do not trigger traditional alerts.",
      "new_caption": "",
      "type": "table",
      "source_file": "contents/vol1/responsible_engr/responsible_engr.qmd"
    },
    "tbl-edge-deployment-constraints": {
      "original_caption": "**Edge Deployment Constraints**: Power and latency requirements across four deployment contexts. Smartphones allow 3W and 100ms latency for photo enhancement and voice assistants. IoT sensors operate at 100mW with 1-second tolerance for anomaly detection. Embedded cameras require 1W at 33ms (30 FPS) for real-time object detection. Wearables budget 500mW with 500ms latency for health monitoring. These concrete constraints transform abstract efficiency discussions into engineering requirements.",
      "current_caption": "**Edge Deployment Constraints**: Power and latency requirements across four deployment contexts. Smartphones allow 3W and 100ms latency for photo enhancement and voice assistants. IoT sensors operate at 100mW with 1-second tolerance for anomaly detection. Embedded cameras require 1W at 33ms (30 FPS) for real-time object detection. Wearables budget 500mW with 500ms latency for health monitoring. These concrete constraints transform abstract efficiency discussions into engineering requirements.",
      "new_caption": "",
      "type": "table",
      "source_file": "contents/vol1/responsible_engr/responsible_engr.qmd"
    },
    "tbl-pre-deployment-assessment": {
      "original_caption": "**Pre-Deployment Assessment Framework**: Critical Path items block deployment until addressed. High Priority items should be completed before or shortly after launch. Systematic coverage of responsibility concerns throughout the ML lifecycle prevents overlooked risks.",
      "current_caption": "**Pre-Deployment Assessment Framework**: Critical Path items block deployment until addressed. High Priority items should be completed before or shortly after launch. Systematic coverage of responsibility concerns throughout the ML lifecycle prevents overlooked risks.",
      "new_caption": "",
      "type": "table",
      "source_file": "contents/vol1/responsible_engr/responsible_engr.qmd"
    },
    "tbl-confusion-group-b": {
      "original_caption": "**Confusion Matrix for Group B (Minority)**: Loan approval outcomes for 2,000 applicants from the minority demographic group. The 60% true positive rate (600 approved of 1,000 qualified) reveals a 30 percentage point disparity compared to Group A, indicating the model applies stricter criteria to minority applicants.",
      "current_caption": "**Confusion Matrix for Group B (Minority)**: Loan approval outcomes for 2,000 applicants from the minority demographic group. The 60% true positive rate (600 approved of 1,000 qualified) reveals a 30 percentage point disparity compared to Group A, indicating the model applies stricter criteria to minority applicants.",
      "new_caption": "",
      "type": "table",
      "source_file": "contents/vol1/responsible_engr/responsible_engr.qmd"
    },
    "tbl-twelve-principles": {
      "original_caption": "**The Twelve Quantitative Invariants of ML Systems Engineering.** Each invariant was introduced in the Part where its governing constraint first becomes visible. Together, they form the complete analytical framework for reasoning about ML system design, optimization, and deployment. The meta-principle that unifies them all is the Conservation of Complexity: you cannot destroy complexity, only move it between Data, Algorithm, and Machine.",
      "current_caption": "**The Twelve Quantitative Invariants of ML Systems Engineering.** Each invariant was introduced in the Part where its governing constraint first becomes visible. Together, they form the complete analytical framework for reasoning about ML system design, optimization, and deployment. The meta-principle that unifies them all is the Conservation of Complexity: you cannot destroy complexity, only move it between Data, Algorithm, and Machine.",
      "new_caption": "",
      "type": "table",
      "source_file": "contents/vol1/conclusion/conclusion.qmd"
    },
    "tbl-lighthouse-journey-mobilenet": {
      "original_caption": "**The Lighthouse Journey (MobileNetV2)**: Tracing one model through the entire systems stack reveals how decisions in one domain (e.g., architecture) propagate constraints and opportunities to every other domain (e.g., hardware acceleration and monitoring).",
      "current_caption": "**The Lighthouse Journey (MobileNetV2)**: Tracing one model through the entire systems stack reveals how decisions in one domain (e.g., architecture) propagate constraints and opportunities to every other domain (e.g., hardware acceleration and monitoring).",
      "new_caption": "",
      "type": "table",
      "source_file": "contents/vol1/conclusion/conclusion.qmd"
    }
  },
  "listings": {},
  "metadata": {
    "creation_time": "2026-02-02T07:59:16.140043",
    "source": "qmd_direct_scan",
    "directories": [
      "contents/vol1/"
    ],
    "qmd_files_scanned": 25,
    "extraction_stats": {
      "figures_found": 206,
      "tables_found": 167,
      "listings_found": 0,
      "markdown_figures": 27,
      "tikz_figures": 147,
      "r_figures": 0,
      "code_figures": 32,
      "extraction_failures": 152,
      "failed_extractions": [
        "tbl-ai-evolution-performance",
        "tbl-lighthouse-workflow-comparison",
        "lst-etl-elt-cost-comparison",
        "lst-data-expectations",
        "lst-delta-time-travel",
        "lst-dvc-workflow",
        "tbl-historical-performance",
        "fig-cnn",
        "fig-example-skip-connection",
        "fig-rnn",
        "tbl-normalization-comparison",
        "lst-self_attention_layer",
        "lst-attention_layer_compute",
        "lst-mlp_layer_matrix",
        "lst-rnn_layer_step",
        "lst-conv_layer_compute",
        "lst-rnn_layer_compute",
        "lst-mlp_layer_compute",
        "lst-conv_layer_spatial",
        "lst-pipeline-parallelism-streams",
        "lst-tf-static-graph",
        "lst-iterable-dataset",
        "lst-torch-compile-benchmark",
        "lst-gradient-accumulation",
        "lst-autocast-usage",
        "lst-dataloader-throughput",
        "lst-torchscript-script",
        "lst-nsight-compute",
        "lst-jax-transformations",
        "lst-training-step-anatomy",
        "lst-state-dict-interface",
        "lst-device-placement-reuse",
        "lst-cuda-device-context",
        "lst-retain-graph",
        "lst-custom-autograd-function",
        "lst-gradient-hooks",
        "lst-forward_mode_dual",
        "lst-nsight-systems",
        "lst-tracing-silent-failure",
        "lst-feature_importance",
        "lst-reverse_memory",
        "lst-nested_modules",
        "lst-torchscript-ir",
        "lst-graph-break-control-flow",
        "lst-framework-hello-world",
        "lst-auto_diff_intro",
        "lst-pytorch-profiler",
        "lst-checkpoint_scheme",
        "lst-torchscript-trace",
        "lst-parameter_registration",
        "lst-higher_order",
        "lst-ad_interface",
        "lst-state_dict",
        "lst-autograd-tape-example",
        "lst-overlap-compute-transfer",
        "lst-parameter_freezing",
        "lst-deep_memory",
        "lst-graph-break-io",
        "lst-cuda-events",
        "lst-module_state",
        "lst-graph-break-shapes",
        "lst-reverse_simple",
        "lst-module_hooks",
        "lst-checkpoint-save-load",
        "lst-graph-break-detect",
        "lst-bf16-training",
        "lst-torchscript-conditional",
        "lst-consistent-device-placement",
        "lst-torch-compile-intro",
        "lst-reverse_forward",
        "lst-parallel_ad",
        "lst-tensor-device-placement",
        "lst-forward_structure",
        "lst-grad-fn-chain",
        "lst-sync-patterns",
        "lst-reverse_simple_nn",
        "lst-custom-collate-fn",
        "lst-torchscript-restrictions",
        "lst-forward_mode_ad",
        "lst-module-to-recursive",
        "lst-train_loop",
        "lst-map-style-dataset",
        "lst-image_sensitivity",
        "lst-device-mismatch-error",
        "lst-detach-vs-data",
        "lst-forward_trace",
        "lst-adam-training",
        "lst-dataloader_usage",
        "lst-adam-internals",
        "lst-mixed-precision",
        "lst-param_update",
        "lst-flash-attention-comparison",
        "lst-gelu-approx",
        "lst-cosine-annealing",
        "lst-gradient-accumulation-loop",
        "lst-flash-attention-migration",
        "lst-el2n-coreset",
        "lst-elementwise-fusion",
        "lst-conv-bn-relu-fusion",
        "lst-attention-fusion",
        "lst-qat-scale-adaptation",
        "lst-qat_example",
        "lst-graph-pattern-matching",
        "lst-pytorch_pruning",
        "lst-pruning_example",
        "lst-fake-quantize-autograd",
        "lst-quantization_example",
        "lst-qat-conv-forward",
        "lst-gemm-fusion",
        "lst-qat-batch-norm",
        "tbl-fusion-benefits",
        "tbl-memory-footprint",
        "lst-matmul_data_movement",
        "lst-linear_matrix_hierarchy",
        "lst-sfu_vector_ops",
        "lst-output_stationary",
        "lst-dense_layer_def",
        "lst-riscv_vector_mac",
        "lst-input_stationary",
        "lst-nonlinear_layer",
        "lst-matrix_unit",
        "lst-loop_blocking",
        "lst-traditional_overhead",
        "lst-tiled_matmul",
        "lst-nonlinear_math",
        "lst-weight_stationary",
        "lst-naive_execution",
        "lst-loop_level_dense",
        "lst-linear_layer_highlevel",
        "lst-arm_sve_vector",
        "lst-dense_expansion",
        "lst-naive_matmul",
        "lst-linear_math_internal",
        "lst-matrix_patterns",
        "lst-tensor_core_op",
        "lst-cuda_simt",
        "lst-loop_linear_layer",
        "tbl-batching-throughput",
        "tbl-resolution-bottleneck",
        "lst-adaptive-batching",
        "lst-resnet-postprocessing",
        "lst-ops-schema-validation",
        "lst-ops-triggered-retraining",
        "lst-ops-validate-skew",
        "lst-feature-store-consistency",
        "lst-ops-shap-debugging",
        "lst-ops-freshness-alert",
        "tbl-tco-summary",
        "tbl-tco-training",
        "tbl-tco-operations",
        "tbl-tco-inference",
        "lst-fairness-metrics-code"
      ],
      "files_with_issues": [
        "contents/vol1/introduction/introduction.qmd",
        "contents/vol1/workflow/workflow.qmd",
        "contents/vol1/data_engineering/data_engineering.qmd",
        "contents/vol1/dl_primer/dl_primer.qmd",
        "contents/vol1/dnn_architectures/dnn_architectures.qmd",
        "contents/vol1/frameworks/frameworks.qmd",
        "contents/vol1/training/training.qmd",
        "contents/vol1/data_selection/data_selection.qmd",
        "contents/vol1/optimizations/model_compression.qmd",
        "contents/vol1/hw_acceleration/hw_acceleration.qmd",
        "contents/vol1/serving/serving.qmd",
        "contents/vol1/ops/ops.qmd",
        "contents/vol1/responsible_engr/responsible_engr.qmd"
      ]
    }
  }
}