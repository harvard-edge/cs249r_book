% Options for packages loaded elsewhere
% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode,linktoc=all,pdfwindowui,pdfpagemode=FullScreen,pdfpagelayout=TwoPageRight}{hyperref}
\PassOptionsToPackage{hyphens}{url}
\PassOptionsToPackage{dvipsnames,svgnames,x11names}{xcolor}
%
\documentclass[
  9pt,
  letterpaper,
  abstract,
  titlepage]{scrbook}
\usepackage{xcolor}
\usepackage[left=1in,marginparwidth=2.0666666666667in,textwidth=4.1333333333333in,marginparsep=0.3in]{geometry}
\usepackage{amsmath,amssymb}
\setcounter{secnumdepth}{3}
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math} % this also loads fontspec
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
\usepackage{lmodern}
\ifPDFTeX\else
  % xetex/luatex font selection
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
% Make \paragraph and \subparagraph free-standing
\makeatletter
\ifx\paragraph\undefined\else
  \let\oldparagraph\paragraph
  \renewcommand{\paragraph}{
    \@ifstar
      \xxxParagraphStar
      \xxxParagraphNoStar
  }
  \newcommand{\xxxParagraphStar}[1]{\oldparagraph*{#1}\mbox{}}
  \newcommand{\xxxParagraphNoStar}[1]{\oldparagraph{#1}\mbox{}}
\fi
\ifx\subparagraph\undefined\else
  \let\oldsubparagraph\subparagraph
  \renewcommand{\subparagraph}{
    \@ifstar
      \xxxSubParagraphStar
      \xxxSubParagraphNoStar
  }
  \newcommand{\xxxSubParagraphStar}[1]{\oldsubparagraph*{#1}\mbox{}}
  \newcommand{\xxxSubParagraphNoStar}[1]{\oldsubparagraph{#1}\mbox{}}
\fi
\makeatother


\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}\usepackage{longtable,booktabs,array}
\usepackage{calc} % for calculating minipage widths
% Correct order of tables after \paragraph or \subparagraph
\usepackage{etoolbox}
\makeatletter
\patchcmd\longtable{\par}{\if@noskipsec\mbox{}\fi\par}{}{}
\makeatother
% Allow footnotes in longtable head/foot
\IfFileExists{footnotehyper.sty}{\usepackage{footnotehyper}}{\usepackage{footnote}}
\makesavenoteenv{longtable}
\usepackage{graphicx}
\makeatletter
\newsavebox\pandoc@box
\newcommand*\pandocbounded[1]{% scales image to fit in text height/width
  \sbox\pandoc@box{#1}%
  \Gscale@div\@tempa{\textheight}{\dimexpr\ht\pandoc@box+\dp\pandoc@box\relax}%
  \Gscale@div\@tempb{\linewidth}{\wd\pandoc@box}%
  \ifdim\@tempb\p@<\@tempa\p@\let\@tempa\@tempb\fi% select the smaller of both
  \ifdim\@tempa\p@<\p@\scalebox{\@tempa}{\usebox\pandoc@box}%
  \else\usebox{\pandoc@box}%
  \fi%
}
% Set default figure placement to htbp
\def\fps@figure{htbp}
\makeatother
% definitions for citeproc citations
\NewDocumentCommand\citeproctext{}{}
\NewDocumentCommand\citeproc{mm}{%
  \begingroup\def\citeproctext{#2}\cite{#1}\endgroup}
\makeatletter
 % allow citations to break across lines
 \let\@cite@ofmt\@firstofone
 % avoid brackets around text for \cite:
 \def\@biblabel#1{}
 \def\@cite#1#2{{#1\if@tempswa , #2\fi}}
\makeatother
\newlength{\cslhangindent}
\setlength{\cslhangindent}{1.5em}
\newlength{\csllabelwidth}
\setlength{\csllabelwidth}{3em}
\newenvironment{CSLReferences}[2] % #1 hanging-indent, #2 entry-spacing
 {\begin{list}{}{%
  \setlength{\itemindent}{0pt}
  \setlength{\leftmargin}{0pt}
  \setlength{\parsep}{0pt}
  % turn on hanging indent if param 1 is 1
  \ifodd #1
   \setlength{\leftmargin}{\cslhangindent}
   \setlength{\itemindent}{-1\cslhangindent}
  \fi
  % set entry spacing
  \setlength{\itemsep}{#2\baselineskip}}}
 {\end{list}}
\usepackage{calc}
\newcommand{\CSLBlock}[1]{\hfill\break\parbox[t]{\linewidth}{\strut\ignorespaces#1\strut}}
\newcommand{\CSLLeftMargin}[1]{\parbox[t]{\csllabelwidth}{\strut#1\strut}}
\newcommand{\CSLRightInline}[1]{\parbox[t]{\linewidth - \csllabelwidth}{\strut#1\strut}}
\newcommand{\CSLIndent}[1]{\hspace{\cslhangindent}#1}

% =============================================================================
% LATEX HEADER CONFIGURATION FOR MLSYSBOOK PDF
% =============================================================================
% This file contains all LaTeX package imports, custom commands, and styling
% definitions for the PDF output of the Machine Learning Systems textbook.
%
% Key Features:
% - Harvard crimson branding throughout
% - Custom part/chapter/section styling
% - Professional table formatting with colored headers
% - Margin notes with custom styling
% - TikZ-based part dividers
% - Page numbering (Roman for frontmatter, Arabic for mainmatter)
%
% Note: This file is included via _quarto-pdf.yml and affects PDF output only.
% HTML/EPUB styling is handled separately via CSS files.
% =============================================================================

% =============================================================================
% PACKAGE IMPORTS
% =============================================================================

% Layout and positioning
% \usepackage[outercaption, ragged]{sidecap}  % Commented out to make figure captions inline instead of in margin
\usepackage{adjustbox}      % Adjusting box dimensions
\usepackage{afterpage}      % Execute commands after page break
\usepackage{morefloats}     % Increase number of floats
\usepackage{array}          % Enhanced table column formatting
\usepackage{atbegshi}       % Insert content at page beginning
%\usepackage{changepage}     % Change page dimensions mid-document
\usepackage{emptypage}      % Clear headers/footers on empty pages

% Language and text
\usepackage[english]{babel} % English language support
\usepackage{microtype}      % Improved typography and hyphenation

% Captions and floats
\usepackage{caption}
% Caption styling configuration
%\captionsetup[table]{belowskip=5pt}
\captionsetup{format=plain}
\DeclareCaptionLabelFormat{mylabel}{#1
#2:\hspace{1.0ex}}
\DeclareCaptionFont{ninept}{\fontsize{7pt}{8}\selectfont #1}

% Figure captions: Small font, bold label, ragged right
\captionsetup[figure]{labelfont={bf,ninept},labelsep=space,
belowskip=2pt,aboveskip=6pt,labelformat=mylabel,
justification=raggedright,singlelinecheck=false,font={ninept}}

% Table captions: Small font, bold label, ragged right
\captionsetup[table]{belowskip=6pt,labelfont={bf,ninept},labelsep=none,
labelformat=mylabel,justification=raggedright,singlelinecheck=false,font={ninept}}

% Typography fine-tuning
\emergencystretch=5pt       % Allow extra stretch to avoid overfull boxes

% Utility packages
\usepackage{etoolbox}       % For patching commands and environments

% Page layout and headers
\usepackage{fancyhdr}       % Custom headers and footers
\usepackage{geometry}       % Page dimensions and margins

% Graphics and figures
\usepackage{graphicx}       % Include graphics
\usepackage{float}          % Improved float placement
\usepackage[skins,breakable]{tcolorbox} % Coloured and framed text boxes
\tcbset{before upper=\setlength{\parskip}{3pt}}

% Tables
\usepackage{longtable}      % Multi-page tables

% Fonts and typography
\usepackage{fontspec}       % Font selection for LuaLaTeX
\usepackage{mathptmx}       % Times-like math fonts
\usepackage{newpxtext}      % Palatino-like font for body text

% Colors and visual elements
\usepackage[dvipsnames]{xcolor}  % Extended color support
\usepackage{tikz}           % Programmatic graphics
\usetikzlibrary{positioning}
\usetikzlibrary{calc}
\usepackage{tikzpagenodes}  % TikZ positioning relative to page

% Code listings
\usepackage{listings}       % Code highlighting

% Hyperlinks
\usepackage{hyperref}       % Clickable links in PDF

% Conditional logic
\usepackage{ifthen}         % If-then-else commands

% Math symbols
\usepackage{amsmath}        % AMS math extensions
\usepackage{amssymb}        % AMS math symbols
\usepackage{latexsym}       % Additional LaTeX symbols
\usepackage{pifont}         % Zapf Dingbats symbols
\providecommand{\blacklozenge}{\ding{117}}  % Black diamond symbol

% Lists
\usepackage{enumitem}       % Customizable lists

% Margin notes and sidenotes
\usepackage{marginfix}      % Fixes margin note overflow
\usepackage{marginnote}     % Margin notes
\usepackage{sidenotes}      % Academic-style sidenotes
\renewcommand\raggedrightmarginnote{\sloppy}
\renewcommand\raggedleftmarginnote{\sloppy}

% Typography improvements
\usepackage{ragged2e}       % Better ragged text
\usepackage[all]{nowidow}   % Prevent widows and orphans
\usepackage{needspace}      % Ensure minimum space on page

% Section formatting
\usepackage[explicit]{titlesec}  % Custom section titles
\usepackage{tocloft}        % Table of contents formatting

% QR codes and icons
\usepackage{fontawesome5}   % Font Awesome icons
\usepackage{qrcode}         % QR code generation
\qrset{link, height=15mm}

% =============================================================================
% FLOAT CONFIGURATION
% =============================================================================
% Allow more floats per page to handle figure-heavy chapters
\extrafloats{100}
\setcounter{topnumber}{10}       % Max floats at top of page
\setcounter{bottomnumber}{10}    % Max floats at bottom of page
\setcounter{totalnumber}{20}     % Max floats per page
\renewcommand{\topfraction}{.9}  % Max fraction of page for top floats
\renewcommand{\bottomfraction}{.9}
\renewcommand{\textfraction}{.1}  % Min fraction of page for text
\renewcommand{\floatpagefraction}{.8}  % Min fraction of float page

% =============================================================================
% COLOR DEFINITIONS
% =============================================================================
% Harvard crimson - primary brand color used throughout
\definecolor{crimson}{HTML}{A51C30}

% Quiz element colors
\definecolor{quiz-question-color1}{RGB}{225,243,248}  % Light blue background
\definecolor{quiz-question-color2}{RGB}{17,158,199}   % Blue border
\definecolor{quiz-answer-color1}{RGB}{250,234,241}    % Light pink background
\definecolor{quiz-answer-color2}{RGB}{152,14,90}      % Magenta border

% =============================================================================
% LIST FORMATTING
% =============================================================================
% Tighter list spacing for academic style
\def\tightlist{}
\setlist{itemsep=1pt, parsep=1pt, topsep=0pt,after={\vspace{0.3\baselineskip}}}
\let\tightlist\relax

\makeatletter
\@ifpackageloaded{framed}{}{\usepackage{framed}}
\@ifpackageloaded{fancyvrb}{}{\usepackage{fancyvrb}}
\makeatother

\makeatletter
%New float "codelisting" has been updated
\AtBeginDocument{%
\floatstyle{ruled}
\newfloat{codelisting}{!htb}{lop}
\floatname{codelisting}{Listing}
\floatplacement{codelisting}{!htb}
\captionsetup[codelisting]{labelfont={bf,ninept},labelformat=mylabel,
  singlelinecheck=false,width=\linewidth,labelsep=none,font={ninept}}%
\renewenvironment{snugshade}{%
   \def\OuterFrameSep{3pt}%
   \def\FrameCommand{\fboxsep=5pt\colorbox{shadecolor}}%
   \MakeFramed{\advance\hsize-\width\FrameRestore}%
   \leftskip 0.5em \rightskip 0.5em%
   \small% decrease font size
   }{\endMakeFramed}%
}
\makeatother

%The space before and after the verbatim environment "Highlighting" has been reduced
\fvset{listparameters=\setlength{\topsep}{0pt}\setlength{\partopsep}{0pt}}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{framesep=0mm,commandchars=\\\{\}}

\makeatletter
\renewcommand\fs@ruled{\def\@fs@cfont{\bfseries}\let\@fs@capt\floatc@ruled
\def\@fs@pre{\hrule height.8pt depth0pt \kern2pt}%
\def\@fs@post{\kern2pt\hrule\relax}%
\def\@fs@mid{\kern2pt\hrule\kern1pt}%space between float and caption
\let\@fs@iftopcapt\iftrue}
\makeatother


% =============================================================================
% HYPHENATION RULES
% =============================================================================
% Explicit hyphenation points for technical terms to avoid bad breaks
\hyphenation{
  light-weight
  light-weight-ed
  de-vel-op-ment
  un-der-stand-ing
  mod-els
  prin-ci-ples
  ex-per-tise
  com-pli-cat-ed
  blue-print
  per‧for‧mance
  com-mu-ni-ca-tion
  par-a-digms
  hy-per-ten-sion
  a-chieved
}

% =============================================================================
% CODE LISTING CONFIGURATION
% =============================================================================
% Settings for code blocks using listings package
\lstset{
breaklines=true,              % Automatic line wrapping
breakatwhitespace=true,       % Break at whitespace only
basicstyle=\ttfamily,         % Monospace font
frame=none,                   % No frame around code
keepspaces=true,              % Preserve spaces
showspaces=false,             % Don't show space characters
showtabs=false,               % Don't show tab characters
columns=flexible,             % Flexible column width
belowskip=0pt,               % Minimal spacing
aboveskip=0pt
}

% =============================================================================
% PAGE GEOMETRY
% =============================================================================
% Book dimensions optimized for print (7.5" × 9.25")
% Wide outer margin for sidenotes (1.5")
\geometry{
  paperwidth=7.5in,
  paperheight=9.25in,
  top=1in,
  bottom=1in,
  inner=1in,                  % Inner margin (binding side)
  outer=2.25in,               % Outer margin (includes 1.5" for sidenotes)
  footskip=30pt,
  marginparwidth=1.5in,       % Width for margin notes
  twoside                     % Different left/right pages
}

% =============================================================================
% SIDENOTE STYLING
% =============================================================================
% Custom sidenote design with crimson vertical bar
\renewcommand{\thefootnote}{\textcolor{crimson}{\arabic{footnote}}}

% Save original sidenote command
\makeatletter
\@ifundefined{oldsidenote}{
  \let\oldsidenote\sidenote%
}{}
\makeatother

% Redefine sidenote with vertical crimson bar
\renewcommand{\sidenote}[1]{%
  \oldsidenote{%
    \noindent
    \color{crimson!100}                        % Crimson vertical line
    \raisebox{0em}{%
      \rule{0.5pt}{1.5em}                      % Thin vertical line
    }
    \hspace{0.3em}                             % Space after line
    \color{black}                              % Reset text color
    \footnotesize #1                           % Sidenote content
  }%
}

% =============================================================================
% FIGURE ENVIRONMENT FIX
% =============================================================================
% Fix for caption display bug on even pages
\makeatletter
\let\oldfigure\figure%
\let\endoldfigure\endfigure%
\renewenvironment{figure}[1][htbp]{%
  \oldfigure[#1]%
}{%
  \endoldfigure%
}
\makeatother

% =============================================================================
% PAGE HEADERS AND FOOTERS
% =============================================================================
% Ensure chapters use fancy page style (not plain)
\patchcmd{\chapter}{\thispagestyle{plain}}{\thispagestyle{fancy}}{}{}

% Main page style with crimson headers
\pagestyle{fancy}
\fancyhf{}                                              % Clear all
\fancyhead[LE]{\small\color{crimson}\nouppercase{\rightmark}}  % Left even: section
\fancyhead[RO]{\color{crimson}\thepage}                 % Right odd: page number
\fancyhead[LO]{\small\color{crimson}\nouppercase{\leftmark}}   % Left odd: chapter
\fancyhead[RE]{\color{crimson}\thepage}                 % Right even: page number
\renewcommand{\headrulewidth}{0.4pt}                    % Thin header line
\renewcommand{\footrulewidth}{0pt}                      % No footer line

% Plain page style (for chapter openings)
\fancypagestyle{plain}{
  \fancyhf{}
  \fancyfoot[C]{\color{crimson}\thepage}                % Centered page number
  \renewcommand{\headrulewidth}{0pt}
  \renewcommand{\footrulewidth}{0pt}
}

% =============================================================================
% KOMA-SCRIPT FONT ADJUSTMENTS
% =============================================================================
% Apply crimson color to all heading levels
\addtokomafont{disposition}{\rmfamily\color{crimson}}
\addtokomafont{chapter}{\color{crimson}}
\addtokomafont{section}{\color{crimson}}
\addtokomafont{subsection}{\color{crimson}}

% =============================================================================
% ABSTRACT ENVIRONMENT
% =============================================================================
\newenvironment{abstract}{
  \chapter*{\abstractname}
  \addcontentsline{toc}{chapter}{\abstractname}
  \small
}{
  \clearpage
}

% =============================================================================
% HYPERLINK CONFIGURATION
% =============================================================================
% Crimson-colored links throughout, two-page PDF layout
\hypersetup{
  linkcolor=crimson,
  citecolor=crimson,
  urlcolor=crimson,
  pdfpagelayout=TwoPageRight,   % Two-page spread view
  pdfstartview=Fit               % Initial zoom fits page
}

% =============================================================================
% PART SUMMARY SYSTEM
% =============================================================================
% Allows adding descriptive text below part titles
\newcommand{\partsummary}{}     % Empty by default
\newif\ifhaspartsummary%
\haspartsummaryfalse%

\newcommand{\setpartsummary}[1]{%
  \renewcommand{\partsummary}{#1}%
  \haspartsummarytrue%
}

% Additional colors for part page backgrounds
\definecolor{BrownLL}{RGB}{233,222,220}
\definecolor{BlueDD}{RGB}{62,100,125}
\colorlet{BlueDD}{magenta}

% ===============================================================================
% PART STYLING SYSTEM
% ===============================================================================
%
% This system provides three distinct visual styles for book organization:
%
% 1. NUMBERED PARTS (\part{title}) - For main book sections
%    - Roman numerals (I, II, III, etc.) in top right corner
%    - Crimson title with horizontal lines above/below
%    - "Part I" label in sidebar
%    - Used for: foundations, principles, optimization, deployment, etc.
%
% 2. UNNUMBERED PARTS (\part*{title}) - For special sections like "Labs"
%    - Division-style geometric background (left side)
%    - No Roman numerals
%    - Used for: labs section
%
% 3. DIVISIONS (\division{title}) - For major book divisions
%    - Clean geometric background with centered title
%    - Used for: frontmatter, main_content, backmatter
%
% The Lua filter (inject-parts.lua) automatically routes parts by {key:xxx} commands
% to the appropriate LaTeX command based on the key name.
% ===============================================================================

% NUMBERED PARTS: Roman numeral styling for main book sections
\titleformat{\part}[display]
{\thispagestyle{empty}}{}{20pt}{
\begin{tikzpicture}[remember picture,overlay]
%%%
%%
\node[crimson,align=flush right,
inner sep=0,outer sep=0mm,draw=none,%
anchor=east,minimum height=31mm, text width=1.2\textwidth,
yshift=-30mm,font={%
\fontsize{98pt}{104}\selectfont\bfseries}]  (BG) at (current page text area.north east){\thepart};
%
\node[black,inner sep=0mm,draw=none,
anchor=mid,text width=1.2\textwidth,
 minimum height=35mm, align=right,
node distance=7mm,below=of BG,
font={\fontsize{30pt}{34}\selectfont}]
(BGG)  {\hyphenchar\font=-1 \color{black}\MakeUppercase {#1}};
\draw [crimson,line width=3pt] ([yshift=0mm]BGG.north west) -- ([yshift=0mm]BGG.north east);
\draw [crimson,line width=2pt] ([yshift=0mm]BGG.south west) -- ([yshift=0mm]BGG.south east);
%
\node[fill=crimson,text=white,rotate=90,%
anchor=south west,minimum height=15mm,
minimum width=40mm,font={%
\fontsize{20pt}{20}\selectfont\bfseries}](BP)  at
(current page text area.south east)
{{\sffamily Part}~\thepart};
%
\path[red](BP.north west)-|coordinate(PS)(BGG.south west);
%
\ifhaspartsummary
\node[inner sep=4pt,text width=0.7\textwidth,draw=none,fill=BrownLL!40,
align=justify,font={\fontsize{9pt}{12}\selectfont},anchor=south west]
at (PS) {\partsummary};
\fi
\end{tikzpicture}
}[]

\renewcommand{\thepart}{\Roman{part}}

% UNNUMBERED PARTS: Division-style background for special sections
\titleformat{name=\part,numberless}[display]
{\thispagestyle{empty}}{}{20pt}{
\begin{tikzpicture}[remember picture,overlay]
%%%
\coordinate(S1)at([yshift=-200mm]current page.north west);
\draw[draw=none,fill=BlueDD!7](S1)--++(45:16)coordinate(S2)-
|(S2|-current page.north west)--(current page.north west)coordinate(S3)--(S1);
%
\coordinate(E1)at([yshift=-98mm]current page.north west);
\draw[draw=none,fill=BlueDD!15](E1)--(current page.north west)coordinate(E2)
--++(0:98mm)coordinate(E3)--(E1);
%
\coordinate(D1)at([yshift=15mm]current page.south west);
\draw[draw=none,fill=BlueDD!40,opacity=0.5](D1)--++(45:5.5)coordinate(D2)
-|(D2|-current page.north west)--(current page.north west)coordinate(D3)--(D1);
%%%%
\path[red](S2)-|(S2-|current page.east)coordinate(SS2);
%PART
\node[crimson,align=flush right,inner sep=0,outer sep=0mm,draw=none,anchor=south,
font={\fontsize{48pt}{48}\selectfont\bfseries}]  (BG) at ($(S2)!0.5!(SS2)$){\hphantom{Part}};
%%%
\path[green]([yshift=15mm]D2)-|coordinate(TPD)(BG.south east);
\node[inner sep=0mm,draw=none,anchor=south east,%text width=0.9\textwidth,
align=right,font={\fontsize{40pt}{40}\selectfont}]
(BGG) at (TPD)  {\color{crimson}\MakeUppercase {#1}};%\MakeUppercase {}
\end{tikzpicture}
}

% Define \numberedpart command for numbered parts
\newcommand{\numberedpart}[1]{%
\clearpage
\thispagestyle{empty}
\stepcounter{part}%
\begin{tikzpicture}[remember picture,overlay]
%%%
%%
\node[crimson,align=flush right,
inner sep=0,outer sep=0mm,draw=none,%
anchor=east,minimum height=31mm, text width=1.2\textwidth,
yshift=-30mm,font={%
\fontsize{98pt}{104}\selectfont\bfseries}]  (BG) at (current page text area.north east){\thepart};
%
\node[black,inner sep=0mm,draw=none,
anchor=mid,text width=1.2\textwidth,
 minimum height=35mm, align=right,
node distance=7mm,below=of BG,
font={\fontsize{30pt}{34}\selectfont}]
(BGG)  {\hyphenchar\font=-1 \color{black}\MakeUppercase {#1}};
\draw [crimson,line width=3pt] ([yshift=0mm]BGG.north west) -- ([yshift=0mm]BGG.north east);
\draw [crimson,line width=2pt] ([yshift=0mm]BGG.south west) -- ([yshift=0mm]BGG.south east);
%
\node[fill=crimson,text=white,rotate=90,%
anchor=south west,minimum height=15mm,
minimum width=40mm,font={%
\fontsize{20pt}{20}\selectfont\bfseries}](BP)  at
(current page text area.south east)
{{\sffamily Part}~\thepart};
%
\path[red](BP.north west)-|coordinate(PS)(BGG.south west);
%
\ifhaspartsummary
\node[inner sep=4pt,text width=0.7\textwidth,draw=none,fill=BrownLL!40,
align=justify,font={\fontsize{9pt}{12}\selectfont},anchor=south west]
at (PS) {\partsummary};
\fi
\end{tikzpicture}
\clearpage
}



% DIVISIONS: Clean geometric styling with subtle tech elements
% Used for frontmatter, main_content, and backmatter divisions
\newcommand{\division}[1]{%
\clearpage
\thispagestyle{empty}
\begin{tikzpicture}[remember picture,overlay]

% Clean geometric background (original design)
\coordinate(S1)at([yshift=-200mm]current page.north west);
\draw[draw=none,fill=BlueDD!7](S1)--++(45:16)coordinate(S2)-
|(S2|-current page.north west)--(current page.north west)coordinate(S3)--(S1);

\coordinate(E1)at([yshift=-98mm]current page.north west);
\draw[draw=none,fill=BlueDD!15](E1)--(current page.north west)coordinate(E2)
--++(0:98mm)coordinate(E3)--(E1);

\coordinate(D1)at([yshift=15mm]current page.south west);
\draw[draw=none,fill=BlueDD!40,opacity=0.5](D1)--++(45:5.5)coordinate(D2)
-|(D2|-current page.north west)--(current page.north west)coordinate(D3)--(D1);

% Subtle tech elements - positioned in white areas for better visibility
% Upper right white area - more visible
\draw[crimson!40, line width=0.8pt] ([xshift=140mm,yshift=-60mm]current page.north west) -- ++(40mm,0);
\draw[crimson!40, line width=0.8pt] ([xshift=150mm,yshift=-70mm]current page.north west) -- ++(30mm,0);
\draw[crimson!35, line width=0.7pt] ([xshift=160mm,yshift=-60mm]current page.north west) -- ++(0,-15mm);
\draw[crimson!35, line width=0.7pt] ([xshift=170mm,yshift=-70mm]current page.north west) -- ++(0,10mm);

% Circuit nodes - upper right
\fill[crimson!50] ([xshift=160mm,yshift=-60mm]current page.north west) circle (1.5mm);
\fill[white] ([xshift=160mm,yshift=-60mm]current page.north west) circle (0.8mm);
\fill[crimson!50] ([xshift=170mm,yshift=-70mm]current page.north west) circle (1.3mm);
\fill[white] ([xshift=170mm,yshift=-70mm]current page.north west) circle (0.6mm);

% Lower right white area - enhanced visibility
\draw[crimson!45, line width=0.9pt] ([xshift=140mm,yshift=-190mm]current page.north west) -- ++(45mm,0);
\draw[crimson!45, line width=0.9pt] ([xshift=150mm,yshift=-200mm]current page.north west) -- ++(35mm,0);
\draw[crimson!40, line width=0.8pt] ([xshift=160mm,yshift=-190mm]current page.north west) -- ++(0,-20mm);
\draw[crimson!40, line width=0.8pt] ([xshift=170mm,yshift=-200mm]current page.north west) -- ++(0,15mm);

% Additional connecting lines in lower right
\draw[crimson!35, line width=0.7pt] ([xshift=130mm,yshift=-180mm]current page.north west) -- ++(25mm,0);
\draw[crimson!35, line width=0.7pt] ([xshift=145mm,yshift=-180mm]current page.north west) -- ++(0,-25mm);

% Circuit nodes - lower right (more prominent)
\fill[crimson!55] ([xshift=160mm,yshift=-190mm]current page.north west) circle (1.6mm);
\fill[white] ([xshift=160mm,yshift=-190mm]current page.north west) circle (0.9mm);
\fill[crimson!55] ([xshift=170mm,yshift=-200mm]current page.north west) circle (1.4mm);
\fill[white] ([xshift=170mm,yshift=-200mm]current page.north west) circle (0.7mm);
\fill[crimson!50] ([xshift=145mm,yshift=-180mm]current page.north west) circle (1.2mm);
\fill[white] ([xshift=145mm,yshift=-180mm]current page.north west) circle (0.6mm);

% Title positioned in center - clean and readable
\node[inner sep=0mm,draw=none,anchor=center,text width=0.8\textwidth,
align=center,font={\fontsize{40pt}{40}\selectfont}]
(BGG) at (current page.center)  {\color{crimson}\MakeUppercase {#1}};

\end{tikzpicture}
\clearpage
}

% LAB DIVISIONS: Circuit-style neural network design for lab sections
% Used specifically for lab platform sections (arduino, xiao, grove, etc.)
\newcommand{\labdivision}[1]{%
\clearpage
\thispagestyle{empty}
\begin{tikzpicture}[remember picture,overlay]
% Circuit background with subtle gradient
\coordinate(S1)at([yshift=-200mm]current page.north west);
\draw[draw=none,fill=BlueDD!5](S1)--++(45:16)coordinate(S2)-
|(S2|-current page.north west)--(current page.north west)coordinate(S3)--(S1);

% TOP AREA: Circuit lines in upper white space
\draw[crimson!50, line width=1.5pt] ([xshift=30mm,yshift=-40mm]current page.north west) -- ++(60mm,0);
\draw[crimson!40, line width=1pt] ([xshift=120mm,yshift=-50mm]current page.north west) -- ++(50mm,0);
\draw[crimson!50, line width=1.5pt] ([xshift=40mm,yshift=-70mm]current page.north west) -- ++(40mm,0);

% Connecting lines in top area
\draw[crimson!30, line width=1pt] ([xshift=60mm,yshift=-40mm]current page.north west) -- ++(0,-20mm);
\draw[crimson!30, line width=1pt] ([xshift=145mm,yshift=-50mm]current page.north west) -- ++(0,10mm);

% Neural nodes in top area
\fill[crimson!70] ([xshift=60mm,yshift=-40mm]current page.north west) circle (2.5mm);
\fill[white] ([xshift=60mm,yshift=-40mm]current page.north west) circle (1.5mm);
\fill[crimson!60] ([xshift=145mm,yshift=-50mm]current page.north west) circle (2mm);
\fill[white] ([xshift=145mm,yshift=-50mm]current page.north west) circle (1mm);
\fill[crimson!80] ([xshift=80mm,yshift=-70mm]current page.north west) circle (2mm);
\fill[white] ([xshift=80mm,yshift=-70mm]current page.north west) circle (1mm);

% BOTTOM AREA: Circuit lines in lower white space
\draw[crimson!50, line width=1.5pt] ([xshift=20mm,yshift=-200mm]current page.north west) -- ++(70mm,0);
\draw[crimson!40, line width=1pt] ([xshift=110mm,yshift=-210mm]current page.north west) -- ++(60mm,0);
\draw[crimson!50, line width=1.5pt] ([xshift=35mm,yshift=-230mm]current page.north west) -- ++(45mm,0);

% Connecting lines in bottom area
\draw[crimson!30, line width=1pt] ([xshift=55mm,yshift=-200mm]current page.north west) -- ++(0,-20mm);
\draw[crimson!30, line width=1pt] ([xshift=140mm,yshift=-210mm]current page.north west) -- ++(0,15mm);

% Neural nodes in bottom area
\fill[crimson!70] ([xshift=55mm,yshift=-200mm]current page.north west) circle (2.5mm);
\fill[white] ([xshift=55mm,yshift=-200mm]current page.north west) circle (1.5mm);
\fill[crimson!60] ([xshift=140mm,yshift=-210mm]current page.north west) circle (2mm);
\fill[white] ([xshift=140mm,yshift=-210mm]current page.north west) circle (1mm);
\fill[crimson!80] ([xshift=80mm,yshift=-230mm]current page.north west) circle (2mm);
\fill[white] ([xshift=80mm,yshift=-230mm]current page.north west) circle (1mm);

% SIDE AREAS: Subtle circuit elements on left and right edges
\draw[crimson!30, line width=1pt] ([xshift=15mm,yshift=-120mm]current page.north west) -- ++(20mm,0);
\draw[crimson!30, line width=1pt] ([xshift=175mm,yshift=-130mm]current page.north west) -- ++(15mm,0);
\fill[crimson!50] ([xshift=25mm,yshift=-120mm]current page.north west) circle (1.5mm);
\fill[white] ([xshift=25mm,yshift=-120mm]current page.north west) circle (0.8mm);
\fill[crimson!50] ([xshift=185mm,yshift=-130mm]current page.north west) circle (1.5mm);
\fill[white] ([xshift=185mm,yshift=-130mm]current page.north west) circle (0.8mm);

% Title positioned in center - CLEAN AREA
\node[inner sep=0mm,draw=none,anchor=center,text width=0.8\textwidth,
align=center,font={\fontsize{44pt}{44}\selectfont\bfseries}]
(BGG) at (current page.center)  {\color{crimson}\MakeUppercase {#1}};

\end{tikzpicture}
\clearpage
}

% Define \lab command for lab styling (different visual treatment)
\newcommand{\lab}[1]{%
\begin{tikzpicture}[remember picture,overlay]
%%%
% Different background pattern for labs
\coordinate(S1)at([yshift=-200mm]current page.north west);
\draw[draw=none,fill=BlueDD!15](S1)--++(45:16)coordinate(S2)-
|(S2|-current page.north west)--(current page.north west)coordinate(S3)--(S1);
%
\coordinate(E1)at([yshift=-98mm]current page.north west);
\draw[draw=none,fill=BlueDD!25](E1)--(current page.north west)coordinate(E2)
--++(0:98mm)coordinate(E3)--(E1);
%
\coordinate(D1)at([yshift=15mm]current page.south west);
\draw[draw=none,fill=BlueDD!60,opacity=0.7](D1)--++(45:5.5)coordinate(D2)
-|(D2|-current page.north west)--(current page.north west)coordinate(D3)--(D1);
%%%%
\path[red](S2)-|(S2-|current page.east)coordinate(SS2);
%LAB - Different styling
\node[crimson,align=flush right,inner sep=0,outer sep=0mm,draw=none,anchor=south,
font={\fontsize{48pt}{48}\selectfont\bfseries}]  (BG) at ($(S2)!0.5!(SS2)$){\hphantom{Workshop}};
%%%
\path[green]([yshift=15mm]D2)-|coordinate(TPD)(BG.south east);
\node[inner sep=0mm,draw=none,anchor=south east,%text width=0.9\textwidth,
align=right,font={\fontsize{40pt}{40}\selectfont}]
(BGG) at (TPD)  {\color{crimson}\MakeUppercase {#1}};%\MakeUppercase {}
\end{tikzpicture}
\thispagestyle{empty}
\clearpage
}

% =============================================================================
% SECTION FORMATTING
% =============================================================================
% All section levels use crimson color and are ragged right

% Section (Large, bold, crimson)
\titleformat{\section}
  {\normalfont\Large\bfseries\color{crimson}\raggedright}
  {\thesection}
  {0.5em}
  {#1}
\titlespacing*{\section}{0pc}{14pt plus 4pt minus 4pt}{6pt plus 2pt minus 2pt}[0pc]

% Subsection (large, bold, crimson)
\titleformat{\subsection}
  {\normalfont\large\bfseries\color{crimson}\raggedright}
  {\thesubsection}
  {0.5em}
  {#1}
\titlespacing*{\subsection}{0pc}{12pt plus 4pt minus 4pt}{5pt plus 1pt minus 2pt}[0pc]

% Subsubsection (normal size, bold, crimson)
\titleformat{\subsubsection}
  {\normalfont\normalsize\bfseries\color{crimson}\raggedright}
  {\thesubsubsection}
  {0.5em}
  {#1}
\titlespacing*{\subsubsection}{0pc}{12pt plus 4pt minus 4pt}{5pt plus 1pt minus 2pt}[0pc]

% Paragraph (run-in, bold, crimson, ends with period)
\titleformat{\paragraph}[runin]
  {\normalfont\normalsize\bfseries\color{crimson}}
  {\theparagraph}
  {0.5em}
  {#1}
  [\textbf{.}]
  \titlespacing*{\paragraph}{0pc}{6pt plus 2pt minus 2pt}{0.5em}[0pc]

% Subparagraph (run-in, italic, crimson, ends with period)
\titleformat{\subparagraph}[runin]
  {\normalfont\normalsize\itshape\color{crimson}}
  {\thesubparagraph}
  {0.5em}
  {#1}
  [\textbf{.}]
  \titlespacing*{\subparagraph}{0pc}{6pt plus 2pt minus 2pt}{0.5em}[0pc]

% =============================================================================
% CHAPTER FORMATTING
% =============================================================================
% Numbered chapters: "Chapter X" prefix, huge crimson title
\titleformat{\chapter}[display]
  {\normalfont\huge\bfseries\color{crimson}}
  {\chaptername\ \thechapter}
  {20pt}
  {\Huge #1}
  []

% Unnumbered chapters: no prefix, huge crimson title
\titleformat{name=\chapter,numberless}
  {\normalfont\huge\bfseries\color{crimson}}
  {}
  {0pt}
  {\Huge #1}
  []

\renewcommand{\chaptername}{Chapter}
% =============================================================================
% TABLE OF CONTENTS FORMATTING
% =============================================================================
\setcounter{tocdepth}{2}                      % Show chapters, sections, subsections

% TOC spacing adjustments for number widths and indentation
\setlength{\cftchapnumwidth}{2em}             % Chapter number width
\setlength{\cftsecnumwidth}{2.75em}           % Section number width
\setlength{\cftsubsecnumwidth}{3.25em}        % Subsection number width
\setlength{\cftsubsubsecnumwidth}{4em}        % Subsubsection number width
\setlength{\cftsubsecindent}{4.25em}          % Subsection indent
\setlength{\cftsubsubsecindent}{7.5em}        % Subsubsection indent

% Chapter entries in TOC: bold crimson with "Chapter" prefix
\renewcommand{\cftchapfont}{\bfseries\color{crimson}}
\renewcommand{\cftchappresnum}{\color{crimson}Chapter~}

% Custom formatting for division entries (styled like parts)
\newcommand{\divisionchapter}[1]{%
  \addvspace{12pt}%
  \noindent\hfil\bfseries\color{crimson}#1\hfil\par%
  \addvspace{6pt}%
}

% Adjust TOC spacing for "Chapter" prefix
\newlength{\xtraspace}
\settowidth{\xtraspace}{\cftchappresnum\cftchapaftersnum}
\addtolength{\cftchapnumwidth}{\xtraspace}

% Unnumbered chapters with TOC entry
\newcommand{\likechapter}[1]{%
    \chapter*{#1}
    \addcontentsline{toc}{chapter}{\textcolor{crimson}{#1}}
}

% =============================================================================
% PAGE NUMBERING SYSTEM
% =============================================================================
% Implements traditional book numbering:
% - Roman numerals (i, ii, iii...) for frontmatter
% - Arabic numerals (1, 2, 3...) for mainmatter
% Automatically switches at first numbered chapter
\makeatletter
\newif\if@firstnumbered%
\@firstnumberedtrue%
\newif\if@firstunnumbered%
\@firstunnumberedtrue%

\newcounter{lastRomanPage}
\setcounter{lastRomanPage}{1}

% Start document with Roman numerals (frontmatter)
\AtBeginDocument{
  \pagenumbering{roman}
  \renewcommand{\thepage}{\roman{page}}
}

% Intercept chapter command
\let\old@chapter\chapter%
\renewcommand{\chapter}{%
  \@ifstar{\unnumbered@chapter}{\numbered@chapter}%
}

% Numbered chapters: switch to Arabic on first occurrence
\newcommand{\numbered@chapter}[1]{%
  \if@firstnumbered%
    \cleardoublepage%
    \setcounter{lastRomanPage}{\value{page}}%
    \pagenumbering{arabic}%
    \@firstnumberedfalse%
  \else
    \setcounter{page}{\value{page}}%
  \fi
  \setcounter{sidenote}{1}                    % Reset footnote counter per chapter
  \old@chapter{#1}%
}

% Unnumbered chapters: stay in Roman numerals
\newcommand{\unnumbered@chapter}[1]{%
  \if@firstunnumbered%
    \clearpage
    \setcounter{lastRomanPage}{\value{page}}%
    \pagenumbering{roman}%
    \@firstunnumberedfalse%
  \fi
  \setcounter{sidenote}{1}
  \old@chapter*{#1}%
}
\makeatother

% =============================================================================
% TABLE SIZING AND SPACING
% =============================================================================
% Make tables slightly smaller to fit more content
\AtBeginEnvironment{longtable}{\scriptsize}

% Increase vertical spacing in table cells (default is 1.0)
\renewcommand{\arraystretch}{1.3}

% Prefer placing tables at the top of pages
\makeatletter
\renewcommand{\fps@table}{t}  % Default placement: top of page
\makeatother

% =============================================================================
% LONGTABLE PAGE BREAKING FIXES (Windows compatibility)
% =============================================================================
% Prevent "Infinite glue shrinkage" errors on Windows LaTeX builds
% by giving longtable more flexibility in page breaking

% Allow more flexible page breaking (vs strict \flushbottom)
\raggedbottom

% Process more rows before attempting page break (default is 20)
\setcounter{LTchunksize}{50}

% Add extra stretch for longtable environments specifically
\AtBeginEnvironment{longtable}{%
  \setlength{\emergencystretch}{3em}%
  \setlength{\parskip}{0pt plus 1pt}%
}

% =============================================================================
% TABLE STYLING - Clean tables with crimson borders
% =============================================================================
% Professional table appearance with:
% - Clean white background (no colored rows)
% - Crimson-colored borders
% - Good spacing for readability
%
% Note: Headers are automatically bolded by Quarto when using **text** in source
\usepackage{booktabs}      % Professional table rules (\toprule, \midrule, \bottomrule)
\usepackage{colortbl}      % For colored borders (\arrayrulecolor)

% Global table styling - crimson borders
\setlength{\arrayrulewidth}{0.5pt}          % Thinner borders than default
%\arrayrulecolor{crimson}                    % Crimson borders matching brand

\setcounter{chapter}{0}
\usepackage{needspace}
\let\Needspace\needspace
\makeatletter
\@ifpackageloaded{float}{}{\usepackage{float}}
\floatstyle{plain}
\@ifundefined{c@chapter}{\newfloat{vid}{h}{lovid}}{\newfloat{vid}{h}{lovid}[chapter]}
\floatname{vid}{Video}
\newcommand*\listofvids{\listof{vid}{List of Videos}}
\makeatother
\makeatletter
\@ifpackageloaded{tcolorbox}{}{\usepackage[skins,breakable]{tcolorbox}}
\@ifpackageloaded{fontawesome5}{}{\usepackage{fontawesome5}}
\definecolor{quarto-callout-color}{HTML}{909090}
\definecolor{quarto-callout-note-color}{HTML}{0758E5}
\definecolor{quarto-callout-important-color}{HTML}{CC1914}
\definecolor{quarto-callout-warning-color}{HTML}{EB9113}
\definecolor{quarto-callout-tip-color}{HTML}{00A047}
\definecolor{quarto-callout-caution-color}{HTML}{FC5300}
\definecolor{quarto-callout-color-frame}{HTML}{acacac}
\definecolor{quarto-callout-note-color-frame}{HTML}{4582ec}
\definecolor{quarto-callout-important-color-frame}{HTML}{d9534f}
\definecolor{quarto-callout-warning-color-frame}{HTML}{f0ad4e}
\definecolor{quarto-callout-tip-color-frame}{HTML}{02b875}
\definecolor{quarto-callout-caution-color-frame}{HTML}{fd7e14}
\makeatother
\makeatletter
\@ifpackageloaded{bookmark}{}{\usepackage{bookmark}}
\makeatother
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\AtBeginDocument{%
\ifdefined\contentsname
  \renewcommand*\contentsname{Table of contents}
\else
  \newcommand\contentsname{Table of contents}
\fi
\ifdefined\listfigurename
  \renewcommand*\listfigurename{List of Figures}
\else
  \newcommand\listfigurename{List of Figures}
\fi
\ifdefined\listtablename
  \renewcommand*\listtablename{List of Tables}
\else
  \newcommand\listtablename{List of Tables}
\fi
\ifdefined\figurename
  \renewcommand*\figurename{Figure}
\else
  \newcommand\figurename{Figure}
\fi
\ifdefined\tablename
  \renewcommand*\tablename{Table}
\else
  \newcommand\tablename{Table}
\fi
}
\@ifpackageloaded{float}{}{\usepackage{float}}
\floatstyle{ruled}
\@ifundefined{c@chapter}{\newfloat{codelisting}{h}{lop}}{\newfloat{codelisting}{h}{lop}[chapter]}
\floatname{codelisting}{Listing}
\newcommand*\listoflistings{\listof{codelisting}{List of Listings}}
\makeatother
\makeatletter
\makeatother
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\@ifpackageloaded{subcaption}{}{\usepackage{subcaption}}
\makeatother
\makeatletter
\@ifpackageloaded{sidenotes}{}{\usepackage{sidenotes}}
\@ifpackageloaded{marginnote}{}{\usepackage{marginnote}}
\makeatother
\newcommand{\fbxIconPath}{assets/images/icons/callouts}
\newcommand{\fbxIconFormat}{pdf}
\makeatletter
\@ifpackageloaded{tcolorbox}{}{\usepackage[many]{tcolorbox}}
\makeatother
%%%% ---foldboxy preamble ----- %%%%%

% Load xstring for string manipulation
\RequirePackage{xstring}

% Icon path and format configuration - can be overridden in filter-metadata
\providecommand{\fbxIconPath}{assets/images/icons/callouts}
\providecommand{\fbxIconFormat}{pdf}

% Helper command to include icon with hyphen-to-underscore conversion
% This ensures consistency: callout-quiz-question -> callout_quiz_question
\newcommand{\fbxIncludeIcon}[2]{%
  \StrSubstitute{#1}{-}{_}[\fbxIconName]%
  \includegraphics[width=#2]{\fbxIconPath/icon_\fbxIconName.\fbxIconFormat}%
}

% Legacy fallback colors (keep for compatibility)
\definecolor{fbx-default-color1}{HTML}{c7c7d0}
\definecolor{fbx-default-color2}{HTML}{a3a3aa}
\definecolor{fbox-color1}{HTML}{c7c7d0}
\definecolor{fbox-color2}{HTML}{a3a3aa}

% arguments: #1 typelabelnummer: #2 titel: #3
\newenvironment{fbx}[3]{%
\begin{tcolorbox}[
  enhanced,
  breakable,
  %fontupper=\fontsize{8pt}{10pt}\selectfont,  % 95% of body text (10pt -> 9.5pt)
  before skip=8pt,  % space above box (increased)
  after skip=8pt,   % space below box (increased)
  attach boxed title to top*={xshift=0pt},
  boxed title style={
  %fuzzy shadow={1pt}{-1pt}{0mm}{0.1mm}{gray},
  arc=1.5pt,
  rounded corners=north,
  sharp corners=south,
  top=6pt,          % Adjusted for ~40px equivalent height
  bottom=5pt,       % Adjusted for ~40px equivalent height
  overlay={
      \node [left,outer sep=0em, black,draw=none,anchor=west,
        rectangle,fill=none,inner sep=0pt]
        at ([xshift=4mm]frame.west) {\fbxIncludeIcon{#1}{4.2mm}};
    },
  },
  colframe=#1-color2,             % Border color (auto-generated from YAML)
  colbacktitle=#1-color1,         % Background color (auto-generated from YAML)
  colback=white,
  coltitle=black,
  titlerule=0mm,
  toprule=0.5pt,
  bottomrule=0.5pt,
  leftrule=2.2pt,
  rightrule=0.5pt,
  outer arc=1.5pt,
  arc=1.5pt,
  left=0.5em,       % increased left padding
  bottomtitle=1.5mm, % increased title bottom margin
  toptitle=1.5mm,    % increased title top margin
  title=\hspace{2.5em}\protect#2\hspace{0.2em}\protect#3, % Protect parameters
  extras middle and last={top=4pt} % increased continuation spacing
]}
{\end{tcolorbox}}


% boxed environment with right border
\newenvironment{fbxSimple}[3]{\begin{tcolorbox}[
  enhanced,
  breakable,
  %fontupper=\fontsize{8pt}{10pt}\selectfont,  % 95% of body text (10pt -> 9.5pt)
  before skip=8pt,  % space above box (increased)
  after skip=8pt,   % space below box (increased)
  attach boxed title to top*={xshift=0pt},
  boxed title style={
  %fuzzy shadow={1pt}{-1pt}{0mm}{0.1mm}{gray},
  arc=1.5pt,
  rounded corners=north,
  sharp corners=south,
  top=6pt,          % Adjusted for ~40px equivalent height
  bottom=5pt,       % Adjusted for ~40px equivalent height
  overlay={
      \node [left,outer sep=0em, black,draw=none,anchor=west,
        rectangle,fill=none,inner sep=0pt]
        at ([xshift=3mm]frame.west) {\fbxIncludeIcon{#1}{4.2mm}};
    },
  },
  colframe=#1-color2,             % Border color (auto-generated from YAML)
  colbacktitle=#1-color1,         % Background color (auto-generated from YAML)
  colback=white,
  coltitle=black,
  titlerule=0mm,
  toprule=0.5pt,
  bottomrule=0.5pt,
  leftrule=2.2pt,
  rightrule=0.5pt,
  outer arc=1.5pt,
  arc=1.5pt,
  left=0.5em,       % increased left padding
  bottomtitle=1.5mm, % increased title bottom margin
  toptitle=1.5mm,    % increased title top margin
  title=\hspace{2.5em}\protect#2\hspace{0.2em}\protect#3, % Protect parameters
  boxsep=1pt,
  extras first={bottom=0pt},
  extras last={top=0pt,bottom=-4pt},
  overlay first={
    \draw[line width=1pt,white] ([xshift=2.2pt]frame.south west)-- ([xshift=-0.5pt]frame.south east);
  },
  overlay last={
    \draw[line width=1pt,white] ([xshift=2.2pt]frame.north west)-- ([xshift=-0.5pt]frame.north east);
   }
]}
{\end{tcolorbox}}

%%%% --- end foldboxy preamble ----- %%%%%
%%==== colors from yaml ===%
\definecolor{callout-quiz-answer-color1}{HTML}{E8F2EA}
\definecolor{callout-quiz-answer-color2}{HTML}{4a7c59}
\definecolor{callout-colab-color1}{HTML}{FFF5E6}
\definecolor{callout-colab-color2}{HTML}{FF6B35}
\definecolor{callout-example-color1}{HTML}{F0F8F6}
\definecolor{callout-example-color2}{HTML}{148F77}
\definecolor{callout-resource-slides-color1}{HTML}{E0F2F1}
\definecolor{callout-resource-slides-color2}{HTML}{20B2AA}
\definecolor{callout-quiz-question-color1}{HTML}{F0F0F8}
\definecolor{callout-quiz-question-color2}{HTML}{5B4B8A}
\definecolor{callout-code-color1}{HTML}{F2F4F8}
\definecolor{callout-code-color2}{HTML}{D1D7E0}
\definecolor{callout-resource-exercises-color1}{HTML}{E0F2F1}
\definecolor{callout-resource-exercises-color2}{HTML}{20B2AA}
\definecolor{callout-definition-color1}{HTML}{F0F4F8}
\definecolor{callout-definition-color2}{HTML}{1B4F72}
\definecolor{callout-chapter-connection-color1}{HTML}{FDF2F7}
\definecolor{callout-chapter-connection-color2}{HTML}{A51C30}
\definecolor{callout-resource-videos-color1}{HTML}{E0F2F1}
\definecolor{callout-resource-videos-color2}{HTML}{20B2AA}
%=============%

\usepackage{hyphenat}
\usepackage{ifthen}
\usepackage{calc}
\usepackage{calculator}



\usepackage{graphicx}
\usepackage{geometry}
\usepackage{afterpage}
\usepackage{tikz}
\usetikzlibrary{calc}
\usetikzlibrary{fadings}
\usepackage[pagecolor=none]{pagecolor}


% Set the titlepage font families







% Set the coverpage font families

\usepackage{bookmark}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\urlstyle{same}
\hypersetup{
  pdftitle={Machine Learning Systems},
  pdfauthor={Vijay Janapa Reddi},
  colorlinks=true,
  linkcolor={Maroon},
  filecolor={Maroon},
  citecolor={Blue},
  urlcolor={Blue},
  pdfcreator={LaTeX via pandoc}}


\title{Machine Learning Systems}
\usepackage{etoolbox}
\makeatletter
\providecommand{\subtitle}[1]{% add subtitle to \maketitle
  \apptocmd{\@title}{\par {\large #1 \par}}{}{}
}
\makeatother
\subtitle{Principles and Practices of Engineering Artificially
Intelligent Systems}
\author{Vijay Janapa Reddi}
\date{December 14, 2025}
\begin{document}
%%%%% begin titlepage extension code

  \begin{frontmatter}

\begin{titlepage}
% This is a combination of Pandoc templating and LaTeX
% Pandoc templating https://pandoc.org/MANUAL.html#templates
% See the README for help

\thispagestyle{empty}

\newgeometry{top=-100in}

% Page color

\newcommand{\coverauthorstyle}[1]{{\fontsize{20}{24.0}\selectfont
{#1}}}

\begin{tikzpicture}[remember picture, overlay, inner sep=0pt, outer sep=0pt]

\tikzfading[name=fadeout, inner color=transparent!0,outer color=transparent!100]
\tikzfading[name=fadein, inner color=transparent!100,outer color=transparent!0]
\node[anchor=south west, rotate=0, opacity=1] at ($(current page.south west)+(0.225\paperwidth, 7)$) {
\includegraphics[width=\paperwidth, keepaspectratio]{assets/images/covers/cover-image-transparent.png}};

% Title
\newcommand{\titlelocationleft}{0.075\paperwidth}
\newcommand{\titlelocationbottom}{0.31\paperwidth}
\newcommand{\titlealign}{left}

\begin{scope}{%
\fontsize{57}{68.39999999999999}\selectfont
\node[anchor=north
west, align=left, rotate=0] (Title1) at ($(current page.south west)+(\titlelocationleft,\titlelocationbottom)$)  [text width = 0.9\paperwidth]  {{\nohyphens{Machine
Learning Systems}}};
}
\end{scope}

% Author
\newcommand{\authorlocationleft}{7in}
\newcommand{\authorlocationbottom}{0.132\paperwidth}
\newcommand{\authoralign}{right}

\begin{scope}
{%
\fontsize{20}{24.0}\selectfont
\node[anchor=north
east, align=right, rotate=0] (Author1) at ($(current page.south west)+(\authorlocationleft,\authorlocationbottom)$)  [text width = 6in]  {\coverauthorstyle{Vijay\\Janapa
Reddi\\}};
}
\end{scope}

% Footer
\newcommand{\footerlocationleft}{0.075\paperwidth}
\newcommand{\footerlocationbottom}{0.38\paperwidth}
\newcommand{\footerlocationalign}{left}

\begin{scope}
{%
\fontsize{25}{30.0}\selectfont
 \node[anchor=north west, align=left, rotate=0] (Footer1) at %
($(current page.south west)+(\footerlocationleft,\footerlocationbottom)$)  [text width = 0.9\paperwidth]  {{\nohyphens{Introduction
to}}};
}
\end{scope}

\end{tikzpicture}
\clearpage
\restoregeometry
%%% TITLE PAGE START

% Set up alignment commands
%Page
\newcommand{\titlepagepagealign}{
\ifthenelse{\equal{left}{right}}{\raggedleft}{}
\ifthenelse{\equal{left}{center}}{\centering}{}
\ifthenelse{\equal{left}{left}}{\raggedright}{}
}


\newcommand{\titleandsubtitle}{
% Title and subtitle
{{\huge{\bfseries{\nohyphens{Machine Learning Systems}}}}\par
}%

\vspace{\betweentitlesubtitle}
{
{\large{\textit{\nohyphens{Principles and Practices of Engineering
Artificially Intelligent Systems}}}}\par
}}
\newcommand{\titlepagetitleblock}{
\titleandsubtitle
}

\newcommand{\authorstyle}[1]{{\large{#1}}}

\newcommand{\affiliationstyle}[1]{{\large{#1}}}

\newcommand{\titlepageauthorblock}{
{\authorstyle{\nohyphens{Vijay Janapa
Reddi}{\textsuperscript{1}}\textsuperscript{,}{\textsuperscript{,*}}}}}

\newcommand{\titlepageaffiliationblock}{
\hangindent=1em
\hangafter=1
{\affiliationstyle{
{1}.~Harvard University


\vspace{1\baselineskip}
* \textit{Correspondence:}~Vijay Janapa Reddi~vj@eecs.harvard.edu
}}
}
\newcommand{\headerstyled}{%
{}
}
\newcommand{\footerstyled}{%
{\large{}}
}
\newcommand{\datestyled}{%
{December 14, 2025}
}


\newcommand{\titlepageheaderblock}{\headerstyled}

\newcommand{\titlepagefooterblock}{
\footerstyled
}

\newcommand{\titlepagedateblock}{
\datestyled
}

%set up blocks so user can specify order
\newcommand{\titleblock}{\newlength{\betweentitlesubtitle}
\setlength{\betweentitlesubtitle}{0.05\textheight}
{

{\titlepagetitleblock}
}

\vspace{4\baselineskip}
}

\newcommand{\authorblock}{{\titlepageauthorblock}

\vspace{2\baselineskip}
}

\newcommand{\affiliationblock}{{\titlepageaffiliationblock}

\vspace{0pt}
}

\newcommand{\logoblock}{}

\newcommand{\footerblock}{}

\newcommand{\dateblock}{{\titlepagedateblock}

\vspace{0pt}
}

\newcommand{\headerblock}{}

\thispagestyle{empty} % no page numbers on titlepages


\newcommand{\vrulecode}{\textcolor{black}{\rule{\vrulewidth}{\textheight}}}
\newlength{\vrulewidth}
\setlength{\vrulewidth}{2pt}
\newlength{\B}
\setlength{\B}{\ifdim\vrulewidth > 0pt 0.05\textwidth\else 0pt\fi}
\newlength{\minipagewidth}
\ifthenelse{\equal{left}{left} \OR \equal{left}{right} }
{% True case
\setlength{\minipagewidth}{\textwidth - \vrulewidth - \B - 0.1\textwidth}
}{
\setlength{\minipagewidth}{\textwidth - 2\vrulewidth - 2\B - 0.1\textwidth}
}
\ifthenelse{\equal{left}{left} \OR \equal{left}{leftright}}
{% True case
\raggedleft % needed for the minipage to work
\vrulecode
\hspace{\B}
}{%
\raggedright % else it is right only and width is not 0
}
% [position of box][box height][inner position]{width}
% [s] means stretch out vertically; assuming there is a vfill
\begin{minipage}[b][\textheight][s]{\minipagewidth}
\titlepagepagealign
\titleblock

Prof.~Vijay Janapa Reddi

School of Engineering and Applied Sciences

Harvard University

\vspace{80mm}

With heartfelt gratitude to the community for their invaluable
contributions and steadfast support.

\vfill

December 14, 2025

\vfill
\par

\end{minipage}\ifthenelse{\equal{left}{right} \OR \equal{left}{leftright} }{
\hspace{\B}
\vrulecode}{}
\clearpage
%%% TITLE PAGE END\end{titlepage}
\setcounter{page}{1}
\end{frontmatter}

%%%%% end titlepage extension code
\renewcommand*\contentsname{Table of contents}
{
\hypersetup{linkcolor=}
\setcounter{tocdepth}{2}
\tableofcontents
}

\mainmatter
\bookmarksetup{startatroot}

\chapter*{Abstract}\label{abstract}
\addcontentsline{toc}{chapter}{Abstract}

\markboth{Abstract}{Abstract}

Machine Learning Systems provides a systematic framework for
understanding and engineering machine learning (ML) systems. This
textbook bridges the gap between theoretical foundations and practical
engineering, emphasizing the systems perspective required to build
effective AI solutions. Unlike resources that focus primarily on
algorithms and model architectures, this book highlights the broader
context in which ML systems operate, including data engineering, model
optimization, hardware-aware training, and inference acceleration.
Readers will develop the ability to reason about ML system architectures
and apply enduring engineering principles for building flexible,
efficient, and robust machine learning systems.

\section*{Support Our Mission}\label{support-our-mission}
\addcontentsline{toc}{section}{Support Our Mission}

\markright{Support Our Mission}

Your support expands educational access and creates new opportunities.
The open source community has embraced this project, with thousands of
GitHub stars from educators, students, and practitioners worldwide.

\textbf{Ways to Support:}

\begin{itemize}
\tightlist
\item
  \textbf{GitHub:} Star the repository at
  \href{https://github.com/harvard-edge/cs249r_book}{github.com/harvard-edge/cs249r\_book}
\item
  \textbf{Open Collective:} Support through
  \href{https://opencollective.com/mlsysbook}{opencollective.com/mlsysbook}
\item
  \textbf{Share:} Help others discover this resource
\end{itemize}

\clearpage

\section*{Why We Wrote This Book}\label{why-we-wrote-this-book}
\addcontentsline{toc}{section}{Why We Wrote This Book}

\markright{Why We Wrote This Book}

\textbf{The Problem:} Students learn to train AI models, but few
understand how to build the systems that actually make them work in
production. When ML systems concepts are taught, students often learn
individual components without grasping the holistic architecture---they
can see the trees but miss the forest.

\textbf{The Future:} As AI becomes more autonomous, the bottleneck won't
be just the algorithms---it will be the AI engineers who can build
efficient, scalable, and sustainable systems.

\begin{quote}
``If you want to go fast, go alone. If you want to go far, go
together.'' --- African Proverb
\end{quote}

\textbf{Our Approach:} This vision emerged from collaborative work in
CS249r at Harvard University, where students, faculty, and industry
partners came together to explore the systems side of ML. The content
was developed through real student contributions during Fall 2023. What
started as class notes has turned into a comprehensive educational
resource we now share globally.

Want the full story? Read our
\href{contents/frontmatter/foreword.qmd}{Author's Note} about the
inspiration and values driving this project.

\section*{Listen to the AI Podcast}\label{listen-to-the-ai-podcast}
\addcontentsline{toc}{section}{Listen to the AI Podcast}

\markright{Listen to the AI Podcast}

A short podcast, created with Google's Notebook LM and inspired by
insights from our
\href{https://web.eng.fiu.edu/gaquan/Papers/ESWEEK24Papers/CPS-Proceedings/pdfs/CODES-ISSS/563900a043/563900a043.pdf}{IEEE
education viewpoint paper}, offers an accessible overview of the book's
key ideas and themes. The podcast explores the systems perspective of
machine learning and discusses why understanding ML systems architecture
is crucial for building effective AI solutions.

\emph{Note: Audio content is available in the online version at
\href{https://mlsysbook.ai}{mlsysbook.ai}}

\section*{Global Outreach}\label{global-outreach}
\addcontentsline{toc}{section}{Global Outreach}

\markright{Global Outreach}

Thank you to all our readers and visitors. Your engagement with the
material keeps us motivated.

This textbook has reached readers across the globe, with visitors from
over 100 countries engaging with the material. The international
community includes students, educators, researchers, and practitioners
who are advancing the field of machine learning systems. From
universities in North America and Europe to research institutions in
Asia and emerging tech hubs worldwide, the content serves diverse
learning needs and cultural contexts.

\emph{Interactive analytics dashboard available in the online version at
\href{https://mlsysbook.ai}{mlsysbook.ai}}

\section*{Want to Help Out?}\label{want-to-help-out}
\addcontentsline{toc}{section}{Want to Help Out?}

\markright{Want to Help Out?}

This is a collaborative project, and your input matters! If you'd like
to contribute, check out our
\href{https://github.com/harvard-edge/cs249r_book/blob/dev/docs/contribute.md}{contribution
guidelines}. Feedback, corrections, and new ideas are welcome. Simply
file a GitHub
\href{https://github.com/harvard-edge/cs249r_book/issues}{issue}.

We warmly invite you to join us on this journey by contributing your
expertise, feedback, and ideas.

\addtocontents{toc}{\par\addvspace{12pt}\noindent\hfil\bfseries\color{crimson}Frontmatter\color{black}\hfil\par\addvspace{6pt}}

\addtocontents{toc}{\par\noindent\hfil{\color{crimson}\rule{0.6\textwidth}{0.5pt}}\hfil\par\addvspace{6pt}}

\division{Frontmatter}

\bookmarksetup{startatroot}

\chapter{Introduction}\label{sec-introduction}

\marginnote{\begin{footnotesize}

\emph{DALL·E 3 Prompt: A detailed, rectangular, flat 2D illustration
depicting a roadmap of a book's chapters on machine learning systems,
set on a crisp, clean white background. The image features a winding
road traveling through various symbolic landmarks. Each landmark
represents a chapter topic: Introduction, ML Systems, Deep Learning, AI
Workflow, Data Engineering, AI Frameworks, AI Training, Efficient AI,
Model Optimizations, AI Acceleration, Benchmarking AI, On-Device
Learning, Embedded AIOps, Security \& Privacy, Responsible AI,
Sustainable AI, AI for Good, Robust AI, Generative AI. The style is
clean, modern, and flat, suitable for a technical book, with each
landmark clearly labeled with its chapter title.}

\end{footnotesize}}

\noindent
\pandocbounded{\includegraphics[keepaspectratio]{contents/core/introduction/images/png/cover_introduction.png}}

\section*{Purpose}\label{purpose}
\addcontentsline{toc}{section}{Purpose}

\markright{Purpose}

\emph{Why must we master the engineering principles that govern systems
capable of learning, adapting, and operating at massive scale?}

Machine learning represents the most significant transformation in
computing since programmable computers, enabling systems whose behavior
emerges from data rather than explicit instructions. This transformation
requires new engineering foundations because traditional software
engineering principles cannot address systems that learn and adapt based
on experience. Every major technological challenge, from climate
modeling and medical diagnosis to autonomous transportation, requires
systems that process vast amounts of data and operate reliably despite
uncertainty. Understanding ML systems engineering determines our ability
to solve complex problems that exceed human cognitive capacity. This
discipline provides the foundation for building systems that can scale
across deployment environments, from massive data centers to
resource-constrained edge devices, establishing the technical groundwork
for technological progress in the 21st century.

\begin{tcolorbox}[enhanced jigsaw, opacityback=0, opacitybacktitle=0.6, leftrule=.75mm, breakable, bottomrule=.15mm, rightrule=.15mm, colframe=quarto-callout-tip-color-frame, arc=.35mm, coltitle=black, titlerule=0mm, colbacktitle=quarto-callout-tip-color!10!white, left=2mm, bottomtitle=1mm, colback=white, toptitle=1mm, toprule=.15mm, title=\textcolor{quarto-callout-tip-color}{\faLightbulb}\hspace{0.5em}{Learning Objectives}]

\begin{itemize}
\item
  Define machine learning systems as integrated computing systems
  comprising data, algorithms, and infrastructure
\item
  Distinguish ML systems engineering from traditional software
  engineering through failure pattern analysis
\item
  Analyze interdependencies between data, algorithms, and computing
  infrastructure using the AI Triangle framework
\item
  Trace the historical evolution of AI paradigms from symbolic systems
  through statistical learning to deep learning
\item
  Evaluate the implications of Sutton's ``Bitter Lesson'' for modern ML
  systems engineering priorities
\item
  Compare silent performance degradation in ML systems with traditional
  software failure modes
\item
  Contrast ML system lifecycle phases with traditional software
  development
\item
  Classify real-world challenges in ML systems across data, model,
  system, and ethical categories
\item
  Apply the five-pillar framework to evaluate ML system architectures
\end{itemize}

\end{tcolorbox}

\section{The Engineering Revolution in Artificial
Intelligence}\label{sec-introduction-engineering-revolution-artificial-intelligence-a3eb}

Engineering practice today stands at an inflection point comparable to
the most transformative periods in technological history. The Industrial
Revolution established mechanical engineering as a discipline for
managing physical forces, while the Digital Revolution formalized
computational engineering to handle algorithmic complexity. Today,
artificial intelligence systems require a new engineering paradigm for
systems that exhibit learned behaviors, autonomous adaptation, and
operational scales that exceed conventional software engineering
methodologies.

This shift reconceptualizes the nature of engineered systems.
Traditional deterministic software architectures operate according to
explicitly programmed instructions, yielding predictable outputs for
given inputs. In contrast, machine learning systems are probabilistic
architectures whose behaviors emerge from statistical patterns extracted
from training data. This transformation introduces engineering
challenges that define the discipline of machine learning systems
engineering: ensuring reliability in systems whose behaviors are learned
rather than programmed, achieving scalability for systems processing
petabyte-scale\sidenote{\textbf{Petabyte-Scale Data}: One petabyte
equals 1,000 terabytes or roughly 1 million gigabytes---enough to store
13.3 years of HD video or the entire written works of humanity 50 times
over. Modern ML systems routinely process petabyte-scale datasets: Meta
processes over 4 petabytes of data daily for its recommendation systems,
while Google's search index contains hundreds of petabytes of web
content. Managing this scale requires distributed storage systems (like
HDFS or S3) that shard data across thousands of servers, parallel
processing frameworks (like Apache Spark) that coordinate computation
across clusters, and sophisticated data engineering pipelines that can
validate, transform, and serve data at rates exceeding 100 GB/s. The
engineering challenge isn't just storage capacity, but the bandwidth,
fault tolerance, and consistency guarantees needed to make petabyte
datasets useful for training and inference. } datasets while serving
billions of concurrent users, and maintaining robustness when
operational data distributions diverge from training distributions.

These challenges establish the theoretical and practical foundations of
ML systems engineering as a distinct academic discipline. This chapter
provides the conceptual foundation for understanding both the historical
evolution that created this field and the engineering principles that
differentiate machine learning systems from traditional software
architectures. The analysis synthesizes perspectives from computer
science, systems engineering, and statistical learning theory to
establish a framework for the systematic study of intelligent systems.

Our investigation begins with the relationship between artificial
intelligence as a research objective and machine learning as the
computational methodology for achieving intelligent behavior. We then
establish what constitutes a machine learning system, the integrated
computing systems comprising data, algorithms, and infrastructure that
this discipline builds. Through historical analysis, we trace the
evolution of AI paradigms from symbolic reasoning systems through
statistical learning approaches to contemporary deep learning
architectures, demonstrating how each transition required new
engineering solutions. This progression illuminates Sutton's ``bitter
lesson'' of AI research: that domain-general computational methods
ultimately supersede hand-crafted knowledge representations, positioning
systems engineering as central to AI advancement.

This historical and technical foundation enables us to formally define
this discipline. Following the pattern established by Computer
Engineering's emergence from Electrical Engineering and Computer
Science, we establish it as a field focused on building reliable,
efficient, and scalable machine learning systems across computational
platforms. This formal definition addresses both the nomenclature used
in practice and the technical scope of what practitioners actually
build.

Building upon this foundation, we introduce the theoretical frameworks
that structure the analysis of ML systems throughout this text. The AI
Triangle provides a conceptual model for understanding the
interdependencies among data, algorithms, and computational
infrastructure. We examine the machine learning system lifecycle,
contrasting it with traditional software development methodologies to
highlight the unique phases of problem formulation, data curation, model
development, validation, deployment, and continuous maintenance that
characterize ML system engineering.

These theoretical frameworks are substantiated through examination of
representative deployment scenarios that demonstrate the diversity of
engineering requirements across application domains. From autonomous
vehicles operating under stringent latency constraints at the network
edge to recommendation systems serving billions of users through cloud
infrastructure, these case studies illustrate how deployment context
shapes system architecture and engineering trade-offs.

The analysis culminates by identifying the core challenges that
establish ML systems engineering as both a necessary and complex
discipline: silent performance degradation patterns that require
specialized monitoring approaches, data quality issues and distribution
shifts that compromise model validity, requirements for model robustness
and interpretability in high-stakes applications, infrastructure
scalability demands that exceed conventional distributed systems, and
ethical considerations that impose new categories of system
requirements. These challenges provide the foundation for the
five-pillar organizational framework that structures this text,
partitioning ML systems engineering into interconnected sub-disciplines
that enable the development of robust, scalable, and responsible
artificial intelligence systems.

This chapter establishes the theoretical foundation for Part I: Systems
Foundations, introducing the principles that underlie all subsequent
analysis of ML systems engineering. The conceptual frameworks introduced
here provide the analytical tools that will be refined and applied
throughout subsequent chapters, culminating in a methodology for
engineering systems capable of reliably delivering artificial
intelligence capabilities in production environments.

\phantomsection\label{quiz-question-sec-introduction-engineering-revolution-artificial-intelligence-a3eb}
\begin{fbx}{callout-quiz-question}{Self-Check: Question 1.1}{}
\phantomsection\label{quiz-question-sec-introduction-engineering-revolution-artificial-intelligence-a3eb}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  What distinguishes machine learning systems from traditional
  deterministic software architectures?

  \begin{enumerate}
  \def\labelenumii{\alph{enumii})}
  \tightlist
  \item
    Machine learning systems operate based on explicitly programmed
    instructions.
  \item
    Traditional software systems can adapt autonomously to new data.
  \item
    Machine learning systems rely on statistical patterns extracted from
    data.
  \item
    Traditional software systems require no maintenance.
  \end{enumerate}
\item
  Explain the significance of the `bitter lesson' in AI research as
  mentioned in the section.
\item
  Which of the following challenges is NOT typically associated with
  machine learning systems engineering?

  \begin{enumerate}
  \def\labelenumii{\alph{enumii})}
  \tightlist
  \item
    Eliminating the need for computational infrastructure
  \item
    Achieving scalability for large datasets
  \item
    Maintaining robustness with changing data distributions
  \item
    Ensuring reliability in learned behaviors
  \end{enumerate}
\item
  How does the AI Triangle framework help in understanding machine
  learning systems?
\end{enumerate}

\noindent\hspace*{1.25em}\hyperref[quiz-answer-sec-introduction-engineering-revolution-artificial-intelligence-a3eb]{\textbf{See Answer~$\rightarrow$}}

\end{fbx}

\section{From Artificial Intelligence Vision to Machine Learning
Practice}\label{sec-introduction-artificial-intelligence-vision-machine-learning-practice-c45a}

Having established AI's transformative impact across society, a question
emerges: How do we actually create these intelligent capabilities?
Understanding the relationship between Artificial Intelligence and
Machine Learning provides the key to answering this question and is
central to everything that follows in this book.

AI represents the broad goal of creating systems that can perform tasks
requiring human-like intelligence: recognizing images, understanding
language, making decisions, and solving problems. AI is the what, the
vision of intelligent machines that can learn, reason, and adapt.

Machine Learning (ML) represents the methodological approach and
practical discipline for creating systems that demonstrate intelligent
behavior. Rather than implementing intelligence through predetermined
rules, machine learning provides the computational techniques to
automatically discover patterns in data through mathematical processes.
This methodology transforms AI's theoretical insights into functioning
systems.

Consider the evolution of chess-playing systems as an example of this
shift. The AI goal remains constant: ``Create a system that can play
chess like a human.'' However, the approaches differ:

\begin{itemize}
\item
  \textbf{Symbolic AI Approach (Pre-ML)}: Program the computer with all
  chess rules and hand-craft strategies like ``control the center'' and
  ``protect the king.'' This requires expert programmers to explicitly
  encode thousands of chess principles, creating brittle systems that
  struggle with novel positions.
\item
  \textbf{Machine Learning Approach}: Have the computer analyze millions
  of chess games to learn winning strategies automatically from data.
  Rather than programming specific moves, the system discovers patterns
  that lead to victory through statistical analysis of game outcomes.
\end{itemize}

This transformation illustrates why ML has become the dominant approach:
In rule-based systems, humans translate domain expertise directly into
code. In ML systems, humans curate training data, design learning
architectures, and define success metrics, allowing the system to
extract its own operational logic from examples. Data-driven systems can
adapt to situations that programmers never anticipated, while rule-based
systems remain constrained by their original programming.

Machine learning systems acquire recognition capabilities through
processes that parallel human learning patterns. Object recognition
develops through exposure to numerous examples, while natural language
processing systems acquire linguistic capabilities through extensive
textual analysis. These learning approaches operationalize theories of
intelligence developed in AI research, building on mathematical
foundations that we establish systematically throughout this text.

The distinction between AI as research vision and ML as engineering
methodology carries significant implications for system design. Modern
ML's data-driven approach requires infrastructure capable of collecting,
processing, and learning from data at massive scale. Machine learning
emerged as a practical approach to artificial intelligence through
extensive research and major paradigm shifts\sidenote{\textbf{Paradigm
Shift}: A term coined by philosopher Thomas Kuhn in 1962
(\citeproc{ref-kuhn1962structure}{Kvasz 2014}) to describe major changes
in scientific approach. In AI, the key paradigm shift was moving from
symbolic reasoning (encoding human knowledge as rules) to statistical
learning (discovering patterns from data). This shift had profound
systems implications: rule-based systems scaled with programmer effort,
requiring manual encoding of each new rule. Data-driven ML scales with
compute and data infrastructure---achieving better performance by adding
more GPUs and training data rather than more programmers. This
transformation made systems engineering critical: success now depends on
building infrastructure to collect massive datasets, train
billion-parameter models, and serve predictions at scale, rather than
encoding expert knowledge. }, transforming theoretical principles about
intelligence into functioning systems that form the algorithmic
foundation of today's intelligent capabilities.

\phantomsection\label{callout-definitionux2a-1.1}
\begin{fbx}{callout-definition}{Definition: }{Key Definitions}
\phantomsection\label{callout-definition*-1.1}
\textbf{\emph{Artificial Intelligence (AI)}} is the field of computer
science focused on creating systems that perform tasks requiring
human-like \emph{intelligence}, including \emph{learning},
\emph{reasoning}, and \emph{adaptation}.

\textbf{\emph{Machine Learning (ML)}} is the approach to AI that enables
systems to automatically learn \emph{patterns} and make \emph{decisions}
from \emph{data} rather than following explicit programmed rules.

\end{fbx}

The evolution from rule-based AI to data-driven ML represents one of the
most significant shifts in computing history. This transformation
explains why ML systems engineering has emerged as a discipline: the
path to intelligent systems now runs through the engineering challenge
of building systems that can effectively learn from data at massive
scale.

\phantomsection\label{quiz-question-sec-introduction-artificial-intelligence-vision-machine-learning-practice-c45a}
\begin{fbx}{callout-quiz-question}{Self-Check: Question 1.2}{}
\phantomsection\label{quiz-question-sec-introduction-artificial-intelligence-vision-machine-learning-practice-c45a}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Which of the following best describes the relationship between
  Artificial Intelligence (AI) and Machine Learning (ML)?

  \begin{enumerate}
  \def\labelenumii{\alph{enumii})}
  \tightlist
  \item
    AI is a subset of ML focused on data-driven techniques.
  \item
    ML is a practical implementation of AI using rule-based systems.
  \item
    AI and ML are completely independent fields.
  \item
    ML is a subset of AI focused on data-driven techniques.
  \end{enumerate}
\item
  Explain why machine learning has become the dominant approach in
  achieving AI goals.
\item
  Order the following steps in the evolution from symbolic AI to machine
  learning: (1) Encoding human knowledge as rules, (2) Discovering
  patterns from data, (3) Scaling with compute and data infrastructure.
\end{enumerate}

\noindent\hspace*{1.25em}\hyperref[quiz-answer-sec-introduction-artificial-intelligence-vision-machine-learning-practice-c45a]{\textbf{See Answer~$\rightarrow$}}

\end{fbx}

\section{Defining ML
Systems}\label{sec-introduction-defining-ml-systems-bf7d}

Before exploring how we arrived at modern machine learning systems, we
must first establish what we mean by an ``ML system.'' This definition
provides the conceptual framework for understanding both the historical
evolution and contemporary challenges that follow.

No universally accepted definition of machine learning systems exists,
reflecting the field's rapid evolution and multidisciplinary nature.
However, building on our understanding that modern ML relies on
data-driven approaches at scale, this textbook adopts a perspective that
encompasses the entire ecosystem in which algorithms operate:

\phantomsection\label{callout-definitionux2a-1.2}
\begin{fbx}{callout-definition}{Definition: }{Machine Learning System}
\phantomsection\label{callout-definition*-1.2}
\textbf{\emph{Machine Learning Systems}} are integrated computing
systems comprising three interdependent components: \emph{data} that
guides behavior, \emph{algorithms} that learn patterns, and
\emph{computational infrastructure} that enables both \emph{training}
and \emph{inference}.

\end{fbx}

As illustrated in Figure~\ref{fig-ai-triangle}, the core of any machine
learning system consists of three interrelated components that form a
triangular dependency: Models/Algorithms, Data, and Computing
Infrastructure. Each element shapes the possibilities of the others. The
model architecture dictates both the computational demands for training
and inference, as well as the volume and structure of data required for
effective learning. The data's scale and complexity influence what
infrastructure is needed for storage and processing, while determining
which model architectures are feasible. The infrastructure capabilities
establish practical limits on both model scale and data processing
capacity, creating a framework within which the other components must
operate.

\begin{figure}[htb]

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/ea5286feb1dcd15909e6144e9effc8835364b832.pdf}}

}

\caption{\label{fig-ai-triangle}\textbf{Component Interdependencies}:
Machine learning system performance relies on the coordinated
interaction of models, data, and computing infrastructure; limitations
in any one component constrain the capabilities of the others. Effective
system design requires balancing these interdependencies to optimize
overall performance and feasibility.}

\end{figure}%

Each component serves a distinct but interconnected purpose:

\begin{itemize}
\item
  \textbf{Algorithms}: Mathematical models and methods that learn
  patterns from data to make predictions or decisions
\item
  \textbf{Data}: Processes and infrastructure for collecting, storing,
  processing, managing, and serving data for both training and inference
\item
  \textbf{Computing}: Hardware and software infrastructure that enables
  training, serving, and operation of models at scale
\end{itemize}

As the triangle illustrates, no single element can function in
isolation. Algorithms require data and computing resources, large
datasets require algorithms and infrastructure to be useful, and
infrastructure requires algorithms and data to serve any purpose.

Space exploration provides an apt analogy for these relationships.
Algorithm developers resemble astronauts exploring new frontiers and
making discoveries. Data science teams function like mission control
specialists ensuring constant flow of critical information and resources
for mission operations. Computing infrastructure engineers resemble
rocket engineers designing and building systems that enable missions.
Just as space missions require seamless integration of astronauts,
mission control, and rocket systems, machine learning systems demand
careful orchestration of algorithms, data, and computing infrastructure.

These interdependencies become clear when examining breakthrough moments
in AI history. The 2012 AlexNet\sidenote{\textbf{AlexNet}: A
breakthrough deep learning model created by Alex Krizhevsky, Ilya
Sutskever, and Geoffrey Hinton that won the 2012 ImageNet competition by
a massive margin, reducing top-5 error rates from 26.2\% to 15.3\%. This
was the ``ImageNet moment'' that proved deep learning could outperform
traditional computer vision approaches and sparked the modern AI
revolution. AlexNet demonstrated that with enough data (1.2 million
images), computing power (two GPUs for 6 days), and clever engineering
(dropout, data augmentation), neural networks could achieve superhuman
performance on complex visual tasks. } breakthrough illustrates the
principle of hardware-software co-design that defines modern ML systems
engineering. This deep learning revolution succeeded because the
algorithmic innovation (convolutional neural networks) matched the
hardware capability (parallel GPU architectures), graphics processing
units originally designed for gaming but repurposed for AI computations,
providing 10-100x speedups over traditional CPUs for machine learning
tasks. Convolutional operations are inherently parallel, making them
naturally suited to GPU's thousands of parallel cores. This co-design
approach continues to shape ML system development across the industry.

With this three-component framework established, we must understand a
fundamental difference that distinguishes ML systems from traditional
software: how failures manifest across the AI Triangle's components.

\phantomsection\label{quiz-question-sec-introduction-defining-ml-systems-bf7d}
\begin{fbx}{callout-quiz-question}{Self-Check: Question 1.3}{}
\phantomsection\label{quiz-question-sec-introduction-defining-ml-systems-bf7d}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Which of the following best describes a machine learning system?

  \begin{enumerate}
  \def\labelenumii{\alph{enumii})}
  \tightlist
  \item
    A computing system that integrates data, algorithms, and computing
    infrastructure.
  \item
    A standalone algorithm that processes data.
  \item
    A software application that uses pre-defined rules to make
    decisions.
  \item
    A data storage system optimized for large datasets.
  \end{enumerate}
\item
  True or False: In a machine learning system, the model architecture
  does not influence the computational demands for training and
  inference.
\item
  In the context of ML systems, what role does computing infrastructure
  play?

  \begin{enumerate}
  \def\labelenumii{\alph{enumii})}
  \tightlist
  \item
    It solely stores and retrieves data.
  \item
    It provides the necessary resources for both training and inference.
  \item
    It is only responsible for serving the model predictions.
  \item
    It determines the model architecture to be used.
  \end{enumerate}
\item
  Consider a scenario where an ML system's data component is limited by
  storage capacity. How might this affect the other components of the
  system?
\end{enumerate}

\noindent\hspace*{1.25em}\hyperref[quiz-answer-sec-introduction-defining-ml-systems-bf7d]{\textbf{See Answer~$\rightarrow$}}

\end{fbx}

\section{How ML Systems Differ from Traditional
Software}\label{sec-introduction-ml-systems-differ-traditional-software-4370}

The AI Triangle framework reveals what ML systems comprise: data that
guides behavior, algorithms that extract patterns, and infrastructure
that enables learning and inference. However, understanding these
components alone does not capture what makes ML systems engineering
fundamentally different from traditional software engineering. The
critical distinction lies in how these systems fail.

Traditional software exhibits explicit failure modes. When code breaks,
applications crash, error messages propagate, and monitoring systems
trigger alerts. This immediate feedback enables rapid diagnosis and
remediation. The system operates correctly or fails observably. Machine
learning systems operate under a fundamentally different paradigm: they
can continue functioning while their performance degrades silently
without triggering conventional error detection mechanisms. The
algorithms continue executing, the infrastructure maintains prediction
serving, yet the learned behavior becomes progressively less accurate or
contextually relevant.

Consider how an autonomous vehicle's perception system illustrates this
distinction. Traditional automotive software exhibits binary operational
states: the engine control unit either manages fuel injection correctly
or triggers diagnostic warnings. The failure mode remains observable
through standard monitoring. An ML-based perception system presents a
qualitatively different challenge: the system's accuracy in detecting
pedestrians might decline from 95\% to 85\% over several months due to
seasonal changes---different lighting conditions, clothing patterns, or
weather phenomena underrepresented in training data. The vehicle
continues operating, successfully detecting most pedestrians, yet the
degraded performance creates safety risks that become apparent only
through systematic monitoring of edge cases and comprehensive
evaluation. Conventional error logging and alerting mechanisms remain
silent while the system becomes measurably less safe.

This silent degradation manifests across all three AI Triangle
components. The data distribution shifts as the world changes: user
behavior evolves, seasonal patterns emerge, new edge cases appear. The
algorithms continue making predictions based on outdated learned
patterns, unaware that their training distribution no longer matches
operational reality. The infrastructure faithfully serves these
increasingly inaccurate predictions at scale, amplifying the problem. A
recommendation system experiencing this degradation might decline from
85\% accuracy to 60\% over six months as user preferences evolve and
training data becomes stale. The system continues generating
recommendations, users receive results, the infrastructure reports
healthy uptime metrics, yet business value silently erodes. This
degradation often stems from training-serving skew, where features
computed differently between training and serving pipelines cause model
performance to degrade despite unchanged code, which is an
infrastructure issue that manifests as algorithmic failure.

This fundamental difference in failure modes distinguishes ML systems
from traditional software in ways that demand new engineering practices.
Traditional software development focuses on eliminating bugs and
ensuring deterministic behavior. ML systems engineering must
additionally address probabilistic behaviors, evolving data
distributions, and performance degradation that occurs without code
changes. The monitoring systems must track not just infrastructure
health but also model performance, data quality, and prediction
distributions. The deployment practices must enable continuous model
updates as data distributions shift. The entire system lifecycle, from
data collection through model training to inference serving, must be
designed with silent degradation in mind.

This operational reality establishes why ML systems developed in
research settings require specialized engineering practices to reach
production deployment. The unique lifecycle and monitoring requirements
that ML systems demand stem directly from this failure characteristic,
establishing the fundamental motivation for ML systems engineering as a
distinct discipline.

Understanding how ML systems fail differently raises an important
question: given the three components of the AI Triangle---data,
algorithms, and infrastructure---which should we prioritize to advance
AI capabilities? Should we invest in better algorithms, larger datasets,
or more powerful computing infrastructure? The answer to this question
reveals why systems engineering has become central to AI progress.

\phantomsection\label{quiz-question-sec-introduction-ml-systems-differ-traditional-software-4370}
\begin{fbx}{callout-quiz-question}{Self-Check: Question 1.4}{}
\phantomsection\label{quiz-question-sec-introduction-ml-systems-differ-traditional-software-4370}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  What is the fundamental difference in failure modes between
  traditional software and ML systems?

  \begin{enumerate}
  \def\labelenumii{\alph{enumii})}
  \tightlist
  \item
    Traditional software crashes visibly while ML systems can degrade
    silently without triggering alerts.
  \item
    Traditional software requires more monitoring than ML systems.
  \item
    ML systems always fail faster than traditional software.
  \item
    Traditional software cannot handle errors while ML systems have
    built-in error recovery.
  \end{enumerate}
\item
  Explain how the concept of `silent performance degradation'
  differentiates machine learning systems from traditional software
  systems.
\item
  True or False: ML systems can maintain optimal performance without
  specialized monitoring approaches beyond traditional software metrics.
\item
  Why do ML systems require different monitoring approaches compared to
  traditional software systems?
\end{enumerate}

\noindent\hspace*{1.25em}\hyperref[quiz-answer-sec-introduction-ml-systems-differ-traditional-software-4370]{\textbf{See Answer~$\rightarrow$}}

\end{fbx}

\section{The Bitter Lesson: Why Systems Engineering
Matters}\label{sec-introduction-bitter-lesson-systems-engineering-matters-dede}

The single biggest lesson from 70 years of AI research is that systems
that can leverage massive computation ultimately win. This is why
systems engineering, not just algorithmic cleverness, has become the
bottleneck for progress in AI.

The evolution from symbolic AI through statistical learning to deep
learning raises a fundamental question for system builders: Should we
focus on developing more sophisticated algorithms, curating better
datasets, or building more powerful infrastructure?

The answer to this question shapes how we approach building AI systems
and reveals why systems engineering has emerged as a discipline.

History provides a consistent answer. Across decades of AI research, the
greatest breakthroughs have not come from better encoding of human
knowledge or more algorithmic techniques, but from finding ways to
leverage greater computational resources more effectively. This pattern,
articulated by reinforcement learning pioneer Richard
Sutton\sidenote{\textbf{Richard Sutton}: A pioneering AI researcher who
transformed how machines learn through reinforcement learning---teaching
AI systems to learn from trial and error, like how you learned to ride a
bike through practice rather than instruction manuals. At the University
of Alberta, Sutton co-authored the foundational textbook ``Reinforcement
Learning: An Introduction'' and developed key algorithms (TD-learning,
policy gradients) that power everything from AlphaGo to modern robotics.
He received the 2024 ACM Turing Award (computing's highest honor, often
called the ``Nobel Prize of Computing'') shared with Andrew Barto for
their decades of foundational contributions to how AI systems learn and
adapt. His ``Bitter Lesson'' essay distills 70 years of AI history into
one profound insight: general methods leveraging computation
consistently beat approaches that encode human expertise. } in his 2019
essay ``The Bitter Lesson'' (\citeproc{ref-sutton2019bitter}{Sutton
2019}), suggests that systems engineering has become the determinant of
AI success.

Sutton observed that approaches emphasizing human expertise and domain
knowledge, while providing short-term improvements, are consistently
surpassed by general methods that can leverage massive computational
resources. He writes: ``The biggest lesson that can be read from 70
years of AI research is that general methods that leverage computation
are ultimately the most effective, and by a large margin.''

This principle finds validation across AI breakthroughs. In chess, IBM's
Deep Blue defeated world champion Garry Kasparov in 1997
(\citeproc{ref-campbell2002deep}{Campbell, Hoane, and Hsu 2002}) not by
encoding chess strategies, but through brute-force search evaluating
millions of positions per second. In Go, DeepMind's AlphaGo
(\citeproc{ref-silver2016mastering}{Silver et al. 2016}) achieved
superhuman performance by learning from self-play rather than studying
centuries of human Go wisdom. In computer vision, convolutional neural
networks that learn features directly from data have surpassed decades
of hand-crafted feature engineering. In speech recognition, end-to-end
deep learning systems have outperformed approaches built on detailed
models of human phonetics and linguistics.

The ``bitter'' aspect of this lesson is that our intuition misleads us.
We naturally assume that encoding human expertise should be the path to
artificial intelligence. Yet repeatedly, systems that leverage
computation to learn from data outperform systems that rely on human
knowledge, given sufficient scale. This pattern has held across symbolic
AI, statistical learning, and deep learning eras---a consistency we'll
examine in detail when we trace AI's historical evolution in the next
section.

Consider modern language models like GPT-4 or image generation systems
like DALL-E. Their capabilities emerge not from linguistic or artistic
theories encoded by humans, but from training general-purpose neural
networks on vast amounts of data using enormous computational resources.
Training GPT-3 consumed approximately 1,287 MWh of energy
(\citeproc{ref-strubell2019energy}{Strubell, Ganesh, and McCallum 2019};
\citeproc{ref-patterson2021carbon}{Patterson et al. 2021}), equivalent
to 120 U.S. homes for a year, while serving the model to millions of
users requires data centers consuming megawatts of continuous power. The
engineering challenge is building systems that can manage this scale:
collecting and processing petabytes of training data, coordinating
training across thousands of GPUs each consuming 300-500 watts, serving
models to millions of users with millisecond latency while managing
thermal and power constraints\sidenote{\textbf{Thermal and Power
Constraints}: The physical limits imposed by heat generation and power
consumption in computing hardware. Modern GPUs consume 300-700W each
(equivalent to 3-7 hair dryers running continuously) and generate
enormous heat that must be removed via sophisticated cooling systems. A
single AI training cluster with 1,000 GPUs consumes 300-700 kW of power
just for computation, plus 30-50\% more for cooling, totaling
\textasciitilde1MW---equivalent to powering 750 homes. Data centers hit
thermal density limits: you can only pack so many hot chips together
before cooling becomes impossible or prohibitively expensive. These
constraints drive hardware design choices (chip architectures optimized
for performance-per-watt), infrastructure decisions (liquid cooling
vs.~air cooling), and economic trade-offs (power costs can exceed
hardware costs over 3-year lifespans). Power/thermal management explains
many ML system architecture decisions, from edge deployment to model
compression. }, and continuously updating systems based on real-world
performance.

These scale requirements reveal a technical reality: the primary
constraint in modern ML systems is not compute capacity but memory
bandwidth\sidenote{\textbf{Memory Bandwidth}: The rate at which data can
be transferred between memory and processors, measured in GB/s
(gigabytes per second). Modern GPUs like the H100 provide
\textasciitilde3TB/s memory bandwidth, while CPUs typically provide
100-200 GB/s. This seemingly large number becomes the bottleneck for ML
workloads: a transformer model with 70 billion parameters requires 140GB
just to store weights, taking 47ms to load at 3TB/s before any
computation begins. The bandwidth constraint explains why ML
accelerators focus on higher bandwidth memory (HBM) rather than just
faster compute units. For comparison, arithmetic operations are
relatively cheap: a GPU can perform trillions of multiply-add operations
in the time it takes to move 1GB from memory, creating a fundamental
tension where processors spend more time waiting for data than
computing. }, the rate at which data can move between storage and
processing units. This memory wall represents the primary bottleneck
that determines system performance. Modern ML systems are memory bound,
with matrix multiply operations achieving only 1-10\% of theoretical
peak FLOPS because processors spend most of their time waiting for data
rather than computing. Moving 1GB from DRAM costs approximately 1000x
more energy than a 32-bit multiply operation, making data movement the
dominant factor in both performance and energy consumption. Amdahl's
Law\sidenote{\textbf{Amdahl's Law}: Formulated by computer architect
Gene Amdahl in 1967, this law quantifies the theoretical speedup of a
program when only part of it can be parallelized. The speedup is limited
by the sequential portion: if P is the fraction that can be
parallelized, maximum speedup = 1/(1-P). For example, if 90\% of a
program can be parallelized, maximum speedup is 10x regardless of
processor count. In ML systems, this explains why memory bandwidth and
data movement often become the primary bottlenecks rather than compute
capacity. } quantifies this fundamental limitation: if data movement
consumes 80\% of execution time, even infinite compute capacity provides
only 1.25x speedup (since only the remaining 20\% can be accelerated).
This memory wall drives all modern architectural innovations, from
in-memory computing and near-data processing to specialized accelerators
that co-locate compute and storage elements. These system-scale
challenges represent core engineering problems that this book explores
systematically.

Sutton's bitter lesson helps explain the motivation for this book. If AI
progress depends on our ability to scale computation effectively, then
understanding how to build, deploy, and maintain these computational
systems becomes the most important skill for AI practitioners. ML
systems engineering has become important because creating modern systems
requires coordinating thousands of GPUs across multiple data centers,
processing petabytes of text data, and serving resulting models to
millions of users with millisecond latency requirements. This challenge
demands expertise in distributed systems\sidenote{\textbf{Distributed
Systems}: Computing systems where components run on multiple networked
machines and coordinate through message passing. Modern ML training
exemplifies distributed systems complexity: training GPT-3 required
coordinating 1,024 V100 GPUs across multiple data centers, each
processing different data batches while synchronizing gradient updates.
Key challenges include fault tolerance (handling machine failures
mid-training), network bottlenecks (all-reduce operations can consume
40\%+ of total training time), and consistency (ensuring all nodes use
the same model weights). Unlike traditional distributed systems focused
on serving requests, ML distributed systems must coordinate massive data
movement and maintain numerical precision across thousands of nodes,
making consensus algorithms and load balancing far more complex. }, data
engineering, hardware optimization, and operational practices that
represent an entirely new engineering discipline.

The convergence of these systems-level challenges suggests that no
existing discipline addresses what modern AI requires. While Computer
Science advances ML algorithms and Electrical Engineering develops
specialized AI hardware, neither discipline alone provides the
engineering principles needed to deploy, optimize, and sustain ML
systems at scale. This gap requires a new engineering discipline. But to
understand why this discipline has emerged now and what form it takes,
we must first trace the evolution of AI itself, from early symbolic
systems to modern machine learning.

\phantomsection\label{quiz-question-sec-introduction-bitter-lesson-systems-engineering-matters-dede}
\begin{fbx}{callout-quiz-question}{Self-Check: Question 1.5}{}
\phantomsection\label{quiz-question-sec-introduction-bitter-lesson-systems-engineering-matters-dede}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  What is the primary lesson from 70 years of AI research according to
  Richard Sutton's `Bitter Lesson'?

  \begin{enumerate}
  \def\labelenumii{\alph{enumii})}
  \tightlist
  \item
    Leveraging massive computational resources
  \item
    Curating better datasets
  \item
    Developing more sophisticated algorithms
  \item
    Encoding human expertise into AI systems
  \end{enumerate}
\item
  Explain why systems engineering has become more critical than
  algorithmic development in modern AI systems.
\item
  True or False: The primary constraint in modern ML systems is compute
  capacity rather than memory bandwidth.
\item
  Which factor is NOT a primary challenge in scaling modern AI systems?

  \begin{enumerate}
  \def\labelenumii{\alph{enumii})}
  \tightlist
  \item
    Thermal and power constraints
  \item
    Memory bandwidth limitations
  \item
    Data center coordination
  \item
    Algorithmic complexity
  \end{enumerate}
\item
  In a production system, how might you address the memory bandwidth
  bottleneck when deploying large-scale ML models?
\end{enumerate}

\noindent\hspace*{1.25em}\hyperref[quiz-answer-sec-introduction-bitter-lesson-systems-engineering-matters-dede]{\textbf{See Answer~$\rightarrow$}}

\end{fbx}

\section{Historical Evolution of AI
Paradigms}\label{sec-introduction-historical-evolution-ai-paradigms-796e}

The systems-centric perspective we've established through the Bitter
Lesson didn't emerge overnight. It developed through decades of AI
research where each major transition revealed new insights about the
relationship between algorithms, data, and computational infrastructure.
Tracing this evolution helps us understand not just technological
progress, but the shifts in approach that explain today's emphasis on
scalable systems.

Understanding why this transition to systems-focused ML is happening now
requires recognizing the convergence of three factors in the last
decade:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Massive Datasets}: The internet age created unprecedented data
  volumes through web content, social media, sensor networks, and
  digital transactions. Public datasets like ImageNet (millions of
  labeled images) and Common Crawl (billions of web pages) provide the
  raw material for learning complex patterns.
\item
  \textbf{Algorithmic Breakthroughs}: Deep learning proved remarkably
  effective across diverse domains, from computer vision to natural
  language processing. Techniques like transformers, attention
  mechanisms, and transfer learning enabled models to learn
  generalizable representations from data.
\item
  \textbf{Hardware Acceleration}: Graphics Processing Units (GPUs)
  originally designed for gaming provided 10-100x speedups for machine
  learning computations. Cloud computing infrastructure made this
  computational power accessible without massive capital investments.
\end{enumerate}

This convergence explains why we've moved from theoretical models to
large-scale deployed systems requiring a new engineering discipline.
Each factor amplified the others: bigger datasets demanded more
computation, better algorithms justified larger datasets, and faster
hardware enabled more algorithms. This convergence transformed AI from
an academic curiosity to a production technology requiring robust
engineering practices.

The evolution of AI, depicted in the timeline shown in
Figure~\ref{fig-ai-timeline}, highlights key milestones such as the
development of the perceptron\sidenote{\textbf{Perceptron}: One of the
first computational learning algorithms (1957), simple enough to
implement in hardware with minimal memory---1950s mainframes could only
store thousands of weights, not millions. This hardware constraint
shaped early AI research toward simple, interpretable models. The
Perceptron's limitation to linearly separable problems wasn't just
algorithmic---multi-layer networks (which could solve non-linear
problems) were proposed in the 1960s but remained computationally
intractable until the 1980s when memory became cheaper and CPUs faster.
This 20-year gap between algorithmic insight and practical
implementation foreshadowed a pattern in AI: breakthrough algorithms
often wait decades for hardware to catch up, explaining why ML systems
engineering focuses on co-designing algorithms with available
infrastructure. } in 1957 by Frank Rosenblatt
(\citeproc{ref-rosenblatt1957perceptron}{Wolfe et al. 2024}), an early
computational learning algorithm. Computer labs in 1965 contained
room-sized mainframes\sidenote{\textbf{Mainframes}: Room-sized computers
that dominated the 1960s-70s, typically costing millions of dollars and
requiring dedicated cooling systems. IBM's System/360 mainframe from
1964 weighed up to 20,000 pounds and had 8KB-1MB of memory depending on
model, about 1/millionth the memory of a modern smartphone, yet
represented the cutting edge of computing power that enabled early AI
research. } running programs that could prove basic mathematical
theorems or play simple games like tic-tac-toe. These early artificial
intelligence systems, though groundbreaking for their time, differed
substantially from today's machine learning systems that detect cancer
in medical images or understand human speech. The timeline shows the
progression from early innovations like the
ELIZA\sidenote{\textbf{ELIZA}: Created by MIT's Joseph Weizenbaum in
1966 (\citeproc{ref-weizenbaum1966eliza}{Weizenbaum 1966}), ELIZA was
one of the first chatbots that could simulate human conversation by
pattern matching and substitution. From a systems perspective, ELIZA ran
on 256KB mainframes using simple pattern matching---no learning, no data
storage, no training phase. This computational simplicity allowed
real-time interaction on 1960s hardware but resulted in brittleness that
motivated the shift to data-driven ML. Modern chatbots like GPT-3
require vastly more infrastructure (350GB model parameters when
uncompressed, \$4.6M training cost estimate, GPU servers for inference)
but handle conversations ELIZA couldn't---illustrating the systems
trade-off: rule-based systems are computationally cheap but brittle,
while ML systems are infrastructure-intensive but flexible. Ironically,
Weizenbaum was horrified when people formed emotional attachments to his
simple program, leading him to become a critic of AI. } chatbot in 1966,
to significant breakthroughs such as IBM's Deep Blue defeating chess
champion Garry Kasparov in 1997
(\citeproc{ref-campbell2002deep}{Campbell, Hoane, and Hsu 2002}). More
recent advancements include the introduction of OpenAI's GPT-3 in 2020
and GPT-4 in 2023 (\citeproc{ref-openai2023gpt4}{OpenAI et al. 2023}),
demonstrating the dramatic evolution and increasing complexity of AI
systems over the decades.

\begin{figure}[t!]

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/526f5dbf27d3fd3cb127e58e2403f7ab771dba45.pdf}}

}

\caption{\label{fig-ai-timeline}\textbf{AI Development Timeline}: Early
AI research focused on symbolic reasoning and rule-based systems, while
modern AI leverages data-driven approaches like neural networks to
achieve increasingly complex tasks. This progression exposes a shift
from hand-coded intelligence to learned intelligence, marked by
milestones such as the perceptron, deep blue, and large language models
like GPT-3.}

\end{figure}%

Examining this timeline reveals several distinct eras of development,
each building upon the lessons of its predecessors while addressing
limitations that prevented earlier approaches from achieving their
promise.

\subsection{Symbolic AI
Era}\label{sec-introduction-symbolic-ai-era-9d27}

The story of machine learning begins at the historic Dartmouth
Conference\sidenote{\textbf{Dartmouth Conference (1956)}: The legendary
8-week workshop at Dartmouth College where AI was officially born.
Organized by John McCarthy, Marvin Minsky, Nathaniel Rochester, and
Claude Shannon, it was the first time researchers gathered specifically
to discuss ``artificial intelligence,'' a term McCarthy coined for the
proposal. The ambitious goal was to make machines ``simulate every
aspect of learning or any other feature of intelligence.'' From a
systems perspective, participants fundamentally underestimated resource
requirements---they assumed AI would fit on 1950s hardware (64KB memory
maximum, kilohertz to low megahertz processors). Reality required
1,000,000x more resources: modern language models use 350GB memory and
exaflops of training compute. This million-fold miscalculation of scale
requirements helps explain why early symbolic AI failed: researchers
focused on algorithmic cleverness while ignoring infrastructure
constraints. The lesson: AI progress requires both algorithmic
innovation AND systems engineering to provide necessary computational
resources. } in 1956, where pioneers like John McCarthy, Marvin Minsky,
and Claude Shannon first coined the term ``artificial intelligence''
(\citeproc{ref-mccarthy1956dartmouth}{McCarthy et al. 1955}). Their
approach assumed that intelligence could be reduced to symbol
manipulation. Daniel Bobrow's STUDENT system from 1964
(\citeproc{ref-bobrow1964student}{Bobrow 1964}) exemplifies this era by
solving algebra word problems through natural language understanding.

\phantomsection\label{callout-exampleux2a-1.3}
\begin{fbx}{callout-example}{Example: }{STUDENT (1964)}
\phantomsection\label{callout-example*-1.3}

\begin{verbatim}
Problem: "If the number of customers Tom gets is twice the
square of 20% of the number of advertisements he runs, and
the number of advertisements is 45, what is the number of
customers Tom gets?"

STUDENT would:

1. Parse the English text
2. Convert it to algebraic equations
3. Solve the equation: n = 2(0.2 × 45)²
4. Provide the answer: 162 customers
\end{verbatim}

\end{fbx}

Early AI like STUDENT suffered from a limitation: they could only handle
inputs that exactly matched their pre-programmed patterns and rules.
This ``brittleness''\sidenote{\textbf{Brittleness in AI Systems}: The
tendency of rule-based systems to fail completely when encountering
inputs that fall outside their programmed scenarios, no matter how
similar those inputs might be to what they were designed to handle. This
contrasts with human intelligence, which can adapt and make reasonable
guesses even in unfamiliar situations. From a systems perspective,
brittleness made deployment infeasible beyond controlled lab
conditions---each new edge case required programmer intervention,
creating unsustainable operational overhead. A speech recognition system
encountering a new accent would fail rather than degrade gracefully,
requiring system updates rather than continuous operation. ML's ability
to generalize enables real-world deployment despite unpredictable
inputs, shifting the challenge from explicit rule programming to
infrastructure for collecting training data and continuously updating
models as new patterns emerge. } meant that while these solutions could
appear intelligent when handling very specific cases they were designed
for, they would break down completely when faced with even minor
variations or real-world complexity. This limitation drove the evolution
toward statistical approaches that we'll examine in the next section.

\subsection{Expert Systems
Era}\label{sec-introduction-expert-systems-era-c7dd}

Recognizing the limitations of symbolic AI, researchers by the mid-1970s
acknowledged that general AI was overly ambitious and shifted their
focus to capturing human expert knowledge in specific, well-defined
domains. MYCIN (\citeproc{ref-shortliffe1976mycin}{Shortliffe 1975}),
developed at Stanford, emerged as one of the first large-scale expert
systems designed to diagnose blood infections.

\phantomsection\label{callout-exampleux2a-1.4}
\begin{fbx}{callout-example}{Example: }{MYCIN (1976)}
\phantomsection\label{callout-example*-1.4}

\begin{verbatim}
Rule Example from MYCIN:
IF
  The infection is primary-bacteremia
  The site of the culture is one of the sterile sites
  The suspected portal of entry is the gastrointestinal tract
THEN
  Found suggestive evidence (0.7) that infection is bacteroid
\end{verbatim}

\end{fbx}

MYCIN represented a major advance in medical AI with 600 expert rules
for diagnosing blood infections, yet it revealed key challenges
persisting in contemporary ML. Getting domain knowledge from human
experts and converting it into precise rules proved time-consuming and
difficult, as doctors often couldn't explain exactly how they made
decisions. MYCIN struggled with uncertain or incomplete information,
unlike human doctors who could make educated guesses. Maintaining and
updating the rule base became more complex as MYCIN grew, as adding new
rules frequently conflicted with existing ones, while medical knowledge
itself continued to evolve. Knowledge capture, uncertainty handling, and
maintenance remain concerns in modern machine learning, addressed
through different technical approaches.

\subsection{Statistical Learning
Era}\label{sec-introduction-statistical-learning-era-8116}

These challenges with knowledge capture and system maintenance drove
researchers toward a different approach. The 1990s marked a
transformation in artificial intelligence as the field shifted from
hand-coded rules toward statistical learning approaches.

Three converging factors made statistical methods possible and powerful.
First, the digital revolution meant massive amounts of data were
available to train algorithms. Second, Moore's Law
(\citeproc{ref-moore1965cramming}{Moore 1998})\sidenote{\textbf{Moore's
Law}: The observation made by Intel co-founder Gordon Moore in 1965 that
the number of transistors on a microchip doubles approximately every two
years, while the cost halves. Moore's Law enabled ML by providing
approximately 1,000x more transistor density from 2000-2020, making
previously impossible algorithms practical---neural networks proposed in
the 1980s became viable only after 2010. However, slowing Moore's Law
(transistor doubling now takes 3-4 years) drives innovation in
specialized accelerators (TPUs provide 15-30x gains over GPUs through
custom ML hardware) and algorithmic efficiency (techniques like
quantization and pruning reduce compute requirements 4-10x). The systems
lesson: when general hardware improvements slow, specialized hardware
and efficient algorithms become critical. } delivered the computational
power needed to process this data effectively. Third, researchers
developed new algorithms like Support Vector Machines and improved
neural networks that could learn patterns from data rather than
following pre-programmed rules.

This combination transformed AI development: rather than encoding human
knowledge directly, machines could discover patterns automatically from
examples, creating more robust and adaptable systems.

Email spam filtering evolution illustrates this transformation. Early
rule-based systems used explicit patterns but exhibited the same
brittleness we saw with symbolic AI systems, proving easily
circumvented. Statistical systems took a different approach: if the word
`viagra' appears in 90\% of spam emails but only 1\% of normal emails,
we can use this pattern to identify spam. Rather than writing explicit
rules, statistical systems learn these patterns automatically from
thousands of example emails, making them adaptable to new spam
techniques. The mathematical foundation relies on Bayes' theorem to
calculate the probability that an email is spam given specific words:
\(P(\text{spam}|\text{word}) = P(\text{word}|\text{spam}) \times P(\text{spam}) / P(\text{word})\).
For emails with multiple words, we combine these probabilities across
the entire message assuming conditional independence of words given the
class (spam or not spam), which allows efficient computation despite the
simplifying assumption that words don't depend on each other.

\phantomsection\label{callout-exampleux2a-1.5}
\begin{fbx}{callout-example}{Example: }{Early Spam Detection Systems}
\phantomsection\label{callout-example*-1.5}

\begin{verbatim}
Rule-based (1980s):
IF contains("viagra") OR contains("winner") THEN spam

Statistical (1990s):
P(spam|word) = (frequency in spam emails) / (total frequency)

Combined using Naive Bayes:
P(spam|email) ∝ P(spam) × ∏ P(word|spam)
\end{verbatim}

\end{fbx}

Statistical approaches introduced three concepts that remain central to
AI development. First, the quality and quantity of training data became
as important as the algorithms themselves. AI could only learn patterns
that were present in its training examples. Second, rigorous evaluation
methods became necessary to measure AI performance, leading to metrics
that could measure success and compare different approaches. Third, a
tension exists between precision (being right when making a prediction)
and recall (catching all the cases we should find), forcing designers to
make explicit trade-offs based on their application's needs. These
challenges require systematic approaches:
\textbf{?@sec-data-engineering} covers data quality and drift detection,
while \textbf{?@sec-benchmarking-ai} addresses evaluation metrics and
precision-recall trade-offs. Spam filters might tolerate some spam to
avoid blocking important emails, while medical diagnosis systems
prioritize catching every potential case despite increased false alarms.

Table~\ref{tbl-ai-evolution-strengths} summarizes the evolutionary
journey of AI approaches, highlighting key strengths and capabilities
emerging with each paradigm. Moving from left to right reveals important
trends. Before examining shallow and deep learning, understanding
trade-offs between existing approaches provides important context.

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 8\tabcolsep) * \real{0.1786}}
  >{\raggedright\arraybackslash}p{(\linewidth - 8\tabcolsep) * \real{0.1929}}
  >{\raggedright\arraybackslash}p{(\linewidth - 8\tabcolsep) * \real{0.1929}}
  >{\raggedright\arraybackslash}p{(\linewidth - 8\tabcolsep) * \real{0.1929}}
  >{\raggedright\arraybackslash}p{(\linewidth - 8\tabcolsep) * \real{0.2214}}@{}}
\caption{\textbf{AI Paradigm Evolution}: Shifting from symbolic AI to
statistical approaches transformed machine learning by prioritizing data
quantity and quality, enabling rigorous performance evaluation, and
necessitating explicit trade-offs between precision and recall to
optimize system behavior for specific applications. The table outlines
how each paradigm addressed these challenges, revealing a progression
towards data-driven systems capable of handling complex, real-world
problems.}\label{tbl-ai-evolution-strengths}\tabularnewline
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Aspect}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Symbolic AI}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Expert Systems}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Statistical Learning}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Shallow / Deep Learning}
\end{minipage} \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Aspect}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Symbolic AI}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Expert Systems}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Statistical Learning}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Shallow / Deep Learning}
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Key Strength} & Logical reasoning & Domain expertise &
Versatility & Pattern recognition \\
\textbf{Best Use Case} & Well-defined, rule-based problems & Specific
domain problems & Various structured data problems & Complex,
unstructured data problems \\
\textbf{Data Handling} & Minimal data needed & Domain knowledge-based &
Moderate data required & Large-scale data processing \\
\textbf{Adaptability} & Fixed rules & Domain-specific adaptability &
Adaptable to various domains & Highly adaptable to diverse tasks \\
\textbf{Problem Complexity} & Simple, logic-based & Complicated, domain-
specific & Complex, structured & Highly complex, unstructured \\
\end{longtable}

This analysis bridges early approaches with recent developments in
shallow and deep learning. It explains why certain approaches gained
prominence in different eras and how each paradigm built upon
predecessors while addressing their limitations. Earlier approaches
continue to influence modern AI techniques, particularly in foundation
model development.

These core concepts that emerged from statistical learning (data
quality, evaluation metrics, and precision-recall trade-offs) became the
foundation for all subsequent developments in machine learning.

\subsection{Shallow Learning
Era}\label{sec-introduction-shallow-learning-era-2500}

Building on these statistical foundations, the 2000s marked a
significant period in machine learning history known as the ``shallow
learning'' era. The term ``shallow'' refers to architectural depth:
shallow learning typically employed one or two processing levels,
contrasting with deep learning's multiple hierarchical layers that
emerged later.

During this time, several algorithms dominated the machine learning
landscape. Each brought unique strengths to different problems: Decision
trees\sidenote{\textbf{Decision Trees}: A machine learning algorithm
that makes predictions by following a series of yes/no questions, much
like a flowchart. Popularized in the 1980s, decision trees are highly
interpretable---you can trace exactly why the algorithm made each
decision. From a systems perspective, decision trees require minimal
memory and compute compared to neural networks: a typical decision tree
model might be 1-10MB versus 100MB-10GB for deep learning models, with
inference taking microseconds on a single CPU core. This makes them
ideal for resource-constrained deployments where model size matters more
than maximum accuracy---embedded systems, mobile devices, or scenarios
requiring real-time decisions with minimal latency. They remain widely
used in medical diagnosis and loan approval where regulations require
explainability. } provided interpretable results by making choices much
like a flowchart. K-nearest neighbors made predictions by finding
similar examples in past data, like asking your most experienced
neighbors for advice. Linear and logistic regression offered
straightforward, interpretable models that worked well for many
real-world problems. Support Vector Machines\sidenote{\textbf{Support
Vector Machines (SVMs)}: A powerful machine learning algorithm developed
by Vladimir Vapnik in the 1990s that finds the optimal boundary between
different categories of data. SVMs were the dominant technique for many
classification problems before deep learning emerged, winning numerous
machine learning competitions. From a systems perspective, SVMs excel
with small datasets (thousands of examples vs millions needed for deep
learning), requiring less training infrastructure---a high-end
workstation can train SVMs that would require GPU clusters for
equivalent deep learning models. However, SVMs don't scale well beyond
\textasciitilde100K data points due to O(n²) to O(n³) training
complexity, limiting their use for massive modern datasets. They remain
deployed in text classification, bioinformatics, and scenarios where
data is limited but accuracy is crucial. } (SVMs) excelled at finding
complex boundaries between categories using the ``kernel
trick''\sidenote{\textbf{Kernel Trick}: A mathematical technique that
allows algorithms like SVMs to find complex, non-linear patterns by
transforming data into higher-dimensional spaces where linear separation
becomes possible. For example, data points that form a circle in 2D
space can be projected into 3D space where they become linearly
separable. From a systems view, the kernel trick trades memory for
computation efficiency: precomputing kernel matrices requires O(n²)
memory, limiting SVMs to datasets under \textasciitilde100K points on
typical hardware (a 100K×100K matrix with 8-byte entries requires 80GB
RAM). This memory constraint explains why deep learning, despite
requiring more computation, scales better to massive datasets---neural
networks' memory requirements grow linearly with data size, not
quadratically. }. This technique transforms complex patterns by
projecting data into higher dimensions where linear separation becomes
possible. These algorithms formed the foundation of practical machine
learning.

A typical computer vision solution from 2005 exemplifies this approach:

\phantomsection\label{callout-exampleux2a-1.6}
\begin{fbx}{callout-example}{Example: }{Traditional Computer Vision Pipeline}
\phantomsection\label{callout-example*-1.6}

\begin{verbatim}
1. Manual Feature Extraction
  - SIFT (Scale-Invariant Feature Transform)
  - HOG (Histogram of Oriented Gradients)
  - Gabor filters
2. Feature Selection/Engineering
3. "Shallow" Learning Model (e.g., SVM)
4. Post-processing
\end{verbatim}

\end{fbx}

This era's hybrid approach combined human-engineered features with
statistical learning. They had strong mathematical foundations
(researchers could prove why they worked). They performed well even with
limited data. They were computationally efficient. They produced
reliable, reproducible results.

The Viola-Jones algorithm (\citeproc{ref-viola2001rapidobject}{Viola and
Jones, n.d.})\sidenote{\textbf{Viola-Jones Algorithm}: A groundbreaking
computer vision algorithm that could detect faces in real-time by using
simple rectangular patterns (like comparing the brightness of eye
regions versus cheek regions) and making decisions in stages, filtering
out non-faces quickly and spending more computation only on promising
candidates. The algorithm achieved real-time performance (24 fps) on
2001 hardware by computing features in \textless0.001ms using integral
images---a clever preprocessing technique that enables constant-time
rectangle sum computation. This efficiency enabled embedded camera
deployment in consumer devices (digital cameras, phones), demonstrating
how algorithm-hardware co-design enables new applications. The cascade
approach reduced computation 10-100x by rejecting easy negatives early,
making real-time vision feasible on CPUs that would be 1000x slower than
modern GPUs. } (2001) exemplifies this era, achieving real-time face
detection using simple rectangular features and cascaded
classifiers\sidenote{\textbf{Cascade of Classifiers}: A multi-stage
decision system where each stage acts as a filter, quickly rejecting
obvious non-matches and passing promising candidates to the next, more
sophisticated stage. This approach is similar to how security screening
works at airports with multiple checkpoints of increasing thoroughness.
From a systems perspective, cascades achieve 10-100x computational
savings by focusing expensive computation only on promising
candidates---early stages might reject 95\% of inputs with 1\% of total
computation. This compute-saving pattern appears throughout edge ML
systems where power budgets matter: modern mobile face detection uses
neural network cascades that process most frames with tiny networks
(\textless1MB), escalating to larger networks (\textgreater10MB) only
for ambiguous cases, enabling continuous face detection on milliwatt
power budgets. }. This algorithm powered digital camera face detection
for nearly a decade.

\subsection{Deep Learning
Era}\label{sec-introduction-deep-learning-era-f6c0}

While Support Vector Machines excelled at finding complex category
boundaries through mathematical transformations, deep learning adopted a
different approach inspired by brain architecture. Rather than relying
on human-engineered features, deep learning employs layers of simple
computational units inspired by brain neurons, with each layer
transforming input data into increasingly abstract representations.
While \textbf{?@sec-dl-primer} establishes the mathematical foundations
of neural networks, \textbf{?@sec-dnn-architectures} explores the
detailed architectures that enable this layered learning approach.

In image processing, this layered approach works systematically. The
first layer detects simple edges and contrasts, subsequent layers
combine these into basic shapes and textures, higher layers recognize
specific features like whiskers and ears, and final layers assemble
these into concepts like ``cat.''

Unlike shallow learning methods requiring carefully engineered features,
deep learning networks automatically discover useful features from raw
data. This layered approach to learning, building from simple patterns
to complex concepts, defines ``deep'' learning and proves effective for
complex, real-world data like images, speech, and text.

AlexNet, shown in Figure~\ref{fig-alexnet}, achieved a breakthrough in
the 2012 ImageNet\sidenote{\textbf{ImageNet}: A massive visual database
containing over 14 million labeled images across 21,841 categories (full
dataset), created by Stanford's Fei-Fei Li starting in 2009
(\citeproc{ref-deng2009imagenet}{Deng et al. 2009}). The annual ImageNet
challenge became the Olympics of computer vision, driving breakthrough
after breakthrough in image recognition until neural networks became so
good they essentially solved the competition. From a systems
perspective, ImageNet's \textasciitilde150GB size (2009) was manageable
on single-server storage systems. Modern vision datasets like LAION-5B
(5 billion image-text pairs, \textasciitilde240TB of images) require
distributed storage infrastructure and parallel data loading pipelines
during training. This 1000x growth in dataset size drove innovations in
distributed data engineering---systems must now shard datasets across
dozens of storage nodes and coordinate parallel data loading to keep
thousands of GPUs fed with training examples. } competition that
transformed machine learning through a perfect alignment of algorithmic
innovation and hardware capability. The network required two NVIDIA GTX
580 GPUs with 3GB memory each, delivering 2.3 TFLOPS peak performance
per GPU, but the real breakthrough was memory bandwidth utilization.
Each GTX 580 provided 192.4 GB/s memory bandwidth, and AlexNet's
convolutional operations required approximately 288 GB/s total memory
bandwidth (theoretical peak) to feed the computation engines---making
this the first neural network specifically designed around memory
bandwidth constraints rather than just compute requirements. The 60
million parameters demanded 240MB storage, while training on 1.2 million
images required sophisticated memory management to split the network
across GPU boundaries and coordinate gradient updates. Training consumed
approximately 1,287 GPU-hours over 6 days, achieving 15.3\% top-5 error
rate compared to 26.2\% for second place, a 42\% relative improvement
that demonstrated the power of hardware-software co-design. This
represented a 10-100x speedup over CPU implementations, reducing
training time from months to days and proving that specialized hardware
could unlock previously intractable algorithms
(\citeproc{ref-krizhevsky2012imagenet}{Krizhevsky, Sutskever, and Hinton
2017}).

The success of AlexNet wasn't just a technical achievement; it was a
watershed moment that demonstrated the practical viability of deep
learning. This breakthrough required both algorithmic innovation and
systems engineering advances. The achievement wasn't just algorithmic,
it was enabled by framework infrastructure like Theano that could
orchestrate GPU parallelism, handle automatic differentiation at scale,
and manage the complex computational workflows that deep learning
demands. Without these framework foundations, the algorithmic insights
would have remained computationally intractable.

This pattern of requiring both algorithmic and systems breakthroughs has
defined every major AI advance since. Modern frameworks represent
infrastructure that transforms algorithmic possibilities into practical
realities. Automatic differentiation (autograd) systems represent
perhaps the most important innovation that makes modern deep learning
possible, handling gradient computation automatically and enabling the
complex architectures we use today. Understanding this framework-centric
perspective (that major AI capabilities emerge from the intersection of
algorithms and systems engineering) is important for building robust,
scalable machine learning systems. This single result triggered an
explosion of research and applications in deep learning that continues
to this day. The infrastructure requirements that enabled this
breakthrough represent the convergence of algorithmic innovation with
systems engineering that this book explores.

\begin{figure}[htb]

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/55af8330bda4876536081f4c475ea1edd454db22.pdf}}

}

\caption{\label{fig-alexnet}\textbf{Convolutional Neural Network
Architecture}: AlexNet demonstrated that deep neural networks could
automatically learn effective features from images, dramatically
outperforming traditional computer vision methods. This breakthrough
showed that with sufficient data and computing power, neural networks
could achieve remarkable accuracy in image recognition tasks.}

\end{figure}%

Deep learning subsequently entered an era of extraordinary scale. By the
late 2010s, companies like Google, Facebook, and OpenAI trained neural
networks thousands of times larger than AlexNet. These massive models,
often called ``foundation models''\sidenote{\textbf{Foundation Models}:
Large-scale AI models trained on broad datasets that serve as the
``foundation'' for many different applications through fine-tuning, like
GPT for language tasks or CLIP for vision tasks. The term was coined by
Stanford's AI researchers in 2021 to capture how these models became the
basis for building more specific AI systems. From a systems perspective,
foundation models' size (10-100GB for inference, 350GB+ for training)
creates deployment challenges---organizations must often choose between
accuracy (deploying the full model requiring expensive GPU servers) and
feasibility (using distilled versions that fit on less expensive
hardware). This trade-off drives the emergence of model-as-a-service
architectures where companies like OpenAI provide API access rather than
distributing models, shifting infrastructure costs to centralized
providers. }, expanded deep learning capabilities to new domains.

GPT-3, released in 2020 (\citeproc{ref-brown2020language}{Brown et al.
2020}), contained 175 billion parameters requiring approximately 350GB
to store parameters (800GB+ for full training infrastructure),
representing a 1,000x scale increase from earlier neural networks like
BERT-Large\sidenote{\textbf{BERT-Large}: A transformer-based language
model developed by Google in 2018 with 340 million parameters,
representing the previous generation of large language models before the
GPT era. BERT (Bidirectional Encoder Representations from Transformers)
was revolutionary for understanding context in both directions of a
sentence, but GPT-3's 175 billion parameters dwarfed it by over 500x,
marking the transition to truly large-scale language models. } (340
million parameters). Training GPT-3 consumed approximately 314
zettaFLOPs\sidenote{\textbf{ZettaFLOPs}: A measure of computational
performance equal to one sextillion (10\^{}21) floating-point operations
per second. Training GPT-3 required approximately 3.14 × 10\^{}23 FLOPS
(roughly 314 zettaFLOPs), which would theoretically take 355 years on a
single V100 GPU. This massive computational requirement illustrates why
modern AI training requires distributed systems with thousands of GPUs
working in parallel. } of computation across 1,024 V100
GPUs\sidenote{\textbf{V100 GPUs}: NVIDIA's data center graphics
processing units designed specifically for AI training, featuring 32GB
of high-bandwidth memory (HBM2) and 125 TFLOPS of mixed-precision deep
learning performance. Each V100 cost approximately \$8,000-\$10,000
(2020 pricing), making the 1,024 GPUs used for GPT-3 training worth
roughly \$8-10 million in hardware alone, highlighting the enormous
infrastructure investment required for cutting-edge AI research. } over
several weeks, with training costs estimated at \$4.6 million. The model
processes text at approximately 1.7GB/s memory bandwidth and requires
specialized infrastructure to serve millions of users with sub-second
latency. These models demonstrated remarkable emergent abilities that
appeared only at scale: writing human-like text, engaging in
sophisticated conversation, generating images from descriptions, and
writing functional computer code. These capabilities emerged from the
scale of computation and data rather than explicit programming.

A key insight emerged: larger neural networks trained on more data
became capable of solving increasingly complex tasks. This scale
introduced significant systems challenges\sidenote{\textbf{Large-Scale
Training Challenges}: Training GPT-3 required approximately 3,640
petaflop-days. At \$2-3 per GPU-hour on cloud platforms (2020 pricing),
this translates to approximately \$4.6M in compute costs alone (Lambda
Labs estimate), excluding data preprocessing, experimentation, and
failed training runs (\citeproc{ref-li2020estimating}{Li 2020}). Rule of
thumb: total project cost is typically 3-5x raw compute cost due to
experimentation overhead, making the full GPT-3 development cost
approximately \$15-20M. Modern foundation models can consume 100+
terabytes of training data and require specialized distributed training
techniques to coordinate thousands of accelerators across multiple data
centers. }. Efficiently training large models requires thousands of
parallel GPUs, storing and serving models hundreds of gigabytes in size,
and handling massive training datasets.

The 2012 deep learning revolution built upon neural network research
dating to the 1950s. The story begins with Frank Rosenblatt's Perceptron
in 1957, which captured the imagination of researchers by showing how a
simple artificial neuron could learn to classify patterns. Though
limited to linearly separable problems, as Minsky and Papert's 1969 book
``Perceptrons'' (\citeproc{ref-minsky1969perceptrons}{Minsky and Papert
2017}) demonstrated, it introduced the core concept of trainable neural
networks. The 1980s brought more important breakthroughs: Rumelhart,
Hinton, and Williams introduced backpropagation
(\citeproc{ref-rumelhart1986learning}{Rumelhart, Hinton, and Williams
1986}) in 1986, providing a systematic way to train multi-layer
networks, while Yann LeCun demonstrated its practical application in
recognizing handwritten digits using specialized neural networks
designed for image processing
(\citeproc{ref-lecun1989backpropagation}{LeCun et al.
1989})\sidenote{\textbf{Convolutional Neural Network (CNN)}: A type of
neural network specially designed for processing images, inspired by how
the human visual system works. The ``convolutional'' part refers to how
it scans images in small chunks, similar to how our eyes focus on
different parts of a scene. From a systems perspective, CNNs' parameter
sharing reduces model size 10-100x compared to fully-connected networks
processing the same images---a CNN might use 5-10 million parameters
where a fully-connected network would need 500 million. This dramatic
reduction makes CNNs deployable on mobile devices: MobileNetV2 achieves
70\% ImageNet accuracy in just 14MB (3.5M parameters), enabling
on-device image recognition that would be impossible with
fully-connected networks requiring gigabytes of storage and memory. }.

These networks largely stagnated through the 1990s and 2000s not because
the ideas were incorrect, but because they preceded necessary
technological developments. The field lacked three important
ingredients: sufficient data to train complex networks, enough
computational power to process this data, and the technical innovations
needed to train very deep networks effectively.

Deep learning's potential required the convergence of the three AI
Triangle components we will explore: sufficient data to train complex
networks, enough computational power to process this data, and
algorithmic breakthroughs needed to train very deep networks
effectively. This extended development period explains why the 2012
ImageNet breakthrough represented the culmination of accumulated
research rather than a sudden revolution. This evolution established
machine learning systems engineering as a discipline bridging
theoretical advancements with practical implementation, operating within
the interconnected framework the AI Triangle represents.

This evolution reveals a crucial insight: as AI progressed from symbolic
reasoning to statistical learning and deep learning, applications became
increasingly ambitious and complex. However, this growth introduced
challenges extending beyond algorithms, necessitating engineering entire
systems capable of deploying and sustaining AI at scale. Understanding
how these modern ML systems operate in practice requires examining their
lifecycle characteristics and deployment patterns, which distinguish
them fundamentally from traditional software systems.

\phantomsection\label{quiz-question-sec-introduction-historical-evolution-ai-paradigms-796e}
\begin{fbx}{callout-quiz-question}{Self-Check: Question 1.6}{}
\phantomsection\label{quiz-question-sec-introduction-historical-evolution-ai-paradigms-796e}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Which of the following factors did NOT contribute to the transition
  towards a systems-focused approach in AI?

  \begin{enumerate}
  \def\labelenumii{\alph{enumii})}
  \tightlist
  \item
    Massive datasets from the internet age
  \item
    The development of symbolic AI in the 1950s
  \item
    Increased availability of low-cost GPUs
  \item
    Algorithmic breakthroughs in deep learning
  \end{enumerate}
\item
  Explain how the convergence of massive datasets, algorithmic
  breakthroughs, and hardware acceleration has transformed AI from an
  academic curiosity to a production technology.
\item
  True or False: The systems-centric approach in AI emerged because
  early AI systems were too complex and required simplification.
\item
  The introduction of \_\_\_\_ by OpenAI in 2020 demonstrated the
  increasing complexity and capability of AI systems.
\end{enumerate}

\noindent\hspace*{1.25em}\hyperref[quiz-answer-sec-introduction-historical-evolution-ai-paradigms-796e]{\textbf{See Answer~$\rightarrow$}}

\end{fbx}

\section{Understanding ML System Lifecycle and
Deployment}\label{sec-introduction-understanding-ml-system-lifecycle-deployment-0ab0}

Having traced AI's evolution from symbolic systems through statistical
learning to deep learning, we can now explore how these modern ML
systems operate in practice. Understanding the ML lifecycle and
deployment landscape is important because these factors shape every
engineering decision we make.

\subsection{The ML Development
Lifecycle}\label{sec-introduction-ml-development-lifecycle-05d8}

ML systems fundamentally differ from traditional software in their
development and operational lifecycle. Traditional software follows
predictable patterns where developers write explicit instructions that
execute deterministically\sidenote{\textbf{Deterministic Execution}:
Traditional software produces the same output every time given the same
input, like a calculator that always returns 4 when adding 2+2. This
predictability makes testing straightforward---you can verify correct
behavior by checking that specific inputs produce expected outputs. ML
systems, by contrast, are probabilistic: the same model might produce
slightly different predictions due to randomness in inference or changes
in underlying data patterns. }. These systems build on decades of
established practices: version control maintains precise code histories,
continuous integration pipelines\sidenote{\textbf{Continuous
Integration/Continuous Deployment (CI/CD)}: Automated systems that
continuously test code changes and deploy them to production. When
developers commit code, CI/CD pipelines automatically run tests, check
for errors, and if everything passes, deploy the changes to users. For
traditional software, this works reliably; for ML systems, it's more
complex because you must also validate data quality, model performance,
and prediction distribution---not just code correctness. } automate
testing, and static analysis tools measure quality. This mature
infrastructure enables reliable software development following
well-defined engineering principles.

Machine learning systems depart from this paradigm. While traditional
systems execute explicit programming logic, ML systems derive their
behavior from data patterns discovered through training. This shift from
code to data as the primary behavior driver introduces complexities that
existing software engineering practices cannot address. These challenges
require specialized workflows that \textbf{?@sec-ai-workflow} addresses.

Figure~\ref{fig-ml_lifecycle_overview} illustrates how ML systems
operate in continuous cycles rather than traditional software's linear
progression from design through deployment.

\begin{figure}[htb]

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/733b6df390bc7dbf840e865a22ef6549242b2ca2.pdf}}

}

\caption{\label{fig-ml_lifecycle_overview}\textbf{ML System Lifecycle}:
Continuous iteration defines successful machine learning systems,
requiring feedback loops to refine models and address performance
degradation across data collection, model training, evaluation, and
deployment. This cyclical process contrasts with traditional software
development and emphasizes the importance of ongoing monitoring and
adaptation to maintain system reliability and accuracy in dynamic
environments.}

\end{figure}%

The data-dependent nature of ML systems creates dynamic lifecycles
requiring continuous monitoring and adaptation. Unlike source code that
changes only through developer modifications, data reflects real-world
dynamics. Distribution shifts can silently alter system behavior without
any code changes. Traditional tools designed for deterministic
code-based systems prove insufficient for managing such data-dependent
systems: version control excels at tracking discrete code changes but
struggles with large, evolving datasets; testing frameworks designed for
deterministic outputs require adaptation for probabilistic predictions.
These challenges require specialized practices:
\textbf{?@sec-data-engineering} addresses data versioning and quality
management, while \textbf{?@sec-ml-operations} covers monitoring
approaches that handle probabilistic behaviors rather than deterministic
outputs.

In production, lifecycle stages create either virtuous or vicious
cycles. Virtuous cycles emerge when high-quality data enables effective
learning, robust infrastructure supports efficient processing, and
well-engineered systems facilitate better data collection. Vicious
cycles occur when poor data quality undermines learning, inadequate
infrastructure hampers processing, and system limitations prevent data
collection improvements---with each problem compounding the others.

\subsection{The Deployment
Spectrum}\label{sec-introduction-deployment-spectrum-06a1}

Managing machine learning systems' complexity varies across different
deployment environments, each presenting unique constraints and
opportunities that shape lifecycle decisions.

At one end of the spectrum, cloud-based ML systems run in massive data
centers\sidenote{\textbf{Data Centers}: Massive facilities housing
thousands of servers, often consuming 100-300 megawatts of power,
equivalent to a small city. Google operates over 20 data centers
globally, each one costing \$1-2 billion to build. These facilities
maintain temperatures of exactly 80°F (27°C) with backup power systems
that can run for days, enabling the reliable operation of AI services
used by billions of people worldwide. }. These systems, including large
language models and recommendation engines, process petabytes of data
while serving millions of users simultaneously. They leverage virtually
unlimited computing resources but manage enormous operational complexity
and costs. The architectural approaches for building such large-scale
systems are covered in \textbf{?@sec-ml-systems} and
\textbf{?@sec-ai-acceleration}.

At the other end, TinyML systems run on
microcontrollers\sidenote{\textbf{Microcontrollers}: Tiny
computers-on-a-chip costing under \$1 each, with just kilobytes of
memory, about 1/millionth the memory of a smartphone. Popular chips like
the Arduino Uno have only 32KB of storage and 2KB of RAM, yet can run
simple AI models that classify sensor data, recognize voice commands, or
detect movement patterns while consuming less power than a digital
watch. } and embedded devices, performing ML tasks with severe memory,
computing power, and energy consumption constraints. Smart home devices
like Alexa or Google Assistant must recognize voice commands using less
power than LED bulbs, while sensors must detect anomalies on battery
power for months or years. The specialized techniques for deploying ML
on such constrained devices are explored in \textbf{?@sec-efficient-ai}
and \textbf{?@sec-model-optimizations}, while the unique challenges of
embedded ML systems are covered in \textbf{?@sec-ondevice-learning}.

Between these extremes lies a rich variety of ML systems adapted for
different contexts. Edge ML systems bring computation closer to data
sources, reducing latency\sidenote{\textbf{Latency}: The time delay
between when a request is made and when a response is received. In ML
systems, this is critical: autonomous vehicles need \textless10ms
latency for safety decisions, while voice assistants target
\textless100ms for natural conversation. For comparison, sending data to
a distant cloud server typically adds 50-100ms, which is why edge
computing became essential for real-time AI applications. } and
bandwidth requirements while managing local computing resources. Mobile
ML systems must balance sophisticated capabilities with severe
constraints: modern smartphones typically have 4-12GB RAM, ARM
processors operating at 1.5-3 GHz, and power budgets of 2-5 watts that
must be shared across all system functions. For example, running a
state-of-the-art image classification model on a smartphone might
consume 100-500mW and complete inference in 10-100ms, compared to cloud
servers that can use 200+ watts but deliver results in under 1ms.
Enterprise ML systems often operate within specific business
constraints, focusing on particular tasks while integrating with
existing infrastructure. Some organizations employ hybrid approaches,
distributing ML capabilities across multiple tiers to balance various
requirements.

\subsection{How Deployment Shapes the
Lifecycle}\label{sec-introduction-deployment-shapes-lifecycle-3531}

The deployment spectrum we've outlined represents more than just
different hardware configurations. Each deployment environment creates
an interplay of requirements, constraints, and trade-offs that impact
every stage of the ML lifecycle, from initial data collection through
continuous operation and evolution.

Performance requirements often drive initial architectural decisions.
Latency-sensitive applications, like autonomous vehicles or real-time
fraud detection, might require edge or embedded architectures despite
their resource constraints. Conversely, applications requiring massive
computational power for training, such as large language models,
naturally gravitate toward centralized cloud architectures. However, raw
performance is just one consideration in a complex decision space.

Resource management varies dramatically across architectures and
directly impacts lifecycle stages. Cloud systems must optimize for cost
efficiency at scale, balancing expensive GPU clusters, storage systems,
and network bandwidth. This affects training strategies (how often to
retrain models), data retention policies (what historical data to keep),
and serving architectures (how to distribute inference load). Edge
systems face fixed resource limits that constrain model complexity and
update frequency. Mobile and embedded systems operate under the
strictest constraints, where every byte of memory and milliwatt of power
matters, forcing aggressive model compression\sidenote{\textbf{Model
Compression}: Techniques for reducing a model's size and computational
requirements while preserving accuracy. Common approaches include
quantization (using 8-bit integers instead of 32-bit floats, reducing
model size by 4x), pruning (removing connections with minimal impact,
potentially achieving 90\% sparsity), and knowledge distillation
(training a small ``student'' model to mimic a large ``teacher'' model).
These techniques can shrink a 500MB model to 50MB while losing only
1-2\% accuracy, making deployment on smartphones and embedded devices
feasible. } and careful scheduling of training updates.

Operational complexity increases with system distribution, creating
cascading effects throughout the lifecycle. While centralized cloud
architectures benefit from mature deployment tools and managed services,
edge and hybrid systems must handle distributed system management
complexity. This manifests across all lifecycle stages: data collection
requires coordination across distributed sensors with varying
connectivity; version control must track models deployed across
thousands of edge devices; evaluation needs to account for varying
hardware capabilities; deployment must handle staged rollouts with
rollback capabilities; and monitoring must aggregate signals from
geographically distributed systems. The systematic approaches to
operational excellence, including incident response and debugging
methodologies for production ML systems, are thoroughly addressed in
\textbf{?@sec-ml-operations}.

Data considerations introduce competing pressures that reshape lifecycle
workflows. Privacy requirements or data sovereignty regulations might
push toward edge or embedded architectures where data stays local,
fundamentally changing data collection and training strategies---perhaps
requiring federated learning\sidenote{\textbf{Federated Learning}: A
training approach where the model learns from data distributed across
many devices without centralizing the data. For example, your
smartphone's keyboard learns your typing patterns locally and only
shares model updates (not your actual messages) with the cloud. This
technique, pioneered by Google in 2016, enables privacy-preserving ML by
keeping sensitive data on-device while still benefiting from collective
learning across millions of users. } approaches where models train on
distributed data without centralization. Yet the need for large-scale
training data might favor cloud approaches with centralized data
aggregation. The velocity and volume of data also influence
architectural choices: real-time sensor data might require edge
processing to manage bandwidth during collection, while batch analytics
might be better suited to cloud processing with periodic model updates.

Evolution and maintenance requirements must be considered from the
initial design. Cloud architectures offer flexibility for system
evolution with easy model updates and A/B testing\sidenote{\textbf{A/B
Testing}: A method of comparing two versions of a system by showing
version A to some users and version B to others, then measuring which
performs better. In ML systems, this might mean deploying a new model to
5\% of users while keeping 95\% on the old model, comparing metrics like
accuracy or user engagement before fully rolling out the new version.
This gradual rollout strategy helps catch problems before they affect
all users. }, but can incur significant ongoing costs. Edge and embedded
systems might be harder to update (requiring over-the-air
updates\sidenote{\textbf{Over-the-Air (OTA) Updates}: Wireless software
updates delivered remotely to devices, like how your smartphone installs
new apps without physical connection. For ML systems on embedded devices
or vehicles, OTA updates enable deploying improved models to thousands
or millions of devices without manual intervention. However, updating a
500MB neural network over cellular networks to a fleet of vehicles
requires careful bandwidth management and rollback capabilities if
updates fail. } with careful bandwidth management), but could offer
lower operational overhead. The continuous cycle of ML systems---collect
data, train models, evaluate performance, deploy updates, monitor
behavior---becomes particularly challenging in distributed
architectures, where updating models and maintaining system health
requires careful orchestration across multiple tiers.

These trade-offs are rarely simple binary choices. Modern ML systems
often adopt hybrid approaches, balancing these considerations based on
specific use cases and constraints. For instance, an autonomous vehicle
might perform real-time perception and control at the edge for latency
reasons, while uploading data to the cloud for model improvement and
downloading updated models periodically. A voice assistant might do
wake-word detection on-device to preserve privacy and reduce latency,
but send full speech to the cloud for complex natural language
processing.

The key insight is understanding how deployment decisions ripple through
the entire system lifecycle. A choice to deploy on embedded devices
doesn't just constrain model size, it affects data collection strategies
(what sensors are feasible), training approaches (whether to use
federated learning), evaluation metrics (accuracy vs.~latency
vs.~power), deployment mechanisms (over-the-air updates), and monitoring
capabilities (what telemetry can be collected). These interconnected
decisions demonstrate the AI Triangle framework in practice, where
constraints in one component create cascading effects throughout the
system.

With this understanding of how ML systems operate across their lifecycle
and deployment spectrum, we can now examine concrete examples that
illustrate these principles in action. The case studies that follow
demonstrate how different deployment choices create distinct engineering
challenges and solutions across the system lifecycle.

\phantomsection\label{quiz-question-sec-introduction-understanding-ml-system-lifecycle-deployment-0ab0}
\begin{fbx}{callout-quiz-question}{Self-Check: Question 1.7}{}
\phantomsection\label{quiz-question-sec-introduction-understanding-ml-system-lifecycle-deployment-0ab0}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  What is a key difference between the lifecycle of ML systems and
  traditional software systems?

  \begin{enumerate}
  \def\labelenumii{\alph{enumii})}
  \tightlist
  \item
    ML systems require continuous monitoring and adaptation.
  \item
    Traditional software systems rely on data for behavior.
  \item
    ML systems have a linear development process.
  \item
    Traditional software systems are probabilistic in nature.
  \end{enumerate}
\item
  How does the deployment environment influence the ML system lifecycle?
\item
  Order the following stages of the ML system lifecycle as depicted in
  the section: (1) Model Training, (2) Model Deployment, (3) Model
  Evaluation, (4) Data Collection, (5) Model Monitoring, (6) Data
  Preparation.
\end{enumerate}

\noindent\hspace*{1.25em}\hyperref[quiz-answer-sec-introduction-understanding-ml-system-lifecycle-deployment-0ab0]{\textbf{See Answer~$\rightarrow$}}

\end{fbx}

\section{Case Studies in Real-World ML
Systems}\label{sec-introduction-case-studies-realworld-ml-systems-a2ba}

Having established the AI Triangle framework, lifecycle stages, and
deployment spectrum, we can now examine these principles operating in
real-world systems. Rather than surveying multiple systems
superficially, we focus on one representative case study, autonomous
vehicles, that illustrates the spectrum of ML systems engineering
challenges across all three components, multiple lifecycle stages, and
complex deployment constraints.

\subsection{Case Study: Autonomous
Vehicles}\label{sec-introduction-case-study-autonomous-vehicles-6f86}

\href{https://waymo.com/}{Waymo}, a subsidiary of Alphabet Inc., stands
at the forefront of autonomous vehicle technology, representing one of
the most ambitious applications of machine learning systems to date.
Evolving from the Google Self-Driving Car Project initiated in 2009,
Waymo's approach to autonomous driving exemplifies how ML systems can
span the entire spectrum from embedded systems to cloud infrastructure.
This case study demonstrates the practical implementation of complex ML
systems in a safety-critical, real-world environment, integrating
real-time decision-making with long-term learning and adaptation.

\subsubsection{Data
Considerations}\label{sec-introduction-data-considerations-bdc4}

The data ecosystem underpinning Waymo's technology is vast and dynamic.
Each vehicle serves as a roving data center, its sensor suite, which
comprises LiDAR\sidenote{\textbf{LiDAR (Light Detection and Ranging)}: A
sensor that uses laser pulses to measure distances, creating detailed 3D
maps of surroundings by measuring how long light takes to bounce back
from objects. A spinning LiDAR sensor might emit millions of laser
pulses per second, detecting objects up to 200+ meters away with
centimeter-level precision. While highly accurate, LiDAR sensors can
cost \$75,000+ (though prices are dropping) and struggle in heavy rain
or fog where water droplets scatter the laser light. },
radar\sidenote{\textbf{Radar (Radio Detection and Ranging)}: A sensor
that uses radio waves to detect objects and measure their distance and
velocity. Unlike LiDAR, radar works well in rain, fog, and darkness,
making it essential for all-weather autonomous driving. Automotive radar
operates at 77 GHz frequency, detecting vehicles up to 250 meters away
and measuring their speed with high accuracy---critical for safely
navigating highways. Modern vehicles use multiple radar units costing
\$150-300 each. }, and high-resolution cameras, generating approximately
one terabyte of data per hour of driving. This real-world data is
complemented by an even more extensive simulated dataset, with Waymo's
vehicles having traversed over 20 billion miles in simulation and more
than 20 million miles on public roads. The challenge lies not just in
the volume of data, but in its heterogeneity and the need for real-time
processing. Waymo must handle both structured (e.g., GPS coordinates)
and unstructured data (e.g., camera images) simultaneously. The data
pipeline spans from edge processing on the vehicle itself to massive
cloud-based storage and processing systems. Sophisticated data cleaning
and validation processes are necessary, given the safety-critical nature
of the application. The representation of the vehicle's environment in a
form amenable to machine learning presents significant challenges,
requiring complex preprocessing to convert raw sensor data into
meaningful features that capture the dynamics of traffic scenarios.

\subsubsection{Algorithmic
Considerations}\label{sec-introduction-algorithmic-considerations-b99f}

Waymo's ML stack represents a sophisticated ensemble of algorithms
tailored to the multifaceted challenge of autonomous driving. The
perception system employs specialized neural networks to process visual
data for object detection and tracking. Prediction models, needed for
anticipating the behavior of other road users, use neural networks that
can understand patterns over time\sidenote{\textbf{Sequential Neural
Networks}: Neural network architectures designed to process data that
occurs in sequences over time, such as predicting where a pedestrian
will move next based on their previous movements. These networks
maintain a form of ``memory'' of previous inputs to inform current
decisions. } in road user behavior. Building such complex multi-model
systems requires the architectural patterns from
\textbf{?@sec-dnn-architectures} and the framework infrastructure
covered in \textbf{?@sec-ai-frameworks}. Waymo has developed custom ML
models like VectorNet for predicting vehicle trajectories. The planning
and decision-making systems may incorporate learning-from-experience
techniques to handle complex traffic scenarios.

\subsubsection{Infrastructure
Considerations}\label{sec-introduction-infrastructure-considerations-248a}

The computing infrastructure supporting Waymo's autonomous vehicles
epitomizes the challenges of deploying ML systems across the full
spectrum from edge to cloud. Each vehicle is equipped with a
custom-designed compute platform capable of processing sensor data and
making decisions in real-time, often leveraging specialized hardware
like GPUs or tensor processing units (TPUs)\sidenote{\textbf{Tensor
Processing Unit (TPU)}: Google's custom AI accelerator chip designed
specifically for neural network operations, named after ``tensors''
(multi-dimensional arrays used in deep learning). First revealed in
2016, TPUs can perform matrix multiplications up to 15-30x faster than
contemporary GPUs for AI workloads while using less power. A single TPU
v4 pod can provide 1.1 exaflops of computing power---roughly equivalent
to 10,000 high-end GPUs---enabling training of massive language models
in days rather than months. }. This edge computing is complemented by
extensive use of cloud infrastructure, leveraging the power of Google's
data centers for training models, running large-scale simulations, and
performing fleet-wide learning. Such systems demand specialized hardware
architectures (\textbf{?@sec-ai-acceleration}) and edge-cloud
coordination strategies (\textbf{?@sec-ml-systems}) to handle real-time
processing at scale. The connectivity between these tiers is critical,
with vehicles requiring reliable, high-bandwidth communication for
real-time updates and data uploading. Waymo's infrastructure must be
designed for robustness and fault tolerance, ensuring safe operation
even in the face of hardware failures or network disruptions. The scale
of Waymo's operation presents significant challenges in data management,
model deployment, and system monitoring across a geographically
distributed fleet of vehicles.

\subsubsection{Future
Implications}\label{sec-introduction-future-implications-c2f2}

Waymo's impact extends beyond technological advancement, potentially
revolutionizing transportation, urban planning, and numerous aspects of
daily life. The launch of Waymo One, a commercial ride-hailing service
using autonomous vehicles in Phoenix, Arizona, represents a significant
milestone in the practical deployment of AI systems in safety-critical
applications. Waymo's progress has broader implications for the
development of robust, real-world AI systems, driving innovations in
sensor technology, edge computing, and AI safety that have applications
far beyond the automotive industry. However, it also raises important
questions about liability, ethics, and the interaction between AI
systems and human society. As Waymo continues to expand its operations
and explore applications in trucking and last-mile delivery, it serves
as an important test bed for advanced ML systems, driving progress in
areas such as continual learning, robust perception, and human-AI
interaction. The Waymo case study underscores both the tremendous
potential of ML systems to transform industries and the complex
challenges involved in deploying AI in the real world.

\subsection{Contrasting Deployment
Scenarios}\label{sec-introduction-contrasting-deployment-scenarios-653a}

While Waymo illustrates the full complexity of hybrid edge-cloud ML
systems, other deployment scenarios present different constraint
profiles.
\href{https://www.microsoft.com/en-us/research/project/farmbeats-iot-agriculture/}{FarmBeats},
a Microsoft Research project for agricultural IoT, operates at the
opposite end of the spectrum---severely resource-constrained edge
deployments in remote locations with limited connectivity. FarmBeats
demonstrates how ML systems engineering adapts to constraints: simpler
models that can run on low-power microcontrollers, innovative
connectivity solutions using TV white spaces, and local processing that
minimizes data transmission. The challenges include maintaining sensor
reliability in harsh conditions, validating data quality with limited
human oversight, and updating models on devices that may be offline for
extended periods.

Conversely,
\href{https://deepmind.google/technologies/alphafold/}{AlphaFold}
(\citeproc{ref-jumper2021highly}{Jumper et al. 2021}) represents purely
cloud-based scientific ML where computational resources are essentially
unlimited but accuracy is paramount. AlphaFold's protein structure
prediction required training on 128 TPUv3 cores for weeks, processing
hundreds of millions of protein sequences from multiple databases. The
systems challenges differ markedly from Waymo or FarmBeats: managing
massive training datasets (the Protein Data Bank contains over 180,000
structures), coordinating distributed training across specialized
hardware, and validating predictions against experimental ground truth.
Unlike Waymo's latency constraints or FarmBeats' power constraints,
AlphaFold prioritizes computational throughput to explore vast search
spaces---training costs exceeded \$100,000 but enabled scientific
breakthroughs.

These three systems---Waymo (hybrid, latency-critical), FarmBeats (edge,
resource-constrained), and AlphaFold (cloud,
compute-intensive)---illustrate how deployment environment shapes every
engineering decision. The fundamental three-component framework applies
to all, but the specific constraints and optimization priorities differ
dramatically. Understanding this deployment diversity is essential for
ML systems engineers, as the same algorithmic insight may require
entirely different system implementations depending on operational
context.

With concrete examples established, we can now examine the challenges
that emerge across different deployment scenarios and lifecycle stages.

\medskip

\phantomsection\label{quiz-question-sec-introduction-case-studies-realworld-ml-systems-a2ba}
\begin{fbx}{callout-quiz-question}{Self-Check: Question 1.8}{}
\phantomsection\label{quiz-question-sec-introduction-case-studies-realworld-ml-systems-a2ba}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Which of the following best describes the primary challenge Waymo
  faces with its data pipeline?

  \begin{enumerate}
  \def\labelenumii{\alph{enumii})}
  \tightlist
  \item
    Limited data storage capacity
  \item
    Heterogeneity and real-time processing of data
  \item
    High cost of data transmission
  \item
    Insufficient data collection from sensors
  \end{enumerate}
\item
  Explain how Waymo's use of both edge and cloud computing supports its
  autonomous vehicle operations.
\item
  Waymo's perception system employs specialized \_\_\_\_ to process
  visual data for object detection and tracking.
\item
  True or False: Waymo's infrastructure is designed to prioritize
  computational throughput over latency.
\item
  In a production system like Waymo, what trade-offs might engineers
  consider between sensor accuracy and cost?
\end{enumerate}

\noindent\hspace*{1.25em}\hyperref[quiz-answer-sec-introduction-case-studies-realworld-ml-systems-a2ba]{\textbf{See Answer~$\rightarrow$}}

\end{fbx}

\section{Core Engineering Challenges in ML
Systems}\label{sec-introduction-core-engineering-challenges-ml-systems-6482}

The Waymo case study and comparative deployment scenarios reveal how the
AI Triangle framework creates interdependent challenges across data,
algorithms, and infrastructure. We've already established how ML systems
differ from traditional software in their failure patterns and
performance degradation. Now we can examine the specific challenge
categories that emerge from this difference.

\subsection{Data
Challenges}\label{sec-introduction-data-challenges-2b0d}

The foundation of any ML system is its data, and managing this data
introduces several core challenges that can silently degrade system
performance. Data quality emerges as the primary concern: real-world
data is often messy, incomplete, and inconsistent. Waymo's sensor suite
must contend with environmental interference (rain obscuring cameras,
LiDAR reflections from wet surfaces), sensor degradation over time, and
data synchronization across multiple sensors capturing information at
different rates. Unlike traditional software where input validation can
catch malformed data, ML systems must handle ambiguity and uncertainty
inherent in real-world observations.

Scale represents another critical dimension. Waymo generates
approximately one terabyte per vehicle per hour---managing this data
volume requires sophisticated infrastructure for collection, storage,
processing, and efficient access during training. The challenge isn't
just storing petabytes of data, but maintaining data quality metadata,
version control for datasets, and efficient retrieval for model
training. As systems scale to thousands of vehicles across multiple
cities, these data management challenges compound exponentially.

Perhaps most serious is data drift\sidenote{\textbf{Data Drift}: The
gradual change in the statistical properties of input data over time,
which can degrade model performance if not properly monitored and
addressed through retraining or model updates. }, the gradual change in
data patterns over time that silently degrades model performance.
Waymo's models encounter new traffic patterns, road configurations,
weather conditions, and driving behaviors that weren't present in
training data. A model trained primarily on Phoenix driving might
perform poorly when deployed in New York due to distribution shift:
denser traffic, more aggressive drivers, different road layouts. Unlike
traditional software where specifications remain constant, ML systems
must adapt as the world they model evolves.

This adaptation requirement introduces an important constraint that is
often overlooked. While ML systems can generalize to unseen situations
through learned statistical patterns, once trained, the model's learned
behavior becomes fixed. The model cannot modify its understanding during
deployment; it can only apply the patterns it learned during training.
When distribution shift occurs, the model follows these outdated learned
patterns just as deterministic code follows outdated rules. If
construction zones triple in frequency, or new vehicle types appear
regularly, the model's fixed responses may prove no more appropriate
than hardcoded logic written for a different operational context. The
advantage of ML emerges not from runtime adaptation but from the
capacity to retrain with new data, a process requiring deliberate
engineering intervention.

Distribution shift manifests through multiple pathways. Seasonal
variations affect sensor performance through changing sun angles and
precipitation patterns. Infrastructure modifications alter road layouts.
Urban growth evolves traffic patterns. Each shift can degrade specific
model components: pedestrian detection accuracy may decline in winter
conditions, while lane following confidence may decrease on newly
repaved roads. Detecting these shifts requires continuous monitoring of
input distributions and model performance across operational contexts.

The systematic approaches to managing these data challenges (quality
assurance, versioning, drift detection, and remediation strategies) are
covered in \textbf{?@sec-data-engineering}. The key insight is that data
challenges in ML systems are continuous and dynamic, requiring ongoing
engineering attention rather than one-time solutions.

\subsection{Model
Challenges}\label{sec-introduction-model-challenges-eef4}

Creating and maintaining the ML models themselves presents another set
of challenges. Modern ML models, particularly in deep learning, can be
complex. Consider a language model like GPT-3, which has hundreds of
billions of parameters that need to be optimized through training
processes\sidenote{\textbf{Backpropagation}: The primary algorithm used
to train neural networks, which calculates how each parameter in the
network should be adjusted to minimize prediction errors by propagating
error gradients backward through the network layers. }. This complexity
creates practical challenges: these models require enormous computing
power to train and run, making it difficult to deploy them in situations
with limited resources, like on mobile phones or IoT devices.

Training these models effectively is itself a significant challenge.
Unlike traditional programming where we write explicit instructions, ML
models learn from examples. This learning process involves many
architectural and hyperparameter choices: How should we structure the
model? How long should we train it? How can we tell if it's learning the
right patterns rather than memorizing training data? Making these
decisions often requires both technical expertise and considerable trial
and error.

Modern practice increasingly relies on transfer learning---reusing
models developed for one task as starting points for related tasks.
Rather than training a new image recognition model from scratch,
practitioners might start with a model pre-trained on millions of images
and adapt it to their specific domain (say, medical imaging or
agricultural monitoring). This approach dramatically reduces both the
training data and computation required, but introduces new challenges
around ensuring the pre-trained model's biases don't transfer to the new
application. These training challenges---transfer learning, distributed
training, and bias mitigation---require systematic approaches that
\textbf{?@sec-ai-training} explores, building on the framework
infrastructure from \textbf{?@sec-ai-frameworks}.

A particularly important challenge is ensuring that models work well in
real-world conditions beyond their training data. This generalization
gap, the difference between training performance and real-world
performance, represents a central challenge in machine learning. A model
might achieve 99\% accuracy on its training data but only 75\% accuracy
in production due to subtle distribution differences. For important
applications like autonomous vehicles or medical diagnosis systems,
understanding and minimizing this gap becomes necessary for safe
deployment.

\subsection{System
Challenges}\label{sec-introduction-system-challenges-0dc0}

Getting ML systems to work reliably in the real world introduces its own
set of challenges. Unlike traditional software that follows fixed rules,
ML systems need to handle uncertainty and variability in their inputs
and outputs. They also typically need both training systems (for
learning from data) and serving systems (for making predictions), each
with different requirements and constraints.

Consider a company building a speech recognition system. They need
infrastructure to collect and store audio data, systems to train models
on this data, and then separate systems to actually process users'
speech in real-time. Each part of this pipeline needs to work reliably
and efficiently, and all the parts need to work together seamlessly. The
engineering principles for building such robust data pipelines are
covered in \textbf{?@sec-data-engineering}, while the operational
practices for maintaining these systems in production are explored in
\textbf{?@sec-ml-operations}.

These systems also need constant monitoring and updating. How do we know
if the system is working correctly? How do we update models without
interrupting service? How do we handle errors or unexpected inputs?
These operational challenges become particularly complex when ML systems
are serving millions of users.

\subsection{Ethical
Considerations}\label{sec-introduction-ethical-considerations-d6a5}

As ML systems become more prevalent in our daily lives, their broader
impacts on society become increasingly important to consider. One major
concern is fairness, as ML systems can sometimes learn to make decisions
that discriminate against certain groups of people. This often happens
unintentionally, as the systems pick up biases present in their training
data. For example, a job application screening system might
inadvertently learn to favor certain demographics if those groups were
historically more likely to be hired. Detecting and mitigating such
biases requires careful auditing of both training data and model
behavior across different demographic groups.

Another important consideration is transparency and interpretability.
Many modern ML models, particularly deep learning models with millions
or billions of parameters, function as black boxes---systems where we
can observe inputs and outputs but struggle to understand the internal
reasoning. Like a radio that receives signals and produces sound without
most users understanding the electronics inside, these models make
predictions through complex mathematical transformations that resist
human interpretation. A deep neural network might correctly diagnose a
medical condition from an X-ray, but explaining why it reached that
diagnosis---which visual features it considered most important---remains
challenging. This opacity becomes particularly problematic when ML
systems make consequential decisions affecting people's lives in domains
like healthcare, criminal justice, or financial services, where
stakeholders reasonably expect explanations for decisions that impact
them.

Privacy is also a major concern. ML systems often need large amounts of
data to work effectively, but this data might contain sensitive personal
information. How do we balance the need for data with the need to
protect individual privacy? How do we ensure that models don't
inadvertently memorize and reveal private information through inference
attacks\sidenote{\textbf{Inference Attack}: A technique where an
adversary attempts to extract sensitive information about the training
data by making careful queries to a trained model, exploiting patterns
the model may have inadvertently memorized during training. }? These
challenges aren't merely technical problems to be solved, but ongoing
considerations that shape how we approach ML system design and
deployment. These concerns require integrated approaches:
\textbf{?@sec-responsible-ai} addresses fairness and bias detection,
\textbf{?@sec-security-privacy} covers privacy-preserving techniques and
inference attack mitigation, while \textbf{?@sec-robust-ai} ensures
system resilience under adversarial conditions.

\subsection{Understanding Challenge
Interconnections}\label{sec-introduction-understanding-challenge-interconnections-3d30}

As the Waymo case study illustrates, challenges cascade and compound
across the AI Triangle. Data quality issues (sensor noise, distribution
shift) degrade model performance. Model complexity constraints (latency
budgets, power limits) force architectural compromises that may affect
fairness (simpler models might show more bias). System-level failures
(over-the-air update problems) can prevent deployment of improved models
that address ethical concerns.

This interdependency explains why ML systems engineering requires
holistic thinking that considers the AI Triangle components together
rather than optimizing them independently. A decision to use a larger
model for better accuracy creates ripple effects: more training data
required, longer training times, higher serving costs, increased
latency, and potentially more pronounced biases if the training data
isn't carefully curated. Successfully navigating these trade-offs
requires understanding how choices in one dimension affect others.

The challenge landscape also explains why many research models fail to
reach production. Academic ML often focuses on maximizing accuracy on
benchmark datasets, potentially ignoring practical constraints like
inference latency, training costs, data privacy, or operational
monitoring. Production ML systems must balance accuracy against
deployment feasibility, operational costs, ethical considerations, and
long-term maintainability. This gap between research priorities and
production realities motivates this book's emphasis on systems
engineering rather than pure algorithmic innovation.

These interconnected challenges, spanning data quality and model
complexity to infrastructure scalability and ethical considerations,
distinguish ML systems from traditional software engineering. The
transition from algorithmic innovation to systems integration
challenges, combined with the unique operational characteristics we've
examined, establishes the need for a distinct engineering discipline. We
call this emerging field AI Engineering.

\phantomsection\label{quiz-question-sec-introduction-core-engineering-challenges-ml-systems-6482}
\begin{fbx}{callout-quiz-question}{Self-Check: Question 1.9}{}
\phantomsection\label{quiz-question-sec-introduction-core-engineering-challenges-ml-systems-6482}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Which of the following is a primary data-related challenge in ML
  systems like Waymo's?

  \begin{enumerate}
  \def\labelenumii{\alph{enumii})}
  \tightlist
  \item
    Algorithmic efficiency
  \item
    Data version control
  \item
    User interface design
  \item
    Network latency
  \end{enumerate}
\item
  Explain how data drift can affect the performance of an ML system like
  Waymo's autonomous vehicles.
\item
  In the context of ML systems, what is a significant challenge when
  scaling data management?

  \begin{enumerate}
  \def\labelenumii{\alph{enumii})}
  \tightlist
  \item
    Reducing algorithm complexity
  \item
    Improving user interface aesthetics
  \item
    Ensuring data quality and efficient retrieval
  \item
    Minimizing hardware costs
  \end{enumerate}
\item
  Discuss the interdependencies between data quality and model
  performance in ML systems. How do these interdependencies affect
  system design?
\end{enumerate}

\noindent\hspace*{1.25em}\hyperref[quiz-answer-sec-introduction-core-engineering-challenges-ml-systems-6482]{\textbf{See Answer~$\rightarrow$}}

\end{fbx}

\section{Defining AI
Engineering}\label{sec-introduction-defining-ai-engineering-b812}

Having explored the historical evolution, lifecycle characteristics,
practical applications, and core challenges of machine learning systems,
we can now formally establish the discipline that addresses these
systems-level concerns.

\phantomsection\label{callout-definitionux2a-1.7}
\begin{fbx}{callout-definition}{Definition: }{AI Engineering}
\phantomsection\label{callout-definition*-1.7}
\textbf{\emph{AI Engineering}} is the engineering discipline focused on
the \emph{systems-level integration} of machine learning
\emph{algorithms}, \emph{data}, and \emph{computational infrastructure}
to build and operate production systems that are \emph{reliable},
\emph{efficient}, and \emph{scalable}.

\end{fbx}

As we've traced through AI's history, a fundamental transformation has
occurred. While AI once encompassed symbolic reasoning, expert systems,
and rule-based approaches, learning-based methods now dominate the
field. When organizations build AI today, they build machine learning
systems. Netflix's recommendation engine processes billions of viewing
events to train models serving millions of subscribers. Waymo's
autonomous vehicles run dozens of neural networks processing sensor data
in real time. Training GPT-4 required coordinating thousands of GPUs
across data centers, consuming megawatts of power. Modern AI is
overwhelmingly machine learning: systems whose capabilities emerge from
learning patterns in data.

This convergence makes ``AI Engineering'' the natural name for the
discipline, even though this text focuses specifically on machine
learning systems as its subject matter. The term reflects how AI is
actually built and deployed in practice today.

AI Engineering encompasses the complete lifecycle of building production
intelligent systems. A breakthrough algorithm requires efficient data
collection and processing, distributed computation across hundreds or
thousands of machines, reliable service to users with strict latency
requirements, and continuous monitoring and updating based on real-world
performance. The discipline addresses fundamental challenges at every
level: designing efficient algorithms for specialized hardware,
optimizing data pipelines that process petabytes daily, implementing
distributed training across thousands of GPUs, deploying models that
serve millions of concurrent users, and maintaining systems whose
behavior evolves as data distributions shift. Energy efficiency is not
an afterthought but a first-class constraint alongside accuracy and
latency. The physics of memory bandwidth limitations, the breakdown of
Dennard scaling, and the energy costs of data movement shape every
architectural decision from chip design to data center deployment.

This emergence of AI Engineering as a distinct discipline mirrors how
Computer Engineering emerged in the late 1960s and early
1970s.\sidenote{The first accredited computer engineering degree program
in the United States was established at Case Western Reserve University
in 1971, marking the formalization of Computer Engineering as a distinct
academic discipline. } As computing systems grew more complex, neither
Electrical Engineering nor Computer Science alone could address the
integrated challenges of building reliable computers. Computer
Engineering emerged as a complete discipline bridging both fields.
Today, AI Engineering faces similar challenges at the intersection of
algorithms, infrastructure, and operational practices. While Computer
Science advances machine learning algorithms and Electrical Engineering
develops specialized AI hardware, neither discipline fully encompasses
the systems-level integration, deployment strategies, and operational
practices required to build production AI systems at scale.

With AI Engineering now formally defined as the discipline, the
remainder of this text discusses the practice of building and operating
machine learning systems. We use ``ML systems engineering'' throughout
to describe this practice---the work of designing, deploying, and
maintaining the machine learning systems that constitute modern AI.
These terms refer to the same discipline: AI Engineering is what we call
it, ML systems engineering is what we do.

Having established AI Engineering as a discipline, we can now organize
its practice into a coherent framework that addresses the challenges
we've identified systematically.

\phantomsection\label{quiz-question-sec-introduction-defining-ai-engineering-b812}
\begin{fbx}{callout-quiz-question}{Self-Check: Question 1.10}{}
\phantomsection\label{quiz-question-sec-introduction-defining-ai-engineering-b812}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  What is the primary focus of AI Engineering as a discipline?

  \begin{enumerate}
  \def\labelenumii{\alph{enumii})}
  \tightlist
  \item
    Developing new AI algorithms
  \item
    Improving symbolic reasoning techniques
  \item
    Building reliable, efficient, and scalable ML systems
  \item
    Enhancing user interfaces for AI applications
  \end{enumerate}
\item
  Explain how the historical shift from symbolic systems to
  learning-based approaches has influenced the emergence of AI
  Engineering.
\item
  Which of the following best describes the role of AI Engineering in
  modern AI systems?

  \begin{enumerate}
  \def\labelenumii{\alph{enumii})}
  \tightlist
  \item
    Focusing solely on algorithmic development
  \item
    Developing symbolic AI techniques
  \item
    Designing user-friendly AI interfaces
  \item
    Integrating and optimizing systems for real-world deployment
  \end{enumerate}
\item
  In a production system, how might AI Engineering address the challenge
  of energy efficiency while maintaining performance?
\end{enumerate}

\noindent\hspace*{1.25em}\hyperref[quiz-answer-sec-introduction-defining-ai-engineering-b812]{\textbf{See Answer~$\rightarrow$}}

\end{fbx}

\section{Organizing ML Systems Engineering: The Five-Pillar
Framework}\label{sec-introduction-organizing-ml-systems-engineering-fivepillar-framework-524d}

The challenges we've explored, from silent performance degradation and
data drift to model complexity and ethical concerns, reveal why ML
systems engineering has emerged as a distinct discipline. The unique
failure patterns we discussed earlier exemplify the need for specialized
approaches: traditional software engineering practices cannot address
systems that degrade quietly rather than failing obviously. These
challenges cannot be addressed through algorithmic innovation alone;
they require systematic engineering practices that span the entire
system lifecycle from initial data collection through continuous
operation and evolution.

This book organizes ML systems engineering around five interconnected
disciplines that directly address the challenge categories we've
identified. These pillars, illustrated in Figure~\ref{fig-pillars},
represent the core engineering capabilities required to bridge the gap
between research prototypes and production systems capable of operating
reliably at scale.

\begin{figure}

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{contents/core/introduction/images/png/book_pillars.png}}

}

\caption{\label{fig-pillars}\textbf{ML System Lifecycle}: Machine
learning systems engineering encompasses five interconnected disciplines
that address the real-world challenges of building, deploying, and
maintaining AI systems at scale. Each pillar represents critical
engineering capabilities needed to bridge the gap between research
prototypes and production systems.}

\end{figure}%

\subsection{The Five Engineering
Disciplines}\label{sec-introduction-five-engineering-disciplines-6eee}

The five-pillar framework shown in Figure~\ref{fig-pillars} emerged
directly from the systems challenges that distinguish ML from
traditional software. Each pillar addresses specific challenge
categories while recognizing their interdependencies:

\textbf{Data Engineering} (\textbf{?@sec-data-engineering}) addresses
the data-related challenges we identified: quality assurance, scale
management, drift detection, and distribution shift. This pillar
encompasses building robust data pipelines that ensure quality, handle
massive scale, maintain privacy, and provide the infrastructure upon
which all ML systems depend. For systems like Waymo, this means managing
terabytes of sensor data per vehicle, validating data quality in
real-time, detecting distribution shifts across different cities and
weather conditions, and maintaining data lineage for debugging and
compliance. The techniques covered include data versioning, quality
monitoring, drift detection algorithms, and privacy-preserving data
processing.

\textbf{Training Systems} (\textbf{?@sec-ai-training}) tackles the
model-related challenges around complexity and scale. This pillar covers
developing training systems that can manage large datasets and complex
models while optimizing computational resource utilization across
distributed environments. Modern foundation models require coordinating
thousands of GPUs, implementing parallelization strategies, managing
training failures and restarts, and balancing training costs against
model quality. The chapter explores distributed training architectures,
optimization algorithms, hyperparameter tuning at scale, and the
frameworks that make large-scale training practical.

\textbf{Deployment Infrastructure} (\textbf{?@sec-ml-operations},
\textbf{?@sec-ondevice-learning}) addresses system-related challenges
around the training-serving divide and operational complexity. This
pillar encompasses building reliable deployment infrastructure that can
serve models at scale, handle failures gracefully, and adapt to evolving
requirements in production environments. Deployment spans the full
spectrum from cloud services handling millions of requests per second to
edge devices operating under severe latency and power constraints. The
techniques include model serving architectures, edge deployment
optimization, A/B testing frameworks, and staged rollout strategies that
minimize risk while enabling rapid iteration.

\textbf{Operations and Monitoring} (\textbf{?@sec-ml-operations},
\textbf{?@sec-benchmarking-ai}) directly addresses the silent
performance degradation patterns we identified as distinctive to ML
systems. This pillar covers creating monitoring and maintenance systems
that ensure continued performance, enable early issue detection, and
support safe system updates in production. Unlike traditional software
monitoring focused on infrastructure metrics, ML operations requires the
four-dimensional monitoring we discussed: infrastructure health, model
performance, data quality, and business impact. The chapter explores
metrics design, alerting strategies, incident response procedures,
debugging techniques for production ML systems, and continuous
evaluation approaches that catch degradation before it impacts users.

\textbf{Ethics and Governance} (\textbf{?@sec-responsible-ai},
\textbf{?@sec-security-privacy}, \textbf{?@sec-sustainable-ai})
addresses the ethical and societal challenges around fairness,
transparency, privacy, and safety. This pillar implements responsible AI
practices throughout the system lifecycle rather than treating ethics as
an afterthought. For safety-critical systems like autonomous vehicles,
this includes formal verification methods, scenario-based testing, bias
detection and mitigation, privacy-preserving learning techniques, and
explainability approaches that support debugging and certification. The
chapters cover both technical methods (differential privacy, fairness
metrics, interpretability techniques) and organizational practices
(ethics review boards, incident response protocols, stakeholder
engagement).

\subsection{Connecting Components, Lifecycle, and
Disciplines}\label{sec-introduction-connecting-components-lifecycle-disciplines-388b}

The five pillars emerge naturally from the AI Triangle framework and
lifecycle stages we established earlier. Each AI Triangle component maps
to specific pillars: Data Engineering handles the data component's full
lifecycle; Training Systems and Deployment Infrastructure address how
algorithms interact with infrastructure during different lifecycle
phases; Operations bridges all components by monitoring their
interactions; Ethics \& Governance cuts across all components, ensuring
responsible practices throughout.

The challenge categories we identified find their solutions within
specific pillars: Data challenges → Data Engineering. Model challenges →
Training Systems. System challenges → Deployment Infrastructure and
Operations. Ethical challenges → Ethics \& Governance. As we established
with the AI Triangle framework, these pillars must coordinate rather
than operate in isolation.

This structure reflects how AI evolved from algorithm-centric research
to systems-centric engineering, shifting focus from ``can we make this
algorithm work?'' to ``can we build systems that reliably deploy,
operate, and maintain these algorithms at scale?'' The five pillars
represent the engineering capabilities required to answer ``yes.''

\subsection{Future Directions in ML Systems
Engineering}\label{sec-introduction-future-directions-ml-systems-engineering-db3b}

While these five pillars provide a stable framework for ML systems
engineering, the field continues evolving. Understanding current trends
helps anticipate how the core challenges and trade-offs will manifest in
future systems.

Application-level innovation increasingly features agentic systems that
move beyond reactive prediction to autonomous action. Systems that can
plan, reason, and execute complex tasks introduce new requirements for
decision-making frameworks and safety constraints. These advances don't
eliminate the five pillars but increase their importance: autonomous
systems that can take consequential actions require even more rigorous
data quality, more reliable deployment infrastructure, more
comprehensive monitoring, and stronger ethical safeguards.

System architecture evolution addresses sustainability and efficiency
concerns that have become critical as models scale. Innovation in model
compression, efficient training techniques, and specialized hardware
stems from both environmental and economic pressures. Future
architectures must balance the pursuit of more powerful models against
growing resource constraints. These efficiency innovations primarily
impact Training Systems and Deployment Infrastructure pillars,
introducing new techniques like quantization, pruning, and neural
architecture search that optimize for multiple objectives
simultaneously.

Infrastructure advances continue reshaping deployment possibilities.
Specialized AI accelerators are emerging across the spectrum from
powerful data center chips to efficient edge processors. This
heterogeneous computing landscape enables dynamic model distribution
across tiers based on capabilities and conditions, blurring traditional
boundaries between cloud, edge, and embedded systems. These
infrastructure innovations affect how all five pillars operate---new
hardware enables new algorithms, which require new training approaches,
which demand new monitoring strategies.

Democratization of AI technology is making ML systems more accessible to
developers and organizations of all sizes. Cloud providers offer
pre-trained models and automated ML platforms that reduce the expertise
barrier for deploying AI solutions. This accessibility trend doesn't
diminish the importance of systems engineering---if anything, it
increases demand for robust, reliable systems that can operate without
constant expert oversight. The five pillars become even more critical as
ML systems proliferate into domains beyond traditional tech companies.

These trends share a common theme: they create ML systems that are more
capable and widespread, but also more complex to engineer reliably. The
five-pillar framework provides the foundation for navigating this
landscape, though specific techniques within each pillar will continue
advancing.

\subsection{The Nature of Systems
Knowledge}\label{sec-introduction-nature-systems-knowledge-1c67}

Machine learning systems engineering differs epistemologically from
purely theoretical computer science disciplines. While fields like
algorithms, complexity theory, or formal verification build knowledge
through mathematical proofs and rigorous derivations, ML systems
engineering is a practice, a craft learned through building, deploying,
and maintaining systems at scale. This distinction becomes apparent in
topics like MLOps, where you'll encounter fewer theorems and more
battle-tested patterns that have emerged from production experience. The
knowledge here isn't about proving optimal solutions exist but about
recognizing which approaches work reliably under real-world constraints.

This practical orientation reflects ML systems engineering's nature as a
systems discipline. Like other engineering fields---civil, electrical,
mechanical---the core challenge lies in managing complexity and
trade-offs rather than deriving closed-form solutions. You'll learn to
reason about latency versus accuracy trade-offs, to recognize when data
quality issues will undermine even sophisticated models, to anticipate
how infrastructure choices propagate through entire system
architectures. This systems thinking develops through experience with
concrete scenarios, debugging production failures, and understanding why
certain design patterns persist across different applications.

The implication for learning is significant: mastery comes through
building intuition about patterns, understanding trade-off spaces, and
recognizing how different system components interact. When you read
about monitoring strategies or deployment architectures, the goal isn't
memorizing specific configurations but developing judgment about which
approaches suit which contexts. This book provides the frameworks,
principles, and representative examples, but expertise ultimately
develops through applying these concepts to real problems, making
mistakes, and building the pattern recognition that distinguishes
experienced systems engineers from those who only understand individual
components.

\subsection{How to Use This
Textbook}\label{sec-introduction-use-textbook-ed24}

For readers approaching this material, the chapters build systematically
on these foundational concepts:

\textbf{Foundation chapters} (\textbf{?@sec-ml-systems},
\textbf{?@sec-dl-primer}, \textbf{?@sec-dnn-architectures}) explore the
algorithmic and architectural fundamentals, providing the technical
background for understanding system-level decisions. These chapters
answer ``what are we building?'' before addressing ``how do we build it
reliably?''

\textbf{Pillar chapters} follow the five-discipline organization, with
each pillar containing multiple chapters that progress from fundamentals
to advanced topics. Readers can follow linearly through all chapters or
focus on specific pillars relevant to their work, though understanding
the interdependencies we've discussed helps appreciate how decisions in
one pillar affect others.

\textbf{Specialized topics} (\textbf{?@sec-ai-good},
\textbf{?@sec-sustainable-ai}, \textbf{?@sec-agi-systems}) examine how
ML systems engineering applies to specific domains and emerging
challenges, demonstrating the framework's flexibility across diverse
applications.

The cross-reference system throughout the book helps navigate
connections---when one chapter discusses a concept covered in detail
elsewhere, references guide you to that material. This interconnected
structure reflects the AI Triangle framework's reality: ML systems
engineering requires understanding how data, algorithms, and
infrastructure interact rather than studying them in isolation.

For more detailed information about the book's learning outcomes, target
audience, prerequisites, and how to maximize your experience with this
resource, please refer to the
\href{../../frontmatter/about/about.qmd}{About the Book} section, which
also provides details about our learning community and additional
resources.

This introduction has established the conceptual foundation for
everything that follows. We began by understanding the relationship
between artificial intelligence as vision and machine learning as
methodology. We defined machine learning systems as the artifacts we
build: integrated computing systems comprising data, algorithms, and
infrastructure. Through the Bitter Lesson and AI's historical evolution,
we discovered why systems engineering has become fundamental to AI
progress and how learning-based approaches came to dominate the field.
This context enabled us to formally define AI Engineering as a distinct
discipline, following the pattern of Computer Engineering's emergence,
establishing it as the field dedicated to building reliable, efficient,
and scalable machine learning systems across all computational
platforms.

The journey ahead explores each pillar of AI Engineering systematically,
providing both conceptual understanding and practical techniques for
building production ML systems. The challenges we've identified---silent
performance degradation, data drift, model complexity, operational
overhead, ethical concerns---recur throughout these chapters, but now
with specific engineering solutions grounded in real-world experience
and best practices.

Welcome to AI Engineering.

\section{Self-Check Answers}\label{self-check-answers}

\phantomsection\label{quiz-answer-sec-introduction-engineering-revolution-artificial-intelligence-a3eb}
\begin{fbx}{callout-quiz-answer}{Self-Check: Answer 1.1}{}
\phantomsection\label{quiz-answer-sec-introduction-engineering-revolution-artificial-intelligence-a3eb}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{What distinguishes machine learning systems from traditional
  deterministic software architectures?}

  \begin{enumerate}
  \def\labelenumii{\alph{enumii})}
  \tightlist
  \item
    Machine learning systems operate based on explicitly programmed
    instructions.
  \item
    Traditional software systems can adapt autonomously to new data.
  \item
    Machine learning systems rely on statistical patterns extracted from
    data.
  \item
    Traditional software systems require no maintenance.
  \end{enumerate}

  \emph{Answer}: The correct answer is C. Machine learning systems rely
  on statistical patterns extracted from data. This is correct because
  ML systems are probabilistic and their behaviors emerge from data,
  unlike deterministic systems which are based on fixed instructions.

  \emph{Learning Objective}: Understand the fundamental difference
  between traditional and ML systems.
\item
  \textbf{Explain the significance of the `bitter lesson' in AI research
  as mentioned in the section.}

  \emph{Answer}: The `bitter lesson' in AI research refers to the
  realization that domain-general computational methods, such as those
  used in machine learning, ultimately outperform hand-crafted knowledge
  representations. For example, deep learning has surpassed symbolic AI
  in many tasks. This is important because it underscores the shift
  towards systems engineering as central to AI advancement.

  \emph{Learning Objective}: Analyze the impact of historical lessons on
  current AI system design.
\item
  \textbf{Which of the following challenges is NOT typically associated
  with machine learning systems engineering?}

  \begin{enumerate}
  \def\labelenumii{\alph{enumii})}
  \tightlist
  \item
    Eliminating the need for computational infrastructure
  \item
    Achieving scalability for large datasets
  \item
    Maintaining robustness with changing data distributions
  \item
    Ensuring reliability in learned behaviors
  \end{enumerate}

  \emph{Answer}: The correct answer is A. Eliminating the need for
  computational infrastructure. This is incorrect because ML systems
  require substantial computational resources to process data and train
  models, unlike traditional systems where infrastructure might be less
  demanding.

  \emph{Learning Objective}: Identify key challenges unique to ML
  systems engineering.
\item
  \textbf{How does the AI Triangle framework help in understanding
  machine learning systems?}

  \emph{Answer}: The AI Triangle framework helps understand machine
  learning systems by illustrating the interdependencies among data,
  algorithms, and computational infrastructure. For example, changes in
  data quality can affect algorithm performance, which in turn impacts
  infrastructure requirements. This is important because it guides the
  design and optimization of ML systems.

  \emph{Learning Objective}: Explain the role of the AI Triangle in ML
  system analysis.
\end{enumerate}

\noindent\hspace*{1.25em}\hyperref[quiz-question-sec-introduction-engineering-revolution-artificial-intelligence-a3eb]{\textbf{$\leftarrow$~Back to Question}}

\end{fbx}

\phantomsection\label{quiz-answer-sec-introduction-artificial-intelligence-vision-machine-learning-practice-c45a}
\begin{fbx}{callout-quiz-answer}{Self-Check: Answer 1.2}{}
\phantomsection\label{quiz-answer-sec-introduction-artificial-intelligence-vision-machine-learning-practice-c45a}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Which of the following best describes the relationship between
  Artificial Intelligence (AI) and Machine Learning (ML)?}

  \begin{enumerate}
  \def\labelenumii{\alph{enumii})}
  \tightlist
  \item
    AI is a subset of ML focused on data-driven techniques.
  \item
    ML is a practical implementation of AI using rule-based systems.
  \item
    AI and ML are completely independent fields.
  \item
    ML is a subset of AI focused on data-driven techniques.
  \end{enumerate}

  \emph{Answer}: The correct answer is D. ML is a subset of AI focused
  on data-driven techniques. AI is the broader goal of creating
  intelligent systems, while ML provides the methods to achieve this
  through learning from data.

  \emph{Learning Objective}: Understand the hierarchical relationship
  between AI and ML.
\item
  \textbf{Explain why machine learning has become the dominant approach
  in achieving AI goals.}

  \emph{Answer}: Machine learning has become dominant because it allows
  systems to automatically discover patterns from data, making them
  adaptable to new situations without the need for explicit programming.
  For example, ML systems can learn to play chess by analyzing game data
  rather than relying on pre-programmed strategies. This adaptability is
  crucial for handling complex, real-world scenarios where rule-based
  systems fall short.

  \emph{Learning Objective}: Analyze the advantages of ML over
  traditional rule-based AI approaches.
\item
  \textbf{Order the following steps in the evolution from symbolic AI to
  machine learning: (1) Encoding human knowledge as rules, (2)
  Discovering patterns from data, (3) Scaling with compute and data
  infrastructure.}

  \emph{Answer}: The correct order is: (1) Encoding human knowledge as
  rules, (2) Discovering patterns from data, (3) Scaling with compute
  and data infrastructure. Initially, AI relied on manually encoded
  rules, but the shift to ML allowed systems to learn from data. This
  evolution further required scaling with infrastructure to handle large
  datasets and complex models.

  \emph{Learning Objective}: Understand the historical progression and
  scaling implications of AI methodologies.
\end{enumerate}

\noindent\hspace*{1.25em}\hyperref[quiz-question-sec-introduction-artificial-intelligence-vision-machine-learning-practice-c45a]{\textbf{$\leftarrow$~Back to Question}}

\end{fbx}

\phantomsection\label{quiz-answer-sec-introduction-defining-ml-systems-bf7d}
\begin{fbx}{callout-quiz-answer}{Self-Check: Answer 1.3}{}
\phantomsection\label{quiz-answer-sec-introduction-defining-ml-systems-bf7d}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Which of the following best describes a machine learning
  system?}

  \begin{enumerate}
  \def\labelenumii{\alph{enumii})}
  \tightlist
  \item
    A computing system that integrates data, algorithms, and computing
    infrastructure.
  \item
    A standalone algorithm that processes data.
  \item
    A software application that uses pre-defined rules to make
    decisions.
  \item
    A data storage system optimized for large datasets.
  \end{enumerate}

  \emph{Answer}: The correct answer is A. A computing system that
  integrates data, algorithms, and computing infrastructure. This is
  correct because an ML system encompasses the entire ecosystem where
  algorithms operate, including data, learning algorithms, and computing
  infrastructure. Options B, C, and D describe only parts of an ML
  system or unrelated concepts.

  \emph{Learning Objective}: Understand the comprehensive definition of
  a machine learning system.
\item
  \textbf{True or False: In a machine learning system, the model
  architecture does not influence the computational demands for training
  and inference.}

  \emph{Answer}: False. This is false because the model architecture
  directly dictates the computational demands for both training and
  inference, influencing the required infrastructure.

  \emph{Learning Objective}: Recognize the interdependencies between
  model architecture and computing infrastructure in ML systems.
\item
  \textbf{In the context of ML systems, what role does computing
  infrastructure play?}

  \begin{enumerate}
  \def\labelenumii{\alph{enumii})}
  \tightlist
  \item
    It solely stores and retrieves data.
  \item
    It provides the necessary resources for both training and inference.
  \item
    It is only responsible for serving the model predictions.
  \item
    It determines the model architecture to be used.
  \end{enumerate}

  \emph{Answer}: The correct answer is B. It provides the necessary
  resources for both training and inference. This is correct because
  computing infrastructure enables the operation of models at scale,
  supporting both the learning process and the application of learned
  knowledge. Options A, C, and D are incorrect as they describe
  incomplete or unrelated functions.

  \emph{Learning Objective}: Identify the role of computing
  infrastructure in the operation of ML systems.
\item
  \textbf{Consider a scenario where an ML system's data component is
  limited by storage capacity. How might this affect the other
  components of the system?}

  \emph{Answer}: If the data component is limited by storage capacity,
  it may restrict the volume and variety of data available for training,
  potentially leading to less effective learning algorithms.
  Additionally, the computing infrastructure may be underutilized if it
  cannot process larger datasets. This interdependency emphasizes the
  need for balanced system design to optimize overall performance.

  \emph{Learning Objective}: Analyze the interdependencies between data,
  algorithms, and computing infrastructure in ML systems.
\end{enumerate}

\noindent\hspace*{1.25em}\hyperref[quiz-question-sec-introduction-defining-ml-systems-bf7d]{\textbf{$\leftarrow$~Back to Question}}

\end{fbx}

\phantomsection\label{quiz-answer-sec-introduction-ml-systems-differ-traditional-software-4370}
\begin{fbx}{callout-quiz-answer}{Self-Check: Answer 1.4}{}
\phantomsection\label{quiz-answer-sec-introduction-ml-systems-differ-traditional-software-4370}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{What is the fundamental difference in failure modes between
  traditional software and ML systems?}

  \begin{enumerate}
  \def\labelenumii{\alph{enumii})}
  \tightlist
  \item
    Traditional software crashes visibly while ML systems can degrade
    silently without triggering alerts.
  \item
    Traditional software requires more monitoring than ML systems.
  \item
    ML systems always fail faster than traditional software.
  \item
    Traditional software cannot handle errors while ML systems have
    built-in error recovery.
  \end{enumerate}

  \emph{Answer}: The correct answer is A. Traditional software crashes
  visibly while ML systems can degrade silently without triggering
  alerts. This is correct because traditional software exhibits explicit
  failure modes with error messages and alerts, while ML systems can
  continue operating with declining performance due to data distribution
  shifts without triggering conventional error detection mechanisms.
  Options B, C, and D misrepresent the actual differences in failure
  characteristics.

  \emph{Learning Objective}: Distinguish between explicit failure modes
  in traditional software and silent degradation in ML systems.
\item
  \textbf{Explain how the concept of `silent performance degradation'
  differentiates machine learning systems from traditional software
  systems.}

  \emph{Answer}: Silent performance degradation in ML systems refers to
  the phenomenon where a system continues to operate without obvious
  errors, but its performance gradually declines. Unlike traditional
  software, which fails visibly, ML systems may degrade due to changes
  in data distribution or model drift, requiring careful monitoring to
  detect and address these issues. This is important because it
  highlights the need for specialized operational practices in ML system
  deployment.

  \emph{Learning Objective}: Understand the concept of silent
  performance degradation and its implications for ML system operation.
\item
  \textbf{True or False: ML systems can maintain optimal performance
  without specialized monitoring approaches beyond traditional software
  metrics.}

  \emph{Answer}: False. ML systems require specialized monitoring beyond
  traditional software metrics because they can degrade silently due to
  data distribution changes, model drift, or environmental shifts
  without triggering conventional error detection. Unlike traditional
  software that fails visibly, ML systems may continue operating with
  declining performance, necessitating comprehensive monitoring of model
  behavior, data quality, and prediction patterns.

  \emph{Learning Objective}: Understand why ML systems require
  specialized monitoring approaches beyond traditional software metrics.
\item
  \textbf{Why do ML systems require different monitoring approaches
  compared to traditional software systems?}

  \emph{Answer}: ML systems require monitoring of infrastructure health,
  model performance, data quality, and prediction distributions, whereas
  traditional software monitoring focuses primarily on infrastructure
  metrics like uptime and latency. This is necessary because ML systems
  can degrade due to data distribution shifts, model drift, or
  environmental changes without any code changes or infrastructure
  failures. For example, a recommendation system's accuracy might
  decline as user preferences evolve, requiring monitoring beyond
  traditional infrastructure metrics. This comprehensive monitoring
  enables early detection of performance degradation before it impacts
  users.

  \emph{Learning Objective}: Analyze the unique monitoring requirements
  that distinguish ML systems from traditional software engineering
  practices.
\end{enumerate}

\noindent\hspace*{1.25em}\hyperref[quiz-question-sec-introduction-ml-systems-differ-traditional-software-4370]{\textbf{$\leftarrow$~Back to Question}}

\end{fbx}

\phantomsection\label{quiz-answer-sec-introduction-bitter-lesson-systems-engineering-matters-dede}
\begin{fbx}{callout-quiz-answer}{Self-Check: Answer 1.5}{}
\phantomsection\label{quiz-answer-sec-introduction-bitter-lesson-systems-engineering-matters-dede}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{What is the primary lesson from 70 years of AI research
  according to Richard Sutton's `Bitter Lesson'?}

  \begin{enumerate}
  \def\labelenumii{\alph{enumii})}
  \tightlist
  \item
    Leveraging massive computational resources
  \item
    Curating better datasets
  \item
    Developing more sophisticated algorithms
  \item
    Encoding human expertise into AI systems
  \end{enumerate}

  \emph{Answer}: The correct answer is A. Leveraging massive
  computational resources. This is correct because Sutton's `Bitter
  Lesson' emphasizes that general methods using computation are the most
  effective.

  \emph{Learning Objective}: Understand the core insight from the
  `Bitter Lesson' regarding the role of computation in AI success.
\item
  \textbf{Explain why systems engineering has become more critical than
  algorithmic development in modern AI systems.}

  \emph{Answer}: Systems engineering is critical because leveraging
  massive computational resources has proven more effective than
  algorithmic improvements. For example, systems like AlphaGo achieve
  success through computation rather than human expertise. This is
  important because effective scaling of computation determines AI
  progress.

  \emph{Learning Objective}: Analyze the shift in focus from algorithmic
  development to systems engineering in AI.
\item
  \textbf{True or False: The primary constraint in modern ML systems is
  compute capacity rather than memory bandwidth.}

  \emph{Answer}: False. This is false because the primary constraint is
  memory bandwidth, which limits data movement and thus system
  performance.

  \emph{Learning Objective}: Understand the technical constraints in
  modern ML systems, specifically the role of memory bandwidth.
\item
  \textbf{Which factor is NOT a primary challenge in scaling modern AI
  systems?}

  \begin{enumerate}
  \def\labelenumii{\alph{enumii})}
  \tightlist
  \item
    Thermal and power constraints
  \item
    Memory bandwidth limitations
  \item
    Data center coordination
  \item
    Algorithmic complexity
  \end{enumerate}

  \emph{Answer}: The correct answer is D. Algorithmic complexity. This
  is correct because the primary challenges are related to
  infrastructure, not the complexity of algorithms.

  \emph{Learning Objective}: Identify the main challenges in scaling AI
  systems from a systems engineering perspective.
\item
  \textbf{In a production system, how might you address the memory
  bandwidth bottleneck when deploying large-scale ML models?}

  \emph{Answer}: To address memory bandwidth bottlenecks, one could use
  high-bandwidth memory (HBM), optimize data movement, or employ
  near-data processing. For example, using specialized accelerators that
  co-locate compute and storage can reduce data transfer times. This is
  important to improve system efficiency and performance.

  \emph{Learning Objective}: Apply knowledge of memory bandwidth
  constraints to practical ML system deployment scenarios.
\end{enumerate}

\noindent\hspace*{1.25em}\hyperref[quiz-question-sec-introduction-bitter-lesson-systems-engineering-matters-dede]{\textbf{$\leftarrow$~Back to Question}}

\end{fbx}

\phantomsection\label{quiz-answer-sec-introduction-historical-evolution-ai-paradigms-796e}
\begin{fbx}{callout-quiz-answer}{Self-Check: Answer 1.6}{}
\phantomsection\label{quiz-answer-sec-introduction-historical-evolution-ai-paradigms-796e}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Which of the following factors did NOT contribute to the
  transition towards a systems-focused approach in AI?}

  \begin{enumerate}
  \def\labelenumii{\alph{enumii})}
  \tightlist
  \item
    Massive datasets from the internet age
  \item
    The development of symbolic AI in the 1950s
  \item
    Increased availability of low-cost GPUs
  \item
    Algorithmic breakthroughs in deep learning
  \end{enumerate}

  \emph{Answer}: The correct answer is B. The development of symbolic AI
  in the 1950s. While symbolic AI was foundational, the transition to
  systems-focused AI was driven by more recent factors like data,
  algorithms, and hardware improvements.

  \emph{Learning Objective}: Understand the key factors that influenced
  the shift towards a systems-centric approach in AI.
\item
  \textbf{Explain how the convergence of massive datasets, algorithmic
  breakthroughs, and hardware acceleration has transformed AI from an
  academic curiosity to a production technology.}

  \emph{Answer}: The convergence allowed AI to scale effectively:
  massive datasets provided the raw material for learning, algorithmic
  breakthroughs like deep learning enabled models to learn from this
  data, and hardware acceleration made it feasible to train and deploy
  these models at scale. This transformation made AI practical for
  real-world applications, requiring robust engineering practices.

  \emph{Learning Objective}: Analyze the interplay of data, algorithms,
  and hardware in transforming AI into a practical technology.
\item
  \textbf{True or False: The systems-centric approach in AI emerged
  because early AI systems were too complex and required
  simplification.}

  \emph{Answer}: False. The systems-centric approach emerged due to the
  need to handle massive datasets, leverage algorithmic breakthroughs,
  and utilize hardware acceleration, not because early systems were too
  complex.

  \emph{Learning Objective}: Correct misconceptions about the reasons
  for the shift to systems-centric AI.
\item
  \textbf{The introduction of \_\_\_\_ by OpenAI in 2020 demonstrated
  the increasing complexity and capability of AI systems.}

  \emph{Answer}: GPT-3. This model exemplified the scale and capability
  of modern AI systems, requiring significant computational resources
  and showcasing emergent abilities.

  \emph{Learning Objective}: Recall key milestones that illustrate the
  evolution and scaling of AI systems.
\end{enumerate}

\noindent\hspace*{1.25em}\hyperref[quiz-question-sec-introduction-historical-evolution-ai-paradigms-796e]{\textbf{$\leftarrow$~Back to Question}}

\end{fbx}

\phantomsection\label{quiz-answer-sec-introduction-understanding-ml-system-lifecycle-deployment-0ab0}
\begin{fbx}{callout-quiz-answer}{Self-Check: Answer 1.7}{}
\phantomsection\label{quiz-answer-sec-introduction-understanding-ml-system-lifecycle-deployment-0ab0}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{What is a key difference between the lifecycle of ML systems
  and traditional software systems?}

  \begin{enumerate}
  \def\labelenumii{\alph{enumii})}
  \tightlist
  \item
    ML systems require continuous monitoring and adaptation.
  \item
    Traditional software systems rely on data for behavior.
  \item
    ML systems have a linear development process.
  \item
    Traditional software systems are probabilistic in nature.
  \end{enumerate}

  \emph{Answer}: The correct answer is A. ML systems require continuous
  monitoring and adaptation. This is because ML systems are data-driven
  and must adjust to changes in data patterns, unlike traditional
  software which follows deterministic logic.

  \emph{Learning Objective}: Understand the fundamental differences in
  lifecycle between ML systems and traditional software.
\item
  \textbf{How does the deployment environment influence the ML system
  lifecycle?}

  \emph{Answer}: The deployment environment dictates resource
  availability, operational complexity, and data management strategies.
  For example, cloud deployments offer scalability but incur high costs,
  while edge deployments reduce latency but face resource constraints.
  These factors influence data collection, model training, and update
  strategies, impacting the entire lifecycle.

  \emph{Learning Objective}: Analyze how different deployment
  environments affect the ML system lifecycle.
\item
  \textbf{Order the following stages of the ML system lifecycle as
  depicted in the section: (1) Model Training, (2) Model Deployment, (3)
  Model Evaluation, (4) Data Collection, (5) Model Monitoring, (6) Data
  Preparation.}

  \emph{Answer}: The correct order is: (4) Data Collection, (6) Data
  Preparation, (1) Model Training, (3) Model Evaluation, (2) Model
  Deployment, (5) Model Monitoring. This order reflects the iterative
  nature of ML systems, emphasizing continuous feedback and adaptation.

  \emph{Learning Objective}: Reinforce understanding of the cyclical
  nature of the ML system lifecycle.
\end{enumerate}

\noindent\hspace*{1.25em}\hyperref[quiz-question-sec-introduction-understanding-ml-system-lifecycle-deployment-0ab0]{\textbf{$\leftarrow$~Back to Question}}

\end{fbx}

\phantomsection\label{quiz-answer-sec-introduction-case-studies-realworld-ml-systems-a2ba}
\begin{fbx}{callout-quiz-answer}{Self-Check: Answer 1.8}{}
\phantomsection\label{quiz-answer-sec-introduction-case-studies-realworld-ml-systems-a2ba}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Which of the following best describes the primary challenge
  Waymo faces with its data pipeline?}

  \begin{enumerate}
  \def\labelenumii{\alph{enumii})}
  \tightlist
  \item
    Limited data storage capacity
  \item
    Heterogeneity and real-time processing of data
  \item
    High cost of data transmission
  \item
    Insufficient data collection from sensors
  \end{enumerate}

  \emph{Answer}: The correct answer is B. Heterogeneity and real-time
  processing of data. Waymo's data pipeline must handle both structured
  and unstructured data in real-time, which requires sophisticated
  processing due to the safety-critical nature of autonomous driving.

  \emph{Learning Objective}: Understand the data challenges faced by ML
  systems in real-world applications like autonomous vehicles.
\item
  \textbf{Explain how Waymo's use of both edge and cloud computing
  supports its autonomous vehicle operations.}

  \emph{Answer}: Waymo uses edge computing to process sensor data and
  make real-time decisions on the vehicle, while cloud computing
  supports large-scale model training and simulation. This combination
  allows for real-time responsiveness and extensive learning
  capabilities. For example, edge computing enables immediate reaction
  to road conditions, whereas cloud computing facilitates continuous
  improvement of driving models. This is important because it balances
  the need for immediate action with the capacity for long-term
  learning.

  \emph{Learning Objective}: Analyze the role of edge and cloud
  computing in supporting complex ML systems in real-time applications.
\item
  \textbf{Waymo's perception system employs specialized \_\_\_\_ to
  process visual data for object detection and tracking.}

  \emph{Answer}: neural networks. Waymo uses neural networks to
  interpret visual data from its sensors, which is crucial for detecting
  and tracking objects in the vehicle's environment.

  \emph{Learning Objective}: Recall the specific technologies used in ML
  systems for perception tasks.
\item
  \textbf{True or False: Waymo's infrastructure is designed to
  prioritize computational throughput over latency.}

  \emph{Answer}: False. Waymo's infrastructure prioritizes latency to
  ensure real-time decision-making capability in safety-critical
  environments like autonomous driving.

  \emph{Learning Objective}: Understand the infrastructure priorities
  for real-time ML systems in safety-critical applications.
\item
  \textbf{In a production system like Waymo, what trade-offs might
  engineers consider between sensor accuracy and cost?}

  \emph{Answer}: Engineers must balance sensor accuracy with cost to
  ensure that the system remains economically viable while maintaining
  safety. For instance, LiDAR provides high accuracy but is expensive,
  so engineers might use a combination of LiDAR and cheaper sensors like
  radar to achieve a cost-effective solution. This trade-off is crucial
  because it affects both the system's performance and its scalability.

  \emph{Learning Objective}: Evaluate the trade-offs involved in
  choosing sensor technologies for ML systems in autonomous vehicles.
\end{enumerate}

\noindent\hspace*{1.25em}\hyperref[quiz-question-sec-introduction-case-studies-realworld-ml-systems-a2ba]{\textbf{$\leftarrow$~Back to Question}}

\end{fbx}

\phantomsection\label{quiz-answer-sec-introduction-core-engineering-challenges-ml-systems-6482}
\begin{fbx}{callout-quiz-answer}{Self-Check: Answer 1.9}{}
\phantomsection\label{quiz-answer-sec-introduction-core-engineering-challenges-ml-systems-6482}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Which of the following is a primary data-related challenge in
  ML systems like Waymo's?}

  \begin{enumerate}
  \def\labelenumii{\alph{enumii})}
  \tightlist
  \item
    Algorithmic efficiency
  \item
    Data version control
  \item
    User interface design
  \item
    Network latency
  \end{enumerate}

  \emph{Answer}: The correct answer is B. Data version control. This is
  correct because managing large datasets over time requires maintaining
  data quality metadata and version control for datasets. Algorithmic
  efficiency, user interface design, and network latency are not
  primarily data-related challenges.

  \emph{Learning Objective}: Understand the core data-related challenges
  in ML systems.
\item
  \textbf{Explain how data drift can affect the performance of an ML
  system like Waymo's autonomous vehicles.}

  \emph{Answer}: Data drift can degrade model performance as the
  statistical properties of input data change over time. For example,
  Waymo's models might encounter new traffic patterns or weather
  conditions not present in training data, leading to reduced accuracy.
  This is important because it requires continuous monitoring and
  adaptation to maintain system reliability.

  \emph{Learning Objective}: Analyze the impact of data drift on ML
  system performance.
\item
  \textbf{In the context of ML systems, what is a significant challenge
  when scaling data management?}

  \begin{enumerate}
  \def\labelenumii{\alph{enumii})}
  \tightlist
  \item
    Reducing algorithm complexity
  \item
    Improving user interface aesthetics
  \item
    Ensuring data quality and efficient retrieval
  \item
    Minimizing hardware costs
  \end{enumerate}

  \emph{Answer}: The correct answer is C. Ensuring data quality and
  efficient retrieval. This is correct because scaling involves managing
  large volumes of data while maintaining quality and ensuring efficient
  access during model training. Other options do not directly relate to
  data management challenges.

  \emph{Learning Objective}: Identify challenges associated with scaling
  data management in ML systems.
\item
  \textbf{Discuss the interdependencies between data quality and model
  performance in ML systems. How do these interdependencies affect
  system design?}

  \emph{Answer}: Data quality directly influences model performance;
  poor data quality can lead to inaccurate predictions. For example,
  sensor noise or distribution shifts can degrade model accuracy. This
  interdependency requires system designs that prioritize robust data
  management and continuous monitoring to ensure reliable performance.

  \emph{Learning Objective}: Understand the interdependencies between
  data quality and model performance and their implications for system
  design.
\end{enumerate}

\noindent\hspace*{1.25em}\hyperref[quiz-question-sec-introduction-core-engineering-challenges-ml-systems-6482]{\textbf{$\leftarrow$~Back to Question}}

\end{fbx}

\phantomsection\label{quiz-answer-sec-introduction-defining-ai-engineering-b812}
\begin{fbx}{callout-quiz-answer}{Self-Check: Answer 1.10}{}
\phantomsection\label{quiz-answer-sec-introduction-defining-ai-engineering-b812}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{What is the primary focus of AI Engineering as a discipline?}

  \begin{enumerate}
  \def\labelenumii{\alph{enumii})}
  \tightlist
  \item
    Developing new AI algorithms
  \item
    Improving symbolic reasoning techniques
  \item
    Building reliable, efficient, and scalable ML systems
  \item
    Enhancing user interfaces for AI applications
  \end{enumerate}

  \emph{Answer}: The correct answer is C. Building reliable, efficient,
  and scalable ML systems. AI Engineering focuses on the entire
  lifecycle of ML systems, emphasizing reliability, efficiency, and
  scalability.

  \emph{Learning Objective}: Understand the core focus of AI Engineering
  as a discipline.
\item
  \textbf{Explain how the historical shift from symbolic systems to
  learning-based approaches has influenced the emergence of AI
  Engineering.}

  \emph{Answer}: The shift from symbolic systems to learning-based
  approaches has led to the dominance of machine learning in AI,
  necessitating a focus on systems-level challenges. This has resulted
  in the emergence of AI Engineering, which addresses the lifecycle of
  building and maintaining scalable and efficient ML systems. This is
  important because it reflects the practical realities of deploying AI
  in production environments.

  \emph{Learning Objective}: Analyze the historical context that led to
  the development of AI Engineering.
\item
  \textbf{Which of the following best describes the role of AI
  Engineering in modern AI systems?}

  \begin{enumerate}
  \def\labelenumii{\alph{enumii})}
  \tightlist
  \item
    Focusing solely on algorithmic development
  \item
    Developing symbolic AI techniques
  \item
    Designing user-friendly AI interfaces
  \item
    Integrating and optimizing systems for real-world deployment
  \end{enumerate}

  \emph{Answer}: The correct answer is D. Integrating and optimizing
  systems for real-world deployment. AI Engineering involves the
  systems-level integration and optimization necessary for deploying AI
  systems effectively.

  \emph{Learning Objective}: Identify the role of AI Engineering in the
  deployment of AI systems.
\item
  \textbf{In a production system, how might AI Engineering address the
  challenge of energy efficiency while maintaining performance?}

  \emph{Answer}: AI Engineering addresses energy efficiency by
  optimizing data pipelines, utilizing specialized hardware, and
  designing algorithms that reduce computational overhead. For example,
  deploying models on energy-efficient GPUs can maintain performance
  while minimizing power consumption. This is important because energy
  costs are a significant factor in large-scale AI deployments.

  \emph{Learning Objective}: Evaluate how AI Engineering balances energy
  efficiency with system performance.
\end{enumerate}

\noindent\hspace*{1.25em}\hyperref[quiz-question-sec-introduction-defining-ai-engineering-b812]{\textbf{$\leftarrow$~Back to Question}}

\end{fbx}

\phantomsection\label{refs}
\begin{CSLReferences}{1}{0}
\bibitem[\citeproctext]{ref-bobrow1964student}
Bobrow, Daniel G. 1964. {``Natural Language Input for a Computer Problem
Solving System.''} \emph{PhD Thesis, MIT}.
\url{https://dspace.mit.edu/handle/1721.1/12962}.

\bibitem[\citeproctext]{ref-brown2020language}
Brown, Tom, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan,
Prafulla Dhariwal, Arvind Neelakantan, et al. 2020. {``Language Models
Are Few-Shot Learners.''} \emph{Advances in Neural Information
Processing Systems} 33: 1877--1901.

\bibitem[\citeproctext]{ref-campbell2002deep}
Campbell, Murray, Jr. Hoane A.Joseph, and Feng-hsiung Hsu. 2002. {``Deep
Blue.''} \emph{Artificial Intelligence} 134 (1-2): 57--83.
\url{https://doi.org/10.1016/s0004-3702(01)00129-1}.

\bibitem[\citeproctext]{ref-deng2009imagenet}
Deng, Jia, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei.
2009. {``ImageNet: A Large-Scale Hierarchical Image Database.''} In
\emph{2009 IEEE Conference on Computer Vision and Pattern Recognition},
248--55. IEEE; IEEE. \url{https://doi.org/10.1109/cvpr.2009.5206848}.

\bibitem[\citeproctext]{ref-jumper2021highly}
Jumper, John, Richard Evans, Alexander Pritzel, Tim Green, Michael
Figurnov, Olaf Ronneberger, Kathryn Tunyasuvunakool, et al. 2021.
{``Highly Accurate Protein Structure Prediction with AlphaFold.''}
\emph{Nature} 596 (7873): 583--89.
\url{https://doi.org/10.1038/s41586-021-03819-2}.

\bibitem[\citeproctext]{ref-krizhevsky2012imagenet}
Krizhevsky, Alex, Ilya Sutskever, and Geoffrey E. Hinton. 2017.
{``ImageNet Classification with Deep Convolutional Neural Networks.''}
\emph{Communications of the ACM} 60 (6): 84--90.
\url{https://doi.org/10.1145/3065386}.

\bibitem[\citeproctext]{ref-kuhn1962structure}
Kvasz, Ladislav. 2014. {``Kuhn's Structure of Scientific Revolutions
Between Sociology and Epistemology.''} \emph{Studies in History and
Philosophy of Science Part A} 46 (June): 78--84.
\url{https://doi.org/10.1016/j.shpsa.2014.02.006}.

\bibitem[\citeproctext]{ref-lecun1989backpropagation}
LeCun, Y., B. Boser, J. S. Denker, D. Henderson, R. E. Howard, W.
Hubbard, and L. D. Jackel. 1989. {``Backpropagation Applied to
Handwritten Zip Code Recognition.''} \emph{Neural Computation} 1 (4):
541--51. \url{https://doi.org/10.1162/neco.1989.1.4.541}.

\bibitem[\citeproctext]{ref-li2020estimating}
Li, Chengwei. 2020. {``Estimating the Training Cost of GPT-3.''}
\url{https://lambdalabs.com/blog/demystifying-gpt-3}.

\bibitem[\citeproctext]{ref-mccarthy1956dartmouth}
McCarthy, John, Marvin L. Minsky, Nathaniel Rochester, and Claude E.
Shannon. 1955. {``A Proposal for the Dartmouth Summer Research Project
on Artificial Intelligence.''} In.
\url{http://www-formal.stanford.edu/jmc/history/dartmouth/dartmouth.html}.

\bibitem[\citeproctext]{ref-minsky1969perceptrons}
Minsky, Marvin, and Seymour A. Papert. 2017. \emph{Perceptrons: An
Introduction to Computational Geometry}. Cambridge, MA: The MIT Press.
\url{https://doi.org/10.7551/mitpress/11301.001.0001}.

\bibitem[\citeproctext]{ref-moore1965cramming}
Moore, G. E. 1998. {``Cramming More Components onto Integrated
Circuits.''} \emph{Proceedings of the IEEE} 86 (1): 82--85.
\url{https://doi.org/10.1109/jproc.1998.658762}.

\bibitem[\citeproctext]{ref-openai2023gpt4}
OpenAI, Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge
Akkaya, Florencia Leoni Aleman, et al. 2023. {``GPT-4 Technical
Report,''} March. \url{http://arxiv.org/abs/2303.08774v6}.

\bibitem[\citeproctext]{ref-patterson2021carbon}
Patterson, David, Joseph Gonzalez, Quoc Le, Chen Liang, Lluis-Miquel
Munguia, Daniel Rothchild, David So, Maud Texier, and Jeff Dean. 2021.
{``Carbon Emissions and Large Neural Network Training.''} \emph{arXiv
Preprint arXiv:2104.10350}, April.
\url{http://arxiv.org/abs/2104.10350v3}.

\bibitem[\citeproctext]{ref-rumelhart1986learning}
Rumelhart, David E., Geoffrey E. Hinton, and Ronald J. Williams. 1986.
{``Learning Representations by Back-Propagating Errors.''} \emph{Nature}
323 (6088): 533--36. \url{https://doi.org/10.1038/323533a0}.

\bibitem[\citeproctext]{ref-shortliffe1976mycin}
Shortliffe, Edward H. 1975. {``Computer-Based Consultations in Clinical
Therapeutics: Explanation and Rule Acquisition Capabilities of the MYCIN
System.''} \emph{Computers and Biomedical Research} 8 (4): 303--20.
\url{https://doi.org/10.1016/0010-4809(75)90009-9}.

\bibitem[\citeproctext]{ref-silver2016mastering}
Silver, David, Aja Huang, Chris J. Maddison, Arthur Guez, Laurent Sifre,
George van den Driessche, Julian Schrittwieser, et al. 2016.
{``Mastering the Game of Go with Deep Neural Networks and Tree
Search.''} \emph{Nature} 529 (7587): 484--89.
\url{https://doi.org/10.1038/nature16961}.

\bibitem[\citeproctext]{ref-strubell2019energy}
Strubell, Emma, Ananya Ganesh, and Andrew McCallum. 2019. {``Energy and
Policy Considerations for Deep Learning in NLP.''} In \emph{Proceedings
of the 57th Annual Meeting of the Association for Computational
Linguistics}, 3645--50. Association for Computational Linguistics.
\url{https://doi.org/10.18653/v1/p19-1355}.

\bibitem[\citeproctext]{ref-sutton2019bitter}
Sutton, Richard S. 2019. {``The Bitter Lesson.''}
\url{http://www.incompleteideas.net/IncIdeas/BitterLesson.html}.

\bibitem[\citeproctext]{ref-viola2001rapidobject}
Viola, P., and M. Jones. n.d. {``Rapid Object Detection Using a Boosted
Cascade of Simple Features.''} In \emph{Proceedings of the 2001 IEEE
Computer Society Conference on Computer Vision and Pattern Recognition.
CVPR 2001}, 1:I-511-I-518. IEEE Comput. Soc.
\url{https://doi.org/10.1109/cvpr.2001.990517}.

\bibitem[\citeproctext]{ref-weizenbaum1966eliza}
Weizenbaum, Joseph. 1966. {``ELIZA---a Computer Program for the Study of
Natural Language Communication Between Man and Machine.''}
\emph{Communications of the ACM} 9 (1): 36--45.
\url{https://doi.org/10.1145/365153.365168}.

\bibitem[\citeproctext]{ref-rosenblatt1957perceptron}
Wolfe, Jeremy M., Dennis M. Levi, Lori L. Holt, Linda M. Bartoshuk,
Rachel S. Herz, Roberta L. Klatzky, and Daniel M. Merfeld. 2024.
{``Perceiving and Recognizing Objects.''} Technical Report. In
\emph{Sensation \&Amp; Perception}. 85-460-1. Cornell Aeronautical
Laboratory; Oxford University Press.
\url{https://doi.org/10.1093/hesc/9780197663813.003.0005}.

\end{CSLReferences}


\backmatter


\end{document}
