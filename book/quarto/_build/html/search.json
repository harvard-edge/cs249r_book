[
  {
    "objectID": "contents/vol2/storage/storage.html#purpose",
    "href": "contents/vol2/storage/storage.html#purpose",
    "title": "Storage Systems for ML",
    "section": "Purpose",
    "text": "Purpose\nHow do storage system architectures shape what machine learning systems can accomplish at production scale?\nMachine learning workloads create distinctive storage demands. Training requires streaming petabytes of data through accelerators at rates that saturate the fastest interconnects, while inference demands millisecond latency access to model weights and feature data across globally distributed serving infrastructure. Storage systems adequate for traditional applications become bottlenecks when confronted with ML access patterns: massive sequential reads during training, random access during feature lookup, and the need to version datasets, models, and artifacts across experimental workflows. The gap between storage capabilities and ML requirements determines training throughput, inference latency, and the feasibility of rapid iteration on model development. Understanding how distributed storage architectures, data lakes, and feature stores address these challenges enables engineers to design systems where storage supports rather than constrains machine learning progress.\n\n\n\n\n\n\nTipLearning Objectives\n\n\n\n\nAnalyze how the ML storage hierarchy extends the classical memory hierarchy and explain why ML workloads invert traditional storage assumptions about access patterns and working set sizes\nCalculate required storage bandwidth for distributed training using the data pipeline throughput equation and apply it to different model types and cluster configurations\nCompare distributed file system architectures (GFS/Colossus, HDFS, Lustre) and object storage systems (S3, GCS) for different ML workload characteristics\nDesign checkpoint strategies using the Young-Daly formula to optimize the tradeoff between checkpoint overhead and recovery time for different model types and cluster sizes\nEvaluate feature store architectures for online and offline serving, understanding point-in-time correctness requirements and why feature stores are critical infrastructure for recommendation systems\nImplement model registry and artifact management systems that ensure reproducibility and lineage tracking across experimental workflows",
    "crumbs": [
      "Volume II: Advanced",
      "Foundations of Scale",
      "Storage Systems for ML"
    ]
  },
  {
    "objectID": "contents/vol2/storage/storage.html#sec-storage-fundamentals",
    "href": "contents/vol2/storage/storage.html#sec-storage-fundamentals",
    "title": "Storage Systems for ML",
    "section": "Storage Fundamentals for ML",
    "text": "Storage Fundamentals for ML\nThe infrastructure foundations established in ?@sec-infrastructure provide the compute fabric and networking topology for large-scale ML systems. Yet even the most powerful GPU clusters remain idle without data to process. Storage systems determine whether thousands of accelerators receive training data at the rates they require, whether model checkpoints can be saved before failures corrupt hours of computation, and whether features can be retrieved within the latency budgets that production inference demands. Understanding storage architecture for ML begins with recognizing how fundamentally ML workloads differ from the applications that shaped traditional storage system design.\nTraditional enterprise storage systems evolved to serve transactional databases and file servers, workloads characterized by small random accesses, strong consistency requirements, and moderate bandwidth demands. A database server might issue thousands of 4KB reads per second to serve user queries, each read potentially touching different storage locations. The storage industry optimized relentlessly for this pattern, developing sophisticated caching algorithms, RAID configurations, and file systems tuned for small-block random access with transactional guarantees.\nML workloads invert nearly every assumption that shaped these systems. Training data access is predominantly sequential, streaming through datasets that may span hundreds of terabytes. Individual accesses are large, often megabytes rather than kilobytes, as models consume batches of images, text sequences, or feature vectors. Consistency requirements are relaxed; slightly stale feature values rarely affect model quality, and training can tolerate occasional data corruption through its inherent noise tolerance. But bandwidth demands are extreme, frequently requiring sustained throughput that would overwhelm systems designed for transactional workloads.\nThis mismatch between traditional storage design and ML requirements creates challenges that surface at every level of the storage hierarchy. Object stores designed for web-scale applications deliver excellent scalability but introduce latencies that starve GPU pipelines. Parallel file systems engineered for scientific computing provide the bandwidth but struggle with the metadata operations that ML checkpointing generates. Local NVMe drives offer the latency characteristics that inference demands but lack the capacity for training datasets. Effective ML storage architecture requires understanding these trade-offs and composing storage tiers that match each phase of the ML lifecycle.\n\nThe ML Storage Hierarchy\nComputer architecture courses teach the memory hierarchy as a fundamental organizing abstraction: registers at nanosecond latencies, caches at microseconds, DRAM at hundreds of nanoseconds, and storage devices at milliseconds. This hierarchy exists because of a persistent truth in computing: faster memory is more expensive per bit, so systems use smaller amounts of fast memory as caches for larger amounts of slower memory. The principle of locality, both temporal (recently accessed data will likely be accessed again) and spatial (nearby data will likely be accessed soon), makes caching effective for most workloads.\nML systems extend this hierarchy with two critical additions: GPU High Bandwidth Memory (HBM) and distributed storage spanning multiple tiers. The extended hierarchy reveals the extreme bandwidth disparities that ML systems must navigate.\n\n\n\nTable 1: Extended memory hierarchy for ML systems. Bandwidth figures represent practical throughput; latency represents typical access times.\n\n\n\n\n\n\n\n\n\n\n\n\nStorage Tier\nTypical Capacity\nBandwidth\nLatency\nCost ($/GB)\n\n\n\n\nGPU HBM\n80 GB\n3.35 TB/s\n~10 ns\n~15.00\n\n\nHost DRAM\n512 GB - 2 TB\n200 GB/s\n~100 ns\n~3.00\n\n\nLocal NVMe SSD\n4-30 TB\n7-25 GB/s\n~10 μs\n~0.10\n\n\nParallel File System\n100+ PB\n1+ TB/s aggregate\n~1 ms\n~0.03\n\n\nObject Storage\nUnlimited\n100 GB/s aggregate\n~50 ms\n~0.02\n\n\nArchive/Cold Storage\nUnlimited\n1 GB/s\nMinutes to hours\n~0.004\n\n\n\n\n\n\nThe bandwidth column in Table 1 deserves particular attention. GPU HBM delivers 3.35 TB/s, roughly 17x faster than host DRAM and 130x faster than the fastest local NVMe drives. This disparity creates the central challenge of ML storage systems: keeping accelerators fed with data at rates that prevent them from idling.\nConsider what happens when an H100 GPU processes a training batch. At 1,979 TFLOPS of FP16 compute capability, the GPU can perform approximately 2 quadrillion floating-point operations per second. A typical transformer forward pass requires roughly 6 FLOPs per parameter per token. For a 7 billion parameter model processing 2048-token sequences with a batch size of 32, each forward pass involves:\n\\[\\text{FLOPs per batch} = 6 \\times 7 \\times 10^9 \\times 2048 \\times 32 \\approx 2.75 \\times 10^{15}\\]\nAt 1,979 TFLOPS, this computation completes in approximately 1.4 seconds, during which the next batch must be ready in GPU memory. If data arrives even slightly slower than the GPU consumes it, expensive accelerator time is wasted waiting for storage.\n\n\nHow ML Workloads Invert Traditional Assumptions\nTraditional storage system design optimizes for workloads with specific characteristics: random access patterns, working sets that fit in cache, and write-heavy transactional loads. ML workloads systematically violate each of these assumptions, requiring fundamentally different storage architectures.\nSequential streaming dominates. Database workloads exhibit random access patterns as queries retrieve specific records from large tables. ML training, by contrast, performs massive sequential scans through datasets. A training epoch reads every sample once, in whatever order the shuffling algorithm produces, before repeating. This access pattern resembles video streaming more than database queries. Storage systems optimized for random IOPS (input/output operations per second) waste their capabilities on ML workloads, while systems optimized for sequential throughput excel.\nWorking sets exceed any cache level. Traditional applications exhibit locality: a web server repeatedly accesses the same popular pages, a database repeatedly queries hot rows. Caching exploits this locality, keeping frequently accessed data in fast memory. ML training datasets are accessed uniformly: each sample is read once per epoch, with no sample more likely to be accessed than any other during training. A 10 TB image dataset cannot be cached in DRAM; each sample is effectively cold when accessed. This lack of locality renders traditional caching strategies ineffective for training data.\nWrite patterns are bursty rather than continuous. Transactional systems generate continuous streams of small writes as users update records. ML systems generate occasional massive writes when saving checkpoints. A 175 billion parameter model checkpoint occupies approximately 700 GB; saving it every 10 minutes generates 70 GB/minute average throughput but concentrated into bursts that may saturate storage bandwidth for 1-2 minutes followed by idle periods. This bursty pattern requires storage systems that can absorb high-bandwidth writes without blocking ongoing reads.\nRead/write ratios vary dramatically by phase. Training reads vastly exceed writes: a typical training run reads the dataset dozens of times (one per epoch) while writing only periodic checkpoints. The read-to-write ratio can exceed 100:1. Inference, conversely, is almost entirely read-only, loading model weights once and then serving requests without writes. Feature stores for recommendation systems present yet another pattern: continuous reads for serving interleaved with batch writes from offline feature computation. No single storage configuration optimizes all three patterns.\n\n\n\nTable 2: Contrast between traditional storage assumptions and ML workload characteristics.\n\n\n\n\n\nWorkload Pattern\nTraditional Assumption\nML Reality\n\n\n\n\nAccess pattern\nRandom access\nSequential streaming\n\n\nWorking set\nFits in cache\nExceeds all cache levels\n\n\nWrite pattern\nContinuous small writes\nBursty large writes\n\n\nRead/write ratio\nBalanced\nPhase-dependent (100:1 to 1:0)\n\n\nLocality\nStrong temporal locality\nNo locality (uniform sampling)\n\n\n\n\n\n\n\n\nAccess Pattern Analysis\nUnderstanding access patterns quantitatively enables storage system selection and capacity planning. ML workloads exhibit distinct patterns across different phases and model types.\nTraining data access follows a streaming sequential pattern with random shuffling. Each epoch reads the entire dataset once, but the order is randomized to prevent the model from learning ordering artifacts. This creates a pattern that is globally sequential (every sample accessed once) but locally random (no predictable next sample). Storage systems must support high sequential bandwidth while handling the pseudo-random access order that shuffling creates.\nFor distributed training, access patterns multiply: \\(N\\) workers each read \\(1/N\\) of the dataset per step, but the global shuffle means no locality between workers. If workers access a shared storage system, the aggregate access pattern appears random even though each worker performs sequential reads of its partition.\nCheckpoint access exhibits extreme write bursts followed by long read intervals. During normal training, checkpoints represent nearly all write traffic. The access pattern is:\n\nWrite: Save complete model state every \\(T_{checkpoint}\\) minutes\nRead: Load most recent checkpoint on training restart\nDelete: Remove old checkpoints after new ones are verified\n\nThe checkpoint write burst must complete before the next training step can safely proceed (for synchronous checkpointing) or within a bounded delay (for asynchronous approaches). Checkpoint reads are infrequent but critical: when failures occur, recovery time depends on checkpoint load bandwidth.\nFeature store access patterns differ fundamentally from training data. Online serving requires point lookups: given a user ID, retrieve that user’s features. This is the random access pattern that traditional storage optimizes for, with latency requirements in single-digit milliseconds. Offline feature computation generates batch writes as feature pipelines process new data. The pattern resembles:\n\nOnline reads: Millions of random point lookups per second, &lt; 10ms latency requirement\nOffline writes: Batch updates every minutes to hours, throughput-optimized\n\nModel serving access loads model weights at startup, then serves inference requests using weights cached in GPU memory. The access pattern is write-once-read-many with extreme read amplification: a 7 billion parameter model loaded once serves millions of inference requests. Storage bandwidth matters only during model loading; once loaded, storage is unused until model updates or server restarts.\n\n\n\nTable 3: Access patterns vary significantly by model type and system phase.\n\n\n\n\n\n\n\n\n\n\n\n\nModel Type\nTraining Data Pattern\nCheckpoint Pattern\nFeature Pattern\nServing Pattern\n\n\n\n\nLLM\nSequential streaming, TB-scale\nInfrequent, 100GB-1TB bursts\nMinimal\nLoad once, cache in GPU\n\n\nRecSys\nLog streaming, continuous\nIncremental embedding updates\nContinuous random lookups\nHot embeddings in memory\n\n\nVision\nSequential with augmentation\nRegular, 1-10GB\nMinimal\nLoad once per model\n\n\nScientific\nIrregular, domain-specific\nRegular or continuous\nDomain-specific\nVaries by application\n\n\n\n\n\n\n\n\nData Pipeline Throughput Requirements\nThe central quantitative question for ML storage is: what bandwidth does training require? The answer depends on cluster size, accelerator utilization targets, sample sizes, and iteration speed.\n\nDefinition: Data Pipeline Throughput EquationThe required storage bandwidth to sustain distributed training is:\n\\[B_{required} = N_{GPUs} \\times U_{target} \\times \\frac{S_{batch}}{T_{iteration}}\\]\nwhere \\(N_{GPUs}\\) is the number of accelerators, \\(U_{target}\\) is the target utilization (typically 0.8-0.95), \\(S_{batch}\\) is the batch size in bytes per GPU, and \\(T_{iteration}\\) is the iteration time in seconds.\n\n\nThis equation reveals that storage bandwidth requirements grow linearly with cluster size. Doubling the number of GPUs doubles the required storage bandwidth, assuming iteration time and batch size remain constant.\n\nExample: ImageNet Training Bandwidth RequirementsConsider training a ResNet-50 model on ImageNet using 256 H100 GPUs. The calculation proceeds as follows:\nGiven values:\n\n\\(N_{GPUs} = 256\\) H100 GPUs\n\\(U_{target} = 0.80\\) (80% utilization)\nImages are 224×224 RGB after resize, but stored as JPEG at ~150 KB average\nBatch size per GPU: 256 images\nTarget iteration time: 200 ms (achievable with optimized training)\n\nBatch size calculation:\n\\[S_{batch} = 256 \\text{ images} \\times 150 \\text{ KB/image} = 38.4 \\text{ MB}\\]\nRequired bandwidth:\n\\[B_{required} = 256 \\times 0.80 \\times \\frac{38.4 \\text{ MB}}{0.2 \\text{ s}} = 39.3 \\text{ GB/s}\\]\nThis 39.3 GB/s requirement exceeds single-node NVMe capabilities (typically 7-25 GB/s) and requires either distributed data loading across multiple nodes or a parallel file system delivering aggregate bandwidth at this scale.\n\n\nThe bandwidth requirement varies dramatically by model type due to differences in sample size and iteration time.\n\nExample: LLM Training Bandwidth RequirementsFor LLM training, samples are tokenized text sequences rather than images, and batch sizes are constrained by GPU memory for activation storage.\nGPT-3 scale training scenario:\n\n\\(N_{GPUs} = 1024\\) A100 GPUs (128 nodes × 8 GPUs)\n\\(U_{target} = 0.85\\)\nSequence length: 2048 tokens\nBatch size per GPU: 8 sequences (memory limited)\nToken storage: 2 bytes per token (int16)\nTarget iteration time: 1.5 seconds\n\nBatch size calculation:\n\\[S_{batch} = 8 \\text{ sequences} \\times 2048 \\text{ tokens} \\times 2 \\text{ bytes} = 32.8 \\text{ KB}\\]\nRequired bandwidth:\n\\[B_{required} = 1024 \\times 0.85 \\times \\frac{32.8 \\text{ KB}}{1.5 \\text{ s}} = 19.0 \\text{ MB/s}\\]\nThis dramatically lower bandwidth requirement (19 MB/s vs. 39 GB/s for ImageNet) explains why LLM training is typically compute-bound rather than I/O-bound. The bottleneck shifts to checkpoint I/O rather than training data streaming.\n\n\n\nExample: Recommendation System Training BandwidthRecommendation systems present unique challenges with massive embedding tables and sparse feature access.\nLarge-scale RecSys scenario (similar to Meta DLRM):\n\n\\(N_{GPUs} = 512\\) GPUs\n\\(U_{target} = 0.75\\) (lower due to embedding communication)\nSamples: User interaction logs with ~100 features each\nFeature encoding: 8 bytes per feature (int64 IDs)\nBatch size per GPU: 65,536 samples (large batches for sparse models)\nTarget iteration time: 100 ms\n\nBatch size calculation:\n\\[S_{batch} = 65536 \\text{ samples} \\times 100 \\text{ features} \\times 8 \\text{ bytes} = 52.4 \\text{ MB}\\]\nRequired bandwidth:\n\\[B_{required} = 512 \\times 0.75 \\times \\frac{52.4 \\text{ MB}}{0.1 \\text{ s}} = 201.3 \\text{ GB/s}\\]\nThis 201 GB/s requirement exceeds even large parallel file systems. Recommendation systems typically address this through data locality: each worker processes a partition of the data stored on local SSDs, eliminating cross-node data transfer. The embedding table accesses, which are random lookups into trillion-parameter tables, become the true storage bottleneck.\n\n\n\n\nConsistency Models for ML Storage\nDistributed storage systems face fundamental tradeoffs between consistency (all readers see the same data), availability (requests succeed even during failures), and partition tolerance (the system operates despite network failures). The CAP theorem (Brewer 2000; Gilbert and Lynch 2002) formalizes that distributed systems can provide at most two of these three guarantees simultaneously.\n\nBrewer, Eric A. 2000. “Towards Robust Distributed Systems.” In PODC, 7:343477–502. 10.1145.\n\nGilbert, Seth, and Nancy Lynch. 2002. “Brewer’s Conjecture and the Feasibility of Consistent, Available, Partition-Tolerant Web Services.” ACM SIGACT News 33 (2): 51–59.\nUnderstanding consistency models precisely enables correct storage system selection for different ML components.\n\nDefinition: Consistency Model DefinitionsLinearizability (Strong Consistency): Every operation appears to take effect instantaneously at some point between its invocation and response. All clients observe operations in the same order, and that order respects real-time ordering. Checkpoint storage requires linearizability because:\n\nAfter checkpoint write completes, any reader must see complete checkpoint\nNo reader should ever see a partial or corrupted state\nRecovery process must read exactly what was written\n\nLinearizable systems are expensive: they require coordination (consensus protocols like Paxos or Raft) on every write. This is acceptable for checkpoints (written every 10-30 minutes) but unacceptable for training data reads (millions per second).\nSequential Consistency: Operations appear in the same order to all clients, but that order may not respect real-time. Sufficient for training data where “everyone sees the same dataset version” is required but “immediately after upload” is not.\nRead-Your-Writes Consistency: After a client writes data, subsequent reads by that same client will see the new value. Required for online feature stores where user actions update features that affect subsequent recommendations.\nEventual Consistency: After a period of no writes, all replicas converge to the same value. Acceptable for training data that is uploaded once and read many times, or for offline feature computation where batch updates propagate over minutes.\nThe key insight: stronger consistency requires more coordination, reducing throughput and increasing latency. Match consistency level to workload requirements.\n\n\nFor ML systems, the appropriate consistency model depends on the storage tier and access pattern.\nTraining data storage can sacrifice strong consistency for availability. Training samples are immutable once created: the same image or text sequence is read identically by all workers. Eventual consistency suffices because:\n\nTraining order does not affect final model quality (samples are shuffled anyway)\nStale reads of a dataset do not corrupt training (reading an older version of a sample causes no harm)\nDataset updates are infrequent (new data is added between training runs, not during)\n\nObject storage systems like S3 and GCS are suitable for training data. Since December 2020, AWS S3 provides strong read-after-write consistency for all operations (Amazon Web Services 2020), eliminating previous eventual consistency concerns. GCS has always provided strong consistency. Both systems prioritize availability and can serve training data reliably at scale.\n\nAmazon Web Services. 2020. “Amazon S3 Strong Consistency.” https://aws.amazon.com/blogs/aws/amazon-s3-update-strong-read-after-write-consistency/.\nCheckpoint storage requires strong consistency for correctness. A checkpoint must be complete and consistent before training can safely continue: a partially written checkpoint that appears complete causes catastrophic failure on recovery. The consistency requirements are:\n\nAtomic writes: A checkpoint is either fully written or not visible at all\nRead-after-write consistency: Immediately after a checkpoint completes, it must be readable\nDurable writes: Once acknowledged, a checkpoint must survive storage system failures\n\nThese requirements favor strongly consistent storage systems or careful application-level protocols that implement atomic writes atop eventually consistent storage.\nFeature store storage requires consistency guarantees that vary by access pattern. Offline features can tolerate eventual consistency: batch computations produce new feature versions that propagate to serving over minutes to hours. Online features for serving require read-your-writes consistency at minimum: after a user action updates their features, subsequent requests must see those updated features. Strong consistency may be necessary for use cases like fraud detection where stale features could enable fraudulent transactions.\n\n\n\nTable 4: Consistency requirements vary by storage tier and access pattern.\n\n\n\n\n\n\n\n\n\n\n\nStorage Tier\nConsistency Requirement\nRationale\nSuitable Systems\n\n\n\n\nTraining data\nStrong consistency (now standard)\nImmutable data, modern object stores provide strong consistency\nS3, GCS, HDFS\n\n\nCheckpoints\nStrong consistency\nPartial checkpoints cause catastrophic failure\nParallel FS with atomic writes\n\n\nOffline features\nEventual consistency\nBatch updates, staleness acceptable\nData warehouses, object storage\n\n\nOnline features\nRead-your-writes or stronger\nUser experience requires fresh features\nRedis, Bigtable, DynamoDB\n\n\nModel weights\nRead-after-write\nModel updates must be immediately visible\nConsistent object storage\n\n\n\n\n\n\n\n\n\n\n\n\nNoteCAP Theorem Implications for ML Storage\n\n\n\nThe CAP theorem’s implications for ML storage differ from traditional applications. As Stoica et al. observe (Stoica et al. 2017), training storage can sacrifice availability for consistency (a brief storage outage during checkpoint writes is acceptable if it ensures checkpoint correctness), while serving storage might sacrifice consistency for availability (serving stale features is preferable to failing requests entirely). Understanding these tradeoffs enables storage architecture decisions that match the actual requirements of each ML system component.\n\n\n\nStoica, Ion, Dawn Song, Raluca Ada Popa, David Patterson, Michael W Mahoney, Randy Katz, Anthony D Joseph, et al. 2017. “A Berkeley View of Systems Challenges for AI.” arXiv Preprint arXiv:1712.05855.\n\n\nTail Latency in Distributed Storage\nAt scale, tail latency dominates storage system behavior. The median access latency tells an incomplete story; what matters for ML systems is the 99th or 99.9th percentile latency (P99, P999). Dean and Barroso’s seminal paper “The Tail at Scale” (Dean and Barroso 2013) formalized why: when a system makes parallel requests to many storage nodes, the overall latency is determined by the slowest response.\n\nDean, Jeffrey, and Luiz André Barroso. 2013. “The Tail at Scale.” Communications of the ACM 56 (2): 74–80.\nWhy tail latency matters for ML:\nConsider a distributed training step that reads data from 100 storage nodes simultaneously. If each node’s latency distribution has a 1% chance of being “slow” (say, 100ms instead of the typical 10ms), then:\n\\[P(\\text{at least one slow}) = 1 - (0.99)^{100} = 0.634\\]\nMore than 63% of training steps will experience at least one slow storage access, making the tail latency the effective latency for the system.\nTail latency sources:\n\nGarbage collection pauses: JVM-based storage systems (HDFS NameNode) can stall for seconds during GC\nDisk queue depth: When too many concurrent requests hit one disk, queue wait time dominates\nNetwork congestion: Shared network fabric experiences transient congestion\nBackground maintenance: Compaction, replication, and verification compete with foreground requests\nResource contention: Multiple tenants sharing storage create interference\n\nMitigation strategies:\n\n\n\n\n\n\n\n\nStrategy\nMechanism\nTrade-off\n\n\n\n\nHedged requests\nIssue redundant reads, take first response\n2x read amplification\n\n\nBackup requests\nIssue second request if first is slow\nLower amplification, higher complexity\n\n\nSelective replica choice\nRoute to least-loaded replica\nRequires load monitoring\n\n\nRequest cancellation\nCancel in-flight requests when first completes\nReduces wasted work\n\n\nDeadline propagation\nDrop requests exceeding deadline\nPrioritizes freshness\n\n\n\nProduction example:\nGoogle’s storage systems implement hedged requests: after waiting for the P50 latency (e.g., 10ms), issue a backup request to a different replica. Take whichever response arrives first. This reduces P99 latency dramatically at the cost of increased read traffic:\n\\[\\text{P99 with hedging} \\approx P50 + P50 \\times \\epsilon\\]\nwhere \\(\\epsilon\\) accounts for the time to detect slowness and issue the backup.\nFor ML training, tail latency tolerance is higher (seconds of delay are acceptable occasionally) than for inference (where P99 latency directly impacts user experience). Checkpoint writes, being infrequent, can tolerate higher tail latency than feature store lookups.\n\n\nStorage System Selection Framework\nGiven the diversity of ML storage requirements, practitioners need a systematic framework for selecting appropriate storage systems. The decision depends on access pattern, scale, latency requirements, and cost constraints.\nFor training data exceeding local storage:\n\nScale &lt; 10 TB: Local NVMe RAID or network-attached storage\nScale 10 TB - 1 PB: Parallel file system (Lustre, GPFS) or high-performance object storage\nScale &gt; 1 PB: Object storage (S3, GCS) with intelligent caching\n\nFor checkpoint storage:\n\nSingle node: Local NVMe with backup to durable storage\nMulti-node, model &lt; 100 GB: Parallel file system with atomic write support\nMulti-node, model &gt; 100 GB: Distributed checkpoint across multiple storage targets\n\nFor feature stores:\n\nOffline only: Data warehouse (Snowflake, BigQuery) or data lake\nOnline serving &lt; 10 ms: In-memory cache (Redis) backed by persistent storage\nOnline serving with scale: Purpose-built feature store (Feast, Tecton, Vertex Feature Store)\n\nThe storage fundamentals established in this section provide the foundation for examining specific storage system architectures in subsequent sections. Training data infrastructure (Section 1.2) examines distributed file systems and object storage at scale. Checkpoint storage systems (Section 1.3) develop quantitative models for checkpoint frequency and distribution. Feature stores (Section 1.4) address the unique requirements of recommendation and real-time ML systems.",
    "crumbs": [
      "Volume II: Advanced",
      "Foundations of Scale",
      "Storage Systems for ML"
    ]
  },
  {
    "objectID": "contents/vol2/storage/storage.html#sec-training-data-infrastructure",
    "href": "contents/vol2/storage/storage.html#sec-training-data-infrastructure",
    "title": "Storage Systems for ML",
    "section": "Training Data Infrastructure",
    "text": "Training Data Infrastructure\nTraining datasets for production ML systems range from terabytes to petabytes, far exceeding single-machine storage capacity. Storing and serving this data requires distributed storage systems designed for high throughput sequential access. This section examines the distributed file systems and object storage architectures that power large-scale ML training, along with the data formats and pipeline architectures that efficiently deliver data to accelerators.\n\nDistributed File Systems\nDistributed file systems provide a POSIX-compatible interface to storage spread across hundreds or thousands of machines. This familiar file system interface (open, read, write, close) enables existing software to access distributed storage without modification, a significant advantage for ML frameworks designed around local file access patterns.\n\nGoogle File System and Colossus\nThe Google File System (GFS) (Ghemawat, Gobioff, and Leung 2003), published in 2003, established the architectural template for large-scale distributed storage. Its design decisions, optimized for Google’s web crawling and indexing workloads, proved remarkably well suited for ML training data.\n\nGhemawat, Sanjay, Howard Gobioff, and Shun-Tak Leung. 2003. “The Google File System.” In Proceedings of the 19th ACM Symposium on Operating Systems Principles, 29–43. ACM.\nGFS makes several unconventional design choices:\nLarge block sizes. Where traditional file systems use 4 KB blocks, GFS uses 64 MB chunks. This reduces metadata overhead: a 1 PB dataset contains only 16 million chunks rather than 256 billion 4 KB blocks. For ML training data, which is read sequentially in large batches, large blocks eliminate seek overhead and maximize throughput.\nSingle master, multiple chunkservers. A single master maintains all file system metadata (namespace, chunk locations, access permissions) while data flows directly between clients and chunkservers. This architecture simplifies consistency guarantees: the master is the single source of truth for metadata. For training data, which is written once and read many times, this works well.\nRelaxed consistency model. GFS provides weak consistency guarantees: concurrent writers may produce undefined file regions, and readers may see stale data during failures. These limitations are acceptable for training data (immutable after creation) but problematic for checkpoints (requiring atomicity).\nColossus, Google’s successor to GFS deployed around 2010, addresses GFS limitations while maintaining its core design. Key improvements include:\n\nDistributed metadata: The single master bottleneck is eliminated by sharding metadata across multiple servers\nSmaller block sizes: 1 MB blocks improve space efficiency for smaller files\nErasure coding: Reduces storage overhead from 3x replication to ~1.5x while maintaining durability\nReed-Solomon encoding: Enables efficient reconstruction of failed blocks\n\nFor ML workloads, Colossus delivers aggregate read bandwidth exceeding 1 TB/s across Google’s infrastructure, sufficient to feed thousands of TPU chips simultaneously.\n\n\nHadoop Distributed File System\nHDFS (Shvachko et al. 2010), the open-source implementation inspired by GFS, became the storage foundation for the Hadoop ecosystem and remains widely deployed for ML training data. Its architecture mirrors GFS:\n\nShvachko, Konstantin, Hairong Kuang, Sanjay Radia, and Robert Chansler. 2010. “The Hadoop Distributed File System.” In 2010 IEEE 26th Symposium on Mass Storage Systems and Technologies (MSST), 1–10. IEEE.\n\nNameNode: Single master maintaining file system namespace and block locations\nDataNodes: Workers storing actual data blocks\nBlock replication: Default 3x replication for durability\n\nHDFS optimizes for the same access patterns as GFS: large sequential reads and writes, with files written once and read many times. A typical HDFS deployment achieves 100-200 MB/s per DataNode, scaling to aggregate cluster throughput of 10-100 GB/s depending on cluster size.\nHDFS limitations for modern ML include:\n\nSingle NameNode bottleneck: File system operations serialize through one server, limiting metadata operations to ~10,000/second\nJVM overhead: Java implementation adds latency and memory overhead compared to native implementations\nSmall file problem: Each file consumes NameNode memory regardless of size; millions of small files exhaust metadata capacity\n\nThese limitations become acute for ML workloads with many small files (e.g., individual images) or high-frequency metadata operations (e.g., checkpoint writes). HDFS works well for datasets stored as large sequential files (TFRecord, Parquet) but struggles with directory structures containing millions of individual samples.\n\n\nLustre and Parallel File Systems\nLustre, developed for high-performance computing (HPC), takes a different architectural approach optimized for parallel access from thousands of compute nodes simultaneously. Where GFS and HDFS prioritize simplicity and fault tolerance, Lustre prioritizes raw throughput.\nLustre architecture separates metadata from data more aggressively:\n\nMetadata Servers (MDS): Handle file system namespace operations (create, open, stat)\nObject Storage Servers (OSS): Store actual file data\nObject Storage Targets (OST): Individual storage devices attached to OSSs\n\nFiles are striped across multiple OSTs, enabling parallel access that aggregates bandwidth from many storage devices. A large file might be striped across 100 OSTs; reading the file in parallel achieves 100x the bandwidth of a single OST.\nLustre performance characteristics:\n\n\n\nTable 5: Lustre scales linearly with storage server count.\n\n\n\n\n\n\n\n\n\n\nConfiguration\nAggregate Bandwidth\nTypical Use\n\n\n\n\nSmall cluster (10 OSTs)\n10-20 GB/s\nResearch lab ML training\n\n\nMedium cluster (100 OSTs)\n100-200 GB/s\nProduction ML training\n\n\nLarge cluster (1000+ OSTs)\n1+ TB/s\nExascale HPC, large LLM training\n\n\n\n\n\n\nFor ML training, Lustre excels when:\n\nTraining data is stored as large files that can be striped across many OSTs\nMultiple training jobs read the same data concurrently (shared datasets)\nCheckpoint writes require high bandwidth to minimize training interruption\n\nLustre’s disadvantage is operational complexity: tuning stripe sizes, managing quota, and handling metadata server failures requires specialized expertise that cloud-native teams may lack.\n\n\nComparative Analysis\nThe choice between distributed file systems depends on workload characteristics, operational expertise, and existing infrastructure.\n\n\n\nTable 6: Distributed file system comparison for ML workloads.\n\n\n\n\n\n\n\n\n\n\n\nSystem\nStrengths\nWeaknesses\nBest For\n\n\n\n\nGFS/Colossus\nMassive scale, Google ecosystem integration\nProprietary to Google\nGoogle Cloud ML\n\n\nHDFS\nOpen source, Hadoop ecosystem\nSingle NameNode, JVM overhead\nSpark-based data processing\n\n\nLustre\nRaw throughput, HPC optimized\nOperational complexity\nOn-premise HPC clusters\n\n\nGPFS/Spectrum Scale\nEnterprise features, mixed workloads\nCost, complexity\nLarge enterprise ML\n\n\nBeeGFS\nEase of deployment, good performance\nSmaller community\nAcademic/research clusters\n\n\n\n\n\n\n\n\n\nObject Storage at Scale\nObject storage provides a simpler abstraction than file systems: objects are stored and retrieved by key, without directories, hierarchies, or POSIX semantics. This simplicity enables massive scale, high durability, and low cost.\n\nObject Storage Architecture\nObject stores organize data as flat namespaces of key-value pairs. An object has:\n\nKey: A unique identifier (often resembling a file path: training-data/imagenet/images/n01440764/n01440764_10026.JPEG)\nValue: The object data (arbitrary bytes, typically KB to GB in size)\nMetadata: User-defined key-value pairs (content type, creation time, checksums)\n\nThis design eliminates the metadata bottlenecks that limit distributed file systems. Without directories to traverse or hierarchies to maintain, object stores scale to billions of objects without architectural changes.\nDurability through redundancy. Object stores achieve extreme durability (S3 advertises 99.999999999% or “eleven nines”) through:\n\nErasure coding: Data is split into fragments with redundant parity fragments, enabling reconstruction if any fragment is lost\nGeographic distribution: Fragments are spread across multiple availability zones or regions\nContinuous verification: Background processes detect and repair bit rot\n\nConsistency model evolution. Historically, object stores provided eventual consistency: after writing an object, some readers might not immediately see the new version. This changed significantly in December 2020 when AWS S3 upgraded to strong read-after-write consistency for all operations at no additional cost. GCS has always provided strong consistency. This evolution simplifies ML storage architecture: checkpoints can now rely on object stores providing immediate visibility after successful writes, though application-level verification remains prudent for critical data.\n\n\nAmazon S3 and Google Cloud Storage\nS3, launched in 2006, pioneered the object storage model and remains the dominant cloud storage service. GCS provides similar capabilities with Google Cloud integration.\nS3 performance characteristics:\n\nSingle object throughput: 100 MB/s per connection\nAggregate throughput: Scales with parallel connections (100s of GB/s possible)\nLatency: 50-100 ms for first byte\nRequest rate: 5,500 GET/s and 3,500 PUT/s per prefix\n\nThe request rate limit is significant for ML: a dataset organized as s3://bucket/class_name/image.jpg limits each class to 5,500 reads per second. Sharding data across random prefixes (s3://bucket/shard_id/sample.dat) avoids this bottleneck.\nGCS performance is similar, with some differences:\n\nSingle object throughput: 100+ MB/s per connection\nStrong consistency: Both GCS and S3 (since December 2020) provide read-after-write consistency\nComposite objects: Multiple objects can be composed into one without re-upload\n\nPractical throughput depends heavily on access patterns:\n\n\n\nTable 7: Object storage throughput varies dramatically by access pattern.\n\n\n\n\n\n\n\n\n\n\nAccess Pattern\nTypical Throughput\nOptimization\n\n\n\n\nSequential single object\n100 MB/s\nUse large objects (100+ MB)\n\n\nParallel multiple objects\n10-100 GB/s\nUse multiple connections, random prefixes\n\n\nSmall object reads\nLimited by request rate\nBatch into larger objects\n\n\nListing operations\n1000 objects/s\nUse flat namespaces, avoid deep hierarchies\n\n\n\n\n\n\n\n\nObject Storage for ML Training\nUsing object storage for training data requires adapting to its characteristics:\nLarge file aggregation. Rather than storing individual images as objects, aggregate samples into large files (TFRecord, WebDataset, Parquet). This converts thousands of small object reads into a few large sequential reads, dramatically improving throughput.\nParallel data loading. Training frameworks should open multiple parallel connections to object storage. PyTorch DataLoader with num_workers &gt; 1 or TensorFlow’s tf.data with num_parallel_calls enables this parallelism.\nPrefetching and buffering. Object storage latency (50-100 ms) would be catastrophic if each batch waited for storage. Data pipelines must prefetch many batches ahead, overlapping storage access with GPU computation.\nLocal caching. For datasets accessed repeatedly across training runs, caching on local NVMe reduces object storage costs and improves performance. The first training run populates the cache; subsequent runs read locally.\n\n\n\nData Format Selection\nThe choice of data format significantly impacts training throughput, storage cost, and pipeline complexity. Modern ML workloads have converged on a few formats optimized for different access patterns.\n\nTFRecord\nTFRecord, TensorFlow’s native format, stores data as sequential records in binary files. Each record contains a serialized protocol buffer with typed fields.\nStructure:\n[length][crc32 of length][data][crc32 of data]\nAdvantages:\n\nSequential read optimization: Records are stored contiguously, enabling high-throughput streaming\nSchema flexibility: Protocol buffers support arbitrary nested structures\nCompression: GZIP or ZSTD compression can be applied transparently\nSplitting: Large datasets can be sharded across multiple TFRecord files\n\nLimitations:\n\nRandom access: Accessing a specific record requires scanning from the beginning\nTensorFlow coupling: While readable from other frameworks, optimized for TensorFlow\nNo indexing: Cannot query or filter without reading entire file\n\nFor large-scale training that processes entire datasets sequentially, TFRecord achieves near-optimal throughput. A well-configured TFRecord pipeline can saturate 10+ GB/s NVMe storage.\n\n\nApache Parquet\nParquet, developed for the Hadoop ecosystem, uses columnar storage that stores all values of a column together rather than all columns of a row together.\nColumnar advantages for ML:\n\nColumn pruning: Read only the columns needed (e.g., skip metadata, read only pixels)\nCompression efficiency: Similar values in a column compress better than mixed values in a row\nPredicate pushdown: Filter data without reading irrelevant rows\n\nParquet structure:\nRow Group 1\n  Column A chunk (values for rows 0-N)\n  Column B chunk\n  ...\nRow Group 2\n  ...\nFooter (schema, statistics, locations)\nThe footer contains statistics (min/max values) for each column chunk, enabling queries to skip row groups that cannot match filter predicates.\nParquet for ML works well when:\n\nDatasets have many columns but training uses few (feature selection)\nFiltering is needed (e.g., train only on samples meeting criteria)\nData is shared with analytics tools (Spark, pandas, DuckDB)\n\nLimitations:\n\nWrite amplification: Updating a single value requires rewriting the row group\nNot optimized for image/binary data: Columnar layout provides little benefit\n\n\n\nWebDataset\nWebDataset stores samples as TAR archives, with each sample’s components (image, label, metadata) as separate files within the archive.\nStructure:\nsample0001.jpg\nsample0001.cls\nsample0001.json\nsample0002.jpg\nsample0002.cls\n...\nWebDataset advantages:\n\nHTTP compatible: TAR files can be streamed directly from web servers or object storage\nSimple format: Standard UNIX tools can inspect and manipulate archives\nShuffling: Shuffle buffers can be applied during streaming\nNo framework dependency: Works with PyTorch, TensorFlow, JAX\n\nFor distributed training, WebDataset enables efficient data loading from object storage:\n\nEach worker is assigned different TAR shards\nWorkers stream shards in parallel, no coordination needed\nLocal shuffle buffers randomize sample order within each worker\n\nThroughput: WebDataset achieves 1-5 GB/s per worker from object storage, scaling linearly with worker count.\n\n\nFormat Selection Guidelines\n\n\n\nTable 8: Format selection depends on data characteristics and access patterns.\n\n\n\n\n\n\n\n\n\n\nModel Type\nRecommended Format\nRationale\n\n\n\n\nLLM\nTFRecord or custom binary\nToken sequences are fixed-length arrays, columnar offers no benefit\n\n\nVision\nWebDataset or TFRecord\nLarge binary blobs (images), sequential access pattern\n\n\nRecSys\nParquet\nMany sparse features, column pruning valuable\n\n\nScientific\nHDF5 or domain-specific\nMulti-dimensional arrays, random access sometimes needed\n\n\nMultimodal\nWebDataset\nDifferent modalities (image, text, audio) naturally grouped\n\n\n\n\n\n\n\n\n\nData Loading Pipelines\nThe data loading pipeline connects storage to accelerators, transforming raw data into training batches. Pipeline design determines whether storage bandwidth is fully utilized and whether GPUs remain fed during training.\n\nPipeline Stages\nA typical data loading pipeline includes:\n\nData reading: Fetch bytes from storage (disk, network, object store)\nDecompression: Decompress compressed formats (GZIP, ZSTD, JPEG)\nDeserialization: Parse structured data (protobuf, JSON)\nTransformation: Apply augmentations (resize, crop, normalize)\nBatching: Collate samples into batches\nTransfer: Move batches to accelerator memory\n\nEach stage has different compute and bandwidth characteristics:\n\n\n\nTable 9: Pipeline stages have different performance characteristics.\n\n\n\n\n\nStage\nBound By\nParallelizable\nTypical Duration\n\n\n\n\nReading\nStorage bandwidth\nYes (sharded data)\n1-100 ms\n\n\nDecompression\nCPU compute\nYes (per sample)\n0.1-10 ms\n\n\nDeserialization\nCPU compute\nYes (per sample)\n0.01-1 ms\n\n\nTransformation\nCPU compute\nYes (per sample)\n0.1-50 ms\n\n\nBatching\nMemory bandwidth\nLimited\n0.1-1 ms\n\n\nTransfer\nPCIe bandwidth\nLimited (few DMA channels)\n0.1-10 ms\n\n\n\n\n\n\n\n\nPrefetching and Pipelining\nThe key to hiding latency is pipelining: while the GPU processes batch \\(N\\), the CPU prepares batch \\(N+1\\), and storage fetches data for batch \\(N+2\\). This requires maintaining multiple batches in flight simultaneously.\nThe prefetch buffer size determines how much latency can be hidden:\n\\[T_{hidden} = N_{prefetch} \\times T_{batch}\\]\nwhere \\(N_{prefetch}\\) is the number of batches buffered and \\(T_{batch}\\) is the GPU batch processing time.\nFor a 200 ms batch time with 100 ms storage latency, prefetching just 1 batch hides the storage latency. However, variance in storage latency requires larger buffers: if storage latency varies from 50-500 ms, prefetching 3-5 batches ensures GPUs never wait.\n\n\nCaching Strategies\nCaching can dramatically improve data loading performance when datasets are accessed repeatedly.\nLocal disk caching: First access fetches from remote storage and writes to local NVMe. Subsequent accesses read from local disk at 7-25 GB/s rather than network speeds. Effective when:\n\nDataset fits on local storage\nMultiple epochs are trained\nNetwork bandwidth is limited\n\nMemory caching: Entire dataset or frequently accessed portions are loaded into DRAM. Achieves 100+ GB/s bandwidth but limited by DRAM capacity (typically 512 GB - 2 TB per node).\nDistributed caching: Services like Alluxio provide a distributed cache layer between compute and storage. Multiple nodes contribute memory to a shared cache, enabling datasets larger than single-node memory to be cached.\nCache invalidation for ML is straightforward: training datasets are immutable. The cache can use simple LRU eviction without concern for consistency.\n\n\nShuffling in Distributed Training\nShuffling is essential for training: without shuffling, the model sees samples in the same order every epoch, potentially learning spurious ordering correlations. Distributed shuffling is challenging because:\n\nEach worker must see different samples\nSamples should be randomly ordered within and across workers\nCommunication for global shuffle is expensive\n\nCommon approaches:\nFile-level shuffle: Assign each worker a random subset of data files. Workers read their files sequentially. Low communication cost but limited randomness: samples within a file are always adjacent.\nShuffle buffer: Each worker maintains a buffer of \\(B\\) samples. New samples are randomly exchanged with buffer contents. Provides good local randomness without communication.\nReservoir sampling: When the dataset is too large to shuffle in memory, reservoir sampling maintains a random subset. Each new sample has probability \\(k/n\\) of entering the reservoir, where \\(k\\) is reservoir size and \\(n\\) is samples seen.\nEpoch boundary shuffle: At epoch boundaries, workers exchange file assignments. This adds randomness across epochs without per-batch communication.\n\nExample: Distributed Data Loading for 1024-GPU TrainingConsider loading ImageNet data for 1024-GPU training with optimal throughput.\nConfiguration:\n\n1024 GPUs across 128 nodes (8 GPUs/node)\nTarget: 80% GPU utilization\nBatch size per GPU: 256 images (150 KB average)\nTarget iteration time: 200 ms\nRequired bandwidth: 157 GB/s (from earlier calculation)\n\nSolution architecture:\n\nData storage: 1.4 TB ImageNet stored as 1000 WebDataset TAR shards in object storage\nPer-node allocation: Each node is assigned ~8 shards (non-overlapping)\nLocal caching: First epoch caches shards to local NVMe (10 TB per node available)\nData loaders: 16 worker processes per node (2 per GPU)\nPrefetch buffer: 4 batches per GPU\n\nBandwidth calculation:\n\nPer-node requirement: 157 GB/s / 128 nodes = 1.23 GB/s\nLocal NVMe provides: 7+ GB/s (ample margin)\nFirst epoch from object storage: Each node needs 11 GB, ~90 seconds to cache\n\nAfter the first epoch, training is entirely from local cache, eliminating network bottlenecks.\n\n\n\n\n\nData Locality Principles\nThe cost of moving data between nodes dominates computation time for many ML workloads. Data locality places computation where data already resides, a fundamental principle underlying both Spark (Zaharia et al. 2016) and Ray’s design.\n\nZaharia, Matei, Reynold S Xin, Patrick Wendell, Tathagata Das, Michael Armbrust, Ankur Dave, Xiangrui Meng, et al. 2016. “Apache Spark: A Unified Engine for Big Data Processing.” Communications of the ACM 59 (11): 56–65.\nLocality Hierarchy:\n\n\n\nLocality Level\nDescription\nTypical Latency\nBandwidth\n\n\n\n\nGPU_LOCAL\nData in GPU HBM\n~10 ns\n3+ TB/s\n\n\nCPU_LOCAL\nData in same-node CPU memory\n~100 ns\n200 GB/s\n\n\nRACK_LOCAL\nData on another node in same rack\n~100 μs\n25 GB/s\n\n\nANY\nData on any node in cluster\n~1 ms\n10 GB/s\n\n\n\nThe 100x bandwidth difference between CPU_LOCAL (200 GB/s) and ANY (10 GB/s via network) makes locality-aware scheduling essential for bandwidth-intensive ML workloads.\nScheduling for Locality:\nRay’s scheduler exemplifies locality-aware scheduling:\n\nWhen a task requires a data object, the scheduler identifies nodes holding that object\nTasks are preferentially scheduled to nodes with data (if capacity available)\nIf no capacity, the task is scheduled elsewhere and data is transferred\nScheduling decisions consider: data size, transfer time, node load, task urgency\n\nFor training data loading:\n\nEach worker is assigned shards stored on its local NVMe\nWorkers read local shards (7+ GB/s) rather than remote (1 GB/s network)\nShuffle operations respect locality: map outputs written locally, reducers fetch as needed\n\nWhen Locality Fails:\nLocality optimization breaks down when:\n\nHot data: Popular embeddings accessed by many workers cannot all be local\nRandom access: Feature store lookups are inherently non-local\nSmall data: Scheduling overhead exceeds data transfer time for small objects\n\nFor these cases, replication and caching substitute for locality.\nQuantitative Example:\nImageNet training with 1000 WebDataset shards across 100 nodes:\n\nLocal access: Each node stores 10 shards, reads at 10 GB/s = 100 MB/shard for 10 shards = 1 GB per node delivered in 0.1 seconds\nRemote access: If shards were centralized, 100 nodes competing for 10 GB/s network = 0.1 GB/s per node\n\nLocality provides 100x throughput improvement for this workload.\n\n\nModel-Type Diversity in Training Data\nTraining data requirements vary dramatically across model types, affecting storage architecture, data format selection, and pipeline design.\n\n\n\nTable 10: Training data characteristics vary significantly by model type.\n\n\n\n\n\n\n\n\n\n\n\nModel Type\nData Format\nTypical Volume\nKey Storage Challenge\n\n\n\n\nLLM\nTokenized text\n10-100 TB\nDeduplication at scale, quality filtering\n\n\nVision\nImages/Video\n100 TB - 10 PB\nAugmentation pipeline throughput\n\n\nRecSys\nUser interaction logs\n1+ PB\nPrivacy compliance, real-time freshness\n\n\nScientific\nSimulations, sensor data\n10+ PB\nIrregular structure, domain-specific formats\n\n\nSpeech/Audio\nWaveforms, spectrograms\n10-100 TB\nVariable length sequences\n\n\n\n\n\n\nLLM training data presents unique challenges at the preprocessing stage rather than during training. Raw web crawls contain duplicate content, low-quality text, and potentially harmful material. Deduplication alone can reduce dataset size by 30-50%. The storage challenge is running these preprocessing pipelines at scale: processing Common Crawl (petabytes of raw HTML) requires distributed processing frameworks like Spark or Dataflow, with intermediate results stored in distributed file systems.\nVision training data is bandwidth-intensive during training due to real-time augmentation. Each image must be decoded, randomly augmented (crop, flip, color jitter), and normalized before being batched. The augmentation pipeline runs on CPU and can become the bottleneck for large vision models. Storing pre-augmented images is impractical (each image would need hundreds of augmented variants), so augmentation must happen during training.\nRecommendation system data involves continuous streams of user interactions rather than static datasets. Each user action (click, purchase, view) becomes a training sample. The storage challenge is maintaining freshness: recommendation models trained on yesterday’s data may miss today’s trends. This requires streaming data architectures (Kafka, Pub/Sub) feeding into training pipelines, with careful attention to data retention policies for privacy compliance.\nScientific ML data often involves domain-specific formats (HDF5, NetCDF, FITS) with multi-dimensional arrays representing simulation outputs or sensor measurements. These formats support chunked access for random access patterns that sequential formats like TFRecord do not handle well. Storage systems must support both high-throughput sequential access for training and random access for data exploration.",
    "crumbs": [
      "Volume II: Advanced",
      "Foundations of Scale",
      "Storage Systems for ML"
    ]
  },
  {
    "objectID": "contents/vol2/storage/storage.html#sec-checkpoint-storage",
    "href": "contents/vol2/storage/storage.html#sec-checkpoint-storage",
    "title": "Storage Systems for ML",
    "section": "Checkpoint Storage Systems",
    "text": "Checkpoint Storage Systems\nTraining large models requires days to months of continuous computation. During this time, hardware failures, software bugs, and preemption events can terminate training at any point. Without checkpoints, a failure after two weeks of training would require restarting from the beginning, wasting millions of dollars in compute. Checkpointing saves model state periodically, enabling recovery from the most recent checkpoint rather than from scratch. The engineering challenge is minimizing checkpoint overhead while ensuring reliable recovery.\n\nCheckpoint Architecture Fundamentals\nA checkpoint captures the complete state needed to resume training exactly where it left off. This includes:\nModel parameters. The current values of all trainable weights. For a 175 billion parameter model in FP16, this is 350 GB of dense data.\nOptimizer state. Optimizers like Adam maintain per-parameter statistics (first and second moments). Adam’s state is 2x the size of model parameters, adding 700 GB for the 175B parameter case.\nLearning rate scheduler state. The current position in the learning rate schedule (step count, warmup progress).\nRandom number generator state. To ensure reproducibility, the RNG state for each worker must be saved.\nData loader state. Which samples have been seen in the current epoch, enabling resumption without repeating or skipping samples.\nThe total checkpoint size for a large model can reach several terabytes:\n\\[S_{checkpoint} = S_{params} + S_{optimizer} + S_{auxiliary}\\]\nFor a 175B parameter model with Adam optimizer:\n\\[S_{checkpoint} = 350 \\text{ GB} + 700 \\text{ GB} + \\text{~1 GB} \\approx 1.05 \\text{ TB}\\]\n\n\nSynchronous vs Asynchronous Checkpointing\nCheckpointing can be performed synchronously (training stops during checkpoint) or asynchronously (checkpoint happens in background while training continues).\nSynchronous checkpointing is simpler but introduces overhead:\n\nTraining pauses\nAll workers save their state to storage\nCoordinator confirms checkpoint complete\nTraining resumes\n\nThe overhead depends on checkpoint size and storage bandwidth:\n\\[T_{checkpoint} = \\frac{S_{checkpoint}}{B_{storage}}\\]\nFor a 1 TB checkpoint writing to a parallel file system at 50 GB/s aggregate bandwidth, checkpoint time is 20 seconds. If checkpoints occur every 10 minutes, the overhead is:\n\\[\\text{Overhead} = \\frac{T_{checkpoint}}{T_{interval}} = \\frac{20 \\text{ s}}{600 \\text{ s}} = 3.3\\%\\]\nAsynchronous checkpointing overlaps checkpoint I/O with training computation:\n\nTraining continues normally\nBackground thread copies model state to CPU memory\nBackground thread writes to storage while training proceeds\nNext checkpoint begins only after previous completes\n\nAsynchronous checkpointing hides I/O latency but introduces complexity:\n\nMemory overhead: Must maintain a copy of model state for checkpointing while training modifies the live copy\nConsistency: Must snapshot state at a consistent point (between optimizer steps)\nBackpressure: If checkpoint I/O is slower than training, memory fills with pending checkpoints\n\nFor most training runs, asynchronous checkpointing reduces overhead to near zero. The memory overhead (one additional copy of model state) is typically acceptable on systems with sufficient host memory.\n\n\nDistributed Checkpointing for Sharded Models\nWhen models are sharded across multiple devices using tensor parallelism or pipeline parallelism, each device holds only a portion of the model. Checkpointing must coordinate across all devices to produce a consistent, complete checkpoint.\nTensor parallel checkpointing is relatively straightforward: each tensor parallel rank saves its shard. Recovery loads shards to the same ranks. The challenge is ensuring all ranks checkpoint at the same logical point in training.\nPipeline parallel checkpointing is more complex because different pipeline stages may be processing different microbatches simultaneously. The checkpoint must capture a consistent cut across the pipeline: all in-flight activations and gradients must be either included or excluded.\nData parallel checkpointing can save just one replica’s state (since all replicas are identical) or save all replicas for verification. Saving one replica reduces storage by the data parallelism degree.\nHybrid parallel checkpointing combines these considerations. Consider a 3D parallel configuration with 8x data parallel, 4x tensor parallel, and 4x pipeline parallel across 128 GPUs. The checkpoint strategy might:\n\nBarrier synchronization across all 128 GPUs\nEach pipeline stage saves its portion of model and optimizer state\nOnly rank 0 of each data parallel group saves (others are identical)\nTotal checkpoint: 1/8 of full model size, distributed across 16 storage targets\n\nCheckpoint aggregation reduces storage overhead by having ranks aggregate their state before writing. Rather than 128 small writes, a few ranks collect and write large aggregated files. This improves storage efficiency (fewer small files) at the cost of additional memory and communication.\n\n\nCheckpoint Failure Handling\nCheckpoint storage must handle failures gracefully. Understanding failure modes and their mitigations is essential for reliable training at scale.\nPartial Write Failure:\nStorage fails after some but not all checkpoint shards are written. Without protection, recovery might load inconsistent state: some shards from the new checkpoint, some from the old.\nSolution: Atomic commit protocol. All shards are first written to temporary locations with unique names (e.g., checkpoint_step_10000.tmp.worker_0). After all workers confirm write completion, a single atomic operation (rename or commit record) marks the checkpoint complete. Recovery reads only committed checkpoints, identified by the presence of the commit marker.\n1. Workers write: checkpoint_step_10000.tmp.worker_{0..N}\n2. Coordinator verifies all workers complete\n3. Coordinator writes: checkpoint_step_10000.COMPLETE\n4. On recovery: only load checkpoints with .COMPLETE marker\nStorage Node Failure During Training:\nA storage server fails while training continues, potentially corrupting a checkpoint in progress.\nSolution: Replication. With 3x replication, losing one storage node does not lose data. The storage system automatically redirects writes to surviving replicas. Checkpoint writes should require quorum acknowledgment (2 of 3 replicas) before confirming success.\nCheckpoint Corruption Detection:\nSilent data corruption (bit flips, media errors) can render checkpoints unusable without obvious failure signals.\nSolution: End-to-end checksums. Each checkpoint shard includes a cryptographic hash of its contents. On recovery:\n\nLoad checkpoint shard\nCompute checksum\nVerify against stored checksum\nIf mismatch: fall back to previous checkpoint, alert operators\n\nExample failure scenario and recovery cost:\n\nTraining at step 10,000, checkpoint initiated\nWorker 0 writes shard successfully\nNetwork partition: Workers 1-3 cannot reach storage\nCoordinator timeout (30 seconds): checkpoint marked failed\nTraining continues from last good checkpoint (step 9,000)\nWork from steps 9,000-10,000 must be repeated\n\nCost calculation:\n\\[\\text{Wasted compute} = 1000 \\text{ steps} \\times 1.5 \\text{ s/step} \\times 1024 \\text{ GPUs} \\times \\$0.001\\text{/GPU-s} = \\$1,536\\]\nThis quantifies why checkpoint reliability matters: a $1,536 loss from one failed checkpoint motivates engineering investment in checkpoint robustness.\nStraggler Mitigation:\nWith 1000+ workers, one slow worker can delay checkpoint completion by minutes, blocking all training.\nStrategies:\n\nTimeout with fallback: If worker does not reach barrier in T seconds, assume failed, abort checkpoint, continue with previous\nAsync checkpointing: Workers checkpoint when ready; coordinator tracks which steps have full coverage\nBackup workers: Redundant workers ensure N-k completion suffices (similar to gradient synchronization strategies)\n\n\n\nOptimal Checkpoint Interval: Young-Daly Formula\nCheckpointing too frequently wastes time on I/O overhead. Checkpointing too infrequently risks losing large amounts of work to failures. The optimal interval balances these concerns.\nThe Young-Daly formula (Young 1974; Daly 2006) provides the optimal checkpoint interval:\n\nYoung, John W. 1974. “A First Order Approximation to the Optimum Checkpoint Interval.” Communications of the ACM 17 (9): 530–31.\n\nDaly, John T. 2006. “A Higher Order Estimate of the Optimum Checkpoint Interval for Restart Dumps.” In Future Generation Computer Systems, 22:303–12. 3. Elsevier.\n\nDefinition: Young-Daly Checkpoint Interval\\[T_{opt} = \\sqrt{2 \\times T_{save} \\times MTBF}\\]\nwhere \\(T_{save}\\) is the time to save a checkpoint and \\(MTBF\\) is the mean time between failures.\n\n\nThis formula minimizes expected wasted work, accounting for both checkpoint overhead and work lost to failures.\n\nExample: Checkpoint Interval for Large-Scale TrainingConsider training GPT-3 scale model on 1024 GPUs:\nGiven: - Checkpoint size: 1 TB - Storage bandwidth: 100 GB/s aggregate - Checkpoint time: \\(T_{save} = 10\\) seconds - GPU MTBF: 30,000 hours per GPU - Cluster MTBF: \\(30000 / 1024 \\approx 29\\) hours\nOptimal interval calculation:\n\\[T_{opt} = \\sqrt{2 \\times 10 \\text{ s} \\times 29 \\times 3600 \\text{ s}} = \\sqrt{2,088,000} \\approx 1445 \\text{ s} \\approx 24 \\text{ min}\\]\nInterpretation: Checkpoint every 24 minutes. This balances the 10-second checkpoint overhead against the 29-hour MTBF.\nEfficiency calculation:\nExpected work lost per failure: \\(T_{opt}/2 = 12\\) minutes\nCheckpoint overhead: \\(T_{save}/T_{opt} = 10/1445 = 0.7\\%\\)\nTotal overhead: Checkpoint overhead + (expected loss rate × loss per failure)\nWith one failure per 29 hours and 12 minutes lost per failure:\n\\[\\text{Loss rate} = \\frac{12 \\text{ min}}{29 \\times 60 \\text{ min}} = 0.7\\%\\]\nTotal overhead: 0.7% (checkpointing) + 0.7% (failures) = 1.4%\n\n\n\n\nIncremental and Delta Checkpointing\nFull checkpoints save the complete model state regardless of how much has changed. For models where only a portion of parameters change significantly between checkpoints, incremental approaches reduce storage and I/O costs.\nIncremental checkpointing saves only parameters that have changed since the last checkpoint. This requires:\n\nTracking which parameters have been modified\nMaintaining a base checkpoint plus deltas\nPeriodically consolidating into a new full checkpoint\n\nFor dense models where all parameters update every step, incremental checkpointing provides little benefit. For sparse models or fine-tuning scenarios where many parameters are frozen, the savings can be substantial.\nDelta compression compresses the difference between consecutive checkpoints rather than the absolute values. If parameters change by small amounts each step, the delta is highly compressible. Techniques include:\n\nXOR encoding: Store the XOR of current and previous parameter values\nFloating-point prediction: Predict next value from previous, store residual\nLossy compression: Accept small errors in checkpoint for large compression ratios\n\nPractical compression ratios depend on model type and training dynamics:\n\n\n\nTable 11: Delta checkpointing effectiveness varies by model type and training phase.\n\n\n\n\n\nModel Type\nFull Checkpoint\nDelta Size\nCompression Ratio\n\n\n\n\nLLM (dense updates)\n1 TB\n50-100 GB\n10-20x\n\n\nRecSys (sparse embedding updates)\n10 TB\n100-500 GB\n20-100x\n\n\nVision (fine-tuning)\n10 GB\n100 MB\n100x\n\n\n\n\n\n\n\n\nCheckpoint Storage Architecture\nThe storage system for checkpoints must satisfy several requirements:\n\nHigh bandwidth: Minimize checkpoint time\nStrong consistency: Partial checkpoints must not appear complete\nDurability: Checkpoints must survive storage failures\nLow latency for reads: Fast recovery after failures\n\nParallel file systems (Lustre, GPFS) provide the highest bandwidth through striping and parallel I/O. A large Lustre deployment can deliver 100+ GB/s aggregate bandwidth, enabling TB-scale checkpoints in seconds. The tradeoff is operational complexity and cost.\nObject storage (S3, GCS) provides durability and low cost but higher latency and lower bandwidth than parallel file systems. Object storage works well for infrequent checkpoints of moderate size (&lt; 100 GB) but becomes bottleneck for TB-scale checkpoints every few minutes.\nTiered storage combines the benefits of both:\n\nWrite checkpoints to fast parallel file system\nAsynchronously copy to durable object storage\nDelete from fast tier after copy completes\nRecover from fast tier if available, otherwise from object storage\n\nThis architecture provides both performance (fast writes) and durability (object storage backup) while managing cost (limited fast storage).\n\n\nModel-Type Checkpoint Considerations\nCheckpoint strategies vary significantly by model architecture:\n\n\n\nTable 12: Checkpoint strategies should be tailored to model size and training dynamics.\n\n\n\n\n\n\n\n\n\n\n\nModel Type\nCheckpoint Size\nTypical Frequency\nRecommended Strategy\n\n\n\n\nLLM (175B)\n700 GB - 1 TB\nEvery 10-30 min\nDistributed async, tiered storage\n\n\nLLM (7B)\n14-28 GB\nEvery 5-15 min\nSingle-node, parallel FS\n\n\nRecSys (10TB embeddings)\n10+ TB\nIncremental every hour\nDelta compression, streaming\n\n\nVision (ResNet-50)\n200 MB\nEvery epoch\nSimple sync, local + remote copy\n\n\nVision (ViT-22B)\n88-175 GB\nEvery 15-30 min\nDistributed, parallel FS\n\n\n\n\n\n\nLarge Language Models checkpoint infrequently (relative to iteration count) because checkpoint size dominates. A 1 TB checkpoint at 100 GB/s still takes 10 seconds, during which thousands of dollars of GPU time is consumed. Asynchronous checkpointing and high-bandwidth storage are essential.\nRecommendation Systems present unique challenges due to massive embedding tables. A DLRM-style model might have 10 TB of embedding parameters but only 1 GB of dense MLP parameters. Incremental checkpointing of modified embeddings (which may be a small fraction of the table) provides order-of-magnitude savings over full checkpoints.\nVision Models at typical scales (&lt; 1 billion parameters) checkpoint easily. The entire checkpoint fits in a few GB, which can be written in under a second even to modest storage. The challenge is ensuring checkpoints are copied to durable storage before being deleted locally.\nFine-tuning runs of any model type can use delta checkpointing efficiently. Only the fine-tuned parameters (often &lt; 1% of total for LoRA-style methods) need to be saved, reducing checkpoint size by 100x or more.",
    "crumbs": [
      "Volume II: Advanced",
      "Foundations of Scale",
      "Storage Systems for ML"
    ]
  },
  {
    "objectID": "contents/vol2/storage/storage.html#sec-feature-stores",
    "href": "contents/vol2/storage/storage.html#sec-feature-stores",
    "title": "Storage Systems for ML",
    "section": "Feature Stores",
    "text": "Feature Stores\nFeature stores represent one of the most consequential but often overlooked components of production ML infrastructure. While academic ML curricula focus heavily on model architectures and training algorithms, production systems at companies like Meta, Google, and Netflix depend critically on feature stores that transform raw data into ML-ready signals. For recommendation systems, the dominant ML workload in production, feature stores are not merely useful but essential: every user request triggers hundreds of feature lookups that must complete in milliseconds.\nThis section develops the architecture and design principles of feature stores, with particular attention to why they matter enormously for recommendation systems while playing a smaller role for LLMs and vision models.\n\nWhy Feature Stores Exist\nThe core problem feature stores solve is the training-serving gap: features computed during training must be reproducible during serving, but the contexts differ dramatically.\nDuring training, features are computed in batch over historical data. There is no latency constraint: a training pipeline can spend hours computing features over millions of training examples. The priority is correctness and coverage.\nDuring serving, features must be available within milliseconds for real-time inference. A recommendation system making personalized content suggestions has perhaps 50ms total latency budget; feature retrieval might consume 5-10ms of that budget. The priority is latency and availability.\nWithout a feature store, teams face painful tradeoffs:\n\nDuplicate implementation: Engineers write feature computation logic twice (once in batch Python/Spark for training, once in low-latency Java/C++ for serving), creating maintenance burden and inconsistency risk\nPoint-in-time bugs: Training features are computed with access to future data that would not be available at serving time, causing training-serving skew\nFreshness problems: Features computed in batch become stale; serving uses outdated information\n\nFeature stores solve these problems by providing:\n\nUnified feature computation: Write feature logic once, execute in both batch and streaming contexts\nPoint-in-time correctness: Retrieve features as they were at a specific historical moment\nLow-latency serving: Pre-computed features available with single-digit millisecond latency\nFeature reuse: Features computed once can be shared across many models\n\n\n\nFeature Store Architecture\nA feature store has two primary components: the offline store for training and the online store for serving.\n\nOffline Store\nThe offline store contains the complete historical record of feature values, enabling training on any time window. It is optimized for throughput rather than latency.\nStorage technologies: Data lakes (S3 + Parquet), data warehouses (BigQuery, Snowflake, Redshift), or specialized time-series databases.\nData organization: Features are stored with timestamps, enabling point-in-time queries:\n| user_id | feature_name      | value | timestamp           |\n|---------|-------------------|-------|---------------------|\n| 12345   | purchase_count_7d | 3     | 2024-01-15 00:00:00 |\n| 12345   | purchase_count_7d | 5     | 2024-01-16 00:00:00 |\n| 12345   | avg_session_length| 4.2   | 2024-01-15 00:00:00 |\nPoint-in-time joins: Training requires joining features to labels at the exact time the label event occurred. If a user clicked an ad at 2024-01-15 14:23:00, training needs the features that were available at 14:22:59, not the features computed later that day.\n\n\n\n\n\n\nWarningPoint-in-Time Correctness\n\n\n\nPoint-in-time correctness is the single most important property of feature stores. Using future information during training (a form of data leakage) produces models that perform well in offline evaluation but fail in production. This bug is insidious because metrics look great until deployment.\nExample: A fraud detection model trained with features including “user_reported_fraud” appears to achieve 99% accuracy. But this feature is only populated after fraud is reported, which happens after the transaction being scored. The model learns to recognize already-reported fraud, not to predict future fraud.\n\n\nQuery patterns: Offline queries retrieve features for millions of training examples in batch:\nSELECT\n    labels.user_id,\n    labels.item_id,\n    labels.clicked,\n    features.purchase_count_7d,\n    features.avg_session_length\nFROM training_labels labels\nLEFT JOIN features\n    ON labels.user_id = features.user_id\n    AND features.timestamp &lt;= labels.event_time\n    AND features.timestamp &gt; labels.event_time - INTERVAL 1 DAY\n\n\nOnline Store\nThe online store provides low-latency access to the most recent feature values for serving. It trades historical depth for speed.\nStorage technologies: Key-value stores optimized for point lookups (Redis, DynamoDB, Bigtable (Chang et al. 2008), Cassandra).\n\nChang, Fay, Jeffrey Dean, Sanjay Ghemawat, Wilson C Hsieh, Deborah A Wallach, Mike Burrows, Tushar Chandra, Andrew Fikes, and Robert E Gruber. 2008. “Bigtable: A Distributed Storage System for Structured Data.” ACM Transactions on Computer Systems (TOCS) 26 (2): 1–26.\nData organization: Features are stored by entity key with only the most recent value:\nKey: user:12345\nValue: {\n    \"purchase_count_7d\": 5,\n    \"avg_session_length\": 4.2,\n    \"last_updated\": \"2024-01-16T14:30:00Z\"\n}\nAccess patterns: Online queries retrieve features for a single entity or small batch:\nfeatures = online_store.get_features(\n    entity_id=\"user:12345\",\n    feature_names=[\"purchase_count_7d\", \"avg_session_length\"],\n)\n# Returns in &lt; 5ms\nConsistency with offline: The online store must reflect the same feature values that the offline store would return for “now”. Feature computation pipelines update both stores, with the offline store receiving the historical record and the online store receiving the current value.\n\n\nFeature Computation Pipelines\nFeatures are computed by pipelines that transform raw data into feature values. These pipelines can operate in batch or streaming mode.\nBatch pipelines compute features over historical data, typically running daily or hourly. They are simpler to implement and debug but produce features that are always somewhat stale.\n# Batch feature: user's purchase count in last 7 days\n@feature(schedule=\"daily\")\ndef purchase_count_7d(user_id: str, date: datetime) -&gt; int:\n    return db.query(\n        f\"\"\"\n        SELECT COUNT(*) FROM purchases\n        WHERE user_id = '{user_id}'\n        AND purchase_date &gt; '{date - timedelta(days=7)}'\n    \"\"\"\n    )\nStreaming pipelines compute features from real-time event streams, providing fresh feature values within seconds of underlying events. They are more complex but essential for use cases where freshness matters (e.g., fraud detection, real-time recommendations).\n# Streaming feature: user's purchases in current session\n@streaming_feature(source=\"purchase_events\")\ndef session_purchase_count(event: PurchaseEvent, state: State) -&gt; int:\n    if event.session_id != state.current_session:\n        state.reset()\n        state.current_session = event.session_id\n    state.count += 1\n    return state.count\nLambda architecture combines both: batch pipelines provide accurate but stale features; streaming pipelines provide fresh but potentially approximate features. The serving layer merges results, preferring fresh streaming values when available.\n\n\n\nFeature Lookup Latency Budget\nOnline serving imposes strict latency constraints. The feature lookup must fit within the overall inference latency budget.\n\nDefinition: Feature Lookup Latency Budget\n\\[L_{feature} &lt; L_{SLO} - L_{model} - L_{network} - L_{margin}\\]\nwhere:\n\n\\(L_{SLO}\\) is the end-to-end latency SLO (e.g., 100ms)\n\\(L_{model}\\) is model inference time (e.g., 20ms)\n\\(L_{network}\\) is network round-trip time (e.g., 10ms)\n\\(L_{margin}\\) is safety margin for variance (e.g., 20ms)\n\n\n\n\nExample: Feature Latency Budget for Recommendation Serving\nA recommendation system has 100ms end-to-end SLO. Breaking down the budget:\n\nModel inference: 30ms (two-tower retrieval + ranking)\nNetwork overhead: 15ms (client-server round trips)\nBusiness logic: 10ms (filtering, deduplication)\nSafety margin: 15ms (P99 variance)\nAvailable for features: 30ms\n\nWith 30ms budget and 200 features to retrieve, each feature lookup must complete in 0.15ms average. This is achievable only with:\n\nBatch lookups (one round-trip for all 200 features)\nFeatures co-located in same key-value store\nIn-memory storage (Redis, not disk-backed)\nSame-region deployment (&lt; 1ms network)\n\n\n\n\n\nEmbedding Table Storage\nRecommendation systems present a unique storage challenge: embedding tables containing vectors for millions or billions of entities. These tables can reach terabytes in size while requiring single-digit millisecond lookup latency.\nScale of embedding tables:\n\n\n\nTable 13: Embedding table sizes for production recommendation systems.\n\n\n\n\n\nApplication\nEntities\nEmbedding Dim\nTotal Size\n\n\n\n\nUser embeddings\n1 billion\n128\n512 GB\n\n\nProduct embeddings\n100 million\n256\n100 GB\n\n\nAd embeddings\n10 million\n512\n20 GB\n\n\nSparse ID embeddings\n100 billion\n64\n25 TB\n\n\n\n\n\n\nStorage strategies:\nIn-memory storage (Redis, Memcached) provides the lowest latency (&lt; 1ms) but highest cost. A 500 GB embedding table requires expensive high-memory instances. Suitable for frequently accessed embeddings with strict latency requirements.\nSSD-backed key-value stores (RocksDB, Cassandra) provide 1-10ms latency at lower cost. The embedding is loaded from SSD on cache miss. Suitable for infrequently accessed embeddings or when cost constraints preclude in-memory storage.\nTiered storage keeps hot embeddings in memory and cold embeddings on SSD. Access patterns in recommendation systems are highly skewed: a small fraction of users and items account for most traffic. Keeping the top 10% of embeddings in memory while the remaining 90% are on SSD can provide good latency at reasonable cost.\nEmbedding sharding distributes large embedding tables across multiple servers:\n\\[\\text{Shard ID} = \\text{hash}(\\text{entity\\_id}) \\mod N_{shards}\\]\nEach shard stores 1/N of the embeddings. Lookup requires determining the correct shard and querying that server. With consistent hashing, adding or removing shards rebalances minimal data.\n\n\nModel-Type Feature Store Requirements\nFeature store importance varies dramatically by model type:\n\n\n\nTable 14: Feature store criticality varies by ML application.\n\n\n\n\n\n\n\n\n\n\nModel Type\nFeature Store Criticality\nRationale\n\n\n\n\nRecSys\nCritical\nMillions of lookups/second, user/item features essential\n\n\nFraud Detection\nCritical\nReal-time features detect fraud patterns\n\n\nAd Ranking\nCritical\nUser context, ad features, bid signals\n\n\nSearch Ranking\nHigh\nQuery understanding, user history\n\n\nLLMs\nLow\nMinimal runtime features, prompt is the input\n\n\nVision\nLow-Medium\nOptional context features, mainly model input\n\n\nSpeech\nLow\nAudio input, minimal runtime features\n\n\n\n\n\n\nWhy feature stores are critical for RecSys but not LLMs:\nRecommendation systems make predictions about user-item interactions, requiring features about both the user and items. At serving time, the system must retrieve:\n\nUser features: Demographics, historical behavior, session context\nItem features: Category, popularity, freshness\nContext features: Time of day, device, location\nCross features: User-item affinity scores, collaborative filtering signals\n\nThese features cannot be derived from the input alone (unlike LLM prompts, which contain all necessary information). The feature store is the only way to provide this information to the model at serving time.\nLLMs, by contrast, receive their input as a prompt. The prompt contains all information the model needs to generate a response. There are no external features to look up. The “feature store” for an LLM is simply the tokenizer and any prompt templates, not a database of entity features.\n\n\nFeature Store Platforms\nSeveral platforms provide feature store functionality, each with different tradeoffs:\nOpen source:\n\nFeast: Most popular open-source feature store. Supports multiple backends (Redis, DynamoDB, BigQuery). Provides point-in-time joins, feature versioning.\nHopsworks: Feature store with MLOps integration. Strong support for feature pipelines and versioning.\n\nCloud-managed:\n\nVertex AI Feature Store (GCP): Managed service with BigQuery integration. Auto-scaling online serving.\nSageMaker Feature Store (AWS): Integrated with SageMaker ML workflow. S3 offline store, DynamoDB-backed online store.\nAzure ML Feature Store: Part of Azure ML ecosystem. Supports feature materialization and serving.\n\nEnterprise:\n\nTecton: Enterprise feature platform built by Feast creators. Sophisticated streaming feature support.\nDatabricks Feature Store: Integrated with Databricks lakehouse. Unity Catalog integration for governance.\n\nBuild vs buy considerations:\n\n\n\nTable 15: Feature store build vs buy tradeoffs.\n\n\n\n\n\nFactor\nBuild In-House\nUse Platform\n\n\n\n\nCustomization\nFull control\nLimited by platform\n\n\nDevelopment cost\nHigh initial investment\nLower initial cost\n\n\nOperations\nRequires dedicated team\nManaged by vendor\n\n\nScale\nRequires expertise\nBuilt-in scaling\n\n\nIntegration\nCustom to stack\nMay require adaptation\n\n\n\n\n\n\nFor most organizations, starting with an open-source solution (Feast) or cloud-managed service provides faster time-to-value than building custom infrastructure. Custom feature stores become worthwhile at massive scale (billions of features, millions of QPS) where platform limitations become constraints.",
    "crumbs": [
      "Volume II: Advanced",
      "Foundations of Scale",
      "Storage Systems for ML"
    ]
  },
  {
    "objectID": "contents/vol2/storage/storage.html#sec-model-registries",
    "href": "contents/vol2/storage/storage.html#sec-model-registries",
    "title": "Storage Systems for ML",
    "section": "Model Registries and Artifact Management",
    "text": "Model Registries and Artifact Management\nTraining produces model weights, but deploying those weights to production requires additional infrastructure. Model registries provide the storage, versioning, and governance layer between training completion and production deployment. They answer questions that become critical at scale: Which model version is currently in production? What training data and hyperparameters produced this model? Who approved this model for deployment?\n\nModel Registry Architecture\nA model registry stores and organizes model artifacts with associated metadata, enabling teams to manage the lifecycle of models from experimentation through deployment to retirement.\nCore components:\nArtifact storage holds the actual model files: weights, configurations, preprocessing artifacts, and any ancillary files needed for inference. Storage backends range from simple object storage (S3, GCS) to specialized artifact stores (Artifactory, Nexus).\nMetadata store maintains information about each model version: training parameters, performance metrics, data lineage, and deployment status. This is typically a database (PostgreSQL, MySQL) or document store (MongoDB).\nVersioning system tracks model versions with semantic versioning or auto-incrementing identifiers. Each version is immutable: once registered, a model version cannot be modified.\nAccess control governs who can register, read, and promote models. Different teams may have different permissions (data scientists can register, MLOps can promote to production, only approved models can be deployed).\n\n\nModel Versioning\nModel versioning differs from code versioning in important ways:\nArtifacts are large and binary. Git handles text diffs efficiently but struggles with multi-GB model files. Model registries use content-addressable storage, storing each unique artifact once regardless of how many versions reference it.\nVersions may not be sequential. Teams often run multiple experiments in parallel, producing model versions that branch from different starting points. The registry must handle non-linear version histories.\nMetadata is as important as artifacts. Knowing what hyperparameters, training data, and code version produced a model is essential for debugging and reproducibility.\nA typical model version record includes:\nmodel_name: \"product_recommender\"\nversion: \"v2.3.1\"\nstatus: \"production\"\nregistered_at: \"2024-01-15T10:23:45Z\"\nregistered_by: \"ml-team-ci\"\n\nartifacts:\n  model_weights: \"s3://models/product_recommender/v2.3.1/model.pt\"\n  config: \"s3://models/product_recommender/v2.3.1/config.yaml\"\n  tokenizer: \"s3://models/product_recommender/v2.3.1/tokenizer/\"\n\ntraining:\n  framework: \"pytorch\"\n  framework_version: \"2.1.0\"\n  training_data: \"s3://datasets/product_interactions/2024-01-01/\"\n  training_data_hash: \"sha256:a3b4c5...\"\n  hyperparameters:\n    learning_rate: 0.001\n    batch_size: 256\n    epochs: 50\n  training_job_id: \"train-20240115-001\"\n  training_duration_hours: 12.5\n\nmetrics:\n  validation_accuracy: 0.847\n  validation_loss: 0.312\n  auc: 0.923\n\nlineage:\n  parent_model: \"product_recommender:v2.2.0\"\n  code_commit: \"git:abc123\"\n  experiment_id: \"exp-2024-01-15-hyperopt\"\n\n\nReproducibility and Lineage Tracking\nReproducibility in ML requires tracking not just the model weights but the entire provenance chain: what data, code, and environment produced this model?\nData lineage tracks which datasets were used for training and validation. This includes:\n\nDataset versions or snapshot identifiers\nData preprocessing pipeline versions\nAny filtering or sampling applied\nHash of the actual data used\n\nCode lineage links models to the code that produced them:\n\nGit commit hash of training code\nContainer image digest for training environment\nFramework versions and dependencies\n\nEnvironment lineage captures the computational environment:\n\nHardware (GPU type, count)\nSoftware (CUDA version, Python version, package versions)\nRandom seeds used\n\nFull lineage enables:\n\nDebugging: When a model behaves unexpectedly, trace back to exact training conditions\nAuditing: Demonstrate to regulators exactly how a model was trained\nReproduction: Train identical model from lineage information\nComparison: Understand why two model versions differ\n\n\n\n\n\n\n\nNoteReproducibility in Practice\n\n\n\nPerfect reproducibility in ML is difficult due to non-determinism in GPU operations, floating-point associativity, and framework internals. Lineage tracking enables approximate reproduction: training with the same data, code, and hyperparameters typically produces a model with similar (but not bit-identical) performance.\n\n\n\n\nModel Lifecycle Stages\nModels progress through lifecycle stages, with the registry tracking current stage and stage transitions:\nDevelopment: Experimental models under active iteration. Many versions may be created and discarded. No quality guarantees.\nStaging: Candidate models undergoing evaluation. Limited access, subjected to validation tests. Models that pass move to production; those that fail return to development.\nProduction: Approved models serving live traffic. Strict change control, monitoring requirements. Only promoted models reach production.\nArchived: Retired models no longer serving traffic but retained for reference, auditing, or rollback. May be moved to cold storage.\nDeprecated: Models scheduled for removal. Alerts generated if still accessed. Fully deleted after retention period.\nDevelopment --&gt; Staging --&gt; Production --&gt; Archived\n     ^              |             |\n     |              v             v\n     +---- (failed) +-- Deprecated -&gt; Deleted\n\n\nArtifact Storage Considerations\nModel artifacts range from megabytes (small classifiers) to terabytes (large foundation models), requiring different storage strategies.\nSmall models (&lt; 1 GB): Object storage (S3, GCS) with standard redundancy. Download to inference servers is fast; model can be fetched on each server start.\nMedium models (1-100 GB): Object storage with regional caching. Pre-deploy models to inference servers to avoid startup latency. Consider compression for network transfer.\nLarge models (&gt; 100 GB): Distributed storage with parallel download. Sharded across multiple files for parallel access. May require local NVMe for serving latency requirements.\nStorage cost optimization:\n\n\n\nTable 16: Artifact storage optimization strategies.\n\n\n\n\n\n\n\n\n\n\nStrategy\nBenefit\nTradeoff\n\n\n\n\nCompression\n2-5x size reduction\nCPU overhead on load\n\n\nDeduplication\nShared layers stored once\nComplexity in artifact management\n\n\nTiered storage\nCold storage for old versions\nRetrieval latency\n\n\nDifferential storage\nStore only changed weights\nRequires base model + diffs\n\n\n\n\n\n\n\n\nModel Registry Platforms\nSeveral platforms provide model registry functionality:\nMLflow Model Registry: Open source, widely adopted. Integrates with MLflow tracking. Supports model stages, versioning, and deployment integration. Backed by file system or database storage.\nWeights & Biases Model Registry: Part of W&B platform. Strong experiment tracking integration. Artifact versioning with lineage.\nVertex AI Model Registry (GCP): Managed service with Vertex AI integration. Model versioning, deployment to endpoints.\nSageMaker Model Registry (AWS): Part of SageMaker MLOps. Model groups, versions, approval workflows.\nAzure ML Model Registry: Part of Azure ML. Model versioning, deployment, monitoring integration.\nComparison:\n\n\n\nTable 17: Model registry platform comparison.\n\n\n\n\n\n\n\n\n\n\n\n\nPlatform\nOpen Source\nDeployment Integration\nLineage\nApproval Workflows\n\n\n\n\nMLflow\nYes\nVia plugins\nBasic\nBasic\n\n\nW&B\nNo\nLimited\nStrong\nLimited\n\n\nVertex AI\nNo\nNative GCP\nGood\nYes\n\n\nSageMaker\nNo\nNative AWS\nGood\nYes\n\n\nAzure ML\nNo\nNative Azure\nGood\nYes\n\n\n\n\n\n\n\n\nRegistry Design Patterns\nEffective model registry usage follows established patterns:\nPattern: Immutable versions. Once registered, a model version cannot be modified. Updates create new versions. This ensures reproducibility and enables safe rollback.\nPattern: Promotion gates. Models must pass automated tests before promotion to production: validation metrics above threshold, bias tests passed, latency requirements met. Human approval may be required for certain model types.\nPattern: Canary metadata. Model versions include canary configuration: what percentage of traffic to receive initially, what metrics to monitor, automatic rollback conditions.\nPattern: Model cards. Each production model has a model card documenting intended use, limitations, performance characteristics, and ethical considerations. Required for governance and user understanding.\nPattern: Retention policies. Old model versions are automatically archived or deleted based on policy: keep last N versions, keep all versions newer than date, never delete production versions.",
    "crumbs": [
      "Volume II: Advanced",
      "Foundations of Scale",
      "Storage Systems for ML"
    ]
  },
  {
    "objectID": "contents/vol2/storage/storage.html#sec-storage-case-studies",
    "href": "contents/vol2/storage/storage.html#sec-storage-case-studies",
    "title": "Storage Systems for ML",
    "section": "Case Studies",
    "text": "Case Studies\nThe storage system principles developed throughout this chapter manifest differently across organizations depending on their dominant workloads, scale, and infrastructure maturity. This section examines four case studies that illustrate how leading ML organizations have designed storage systems for their specific requirements.\n\nGoogle Colossus: Storage for LLM Training\nGoogle’s Colossus file system, the successor to the original Google File System (GFS), demonstrates storage architecture optimized for massive-scale sequential workloads including LLM training.\nScale and requirements:\nGoogle trains models including PaLM (540 billion parameters) and Gemini across thousands of TPUs. Training data for these models spans tens of terabytes of tokenized text, requiring sustained read throughput of hundreds of GB/s across the training cluster. Checkpoints for the largest models exceed 1 TB and must be saved within minutes to minimize training interruption.\nArchitecture decisions:\nDistributed metadata. Unlike GFS’s single master, Colossus distributes file system metadata across multiple servers (Curators). This eliminates the metadata bottleneck that would otherwise limit operations to ~10,000/second, enabling the high-frequency checkpoint writes and metadata operations that large-scale training requires.\nErasure coding. Colossus uses Reed-Solomon erasure coding rather than simple replication. A typical configuration stores 9 data fragments plus 3 parity fragments, achieving durability equivalent to 3x replication while using only 1.33x storage. For petabyte-scale training data, this reduces storage costs by billions of dollars.\nD (Disk) servers. Colossus separates the storage layer into D servers that manage physical disks. This abstraction enables flexible placement of data across disk types (HDD for cold data, SSD for hot data) without changing the file system interface.\nIntegration with TPU architecture:\nColossus integrates deeply with TPU training infrastructure:\n\nData is striped to enable parallel reads from many D servers simultaneously\nTPU hosts run Colossus clients that prefetch training data during computation\nCheckpoint writes use dedicated bandwidth allocation to prevent interference with training data reads\n\nLessons:\n\nAt Google scale, the file system must be redesigned, not just scaled. GFS concepts (large blocks, append-optimized, single namespace) remain valid, but implementation must be distributed.\nErasure coding is essential for cost-effective storage at petabyte scale.\nTight integration between storage and compute systems enables efficiencies impossible with generic storage.\n\n\n\nMeta Feature Store: Recommendation at Scale\nMeta’s recommendation systems serve billions of users across Facebook, Instagram, and WhatsApp, requiring feature store infrastructure that handles trillions of feature lookups daily while maintaining single-digit millisecond latency.\nScale and requirements:\n\nBillions of users, each with hundreds of features\nTrillions of feature lookups per day\nP99 latency requirement: &lt; 10ms\nFeatures updated continuously from user activity streams\nEmbedding tables exceeding 10 TB\n\nArchitecture decisions:\nHybrid online store. Meta’s feature store uses a tiered architecture:\n\nL1 cache: Process-local cache on serving machines. Sub-millisecond access for hot features.\nL2 cache: Distributed cache (Memcached) for warm features. 1-2ms access.\nPersistent store: Distributed key-value store (similar to RocksDB) for cold features. 5-10ms access.\n\nThis tiering exploits the power-law distribution of feature access: the most active 1% of users account for a disproportionate share of requests.\nStreaming feature computation. Features are computed by streaming pipelines (similar to Flink) processing user activity in real-time. A user’s “items_viewed_last_hour” feature updates within seconds of each view event, enabling recommendations that reflect immediate interests.\nEmbedding table sharding. User embeddings are sharded across thousands of servers using consistent hashing. Each server holds a fraction of the embedding table in memory. Lookup involves hashing the user ID to determine the shard, then a single network hop to retrieve the embedding.\nPoint-in-time correctness. The offline feature store maintains timestamped feature values, enabling training data generation with correct historical features. Training pipelines join labels (user interactions) with features as they existed at interaction time, preventing data leakage.\nLessons:\n\nFeature store design is dominated by the access pattern: billions of point lookups per second require in-memory storage for hot data.\nStreaming feature computation is essential for recommendation freshness.\nTiered caching exploits access pattern skew to provide good latency at reasonable cost.\nPoint-in-time correctness is a non-negotiable requirement; retrofitting it is extremely difficult.\n\n\n\nTesla: Video Data Pipeline for Vision Training\nTesla’s Autopilot and Full Self-Driving systems are trained on video data collected from millions of vehicles, presenting unique storage challenges for vision model training at scale.\nScale and requirements:\n\nFleet of millions of vehicles continuously collecting video\nPetabytes of video ingested daily (before filtering)\nTraining datasets of selected clips reaching hundreds of TB\nVideo must be decoded, augmented, and streamed to training GPUs\nData selection is as important as data quantity\n\nArchitecture decisions:\nHierarchical data selection. Not all collected video is valuable for training. Tesla’s pipeline implements progressive filtering:\n\nOn-vehicle filtering: Neural networks on vehicle hardware identify interesting scenarios (edge cases, novel situations)\nUpload filtering: Only flagged clips are uploaded over cellular/WiFi\nOffline filtering: More sophisticated models further filter uploaded data\nLabeling queue: High-value clips are prioritized for human labeling\n\nThis filtering reduces storage requirements by orders of magnitude while focusing training on the most valuable data.\nObject storage with intelligent tiering. Raw video is stored in object storage with automatic tiering:\n\nRecent uploads in hot storage for immediate processing\nProcessed clips moved to warm storage\nArchived raw footage in cold storage for potential re-processing\n\nCustom data format. Tesla developed custom video formats optimized for ML training:\n\nTemporal compression aware of training access patterns (random frame access)\nMultiple resolution variants for different training stages\nSensor metadata (GPS, IMU, camera calibration) embedded in format\n\nDistributed video decoding. Video decoding is CPU-intensive. Tesla’s data pipeline distributes decoding across many CPU workers, with decoded frames streamed to GPUs. This decouples decode throughput from GPU count.\nLessons:\n\nFor video data, the storage problem is inseparable from the data selection problem. Storing everything is infeasible; intelligent filtering is essential.\nCustom data formats can provide significant efficiency gains when standard formats impose unacceptable overhead.\nDecoding and augmentation pipelines require dedicated compute; they cannot be an afterthought.\n\n\n\nSpotify: Hybrid ML Platform Storage\nSpotify combines recommendation systems (user-music matching) with audio understanding (content analysis), requiring storage infrastructure that serves both workload types efficiently.\nScale and requirements:\n\nHundreds of millions of users with listening history\nTens of millions of tracks, podcasts, and audiobooks\nRecommendation serving at millions of QPS\nAudio analysis models processing newly uploaded content\nFeatures spanning user behavior and audio content\n\nArchitecture decisions:\nUnified feature platform. Spotify’s feature platform serves both recommendation and audio ML:\n\nUser features: Listening history, preferences, demographics\nContent features: Audio embeddings, genre classification, tempo\nContextual features: Time of day, device, location\n\nA single feature store serves all models, enabling feature reuse across teams.\nContent embedding pipeline. New audio content flows through embedding pipelines:\n\nUpload to object storage (GCS)\nAudio analysis models extract embeddings\nEmbeddings written to feature store\nContent available for recommendation within hours of upload\n\nBatch and streaming feature computation. Some features are batch-computed (user’s “top genres last month”), while others are streaming (“songs played this session”). Both types flow into the same feature store with appropriate freshness guarantees.\nGCP-native storage stack. Spotify runs primarily on Google Cloud:\n\nBigQuery for offline feature store and analytics\nBigtable for online feature serving\nGCS for raw data and model artifacts\nDataflow for feature computation pipelines\n\nThis cloud-native approach reduces operational burden while providing the scale needed for Spotify’s workloads.\nModel versioning for A/B testing. Spotify runs continuous A/B tests with many model variants serving traffic simultaneously. The model registry tracks which variants are in each test, enabling analysis of model performance in production.\nLessons:\n\nDifferent ML workloads (recommendation, content understanding) can share storage infrastructure when designed with flexibility.\nCloud-managed services provide operational simplicity at the cost of some customization.\nFeature platforms that serve multiple teams create significant organizational value through feature reuse.\n\n\n\nCross-Cutting Themes\nSeveral themes emerge across these case studies:\nScale requires architectural adaptation. Solutions that work at small scale fail at large scale. Each organization redesigned storage architecture as scale increased, rather than simply adding capacity.\nWorkload characteristics drive design. LLM training (sequential, checkpoint-heavy) requires different storage than recommendation (random access, latency-critical). Organizations must understand their workload mix.\nData quality infrastructure is as important as data quantity. Tesla and Google invest heavily in data selection and quality, recognizing that more data is not always better data.\nFeature stores are production-critical for recommendation. Meta and Spotify treat feature stores as tier-1 infrastructure with the same reliability requirements as serving systems.\nCloud vs on-premise tradeoffs remain. Google builds custom infrastructure; Spotify uses cloud services. Both approaches work; the choice depends on scale, expertise, and strategic priorities.",
    "crumbs": [
      "Volume II: Advanced",
      "Foundations of Scale",
      "Storage Systems for ML"
    ]
  },
  {
    "objectID": "contents/vol2/storage/storage.html#sec-storage-summary",
    "href": "contents/vol2/storage/storage.html#sec-storage-summary",
    "title": "Storage Systems for ML",
    "section": "Chapter Summary",
    "text": "Chapter Summary\nStorage systems form the foundation upon which large-scale ML training and serving are built. This chapter developed the principles and architectures that enable storage systems to meet the distinctive requirements of machine learning workloads.\nWe began by examining how ML workloads systematically invert traditional storage assumptions: sequential streaming replaces random access, working sets exceed cache capacity, write patterns are bursty rather than continuous, and read/write ratios vary dramatically by phase. The data pipeline throughput equation (\\(B_{required} = N_{GPUs} \\times U_{target} \\times S_{batch}/T_{iteration}\\)) provides quantitative guidance for storage capacity planning.\nTraining data infrastructure comprises distributed file systems (GFS/Colossus, HDFS, Lustre) and object storage (S3, GCS), each optimized for different access patterns and scales. Data format selection (TFRecord, Parquet, WebDataset) significantly impacts pipeline throughput, with different formats suited to different model types. Data loading pipelines must prefetch and buffer data to hide storage latency and keep accelerators fully utilized.\nCheckpoint storage enables fault tolerance for long-running training jobs. The Young-Daly formula (\\(T_{opt} = \\sqrt{2 \\times T_{save} \\times MTBF}\\)) provides the optimal checkpoint interval balancing overhead against recovery time. Distributed and incremental checkpointing techniques address the challenges of TB-scale model state.\nFeature stores bridge training and serving for models requiring runtime features. They are critical infrastructure for recommendation systems (trillions of lookups daily) while playing minimal roles for LLMs (prompt contains all information). Point-in-time correctness prevents training-serving skew that causes models to fail silently in production.\nModel registries provide versioning, lineage tracking, and governance for model artifacts. They enable reproducibility by connecting trained models to the data, code, and environment that produced them.\n\n\n\n\n\n\nImportant3 Things Students MUST Remember\n\n\n\n\nStorage bandwidth, not capacity, limits training throughput. A cluster with petabytes of storage but insufficient bandwidth will leave GPUs idle. The data pipeline throughput equation quantifies required bandwidth; storage planning must start with bandwidth, not capacity.\nFeature stores are essential infrastructure for recommendation systems but nearly irrelevant for LLMs. Recommendation systems are the dominant production ML workload, making feature stores critical knowledge for ML engineers, yet they receive minimal attention in LLM-focused curricula. Understanding when feature stores matter (and when they do not) distinguishes production-ready engineers.\nCheckpoint overhead scales with cluster size. The Young-Daly formula (\\(T_{opt} = \\sqrt{2 \\times T_{save} \\times MTBF}\\)) provides quantitative guidance. As clusters grow, MTBF decreases and checkpoint frequency must increase, making high-bandwidth checkpoint storage essential for large-scale training.",
    "crumbs": [
      "Volume II: Advanced",
      "Foundations of Scale",
      "Storage Systems for ML"
    ]
  },
  {
    "objectID": "contents/vol2/storage/storage.html#sec-storage-fallacies-pitfalls",
    "href": "contents/vol2/storage/storage.html#sec-storage-fallacies-pitfalls",
    "title": "Storage Systems for ML",
    "section": "Fallacies and Pitfalls",
    "text": "Fallacies and Pitfalls\nStorage systems present numerous opportunities for costly misconceptions, particularly when assumptions from traditional computing fail to transfer to ML workloads.\nFallacy: Object storage latency is acceptable for training data.\nObject stores like S3 and GCS offer seemingly infinite capacity at low cost, but their latency characteristics poorly match ML training patterns. First-byte latency of 50-200ms for object storage versus 1-10ms for file systems means that small-batch access patterns incur unacceptable overhead. A training pipeline reading 1000 small files sequentially from S3 spends more time waiting for first bytes than transferring data.\nThe solution is not to avoid object storage (it remains the most cost-effective option for large datasets) but to design pipelines around its characteristics: large sequential reads, extensive prefetching, and local caching. The common antipattern of treating object storage as a drop-in replacement for NFS leads to training throughput 3-5x lower than achievable.\nPitfall: Sizing checkpoint storage by capacity rather than bandwidth.\nOrganizations provision checkpoint storage based on model size: “Our model is 500GB, so we need 5TB for 10 checkpoints.” This ignores the critical constraint: bandwidth during checkpoint writes.\nA 500GB checkpoint written to NFS at 1 GB/s takes 8+ minutes. For a 1000-GPU training job with MTBF of 4 hours, the Young-Daly optimal interval is approximately 23 minutes. Spending 8 minutes (35% of the interval) on checkpoint writes is unacceptable overhead. The same checkpoint written to parallel file system at 100 GB/s takes 5 seconds (0.4% overhead).\nCapacity planning must start with bandwidth: “We need to write 500GB in under 30 seconds” leads to very different architecture than “we need 5TB of capacity.”\nFallacy: Checkpoint storage is a solved problem.\nModern frameworks checkpoint transparently, creating the illusion that checkpointing “just works.” This masks the coordination overhead that dominates checkpoint time at scale.\nWith 1000 GPUs, each writing 500MB, the aggregate checkpoint is 500GB. But achieving this requires:\n\nAll ranks reaching checkpoint barrier synchronously\nCoordinated access to storage (avoiding hotspots)\nVerification that all shards completed successfully\nAtomic update of “latest checkpoint” pointer\n\nThe actual wall-clock time often exceeds the theoretical write time by 2-3x due to stragglers, coordination, and verification. Organizations that benchmark single-node checkpointing and extrapolate are consistently surprised by distributed checkpoint overhead.\nPitfall: Assuming training data locality exists.\nTraditional storage optimization assumes data locality: frequently accessed data should be near compute. For training data, this assumption fails fundamentally. Each training sample is accessed once per epoch, and randomized shuffling ensures no sample is accessed more frequently than others.\nCaching strategies designed for hot data provide zero benefit: the working set equals the dataset. Prefetching strategies that predict future access based on past access fail: shuffling makes access unpredictable. Storage systems designed for “hot tier” and “cold tier” find all training data equally tepid.\nEffective training storage optimizes for sequential bandwidth and prefetch depth rather than cache hit rates.\nFallacy: Feature store latency does not matter because serving latency is dominated by model inference.\nFor LLMs, this is true: model inference of 50-500ms dwarfs any feature lookup. For recommendation systems, it is catastrophically false.\nA recommendation model inference might take 2ms. With 100 features requiring 50th-percentile lookup of 1ms each, feature retrieval would dominate latency. Production feature stores must achieve sub-millisecond P50 and single-digit-millisecond P99 lookups. Organizations that treat feature stores as “just another cache” discover too late that they are on the critical path for serving latency.\nPitfall: Underestimating point-in-time correctness requirements.\nTraining a recommendation model on features that include the label (e.g., using “user clicked on item X” as a feature when predicting “will user click on item X?”) creates models that work perfectly during training and fail catastrophically in production.\nPoint-in-time correctness requires that features used during training reflect only information available before the prediction event. This constraint is easy to state and deceptively difficult to enforce. Feature pipelines that aggregate over time windows, join multiple data sources, or depend on asynchronous updates can all violate point-in-time correctness in subtle ways.\nThe failure mode is insidious: models train well, validate well, and then underperform in production without clear explanation. Engineering teams often chase phantom bugs for weeks before identifying temporal leakage.",
    "crumbs": [
      "Volume II: Advanced",
      "Foundations of Scale",
      "Storage Systems for ML"
    ]
  },
  {
    "objectID": "contents/vol2/storage/storage.html#sec-storage-looking-forward",
    "href": "contents/vol2/storage/storage.html#sec-storage-looking-forward",
    "title": "Storage Systems for ML",
    "section": "Looking Forward",
    "text": "Looking Forward\nThe storage architectures developed in this chapter enable the distributed training systems examined in ?@sec-distributed-training. Understanding checkpoint storage is prerequisite to understanding how distributed training maintains progress across failures. Training data pipelines must integrate with parallelism strategies: data parallel training requires different data sharding than model parallel training.\nFeature stores connect to the inference systems covered in ?@sec-inference, where serving latency budgets constrain feature lookup time. Model registries interface with deployment pipelines and the operational concerns addressed in ?@sec-ops-scale.\nThe storage principles here, bandwidth-first planning, workload-appropriate consistency models, and tiered architectures matching access patterns to storage characteristics, apply throughout the distributed systems stack. These foundations prepare readers to understand how storage integrates with compute, networking, and orchestration to enable ML at scale.\n\n\n\n\n\n\nImportantKey Takeaways\n\n\n\n\nStorage bandwidth, not capacity, typically limits ML training throughput: systems must be designed to saturate accelerator memory bandwidth with training data, requiring careful attention to prefetching, data format optimization, and parallel I/O\nStorage requirements differ dramatically by model type: LLMs need massive text corpora and terabyte-scale checkpoints, recommendation systems require real-time feature stores with sub-millisecond latency, and vision models demand efficient image pipeline formats\nFeature stores are critical infrastructure for recommendation systems where feature lookup latency directly impacts serving time, but less relevant for LLMs where training data pipelines dominate\nCheckpoint storage strategy must balance recovery granularity against overhead: frequent checkpoints minimize lost work but consume I/O bandwidth, requiring tiered approaches with local NVMe for speed and distributed storage for durability",
    "crumbs": [
      "Volume II: Advanced",
      "Foundations of Scale",
      "Storage Systems for ML"
    ]
  },
  {
    "objectID": "contents/vol2/ops_scale/ops_scale.html#purpose",
    "href": "contents/vol2/ops_scale/ops_scale.html#purpose",
    "title": "ML Operations at Scale",
    "section": "Purpose",
    "text": "Purpose\nWhy do the operational practices that suffice for individual ML projects fail catastrophically when organizations deploy hundreds of models across distributed infrastructure?\nOperating machine learning systems at organizational scale introduces challenges fundamentally different from managing individual models: teams must coordinate across dozens of models with interdependent data pipelines, version artifacts across complex experimental workflows, and maintain reliability for systems where gradual degradation affects millions of users. The practices that enable a single team to iterate on one model become unsustainable when multiplied across an enterprise, requiring platform abstractions that provide self-service capabilities while maintaining governance and consistency. Organizations discover that operational excellence at scale demands new organizational structures, communication patterns, and engineering cultures alongside improved tooling. Understanding how platform engineering, multi-model management, and infrastructure-as-code practices address these challenges enables engineers to build ML operations that scale with organizational growth rather than becoming bottlenecks that constrain what teams can accomplish.\n\n\n\n\n\n\nTipLearning Objectives\n\n\n\nBy the end of this chapter, you will be able to:\n\nExplain why multi-model platform operations require fundamentally different approaches than scaling single-model MLOps practices\nAnalyze the economics of shared ML infrastructure and calculate platform ROI for organizations with diverse model portfolios\nDesign model registry architectures that handle ensemble dependencies, versioning, and lifecycle management across hundreds of models\nImplement CI/CD pipelines appropriate for different model types, including staged rollouts for LLMs, interleaving experiments for recommendation systems, and rapid iteration for fraud detection\nCalculate false alert rates at scale and design hierarchical monitoring systems that prevent alert fatigue while maintaining detection coverage\nEvaluate platform engineering abstractions that balance self-service capabilities with governance requirements for multi-tenant ML infrastructure\nArchitect feature store operations that maintain freshness SLOs, point-in-time correctness, and versioning across petabyte-scale feature repositories\nCompare organizational patterns for ML platform teams and recommend structures appropriate for different organizational contexts and model portfolios",
    "crumbs": [
      "Volume II: Advanced",
      "Deployment at Scale",
      "ML Operations at Scale"
    ]
  },
  {
    "objectID": "contents/vol2/ops_scale/ops_scale.html#sec-ops-scale-single-to-platform",
    "href": "contents/vol2/ops_scale/ops_scale.html#sec-ops-scale-single-to-platform",
    "title": "ML Operations at Scale",
    "section": "From Single-Model to Platform Operations",
    "text": "From Single-Model to Platform Operations\nThe transition from managing individual machine learning models to operating enterprise-scale ML platforms represents a fundamental shift in operational complexity. While ?@sec-ml-operations established the principles of MLOps for single-model systems, this chapter addresses the distinct challenges that emerge when organizations deploy tens, hundreds, or thousands of models across distributed infrastructure. The practices that enable a single team to successfully develop, deploy, and maintain one model become unsustainable when applied at scale, not because they are wrong, but because they fail to account for the interactions, dependencies, and coordination requirements that characterize multi-model environments.\nEvery organization that successfully deploys machine learning at scale discovers this transition point through experience. The first few models can be managed with spreadsheets, manual deployments, and ad hoc monitoring. Each model team develops its own practices, optimized for their specific requirements. This approach works initially because the models operate independently; what happens to the recommendation system does not affect the fraud detection model.\nBut this independence proves illusory as model count grows. Models begin sharing data sources, and changes to upstream data pipelines cascade through multiple consumers. Infrastructure becomes contested; deployment of one model delays deployment of another. Monitoring dashboards multiply until no single team can observe the complete system state. On-call rotations expand from single-model responsibility to cross-model coordination that requires understanding interactions between systems developed by different teams with different assumptions.\nThe economics of scale compound these challenges. A single model team might accept 20% GPU utilization because optimizing further is not worth the engineering investment. Multiply by 100 models, and that underutilization represents millions of dollars in wasted infrastructure. Similarly, a single model’s occasional production incident is manageable, but 100 models with independent failure modes produce a constant stream of alerts that exhaust on-call engineers and mask genuine emergencies beneath noise.\nThese challenges demand a fundamentally different approach. Platform thinking emerges as the organizational response. Rather than treating each model as an independent system with its own infrastructure, platforms provide shared services that amortize operational costs across the entire model portfolio. Feature stores eliminate redundant feature computation. Unified deployment pipelines ensure consistent rollout practices. Centralized monitoring aggregates signals across models to detect system-wide issues and enable capacity planning. This chapter examines how to design, implement, and operate these platforms.\n\nThe N-Models Problem\nConsider a typical technology organization’s journey with machine learning. The first model might be a recommendation system for the homepage, followed by a search ranking model, then a fraud detection system, then content moderation, and so on. Each model team initially operates independently, developing bespoke pipelines for data processing, training, validation, and deployment. This approach works well initially because each team can optimize for their specific requirements without coordination overhead.\nHowever, as the number of models grows, several problems emerge that are not simply multiplicative but combinatorial. The challenge is not that 100 models require 100 times the operational effort of one model. Rather, 100 models introduce dependencies and interactions that create superlinear growth in operational complexity.\n\n\n\nTable 1: Operational complexity growth as model count increases\n\n\n\n\n\n\n\n\n\n\n\nOperational Aspect\nSingle Model\n10 Models\n100 Models\n\n\n\n\nDeployment coordination\nNone\nAd hoc\nCritical path\n\n\nShared data dependencies\nNone\nSome overlap\nDense graph\n\n\nMonitoring dashboards\n1\n10\nUnmanageable\n\n\nOn-call rotation scope\nSingle team\nMultiple teams\nOrganization-wide\n\n\nInfrastructure utilization\nOften idle\nModerate sharing\nEfficiency critical\n\n\nDebugging complexity\nLocal\nCross-team\nDistributed tracing required\n\n\n\n\n\n\nThis table reveals the fundamental insight: per-model operational practices do not compose. When Model A depends on features computed by Pipeline B, which uses embeddings from Model C, changes to any component can cascade unpredictably. A seemingly innocuous update to Model C’s embedding layer might shift the feature distributions that Model A depends upon, degrading its performance even though Model A itself has not changed.\n\n\n\n\n\n\nNoteThe Complexity Explosion\n\n\n\nManaging 100 models is not 100 times the work of managing 1 model. It is fundamentally different due to dependencies, interactions, and organizational complexity. As Jeff Dean observes, the challenge shifts from individual model optimization to system-level coordination where the interactions between models often matter more than the models themselves.\n\n\n\n\nQuantifying Platform Economics\nThe economic case for platform operations rests on understanding both the costs of fragmented approaches and the returns from shared infrastructure. The platform return on investment can be quantified as:\n\\[ROI_{platform} = \\frac{N_{models} \\times T_{saved} \\times C_{engineer}}{C_{platform}} \\tag{1}\\]\nwhere \\(N_{models}\\) represents the number of models benefiting from the platform, \\(T_{saved}\\) is the engineering time saved per model per period, \\(C_{engineer}\\) is the fully-loaded cost per engineer hour, and \\(C_{platform}\\) is the total platform cost including development, infrastructure, and maintenance.\nThis equation reveals why platform investments make sense only at sufficient scale. For a small organization with five models, the denominator might exceed the numerator even with significant per-model savings. As model count grows, however, the numerator scales linearly with \\(N_{models}\\) while platform costs grow much more slowly, typically sublinearly due to infrastructure amortization.\nWorked Example: Platform ROI Calculation\nConsider an organization evaluating whether to build a centralized ML platform. Current state:\n\n50 production models across 8 teams\nEach model requires 40 engineer-hours monthly for operational tasks\nEngineers cost $150 per hour fully loaded\nPlatform development cost: $2 million over 18 months\nExpected time savings: 30 hours per model per month post-platform\n\nBefore platform (annual operational cost): \\[C_{current} = 50 \\times 40 \\times 12 \\times \\$150 = \\$3,600,000\\]\nAfter platform (annual operational cost plus amortized platform cost): \\[C_{after} = 50 \\times 10 \\times 12 \\times \\$150 + \\frac{\\$2,000,000}{3} = \\$900,000 + \\$667,000 = \\$1,567,000\\]\nThis yields annual savings of $2,033,000, representing a 56% reduction in operational costs. The platform pays for itself within the first year.\nThis analysis explains why large technology companies have invested heavily in ML platforms while smaller organizations often struggle to justify similar investments. The economic threshold typically falls between 20 and 50 models, depending on model complexity and organizational structure.\n\n\nHow Operations Differ at Scale\nThe operational requirements for multi-model platforms differ qualitatively, not just quantitatively, from single-model operations. Table 2 summarizes these distinctions:\n\n\n\nTable 2: Qualitative differences between single-model and platform operations\n\n\n\n\n\n\n\n\n\n\nAspect\nSingle-Model Operations\nMulti-Model Platform (100+)\n\n\n\n\nDeployment\nSimple rollout, team-controlled\nDependency-aware scheduling, platform-coordinated\n\n\nMonitoring\nModel-centric metrics\nSystem-centric with model aggregation\n\n\nDebugging\nLocal to model and data\nDistributed tracing across model boundaries\n\n\nResource Management\nDedicated allocation\nShared pools with multi-tenant isolation\n\n\nGovernance\nTeam-specific policies\nOrganization-wide standards and automation\n\n\nOrganization\nSingle team ownership\nPlatform team plus consumer teams\n\n\n\n\n\n\nDeployment Complexity\nThese differences manifest most clearly in deployment operations. Single-model deployment is straightforward: validate the new version, deploy to a canary, monitor for regressions, and proceed to full rollout. Platform-scale deployment must consider:\n\nDependency ordering: Models that consume features from other models cannot be updated independently\nRollback coordination: Reverting one model may require reverting dependent models\nResource contention: Multiple deployments competing for GPU memory or network bandwidth\nBlast radius management: Limiting the impact of any single deployment failure\n\nFor recommendation systems, this complexity is particularly acute. A typical recommendation request might involve 10 to 50 models executing in sequence or parallel: candidate retrieval models, ranking models, diversity filters, and business rule layers. Updating any component requires understanding its interactions with all others.\nMonitoring Evolution\nMonitoring requirements evolve similarly. At single-model scale, monitoring focuses on model-specific metrics: prediction accuracy, inference latency, data drift indicators. At platform scale, this approach becomes untenable. With 100 models, 100 independent dashboards create information overload that prevents effective incident response.\nPlatform monitoring must therefore aggregate across models while maintaining the ability to drill down into specifics. This requires hierarchical metrics:\n\nBusiness metrics: Overall system health (revenue, engagement, user satisfaction)\nPortfolio metrics: Aggregate model performance by domain or business unit\nModel metrics: Individual model accuracy, latency, drift\nInfrastructure metrics: GPU utilization, memory pressure, network throughput\n\nEffective platforms present high-level dashboards by default and enable investigation into lower levels only when anomalies are detected.\n\n\nModel-Type Operations Diversity\nBeyond scale considerations, different model types require fundamentally different operational patterns. The practices appropriate for deploying a large language model are entirely inappropriate for a fraud detection system, and vice versa.\n\n\n\nTable 3: Operational patterns vary dramatically by model type\n\n\n\n\n\n\n\n\n\n\n\n\nModel Type\nUpdate Frequency\nDeployment Pattern\nPrimary Risk\nRollback Speed\n\n\n\n\nLLMs\nMonthly to quarterly\nStaged, careful\nQuality regression, safety\nHours to days\n\n\nRecommendation\nDaily to weekly\nShadow, interleaving\nEngagement drop\nMinutes\n\n\nFraud Detection\nHourly to daily\nRapid with instant rollback\nFalse negatives\nSeconds\n\n\nVision (Classification)\nWeekly to monthly\nCanary\nAccuracy regression\nMinutes\n\n\nSearch Ranking\nDaily\nA/B with holdout\nRelevance degradation\nMinutes\n\n\n\n\n\n\nLLM Operations\nThese variations reflect fundamentally different risk profiles and operational constraints. Large language models present unique operational challenges due to their size, cost, and potential for subtle quality regressions. A minor degradation in response quality might not appear in automated metrics but could significantly impact user satisfaction. Consequently, LLM updates typically involve:\n\nExtended shadow deployment periods where new versions serve traffic without affecting users\nHuman evaluation alongside automated metrics\nStaged rollouts over days or weeks rather than hours\nExtensive safety evaluation before any production exposure\n\nThe operational cadence for LLMs is measured in weeks to months, with each update treated as a significant event requiring cross-functional coordination.\nRecommendation System Operations\nRecommendation systems operate at the opposite end of the operational spectrum. User preferences shift continuously, new content arrives constantly, and the systems must adapt rapidly to remain relevant. A recommendation system that cannot update for a month will show measurable engagement degradation.\nIn response to these dynamics, operational patterns for recommendation systems emphasize:\n\nContinuous training pipelines that produce daily or weekly model updates\nInterleaving experiments that compare multiple model variants on the same requests\nRapid iteration cycles where changes can reach production within hours\nSophisticated A/B testing infrastructure with statistical rigor\n\nThe key insight is that recommendation operations is fundamentally about ensemble management. A single recommendation request might invoke 10 to 50 distinct models, each requiring its own update cadence while maintaining coherent behavior as a system.\nFraud Detection Operations\nFraud detection systems face yet another distinct set of operational challenges. Adversarial dynamics impose unique requirements. Fraudsters actively probe systems to find exploits, then rapidly shift tactics once detected. A fraud model that cannot adapt within hours provides a window of vulnerability.\nThese adversarial dynamics dictate operational requirements:\n\nHourly or more frequent model updates in response to emerging patterns\nInstant rollback capability when false positive rates spike\nShadow scoring of all transactions for rapid model comparison\nFeature velocity monitoring to detect sudden distribution shifts\n\nThe risk profile is asymmetric: false negatives (missed fraud) cause direct financial losses, while false positives (legitimate transactions blocked) cause customer friction. Operations must balance these competing concerns in real time.\n\n\nThe MLOps Maturity Hierarchy\nOrganizations progress through distinct maturity levels as their ML operations capabilities develop. This progression parallels capability maturity models in software engineering but addresses ML-specific challenges.\n\n\n\nTable 4: MLOps maturity levels\n\n\n\n\n\n\n\n\n\n\n\n\nLevel\nScope\nPractices\nAutomation\nTypical Organization\n\n\n\n\n0\nManual\nAd hoc scripts, manual deployment\nNone\nEarly ML adoption\n\n\n1\nPer-Model\nCI/CD per model, basic monitoring\nPer-model pipelines\nGrowing ML practice\n\n\n2\nPlatform\nShared infrastructure, standardized tools\nPlatform-level\nMature ML organization\n\n\n3\nEnterprise\nGovernance, multi-team coordination\nOrganization-wide\nML-native companies\n\n\n\n\n\n\nLevel 0: Manual Operations\nAt Level 0, machine learning operates through manual processes. Data scientists train models in notebooks, export artifacts manually, and coordinate with operations teams for deployment. Monitoring consists of periodic manual checks, and retraining happens when someone notices performance degradation.\nThis level is appropriate for proof-of-concept projects and organizations with one or two models. However, it becomes unsustainable rapidly. Organizations typically transition to Level 1 after their first production incident caused by operational gaps.\nLevel 1: Per-Model Automation\nLevel 1 introduces automation but scopes it to individual models. Each model team develops its own CI/CD pipeline, monitoring dashboard, and retraining triggers. This works reasonably well for organizations with up to 10 to 20 models operated by distinct teams.\nThe limitation of Level 1 is duplication and inconsistency. Each team reinvents solutions to common problems, leading to varied quality and difficulty coordinating across models. When models begin to interact through shared features or cascading predictions, Level 1 practices strain under the coordination requirements.\nLevel 2: Platform Operations\nLevel 2 centralizes common capabilities into a platform that model teams consume. The platform provides standardized interfaces for model registration, deployment, monitoring, and feature management. Individual teams focus on model development while platform teams manage operational infrastructure.\nThis level requires significant investment but enables scale. Organizations at Level 2 typically operate 50 to 200 models with 10 to 50 model developers supported by a platform team of 5 to 15 engineers. The platform provides self-service capabilities while enforcing consistency and enabling cross-model coordination.\nLevel 3: Enterprise Operations\nLevel 3 extends platform capabilities with organization-wide governance, sophisticated multi-team coordination, and strategic resource allocation. At this level, ML operations becomes a strategic capability rather than a tactical necessity.\nCharacteristics of Level 3 include:\n\nAutomated governance enforcement across all models\nOrganization-wide A/B testing infrastructure with statistical guardrails\nStrategic capacity planning for ML infrastructure\nML-specific incident management and on-call practices\nCross-functional coordination with legal, compliance, and business stakeholders\n\nMost organizations are at Level 1. This chapter teaches the principles and practices required to progress to Levels 2 and 3, where platform operations provide superlinear returns on infrastructure investment.\n\n\nPlatform Team Justification\nEstablishing a dedicated ML platform team requires organizational commitment and clear justification. The decision involves both quantitative factors (cost savings, velocity improvements) and qualitative factors (consistency, governance, talent retention).\nQuantitative Justification\nThe ROI calculation presented earlier provides the primary quantitative argument. Additional quantitative benefits include:\nInfrastructure efficiency: Shared GPU clusters achieve 70-80% utilization versus 30-40% for dedicated per-team resources. For an organization with 100 GPUs at $2 per GPU-hour, moving from 35% to 75% effective utilization saves approximately $700,000 annually.\nTime to production: Platform abstractions reduce the time from trained model to production deployment. Organizations report reductions from weeks to days or hours. If this acceleration enables one additional high-value model to reach production per quarter, the business value typically exceeds platform costs.\nIncident reduction: Standardized deployments and monitoring reduce production incidents. Industry data suggests that mature platforms reduce ML-related incidents by 60-80%, translating to both direct cost savings and improved user experience.\nQualitative Justification\nBeyond quantitative metrics, platform teams provide qualitative benefits:\nConsistency: Standardized practices ensure that all models meet baseline quality standards for monitoring, rollback capability, and documentation.\nKnowledge sharing: Centralized teams accumulate operational expertise that benefits all model teams rather than remaining siloed.\nCareer development: Platform roles provide career paths for ML engineers interested in infrastructure, improving retention.\nGovernance readiness: As regulatory requirements for AI increase, platform-level controls provide the foundation for compliance.\nThe decision to establish a platform team typically occurs when organizations recognize that the alternative, allowing fragmentation to continue, imposes costs exceeding the platform investment. This recognition often follows a significant production incident that revealed cross-model dependencies or operational gaps.\n\n\n\n\n\n\nImportantKey Insight\n\n\n\nPlatform operations provide superlinear returns on investment. As model count grows, the value of shared infrastructure increases faster than its cost, creating increasingly favorable economics for platform investments. Organizations that delay platform investment accumulate operational debt that becomes progressively more expensive to address.",
    "crumbs": [
      "Volume II: Advanced",
      "Deployment at Scale",
      "ML Operations at Scale"
    ]
  },
  {
    "objectID": "contents/vol2/ops_scale/ops_scale.html#sec-ops-scale-multi-model",
    "href": "contents/vol2/ops_scale/ops_scale.html#sec-ops-scale-multi-model",
    "title": "ML Operations at Scale",
    "section": "Multi-Model Management",
    "text": "Multi-Model Management\nManaging multiple machine learning models in production introduces coordination challenges absent from single-model operations. When models share features, feed predictions into one another, or compete for shared infrastructure resources, their individual behaviors become interdependent. This section examines the systems and practices required to manage model portfolios effectively, with particular attention to the ensemble architectures that characterize recommendation systems and other multi-model deployments.\n\nModel Registries at Scale\nEffective multi-model management begins with proper artifact tracking. A model registry serves as the central catalog for all machine learning artifacts in an organization. While basic registries track model versions and metadata, enterprise-scale registries must support hundreds of models with complex interdependencies.\nCore Registry Requirements\nAn effective model registry provides:\nVersion management: Every model artifact receives a unique version identifier. The registry tracks the lineage of each version, including the training data, hyperparameters, code commit, and evaluation metrics that produced it.\nMetadata storage: Beyond the model weights, registries store extensive metadata: training configuration, evaluation results, hardware requirements, serving configuration, and ownership information.\nArtifact storage: Model binaries must be stored durably and retrieved efficiently. Large models (LLMs can exceed 100GB) require distributed storage with caching at serving locations.\nAccess control: Different teams require different permissions. Model developers need read-write access to their models; platform operators need administrative access; other teams may need read-only access for dependencies.\nDependency Tracking\nBeyond these core requirements, the distinguishing feature of enterprise registries is explicit dependency tracking. When Model A consumes features computed by Model B, this relationship must be recorded and enforced.\nThe necessity of dependency tracking becomes clear when considering a recommendation system where:\n\nEmbedding Model E produces user and item embeddings\nRetrieval Model R uses embeddings from E to generate candidates\nRanking Models R1, R2, R3 score candidates using embeddings from E\nEnsemble Model M combines outputs from R1, R2, R3\n\nThis dependency graph must be explicit in the registry. When Embedding Model E is updated, the registry should:\n\nIdentify all dependent models (R, R1, R2, R3, M)\nTrigger re-evaluation of dependent models with new embeddings\nBlock deployment of the new E until compatibility is verified\nCoordinate deployment order if updates proceed\n\nWithout explicit dependency tracking, organizations discover dependencies through production failures when an upstream model change breaks downstream consumers.\nRegistry Schema Example\nA registry entry might include:\nmodel:\n  name: user_embedding_v3\n  version: \"3.2.1\"\n  type: embedding_model\n  domain: recommendation\n\nartifact:\n  path: gs://models/user_embedding_v3/3.2.1/\n  format: tensorflow_savedmodel\n  size_bytes: 4294967296\n\ntraining:\n  data_version: user_interaction_2024_01\n  code_commit: abc123def\n  started_at: 2024-01-15T10:00:00Z\n  duration_hours: 48\n  hardware: 8xA100-80GB\n\nevaluation:\n  metrics:\n    recall_at_100: 0.342\n    embedding_quality: 0.891\n  evaluation_set: eval_2024_01\n\ndependencies:\n  upstream:\n    - feature_store/user_features_v2\n    - feature_store/interaction_features_v1\n  downstream:\n    - models/candidate_retrieval_v4\n    - models/ranking_ensemble_v2\n\nserving:\n  min_replicas: 10\n  max_replicas: 100\n  latency_p99_target_ms: 5\n  memory_gb: 16\n\nownership:\n  team: recommendation-core\n  oncall: recsys-oncall@company.com\n\n\nEnsemble Management\nRecommendation systems exemplify the multi-model management challenge because they operate as ensembles of 10 to 50 models per request. Understanding ensemble management is essential for operating these systems effectively.\nWhy Ensembles Dominate Recommendation\nModern recommendation systems use ensemble architectures for several reasons:\nDiverse objectives: A single model cannot optimize for engagement, diversity, freshness, and business constraints simultaneously. Separate models specialize in each objective, and an ensemble combines their outputs.\nStaged filtering: Processing billions of candidates with a single model is computationally infeasible. Multi-stage architectures progressively filter candidates: retrieval (billions to thousands), coarse ranking (thousands to hundreds), fine ranking (hundreds to tens), re-ranking (final ordering).\nExperimentation velocity: Ensemble architectures allow updating individual components without retraining the entire system. Teams can iterate on specific models while others remain stable.\nRisk management: If one model fails or produces poor results, others can compensate. Ensemble architectures provide natural resilience.\nEnsemble Deployment Patterns\nDeploying ensemble updates requires coordination that single-model deployments do not. Consider updating the fine ranking model within a recommendation ensemble:\n\n\n\nTable 5: Staged deployment for ensemble component updates\n\n\n\n\n\n\n\n\n\n\n\nDeployment Stage\nActions\nDuration\nRollback Trigger\n\n\n\n\nShadow\nNew model scores alongside production, results logged but not served\n24-48 hours\nQuality metrics below threshold\n\n\nCanary\n1% traffic receives new model results\n4-8 hours\nStatistical significance of regression\n\n\nStaged Rollout\n5% → 25% → 50% → 100%\n24-72 hours\nBusiness metric degradation\n\n\nSoak\nFull traffic, extended monitoring\n7-14 days\nDelayed effects emerge\n\n\n\n\n\n\nThe extended timeline reflects the difficulty of detecting regressions in ensemble systems. A component change that improves its local metrics might degrade system-level performance through subtle interactions with other components.\nInteraction Effects\nEnsemble components interact in complex ways that complicate operations. Common interaction patterns include:\nCompensation effects: If the retrieval model starts returning lower-quality candidates, the ranking model may learn to compensate by upweighting quality signals. When retrieval is fixed, ranking over-compensates, degrading results.\nDistribution shift propagation: Updating an upstream model changes the distribution of inputs to downstream models. Even if the upstream model improves, downstream models trained on the old distribution may degrade.\nFeedback loops: Ranking decisions affect which items users interact with, which becomes training data for future models. Changes propagate through this feedback loop over days to weeks.\nManaging these interactions requires:\n\nHoldout groups that experience no changes, providing stable baselines\nExtensive logging of intermediate model outputs, not just final recommendations\nLong-term monitoring (weeks to months) for feedback loop effects\nPeriodic “ensemble reset” experiments that retrain all components together\n\n\n\nModel Lifecycle Management\nModels progress through distinct lifecycle stages, each with different operational requirements.\nDevelopment → Staging → Canary → Production → Deprecation → Archive\nDevelopment Stage\nIn development, models exist as experimental artifacts. Operations requirements are minimal: storage of experimental results, basic version tracking, reproducibility for successful experiments.\nThe operational concern at this stage is ensuring that promising models can transition to staging. This requires:\n\nClear criteria for production readiness\nAutomated evaluation against production-equivalent data\nDocumentation requirements before staging promotion\n\nStaging Stage\nStaging provides a production-like environment for pre-deployment validation. Models in staging should:\n\nProcess production traffic in shadow mode (predictions logged but not served)\nRun against production feature pipelines\nExecute on production-equivalent hardware\nMeet latency and throughput requirements\n\nThe staging to production gate often involves both automated checks (metrics thresholds, latency requirements) and human review (model behavior analysis, risk assessment).\nProduction Stage\nProduction models serve live traffic and require full operational support:\n\nContinuous monitoring with alerting\nCapacity for traffic fluctuations\nRollback procedures\nOn-call support\n\nProduction is not a terminal state. Models require ongoing maintenance:\n\nRegular retraining as data distributions shift\nFeature pipeline updates as upstream data changes\nInfrastructure updates as serving systems evolve\nPeriodic re-evaluation against newer baseline models\n\nDeprecation and Archive\nModels eventually become obsolete as better alternatives emerge or business requirements change. Deprecation involves:\n\nIdentifying dependent systems that must migrate\nProviding migration path and timeline to consumers\nMaintaining the old model until migration completes\nArchiving artifacts for reproducibility and audit purposes\n\nOrganizations often underinvest in deprecation, leading to accumulation of zombie models that consume resources but provide questionable value. Platform-level lifecycle enforcement helps address this pattern.\n\n\nDeployment Patterns by Model Count\nThe appropriate deployment pattern depends on the number and interdependence of models being updated.\n\n\n\nTable 6: Deployment patterns by model count and update frequency\n\n\n\n\n\nPattern\nModel Count\nUpdate Frequency\nExample\n\n\n\n\nSingle Model\n1\nMonthly\nVision classifier\n\n\nPipeline\n3-5\nWeekly\nNLP processing pipeline\n\n\nEnsemble\n10-50\nDaily\nRecommendation system\n\n\nPlatform\n100s\nContinuous\nEnterprise ML platform\n\n\n\n\n\n\nSingle Model Deployment\nFor isolated models with no dependencies, standard deployment patterns suffice. Canary deployments, blue-green switches, and gradual rollouts all work effectively.\nPipeline Deployment\nPipelines involve models that execute in sequence, where each model’s output feeds the next. Deployment must respect this ordering:\n\nDeploy models in dependency order (upstream before downstream)\nValidate each stage before proceeding\nMaintain version compatibility between stages\nRoll back as a unit if any stage fails\n\nEnsemble Deployment\nEnsemble deployment coordinates multiple models that may execute in parallel or in complex graphs. Key considerations:\n\nModels may be developed by different teams with different schedules\nPartial updates (changing some components) are common\nSystem behavior emerges from component interactions\nTesting in isolation is insufficient; integration testing is essential\n\nPlatform Deployment\nAt platform scale, continuous deployment means some model is always being updated somewhere. Platform deployment requires:\n\nAutomated rollout policies based on model risk classification\nCross-model impact analysis before deployment approval\nGlobal rate limiting to prevent simultaneous high-risk deployments\nAutomated correlation of incidents with recent deployments\n\n\n\nCross-Model Dependencies in Practice\nDependencies between models create operational complexity that requires explicit management. Consider a concrete example from e-commerce:\nExample: E-Commerce Model Ecosystem\nAn e-commerce platform might operate the following models:\n\nUser Embedding Model: Generates user representations from behavior history\nProduct Embedding Model: Generates product representations from attributes and interactions\nCandidate Retrieval Model: Uses embeddings to retrieve relevant products\nPrice Sensitivity Model: Predicts user sensitivity to pricing\nRanking Model: Scores candidates using embeddings and auxiliary models\nDiversity Model: Adjusts rankings for result diversity\nBusiness Rules Model: Applies promotional and inventory constraints\n\nThe dependency graph reveals operational implications:\nUser Embedding ─┬──────────────────────────────┐\n                │                              │\n                ├─► Candidate Retrieval ──────►│\n                │                              │\nProduct Embed. ─┴─► Price Sensitivity ────────►├─► Ranking ─► Diversity ─► Business Rules\n                                               │\n                                               │\nUpdating User Embedding affects four downstream models. Operational procedures must:\n\nRe-evaluate all downstream models with new embeddings before deployment\nConsider simultaneous deployment of related components\nMonitor both direct metrics (embedding quality) and downstream metrics (ranking performance)\nMaintain embedding version compatibility or coordinate synchronized updates\n\nThis example illustrates why multi-model management requires explicit dependency tracking and coordinated deployment procedures.",
    "crumbs": [
      "Volume II: Advanced",
      "Deployment at Scale",
      "ML Operations at Scale"
    ]
  },
  {
    "objectID": "contents/vol2/ops_scale/ops_scale.html#sec-ops-scale-cicd",
    "href": "contents/vol2/ops_scale/ops_scale.html#sec-ops-scale-cicd",
    "title": "ML Operations at Scale",
    "section": "CI/CD for ML at Scale",
    "text": "CI/CD for ML at Scale\nContinuous integration and continuous deployment practices for machine learning differ fundamentally from traditional software CI/CD. While software CI/CD focuses on code correctness and deployment reliability, ML CI/CD must additionally validate data, verify model performance, and manage the complex interactions between code, data, and learned parameters. At platform scale, these challenges multiply as pipelines must coordinate across hundreds of models with varying requirements.\n\nTraining Pipeline Automation\nCI/CD for machine learning begins with automation of the training process itself. Automated training pipelines form the foundation of ML CI/CD. A well-designed training pipeline executes reproducibly, handles failures gracefully, and produces artifacts suitable for deployment validation.\nPipeline Stages\nA complete training pipeline includes:\n\nData Validation: Verify input data meets schema requirements and statistical expectations\nFeature Engineering: Transform raw data into model inputs, ensuring consistency with serving\nTraining: Execute model training with tracked hyperparameters\nEvaluation: Compute metrics on held-out data\nArtifact Generation: Package model with serving configuration\nRegistration: Record artifact in model registry with full lineage\n\nEach stage should be independently executable and idempotent. If the pipeline fails at evaluation, restarting should not re-execute data validation and feature engineering unless their inputs have changed.\nPipeline Orchestration\nTraining pipelines require orchestration systems that handle:\n\nDAG execution with dependency tracking\nRetry policies for transient failures\nResource allocation (GPU scheduling, memory management)\nCaching of intermediate results\nLogging and artifact storage\n\nCommon orchestration choices include Kubeflow Pipelines, Airflow with ML extensions, and cloud-native solutions like Vertex AI Pipelines or SageMaker Pipelines. The choice depends on existing infrastructure, team expertise, and scale requirements.\nPipeline Parameterization\nEffective pipelines separate configuration from code:\ntraining_pipeline:\n  model_type: transformer_ranking\n  data:\n    train_path: gs://data/train/2024-01-*\n    eval_path: gs://data/eval/2024-01-15\n    schema_version: v3.2\n  features:\n    user_features: [embedding, history, demographics]\n    item_features: [embedding, attributes, popularity]\n  training:\n    epochs: 10\n    batch_size: 4096\n    learning_rate: 0.001\n    optimizer: adam\n    hardware: 4xA100\n  evaluation:\n    metrics: [ndcg@10, mrr, coverage]\n    baseline_model: ranking_v2.1.0\nThis separation enables:\n\nRunning identical code with different data versions\nSystematic hyperparameter exploration\nClear reproducibility from configuration alone\nEnvironment-specific overrides (dev vs. production resources)\n\n\n\nValidation Gates\nValidation gates determine whether a trained model should proceed toward production. Effective gates balance thoroughness against deployment velocity.\nPerformance Gates\nPerformance validation compares the candidate model against:\n\nAbsolute thresholds: Model must exceed minimum acceptable performance\nRelative baselines: Model must match or exceed current production performance\nHistorical trends: Model should not regress from recent performance trajectory\n\ndef evaluate_performance_gate(\n    candidate_metrics, production_metrics, thresholds\n):\n    \"\"\"\n    Evaluate whether candidate model passes performance gates.\n\n    Returns tuple of (passed: bool, reasons: list)\n    \"\"\"\n    reasons = []\n\n    # Absolute threshold check\n    if candidate_metrics[\"ndcg@10\"] &lt; thresholds[\"min_ndcg\"]:\n        reasons.append(\n            f\"NDCG@10 {candidate_metrics['ndcg@10']:.4f} below minimum {thresholds['min_ndcg']}\"\n        )\n\n    # Relative improvement check\n    relative_improvement = (\n        candidate_metrics[\"ndcg@10\"] - production_metrics[\"ndcg@10\"]\n    ) / production_metrics[\"ndcg@10\"]\n    if relative_improvement &lt; thresholds[\"min_improvement\"]:\n        reasons.append(\n            f\"Improvement {relative_improvement:.2%} below minimum {thresholds['min_improvement']:.2%}\"\n        )\n\n    # Regression check on secondary metrics\n    for metric in [\"mrr\", \"coverage\"]:\n        if candidate_metrics[metric] &lt; production_metrics[metric] * (\n            1 - thresholds[\"max_regression\"]\n        ):\n            reasons.append(\n                f\"{metric} regression exceeds {thresholds['max_regression']:.2%} tolerance\"\n            )\n\n    return (len(reasons) == 0, reasons)\nLatency Gates\nProduction models must meet latency requirements. Validation should:\n\nMeasure inference latency on representative hardware\nTest at expected throughput levels\nVerify both p50 and p99 latency meet requirements\nAccount for batching effects if applicable\n\n\n\n\nTable 7: Latency gate thresholds by model type\n\n\n\n\n\nModel Type\np50 Target\np99 Target\nGate Action if Exceeded\n\n\n\n\nLLM\n500ms\n2000ms\nBlock deployment, require optimization\n\n\nRecommendation\n10ms\n50ms\nBlock deployment\n\n\nFraud Detection\n5ms\n20ms\nBlock deployment, high priority\n\n\nVision\n50ms\n200ms\nWarning, conditional approval\n\n\n\n\n\n\nFairness Gates\nFor models affecting users, fairness validation ensures equitable treatment across demographic groups:\n\\[\\text{Demographic Parity}: |P(\\hat{Y}=1|A=a) - P(\\hat{Y}=1|A=b)| &lt; \\epsilon \\tag{2}\\]\n\\[\\text{Equalized Odds}: |P(\\hat{Y}=1|Y=y, A=a) - P(\\hat{Y}=1|Y=y, A=b)| &lt; \\epsilon \\tag{3}\\]\nwhere \\(A\\) represents the protected attribute, \\(\\hat{Y}\\) is the model prediction, and \\(Y\\) is the true outcome.\nFairness gates should:\n\nEvaluate multiple fairness definitions (different contexts require different definitions)\nCompare against historical baselines, not just thresholds\nFlag improvements as well as regressions for review\nIntegrate with human review for borderline cases\n\nData Quality Gates\nBefore training or deployment, data quality validation ensures:\n\nSchema conformance: All required fields present with correct types\nStatistical properties: Feature distributions within expected bounds\nFreshness: Data not stale beyond acceptable thresholds\nCompleteness: Missing data rates within tolerance\n\nData quality gates catch issues that would otherwise manifest as mysterious model degradation.\n\n\nStaged Rollout Strategies\nDeploying models to production should proceed gradually, with increasing traffic exposure contingent on continued acceptable performance.\nBlue-Green Deployment\nBlue-green deployment maintains two identical production environments. The current version (blue) serves traffic while the new version (green) is prepared. Once ready, traffic switches instantaneously to green.\nAdvantages:\n\nSimple mental model\nInstant rollback (switch back to blue)\nFull testing in production-equivalent environment before exposure\n\nDisadvantages:\n\nRequires duplicate infrastructure during transition\nNo gradual exposure to detect subtle issues\nBinary switch may miss issues that emerge only at scale\n\nBlue-green is appropriate for low-risk changes or models where gradual rollout provides limited additional safety.\nCanary Deployment\nCanary deployment routes a small percentage of traffic to the new version while monitoring for regressions. If metrics remain acceptable, traffic percentage increases until the new version serves all traffic.\nTypical progression: 1% → 5% → 25% → 50% → 100%\nThe key question is: how long should each stage last?\n\\[t_{stage} = \\frac{n_{samples\\_needed}}{r_{requests} \\times p_{stage}} \\tag{4}\\]\nwhere \\(t_{stage}\\) is the duration required at a given percentage, \\(n_{samples\\_needed}\\) is the number of observations needed for statistical significance, \\(r_{requests}\\) is the request rate, and \\(p_{stage}\\) is the traffic percentage.\nWorked Example: Canary Duration Calculation\nA model serves 1 million requests per hour. To detect a 1% change in click-through rate with 95% confidence requires approximately 10,000 samples per variant.\nAt 1% canary traffic: \\[t_{1\\%} = \\frac{10,000}{1,000,000 \\times 0.01} = 1 \\text{ hour}\\]\nAt 5% canary traffic: \\[t_{5\\%} = \\frac{10,000}{1,000,000 \\times 0.05} = 0.2 \\text{ hours} = 12 \\text{ minutes}\\]\nThe organization might configure:\n\n1% for 2 hours (2x minimum for buffer)\n5% for 30 minutes\n25% for 30 minutes\n50% for 1 hour\n100% deployment\n\nTotal rollout: approximately 4 hours for a confident deployment.\nShadow Deployment\nShadow deployment runs the new model in parallel with production, receiving the same inputs and logging outputs, but not affecting user-visible results. This enables:\n\nComparison of new model outputs against current production\nDetection of unexpected behaviors before any user exposure\nPerformance measurement at production scale and traffic patterns\n\nShadow deployment is particularly valuable for high-risk changes: new model architectures, significant retraining, or models affecting sensitive decisions.\nInterleaving Experiments\nRecommendation systems use interleaving experiments for more efficient comparison than traditional A/B testing. Rather than splitting users between variants, interleaving presents items from both variants to each user, then measures which items users engage with.\nThe key insight is statistical efficiency. An interleaving experiment requires 10x to 100x fewer samples to detect the same effect size compared to A/B testing, because each user provides direct comparison signals rather than contributing to aggregate statistics.\nInterleaving implementation:\n\nBoth model variants score all candidates\nResults are interleaved using team draft or probabilistic interleaving\nUser interactions attribute credit to the originating variant\nStatistical tests determine winning variant\n\nThis pattern is essential for recommendation systems where detecting small engagement changes quickly enables rapid iteration.\n\n\nRollout Risk Management\nNot all deployments carry equal risk. Effective CI/CD systems classify and handle deployments based on their risk profile.\nRisk Classification\nThe risk of a deployment can be quantified as:\n\\[R_{rollout} = P_{regression} \\times I_{regression} \\times E_{exposure} \\tag{5}\\]\nwhere \\(P_{regression}\\) is the probability that the change causes a regression, \\(I_{regression}\\) is the impact severity if regression occurs, and \\(E_{exposure}\\) is the exposure level during the rollout period.\nThis framework suggests risk mitigation strategies:\n\nReduce \\(P_{regression}\\): More thorough testing before deployment\nReduce \\(I_{regression}\\): Architectural patterns that limit blast radius\nReduce \\(E_{exposure}\\): Slower rollouts with lower initial traffic percentages\n\nRisk Categories\n\n\n\nTable 8: Risk-based rollout strategy selection\n\n\n\n\n\n\n\n\n\n\n\nCategory\n\\(P_{regression}\\)\n\\(I_{regression}\\)\nRollout Strategy\n\n\n\n\nLow\nMinor code fix\nLimited user impact\nFast canary\n\n\nMedium\nRetrained model\nEngagement effects\nStandard canary\n\n\nHigh\nNew architecture\nRevenue impact\nExtended shadow + slow canary\n\n\nCritical\nCore model change\nSafety implications\nShadow + human review + staged\n\n\n\n\n\n\nAutomated Rollback Triggers\nRollback should be automated based on metric degradation:\nrollback_config = {\n    \"metrics\": {\n        \"engagement_rate\": {\n            \"threshold\": -0.02,  # 2% relative decline triggers rollback\n            \"window_minutes\": 15,\n            \"min_samples\": 1000,\n        },\n        \"error_rate\": {\n            \"threshold\": 0.01,  # 1% absolute increase triggers rollback\n            \"window_minutes\": 5,\n            \"min_samples\": 500,\n        },\n        \"latency_p99\": {\n            \"threshold\": 1.5,  # 50% relative increase triggers rollback\n            \"window_minutes\": 5,\n            \"min_samples\": 100,\n        },\n    },\n    \"rollback_action\": \"immediate\",  # or 'gradual' for less severe issues\n    \"notification\": [\"oncall\", \"model-owner\"],\n}\nAutomated rollback must balance sensitivity against false triggers. The statistical significance requirements (minimum samples, window duration) prevent premature rollback from random fluctuation while enabling rapid response to genuine regressions.\n\n\nCI/CD Patterns by Model Type\nDifferent model types require different CI/CD approaches, reflecting their distinct operational characteristics.\n\n\n\nTable 9: CI/CD patterns by model type\n\n\n\n\n\n\n\n\n\n\n\n\nPattern\nModel Type\nValidation Focus\nRollout Speed\nRollback Speed\n\n\n\n\nQuality-gated\nLLM\nHuman eval, safety\nDays to weeks\nHours\n\n\nMetric-driven\nRecommendation\nEngagement metrics\nHours to days\nMinutes\n\n\nThreshold-gated\nFraud\nPrecision/recall\nHours\nSeconds\n\n\nAccuracy-focused\nVision\nClassification metrics\nDays\nMinutes\n\n\n\n\n\n\nLLM CI/CD\nLarge language models require extended validation due to the difficulty of automated quality assessment:\n\nAutomated evaluation on benchmark datasets (MMLU, HumanEval, etc.)\nHuman evaluation on sample outputs across capability categories\nSafety evaluation (red teaming, toxicity detection)\nShadow deployment measuring user satisfaction signals\nSlow staged rollout with extended soak periods\n\nThe full cycle may take 2-4 weeks from candidate model to full deployment.\nRecommendation CI/CD\nRecommendation systems prioritize iteration velocity:\n\nAutomated evaluation on offline metrics (NDCG, recall)\nInterleaving experiment against production baseline\nStatistical significance testing on engagement metrics\nRapid canary with automated promotion/rollback\n\nThe full cycle may complete in 24-48 hours for routine updates.\nFraud Detection CI/CD\nFraud models balance quality validation against deployment urgency:\n\nAutomated evaluation on labeled fraud cases\nFalse positive rate validation on legitimate traffic sample\nShadow scoring with precision/recall analysis\nRapid deployment with instant rollback capability\n\nThe full cycle may complete in 4-12 hours, with ability to deploy emergency updates in under 1 hour when new fraud patterns emerge.",
    "crumbs": [
      "Volume II: Advanced",
      "Deployment at Scale",
      "ML Operations at Scale"
    ]
  },
  {
    "objectID": "contents/vol2/ops_scale/ops_scale.html#sec-ops-scale-monitoring",
    "href": "contents/vol2/ops_scale/ops_scale.html#sec-ops-scale-monitoring",
    "title": "ML Operations at Scale",
    "section": "Monitoring at Scale",
    "text": "Monitoring at Scale\nMonitoring machine learning systems at scale presents challenges fundamentally different from monitoring individual models. When an organization operates hundreds of models, the naive approach of applying single-model monitoring practices to each model independently leads to alert fatigue, missed correlations, and operational chaos. This section develops monitoring strategies appropriate for enterprise-scale ML platforms.\n\nThe Alert Fatigue Problem\nThe mathematical reality of monitoring at scale exposes the limitations of per-model alerting. Consider the mathematics of monitoring 100 models with independent alerting. If each model has 10 monitored metrics, and each metric generates alerts at a 5% false positive rate, the expected number of false alerts is substantial.\nFor a single metric with false positive rate \\(\\alpha\\), the probability of at least one false alert across \\(N\\) independent tests is:\n\\[P(\\text{at least one false alert}) = 1 - (1 - \\alpha)^N \\tag{6}\\]\nWith \\(\\alpha = 0.05\\) and \\(N = 1000\\) (100 models × 10 metrics):\n\\[P(\\text{false alert}) = 1 - (1 - 0.05)^{1000} = 1 - 0.95^{1000} \\approx 1.0\\]\nThe probability is essentially 100%. At this scale, the monitoring system will generate false alerts continuously. This creates a destructive dynamic: operators learn to ignore alerts because most are false, genuine issues get lost in the noise, and the monitoring system provides negative rather than positive value.\nWorked Example: Alert Volume Calculation\nAn ML platform monitors 100 models with the following configuration:\n\n10 metrics per model (accuracy, latency p50, latency p99, throughput, error rate, data freshness, feature drift, memory usage, GPU utilization, request volume)\nAlert threshold at 2 standard deviations (approximately 5% false positive rate per metric)\nMetrics checked every 5 minutes\n\nExpected daily false alerts: \\[\\text{Daily false alerts} = 100 \\times 10 \\times 0.05 \\times \\frac{24 \\times 60}{5} = 14,400\\]\nEven if 99% of these are deduplicated or auto-resolved, the remaining 144 alerts daily overwhelm any on-call team. The monitoring system becomes useless despite (or rather, because of) comprehensive coverage.\n\n\nHierarchical Monitoring Architecture\nThe alert fatigue problem demands a fundamentally different approach. The solution is hierarchical monitoring that presents different levels of detail to different audiences and aggregates signals to reduce alert volume while maintaining detection capability.\nLevel 1: Business Metrics\nThe highest monitoring level tracks business outcomes that ML systems affect:\n\nRevenue or conversion metrics attributed to ML recommendations\nUser engagement indicators (session length, return rate)\nOperational efficiency metrics (automation rate, human review volume)\n\nBusiness metric monitoring involves few metrics with high signal. Alerts at this level warrant immediate executive attention because they indicate significant business impact.\nLevel 2: Portfolio Metrics\nPortfolio metrics aggregate across groups of related models:\n\nRecommendation portfolio: Overall engagement lift, diversity metrics\nFraud portfolio: Total fraud caught, false positive rate\nContent moderation portfolio: Violation detection rate, appeal rate\n\nAggregation at this level reduces the number of monitored signals while maintaining actionability. A regression in portfolio metrics triggers investigation into constituent models.\nLevel 3: Model Metrics\nIndividual model metrics track the health of specific models:\n\nAccuracy/quality metrics specific to each model’s task\nLatency distribution (p50, p95, p99)\nThroughput and error rates\nResource utilization\n\nModel-level alerts should be rare, triggered only by significant deviations, because investigation happens at this level when higher-level metrics indicate problems.\nLevel 4: Infrastructure Metrics\nInfrastructure metrics track the systems supporting ML operations:\n\nGPU cluster utilization and availability\nFeature store latency and throughput\nTraining pipeline execution times\nServing cluster health\n\nInfrastructure alerts typically route to platform teams rather than model teams.\n\n\nAnomaly Detection Across the Fleet\nRather than alerting on individual metric thresholds, fleet-wide anomaly detection identifies unusual patterns across the model portfolio.\nStatistical Process Control\nControl charts adapted for ML monitoring track whether metric distributions remain stable over time. The core idea is distinguishing common cause variation (normal fluctuation) from special cause variation (genuine anomalies).\nFor a metric \\(X\\) with established mean \\(\\mu\\) and standard deviation \\(\\sigma\\):\n\nUpper Control Limit: \\(UCL = \\mu + 3\\sigma\\)\nLower Control Limit: \\(LCL = \\mu - 3\\sigma\\)\n\nPoints outside control limits or systematic patterns (7 consecutive points above/below mean) trigger investigation.\nFleet-Wide Correlation\nWhen multiple models exhibit similar anomalies simultaneously, the root cause is likely shared infrastructure or data rather than individual model issues. Correlation analysis across models enables:\n\nAutomatic attribution of anomalies to likely causes (deployment, data issue, infrastructure)\nDeduplication of alerts that have common causes\nPrioritization based on breadth of impact\n\ndef detect_fleet_anomaly(model_metrics, threshold=0.6):\n    \"\"\"\n    Detect correlated anomalies across model fleet.\n\n    Returns list of (timestamp, affected_models, likely_cause) tuples.\n    \"\"\"\n    anomalies = []\n\n    for timestamp in model_metrics.timestamps:\n        # Identify models with anomalous metrics at this time\n        anomalous_models = []\n        for model in model_metrics.models:\n            if is_anomalous(model_metrics[model][timestamp]):\n                anomalous_models.append(model)\n\n        # Check if anomaly fraction exceeds correlation threshold\n        if (\n            len(anomalous_models) / len(model_metrics.models)\n            &gt; threshold\n        ):\n            # Many models affected -&gt; likely shared cause\n            cause = attribute_to_shared_cause(\n                timestamp, anomalous_models\n            )\n            anomalies.append((timestamp, anomalous_models, cause))\n\n    return anomalies\nDrift Detection\nData drift represents gradual shifts in input distributions that degrade model performance over time. Detecting drift requires statistical tests that compare current distributions against reference distributions.\nFor continuous features, the Population Stability Index (PSI) quantifies distribution shift:\n\\[PSI = \\sum_{i=1}^{n} (A_i - E_i) \\times \\ln\\left(\\frac{A_i}{E_i}\\right) \\tag{7}\\]\nwhere \\(A_i\\) is the proportion in bucket \\(i\\) of the actual (current) distribution, \\(E_i\\) is the proportion in bucket \\(i\\) of the expected (reference) distribution, and \\(n\\) is the number of buckets.\nInterpretation:\n\nPSI &lt; 0.1: No significant shift\n0.1 ≤ PSI &lt; 0.25: Moderate shift, investigation recommended\nPSI ≥ 0.25: Significant shift, action required\n\nFleet-wide drift monitoring tracks PSI for critical features across all models, alerting when drift affects multiple models or critical features.\n\n\nModel-Type Specific Monitoring\nDifferent model types require different monitoring strategies, reflecting their distinct failure modes and operational requirements.\n\n\n\nTable 10: Model-type specific monitoring parameters\n\n\n\n\n\n\n\n\n\n\n\nModel Type\nPrimary Metrics\nAlert Thresholds\nMonitoring Frequency\n\n\n\n\nRecommendation\nCTR, engagement lift\n5% relative drop\nReal-time\n\n\nFraud Detection\nPrecision, recall, fraud rate\n1% degradation\nReal-time\n\n\nLLM\nQuality scores, safety metrics\nPer-model calibration\nHourly\n\n\nVision\nAccuracy by class\nDataset-specific\nDaily\n\n\nSearch Ranking\nNDCG, click position\n2% degradation\nReal-time\n\n\n\n\n\n\nRecommendation System Monitoring\nRecommendation systems require real-time monitoring because their impact is immediately visible in user engagement:\nEngagement metrics: Click-through rate, dwell time, conversion rate attributed to recommendations. These metrics should be compared against:\n\nHistorical baseline for the same time period (day of week, hour of day)\nControl group receiving non-ML recommendations (if available)\nPrevious model version for recently deployed changes\n\nDiversity metrics: Recommendation diversity, coverage of catalog, filter bubble indicators. Optimization for engagement can inadvertently reduce diversity, creating long-term user experience issues.\nBusiness metrics: Revenue attributed to recommendations, promotional inventory utilization, cross-selling effectiveness.\nFraud Detection Monitoring\nFraud monitoring must balance detection rate against false positive rate, with real-time alerting because missed fraud causes immediate financial loss:\nDetection metrics: Fraud caught rate, dollar amount prevented, detection latency (time from fraudulent action to detection).\nFalse positive metrics: False positive rate, customer friction events (blocked legitimate transactions), manual review volume.\nAdversarial indicators: Unusual probing patterns, exploit attempts, distribution shifts in fraudulent behavior.\nLLM Monitoring\nLLM quality is difficult to assess automatically, requiring hybrid approaches:\nAutomated metrics: Response latency, token generation rate, error rates, safety classifier scores.\nQuality signals: User satisfaction indicators (thumbs up/down, regeneration rate), task completion proxies.\nSafety metrics: Toxicity detection, refusal rate, hallucination indicators (where detectable).\nLLM monitoring often includes delayed human evaluation: sampling outputs for manual review to detect issues automated metrics miss.\n\n\nObservability Architecture\nEffective monitoring requires observability infrastructure that captures, stores, and enables analysis of operational data.\nMetrics Collection\nMetrics should be collected at multiple granularities:\n\nReal-time streaming: For alerting and dashboards (resolution: seconds to minutes)\nAggregated time series: For trend analysis and capacity planning (resolution: minutes to hours)\nRaw logs: For detailed investigation (retained for days to weeks)\n\nDistributed Tracing\nIn multi-model systems, a single user request may traverse multiple models. Distributed tracing tracks requests across model boundaries, enabling:\n\nEnd-to-end latency decomposition\nCross-model dependency analysis\nRoot cause identification when multi-model interactions fail\n\nEach request receives a trace ID propagated across all model invocations. Traces capture timing, inputs, outputs, and resource usage for each component.\nLog Aggregation\nCentralized log aggregation enables correlation of events across the model fleet:\n\nStructured logging with consistent schema across models\nIndexed search for rapid investigation\nAnomaly detection on log patterns (unusual error rates, new error types)\n\nPrediction Logging\nFor detailed model analysis, logging predictions enables:\n\nOffline accuracy assessment against delayed labels\nTraining data generation for model updates\nDebugging specific prediction failures\n\nPrediction logging generates substantial data volume. Sampling strategies (log 1% of predictions, log all predictions for specific users) balance storage cost against analysis capability.\n\n\nDashboard Design\nDashboards translate monitoring data into actionable information. Effective ML platform dashboards follow consistent design principles.\nExecutive Dashboard\nA single-page view showing:\n\nOverall platform health (green/yellow/red)\nBusiness impact summary (revenue attribution, engagement trends)\nActive incidents and ongoing deployments\nKey trends requiring attention\n\nPortfolio Dashboard\nPer-domain views showing:\n\nModel inventory and health summary\nPortfolio-level metrics with trends\nRecent deployments and their impact\nResource utilization and cost\n\nModel Dashboard\nDetailed per-model views showing:\n\nCurrent metrics versus historical baselines\nDeployment history and rollback points\nFeature importance and drift indicators\nResource consumption and cost attribution\n\nInvestigation Dashboard\nInteractive analysis tools for incident response:\n\nCross-model correlation analysis\nTime-series overlay for root cause identification\nLog search integrated with metric views\nTrace exploration for request-level debugging",
    "crumbs": [
      "Volume II: Advanced",
      "Deployment at Scale",
      "ML Operations at Scale"
    ]
  },
  {
    "objectID": "contents/vol2/ops_scale/ops_scale.html#sec-ops-scale-platform",
    "href": "contents/vol2/ops_scale/ops_scale.html#sec-ops-scale-platform",
    "title": "ML Operations at Scale",
    "section": "Platform Engineering",
    "text": "Platform Engineering\nPlatform engineering for machine learning creates shared infrastructure that enables model teams to develop, deploy, and operate models without managing underlying complexity. Effective platforms balance self-service capabilities that accelerate development against governance requirements that ensure consistency and reliability.\n\nAbstraction Levels\nML platforms can operate at different abstraction levels, each representing different tradeoffs between flexibility and convenience.\nLevel 1: Bare Infrastructure\nAt the lowest level, platforms provide access to raw compute resources:\n\nGPU allocations\nStorage volumes\nNetwork connectivity\nBasic orchestration (Kubernetes namespaces)\n\nModel teams handle all ML-specific concerns: training code, serving infrastructure, monitoring, and deployment. This level offers maximum flexibility but requires deep infrastructure expertise on every model team.\nLevel 2: Container Orchestration\nThe next level adds containerization and orchestration:\n\nStandardized container images for common frameworks\nKubernetes integration with ML-aware scheduling\nPersistent volume management for datasets and artifacts\nBasic service mesh for model-to-model communication\n\nModel teams package their code in containers but manage ML-specific workflows independently. This level reduces infrastructure burden while maintaining flexibility.\nLevel 3: ML-Aware Scheduling\nSpecialized ML orchestration adds:\n\nTraining job scheduling with GPU awareness\nHyperparameter tuning infrastructure\nDistributed training coordination\nModel serving with autoscaling\n\nPlatforms at this level include Kubeflow, Ray, and similar frameworks. Model teams focus on model code while the platform handles operational complexity.\nLevel 4: Full Platform\nComplete ML platforms provide end-to-end capabilities:\n\nIntegrated development environments\nFeature store integration\nExperiment tracking and model registry\nAutomated CI/CD for models\nMonitoring and alerting\nCost attribution and governance\n\nPlatforms at this level include Vertex AI, SageMaker, and internal platforms at major technology companies. Model teams interact through high-level APIs while the platform manages all operational concerns.\n\n\nSelf-Service Model Deployment\nSelf-service deployment enables model teams to push models to production without platform team involvement for routine operations.\nDeployment API Design\nA well-designed deployment API abstracts operational complexity:\ndeployment:\n  model:\n    registry_path: models/recommendation/ranking_v3\n    version: \"3.2.1\"\n\n  serving:\n    replicas:\n      min: 5\n      max: 50\n    resources:\n      gpu: nvidia-t4\n      memory: 16Gi\n    autoscaling:\n      metric: requests_per_second\n      target: 1000\n\n  traffic:\n    strategy: canary\n    canary_percentage: 5\n    promotion_criteria:\n      - metric: error_rate\n        threshold: 0.01\n      - metric: latency_p99_ms\n        threshold: 100\n\n  monitoring:\n    alerts:\n      - metric: accuracy_degradation\n        threshold: 0.05\n        notification: model-team@company.com\nThe platform translates this specification into:\n\nKubernetes deployments with appropriate resource requests\nLoad balancer configuration for traffic routing\nPrometheus metrics collection\nAlertmanager rules for notifications\nIstio service mesh configuration for traffic splitting\n\nModel teams specify what they need; the platform handles how to provide it.\nGuardrails and Governance\nSelf-service must operate within governance constraints:\nResource quotas: Teams have GPU and compute budgets. Deployments exceeding quotas require approval.\nSecurity requirements: Models accessing sensitive data must meet security controls. The platform enforces requirements automatically.\nQuality gates: Deployments must pass validation checks. The platform rejects deployments that fail required gates.\nDeployment windows: High-risk deployments may be restricted to certain times. The platform enforces scheduling constraints.\n\n\nResource Management\nEfficient resource utilization is essential for platform economics. ML workloads have distinct resource patterns that require specialized management.\nTraining Resource Management\nTraining workloads are batch-oriented with predictable resource requirements:\n\nJobs have defined start and end\nGPU memory requirements are known in advance\nJobs can often be preempted and restarted\nScheduling can optimize for cluster utilization\n\nEffective training resource management includes:\nJob scheduling: Priority queues, fair sharing across teams, deadline-aware scheduling for urgent jobs.\nPreemption policies: Low-priority jobs can be preempted for high-priority work, with checkpointing to avoid lost progress.\nSpot/preemptible instances: Training can often use discounted preemptible compute, with automatic retry on preemption.\nServing Resource Management\nServing workloads are online with variable demand:\n\nMust respond within latency bounds\nDemand fluctuates by time of day, events, and seasonality\nCannot be preempted without user impact\nScaling must be faster than demand changes\n\nEffective serving resource management includes:\nAutoscaling: Horizontal scaling based on request rate, latency, or custom metrics. Scale-up must be fast enough to handle demand spikes.\nResource isolation: Models should not interfere with each other. Noisy neighbor prevention through resource limits and scheduling constraints.\nCost optimization: Right-sizing instances, using reserved capacity for baseline demand, spot instances for overflow.\nPlatform Utilization Metrics\nPlatform efficiency can be measured by:\n\\[U_{platform} = \\frac{\\sum_{i} U_i \\times R_i}{\\sum_{i} R_i} \\tag{8}\\]\nwhere \\(U_i\\) is the utilization of resource \\(i\\) and \\(R_i\\) is the capacity of resource \\(i\\).\nHowever, raw utilization is incomplete. Effective utilization must also consider:\n\nUtilization quality: Are GPUs doing productive work or waiting on data?\nUtilization fairness: Is utilization distributed appropriately across teams?\nUtilization cost: Is utilization efficient in terms of cost per unit of ML output?\n\nWorked Example: GPU Cluster Efficiency\nA platform operates a 100-GPU cluster for ML training. Current metrics:\n\nAverage GPU utilization: 65%\nGPU memory utilization: 80%\nJobs waiting in queue: average 4 hours\nCost per GPU-hour: $2.50\n\nAnalysis reveals:\n\nHigh memory utilization suggests jobs are sized correctly\nModerate compute utilization suggests some jobs are I/O bound\nQueue times indicate demand exceeds supply\n\nRecommendations:\n\nAdd data loading optimization to reduce I/O bottlenecks (target: 80% compute utilization)\nExpand cluster or implement job scheduling optimization\nCurrent cost: \\(100 \\times 24 \\times 0.65 \\times \\$2.50 = \\$3,900/day\\)\nAfter optimization: \\(100 \\times 24 \\times 0.80 \\times \\$2.50 = \\$4,800/day\\) in effective value from same cost\n\n\n\nMulti-Tenancy and Isolation\nEnterprise platforms serve multiple teams with different requirements, creating multi-tenancy challenges.\nIsolation Requirements\nTenants need isolation at multiple levels:\nPerformance isolation: One team’s workload should not impact another’s. Resource limits, scheduling fairness, and network quality of service enforce performance boundaries.\nSecurity isolation: Teams may work with different data sensitivity levels. Access controls, network segmentation, and encryption protect sensitive workloads.\nCost isolation: Each team’s usage should be attributable. Metering and chargeback enable cost accountability.\nNamespace Architecture\nA typical multi-tenant architecture uses hierarchical namespaces:\nPlatform\n├── Team A\n│   ├── Development\n│   ├── Staging\n│   └── Production\n├── Team B\n│   ├── Development\n│   ├── Staging\n│   └── Production\n└── Shared\n    ├── Feature Store\n    ├── Model Registry\n    └── Monitoring\nEach team receives dedicated namespaces with resource quotas, while shared services operate in common namespaces with appropriate access controls.\nNoisy Neighbor Prevention\nWithout controls, one team’s demanding workload can degrade performance for others. Prevention strategies include:\nRequest limits: Cap the resources any single request can consume Rate limiting: Limit request rates per tenant to prevent overwhelming shared services Priority classes: Ensure critical workloads receive resources even under contention Burst budgets: Allow temporary resource overages while maintaining long-term fairness\n\n\nCost Allocation and Chargeback\nPlatform costs must be attributed to consuming teams for accountability and planning.\nCost Components\nML platform costs include:\n\nCompute: GPU and CPU time for training and serving\nStorage: Dataset storage, model artifacts, feature store\nNetwork: Data transfer between services and regions\nPlatform overhead: Platform team salaries, development costs, tools\n\nAttribution Models\nSeveral attribution approaches exist:\nDirect metering: Charge teams exactly for resources consumed. Most accurate but creates complex incentives (teams may under-provision to reduce costs).\nAllocation-based: Charge based on reserved capacity rather than actual usage. Simpler but may not reflect actual consumption.\nHybrid: Base charge for allocation plus variable charge for excess usage. Balances predictability with efficiency incentives.\nChargeback Implementation\nEffective chargeback requires:\n\nFine-grained metering at the resource level\nAttribution rules mapping resources to teams\nReporting dashboards showing cost by team, project, model\nForecasting tools to help teams plan budgets\nAnomaly detection for unexpected cost increases",
    "crumbs": [
      "Volume II: Advanced",
      "Deployment at Scale",
      "ML Operations at Scale"
    ]
  },
  {
    "objectID": "contents/vol2/ops_scale/ops_scale.html#sec-ops-scale-feature-store",
    "href": "contents/vol2/ops_scale/ops_scale.html#sec-ops-scale-feature-store",
    "title": "ML Operations at Scale",
    "section": "Feature Store Operations",
    "text": "Feature Store Operations\nFeature stores have emerged as critical infrastructure for ML platforms, particularly for recommendation systems where feature engineering complexity and serving latency requirements demand specialized solutions. Operating feature stores at scale presents unique challenges in freshness, consistency, and performance.\n\nFeature Store Architecture\nA feature store serves as the central repository for feature data, providing consistent features across training and serving while managing the complexity of feature computation and storage.\nOnline Store\nThe online store provides low-latency feature serving for inference requests:\n\nStorage: Key-value stores optimized for point lookups (Redis, DynamoDB, Bigtable)\nLatency target: Sub-10ms for feature retrieval\nScale: Millions to billions of features, thousands to millions of requests per second\n\nOffline Store\nThe offline store provides historical feature data for training:\n\nStorage: Data warehouse or lake (BigQuery, Snowflake, Delta Lake)\nQuery patterns: Large scans for training data generation\nScale: Petabytes of historical feature data\n\nFeature Computation\nFeatures are computed through:\n\nBatch pipelines: Daily or hourly aggregations over historical data\nStreaming pipelines: Real-time updates from event streams\nOn-demand computation: Features calculated at request time when freshness requirements exceed batch frequency\n\n\n\nFreshness SLOs\nFeature freshness represents the delay between real-world events and their reflection in feature values. Different features have different freshness requirements.\n\n\n\nTable 11: Feature freshness requirements by type\n\n\n\n\n\nFeature Type\nExample\nFreshness SLO\nComputation Pattern\n\n\n\n\nStatic\nUser demographics\nDays\nBatch\n\n\nSlowly changing\nUser preferences\nHours\nBatch\n\n\nSession-level\nCurrent session context\nMinutes\nStreaming\n\n\nReal-time\nLast action\nSeconds\nStreaming/On-demand\n\n\n\n\n\n\nFreshness Monitoring\nFeature freshness monitoring tracks:\n\\[\\text{Staleness} = t_{current} - t_{feature\\_update} \\tag{9}\\]\nAlerts trigger when staleness exceeds SLO thresholds. For streaming features, staleness spikes indicate pipeline issues. For batch features, staleness increases linearly between updates.\nWorked Example: Freshness Impact on Model Quality\nA recommendation system uses user interaction features with different freshness levels. Testing on historical data:\n\n\n\nFeature Freshness\nEngagement Lift vs. Baseline\n\n\n\n\nReal-time (&lt; 1 min)\n+12.3%\n\n\nNear real-time (&lt; 5 min)\n+11.8%\n\n\nHourly\n+10.2%\n\n\nDaily\n+8.1%\n\n\n\nThe engagement difference between hourly and real-time features is 2.1 percentage points. If this translates to $10 million in annual engagement value, investing in real-time feature infrastructure may be justified if costs are below this value.\n\n\nPoint-in-Time Correctness\nTraining data must use features as they existed at the time of each training example. Using current feature values to label historical events creates data leakage that inflates offline metrics but fails in production.\nThe Leakage Problem\nConsider training a fraud detection model. If the training data uses current user features (which include information about whether the user was later determined to be fraudulent), the model learns to detect fraud based on information that would not be available at prediction time.\nPoint-in-Time Joins\nFeature stores implement point-in-time joins that retrieve feature values as of specific timestamps:\nSELECT\n    e.user_id,\n    e.event_timestamp,\n    e.label,\n    f.feature_1,\n    f.feature_2\nFROM events e\nLEFT JOIN LATERAL (\n    SELECT feature_1, feature_2\n    FROM features f\n    WHERE f.user_id = e.user_id\n      AND f.feature_timestamp &lt;= e.event_timestamp\n    ORDER BY f.feature_timestamp DESC\n    LIMIT 1\n) f ON TRUE\nThis query retrieves the most recent feature values that existed before each event, ensuring training data reflects production reality.\nStorage Implications\nPoint-in-time correctness requires storing feature history, not just current values. This multiplies storage requirements:\n\\[\\text{Storage} = N_{entities} \\times N_{features} \\times \\frac{T_{retention}}{T_{update}}\\]\nFor 100 million users, 1000 features, 1 year retention, and hourly updates:\n\\[\\text{Storage} = 10^8 \\times 10^3 \\times \\frac{365 \\times 24}{1} = 8.76 \\times 10^{14} \\text{ feature values}\\]\nAt 100 bytes per value, this represents approximately 87 petabytes before compression. Efficient feature stores use compression, columnar storage, and retention policies to manage this scale.\n\n\nFeature Versioning and Lineage\nFeatures evolve over time as definitions change, bugs are fixed, and requirements shift. Versioning enables managing this evolution without breaking dependent models.\nVersion Schema\nFeatures should include:\n\nDefinition version: The computation logic version\nData version: The source data version\nSchema version: The output schema version\n\nChanges to any component create a new version. Models declare which feature versions they depend on.\nLineage Tracking\nFeature lineage records the complete provenance of each feature value:\n\nSource data tables and their versions\nTransformation code and its version\nComputation timestamp and environment\nQuality metrics at computation time\n\nLineage enables:\n\nDebugging unexpected feature behavior by tracing to sources\nImpact analysis when source data changes\nReproducibility for auditing and compliance\n\n\n\nBackfill Procedures\nWhen feature definitions change, historical feature values may need recomputation for model retraining.\nBackfill Challenges\nBackfilling features at scale involves:\n\nComputing features over historical data that may be in cold storage\nManaging compute resources for potentially massive historical periods\nValidating backfilled features against original computations\nCoordinating with dependent pipelines during backfill\n\nBackfill Best Practices\n\nIncremental backfill: Process historical data in date partitions, validating each before proceeding\nDual-write period: Run old and new feature computations in parallel before cutover\nValidation checks: Compare backfilled features against production features for overlapping periods\nRollback capability: Maintain ability to revert to previous feature versions if issues emerge\n\n\n\nScale Challenges\nFeature stores at recommendation system scale face extreme requirements.\nRequest Volume\nMajor recommendation systems process billions of feature requests daily:\n\n1 billion daily recommendations\n100 features per recommendation\n100 billion feature lookups per day\n1.1 million lookups per second average, 5-10x peaks\n\nLatency Requirements\nFeature retrieval must complete within the overall latency budget:\n\nTotal recommendation latency budget: 50ms\nFeature retrieval allocation: 5-10ms\nNetwork overhead: 1-2ms\nRemaining for store lookup: 3-8ms\n\nThis requires in-memory stores with geographic distribution to minimize network latency.\nStorage Scale\nProduction feature stores manage:\n\nBillions of entities (users, items)\nThousands of features per entity\nTerabytes of online data, petabytes of historical data\nMulti-region replication for availability and latency",
    "crumbs": [
      "Volume II: Advanced",
      "Deployment at Scale",
      "ML Operations at Scale"
    ]
  },
  {
    "objectID": "contents/vol2/ops_scale/ops_scale.html#sec-ops-scale-organizational",
    "href": "contents/vol2/ops_scale/ops_scale.html#sec-ops-scale-organizational",
    "title": "ML Operations at Scale",
    "section": "Organizational Patterns",
    "text": "Organizational Patterns\nTechnical infrastructure alone is insufficient for ML operations at scale. Organizational structure determines how effectively teams can leverage platform capabilities. This section examines organizational patterns for ML platform teams and the tradeoffs each presents.\n\nCentralized Platform Team\nA centralized ML platform team builds and maintains shared infrastructure while model teams focus on model development.\nStructure\nML Platform Team (15-30 engineers)\n├── Infrastructure: Compute, storage, networking\n├── ML Systems: Training pipelines, serving infrastructure\n├── Data Platform: Feature store, data pipelines\n├── Developer Experience: APIs, SDKs, documentation\n└── Reliability: Monitoring, on-call, incident response\n\nModel Teams (5-10 engineers each)\n├── Model development and experimentation\n├── Model-specific data pipelines\n└── Business integration\nAdvantages\nConsistency: Centralized teams enforce standards across the organization. All models use consistent deployment, monitoring, and governance practices.\nEfficiency: Platform investments benefit all model teams. Improvements to training infrastructure or serving systems immediately help everyone.\nExpertise concentration: Platform engineers develop deep infrastructure expertise that would be difficult to replicate across many teams.\nCareer paths: Centralized teams provide clear career progression for ML infrastructure engineers.\nDisadvantages\nBottleneck risk: All platform requests route through one team, which can become overwhelmed with competing priorities.\nDistance from problems: Platform engineers may not fully understand model team requirements, leading to suboptimal solutions.\nPrioritization conflicts: With many consuming teams, platform prioritization inevitably leaves some teams unsatisfied.\n\n\nEmbedded ML Engineers\nAn alternative places ML infrastructure expertise within model teams, with coordination through communities of practice rather than organizational structure.\nStructure\nModel Team A (8-12 engineers)\n├── ML Engineers (3-4): Models, experiments\n├── Platform Engineer (1): Infrastructure, ops\n└── Data Engineers (2-3): Pipelines, features\n\nModel Team B (8-12 engineers)\n├── ML Engineers (3-4): Models, experiments\n├── Platform Engineer (1): Infrastructure, ops\n└── Data Engineers (2-3): Pipelines, features\n\nML Community of Practice\n├── Weekly sync across embedded platform engineers\n├── Shared documentation and patterns\n└── Coordinated tool selection\nAdvantages\nResponsiveness: Platform expertise is directly available to model teams without cross-team coordination.\nContext: Embedded engineers deeply understand their team’s specific requirements and constraints.\nOwnership: Teams own their full stack, enabling rapid iteration without external dependencies.\nDisadvantages\nFragmentation: Without strong coordination, teams develop incompatible solutions to common problems.\nDuplication: Each team may solve the same problems independently, wasting organization-wide effort.\nCareer isolation: Embedded platform engineers may lack career growth opportunities without a larger team context.\nInconsistency: Platform quality varies across teams based on embedded engineer skill and attention.\n\n\nHybrid Models\nMost mature organizations adopt hybrid approaches that balance centralization and distribution.\nTiered Platform Model\nCore infrastructure is centralized while domain-specific components are distributed:\nCentral Platform Team\n├── Core infrastructure (compute, storage, networking)\n├── Common ML systems (training, serving, monitoring)\n└── Cross-cutting concerns (security, compliance, cost)\n\nDomain Platform Teams\n├── Recommendation team: RecSys-specific infrastructure\n├── NLP team: LLM-specific infrastructure\n├── Vision team: Vision-specific infrastructure\nThis model recognizes that generic infrastructure benefits from centralization while domain-specific components require proximity to model teams.\nFederated Platform Model\nMultiple teams contribute to a shared platform with coordinated governance:\nPlatform Governance Board\n├── Representatives from major contributing teams\n├── Architectural decisions and standards\n└── Prioritization of shared components\n\nContributing Teams\n├── Team A: Maintains feature store components\n├── Team B: Maintains serving infrastructure\n├── Team C: Maintains monitoring systems\nThis model distributes platform work while maintaining coordination through governance structures.\n\n\nOrganizational Pattern Selection\nThe appropriate organizational pattern depends on several factors:\n\n\n\nTable 12: Factors influencing organizational pattern choice\n\n\n\n\n\nFactor\nFavors Centralized\nFavors Distributed\n\n\n\n\nModel count\nHigher (100+)\nLower (10-20)\n\n\nModel similarity\nHomogeneous\nHeterogeneous\n\n\nOrganization size\nLarger\nSmaller\n\n\nRegulatory requirements\nStricter\nLighter\n\n\nInfrastructure maturity\nEarlier stage\nLater stage\n\n\n\n\n\n\nWorked Example: Organizational Design\nA technology company with 50 ML engineers across 8 teams is evaluating organizational structure. Current state:\n\n80 production models across diverse domains (recommendation, fraud, search, ads)\nEach team maintains its own deployment and monitoring\nSignificant duplication of infrastructure work\nInconsistent practices create integration challenges\n\nAnalysis:\n\nModel count (80) suggests centralization benefits\nDomain diversity suggests some distributed expertise needed\nCurrent duplication indicates centralization opportunity\nIntegration challenges require standardization\n\nRecommendation: Hybrid model with:\n\nCentral platform team (12-15 engineers) for core infrastructure\nDomain-specific platform leads embedded in major teams\nCommunity of practice for coordination\nShared contribution model for domain-specific components",
    "crumbs": [
      "Volume II: Advanced",
      "Deployment at Scale",
      "ML Operations at Scale"
    ]
  },
  {
    "objectID": "contents/vol2/ops_scale/ops_scale.html#sec-ops-scale-case-studies",
    "href": "contents/vol2/ops_scale/ops_scale.html#sec-ops-scale-case-studies",
    "title": "ML Operations at Scale",
    "section": "Case Studies",
    "text": "Case Studies\nExamining how leading technology companies have built ML operations at scale provides concrete examples of the principles discussed throughout this chapter.\n\nUber Michelangelo\nUber’s Michelangelo platform represents one of the most comprehensive public descriptions of enterprise ML infrastructure.\nScale and Scope\n\nHundreds of production models across diverse domains\nDomains include: demand forecasting, ETA prediction, fraud detection, safety, customer support\nMillions of predictions per second across all models\nTraining jobs run continuously across thousands of GPUs\n\nArchitecture Highlights\nUnified platform: Michelangelo provides end-to-end capabilities from feature engineering through serving. Model teams interact through consistent interfaces regardless of use case.\nFeature store: Centralized feature management with offline and online stores. Features are computed once and shared across models, reducing duplication and ensuring consistency.\nDSL for feature engineering: A domain-specific language enables feature definition that works identically in training and serving, eliminating training-serving skew.\nStandardized deployment: All models deploy through the same pipeline with consistent canary, validation, and monitoring patterns.\nLessons\nMichelangelo demonstrates the value of standardization. By providing consistent tools for diverse use cases, Uber enables hundreds of models to operate with a platform team that would be insufficient if each model required custom infrastructure.\n\n\nMeta ML Platform\nMeta operates ML at unprecedented scale, with recommendation systems that serve billions of users.\nScale and Scope\n\nThousands of production models\nRecommendation systems account for majority of model count and request volume\nFeature store manages trillions of feature values\nBillions of predictions per minute during peak\n\nArchitecture Highlights\nFeature engineering at scale: Meta’s feature platform processes exabytes of data daily to compute features. Real-time features update within seconds of user actions.\nEnsemble management: Recommendation requests invoke dozens of models in complex graphs. The platform manages dependencies and coordinates updates.\nExperimentation infrastructure: Sophisticated A/B testing with multiple simultaneous experiments, automated analysis, and guardrail metrics.\nHardware optimization: Custom hardware (training accelerators, inference servers) optimized for Meta’s specific workload patterns.\nLessons\nMeta’s scale requires optimization at every layer. Generic solutions are insufficient; custom development is necessary for cost-effective operation at this scale.\n\n\nNetflix ML Infrastructure\nNetflix combines recommendation systems with content analysis in a unified ML platform.\nScale and Scope\n\nRecommendations for 200+ million subscribers\nModels for personalization, search, content understanding, encoding optimization\nEmphasis on experimentation velocity over raw scale\n\nArchitecture Highlights\nExperimentation focus: Netflix’s platform emphasizes rapid experimentation. Features like Cosmos (ML workflow management) and Meson (ML feature store) prioritize experiment velocity.\nVideo-specific models: Beyond traditional recommendations, Netflix operates sophisticated models for video encoding (per-title encoding optimization), content analysis, and quality of experience.\nFederated ML: Some personalization runs on device, requiring orchestration of on-device and cloud models.\nLessons\nNetflix demonstrates that platform design should align with organizational priorities. Netflix’s emphasis on experimentation velocity shapes platform features differently than organizations prioritizing operational efficiency.\n\n\nGoogle Vertex AI\nGoogle’s Vertex AI provides a cloud platform perspective on ML operations.\nPlatform Capabilities\nManaged training: Distributed training with automatic scaling and fault tolerance.\nFeature Store: Fully managed feature serving with online and offline stores.\nModel Registry: Versioning, lineage tracking, and deployment management.\nPrediction serving: Autoscaling model serving with traffic splitting and monitoring.\nPipelines: Managed ML workflow orchestration.\nLessons\nVertex AI illustrates how platform capabilities can be productized. Organizations that cannot justify building custom platforms can achieve similar capabilities through cloud services, though with less customization.\n\n\nSpotify ML Platform\nSpotify’s ML platform serves both recommendation and content analysis workloads.\nScale and Scope\n\nRecommendations for hundreds of millions of users\nModels for music recommendation, podcast recommendation, search, and audio analysis\nEmphasis on audio understanding alongside traditional recommendation\n\nArchitecture Highlights\nAudio ML: Spotify operates specialized infrastructure for audio feature extraction and analysis, including models for music classification, speech recognition, and audio quality.\nRecommendation diversity: Platform features support recommendation diversity goals, balancing engagement optimization with music discovery.\nCreator tools: ML powers tools for artists and podcasters, requiring different SLOs than consumer-facing recommendations.\nLessons\nSpotify demonstrates how domain-specific requirements (audio processing) integrate with general ML platform capabilities. Platforms must accommodate specialized workloads while maintaining common infrastructure benefits.",
    "crumbs": [
      "Volume II: Advanced",
      "Deployment at Scale",
      "ML Operations at Scale"
    ]
  },
  {
    "objectID": "contents/vol2/ops_scale/ops_scale.html#sec-ops-scale-debugging",
    "href": "contents/vol2/ops_scale/ops_scale.html#sec-ops-scale-debugging",
    "title": "ML Operations at Scale",
    "section": "Production Debugging and Incident Response",
    "text": "Production Debugging and Incident Response\nEngineers spend 30-50% of their time debugging production issues. At platform scale, the complexity multiplies: failures may originate in data pipelines, model code, infrastructure, or emergent interactions between components. Effective incident response requires systematic approaches that go beyond the single-model debugging techniques covered in Volume I.\n\nIncident Classification\nML incidents fall into distinct categories, each requiring different response strategies:\nData incidents involve problems with input data:\n\nPipeline failures preventing fresh data from reaching models\nSchema changes breaking downstream consumers\nData quality degradation (missing values, distribution shifts)\nFeature staleness exceeding SLO thresholds\n\nData incidents often manifest as accuracy degradation across multiple models that share data sources. The first diagnostic step should always check data pipeline health.\nModel incidents involve problems with model behavior:\n\nAccuracy degradation beyond acceptable thresholds\nLatency spikes indicating computational issues\nMemory exhaustion from growing state (KV cache, buffers)\nPrediction bias shifts detected by fairness monitoring\n\nModel incidents typically affect individual models. If multiple unrelated models degrade simultaneously, suspect a shared data or infrastructure issue rather than independent model problems.\nInfrastructure incidents involve problems with the serving platform:\n\nGPU failures causing request errors\nNetwork partitions between model shards\nLoad balancer misconfigurations routing traffic poorly\nContainer orchestration issues affecting deployments\n\nInfrastructure incidents tend to produce error rate spikes and timeout patterns rather than gradual accuracy degradation.\nBusiness metric incidents involve unexpected changes to downstream KPIs:\n\nEngagement drops without clear model or data cause\nRevenue anomalies during normal model operation\nUser behavior shifts that affect model efficacy\n\nBusiness metric incidents are the hardest to attribute. They may stem from external factors (competition, seasonality, marketing campaigns) rather than ML system problems.\n\n\nAttribution Analysis\nWhen metrics degrade, determine the root cause before implementing fixes:\nTemporal correlation analysis:\nSymptom: Recommendation engagement dropped 5% in past hour\n\nStep 1: Check recent deployments\n        → No model deployments in past 4 hours\n        → Eliminate model change as cause\n\nStep 2: Check feature freshness SLOs\n        → user_features: 3 hours stale (SLO: 1 hour)\n        → Feature pipeline delayed\n\nStep 3: Check feature pipeline status\n        → Kafka consumer lag: 10M events (normal: 10K)\n        → Data ingestion bottleneck\n\nStep 4: Investigate Kafka cluster\n        → Broker disk 95% full on partition 7\n        → Root cause identified\nModel vs. data attribution:\nWhen a model’s accuracy drops, distinguish between:\n\nData drift: Input distribution shifted (new user demographics, seasonal patterns)\nFeature staleness: Pipeline delays causing stale predictions\nModel decay: Concept drift where true relationships changed\nUpstream model change: A model this model depends on was updated\n\nAttribution flow:\n\nCompare current input distribution to training distribution\nCheck feature freshness across all input features\nExamine performance on stable evaluation sets\nTrace dependency graph for recent changes\n\nCross-model correlation:\nAt platform scale, failures often span multiple models:\n\n\n\nPattern\nLikely Cause\n\n\n\n\nAll RecSys models degraded\nFeature store issue\n\n\nAll vision models degraded\nImage preprocessing pipeline\n\n\nSingle model degraded\nModel-specific issue\n\n\nGeographic pattern\nRegional infrastructure\n\n\nTime-based pattern\nBatch job scheduling\n\n\n\n\n\nRunbook Development\nRunbooks encode institutional knowledge about incident response:\nStructure for ML runbooks:\n## Runbook: Recommendation Engagement Drop\n\n### Symptoms\n- Engagement metrics (CTR, conversion) dropped &gt;3% vs. 7-day baseline\n- Alert from monitoring system: rec_engagement_anomaly\n\n### Diagnostic Steps\n1. Check MetricsDashboard for engagement trend\n2. Query FeatureStore for freshness violations\n3. Review ModelRegistry for recent deployments\n4. Check InfraMonitoring for GPU/network issues\n\n### Decision Tree\nIF recent_deployment AND rollback_available:\n    Execute rollback, observe metrics for 15 min\n    IF metrics recover: Investigate deployment offline\n    IF metrics persist: Continue diagnosis\n\nIF feature_freshness_violated:\n    Page data engineering on-call\n    Check pipeline job status in Airflow\n\nIF no_obvious_cause:\n    Engage ML platform on-call\n    Consider shadow deployment to compare model versions\n\n### Escalation\n- 15 min without progress: Page ML platform lead\n- 30 min without progress: Page engineering manager\n- User-visible impact &gt;1 hour: Executive notification\nRunbook anti-patterns:\n\nToo specific: “If BERT model fails, restart container” - doesn’t generalize\nToo vague: “Investigate the issue” - provides no actionable guidance\nOutdated: References deprecated systems or contacts\n\n\n\nPost-Incident Reviews\nPost-incident reviews (PIRs) transform incidents into organizational learning:\nPIR template for ML incidents:\n## Incident Summary\n- Duration: 2 hours 15 minutes\n- Impact: 4.2% engagement drop, affecting 12M users\n- Severity: SEV-2 (significant user impact)\n\n## Timeline\n09:15 - Feature pipeline job failed silently\n10:30 - Monitoring detected engagement anomaly\n10:45 - On-call engineer paged\n11:00 - Root cause identified (Kafka broker disk full)\n11:30 - Disk space cleared, pipeline resumed\n11:45 - Features refreshed, engagement recovered\n\n## Root Causes\n1. Primary: Disk monitoring threshold too high (alert at 90%, issue at 95%)\n2. Contributing: Feature pipeline no health check on data freshness\n3. Contributing: Engagement monitoring delay of 75 minutes\n\n## Corrective Actions\n1. Lower disk alert threshold to 80% (Owner: Infra, Due: 1 week)\n2. Add feature freshness monitoring to pipeline (Owner: Data, Due: 2 weeks)\n3. Reduce engagement anomaly detection latency (Owner: ML, Due: 3 weeks)\n\n## Lessons Learned\n- Silent failures in data pipelines eventually surface as model quality issues\n- Monitoring latency directly extends incident duration\n- Cross-team dependencies require explicit SLO definitions\nPIR culture:\nEffective PIRs require psychological safety. Focus on systemic improvements rather than individual blame. Questions should be:\n\n“What systems allowed this to happen?” not “Who caused this?”\n“What would have detected this earlier?” not “Why didn’t someone notice?”\n“How do we prevent this class of failure?” not “How do we prevent this exact failure?”\n\n\n\nDebugging Distributed ML Systems\nDistributed training and inference introduce debugging challenges absent from single-machine systems:\nCommunication failures:\nNCCL collective operations can fail silently or hang indefinitely. Debug tools include:\n# Enable NCCL debug logging\nexport NCCL_DEBUG=INFO\nexport NCCL_DEBUG_SUBSYS=ALL\n\n# Identify slow/failed ranks\n# Look for: \"Waiting for\" messages indicating a rank is blocking others\nWhen a collective hangs: 1. Identify which ranks completed vs. blocked 2. Check network connectivity between problematic ranks 3. Examine GPU memory pressure on blocked ranks 4. Look for asymmetric workloads causing timing differences\nGradient debugging at scale:\nTraining instabilities often manifest as gradient issues:\n\n\n\n\n\n\n\n\nSymptom\nLikely Cause\nDiagnostic\n\n\n\n\nLoss NaN\nGradient explosion\nLog gradient norms\n\n\nLoss stuck\nVanishing gradients\nCheck per-layer norms\n\n\nSlow convergence\nLearning rate mismatch\nCompare to single-GPU baseline\n\n\nRank divergence\nNon-determinism\nCompare rank-specific losses\n\n\n\nMemory debugging:\nOOM errors at scale require tracking memory across devices:\n# Memory tracking per rank\nfor rank in range(world_size):\n    if torch.distributed.get_rank() == rank:\n        print(f\"Rank {rank}:\")\n        print(\n            f\"  Allocated: {torch.cuda.memory_allocated() / 1e9:.2f} GB\"\n        )\n        print(\n            f\"  Reserved: {torch.cuda.memory_reserved() / 1e9:.2f} GB\"\n        )\n        print(\n            f\"  Max allocated: {torch.cuda.max_memory_allocated() / 1e9:.2f} GB\"\n        )\n    torch.distributed.barrier()\nMemory leaks in distributed training often occur at:\n\nGradient accumulation buffers not freed\nCommunication buffers retained across iterations\nActivation checkpointing not releasing properly\n\nDistributed profiling:\nProfile across all ranks to identify stragglers:\n# Per-rank profiling with synchronization\nwith torch.profiler.profile() as prof:\n    # Training iteration\n    ...\n\n# Gather profiles from all ranks\nall_profiles = gather_profiles(prof)\n# Identify slowest rank and operation\nThe slowest rank determines overall throughput. Straggler causes include:\n\nThermal throttling on specific GPUs\nNetwork congestion on particular switches\nUneven data loading across ranks\nGPU hardware degradation\n\n\n\nOn-Call Practices for ML Teams\nML systems require specialized on-call practices:\nRotation design:\n\n\n\n\n\n\n\nAspect\nRecommendation\n\n\n\n\nRotation length\n1 week (shorter causes context switching, longer causes burnout)\n\n\nPrimary + secondary\nAlways have backup; ML incidents often require multiple experts\n\n\nHandoff overlap\n30 min overlap for incident context transfer\n\n\nFollow-the-sun\nFor global teams, hand off with timezone; 8-hour shifts maximum\n\n\n\nAlert fatigue mitigation:\nSigns of alert fatigue:\n\nOn-call ignoring alerts (assuming false positives)\nIncreasing time to acknowledge\nAlerts auto-resolved without investigation\n\nMitigation strategies: 1. Tune alert thresholds quarterly based on false positive rate 2. Deduplicate related alerts (one incident = one page) 3. Add runbook links to every alert 4. Track alert-to-action ratio; aim for &gt;80%\nML-specific on-call skills:\nBeyond general SRE skills, ML on-call requires:\n\nInterpreting model quality metrics\nUnderstanding data pipeline dependencies\nDistinguishing model bugs from data drift\nMaking rollback vs. investigate decisions under pressure\n\nToil reduction:\nTrack time spent on recurring manual tasks. Target: &lt;25% on-call time on toil.\nCommon ML toil:\n\nManually restarting failed training jobs\nManually approving routine deployments\nInvestigating alerts that require no action\nGenerating recurring reports\n\nAutomate aggressively. Every hour of automation development that saves 10 minutes per incident per on-call pays back within a quarter.",
    "crumbs": [
      "Volume II: Advanced",
      "Deployment at Scale",
      "ML Operations at Scale"
    ]
  },
  {
    "objectID": "contents/vol2/ops_scale/ops_scale.html#sec-ops-scale-fallacies",
    "href": "contents/vol2/ops_scale/ops_scale.html#sec-ops-scale-fallacies",
    "title": "ML Operations at Scale",
    "section": "Fallacies and Pitfalls",
    "text": "Fallacies and Pitfalls\nUnderstanding common misconceptions helps avoid costly mistakes when building ML operations at scale.\n\n\n\n\n\n\nWarningFallacy\n\n\n\nOne monitoring dashboard fits all models.\nReality: Different model types have fundamentally different metrics, failure modes, and operational requirements. A dashboard designed for recommendation engagement metrics provides no value for fraud detection precision/recall tradeoffs. Effective monitoring requires model-type-specific dashboards within a common infrastructure.\n\n\n\n\n\n\n\n\nWarningPitfall\n\n\n\nWe can scale our single-model CI/CD to 100 models.\nCopying per-model CI/CD pipelines 100 times creates an unmanageable proliferation of pipelines, each requiring individual maintenance. Platform-level orchestration with parameterized pipelines is essential. The shift from per-model to platform CI/CD typically requires fundamental rearchitecting, not incremental expansion.\n\n\n\n\n\n\n\n\nWarningFallacy\n\n\n\nML platform engineering is just DevOps for ML.\nWhile ML platforms build on DevOps principles, they address unique challenges: data versioning, feature management, experiment tracking, model-specific validation, and training-serving consistency. Platform engineers need ML domain knowledge, not just infrastructure skills. Organizations that staff ML platforms with pure DevOps engineers often struggle with ML-specific requirements.\n\n\n\n\n\n\n\n\nWarningPitfall\n\n\n\nWe can defer platform investment until we have more models.\nThe cost of fragmentation compounds over time. Each team that builds custom infrastructure creates technical debt that becomes harder to consolidate later. Organizations that wait too long face painful migrations that could have been avoided with earlier platform investment. The threshold for platform investment is typically 10-20 models, not 100+.\n\n\n\n\n\n\n\n\nWarningFallacy\n\n\n\nAll model updates carry equal risk.\nA minor parameter adjustment to a vision classifier carries different risk than a major retraining of a fraud detection system. Risk-based deployment policies should match rollout rigor to change risk. Treating all changes identically either over-burdens low-risk changes or under-protects high-risk changes.\n\n\n\n\n\n\n\n\nWarningPitfall\n\n\n\nFeature freshness is a nice-to-have.\nFor many ML applications, feature freshness directly impacts model quality. A recommendation system using day-old features may underperform one using real-time features by several percentage points. Organizations should quantify freshness impact and invest accordingly rather than defaulting to batch computation for all features.",
    "crumbs": [
      "Volume II: Advanced",
      "Deployment at Scale",
      "ML Operations at Scale"
    ]
  },
  {
    "objectID": "contents/vol2/ops_scale/ops_scale.html#summary",
    "href": "contents/vol2/ops_scale/ops_scale.html#summary",
    "title": "ML Operations at Scale",
    "section": "Summary",
    "text": "Summary\n\n\n\n\n\n\nImportantThe 3 Things Students Must Remember\n\n\n\n\nPlatform operations provide superlinear returns. Shared infrastructure value grows faster than model count. Organizations that defer platform investment accumulate operational debt with compounding interest. The economics favor platform investment once model count exceeds 10-20.\nMulti-model systems require ensemble-aware management. Recommendation systems operate as ensembles of 10-50 models per request. Single-model management practices fail when applied to interdependent model portfolios. Dependency tracking, coordinated deployment, and system-level monitoring are essential.\nMonitoring at scale requires aggregation, not enumeration. With 100+ models, per-model alerts create alert fatigue that makes monitoring worse than useless. Hierarchical monitoring with fleet-wide anomaly detection maintains detection capability while managing alert volume.\n\n\n\nThis chapter has examined the transition from single-model MLOps to enterprise-scale ML platform operations. The key insight is that this transition involves qualitative changes in approach, not merely quantitative scaling of existing practices.\nWe began by analyzing the N-models problem: why managing 100 models is fundamentally different from managing one model 100 times. Dependencies, interactions, and organizational complexity grow superlinearly with model count, requiring platform abstractions that address these challenges.\nMulti-model management extends beyond individual model lifecycles to encompass ensemble architectures, dependency graphs, and coordinated deployment. Recommendation systems exemplify these challenges with their complex model compositions and rapid iteration requirements.\nCI/CD for ML at scale requires validation gates that assess not just model performance but latency, fairness, and system-level impact. Staged rollout strategies must match deployment risk profiles that vary dramatically by model type: slow and careful for LLMs, rapid with instant rollback for fraud detection.\nMonitoring at scale demands hierarchical approaches that aggregate signals to prevent alert fatigue while maintaining detection capability. The mathematics of multiple testing make per-model alerting untenable at scale; fleet-wide anomaly detection provides a scalable alternative.\nPlatform engineering creates the infrastructure that enables these capabilities through self-service interfaces, resource management, and multi-tenancy. Effective platforms balance flexibility for model teams against consistency requirements for operations.\nFeature stores emerge as critical infrastructure for recommendation systems, where feature complexity and latency requirements demand specialized solutions. Operating feature stores at scale involves challenges in freshness, point-in-time correctness, and versioning.\nFinally, organizational patterns determine how effectively teams leverage platform capabilities. The choice between centralized, embedded, and hybrid models depends on organizational context, with most mature organizations adopting hybrid approaches.\nThe organizations that master ML operations at scale share a common characteristic: they recognize that operational excellence enables rather than constrains ML innovation. By investing in platform capabilities, they free model teams to focus on models rather than infrastructure, accelerating the pace at which ML capabilities translate into business value.\n\n\n\n\n\n\nImportantKey Takeaways\n\n\n\n\nPlatform operations provide superlinear returns: shared infrastructure value grows faster than model count, making platform investment essential once organizations operate more than 10-20 models\nMulti-model systems, especially recommendation ensembles with 10-50 models per request, require fundamentally different management approaches than single-model operations, including dependency tracking and coordinated deployment\nMonitoring at scale requires hierarchical aggregation rather than per-model alerting: with 100+ models and 5% false positive rates, per-model alerts generate 5 false alarms daily, creating alert fatigue that degrades detection capability\nDeployment strategies must match risk profiles: LLMs warrant slow staged rollouts over days, while fraud detection models need rapid deployment with instant rollback capabilities",
    "crumbs": [
      "Volume II: Advanced",
      "Deployment at Scale",
      "ML Operations at Scale"
    ]
  },
  {
    "objectID": "contents/vol2/distributed_training/distributed_training.html#purpose",
    "href": "contents/vol2/distributed_training/distributed_training.html#purpose",
    "title": "Distributed Training Systems",
    "section": "Purpose",
    "text": "Purpose\nWhat makes distributed training a fundamental shift in how we approach machine learning at scale, rather than merely a technical optimization?\nDistributed training becomes an architectural necessity when training demands exceed the capabilities of individual machines, and the principles governing coordination across distributed systems fundamentally reshape how we approach optimization, fault tolerance, and cost efficiency. Moving from single-machine to distributed training requires understanding how communication overhead, synchronization costs, and system heterogeneity create entirely new constraints that affect algorithm design and system architecture. The challenges of distributed training, including parameter synchronization across thousands of devices, absorbing failures without losing progress, and balancing communication with computation, demand different engineering solutions than single-device scenarios. As models grow to billions of parameters and training datasets expand to continental scales, distributed training becomes the foundation upon which feasible machine learning systems are built, directly determining which problems can be tackled, which timelines are realistic, and which organizations can meaningfully participate in advanced AI development.\n\n\n\n\n\n\nTipLearning Objectives\n\n\n\n\nExplain data parallelism mechanisms including gradient computation, synchronization via AllReduce algorithms (ring, tree, hierarchical), and the relationship between batch size scaling and convergence behavior\nAnalyze multi-machine training requirements by identifying when models exceed single-device memory, when training duration becomes unacceptable, and when datasets exceed single-machine storage, using quantitative thresholds\nImplement data parallel training by applying gradient synchronization algorithms and achieving target parallel efficiency of 85-95% in the linear scaling regime (2-32 devices)\nDesign model parallelism strategies using tensor parallelism, pipeline parallelism with microbatching, and embedding sharding to train models exceeding single-device memory while managing pipeline bubble overhead\nConstruct hybrid parallelism systems combining data, model, and pipeline strategies across multi-node clusters, selecting appropriate combinations for different model architectures (transformers, recommendation systems, vision models)\nEvaluate distributed training efficiency using quantitative metrics including communication-computation ratio, scaling efficiency, bandwidth utilization, and synchronization costs",
    "crumbs": [
      "Volume II: Advanced",
      "Distributed Training",
      "Distributed Training Systems"
    ]
  },
  {
    "objectID": "contents/vol2/distributed_training/distributed_training.html#sec-distributed-training-multimachine-scaling-fundamentals",
    "href": "contents/vol2/distributed_training/distributed_training.html#sec-distributed-training-multimachine-scaling-fundamentals",
    "title": "Distributed Training Systems",
    "section": "Multi-Machine Scaling Fundamentals",
    "text": "Multi-Machine Scaling Fundamentals\nPart I established the infrastructure foundations that make distributed training possible. The datacenter architectures and accelerator topologies examined in ?@sec-infrastructure provide the compute fabric, while the distributed storage systems and data pipelines developed in ?@sec-storage ensure training data flows efficiently to thousands of workers. These foundations answer the question of what physical resources exist. Part II addresses the central question those resources enable: how do we coordinate training across distributed infrastructure to achieve performance that no single machine could deliver?\nThe transition from single-machine to distributed training represents a major shift in optimization strategy and system complexity. Single-machine optimization focuses on efficiently utilizing available resources through techniques such as prefetching, mixed precision, and gradient accumulation. Distributed training introduces different challenges: communication overhead, fault tolerance, and synchronization that require new conceptual frameworks and engineering approaches.\n\nMulti-Machine Training Requirements\nThree concrete signals indicate when distributed training becomes necessary rather than merely beneficial. First, memory exhaustion occurs when model parameters, optimizer states, and activation storage exceed single-device capacity even after applying gradient checkpointing and mixed precision. For transformer models, this threshold typically occurs around 10-20 billion parameters on current generation GPUs with 40-80GB memory (Rajbhandari et al. 2020). Second, unacceptable training duration emerges when single-device training would require weeks or months to converge, making iteration impossible. Training GPT-3 on a single V100 GPU would require approximately 355 years (Brown et al. 2020), making distributed approaches not optional but essential. Third, dataset scale exceeds single-machine storage when training data reaches multiple terabytes, as occurs in large-scale vision or language modeling tasks.\n\nRajbhandari, Samyam, Jeff Rasley, Olatunji Rber, and Yuxiong He. 2020. “ZeRO: Memory Optimizations Toward Training Trillion Parameter Models.” arXiv Preprint arXiv:1910.02054.\n\nBrown, Tom et al. 2020. “Language Models Are Few-Shot Learners.” Advances in Neural Information Processing Systems 33: 1877–1901.\n\n\nDistributed Training Complexity Trade-offs\nDistributed training introduces three primary complexity dimensions absent from single-machine scenarios. Communication overhead emerges from gradient synchronization, where each training step must aggregate gradients across all devices. For a model with \\(N\\) parameters distributed across \\(D\\) devices, all-reduce operations must transfer approximately \\(2N(D-1)/D\\) bytes per step. On commodity network infrastructure (10-100 Gbps), this communication can dominate computation time for models under 1 billion parameters (Sergeev and Del Balso 2018). Fault tolerance requirements increase exponentially with cluster size: a 100-node cluster with 99.9% per-node reliability experiences failures every few hours on average, necessitating checkpoint and recovery mechanisms. Algorithmic considerations change because distributed training alters optimization dynamics—large batch sizes from data parallelism affect convergence behavior, requiring learning rate scaling and warmup strategies that single-machine training does not require (Goyal et al. 2017).\n\nSergeev, Alexander, and Mike Del Balso. 2018. “Horovod: Fast and Easy Distributed Deep Learning in TensorFlow.” In arXiv Preprint arXiv:1802.05799.\n\nGoyal, Priya et al. 2017. “Accurate, Large Minibatch SGD: Training ImageNet in 1 Hour.” In arXiv Preprint arXiv:1706.02677.\n\n\nSingle-Machine to Distributed Transition\nThe systematic optimization methodology established for single-machine training extends to distributed environments with important adaptations. Profiling must now capture inter-device communication patterns and synchronization overhead in addition to computation and memory metrics. Tools like NVIDIA Nsight Systems and PyTorch’s distributed profiler reveal whether training is communication-bound or computation-bound, guiding the choice between parallelization strategies. The solution space expands from single-machine techniques to include data parallelism (distributing training examples), model parallelism (distributing model parameters), pipeline parallelism (distributing model layers), and hybrid approaches that combine multiple strategies. The principles remain consistent—identify bottlenecks, select appropriate techniques, compose solutions—but the implementation complexity increases substantially.",
    "crumbs": [
      "Volume II: Advanced",
      "Distributed Training",
      "Distributed Training Systems"
    ]
  },
  {
    "objectID": "contents/vol2/distributed_training/distributed_training.html#sec-distributed-training-fundamentals",
    "href": "contents/vol2/distributed_training/distributed_training.html#sec-distributed-training-fundamentals",
    "title": "Distributed Training Systems",
    "section": "Distributed Training Fundamentals",
    "text": "Distributed Training Fundamentals\nBuilding upon single-machine optimization foundations, distributed training extends systematic optimization to multiple machines. When single-machine techniques have been exhausted—prefetching eliminates data loading bottlenecks, mixed-precision maximizes memory efficiency, and gradient accumulation reaches practical limits—distributed approaches provide the next level of scaling capability.\n\nDefinition: Distributed TrainingDistributed Training is the parallelization of model training across multiple compute devices through coordinated data partitioning and gradient synchronization, enabling training of models that exceed single-device memory or time constraints.\n\n\nThe progression from single-machine to distributed training follows a natural scaling path: optimize locally first, then scale horizontally. This progression ensures that distributed systems operate efficiently at each node while adding the coordination mechanisms necessary for multi-machine training. Training machine learning models often requires scaling beyond a single machine due to increasing model complexity and dataset sizes. The demand for computational power, memory, and storage can exceed the capacity of individual devices, especially in domains like natural language processing, computer vision, and scientific computing. Distributed training1 addresses this challenge by spreading the workload across multiple machines, which coordinate to train a single model efficiently.\n1 Distributed Training: Google’s DistBelief (2012) pioneered large-scale distributed neural network training, enabling models with billions of parameters across thousands of machines. This breakthrough led to modern frameworks like Horovod (2017) and PyTorch’s DistributedDataParallel, democratizing distributed training for researchers worldwide.This coordination relies on consensus protocols and synchronization primitives to ensure parameter updates remain consistent across nodes. While basic barrier synchronization suffices for research, production deployments require careful fault tolerance, checkpointing, and recovery mechanisms covered in ?@sec-fault-tolerance.\nThe path from single-device to distributed training involves distinct complexity stages, each building upon the previous level’s challenges. Single GPU training requires only local memory management and straightforward forward/backward passes, establishing the baseline computational patterns. Scaling to multiple GPUs within a single node introduces high-bandwidth communication requirements, typically handled through NVLink2 or PCIe connections with NCCL3 optimization, while preserving the single-machine simplicity of fault tolerance and scheduling.\n2 NVLink: NVIDIA’s proprietary high-speed interconnect providing up to 600 GB/s bidirectional bandwidth between GPUs, roughly 10x faster than PCIe Gen4. Essential for efficient multi-GPU training as it enables rapid gradient synchronization and tensor exchanges between devices.3 NCCL (NVIDIA Collective Communications Library): Optimized library implementing efficient collective operations (AllReduce, Broadcast, etc.) for multi-GPU and multi-node communication. Automatically selects optimal communication patterns based on hardware topology, critical for distributed training performance. The leap to multi-node distributed training brings substantially greater complexity: network communication overhead, fault tolerance requirements, and cluster orchestration challenges. Each scaling stage compounds the previous challenges—communication bottlenecks intensify, synchronization overhead grows, and failure probability increases. This progression underscores why practitioners should optimize single-GPU performance before scaling, ensuring efficient resource utilization at each level.\n\n\n\n\n\nNotePractical Distributed Training Complexity\n\n\n\nWhile frameworks like PyTorch (FSDP) and DeepSpeed abstract away much of the complexity, implementing distributed training efficiently remains a significant engineering challenge. Production deployments require careful network configuration (e.g., InfiniBand), infrastructure management (e.g., Kubernetes, Slurm), and debugging of complex, non-local issues like synchronization hangs or communication bottlenecks. For most teams, leveraging managed cloud services is often more practical than building and maintaining this infrastructure from scratch.\n\n\nThe distributed training process itself involves splitting the dataset into non-overlapping subsets, assigning each subset to a different GPU, and performing forward and backward passes independently on each device. Once gradients are computed on each GPU, they are synchronized and aggregated before updating the model parameters, ensuring that all devices learn in a consistent manner. Figure 1 illustrates this process, showing how input data is divided, assigned to multiple GPUs for computation, and later synchronized to update the model collectively.\n\n\n\n\n\n\nFigure 1: Distributed machine learning scales model training by partitioning datasets across multiple GPUs, enabling concurrent computation of gradients, and then aggregating these gradients to update shared model parameters. This approach accelerates training through parallel processing while maintaining model consistency across all devices.\n\n\n\nThis coordination introduces several key challenges that distributed training systems must address. A distributed training system must orchestrate multi-machine computation by splitting up the work, managing communication between machines, and maintaining synchronization throughout the training process. Understanding these basic requirements provides the foundation for examining the main approaches to distributed training: data parallelism, which divides the training data across machines; model parallelism, which splits the model itself; pipeline parallelism, which combines aspects of both; and hybrid approaches that integrate multiple strategies.",
    "crumbs": [
      "Volume II: Advanced",
      "Distributed Training",
      "Distributed Training Systems"
    ]
  },
  {
    "objectID": "contents/vol2/distributed_training/distributed_training.html#sec-distributed-training-efficiency-metrics",
    "href": "contents/vol2/distributed_training/distributed_training.html#sec-distributed-training-efficiency-metrics",
    "title": "Distributed Training Systems",
    "section": "Distributed Training Efficiency Metrics",
    "text": "Distributed Training Efficiency Metrics\nBefore examining specific parallelism strategies, understanding the quantitative metrics that govern distributed training efficiency is essential. These metrics provide the foundation for making informed decisions about scaling strategies, hardware selection, and optimization approaches.\nCommunication overhead represents the primary bottleneck in distributed training systems. AllReduce operations consume 10-40% of total training time in data parallel systems, with this overhead increasing significantly at scale. For BERT-Large on 128 GPUs, communication overhead reaches 35% of total runtime, while GPT-3 scale models experience 55% overhead on 1,024 GPUs.\n\n\n\n\n\n\nNoteAllReduce Communication Complexity\n\n\n\nAllReduce complexity depends on two components: latency (\\(\\alpha\\)) and bandwidth (\\(\\beta\\)). For a message of size \\(M\\) across \\(N\\) workers:\nRing AllReduce:\n\nTime: \\(2(N-1) \\cdot \\alpha + 2 \\cdot \\frac{N-1}{N} \\cdot M \\cdot \\beta\\)\nBandwidth utilization: \\((N-1)/N\\), approaching optimal as \\(N\\) grows\nEach device sends/receives approximately \\(2M\\) bytes total (not \\(2M \\cdot N\\))\n\nTree AllReduce:\n\nTime: \\(2 \\log_2(N) \\cdot \\alpha + 2 \\log_2(N) \\cdot M \\cdot \\beta\\)\nBandwidth utilization: \\(1/\\log_2(N)\\), decreasing with scale\nLatency: \\(O(\\log N)\\) steps\n\nThe crossover point depends on message size: tree AllReduce wins for small messages (latency-dominated), while ring AllReduce wins for large gradients (bandwidth-dominated). Modern implementations like NCCL use hierarchical algorithms that achieve tree latency within nodes (using NVLink) and ring bandwidth between nodes (using InfiniBand).\n\n\nInterconnect selection is critical for large-scale deployments, as these complexity characteristics determine when communication dominates computation.\nThe bandwidth requirements for efficient distributed training are substantial, particularly for transformer models. Efficient systems require 100-400 GB/s aggregate bandwidth per node for transformer architectures. BERT-Base needs 8 GB parameter synchronization per iteration across 64 GPUs, demanding 200 GB/s sustained bandwidth for &lt;50ms synchronization latency. Language models with 175B parameters require 700 GB/s aggregate bandwidth to maintain 80% parallel efficiency, necessitating InfiniBand HDR or equivalent interconnects.\nSynchronization frequency presents a trade-off between communication efficiency and convergence behavior. Gradient accumulation reduces synchronization frequency but increases memory requirements and may impact convergence. Synchronizing every 4 steps reduces communication overhead by 60% while increasing memory usage by 3x for gradient storage. Asynchronous methods eliminate synchronization costs entirely but introduce staleness that degrades convergence by 15-30% for large learning rates.\nScaling efficiency follows predictable patterns across different GPU counts. In the linear scaling regime of 2-32 GPUs, systems typically achieve 85-95% parallel efficiency as communication overhead remains minimal. The communication bound regime emerges at 64-256 GPUs, where efficiency drops to 60-80% even with optimal interconnects. Beyond 512 GPUs, coordination overhead becomes dominant, limiting efficiency to 40-60% due to collective operation latency.\nHardware selection critically impacts these scaling characteristics. NVIDIA DGX systems with NVLink achieve 600 GB/s bisection bandwidth, enabling 90% parallel efficiency up to 8 GPUs per node. Multi-node scaling requires InfiniBand networks, where EDR (100 Gbps) supports efficient training up to 64 nodes, while HDR (200 Gbps) enables scaling to 256+ nodes with &gt;70% efficiency.\nThese efficiency metrics directly influence the choice of parallelism strategy. Data parallelism works well in the linear scaling regime but becomes communication-bound at scale. Model parallelism addresses memory constraints but introduces sequential dependencies that limit efficiency. Pipeline parallelism reduces device idle time but introduces complexity in managing microbatches. Understanding these trade-offs enables architects to select appropriate strategies for their specific workloads.",
    "crumbs": [
      "Volume II: Advanced",
      "Distributed Training",
      "Distributed Training Systems"
    ]
  },
  {
    "objectID": "contents/vol2/distributed_training/distributed_training.html#sec-distributed-training-data-parallelism",
    "href": "contents/vol2/distributed_training/distributed_training.html#sec-distributed-training-data-parallelism",
    "title": "Distributed Training Systems",
    "section": "Data Parallelism",
    "text": "Data Parallelism\nBuilding on the efficiency characteristics outlined above, data parallelism represents the most straightforward distributed approach, particularly effective in the linear scaling regime where communication overhead remains manageable. This method distributes the training process across multiple devices by splitting the dataset into smaller subsets. Each device trains a complete copy of the model using its assigned subset of the data. For example, when training an image classification model on 1 million images using 4 GPUs, each GPU would process 250,000 images while maintaining an identical copy of the model architecture.\nIt is particularly effective when the dataset size is large but the model size is manageable, as each device must store a full copy of the model in memory. This method is widely used in scenarios such as image classification and natural language processing, where the dataset can be processed in parallel without dependencies between data samples. For instance, when training a ResNet model on ImageNet, each GPU can independently process its portion of images since the classification of one image doesn’t depend on the results of another.\nThe effectiveness of data parallelism stems from a property of stochastic gradient descent established in optimization foundations. Gradients computed on different minibatches can be averaged while preserving mathematical equivalence to single-device training. This property enables parallel computation across devices, with the mathematical foundation following directly from the linearity of expectation.\nConsider a model with parameters \\(θ\\) training on a dataset \\(D\\). The loss function for a single data point \\(x_i\\) is \\(L(θ, x_i)\\). In standard SGD with batch size \\(B\\), the gradient update for a minibatch is: \\[\ng = \\frac{1}{B} \\sum_{i=1}^B \\nabla_θ L(θ, x_i)\n\\]\nIn data parallelism with \\(N\\) devices, each device \\(k\\) computes gradients on its own minibatch \\(B_k\\): \\[\ng_k = \\frac{1}{|B_k|} \\sum_{x_i \\in B_k} \\nabla_θ L(θ, x_i)\n\\]\nThe global update averages these local gradients: \\[\ng_{\\text{global}} = \\frac{1}{N} \\sum_{k=1}^N g_k\n\\]\nThis averaging is mathematically equivalent to computing the gradient on the combined batch \\(B_{\\text{total}} = \\bigcup_{k=1}^N B_k\\): \\[\ng_{\\text{global}} = \\frac{1}{|B_{\\text{total}}|} \\sum_{x_i \\in B_{\\text{total}}} \\nabla_θ L(θ, x_i)\n\\]\nThis equivalence shows why data parallelism maintains the statistical properties of SGD training. The approach distributes distinct data subsets across devices, computes local gradients independently, and averages these gradients to approximate the full-batch gradient.\nThe method parallels gradient accumulation, where a single device accumulates gradients over multiple forward passes before updating parameters. Both techniques use the additive properties of gradients to process large batches efficiently.\n\n\n\n\n\n\nNoteProduction Reality: Data Parallelism at Scale\n\n\n\nData parallelism in production environments involves several operational considerations beyond the theoretical framework:\n\nCommunication efficiency: AllReduce operations for gradient synchronization become the bottleneck at scale. Production systems use optimized libraries like NCCL with ring or tree communication patterns to minimize overhead\nFault tolerance: Node failures during large-scale training require checkpoint/restart strategies. Production systems implement hierarchical checkpointing with both local and distributed storage\nDynamic scaling: Cloud environments require elastic scaling capabilities to add/remove workers based on demand and cost constraints, complicated by the need to maintain gradient synchronization\nCost optimization: Production data parallelism considers cost per GPU-hour across different instance types and preemptible instances, balancing training time against infrastructure costs\nNetwork bandwidth requirements: Large models require careful network topology planning as gradient communication can consume 10-50% of training time depending on model size and batch size\n\nProduction teams typically benchmark communication patterns and scaling efficiency before deploying large distributed training jobs to identify optimal configurations.\n\n\n\nData Parallelism Implementation\nThe process of data parallelism can be broken into a series of distinct steps, each with its role in ensuring the system operates efficiently. These steps are illustrated in Figure 2.\n\n\n\n\n\n\nFigure 2: Distributed training replicates the model across multiple devices, each processing a subset of the data before aggregating gradients to update model parameters. This accelerates the training process compared to single-device training, and contrasts with model parallelism where the model itself is partitioned across devices.\n\n\n\n\nDataset Splitting\nThe first step in data parallelism involves dividing the dataset into smaller, non-overlapping subsets. This ensures that each device processes a unique portion of the data, avoiding redundancy and enabling efficient utilization of available hardware. For instance, with a dataset of 100,000 training examples and 4 GPUs, each GPU would be assigned 25,000 examples. Modern frameworks like PyTorch’s DistributedSampler handle this distribution automatically, implementing prefetching and caching mechanisms to ensure data is readily available for processing.\n\n\nDevice Forward Pass\nOnce the data subsets are distributed, each device performs the forward pass independently. During this stage, the model processes its assigned batch of data, generating predictions and calculating the loss. For example, in a ResNet-50 model, each GPU would independently compute the convolutions, activations, and final loss for its batch. The forward pass is computationally intensive and benefits from hardware accelerators like NVIDIA V100 GPUs or Google TPUs, which are optimized for matrix operations.\n\n\nBackward Pass and Calculation\nFollowing the forward pass, each device computes the gradients of the loss with respect to the model’s parameters during the backward pass. Modern frameworks like PyTorch and TensorFlow handle this automatically through their autograd systems. For instance, if a model has 50 million parameters, each device calculates gradients for all parameters but based only on its local data subset.\n\n\nGradient Synchronization\nTo maintain consistency across the distributed system, the gradients computed by each device must be synchronized. This coordination represents a distributed systems challenge: achieving global consensus while minimizing communication complexity. The ring all-reduce algorithm exemplifies this trade-off, organizing devices in a logical ring where each GPU communicates only with its neighbors. The algorithm complexity is O(n) in communication rounds but requires sequential dependencies that can limit parallelism.\nFor example, with 8 GPUs sharing gradients for a 100 MB model, ring all-reduce requires only 7 communication steps instead of the 56 steps needed for naive all-to-all synchronization. The ring topology creates bottlenecks: the slowest link in the ring determines the overall synchronization time, and network partitions can halt the entire training process. Alternative algorithms like tree-reduce achieve O(log n) latency at the cost of increased bandwidth requirements on root nodes. Modern systems often implement hierarchical topologies, using high-speed links within nodes and lower-bandwidth connections between nodes to optimize these trade-offs.\n\n\nSynchronization Models\nDistributed training systems operate under explicit synchronization models that govern when workers observe each other’s updates. Understanding these models is essential for reasoning about correctness and performance.\nThe default model, Bulk Synchronous Parallel (BSP), requires all workers to complete their local computation (forward and backward pass), synchronize gradients through a barrier (AllReduce), and then simultaneously update parameters. BSP provides strong guarantees: every worker sees identical parameter values at each step, ensuring mathematical equivalence to single-device training. The cost is that the slowest worker determines iteration time, creating the “straggler problem.”\nStale Synchronous Parallel (SSP) relaxes this constraint, allowing workers to proceed up to \\(s\\) iterations ahead of the slowest worker before blocking. This bounds staleness while reducing synchronization delays. SSP requires careful learning rate tuning since workers compute gradients on slightly different parameter versions. The bounded staleness guarantee (\\(s\\) typically 2-5) provides a middle ground between BSP’s strong consistency and fully asynchronous approaches.\nAsynchronous SGD eliminates synchronization barriers entirely, with workers updating parameters independently. This maximizes hardware utilization but introduces gradient staleness that can degrade convergence. When a worker computes gradients on parameters that are already \\(\\tau\\) steps stale, the effective learning rate decreases. Compensation techniques include learning rate scaling (\\(\\eta' = \\eta / \\sqrt{\\tau}\\)) or momentum correction.\n\n\n\n\n\n\nNoteSynchronization Model Trade-offs\n\n\n\n\n\n\n\n\n\n\n\n\n\nModel\nConsistency\nThroughput\nConvergence\nUse Case\n\n\n\n\nBSP\nStrong\nBounded by slowest worker\nEquivalent to single-GPU\nFinal training runs, reproducibility\n\n\nSSP\nBounded staleness\nHigher than BSP\nNear-equivalent with tuning\nHyperparameter search\n\n\nAsync\nWeak\nMaximum\nDegraded, requires compensation\nLarge heterogeneous clusters\n\n\n\n\n\nThe choice of synchronization model directly affects both system throughput and model convergence. Production systems typically use BSP for final training runs to ensure reproducibility, while exploring SSP or async approaches during hyperparameter search where exact reproducibility is less critical.\n\n\nBarrier Semantics and Failure Modes\nAllReduce operations implement implicit barriers: no worker can proceed until all workers have contributed their gradients. This coupling creates failure modes absent from single-device training.\nWorker failures during AllReduce cause all other workers to block indefinitely, waiting for the missing contribution. Without timeout mechanisms, the entire training job hangs rather than failing cleanly. Production systems implement watchdog timers, typically set to 5-10 minutes, to detect and terminate stuck jobs.\nGradient mismatches occur when workers disagree on which tensors to synchronize due to conditional computation paths or dynamic batching. AllReduce operations may block waiting for tensors that some workers never send. This commonly occurs with variable-length sequences in NLP models, dynamic computation graphs, and mixture-of-experts with different routing decisions.\nStraggler-induced delays arise because iteration time equals the slowest worker’s time plus synchronization overhead. A single slow worker, whether due to thermal throttling, network congestion, or OS jitter, delays all workers and reduces cluster utilization. At 1000 GPUs with 1% probability of straggler per GPU per step, approximately 10 GPUs straggle every iteration.\nProduction systems address these issues through:\n\nTimeouts: AllReduce operations with configurable timeouts that trigger failure handling rather than indefinite blocking\nHeartbeat monitoring: Detecting unresponsive workers before AllReduce blocks\nElastic training: Removing failed workers and continuing with reduced parallelism (see ?@sec-fault-tolerance)\nBackup workers: Redundant computation to mask stragglers\n\n\n\nParameter Updating\nAfter gradient aggregation, each device independently updates model parameters using the chosen optimization algorithm, such as SGD with momentum or Adam. This decentralized update strategy, implemented in frameworks like PyTorch’s DistributedDataParallel (DDP), enables efficient parameter updates without requiring a central coordination server. Since all devices have identical gradient values after synchronization, they perform mathematically equivalent updates to maintain model consistency across the distributed system.\nFor example, in a system with 8 GPUs training a ResNet model, each GPU computes local gradients based on its data subset. After gradient averaging via ring all-reduce, every GPU has the same global gradient values. Each device then independently applies these gradients using the optimizer’s update rule. If using SGD with learning rate 0.1, the update would be weights = weights - 0.1 * gradients. This process maintains mathematical equivalence to single-device training while enabling distributed computation.\nThis process, which involves splitting data, performing computations, synchronizing results, and updating parameters, repeats for each batch of data. Modern frameworks automate this cycle, allowing developers to focus on model architecture and hyperparameter tuning rather than distributed computing logistics.\n\n\n\nData Parallelism Advantages\nData parallelism offers several key benefits that make it the predominant approach for distributed training. By splitting the dataset across multiple devices and allowing each device to train an identical copy of the model, this approach effectively addresses the core challenges in modern AI training systems.\nThe primary advantage of data parallelism is its linear scaling capability with large datasets. As datasets grow into the terabyte range, processing them on a single machine becomes prohibitively time-consuming. For example, training a vision transformer on ImageNet (1.2 million images) might take weeks on a single GPU, but only days when distributed across 8 GPUs. This scalability is particularly valuable in domains like language modeling, where datasets can exceed billions of tokens.\nHardware utilization efficiency represents another important benefit. Data parallelism maintains high GPU utilization rates, typically, above 85%, by ensuring each device actively processes its data portion. Modern implementations achieve this through asynchronous data loading and gradient computation overlapping with communication. For instance, while one batch computes gradients, the next batch’s data is already being loaded and preprocessed.\nImplementation simplicity sets data parallelism apart from other distribution strategies. Modern frameworks have reduced complex distributed training to just a few lines of code. For example, converting a PyTorch model to use data parallelism often requires only wrapping it in DistributedDataParallel and initializing a distributed environment. This accessibility has contributed significantly to its widespread adoption in both research and industry.\nThe approach also offers flexibility across model architectures. Whether training a ResNet (vision), BERT (language), or Graph Neural Network (graph data), the same data parallelism principles apply without modification. This universality makes it particularly valuable as a default choice for distributed training.\nTraining time reduction is perhaps the most immediate benefit. Given proper implementation, data parallelism can achieve near-linear speedup with additional devices. Training that takes 100 hours on a single GPU might complete in roughly 13 hours on 8 GPUs, assuming efficient gradient synchronization and minimal communication overhead.\nWhile these benefits make data parallelism compelling, achieving these advantages requires careful system design. Several challenges must be addressed to fully realize these benefits.\n\n\n\n\n\n\nTipGPT-2 Data Parallel Scaling: 1→8→32 GPUs\n\n\n\n\n\nThis example demonstrates how data parallelism scales in practice, including efficiency degradation.\nSingle GPU Baseline\n\nBatch size: 16 (with gradient checkpointing, fits in 32GB)\nTime per step: 1.8 seconds\nTraining throughput: ~9 samples/second\nTime to 50K steps: 25 hours\n\n8 GPUs: Single Node with NVLink\nConfiguration:\n\nPer-GPU batch: 16, global batch: 128\nGradient synchronization: 3GB @ 600 GB/s (NVLink) = 5ms\n\nPerformance results:\n\nComputation: 180ms per step\nCommunication: 5ms per step\nTotal: 185ms per step\nSpeedup: 1.8s ÷ 0.185s = 9.7× (not quite 8×)\nParallel efficiency: 9.7 ÷ 8 = 121%\n\nWhy over 100% efficiency? Larger global batch (128 vs 16) improves GPU utilization from 72% to 89%. This “super-linear” speedup is common in ML at small scales when the baseline has poor utilization.\nTraining time: 25 hours ÷ 9.7 = 2.6 hours\n32 GPUs: 4 Nodes with InfiniBand\nConfiguration:\n\nPer-GPU batch: 16, global batch: 512\nIntra-node communication: 5ms (NVLink)\nInter-node communication: 3GB @ 12.5 GB/s (InfiniBand) = 240ms\n\nPerformance results:\n\nComputation: 180ms (42% of time)\nCommunication: 245ms (58% of time)\nTotal: 425ms per step\nSpeedup: 1.8s ÷ 0.425s = 4.2× faster → 5.9 hours\nParallel efficiency: 4.2 ÷ 32 = 13%\n\nCommunication dominates and becomes the bottleneck.\nBetter Approach: 8 GPUs with Gradient Accumulation\n\nConfiguration: 8 GPUs × batch 16 × 4 accumulation steps = 512 effective batch\nCommunication overhead: 5ms ÷ (4 × 180ms) = 0.7%\nTraining time: 3.8 hours\nCost: $128/hour × 3.8 hours = $486 vs. $3,021 for 32 GPUs\nSavings: $2,535 (84% reduction) with only 1 hour longer training\n\nKey Insights\n\nNVLink enables efficient scaling within single nodes (97% efficiency)\nInter-node communication kills efficiency (drops to 13%)\nGradient accumulation beats naive scaling for memory-bound models\nSweet spot for GPT-2: 8 GPUs per node with gradient accumulation, not naive scaling to 32+ GPUs\n\nOpenAI’s GPT-2 paper reports training on 32 V100s across 4 nodes using optimized communication (likely gradient accumulation combined with pipeline parallelism), not pure data parallelism.\n\n\n\n\n\nData Parallelism Limitations\nWhile data parallelism is an effective approach for distributed training, it introduces several challenges that must be addressed to achieve efficient and scalable training systems. These challenges stem from the inherent trade-offs between computation and communication, as well as the limitations imposed by hardware and network infrastructures.\nCommunication overhead represents the most significant bottleneck in data parallelism. During gradient synchronization, each device must exchange gradient updates, often hundreds of megabytes per step for large models. With 8 GPUs training a 1-billion-parameter model, each synchronization step might require transferring several gigabytes of data across the network. While high-speed interconnects like NVLink (300 GB/s) or InfiniBand (200 Gb/s) help, the overhead remains substantial. NCCL’s ring-allreduce algorithm4 reduces this burden by organizing devices in a ring topology, but communication costs still grow with model size and device count.\n4 AllReduce Algorithm: A collective communication primitive where each process contributes data and all processes receive the same combined result (typically a sum). Naive implementations require O(n²) messages for n devices. The ring-allreduce algorithm, developed for high-performance computing in the 1980s, reduces this to O(n) by organizing devices in a logical ring where each device communicates only with its neighbors, making it scalable for modern ML with hundreds of GPUs.Scalability limitations become apparent as device count increases. While 8 GPUs might achieve \\(7\\times\\) speedup (87.5% scaling efficiency), scaling to 64 GPUs typically yields only 45-50\\(\\times\\) speedup (70-78% efficiency) due to growing synchronization costs. Scaling efficiency, calculated as speedup divided by the number of devices—quantifies how effectively additional hardware translates to reduced training time. Perfect linear scaling would yield 100% efficiency, but communication overhead and synchronization barriers typically degrade efficiency as device count grows. This non-linear scaling means that doubling the number of devices rarely halves the training time, particularly in configurations exceeding 16-32 devices.\nMemory constraints present a hard limit for data parallelism. Consider a transformer model with 175 billion parameters, which requires approximately 350 GB just to store model parameters in FP32. When accounting for optimizer states and activation memories, the total requirement often exceeds 1 TB per device. Since even high-end GPUs typically offer 80 GB or less, such models cannot use pure data parallelism.\nWorkload imbalance affects heterogeneous systems significantly. In a cluster mixing A100 and V100 GPUs, the A100s might process batches \\(1.7\\times\\) faster, forcing them to wait for the V100s to catch up. This idle time can reduce cluster utilization by 20-30% without proper load balancing mechanisms.\nFinally, there are critical challenges related to fault tolerance and reliability in distributed training systems. Node failures become inevitable at scale: with 100 GPUs running continuously, hardware failures occur multiple times per week. A training run that costs millions of dollars cannot restart from scratch each time a single GPU fails. Modern distributed training systems implement sophisticated checkpointing strategies, storing model state every N iterations to minimize lost computation. Checkpoint frequency creates trade-offs: frequent checkpointing reduces the potential loss from failures but increases storage I/O overhead and training latency. Production systems typically checkpoint every 100-1000 iterations, balancing fault tolerance against performance.\nImplementation complexity compounds these reliability challenges. While modern frameworks abstract much of the complexity, implementing robust distributed training systems requires significant engineering expertise. Graceful degradation when subsets of nodes fail, consistent gradient synchronization despite network partitions, and automatic recovery from transient failures demand deep understanding of both machine learning and distributed systems principles.\nDespite these challenges, data parallelism remains an important technique for distributed training, with many strategies available to address its limitations. Monitoring these distributed systems requires specialized tooling for tracking gradient norms, communication patterns, and hardware utilization across nodes. Before examining model parallelism, we first explore memory optimization techniques that extend data parallelism to larger models.\n\n\nMemory-Efficient Data Parallelism: ZeRO and FSDP\nThe memory constraints of data parallelism motivate a family of techniques that shard memory state across workers while preserving the simplicity of data parallel training. ZeRO (Zero Redundancy Optimizer) and its PyTorch implementation FSDP (Fully Sharded Data Parallel) enable training models that would otherwise require model parallelism.\nIn standard data parallelism, each GPU maintains a complete copy of:\n\nModel parameters: 4 bytes/param (FP32) or 2 bytes/param (FP16/BF16)\nGradients: Same size as parameters\nOptimizer states: For Adam, 8 bytes/param (momentum + variance in FP32)\n\nFor a 7B parameter model with Adam optimizer, each GPU requires: \\(7B \\times (4 + 4 + 8) = 112\\) GB, exceeding A100-80GB capacity even before accounting for activations.\nZeRO addresses this redundancy through progressive sharding:\n\n\n\n\n\n\n\n\n\nStage\nWhat is Sharded\nMemory Reduction\nCommunication Overhead\n\n\n\n\nZeRO-1\nOptimizer states only\n~4x\nNone (same as DDP)\n\n\nZeRO-2\n+ Gradients\n~8x\nReduceScatter replaces AllReduce\n\n\nZeRO-3 / FSDP\n+ Parameters\n~\\(N\\) (linear in workers)\nAllGather before each layer\n\n\n\nZeRO-1 shards optimizer states across GPUs. Each GPU stores only \\(1/N\\) of the Adam momentum and variance tensors. After gradient AllReduce, each GPU updates only its shard of parameters, then broadcasts updates to other GPUs. Memory savings: optimizer states reduced from \\(8N\\) bytes/param to \\(8\\) bytes/param total across cluster.\nZeRO-2 additionally shards gradients. Instead of AllReduce, which leaves full gradients on each GPU, ZeRO-2 uses ReduceScatter so each GPU receives \\(1/N\\) of the reduced gradients. Memory savings: gradients reduced from \\(4N\\) bytes/param to \\(4\\) bytes/param total.\nZeRO-3 and FSDP shard parameters themselves. Each GPU stores only \\(1/N\\) of the model. Before each layer’s forward pass, parameters are gathered via AllGather; after backward pass, gradients are reduced via ReduceScatter, then parameters are discarded. This achieves maximum memory efficiency at the cost of additional communication.\n\n\n\n\n\n\nNoteFSDP Communication Analysis\n\n\n\nFSDP introduces communication on the critical path that DDP avoids:\n\nForward pass: AllGather to reconstruct parameters (\\(M\\) bytes × 2 for each layer)\nBackward pass: ReduceScatter for gradients (\\(M\\) bytes × 2 for each layer)\n\nFor a model with \\(L\\) layers, FSDP performs \\(2L\\) collective operations per training step versus 1 AllReduce for DDP. However, FSDP enables overlapping: while layer \\(i\\) computes, layer \\(i+1\\) can prefetch parameters.\nTotal FSDP communication volume: approximately \\(3M\\) bytes (vs. \\(2M\\) for DDP AllReduce), but spread across more operations with overlap opportunities.\n\n\nThe choice between FSDP and DDP depends on model size and memory constraints. Use DDP when the model fits in GPU memory with room for activations, as it has lower overhead. Use FSDP ZeRO-2 when the model barely fits or requires activation checkpointing. Use FSDP ZeRO-3 when model parameters exceed single-GPU memory. For training 70B+ models on 80GB GPUs, combine FSDP with tensor parallelism.\nFSDP configuration requires careful tuning of sharding strategy (by layer, by transformer block, or flat) and mixed precision settings. The auto_wrap_policy determines sharding granularity, with finer sharding reducing memory but increasing communication frequency.",
    "crumbs": [
      "Volume II: Advanced",
      "Distributed Training",
      "Distributed Training Systems"
    ]
  },
  {
    "objectID": "contents/vol2/distributed_training/distributed_training.html#sec-distributed-training-model-parallelism",
    "href": "contents/vol2/distributed_training/distributed_training.html#sec-distributed-training-model-parallelism",
    "title": "Distributed Training Systems",
    "section": "Model Parallelism",
    "text": "Model Parallelism\nWhile data parallelism scales dataset processing, some models themselves exceed the memory capacity of individual devices. Model parallelism splits neural networks across multiple computing devices when the model’s parameters exceed single-device memory limits. Unlike data parallelism, where each device contains a complete model copy, model parallelism assigns different model components to different devices (Shazeer et al. 2017).\n\nShazeer, Noam et al. 2017. “Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer.” arXiv Preprint arXiv:1701.06538.\nSeveral implementations of model parallelism exist. In layer-based splitting, devices process distinct groups of layers sequentially. For instance, the first device might compute layers 1-4 while the second handles layers 5-8. Channel-based splitting divides the channels within each layer across devices, such as the first device processing 512 channels while the second manages the remaining ones. For transformer architectures, attention head splitting distributes different attention heads to separate devices.\nThis distribution method enables training of large-scale models. GPT-3, with 175 billion parameters, relies on model parallelism for training. Vision transformers processing high-resolution 16k × 16k pixel images use model parallelism to manage memory constraints. Mixture-of-Expert architectures use this approach to distribute their conditional computation paths across hardware.\nDevice coordination follows a specific pattern during training. In the forward pass, data flows sequentially through model segments on different devices. The backward pass propagates gradients in reverse order through these segments. During parameter updates, each device modifies only its assigned portion of the model. This coordination ensures mathematical equivalence to training on a single device while enabling the handling of models that exceed individual device memory capacities.\n\nModel Parallelism Implementation\nModel parallelism divides neural networks across multiple computing devices, with each device computing a distinct portion of the model’s operations. This division allows training of models whose parameter counts exceed single-device memory capacity. The technique encompasses device coordination, data flow management, and gradient computation across distributed model segments. The mechanics of model parallelism are illustrated in Figure 3. These steps are described next:\n\n\n\n\n\n\nFigure 3: Distributing a neural network across multiple devices enables training models larger than the memory capacity of a single device. This approach requires careful coordination of data flow and gradient computation between devices to maintain training efficiency.\n\n\n\n\nModel Partitioning\nThe first step in model parallelism is dividing the model into smaller segments. For instance, in a deep neural network, layers are often divided among devices. In a system with two GPUs, the first half of the layers might reside on GPU 1, while the second half resides on GPU 2. Another approach is to split computations within a single layer, such as dividing matrix multiplications in transformer models across devices.\n\n\nModel Forward Pass\nDuring the forward pass, data flows sequentially through the partitions. For example, data processed by the first set of layers on GPU 1 is sent to GPU 2 for processing by the next set of layers. This sequential flow ensures that the entire model is used, even though it is distributed across multiple devices. Efficient inter-device communication is important to minimize delays during this step (Rasley et al. 2020).\n\nRasley, Jeff, Samyam Rajbhandari, Olatunji Ruwase, and Yuxiong He. 2020. “DeepSpeed: System Optimizations Enable Training Deep Learning Models with over 100 Billion Parameters.” arXiv Preprint arXiv:2020.12.\n\n\nBackward Pass and Calculation\nThe backward pass computes gradients through the distributed model segments in reverse order. Each device calculates local gradients for its parameters and propagates necessary gradient information to previous devices. In transformer models, this means backpropagating through attention computations and feed-forward networks across device boundaries.\nFor example, in a two-device setup with attention mechanisms split between devices, the backward computation works as follows: The second device computes gradients for the final feed-forward layers and attention heads. It then sends the gradient tensors for the attention output to the first device. The first device uses these received gradients to compute updates for its attention parameters and earlier layer weights.\n\n\nParameter Updates\nParameter updates occur independently on each device using the computed gradients and an optimization algorithm. A device holding attention layer parameters applies updates using only the gradients computed for those specific parameters. This localized update approach differs from data parallelism, which requires gradient averaging across devices.\nThe optimization step proceeds as follows: Each device applies its chosen optimizer (such as Adam or AdaFactor) to update its portion of the model parameters. A device holding the first six transformer layers updates only those layers’ weights and biases. This local parameter update eliminates the need for cross-device synchronization during the optimization step, reducing communication overhead.\n\n\nIterative Process\nLike other training strategies, model parallelism repeats these steps for every batch of data. As the dataset is processed over multiple iterations, the distributed model converges toward optimal performance.\n\n\n\nParallelism Variations\nModel parallelism can be implemented through different strategies for dividing the model across devices. The three primary approaches are layer-wise partitioning, operator-level partitioning, and pipeline parallelism, each suited to different model structures and computational needs.\n\nLayer-wise Partitioning\nLayer-wise partitioning assigns distinct model layers to separate computing devices. In transformer architectures, this translates to specific devices managing defined sets of attention and feed-forward blocks. As illustrated in Figure 4, a 24-layer transformer model distributed across four devices assigns six consecutive transformer blocks to each device. Device 1 processes blocks 1-6, device 2 handles blocks 7-12, and so forth.\n\n\n\n\n\n\nFigure 4: Layer-Wise Model Parallelism: Distributing a transformer model across multiple gpus assigns consecutive layers to each device, enabling parallel processing of input data and accelerating training. This partitioning strategy allows each GPU to operate on a subset of the model’s layers, reducing the memory footprint and computational load per device.\n\n\n\nThis sequential processing introduces device idle time, as each device must wait for the previous device to complete its computation before beginning work. For example, while device 1 processes the initial blocks, devices 2, 3, and 4 remain inactive. Similarly, when device 2 begins its computation, device 1 sits idle. This pattern of waiting and idle time reduces hardware utilization efficiency compared to other parallelization strategies.\n\n\nPipeline Parallelism\nPipeline parallelism extends layer-wise partitioning by introducing microbatching to minimize device idle time, as illustrated in Figure 5. Instead of waiting for an entire batch to sequentially pass through all devices, the computation is divided into smaller segments called microbatches (Harlap et al. 2019). Each device, as represented by the rows in the drawing, processes its assigned model layers for different microbatches simultaneously. For example, the forward pass involves devices passing activations to the next stage (e.g., \\(F_{0,0}\\) to \\(F_{1,0}\\)). The backward pass transfers gradients back through the pipeline (e.g., \\(B_{3,3}\\) to \\(B_{2,3}\\)). This overlapping computation reduces idle time and increases throughput while maintaining the logical sequence of operations across devices.\n\nHarlap, Aaron et al. 2019. “PipeDream: Fast and Efficient Pipeline Parallel DNN Training.” In Proceedings of the 27th ACM Symposium on Operating Systems Principles, 1–15.\n\n\n\n\n\n\nFigure 5: With model layers distributed across devices, microbatching divides each training batch into smaller segments that flow through the pipeline concurrently. This approach minimizes device idle time during both forward and backward passes by allowing multiple microbatches to be processed simultaneously at different pipeline stages. Activations flow sequentially between devices during the forward pass, while gradients propagate in reverse during backpropagation.\n\n\n\nIn a transformer model distributed across four devices, device 1 would process blocks 1-6 for microbatch \\(N+1\\) while device 2 computes blocks 7-12 for microbatch \\(N\\). Simultaneously, device 3 executes blocks 13-18 for microbatch \\(N-1\\), and device 4 processes blocks 19-24 for microbatch \\(N-2\\). Each device maintains its assigned transformer blocks but operates on a different microbatch, creating a continuous flow of computation.\nThe transfer of hidden states between devices occurs continuously rather than in distinct phases. When device 1 completes processing a microbatch, it immediately transfers the output tensor of shape [microbatch_size, sequence_length, hidden_dimension] to device 2 and begins processing the next microbatch. This overlapping computation pattern maintains full hardware utilization while preserving the model’s mathematical properties.\n\n\nTensor Parallelism\nTensor parallelism (also called operator-level or intra-layer parallelism) divides individual neural network operations across devices. Unlike pipeline parallelism which assigns complete layers to devices, tensor parallelism splits the weight matrices within each layer. This distinction is critical: tensor parallelism requires high-bandwidth interconnects (NVLink) due to fine-grained communication within each layer, while pipeline parallelism tolerates lower bandwidth between stages.\n\n\n\n\n\n\nNoteTerminology: Tensor Parallelism vs. Pipeline Parallelism\n\n\n\nModern literature distinguishes two forms of model parallelism:\nTensor Parallelism (intra-layer): Splits individual operations (matrix multiplies, attention heads) across devices. Requires high-bandwidth interconnects (NVLink) due to fine-grained communication within each layer.\nPipeline Parallelism (inter-layer): Assigns complete layers to different devices. Requires only point-to-point communication between pipeline stages, tolerating lower bandwidth interconnects.\nThe Megatron-LM framework popularized this distinction, using tensor parallelism within nodes (8 GPUs on NVLink) and pipeline parallelism across nodes (InfiniBand).\n\n\nMegatron-style tensor parallelism partitions matrix multiplications in two ways.\nColumn-parallel linear layers split weights along columns. For input \\(X\\) and weight matrix \\(W = [W_1 | W_2]\\) split across 2 GPUs: \\[Y = XW = X[W_1 | W_2] = [XW_1 | XW_2]\\] Each GPU computes its partition independently. Outputs are concatenated (no communication needed if followed by row-parallel layer).\nRow-parallel linear layers split weights along rows. For \\(W = \\begin{bmatrix} W_1 \\\\ W_2 \\end{bmatrix}\\): \\[Y = XW = X_1 W_1 + X_2 W_2\\] Each GPU computes a partial sum. Outputs require AllReduce to combine.\nIn transformer architectures, Megatron applies this pattern:\n\nQKV projection: Column-parallel (weights split, outputs concatenated across heads)\nAttention output projection: Row-parallel (requires AllReduce after)\nFirst FFN layer: Column-parallel (split intermediate dimension)\nSecond FFN layer: Row-parallel (requires AllReduce after)\n\nThis design places AllReduce operations strategically: one after attention, one after FFN, totaling 2 AllReduce operations per transformer layer.\nCommunication volume per transformer layer depends on sequence length \\(S\\), hidden dimension \\(H\\), and batch size \\(B\\): \\[\\text{Communication} = 2 \\times B \\times S \\times H \\times \\text{sizeof(dtype)}\\]\nWith \\(S=2048\\), \\(H=4096\\), \\(B=4\\), and FP16: \\(2 \\times 4 \\times 2048 \\times 4096 \\times 2 = 134\\) MB per layer. For a 96-layer model, this totals 12.6 GB per training step, requiring NVLink bandwidth to avoid becoming the bottleneck.\nTensor parallelism scaling degrades rapidly beyond 8-way parallelism because:\n\nCommunication volume grows linearly with tensor parallel degree\nComputation per GPU decreases (less work to hide communication latency)\nNVLink bandwidth becomes saturated\n\nProduction systems (GPT-4, LLaMA, Gemini) use 8-way tensor parallelism within nodes, combined with pipeline parallelism across nodes, achieving the best balance of memory distribution and communication efficiency.\n\n\n\nModel Parallelism Advantages\nModel parallelism offers several significant benefits, making it an essential strategy for training large-scale models that exceed the capacity of individual devices. These advantages stem from its ability to partition the workload across multiple devices, enabling the training of more complex and resource-intensive architectures.\nMemory scaling represents the primary advantage of model parallelism. Current transformer architectures contain up to hundreds of billions of parameters. A 175 billion parameter model with 32-bit floating point precision requires 700 GB of memory just to store its parameters. When accounting for activations, optimizer states, and gradients during training, the memory requirement multiplies several fold. Model parallelism makes training such architectures feasible by distributing these memory requirements across devices.\nAnother key advantage is the efficient utilization of device memory and compute power. Since each device only needs to store and process a portion of the model, memory usage is distributed across the system. This allows practitioners to work with larger batch sizes or more complex layers without exceeding memory limits, which can also improve training stability and convergence.\nModel parallelism also provides flexibility for different model architectures. Whether the model is sequential, as in many natural language processing tasks, or composed of computationally intensive operations, as in attention-based models or convolutional networks, there is a partitioning strategy that fits the architecture. This adaptability makes model parallelism applicable to a wide variety of tasks and domains.\nFinally, model parallelism is a natural complement to other distributed training strategies, such as data parallelism and pipeline parallelism. By combining these approaches, it becomes possible to train models that are both large in size and require extensive data. This hybrid flexibility is especially valuable in advanced research and production environments, where scaling models and datasets simultaneously is critical for achieving optimal performance.\nWhile model parallelism offers these benefits, its effectiveness depends on careful partitioning strategy design, with specific challenges addressed in the following sections and the trade-offs involved in its use.\n\n\nModel Parallelism Limitations\nWhile model parallelism provides an effective approach for training large-scale models, it also introduces unique challenges. These challenges arise from the complexity of partitioning the model and the dependencies between partitions during training. Addressing these issues requires careful system design and optimization.\nOne major challenge in model parallelism is balancing the workload across devices. Not all parts of a model require the same amount of computation. For instance, in layer-wise partitioning, some layers may perform significantly more operations than others, leading to an uneven distribution of work. Devices responsible for the heavier computations may become bottlenecks, leaving others underutilized. This imbalance reduces overall efficiency and slows down training. Identifying optimal partitioning strategies is critical to ensuring all devices contribute evenly.\nAnother challenge is data dependency between devices. During the forward pass, activation tensors of shape [batch_size, sequence_length, hidden_dimension] must transfer between devices. For a typical transformer model with batch size 32, sequence length 2048, and hidden dimension 2048, each transfer moves approximately 512 MB of data at float32 precision. With gradient transfers in the backward pass, a single training step can require several gigabytes of inter-device communication. On systems using PCIe interconnects with 64 GB/s theoretical bandwidth, these transfers introduce significant latency.\nModel parallelism also increases the complexity of implementation and debugging. Partitioning the model, ensuring proper data flow, and synchronizing gradients across devices require detailed coordination. Errors in any of these steps can lead to incorrect gradient updates or even model divergence. Debugging such errors is often more difficult in distributed systems, as issues may arise only under specific conditions or workloads.\nA further challenge is pipeline bubbles in pipeline parallelism. With \\(m\\) pipeline stages, the first \\(m-1\\) steps operate at reduced efficiency as the pipeline fills. For example, in an 8-device pipeline, the first device begins processing immediately, but the eighth device remains idle for 7 steps. This warmup period reduces hardware utilization by a fraction of approximately \\((m-1)/b\\), where \\(b\\) is the number of microbatches in the training step.\nFinally, model parallelism may be less effective for certain architectures, such as models with highly interdependent operations. In these cases, splitting the model may lead to excessive communication overhead, outweighing the benefits of parallel computation. For such models, alternative strategies like data parallelism or hybrid approaches might be more suitable.\nDespite these challenges, model parallelism remains an indispensable tool for training large models. With careful optimization and the use of modern frameworks, many of these issues can be mitigated, enabling efficient distributed training at scale.",
    "crumbs": [
      "Volume II: Advanced",
      "Distributed Training",
      "Distributed Training Systems"
    ]
  },
  {
    "objectID": "contents/vol2/distributed_training/distributed_training.html#sec-distributed-training-hybrid-parallelism",
    "href": "contents/vol2/distributed_training/distributed_training.html#sec-distributed-training-hybrid-parallelism",
    "title": "Distributed Training Systems",
    "section": "Hybrid Parallelism",
    "text": "Hybrid Parallelism\nRecognizing that both data and model constraints can occur simultaneously, hybrid parallelism combines model parallelism and data parallelism when training neural networks (Narayanan et al. 2021). A model might be too large to store on one GPU (requiring model parallelism) while simultaneously needing to process large batches of data efficiently (requiring data parallelism).\n\nNarayanan, Deepak et al. 2021. “Efficient Large-Scale Language Model Training on GPU Clusters Using Megatron-LM.” arXiv Preprint arXiv:2104.04473.\nTraining a 175-billion parameter language model on a dataset of 300 billion tokens demonstrates hybrid parallelism in practice. The neural network layers distribute across multiple GPUs through model parallelism, while data parallelism enables different GPU groups to process separate batches. The hybrid approach coordinates these two forms of parallelization.\nThis strategy addresses two key constraints. First, memory constraints arise when model parameters exceed single-device memory capacity. Second, computational demands increase when dataset size necessitates distributed processing.\n\nHybrid Parallelism Implementation\nHybrid parallelism operates by combining the processes of model partitioning and dataset splitting, ensuring efficient utilization of both memory and computation across devices. This integration allows large-scale machine learning systems to overcome the constraints imposed by individual parallelism strategies.\n\nModel and Data Partitioning\nHybrid parallelism divides both model architecture and training data across devices. The model divides through layer-wise or operator-level partitioning, where GPUs process distinct neural network segments. Simultaneously, the dataset splits into subsets, allowing each device group to train on different batches. A transformer model might distribute its attention layers across four GPUs, while each GPU group processes a unique 1,000-example batch. This dual partitioning distributes memory requirements and computational workload.\n\n\nForward Pass\nDuring the forward pass, input data flows through the distributed model. Each device processes its assigned portion of the model using the data subset it holds. For example, in a hybrid system with four devices, two devices might handle different layers of the model (model parallelism) while simultaneously processing distinct data batches (data parallelism). Communication between devices ensures that intermediate outputs from model partitions are passed seamlessly to subsequent partitions.\n\n\nBackward Pass and Gradient Calculation\nDuring the backward pass, gradients are calculated for the model partitions stored on each device. Data-parallel devices that process the same subset of the model but different data batches aggregate their gradients, ensuring that updates reflect contributions from the entire dataset. For model-parallel devices, gradients are computed locally and passed to the next layer in reverse order. In a two-device model-parallel configuration, for example, the first device computes gradients for layers 1-3, then transmits these to the second device for layers 4-6. This combination of gradient synchronization and inter-device communication ensures consistency across the distributed system.\n\n\nParameter Updates\nAfter gradient synchronization, model parameters are updated using the chosen optimization algorithm. Devices working in data parallelism update their shared model partitions consistently, while model-parallel devices apply updates to their local segments. Efficient communication is critical in this step to minimize delays and ensure that updates are correctly propagated across all devices.\n\n\nIterative Process\nHybrid parallelism follows an iterative process similar to other training strategies. The combination of model and data distribution allows the system to process large datasets and complex models efficiently over multiple training epochs. By balancing the computational workload and memory requirements, hybrid parallelism enables the training of advanced machine learning models that would otherwise be infeasible.\n\n\n\nParallelism Variations\nHybrid parallelism can be implemented in different configurations, depending on the model architecture, dataset characteristics, and available hardware. These variations allow for tailored solutions that optimize performance and scalability for specific training requirements.\n\nHierarchical Parallelism\nHierarchical hybrid parallelism applies model parallelism to divide the model across devices first and then layers data parallelism on top to handle the dataset distribution. For example, in a system with eight devices, four devices may hold different partitions of the model, while each partition is replicated across the other four devices for data parallel processing. This approach is well-suited for large models with billions of parameters, where memory constraints are a primary concern.\nHierarchical hybrid parallelism ensures that the model size is distributed across devices, reducing memory requirements, while data parallelism ensures that multiple data samples are processed simultaneously, improving throughput. This dual-layered approach is particularly effective for models like transformers, where each layer may have a significant memory footprint.\n\n\nIntra-layer Parallelism\nIntra-layer hybrid parallelism combines model and data parallelism within individual layers of the model. For instance, in a transformer architecture, the attention mechanism can be split across multiple devices (model parallelism), while each device processes distinct batches of data (data parallelism). This fine-grained integration allows the system to optimize resource usage at the level of individual operations, enabling training for models with extremely large intermediate computations.\nThis variation is particularly useful in scenarios where specific layers, such as attention or feedforward layers, have computationally intensive operations that are difficult to distribute effectively using model or data parallelism alone. Intra-layer hybrid parallelism addresses this challenge by applying both strategies simultaneously.\n\n\nInter-layer Parallelism\nInter-layer hybrid parallelism focuses on distributing the workload between model and data parallelism at the level of distinct model layers. For example, early layers of a neural network may be distributed using model parallelism, while later layers use data parallelism. This approach aligns with the observation that certain layers in a model may be more memory-intensive, while others benefit from increased data throughput.\nThis configuration allows for dynamic allocation of resources, adapting to the specific demands of different layers in the model. By tailoring the parallelism strategy to the unique characteristics of each layer, inter-layer hybrid parallelism achieves an optimal balance between memory usage and computational efficiency.\n\n\n\nHybrid Parallelism Advantages\nThe adoption of hybrid parallelism in machine learning systems addresses some of the most significant challenges posed by the ever-growing scale of models and datasets. By blending the strengths of model parallelism and data parallelism, this approach provides a solution to scaling modern machine learning workloads.\nOne of the most prominent benefits of hybrid parallelism is its ability to scale seamlessly across both the model and the dataset. Modern neural networks, particularly transformers used in natural language processing and vision applications, often contain billions of parameters. These models, paired with massive datasets, make training on a single device impractical or even impossible. Hybrid parallelism enables the division of the model across multiple devices to manage memory constraints while simultaneously distributing the dataset to process vast amounts of data efficiently. This dual capability ensures that training systems can handle the computational and memory demands of the largest models and datasets without compromise.\nAnother critical advantage lies in hardware utilization. In many distributed training systems, inefficiencies can arise when devices sit idle during different stages of computation or synchronization. Hybrid parallelism mitigates this issue by ensuring that all devices are actively engaged. Whether a device is computing forward passes through its portion of the model or processing data batches, hybrid strategies maximize resource usage, leading to faster training times and improved throughput.\nFlexibility is another hallmark of hybrid parallelism. Machine learning models vary widely in architecture and computational demands. For instance, convolutional neural networks prioritize spatial data processing, while transformers require intensive operations like matrix multiplications in attention mechanisms. Hybrid parallelism adapts to these diverse needs by allowing practitioners to apply model and data parallelism selectively. This adaptability ensures that hybrid approaches can be tailored to the specific requirements of a given model, making it a versatile solution for diverse training scenarios.\nHybrid parallelism reduces communication bottlenecks, a common issue in distributed systems. By striking a balance between distributing model computations and spreading data processing, hybrid strategies minimize the amount of inter-device communication required during training. This efficient coordination not only speeds up the training process but also enables the effective use of large-scale distributed systems where network latency might otherwise limit performance.\nFinally, hybrid parallelism supports the ambitious scale of modern AI research and development. It provides a framework for leveraging advanced hardware infrastructures, including clusters of GPUs or TPUs, to train models that push the boundaries of what’s possible. Without hybrid parallelism, many of the breakthroughs in AI, including large language models and advanced vision systems, would remain unattainable due to resource limitations.\nBy enabling scalability, maximizing hardware efficiency, and offering flexibility, hybrid parallelism has become an essential strategy for training the most complex machine learning systems. It is not just a solution to today’s challenges but also a foundation for the future of AI, where models and datasets will continue to grow in complexity and size.\n\n\nHybrid Parallelism Limitations\nWhile hybrid parallelism provides a robust framework for scaling machine learning training, it also introduces complexities that require careful consideration. These challenges stem from the intricate coordination needed to integrate both model and data parallelism effectively. Understanding these obstacles is crucial for designing efficient hybrid systems and avoiding potential bottlenecks.\nOne of the primary challenges of hybrid parallelism is communication overhead. Both model and data parallelism involve significant inter-device communication. In model parallelism, devices must exchange intermediate outputs and gradients to maintain the sequential flow of computation. In data parallelism, gradients computed on separate data subsets must be synchronized across devices. Hybrid parallelism compounds these demands, as it requires efficient communication for both processes simultaneously. If not managed properly, the resulting overhead can negate the benefits of parallelization, particularly in large-scale systems with slower interconnects or high network latency.\nAnother critical challenge is the complexity of implementation. Hybrid parallelism demands a nuanced understanding of both model and data parallelism techniques, as well as the underlying hardware and software infrastructure. Designing efficient hybrid strategies involves making decisions about how to partition the model, how to distribute data, and how to synchronize computations across devices. This process often requires extensive experimentation and optimization, particularly for custom architectures or non-standard hardware setups. While modern frameworks like PyTorch and TensorFlow provide tools for distributed training, implementing hybrid parallelism at scale still requires significant engineering expertise.\nWorkload balancing also presents a challenge in hybrid parallelism. In a distributed system, not all devices may have equal computational capacity. Some devices may process data or compute gradients faster than others, leading to inefficiencies as faster devices wait for slower ones to complete their tasks. Additionally, certain model layers or operations may require more resources than others, creating imbalances in computational load. Managing this disparity requires careful tuning of partitioning strategies and the use of dynamic workload distribution techniques.\nMemory constraints remain a concern, even in hybrid setups. While model parallelism addresses the issue of fitting large models into device memory, the additional memory requirements for data parallelism, such as storing multiple data batches and gradient buffers, can still exceed available capacity. This is especially true for models with extremely large intermediate computations, such as transformers with high-dimensional attention mechanisms. Balancing memory usage across devices is essential to prevent resource exhaustion during training.\nLastly, hybrid parallelism poses challenges related to fault tolerance and debugging. Distributed systems are inherently more prone to hardware failures and synchronization errors. Debugging issues in hybrid setups can be significantly more complex than in standalone model or data parallelism systems, as errors may arise from interactions between the two approaches. Ensuring robust fault-tolerance mechanisms and designing tools for monitoring and debugging distributed systems are essential for maintaining reliability.\nDespite these challenges, hybrid parallelism remains an indispensable strategy for training large-scale machine learning models. By addressing these obstacles through optimized communication protocols, intelligent partitioning strategies, and robust fault-tolerance systems, practitioners can unlock the full potential of hybrid parallelism and drive innovation in AI research and applications.",
    "crumbs": [
      "Volume II: Advanced",
      "Distributed Training",
      "Distributed Training Systems"
    ]
  },
  {
    "objectID": "contents/vol2/distributed_training/distributed_training.html#sec-distributed-training-parallelism-strategy-comparison",
    "href": "contents/vol2/distributed_training/distributed_training.html#sec-distributed-training-parallelism-strategy-comparison",
    "title": "Distributed Training Systems",
    "section": "Parallelism Strategy Comparison",
    "text": "Parallelism Strategy Comparison\nThe features of data parallelism, model parallelism, pipeline parallelism, and hybrid parallelism are summarized in Table 1. This comparison highlights their respective focuses, memory requirements, communication overheads, scalability, implementation complexity, and ideal use cases. By examining these factors, practitioners can determine the most suitable approach for their training needs.\n\n\n\nTable 1: Parallel Training Strategies: Data, model, pipeline, and hybrid parallelism each address the challenges of scaling machine learning training by distributing workload across devices, differing in how they partition data and model parameters to optimize memory usage, communication, and scalability. Understanding these trade-offs enables practitioners to select the most effective approach for their specific model and infrastructure.\n\n\n\n\n\n\n\n\n\n\n\n\nAspect\nData Parallelism\nModel Parallelism\nPipeline Parallelism\nHybrid Parallelism\n\n\n\n\nFocus\nDistributes dataset across devices, each with a full model copy\nDistributes the model across devices, each handling a portion of the model\nDistributes model stages in pipeline, processing microbatches concurrently\nCombines multiple parallelism strategies for balanced scalability\n\n\nMemory Requirement per Device\nHigh (entire model on each device)\nLow (model split across devices)\nLow to Moderate (stages split across devices)\nModerate (splits model and dataset across devices)\n\n\nCommunication Overhead\nModerate to High (gradient synchronization across devices)\nHigh (communication for intermediate activations and gradients)\nModerate (activation passing between stages)\nVery High (requires synchronization for both model and data)\n\n\nScalability\nGood for large datasets with moderate model sizes\nGood for very large models with smaller datasets\nGood for deep models with many layers\nExcellent for extremely large models and datasets\n\n\nImplementation Complexity\nLow to Moderate (relatively straightforward with existing tools)\nModerate to High (requires careful partitioning and coordination)\nModerate to High (requires pipeline scheduling and microbatch management)\nHigh (complex integration of multiple parallelism strategies)\n\n\nIdeal Use Case\nLarge datasets where model fits within a single device\nExtremely large models that exceed single-device memory limits\nDeep models with sequential stages that can tolerate microbatch latency\nTraining massive models on vast datasets in large-scale systems\n\n\n\n\n\n\nFigure 6 provides a general guideline for selecting parallelism strategies in distributed training systems. While the chart offers a structured decision-making process based on model size, dataset size, and scaling constraints, it is intentionally simplified. Real-world scenarios often involve additional complexities such as hardware heterogeneity, communication bandwidth, and workload imbalance, which may influence the choice of parallelism techniques. The chart is best viewed as a foundational tool for understanding the trade-offs and decision points in parallelism strategy selection. Practitioners should consider this guideline as a starting point and adapt it to the specific requirements and constraints of their systems to achieve optimal performance.\n\n\n\n\n\n\nFigure 6: Distributed training systems use data, model, or hybrid parallelism based on model size, dataset size, and scaling constraints to accelerate training and efficiently utilize resources. This flowchart guides practitioners through a decision process, recognizing that real-world deployments often require adaptation due to factors like hardware heterogeneity and workload imbalance.",
    "crumbs": [
      "Volume II: Advanced",
      "Distributed Training",
      "Distributed Training Systems"
    ]
  },
  {
    "objectID": "contents/vol2/distributed_training/distributed_training.html#sec-distributed-training-framework-integration",
    "href": "contents/vol2/distributed_training/distributed_training.html#sec-distributed-training-framework-integration",
    "title": "Distributed Training Systems",
    "section": "Framework Integration",
    "text": "Framework Integration\nWhile the theoretical foundations of distributed training establish the mathematical principles for scaling across multiple devices, modern frameworks provide abstractions that make these concepts accessible to practitioners. Understanding how frameworks like PyTorch translate distributed training theory into practical APIs bridges the gap between mathematical concepts and implementation.\n\nData Parallel Framework APIs\nThe data parallelism mechanisms we explored earlier—gradient averaging, AllReduce communication, and parameter synchronization—are abstracted through framework APIs that handle the complex coordination automatically. PyTorch provides two primary approaches that demonstrate different trade-offs between simplicity and performance.\ntorch.nn.DataParallel represents the simpler approach, automatically replicating the model across available GPUs within a single node. This API abstracts the gradient collection and averaging process, requiring minimal code changes to existing single-GPU training scripts. However, this simplicity comes with performance limitations, as the implementation uses a parameter server approach that can create communication bottlenecks when scaling beyond 4-8 GPUs.\n# Simple data parallelism - framework handles gradient synchronization\nmodel = torch.nn.DataParallel(model)\n# Training loop remains unchanged - framework automatically:\n# 1. Splits batch across GPUs\n# 2. Replicates model on each device\n# 3. Gathers gradients and averages them\n# 4. Broadcasts updated parameters\nFor production scale training, torch.distributed provides the high-performance alternative that implements the efficient AllReduce communication patterns discussed earlier. This API requires explicit initialization of process groups and distributed coordination but enables the linear scaling characteristics essential for large-scale training.\n# Production distributed training - explicit control over communication\nimport torch.distributed as dist\n\ndist.init_process_group(backend=\"nccl\")  # NCCL for GPU communication\nmodel = torch.nn.parallel.DistributedDataParallel(model)\n# Framework now uses optimized AllReduce instead of parameter server\nThe key insight is that DistributedDataParallel implements the efficient ring AllReduce algorithm automatically, transforming the O(n) communication complexity we discussed into practical code that achieves 90%+ parallel efficiency at scale. The framework handles device placement, gradient bucketing for efficient communication, and overlapping computation with communication.\n\n\nModel Parallel Framework Support\nModel parallelism requires more explicit coordination since frameworks must manage cross-device tensor placement and data flow. PyTorch addresses this through manual device placement and the emerging torch.distributed.pipeline API for pipeline parallelism.\n# Manual model parallelism - explicit device placement\nclass ModelParallelNet(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.layers_gpu0 = nn.Sequential(...).to(\"cuda:0\")\n        self.layers_gpu1 = nn.Sequential(...).to(\"cuda:1\")\n\n    def forward(self, x):\n        x = self.layers_gpu0(x.to(\"cuda:0\"))\n        x = self.layers_gpu1(\n            x.to(\"cuda:1\")\n        )  # Cross-GPU data transfer\n        return x\nThis manual approach exposes the sequential dependencies and communication overhead inherent in model parallelism, requiring careful management of tensor movement between devices. The framework automatically handles the backward pass gradient flow across device boundaries, but practitioners must consider the performance implications of frequent device transfers.\n\n\nCommunication Primitives\nModern frameworks expose the communication operations that enable distributed training through high-level APIs. These primitives abstract the low-level NCCL operations while maintaining performance:\n# Framework-provided collective operations\ndist.all_reduce(tensor)  # Gradient averaging across all devices\ndist.broadcast(tensor, src=0)  # Parameter broadcasting from master\ndist.all_gather(\n    tensor_list, tensor\n)  # Collecting tensors from all devices\nThese APIs translate directly to the NCCL collective operations that implement the efficient communication patterns discussed earlier, demonstrating how frameworks provide accessible interfaces to complex distributed systems concepts while maintaining the performance characteristics essential for production training.\nThe framework abstractions enable practitioners to focus on model architecture and training dynamics while leveraging sophisticated distributed systems optimizations. This separation of concerns—mathematical foundations handled by the framework, model design controlled by the practitioner—exemplifies how modern ML systems balance accessibility with performance.",
    "crumbs": [
      "Volume II: Advanced",
      "Distributed Training",
      "Distributed Training Systems"
    ]
  },
  {
    "objectID": "contents/vol2/distributed_training/distributed_training.html#sec-distributed-training-hardware-infrastructure",
    "href": "contents/vol2/distributed_training/distributed_training.html#sec-distributed-training-hardware-infrastructure",
    "title": "Distributed Training Systems",
    "section": "Hardware Infrastructure for Scale",
    "text": "Hardware Infrastructure for Scale\nThe parallelism strategies examined in previous sections assume underlying hardware capable of efficient inter-device communication. This section examines the hardware architectures that enable AI systems to scale from individual accelerators to warehouse-scale computing, analyzing how multi-chip systems introduce qualitatively different constraints around communication overhead, memory coherence, and fault tolerance.\n\nMulti-Chip AI Acceleration\nThe transition from single-chip to multi-chip architectures represents more than simple replication. It requires rethinking how computations distribute across processors, how data flows between chips, and how systems maintain coherence at scale. Where single-chip optimization focuses on maximizing utilization within fixed resources, multi-chip systems must balance computational distribution against communication overhead, memory coherence costs, and synchronization complexity. These challenges fundamentally transform the optimization landscape, requiring new abstractions and techniques beyond those developed for individual accelerators.\nThe scaling of AI systems follows a natural progression: integration within a single package through chiplet architectures, multi-GPU configurations within a server, distributed accelerator pods, and wafer-scale integration. Each approach presents unique trade-offs between computational density, communication overhead, and system complexity. Chiplet architectures maintain high-speed interconnects within a package, while distributed systems sacrifice communication latency for massive parallelism.\n\nChiplet-Based Architectures\nChiplet architectures achieve scaling by partitioning large designs into smaller, modular dies that are interconnected within a single package. Small, specialized semiconductor dies connect together within a single package to create larger, more complex processors. AMD’s EPYC processors use up to 8 chiplets connected via Infinity Fabric, achieving yields above 80% versus 20% for equivalent monolithic designs. This modular approach reduces manufacturing costs and enables mixing different technologies, with compute chiplets in 7 nm paired with I/O chiplets in 14 nm, optimizing each function independently.\nModern AI accelerators partition large designs into smaller chiplets and connect them via high-bandwidth interconnects, enabling scalability beyond monolithic die limitations and improving manufacturing yields. HBM stacks provide fast access to data, crucial for the memory-intensive workloads common in machine learning. AMD’s Instinct MI300 exemplifies this approach by integrating multiple compute chiplets alongside memory chiplets, linked by high-speed die-to-die interconnects. This modular design allows manufacturers to bypass the manufacturing limits of monolithic chips while still achieving high-density compute.\nHowever, even within a single package, scaling is not without challenges. Inter-chiplet communication latency, memory coherence, and thermal management become critical factors as more chiplets are integrated. Memory coherence presents particular complexity: ensuring all processors in a system see the same consistent view of shared memory when multiple cores or chips access the same data. Traditional cache coherence protocols like MESI add 10-50 ns latency for multi-core CPUs. For AI accelerators with thousands of cores, coherence becomes prohibitively expensive, so most ML hardware instead uses explicit memory management where programmers control data placement and synchronization manually.\n\n\nMulti-GPU Systems\nBeyond chiplet-based designs, AI workloads often require multiple discrete GPUs working together. In multi-GPU systems, each accelerator has its own dedicated memory and compute resources, but they must efficiently share data and synchronize execution.\nA common example is NVIDIA DGX systems, which integrate multiple GPUs connected via NVLink or PCIe. This architecture enables workloads to be split across GPUs, typically using data parallelism (where each GPU processes a different batch of data) or model parallelism (where different GPUs handle different parts of a neural network).\nNVSwitch interconnects enable high-speed communication between GPUs, reducing bottlenecks in distributed training. However, scaling up the number of GPUs introduces fundamental distributed coordination challenges that become the dominant performance constraint. The arithmetic intensity of transformer training (0.5-2 FLOPS/byte) forces frequent gradient synchronization across GPUs, where AllReduce operations must aggregate 175 billion parameters in GPT-3 scale models. NVSwitch provides 600 GB/s bidirectional bandwidth, but even this substantial interconnect becomes bandwidth-bound when 8 H100 GPUs simultaneously exchange gradients, creating a 4.8 TB/s aggregate demand that exceeds available capacity.\nThe coordination complexity compounds exponentially. While two GPUs require a single communication channel, eight GPUs need 28 interconnect paths, and fault tolerance requirements mandate redundant communication patterns. Memory consistency protocols further complicate coordination as different GPUs may observe weight updates at different times, requiring sophisticated synchronization primitives that can add 10-50 microseconds latency per training step. These seemingly small delays aggregate to hours of training time across million-iteration runs.\n\n\nCommunication Overhead and Amdahl’s Law\nThe fundamental limitation of distributed AI training stems from Amdahl’s Law, which quantifies how communication overhead constrains parallel speedup regardless of available compute power. For distributed neural network training, communication overhead during gradient synchronization creates a sequential bottleneck that limits scalability even with infinite parallelism.\nThe maximum speedup achievable with distributed training is bound by Amdahl’s Law: \\[\n\\text{Speedup} = \\frac{1}{(1-P) + \\frac{P}{N}}\n\\] where \\(P\\) is the fraction of work that can be parallelized and \\(N\\) is the number of processors. For AI training, the correct formulation accounts for communication time that does not decrease with more workers: \\[\n\\text{Speedup} = \\frac{T_{compute}}{T_{compute}/N + T_{comm}}\n\\] where \\(T_{comm}\\) is largely independent of \\(N\\) for ring AllReduce (or grows as \\(\\log N\\) for tree-based approaches). This can be rewritten as: \\[\n\\text{Speedup} = \\frac{N}{1 + N \\cdot (T_{comm}/T_{compute})}\n\\]\nConsider training a 175 B parameter model with 1000 H100 GPUs as a concrete example:\n\nComputation time per iteration: 100 ms of forward/backward passes per GPU\nGradient size: 175 B parameters × 4 bytes = 700 GB in FP32\nRing AllReduce time: For ring AllReduce, each GPU sends/receives \\(2 \\times (N-1)/N \\times 700\\text{GB} \\approx 1.4\\text{TB}\\)\n\nWith ring AllReduce across 1000 GPUs connected via 600 GB/s links:\n\nIntra-node (8 GPUs via NVLink): \\(1.4\\text{TB} / 600\\text{GB/s} \\approx 2.3\\) seconds\nInter-node adds latency: \\(1000 \\times \\alpha\\) where \\(\\alpha \\approx 1\\mu s\\) per hop\n\nThe resulting scaling efficiency is: \\[\n\\text{Efficiency} = \\frac{T_{compute}}{T_{compute} + T_{comm}} = \\frac{100\\text{ms}}{100\\text{ms} + 2300\\text{ms}} \\approx 4\\%\n\\]\nThis is why real systems use mixed precision (FP16 = 350 GB, halving communication), gradient compression, and pipeline parallelism. With FP16 and 4-way pipeline parallelism reducing synchronization to 1/4 of parameters per stage, efficiency improves dramatically: \\[\n\\text{Efficiency} = \\frac{100\\text{ms}}{100\\text{ms} + 290\\text{ms}} \\approx 26\\%\n\\]\nThis demonstrates why pure data parallelism fails at scale and why hybrid strategies are essential.\nCommunication requirements scale superlinearly with model size and linearly with the number of parameters. Modern transformer models require gradient synchronization across all parameters during each training step:\n\nGPT-3 (175 B parameters): 700 GB gradient exchange per step\nGPT-4 (estimated 1.8 T parameters): approximately 7 TB gradient exchange per step\nFuture 10 T parameter models: approximately 40 TB gradient exchange per step\n\nEven with advanced interconnects like NVLink 4.0 (1.8 TB/s), gradient synchronization for 10 T parameter models would require 22+ seconds per training step, making distributed training impractical without algorithmic innovations like gradient compression or asynchronous updates.\n\n\nTPU Pods\nAs models and datasets continue to expand, training and inference workloads must extend beyond single-server configurations. Google’s TPU Pods represent a pioneering approach to this challenge, interconnecting hundreds of TPUs to function as a unified system.\nThe architectural design of TPU Pods differs fundamentally from traditional multi-GPU systems. While multi-GPU configurations typically rely on NVLink or PCIe connections within a single machine, TPU Pods employ high-bandwidth optical links to interconnect accelerators at data center scale. This design implements a 2D torus interconnect topology, enabling efficient data exchange between accelerators while minimizing communication bottlenecks as workloads scale across nodes.\nThe effectiveness of this architecture is demonstrated in its performance scaling capabilities. TPU Pod performance exhibits near-linear scaling when running ResNet-50, from quarter-pod to full-pod configurations. The system achieves a remarkable 33.0x speedup when scaled to 1024 chips compared to a 16-TPU baseline.\nHowever, distributing AI workloads across an entire data center introduces distributed coordination challenges that fundamentally differ from single-node systems. The 2D torus interconnect, while providing high bisection bandwidth, creates communication bottlenecks when training large transformer models that require AllReduce operations across all 1,024 TPUs. Each parameter gradient must traverse multiple hops through the torus network, with worst-case communication requiring 32 hops between distant TPUs, creating latency penalties that compound with model size.\nThe energy cost of coordination also scales dramatically: moving data across the pod’s optical interconnect consumes 1000x more energy than on-chip communication within individual TPUs, transforming distributed training into a careful balance between computation parallelism and communication efficiency where AllReduce bandwidth, not compute capacity, determines overall training throughput.\n\n\nWafer-Scale AI\nAt the frontier of AI scaling, wafer-scale integration represents a paradigm shift that abandons traditional multi-chip architectures in favor of a single, massive AI processor. Rather than partitioning computation across discrete chips, this approach treats an entire silicon wafer as a unified compute fabric, eliminating the inefficiencies of inter-chip communication.\nWafer-scale integration uses an entire 300 mm silicon wafer as a single processor instead of cutting it into individual chips. Cerebras WSE-3 contains 4 trillion transistors across 850,000 cores, 125x more than the largest GPUs. Manufacturing challenges include 100% yield requirements (solved with redundant cores) and cooling 23 kW of power. This approach eliminates inter-chip communication delays but costs $2-3 million per wafer versus $40,000 for equivalent GPU clusters.\nThe primary advantage of wafer-scale AI is its ultra-fast on-die communication. Unlike chiplets, GPUs, or TPU Pods, where data must traverse physical boundaries between separate devices, wafer-scale AI enables near-instantaneous data transfer across its vast compute array. This architecture drastically reduces communication latency, achieving performance levels that remain out of reach for conventional multi-chip systems.\nAchieving this level of integration introduces substantial engineering challenges. Thermal dissipation, fault tolerance, and manufacturing yield become major constraints when fabricating a processor of this scale. Unlike distributed TPU systems, which mitigate failures by dynamically re-routing workloads, wafer-scale AI must incorporate built-in redundancy mechanisms to tolerate localized defects in the silicon.\n\n\n\nHardware Scaling Trade-offs\nThe progressive scaling of AI acceleration introduces new challenges at each step, from single-chip processors to increasingly complex architectures. Table 2 summarizes these trade-offs across different scaling approaches.\n\n\n\nTable 2: AI Acceleration Scaling Trade-offs: Each approach introduces unique trade-offs between modularity, latency, and complexity, demanding careful consideration of interconnect efficiency and workload distribution.\n\n\n\n\n\n\n\n\n\n\nScaling Approach\nKey Feature\nChallenges\n\n\n\n\nChiplets\nModular scaling within a package\nInter-chiplet latency, memory coherence\n\n\nMulti-GPU\nExternal GPU interconnects (NVLink)\nSynchronization overhead, communication bottlenecks\n\n\nTPU Pods\nDistributed accelerator clusters\nInterconnect congestion, workload partitioning\n\n\nWafer-Scale AI\nEntire wafer as a single processor\nThermal dissipation, fault tolerance\n\n\n\n\n\n\nWhile chiplets enable modular scaling within a package, they introduce latency and memory coherence issues. Multi-GPU systems rely on high-speed interconnects like NVLink but face synchronization and communication bottlenecks. TPU Pods push scalability further by distributing workloads across clusters, yet they must contend with interconnect congestion and workload partitioning. At the extreme end, wafer-scale AI integrates an entire wafer into a single computational unit, presenting unique challenges in thermal management and fault tolerance.\n\n\nMulti-Chip Execution Strategies\nAs AI systems scale from single-chip accelerators to multi-chip architectures, the fundamental challenges in computation and memory evolve. Computation must now be distributed across multiple accelerators, and memory access patterns become constrained by interconnect bandwidth and communication overhead.\nExecution mapping in multi-chip systems requires computation placement that considers workload partitioning across multiple accelerators, with explicit coordination of execution order and dependencies. Computation scheduling must be interconnect-aware to manage communication delays effectively. Load balancing across accelerators is vital, as uneven task distribution results in some accelerators remaining underutilized while others operate at full capacity.\nDistributed memory allocation requires each accelerator to manage its own local memory, necessitating explicit allocation of model parameters, activations, and intermediate data across devices. Unlike single-chip execution where data is fetched once and reused, multi-chip setups require deliberate strategies to minimize redundant data transfers.\nData movement optimization addresses inter-chip data transfer, which becomes the primary bottleneck rather than memory hierarchy latency. Techniques include overlapping computation and communication so accelerators process data while simultaneously sending and receiving, and locality-aware scheduling that places computations on accelerators already holding required data.\nCompiler and runtime adaptations extend single-chip execution models to handle dynamic workload distribution across accelerators. Interconnect-aware workload partitioning enables compilers to distribute computations strategically based on communication cost. In TPU Pods, the runtime schedules computations across multiple TPU cores to minimize communication bottlenecks. In multi-GPU frameworks like PyTorch and TensorFlow, runtime execution synchronizes operations across GPUs while maintaining execution order.",
    "crumbs": [
      "Volume II: Advanced",
      "Distributed Training",
      "Distributed Training Systems"
    ]
  },
  {
    "objectID": "contents/vol2/distributed_training/distributed_training.html#sec-distributed-training-summary",
    "href": "contents/vol2/distributed_training/distributed_training.html#sec-distributed-training-summary",
    "title": "Distributed Training Systems",
    "section": "Summary",
    "text": "Summary\nDistributed training transforms the computational challenge of training large-scale machine learning models into an orchestration problem across multiple devices and hardware architectures. Throughout this chapter, we have examined how parallelism strategies—data, model, pipeline, and hybrid—address different constraints in the training process. Data parallelism scales dataset processing by distributing training examples across devices while synchronizing gradients, achieving near-linear speedups when communication overhead remains manageable. Model parallelism addresses memory constraints by partitioning models across devices, enabling training of architectures that exceed single-device capacity. Pipeline parallelism improves hardware utilization through microbatching, while hybrid approaches combine these strategies for the largest-scale training workloads.\nThe hardware infrastructure enabling distributed training spans multiple scales: chiplet-based architectures within a single package, multi-GPU systems connected via NVLink and NVSwitch, TPU Pods with 2D torus interconnects, and wafer-scale integration. Each approach presents distinct trade-offs between communication bandwidth, memory coherence, and system complexity. Amdahl’s Law quantifies the fundamental scaling limits: communication overhead during gradient synchronization creates sequential bottlenecks that constrain speedup regardless of available compute power, explaining why adding GPUs beyond approximately 100 provides diminishing returns without algorithmic innovations.\nThe efficiency metrics governing distributed training—communication overhead, scaling efficiency, and synchronization costs—directly influence system design decisions. Understanding these trade-offs enables architects to select appropriate strategies for their specific workloads, balancing the complexity of distributed coordination against the computational benefits of parallelization. Framework abstractions like PyTorch’s DistributedDataParallel translate these concepts into practical implementations, allowing practitioners to leverage sophisticated distributed systems optimizations while focusing on model development.\n\n\n\n\n\n\nImportantKey Takeaways\n\n\n\n\nData parallelism achieves near-linear scaling when communication overhead remains below 30-40% of training time\nModel parallelism enables training of models exceeding single-device memory but introduces sequential dependencies\nPipeline parallelism reduces device idle time through microbatching, improving hardware utilization\nHybrid parallelism combines strategies for training the largest models on the largest datasets\nMulti-chip hardware (chiplets, multi-GPU, TPU Pods, wafer-scale) each present distinct trade-offs between integration density, interconnect bandwidth, and system complexity\nAmdahl’s Law quantifies scaling limits: communication overhead constrains speedup regardless of available compute power\nFramework APIs abstract distributed complexity while preserving the performance characteristics essential for production training",
    "crumbs": [
      "Volume II: Advanced",
      "Distributed Training",
      "Distributed Training Systems"
    ]
  },
  {
    "objectID": "contents/vol2/distributed_training/distributed_training.html#sec-distributed-training-fallacies-pitfalls",
    "href": "contents/vol2/distributed_training/distributed_training.html#sec-distributed-training-fallacies-pitfalls",
    "title": "Distributed Training Systems",
    "section": "Fallacies and Pitfalls",
    "text": "Fallacies and Pitfalls\nDistributed training involves counterintuitive behavior that leads to common misconceptions. These fallacies and pitfalls capture errors that waste compute resources and delay research progress.\nLinear speedup remains theoretically impossible regardless of engineering effort. Amdahl’s Law establishes hard limits: any sequential component bounds maximum speedup regardless of parallelism. In distributed training, gradient synchronization is inherently sequential since all gradients must be collected before any update proceeds.\nEven with perfect overlap and optimal algorithms, communication overhead grows with cluster size. For data parallelism, AllReduce time increases logarithmically with tree algorithms or linearly in the latency term with ring algorithms as GPU count grows. A 1000-GPU cluster will never train 1000x faster than a single GPU; achieving 500x speedup would be exceptional, and 100-200x is more typical for communication-heavy workloads.\nOrganizations that budget projects assuming linear scaling inevitably miss deadlines and overspend on compute.\nHyperparameters tuned on small clusters fail catastrophically at large scale. The most critical is learning rate: as batch size increases with data parallelism, learning rate typically must increase proportionally to maintain convergence rate. The “linear scaling rule” suggests \\(\\eta_{large} = \\eta_{base} \\times (B_{large}/B_{base})\\).\nHowever, this rule has limits. Beyond the “critical batch size” (model and dataset dependent, often 8K-32K for vision models), increasing batch size provides diminishing returns. Larger batches find sharper minima that generalize poorly. Training that converges beautifully at 256 GPUs may diverge or produce worse models at 1024 GPUs with naive scaling.\nWarmup schedules, weight decay, and dropout rates also require adjustment. The only reliable approach is systematic scaling studies that validate hyperparameters at target scale.\nData parallelism does not scale indefinitely by adding more GPUs. Data parallelism increases effective batch size proportionally with GPU count, but statistical efficiency (loss reduction per sample) decreases with batch size beyond model-specific thresholds. A 100K-sample batch may provide only 2x the gradient quality of a 10K-sample batch, not 10x.\nThe critical batch size defines where marginal returns collapse. Beyond this point, additional GPUs increase throughput (samples per second) but not training efficiency (loss reduction per compute dollar). For BERT, critical batch size is approximately 8K; for ResNet, approximately 32K. Scaling beyond critical batch size wastes compute.\nLarge organizations have trained models to convergence using 1024 GPUs in the same wall-clock time as 512 GPUs at 2x the cost, because they exceeded critical batch size.\nPipeline parallelism and tensor parallelism both distribute model weights across devices, but their memory and compute characteristics differ dramatically.\nTensor parallelism splits each layer across devices, requiring AllReduce communication within each layer. This reduces memory proportionally but introduces communication overhead on the critical path. Pipeline parallelism assigns complete layers to devices, requiring only point-to-point communication between stages but introducing bubble overhead.\nFor memory-constrained scenarios where a model barely fits with splitting, tensor parallelism’s even distribution helps. For throughput-maximizing scenarios with adequate memory, pipeline parallelism’s lower communication overhead helps. Choosing based on one dimension (memory or compute) without considering the other leads to suboptimal configurations.\nFSDP and ZeRO do not always improve training efficiency. FSDP (Fully Sharded Data Parallel) and ZeRO reduce memory footprint by sharding optimizer state and gradients across GPUs. This enables larger batch sizes or larger models per GPU. However, sharding introduces communication overhead: AllGather before forward pass, ReduceScatter after backward pass.\nFor models that fit comfortably in GPU memory without sharding, FSDP adds overhead without benefit. A 7B model training on A100-80GB with batch size 4 runs faster with DDP than FSDP because the model fits entirely with room for activations.\nFSDP provides value when:\n\nModel + optimizer state exceeds single-GPU memory\nEnabling larger batch sizes justifies communication overhead\nZeRO-Offload to CPU extends effective memory\n\nApplying FSDP universally, as some tutorials suggest, degrades performance for models that do not require it.\nParallelism overhead is roughly constant regardless of model size: AllReduce time depends on gradient size, not model computation time. For small models where forward/backward pass takes 10ms and AllReduce takes 5ms, communication overhead is 50%. For large models where forward/backward takes 1000ms and AllReduce takes 5ms, overhead is 0.5%.\nDecisions made based on small-model benchmarks (“pipeline parallelism is always slower”) invert at scale. The 20% overhead acceptable for a 1B model becomes 0.2% for a 100B model. Parallelism strategy evaluation must occur at target scale, or at minimum with analytical models that extrapolate appropriately.\nGradient accumulation is not free. Gradient accumulation simulates larger batch sizes by accumulating gradients across multiple forward/backward passes before synchronizing. This reduces communication frequency proportionally. However, accumulation has costs:\n\nMemory: Accumulated gradients consume memory throughout the accumulation window\nLatency: Effective step time increases proportionally with accumulation steps\nPrecision: Accumulated FP16 gradients may overflow or underflow\n\nFor loss-sensitive early training, gradient accumulation can introduce instability from accumulated numerical errors. Organizations that use gradient accumulation to work around infrastructure limitations (slow network, small GPUs) sometimes discover training divergence that disappears with proper infrastructure.\nThe principles established in this chapter provide the foundation for understanding fault tolerance mechanisms, which become increasingly critical as distributed training scales to thousands of devices where failures become statistically inevitable.",
    "crumbs": [
      "Volume II: Advanced",
      "Distributed Training",
      "Distributed Training Systems"
    ]
  },
  {
    "objectID": "contents/vol2/conclusion/conclusion.html#sec-vol2-conclusion-synthesis",
    "href": "contents/vol2/conclusion/conclusion.html#sec-vol2-conclusion-synthesis",
    "title": "Conclusion",
    "section": "Synthesizing Distributed ML Systems",
    "text": "Synthesizing Distributed ML Systems\nThis textbook has addressed the engineering challenges that emerge when machine learning systems operate beyond single machines. The transition from single-node development to distributed production constitutes a fundamental shift in engineering methodology. Assumptions that hold for individual systems break down at scale, and new constraints emerge as dominant concerns. This synthesis integrates the insights developed across these chapters into a unified understanding of ML systems engineering at production scale.\nTo build this understanding, the progression through this textbook followed a deliberate structure. Infrastructure chapters (?@sec-infrastructure, ?@sec-storage, ?@sec-communication) revealed how datacenters, storage systems, and communication networks enable distributed ML workloads. Distributed systems chapters (?@sec-distributed-training, ?@sec-inference-at-scale, ?@sec-fault-tolerance) developed techniques for training and inference across thousands of machines. Production challenges chapters (?@sec-ops-scale, ?@sec-robust-ai, ?@sec-security-privacy) addressed operational realities, adversarial threats, and privacy requirements. Responsible deployment chapters (?@sec-responsible-ai, ?@sec-sustainable-ai, ?@sec-ai-good) ensured that technical capability serves human welfare.\nUnderstanding this complete stack enables informed decisions at every level: from algorithm selection through infrastructure design to governance frameworks.",
    "crumbs": [
      "Volume II: Advanced",
      "Responsible AI at Scale",
      "Conclusion"
    ]
  },
  {
    "objectID": "contents/vol2/conclusion/conclusion.html#sec-vol2-conclusion-principles",
    "href": "contents/vol2/conclusion/conclusion.html#sec-vol2-conclusion-principles",
    "title": "Conclusion",
    "section": "Six Principles of Distributed ML Systems",
    "text": "Six Principles of Distributed ML Systems\nSix principles emerged from the material in this textbook, capturing the distinctive character of distributed ML systems engineering. These principles do not merely extend single-machine thinking to larger scales. They represent qualitatively different engineering challenges that require new mental models.\n\n\n\nTable 1: Six Principles of Distributed ML Systems Engineering: These principles capture the qualitative shifts that occur when ML systems move from single machines to distributed production. Each principle connects to specific metrics and chapters where the concept is developed in depth.\n\n\n\n\n\n\n\n\n\n\n\nPrinciple\nCore Question\nKey Metric\nChapter Reference\n\n\n\n\n1. Communication Dominates\nWhat is the bottleneck?\nNetwork bandwidth utilization\n?@sec-communication\n\n\n2. Failure is Routine\nHow do we recover?\nMTBF, checkpoint overhead\n?@sec-fault-tolerance\n\n\n3. Infrastructure Determines\nWhat is possible?\nFLOPS, memory bandwidth\n?@sec-infrastructure\n\n\n4. Responsible AI\nWho is affected?\nFairness metrics, audit trails\n?@sec-responsible-ai\n\n\n5. Sustainability Constraints\nWhat is the cost?\nkWh/training, carbon footprint\n?@sec-sustainable-ai\n\n\n6. Scale Creates Change\nWhat breaks at 1000x?\nScaling efficiency\n?@sec-distributed-training\n\n\n\n\n\n\nPrinciple 1: Communication Dominates Computation\nPerhaps no insight proves more fundamental than understanding that communication, not computation, becomes the dominant constraint at scale. Training a large model across hundreds of GPUs spends more time synchronizing gradients than computing them. Production inference systems become latency-bound by tail effects, where the slowest worker determines response time regardless of how fast others complete.\nThis principle emerged throughout the distributed training techniques in ?@sec-distributed-training and the communication systems in ?@sec-communication. Ring AllReduce, gradient compression, and overlapping computation with communication all address communication bottlenecks. Network architectures for ML exist precisely because standard datacenter networking proves insufficient. Understanding that communication dominates enables recognition of when algorithmic optimizations will help versus when they merely shift work between equally constrained resources.\nPrinciple 2: Failure is Routine, Not Exceptional\nWhile communication bottlenecks limit performance, system reliability poses an equally critical challenge. At distributed scale, component failures occur not occasionally but continuously. A system with 10,000 GPUs, each with a mean time between failures of 10,000 hours, will average one GPU failure per hour. Hardware failures, network partitions, and service disruptions are routine occurrences that systems must handle without human intervention.\nThis principle, examined in ?@sec-fault-tolerance, demands that failure handling be embedded in architecture from the beginning. Checkpointing strategies balance recovery granularity against overhead. Elastic training dynamically adjusts to changing cluster membership. Graceful degradation maintains service quality as capacity diminishes. Systems that treat failure as exceptional will not survive production deployment.\nPrinciple 3: Infrastructure Determines Capability\nBeyond reliability concerns, the physical constraints of infrastructure establish hard boundaries on what ML systems can achieve. The infrastructure examined in ?@sec-infrastructure does not merely support ML workloads; it determines which workloads are possible. Organizations cannot access frontier capabilities without mastering the physical and software systems that make large-scale computation possible. Datacenter design, high-bandwidth networking, and distributed systems orchestration establish hard boundaries on what can be achieved.\nThis principle extends through storage systems (?@sec-storage), where data bandwidth must match accelerator throughput, and communication systems (?@sec-communication), where network topology determines which collective operations are efficient. Generic cloud infrastructure that serves web applications adequately proves insufficient for frontier ML.\nPrinciple 4: Responsible AI is an Engineering Constraint\nWhile infrastructure determines technical possibility, responsible deployment determines what should be built. The responsible AI practices examined in ?@sec-responsible-ai transform abstract ethical principles into concrete engineering constraints. Fairness, transparency, accountability, privacy, and safety are not optional considerations but first-class requirements that shape system architecture throughout the ML lifecycle.\nThis principle emerged from understanding that bias baked into training data propagates through systems regardless of algorithmic sophistication. Systems must be designed for fairness from inception, with monitoring infrastructure detecting degradation across demographic groups. The engineering methods for implementing responsible AI, from bias detection to explainability mechanisms, are as essential as performance optimization.\nPrinciple 5: Sustainability is a First-Class Design Constraint\nEthical considerations extend beyond fairness to environmental responsibility. The environmental impact of large-scale ML, examined in ?@sec-sustainable-ai, elevates resource efficiency from an optional consideration to a primary engineering constraint. Training frontier models consumes electricity equivalent to powering thousands of homes. Computational demands grow exponentially faster than hardware efficiency improvements.\nThis principle transforms sustainability from environmental concern to engineering discipline. Energy costs can exceed model development budgets. Thermal limits restrict hardware density. Power infrastructure requirements limit deployment locations. Carbon-aware scheduling, lifecycle assessment, and efficiency optimization become essential engineering competencies alongside traditional performance metrics.\nPrinciple 6: Scale Creates Qualitative Change\nThe preceding principles converge on a final insight that unifies them all: scale transforms systems qualitatively, not just quantitatively. Systems that work at modest scale exhibit qualitatively different behaviors at production scale. A training job running on 8 GPUs may encounter communication bottlenecks, load imbalance, or synchronization overhead when scaled to 8,000 GPUs that did not manifest at smaller scale. With 100,000 concurrent user sessions, edge cases that occur one in a million times happen hundreds of times daily.\nThis principle explains why distributed ML requires fundamentally different engineering approaches. The techniques that optimize single-machine performance, while necessary, prove insufficient. New phenomena emerge: stragglers that bottleneck clusters, network partitions that split training, and heterogeneity across hardware generations that complicates load balancing.",
    "crumbs": [
      "Volume II: Advanced",
      "Responsible AI at Scale",
      "Conclusion"
    ]
  },
  {
    "objectID": "contents/vol2/conclusion/conclusion.html#sec-vol2-conclusion-complete-system",
    "href": "contents/vol2/conclusion/conclusion.html#sec-vol2-conclusion-complete-system",
    "title": "Conclusion",
    "section": "The Complete Production System",
    "text": "The Complete Production System\nThese six principles operate not in isolation but as interconnected forces shaping system design. The chapters of this textbook collectively describe a production ML system as an integrated whole, where no component operates independently. Each creates requirements and constraints that ripple through the entire stack.\nInfrastructure Provides the Foundation\nThe infrastructure examined in ?@sec-infrastructure aggregates computational resources through carefully designed power, cooling, and networking systems. Accelerator clusters connected by high-bandwidth, low-latency networks enable the collective operations that distributed training requires. Without appropriate infrastructure, the distributed techniques explored throughout this textbook cannot achieve their potential.\nStorage and Communication Enable Distribution\nThe storage systems in ?@sec-storage provide capacity and bandwidth to serve training data at rates matching accelerator throughput. The communication systems in ?@sec-communication connect distributed workers through collective operations that synchronize computation. These systems must be designed together: storage bandwidth that exceeds communication capacity wastes resources, while communication paths that exceed storage throughput leave accelerators waiting.\nDistributed Training Transforms Clusters into Capability\nThe distributed training techniques in ?@sec-distributed-training convert clusters into systems capable of training models that exceed single-device capabilities. Data parallelism, model parallelism, and pipeline parallelism each address different constraints. Hybrid strategies combine these approaches for large language models and recommendation systems.\nInference and Operations Deliver Value\nModels create value only when they serve predictions, a transition examined in ?@sec-inference-at-scale. The operational practices in ?@sec-ops-scale enable systems to evolve as distributions shift and requirements change. Security and privacy techniques in ?@sec-security-privacy protect against threats unique to ML systems. Responsible deployment practices in ?@sec-responsible-ai, ?@sec-sustainable-ai, and ?@sec-ai-good ensure that capability serves human welfare.",
    "crumbs": [
      "Volume II: Advanced",
      "Responsible AI at Scale",
      "Conclusion"
    ]
  },
  {
    "objectID": "contents/vol2/conclusion/conclusion.html#sec-vol2-conclusion-path-forward",
    "href": "contents/vol2/conclusion/conclusion.html#sec-vol2-conclusion-path-forward",
    "title": "Conclusion",
    "section": "The Path Forward",
    "text": "The Path Forward\nWhile the specific technologies examined in this textbook will be superseded by new hardware, frameworks, and techniques, the six principles identified here will intensify rather than diminish. Understanding how these principles will evolve prepares you for the challenges ahead.\nScale Continues Increasing\nModel scale grows with each generation. Frontier models already require thousands of GPUs training for months. Future systems will require innovations in parallelism strategies, communication efficiency, and memory optimization. The communication bottleneck will intensify, demanding novel interconnects and algorithmic innovations.\nGovernance Requirements Intensify\nAs ML systems take on increasingly consequential roles, regulatory frameworks will mature. The responsible AI techniques in ?@sec-responsible-ai will evolve from best practices to compliance requirements. Engineers who understand these requirements will be better positioned than those who treat them as afterthoughts.\nSustainability Becomes Constraint\nCarbon accounting will become standard practice, with emissions factored into architectural decisions alongside performance and cost. The most impactful systems may prove to be those enabling broader sustainability: climate models, smart grid optimization, and efficiency improvements where ML capabilities amplify benefits.",
    "crumbs": [
      "Volume II: Advanced",
      "Responsible AI at Scale",
      "Conclusion"
    ]
  },
  {
    "objectID": "contents/vol2/conclusion/conclusion.html#sec-vol2-conclusion-mastered",
    "href": "contents/vol2/conclusion/conclusion.html#sec-vol2-conclusion-mastered",
    "title": "Conclusion",
    "section": "What You Have Mastered",
    "text": "What You Have Mastered\nCompleting this textbook positions you to contribute to ML systems engineering at multiple levels.\nYou understand distributed systems. You understand how parallelism strategies enable training at scales impossible for single machines. You can analyze communication patterns, select network architectures, and design fault-tolerant systems.\nYou can operate production systems. You understand monitoring, deployment, and incident response practices. You can detect performance degradation, manage model updates, and respond to failures.\nYou can address governance requirements. You understand the threat landscape for ML systems and defenses that mitigate attacks. You can implement privacy-preserving techniques and establish accountability frameworks.\nYou can ensure responsible deployment. You understand how to evaluate systems for fairness and assess environmental impact. You can integrate technical excellence with ethical commitment.",
    "crumbs": [
      "Volume II: Advanced",
      "Responsible AI at Scale",
      "Conclusion"
    ]
  },
  {
    "objectID": "contents/vol2/conclusion/conclusion.html#sec-vol2-conclusion-engineering-intelligence",
    "href": "contents/vol2/conclusion/conclusion.html#sec-vol2-conclusion-engineering-intelligence",
    "title": "Conclusion",
    "section": "Engineering Intelligence at Scale",
    "text": "Engineering Intelligence at Scale\nThe systems you will build affect human lives at unprecedented scale. Recommendation systems shape what billions of people see. Medical AI influences healthcare decisions. Climate models inform policy affecting generations. The engineering decisions you make carry ethical weight extending far beyond technical metrics.\nThis responsibility demands combining technical depth with operational maturity and ethical commitment. We need practitioners who can train models across thousands of GPUs and who understand why some populations should not face automated decisions without oversight. We need architects who can design fault-tolerant distributed systems and who recognize when systems should include human judgment.\nThe intelligent systems that will define this century await engineering leadership. Climate models, medical systems, educational technologies, and accessibility tools all require the capabilities developed in this textbook: the technical depth to make them work, the systems thinking to make them scale, the operational maturity to make them reliable, and the ethical commitment to make them beneficial.\nGo build systems that scale. Go build systems that endure. Go build systems that serve humanity well.\nProf. Vijay Janapa Reddi, Harvard University\n\n\n\n\n\n\nImportantKey Takeaways\n\n\n\n\nSix principles define distributed ML systems engineering: communication dominance, routine failure, infrastructure determination, responsible engineering, sustainability constraints, and qualitative scale effects\nThe transition from single-machine to distributed systems is qualitative, not merely quantitative: new phenomena emerge at scale that require distinct engineering approaches\nProduction ML systems integrate infrastructure, distributed training, fault tolerance, operations, security, and governance as an interconnected whole where no component operates in isolation\nThe engineering decisions made in building ML systems carry ethical weight extending far beyond technical metrics, affecting billions of lives through recommendation systems, medical AI, climate models, and accessibility tools",
    "crumbs": [
      "Volume II: Advanced",
      "Responsible AI at Scale",
      "Conclusion"
    ]
  }
]