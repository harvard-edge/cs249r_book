<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.8.25">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>ML Systems Textbook</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
html { -webkit-text-size-adjust: 100%; }
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="../../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../../">
<link href="../../../contents/vol2/communication/communication.html" rel="next">
<link href="../../../contents/vol2/storage/storage.html" rel="prev">
<link href="../../../assets/images/icons/favicon.png" rel="icon" type="image/png">
<script src="../../../site_libs/quarto-html/quarto.js" type="module"></script>
<script src="../../../site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="../../../site_libs/quarto-html/axe/axe-check.js" type="module"></script>
<script src="../../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../../site_libs/quarto-html/quarto-syntax-highlighting-90ccfe836b84d5d55b6a529ab96c6775.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="../../../site_libs/quarto-html/quarto-syntax-highlighting-dark-90ccfe836b84d5d55b6a529ab96c6775.css" rel="stylesheet" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<link href="../../../site_libs/quarto-html/quarto-syntax-highlighting-90ccfe836b84d5d55b6a529ab96c6775.css" rel="stylesheet" class="quarto-color-scheme-extra" id="quarto-text-highlighting-styles">
<script src="../../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../../site_libs/bootstrap/bootstrap-e25a564e8f017b9ed72db62fe60e0389.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="../../../site_libs/bootstrap/bootstrap-dark-ebbfa67f8ed217fdb894da17fb17d187.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="light">
<link href="../../../site_libs/bootstrap/bootstrap-e25a564e8f017b9ed72db62fe60e0389.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme-extra" id="quarto-bootstrap" data-mode="light">
<link href="../../../site_libs/quarto-contrib/foldbox/foldbox.css" rel="stylesheet">
<script src="../../../site_libs/quarto-contrib/glightbox/glightbox.min.js"></script>
<link href="../../../site_libs/quarto-contrib/glightbox/glightbox.min.css" rel="stylesheet">
<link href="../../../site_libs/quarto-contrib/glightbox/lightbox.css" rel="stylesheet">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "/"
  ],
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script async="" src="https://www.googletagmanager.com/gtag/js?id=G-M21L0CBCVN"></script>

<script type="text/javascript">

window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'G-M21L0CBCVN', { 'anonymize_ip': true});
</script>
<script type="application/json" class="js-hypothesis-config">
{
  "theme": "clean",
  "openSidebar": false
}
</script>
<script async="" src="https://hypothes.is/embed.js"></script>
<script>
  window.document.addEventListener("DOMContentLoaded", function (_event) {
    document.body.classList.add('hypothesis-enabled');
  });
</script>
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css">
<link rel="preconnect" href="https://fonts.googleapis.com">
<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin="">
<link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&amp;family=JetBrains+Mono:wght@400;500&amp;display=swap" rel="stylesheet">
<link rel="manifest" href="../../../site.webmanifest">
<link rel="apple-touch-icon" href="../../../assets/images/icons/favicon.png">
<meta name="theme-color" content="#A51C30">

<script type="module" src="../../../tools/scripts/socratiQ/bundle.js" defer=""></script>
<script src="../../../assets/scripts/sidebar-auto-collapse.js" defer=""></script>
<script src="../../../assets/scripts/version-link.js" defer=""></script>
<script src="../../../assets/scripts/subscribe-modal.js" defer=""></script>
<style>
.callout-example {
  --color1: #F0F8F6;
  --color2: #148F77;
}
.callout-chapter-connection {
  --color1: #FDF2F7;
  --color2: #A51C30;
}
.callout-quiz-question {
  --color1: #F0F0F8;
  --color2: #5B4B8A;
}
.callout-quiz-answer {
  --color1: #E8F2EA;
  --color2: #4a7c59;
}
.callout-colab {
  --color1: #FFF5E6;
  --color2: #FF6B35;
}
.callout-resource-videos {
  --color1: #E0F2F1;
  --color2: #20B2AA;
}
.callout-code {
  --color1: #F2F4F8;
  --color2: #D1D7E0;
}
.callout-resource-exercises {
  --color1: #E0F2F1;
  --color2: #20B2AA;
}
.callout-definition {
  --color1: #F0F4F8;
  --color2: #1B4F72;
}
.callout-resource-slides {
  --color1: #E0F2F1;
  --color2: #20B2AA;
}
</style>
<style>
details.callout-example > summary::before {
  background-image: url("../../../assets/images/icons/callouts/icon_callout_example.png");
}
details.callout-chapter-connection > summary::before {
  background-image: url("../../../assets/images/icons/callouts/icon_callout_chapter_connection.png");
}
details.callout-quiz-question > summary::before {
  background-image: url("../../../assets/images/icons/callouts/icon_callout_quiz_question.png");
}
details.callout-quiz-answer > summary::before {
  background-image: url("../../../assets/images/icons/callouts/icon_callout_quiz_answer.png");
}
details.callout-colab > summary::before {
  background-image: url("../../../assets/images/icons/callouts/icon_callout_colab.png");
}
details.callout-resource-videos > summary::before {
  background-image: url("../../../assets/images/icons/callouts/icon_callout_resource_videos.png");
}
details.callout-code > summary::before {
  background-image: url("../../../assets/images/icons/callouts/icon_callout_code.png");
}
details.callout-resource-exercises > summary::before {
  background-image: url("../../../assets/images/icons/callouts/icon_callout_resource_exercises.png");
}
details.callout-definition > summary::before {
  background-image: url("../../../assets/images/icons/callouts/icon_callout_definition.png");
}
details.callout-resource-slides > summary::before {
  background-image: url("../../../assets/images/icons/callouts/icon_callout_resource_slides.png");
}
</style>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN" && texText && texText.data) {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<meta property="og:title" content="ML Systems Textbook">
<meta property="og:image" content="https://mlsysbook.ai/book/contents/vol2/distributed_training/assets/images/covers/cover-hardcover-book.png">
<meta property="og:site_name" content="Machine Learning Systems">
<meta property="og:locale" content="en_US">
<meta name="twitter:title" content="ML Systems Textbook">
<meta name="twitter:image" content="https://mlsysbook.ai/book/contents/vol2/distributed_training/assets/images/covers/cover-hardcover-book.png">
<meta name="twitter:card" content="summary_large_image">
</head>

<body class="nav-sidebar floating nav-fixed slimcontent quarto-light"><script id="quarto-html-before-body" type="application/javascript">
    const toggleBodyColorMode = (bsSheetEl) => {
      const mode = bsSheetEl.getAttribute("data-mode");
      const bodyEl = window.document.querySelector("body");
      if (mode === "dark") {
        bodyEl.classList.add("quarto-dark");
        bodyEl.classList.remove("quarto-light");
      } else {
        bodyEl.classList.add("quarto-light");
        bodyEl.classList.remove("quarto-dark");
      }
    }
    const toggleBodyColorPrimary = () => {
      const bsSheetEl = window.document.querySelector("link#quarto-bootstrap:not([rel=disabled-stylesheet])");
      if (bsSheetEl) {
        toggleBodyColorMode(bsSheetEl);
      }
    }
    const setColorSchemeToggle = (alternate) => {
      const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
      for (let i=0; i < toggles.length; i++) {
        const toggle = toggles[i];
        if (toggle) {
          if (alternate) {
            toggle.classList.add("alternate");
          } else {
            toggle.classList.remove("alternate");
          }
        }
      }
    };
    const toggleColorMode = (alternate) => {
      // Switch the stylesheets
      const primaryStylesheets = window.document.querySelectorAll('link.quarto-color-scheme:not(.quarto-color-alternate)');
      const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
      manageTransitions('#quarto-margin-sidebar .nav-link', false);
      if (alternate) {
        // note: dark is layered on light, we don't disable primary!
        enableStylesheet(alternateStylesheets);
        for (const sheetNode of alternateStylesheets) {
          if (sheetNode.id === "quarto-bootstrap") {
            toggleBodyColorMode(sheetNode);
          }
        }
      } else {
        disableStylesheet(alternateStylesheets);
        enableStylesheet(primaryStylesheets)
        toggleBodyColorPrimary();
      }
      manageTransitions('#quarto-margin-sidebar .nav-link', true);
      // Switch the toggles
      setColorSchemeToggle(alternate)
      // Hack to workaround the fact that safari doesn't
      // properly recolor the scrollbar when toggling (#1455)
      if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
        manageTransitions("body", false);
        window.scrollTo(0, 1);
        setTimeout(() => {
          window.scrollTo(0, 0);
          manageTransitions("body", true);
        }, 40);
      }
    }
    const disableStylesheet = (stylesheets) => {
      for (let i=0; i < stylesheets.length; i++) {
        const stylesheet = stylesheets[i];
        stylesheet.rel = 'disabled-stylesheet';
      }
    }
    const enableStylesheet = (stylesheets) => {
      for (let i=0; i < stylesheets.length; i++) {
        const stylesheet = stylesheets[i];
        if(stylesheet.rel !== 'stylesheet') { // for Chrome, which will still FOUC without this check
          stylesheet.rel = 'stylesheet';
        }
      }
    }
    const manageTransitions = (selector, allowTransitions) => {
      const els = window.document.querySelectorAll(selector);
      for (let i=0; i < els.length; i++) {
        const el = els[i];
        if (allowTransitions) {
          el.classList.remove('notransition');
        } else {
          el.classList.add('notransition');
        }
      }
    }
    const isFileUrl = () => {
      return window.location.protocol === 'file:';
    }
    const hasAlternateSentinel = () => {
      let styleSentinel = getColorSchemeSentinel();
      if (styleSentinel !== null) {
        return styleSentinel === "alternate";
      } else {
        return false;
      }
    }
    const setStyleSentinel = (alternate) => {
      const value = alternate ? "alternate" : "default";
      if (!isFileUrl()) {
        window.localStorage.setItem("quarto-color-scheme", value);
      } else {
        localAlternateSentinel = value;
      }
    }
    const getColorSchemeSentinel = () => {
      if (!isFileUrl()) {
        const storageValue = window.localStorage.getItem("quarto-color-scheme");
        return storageValue != null ? storageValue : localAlternateSentinel;
      } else {
        return localAlternateSentinel;
      }
    }
    const toggleGiscusIfUsed = (isAlternate, darkModeDefault) => {
      const baseTheme = document.querySelector('#giscus-base-theme')?.value ?? 'light';
      const alternateTheme = document.querySelector('#giscus-alt-theme')?.value ?? 'dark';
      let newTheme = '';
      if(authorPrefersDark) {
        newTheme = isAlternate ? baseTheme : alternateTheme;
      } else {
        newTheme = isAlternate ? alternateTheme : baseTheme;
      }
      const changeGiscusTheme = () => {
        // From: https://github.com/giscus/giscus/issues/336
        const sendMessage = (message) => {
          const iframe = document.querySelector('iframe.giscus-frame');
          if (!iframe) return;
          iframe.contentWindow.postMessage({ giscus: message }, 'https://giscus.app');
        }
        sendMessage({
          setConfig: {
            theme: newTheme
          }
        });
      }
      const isGiscussLoaded = window.document.querySelector('iframe.giscus-frame') !== null;
      if (isGiscussLoaded) {
        changeGiscusTheme();
      }
    };
    const authorPrefersDark = false;
    const queryPrefersDark = window.matchMedia('(prefers-color-scheme: dark)');
    const darkModeDefault = queryPrefersDark.matches;
      document.querySelector('link#quarto-text-highlighting-styles.quarto-color-scheme-extra').rel = 'disabled-stylesheet';
      document.querySelector('link#quarto-bootstrap.quarto-color-scheme-extra').rel = 'disabled-stylesheet';
    let localAlternateSentinel = darkModeDefault ? 'alternate' : 'default';
    // Dark / light mode switch
    window.quartoToggleColorScheme = () => {
      // Read the current dark / light value
      let toAlternate = !hasAlternateSentinel();
      toggleColorMode(toAlternate);
      setStyleSentinel(toAlternate);
      toggleGiscusIfUsed(toAlternate, darkModeDefault);
      window.dispatchEvent(new Event('resize'));
    };
    queryPrefersDark.addEventListener("change", e => {
      if(window.localStorage.getItem("quarto-color-scheme") !== null)
        return;
      const alternate = e.matches
      toggleColorMode(alternate);
      localAlternateSentinel = e.matches ? 'alternate' : 'default'; // this is used alongside local storage!
      toggleGiscusIfUsed(alternate, darkModeDefault);
    });
    // Switch to dark mode if need be
    if (hasAlternateSentinel()) {
      toggleColorMode(true);
    } else {
      toggleColorMode(false);
    }
  </script>

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-md " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a href="../../../index.html" class="navbar-brand navbar-brand-logo">
    <img src="../../../assets/images/icons/favicon.png" alt="" class="navbar-logo light-content">
    <img src="../../../assets/images/icons/favicon.png" alt="" class="navbar-logo dark-content">
    </a>
    <a class="navbar-brand" href="../../../index.html">
    <span class="navbar-title">Machine Learning Systems</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-textbook" role="link" data-bs-toggle="dropdown" aria-expanded="false">
 <span class="menu-text">Textbook</span>
    </a>
    <ul class="dropdown-menu" aria-labelledby="nav-menu-textbook">    
        <li>
    <a class="dropdown-item" href="../../../../book/"><i class="bi bi-book-half" role="img">
</i> 
 <span class="dropdown-text">Textbook</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../../../tinytorch/"><i class="bi bi-fire" role="img">
</i> 
 <span class="dropdown-text">TinyTorch</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../../../kits/"><i class="bi bi-cpu" role="img">
</i> 
 <span class="dropdown-text">Hardware Kits</span></a>
  </li>  
        <li><hr class="dropdown-divider"></li>
        <li>
    <a class="dropdown-item" href="../../../../collabs/"><i class="bi bi-lightbulb" role="img">
</i> 
 <span class="dropdown-text">Co-Labs (Coming 2026)</span></a>
  </li>  
    </ul>
  </li>
</ul>
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-downloads" role="link" data-bs-toggle="dropdown" aria-expanded="false">
      <i class="bi bi-download" role="img">
</i> 
 <span class="menu-text">Downloads</span>
    </a>
    <ul class="dropdown-menu dropdown-menu-end" aria-labelledby="nav-menu-downloads">    
        <li>
    <a class="dropdown-item" href="../../../assets/downloads/Machine-Learning-Systems.pdf" target="_blank"><i class="bi bi-file-pdf" role="img">
</i> 
 <span class="dropdown-text">Textbook PDF</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../../assets/downloads/Machine-Learning-Systems.epub" target="_blank"><i class="bi bi-journal-text" role="img">
</i> 
 <span class="dropdown-text">Textbook EPUB</span></a>
  </li>  
    </ul>
  </li>
  <li class="nav-item">
    <a class="nav-link" href="https://github.com/harvard-edge/cs249r_book#support-this-work" target="_blank"> <i class="bi bi-star" role="img">
</i> 
<span class="menu-text">Star</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="https://opencollective.com/mlsysbook" target="_blank"> <i class="bi bi-heart" role="img">
</i> 
<span class="menu-text">Support</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../../#subscribe"> <i class="bi bi-envelope" role="img">
</i> 
<span class="menu-text">Subscribe</span></a>
  </li>  
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-github" role="link" data-bs-toggle="dropdown" aria-expanded="false">
      <i class="bi bi-github" role="img">
</i> 
 <span class="menu-text">GitHub</span>
    </a>
    <ul class="dropdown-menu dropdown-menu-end" aria-labelledby="nav-menu-github">    
        <li>
    <a class="dropdown-item" href="https://github.com/harvard-edge/cs249r_book" target="_blank"><i class="bi bi-pencil" role="img">
</i> 
 <span class="dropdown-text">Edit this page</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="https://github.com/harvard-edge/cs249r_book/issues/new" target="_blank"><i class="bi bi-bug" role="img">
</i> 
 <span class="dropdown-text">Report an issue</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="https://github.com/harvard-edge/cs249r_book/discussions" target="_blank"><i class="bi bi-chat" role="img">
</i> 
 <span class="dropdown-text">Discussions</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="https://github.com/harvard-edge/cs249r_book" target="_blank"><i class="bi bi-code" role="img">
</i> 
 <span class="dropdown-text">View source</span></a>
  </li>  
    </ul>
  </li>
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
  <a href="" class="quarto-color-scheme-toggle quarto-navigation-tool  px-1" onclick="window.quartoToggleColorScheme(); return false;" title="Toggle dark mode"><i class="bi"></i></a>
</div>
      </div> <!-- /container-fluid -->
    </nav>
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item">Volume II: Advanced</li><li class="breadcrumb-item"><a href="../../../contents/vol2/distributed_training/distributed_training.html">Distributed Training</a></li><li class="breadcrumb-item"><a href="../../../contents/vol2/distributed_training/distributed_training.html">Distributed Training Systems</a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
    </div>
  </nav>
  <div id="quarto-announcement" data-announcement-id="552a22699935f4b8120ea0a4f833ae38" class="alert alert-primary hidden"><i class="bi bi-megaphone quarto-announcement-icon"></i><div class="quarto-announcement-content">
<p>ðŸ“š <strong>Two-Volume Edition:</strong> Now organized as Volume I (Foundations) and Volume II (Advanced). <a href="contents/frontmatter/about/about.qmd">Learn more â†’</a><br> ðŸ”¥ <strong>New Release:</strong> TinyTorch ML framework. Donâ€™t import torch. <a href="https://mlsysbook.ai/tinytorch">Build it â†’</a><br> ðŸ“¦ <strong>Hardware Kits:</strong> Arduino, Seeed &amp; Raspberry Pi labs. <a href="https://mlsysbook.ai/kits">Explore â†’</a></p>
</div><i class="bi bi-x-lg quarto-announcement-action"></i></div>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto page-columns page-full">
    <div class="pt-lg-2 mt-2 text-left sidebar-header sidebar-header-stacked">
      <a href="../../../index.html" class="sidebar-logo-link">
      </a>
      </div>
    <div class="sidebar-menu-container page-columns page-full"> 
    <ul class="list-unstyled mt-1 page-columns page-full">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Homepage</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/frontmatter/foreword.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Foreword</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/frontmatter/about/about.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">About</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/frontmatter/changelog/changelog.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Book Changelog</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/frontmatter/acknowledgements/acknowledgements.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Acknowledgements</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/frontmatter/socratiq/socratiq.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">SocratiQ AI</span></a>
  </div>
</li>
        <li class="px-0"><hr class="sidebar-divider hi "></li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true">
 <span class="menu-text">Volume I: Introduction</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="false">
 <span class="menu-text">Foundations</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth2 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/vol1/introduction/introduction.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Introduction</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/vol1/ml_systems/ml_systems.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">ML Systems</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/vol1/workflow/workflow.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">AI Workflow</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/vol1/data_engineering/data_engineering.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Data Engineering</span></a>
  </div>
</li>
      </ul>
  </li>
          <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="false">
 <span class="menu-text">Build</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth2 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/vol1/dl_primer/dl_primer.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Deep Learning Primer</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/vol1/dnn_architectures/dnn_architectures.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Neural Network Architectures</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/vol1/frameworks/frameworks.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">AI Frameworks</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/vol1/training/training.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">AI Training</span></a>
  </div>
</li>
      </ul>
  </li>
          <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" role="navigation" aria-expanded="false">
 <span class="menu-text">Optimize</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" role="navigation" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-4" class="collapse list-unstyled sidebar-section depth2 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/vol1/efficient_ai/efficient_ai.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Efficient AI</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/vol1/optimizations/optimizations.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Model Optimizations</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/vol1/hw_acceleration/hw_acceleration.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">AI Acceleration</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/vol1/benchmarking/benchmarking.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Benchmarking AI</span></a>
  </div>
</li>
      </ul>
  </li>
          <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" role="navigation" aria-expanded="false">
 <span class="menu-text">Deploy</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" role="navigation" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-5" class="collapse list-unstyled sidebar-section depth2 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/vol1/serving/serving.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Serving</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/vol1/ops/ops.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">ML Operations</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/vol1/responsible_engr/responsible_engr.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Responsible Engineering</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/vol1/conclusion/conclusion.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Conclusion</span></a>
  </div>
</li>
      </ul>
  </li>
      </ul>
  </li>
        <li class="px-0"><hr class="sidebar-divider hi "></li>
        <li class="sidebar-item sidebar-item-section page-columns page-full">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-6" role="navigation" aria-expanded="true">
 <span class="menu-text">Volume II: Advanced</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-6" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-6" class="collapse list-unstyled sidebar-section depth1 show page-columns page-full">  
          <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-7" role="navigation" aria-expanded="false">
 <span class="menu-text">Foundations of Scale</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-7" role="navigation" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-7" class="collapse list-unstyled sidebar-section depth2 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/vol2/introduction/introduction.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Introduction</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/vol2/infrastructure/infrastructure.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Large-Scale ML Infrastructure</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/vol2/storage/storage.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Storage Systems for ML</span></a>
  </div>
</li>
      </ul>
  </li>
          <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-8" role="navigation" aria-expanded="true">
 <span class="menu-text">Distributed Training</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-8" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-8" class="collapse list-unstyled sidebar-section depth2 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/vol2/distributed_training/distributed_training.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text">Distributed Training Systems</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/vol2/communication/communication.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Communication and Collective Operations</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/vol2/fault_tolerance/fault_tolerance.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Fault Tolerance and Reliability</span></a>
  </div>
</li>
      </ul>
  </li>
          <li class="sidebar-item sidebar-item-section page-columns page-full">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-9" role="navigation" aria-expanded="false">
 <span class="menu-text">Deployment at Scale</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-9" role="navigation" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-9" class="collapse list-unstyled sidebar-section depth2 page-columns page-full">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/vol2/inference/inference.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Inference at Scale</span></a>
  </div>
</li>
          <li class="sidebar-item page-columns page-full">
  <div class="sidebar-item-container page-columns page-full"> 
  <a href="../../../contents/vol2/edge_intelligence/edge_intelligence.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><!--</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="/contents/vol2/ops_scale/ops_scale.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">ML Operations at Scale</span></a>
  </div>
</li>
      </ul>
  </li>
          <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-10"  role="navigation" aria-expanded="false">
 <span class="menu-text">Production Concerns</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-10" role="navigation" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-10" class="collapse list-unstyled sidebar-section depth2 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="/contents/vol2/privacy_security/privacy_security.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Security & Privacy</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="/contents/vol2/robust_ai/robust_ai.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Robust AI</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="/contents/vol2/sustainable_ai/sustainable_ai.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Sustainable AI</span></a>
  </div>
</li>
      </ul>
  </li>
          <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-11"  role="navigation" aria-expanded="false">
 <span class="menu-text">Responsible AI at Scale</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-11" role="navigation" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-11" class="collapse list-unstyled sidebar-section depth2 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="/contents/vol2/responsible_ai/responsible_ai.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Responsible AI</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="/contents/vol2/ai_for_good/ai_for_good.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">AI for Good</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="/contents/vol2/frontiers/frontiers.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">AGI Systems</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="/contents/vol2/conclusion/conclusion.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Conclusion</span></a>
  </div>
</li>
      </ul>
  </li>
      </ul>
  </li>
        <li class="px-0"><hr class="sidebar-divider hi "></li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-12"  role="navigation" aria-expanded="true">
 <span class="menu-text">Glossary</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-12" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-12" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="/contents/backmatter/glossary/glossary.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Complete Glossary</span></a>
  </div>
</li>
      </ul>
  </li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" ></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active" data-toc-expanded="99"><a href="../../../contents/vol2/edge_intelligence/edge_intelligence.html" class="sidebar-item-text sidebar-link">
    <h2 id="toc-title">On this page</h2>
   
  </a><ul class="collapse"><a href="../../../contents/vol2/edge_intelligence/edge_intelligence.html" class="sidebar-item-text sidebar-link nav-link active" data-scroll-target="../../../contents/vol2/edge_intelligence/edge_intelligence.html">
  </a><li><a href="../../../contents/vol2/edge_intelligence/edge_intelligence.html" class="sidebar-item-text sidebar-link nav-link" data-scroll-target="../../../contents/vol2/edge_intelligence/edge_intelligence.html"></a><a href="#sec-distributed-training" id="toc-sec-distributed-training" class="nav-link" data-scroll-target="#sec-distributed-training">Distributed Training Systems</a>
  <ul class="collapse">
  <li><a href="#purpose" id="toc-purpose" class="nav-link" data-scroll-target="#purpose">Purpose</a></li>
  <li><a href="#sec-distributed-training-multimachine-scaling-fundamentals" id="toc-sec-distributed-training-multimachine-scaling-fundamentals" class="nav-link" data-scroll-target="#sec-distributed-training-multimachine-scaling-fundamentals">Multi-Machine Scaling Fundamentals</a>
  <ul class="collapse">
  <li><a href="#sec-distributed-training-multimachine-training-requirements" id="toc-sec-distributed-training-multimachine-training-requirements" class="nav-link" data-scroll-target="#sec-distributed-training-multimachine-training-requirements">Multi-Machine Training Requirements</a></li>
  <li><a href="#sec-distributed-training-complexity-tradeoffs" id="toc-sec-distributed-training-complexity-tradeoffs" class="nav-link" data-scroll-target="#sec-distributed-training-complexity-tradeoffs">Distributed Training Complexity Trade-offs</a></li>
  <li><a href="#sec-distributed-training-singlemachine-distributed-transition" id="toc-sec-distributed-training-singlemachine-distributed-transition" class="nav-link" data-scroll-target="#sec-distributed-training-singlemachine-distributed-transition">Single-Machine to Distributed Transition</a></li>
  </ul></li>
  <li><a href="#sec-distributed-training-fundamentals" id="toc-sec-distributed-training-fundamentals" class="nav-link" data-scroll-target="#sec-distributed-training-fundamentals">Distributed Training Fundamentals</a></li>
  <li><a href="#sec-distributed-training-efficiency-metrics" id="toc-sec-distributed-training-efficiency-metrics" class="nav-link" data-scroll-target="#sec-distributed-training-efficiency-metrics">Distributed Training Efficiency Metrics</a></li>
  <li><a href="#sec-distributed-training-data-parallelism" id="toc-sec-distributed-training-data-parallelism" class="nav-link" data-scroll-target="#sec-distributed-training-data-parallelism">Data Parallelism</a>
  <ul class="collapse">
  <li><a href="#sec-distributed-training-data-parallelism-implementation" id="toc-sec-distributed-training-data-parallelism-implementation" class="nav-link" data-scroll-target="#sec-distributed-training-data-parallelism-implementation">Data Parallelism Implementation</a>
  <ul class="collapse">
  <li><a href="#sec-distributed-training-dataset-splitting" id="toc-sec-distributed-training-dataset-splitting" class="nav-link" data-scroll-target="#sec-distributed-training-dataset-splitting">Dataset Splitting</a></li>
  <li><a href="#sec-distributed-training-device-forward-pass" id="toc-sec-distributed-training-device-forward-pass" class="nav-link" data-scroll-target="#sec-distributed-training-device-forward-pass">Device Forward Pass</a></li>
  <li><a href="#sec-distributed-training-backward-pass-calculation" id="toc-sec-distributed-training-backward-pass-calculation" class="nav-link" data-scroll-target="#sec-distributed-training-backward-pass-calculation">Backward Pass and Calculation</a></li>
  <li><a href="#sec-distributed-training-gradient-synchronization" id="toc-sec-distributed-training-gradient-synchronization" class="nav-link" data-scroll-target="#sec-distributed-training-gradient-synchronization">Gradient Synchronization</a></li>
  <li><a href="#sec-distributed-training-sync-models" id="toc-sec-distributed-training-sync-models" class="nav-link" data-scroll-target="#sec-distributed-training-sync-models">Synchronization Models</a></li>
  <li><a href="#sec-distributed-training-barrier-failures" id="toc-sec-distributed-training-barrier-failures" class="nav-link" data-scroll-target="#sec-distributed-training-barrier-failures">Barrier Semantics and Failure Modes</a></li>
  <li><a href="#sec-distributed-training-parameter-updating" id="toc-sec-distributed-training-parameter-updating" class="nav-link" data-scroll-target="#sec-distributed-training-parameter-updating">Parameter Updating</a></li>
  </ul></li>
  <li><a href="#sec-distributed-training-data-parallelism-advantages" id="toc-sec-distributed-training-data-parallelism-advantages" class="nav-link" data-scroll-target="#sec-distributed-training-data-parallelism-advantages">Data Parallelism Advantages</a></li>
  <li><a href="#sec-distributed-training-data-parallelism-limitations" id="toc-sec-distributed-training-data-parallelism-limitations" class="nav-link" data-scroll-target="#sec-distributed-training-data-parallelism-limitations">Data Parallelism Limitations</a></li>
  <li><a href="#sec-distributed-training-zero-fsdp" id="toc-sec-distributed-training-zero-fsdp" class="nav-link" data-scroll-target="#sec-distributed-training-zero-fsdp">Memory-Efficient Data Parallelism: ZeRO and FSDP</a></li>
  </ul></li>
  <li><a href="#sec-distributed-training-model-parallelism" id="toc-sec-distributed-training-model-parallelism" class="nav-link" data-scroll-target="#sec-distributed-training-model-parallelism">Model Parallelism</a>
  <ul class="collapse">
  <li><a href="#sec-distributed-training-model-parallelism-implementation" id="toc-sec-distributed-training-model-parallelism-implementation" class="nav-link" data-scroll-target="#sec-distributed-training-model-parallelism-implementation">Model Parallelism Implementation</a>
  <ul class="collapse">
  <li><a href="#sec-distributed-training-model-partitioning" id="toc-sec-distributed-training-model-partitioning" class="nav-link" data-scroll-target="#sec-distributed-training-model-partitioning">Model Partitioning</a></li>
  <li><a href="#sec-distributed-training-model-forward-pass" id="toc-sec-distributed-training-model-forward-pass" class="nav-link" data-scroll-target="#sec-distributed-training-model-forward-pass">Model Forward Pass</a></li>
  <li><a href="#sec-distributed-training-backward-pass-calculation-model" id="toc-sec-distributed-training-backward-pass-calculation-model" class="nav-link" data-scroll-target="#sec-distributed-training-backward-pass-calculation-model">Backward Pass and Calculation</a></li>
  <li><a href="#sec-distributed-training-parameter-updates-model" id="toc-sec-distributed-training-parameter-updates-model" class="nav-link" data-scroll-target="#sec-distributed-training-parameter-updates-model">Parameter Updates</a></li>
  <li><a href="#sec-distributed-training-iterative-process" id="toc-sec-distributed-training-iterative-process" class="nav-link" data-scroll-target="#sec-distributed-training-iterative-process">Iterative Process</a></li>
  </ul></li>
  <li><a href="#sec-distributed-training-parallelism-variations" id="toc-sec-distributed-training-parallelism-variations" class="nav-link" data-scroll-target="#sec-distributed-training-parallelism-variations">Parallelism Variations</a>
  <ul class="collapse">
  <li><a href="#sec-distributed-training-layerwise-partitioning" id="toc-sec-distributed-training-layerwise-partitioning" class="nav-link" data-scroll-target="#sec-distributed-training-layerwise-partitioning">Layer-wise Partitioning</a></li>
  <li><a href="#sec-distributed-training-pipeline-parallelism" id="toc-sec-distributed-training-pipeline-parallelism" class="nav-link" data-scroll-target="#sec-distributed-training-pipeline-parallelism">Pipeline Parallelism</a></li>
  <li><a href="#sec-distributed-training-tensor-parallelism" id="toc-sec-distributed-training-tensor-parallelism" class="nav-link" data-scroll-target="#sec-distributed-training-tensor-parallelism">Tensor Parallelism</a></li>
  </ul></li>
  <li><a href="#sec-distributed-training-model-parallelism-advantages" id="toc-sec-distributed-training-model-parallelism-advantages" class="nav-link" data-scroll-target="#sec-distributed-training-model-parallelism-advantages">Model Parallelism Advantages</a></li>
  <li><a href="#sec-distributed-training-model-parallelism-limitations" id="toc-sec-distributed-training-model-parallelism-limitations" class="nav-link" data-scroll-target="#sec-distributed-training-model-parallelism-limitations">Model Parallelism Limitations</a></li>
  </ul></li>
  <li><a href="#sec-distributed-training-hybrid-parallelism" id="toc-sec-distributed-training-hybrid-parallelism" class="nav-link" data-scroll-target="#sec-distributed-training-hybrid-parallelism">Hybrid Parallelism</a>
  <ul class="collapse">
  <li><a href="#sec-distributed-training-hybrid-parallelism-implementation" id="toc-sec-distributed-training-hybrid-parallelism-implementation" class="nav-link" data-scroll-target="#sec-distributed-training-hybrid-parallelism-implementation">Hybrid Parallelism Implementation</a>
  <ul class="collapse">
  <li><a href="#sec-distributed-training-model-data-partitioning" id="toc-sec-distributed-training-model-data-partitioning" class="nav-link" data-scroll-target="#sec-distributed-training-model-data-partitioning">Model and Data Partitioning</a></li>
  <li><a href="#sec-distributed-training-forward-pass-hybrid" id="toc-sec-distributed-training-forward-pass-hybrid" class="nav-link" data-scroll-target="#sec-distributed-training-forward-pass-hybrid">Forward Pass</a></li>
  <li><a href="#sec-distributed-training-backward-pass-gradient-calculation-hybrid" id="toc-sec-distributed-training-backward-pass-gradient-calculation-hybrid" class="nav-link" data-scroll-target="#sec-distributed-training-backward-pass-gradient-calculation-hybrid">Backward Pass and Gradient Calculation</a></li>
  <li><a href="#sec-distributed-training-parameter-updates-hybrid" id="toc-sec-distributed-training-parameter-updates-hybrid" class="nav-link" data-scroll-target="#sec-distributed-training-parameter-updates-hybrid">Parameter Updates</a></li>
  <li><a href="#sec-distributed-training-iterative-process-hybrid" id="toc-sec-distributed-training-iterative-process-hybrid" class="nav-link" data-scroll-target="#sec-distributed-training-iterative-process-hybrid">Iterative Process</a></li>
  </ul></li>
  <li><a href="#sec-distributed-training-parallelism-variations-hybrid" id="toc-sec-distributed-training-parallelism-variations-hybrid" class="nav-link" data-scroll-target="#sec-distributed-training-parallelism-variations-hybrid">Parallelism Variations</a>
  <ul class="collapse">
  <li><a href="#sec-distributed-training-hierarchical-parallelism" id="toc-sec-distributed-training-hierarchical-parallelism" class="nav-link" data-scroll-target="#sec-distributed-training-hierarchical-parallelism">Hierarchical Parallelism</a></li>
  <li><a href="#sec-distributed-training-intralayer-parallelism" id="toc-sec-distributed-training-intralayer-parallelism" class="nav-link" data-scroll-target="#sec-distributed-training-intralayer-parallelism">Intra-layer Parallelism</a></li>
  <li><a href="#sec-distributed-training-interlayer-parallelism" id="toc-sec-distributed-training-interlayer-parallelism" class="nav-link" data-scroll-target="#sec-distributed-training-interlayer-parallelism">Inter-layer Parallelism</a></li>
  </ul></li>
  <li><a href="#sec-distributed-training-hybrid-parallelism-advantages" id="toc-sec-distributed-training-hybrid-parallelism-advantages" class="nav-link" data-scroll-target="#sec-distributed-training-hybrid-parallelism-advantages">Hybrid Parallelism Advantages</a></li>
  <li><a href="#sec-distributed-training-hybrid-parallelism-limitations" id="toc-sec-distributed-training-hybrid-parallelism-limitations" class="nav-link" data-scroll-target="#sec-distributed-training-hybrid-parallelism-limitations">Hybrid Parallelism Limitations</a></li>
  </ul></li>
  <li><a href="#sec-distributed-training-parallelism-strategy-comparison" id="toc-sec-distributed-training-parallelism-strategy-comparison" class="nav-link" data-scroll-target="#sec-distributed-training-parallelism-strategy-comparison">Parallelism Strategy Comparison</a></li>
  <li><a href="#sec-distributed-training-framework-integration" id="toc-sec-distributed-training-framework-integration" class="nav-link" data-scroll-target="#sec-distributed-training-framework-integration">Framework Integration</a>
  <ul class="collapse">
  <li><a href="#sec-distributed-training-data-parallel-framework-apis" id="toc-sec-distributed-training-data-parallel-framework-apis" class="nav-link" data-scroll-target="#sec-distributed-training-data-parallel-framework-apis">Data Parallel Framework APIs</a></li>
  <li><a href="#sec-distributed-training-model-parallel-framework-support" id="toc-sec-distributed-training-model-parallel-framework-support" class="nav-link" data-scroll-target="#sec-distributed-training-model-parallel-framework-support">Model Parallel Framework Support</a></li>
  <li><a href="#sec-distributed-training-communication-primitives" id="toc-sec-distributed-training-communication-primitives" class="nav-link" data-scroll-target="#sec-distributed-training-communication-primitives">Communication Primitives</a></li>
  </ul></li>
  <li><a href="#sec-distributed-training-hardware-infrastructure" id="toc-sec-distributed-training-hardware-infrastructure" class="nav-link" data-scroll-target="#sec-distributed-training-hardware-infrastructure">Hardware Infrastructure for Scale</a>
  <ul class="collapse">
  <li><a href="#sec-distributed-training-multichip-acceleration" id="toc-sec-distributed-training-multichip-acceleration" class="nav-link" data-scroll-target="#sec-distributed-training-multichip-acceleration">Multi-Chip AI Acceleration</a>
  <ul class="collapse">
  <li><a href="#sec-distributed-training-chiplet-architectures" id="toc-sec-distributed-training-chiplet-architectures" class="nav-link" data-scroll-target="#sec-distributed-training-chiplet-architectures">Chiplet-Based Architectures</a></li>
  <li><a href="#sec-distributed-training-multi-gpu-systems" id="toc-sec-distributed-training-multi-gpu-systems" class="nav-link" data-scroll-target="#sec-distributed-training-multi-gpu-systems">Multi-GPU Systems</a></li>
  <li><a href="#sec-distributed-training-amdahl-analysis" id="toc-sec-distributed-training-amdahl-analysis" class="nav-link" data-scroll-target="#sec-distributed-training-amdahl-analysis">Communication Overhead and Amdahlâ€™s Law</a></li>
  <li><a href="#sec-distributed-training-tpu-pods" id="toc-sec-distributed-training-tpu-pods" class="nav-link" data-scroll-target="#sec-distributed-training-tpu-pods">TPU Pods</a></li>
  <li><a href="#sec-distributed-training-wafer-scale" id="toc-sec-distributed-training-wafer-scale" class="nav-link" data-scroll-target="#sec-distributed-training-wafer-scale">Wafer-Scale AI</a></li>
  </ul></li>
  <li><a href="#sec-distributed-training-scaling-tradeoffs" id="toc-sec-distributed-training-scaling-tradeoffs" class="nav-link" data-scroll-target="#sec-distributed-training-scaling-tradeoffs">Hardware Scaling Trade-offs</a></li>
  <li><a href="#sec-distributed-training-execution-strategies" id="toc-sec-distributed-training-execution-strategies" class="nav-link" data-scroll-target="#sec-distributed-training-execution-strategies">Multi-Chip Execution Strategies</a></li>
  </ul></li>
  <li><a href="#sec-distributed-training-summary" id="toc-sec-distributed-training-summary" class="nav-link" data-scroll-target="#sec-distributed-training-summary">Summary</a></li>
  <li><a href="#sec-distributed-training-fallacies-pitfalls" id="toc-sec-distributed-training-fallacies-pitfalls" class="nav-link" data-scroll-target="#sec-distributed-training-fallacies-pitfalls">Fallacies and Pitfalls</a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
</span></a><main class="content page-columns page-full" id="quarto-document-content"><a href="../../../contents/vol2/edge_intelligence/edge_intelligence.html" class="sidebar-item-text sidebar-link">


<header id="title-block-header" class="quarto-title-block default"><nav class="quarto-page-breadcrumbs quarto-title-breadcrumbs d-none d-lg-block" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item">Volume II: Advanced</li><li class="breadcrumb-item"><a href="../../../contents/vol2/distributed_training/distributed_training.html">Distributed Training</a></li><li class="breadcrumb-item"><a href="../../../contents/vol2/distributed_training/distributed_training.html">Distributed Training Systems</a></li></ol></nav>
<div class="quarto-title">
<h1 class="title">Distributed Training Systems</h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>

</a>
<!--
================================================================================
EDITORIAL GUIDELINES: MODEL-TYPE DIVERSITY FOR DISTRIBUTED TRAINING
================================================================================

CORE PRINCIPLE: Distributed training techniques apply across ALL model types,
not just LLMs. Ensure examples span the full spectrum of production ML.

MODEL-SPECIFIC PARALLELISM CONSIDERATIONS:

| Model Type      | Primary Strategy      | Key Challenge                        |
|-----------------|-----------------------|--------------------------------------|
| LLMs            | Tensor + Pipeline     | Attention memory, autoregressive     |
| Recommendation  | Embedding sharding    | Trillion-param embedding tables      |
| Vision (ResNet) | Data parallelism      | Batch size scaling, BN sync          |
| Vision (ViT)    | Tensor parallelism    | Large attention layers               |
| Scientific/GNN  | Graph partitioning    | Irregular communication patterns     |
| Speech          | Data parallelism      | Variable sequence lengths            |

REQUIRED COVERAGE FOR THIS CHAPTER:

DATA PARALLELISM:

- ResNet/EfficientNet: Classic example, batch norm synchronization
- BERT: Widely used, good baseline for transformer data parallelism
- Recommendation: Different gradient sparsity patterns

MODEL PARALLELISM:

- GPT/Megatron: Tensor parallelism for large transformers
- DLRM: Embedding table sharding (FUNDAMENTALLY DIFFERENT from tensor parallelism)
- Include: Why embedding parallelism differs from attention parallelism

PIPELINE PARALLELISM:

- GPipe: Original work on vision models
- Megatron-LM: Application to transformers
- Include: Micro-batch scheduling differences by model type

HYBRID PARALLELISM:

- 3D parallelism for LLMs (data + tensor + pipeline)
- Embedding + data parallelism for recommendation
- Include: Why different model types need different hybrid strategies

CASE STUDIES TO INCLUDE:

- Meta DLRM training infrastructure (recommendation)
- Google BERT/T5 training (NLP)
- ResNet ImageNet training (vision baseline)
- AlphaFold distributed training (scientific)

QUANTITATIVE DIVERSITY:

- Communication/computation ratios differ by model type
- Scaling efficiency curves differ (dense vs sparse models)
- Memory footprint breakdown differs (activations vs embeddings vs weights)

ANTI-PATTERNS TO AVOID:

- Framing all parallelism as "for large language models"
- Ignoring embedding table challenges unique to recommendation
- Assuming dense gradients (recommendation has sparse gradients)
- Only showing transformer examples for tensor parallelism

================================================================================
-->
<section id="sec-distributed-training" class="level1 page-columns page-full">
<h1>Distributed Training Systems</h1>
<div class="{layout-narrow} page-columns page-full">

<div class="no-row-height column-margin column-container"><div class="">
<p><em>DALLÂ·E 3 Prompt: A technical illustration showing multiple interconnected GPU clusters working in coordination to train a massive neural network. The scene depicts hundreds of GPU nodes arranged in a circular pattern, connected by luminous data streams representing gradient synchronization. At the center, a giant neural network model spans across all nodes, with each node responsible for a different portion. Visual elements include ring allreduce patterns showing data flowing between nodes, pipeline stages depicted as sequential processing units, and synchronization barriers represented as glowing checkpoints. The color palette uses deep blues and electric purples for computation, with bright orange and gold for communication paths. The style is technical and precise, suitable for an advanced distributed systems textbook.</em></p>
</div></div><p> <img src="images/png/cover_distributed.png" class="img-fluid"></p>
</div>
<section id="purpose" class="level2 unnumbered">
<h2 class="unnumbered anchored" data-anchor-id="purpose">Purpose</h2>
<p><em>What makes distributed training a fundamental shift in how we approach machine learning at scale, rather than merely a technical optimization?</em></p>
<p>Distributed training becomes an architectural necessity when training demands exceed the capabilities of individual machines, and the principles governing coordination across distributed systems fundamentally reshape how we approach optimization, fault tolerance, and cost efficiency. Moving from single-machine to distributed training requires understanding how communication overhead, synchronization costs, and system heterogeneity create entirely new constraints that affect algorithm design and system architecture. The challenges of distributed training, including parameter synchronization across thousands of devices, absorbing failures without losing progress, and balancing communication with computation, demand different engineering solutions than single-device scenarios. As models grow to billions of parameters and training datasets expand to continental scales, distributed training becomes the foundation upon which feasible machine learning systems are built, directly determining which problems can be tackled, which timelines are realistic, and which organizations can meaningfully participate in advanced AI development.</p>
<div class="callout callout-style-default callout-tip callout-titled" title="Learning Objectives">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Tip</span>Learning Objectives
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li><p>Explain data parallelism mechanisms including gradient computation, synchronization via AllReduce algorithms (ring, tree, hierarchical), and the relationship between batch size scaling and convergence behavior</p></li>
<li><p>Analyze multi-machine training requirements by identifying when models exceed single-device memory, when training duration becomes unacceptable, and when datasets exceed single-machine storage, using quantitative thresholds</p></li>
<li><p>Implement data parallel training by applying gradient synchronization algorithms and achieving target parallel efficiency of 85-95% in the linear scaling regime (2-32 devices)</p></li>
<li><p>Design model parallelism strategies using tensor parallelism, pipeline parallelism with microbatching, and embedding sharding to train models exceeding single-device memory while managing pipeline bubble overhead</p></li>
<li><p>Construct hybrid parallelism systems combining data, model, and pipeline strategies across multi-node clusters, selecting appropriate combinations for different model architectures (transformers, recommendation systems, vision models)</p></li>
<li><p>Evaluate distributed training efficiency using quantitative metrics including communication-computation ratio, scaling efficiency, bandwidth utilization, and synchronization costs</p></li>
</ul>
</div>
</div>
</section>
<section id="sec-distributed-training-multimachine-scaling-fundamentals" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="sec-distributed-training-multimachine-scaling-fundamentals">Multi-Machine Scaling Fundamentals</h2>
<p>Part I established the infrastructure foundations that make distributed training possible. The datacenter architectures and accelerator topologies examined in <strong>?@sec-infrastructure</strong> provide the compute fabric, while the distributed storage systems and data pipelines developed in <strong>?@sec-storage</strong> ensure training data flows efficiently to thousands of workers. These foundations answer the question of what physical resources exist. Part II addresses the central question those resources enable: how do we coordinate training across distributed infrastructure to achieve performance that no single machine could deliver?</p>
<p>The transition from single-machine to distributed training represents a major shift in optimization strategy and system complexity. Single-machine optimization focuses on efficiently utilizing available resources through techniques such as prefetching, mixed precision, and gradient accumulation. Distributed training introduces different challenges: communication overhead, fault tolerance, and synchronization that require new conceptual frameworks and engineering approaches.</p>
<section id="sec-distributed-training-multimachine-training-requirements" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="sec-distributed-training-multimachine-training-requirements">Multi-Machine Training Requirements</h3>
<p>Three concrete signals indicate when distributed training becomes necessary rather than merely beneficial. First, memory exhaustion occurs when model parameters, optimizer states, and activation storage exceed single-device capacity even after applying gradient checkpointing and mixed precision. For transformer models, this threshold typically occurs around 10-20 billion parameters on current generation GPUs with 40-80GB memory <span class="citation" data-cites="rajbhandari2020zero">(<a href="#ref-rajbhandari2020zero" role="doc-biblioref">Rajbhandari et al. 2020</a>)</span>. Second, unacceptable training duration emerges when single-device training would require weeks or months to converge, making iteration impossible. Training GPT-3 on a single V100 GPU would require approximately 355 years <span class="citation" data-cites="brown2020language">(<a href="#ref-brown2020language" role="doc-biblioref">Brown et al. 2020</a>)</span>, making distributed approaches not optional but essential. Third, dataset scale exceeds single-machine storage when training data reaches multiple terabytes, as occurs in large-scale vision or language modeling tasks.</p>
<div class="no-row-height column-margin column-container"><div id="ref-rajbhandari2020zero" class="csl-entry" role="listitem">
Rajbhandari, Samyam, Jeff Rasley, Olatunji Rber, and Yuxiong He. 2020. <span>â€œZeRO: Memory Optimizations Toward Training Trillion Parameter Models.â€</span> <em>arXiv Preprint arXiv:1910.02054</em>.
</div><div id="ref-brown2020language" class="csl-entry" role="listitem">
Brown, Tom et al. 2020. <span>â€œLanguage Models Are Few-Shot Learners.â€</span> <em>Advances in Neural Information Processing Systems</em> 33: 1877â€“1901.
</div></div></section>
<section id="sec-distributed-training-complexity-tradeoffs" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="sec-distributed-training-complexity-tradeoffs">Distributed Training Complexity Trade-offs</h3>
<p>Distributed training introduces three primary complexity dimensions absent from single-machine scenarios. Communication overhead emerges from gradient synchronization, where each training step must aggregate gradients across all devices. For a model with <span class="math inline">\(N\)</span> parameters distributed across <span class="math inline">\(D\)</span> devices, all-reduce operations must transfer approximately <span class="math inline">\(2N(D-1)/D\)</span> bytes per step. On commodity network infrastructure (10-100 Gbps), this communication can dominate computation time for models under 1 billion parameters <span class="citation" data-cites="sergeev2018horovod">(<a href="#ref-sergeev2018horovod" role="doc-biblioref">Sergeev and Del Balso 2018</a>)</span>. Fault tolerance requirements increase exponentially with cluster size: a 100-node cluster with 99.9% per-node reliability experiences failures every few hours on average, necessitating checkpoint and recovery mechanisms. Algorithmic considerations change because distributed training alters optimization dynamicsâ€”large batch sizes from data parallelism affect convergence behavior, requiring learning rate scaling and warmup strategies that single-machine training does not require <span class="citation" data-cites="goyal2017accurate">(<a href="#ref-goyal2017accurate" role="doc-biblioref">Goyal et al. 2017</a>)</span>.</p>
<div class="no-row-height column-margin column-container"><div id="ref-sergeev2018horovod" class="csl-entry" role="listitem">
Sergeev, Alexander, and Mike Del Balso. 2018. <span>â€œHorovod: Fast and Easy Distributed Deep Learning in TensorFlow.â€</span> In <em>arXiv Preprint arXiv:1802.05799</em>.
</div><div id="ref-goyal2017accurate" class="csl-entry" role="listitem">
Goyal, Priya et al. 2017. <span>â€œAccurate, Large Minibatch SGD: Training ImageNet in 1 Hour.â€</span> In <em>arXiv Preprint arXiv:1706.02677</em>.
</div></div></section>
<section id="sec-distributed-training-singlemachine-distributed-transition" class="level3">
<h3 class="anchored" data-anchor-id="sec-distributed-training-singlemachine-distributed-transition">Single-Machine to Distributed Transition</h3>
<p>The systematic optimization methodology established for single-machine training extends to distributed environments with important adaptations. Profiling must now capture inter-device communication patterns and synchronization overhead in addition to computation and memory metrics. Tools like NVIDIA Nsight Systems and PyTorchâ€™s distributed profiler reveal whether training is communication-bound or computation-bound, guiding the choice between parallelization strategies. The solution space expands from single-machine techniques to include data parallelism (distributing training examples), model parallelism (distributing model parameters), pipeline parallelism (distributing model layers), and hybrid approaches that combine multiple strategies. The principles remain consistentâ€”identify bottlenecks, select appropriate techniques, compose solutionsâ€”but the implementation complexity increases substantially.</p>
</section>
</section>
<section id="sec-distributed-training-fundamentals" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="sec-distributed-training-fundamentals">Distributed Training Fundamentals</h2>
<p>Building upon single-machine optimization foundations, distributed training extends systematic optimization to multiple machines. When single-machine techniques have been exhaustedâ€”prefetching eliminates data loading bottlenecks, mixed-precision maximizes memory efficiency, and gradient accumulation reaches practical limitsâ€”distributed approaches provide the next level of scaling capability.</p>
<div id="callout-definition*-1.1" class="callout callout-definition" title="Distributed Training">
<p></p><details class="callout-definition fbx-default closebutton" open=""><summary><strong>Definition: </strong>Distributed Training</summary><div><strong><em>Distributed Training</em></strong> is the parallelization of model training across <em>multiple compute devices</em> through coordinated <em>data partitioning</em> and <em>gradient synchronization</em>, enabling training of models that exceed single-device memory or time constraints.<p></p>
</div></details>
</div>
<p>The progression from single-machine to distributed training follows a natural scaling path: optimize locally first, then scale horizontally. This progression ensures that distributed systems operate efficiently at each node while adding the coordination mechanisms necessary for multi-machine training. Training machine learning models often requires scaling beyond a single machine due to increasing model complexity and dataset sizes. The demand for computational power, memory, and storage can exceed the capacity of individual devices, especially in domains like natural language processing, computer vision, and scientific computing. Distributed training<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a> addresses this challenge by spreading the workload across multiple machines, which coordinate to train a single model efficiently.</p>
<div class="no-row-height column-margin column-container"><div id="fn1"><p><sup>1</sup>&nbsp;<strong>Distributed Training</strong>: Googleâ€™s DistBelief (2012) pioneered large-scale distributed neural network training, enabling models with billions of parameters across thousands of machines. This breakthrough led to modern frameworks like Horovod (2017) and PyTorchâ€™s DistributedDataParallel, democratizing distributed training for researchers worldwide.</p></div></div><p>This coordination relies on consensus protocols and synchronization primitives to ensure parameter updates remain consistent across nodes. While basic barrier synchronization suffices for research, production deployments require careful fault tolerance, checkpointing, and recovery mechanisms covered in <strong>?@sec-fault-tolerance</strong>.</p>
<p>The path from single-device to distributed training involves distinct complexity stages, each building upon the previous levelâ€™s challenges. Single GPU training requires only local memory management and straightforward forward/backward passes, establishing the baseline computational patterns. Scaling to multiple GPUs within a single node introduces high-bandwidth communication requirements, typically handled through NVLink<a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a> or PCIe connections with NCCL<a href="#fn3" class="footnote-ref" id="fnref3" role="doc-noteref"><sup>3</sup></a> optimization, while preserving the single-machine simplicity of fault tolerance and scheduling.</p>
<div class="no-row-height column-margin column-container"><div id="fn2"><p><sup>2</sup>&nbsp;<strong>NVLink</strong>: NVIDIAâ€™s proprietary high-speed interconnect providing up to 600 GB/s bidirectional bandwidth between GPUs, roughly 10x faster than PCIe Gen4. Essential for efficient multi-GPU training as it enables rapid gradient synchronization and tensor exchanges between devices.</p></div><div id="fn3"><p><sup>3</sup>&nbsp;<strong>NCCL (NVIDIA Collective Communications Library)</strong>: Optimized library implementing efficient collective operations (AllReduce, Broadcast, etc.) for multi-GPU and multi-node communication. Automatically selects optimal communication patterns based on hardware topology, critical for distributed training performance. The leap to multi-node distributed training brings substantially greater complexity: network communication overhead, fault tolerance requirements, and cluster orchestration challenges. Each scaling stage compounds the previous challengesâ€”communication bottlenecks intensify, synchronization overhead grows, and failure probability increases. This progression underscores why practitioners should optimize single-GPU performance before scaling, ensuring efficient resource utilization at each level.</p></div></div><div class="callout callout-style-default callout-note callout-titled" title="Practical Distributed Training Complexity">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Note</span>Practical Distributed Training Complexity
</div>
</div>
<div class="callout-body-container callout-body">
<p>While frameworks like PyTorch (FSDP) and DeepSpeed abstract away much of the complexity, implementing distributed training efficiently remains a significant engineering challenge. Production deployments require careful network configuration (e.g., InfiniBand), infrastructure management (e.g., Kubernetes, Slurm), and debugging of complex, non-local issues like synchronization hangs or communication bottlenecks. For most teams, leveraging managed cloud services is often more practical than building and maintaining this infrastructure from scratch.</p>
</div>
</div>
<p>The distributed training process itself involves splitting the dataset into non-overlapping subsets, assigning each subset to a different GPU, and performing forward and backward passes independently on each device. Once gradients are computed on each GPU, they are synchronized and aggregated before updating the model parameters, ensuring that all devices learn in a consistent manner. <a href="#fig-distributed-training" class="quarto-xref">Figure&nbsp;1</a> illustrates this process, showing how input data is divided, assigned to multiple GPUs for computation, and later synchronized to update the model collectively.</p>
<div id="fig-distributed-training" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-env="figure" data-fig-pos="htb">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-distributed-training-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="8f58d7af19a842ea6a9b80429b6da3369b100602.svg" class="lightbox" data-gallery="quarto-lightbox-gallery-1" title="Figure&nbsp;1: Distributed machine learning scales model training by partitioning datasets across multiple GPUs, enabling concurrent computation of gradients, and then aggregating these gradients to update shared model parameters. This approach accelerates training through parallel processing while maintaining model consistency across all devices."><img src="distributed_training_files/mediabag/8f58d7af19a842ea6a9b80429b6da3369b100602.svg" class="img-fluid figure-img"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-distributed-training-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;1: Distributed machine learning scales model training by partitioning datasets across multiple GPUs, enabling concurrent computation of gradients, and then aggregating these gradients to update shared model parameters. This approach accelerates training through parallel processing while maintaining model consistency across all devices.
</figcaption>
</figure>
</div>
<p>This coordination introduces several key challenges that distributed training systems must address. A distributed training system must orchestrate multi-machine computation by splitting up the work, managing communication between machines, and maintaining synchronization throughout the training process. Understanding these basic requirements provides the foundation for examining the main approaches to distributed training: data parallelism, which divides the training data across machines; model parallelism, which splits the model itself; pipeline parallelism, which combines aspects of both; and hybrid approaches that integrate multiple strategies.</p>
</section>
<section id="sec-distributed-training-efficiency-metrics" class="level2">
<h2 class="anchored" data-anchor-id="sec-distributed-training-efficiency-metrics">Distributed Training Efficiency Metrics</h2>
<p>Before examining specific parallelism strategies, understanding the quantitative metrics that govern distributed training efficiency is essential. These metrics provide the foundation for making informed decisions about scaling strategies, hardware selection, and optimization approaches.</p>
<p>Communication overhead represents the primary bottleneck in distributed training systems. AllReduce operations consume 10-40% of total training time in data parallel systems, with this overhead increasing significantly at scale. For BERT-Large on 128 GPUs, communication overhead reaches 35% of total runtime, while GPT-3 scale models experience 55% overhead on 1,024 GPUs.</p>
<div class="callout callout-style-default callout-note callout-titled" title="AllReduce Communication Complexity">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Note</span>AllReduce Communication Complexity
</div>
</div>
<div class="callout-body-container callout-body">
<p>AllReduce complexity depends on two components: latency (<span class="math inline">\(\alpha\)</span>) and bandwidth (<span class="math inline">\(\beta\)</span>). For a message of size <span class="math inline">\(M\)</span> across <span class="math inline">\(N\)</span> workers:</p>
<p><strong>Ring AllReduce</strong>:</p>
<ul>
<li>Time: <span class="math inline">\(2(N-1) \cdot \alpha + 2 \cdot \frac{N-1}{N} \cdot M \cdot \beta\)</span></li>
<li>Bandwidth utilization: <span class="math inline">\((N-1)/N\)</span>, approaching optimal as <span class="math inline">\(N\)</span> grows</li>
<li>Each device sends/receives approximately <span class="math inline">\(2M\)</span> bytes total (not <span class="math inline">\(2M \cdot N\)</span>)</li>
</ul>
<p><strong>Tree AllReduce</strong>:</p>
<ul>
<li>Time: <span class="math inline">\(2 \log_2(N) \cdot \alpha + 2 \log_2(N) \cdot M \cdot \beta\)</span></li>
<li>Bandwidth utilization: <span class="math inline">\(1/\log_2(N)\)</span>, decreasing with scale</li>
<li>Latency: <span class="math inline">\(O(\log N)\)</span> steps</li>
</ul>
<p>The crossover point depends on message size: tree AllReduce wins for small messages (latency-dominated), while ring AllReduce wins for large gradients (bandwidth-dominated). Modern implementations like NCCL use hierarchical algorithms that achieve tree latency within nodes (using NVLink) and ring bandwidth between nodes (using InfiniBand).</p>
</div>
</div>
<p>Interconnect selection is critical for large-scale deployments, as these complexity characteristics determine when communication dominates computation.</p>
<p>The bandwidth requirements for efficient distributed training are substantial, particularly for transformer models. Efficient systems require 100-400 GB/s aggregate bandwidth per node for transformer architectures. BERT-Base needs 8 GB parameter synchronization per iteration across 64 GPUs, demanding 200 GB/s sustained bandwidth for &lt;50ms synchronization latency. Language models with 175B parameters require 700 GB/s aggregate bandwidth to maintain 80% parallel efficiency, necessitating InfiniBand HDR or equivalent interconnects.</p>
<p>Synchronization frequency presents a trade-off between communication efficiency and convergence behavior. Gradient accumulation reduces synchronization frequency but increases memory requirements and may impact convergence. Synchronizing every 4 steps reduces communication overhead by 60% while increasing memory usage by 3x for gradient storage. Asynchronous methods eliminate synchronization costs entirely but introduce staleness that degrades convergence by 15-30% for large learning rates.</p>
<p>Scaling efficiency follows predictable patterns across different GPU counts. In the linear scaling regime of 2-32 GPUs, systems typically achieve 85-95% parallel efficiency as communication overhead remains minimal. The communication bound regime emerges at 64-256 GPUs, where efficiency drops to 60-80% even with optimal interconnects. Beyond 512 GPUs, coordination overhead becomes dominant, limiting efficiency to 40-60% due to collective operation latency.</p>
<p>Hardware selection critically impacts these scaling characteristics. NVIDIA DGX systems with NVLink achieve 600 GB/s bisection bandwidth, enabling 90% parallel efficiency up to 8 GPUs per node. Multi-node scaling requires InfiniBand networks, where EDR (100 Gbps) supports efficient training up to 64 nodes, while HDR (200 Gbps) enables scaling to 256+ nodes with &gt;70% efficiency.</p>
<p>These efficiency metrics directly influence the choice of parallelism strategy. Data parallelism works well in the linear scaling regime but becomes communication-bound at scale. Model parallelism addresses memory constraints but introduces sequential dependencies that limit efficiency. Pipeline parallelism reduces device idle time but introduces complexity in managing microbatches. Understanding these trade-offs enables architects to select appropriate strategies for their specific workloads.</p>
</section>
<section id="sec-distributed-training-data-parallelism" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="sec-distributed-training-data-parallelism">Data Parallelism</h2>
<p>Building on the efficiency characteristics outlined above, data parallelism represents the most straightforward distributed approach, particularly effective in the linear scaling regime where communication overhead remains manageable. This method distributes the training process across multiple devices by splitting the dataset into smaller subsets. Each device trains a complete copy of the model using its assigned subset of the data. For example, when training an image classification model on 1 million images using 4 GPUs, each GPU would process 250,000 images while maintaining an identical copy of the model architecture.</p>
<p>It is particularly effective when the dataset size is large but the model size is manageable, as each device must store a full copy of the model in memory. This method is widely used in scenarios such as image classification and natural language processing, where the dataset can be processed in parallel without dependencies between data samples. For instance, when training a ResNet model on ImageNet, each GPU can independently process its portion of images since the classification of one image doesnâ€™t depend on the results of another.</p>
<p>The effectiveness of data parallelism stems from a property of stochastic gradient descent established in optimization foundations. Gradients computed on different minibatches can be averaged while preserving mathematical equivalence to single-device training. This property enables parallel computation across devices, with the mathematical foundation following directly from the linearity of expectation.</p>
<p>Consider a model with parameters <span class="math inline">\(Î¸\)</span> training on a dataset <span class="math inline">\(D\)</span>. The loss function for a single data point <span class="math inline">\(x_i\)</span> is <span class="math inline">\(L(Î¸, x_i)\)</span>. In standard SGD with batch size <span class="math inline">\(B\)</span>, the gradient update for a minibatch is: <span class="math display">\[
g = \frac{1}{B} \sum_{i=1}^B \nabla_Î¸ L(Î¸, x_i)
\]</span></p>
<p>In data parallelism with <span class="math inline">\(N\)</span> devices, each device <span class="math inline">\(k\)</span> computes gradients on its own minibatch <span class="math inline">\(B_k\)</span>: <span class="math display">\[
g_k = \frac{1}{|B_k|} \sum_{x_i \in B_k} \nabla_Î¸ L(Î¸, x_i)
\]</span></p>
<p>The global update averages these local gradients: <span class="math display">\[
g_{\text{global}} = \frac{1}{N} \sum_{k=1}^N g_k
\]</span></p>
<p>This averaging is mathematically equivalent to computing the gradient on the combined batch <span class="math inline">\(B_{\text{total}} = \bigcup_{k=1}^N B_k\)</span>: <span class="math display">\[
g_{\text{global}} = \frac{1}{|B_{\text{total}}|} \sum_{x_i \in B_{\text{total}}} \nabla_Î¸ L(Î¸, x_i)
\]</span></p>
<p>This equivalence shows why data parallelism maintains the statistical properties of SGD training. The approach distributes distinct data subsets across devices, computes local gradients independently, and averages these gradients to approximate the full-batch gradient.</p>
<p>The method parallels gradient accumulation, where a single device accumulates gradients over multiple forward passes before updating parameters. Both techniques use the additive properties of gradients to process large batches efficiently.</p>
<div class="callout callout-style-default callout-note callout-titled" title="Production Reality: Data Parallelism at Scale">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Note</span>Production Reality: Data Parallelism at Scale
</div>
</div>
<div class="callout-body-container callout-body">
<p>Data parallelism in production environments involves several operational considerations beyond the theoretical framework:</p>
<ul>
<li><strong>Communication efficiency</strong>: AllReduce operations for gradient synchronization become the bottleneck at scale. Production systems use optimized libraries like NCCL with ring or tree communication patterns to minimize overhead</li>
<li><strong>Fault tolerance</strong>: Node failures during large-scale training require checkpoint/restart strategies. Production systems implement hierarchical checkpointing with both local and distributed storage</li>
<li><strong>Dynamic scaling</strong>: Cloud environments require elastic scaling capabilities to add/remove workers based on demand and cost constraints, complicated by the need to maintain gradient synchronization</li>
<li><strong>Cost optimization</strong>: Production data parallelism considers cost per GPU-hour across different instance types and preemptible instances, balancing training time against infrastructure costs</li>
<li><strong>Network bandwidth requirements</strong>: Large models require careful network topology planning as gradient communication can consume 10-50% of training time depending on model size and batch size</li>
</ul>
<p>Production teams typically benchmark communication patterns and scaling efficiency before deploying large distributed training jobs to identify optimal configurations.</p>
</div>
</div>
<section id="sec-distributed-training-data-parallelism-implementation" class="level3">
<h3 class="anchored" data-anchor-id="sec-distributed-training-data-parallelism-implementation">Data Parallelism Implementation</h3>
<p>The process of data parallelism can be broken into a series of distinct steps, each with its role in ensuring the system operates efficiently. These steps are illustrated in <a href="#fig-train-data-parallelism" class="quarto-xref">Figure&nbsp;2</a>.</p>
<div id="fig-train-data-parallelism" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-env="figure" data-fig-pos="htb">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-train-data-parallelism-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="61517aa2f929d778d4fee9657218bd1ea61c5d29.svg" class="lightbox" data-gallery="quarto-lightbox-gallery-2" title="Figure&nbsp;2: Distributed training replicates the model across multiple devices, each processing a subset of the data before aggregating gradients to update model parameters. This accelerates the training process compared to single-device training, and contrasts with model parallelism where the model itself is partitioned across devices."><img src="distributed_training_files/mediabag/61517aa2f929d778d4fee9657218bd1ea61c5d29.svg" class="img-fluid figure-img"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-train-data-parallelism-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;2: Distributed training replicates the model across multiple devices, each processing a subset of the data before aggregating gradients to update model parameters. This accelerates the training process compared to single-device training, and contrasts with model parallelism where the model itself is partitioned across devices.
</figcaption>
</figure>
</div>
<section id="sec-distributed-training-dataset-splitting" class="level4">
<h4 class="anchored" data-anchor-id="sec-distributed-training-dataset-splitting">Dataset Splitting</h4>
<p>The first step in data parallelism involves dividing the dataset into smaller, non-overlapping subsets. This ensures that each device processes a unique portion of the data, avoiding redundancy and enabling efficient utilization of available hardware. For instance, with a dataset of 100,000 training examples and 4 GPUs, each GPU would be assigned 25,000 examples. Modern frameworks like PyTorchâ€™s DistributedSampler handle this distribution automatically, implementing prefetching and caching mechanisms to ensure data is readily available for processing.</p>
</section>
<section id="sec-distributed-training-device-forward-pass" class="level4">
<h4 class="anchored" data-anchor-id="sec-distributed-training-device-forward-pass">Device Forward Pass</h4>
<p>Once the data subsets are distributed, each device performs the forward pass independently. During this stage, the model processes its assigned batch of data, generating predictions and calculating the loss. For example, in a ResNet-50 model, each GPU would independently compute the convolutions, activations, and final loss for its batch. The forward pass is computationally intensive and benefits from hardware accelerators like NVIDIA V100 GPUs or Google TPUs, which are optimized for matrix operations.</p>
</section>
<section id="sec-distributed-training-backward-pass-calculation" class="level4">
<h4 class="anchored" data-anchor-id="sec-distributed-training-backward-pass-calculation">Backward Pass and Calculation</h4>
<p>Following the forward pass, each device computes the gradients of the loss with respect to the modelâ€™s parameters during the backward pass. Modern frameworks like PyTorch and TensorFlow handle this automatically through their autograd systems. For instance, if a model has 50 million parameters, each device calculates gradients for all parameters but based only on its local data subset.</p>
</section>
<section id="sec-distributed-training-gradient-synchronization" class="level4">
<h4 class="anchored" data-anchor-id="sec-distributed-training-gradient-synchronization">Gradient Synchronization</h4>
<p>To maintain consistency across the distributed system, the gradients computed by each device must be synchronized. This coordination represents a distributed systems challenge: achieving global consensus while minimizing communication complexity. The ring all-reduce algorithm exemplifies this trade-off, organizing devices in a logical ring where each GPU communicates only with its neighbors. The algorithm complexity is O(n) in communication rounds but requires sequential dependencies that can limit parallelism.</p>
<p>For example, with 8 GPUs sharing gradients for a 100 MB model, ring all-reduce requires only 7 communication steps instead of the 56 steps needed for naive all-to-all synchronization. The ring topology creates bottlenecks: the slowest link in the ring determines the overall synchronization time, and network partitions can halt the entire training process. Alternative algorithms like tree-reduce achieve O(log n) latency at the cost of increased bandwidth requirements on root nodes. Modern systems often implement hierarchical topologies, using high-speed links within nodes and lower-bandwidth connections between nodes to optimize these trade-offs.</p>
</section>
<section id="sec-distributed-training-sync-models" class="level4">
<h4 class="anchored" data-anchor-id="sec-distributed-training-sync-models">Synchronization Models</h4>
<p>Distributed training systems operate under explicit synchronization models that govern when workers observe each otherâ€™s updates. Understanding these models is essential for reasoning about correctness and performance.</p>
<p>The default model, Bulk Synchronous Parallel (BSP), requires all workers to complete their local computation (forward and backward pass), synchronize gradients through a barrier (AllReduce), and then simultaneously update parameters. BSP provides strong guarantees: every worker sees identical parameter values at each step, ensuring mathematical equivalence to single-device training. The cost is that the slowest worker determines iteration time, creating the â€œstraggler problem.â€</p>
<p>Stale Synchronous Parallel (SSP) relaxes this constraint, allowing workers to proceed up to <span class="math inline">\(s\)</span> iterations ahead of the slowest worker before blocking. This bounds staleness while reducing synchronization delays. SSP requires careful learning rate tuning since workers compute gradients on slightly different parameter versions. The bounded staleness guarantee (<span class="math inline">\(s\)</span> typically 2-5) provides a middle ground between BSPâ€™s strong consistency and fully asynchronous approaches.</p>
<p>Asynchronous SGD eliminates synchronization barriers entirely, with workers updating parameters independently. This maximizes hardware utilization but introduces gradient staleness that can degrade convergence. When a worker computes gradients on parameters that are already <span class="math inline">\(\tau\)</span> steps stale, the effective learning rate decreases. Compensation techniques include learning rate scaling (<span class="math inline">\(\eta' = \eta / \sqrt{\tau}\)</span>) or momentum correction.</p>
<div class="callout callout-style-default callout-note callout-titled" title="Synchronization Model Trade-offs">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Note</span>Synchronization Model Trade-offs
</div>
</div>
<div class="callout-body-container callout-body">
<table class="caption-top table">
<colgroup>
<col style="width: 12%">
<col style="width: 23%">
<col style="width: 21%">
<col style="width: 23%">
<col style="width: 18%">
</colgroup>
<thead>
<tr class="header">
<th>Model</th>
<th>Consistency</th>
<th>Throughput</th>
<th>Convergence</th>
<th>Use Case</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>BSP</td>
<td>Strong</td>
<td>Bounded by slowest worker</td>
<td>Equivalent to single-GPU</td>
<td>Final training runs, reproducibility</td>
</tr>
<tr class="even">
<td>SSP</td>
<td>Bounded staleness</td>
<td>Higher than BSP</td>
<td>Near-equivalent with tuning</td>
<td>Hyperparameter search</td>
</tr>
<tr class="odd">
<td>Async</td>
<td>Weak</td>
<td>Maximum</td>
<td>Degraded, requires compensation</td>
<td>Large heterogeneous clusters</td>
</tr>
</tbody>
</table>
</div>
</div>
<p>The choice of synchronization model directly affects both system throughput and model convergence. Production systems typically use BSP for final training runs to ensure reproducibility, while exploring SSP or async approaches during hyperparameter search where exact reproducibility is less critical.</p>
</section>
<section id="sec-distributed-training-barrier-failures" class="level4">
<h4 class="anchored" data-anchor-id="sec-distributed-training-barrier-failures">Barrier Semantics and Failure Modes</h4>
<p>AllReduce operations implement implicit barriers: no worker can proceed until all workers have contributed their gradients. This coupling creates failure modes absent from single-device training.</p>
<p>Worker failures during AllReduce cause all other workers to block indefinitely, waiting for the missing contribution. Without timeout mechanisms, the entire training job hangs rather than failing cleanly. Production systems implement watchdog timers, typically set to 5-10 minutes, to detect and terminate stuck jobs.</p>
<p>Gradient mismatches occur when workers disagree on which tensors to synchronize due to conditional computation paths or dynamic batching. AllReduce operations may block waiting for tensors that some workers never send. This commonly occurs with variable-length sequences in NLP models, dynamic computation graphs, and mixture-of-experts with different routing decisions.</p>
<p>Straggler-induced delays arise because iteration time equals the slowest workerâ€™s time plus synchronization overhead. A single slow worker, whether due to thermal throttling, network congestion, or OS jitter, delays all workers and reduces cluster utilization. At 1000 GPUs with 1% probability of straggler per GPU per step, approximately 10 GPUs straggle every iteration.</p>
<p>Production systems address these issues through:</p>
<ul>
<li><strong>Timeouts</strong>: AllReduce operations with configurable timeouts that trigger failure handling rather than indefinite blocking</li>
<li><strong>Heartbeat monitoring</strong>: Detecting unresponsive workers before AllReduce blocks</li>
<li><strong>Elastic training</strong>: Removing failed workers and continuing with reduced parallelism (see <strong>?@sec-fault-tolerance</strong>)</li>
<li><strong>Backup workers</strong>: Redundant computation to mask stragglers</li>
</ul>
</section>
<section id="sec-distributed-training-parameter-updating" class="level4">
<h4 class="anchored" data-anchor-id="sec-distributed-training-parameter-updating">Parameter Updating</h4>
<p>After gradient aggregation, each device independently updates model parameters using the chosen optimization algorithm, such as SGD with momentum or Adam. This decentralized update strategy, implemented in frameworks like PyTorchâ€™s DistributedDataParallel (DDP), enables efficient parameter updates without requiring a central coordination server. Since all devices have identical gradient values after synchronization, they perform mathematically equivalent updates to maintain model consistency across the distributed system.</p>
<p>For example, in a system with 8 GPUs training a ResNet model, each GPU computes local gradients based on its data subset. After gradient averaging via ring all-reduce, every GPU has the same global gradient values. Each device then independently applies these gradients using the optimizerâ€™s update rule. If using SGD with learning rate 0.1, the update would be <code>weights = weights - 0.1 * gradients</code>. This process maintains mathematical equivalence to single-device training while enabling distributed computation.</p>
<p>This process, which involves splitting data, performing computations, synchronizing results, and updating parameters, repeats for each batch of data. Modern frameworks automate this cycle, allowing developers to focus on model architecture and hyperparameter tuning rather than distributed computing logistics.</p>
</section>
</section>
<section id="sec-distributed-training-data-parallelism-advantages" class="level3">
<h3 class="anchored" data-anchor-id="sec-distributed-training-data-parallelism-advantages">Data Parallelism Advantages</h3>
<p>Data parallelism offers several key benefits that make it the predominant approach for distributed training. By splitting the dataset across multiple devices and allowing each device to train an identical copy of the model, this approach effectively addresses the core challenges in modern AI training systems.</p>
<p>The primary advantage of data parallelism is its linear scaling capability with large datasets. As datasets grow into the terabyte range, processing them on a single machine becomes prohibitively time-consuming. For example, training a vision transformer on ImageNet (1.2 million images) might take weeks on a single GPU, but only days when distributed across 8 GPUs. This scalability is particularly valuable in domains like language modeling, where datasets can exceed billions of tokens.</p>
<p>Hardware utilization efficiency represents another important benefit. Data parallelism maintains high GPU utilization rates, typically, above 85%, by ensuring each device actively processes its data portion. Modern implementations achieve this through asynchronous data loading and gradient computation overlapping with communication. For instance, while one batch computes gradients, the next batchâ€™s data is already being loaded and preprocessed.</p>
<p>Implementation simplicity sets data parallelism apart from other distribution strategies. Modern frameworks have reduced complex distributed training to just a few lines of code. For example, converting a PyTorch model to use data parallelism often requires only wrapping it in <code>DistributedDataParallel</code> and initializing a distributed environment. This accessibility has contributed significantly to its widespread adoption in both research and industry.</p>
<p>The approach also offers flexibility across model architectures. Whether training a ResNet (vision), BERT (language), or Graph Neural Network (graph data), the same data parallelism principles apply without modification. This universality makes it particularly valuable as a default choice for distributed training.</p>
<p>Training time reduction is perhaps the most immediate benefit. Given proper implementation, data parallelism can achieve near-linear speedup with additional devices. Training that takes 100 hours on a single GPU might complete in roughly 13 hours on 8 GPUs, assuming efficient gradient synchronization and minimal communication overhead.</p>
<p>While these benefits make data parallelism compelling, achieving these advantages requires careful system design. Several challenges must be addressed to fully realize these benefits.</p>
<div class="callout callout-style-default callout-tip callout-titled" title="GPT-2 Data Parallel Scaling: 1â†’8â†’32 GPUs">
<div class="callout-header d-flex align-content-center collapsed" data-bs-toggle="collapse" data-bs-target=".callout-6-contents" aria-controls="callout-6" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Tip</span>GPT-2 Data Parallel Scaling: 1â†’8â†’32 GPUs
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-6" class="callout-6-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>This example demonstrates how data parallelism scales in practice, including efficiency degradation.</p>
<p><strong>Single GPU Baseline</strong></p>
<ul>
<li>Batch size: 16 (with gradient checkpointing, fits in 32GB)</li>
<li>Time per step: 1.8 seconds</li>
<li>Training throughput: ~9 samples/second</li>
<li>Time to 50K steps: <strong>25 hours</strong></li>
</ul>
<p><strong>8 GPUs: Single Node with NVLink</strong></p>
<p>Configuration:</p>
<ul>
<li>Per-GPU batch: 16, global batch: 128</li>
<li>Gradient synchronization: 3GB @ 600 GB/s (NVLink) = 5ms</li>
</ul>
<p>Performance results:</p>
<ul>
<li>Computation: 180ms per step</li>
<li>Communication: 5ms per step</li>
<li>Total: 185ms per step</li>
<li>Speedup: 1.8s Ã· 0.185s = 9.7Ã— (not quite 8Ã—)</li>
<li>Parallel efficiency: 9.7 Ã· 8 = 121%</li>
</ul>
<p>Why over 100% efficiency? Larger global batch (128 vs 16) improves GPU utilization from 72% to 89%. This â€œsuper-linearâ€ speedup is common in ML at small scales when the baseline has poor utilization.</p>
<p>Training time: 25 hours Ã· 9.7 = <strong>2.6 hours</strong></p>
<p><strong>32 GPUs: 4 Nodes with InfiniBand</strong></p>
<p>Configuration:</p>
<ul>
<li>Per-GPU batch: 16, global batch: 512</li>
<li>Intra-node communication: 5ms (NVLink)</li>
<li>Inter-node communication: 3GB @ 12.5 GB/s (InfiniBand) = 240ms</li>
</ul>
<p>Performance results:</p>
<ul>
<li>Computation: 180ms (42% of time)</li>
<li>Communication: 245ms (58% of time)</li>
<li>Total: 425ms per step</li>
<li>Speedup: 1.8s Ã· 0.425s = 4.2Ã— faster â†’ 5.9 hours</li>
<li>Parallel efficiency: 4.2 Ã· 32 = 13%</li>
</ul>
<p>Communication dominates and becomes the bottleneck.</p>
<p><strong>Better Approach: 8 GPUs with Gradient Accumulation</strong></p>
<ul>
<li>Configuration: 8 GPUs Ã— batch 16 Ã— 4 accumulation steps = 512 effective batch</li>
<li>Communication overhead: 5ms Ã· (4 Ã— 180ms) = 0.7%</li>
<li>Training time: 3.8 hours</li>
<li>Cost: $128/hour Ã— 3.8 hours = $486 vs.&nbsp;$3,021 for 32 GPUs</li>
<li>Savings: $2,535 (84% reduction) with only 1 hour longer training</li>
</ul>
<p><strong>Key Insights</strong></p>
<ol type="1">
<li>NVLink enables efficient scaling within single nodes (97% efficiency)</li>
<li>Inter-node communication kills efficiency (drops to 13%)</li>
<li>Gradient accumulation beats naive scaling for memory-bound models</li>
<li>Sweet spot for GPT-2: 8 GPUs per node with gradient accumulation, not naive scaling to 32+ GPUs</li>
</ol>
<p>OpenAIâ€™s GPT-2 paper reports training on 32 V100s across 4 nodes using optimized communication (likely gradient accumulation combined with pipeline parallelism), not pure data parallelism.</p>
</div>
</div>
</div>
</section>
<section id="sec-distributed-training-data-parallelism-limitations" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="sec-distributed-training-data-parallelism-limitations">Data Parallelism Limitations</h3>
<p>While data parallelism is an effective approach for distributed training, it introduces several challenges that must be addressed to achieve efficient and scalable training systems. These challenges stem from the inherent trade-offs between computation and communication, as well as the limitations imposed by hardware and network infrastructures.</p>
<p>Communication overhead represents the most significant bottleneck in data parallelism. During gradient synchronization, each device must exchange gradient updates, often hundreds of megabytes per step for large models. With 8 GPUs training a 1-billion-parameter model, each synchronization step might require transferring several gigabytes of data across the network. While high-speed interconnects like NVLink (300 GB/s) or InfiniBand (200 Gb/s) help, the overhead remains substantial. NCCLâ€™s ring-allreduce algorithm<a href="#fn4" class="footnote-ref" id="fnref4" role="doc-noteref"><sup>4</sup></a> reduces this burden by organizing devices in a ring topology, but communication costs still grow with model size and device count.</p>
<div class="no-row-height column-margin column-container"><div id="fn4"><p><sup>4</sup>&nbsp;<strong>AllReduce Algorithm</strong>: A collective communication primitive where each process contributes data and all processes receive the same combined result (typically a sum). Naive implementations require O(nÂ²) messages for n devices. The ring-allreduce algorithm, developed for high-performance computing in the 1980s, reduces this to O(n) by organizing devices in a logical ring where each device communicates only with its neighbors, making it scalable for modern ML with hundreds of GPUs.</p></div></div><p>Scalability limitations become apparent as device count increases. While 8 GPUs might achieve <span class="math inline">\(7\times\)</span> speedup (87.5% scaling efficiency), scaling to 64 GPUs typically yields only 45-50<span class="math inline">\(\times\)</span> speedup (70-78% efficiency) due to growing synchronization costs. Scaling efficiency, calculated as speedup divided by the number of devicesâ€”quantifies how effectively additional hardware translates to reduced training time. Perfect linear scaling would yield 100% efficiency, but communication overhead and synchronization barriers typically degrade efficiency as device count grows. This non-linear scaling means that doubling the number of devices rarely halves the training time, particularly in configurations exceeding 16-32 devices.</p>
<p>Memory constraints present a hard limit for data parallelism. Consider a transformer model with 175 billion parameters, which requires approximately 350 GB just to store model parameters in FP32. When accounting for optimizer states and activation memories, the total requirement often exceeds 1 TB per device. Since even high-end GPUs typically offer 80 GB or less, such models cannot use pure data parallelism.</p>
<p>Workload imbalance affects heterogeneous systems significantly. In a cluster mixing A100 and V100 GPUs, the A100s might process batches <span class="math inline">\(1.7\times\)</span> faster, forcing them to wait for the V100s to catch up. This idle time can reduce cluster utilization by 20-30% without proper load balancing mechanisms.</p>
<p>Finally, there are critical challenges related to fault tolerance and reliability in distributed training systems. Node failures become inevitable at scale: with 100 GPUs running continuously, hardware failures occur multiple times per week. A training run that costs millions of dollars cannot restart from scratch each time a single GPU fails. Modern distributed training systems implement sophisticated checkpointing strategies, storing model state every N iterations to minimize lost computation. Checkpoint frequency creates trade-offs: frequent checkpointing reduces the potential loss from failures but increases storage I/O overhead and training latency. Production systems typically checkpoint every 100-1000 iterations, balancing fault tolerance against performance.</p>
<p>Implementation complexity compounds these reliability challenges. While modern frameworks abstract much of the complexity, implementing robust distributed training systems requires significant engineering expertise. Graceful degradation when subsets of nodes fail, consistent gradient synchronization despite network partitions, and automatic recovery from transient failures demand deep understanding of both machine learning and distributed systems principles.</p>
<p>Despite these challenges, data parallelism remains an important technique for distributed training, with many strategies available to address its limitations. Monitoring these distributed systems requires specialized tooling for tracking gradient norms, communication patterns, and hardware utilization across nodes. Before examining model parallelism, we first explore memory optimization techniques that extend data parallelism to larger models.</p>
</section>
<section id="sec-distributed-training-zero-fsdp" class="level3">
<h3 class="anchored" data-anchor-id="sec-distributed-training-zero-fsdp">Memory-Efficient Data Parallelism: ZeRO and FSDP</h3>
<p>The memory constraints of data parallelism motivate a family of techniques that shard memory state across workers while preserving the simplicity of data parallel training. ZeRO (Zero Redundancy Optimizer) and its PyTorch implementation FSDP (Fully Sharded Data Parallel) enable training models that would otherwise require model parallelism.</p>
<p>In standard data parallelism, each GPU maintains a complete copy of:</p>
<ul>
<li><strong>Model parameters</strong>: 4 bytes/param (FP32) or 2 bytes/param (FP16/BF16)</li>
<li><strong>Gradients</strong>: Same size as parameters</li>
<li><strong>Optimizer states</strong>: For Adam, 8 bytes/param (momentum + variance in FP32)</li>
</ul>
<p>For a 7B parameter model with Adam optimizer, each GPU requires: <span class="math inline">\(7B \times (4 + 4 + 8) = 112\)</span> GB, exceeding A100-80GB capacity even before accounting for activations.</p>
<p>ZeRO addresses this redundancy through progressive sharding:</p>
<table class="caption-top table">
<colgroup>
<col style="width: 10%">
<col style="width: 25%">
<col style="width: 27%">
<col style="width: 36%">
</colgroup>
<thead>
<tr class="header">
<th>Stage</th>
<th>What is Sharded</th>
<th>Memory Reduction</th>
<th>Communication Overhead</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>ZeRO-1</td>
<td>Optimizer states only</td>
<td>~4x</td>
<td>None (same as DDP)</td>
</tr>
<tr class="even">
<td>ZeRO-2</td>
<td>+ Gradients</td>
<td>~8x</td>
<td>ReduceScatter replaces AllReduce</td>
</tr>
<tr class="odd">
<td>ZeRO-3 / FSDP</td>
<td>+ Parameters</td>
<td>~<span class="math inline">\(N\)</span> (linear in workers)</td>
<td>AllGather before each layer</td>
</tr>
</tbody>
</table>
<p>ZeRO-1 shards optimizer states across GPUs. Each GPU stores only <span class="math inline">\(1/N\)</span> of the Adam momentum and variance tensors. After gradient AllReduce, each GPU updates only its shard of parameters, then broadcasts updates to other GPUs. Memory savings: optimizer states reduced from <span class="math inline">\(8N\)</span> bytes/param to <span class="math inline">\(8\)</span> bytes/param total across cluster.</p>
<p>ZeRO-2 additionally shards gradients. Instead of AllReduce, which leaves full gradients on each GPU, ZeRO-2 uses ReduceScatter so each GPU receives <span class="math inline">\(1/N\)</span> of the reduced gradients. Memory savings: gradients reduced from <span class="math inline">\(4N\)</span> bytes/param to <span class="math inline">\(4\)</span> bytes/param total.</p>
<p>ZeRO-3 and FSDP shard parameters themselves. Each GPU stores only <span class="math inline">\(1/N\)</span> of the model. Before each layerâ€™s forward pass, parameters are gathered via AllGather; after backward pass, gradients are reduced via ReduceScatter, then parameters are discarded. This achieves maximum memory efficiency at the cost of additional communication.</p>
<div class="callout callout-style-default callout-note callout-titled" title="FSDP Communication Analysis">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Note</span>FSDP Communication Analysis
</div>
</div>
<div class="callout-body-container callout-body">
<p>FSDP introduces communication on the critical path that DDP avoids:</p>
<ul>
<li><strong>Forward pass</strong>: AllGather to reconstruct parameters (<span class="math inline">\(M\)</span> bytes Ã— 2 for each layer)</li>
<li><strong>Backward pass</strong>: ReduceScatter for gradients (<span class="math inline">\(M\)</span> bytes Ã— 2 for each layer)</li>
</ul>
<p>For a model with <span class="math inline">\(L\)</span> layers, FSDP performs <span class="math inline">\(2L\)</span> collective operations per training step versus 1 AllReduce for DDP. However, FSDP enables overlapping: while layer <span class="math inline">\(i\)</span> computes, layer <span class="math inline">\(i+1\)</span> can prefetch parameters.</p>
<p>Total FSDP communication volume: approximately <span class="math inline">\(3M\)</span> bytes (vs.&nbsp;<span class="math inline">\(2M\)</span> for DDP AllReduce), but spread across more operations with overlap opportunities.</p>
</div>
</div>
<p>The choice between FSDP and DDP depends on model size and memory constraints. Use DDP when the model fits in GPU memory with room for activations, as it has lower overhead. Use FSDP ZeRO-2 when the model barely fits or requires activation checkpointing. Use FSDP ZeRO-3 when model parameters exceed single-GPU memory. For training 70B+ models on 80GB GPUs, combine FSDP with tensor parallelism.</p>
<p>FSDP configuration requires careful tuning of sharding strategy (by layer, by transformer block, or flat) and mixed precision settings. The <code>auto_wrap_policy</code> determines sharding granularity, with finer sharding reducing memory but increasing communication frequency.</p>
</section>
</section>
<section id="sec-distributed-training-model-parallelism" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="sec-distributed-training-model-parallelism">Model Parallelism</h2>
<p>While data parallelism scales dataset processing, some models themselves exceed the memory capacity of individual devices. Model parallelism splits neural networks across multiple computing devices when the modelâ€™s parameters exceed single-device memory limits. Unlike data parallelism, where each device contains a complete model copy, model parallelism assigns different model components to different devices <span class="citation" data-cites="shazeer_mixture_of_experts_2017">(<a href="#ref-shazeer_mixture_of_experts_2017" role="doc-biblioref">Shazeer et al. 2017</a>)</span>.</p>
<div class="no-row-height column-margin column-container"><div id="ref-shazeer_mixture_of_experts_2017" class="csl-entry" role="listitem">
Shazeer, Noam et al. 2017. <span>â€œOutrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer.â€</span> <em>arXiv Preprint arXiv:1701.06538</em>.
</div></div><p>Several implementations of model parallelism exist. In layer-based splitting, devices process distinct groups of layers sequentially. For instance, the first device might compute layers 1-4 while the second handles layers 5-8. Channel-based splitting divides the channels within each layer across devices, such as the first device processing 512 channels while the second manages the remaining ones. For transformer architectures, attention head splitting distributes different attention heads to separate devices.</p>
<p>This distribution method enables training of large-scale models. GPT-3, with 175 billion parameters, relies on model parallelism for training. Vision transformers processing high-resolution 16k Ã— 16k pixel images use model parallelism to manage memory constraints. Mixture-of-Expert architectures use this approach to distribute their conditional computation paths across hardware.</p>
<p>Device coordination follows a specific pattern during training. In the forward pass, data flows sequentially through model segments on different devices. The backward pass propagates gradients in reverse order through these segments. During parameter updates, each device modifies only its assigned portion of the model. This coordination ensures mathematical equivalence to training on a single device while enabling the handling of models that exceed individual device memory capacities.</p>
<section id="sec-distributed-training-model-parallelism-implementation" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="sec-distributed-training-model-parallelism-implementation">Model Parallelism Implementation</h3>
<p>Model parallelism divides neural networks across multiple computing devices, with each device computing a distinct portion of the modelâ€™s operations. This division allows training of models whose parameter counts exceed single-device memory capacity. The technique encompasses device coordination, data flow management, and gradient computation across distributed model segments. The mechanics of model parallelism are illustrated in <a href="#fig-model-parallelism" class="quarto-xref">Figure&nbsp;3</a>. These steps are described next:</p>
<div id="fig-model-parallelism" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-env="figure" data-fig-pos="htb">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-model-parallelism-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="1f537eec066add14e7639d02ba60639295632226.svg" class="lightbox" data-gallery="quarto-lightbox-gallery-3" title="Figure&nbsp;3: Distributing a neural network across multiple devices enables training models larger than the memory capacity of a single device. This approach requires careful coordination of data flow and gradient computation between devices to maintain training efficiency."><img src="distributed_training_files/mediabag/1f537eec066add14e7639d02ba60639295632226.svg" class="img-fluid figure-img"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-model-parallelism-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;3: Distributing a neural network across multiple devices enables training models larger than the memory capacity of a single device. This approach requires careful coordination of data flow and gradient computation between devices to maintain training efficiency.
</figcaption>
</figure>
</div>
<section id="sec-distributed-training-model-partitioning" class="level4">
<h4 class="anchored" data-anchor-id="sec-distributed-training-model-partitioning">Model Partitioning</h4>
<p>The first step in model parallelism is dividing the model into smaller segments. For instance, in a deep neural network, layers are often divided among devices. In a system with two GPUs, the first half of the layers might reside on GPU 1, while the second half resides on GPU 2. Another approach is to split computations within a single layer, such as dividing matrix multiplications in transformer models across devices.</p>
</section>
<section id="sec-distributed-training-model-forward-pass" class="level4 page-columns page-full">
<h4 class="anchored" data-anchor-id="sec-distributed-training-model-forward-pass">Model Forward Pass</h4>
<p>During the forward pass, data flows sequentially through the partitions. For example, data processed by the first set of layers on GPU 1 is sent to GPU 2 for processing by the next set of layers. This sequential flow ensures that the entire model is used, even though it is distributed across multiple devices. Efficient inter-device communication is important to minimize delays during this step <span class="citation" data-cites="deepspeed_training_system_2021">(<a href="#ref-deepspeed_training_system_2021" role="doc-biblioref">Rasley et al. 2020</a>)</span>.</p>
<div class="no-row-height column-margin column-container"><div id="ref-deepspeed_training_system_2021" class="csl-entry" role="listitem">
Rasley, Jeff, Samyam Rajbhandari, Olatunji Ruwase, and Yuxiong He. 2020. <span>â€œDeepSpeed: System Optimizations Enable Training Deep Learning Models with over 100 Billion Parameters.â€</span> <em>arXiv Preprint arXiv:2020.12</em>.
</div></div></section>
<section id="sec-distributed-training-backward-pass-calculation-model" class="level4">
<h4 class="anchored" data-anchor-id="sec-distributed-training-backward-pass-calculation-model">Backward Pass and Calculation</h4>
<p>The backward pass computes gradients through the distributed model segments in reverse order. Each device calculates local gradients for its parameters and propagates necessary gradient information to previous devices. In transformer models, this means backpropagating through attention computations and feed-forward networks across device boundaries.</p>
<p>For example, in a two-device setup with attention mechanisms split between devices, the backward computation works as follows: The second device computes gradients for the final feed-forward layers and attention heads. It then sends the gradient tensors for the attention output to the first device. The first device uses these received gradients to compute updates for its attention parameters and earlier layer weights.</p>
</section>
<section id="sec-distributed-training-parameter-updates-model" class="level4">
<h4 class="anchored" data-anchor-id="sec-distributed-training-parameter-updates-model">Parameter Updates</h4>
<p>Parameter updates occur independently on each device using the computed gradients and an optimization algorithm. A device holding attention layer parameters applies updates using only the gradients computed for those specific parameters. This localized update approach differs from data parallelism, which requires gradient averaging across devices.</p>
<p>The optimization step proceeds as follows: Each device applies its chosen optimizer (such as Adam or AdaFactor) to update its portion of the model parameters. A device holding the first six transformer layers updates only those layersâ€™ weights and biases. This local parameter update eliminates the need for cross-device synchronization during the optimization step, reducing communication overhead.</p>
</section>
<section id="sec-distributed-training-iterative-process" class="level4">
<h4 class="anchored" data-anchor-id="sec-distributed-training-iterative-process">Iterative Process</h4>
<p>Like other training strategies, model parallelism repeats these steps for every batch of data. As the dataset is processed over multiple iterations, the distributed model converges toward optimal performance.</p>
</section>
</section>
<section id="sec-distributed-training-parallelism-variations" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="sec-distributed-training-parallelism-variations">Parallelism Variations</h3>
<p>Model parallelism can be implemented through different strategies for dividing the model across devices. The three primary approaches are layer-wise partitioning, operator-level partitioning, and pipeline parallelism, each suited to different model structures and computational needs.</p>
<section id="sec-distributed-training-layerwise-partitioning" class="level4">
<h4 class="anchored" data-anchor-id="sec-distributed-training-layerwise-partitioning">Layer-wise Partitioning</h4>
<p>Layer-wise partitioning assigns distinct model layers to separate computing devices. In transformer architectures, this translates to specific devices managing defined sets of attention and feed-forward blocks. As illustrated in <a href="#fig-layers-blocks" class="quarto-xref">Figure&nbsp;4</a>, a 24-layer transformer model distributed across four devices assigns six consecutive transformer blocks to each device. Device 1 processes blocks 1-6, device 2 handles blocks 7-12, and so forth.</p>
<div id="fig-layers-blocks" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-env="figure" data-fig-pos="htb">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-layers-blocks-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="f029dbda402733387fa860bf55b820dce764544e.svg" class="lightbox" data-gallery="quarto-lightbox-gallery-4" title="Figure&nbsp;4: Layer-Wise Model Parallelism: Distributing a transformer model across multiple gpus assigns consecutive layers to each device, enabling parallel processing of input data and accelerating training. This partitioning strategy allows each GPU to operate on a subset of the modelâ€™s layers, reducing the memory footprint and computational load per device."><img src="distributed_training_files/mediabag/f029dbda402733387fa860bf55b820dce764544e.svg" class="img-fluid figure-img"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-layers-blocks-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;4: <strong>Layer-Wise Model Parallelism</strong>: Distributing a transformer model across multiple gpus assigns consecutive layers to each device, enabling parallel processing of input data and accelerating training. This partitioning strategy allows each GPU to operate on a subset of the modelâ€™s layers, reducing the memory footprint and computational load per device.
</figcaption>
</figure>
</div>
<p>This sequential processing introduces device idle time, as each device must wait for the previous device to complete its computation before beginning work. For example, while device 1 processes the initial blocks, devices 2, 3, and 4 remain inactive. Similarly, when device 2 begins its computation, device 1 sits idle. This pattern of waiting and idle time reduces hardware utilization efficiency compared to other parallelization strategies.</p>
</section>
<section id="sec-distributed-training-pipeline-parallelism" class="level4 page-columns page-full">
<h4 class="anchored" data-anchor-id="sec-distributed-training-pipeline-parallelism">Pipeline Parallelism</h4>
<p>Pipeline parallelism extends layer-wise partitioning by introducing microbatching to minimize device idle time, as illustrated in <a href="#fig-pipline-parallelism" class="quarto-xref">Figure&nbsp;5</a>. Instead of waiting for an entire batch to sequentially pass through all devices, the computation is divided into smaller segments called microbatches <span class="citation" data-cites="harlap2018pipedream">(<a href="#ref-harlap2018pipedream" role="doc-biblioref">Harlap et al. 2019</a>)</span>. Each device, as represented by the rows in the drawing, processes its assigned model layers for different microbatches simultaneously. For example, the forward pass involves devices passing activations to the next stage (e.g., <span class="math inline">\(F_{0,0}\)</span> to <span class="math inline">\(F_{1,0}\)</span>). The backward pass transfers gradients back through the pipeline (e.g., <span class="math inline">\(B_{3,3}\)</span> to <span class="math inline">\(B_{2,3}\)</span>). This overlapping computation reduces idle time and increases throughput while maintaining the logical sequence of operations across devices.</p>
<div class="no-row-height column-margin column-container"><div id="ref-harlap2018pipedream" class="csl-entry" role="listitem">
Harlap, Aaron et al. 2019. <span>â€œPipeDream: Fast and Efficient Pipeline Parallel DNN Training.â€</span> In <em>Proceedings of the 27th ACM Symposium on Operating Systems Principles</em>, 1â€“15.
</div></div><div id="fig-pipline-parallelism" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-pipline-parallelism-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="d2dcdaba25dad3b1589c3e63d79b33779462d696.svg" class="lightbox" data-gallery="quarto-lightbox-gallery-5" title="Figure&nbsp;5: With model layers distributed across devices, microbatching divides each training batch into smaller segments that flow through the pipeline concurrently. This approach minimizes device idle time during both forward and backward passes by allowing multiple microbatches to be processed simultaneously at different pipeline stages. Activations flow sequentially between devices during the forward pass, while gradients propagate in reverse during backpropagation."><img src="distributed_training_files/mediabag/d2dcdaba25dad3b1589c3e63d79b33779462d696.svg" class="img-fluid figure-img"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-pipline-parallelism-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;5: With model layers distributed across devices, microbatching divides each training batch into smaller segments that flow through the pipeline concurrently. This approach minimizes device idle time during both forward and backward passes by allowing multiple microbatches to be processed simultaneously at different pipeline stages. Activations flow sequentially between devices during the forward pass, while gradients propagate in reverse during backpropagation.
</figcaption>
</figure>
</div>
<p>In a transformer model distributed across four devices, device 1 would process blocks 1-6 for microbatch <span class="math inline">\(N+1\)</span> while device 2 computes blocks 7-12 for microbatch <span class="math inline">\(N\)</span>. Simultaneously, device 3 executes blocks 13-18 for microbatch <span class="math inline">\(N-1\)</span>, and device 4 processes blocks 19-24 for microbatch <span class="math inline">\(N-2\)</span>. Each device maintains its assigned transformer blocks but operates on a different microbatch, creating a continuous flow of computation.</p>
<p>The transfer of hidden states between devices occurs continuously rather than in distinct phases. When device 1 completes processing a microbatch, it immediately transfers the output tensor of shape [microbatch_size, sequence_length, hidden_dimension] to device 2 and begins processing the next microbatch. This overlapping computation pattern maintains full hardware utilization while preserving the modelâ€™s mathematical properties.</p>
</section>
<section id="sec-distributed-training-tensor-parallelism" class="level4">
<h4 class="anchored" data-anchor-id="sec-distributed-training-tensor-parallelism">Tensor Parallelism</h4>
<p>Tensor parallelism (also called operator-level or intra-layer parallelism) divides individual neural network operations across devices. Unlike pipeline parallelism which assigns complete layers to devices, tensor parallelism splits the weight matrices within each layer. This distinction is critical: tensor parallelism requires high-bandwidth interconnects (NVLink) due to fine-grained communication within each layer, while pipeline parallelism tolerates lower bandwidth between stages.</p>
<div class="callout callout-style-default callout-note callout-titled" title="Terminology: Tensor Parallelism vs. Pipeline Parallelism">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Note</span>Terminology: Tensor Parallelism vs.&nbsp;Pipeline Parallelism
</div>
</div>
<div class="callout-body-container callout-body">
<p>Modern literature distinguishes two forms of model parallelism:</p>
<p><strong>Tensor Parallelism</strong> (intra-layer): Splits individual operations (matrix multiplies, attention heads) across devices. Requires high-bandwidth interconnects (NVLink) due to fine-grained communication within each layer.</p>
<p><strong>Pipeline Parallelism</strong> (inter-layer): Assigns complete layers to different devices. Requires only point-to-point communication between pipeline stages, tolerating lower bandwidth interconnects.</p>
<p>The Megatron-LM framework popularized this distinction, using tensor parallelism within nodes (8 GPUs on NVLink) and pipeline parallelism across nodes (InfiniBand).</p>
</div>
</div>
<p>Megatron-style tensor parallelism partitions matrix multiplications in two ways.</p>
<p>Column-parallel linear layers split weights along columns. For input <span class="math inline">\(X\)</span> and weight matrix <span class="math inline">\(W = [W_1 | W_2]\)</span> split across 2 GPUs: <span class="math display">\[Y = XW = X[W_1 | W_2] = [XW_1 | XW_2]\]</span> Each GPU computes its partition independently. Outputs are concatenated (no communication needed if followed by row-parallel layer).</p>
<p>Row-parallel linear layers split weights along rows. For <span class="math inline">\(W = \begin{bmatrix} W_1 \\ W_2 \end{bmatrix}\)</span>: <span class="math display">\[Y = XW = X_1 W_1 + X_2 W_2\]</span> Each GPU computes a partial sum. Outputs require AllReduce to combine.</p>
<p>In transformer architectures, Megatron applies this pattern:</p>
<ol type="1">
<li><strong>QKV projection</strong>: Column-parallel (weights split, outputs concatenated across heads)</li>
<li><strong>Attention output projection</strong>: Row-parallel (requires AllReduce after)</li>
<li><strong>First FFN layer</strong>: Column-parallel (split intermediate dimension)</li>
<li><strong>Second FFN layer</strong>: Row-parallel (requires AllReduce after)</li>
</ol>
<p>This design places AllReduce operations strategically: one after attention, one after FFN, totaling 2 AllReduce operations per transformer layer.</p>
<p>Communication volume per transformer layer depends on sequence length <span class="math inline">\(S\)</span>, hidden dimension <span class="math inline">\(H\)</span>, and batch size <span class="math inline">\(B\)</span>: <span class="math display">\[\text{Communication} = 2 \times B \times S \times H \times \text{sizeof(dtype)}\]</span></p>
<p>With <span class="math inline">\(S=2048\)</span>, <span class="math inline">\(H=4096\)</span>, <span class="math inline">\(B=4\)</span>, and FP16: <span class="math inline">\(2 \times 4 \times 2048 \times 4096 \times 2 = 134\)</span> MB per layer. For a 96-layer model, this totals 12.6 GB per training step, requiring NVLink bandwidth to avoid becoming the bottleneck.</p>
<p>Tensor parallelism scaling degrades rapidly beyond 8-way parallelism because:</p>
<ul>
<li>Communication volume grows linearly with tensor parallel degree</li>
<li>Computation per GPU decreases (less work to hide communication latency)</li>
<li>NVLink bandwidth becomes saturated</li>
</ul>
<p>Production systems (GPT-4, LLaMA, Gemini) use 8-way tensor parallelism within nodes, combined with pipeline parallelism across nodes, achieving the best balance of memory distribution and communication efficiency.</p>
</section>
</section>
<section id="sec-distributed-training-model-parallelism-advantages" class="level3">
<h3 class="anchored" data-anchor-id="sec-distributed-training-model-parallelism-advantages">Model Parallelism Advantages</h3>
<p>Model parallelism offers several significant benefits, making it an essential strategy for training large-scale models that exceed the capacity of individual devices. These advantages stem from its ability to partition the workload across multiple devices, enabling the training of more complex and resource-intensive architectures.</p>
<p>Memory scaling represents the primary advantage of model parallelism. Current transformer architectures contain up to hundreds of billions of parameters. A 175 billion parameter model with 32-bit floating point precision requires 700 GB of memory just to store its parameters. When accounting for activations, optimizer states, and gradients during training, the memory requirement multiplies several fold. Model parallelism makes training such architectures feasible by distributing these memory requirements across devices.</p>
<p>Another key advantage is the efficient utilization of device memory and compute power. Since each device only needs to store and process a portion of the model, memory usage is distributed across the system. This allows practitioners to work with larger batch sizes or more complex layers without exceeding memory limits, which can also improve training stability and convergence.</p>
<p>Model parallelism also provides flexibility for different model architectures. Whether the model is sequential, as in many natural language processing tasks, or composed of computationally intensive operations, as in attention-based models or convolutional networks, there is a partitioning strategy that fits the architecture. This adaptability makes model parallelism applicable to a wide variety of tasks and domains.</p>
<p>Finally, model parallelism is a natural complement to other distributed training strategies, such as data parallelism and pipeline parallelism. By combining these approaches, it becomes possible to train models that are both large in size and require extensive data. This hybrid flexibility is especially valuable in advanced research and production environments, where scaling models and datasets simultaneously is critical for achieving optimal performance.</p>
<p>While model parallelism offers these benefits, its effectiveness depends on careful partitioning strategy design, with specific challenges addressed in the following sections and the trade-offs involved in its use.</p>
</section>
<section id="sec-distributed-training-model-parallelism-limitations" class="level3">
<h3 class="anchored" data-anchor-id="sec-distributed-training-model-parallelism-limitations">Model Parallelism Limitations</h3>
<p>While model parallelism provides an effective approach for training large-scale models, it also introduces unique challenges. These challenges arise from the complexity of partitioning the model and the dependencies between partitions during training. Addressing these issues requires careful system design and optimization.</p>
<p>One major challenge in model parallelism is balancing the workload across devices. Not all parts of a model require the same amount of computation. For instance, in layer-wise partitioning, some layers may perform significantly more operations than others, leading to an uneven distribution of work. Devices responsible for the heavier computations may become bottlenecks, leaving others underutilized. This imbalance reduces overall efficiency and slows down training. Identifying optimal partitioning strategies is critical to ensuring all devices contribute evenly.</p>
<p>Another challenge is data dependency between devices. During the forward pass, activation tensors of shape [batch_size, sequence_length, hidden_dimension] must transfer between devices. For a typical transformer model with batch size 32, sequence length 2048, and hidden dimension 2048, each transfer moves approximately 512 MB of data at float32 precision. With gradient transfers in the backward pass, a single training step can require several gigabytes of inter-device communication. On systems using PCIe interconnects with 64 GB/s theoretical bandwidth, these transfers introduce significant latency.</p>
<p>Model parallelism also increases the complexity of implementation and debugging. Partitioning the model, ensuring proper data flow, and synchronizing gradients across devices require detailed coordination. Errors in any of these steps can lead to incorrect gradient updates or even model divergence. Debugging such errors is often more difficult in distributed systems, as issues may arise only under specific conditions or workloads.</p>
<p>A further challenge is pipeline bubbles in pipeline parallelism. With <span class="math inline">\(m\)</span> pipeline stages, the first <span class="math inline">\(m-1\)</span> steps operate at reduced efficiency as the pipeline fills. For example, in an 8-device pipeline, the first device begins processing immediately, but the eighth device remains idle for 7 steps. This warmup period reduces hardware utilization by a fraction of approximately <span class="math inline">\((m-1)/b\)</span>, where <span class="math inline">\(b\)</span> is the number of microbatches in the training step.</p>
<p>Finally, model parallelism may be less effective for certain architectures, such as models with highly interdependent operations. In these cases, splitting the model may lead to excessive communication overhead, outweighing the benefits of parallel computation. For such models, alternative strategies like data parallelism or hybrid approaches might be more suitable.</p>
<p>Despite these challenges, model parallelism remains an indispensable tool for training large models. With careful optimization and the use of modern frameworks, many of these issues can be mitigated, enabling efficient distributed training at scale.</p>
</section>
</section>
<section id="sec-distributed-training-hybrid-parallelism" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="sec-distributed-training-hybrid-parallelism">Hybrid Parallelism</h2>
<p>Recognizing that both data and model constraints can occur simultaneously, hybrid parallelism combines model parallelism and data parallelism when training neural networks <span class="citation" data-cites="narayanan_pipeline_parallelism_2021">(<a href="#ref-narayanan_pipeline_parallelism_2021" role="doc-biblioref">Narayanan et al. 2021</a>)</span>. A model might be too large to store on one GPU (requiring model parallelism) while simultaneously needing to process large batches of data efficiently (requiring data parallelism).</p>
<div class="no-row-height column-margin column-container"><div id="ref-narayanan_pipeline_parallelism_2021" class="csl-entry" role="listitem">
Narayanan, Deepak et al. 2021. <span>â€œEfficient Large-Scale Language Model Training on GPU Clusters Using Megatron-LM.â€</span> <em>arXiv Preprint arXiv:2104.04473</em>.
</div></div><p>Training a 175-billion parameter language model on a dataset of 300 billion tokens demonstrates hybrid parallelism in practice. The neural network layers distribute across multiple GPUs through model parallelism, while data parallelism enables different GPU groups to process separate batches. The hybrid approach coordinates these two forms of parallelization.</p>
<p>This strategy addresses two key constraints. First, memory constraints arise when model parameters exceed single-device memory capacity. Second, computational demands increase when dataset size necessitates distributed processing.</p>
<section id="sec-distributed-training-hybrid-parallelism-implementation" class="level3">
<h3 class="anchored" data-anchor-id="sec-distributed-training-hybrid-parallelism-implementation">Hybrid Parallelism Implementation</h3>
<p>Hybrid parallelism operates by combining the processes of model partitioning and dataset splitting, ensuring efficient utilization of both memory and computation across devices. This integration allows large-scale machine learning systems to overcome the constraints imposed by individual parallelism strategies.</p>
<section id="sec-distributed-training-model-data-partitioning" class="level4">
<h4 class="anchored" data-anchor-id="sec-distributed-training-model-data-partitioning">Model and Data Partitioning</h4>
<p>Hybrid parallelism divides both model architecture and training data across devices. The model divides through layer-wise or operator-level partitioning, where GPUs process distinct neural network segments. Simultaneously, the dataset splits into subsets, allowing each device group to train on different batches. A transformer model might distribute its attention layers across four GPUs, while each GPU group processes a unique 1,000-example batch. This dual partitioning distributes memory requirements and computational workload.</p>
</section>
<section id="sec-distributed-training-forward-pass-hybrid" class="level4">
<h4 class="anchored" data-anchor-id="sec-distributed-training-forward-pass-hybrid">Forward Pass</h4>
<p>During the forward pass, input data flows through the distributed model. Each device processes its assigned portion of the model using the data subset it holds. For example, in a hybrid system with four devices, two devices might handle different layers of the model (model parallelism) while simultaneously processing distinct data batches (data parallelism). Communication between devices ensures that intermediate outputs from model partitions are passed seamlessly to subsequent partitions.</p>
</section>
<section id="sec-distributed-training-backward-pass-gradient-calculation-hybrid" class="level4">
<h4 class="anchored" data-anchor-id="sec-distributed-training-backward-pass-gradient-calculation-hybrid">Backward Pass and Gradient Calculation</h4>
<p>During the backward pass, gradients are calculated for the model partitions stored on each device. Data-parallel devices that process the same subset of the model but different data batches aggregate their gradients, ensuring that updates reflect contributions from the entire dataset. For model-parallel devices, gradients are computed locally and passed to the next layer in reverse order. In a two-device model-parallel configuration, for example, the first device computes gradients for layers 1-3, then transmits these to the second device for layers 4-6. This combination of gradient synchronization and inter-device communication ensures consistency across the distributed system.</p>
</section>
<section id="sec-distributed-training-parameter-updates-hybrid" class="level4">
<h4 class="anchored" data-anchor-id="sec-distributed-training-parameter-updates-hybrid">Parameter Updates</h4>
<p>After gradient synchronization, model parameters are updated using the chosen optimization algorithm. Devices working in data parallelism update their shared model partitions consistently, while model-parallel devices apply updates to their local segments. Efficient communication is critical in this step to minimize delays and ensure that updates are correctly propagated across all devices.</p>
</section>
<section id="sec-distributed-training-iterative-process-hybrid" class="level4">
<h4 class="anchored" data-anchor-id="sec-distributed-training-iterative-process-hybrid">Iterative Process</h4>
<p>Hybrid parallelism follows an iterative process similar to other training strategies. The combination of model and data distribution allows the system to process large datasets and complex models efficiently over multiple training epochs. By balancing the computational workload and memory requirements, hybrid parallelism enables the training of advanced machine learning models that would otherwise be infeasible.</p>
</section>
</section>
<section id="sec-distributed-training-parallelism-variations-hybrid" class="level3">
<h3 class="anchored" data-anchor-id="sec-distributed-training-parallelism-variations-hybrid">Parallelism Variations</h3>
<p>Hybrid parallelism can be implemented in different configurations, depending on the model architecture, dataset characteristics, and available hardware. These variations allow for tailored solutions that optimize performance and scalability for specific training requirements.</p>
<section id="sec-distributed-training-hierarchical-parallelism" class="level4">
<h4 class="anchored" data-anchor-id="sec-distributed-training-hierarchical-parallelism">Hierarchical Parallelism</h4>
<p>Hierarchical hybrid parallelism applies model parallelism to divide the model across devices first and then layers data parallelism on top to handle the dataset distribution. For example, in a system with eight devices, four devices may hold different partitions of the model, while each partition is replicated across the other four devices for data parallel processing. This approach is well-suited for large models with billions of parameters, where memory constraints are a primary concern.</p>
<p>Hierarchical hybrid parallelism ensures that the model size is distributed across devices, reducing memory requirements, while data parallelism ensures that multiple data samples are processed simultaneously, improving throughput. This dual-layered approach is particularly effective for models like transformers, where each layer may have a significant memory footprint.</p>
</section>
<section id="sec-distributed-training-intralayer-parallelism" class="level4">
<h4 class="anchored" data-anchor-id="sec-distributed-training-intralayer-parallelism">Intra-layer Parallelism</h4>
<p>Intra-layer hybrid parallelism combines model and data parallelism within individual layers of the model. For instance, in a transformer architecture, the attention mechanism can be split across multiple devices (model parallelism), while each device processes distinct batches of data (data parallelism). This fine-grained integration allows the system to optimize resource usage at the level of individual operations, enabling training for models with extremely large intermediate computations.</p>
<p>This variation is particularly useful in scenarios where specific layers, such as attention or feedforward layers, have computationally intensive operations that are difficult to distribute effectively using model or data parallelism alone. Intra-layer hybrid parallelism addresses this challenge by applying both strategies simultaneously.</p>
</section>
<section id="sec-distributed-training-interlayer-parallelism" class="level4">
<h4 class="anchored" data-anchor-id="sec-distributed-training-interlayer-parallelism">Inter-layer Parallelism</h4>
<p>Inter-layer hybrid parallelism focuses on distributing the workload between model and data parallelism at the level of distinct model layers. For example, early layers of a neural network may be distributed using model parallelism, while later layers use data parallelism. This approach aligns with the observation that certain layers in a model may be more memory-intensive, while others benefit from increased data throughput.</p>
<p>This configuration allows for dynamic allocation of resources, adapting to the specific demands of different layers in the model. By tailoring the parallelism strategy to the unique characteristics of each layer, inter-layer hybrid parallelism achieves an optimal balance between memory usage and computational efficiency.</p>
</section>
</section>
<section id="sec-distributed-training-hybrid-parallelism-advantages" class="level3">
<h3 class="anchored" data-anchor-id="sec-distributed-training-hybrid-parallelism-advantages">Hybrid Parallelism Advantages</h3>
<p>The adoption of hybrid parallelism in machine learning systems addresses some of the most significant challenges posed by the ever-growing scale of models and datasets. By blending the strengths of model parallelism and data parallelism, this approach provides a solution to scaling modern machine learning workloads.</p>
<p>One of the most prominent benefits of hybrid parallelism is its ability to scale seamlessly across both the model and the dataset. Modern neural networks, particularly transformers used in natural language processing and vision applications, often contain billions of parameters. These models, paired with massive datasets, make training on a single device impractical or even impossible. Hybrid parallelism enables the division of the model across multiple devices to manage memory constraints while simultaneously distributing the dataset to process vast amounts of data efficiently. This dual capability ensures that training systems can handle the computational and memory demands of the largest models and datasets without compromise.</p>
<p>Another critical advantage lies in hardware utilization. In many distributed training systems, inefficiencies can arise when devices sit idle during different stages of computation or synchronization. Hybrid parallelism mitigates this issue by ensuring that all devices are actively engaged. Whether a device is computing forward passes through its portion of the model or processing data batches, hybrid strategies maximize resource usage, leading to faster training times and improved throughput.</p>
<p>Flexibility is another hallmark of hybrid parallelism. Machine learning models vary widely in architecture and computational demands. For instance, convolutional neural networks prioritize spatial data processing, while transformers require intensive operations like matrix multiplications in attention mechanisms. Hybrid parallelism adapts to these diverse needs by allowing practitioners to apply model and data parallelism selectively. This adaptability ensures that hybrid approaches can be tailored to the specific requirements of a given model, making it a versatile solution for diverse training scenarios.</p>
<p>Hybrid parallelism reduces communication bottlenecks, a common issue in distributed systems. By striking a balance between distributing model computations and spreading data processing, hybrid strategies minimize the amount of inter-device communication required during training. This efficient coordination not only speeds up the training process but also enables the effective use of large-scale distributed systems where network latency might otherwise limit performance.</p>
<p>Finally, hybrid parallelism supports the ambitious scale of modern AI research and development. It provides a framework for leveraging advanced hardware infrastructures, including clusters of GPUs or TPUs, to train models that push the boundaries of whatâ€™s possible. Without hybrid parallelism, many of the breakthroughs in AI, including large language models and advanced vision systems, would remain unattainable due to resource limitations.</p>
<p>By enabling scalability, maximizing hardware efficiency, and offering flexibility, hybrid parallelism has become an essential strategy for training the most complex machine learning systems. It is not just a solution to todayâ€™s challenges but also a foundation for the future of AI, where models and datasets will continue to grow in complexity and size.</p>
</section>
<section id="sec-distributed-training-hybrid-parallelism-limitations" class="level3">
<h3 class="anchored" data-anchor-id="sec-distributed-training-hybrid-parallelism-limitations">Hybrid Parallelism Limitations</h3>
<p>While hybrid parallelism provides a robust framework for scaling machine learning training, it also introduces complexities that require careful consideration. These challenges stem from the intricate coordination needed to integrate both model and data parallelism effectively. Understanding these obstacles is crucial for designing efficient hybrid systems and avoiding potential bottlenecks.</p>
<p>One of the primary challenges of hybrid parallelism is communication overhead. Both model and data parallelism involve significant inter-device communication. In model parallelism, devices must exchange intermediate outputs and gradients to maintain the sequential flow of computation. In data parallelism, gradients computed on separate data subsets must be synchronized across devices. Hybrid parallelism compounds these demands, as it requires efficient communication for both processes simultaneously. If not managed properly, the resulting overhead can negate the benefits of parallelization, particularly in large-scale systems with slower interconnects or high network latency.</p>
<p>Another critical challenge is the complexity of implementation. Hybrid parallelism demands a nuanced understanding of both model and data parallelism techniques, as well as the underlying hardware and software infrastructure. Designing efficient hybrid strategies involves making decisions about how to partition the model, how to distribute data, and how to synchronize computations across devices. This process often requires extensive experimentation and optimization, particularly for custom architectures or non-standard hardware setups. While modern frameworks like PyTorch and TensorFlow provide tools for distributed training, implementing hybrid parallelism at scale still requires significant engineering expertise.</p>
<p>Workload balancing also presents a challenge in hybrid parallelism. In a distributed system, not all devices may have equal computational capacity. Some devices may process data or compute gradients faster than others, leading to inefficiencies as faster devices wait for slower ones to complete their tasks. Additionally, certain model layers or operations may require more resources than others, creating imbalances in computational load. Managing this disparity requires careful tuning of partitioning strategies and the use of dynamic workload distribution techniques.</p>
<p>Memory constraints remain a concern, even in hybrid setups. While model parallelism addresses the issue of fitting large models into device memory, the additional memory requirements for data parallelism, such as storing multiple data batches and gradient buffers, can still exceed available capacity. This is especially true for models with extremely large intermediate computations, such as transformers with high-dimensional attention mechanisms. Balancing memory usage across devices is essential to prevent resource exhaustion during training.</p>
<p>Lastly, hybrid parallelism poses challenges related to fault tolerance and debugging. Distributed systems are inherently more prone to hardware failures and synchronization errors. Debugging issues in hybrid setups can be significantly more complex than in standalone model or data parallelism systems, as errors may arise from interactions between the two approaches. Ensuring robust fault-tolerance mechanisms and designing tools for monitoring and debugging distributed systems are essential for maintaining reliability.</p>
<p>Despite these challenges, hybrid parallelism remains an indispensable strategy for training large-scale machine learning models. By addressing these obstacles through optimized communication protocols, intelligent partitioning strategies, and robust fault-tolerance systems, practitioners can unlock the full potential of hybrid parallelism and drive innovation in AI research and applications.</p>
</section>
</section>
<section id="sec-distributed-training-parallelism-strategy-comparison" class="level2">
<h2 class="anchored" data-anchor-id="sec-distributed-training-parallelism-strategy-comparison">Parallelism Strategy Comparison</h2>
<p>The features of data parallelism, model parallelism, pipeline parallelism, and hybrid parallelism are summarized in <a href="#tbl-parallelism-compare" class="quarto-xref">Table&nbsp;1</a>. This comparison highlights their respective focuses, memory requirements, communication overheads, scalability, implementation complexity, and ideal use cases. By examining these factors, practitioners can determine the most suitable approach for their training needs.</p>
<div id="tbl-parallelism-compare" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-tbl figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-parallelism-compare-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;1: <strong>Parallel Training Strategies</strong>: Data, model, pipeline, and hybrid parallelism each address the challenges of scaling machine learning training by distributing workload across devices, differing in how they partition data and model parameters to optimize memory usage, communication, and scalability. Understanding these trade-offs enables practitioners to select the most effective approach for their specific model and infrastructure.
</figcaption>
<div aria-describedby="tbl-parallelism-compare-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<table class="caption-top table">
<colgroup>
<col style="width: 19%">
<col style="width: 19%">
<col style="width: 19%">
<col style="width: 19%">
<col style="width: 20%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;"><strong>Aspect</strong></th>
<th style="text-align: left;"><strong>Data Parallelism</strong></th>
<th style="text-align: left;"><strong>Model Parallelism</strong></th>
<th style="text-align: left;"><strong>Pipeline Parallelism</strong></th>
<th style="text-align: left;"><strong>Hybrid Parallelism</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;"><strong>Focus</strong></td>
<td style="text-align: left;">Distributes dataset across devices, each with a full model copy</td>
<td style="text-align: left;">Distributes the model across devices, each handling a portion of the model</td>
<td style="text-align: left;">Distributes model stages in pipeline, processing microbatches concurrently</td>
<td style="text-align: left;">Combines multiple parallelism strategies for balanced scalability</td>
</tr>
<tr class="even">
<td style="text-align: left;"><strong>Memory Requirement per Device</strong></td>
<td style="text-align: left;">High (entire model on each device)</td>
<td style="text-align: left;">Low (model split across devices)</td>
<td style="text-align: left;">Low to Moderate (stages split across devices)</td>
<td style="text-align: left;">Moderate (splits model and dataset across devices)</td>
</tr>
<tr class="odd">
<td style="text-align: left;"><strong>Communication Overhead</strong></td>
<td style="text-align: left;">Moderate to High (gradient synchronization across devices)</td>
<td style="text-align: left;">High (communication for intermediate activations and gradients)</td>
<td style="text-align: left;">Moderate (activation passing between stages)</td>
<td style="text-align: left;">Very High (requires synchronization for both model and data)</td>
</tr>
<tr class="even">
<td style="text-align: left;"><strong>Scalability</strong></td>
<td style="text-align: left;">Good for large datasets with moderate model sizes</td>
<td style="text-align: left;">Good for very large models with smaller datasets</td>
<td style="text-align: left;">Good for deep models with many layers</td>
<td style="text-align: left;">Excellent for extremely large models and datasets</td>
</tr>
<tr class="odd">
<td style="text-align: left;"><strong>Implementation Complexity</strong></td>
<td style="text-align: left;">Low to Moderate (relatively straightforward with existing tools)</td>
<td style="text-align: left;">Moderate to High (requires careful partitioning and coordination)</td>
<td style="text-align: left;">Moderate to High (requires pipeline scheduling and microbatch management)</td>
<td style="text-align: left;">High (complex integration of multiple parallelism strategies)</td>
</tr>
<tr class="even">
<td style="text-align: left;"><strong>Ideal Use Case</strong></td>
<td style="text-align: left;">Large datasets where model fits within a single device</td>
<td style="text-align: left;">Extremely large models that exceed single-device memory limits</td>
<td style="text-align: left;">Deep models with sequential stages that can tolerate microbatch latency</td>
<td style="text-align: left;">Training massive models on vast datasets in large-scale systems</td>
</tr>
</tbody>
</table>
</div>
</figure>
</div>
<p><a href="#fig-parallelism-flowchart" class="quarto-xref">Figure&nbsp;6</a> provides a general guideline for selecting parallelism strategies in distributed training systems. While the chart offers a structured decision-making process based on model size, dataset size, and scaling constraints, it is intentionally simplified. Real-world scenarios often involve additional complexities such as hardware heterogeneity, communication bandwidth, and workload imbalance, which may influence the choice of parallelism techniques. The chart is best viewed as a foundational tool for understanding the trade-offs and decision points in parallelism strategy selection. Practitioners should consider this guideline as a starting point and adapt it to the specific requirements and constraints of their systems to achieve optimal performance.</p>
<div id="fig-parallelism-flowchart" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-env="figure" data-fig-pos="htb">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-parallelism-flowchart-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="a326edcb4fecf163c3b9dda854545568b3807fff.svg" class="lightbox" data-gallery="quarto-lightbox-gallery-6" title="Figure&nbsp;6: Distributed training systems use data, model, or hybrid parallelism based on model size, dataset size, and scaling constraints to accelerate training and efficiently utilize resources. This flowchart guides practitioners through a decision process, recognizing that real-world deployments often require adaptation due to factors like hardware heterogeneity and workload imbalance."><img src="distributed_training_files/mediabag/a326edcb4fecf163c3b9dda854545568b3807fff.svg" class="img-fluid figure-img"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-parallelism-flowchart-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;6: Distributed training systems use data, model, or hybrid parallelism based on model size, dataset size, and scaling constraints to accelerate training and efficiently utilize resources. This flowchart guides practitioners through a decision process, recognizing that real-world deployments often require adaptation due to factors like hardware heterogeneity and workload imbalance.
</figcaption>
</figure>
</div>
</section>
<section id="sec-distributed-training-framework-integration" class="level2">
<h2 class="anchored" data-anchor-id="sec-distributed-training-framework-integration">Framework Integration</h2>
<p>While the theoretical foundations of distributed training establish the mathematical principles for scaling across multiple devices, modern frameworks provide abstractions that make these concepts accessible to practitioners. Understanding how frameworks like PyTorch translate distributed training theory into practical APIs bridges the gap between mathematical concepts and implementation.</p>
<section id="sec-distributed-training-data-parallel-framework-apis" class="level3">
<h3 class="anchored" data-anchor-id="sec-distributed-training-data-parallel-framework-apis">Data Parallel Framework APIs</h3>
<p>The data parallelism mechanisms we explored earlierâ€”gradient averaging, AllReduce communication, and parameter synchronizationâ€”are abstracted through framework APIs that handle the complex coordination automatically. PyTorch provides two primary approaches that demonstrate different trade-offs between simplicity and performance.</p>
<p><code>torch.nn.DataParallel</code> represents the simpler approach, automatically replicating the model across available GPUs within a single node. This API abstracts the gradient collection and averaging process, requiring minimal code changes to existing single-GPU training scripts. However, this simplicity comes with performance limitations, as the implementation uses a parameter server approach that can create communication bottlenecks when scaling beyond 4-8 GPUs.</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Simple data parallelism - framework handles gradient synchronization</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> torch.nn.DataParallel(model)</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Training loop remains unchanged - framework automatically:</span></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="co"># 1. Splits batch across GPUs</span></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="co"># 2. Replicates model on each device</span></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="co"># 3. Gathers gradients and averages them</span></span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a><span class="co"># 4. Broadcasts updated parameters</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>For production scale training, <code>torch.distributed</code> provides the high-performance alternative that implements the efficient AllReduce communication patterns discussed earlier. This API requires explicit initialization of process groups and distributed coordination but enables the linear scaling characteristics essential for large-scale training.</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Production distributed training - explicit control over communication</span></span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.distributed <span class="im">as</span> dist</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a>dist.init_process_group(backend<span class="op">=</span><span class="st">"nccl"</span>)  <span class="co"># NCCL for GPU communication</span></span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> torch.nn.parallel.DistributedDataParallel(model)</span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Framework now uses optimized AllReduce instead of parameter server</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>The key insight is that <code>DistributedDataParallel</code> implements the efficient ring AllReduce algorithm automatically, transforming the O(n) communication complexity we discussed into practical code that achieves 90%+ parallel efficiency at scale. The framework handles device placement, gradient bucketing for efficient communication, and overlapping computation with communication.</p>
</section>
<section id="sec-distributed-training-model-parallel-framework-support" class="level3">
<h3 class="anchored" data-anchor-id="sec-distributed-training-model-parallel-framework-support">Model Parallel Framework Support</h3>
<p>Model parallelism requires more explicit coordination since frameworks must manage cross-device tensor placement and data flow. PyTorch addresses this through manual device placement and the emerging <code>torch.distributed.pipeline</code> API for pipeline parallelism.</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Manual model parallelism - explicit device placement</span></span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> ModelParallelNet(nn.Module):</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>):</span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.layers_gpu0 <span class="op">=</span> nn.Sequential(...).to(<span class="st">"cuda:0"</span>)</span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.layers_gpu1 <span class="op">=</span> nn.Sequential(...).to(<span class="st">"cuda:1"</span>)</span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.layers_gpu0(x.to(<span class="st">"cuda:0"</span>))</span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.layers_gpu1(</span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a>            x.to(<span class="st">"cuda:1"</span>)</span>
<span id="cb3-12"><a href="#cb3-12" aria-hidden="true" tabindex="-1"></a>        )  <span class="co"># Cross-GPU data transfer</span></span>
<span id="cb3-13"><a href="#cb3-13" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> x</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>This manual approach exposes the sequential dependencies and communication overhead inherent in model parallelism, requiring careful management of tensor movement between devices. The framework automatically handles the backward pass gradient flow across device boundaries, but practitioners must consider the performance implications of frequent device transfers.</p>
</section>
<section id="sec-distributed-training-communication-primitives" class="level3">
<h3 class="anchored" data-anchor-id="sec-distributed-training-communication-primitives">Communication Primitives</h3>
<p>Modern frameworks expose the communication operations that enable distributed training through high-level APIs. These primitives abstract the low-level NCCL operations while maintaining performance:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Framework-provided collective operations</span></span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a>dist.all_reduce(tensor)  <span class="co"># Gradient averaging across all devices</span></span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a>dist.broadcast(tensor, src<span class="op">=</span><span class="dv">0</span>)  <span class="co"># Parameter broadcasting from master</span></span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a>dist.all_gather(</span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a>    tensor_list, tensor</span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a>)  <span class="co"># Collecting tensors from all devices</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>These APIs translate directly to the NCCL collective operations that implement the efficient communication patterns discussed earlier, demonstrating how frameworks provide accessible interfaces to complex distributed systems concepts while maintaining the performance characteristics essential for production training.</p>
<p>The framework abstractions enable practitioners to focus on model architecture and training dynamics while leveraging sophisticated distributed systems optimizations. This separation of concernsâ€”mathematical foundations handled by the framework, model design controlled by the practitionerâ€”exemplifies how modern ML systems balance accessibility with performance.</p>
</section>
</section>
<section id="sec-distributed-training-hardware-infrastructure" class="level2">
<h2 class="anchored" data-anchor-id="sec-distributed-training-hardware-infrastructure">Hardware Infrastructure for Scale</h2>
<p>The parallelism strategies examined in previous sections assume underlying hardware capable of efficient inter-device communication. This section examines the hardware architectures that enable AI systems to scale from individual accelerators to warehouse-scale computing, analyzing how multi-chip systems introduce qualitatively different constraints around communication overhead, memory coherence, and fault tolerance.</p>
<section id="sec-distributed-training-multichip-acceleration" class="level3">
<h3 class="anchored" data-anchor-id="sec-distributed-training-multichip-acceleration">Multi-Chip AI Acceleration</h3>
<p>The transition from single-chip to multi-chip architectures represents more than simple replication. It requires rethinking how computations distribute across processors, how data flows between chips, and how systems maintain coherence at scale. Where single-chip optimization focuses on maximizing utilization within fixed resources, multi-chip systems must balance computational distribution against communication overhead, memory coherence costs, and synchronization complexity. These challenges fundamentally transform the optimization landscape, requiring new abstractions and techniques beyond those developed for individual accelerators.</p>
<p>The scaling of AI systems follows a natural progression: integration within a single package through chiplet architectures, multi-GPU configurations within a server, distributed accelerator pods, and wafer-scale integration. Each approach presents unique trade-offs between computational density, communication overhead, and system complexity. Chiplet architectures maintain high-speed interconnects within a package, while distributed systems sacrifice communication latency for massive parallelism.</p>
<section id="sec-distributed-training-chiplet-architectures" class="level4">
<h4 class="anchored" data-anchor-id="sec-distributed-training-chiplet-architectures">Chiplet-Based Architectures</h4>
<p>Chiplet architectures achieve scaling by partitioning large designs into smaller, modular dies that are interconnected within a single package. Small, specialized semiconductor dies connect together within a single package to create larger, more complex processors. AMDâ€™s EPYC processors use up to 8 chiplets connected via Infinity Fabric, achieving yields above 80% versus 20% for equivalent monolithic designs. This modular approach reduces manufacturing costs and enables mixing different technologies, with compute chiplets in 7 nm paired with I/O chiplets in 14 nm, optimizing each function independently.</p>
<p>Modern AI accelerators partition large designs into smaller chiplets and connect them via high-bandwidth interconnects, enabling scalability beyond monolithic die limitations and improving manufacturing yields. HBM stacks provide fast access to data, crucial for the memory-intensive workloads common in machine learning. AMDâ€™s Instinct MI300 exemplifies this approach by integrating multiple compute chiplets alongside memory chiplets, linked by high-speed die-to-die interconnects. This modular design allows manufacturers to bypass the manufacturing limits of monolithic chips while still achieving high-density compute.</p>
<p>However, even within a single package, scaling is not without challenges. Inter-chiplet communication latency, memory coherence, and thermal management become critical factors as more chiplets are integrated. Memory coherence presents particular complexity: ensuring all processors in a system see the same consistent view of shared memory when multiple cores or chips access the same data. Traditional cache coherence protocols like MESI add 10-50 ns latency for multi-core CPUs. For AI accelerators with thousands of cores, coherence becomes prohibitively expensive, so most ML hardware instead uses explicit memory management where programmers control data placement and synchronization manually.</p>
</section>
<section id="sec-distributed-training-multi-gpu-systems" class="level4">
<h4 class="anchored" data-anchor-id="sec-distributed-training-multi-gpu-systems">Multi-GPU Systems</h4>
<p>Beyond chiplet-based designs, AI workloads often require multiple discrete GPUs working together. In multi-GPU systems, each accelerator has its own dedicated memory and compute resources, but they must efficiently share data and synchronize execution.</p>
<p>A common example is NVIDIA DGX systems, which integrate multiple GPUs connected via NVLink or PCIe. This architecture enables workloads to be split across GPUs, typically using data parallelism (where each GPU processes a different batch of data) or model parallelism (where different GPUs handle different parts of a neural network).</p>
<p>NVSwitch interconnects enable high-speed communication between GPUs, reducing bottlenecks in distributed training. However, scaling up the number of GPUs introduces fundamental distributed coordination challenges that become the dominant performance constraint. The arithmetic intensity of transformer training (0.5-2 FLOPS/byte) forces frequent gradient synchronization across GPUs, where AllReduce operations must aggregate 175 billion parameters in GPT-3 scale models. NVSwitch provides 600 GB/s bidirectional bandwidth, but even this substantial interconnect becomes bandwidth-bound when 8 H100 GPUs simultaneously exchange gradients, creating a 4.8 TB/s aggregate demand that exceeds available capacity.</p>
<p>The coordination complexity compounds exponentially. While two GPUs require a single communication channel, eight GPUs need 28 interconnect paths, and fault tolerance requirements mandate redundant communication patterns. Memory consistency protocols further complicate coordination as different GPUs may observe weight updates at different times, requiring sophisticated synchronization primitives that can add 10-50 microseconds latency per training step. These seemingly small delays aggregate to hours of training time across million-iteration runs.</p>
</section>
<section id="sec-distributed-training-amdahl-analysis" class="level4">
<h4 class="anchored" data-anchor-id="sec-distributed-training-amdahl-analysis">Communication Overhead and Amdahlâ€™s Law</h4>
<p>The fundamental limitation of distributed AI training stems from Amdahlâ€™s Law, which quantifies how communication overhead constrains parallel speedup regardless of available compute power. For distributed neural network training, communication overhead during gradient synchronization creates a sequential bottleneck that limits scalability even with infinite parallelism.</p>
<p>The maximum speedup achievable with distributed training is bound by Amdahlâ€™s Law: <span class="math display">\[
\text{Speedup} = \frac{1}{(1-P) + \frac{P}{N}}
\]</span> where <span class="math inline">\(P\)</span> is the fraction of work that can be parallelized and <span class="math inline">\(N\)</span> is the number of processors. For AI training, the correct formulation accounts for communication time that does not decrease with more workers: <span class="math display">\[
\text{Speedup} = \frac{T_{compute}}{T_{compute}/N + T_{comm}}
\]</span> where <span class="math inline">\(T_{comm}\)</span> is largely independent of <span class="math inline">\(N\)</span> for ring AllReduce (or grows as <span class="math inline">\(\log N\)</span> for tree-based approaches). This can be rewritten as: <span class="math display">\[
\text{Speedup} = \frac{N}{1 + N \cdot (T_{comm}/T_{compute})}
\]</span></p>
<p>Consider training a 175 B parameter model with 1000 H100 GPUs as a concrete example:</p>
<ul>
<li><strong>Computation time per iteration</strong>: 100 ms of forward/backward passes per GPU</li>
<li><strong>Gradient size</strong>: 175 B parameters Ã— 4 bytes = 700 GB in FP32</li>
<li><strong>Ring AllReduce time</strong>: For ring AllReduce, each GPU sends/receives <span class="math inline">\(2 \times (N-1)/N \times 700\text{GB} \approx 1.4\text{TB}\)</span></li>
</ul>
<p>With ring AllReduce across 1000 GPUs connected via 600 GB/s links:</p>
<ul>
<li><strong>Intra-node (8 GPUs via NVLink)</strong>: <span class="math inline">\(1.4\text{TB} / 600\text{GB/s} \approx 2.3\)</span> seconds</li>
<li><strong>Inter-node adds latency</strong>: <span class="math inline">\(1000 \times \alpha\)</span> where <span class="math inline">\(\alpha \approx 1\mu s\)</span> per hop</li>
</ul>
<p>The resulting scaling efficiency is: <span class="math display">\[
\text{Efficiency} = \frac{T_{compute}}{T_{compute} + T_{comm}} = \frac{100\text{ms}}{100\text{ms} + 2300\text{ms}} \approx 4\%
\]</span></p>
<p>This is why real systems use mixed precision (FP16 = 350 GB, halving communication), gradient compression, and pipeline parallelism. With FP16 and 4-way pipeline parallelism reducing synchronization to 1/4 of parameters per stage, efficiency improves dramatically: <span class="math display">\[
\text{Efficiency} = \frac{100\text{ms}}{100\text{ms} + 290\text{ms}} \approx 26\%
\]</span></p>
<p>This demonstrates why pure data parallelism fails at scale and why hybrid strategies are essential.</p>
<p>Communication requirements scale superlinearly with model size and linearly with the number of parameters. Modern transformer models require gradient synchronization across all parameters during each training step:</p>
<ul>
<li><strong>GPT-3 (175 B parameters)</strong>: 700 GB gradient exchange per step</li>
<li><strong>GPT-4 (estimated 1.8 T parameters)</strong>: approximately 7 TB gradient exchange per step</li>
<li><strong>Future 10 T parameter models</strong>: approximately 40 TB gradient exchange per step</li>
</ul>
<p>Even with advanced interconnects like NVLink 4.0 (1.8 TB/s), gradient synchronization for 10 T parameter models would require 22+ seconds per training step, making distributed training impractical without algorithmic innovations like gradient compression or asynchronous updates.</p>
</section>
<section id="sec-distributed-training-tpu-pods" class="level4">
<h4 class="anchored" data-anchor-id="sec-distributed-training-tpu-pods">TPU Pods</h4>
<p>As models and datasets continue to expand, training and inference workloads must extend beyond single-server configurations. Googleâ€™s TPU Pods represent a pioneering approach to this challenge, interconnecting hundreds of TPUs to function as a unified system.</p>
<p>The architectural design of TPU Pods differs fundamentally from traditional multi-GPU systems. While multi-GPU configurations typically rely on NVLink or PCIe connections within a single machine, TPU Pods employ high-bandwidth optical links to interconnect accelerators at data center scale. This design implements a 2D torus interconnect topology, enabling efficient data exchange between accelerators while minimizing communication bottlenecks as workloads scale across nodes.</p>
<p>The effectiveness of this architecture is demonstrated in its performance scaling capabilities. TPU Pod performance exhibits near-linear scaling when running ResNet-50, from quarter-pod to full-pod configurations. The system achieves a remarkable 33.0x speedup when scaled to 1024 chips compared to a 16-TPU baseline.</p>
<p>However, distributing AI workloads across an entire data center introduces distributed coordination challenges that fundamentally differ from single-node systems. The 2D torus interconnect, while providing high bisection bandwidth, creates communication bottlenecks when training large transformer models that require AllReduce operations across all 1,024 TPUs. Each parameter gradient must traverse multiple hops through the torus network, with worst-case communication requiring 32 hops between distant TPUs, creating latency penalties that compound with model size.</p>
<p>The energy cost of coordination also scales dramatically: moving data across the podâ€™s optical interconnect consumes 1000x more energy than on-chip communication within individual TPUs, transforming distributed training into a careful balance between computation parallelism and communication efficiency where AllReduce bandwidth, not compute capacity, determines overall training throughput.</p>
</section>
<section id="sec-distributed-training-wafer-scale" class="level4">
<h4 class="anchored" data-anchor-id="sec-distributed-training-wafer-scale">Wafer-Scale AI</h4>
<p>At the frontier of AI scaling, wafer-scale integration represents a paradigm shift that abandons traditional multi-chip architectures in favor of a single, massive AI processor. Rather than partitioning computation across discrete chips, this approach treats an entire silicon wafer as a unified compute fabric, eliminating the inefficiencies of inter-chip communication.</p>
<p>Wafer-scale integration uses an entire 300 mm silicon wafer as a single processor instead of cutting it into individual chips. Cerebras WSE-3 contains 4 trillion transistors across 850,000 cores, 125x more than the largest GPUs. Manufacturing challenges include 100% yield requirements (solved with redundant cores) and cooling 23 kW of power. This approach eliminates inter-chip communication delays but costs $2-3 million per wafer versus $40,000 for equivalent GPU clusters.</p>
<p>The primary advantage of wafer-scale AI is its ultra-fast on-die communication. Unlike chiplets, GPUs, or TPU Pods, where data must traverse physical boundaries between separate devices, wafer-scale AI enables near-instantaneous data transfer across its vast compute array. This architecture drastically reduces communication latency, achieving performance levels that remain out of reach for conventional multi-chip systems.</p>
<p>Achieving this level of integration introduces substantial engineering challenges. Thermal dissipation, fault tolerance, and manufacturing yield become major constraints when fabricating a processor of this scale. Unlike distributed TPU systems, which mitigate failures by dynamically re-routing workloads, wafer-scale AI must incorporate built-in redundancy mechanisms to tolerate localized defects in the silicon.</p>
</section>
</section>
<section id="sec-distributed-training-scaling-tradeoffs" class="level3">
<h3 class="anchored" data-anchor-id="sec-distributed-training-scaling-tradeoffs">Hardware Scaling Trade-offs</h3>
<p>The progressive scaling of AI acceleration introduces new challenges at each step, from single-chip processors to increasingly complex architectures. <a href="#tbl-distributed-scaling-trajectory" class="quarto-xref">Table&nbsp;2</a> summarizes these trade-offs across different scaling approaches.</p>
<div id="tbl-distributed-scaling-trajectory" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-tbl figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-distributed-scaling-trajectory-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;2: <strong>AI Acceleration Scaling Trade-offs</strong>: Each approach introduces unique trade-offs between modularity, latency, and complexity, demanding careful consideration of interconnect efficiency and workload distribution.
</figcaption>
<div aria-describedby="tbl-distributed-scaling-trajectory-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<table class="caption-top table">
<colgroup>
<col style="width: 19%">
<col style="width: 32%">
<col style="width: 46%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;"><strong>Scaling Approach</strong></th>
<th style="text-align: left;"><strong>Key Feature</strong></th>
<th style="text-align: left;"><strong>Challenges</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;"><strong>Chiplets</strong></td>
<td style="text-align: left;">Modular scaling within a package</td>
<td style="text-align: left;">Inter-chiplet latency, memory coherence</td>
</tr>
<tr class="even">
<td style="text-align: left;"><strong>Multi-GPU</strong></td>
<td style="text-align: left;">External GPU interconnects (NVLink)</td>
<td style="text-align: left;">Synchronization overhead, communication bottlenecks</td>
</tr>
<tr class="odd">
<td style="text-align: left;"><strong>TPU Pods</strong></td>
<td style="text-align: left;">Distributed accelerator clusters</td>
<td style="text-align: left;">Interconnect congestion, workload partitioning</td>
</tr>
<tr class="even">
<td style="text-align: left;"><strong>Wafer-Scale AI</strong></td>
<td style="text-align: left;">Entire wafer as a single processor</td>
<td style="text-align: left;">Thermal dissipation, fault tolerance</td>
</tr>
</tbody>
</table>
</div>
</figure>
</div>
<p>While chiplets enable modular scaling within a package, they introduce latency and memory coherence issues. Multi-GPU systems rely on high-speed interconnects like NVLink but face synchronization and communication bottlenecks. TPU Pods push scalability further by distributing workloads across clusters, yet they must contend with interconnect congestion and workload partitioning. At the extreme end, wafer-scale AI integrates an entire wafer into a single computational unit, presenting unique challenges in thermal management and fault tolerance.</p>
</section>
<section id="sec-distributed-training-execution-strategies" class="level3">
<h3 class="anchored" data-anchor-id="sec-distributed-training-execution-strategies">Multi-Chip Execution Strategies</h3>
<p>As AI systems scale from single-chip accelerators to multi-chip architectures, the fundamental challenges in computation and memory evolve. Computation must now be distributed across multiple accelerators, and memory access patterns become constrained by interconnect bandwidth and communication overhead.</p>
<p>Execution mapping in multi-chip systems requires computation placement that considers workload partitioning across multiple accelerators, with explicit coordination of execution order and dependencies. Computation scheduling must be interconnect-aware to manage communication delays effectively. Load balancing across accelerators is vital, as uneven task distribution results in some accelerators remaining underutilized while others operate at full capacity.</p>
<p>Distributed memory allocation requires each accelerator to manage its own local memory, necessitating explicit allocation of model parameters, activations, and intermediate data across devices. Unlike single-chip execution where data is fetched once and reused, multi-chip setups require deliberate strategies to minimize redundant data transfers.</p>
<p>Data movement optimization addresses inter-chip data transfer, which becomes the primary bottleneck rather than memory hierarchy latency. Techniques include overlapping computation and communication so accelerators process data while simultaneously sending and receiving, and locality-aware scheduling that places computations on accelerators already holding required data.</p>
<p>Compiler and runtime adaptations extend single-chip execution models to handle dynamic workload distribution across accelerators. Interconnect-aware workload partitioning enables compilers to distribute computations strategically based on communication cost. In TPU Pods, the runtime schedules computations across multiple TPU cores to minimize communication bottlenecks. In multi-GPU frameworks like PyTorch and TensorFlow, runtime execution synchronizes operations across GPUs while maintaining execution order.</p>
</section>
</section>
<section id="sec-distributed-training-summary" class="level2">
<h2 class="anchored" data-anchor-id="sec-distributed-training-summary">Summary</h2>
<p>Distributed training transforms the computational challenge of training large-scale machine learning models into an orchestration problem across multiple devices and hardware architectures. Throughout this chapter, we have examined how parallelism strategiesâ€”data, model, pipeline, and hybridâ€”address different constraints in the training process. Data parallelism scales dataset processing by distributing training examples across devices while synchronizing gradients, achieving near-linear speedups when communication overhead remains manageable. Model parallelism addresses memory constraints by partitioning models across devices, enabling training of architectures that exceed single-device capacity. Pipeline parallelism improves hardware utilization through microbatching, while hybrid approaches combine these strategies for the largest-scale training workloads.</p>
<p>The hardware infrastructure enabling distributed training spans multiple scales: chiplet-based architectures within a single package, multi-GPU systems connected via NVLink and NVSwitch, TPU Pods with 2D torus interconnects, and wafer-scale integration. Each approach presents distinct trade-offs between communication bandwidth, memory coherence, and system complexity. Amdahlâ€™s Law quantifies the fundamental scaling limits: communication overhead during gradient synchronization creates sequential bottlenecks that constrain speedup regardless of available compute power, explaining why adding GPUs beyond approximately 100 provides diminishing returns without algorithmic innovations.</p>
<p>The efficiency metrics governing distributed trainingâ€”communication overhead, scaling efficiency, and synchronization costsâ€”directly influence system design decisions. Understanding these trade-offs enables architects to select appropriate strategies for their specific workloads, balancing the complexity of distributed coordination against the computational benefits of parallelization. Framework abstractions like PyTorchâ€™s DistributedDataParallel translate these concepts into practical implementations, allowing practitioners to leverage sophisticated distributed systems optimizations while focusing on model development.</p>
<div class="callout callout-style-default callout-important callout-titled" title="Key Takeaways">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Important</span>Key Takeaways
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li>Data parallelism achieves near-linear scaling when communication overhead remains below 30-40% of training time</li>
<li>Model parallelism enables training of models exceeding single-device memory but introduces sequential dependencies</li>
<li>Pipeline parallelism reduces device idle time through microbatching, improving hardware utilization</li>
<li>Hybrid parallelism combines strategies for training the largest models on the largest datasets</li>
<li>Multi-chip hardware (chiplets, multi-GPU, TPU Pods, wafer-scale) each present distinct trade-offs between integration density, interconnect bandwidth, and system complexity</li>
<li>Amdahlâ€™s Law quantifies scaling limits: communication overhead constrains speedup regardless of available compute power</li>
<li>Framework APIs abstract distributed complexity while preserving the performance characteristics essential for production training</li>
</ul>
</div>
</div>
</section>
<section id="sec-distributed-training-fallacies-pitfalls" class="level2">
<h2 class="anchored" data-anchor-id="sec-distributed-training-fallacies-pitfalls">Fallacies and Pitfalls</h2>
<p>Distributed training involves counterintuitive behavior that leads to common misconceptions. These fallacies and pitfalls capture errors that waste compute resources and delay research progress.</p>
<p>Linear speedup remains theoretically impossible regardless of engineering effort. Amdahlâ€™s Law establishes hard limits: any sequential component bounds maximum speedup regardless of parallelism. In distributed training, gradient synchronization is inherently sequential since all gradients must be collected before any update proceeds.</p>
<p>Even with perfect overlap and optimal algorithms, communication overhead grows with cluster size. For data parallelism, AllReduce time increases logarithmically with tree algorithms or linearly in the latency term with ring algorithms as GPU count grows. A 1000-GPU cluster will never train 1000x faster than a single GPU; achieving 500x speedup would be exceptional, and 100-200x is more typical for communication-heavy workloads.</p>
<p>Organizations that budget projects assuming linear scaling inevitably miss deadlines and overspend on compute.</p>
<p>Hyperparameters tuned on small clusters fail catastrophically at large scale. The most critical is learning rate: as batch size increases with data parallelism, learning rate typically must increase proportionally to maintain convergence rate. The â€œlinear scaling ruleâ€ suggests <span class="math inline">\(\eta_{large} = \eta_{base} \times (B_{large}/B_{base})\)</span>.</p>
<p>However, this rule has limits. Beyond the â€œcritical batch sizeâ€ (model and dataset dependent, often 8K-32K for vision models), increasing batch size provides diminishing returns. Larger batches find sharper minima that generalize poorly. Training that converges beautifully at 256 GPUs may diverge or produce worse models at 1024 GPUs with naive scaling.</p>
<p>Warmup schedules, weight decay, and dropout rates also require adjustment. The only reliable approach is systematic scaling studies that validate hyperparameters at target scale.</p>
<p>Data parallelism does not scale indefinitely by adding more GPUs. Data parallelism increases effective batch size proportionally with GPU count, but statistical efficiency (loss reduction per sample) decreases with batch size beyond model-specific thresholds. A 100K-sample batch may provide only 2x the gradient quality of a 10K-sample batch, not 10x.</p>
<p>The critical batch size defines where marginal returns collapse. Beyond this point, additional GPUs increase throughput (samples per second) but not training efficiency (loss reduction per compute dollar). For BERT, critical batch size is approximately 8K; for ResNet, approximately 32K. Scaling beyond critical batch size wastes compute.</p>
<p>Large organizations have trained models to convergence using 1024 GPUs in the same wall-clock time as 512 GPUs at 2x the cost, because they exceeded critical batch size.</p>
<p>Pipeline parallelism and tensor parallelism both distribute model weights across devices, but their memory and compute characteristics differ dramatically.</p>
<p>Tensor parallelism splits each layer across devices, requiring AllReduce communication within each layer. This reduces memory proportionally but introduces communication overhead on the critical path. Pipeline parallelism assigns complete layers to devices, requiring only point-to-point communication between stages but introducing bubble overhead.</p>
<p>For memory-constrained scenarios where a model barely fits with splitting, tensor parallelismâ€™s even distribution helps. For throughput-maximizing scenarios with adequate memory, pipeline parallelismâ€™s lower communication overhead helps. Choosing based on one dimension (memory or compute) without considering the other leads to suboptimal configurations.</p>
<p>FSDP and ZeRO do not always improve training efficiency. FSDP (Fully Sharded Data Parallel) and ZeRO reduce memory footprint by sharding optimizer state and gradients across GPUs. This enables larger batch sizes or larger models per GPU. However, sharding introduces communication overhead: AllGather before forward pass, ReduceScatter after backward pass.</p>
<p>For models that fit comfortably in GPU memory without sharding, FSDP adds overhead without benefit. A 7B model training on A100-80GB with batch size 4 runs faster with DDP than FSDP because the model fits entirely with room for activations.</p>
<p>FSDP provides value when:</p>
<ul>
<li>Model + optimizer state exceeds single-GPU memory</li>
<li>Enabling larger batch sizes justifies communication overhead</li>
<li>ZeRO-Offload to CPU extends effective memory</li>
</ul>
<p>Applying FSDP universally, as some tutorials suggest, degrades performance for models that do not require it.</p>
<p>Parallelism overhead is roughly constant regardless of model size: AllReduce time depends on gradient size, not model computation time. For small models where forward/backward pass takes 10ms and AllReduce takes 5ms, communication overhead is 50%. For large models where forward/backward takes 1000ms and AllReduce takes 5ms, overhead is 0.5%.</p>
<p>Decisions made based on small-model benchmarks (â€œpipeline parallelism is always slowerâ€) invert at scale. The 20% overhead acceptable for a 1B model becomes 0.2% for a 100B model. Parallelism strategy evaluation must occur at target scale, or at minimum with analytical models that extrapolate appropriately.</p>
<p>Gradient accumulation is not free. Gradient accumulation simulates larger batch sizes by accumulating gradients across multiple forward/backward passes before synchronizing. This reduces communication frequency proportionally. However, accumulation has costs:</p>
<ol type="1">
<li><strong>Memory</strong>: Accumulated gradients consume memory throughout the accumulation window</li>
<li><strong>Latency</strong>: Effective step time increases proportionally with accumulation steps</li>
<li><strong>Precision</strong>: Accumulated FP16 gradients may overflow or underflow</li>
</ol>
<p>For loss-sensitive early training, gradient accumulation can introduce instability from accumulated numerical errors. Organizations that use gradient accumulation to work around infrastructure limitations (slow network, small GPUs) sometimes discover training divergence that disappears with proper infrastructure.</p>
<p>The principles established in this chapter provide the foundation for understanding fault tolerance mechanisms, which become increasingly critical as distributed training scales to thousands of devices where failures become statistically inevitable.</p>



</section>
</section>


<a onclick="window.scrollTo(0, 0); return false;" role="button" id="quarto-back-to-top"><i class="bi bi-arrow-up"></i> Back to top</a></main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    // Ensure there is a toggle, if there isn't float one in the top right
    if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
      const a = window.document.createElement('a');
      a.classList.add('top-right');
      a.classList.add('quarto-color-scheme-toggle');
      a.href = "";
      a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
      const i = window.document.createElement("i");
      i.classList.add('bi');
      a.appendChild(i);
      window.document.body.appendChild(a);
    }
    setColorSchemeToggle(hasAlternateSentinel())
    const icon = "î§‹";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
      const outerScaffold = trigger.parentElement.cloneNode(true);
      const codeEl = outerScaffold.querySelector('code');
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp("https:\/\/mlsysbook\.ai\/book\/");
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
            // target, if specified
            link.setAttribute("target", "_blank");
            if (link.getAttribute("rel") === null) {
              link.setAttribute("rel", "noopener");
            }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
  });
  </script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="../../../contents/vol2/storage/storage.html" class="pagination-link" aria-label="Storage Systems for ML">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text">Storage Systems for ML</span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="../../../contents/vol2/communication/communication.html" class="pagination-link" aria-label="Communication and Collective Operations">
        <span class="nav-page-text">Communication and Collective Operations</span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
<p>Â© 2024-2025 Harvard University. Licensed under <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC-BY-NC-SA 4.0</a></p>
</div>   
    <div class="nav-footer-center">
<p>Written, edited and curated by Prof.&nbsp;Vijay Janapa Reddi (Harvard University)</p>
</div>
    <div class="nav-footer-right">
      <ul class="footer-items list-unstyled">
    <li class="nav-item compact">
    <a class="nav-link active" href="https://github.com/harvard-edge/cs249r_book" aria-current="page">
      <i class="bi bi-github" role="img" aria-label="View source on GitHub">
</i> 
    </a>
  </li>  
    <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/harvard-edge/cs249r_book">
      <i class="bi bi-star" role="img" aria-label="Star this repository">
</i> 
    </a>
  </li>  
    <li class="nav-item">
    <a class="nav-link" href="https://www.netlify.com">
<p><img src="https://www.netlify.com/v3/img/components/netlify-color-accent.svg" alt="Deploys by Netlify" style="height: 15px; vertical-align: middle; margin-left: 3px;"></p>
</a>
  </li>  
</ul>
    </div>
  </div>
</footer>
<script>var lightboxQuarto = GLightbox({"closeEffect":"zoom","descPosition":"bottom","loop":false,"openEffect":"zoom","selector":".lightbox"});
(function() {
  let previousOnload = window.onload;
  window.onload = () => {
    if (previousOnload) {
      previousOnload();
    }
    lightboxQuarto.on('slide_before_load', (data) => {
      const { slideIndex, slideNode, slideConfig, player, trigger } = data;
      const href = trigger.getAttribute('href');
      if (href !== null) {
        const imgEl = window.document.querySelector(`a[href="${href}"] img`);
        if (imgEl !== null) {
          const srcAttr = imgEl.getAttribute("src");
          if (srcAttr && srcAttr.startsWith("data:")) {
            slideConfig.href = srcAttr;
          }
        }
      } 
    });
  
    lightboxQuarto.on('slide_after_load', (data) => {
      const { slideIndex, slideNode, slideConfig, player, trigger } = data;
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(slideNode);
      }
    });
  
  };
  
})();
          </script>




</li></ul></li></ul></li></ul></div></nav></div></body></html>