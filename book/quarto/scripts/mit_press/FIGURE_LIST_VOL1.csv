Chapter,Figure Number,Label,Caption,Alt-Text
"Introduction","1.1","fig-ai-timeline","**AI Development Timeline.** A chronological curve traces AI research activity from the 1950s to the 2020s, with gray bands marking the two AI Winter periods (1974 to 1980, 1987 to 1993). Callout boxes highlight key milestones including the Turing Test [@turing1950computing], the Dartmouth conference [@mccarthy1955dartmouth], the Perceptron, ELIZA, Deep Blue, and GPT-3.","Timeline from 1950 to 2020 with red line showing AI publication frequency. Gray bands mark two AI Winters (1974-1980, 1987-1993). Callout boxes mark milestones: Turing 1950, Dartmouth 1956, Perceptron 1957, ELIZA 1966, Deep Blue 1997, GPT-3 2020."
"Introduction","1.2","fig-alexnet","**AlexNet Architecture.** The network that launched the deep learning revolution at ImageNet 2012. Two parallel GPU streams process 224x224 input images through convolutional layers (green blocks) that extract spatial features at decreasing resolutions, converging through three fully connected layers to 1,000 output classes. With 60 million parameters trained across two GTX 580 GPUs, AlexNet achieved 15.3% top-5 error, a 42% relative improvement over the second-place entry.","3D diagram of AlexNet with two parallel GPU streams. Green blocks show convolutional layers decreasing from 224x224 input. Red kernels overlay green blocks. Right side shows three dense layers converging to 1000 outputs."
"Introduction","1.3","fig-ai-triad","**The AI Triad**: Interdependent relationship between Data, Algorithm, and Machine. Each node (dataset, model, and infrastructure) constrains the capabilities of the others. ML systems engineering is the discipline of balancing this triad; optimizing one component in isolation often shifts the system bottleneck to another vertex rather than eliminating it.","Triangle diagram with three circles at vertices labeled Model, Data, and Machine. Double-headed purple arrows connect all three nodes, showing bidirectional dependencies. Icons inside circles depict neural network, database cylinders, and cloud."
"Introduction","1.4","fig-evolution-efficiency","**Historical Efficiency Trends.** A three-track timeline from 1980 to 2023 shows parallel progress in Algorithmic Efficiency (blue), Compute Efficiency (yellow), and Data Selection (green). Each track progresses through distinct eras: algorithms advance from early methods through deep learning to modern efficiency techniques; compute evolves from general-purpose CPUs through accelerated hardware to sustainable computing; data practices shift from scarcity through big data to data-centric AI.","Timeline with three horizontal tracks from 1980 to 2023. Blue track shows Algorithmic Efficiency progressing through Deep Learning Era to Modern Efficiency. Yellow shows Compute Efficiency from General-Purpose through Accelerated to Sustainable Computing. Green shows Data Selection from Scarcity through Big Data to Data-Centric AI."
"Introduction","1.5","fig-algo-efficiency","Algorithmic Efficiency Trajectory. Training efficiency factor relative to AlexNet (2012 baseline) for ImageNet classification. Each point represents a model architecture that achieves comparable accuracy with fewer computational resources. The trajectory from AlexNet (1x) through VGG, ResNet, MobileNet, and ShuffleNet to EfficientNet (44x) demonstrates that algorithmic innovation has delivered a 44-fold reduction in required compute over eight years, independent of hardware improvements.","Scatter plot showing training efficiency factor from 2012 to 2020. Red dots mark models from AlexNet at 1x to EfficientNet at 44x. Dashed trend line curves upward. Labels identify VGG, ResNet, MobileNet, ShuffleNet versions at their positions."
"Introduction","1.6","fig-ai-training-compute-growth","The Era of Scale. Training Compute (FLOPs) vs. Year (Log Scale). While early Deep Learning (blue) showed rapid growth, the Transformer Era (red) accelerated this trend significantly. From AlexNet (2012) to GPT-4 (2023), compute requirements increased by $10^8$ (100 million times), far outpacing Moore's Law. This exponential demand drives the specialized infrastructure described in this book.","Scatter plot of Training Compute FLOPs vs Year. Blue dots (2012-2018) show deep learning models like ResNet. Red dots (2018-2024) show large scale models like GPT-4, rising much faster on the log scale."
"Introduction","1.7","fig-ml_lifecycle_overview","**ML System Lifecycle.** A six-box flowchart depicting Data Collection, Preparation, Model Training, Evaluation, Deployment, and Monitoring. Two feedback loops distinguish this cycle from linear software development: evaluation returns to preparation when results is insufficient, and monitoring triggers new data collection when performance degrades.","Flowchart showing cyclical ML lifecycle. Six boxes: Data Collection, Preparation, Model Training, Evaluation, Deployment, Monitoring. Two loops: evaluation returns to preparation; monitoring triggers collection."
"Introduction","1.8","fig-pillars","Five-Pillar Framework.\index{Five-Pillar Framework!disciplines} Five labeled columns represent Data Engineering, Training Systems, Deployment Infrastructure, Operations and Monitoring, and Ethics and Governance. The pillars rest on a shared foundation labeled Performance Optimization and Hardware Acceleration, indicating the technical imperatives that support all five disciplines.","Five pillars diagram: Data Engineering, Training Systems, Deployment Infrastructure, Operations and Monitoring, Ethics and Governance. Pillars rest on foundation labeled Performance Optimization and Hardware Acceleration."
"Ml Systems","2.1","fig-cloud-edge-TinyML-comparison","**Distributed Intelligence Spectrum**: Machine learning deployment spans from centralized cloud infrastructure to resource-constrained TinyML devices, each balancing processing location, device capability, and network dependence. Source: [@abiresearch2024tinyml].","Horizontal spectrum showing 5 deployment tiers from left to right: ultra-low-power devices and sensors, intelligent device, gateway, on-premise servers, and cloud. Arrows indicate TinyML, Edge AI, and Cloud AI spans across the spectrum."
"Ml Systems","2.2","fig-cloud-ml","**Cloud ML Decomposition.** Characteristics, benefits, challenges, and representative applications of cloud machine learning, where centralized infrastructure and specialized hardware address scale, complexity, and resource management for large datasets and complex computations.","Tree diagram with Cloud ML branching to four categories: Characteristics, Benefits, Challenges, and Examples. Each lists items like computational power, scalability, vendor lock-in, and virtual assistants."
"Ml Systems","2.3","fig-cloudml-example","Cloud Data Center Scale: Rows of server racks illuminated by blue LEDs extend across a Google Cloud TPU data center floor, housing thousands of specialized AI accelerator chips that collectively deliver petaflop-scale training throughput. Source: [@google2024gemini].","Aerial view of Google Cloud TPU data center with long rows of server racks illuminated by blue LEDs extending toward the horizon across a large facility floor."
"Ml Systems","2.4","fig-edge-ml","**Edge ML Decomposition.** Characteristics, benefits, challenges, and representative applications of edge machine learning, where decentralized processing on nearby hardware reduces latency and network dependence at the cost of constrained compute and memory.","Tree diagram with Edge ML branching to four categories: Characteristics, Benefits, Challenges, and Examples, listing items like decentralized processing, reduced latency, security concerns, and industrial IoT."
"Ml Systems","2.5","fig-edgeml-example","Edge Device Deployment: Diverse IoT devices, from wearables to home appliances, enable decentralized machine learning by performing inference locally, reducing reliance on cloud connectivity and improving response times. Source: Edge Impulse.","Collection of IoT devices arranged on a surface: smart home sensors, fitness wearables, environmental monitors, and connected appliances in various sizes and form factors."
"Ml Systems","2.6","fig-mobile-ml","**Mobile ML Decomposition.** Characteristics, benefits, challenges, and representative applications of mobile machine learning, where on-device processing and hardware acceleration balance computational efficiency, battery life, and model performance on smartphones and tablets.","Tree diagram with Mobile ML branching to four categories: Characteristics, Benefits, Challenges, and Examples. Each lists items like on-device processing, real-time response, battery constraints, and voice recognition."
"Ml Systems","2.7","fig-TinyML-example","TinyML System Scale: Small development boards, including Arduino Nano BLE Sense and similar microcontroller kits approximately 2 to 5 cm in length, with visible processor chips and pin connectors that enable sensor integration for always-on ML inference at milliwatt power budgets. Source: [@warden2018speech]","Small development boards including Arduino Nano BLE Sense and similar microcontroller kits arranged on a surface, each approximately 2-5 cm in length with visible chips and connectors."
"Ml Systems","2.8","fig-tiny-ml","**TinyML Decomposition.** Characteristics, benefits, challenges, and representative applications of TinyML, where milliwatt power budgets and kilobyte memory limits enable always-on sensing and localized intelligence in embedded applications.","Tree diagram with TinyML branching to four categories: Characteristics, Benefits, Challenges, and Examples, listing items like low-power operation, always-on capability, resource limitations, and predictive maintenance."
"Ml Systems","2.9","fig-op_char","**Paradigm Comparison Radar Plots.** Two radar plots quantify performance and operational characteristics across cloud, edge, mobile, and TinyML paradigms. The left plot contrasts compute power, latency, scalability, and energy efficiency; the right plot contrasts connectivity, privacy, real-time capability, and offline operation.","Two radar plots with four overlapping polygons each. Left plot axes: compute power, latency, scalability, energy. Right plot axes: connectivity, privacy, real-time, offline capability."
"Ml Systems","2.10","fig-mlsys-playbook-flowchart","**Deployment Decision Logic**: This flowchart guides selection of an appropriate machine learning deployment paradigm by systematically evaluating privacy requirements and processing constraints, ultimately balancing performance, cost, and data security. Navigating the decision tree helps practitioners determine whether cloud, edge, mobile, or tiny machine learning best suits a given application.","Decision flowchart with four layers: Privacy, Performance, Compute Needs, and Cost. Each layer filters toward deployment options: Cloud ML, Edge ML, Mobile ML, or TinyML based on constraints."
"Ml Systems","2.11","fig-hybrid","**Hybrid System Interactions**: Data flows upward from sensors through processing layers to cloud analytics, while trained models deploy downward to edge, mobile, and TinyML inference points. Five connection types (deploy, data, results, assist, and sync) establish a distributed architecture where each paradigm contributes unique capabilities.","System diagram with four ML paradigms: TinyML sensors, Edge inference, Mobile processing, and Cloud training. Arrows show deploy, data, results, sync, and assist flows between tiers."
"Ml Systems","2.12","fig-ml-systems-convergence","**Convergence of ML Systems**: Three-layer structure showing how diverse deployments converge. The top layer lists four paradigms (Cloud, Edge, Mobile, TinyML); the middle layer identifies shared foundations (data pipelines, resource management, architecture principles); and the bottom layer presents cross-cutting concerns (optimization, operations, trustworthy AI) that apply across all paradigms.","Three-layer diagram. Top: Cloud, Edge, Mobile, TinyML implementations. Middle: data pipeline, resource management, architecture principles. Bottom: optimization, operations, trustworthy AI. Arrows connect layers."
"Workflow","3.1","fig-ml-lifecycle","**Dual-Pipeline ML Development**: The data pipeline (green, top) progresses from collection through ingestion, analysis, labeling, validation, and preparation. The model pipeline (blue, bottom) takes prepared datasets through training, evaluation, validation, and deployment. Feedback arrows show how monitoring insights inform data refinements, evaluation results trigger model improvements, and deployment experiences reshape collection strategies.","Two parallel pipelines: data pipeline (green, top) with 6 stages from collection to preparation; model pipeline (blue, bottom) with 4 stages. Curved feedback arrows connect deployment back to collection and training stages."
"Workflow","3.2","fig-ds-time","**Data Scientist Time Allocation**: Data preparation consumes up to 60% of data science effort, with data collection accounting for an additional 19%. Model-focused activities such as pattern mining, training set construction, and algorithm refinement together represent roughly 18% of total time. Source: CrowdFlower 2016 Data Science Report.","Pie chart showing data scientist time allocation: 60% cleaning and organizing data, 19% collecting datasets, 9% mining for patterns, 5% building training sets, 4% refining algorithms, 3% other tasks."
"Workflow","3.3","fig-lifecycle-overview","**Simplified Lifecycle with Feedback**: Six stages progress from problem definition through data collection, model development, evaluation, deployment, and monitoring. The feedback loop from monitoring back to data collection captures the essential insight that production insights drive continuous refinement across earlier stages, because data distributions shift, model performance drifts, and operational requirements evolve.","Linear flowchart with 6 boxes: Problem Definition, Data Collection, Model Development, Evaluation, Deployment, Monitoring. Feedback loop arrow curves from Monitoring back to Data Collection."
"Workflow","3.4","fig-eye-dr","Retinal Hemorrhages: Diabetic retinopathy causes visible hemorrhages in retinal images. While this appears to be straightforward image classification, the path from laboratory success to clinical deployment illustrates every aspect of AI lifecycle complexity. Source: Google.","Two side-by-side retinal fundus images: left shows healthy retina, right shows diabetic retinopathy with dark red hemorrhage spots scattered across the retina."
"Workflow","3.5","fig-ml-lifecycle-feedback","**Feedback Paths Across Lifecycle Stages**: Six labeled feedback arrows connect the lifecycle stages. Data gaps identified during evaluation flow back to collection. Validation issues inform training adjustments. Performance insights from monitoring trigger pipeline refinements. Model updates propagate from monitoring to training. Data quality issues feed back to preparation. Deployment constraints propagate backward to influence model design.","Diagram with 6 boxes: Data Collection, Preparation, Training, Evaluation, Deployment, Monitoring. Labeled feedback arrows show data gaps, validation issues, performance insights, and deployment constraints flowing between stages."
"Workflow","3.6","fig-mlops-leverage","The MLOps Leverage: Why infrastructure investment yields exponential returns. Manual workflows (red) scale linearly with team size but eventually saturate due to the Coordination Tax—the overhead of managing conflicting experiments and untracked artifacts. In contrast, an automated MLOps platform (blue) enables the Flywheel Effect, where shared components (feature stores, pipelines) allow experimentation velocity to scale super-linearly with team size.","Line chart comparing experimentation velocity versus team size. Red line for manual workflows shows linear then saturating growth. Blue line for MLOps platform shows super-linear growth demonstrating leverage effect."
"Data Engineering","4.1","fig-data-quality-multiplier","The Data Quality Multiplier: Model Accuracy vs. Dataset Size (Log Scale) for Clean vs. Noisy Data. High-quality data (Green) follows a steeper power law, reaching higher accuracy with fewer samples. Low-quality data (Red) hits a 'Statistical Ceiling' earlier, where adding more data yields diminishing returns due to irreducible label noise. This gap illustrates why data cleaning is often a higher-leverage optimization than model scaling.","Line plot showing accuracy vs. dataset size. Green line (Clean Data) rises steeply. Red line (Noisy Data) flattens out. Shaded area represents the 'Quality Gap'."
"Data Engineering","4.2","fig-cascades","**Data Quality Cascades**: Errors introduced early in the machine learning workflow amplify across subsequent stages, increasing costs and potentially leading to flawed predictions or harmful outcomes. Source: [@sambasivan2021everyone].","Timeline with 7 stages from problem statement to deployment. Colored arcs show errors from data collection propagating to evaluation and deployment stages."
"Data Engineering","4.3","fig-four-pillars","**The Four Pillars of Data Engineering**: Quality, Reliability, Scalability, and Governance form the foundational framework for ML data systems. Each pillar contributes essential capabilities (solid arrows), while trade-offs between pillars (dashed lines) require careful balancing: validation overhead affects throughput, consistency constraints limit distributed scale, privacy requirements impact performance, and bias mitigation may reduce available training data.","Four boxes labeled Quality, Reliability, Scalability, and Governance surround a central ML Data System circle. Solid arrows connect each box to center showing contributions; dashed lines between boxes indicate trade-offs."
"Data Engineering","4.4","fig-keywords","Keyword Spotting System: A voice-activated device uses a lightweight, always-on wake word detector that listens continuously and triggers the main voice assistant upon keyword detection.","Diagram showing voice-activated device with microphone, always-on wake word detector, and connection to main voice assistant that activates upon keyword detection."
"Data Engineering","4.5","fig-misalignment","**Shared Dataset Bias Propagation**: Five models (A through E) all train on a single central dataset repository. Arrows show how shared limitations, biases, and blind spots propagate from the common dataset to every downstream model, leading to correlated failures across the ecosystem.","Five model boxes labeled A through E at center all connect upward to one central training dataset repository. Arrows downward show shared limitations, biases, and blind spots propagating to all models."
"Data Engineering","4.6","fig-traffic-light","Data Source Noise: A black-and-white photograph from 1914 showing early manual semaphore traffic signals, illustrating how historical images can appear in modern web scraping results for contemporary queries. Such anachronistic content requires systematic validation and filtering to prevent spurious correlations in training data. Source: Vox.","Historical black-and-white photograph from 1914 showing early traffic control with manual semaphore signals, illustrating how outdated images can appear in modern web scraping results."
"Data Engineering","4.7","fig-synthetic-data","**Synthetic Data Augmentation**: A four-node pipeline where historical data and simulation outputs feed into a synthetic data generation process, producing an expanded combined training dataset with greater size and diversity than either source alone. Source: AnyLogic [@anylogic_synthetic].","Diagram showing historical data icon and simulation cloud icon both feeding into synthetic data generation process, producing an expanded combined training dataset."
"Data Engineering","4.8","fig-pipeline-flow","**Three-Stage Pipeline Flow**: Raw data sources and APIs feed into batch and stream ingestion at the middle layer, then flow to data warehouse and storage destinations at the bottom. Each stage scales independently, enabling modular quality control across the pipeline.","Three-tier flow diagram with raw data sources and APIs at top, batch and stream ingestion in middle layer, and data warehouse and storage destinations at bottom connected by arrows."
"Data Engineering","4.9","fig-dataloader-choke-point","The Dataloader Choke Point. Training Throughput (img/s) vs. Number of DataLoader Workers. The blue curve shows CPU throughput scaling linearly with workers until hitting disk limits. The red dashed line is the GPU's consumption capacity (e.g., ResNet-50 consuming 3,000 img/s). The system is bottlenecked by whichever is lower. In the 'Starvation Region' (left), the GPU is idle waiting for data. In the 'Saturated Region' (right), the GPU is fully utilized, and adding more workers wastes CPU memory.","Line chart of Throughput vs Workers. Blue line (CPU) rises linearly. Red line (GPU) is flat. Where CPU < GPU, system is starved. Where CPU > GPU, system is saturated."
"Data Engineering","4.10","fig-etl-vs-elt","**ETL vs. ELT Comparison**: Side-by-side view of two pipeline paradigms. ETL transforms data before loading into a data warehouse, while ELT loads raw data first and transforms within the warehouse. The choice depends on data volume, transformation complexity, and target storage capabilities.","Side-by-side comparison showing ETL pipeline with extract, transform, then load sequence versus ELT pipeline with extract, load, then transform sequence within the data warehouse."
"Data Engineering","4.11","fig-spectrogram-example","Audio Feature Transformation: Advanced audio features compress raw audio waveforms into representations that emphasize perceptually relevant characteristics for machine learning tasks. This transformation reduces noise and data dimensionality while preserving essential speech information, improving model performance in applications like keyword spotting.","Two-panel visualization showing raw audio waveform on left transforming into spectrogram on right, with time on horizontal axis and frequency on vertical axis indicated by color intensity."
"Data Engineering","4.12","fig-tfx-pipeline-example","**TFX End-to-End Pipeline**: A TensorFlow Extended pipeline traces the complete flow from data ingestion through validation, transformation, training, evaluation, and deployment. Each component is independently versioned, tested, and scaled.","Linear flow diagram showing TensorFlow Extended pipeline: data ingestion, validation, transformation, training, evaluation, and deployment stages connected by arrows from left to right."
"Data Engineering","4.13","fig-labels","Data Annotation Granularity: Three versions of the same street scene show increasing annotation detail: a simple classification label, bounding boxes around vehicles and pedestrians, and pixel-level semantic segmentation with distinct colors. Each level increases labeling cost and storage requirements while providing richer training signal.","Three versions of same street scene showing increasing annotation detail: simple classification label, bounding boxes around vehicles and pedestrians, and pixel-level semantic segmentation with distinct colors."
"Data Engineering","4.14","fig-hard-labels","Labeling Ambiguity: How subjective or difficult examples, such as blurry images or rare species, can introduce errors during data labeling, highlighting the need for careful quality control and potentially expert annotation. Source: [@northcutt2021pervasive].","Grid of example images showing labeling challenges: blurred animal photos where species is unclear, rare specimens requiring expert knowledge, and ambiguous object boundaries causing annotator disagreement."
"Data Engineering","4.15","fig-weak-supervision","**AI-Augmented Labeling Decision Hierarchy**: A top-level question about obtaining labeled data branches into four paths: traditional supervision, semi-supervised learning, weak supervision, and transfer learning, with active learning as a cost-saving alternative. Lower-cost strategies trade labeling precision for throughput. Source: Stanford AI Lab.","Hierarchical diagram with question about getting labeled data at top. Four branches: traditional supervision, semi-supervised, weak supervision, and transfer learning. Active learning branches as cost-saving alternative."
"Data Engineering","4.16","fig-mswc","Multilingual Data Preparation: Forced alignment and segmentation transform paired audio-text data into labeled one-second segments, creating a large-scale corpus for training keyword spotting models across 50+ languages. This automated process enables scalable development of KWS systems by efficiently generating training examples from readily available speech resources like common voice and multilingual captioned content.","Pipeline showing audio waveform and text transcript inputs processed through forced alignment stage, then segmented into individual one-second labeled keyword samples for KWS training."
"Data Engineering","4.17","fig-debug-flowchart","**Data Pipeline Debugging Flowchart**: Four sequential decision nodes guide root cause diagnosis: (1) accuracy degrades over time leads to Data Drift, (2) training accuracy exceeds validation leads to Overfitting, (3) validation exceeds production accuracy leads to Training-Serving Skew, and (4) subgroup inconsistency leads to Bias. If all answers are no, the issue points to Model Architecture.","Vertical flowchart with four blue diamond decision nodes and red result boxes. Top diamond asks if accuracy degrades over time, leading to Data Drift result. Second asks if training accuracy exceeds validation, leading to Overfitting. Third asks if validation exceeds production accuracy, leading to Training-Serving Skew. Fourth asks about subgroup inconsistency, leading to Bias. Gray box at bottom shows Model Architecture issue if all answers are no."
"Dl Primer","5.1","fig-ai-ml-dl","AI Hierarchy: Neural networks form a core component of deep learning within machine learning and artificial intelligence by modeling patterns in large datasets. Machine learning algorithms enable systems to learn from data as a subset of the broader AI field.","Nested circles diagram showing AI as outermost circle containing Machine Learning, which contains Deep Learning, which contains Neural Networks at the center. Arrows indicate progression from broad AI concepts to specific neural network implementations."
"Dl Primer","5.2","fig-breakout","**Breakout Collision Rules**: The game program uses explicit if-then rules for collision detection, specifying ball direction reversal and brick removal upon contact. While effective for a game with clear physics and limited states, this approach illustrates how rule-based systems must anticipate every possible scenario.","Breakout game grid with 3 rows of 5 colored bricks at top, brown paddle at bottom, and ball with trajectory arrow. Code snippet shows explicit if-then rules for collision detection: removeBrick, update ball velocity."
"Dl Primer","5.3","fig-traditional","**Traditional Programming Flow**: Rules and data serve as inputs to a traditional program, which produces answers as output. This input-output pattern formed the basis for early AI systems but lacks the adaptability needed for complex pattern recognition tasks.","Flow diagram with three boxes: Rules and Data as inputs flowing into central Traditional Programming box, which outputs Answers. Arrows show data flow direction from inputs to output."
"Dl Primer","5.4","fig-activity-rules","Activity Classification Decision Tree: A rule-based decision tree classifies human activity by branching on speed thresholds, with values below 4 mph mapped to walking, 4 to 15 mph to running, and above 15 mph to biking. Real-world edge cases and transitions between activities demand increasingly complex branching logic.","Decision tree flowchart for activity classification. Branches split on conditions like speed less than 4 mph leading to walking, 4-15 mph to running, greater than 15 mph to biking. Additional branches handle edge cases and transitions."
"Dl Primer","5.5","fig-hog","HOG Method: Identifies edges in images to create a histogram of gradients, transforming pixel values into shape descriptors that are invariant to lighting changes.","Three-panel image showing HOG feature extraction: original grayscale photo of person on left, gradient magnitude visualization in center, and HOG descriptor grid overlay on right showing edge orientation histograms per cell."
"Dl Primer","5.6","fig-deeplearning","**Data-Driven Rule Discovery**: The flow diagram inverts the traditional programming pattern: data and answers serve as inputs to the machine learning process, which produces learned rules as output. This inversion eliminates the need for manually specified rules and enables automated feature extraction from raw inputs.","Flow diagram with three boxes: Answers and Data as inputs flowing into central Machine Learning box, which outputs Rules. Arrows show inverted flow compared to traditional programming, with rules as output rather than input."
"Dl Primer","5.7","fig-double-descent","The Double Descent Phenomenon: Why modern deep learning defies classical statistics. In the Classical Regime (left), increasing model complexity eventually leads to overfitting (the \","Line chart showing test error versus model complexity. The curve forms a U-shape in the classical regime, rises at the interpolation threshold, then descends again in the modern regime, demonstrating double descent."
"Dl Primer","5.8","fig-bio_nn2ai_nn","Biological-to-Artificial Neuron Mapping: Side-by-side comparison showing how biological neuron structures map to artificial neuron components. Dendrites correspond to inputs, synapses to weights, the cell body to the summation function, and the axon to the activation output. This mapping established the ""Compute-Aggregate-Activate"" pattern central to neural network design.","Side-by-side comparison of biological neuron and artificial neuron. Left shows biological cell with dendrites, cell body, and axon. Right shows mathematical model with inputs x, weights w, summation node, activation function, and output. Arrows map corresponding components between the two."
"Dl Primer","5.9","fig-trends","Computational Growth: Log-scale scatter plot showing training compute in FLOPS from 1952 to 2022. Computational power grew at a 1.4x rate from 1952 to 2010, then accelerated to a doubling every 3.4 months from 2012 to 2022. Large-scale models after 2015 followed an even faster 10-month doubling cycle, addressing the historical bottleneck of training complex neural networks.","Log-scale scatter plot showing training compute in FLOPS from 1950 to 2022. Points represent AI models, with different colors for pre-deep-learning era, deep learning era, and large-scale models. Trend lines show 1.4x growth before 2010 and 3.4-month doubling after 2012."
"Dl Primer","5.10","fig-virtuous-cycle","**Deep Learning Virtuous Cycle**: Three mutually reinforcing factors, data availability, algorithmic innovations, and computing infrastructure, form a self-reinforcing loop where breakthroughs in one area create opportunities in the others.","Three connected boxes in a cycle: green Data Availability flows to blue Algorithmic Innovations, which flows to red Computing Infrastructure, which loops back to Data Availability. Yellow background box labeled Key Breakthroughs contains all three elements."
"Dl Primer","5.11","fig-perceptron","**Perceptron Architecture**: The fundamental computational unit of neural networks, showing inputs multiplied by weights, summed with bias, and passed through an activation function to produce output.","Perceptron diagram with inputs x1 through xi on left, each connected to weight circles w1j through wij. Weights feed into red summation node, which receives bias b from below. Output z flows to blue sigma activation function box, producing output y-hat on right."
"Dl Primer","5.12","fig-activation-functions","**Common Activation Functions**: Four nonlinear activation functions plotted with their output ranges. Sigmoid maps inputs to $(0,1)$ with smooth gradients, tanh provides zero-centered outputs in $(-1,1)$, ReLU introduces sparsity by outputting zero for negative inputs, and softmax converts logits into probability distributions.","Four plots arranged in 2x2 grid. Top-left: Sigmoid S-curve from 0 to 1. Top-right: Tanh S-curve from -1 to 1. Bottom-left: ReLU showing zero for negative x, linear for positive x. Bottom-right: Softmax showing exponential curve approaching small positive values."
"Dl Primer","5.13","fig-nonlinear","**Linear vs. Nonlinear Decision Boundaries**: Two scatter plots compare classification with and without activation functions. Without activation, a straight line fails to separate the two classes. With a nonlinear activation function applied, the network produces a curved decision boundary that correctly separates the points.","Two scatter plots side by side. Left plot shows cyan and green points with straight red line failing to separate them, labeled NN without Activation Function. Right plot shows same points with curved red decision boundary successfully separating classes, labeled NN with Activation Function."
"Dl Primer","5.14","fig-layers","**Layered Network Architecture**: Deep neural networks transform data through successive layers, enabling the extraction of increasingly complex features and patterns. Each layer applies non-linear transformations to the outputs of the previous layer, ultimately mapping raw inputs to desired outputs.","Neural network diagram showing input layer on left with multiple nodes, two hidden layers in middle with interconnected nodes, and output layer on right. Arrows show data flow from left to right through fully connected layers."
"Dl Primer","5.15","fig-connections","**Fully-Connected Layers**: A three-layer network with dense connections between layers, where each neuron integrates information from all neurons in the preceding layer. Weight matrices between layers determine connection strengths, with labeled values shown on each edge alongside computed activation values at each node.","Three-layer network with 3 green input nodes, 4 blue hidden nodes, and 2 red output nodes. Labeled arrows show weight values on each connection. Input layer shows values 1.0, 5.0, 9.0. Hidden nodes show activation values. Bias values labeled at each layer."
"Dl Primer","5.16","fig-mnist-topology-1","**MNIST Network Topology**: Two panels show the network architecture for digit recognition. Panel (a) displays a 28x28 pixel image of a digit connected through hidden layers to 10 output nodes. Panel (b) shows the same architecture with the input image flattened into a 784-element vector, illustrating how spatial data enters the network.","Two panels showing MNIST digit recognition. Panel a: 28x28 pixel image of digit 7 connected to hidden layer circles, then to 10 output nodes with one highlighted for digit classification. Panel b: Same architecture with flattened 784-pixel vector representation of input image."
"Dl Primer","5.17","fig-forward-propagation","**Training Loop Architecture**: Complete neural network training flow showing forward propagation through layers to generate prediction, comparison with true value via loss function, and backward propagation of gradients through optimizer to update weights and biases.","Neural network training diagram. Left side shows input X flowing through blue, red, and green node layers via forward propagation (red arrow). Right side shows prediction and true value boxes feeding into loss function, which outputs loss score to optimizer, which updates weights and biases. Orange arrow shows backward propagation path."
"Dl Primer","5.18","fig-training-vs-inference","**Inference vs. Training Flow**: During inference, neural networks utilize learned weights for forward pass computation only, simplifying the data flow and reducing computational cost compared to training, which requires both forward and backward passes for weight updates. This streamlined process enables efficient deployment of trained models for real-time predictions.","Two parallel diagrams comparing inference and training. Both show stacked rectangles representing batches feeding into network layers and output nodes. Inference section shows smaller varied batch sizes with dashed outlines. Training section shows larger fixed batches with solid outlines. Network architecture identical in both with fully connected layers."
"Dl Primer","5.19","fig-usps-digit-examples","Handwritten Digit Variability: Real-world handwritten digits exhibit significant variations in stroke width, slant, and character formation, posing challenges for automated recognition systems like those used by the USPS. These examples demonstrate the need for effective feature extraction and model generalization to achieve high accuracy in optical character recognition (OCR) tasks.","Grid of handwritten digit samples from USPS dataset showing digits 0-9 in multiple rows. Each digit appears in several variations demonstrating different handwriting styles, stroke widths, slants, and character formations that OCR systems must recognize."
"Dl Primer","5.20","fig-usps-inference-pipeline","**USPS Inference Pipeline**: The mail sorting pipeline combines traditional computing stages (green) with neural network inference (blue). Raw envelope images undergo preprocessing, including thresholding, segmentation, and normalization, before the neural network classifies individual digits. Post-processing applies confidence thresholds and formats sorting instructions for the physical sorting machinery.","Linear pipeline with 6 boxes connected by arrows. From left: Raw Input and Pre-processing in green Traditional Computing section, Neural Network in orange Deep Learning section, then Raw Output, Post-processing, and Final Output in green Traditional Computing section."
"Dnn Architectures","6.1","fig-efficiency-frontier","The Efficiency Frontier: ImageNet Top-1 Accuracy vs. Computational Cost (GFLOPs). The dashed 'Pareto Frontier' represents the optimal trade-off between representational power and computational efficiency. Notice the progression from dense CNNs (blue) to efficient Mobile architectures (green) that minimized compute, and finally to Transformers (red) that push the accuracy boundary at significantly higher computational costs.","Scatter plot of ImageNet Accuracy vs Compute Cost (log scale). Clusters show Dense CNNs (high compute), MobileNets (low compute, good accuracy), and Transformers (very high compute, highest accuracy). A dashed line connects the Pareto optimal models."
"Dnn Architectures","6.2","fig-mlp","**Multi-Layer Perceptron Architecture**: Three fully-connected layers where every neuron connects to all neurons in adjacent layers. The highlighted neuron receives weighted contributions from all inputs, illustrating the dense $O(N \times M)$ connectivity pattern implemented through matrix multiplications. For MNIST classification, a 784-dimensional input connects to 100 hidden neurons through a $784 \times 100$ weight matrix, requiring 78,400 multiply-accumulate operations per sample. Adapted from [@reagen2017deep].","Three-layer neural network with 4 input nodes, 5 hidden nodes, and 2 output nodes. Lines connect every node to all nodes in adjacent layers. One highlighted node shows weighted connections from all inputs, demonstrating dense O(N x M) connectivity."
"Dnn Architectures","6.3","fig-cnn-spatial-processing","**Spatial Feature Extraction**: Convolutional neural networks identify patterns independent of their location in an image by applying learnable filters across the input, enabling robust object recognition. These filters detect local features, and their repeated application across the image creates translation invariance, the ability to recognize a pattern regardless of its position.","Two identical zebra images at different positions in input frames. Arrows show same filter applied to both, producing matching feature activations. Demonstrates translation invariance: detecting patterns regardless of spatial position in image."
"Dnn Architectures","6.4","fig-cnn","**Convolution Operation**: A $3 \times 3$ filter slides across the input, computing local dot products at each position. The highlighted purple region shows the receptive field producing one output value, requiring only 9 multiply-accumulate operations compared to 784 for an equivalent MLP connection. This parameter sharing reduces memory by $5{,}000\times$ while encoding translation equivariance: the same edge detector works at any image location.","Convolution diagram with 3x3 violet filter overlaid on 7x7 input grid. Blue lines connect filter to 3x3 orange output region. Shows receptive field producing one output value through 9 multiply-accumulate operations with parameter sharing."
"Dnn Architectures","6.5","fig-rnn","**Recurrent Neural Network Unfolding**: Left panel shows the compact recurrent loop; right panel unfolds this across time steps. Three weight matrices are shared across all steps: $W_{hx}$ (input-to-hidden), $W_{hh}$ (hidden-to-hidden), and $W_{yh}$ (hidden-to-output). This weight sharing keeps parameter count constant at $O(h^2)$ regardless of sequence length. For a 128-dimensional hidden state, each time step requires 16,384 MACs for recurrent connections plus 12,800 for input projection.","Left: single green hidden node with self-loop labeled W_hh. Right: unfolded across 3 time steps showing purple inputs x_t, green hidden states h_t, and orange outputs y_t. Arrows labeled W_hx, W_hh, W_yh show shared weights across all steps."
"Dnn Architectures","6.6","fig-transformer-attention-visualized","**Attention Weights Visualization**: Attention head (layer 4, head 2) resolving the pronoun \","Sentence tokens listed vertically with cyan attention lines from highlighted word they connecting to all other tokens. Thick lines to student and finish show high attention weights. Demonstrates pronoun-referent linking across arbitrary distances."
"Dnn Architectures","6.7","fig-attention","**Query-Key-Value Attention Mechanism**: For a 6-token sequence, queries (cyan) match against keys (red) to produce a $6 \times 6$ attention matrix with $O(N^2)$ entries. Color intensity indicates attention weight: darker cells show stronger relationships. Each output position aggregates information from all values (green) weighted by its attention row. The matrix structure reveals both the computational pattern (36 similarity computations) and the memory bottleneck (storing $N^2$ attention weights). Source: Transformer Explainer [@transformer_explainer].","6x6 attention matrix with gradient coloring from blue to red indicating attention weights. Cyan query vectors enter from left, red key vectors from top, green value vectors from below. Output vectors exit right, showing weighted aggregation pattern."
"Dnn Architectures","6.8","fig-attention-weightcalc","**QKV Projection Computation**: The embedding matrix $(6 \times 768)$ multiplies with QKV weight matrices $(768 \times 2304)$ plus bias to produce combined projections $(6 \times 2304)$. The 2304 output dimension contains concatenated query, key, and value projections (each 768-dimensional). This single batched matrix multiplication, requiring $6 \times 768 \times 2304 = 10.6$ million MACs, replaces three separate projection operations for efficiency. Source: Transformer Explainer [@transformer_explainer].","Matrix multiplication: 6x768 embedding times 768x2304 QKV weights plus 2304 bias equals 6x2304 output. Blue and red regions show concatenated query, key, value projections. Token labels Data, visualization, em, powers, users, to."
"Dnn Architectures","6.9","fig-transformer","**Transformer Architecture (Encoder-Decoder)**: Complete architecture from Vaswani et al. The encoder (left, repeated $N$ times) consists of multi-head attention followed by feed-forward layers, each with residual connections (arrows bypassing blocks) and layer normalization. The decoder (right) adds masked attention to prevent attending to future tokens during autoregressive generation. Positional encodings\index{Positional Encoding} (sine waves) [@su2024roformer] inject sequence order information absent from the permutation-invariant attention operation. This design enables training parallelism across all positions while the decoder maintains autoregressive causality during inference. Source: Vaswani et al. [@vaswani2017attention].","Encoder-decoder architecture. Encoder: multi-head attention, add-norm, feed-forward, add-norm, repeated Nx. Decoder adds masked attention. Positional encoding sine waves at inputs. Skip connections bypass sublayers. Linear and softmax at top."
"Dnn Architectures","6.10","fig-context-explosion","The Context Explosion: Maximum supported context window (tokens) over time (Log Scale). The transition from 'Standard' windows (512–2k tokens) to 'Massive' windows (1M+ tokens) represents a fundamental shift in how ML systems handle long-range dependencies, increasingly favoring in-context reasoning over traditional retrieval-based approaches.","Step plot showing context windows growing from 512 (BERT) to 1M+ (Gemini 1.5). The log scale highlights the orders-of-magnitude leap in 2023-2024."
"Dnn Architectures","6.11","fig-example-skip-connection","**Residual Connection Block**: The skip connection implements $y = \mathcal{F}(x) + x$, creating an identity shortcut that bypasses the weight layers. During backpropagation, gradients flow through both the residual path (via $\mathcal{F}'(x)$) and the identity path (constant gradient of 1), ensuring gradients reach early layers even in 100+ layer networks. This architectural pattern enabled ResNet-50 to achieve 93% accuracy on CIFAR-10 where equivalent plain networks stagnate at 45%, and has become a required component in all modern deep architectures including Transformers.","Two blue weight layer blocks with ReLU between them. Input x enters left, passes through layers producing F(x), then adds with identity shortcut that bypasses both layers. Brown plus circle combines paths. Output shows F(x) + x with final ReLU."
"Dnn Architectures","6.12","fig-im2col-diagram","**im2col Transformation**: Converts convolution to GEMM by rearranging image patches into columns. The input feature maps (cyan/orange grids, $3 \times 3$) are unfolded so each sliding window position becomes a matrix column, while filter kernels (green/yellow, $2 \times 2$) become rows. The resulting $4 \times 8$ matrix multiplication produces all output positions in one operation. This transformation trades 2x memory overhead (duplicating overlapping pixels) for 5-10x speedup by leveraging decades of BLAS optimizations and enabling efficient GPU parallelization.","Left: two 3x3 input feature maps in cyan and orange. Center: 4x8 transformed matrix with unfolded patches as columns. Right: 8x1 filter kernel vector. Red boxes highlight how sliding windows become matrix columns for GEMM."
"Dnn Architectures","6.13","fig-collective-comm","**Data Movement Primitives**: Four fundamental patterns govern information flow in neural network computation. **Broadcast** (top-left) replicates a single value to all destinations, used when sharing weights across batch elements. **Scatter** (top-right) distributes distinct elements to different destinations, enabling work partitioning. **Gather** (bottom-left) collects distributed values to a single location, as in attention pooling. **Reduction** (bottom-right) combines multiple values through aggregation (sum, max), appearing in gradient synchronization and attention scoring. Moving data typically costs 100-1000x more energy than computation, making these patterns critical optimization targets.","Four diagrams with nodes and arrows. Broadcast: one red square to four nodes. Scatter: four colored squares to four nodes. Gather: four nodes with colored squares to one. Reduction: four colored nodes combine through aggregation to one."
"Dnn Architectures","6.14","fig-dnn-fm-framework","**Architecture Selection Decision Framework**: A systematic flowchart for choosing neural network architectures based on data characteristics and deployment constraints. The process begins with data type identification (text/sequences/images/tabular) to select initial architecture candidates (Transformers/RNNs/CNNs/MLPs), then iteratively evaluates memory budget, computational cost, inference speed, accuracy targets, and hardware compatibility.","Flowchart from Define Problem branching by data type to Transformers, RNNs, CNNs, or MLPs. Diamond nodes check memory, compute, speed, accuracy, deployment. No paths loop to scale down or increase capacity. Yes path leads to selected."
"Frameworks","7.1","fig-mlfm-timeline","**Computational Library Evolution**: Modern machine learning frameworks build upon decades of numerical computing advancements, transitioning from low-level routines like BLAS and LAPACK to high-level abstractions in NumPy and SciPy, and finally to deep learning frameworks such as Theano [@bergstra2010theano], TensorFlow, and PyTorch. SciPy was first released in 2001; the timeline shows its adoption alongside Theano in the 2007 period when both contributed to establishing Python's scientific computing ecosystem.","Horizontal timeline from 1979 to 2018 with colored boxes marking key years. Dashed arrows connect to milestones below: 1979 BLAS introduced, 1992 LAPACK extends BLAS, 2006 NumPy becomes Python's numerical backbone, 2007 SciPy and Theano introduce computational graphs, 2015 TensorFlow revolutionizes distributed ML, 2016 PyTorch introduces dynamic graphs, 2018 JAX introduces functional paradigms."
"Frameworks","7.2","fig-comp-graph","**Simple Computational Graph.** A directed acyclic graph representing the computation $z = x \\times y$, where nodes define operations and edges specify the flow of data between them.","Simple directed graph with nodes x and y flowing into function f(x,y) which outputs z."
"Frameworks","7.3","fig-mlfm-comp-graph","**Computation Graph with System Interactions.** A neural network represented as a directed acyclic graph (left), with system components including memory management and device placement (right) that interact with the graph to optimize resource allocation before execution.","Left side shows computational graph with 6 operation nodes connected by data flow edges. Right side shows system components box with Memory Management and Device Placement nodes that interact with the computational graph."
"Frameworks","7.4","fig-mlfm-dynamic-graph-flow","**Dynamic Graph Execution Flow**: In eager execution, each operation is defined and immediately executed before the next operation begins. This define-by-run model enables natural debugging and data-dependent control flow at the cost of optimization opportunities.","Flow diagram showing Start to Operation 1 to Operation 1 Executed to Operation 2 to Operation 2 Executed to End. Above arrows show Define Operation, Execute Operation, Define Next Operation, Execute Operation, Repeat Until Done."
"Frameworks","7.5","fig-mlfm-static-graph","**Static Graph: Define then Execute.** The two phases of static graph execution. The definition phase (left) declares operations and builds the graph. The execution phase (right) loads data, runs the optimized graph, and produces results.","Flow diagram showing two phases. Definition Phase: Define Operations, Declare Variables, Build Graph. Execution Phase: Load Data, Run Graph, Get Results. Arrows connect boxes left to right."
"Frameworks","7.6","fig-python-tax","The Python Tax: Visualizing the overhead analysis from the preceding callout. In Eager Mode (top), the GPU (blue) finishes processing each op in microseconds but must sit idle while the Python interpreter (red) dispatches the next kernel launch. Compilation (bottom) fuses these operations into a single kernel, effectively hiding the dispatch latency and maximizing GPU utilization.","Gantt chart of execution timeline. Eager mode shows alternating red (Python) and blue (GPU) blocks with gaps. Compiled mode shows one small red block followed by one long blue block."
"Frameworks","7.7","fig-compilation-continuum","The Compilation Continuum: Optimal execution strategy depends on development-to-production ratio. Left region (high dev iterations): eager mode dominates. Right region (high prod executions): compilation dominates. The crossover point depends on compilation cost and per-execution speedup.","Graph with x-axis 'Production Executions' (log scale) and y-axis 'Total Time'. Three lines: Eager (steep slope), JIT (moderate slope with offset), Static (gentle slope with larger offset). Lines cross at different points showing when compilation becomes beneficial."
"Frameworks","7.8","fig-tensor-data-structure-a","**Tensor Rank Hierarchy.** Four shapes illustrating tensor ranks from left to right: a single value (rank 0, scalar), a column of values (rank 1, vector), a grid of values (rank 2, matrix), and a cube of values (rank 3, three-dimensional tensor).","Four shapes showing tensor ranks left to right: single box labeled Rank 0, vertical column of numbers labeled Rank 1, 2D grid of numbers labeled Rank 2, and 3D cube labeled Rank 3."
"Frameworks","7.9","fig-tensor-data-structure-b","**Image as RGB Tensor.** Three stacked grids representing the red, green, and blue color channels of an image, with dimension labels showing width, height, and channel depth forming a rank-3 tensor. *Credit: Niklas Lang [https://towardsdatascience.com/what-are-tensors-in-machine-learning-5671814646ff](https://towardsdatascience.com/what-are-tensors-in-machine-learning-5671814646ff)*.","Three stacked 3x3 grids in red, green, and blue representing RGB color channels. Dimension labels show width 3 pixels, height 3 pixels, and 3 color channels forming a 3D tensor for image data."
"Frameworks","7.10","fig-tensor-memory-layout","**Tensor Memory Layout**: A 2×3 tensor can be stored in linear memory using either row-major (C-style) or column-major (Fortran-style) ordering. Strides define the number of elements to skip in each dimension when moving through memory, enabling frameworks to calculate memory addresses for tensor[i,j] as base_address + i×stride[0] + j×stride[1]. The choice of memory layout significantly impacts cache performance and computational efficiency.","Left: 2x3 tensor grid with values 1-6. Right: two linear arrays showing row-major layout (1,2,3,4,5,6) and column-major layout (1,4,2,5,3,6). Below: stride calculations for row-major [3,1] and column-major [1,2]."
"Frameworks","7.11","fig-3d-parallelism","**3D Parallelism.** A grid of eight accelerator clusters arranged in two rows and four columns, each containing stacked computational units. Distinct colors encode the three parallelism dimensions: data parallelism across columns, pipeline parallelism across rows, and model parallelism within each cluster.","Grid of 8 GPU clusters in 2 rows and 4 columns. Each cluster contains 4 stacked cubes. Colors vary: blue, red, green, orange in bottom row; olive, yellow, brown, pink in top row."
"Frameworks","7.12","fig-mlfm-core-ops","**Core Operations Stack.** Three grouped layers showing how frameworks bridge Python code to hardware. The top layer contains system-level operations (scheduling, memory management, resource optimization), the middle layer holds numerical operations (GEMM, BLAS, element-wise), and the bottom layer provides hardware abstraction (kernel management, memory abstraction, execution control).","Three grouped boxes connected by arrows. System-Level: Scheduling, Memory Management, Resource Optimization. Numerical: GEMM, BLAS, Element-wise Operations. Hardware: Kernel Management, Memory Abstraction, Execution Control."
"Frameworks","7.13","fig-tensorflow-architecture","**TensorFlow Training-to-Deployment Pipeline.** Two-column diagram showing the training path (left) from data preprocessing through tf.keras and distribution strategy across CPU, GPU, and TPU, and the deployment path (right) from SavedModel export to TensorFlow Serving, Lite, JS, and language bindings. Source: [TensorFlow.](https://blog.tensorflow.org/2019/01/whats-coming-in-tensorflow-2-0.html).","Two-column diagram. Training: data preprocessing, tf.keras, TensorFlow Hub, Premade Estimators, Distribution Strategy across CPU/GPU/TPU. Deployment via SavedModel to TensorFlow Serving, Lite, JS, and language bindings."
"Frameworks","7.14","fig-onnx","Framework Interoperability: ONNX enables model portability across frameworks, allowing training in one framework and deployment in another.","Hub diagram with ONNX logo at center. Left side: PyTorch, TensorFlow, Keras with arrows pointing inward. Right side: TF Lite, ONNX Runtime with arrows outward."
"Training","8.1","fig-communication-tax","The Communication Tax: Effective Throughput vs. GPU Count (Log-Log Scale). Ideal scaling (dashed gray) represents the linear ceiling. Compute-bound workloads like ResNet (Blue) maintain high efficiency. Balanced workloads like LLMs with high-speed interconnects (Green) show slight degradation, while bandwidth-bound workloads (Red) suffer from the full 'Communication Tax' (shaded region). This divergence reveals why network topology and bandwidth become the dominant constraints when scaling to massive clusters.","Log-log plot of Throughput vs. GPUs (up to 256). Three lines show varying scaling efficiencies: Blue (95%), Green (85%), and Red (60%), with a shaded red region illustrating the cumulative communication tax."
"Training","8.2","fig-activation-perf","**Activation Function Execution Time**: CPU benchmarks on Apple M2 hardware reveal significant variation: Tanh completes in 0.61 seconds, ReLU in 0.45 seconds, Softmax in 0.91 seconds, and Sigmoid in 1.10 seconds. These differences directly affect training throughput and real-time inference latency, making activation function selection a system-level design decision.","Bar chart comparing CPU execution times: Sigmoid at 1.1 seconds, Tanh at 0.61 seconds, ReLU at 0.45 seconds, and Softmax at 0.91 seconds."
"Training","8.3","fig-training-roofline","Training Roofline Model: GPT-2 training operations mapped against arithmetic intensity on a log-log roofline diagram. Matrix multiplications operate in the compute-bound regime (right of the ridge point), while normalization and activation operations fall in the memory-bound region (left). FlashAttention shifts standard attention from below to above the ridge point, demonstrating how algorithmic redesign can move operations into a more efficient regime.","Log-log plot showing roofline model with memory-bound slope and compute-bound ceiling. Points show different training operations: MatMul above ridge point, LayerNorm and Softmax below. Arrow shows FlashAttention improvement."
"Training","8.4","fig-training-pipeline","**Training System Overview**: Machine learning systems organize training through interconnected data, training, and evaluation pipelines. Data flows sequentially through these components, with evaluation metrics providing feedback to guide iterative model refinement and ensure reproducible results.","Block diagram with three connected boxes: Data Pipeline, Training Loop, and Evaluation Pipeline. Arrows show data flow with feedback from evaluation."
"Training","8.5","fig-training-loop","**Single-GPU Training Loop**: The three sequential steps of one training iteration: the forward pass generates predictions, gradient computation propagates error signals backward, and the optimizer applies parameter updates. GPUs parallelize the underlying matrix operations, accelerating both the forward and backward passes.","Neural network diagram showing data cylinders feeding into a network of connected nodes. A GPU box at bottom processes the forward and backward pass computations."
"Training","8.6","fig-data-pipeline","**CPU-to-GPU Data Flow**: Three distinct zones compose the data pipeline: the storage zone houses raw data on disk, the CPU preprocessing zone handles format conversion, processing, and batching, and the GPU training zone distributes preprocessed batches across multiple GPU workers for parallel computation.","Block diagram showing data flow through three zones: Storage Zone with raw data, CPU Preprocessing Zone with format, process, and batch stages, and GPU Training Zone with three GPU workers."
"Training","8.7","fig-galore-llm-memory-breakdown","**Memory Footprint Breakdown**: Memory usage of LLaMA-7B across four optimizer configurations, decomposed into weights, activations, optimizer state, weight gradients, and other components. The dashed red line marks the RTX 4090 24 GB memory limit, illustrating how standard Adam exceeds single-GPU capacity while GaLoRE compression reduces optimizer state enough to fit within this budget.","Stacked horizontal bar chart comparing memory usage across four optimizers for LLaMA-7B. Shows components: others, weight gradient, optimization, activation, and weight. Dashed red line marks RTX 4090 memory limit at 24 GB."
"Training","8.8","fig-linear-scaling-failure","The Linear Scaling Failure. Training Loss vs. Epochs. Curve A (Blue) represents a standard baseline batch size. Curve B (Gray) shows what happens when batch size is increased 8x without tuning: convergence slows dramatically because weight updates are too infrequent (per epoch). Curve C (Green) restores convergence by scaling the learning rate linearly (8x LR), allowing the model to take larger steps to compensate for fewer updates.","Line chart of Loss vs Epochs. Blue line (Baseline) converges fast. Gray line (Large Batch Naive) converges slow. Green line (Scaled LR) matches the baseline."
"Training","8.9","fig-tf-bottleneck-trace","Data-Bound Profiler Trace: TensorFlow profiler output capturing a data loading bottleneck during training. The gaps in GPU activity (white regions between compute blocks) indicate periods where the device idles while waiting for input data, with utilization dropping to zero during data loading phases.","TensorFlow profiler screenshot showing GPU activity timeline. Colored blocks indicate computation periods with white gaps revealing idle time when GPU waits for data loading to complete."
"Training","8.10","fig-optimization-flowchart","**Training Optimization Decision Flowchart**: Systematic approach to optimization selection based on profiling results. Begin by measuring GPU utilization, then follow the decision path to identify whether the bottleneck is data-bound, memory-bound, or compute-bound. Each path leads to specific techniques that address the identified constraint.","Flowchart showing optimization decision tree starting from Profile Training Run, branching based on GPU utilization and memory pressure to different optimization techniques."
"Training","8.11","fig-fetching-naive","**Sequential Data Fetching**: File open, read, and train operations execute serially across two epochs, with the GPU remaining idle during all file operations. The full sequential pipeline spans approximately 90 seconds, establishing the baseline that overlapped prefetching improves upon.","Gantt chart showing sequential data pipeline over two epochs. Four rows: Open, Read, Train, and Epoch. Operations execute serially with gaps between phases, spanning from 00:00 to 01:30."
"Training","8.12","fig-fetching-optimized","**Overlapped Data Prefetching**: Read and train operations execute concurrently, with each time slice overlapping data loading for the next batch with computation on the current batch. Two epochs complete in approximately 55 seconds compared to 90 seconds with sequential fetching, a 40% speedup.","Gantt chart showing optimized pipeline with overlapping operations. Read and Train execute in parallel across time slices. Two epochs complete in approximately 55 seconds total."
"Training","8.13","fig-mixed-precision","**Mixed Precision Training**: The seven-step cycle: (1) FP32 master weights convert to FP16 for the forward pass, (2) loss is scaled to prevent gradient underflow, (3) backpropagation computes scaled FP16 gradients, (4) gradients copy to FP32, (5) loss scaling is removed, (6) FP32 gradients update master weights, and (7) the cycle repeats. This approach achieves Tensor Core speedups while preserving numerical stability.","Flowchart showing 7-step mixed precision training cycle. FP32 master weights convert to FP16 for forward pass, loss scaling protects gradients during backpropagation, then gradients update FP32 weights."
"Training","8.14","fig-grad-accumulation","**Gradient Accumulation**: Three micro-batches each compute independent losses and gradients, which sum into a single combined gradient for one parameter update. This simulates training with a batch three times larger without requiring the memory to hold all samples simultaneously.","Block diagram showing three batches computing individual losses and gradients. Arrows flow from Batch 1, 2, 3 through Losses to Gradients boxes, then combine into a single summed gradient output."
"Training","8.15","fig-activation-checkpointing","**Activation Checkpointing**: Trading memory usage for recomputation during backpropagation enables training deeper neural networks. By storing only a subset of activations from the forward pass and recomputing others on demand, this technique reduces peak memory requirements at the cost of increased training time.","Two-row diagram showing activation checkpointing. Top row: forward pass with checkpointed nodes (filled) and discarded nodes (dashed). Bottom row: backward pass recomputing discarded activations from checkpoints."
"Training","8.16","fig-evolution-systems","**Computing System Evolution**: Hardware advancements continuously adapted to the increasing demands of machine learning workloads, transitioning from centralized mainframes to specialized architectures optimized for parallel processing and massive datasets.","Timeline spanning 1950s to 2020s showing evolution from mainframes through HPC and warehouse-scale computing to AI hypercomputing with GPUs and TPUs."
"Training","8.17","fig-train-data-parallelism","**Data Parallelism**: Each GPU holds a complete model copy, processes different data batches, then synchronizes gradients. This approach scales training throughput linearly with GPU count when models fit in single-GPU memory.","Diagram showing input data splitting into 4 batches, each assigned to a GPU for forward/backward pass, with gradients aggregating for model update."
"Training","8.18","fig-model-parallelism","**Model Parallelism**: The model is partitioned across devices, with intermediate activations passing between them. This enables training models larger than single-GPU memory at the cost of sequential dependencies.","Diagram showing input flowing through model parts on different devices, with forward pass going right and backward pass returning left."
"Training","8.19","fig-layers-blocks","**Layer-wise Partitioning**: A 24-layer transformer distributed across four devices, with each device responsible for six consecutive transformer blocks. Communication occurs only at partition boundaries.","Diagram showing transformer blocks 1-6 on GPU 1, blocks 7-12 on GPU 2, blocks 13-18 on GPU 3, and blocks 19-24 on GPU 4."
"Data Selection","9.1","fig-running-out-of-human-data","Dataset Growth Approaching Limits: Foundation models are increasingly trained on vast datasets, approaching the total stock of human-generated text. Current projections suggest that high-quality public text data faces exhaustion on a near-term horizon, forcing a shift toward data selection, synthetic generation, and multimodal learning. Source: @villalobos_ho_sevilla_besiroglu_heim_hobbhahn_2024.","Line chart showing dataset size in tokens on y-axis from 10^10 to 10^14 versus year on x-axis from 2010 to 2030. Blue line shows training data growth with markers for models like GPT-2, GPT-3, and Chinchilla. Orange shaded region shows projected high-quality text exhaustion in the near term."
"Data Selection","9.2","fig-optimization-triad","**The Optimization Triad**: Machine learning performance relies on three pillars: Algorithms (models), Systems (hardware/software), and Data Selection. While algorithms and systems have traditionally received the most attention, optimizing data selection (Input Optimization) offers a third, powerful lever for scaling performance.","A triangular diagram with three nodes: Algorithms (Model), Systems (Hardware), and Data Selection. Bidirectional arrows connect all three with edge labels: Compute Bound between Algorithms and Systems, I/O Bound between Systems and Data Selection, and Sample Efficiency between Data Selection and Algorithms. Data Selection is highlighted with a bold border. ML Scale appears at the center."
"Data Selection","9.3","fig-data-selection-pipeline","**The Data Selection Pipeline**: A structured approach to increasing data value. Raw data is first pruned to remove redundancy (Static Pruning), then dynamically selected during training (Active Learning), and finally augmented to increase diversity (Synthesis). Each stage increases the Information-Compute Ratio (ICR).","A flow diagram showing the progression of data: Raw Data -> Static Pruning -> Dynamic Selection -> Synthetic Generation -> High Value Model. Arrows indicate the flow."
"Data Selection","9.4","fig-coreset-selection","**Coreset Selection Strategy**: Random sampling (left) selects uniformly, wasting budget on easy samples far from the decision boundary. Coreset selection (right) prioritizes samples near the boundary where the model is uncertain, capturing more information per sample.","Two scatter plots with a diagonal decision boundary. Left plot shows random dots selected. Right plot highlights dots near the boundary as selected."
"Data Selection","9.5","fig-active-learning-loop","**Active Learning Loop**: Instead of labeling all data, the model selects the most 'confusing' or informative samples from an unlabeled pool. These samples are sent to an Oracle (human annotator) and added to the training set. The model is retrained, and the cycle repeats, creating a feedback loop that maximizes information gain per label.","A cycle diagram: Unlabeled Pool -> Selection Strategy -> Oracle -> Labeled Set -> Model Training -> back to Selection Strategy."
"Data Selection","9.6","fig-active-learning-multiplier","The Active Learning Multiplier: Model Accuracy vs. Number of Labeled Samples (Log Scale). Random sampling (gray dashed) yields linear improvements, often requiring massive datasets to capture rare edge cases. Active Learning (green solid) specifically targets informative samples, achieving the same 90% accuracy with 4x fewer labels. The green shaded region represents the direct economic value (labeling cost saved) of intelligent data selection.","Line chart of Accuracy vs Labeled Samples (log scale). Green line (Active Learning) rises much faster than gray line (Random Sampling). Shaded area between them shows cost savings."
"Data Selection","9.7","fig-amortization-comparison","**Cost Amortization in Foundation Models**: Training from scratch (left) requires 1,000 GPU-hours per task (10,000 total for 10 tasks). The foundation model approach (right) pays 10,000 GPU-hours upfront for pre-training but reduces each subsequent task to just 50 GPU-hours. At 10 tasks the totals are comparable (10,000 vs 10,500), but the per-task marginal cost drops by 20x, and the crossover favoring the foundation model occurs around 11 tasks.","Two bar charts side by side. Left (Train from Scratch) shows 10 equal bars of 1,000 GPU-hours each, totaling 10,000 hours. Right (Foundation Model) shows one tall pre-training bar of 10,000 GPU-hours followed by 10 short fine-tuning bars of 50 GPU-hours each, totaling 10,500 hours. The per-task marginal cost drops dramatically from 1,000 to 50 GPU-hours."
"Data Selection","9.8","fig-domain-gap","**The Domain Gap Problem**: Synthetic data (blue) and real data (orange) have different distributions. A model trained on synthetic data alone learns a boundary that fails on real data. Domain adaptation techniques aim to align these distributions or learn domain-invariant features.","Two overlapping bell curves representing synthetic and real data distributions, with a decision boundary that works for synthetic but misses real data."
"Data Selection","9.9","fig-technique-decision-tree","**Data Selection Technique Selection Tree**: Start at the top by identifying your primary bottleneck, then follow the branches to find the most appropriate technique. Leaf nodes show recommended methods. Multiple paths may apply; combine techniques as needed.","A decision tree flowchart with diamond decision nodes and rectangular technique recommendations. Starts with bottleneck identification and branches to specific techniques."
"Data Selection","9.10","fig-selection-inequality","**The Selection Inequality**\index{Data Selection Systems!selection inequality}: Data selection only improves end-to-end efficiency if the overhead of selection plus training on the subset is less than training on the full dataset. A lightweight selection function (proxy model, cached embeddings) keeps selection overhead low; an expensive selection function (full model forward pass) can negate the savings.","Stacked bar chart comparing three approaches: Baseline shows a single tall bar (100) for full training; Efficient Selection shows two short stacked bars (5 selection overhead plus 40 subset training) totaling 45 with a 55 percent savings annotation; Expensive Selection shows two stacked bars (60 selection overhead plus 40 subset training) totaling 100 with a No savings annotation."
"Data Selection","9.11","fig-optimization-stack","**The Optimization Stack**: The complete pipeline from raw data to deployed system, showing how optimizations at each stage propagate downstream. Data artifacts (rounded boxes) flow through processing stages (rectangular boxes). Optimizations early in the pipeline---particularly data selection---have multiplicative effects because they reduce the workload for all subsequent stages.","Pipeline diagram with two rows. Top row shows Raw Data flowing through Data Selection to Curated Data, then through Training to produce a Model. Bottom row shows the Model flowing through Compression to a Compact Model, then through Hardware optimization to a Deployed System. Arrows indicate the flow direction between stages."
"Data Selection","9.12","fig-compute-optimal-frontier","**The Compute-Optimal Frontier**: For any training compute budget, there is a best achievable performance when data and model size are optimally balanced (green curve). Operating points below the frontier indicate inefficiency. **Data-starved** systems (orange) have compute capacity but insufficient quality data; the techniques in this chapter move them toward the frontier. **Compute-starved** systems (red) have quality data but insufficient training budget; hardware acceleration or distributed training helps here. The goal is to operate *on* the frontier, extracting maximum performance from available resources.","A log-log plot with Training Compute on x-axis and Model Performance on y-axis. A green curve shows the optimal frontier. Orange point below curve labeled Data-starved. Red point below curve labeled Compute-starved. Purple point on curve labeled Optimal."
"Data Selection","9.13","fig-ppd-curve","**Diminishing Returns of Data**: Random sampling (gray) versus data-efficient selection (blue). The efficient strategy achieves higher performance with less data, reaching the convergence plateau much earlier. The red arrow shows the efficiency gap at a fixed dataset size.","A plot with X-axis 'Dataset Size' and Y-axis 'Performance'. Two curves start at 0. The 'Random' curve rises slowly. The 'Efficient' curve rises steeply and plateaus early."
"Hw Acceleration","10.1","fig-iron-law-heatmap","The Iron Law Heatmap: Total system speedup as a function of Accelerator Speed ($S$) and Parallel Fraction ($P$). The 'Accelerator Wall' at the top reveals that if a workload is even slightly serial ($P < 0.9$), increasing hardware speed yields almost no benefit. Realizing the potential of 1000x accelerators requires engineering workloads with near-perfect ($P > 0.999$) parallelism.","Heatmap of Speedup vs Accelerator Speed and Parallel Fraction. High speedup (green/yellow) is only achieved in the bottom right corner where Parallel Fraction is near 1.0. The rest of the map is dominated by blue (low speedup), showing the serial bottleneck."
"Hw Acceleration","10.2","fig-timeline","**Hardware Specialization Timeline.** Computing architectures progressively incorporate specialized accelerators to address emerging performance bottlenecks, from floating-point units to graphics processors and machine learning accelerators. Each era produced hardware tailored to the dominant computational patterns of its period.","Timeline spanning 1980s to 2020s showing hardware evolution: floating-point units, GPUs with hardware transform and lighting, media codecs, TPUs with tensor cores, and application-specific AI engines."
"Hw Acceleration","10.3","fig-systems-gap","The Systems Gap: Relative Compute Growth (Log Scale) comparing Model Demand to Hardware Supply. The gray dotted line (CPU) and blue dashed line (GPU) reflect hardware progress, which significantly lags behind the exponential red solid line (Model Demand). The massive purple shaded region represents the 'Systems Gap': a physical and economic deficit that cannot be closed by faster chips alone, but must be bridged through parallelism, architectural innovation, and hardware-software co-design.","Log-scale line chart from 2012 to 2024. Red line (Model Demand) rises steeply. Blue line (GPU Supply) rises moderately. Gray line (CPU Trend) rises slowly. A large purple shaded area between Red and Blue is labeled 'THE SYSTEMS GAP'."
"Hw Acceleration","10.4","fig-tech-s-curve","The Twin S-Curves of Modern Computing. General-purpose CPUs (gray) enjoyed decades of exponential growth driven by Moore's Law and Dennard Scaling. As physics constrained this curve around 2010 (Saturation), the industry was forced to jump to a new curve: Domain Specific Architectures (blue). We are currently in the Take-off phase of this new paradigm, where massive efficiency gains come from specializing hardware for linear algebra, albeit at the cost of general programmability.","Two overlapping S-curves plotting performance over time. Gray curve shows general-purpose CPUs reaching saturation around 2010. Blue curve shows domain-specific architectures in take-off phase starting 2015."
"Hw Acceleration","10.5","fig-accelerator-anatomy","**Anatomy of a Modern AI Accelerator**: AI accelerators integrate specialized processing elements containing tensor cores, vector units, and special function units, supported by a hierarchical memory system from high-bandwidth memory down to local caches. This architecture maximizes data reuse and parallel execution while minimizing energy-intensive data movement, forming the foundation for 100-1000× performance improvements over general-purpose processors.","Block diagram showing AI accelerator architecture: CPU connects to DRAM stacks and processing element grid containing tensor cores, vector units, and local caches in hierarchical arrangement."
"Hw Acceleration","10.6","fig-ai-performance","GPU Performance Scaling: NVIDIA GPUs experienced approximately a ~1,000$\times$ increase in integer 8-bit TOPS (tera operations per second) over a decade, from 4 TOPS on the K20X to 4,000 TOPS on the H100. This three-orders-of-magnitude gain was driven by architectural innovations transitioning from floating-point to tensor core acceleration.","Line graph of NVIDIA GPU INT8 performance from 2012 to 2023 showing exponential growth from K20X at 4 TOPS to H100 at 4000 TOPS, a 1000x increase over the decade."
"Hw Acceleration","10.7","fig-sparse-formats","**Sparse Storage Formats**: Hardware efficiency depends on how sparse matrices are stored. **Dense** storage (top left) is simple but wasteful for zeros. **Block Sparse** (top right) and **CSR** (bottom) compress the matrix by storing only non-zero values and their indices. Structured sparsity (like N:M or Blocks) makes this indexing predictable, allowing hardware to fetch data and skip zeros efficiently.","Grid of 3x3 matrix blocks. Top left: Dense Matrix. Top right: Block Sparse Matrix showing dense sub-blocks. Bottom: Sparse Matrix (CSR) and Block Sparse (BSR) representations showing values and index arrays."
"Hw Acceleration","10.8","fig-systolic-array","**Systolic Array Dataflow**: A control unit feeds input data streams into a grid of processing elements, each performing multiply-accumulate operations. Data flows horizontally and vertically through the array in a pipelined manner, maximizing operand reuse and minimizing memory access, as exemplified by Google's TPUv4.","Systolic array diagram with control unit feeding data streams into processing element grid. Elements perform multiply-accumulate operations with results flowing through accumulator chain."
"Hw Acceleration","10.9","fig-energy-hierarchy","The Energy Hierarchy: Energy cost per operation (Log Scale) based on the 'Horowitz Numbers.' Fetching data from off-chip DRAM costs ~128x more energy than an SRAM access and ~20,000x more than an INT8 addition. This fundamental physical disparity dictates that AI accelerators must prioritize data locality (keeping weights in SRAM/Registers) over raw arithmetic throughput to remain within power budgets.","Horizontal bar chart of Energy (pJ) per operation on log scale. INT8 Add is tiny (0.03). DRAM Read is huge (640). An arrow highlights the massive gap between computation and memory access."
"Hw Acceleration","10.10","fig-compute-memory-imbalance","The Compute-Bandwidth Divergence: Computational capability and memory bandwidth plotted on a log scale (2000–2025). While arithmetic throughput (FLOPs) has grown exponentially, memory bandwidth has improved at a significantly slower linear rate. This widening 'Systems Gap' defines the AI Memory Wall, forcing architects to design systems that minimize data movement to avoid idling powerful compute units.","Line graph comparing compute performance and memory bandwidth from 2000 to 2025 on log scale. Compute grows exponentially; bandwidth grows linearly. Shaded gap labeled Memory Wall widens over time."
"Hw Acceleration","10.11","fig-rising-ridge","The Rising Ridge: Hardware Arithmetic Intensity (FLOPs/Byte) over time. As compute capability (FLOPs) grows faster than memory bandwidth (Bytes/s), the 'Ridge Point' (the intensity required to saturate the chip) skyrockets. This trend explains why architectures with high data reuse (like Transformers) have flourished while sparse or low-reuse architectures (like RNNs) face a growing 'Hardware Tax' that makes them increasingly inefficient on modern silicon.","Line plot showing the Arithmetic Intensity Ridge Point growing from ~140 in 2017 (V100) to over 500 in 2024 (B200). Shaded regions indicate 'Memory-Rich' and 'Compute-Dense' zones."
"Hw Acceleration","10.12","fig-memory-wall","Model Size vs. Hardware Bandwidth. Model parameter counts and hardware memory bandwidth plotted from 2012 to 2024, showing how model growth from AlexNet to trillion-parameter models has far outpaced bandwidth improvements across GPU and TPU generations.","Scatter plot with trend lines comparing AI model parameters (red) and hardware bandwidth (blue) from 2012 to 2024. Models grow from AlexNet to Gemini 1. Shaded gap shows widening memory wall."
"Hw Acceleration","10.13","fig-host-accelerator-data-movement","**Host-Accelerator Data Transfer**: AI workloads require frequent data movement between CPU memory and accelerators. The four sequential steps of copying input data, issuing execution instructions, parallel computation, and transferring results each introduce potential performance bottlenecks.","Four-step data flow diagram: (1) copy data from main memory to GPU memory, (2) CPU instructs GPU, (3) GPU executes in parallel, (4) results copy back to main memory."
"Hw Acceleration","10.14","fig-tiling-diagram","**Matrix Tiling**: Partitioning large matrices into smaller tiles optimizes data reuse and reduces memory access overhead during computation. This technique improves performance on AI accelerators by enabling efficient loading and processing of data in fast memory, minimizing transfers from slower main memory.","Three matrices A, B, C with highlighted tiles showing how matrix multiplication partitions into smaller blocks. Dimensions labeled M, N, K with corresponding tile sizes Mtile, Ntile, Ktile."
"Benchmarking","11.1","fig-imagenet-gpus","**GPU Adoption and Error Reduction**: As GPU entries in ImageNet surged from 0 to 110 between 2010 and 2014, top-5 error rates dropped from 28.2% to 7.3%, demonstrating the co-evolution of hardware capabilities and algorithmic advances.","Dual-axis chart with blue line showing top-5 error rate declining from 28% to 7% and green bars showing GPU entries rising from 0 to 110 between 2010 and 2014."
"Benchmarking","11.2","fig-granularity","**Benchmarking Granularity**: Four-panel block diagram showing micro, model, application, and end-to-end evaluation layers. Each panel maps a distinct scope of assessment, from isolated kernel operations through full-system deployment, enabling targeted optimization at every level of the ML stack.","Block diagram showing three evaluation layers: neural network nodes on left, model components in center, and end-to-end application with compute nodes on right, connected by dashed lines."
"Benchmarking","11.3","fig-benchmark-tradeoffs","**Isolation vs. Representativeness**: The core trade-off in benchmarking granularity. Micro-benchmarks provide high diagnostic precision but limited real-world relevance, while end-to-end benchmarks capture realistic system behavior but offer less precise component-level insights. Effective ML system evaluation requires strategic combination of all three levels.","Scatter plot with three labeled points along diagonal: micro-benchmarks at high isolation, macro-benchmarks at medium, and end-to-end benchmarks at high representativeness."
"Benchmarking","11.4","fig-benchmark-components","**Anomaly Detection Pipeline**: Nine-stage benchmark workflow applied to an industrial audio anomaly detection task. The pipeline progresses from problem definition through dataset selection, model training, quantization, and ARM embedded deployment, illustrating how each benchmark component feeds the next.","Workflow diagram showing nine stages from problem definition through deployment, with detailed views of anomaly detection system, model training, quantization, and ARM embedded implementation."
"Benchmarking","11.5","fig-mlperf-training-improve","**MLPerf Training Progress**: Standardized benchmarks reveal that machine learning training performance consistently surpasses Moore's Law, indicating substantial gains from systems-level optimizations. These trends emphasize how focused measurement and iterative improvement drive rapid advancements in ML training efficiency and scalability. Source: [@tschand2024mlperf].","Line chart with nine model benchmarks from 2018 to 2024 showing relative performance gains up to 48x for Mask R-CNN, all exceeding the Moore's Law baseline of 6.6x."
"Benchmarking","11.6","fig-power-differentials","Power Consumption Differentials: Power usage spans six orders of magnitude across ML system types, from milliwatts in tinyML devices through watts at the edge to kilowatts in datacenter inference and hundreds of kilowatts for training clusters. These differentials shape deployment trade-offs between latency, cost, and energy efficiency.","Dumbbell chart showing power consumption ranges: Tiny 5.6 to 167 mW, Edge 3.9 to 1100 W, Datacenter 267 to 6300 W, Training 5.5 to 498,000 W on logarithmic scale."
"Benchmarking","11.7","fig-power-diagram","**Power Measurement Boundaries**: MLPerf defines system boundaries for power measurement, ranging from single-chip devices to full data center nodes, to enable fair comparisons of energy efficiency across diverse hardware platforms. These boundaries delineate which components' power consumption is included in reported metrics, impacting the interpretation of performance results. Source: [@tschand2024mlperf].","System diagram showing four measurement boundaries: Tiny SoC with compute units, Inference SoC with accelerators and DRAM, Inference Node with cooling and NIC, and Training Rack with compute nodes."
"Benchmarking","11.8","fig-power-trends","**Energy Efficiency Gains**: Successive MLPerf inference benchmark versions show energy efficiency (samples per watt) improving up to 378x for datacenter workloads and 1070x for tinyML deployments across successive releases. Standardized measurement protocols enable meaningful cross-platform comparisons, driving sector-wide progress toward sustainable AI. Source: [@tschand2024mlperf].","Three line charts showing normalized energy efficiency across MLPerf versions: datacenter models up to 378x gain, edge models up to 4x, and tiny models up to 1070x improvement."
"Benchmarking","11.9","fig-hw-lottery","**Hardware-Dependent Accuracy**: Model performance varies significantly across hardware platforms, indicating that architectural efficiency is not solely determined by design but also by hardware compatibility. Multi-hardware models exhibit comparable accuracy to MobileNetV3 Large on CPU and GPU configurations, yet achieve substantial gains on EdgeTPU and DSP, emphasizing the importance of hardware-aware model optimization for specialized computing environments. Source: [@chu2021discovering].","Five scatter plots comparing model accuracy versus latency across CPU, GPU, EdgeTPU, and DSP platforms, with arrow showing MobileNetV3 gaining on EdgeTPU and DSP versus CPU and GPU."
"Benchmarking","11.10","fig-sciml-graph","**Performance Spectrum**: Scientific applications and edge devices demand vastly different computational resources, spanning multiple orders of magnitude in data rates and latency requirements. Consequently, traditional benchmarks focused solely on accuracy are insufficient; specialized evaluation metrics and benchmarks like MLPerf become essential for optimizing AI systems across diverse deployment scenarios. Source: [@duarte2022fastml].","Log-scale scatter plot of data rate versus computation time, showing scientific applications from LHC sensors at 10^14 B/s and nanoseconds to mobile devices at 10^4 B/s and seconds."
"Benchmarking","11.11","fig-imagenet-challenge","ImageNet Challenge Progression: Neural networks have reduced error rates from 28.2% in 2010 to 3.57% by 2015, highlighting the impact of architectural advancements on classification accuracy. These milestones establish the baselines against which compression techniques are evaluated.","Line graph showing ImageNet top-5 error decreasing from 28.2% in 2010 to 3.57% in 2015, with model labels marking AlexNet, ZFNet, VGGNet, GoogleNet, and ResNet milestones."
"Benchmarking","11.12","fig-model-vs-data","**Development Paradigms**: Model-centric AI prioritizes architectural innovation with fixed datasets, while data-centric AI systematically improves dataset quality (annotations, diversity, and bias) with consistent model architectures to achieve performance gains. Modern research indicates that strategic data enhancement often yields greater improvements than solely refining model complexity.","Side-by-side diagrams: model-centric AI shows data cylinders feeding CPU with feedback loop to model, data-centric AI shows feedback loop to data instead. Double arrow indicates complementary approaches."
"Benchmarking","11.13","fig-dataset-saturation","**Dataset Saturation**: AI systems surpass human performance on five benchmark capabilities: handwriting recognition, speech recognition, image recognition, reading comprehension, and language understanding, each crossing the human baseline between 1998 and 2020. This saturation underscores the need for dynamic benchmarks that remain challenging as model capabilities improve. Source: [@kiela2021dynabench].","Line chart showing five AI capabilities crossing human performance baseline from 1998 to 2020: handwriting, speech, image recognition, reading comprehension, and language understanding."
"Serving","12.1","fig-tail-latency-explosion","The Tail Latency Explosion: Request Latency vs. System Utilization ($\\rho$). While median latency (Blue) remains stable, tail latency (Red, p99) explodes exponentially once utilization passes the 'Knee' at ~70%. This physics-driven behavior dictates that production serving clusters must maintain headroom (40-60% utilization) to guarantee stable response times and avoid queue collapse under stochastic load.","Line plot showing latency growing with utilization. Blue line (Median) is flat then rises. Red line (Tail) curves upward sharply at 70% utilization. Shaded regions indicate 'Safe Zone' and 'Danger Zone'."
"Serving","12.2","fig-intelligence-deflation","Intelligence Deflation: Cost per 1M tokens (USD) over time (Log Scale). The cost of token generation has collapsed by multiple orders of magnitude (2020–2025). Initially driven by OpenAI's GPT series, the market has entered a phase of intense price competition with entrants like Anthropic (Claude), Google (Gemini), and DeepSeek pushing costs below $0.10/million tokens. This hyper-deflation transforms the economics of automated AI workflows.","Line plot showing token pricing collapsing from $20/M tokens in 2020 to <$0.10/M tokens in 2025. Log scale highlights the deflationary trend with models from OpenAI, Anthropic, Google, and DeepSeek."
"Serving","12.3","fig-serving-inference-pipeline","**The Inference Pipeline**: ML serving systems transform raw inputs into final outputs through sequential stages: preprocessing, neural network computation, and postprocessing. The neural network represents just one component; preprocessing and postprocessing rely on traditional computing and often dominate total latency in optimized systems.","Flow diagram showing six connected boxes: Raw Input, Preprocessing, Neural Network, Raw Output, Postprocessing, Final Output. Preprocessing and postprocessing are labeled Traditional Computing; neural network is labeled Deep Learning."
"Serving","12.4","fig-server-anatomy","**Inference Server Anatomy**: A modern inference server decouples network handling from accelerator execution through a staged pipeline. Each stage isolates a concern, from absorbing bursty traffic to forming efficient batches, so the hardware accelerator stays highly utilized despite irregular arrival patterns.","Flowchart showing 6-stage inference server pipeline: Client to Network Ingress to Request Queue (cylinder) to Dynamic Batcher, then down to Inference Runner to Accelerator. Arrows connect stages sequentially."
"Serving","12.5","fig-serving-pipeline-timing","**Request Pipelining**: Pipelining hides latency by overlapping independent operations across different hardware resources. In pipelined execution (B), the CPU processes the next request's data while the GPU executes the current request's inference. This increases the GPU duty cycle toward 100%, effectively doubling or tripling throughput on the same hardware without changing the model.","Two timing diagrams. A (Serial): alternating CPU preprocessing, GPU inference, and idle blocks in sequence. B (Pipelined): two parallel rows where CPU preprocessing overlaps with GPU inference, eliminating idle time."
"Serving","12.6","fig-throughput-latency-knee","The Throughput-Latency Knee. Batch Size vs. Throughput (Blue) and Latency (Orange). Throughput increases with batch size as hardware utilization improves, but eventually saturates. Latency remains relatively flat (hidden by parallel resources) until the 'Knee,' after which it spikes linearly due to queuing. The optimal operating point lies just before this spike.","Dual-axis line chart. Blue line (Throughput) rises and plateaus. Orange line (Latency) stays low then spikes upward. A vertical line marks the optimal point where throughput is high before latency explodes."
"Serving","12.7","fig-kv-cache-growth","The KV-Cache Explosion: Memory usage vs. Context Length for a 70B parameter model. The linear growth of the Key-Value cache (storing attention history) quickly consumes available GPU memory (red dashed line). For batch size 32 (purple), the system hits the 'OOM Zone' at just 8k context length, forcing a trade-off between batch size (throughput) and context window (capability).","Line chart showing memory usage increasing linearly with context length. Multiple lines for different batch sizes. Red dashed horizontal line marks GPU memory limit. Purple line for batch 32 crosses into OOM zone at 8k context."
"Ops","13.1","fig-mlops-diagram","**Iterative MLOps Loop.** MLOps extends DevOps principles to manage the unique challenges of machine learning systems, including data versioning, model retraining, and continuous monitoring. The iterative workflow encompasses data engineering, model development, and reliable deployment for sustained performance in production.","Infinity-loop diagram with three phases. Design phase: requirements, use-case prioritization, data availability. Model Development: data engineering, model engineering, testing. Operations: deployment, CI/CD pipeline, monitoring and triggering."
"Ops","13.2","fig-technical-debt","**Hidden Infrastructure of ML Systems.** Most engineering effort in a typical machine learning system concentrates on components surrounding the model itself: data collection, feature engineering, and system configuration rather than the model code. The distribution reveals the operational challenges and potential for technical debt arising from these often-overlooked surrounding components. Source: [@sculley2015hidden].","Hub-and-spoke diagram with ML system at center. Ten surrounding components connected by arrows: data collection, verification, feature extraction, configuration, resource management, serving infrastructure, monitoring, analysis tools, and ML code."
"Ops","13.3","fig-technical-debt-taxonomy","**ML Technical Debt Taxonomy.** Machine learning systems accumulate distinct forms of technical debt from data dependencies, model interactions, and evolving requirements. Six primary debt patterns radiate from a central hub: boundary erosion undermines modularity, correction cascades propagate fixes through dependencies, feedback loops create hidden coupling, while data, configuration, and pipeline debt reflect poorly managed artifacts and workflows.","Hub-and-spoke diagram with Hidden Technical Debt at center. Six debt categories radiate outward: Configuration Debt, Feedback Loops, Data Debt, Pipeline Debt, Correction Cascades, and Boundary Erosion, each annotated with specific failure patterns."
"Ops","13.4","fig-correction-cascades-flowchart","**Correction Cascades**: Iterative refinements in ML systems often trigger dependent fixes across the workflow, propagating from initial adjustments through data, model, and deployment stages. Color-coded arcs represent corrective actions stemming from sources of instability, while red arrows and the dotted line indicate escalating revisions, potentially requiring a full system restart.","Timeline diagram with seven ML stages from problem statement to deployment. Color-coded arcs show correction cascades: red for domain expertise gaps, blue for real-world brittleness, orange for poor documentation. Dashed arrows indicate restarts."
"Ops","13.5","fig-ops-layers","**MLOps Stack Layers.** Five tiers organize the ML system stack: ML Models at the top, followed by Frameworks, Orchestration, Infrastructure, and Hardware. MLOps spans orchestration tasks (data management through model serving) and infrastructure tasks (job scheduling through monitoring), enabling automation, reproducibility, and scalable deployment.","Layered architecture diagram. Top row: ML Models, Frameworks, Orchestration, Infrastructure, Hardware. MLOps section spans orchestration tasks (data management through model serving) and infrastructure tasks (job scheduling through monitoring)."
"Ops","13.6","fig-ops-cicd","**ML CI/CD Pipeline.** The pipeline begins with dataset and feature repositories, flows through data validation, transformation, training, evaluation, and model registration stages, then deploys to production. Retraining triggers initiate the cycle automatically, while metadata and artifact repositories ensure reproducibility and governance. Source: HarvardX.","Pipeline diagram showing continuous training workflow. Central box contains data validation, transformation, training, evaluation, and registration stages. Three repositories connect: dataset and feature, metadata and artifact, model."
"Ops","13.7","fig-data-drift","**Data Drift Impact**: Declining model performance over time results from data drift, where the characteristics of production data diverge from the training dataset. Monitoring key metrics longitudinally allows MLOps engineers to detect this drift and trigger model retraining or data pipeline adjustments to maintain accuracy.","Three-panel visualization over time. Top: incoming data samples coded green or orange. Middle: feature distribution shifting from online to offline sales channel. Bottom: line graph showing model accuracy declining as distribution shifts increase."
"Ops","13.8","fig-rotting-asset-curve","The Rotting Asset Curve: Model Accuracy vs. Time (Days) showing the impact of statistical drift. Unlike traditional software that remains static unless modified, ML models decay as the world changes. Periodic retraining (sawtooth) and triggered retraining (green) are the primary engineering responses to prevent this silent failure. Monitoring is the mechanism that transforms model 'decay' into a manageable maintenance schedule.","Line plot showing accuracy decaying over time. Sawtooth pattern shows periodic retraining returning accuracy to baseline. Green line shows triggered retraining maintaining higher accuracy bounds."
"Ops","13.9","fig-business-cost-curve","The Business Cost Curve. Expected Cost vs. Classification Threshold. Technical metrics like ROC curves hide the economic reality: errors have different costs. In this fraud detection scenario, a False Negative (missed fraud) costs $1000, while a False Positive (blocked user) costs $10. The optimal threshold ($T=0.85$) is shifted far to the right to minimize total cost, even if it reduces aggregate accuracy. MLOps is the discipline of tuning this threshold dynamically as costs change.","Line chart showing Expected Cost versus Classification Threshold from 0 to 1. Three curves are plotted: False Positive cost decreasing from left to right, False Negative cost increasing from left to right, and Total Cost as their sum forming a U-shape. The optimal threshold is marked at T=0.85 where Total Cost is minimized, shifted right due to the asymmetric cost ratio of $1000 for missed fraud versus $10 for blocked users."
"Ops","13.10","fig-uptime-iceberg","**Uptime Dependency Stack.** An iceberg visualization where visible service uptime floats above the waterline, supported by hidden threats below: model accuracy degradation, data drift, concept drift, broken pipelines, schema changes, model bias, data outages, and underperforming segments. Labels group these threats into data health, model health, and service health categories.","Iceberg diagram with uptime visible above waterline. Hidden below: model accuracy, data drift, concept drift, broken pipelines, schema changes, model bias, data outages, underperforming segments. Labels indicate data, model, and service health."
"Ops","13.11","fig-clinaiops","**ClinAIOps Feedback Loops**: The cyclical framework coordinates data flow between patients, clinicians, and AI systems to support continuous model improvement and safe clinical integration. These interconnected loops enable iterative refinement of AI models based on real-world performance and clinical feedback, fostering trust and accountability in healthcare applications. Source: [@chen2023framework].","Circular diagram with three nodes: patient, clinician, and AI system. Arrows form cyclic flow: patient provides monitoring data, clinician sets therapy regimen, AI generates alerts and recommendations. Inner and outer loops show feedback pathways."
"Ops","13.12","fig-interactive-loop","**Hypertension Management Loops.** Three feedback loops operate in parallel: the patient-AI loop enables bounded self-management through blood pressure monitoring and titration recommendations; the clinician-AI loop provides oversight via trend summaries and clinical risk alerts; and the patient-clinician loop shifts appointments toward therapy trends and lifestyle modifiers. Source: [@chen2023framework].","Three-panel diagram showing ClinAIOps loops. Patient-AI loop: patient monitors blood pressure, AI recommends titrations. Clinician-AI loop: clinician sets limits, AI sends alerts. Patient-clinician loop: both discuss therapy trends and modifiers."
"Responsible Engr","14.1","fig-fairness-frontier","The Fairness-Accuracy Pareto Frontier. Model Accuracy vs. Demographic Disparity. Point A represents unconstrained optimization (maximum accuracy, high disparity). Point C represents strict equality constraints (zero disparity, significant accuracy drop). Point B is the 'Sweet Spot' where engineers can often reduce disparity by 80% while sacrificing less than 1% of aggregate accuracy. Responsible engineering is the practice of finding and implementing Point B.","Curve showing trade-off between Accuracy (y-axis) and Disparity (x-axis). Point A is top-right (high acc, high disparity). Point C is left (low disparity, lower acc). Point B is top-left (high acc, low disparity), showing the optimal trade-off."
"Responsible Engr","14.2","fig-governance-layers","Responsible AI Governance Layers. Nested governance structures surround engineering practice. At the center, engineering teams implement technical safeguards. Successive layers represent organizational safety culture, industry certification and external review, and government regulation. Technical excellence at the center enables compliance with requirements flowing inward from outer layers.","Nested oval diagram showing governance layers from innermost to outermost: Team (reliable systems, software engineering), Organization (safety culture, organizational design), Industry (trustworthy certification, external reviews), and Government Regulation."
"Responsible Engr","14.3","fig-fairness-threshold","Threshold Effects on Subgroup Outcomes. A single classification threshold (vertical lines) applied to two subgroups with different score distributions produces disparate outcomes. Circles represent positive outcomes (loan repayment), crosses represent negative outcomes (default). The 75% threshold approves most of Subgroup A but rejects most of Subgroup B, even when qualified individuals exist in both groups. The 81.25% threshold shows how threshold adjustment changes the fairness-accuracy tradeoff. This visualization explains why aggregate accuracy can mask severe subgroup disparities.","Diagram showing two subgroups A and B with different score distributions. Vertical threshold lines at 75% and 81.25% show how the same threshold produces different approval rates for each group."
"Responsible Engr","14.4","fig-interpretability-spectrum","Model Interpretability Spectrum. A horizontal spectrum arranges model architectures from most interpretable on the left (decision trees, linear regression, logistic regression) to least interpretable on the right (random forests, neural networks, convolutional neural networks). Models on the left allow direct inspection of decision logic, while those on the right require post-hoc explanation techniques such as LIME or SHAP. High-stakes regulatory requirements may constrain model selection toward the interpretable end of this spectrum.","Horizontal spectrum showing model types from more interpretable (decision trees, linear regression, logistic regression) to less interpretable (random forest, neural network, convolutional neural network)."
"Responsible Engr","14.5","fig-data-governance-pillars","**Data Governance Pillars**: Robust data governance establishes ethical and reliable machine learning systems by prioritizing privacy, fairness, transparency, and accountability throughout the data lifecycle. These interconnected pillars address unique challenges in ML workflows, ensuring responsible data usage and auditable decision-making processes.","Central stacked database icon surrounded by four governance elements: privacy shield, security lock, compliance checklist, and transparency document. Gear icons show interconnections between all elements."
"Responsible Engr","14.6","fig-data-card","**Data Governance Documentation**: Data cards standardize critical dataset information, enabling transparency and accountability required for regulatory compliance with laws like GDPR and HIPAA. By providing a structured overview of dataset characteristics, intended uses, and potential risks, data cards facilitate responsible AI practices and support data subject rights.","Sample data card template showing structured fields: dataset name and description at top, authorship and funding details in middle sections, and intended uses with potential risks at bottom."
"Conclusion","15.1","fig-invariants-cycle","**The Cycle of ML Systems (The 12 Invariants)**: The complete systems engineering lifecycle. The meta-principle of *Conservation of Complexity* (center) unifies the process: complexity is neither created nor destroyed, only shifted between Data, Model, Hardware, and Operations. Each transition is governed by specific quantitative invariants that constrain valid engineering decisions.","Circular diagram with four phases: Foundations (Data) in green, Build (Model) in blue, Optimize (Hardware) in orange, and Deploy (Operations) in violet. Arrows connect each phase in a cycle, with the 12 invariants labeled on each transition. Conservation of Complexity is shown in the center as a dashed circle."
"Appendix Algorithm","A1.1","fig-broadcasting-rules","**Tensor Broadcasting Rules**: Two tensors are compatible if, starting from the trailing (rightmost) dimension, the dimensions are equal or one of them is 1. Dimensions of size 1 are 'stretched' to match the other tensor. This stretching is a virtual operation that modifies strides without allocating new memory.","Diagram showing two tensors (3,1) and (1,4) expanding to a shared (3,4) grid."
"Appendix Algorithm","A1.2","fig-backprop-graph","**Backpropagation Computational Graph**: A two-layer network showing the forward pass (black arrows) and backward pass (red dashed arrows). Each node caches values during the forward pass that are reused during the backward pass.","A computational graph with four nodes labeled x, h, y, and L connected left to right. Solid black arrows show the forward pass with weights W1 and W2. Dashed red arrows curve backward showing gradient flow with partial derivative notation."
"Appendix Data","A4.1","fig-row-vs-col","**Storage Layouts**: Row-oriented formats pack data together by record (good for transactions). Column-oriented formats pack data by feature (good for analytics).","Diagram contrasting Row Store vs Column Store. Row store shows Record 1 [ID, Name, Age] followed by Record 2. Column store shows Column 1 [ID1, ID2...] followed by Column 2 [Name1, Name2...]."
"Appendix Machine","A5.1","fig-roofline","**The Roofline Model**: Performance ceiling for a hypothetical accelerator. The sloped line represents memory bandwidth limits; the horizontal line represents peak compute. Every workload can be plotted on this diagram to determine its optimization strategy.","A plot with arithmetic intensity on the x-axis and performance on the y-axis. Two lines form a roofline shape: a diagonal line rising from the origin labeled Memory Bound, and a horizontal line labeled Compute Bound. They meet at the Ridge Point."
"Appendix Machine","A5.2","fig-memory-hierarchy","**The Memory Hierarchy**: Performance depends on data proximity. Accessing HBM is ~100x slower than registers; accessing SSD is ~100,000x slower.","Pyramid showing Registers at top, followed by Cache, HBM/DRAM, and Storage at bottom."
"Appendix Machine","A5.3","fig-float-formats","**Numerical Format Bit Layouts**: A visual comparison of bit allocations. Note how **BF16** (Brain Float 16) preserves the 8-bit exponent of **FP32**, ensuring the same dynamic range for training stability. **FP16** trades range for precision, often requiring loss scaling to prevent underflow.","Stacked horizontal bars showing bit breakdown. FP32: 1 Sign, 8 Exp, 23 Mantissa. BF16: 1 Sign, 8 Exp, 7 Mantissa. FP16: 1 Sign, 5 Exp, 10 Mantissa. INT8: 8 Integer bits."