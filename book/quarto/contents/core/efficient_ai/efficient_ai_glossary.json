{
  "metadata": {
    "chapter": "efficient_ai",
    "version": "1.0.0",
    "generated": "2025-09-15T14:08:03.494969",
    "total_terms": 48,
    "standardized": true,
    "last_updated": "2025-09-15T15:01:37.285910"
  },
  "terms": [
    {
      "term": "active learning",
      "definition": "Iteratively selecting the most informative samples for labeling to maximize learning efficiency, achieving target performance with 50-90% less labeled data compared to random sampling strategies.",
      "chapter_source": "efficient_ai",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "algorithmic efficiency",
      "definition": "The design and optimization of algorithms to maximize performance within given resource constraints, focusing on techniques like model compression, architectural optimization, and algorithmic refinement.",
      "chapter_source": "efficient_ai",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "automl",
      "definition": "Automated machine learning that enables exploration of different model architectures, hyperparameter configurations, and feature engineering techniques to identify optimal performance-efficiency balances.",
      "chapter_source": "efficient_ai",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "autoregressive",
      "definition": "Models that generate sequences by predicting the next element based on previous elements, such as GPT models that generate text one token at a time.",
      "chapter_source": "efficient_ai",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "cifar10",
      "definition": "Canadian Institute for Advanced Research dataset with 60,000 32\u00d732 color images across 10 classes, serving as a standard benchmark in computer vision despite its small image size by modern standards.",
      "chapter_source": "efficient_ai",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "co design",
      "definition": "Holistic approach where model architectures, hardware platforms, and data pipelines are designed in tandem to work seamlessly together, mitigating trade-offs through end-to-end optimization.",
      "chapter_source": "efficient_ai",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "compute efficiency",
      "definition": "The optimization of computational resources including hardware and energy utilization to maximize processing speed while minimizing resource consumption during training and deployment.",
      "chapter_source": "efficient_ai",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "curriculum learning",
      "definition": "Training strategy where models learn from easy examples before progressing to harder ones, mimicking human education and improving convergence speed by 25-50%.",
      "chapter_source": "efficient_ai",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "data augmentation",
      "definition": "Artificially expanding datasets through transformations like rotations, crops, or noise to improve model performance by 5-15% and reduce overfitting when labeled data is scarce.",
      "chapter_source": "efficient_ai",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "data centric ai",
      "definition": "Paradigm shift from model-centric to data-centric development that focuses on systematically improving data quality rather than just model architecture, often yielding greater performance gains.",
      "chapter_source": "efficient_ai",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "data efficiency",
      "definition": "Optimizing the amount and quality of data required to train machine learning models effectively, focusing on maximizing information gained while minimizing required data volume.",
      "chapter_source": "efficient_ai",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "data parallelism",
      "definition": "Training method where the same model runs on multiple processors with different data batches, enabling massive scale training like GPT-3 across thousands of GPUs.",
      "chapter_source": "efficient_ai",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "efficientnet",
      "definition": "Neural network architecture achieving state-of-the-art accuracy with 10\u00d7 fewer parameters than previous models, demonstrating that clever design can dramatically improve efficiency.",
      "chapter_source": "efficient_ai",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "ensemble methods",
      "definition": "Techniques combining multiple models to improve performance, like Random Forest and Gradient Boosting, which dominated machine learning competitions before deep learning.",
      "chapter_source": "efficient_ai",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "flops",
      "definition": "Floating-Point Operations Per Second, the standard measure of computational throughput used to quantify training and inference computational requirements.",
      "chapter_source": "efficient_ai",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "foundation models",
      "definition": "Large-scale, general-purpose AI models trained on broad data that can be adapted for many tasks, including models like GPT-3, BERT, and DALL-E with billions of parameters.",
      "chapter_source": "efficient_ai",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "gpt3",
      "definition": "OpenAI's 175-billion parameter language model released in 2020, costing an estimated $4.6 million to train and consuming approximately 1,287 MWh of electricity.",
      "chapter_source": "efficient_ai",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "gpt4",
      "definition": "OpenAI's most advanced language model as of 2023, reportedly using a mixture-of-experts architecture with approximately 1.8 trillion parameters and training costs exceeding $100 million.",
      "chapter_source": "efficient_ai",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "imagenet",
      "definition": "Large-scale visual recognition dataset with 14+ million images across 20,000+ categories that drove computer vision breakthroughs through annual competitions from 2010-2017.",
      "chapter_source": "efficient_ai",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "knowledge distillation",
      "definition": "Training small \"student\" models to mimic large \"teacher\" models, such as DistilBERT achieving 97% of BERT's performance with 40% fewer parameters and 60% faster inference.",
      "chapter_source": "efficient_ai",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "mnist",
      "definition": "Modified National Institute of Standards and Technology database of handwritten digits containing 70,000 28\u00d728 pixel images, serving as the \"Hello World\" of computer vision.",
      "chapter_source": "efficient_ai",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "mobilenet",
      "definition": "Efficient neural network architecture using depthwise separable convolutions, achieving approximately 50\u00d7 fewer parameters than traditional models while enabling smartphone deployment.",
      "chapter_source": "efficient_ai",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "model parallelism",
      "definition": "Distributing model components across multiple processors, contrasting with data parallelism and necessary for models like GPT-3 whose parameters exceed single GPU memory capacity.",
      "chapter_source": "efficient_ai",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "moores law",
      "definition": "Intel co-founder Gordon Moore's 1965 observation that transistor density doubles every 2 years, with hardware improvements following this trend while AI algorithmic efficiency improved 44\u00d7 in 7 years.",
      "chapter_source": "efficient_ai",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "neural architecture search",
      "definition": "Automated design of model architectures tailored to specific hardware or deployment scenarios, evaluating architectural possibilities to maximize performance while minimizing computational demands.",
      "chapter_source": "efficient_ai",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "parameter efficient finetuning",
      "definition": "Methods like LoRA and Adapters that update less than 1% of model parameters while achieving full fine-tuning performance, reducing memory requirements from gigabytes to megabytes.",
      "chapter_source": "efficient_ai",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "perplexity",
      "definition": "Measurement of how well a language model predicts text, calculated as 2^(cross-entropy loss), with lower values indicating better prediction capability.",
      "chapter_source": "efficient_ai",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "principal component analysis",
      "definition": "Dimensionality reduction technique that identifies the most important directions of variation in data, reducing computational complexity while preserving 90%+ of data variance.",
      "chapter_source": "efficient_ai",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "pruning",
      "definition": "Removing unimportant neural network connections to reduce model size, with techniques achieving 9\u00d7 compression on models like AlexNet without accuracy loss.",
      "chapter_source": "efficient_ai",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "quantization",
      "definition": "Reducing numerical precision from 32-bit floats to 8-bit integers or lower, achieving 4\u00d7 memory reduction and speed improvements while maintaining approximately 99% of original accuracy.",
      "chapter_source": "efficient_ai",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "resnet",
      "definition": "Residual Network architecture enabling training of very deep networks through skip connections, achieving the first superhuman performance on ImageNet with 3.6% error rate.",
      "chapter_source": "efficient_ai",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "scaling laws",
      "definition": "Empirical relationships that quantify the correlation between model performance and training resources, following predictable power-law relationships with model size, dataset size, and compute budget.",
      "chapter_source": "efficient_ai",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "self supervised learning",
      "definition": "Training method where models create their own labels from input data structure, enabling learning from billions of unlabeled examples and revolutionizing NLP and computer vision.",
      "chapter_source": "efficient_ai",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "squeezenet",
      "definition": "Compact CNN architecture achieving AlexNet-level accuracy with 50\u00d7 fewer parameters, demonstrating that clever architecture design can dramatically reduce model size without sacrificing performance.",
      "chapter_source": "efficient_ai",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "stochastic gradient descent",
      "definition": "Optimization algorithm using random data samples that made neural network training practical by reducing memory requirements from full-batch to single-sample updates.",
      "chapter_source": "efficient_ai",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "support vector machines",
      "definition": "Machine learning algorithm using the \"kernel trick\" to find optimal decision boundaries, dominating competitions before deep learning until neural networks gained prominence around 2010.",
      "chapter_source": "efficient_ai",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "system efficiency",
      "definition": "Optimization of machine learning systems across algorithmic, compute, and data efficiency dimensions to minimize computational, memory, and energy demands while maintaining performance.",
      "chapter_source": "efficient_ai",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "test time compute",
      "definition": "Dynamic resource allocation during inference that adjusts computational effort based on task complexity or importance, enabling flexible performance-accuracy trade-offs.",
      "chapter_source": "efficient_ai",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "tinyml",
      "definition": "Machine learning on microcontrollers and edge devices with less than 1KB-1MB memory and less than 1mW power consumption, enabling AI in IoT devices where traditional deployment is impossible.",
      "chapter_source": "efficient_ai",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "tokens",
      "definition": "Individual units of text that language models process, typically words or subword pieces, with modern models like GPT-3 trained on hundreds of billions of tokens.",
      "chapter_source": "efficient_ai",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "tpu",
      "definition": "Tensor Processing Unit, Google's custom silicon designed specifically for machine learning workloads, delivering 275 teraFLOPs with 90% lower energy per operation compared to general-purpose processors.",
      "chapter_source": "efficient_ai",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "transfer learning",
      "definition": "Technique where models pre-trained on large datasets are fine-tuned for specific tasks, enabling high accuracy on new tasks with fewer than 1000 labeled examples versus millions needed from scratch.",
      "chapter_source": "efficient_ai",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "transformer",
      "definition": "Neural network architecture introduced by Vaswani et al. in 2017 that revolutionized natural language processing through attention mechanisms, forming the foundation of models like GPT, BERT, and T5.",
      "chapter_source": "efficient_ai",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "uci machine learning repository",
      "definition": "Established in 1987 by the University of California Irvine, one of the most widely-used resources for machine learning datasets containing over 600 datasets cited in thousands of research papers.",
      "chapter_source": "efficient_ai",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "compute-optimal training",
      "definition": "Training strategies that optimally balance model size and training compute budget according to scaling laws, achieving maximum performance for a given computational budget.",
      "chapter_source": "efficient_ai",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "data scaling regimes",
      "definition": "Different phases of model training where data requirements scale according to predictable patterns, informing decisions about dataset size versus computational investment.",
      "chapter_source": "efficient_ai",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "hardware-software co-design",
      "definition": "Collaborative design methodology where hardware accelerators and software algorithms are jointly optimized to achieve maximum efficiency and performance.",
      "chapter_source": "efficient_ai",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "performance-efficiency scaling",
      "definition": "Mathematical relationships describing how computational efficiency improvements translate to performance gains across different model architectures and training regimes.",
      "chapter_source": "efficient_ai",
      "aliases": [],
      "see_also": []
    }
  ]
}
