{
  "metadata": {
    "chapter": "workflow",
    "version": "1.0.0",
    "generated": "2025-09-15T14:08:03.504254",
    "total_terms": 37,
    "standardized": true,
    "last_updated": "2025-09-15T15:01:37.291780"
  },
  "terms": [
    {
      "term": "a/b testing",
      "definition": "A controlled experimental method for comparing two versions of a system or model by randomly dividing users into groups and measuring performance differences between the variants.",
      "chapter_source": "workflow",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "continuous integration",
      "definition": "A software development practice where code changes are automatically integrated, tested, and validated multiple times per day to detect issues early in the development cycle.",
      "chapter_source": "workflow",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "crisp-dm",
      "definition": "Cross-Industry Standard Process for Data Mining, a structured methodology developed in 1996 that defines six phases for data projects: business understanding, data understanding, data preparation, modeling, evaluation, and deployment.",
      "chapter_source": "workflow",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "data curation",
      "definition": "The process of selecting, organizing, and maintaining high-quality datasets by removing irrelevant information, correcting errors, and ensuring data meets specific standards for machine learning applications.",
      "chapter_source": "workflow",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "data drift",
      "definition": "The phenomenon where the statistical properties of input data change over time, causing machine learning model performance to degrade even when the underlying code remains unchanged.",
      "chapter_source": "workflow",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "data ingestion",
      "definition": "The process of collecting and importing raw data from various sources into a system where it can be stored, processed, and prepared for machine learning applications.",
      "chapter_source": "workflow",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "data validation",
      "definition": "The systematic verification that collected data meets quality standards, is properly formatted, and contains accurate information suitable for machine learning model training and evaluation.",
      "chapter_source": "workflow",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "data versioning",
      "definition": "The practice of tracking and managing different versions of datasets over time, similar to code versioning, to ensure reproducibility and enable rollback to previous data states when needed.",
      "chapter_source": "workflow",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "deployment constraints",
      "definition": "Operational limitations such as hardware resources, network connectivity, regulatory requirements, and integration requirements that influence how machine learning models are implemented in production environments.",
      "chapter_source": "workflow",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "diabetic retinopathy",
      "definition": "A diabetes complication that damages blood vessels in the retina, serving as a leading cause of preventable blindness and a key application area for medical AI screening systems.",
      "chapter_source": "workflow",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "edge deployment",
      "definition": "A deployment strategy where machine learning models run locally on devices at the network edge rather than in centralized cloud servers, reducing latency and enabling operation without constant internet connectivity.",
      "chapter_source": "workflow",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "emergent behaviors",
      "definition": "Unexpected system-wide patterns or characteristics that arise from the interaction of individual components, often becoming apparent only when systems operate at scale or in real-world conditions.",
      "chapter_source": "workflow",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "experiment tracking",
      "definition": "The systematic recording and management of machine learning experiments, including hyperparameters, model versions, training data, and performance metrics, to enable comparison and reproducibility.",
      "chapter_source": "workflow",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "federated learning",
      "definition": "A machine learning approach where models are trained across multiple decentralized devices or servers without centralizing the raw training data, preserving privacy while enabling collaborative learning.",
      "chapter_source": "workflow",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "feedback loops",
      "definition": "Cyclical processes where outputs from later stages of the machine learning lifecycle inform and influence decisions in earlier stages, enabling continuous system improvement and adaptation.",
      "chapter_source": "workflow",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "hyperparameters",
      "definition": "Configuration settings that control the learning process of machine learning algorithms but are not learned from data, such as learning rate, batch size, and network architecture parameters.",
      "chapter_source": "workflow",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "key performance indicators",
      "definition": "Specific, measurable metrics used to evaluate the success and effectiveness of machine learning systems, such as accuracy, precision, recall, latency, and throughput.",
      "chapter_source": "workflow",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "machine learning lifecycle",
      "definition": "A structured, iterative process that encompasses all stages involved in developing, deploying, and maintaining machine learning systems, from problem definition through ongoing monitoring and improvement.",
      "chapter_source": "workflow",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "machine learning operations",
      "definition": "The practice and set of tools focused on operationalizing machine learning models through automation, monitoring, and management of the entire ML pipeline from development to production.",
      "chapter_source": "workflow",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "model deployment",
      "definition": "The process of integrating trained machine learning models into production systems where they can make predictions on new data and provide value to end users.",
      "chapter_source": "workflow",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "model drift",
      "definition": "The degradation of machine learning model performance over time due to changes in data patterns, user behavior, or environmental conditions that differ from the original training conditions.",
      "chapter_source": "workflow",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "model evaluation",
      "definition": "The systematic assessment of machine learning model performance using various metrics and validation techniques to determine whether the model meets requirements and is ready for deployment.",
      "chapter_source": "workflow",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "model optimization",
      "definition": "The process of improving machine learning models for better performance, reduced resource consumption, or compatibility with deployment constraints while maintaining acceptable accuracy levels.",
      "chapter_source": "workflow",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "model training",
      "definition": "The process of using machine learning algorithms to learn patterns from training data, adjusting model parameters to minimize prediction errors and create a functional predictive system.",
      "chapter_source": "workflow",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "model validation",
      "definition": "The process of testing machine learning models on independent datasets to assess their generalization ability and ensure they perform reliably on unseen data.",
      "chapter_source": "workflow",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "model versioning",
      "definition": "The systematic tracking and management of different versions of machine learning models, including their parameters, training data, and performance metrics, to enable comparison and rollback capabilities.",
      "chapter_source": "workflow",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "monitoring",
      "definition": "The continuous observation and measurement of machine learning system performance, data quality, and operational metrics in production to detect issues and trigger maintenance actions.",
      "chapter_source": "workflow",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "performance insights",
      "definition": "Analytical observations derived from monitoring production machine learning systems that reveal opportunities for improvement in model accuracy, system efficiency, or user experience.",
      "chapter_source": "workflow",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "problem definition",
      "definition": "The initial stage of machine learning development that involves clearly specifying objectives, constraints, success metrics, and operational requirements to guide all subsequent development decisions.",
      "chapter_source": "workflow",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "retinal fundus photographs",
      "definition": "Medical images of the interior surface of the eye, including the retina, optic disc, and blood vessels, commonly used for diagnosing eye diseases and training medical AI systems.",
      "chapter_source": "workflow",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "scalability",
      "definition": "The ability of machine learning systems to handle increasing amounts of data, users, or computational demands without significant degradation in performance or user experience.",
      "chapter_source": "workflow",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "systems thinking",
      "definition": "A holistic approach to analysis that views machine learning systems as interconnected wholes rather than individual components, emphasizing relationships and dependencies between different lifecycle stages.",
      "chapter_source": "workflow",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "transfer learning",
      "definition": "A machine learning technique where models pre-trained on large datasets are adapted for specific tasks, dramatically reducing training time and data requirements while achieving high performance.",
      "chapter_source": "workflow",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "validation issues",
      "definition": "Problems identified during model testing that indicate poor performance, overfitting, data quality problems, or other issues that must be resolved before deployment.",
      "chapter_source": "workflow",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "workflow orchestration",
      "definition": "Automated coordination and management of complex ML pipeline sequences, ensuring proper execution order, dependency management, and error handling across distributed systems.",
      "chapter_source": "workflow",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "stage-specific metrics",
      "definition": "Performance indicators tailored to individual lifecycle phases, such as data quality metrics during preparation, training convergence during modeling, and latency metrics during deployment.",
      "chapter_source": "workflow",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "lifecycle coherence",
      "definition": "The principle that all stages of ML development should align with overall system objectives, maintaining consistency in data handling, model architecture, and evaluation criteria.",
      "chapter_source": "workflow",
      "aliases": [],
      "see_also": []
    }
  ]
}
