{
  "metadata": {
    "chapter": "ml_systems",
    "version": "1.0.0",
    "generated": "2025-09-15T14:08:03.499150",
    "total_terms": 27,
    "standardized": true,
    "last_updated": "2025-09-15T15:01:37.288351"
  },
  "terms": [
    {
      "term": "artificial intelligence",
      "definition": "The simulation of human intelligence processes by machines, particularly computer systems, encompassing learning, reasoning, and self-correction capabilities.",
      "chapter_source": "ml_systems",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "machine learning",
      "definition": "The deployment of machine learning models on centralized computing infrastructures such as data centers, offering scalability and computational capacity for large-scale datasets and complex models.",
      "chapter_source": "ml_systems",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "data center",
      "definition": "A facility that houses computer systems and associated components such as telecommunications and storage systems, typically containing thousands of servers for cloud computing operations.",
      "chapter_source": "ml_systems",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "distributed intelligence",
      "definition": "The placement of computational capabilities across multiple devices and locations rather than relying on a single centralized system, enabling local processing and decision-making.",
      "chapter_source": "ml_systems",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "edge computing",
      "definition": "A distributed computing paradigm that brings computation and data storage closer to the sources of data, reducing latency and bandwidth usage.",
      "chapter_source": "ml_systems",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "embedded systems",
      "definition": "Computer systems with dedicated functions within larger mechanical or electrical systems, typically designed for specific tasks with real-time computing constraints.",
      "chapter_source": "ml_systems",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "federated learning",
      "definition": "A machine learning approach that trains algorithms across decentralized edge devices or servers holding local data samples, without exchanging the raw data.",
      "chapter_source": "ml_systems",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "foundation model",
      "definition": "Large-scale machine learning models trained on broad data that can be adapted to a wide range of downstream tasks, serving as a base for specialized applications.",
      "chapter_source": "ml_systems",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "gpu",
      "definition": "Graphics Processing Unit, a specialized electronic circuit designed to rapidly manipulate and alter memory to accelerate the creation of images and parallel processing tasks.",
      "chapter_source": "ml_systems",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "hierarchical processing",
      "definition": "A multi-tier system architecture where data and intelligence flow between different levels of the computing stack, from sensors to edge devices to cloud systems.",
      "chapter_source": "ml_systems",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "hybrid machine learning",
      "definition": "The integration of multiple ML paradigms such as cloud, edge, mobile, and tiny ML to form unified distributed systems that leverage complementary strengths.",
      "chapter_source": "ml_systems",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "hyperscale data center",
      "definition": "Large-scale data center facilities containing thousands of servers and covering extensive floor space, designed to efficiently support massive computing workloads.",
      "chapter_source": "ml_systems",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "batch inference",
      "definition": "The process of using a trained machine learning model to make predictions or decisions on new, previously unseen data.",
      "chapter_source": "ml_systems",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "internet of things",
      "definition": "A network of physical objects embedded with sensors, software, and other technologies that connect and exchange data with other devices and systems over the internet.",
      "chapter_source": "ml_systems",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "latency",
      "definition": "The time delay between a request for data and the delivery of that data, critical in real-time applications where immediate responses are required.",
      "chapter_source": "ml_systems",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "microcontroller",
      "definition": "A small computer on a single integrated circuit containing a processor core, memory, and programmable input/output peripherals, commonly used in embedded systems.",
      "chapter_source": "ml_systems",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "mobile machine learning",
      "definition": "The execution of machine learning models directly on portable, battery-powered devices like smartphones and tablets, enabling personalized and responsive applications.",
      "chapter_source": "ml_systems",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "model compression",
      "definition": "Techniques used to reduce the size and computational requirements of machine learning models while preserving accuracy, enabling deployment on resource-constrained devices.",
      "chapter_source": "ml_systems",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "model quantization",
      "definition": "The process of reducing the precision of numerical representations in machine learning models, typically from 32-bit to 8-bit integers, to decrease model size and increase inference speed.",
      "chapter_source": "ml_systems",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "neural processing unit (npu)",
      "definition": "Specialized processors designed specifically for accelerating neural network operations and machine learning computations, optimized for parallel processing of AI workloads.",
      "chapter_source": "ml_systems",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "real-time processing",
      "definition": "The processing of data as it becomes available, with guaranteed response times that meet strict timing constraints for immediate decision-making.",
      "chapter_source": "ml_systems",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "system on chip",
      "definition": "An integrated circuit that incorporates most or all components of a computer or electronic system, including CPU, GPU, memory, and specialized processors on a single chip.",
      "chapter_source": "ml_systems",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "tensor processing unit (tpu)",
      "definition": "Google's custom application-specific integrated circuit designed specifically for neural network machine learning, optimized for TensorFlow operations.",
      "chapter_source": "ml_systems",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "tiny machine learning",
      "definition": "The execution of machine learning models on ultra-constrained devices such as microcontrollers and sensors, operating in the milliwatt to sub-watt power range.",
      "chapter_source": "ml_systems",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "training",
      "definition": "The process of teaching a machine learning algorithm to make accurate predictions by feeding it large amounts of labeled data and allowing it to learn patterns.",
      "chapter_source": "ml_systems",
      "aliases": [],
      "see_also": []
    }
  ]
}
