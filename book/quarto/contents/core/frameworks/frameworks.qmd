---
bibliography: frameworks.bib
quiz: footnote_context_quizzes.json
concepts: frameworks_concepts.yml
glossary: frameworks_glossary.json
crossrefs: frameworks_xrefs.json
---

# AI Frameworks {#sec-ai-frameworks}

::: {layout-narrow}
::: {.column-margin}
*DALLÂ·E 3 Prompt: Illustration in a rectangular format, designed for a professional textbook, where the content spans the entire width. The vibrant chart represents training and inference frameworks for ML. Icons for TensorFlow, Keras, PyTorch, ONNX, and TensorRT are spread out, filling the entire horizontal space, and aligned vertically. Each icon is accompanied by brief annotations detailing their features. The lively colors like blues, greens, and oranges highlight the icons and sections against a soft gradient background. The distinction between training and inference frameworks is accentuated through color-coded sections, with clean lines and modern typography maintaining clarity and focus.*
:::

\noindent
![](images/png/cover_ml_frameworks.png)

:::

## Purpose {.unnumbered}

**DLACZEGO frameworki istniejÄ…:** Bez nich musiaÅ‚byÅ› reimplementowaÄ‡ backprop, kernele GPU i distributed training dla kaÅ¼dego projektu. Frameworki abstrahujÄ… hardware, umoÅ¼liwiajÄ… automatic differentiation i zaÅ‚atwiajÄ… inÅ¼ynierski baÅ‚agan, Å¼ebyÅ› mÃ³gÅ‚ skupiÄ‡ siÄ™ na modelach.

**Czego siÄ™ nauczysz:**
- Jak computational graphs faktycznie dziaÅ‚ajÄ… (static vs dynamic)
- Dlaczego PyTorch wybraÅ‚ dynamic graphs a TensorFlow static (potem siÄ™ przerzucili ðŸ™„)
- Implementacja automatic differentiation
- Hardware abstraction layers
- WybÃ³r frameworka (przestaÅ„ Å›lepo kopiowaÄ‡ PyTorch do wszystkiego)

::: {.callout-tip title="Learning Objectives"}

- ZrozumieÄ‡ reprezentacjÄ™ computational graph i execution models
- WyjaÅ›niÄ‡ automatic differentiation (forward vs reverse mode)
- PorÃ³wnaÄ‡ architektury: TensorFlow, PyTorch, JAX
- WybieraÄ‡ odpowiednie frameworki bazujÄ…c na deployment constraints
- IdentyfikowaÄ‡ anti-patterny przy wyborze frameworka

:::

## Framework Abstraction and Necessity {#sec-ai-frameworks-framework-abstraction-necessity-48f9}

**Problem:** Trenowanie wspÃ³Å‚czesnego transformera wymaga koordynacji miliardÃ³w FLOPÃ³w na rozproszonych GPU, zarzÄ…dzajÄ…c pamiÄ™ciÄ…, precyzjÄ… numerycznÄ… i obliczaniem gradientÃ³w.

**Bez frameworkÃ³w:** Implementacja backprop dla 3-warstwowego MLP rÄ™cznie = setki linii kodu z rachunkiem rÃ³Å¼niczkowym i macierzami, ktÃ³re prawdopodobnie sÄ… bÅ‚Ä™dne.

**Z frameworkami:** `loss.backward()`

To caÅ‚a propozycja wartoÅ›ci.

:::{.callout-definition title="Machine Learning Frameworks"}
***Machine Learning Frameworks*** abstrahujÄ… infrastrukturÄ™ obliczeniowÄ… przez standardowe API do development, treningu i deployment modeli. DostarczajÄ… computational graphs, automatic differentiation i akceleracjÄ™ sprzÄ™towÄ… bez dotykania CUDA.
:::

**Podstawowe moÅ¼liwoÅ›ci:**
1. **Automatic differentiation** - Automatyczne obliczanie gradientÃ³w przez chain rule
2. **Hardware abstraction** - Ten sam kod dziaÅ‚a na CPU/GPU/TPU
3. **Optimization** - Kernel fusion, memory planning, graph rewrites
4. **Distribution** - Koordynacja treningu multi-GPU/multi-node

Frameworki ewoluowaÅ‚y: BLAS (1979) â†’ NumPy (2006) â†’ Theano (2007) â†’ TensorFlow/PyTorch (2015-2016). KaÅ¼da warstwa dodawaÅ‚a abstrakcjÄ™ zachowujÄ…c performance.

## Historical Development Trajectory {#sec-ai-frameworks-historical-development-trajectory-9519}

**Timeline:**
- **1979:** BLAS - zoptymalizowane operacje macierzowe
- **1992:** LAPACK - dekompozycje macierzy, solvery liniowe
- **2006:** NumPy - Python arrays oparte na BLAS/LAPACK
- **2007:** Theano - computational graphs + akceleracja GPU
- **2015:** TensorFlow - static graphs + distributed training
- **2016:** PyTorch - dynamic graphs + focus na research
- **2018:** JAX - functional programming + automatic transformation

### Foundational Mathematical Computing Infrastructure {#sec-ai-frameworks-foundational-mathematical-computing-infrastructure-f41c}

Sieci neuronowe = uÅ‚oÅ¼one mnoÅ¼enia macierzy. BLAS dostarczyÅ‚ podstawowe operacje algebry liniowej (GEMM, GEMV). LAPACK dodaÅ‚ dekompozycje macierzy. NumPy opakowaÅ‚ to w Python arrays.

**Dlaczego towaÅ¼ne:** KaÅ¼dy framework ostatecznie wywoÅ‚uje BLAS/LAPACK (lub zoptymalizowane warianty jak Intel MKL, OpenBLAS). Zrozumienie tego stacku wyjaÅ›nia dlaczego operacje macierzowe sÄ… szybkie, a element-wise czÄ™sto nie.

[^fn-frameworks-blas]: **BLAS (Basic Linear Algebra Subprograms)**: Oryginalnie stworzony w Argonne National Laboratory, BLAS staÅ‚ siÄ™ de facto standardem dla operacji algebry liniowej, z Level 1 (vector-vector), Level 2 (matrix-vector) i Level 3 (matrix-matrix) operacjami, ktÃ³re wciÄ…Å¼ stanowiÄ… fundament kaÅ¼dego wspÃ³Å‚czesnego ML frameworka.

[^fn-lapack]: **LAPACK (Linear Algebra Package)**: NastÄ™pca LINPACK i EISPACK, wprowadziÅ‚ block algorithms drastycznie poprawiajÄ…ce cache efficiency i parallel execution, innowacje ktÃ³re staÅ‚y siÄ™ niezbÄ™dne gdy datasety urosÅ‚y z megabajtÃ³w do terabajtÃ³w.

### Early Machine Learning Platform Development {#sec-ai-frameworks-early-machine-learning-platform-development-3873}

**Scikit-learn (2007):** Abstrakcja algorytmÃ³w ML za `.fit()` API. Dobry dla classical ML, bezuÅ¼yteczny dla deep learning.

**Theano (2007):** Pierwszy framework z:
- Computational graphs (DAG operacji)
- Automatic differentiation
- KompilacjÄ… GPU

Theano byÅ‚ pionierem architektury uÅ¼ywanej przez wspÃ³Å‚czesne frameworki. Development zakoÅ„czony 2017 ale wpÅ‚ynÄ…Å‚ na wszystko po nim.

[^fn-theano]: **Theano**: Nazwany od staroÅ¼ytnej greckiej matematyczki Theano z Krotonu, ten framework byÅ‚ pionierem konceptu symbolicznych wyraÅ¼eÅ„ matematycznych w Pythonie, kÅ‚adÄ…c podwaliny pod kaÅ¼dy wspÃ³Å‚czesny deep learning framework.

[^fn-comp-graphs]: **Computational Graphs**: Po raz pierwszy sformalizowane w literaturze automatic differentiation przez Wengerta (1964), ta reprezentacja staÅ‚a siÄ™ krÄ™gosÅ‚upem wspÃ³Å‚czesnych ML frameworkÃ³w, umoÅ¼liwiajÄ…c forward i reverse-mode differentiation na niespotykanÄ… skalÄ™.

**Torch7 (2002):** Oparty na Lua, immediate execution. WpÅ‚ynÄ…Å‚ na filozofiÄ™ designu PyTorch: developer experience > optymalizacja performance.

[^fn-eager-execution]: **Eager Execution**: Model wykonania gdzie operacje sÄ… ewaluowane natychmiast gdy sÄ… wywoÅ‚ywane, podobnie do standardowego wykonania Pythona. Pionierem byÅ‚ Torch w 2002, to podejÅ›cie priorytetyzuje produktywnoÅ›Ä‡ developera i Å‚atwoÅ›Ä‡ debugowania nad optymalizacjÄ… performance, stajÄ…c siÄ™ domyÅ›lnym trybem w nowoczesnych frameworkach jak PyTorch i TensorFlow 2.x.

### Deep Learning Computational Platform Innovation {#sec-ai-frameworks-deep-learning-computational-platform-innovation-d3db}

**TensorFlow (2015):** Google open-sourcowaÅ‚ nastÄ™pcÄ™ ich wewnÄ™trznego DistBelief. Rewolucyjne features:
- Static computation graphs (definicja caÅ‚ego modelu z gÃ³ry)
- Distributed training przez clustery
- Focus na production deployment
- Agresywna optymalizacja (XLA compiler, graph rewrites)

**Trade-off:** Performance vs usability. Static graphs = szybkie ale bolesny debugging.

[^fn-tensorflow]: **TensorFlow**: Nazwany od tensor operations przepÅ‚ywajÄ…cych przez computational graphs, ten framework zdemokratyzowaÅ‚ distributed machine learning przez open-sourcing wewnÄ™trznego systemu Google DistBelief, natychmiast dajÄ…c badaczom dostÄ™p do infrastruktury ktÃ³ra wczeÅ›niej wymagaÅ‚a masywnych zasobÃ³w korporacyjnych.

[^fn-static-graph]: **Static Computational Graph**: Pre-definiowana struktura obliczeniowa gdzie caÅ‚a architektura modelu jest specyfikowana przed wykonaniem, umoÅ¼liwiajÄ…c globalne optymalizacje i efektywne planowanie pamiÄ™ci. Pionierem byÅ‚ TensorFlow 1.x, to podejÅ›cie poÅ›wiÄ™ca runtime flexibility dla maksymalnej optymalizacji performance, czyniÄ…c go idealnym dla production deployments.

[^fn-kernel-fusion]: **Kernel Fusion**: Technika optymalizacyjna Å‚Ä…czÄ…ca wiele oddzielnych operacji (jak matrix multiplication po ktÃ³rym nastÄ™puje bias addition i activation) w pojedynczy GPU kernel, redukujÄ…c memory bandwidth requirements do 10x i eliminujÄ…c intermediate memory allocations. Ta optymalizacja jest szczegÃ³lnie kluczowa dla zÅ‚oÅ¼onych deep learning modeli z tysiÄ…cami operacji.

[^fn-memory-planning]: **Memory Planning**: Optymalizacja frameworka ktÃ³ra pre-analizuje computational graphs aby okreÅ›liÄ‡ optymalne strategie alokacji pamiÄ™ci, umoÅ¼liwiajÄ…c techniki jak in-place operations i memory reuse patterns ktÃ³re mogÄ… zredukowaÄ‡ peak memory usage o 40-60% podczas treningu.

**PyTorch (2016):** OdpowiedÅº Facebooka. Dynamic computation graphs.
- Define-by-run: graph budowany podczas wykonania
- Pythonic API
- Åatwy debugging (standardowy Python debugger dziaÅ‚a)
- Research-friendly

**Trade-off:** Usability vs performance. Dynamic graphs = flexible ale trudniejsze do optymalizacji.

[^fn-pytorch]: **PyTorch**: Zainspirowany oryginalnym Torch frameworkiem z NYU, PyTorch przyniÃ³sÅ‚ "define-by-run" semantykÄ™ do Pythona, umoÅ¼liwiajÄ…c badaczom modyfikacjÄ™ modeli podczas wykonania, przeÅ‚om ktÃ³ry przyspieszyÅ‚ research czyniÄ…c debugging tak prostym jak uÅ¼ywanie standardowego Python debuggera.

**JAX (2018):** PodejÅ›cie Google Research do functional programming.
- Composable transformations: `grad()`, `jit()`, `vmap()`, `pmap()`
- KompatybilnoÅ›Ä‡ z NumPy API
- Automatyczna wektoryzacja i paralelizacja
- Functional purity (brak side effects)

[^fn-jax]: **JAX**: Oznacza "Just After eXecution" i Å‚Ä…czy NumPy API z functional programming transforms (jit, grad, vmap, pmap), umoÅ¼liwiajÄ…c badaczom pisanie zwiÄ™zÅ‚ego kodu ktÃ³ry automatycznie skaluje na TPU i GPU clusters zachowujÄ…c kompatybilnoÅ›Ä‡ z NumPy.

### Hardware-Driven Framework Architecture Evolution {#sec-ai-frameworks-hardwaredriven-framework-architecture-evolution-2605}

**GPU acceleration (2007):** NVIDIA CUDA umoÅ¼liwiÅ‚ rÃ³wnolegÅ‚e operacje macierzowe na GPU. CPU: 1-2 TFLOPS. GPU (A100): 312 TFLOPS. Dlatego frameworki dbajÄ… o wsparcie GPU.

[^fn-cuda]: **CUDA (Compute Unified Device Architecture)**: Platforma parallel computing NVIDIA ktÃ³ra zdemokratyzowaÅ‚a programowanie GPU, przeksztaÅ‚cajÄ…c procesory graficzne w general-purpose scientific computing accelerators i umoÅ¼liwiajÄ…c rewolucjÄ™ deep learningu.

**Memory bandwidth bottleneck:** GPU majÄ… wysokie compute (TFLOPS) ale memory bandwidth je limituje. A100: 312 TFLOPS compute ale tylko 1.6 TB/s memory bandwidth.

**RozwiÄ…zanie:** Operator fusion. ÅÄ…czenie memory-bound operations (activation functions, batch norm) w pojedyncze kernele. Redukuje memory traffic 3-10x.

**TPU (2016):** Customowy ASIC Google dla tensor operations. Systolic arrays zoptymalizowane pod matrix multiplication. 275 TFLOPS BF16 @ 1.375 TFLOPS/W efficiency. 3-5x bardziej energy efficient niÅ¼ GPU dla dense matrix ops.

[^fn-frameworks-tpu]: **TPU (Tensor Processing Unit)**: Pierwsza generacja TPU Google (v1) osiÄ…gnÄ™Å‚a 15-30x lepszy performance-per-watt niÅ¼ wspÃ³Å‚czesne GPU i CPU dla sieci neuronowych, dowodzÄ…c Å¼e domain-specific architectures mogÄ… przewyÅ¼szaÄ‡ general-purpose procesory dla ML workloads.

[^fn-systolic-array]: **Systolic Array**: Specjalistyczna architektura parallel computing wynaleziona przez H.T. Kunga (CMU) i Charlesa Leisersona (MIT) w 1978, gdzie dane przepÅ‚ywajÄ… przez grid processing elements w rytmicznej, pipeline fashion. KaÅ¼dy element wykonuje proste operacje na danych pÅ‚ynÄ…cych od sÄ…siadÃ³w, czyniÄ…c to wyjÄ…tkowo efektywnym dla operacji macierzowych, ktÃ³re sÄ… sercem obliczeÅ„ sieci neuronowych.

**Mobile accelerators:** Apple Neural Engine (35 TOPS INT8 @ 7.2 TOPS/W), Qualcomm NPU. Focus: energy efficiency > surowa performance.

Mixed precision (INT8 vs FP32): 10-15x poprawa energy efficiency. Frameworki muszÄ… wspieraÄ‡ quantization-aware training.

[^fn-asic-ml]: **ASIC (Application-Specific Integrated Circuit)**: Customowe chipy krzemowe zaprojektowane dla konkretnych zadaÅ„, kontrastujÄ…ce z general-purpose CPU. W kontekÅ›cie ML, ASICi jak Google TPU i Tesla FSD chips poÅ›wiÄ™cajÄ… elastycznoÅ›Ä‡ dla 10-100x wzrostu efficiency w operacjach macierzowych, choÄ‡ wymagajÄ… 2-4 lata czasu development i milionÃ³w upfront costs.

[^fn-intermediate-representation]: **Intermediate Representation (IR)**: Format wewnÄ™trzny frameworka ktÃ³ry siedzi miÄ™dzy high-level user code a hardware-specific machine code, umoÅ¼liwiajÄ…c optymalizacje i cross-platform deployment. WspÃ³Å‚czesne ML frameworki uÅ¼ywajÄ… IR jak TensorFlow XLA lub PyTorch TorchScript aby kompilowaÄ‡ ten sam model dla CPU, GPU, TPU i mobile devices.

## Fundamental Concepts {#sec-ai-frameworks-fundamental-concepts-a6cf}

WspÃ³Å‚czesne frameworki majÄ… 4 warstwy:

1. **Fundamentals:** Computational graphs (reprezentacja DAG)
2. **Data Handling:** Tensory, memory management, device placement
3. **Developer Interface:** Programming models (imperative vs symbolic), execution models (eager vs graph)
4. **Execution & Abstraction:** Hardware-specific kernele, resource allocation

::: {#fig-fm_blocks fig-env="figure" fig-pos="htb"}
```{.tikz}
\resizebox{.85\textwidth}{!}{%
\begin{tikzpicture}[font=\small\usefont{T1}{phv}{m}{n}]
%
\tikzset{%
    Line/.style={line width=1.0pt,black!50,text=black
},
  Box/.style={align=flush center,
    inner xsep=2pt,
    node distance=1.4,
    draw=BlueLine,
    line width=0.75pt,
    fill=BlueL,
    text width=34mm,
    minimum width=34mm, minimum height=10mm
  },
 Text/.style={%
    inner sep=3pt,
    draw=none,
    line width=0.75pt,
    fill=TextColor!80,
    text=black,
    font=\usefont{T1}{phv}{m}{n}\footnotesize,
    align=flush center,
    minimum width=7mm, minimum height=5mm
  },
}
\node[Box,fill=OrangeL,draw=OrangeLine](B1){Execution Models};
\node[Box,node distance=4.2,right=of B1,fill=OliveL,
              draw=OliveLine](B2){Programming Models};
%
\scoped[on background layer]
\node[draw=BackLine,inner xsep=6mm,inner ysep=4mm,yshift=2mm,
           fill=BackColor,fit=(B1)(B2),line width=0.75pt](BB1){};
\node[below=2pt of  BB1.north,anchor=north]{Developer Interface};
%
\node[Box,below=1.75 of B1,fill=VioletL,
              draw=VioletLine](2B1){Computational Graphs};
%
\scoped[on background layer]
\node[draw=BackLine,inner xsep=8mm,inner ysep=4mm,yshift=2mm,xshift=2mm,
           fill=BackColor,fit=(2B1),line width=0.75pt](BB1){};
\node[below=2pt of  BB1.north east,anchor=north east]{Fundamentals};
%
\begin{scope}[shift={(0,-5.55)}]
\node[Box,fill=GreenL,draw=GreenLine](3B1){Memory Management and Device Placement};
\node[Box,node distance=4.2,right=of 3B1,fill=GreenL,
              draw=GreenLine](3B2){Specialized Data Structures};
%
\scoped[on background layer]
\node[draw=BackLine,inner xsep=6mm,inner ysep=4mm,yshift=2mm,
           fill=BackColor,fit=(3B1)(3B2),line width=0.75pt](BB2){};
\node[below=1pt of  BB2.north,anchor=north]{Data Handling};
\end{scope}
%
\node[Box,below=1.75 of $(3B1)!0.5!(3B2)$,fill=BlueL,
              draw=BlueLine](4B1){Core Operations};
%
\scoped[on background layer]
\node[draw=BackLine,inner xsep=6mm,inner ysep=4mm,yshift=2mm,
           fill=BackColor,fit=(4B1),line width=0.75pt](BB2){};
\node[below=1pt of  BB2.north,anchor=north]{Execution and Abstraction};
% Arrows
\draw[-latex,Line](B1)--node[Text,pos=0.4]{Generates}(2B1);
\draw[-latex,Line](B2)--node[Text,pos=0.4]{Defines}(B1);
\draw[-latex,Line](2B1)--node[Text,pos=0.4]{Optimizes Execution}(3B1);
\draw[-latex,Line](B2.210)--node[Text,pos=0.35]{Shapes Execution\\ Behavior}
             ++(270:3.1)--++(180:1.5)|-(3B1);
\draw[-latex,Line](B2.330)--node[Text,pos=0.55]{Influences\\ Data Flow}(3B2.30);
\draw[-latex,Line](2B1)-|node[Text,pos=0.25]{Provides\\ Structure For}(3B2.130);
\draw[-latex,Line](3B1)|-node[Text,pos=0.25]{Coordinates\\ with}(4B1);
\draw[-latex,Line](3B2)|-node[Text,pos=0.25]{Feeds\\ Data Into}(4B1);
\end{tikzpicture}}
```
**Framework Layer Interaction**: WspÃ³Å‚czesne ML frameworki organizujÄ… funkcjonalnoÅ›Ä‡ w warstwy ktÃ³re wspÃ³Å‚pracujÄ… aby usprawniÄ‡ budowanie i deployment modeli. Ta architektura umoÅ¼liwia modularnoÅ›Ä‡ i pozwala developerom skupiÄ‡ siÄ™ na konkretnych aspektach bez zarzÄ…dzania low-level infrastructure.
:::

### Computational Graphs {#sec-ai-frameworks-computational-graphs-f0ff}

**Co to jest:** Directed Acyclic Graph (DAG) gdzie nodes = operacje, edges = przepÅ‚yw danych.

**Po co istnieje:** UmoÅ¼liwia automatic differentiation i optymalizacjÄ™.

#### Computational Graph Fundamentals {#sec-ai-frameworks-computational-graph-fundamentals-4979}

Prosty przykÅ‚ad: `z = x * y`

[^fn-dag-ml]: **Directed Acyclic Graph (DAG)**: W machine learning frameworkach, DAGi reprezentujÄ… obliczenia gdzie nodes sÄ… operacjami (jak matrix multiplication lub activation functions) a edges sÄ… zaleÅ¼noÅ›ciami danych. W przeciwieÅ„stwie do ogÃ³lnych DAGÃ³w w computer science, ML computational graphs specyficznie optymalizujÄ… pod automatic differentiation, umoÅ¼liwiajÄ…c frameworkom obliczanie gradientÃ³w przez traversowanie grafu w odwrotnej kolejnoÅ›ci.

::: {#fig-comp-graph fig-env="figure" fig-pos="htb"}
```{.tikz}
\begin{tikzpicture}[font=\usefont{T1}{phv}{m}{n}\small]
%
\tikzset{%
    Line/.style={line width=1.0pt,black!50,rounded corners
},
  Box/.style={align=flush center,
   shape=circle,
    inner xsep=1pt,
    node distance=1.4,
    draw=BlueLine,
    line width=0.75pt,
    fill=BlueL,
    minimum width=8mm,
  },
}
\node[Box,fill=GreenL,draw=GreenLine,minimum width=13mm, ](B1){$f(x,y)$};
\node[Box,right=of B1,fill=OliveL,draw=OliveLine](B2){$z$};
\node[Box,above left=0.1 and 2 of B1,fill=OliveL,draw=OliveLine](B3){$x$};
\node[Box,below left=0.1 and 2 of B1,fill=OliveL,draw=OliveLine](B4){$y$};
\draw[-latex,Line](B1)--(B2);
\draw[-latex,Line](B3)to[bend left=25](B1);
\draw[-latex,Line](B4)to[bend right=25](B1);
\end{tikzpicture}
```
**Computational Graph**: DAG reprezentujÄ…cy $z = x \times y$ gdzie nodes = operacje, edges = przepÅ‚yw danych.
:::

Prawdziwe sieci neuronowe = zÅ‚oÅ¼one wielowarstwowe grafy z convolution, pooling, normalization, activation functions.

##### Layers and Tensors {#sec-ai-frameworks-layers-tensors-5008}

**Layers:** Jednostki obliczeniowe (Conv2D, Dense, BatchNorm). TrzymajÄ… parametry (weights, biases).

**Tensors:** Immutable n-wymiarowe arrays. Format danych dla operacji.

Gdy piszesz `tf.keras.layers.Conv2D()`, framework konstruuje graph nodes dla convolution ops, parameter management i data flow.

##### Neural Network Construction {#sec-ai-frameworks-neural-network-construction-e6ef}

Activation functions (ReLU, sigmoid, tanh) = graph nodes aplikujÄ…ce non-linear transformations.

Pre-built architectures (ResNet, MobileNet) = pre-configured computational graphs. Customizuj layery, uÅ¼ywaj transfer learning.

##### System-Level Consequences {#sec-ai-frameworks-systemlevel-consequences-3032}

**KorzyÅ›ci z grafÃ³w:**
1. Automatic differentiation (obliczanie gradientÃ³w przez chain rule)
2. Hardware portability (ten sam model dziaÅ‚a na CPU/GPU/TPU)
3. Optimization (operation fusion, memory planning, graph rewrites)
4. Serialization (save/load/deploy modeli)

Neural network diagrams (wizualizacja) â‰  computational graphs (reprezentacja wykonania).

#### Pre-Defined Computational Structure {#sec-ai-frameworks-predefined-computational-structure-2f49}

**Static graphs (TensorFlow 1.x):** Model define-then-run.

**Phase 1 - Definition:** Buduj kompletny graf (wszystkie operacje, zmienne, poÅ‚Ä…czenia). Å»adne obliczenia siÄ™ nie dziejÄ….

**Phase 2 - Execution:** Uruchom graf z faktycznymi danymi.

**Zalety:**
- Kompletna analiza grafu umoÅ¼liwia agresywnÄ… optymalizacjÄ™
- XLA compiler: kernel fusion, hardware-specific code generation
- Memory planning: pre-alokacja bazujÄ…ca na strukturze grafu
- Production deployment: zwalidowany graf, przewidywalna performance

**Wady:**
- Debugging jest beznadziejny (nie moÅ¼esz inspektowaÄ‡ intermediate values podczas definicji)
- Control flow nieporÄ™czny (tf.cond, tf.while_loop zamiast Python if/while)
- Dynamic architectures bolesne (variable-length sequences, recursive networks)

[^fn-xla]: **XLA (Accelerated Linear Algebra)**: Domain-specific compiler Google dla algebry liniowej ktÃ³ry optymalizuje obliczenia TensorFlow. XLA wykonuje operation fusion, memory optimization i hardware-specific code generation, osiÄ…gajÄ…c 10-80% speedupy nad nieooptymalizowanym wykonaniem.

::: {#fig-mlfm-static-graph fig-env="figure" fig-pos="htb"}
```{.tikz}
\begin{tikzpicture}[font=\usefont{T1}{phv}{m}{n}\small]
%
\tikzset{Line/.style={line width=1.0pt,black!50,rounded corners
},
  Box/.style={align=flush center,
    inner xsep=2pt,
    node distance=0.7,
    draw=BlueLine,
    line width=0.75pt,
    fill=BlueL,
    text width=18mm,
    minimum width=18mm, minimum height=10mm
  },
}
\node[Box,fill=VioletL,draw=VioletLine](B1){Define Operations};
\node[Box,fill=VioletL,draw=VioletLine,right=of B1](B2){Declare Variables};
\node[Box,fill=VioletL,draw=VioletLine,right=of B2](B3){Build Graph};
%
\scoped[on background layer]
\node[draw=BackLine,inner xsep=4mm,inner ysep=6mm,yshift=2mm,
           fill=BackColor!80,fit=(B1)(B2)(B3),line width=0.75pt](BB1){};
\node[below=2pt of  BB1.north,anchor=north]{Definition Phase};
%
\node[Box,node distance=1.5,fill=BrownL,draw=BrownLine,right=of B3](B4){Load Data};
\node[Box,fill=BrownL,draw=BrownLine,right=of B4](B5){Run Graph};
\node[Box,fill=BrownL,draw=BrownLine,right=of B5](B6){Get Results};
%
\scoped[on background layer]
\node[draw=GreenLine,inner xsep=4mm,inner ysep=6mm,yshift=2mm,
           fill=GreenL!20,fit=(B4)(B5)(B6),line width=0.75pt](BB2){};
\node[below=2pt of  BB2.north,anchor=north]{Execution Phase};
%
\foreach \x/\y in{1/2,2/3,3/4,4/5,5/6}
\draw[-latex,Line](B\x)--(B\y);
\end{tikzpicture}
```
**Static Computation Graph**: Dwufazowe wykonanie. Faza 1: zbuduj kompletny graf. Faza 2: wykonaj z danymi. UmoÅ¼liwia globalnÄ… optymalizacjÄ™ ale komplikuje debugging.
:::

*[UWAGA: To jest tylko poczÄ…tek (~20% oryginalnego pliku). PeÅ‚na wersja to 3491 linii. KontynuowaÄ‡?]*
