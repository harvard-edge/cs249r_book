---
bibliography: frameworks.bib
quiz: footnote_context_quizzes.json
concepts: frameworks_concepts.yml
glossary: frameworks_glossary.json
crossrefs: frameworks_xrefs.json
---

# AI Frameworks {#sec-ai-frameworks}

::: {layout-narrow}
::: {.column-margin}
*DALL¬∑E 3 Prompt: Illustration in a rectangular format, designed for a professional textbook, where the content spans the entire width. The vibrant chart represents training and inference frameworks for ML. Icons for TensorFlow, Keras, PyTorch, ONNX, and TensorRT are spread out, filling the entire horizontal space, and aligned vertically. Each icon is accompanied by brief annotations detailing their features. The lively colors like blues, greens, and oranges highlight the icons and sections against a soft gradient background. The distinction between training and inference frameworks is accentuated through color-coded sections, with clean lines and modern typography maintaining clarity and focus.*
:::

\noindent
![](images/png/cover_ml_frameworks.png)

:::

## Purpose {.unnumbered}

**DLACZEGO frameworki istniejƒÖ:** Bez nich musia≈Çby≈õ reimplementowaƒá backprop, kernele GPU i distributed training dla ka≈ºdego projektu. Frameworki abstrahujƒÖ hardware, umo≈ºliwiajƒÖ automatic differentiation i za≈ÇatwiajƒÖ in≈ºynierski ba≈Çagan, ≈ºeby≈õ m√≥g≈Ç skupiƒá siƒô na modelach.

**Czego siƒô nauczysz:**
- Jak computational graphs faktycznie dzia≈ÇajƒÖ (static vs dynamic)
- Dlaczego PyTorch wybra≈Ç dynamic graphs a TensorFlow static (potem siƒô przerzucili üôÑ)
- Implementacja automatic differentiation
- Hardware abstraction layers
- Wyb√≥r frameworka (przesta≈Ñ ≈õlepo kopiowaƒá PyTorch do wszystkiego)

::: {.callout-tip title="Learning Objectives"}

- Zrozumieƒá reprezentacjƒô computational graph i execution models
- Wyja≈õniƒá automatic differentiation (forward vs reverse mode)
- Por√≥wnaƒá architektury: TensorFlow, PyTorch, JAX
- Wybieraƒá odpowiednie frameworki bazujƒÖc na deployment constraints
- Identyfikowaƒá anti-patterny przy wyborze frameworka

:::

## Framework Abstraction and Necessity {#sec-ai-frameworks-framework-abstraction-necessity-48f9}

**Problem:** Trenowanie wsp√≥≈Çczesnego transformera wymaga koordynacji miliard√≥w FLOP√≥w na rozproszonych GPU, zarzƒÖdzajƒÖc pamiƒôciƒÖ, precyzjƒÖ numerycznƒÖ i obliczaniem gradient√≥w.

**Bez framework√≥w:** Implementacja backprop dla 3-warstwowego MLP rƒôcznie = setki linii kodu z rachunkiem r√≥≈ºniczkowym i macierzami, kt√≥re prawdopodobnie sƒÖ b≈Çƒôdne.

**Z frameworkami:** `loss.backward()`

To ca≈Ça propozycja warto≈õci.

:::{.callout-definition title="Machine Learning Frameworks"}
***Machine Learning Frameworks*** abstrahujƒÖ infrastrukturƒô obliczeniowƒÖ przez standardowe API do development, treningu i deployment modeli. DostarczajƒÖ computational graphs, automatic differentiation i akceleracjƒô sprzƒôtowƒÖ bez dotykania CUDA.
:::

**Podstawowe mo≈ºliwo≈õci:**
1. **Automatic differentiation** - Automatyczne obliczanie gradient√≥w przez chain rule
2. **Hardware abstraction** - Ten sam kod dzia≈Ça na CPU/GPU/TPU
3. **Optimization** - Kernel fusion, memory planning, graph rewrites
4. **Distribution** - Koordynacja treningu multi-GPU/multi-node

Frameworki ewoluowa≈Çy: BLAS (1979) ‚Üí NumPy (2006) ‚Üí Theano (2007) ‚Üí TensorFlow/PyTorch (2015-2016). Ka≈ºda warstwa dodawa≈Ça abstrakcjƒô zachowujƒÖc performance.

## Historical Development Trajectory {#sec-ai-frameworks-historical-development-trajectory-9519}

**Timeline:**
- **1979:** BLAS - zoptymalizowane operacje macierzowe
- **1992:** LAPACK - dekompozycje macierzy, solvery liniowe
- **2006:** NumPy - Python arrays oparte na BLAS/LAPACK
- **2007:** Theano - computational graphs + akceleracja GPU
- **2015:** TensorFlow - static graphs + distributed training
- **2016:** PyTorch - dynamic graphs + focus na research
- **2018:** JAX - functional programming + automatic transformation

### Foundational Mathematical Computing Infrastructure {#sec-ai-frameworks-foundational-mathematical-computing-infrastructure-f41c}

Sieci neuronowe = u≈Ço≈ºone mno≈ºenia macierzy. BLAS dostarczy≈Ç podstawowe operacje algebry liniowej (GEMM, GEMV). LAPACK doda≈Ç dekompozycje macierzy. NumPy opakowa≈Ç to w Python arrays.

**Dlaczego towa≈ºne:** Ka≈ºdy framework ostatecznie wywo≈Çuje BLAS/LAPACK (lub zoptymalizowane warianty jak Intel MKL, OpenBLAS). Zrozumienie tego stacku wyja≈õnia dlaczego operacje macierzowe sƒÖ szybkie, a element-wise czƒôsto nie.

[^fn-frameworks-blas]: **BLAS (Basic Linear Algebra Subprograms)**: Oryginalnie stworzony w Argonne National Laboratory, BLAS sta≈Ç siƒô de facto standardem dla operacji algebry liniowej, z Level 1 (vector-vector), Level 2 (matrix-vector) i Level 3 (matrix-matrix) operacjami, kt√≥re wciƒÖ≈º stanowiƒÖ fundament ka≈ºdego wsp√≥≈Çczesnego ML frameworka.

[^fn-lapack]: **LAPACK (Linear Algebra Package)**: Nastƒôpca LINPACK i EISPACK, wprowadzi≈Ç block algorithms drastycznie poprawiajƒÖce cache efficiency i parallel execution, innowacje kt√≥re sta≈Çy siƒô niezbƒôdne gdy datasety uros≈Çy z megabajt√≥w do terabajt√≥w.

### Early Machine Learning Platform Development {#sec-ai-frameworks-early-machine-learning-platform-development-3873}

**Scikit-learn (2007):** Abstrakcja algorytm√≥w ML za `.fit()` API. Dobry dla classical ML, bezu≈ºyteczny dla deep learning.

**Theano (2007):** Pierwszy framework z:
- Computational graphs (DAG operacji)
- Automatic differentiation
- KompilacjƒÖ GPU

Theano by≈Ç pionierem architektury u≈ºywanej przez wsp√≥≈Çczesne frameworki. Development zako≈Ñczony 2017 ale wp≈ÇynƒÖ≈Ç na wszystko po nim.

[^fn-theano]: **Theano**: Nazwany od staro≈ºytnej greckiej matematyczki Theano z Krotonu, ten framework by≈Ç pionierem konceptu symbolicznych wyra≈ºe≈Ñ matematycznych w Pythonie, k≈ÇadƒÖc podwaliny pod ka≈ºdy wsp√≥≈Çczesny deep learning framework.

[^fn-comp-graphs]: **Computational Graphs**: Po raz pierwszy sformalizowane w literaturze automatic differentiation przez Wengerta (1964), ta reprezentacja sta≈Ça siƒô krƒôgos≈Çupem wsp√≥≈Çczesnych ML framework√≥w, umo≈ºliwiajƒÖc forward i reverse-mode differentiation na niespotykanƒÖ skalƒô.

**Torch7 (2002):** Oparty na Lua, immediate execution. Wp≈ÇynƒÖ≈Ç na filozofiƒô designu PyTorch: developer experience > optymalizacja performance.

[^fn-eager-execution]: **Eager Execution**: Model wykonania gdzie operacje sƒÖ ewaluowane natychmiast gdy sƒÖ wywo≈Çywane, podobnie do standardowego wykonania Pythona. Pionierem by≈Ç Torch w 2002, to podej≈õcie priorytetyzuje produktywno≈õƒá developera i ≈Çatwo≈õƒá debugowania nad optymalizacjƒÖ performance, stajƒÖc siƒô domy≈õlnym trybem w nowoczesnych frameworkach jak PyTorch i TensorFlow 2.x.

### Deep Learning Computational Platform Innovation {#sec-ai-frameworks-deep-learning-computational-platform-innovation-d3db}

**TensorFlow (2015):** Google open-sourcowa≈Ç nastƒôpcƒô ich wewnƒôtrznego DistBelief. Rewolucyjne features:
- Static computation graphs (definicja ca≈Çego modelu z g√≥ry)
- Distributed training przez clustery
- Focus na production deployment
- Agresywna optymalizacja (XLA compiler, graph rewrites)

**Trade-off:** Performance vs usability. Static graphs = szybkie ale bolesny debugging.

[^fn-tensorflow]: **TensorFlow**: Nazwany od tensor operations przep≈ÇywajƒÖcych przez computational graphs, ten framework zdemokratyzowa≈Ç distributed machine learning przez open-sourcing wewnƒôtrznego systemu Google DistBelief, natychmiast dajƒÖc badaczom dostƒôp do infrastruktury kt√≥ra wcze≈õniej wymaga≈Ça masywnych zasob√≥w korporacyjnych.

[^fn-static-graph]: **Static Computational Graph**: Pre-definiowana struktura obliczeniowa gdzie ca≈Ça architektura modelu jest specyfikowana przed wykonaniem, umo≈ºliwiajƒÖc globalne optymalizacje i efektywne planowanie pamiƒôci. Pionierem by≈Ç TensorFlow 1.x, to podej≈õcie po≈õwiƒôca runtime flexibility dla maksymalnej optymalizacji performance, czyniƒÖc go idealnym dla production deployments.

[^fn-kernel-fusion]: **Kernel Fusion**: Technika optymalizacyjna ≈ÇƒÖczƒÖca wiele oddzielnych operacji (jak matrix multiplication po kt√≥rym nastƒôpuje bias addition i activation) w pojedynczy GPU kernel, redukujƒÖc memory bandwidth requirements do 10x i eliminujƒÖc intermediate memory allocations. Ta optymalizacja jest szczeg√≥lnie kluczowa dla z≈Ço≈ºonych deep learning modeli z tysiƒÖcami operacji.

[^fn-memory-planning]: **Memory Planning**: Optymalizacja frameworka kt√≥ra pre-analizuje computational graphs aby okre≈õliƒá optymalne strategie alokacji pamiƒôci, umo≈ºliwiajƒÖc techniki jak in-place operations i memory reuse patterns kt√≥re mogƒÖ zredukowaƒá peak memory usage o 40-60% podczas treningu.

**PyTorch (2016):** Odpowied≈∫ Facebooka. Dynamic computation graphs.
- Define-by-run: graph budowany podczas wykonania
- Pythonic API
- ≈Åatwy debugging (standardowy Python debugger dzia≈Ça)
- Research-friendly

**Trade-off:** Usability vs performance. Dynamic graphs = flexible ale trudniejsze do optymalizacji.

[^fn-pytorch]: **PyTorch**: Zainspirowany oryginalnym Torch frameworkiem z NYU, PyTorch przyni√≥s≈Ç "define-by-run" semantykƒô do Pythona, umo≈ºliwiajƒÖc badaczom modyfikacjƒô modeli podczas wykonania, prze≈Çom kt√≥ry przyspieszy≈Ç research czyniƒÖc debugging tak prostym jak u≈ºywanie standardowego Python debuggera.

**JAX (2018):** Podej≈õcie Google Research do functional programming.
- Composable transformations: `grad()`, `jit()`, `vmap()`, `pmap()`
- Kompatybilno≈õƒá z NumPy API
- Automatyczna wektoryzacja i paralelizacja
- Functional purity (brak side effects)

[^fn-jax]: **JAX**: Oznacza "Just After eXecution" i ≈ÇƒÖczy NumPy API z functional programming transforms (jit, grad, vmap, pmap), umo≈ºliwiajƒÖc badaczom pisanie zwiƒôz≈Çego kodu kt√≥ry automatycznie skaluje na TPU i GPU clusters zachowujƒÖc kompatybilno≈õƒá z NumPy.

### Hardware-Driven Framework Architecture Evolution {#sec-ai-frameworks-hardwaredriven-framework-architecture-evolution-2605}

**GPU acceleration (2007):** NVIDIA CUDA umo≈ºliwi≈Ç r√≥wnoleg≈Çe operacje macierzowe na GPU. CPU: 1-2 TFLOPS. GPU (A100): 312 TFLOPS. Dlatego frameworki dbajƒÖ o wsparcie GPU.

[^fn-cuda]: **CUDA (Compute Unified Device Architecture)**: Platforma parallel computing NVIDIA kt√≥ra zdemokratyzowa≈Ça programowanie GPU, przekszta≈ÇcajƒÖc procesory graficzne w general-purpose scientific computing accelerators i umo≈ºliwiajƒÖc rewolucjƒô deep learningu.

**Memory bandwidth bottleneck:** GPU majƒÖ wysokie compute (TFLOPS) ale memory bandwidth je limituje. A100: 312 TFLOPS compute ale tylko 1.6 TB/s memory bandwidth.

**RozwiƒÖzanie:** Operator fusion. ≈ÅƒÖczenie memory-bound operations (activation functions, batch norm) w pojedyncze kernele. Redukuje memory traffic 3-10x.

**TPU (2016):** Customowy ASIC Google dla tensor operations. Systolic arrays zoptymalizowane pod matrix multiplication. 275 TFLOPS BF16 @ 1.375 TFLOPS/W efficiency. 3-5x bardziej energy efficient ni≈º GPU dla dense matrix ops.

[^fn-frameworks-tpu]: **TPU (Tensor Processing Unit)**: Pierwsza generacja TPU Google (v1) osiƒÖgnƒô≈Ça 15-30x lepszy performance-per-watt ni≈º wsp√≥≈Çczesne GPU i CPU dla sieci neuronowych, dowodzƒÖc ≈ºe domain-specific architectures mogƒÖ przewy≈ºszaƒá general-purpose procesory dla ML workloads.

[^fn-systolic-array]: **Systolic Array**: Specjalistyczna architektura parallel computing wynaleziona przez H.T. Kunga (CMU) i Charlesa Leisersona (MIT) w 1978, gdzie dane przep≈ÇywajƒÖ przez grid processing elements w rytmicznej, pipeline fashion. Ka≈ºdy element wykonuje proste operacje na danych p≈ÇynƒÖcych od sƒÖsiad√≥w, czyniƒÖc to wyjƒÖtkowo efektywnym dla operacji macierzowych, kt√≥re sƒÖ sercem oblicze≈Ñ sieci neuronowych.

**Mobile accelerators:** Apple Neural Engine (35 TOPS INT8 @ 7.2 TOPS/W), Qualcomm NPU. Focus: energy efficiency > surowa performance.

Mixed precision (INT8 vs FP32): 10-15x poprawa energy efficiency. Frameworki muszƒÖ wspieraƒá quantization-aware training.

[^fn-asic-ml]: **ASIC (Application-Specific Integrated Circuit)**: Customowe chipy krzemowe zaprojektowane dla konkretnych zada≈Ñ, kontrastujƒÖce z general-purpose CPU. W kontek≈õcie ML, ASICi jak Google TPU i Tesla FSD chips po≈õwiƒôcajƒÖ elastyczno≈õƒá dla 10-100x wzrostu efficiency w operacjach macierzowych, choƒá wymagajƒÖ 2-4 lata czasu development i milion√≥w upfront costs.

[^fn-intermediate-representation]: **Intermediate Representation (IR)**: Format wewnƒôtrzny frameworka kt√≥ry siedzi miƒôdzy high-level user code a hardware-specific machine code, umo≈ºliwiajƒÖc optymalizacje i cross-platform deployment. Wsp√≥≈Çczesne ML frameworki u≈ºywajƒÖ IR jak TensorFlow XLA lub PyTorch TorchScript aby kompilowaƒá ten sam model dla CPU, GPU, TPU i mobile devices.

## Fundamental Concepts {#sec-ai-frameworks-fundamental-concepts-a6cf}

Wsp√≥≈Çczesne frameworki majƒÖ 4 warstwy:

1. **Fundamentals:** Computational graphs (reprezentacja DAG)
2. **Data Handling:** Tensory, memory management, device placement
3. **Developer Interface:** Programming models (imperative vs symbolic), execution models (eager vs graph)
4. **Execution & Abstraction:** Hardware-specific kernele, resource allocation

::: {#fig-fm_blocks fig-env="figure" fig-pos="htb"}
```{.tikz}
\resizebox{.85\textwidth}{!}{%
\begin{tikzpicture}[font=\small\usefont{T1}{phv}{m}{n}]
%
\tikzset{%
    Line/.style={line width=1.0pt,black!50,text=black
},
  Box/.style={align=flush center,
    inner xsep=2pt,
    node distance=1.4,
    draw=BlueLine,
    line width=0.75pt,
    fill=BlueL,
    text width=34mm,
    minimum width=34mm, minimum height=10mm
  },
 Text/.style={%
    inner sep=3pt,
    draw=none,
    line width=0.75pt,
    fill=TextColor!80,
    text=black,
    font=\usefont{T1}{phv}{m}{n}\footnotesize,
    align=flush center,
    minimum width=7mm, minimum height=5mm
  },
}
\node[Box,fill=OrangeL,draw=OrangeLine](B1){Execution Models};
\node[Box,node distance=4.2,right=of B1,fill=OliveL,
              draw=OliveLine](B2){Programming Models};
%
\scoped[on background layer]
\node[draw=BackLine,inner xsep=6mm,inner ysep=4mm,yshift=2mm,
           fill=BackColor,fit=(B1)(B2),line width=0.75pt](BB1){};
\node[below=2pt of  BB1.north,anchor=north]{Developer Interface};
%
\node[Box,below=1.75 of B1,fill=VioletL,
              draw=VioletLine](2B1){Computational Graphs};
%
\scoped[on background layer]
\node[draw=BackLine,inner xsep=8mm,inner ysep=4mm,yshift=2mm,xshift=2mm,
           fill=BackColor,fit=(2B1),line width=0.75pt](BB1){};
\node[below=2pt of  BB1.north east,anchor=north east]{Fundamentals};
%
\begin{scope}[shift={(0,-5.55)}]
\node[Box,fill=GreenL,draw=GreenLine](3B1){Memory Management and Device Placement};
\node[Box,node distance=4.2,right=of 3B1,fill=GreenL,
              draw=GreenLine](3B2){Specialized Data Structures};
%
\scoped[on background layer]
\node[draw=BackLine,inner xsep=6mm,inner ysep=4mm,yshift=2mm,
           fill=BackColor,fit=(3B1)(3B2),line width=0.75pt](BB2){};
\node[below=1pt of  BB2.north,anchor=north]{Data Handling};
\end{scope}
%
\node[Box,below=1.75 of $(3B1)!0.5!(3B2)$,fill=BlueL,
              draw=BlueLine](4B1){Core Operations};
%
\scoped[on background layer]
\node[draw=BackLine,inner xsep=6mm,inner ysep=4mm,yshift=2mm,
           fill=BackColor,fit=(4B1),line width=0.75pt](BB2){};
\node[below=1pt of  BB2.north,anchor=north]{Execution and Abstraction};
% Arrows
\draw[-latex,Line](B1)--node[Text,pos=0.4]{Generates}(2B1);
\draw[-latex,Line](B2)--node[Text,pos=0.4]{Defines}(B1);
\draw[-latex,Line](2B1)--node[Text,pos=0.4]{Optimizes Execution}(3B1);
\draw[-latex,Line](B2.210)--node[Text,pos=0.35]{Shapes Execution\\ Behavior}
             ++(270:3.1)--++(180:1.5)|-(3B1);
\draw[-latex,Line](B2.330)--node[Text,pos=0.55]{Influences\\ Data Flow}(3B2.30);
\draw[-latex,Line](2B1)-|node[Text,pos=0.25]{Provides\\ Structure For}(3B2.130);
\draw[-latex,Line](3B1)|-node[Text,pos=0.25]{Coordinates\\ with}(4B1);
\draw[-latex,Line](3B2)|-node[Text,pos=0.25]{Feeds\\ Data Into}(4B1);
\end{tikzpicture}}
```
**Framework Layer Interaction**: Wsp√≥≈Çczesne ML frameworki organizujƒÖ funkcjonalno≈õƒá w warstwy kt√≥re wsp√≥≈ÇpracujƒÖ aby usprawniƒá budowanie i deployment modeli. Ta architektura umo≈ºliwia modularno≈õƒá i pozwala developerom skupiƒá siƒô na konkretnych aspektach bez zarzƒÖdzania low-level infrastructure.
:::

### Computational Graphs {#sec-ai-frameworks-computational-graphs-f0ff}

**Co to jest:** Directed Acyclic Graph (DAG) gdzie nodes = operacje, edges = przep≈Çyw danych.

**Po co istnieje:** Umo≈ºliwia automatic differentiation i optymalizacjƒô.

#### Computational Graph Fundamentals {#sec-ai-frameworks-computational-graph-fundamentals-4979}

Prosty przyk≈Çad: `z = x * y`

[^fn-dag-ml]: **Directed Acyclic Graph (DAG)**: W machine learning frameworkach, DAGi reprezentujƒÖ obliczenia gdzie nodes sƒÖ operacjami (jak matrix multiplication lub activation functions) a edges sƒÖ zale≈ºno≈õciami danych. W przeciwie≈Ñstwie do og√≥lnych DAG√≥w w computer science, ML computational graphs specyficznie optymalizujƒÖ pod automatic differentiation, umo≈ºliwiajƒÖc frameworkom obliczanie gradient√≥w przez traversowanie grafu w odwrotnej kolejno≈õci.

::: {#fig-comp-graph fig-env="figure" fig-pos="htb"}
```{.tikz}
\begin{tikzpicture}[font=\usefont{T1}{phv}{m}{n}\small]
%
\tikzset{%
    Line/.style={line width=1.0pt,black!50,rounded corners
},
  Box/.style={align=flush center,
   shape=circle,
    inner xsep=1pt,
    node distance=1.4,
    draw=BlueLine,
    line width=0.75pt,
    fill=BlueL,
    minimum width=8mm,
  },
}
\node[Box,fill=GreenL,draw=GreenLine,minimum width=13mm, ](B1){$f(x,y)$};
\node[Box,right=of B1,fill=OliveL,draw=OliveLine](B2){$z$};
\node[Box,above left=0.1 and 2 of B1,fill=OliveL,draw=OliveLine](B3){$x$};
\node[Box,below left=0.1 and 2 of B1,fill=OliveL,draw=OliveLine](B4){$y$};
\draw[-latex,Line](B1)--(B2);
\draw[-latex,Line](B3)to[bend left=25](B1);
\draw[-latex,Line](B4)to[bend right=25](B1);
\end{tikzpicture}
```
**Computational Graph**: DAG reprezentujƒÖcy $z = x \times y$ gdzie nodes = operacje, edges = przep≈Çyw danych.
:::

Prawdziwe sieci neuronowe = z≈Ço≈ºone wielowarstwowe grafy z convolution, pooling, normalization, activation functions.

##### Layers and Tensors {#sec-ai-frameworks-layers-tensors-5008}

**Layers:** Jednostki obliczeniowe (Conv2D, Dense, BatchNorm). TrzymajƒÖ parametry (weights, biases).

**Tensors:** Immutable n-wymiarowe arrays. Format danych dla operacji.

Gdy piszesz `tf.keras.layers.Conv2D()`, framework konstruuje graph nodes dla convolution ops, parameter management i data flow.

##### Neural Network Construction {#sec-ai-frameworks-neural-network-construction-e6ef}

Activation functions (ReLU, sigmoid, tanh) = graph nodes aplikujƒÖce non-linear transformations.

Pre-built architectures (ResNet, MobileNet) = pre-configured computational graphs. Customizuj layery, u≈ºywaj transfer learning.

##### System-Level Consequences {#sec-ai-frameworks-systemlevel-consequences-3032}

**Korzy≈õci z graf√≥w:**
1. Automatic differentiation (obliczanie gradient√≥w przez chain rule)
2. Hardware portability (ten sam model dzia≈Ça na CPU/GPU/TPU)
3. Optimization (operation fusion, memory planning, graph rewrites)
4. Serialization (save/load/deploy modeli)

Neural network diagrams (wizualizacja) ‚â† computational graphs (reprezentacja wykonania).

#### Pre-Defined Computational Structure {#sec-ai-frameworks-predefined-computational-structure-2f49}

**Static graphs (TensorFlow 1.x):** Model define-then-run.

**Phase 1 - Definition:** Buduj kompletny graf (wszystkie operacje, zmienne, po≈ÇƒÖczenia). ≈ªadne obliczenia siƒô nie dziejƒÖ.

**Phase 2 - Execution:** Uruchom graf z faktycznymi danymi.

**Zalety:**
- Kompletna analiza grafu umo≈ºliwia agresywnƒÖ optymalizacjƒô
- XLA compiler: kernel fusion, hardware-specific code generation
- Memory planning: pre-alokacja bazujƒÖca na strukturze grafu
- Production deployment: zwalidowany graf, przewidywalna performance

**Wady:**
- Debugging jest beznadziejny (nie mo≈ºesz inspektowaƒá intermediate values podczas definicji)
- Control flow nieporƒôczny (tf.cond, tf.while_loop zamiast Python if/while)
- Dynamic architectures bolesne (variable-length sequences, recursive networks)

[^fn-xla]: **XLA (Accelerated Linear Algebra)**: Domain-specific compiler Google dla algebry liniowej kt√≥ry optymalizuje obliczenia TensorFlow. XLA wykonuje operation fusion, memory optimization i hardware-specific code generation, osiƒÖgajƒÖc 10-80% speedupy nad nieooptymalizowanym wykonaniem.

::: {#fig-mlfm-static-graph fig-env="figure" fig-pos="htb"}
```{.tikz}
\begin{tikzpicture}[font=\usefont{T1}{phv}{m}{n}\small]
%
\tikzset{Line/.style={line width=1.0pt,black!50,rounded corners
},
  Box/.style={align=flush center,
    inner xsep=2pt,
    node distance=0.7,
    draw=BlueLine,
    line width=0.75pt,
    fill=BlueL,
    text width=18mm,
    minimum width=18mm, minimum height=10mm
  },
}
\node[Box,fill=VioletL,draw=VioletLine](B1){Define Operations};
\node[Box,fill=VioletL,draw=VioletLine,right=of B1](B2){Declare Variables};
\node[Box,fill=VioletL,draw=VioletLine,right=of B2](B3){Build Graph};
%
\scoped[on background layer]
\node[draw=BackLine,inner xsep=4mm,inner ysep=6mm,yshift=2mm,
           fill=BackColor!80,fit=(B1)(B2)(B3),line width=0.75pt](BB1){};
\node[below=2pt of  BB1.north,anchor=north]{Definition Phase};
%
\node[Box,node distance=1.5,fill=BrownL,draw=BrownLine,right=of B3](B4){Load Data};
\node[Box,fill=BrownL,draw=BrownLine,right=of B4](B5){Run Graph};
\node[Box,fill=BrownL,draw=BrownLine,right=of B5](B6){Get Results};
%
\scoped[on background layer]
\node[draw=GreenLine,inner xsep=4mm,inner ysep=6mm,yshift=2mm,
           fill=GreenL!20,fit=(B4)(B5)(B6),line width=0.75pt](BB2){};
\node[below=2pt of  BB2.north,anchor=north]{Execution Phase};
%
\foreach \x/\y in{1/2,2/3,3/4,4/5,5/6}
\draw[-latex,Line](B\x)--(B\y);
\end{tikzpicture}
```
**Static Computation Graph**: Dwufazowe wykonanie. Faza 1: zbuduj kompletny graf. Faza 2: wykonaj z danymi. Umo≈ºliwia globalnƒÖ optymalizacjƒô ale komplikuje debugging.
:::

#### Runtime-Adaptive Computational Structure {#sec-ai-frameworks-runtimeadaptive-computational-structure-156d}

**Dynamic graphs (PyTorch):** Model define-by-run. Graf budowany podczas wykonania.

**Zalety:**
- Flexibility w control flow (if/while/for dzia≈Ça naturalnie)
- Debugging: standardowy Python debugger
- Variable-length sequences: bez problemu
- Research velocity: szybsze iteracje

**Wady:**
- Memory fragmentation (alokacja w runtime)
- Brak global optimizations (compiler nie widzi pe≈Çnego grafu)
- Overhead z graph construction przy ka≈ºdym forward pass

::: {#fig-mlfm-dynamic-graph-flow fig-env="figure" fig-pos="htb"}
```{.tikz}
\begin{tikzpicture}[font=\usefont{T1}{phv}{m}{n}\small]
\tikzset{Line/.style={line width=1.0pt,black!50
},
  Box/.style={align=flush center,
    inner xsep=2pt,
    node distance=1.0,
    draw=BlueLine,
    line width=0.75pt,
    fill=BlueL,
    text width=18mm,
    minimum width=18mm,
    minimum height=10mm
  },
   Text/.style={%
    inner sep=4pt,
    draw=none,
    line width=0.75pt,
    fill=TextColor!80,
    text=black,
    font=\usefont{T1}{phv}{m}{n}\footnotesize,
    align=flush center,
    minimum width=7mm, minimum height=5mm
  },
}
\node[Box,text width=12mm,minimum width=14mm,
             fill=OliveL!70,draw=OliveLine](B1){Start};
\node[Box,fill=VioletL,draw=VioletLine,right=of B1](B2){Operation 1};
\node[Box,fill=GreenL,draw=GreenLine,right=of B2,
            minimum height=14mm](B3){Operation 1 Executed};
\node[Box,node distance=2.1,fill=VioletL,draw=VioletLine,right=of B3](B4){Operation 2};
\node[Box,fill=GreenL,draw=GreenLine,right=of B4,
            minimum height=14mm](B5){Operation 2 Executed};
\node[Box,right=of B5,text width=12mm,minimum width=14mm,
             fill=OliveL!70,draw=OliveLine](B6){End};
%%
\foreach \x/\y in{1/2,2/3,3/4,4/5,5/6}
\draw[-latex,Line](B\x)--(B\y);
\def\vi{15mm}
\draw[thick]($(B1.east)!0.5!(B2.west)$)--++(90:\vi)
node[Text]{Define\\ Operation};
\draw[thick]($(B2.east)!0.5!(B3.west)$)--++(90:\vi)
node[Text]{Execute\\ Operation};
\draw[thick]($(B3.east)!0.5!(B4.west)$)--++(90:\vi)
node[Text]{Define Next\\ Operation};
\draw[thick]($(B4.east)!0.5!(B5.west)$)--++(90:\vi)
node[Text]{Execute\\ Operation};
\draw[thick]($(B5.east)!0.5!(B6.west)$)--++(90:\vi)
node[Text](BB6){Repeat\\ Until Done};
%
\scoped[on background layer]
\node[draw=BackLine,inner xsep=4mm,inner ysep=8mm,yshift=2mm,
           fill=BackColor!80,fit=(B1)(B2)(BB6)(B6),line width=0.75pt](BB2){};
\node[below=2pt of  BB2.north,anchor=north]{Runtime Execution};
\end{tikzpicture}
```
**Dynamic Graph Execution**: Definiuj-wykonaj-powt√≥rz. Ka≈ºda operacja jest zdefiniowana, wykonana i sko≈Ñczona zanim przejdziesz do nastƒôpnej. Umo≈ºliwia flexible model construction i immediate debugging.
:::

#### Framework Architecture Trade-offs {#sec-ai-frameworks-framework-architecture-tradeoffs-6f3c}

**Static vs Dynamic - kluczowe r√≥≈ºnice:**

+----------------------------------+------------------------------------------------------+-------------------------------------------------+
| **Aspect**                       | **Static Graphs**                                    | **Dynamic Graphs**                              |
+:=================================+:=====================================================+:================================================+
| **Memory Management**            | Precise allocation planning, optimized memory usage  | Flexible but likely less efficient allocation   |
+----------------------------------+------------------------------------------------------+-------------------------------------------------+
| **Optimization Potential**       | Comprehensive graph-level optimizations possible     | Limited to local optimizations due to runtime   |
+----------------------------------+------------------------------------------------------+-------------------------------------------------+
| **Hardware Utilization**         | Can generate highly optimized hardware-specific code | May sacrifice  hardware-specific optimizations  |
+----------------------------------+------------------------------------------------------+-------------------------------------------------+
| **Development Experience**       | Requires more upfront planning, harder to debug      | Better debugging, faster iteration cycles       |
+----------------------------------+------------------------------------------------------+-------------------------------------------------+
| **Debugging Workflow**           | Framework-specific tools, disconnected stack traces  | Standard Python debugging (pdb, print, inspect) |
+----------------------------------+------------------------------------------------------+-------------------------------------------------+
| **Error Reporting**              | Execution-time errors disconnected from definition   | Intuitive stack traces pointing to exact lines  |
+----------------------------------+------------------------------------------------------+-------------------------------------------------+
| **Research Velocity**            | Slower iteration due to define-then-run requirement  | Faster prototyping and model experimentation    |
+----------------------------------+------------------------------------------------------+-------------------------------------------------+
| **Runtime Flexibility**          | Fixed computation structure                          | Can adapt to runtime conditions                 |
+----------------------------------+------------------------------------------------------+-------------------------------------------------+
| **Production Performance**       | Generally better performance at scale                | May have overhead from graph construction       |
+----------------------------------+------------------------------------------------------+-------------------------------------------------+
| **Integration with Legacy Code** | More separation between definition and execution     | Natural integration with imperative code        |
+----------------------------------+------------------------------------------------------+-------------------------------------------------+
| **Memory Overhead**              | Lower memory overhead due to planned allocations     | Higher  overhead due to dynamic allocations     |
+----------------------------------+------------------------------------------------------+-------------------------------------------------+
| **Deployment Complexity**        | Simpler deployment due to fixed structure            | May require additional runtime support          |
+----------------------------------+------------------------------------------------------+-------------------------------------------------+

: **Graph Computation Modes**: Static = optymalizacja, Dynamic = flexibility. {#tbl-mlfm-graphs}

##### Memory Management {#sec-ai-frameworks-memory-management-10a6}

**Static graphs:** Pre-allocation mo≈ºliwa. Framework widzi ca≈Çy graf ‚Üí planuje pamiƒôƒá z g√≥ry ‚Üí memory reuse, mniej fragmentacji. Dla du≈ºych modeli kluczowe: 100GB/s dla ma≈Çych modeli, >1TB/s dla LLM z miliardami parametr√≥w.

**Dynamic graphs:** Runtime allocation. Flexible ale higher overhead. Memory fragmentation problem dla long-running tasks. Utilization mo≈ºe spa≈õƒá <50% przez suboptimal access patterns.

##### Device Placement {#sec-ai-frameworks-device-placement-fb7e}

**Static:** Pre-execution analysis ‚Üí mapuj compute-intensive ops na odpowiedni hardware ‚Üí minimize communication overhead. Dobry dla specialized hardware (TPU).

**Dynamic:** Runtime placement. Adaptacja do changing conditions (hardware availability) ale trudniej optimizowaƒá fully ‚Üí potencjalne inefficiencies w distributed setups.

#### Graph-Based Gradient Computation Implementation {#sec-ai-frameworks-graphbased-gradient-computation-implementation-2731}

**Kluczowy insight:** Computational graph ‚â† tylko execution plan. To data structure kt√≥ra umo≈ºliwia reverse-mode automatic differentiation.

**Forward pass:** Framework buduje graf gdzie ka≈ºdy node = operacja + result + gradient info. Graf to faktyczna struktura w pamiƒôci, nie wizualizacja.

**Backward pass:** `loss.backward()` ‚Üí reverse traversal grafu w reverse topological order ‚Üí systematyczna aplikacja chain rule w ka≈ºdym node.

**Dlaczego to dzia≈Ça:** Graf structure enkoduje dependency relationships dla chain rule. Ka≈ºdy edge = partial derivative. Reverse traversal automatycznie komponuje partial derivatives wed≈Çug chain rule.

**Skalowalno≈õƒá:** Complexity liniowa wzglƒôdem liczby operacji (nie eksponencjalna wzglƒôdem liczby zmiennych). Graf zapewnia ≈ºe ka≈ºdy gradient jest obliczany dok≈Çadnie raz.

### Automatic Differentiation {#sec-ai-frameworks-automatic-differentiation-e286}

**Problem:** Oblicz pochodne przez z≈Ço≈ºone ≈Ça≈Ñcuchy operacji matematycznych. Potrzebne do treningu - jak dostosowaƒá miliony parametr√≥w aby poprawiƒá performance modelu [@baydin2018].

::: {#lst-auto_diff_intro lst-cap="**Automatic Differentiation**: Efektywne obliczanie gradient√≥w dla z≈Ço≈ºonych funkcji, kluczowe dla optymalizacji parametr√≥w sieci neuronowych."}
```{.python}
def f(x):
    a = x * x  # Square
    b = sin(x)  # Sine
    return a * b  # Product
```
:::

Nawet ten prosty przyk≈Çad wymaga: product rule, chain rule, pochodne trigonometric functions. Dla sieci neuronowej z milionami operacji - manualna kalkulacja niemo≈ºliwa.

[^fn-auto-diff]: **Automatic Differentiation**: Wynaleziony przez Robert Edwin Wengert (1964), osiƒÖga machine precision derivatives przez aplikacjƒô chain rule na poziomie elementary operations, czyniƒÖc trening sieci neuronowych computationally feasible dla sieci z milionami parametr√≥w.

**Jak AD dzia≈Ça:** Rozk≈Çada funkcjƒô na elementary operations:
1. `a = x * x` ‚Üí derivative: `d(x¬≤)/dx = 2x`
2. `b = sin(x)` ‚Üí derivative: `d(sin(x))/dx = cos(x)`
3. `result = a * b` ‚Üí product rule: `d(uv)/dx = u(dv/dx) + v(du/dx)`

Tracking kombinacji operacji + systematyczna aplikacja chain rule = exact derivatives przez ca≈Çe obliczenie.

#### Forward and Reverse Mode Differentiation {#sec-ai-frameworks-forward-reverse-mode-differentiation-f82b}

Automatic differentiation ma 2 tryby: forward i reverse. R√≥≈ºne charakterystyki efficiency, memory usage, applicability.

##### Forward Mode {#sec-ai-frameworks-forward-mode-3b45}

**Idea:** Oblicz derivatives razem z original computation. ≈öled≈∫ jak zmiany propagujƒÖ from input to output.

::: {#lst-forward_mode_ad lst-cap="**Forward Mode Automatic Differentiation**: Oblicza derivatives obok function evaluations u≈ºywajƒÖc product rule."}
```{.python}
def f(x):  # Computing both value and derivative
    # Step 1: x -> x¬≤
    a = x * x  # Value: x¬≤
    da = 2 * x  # Derivative: 2x

    # Step 2: x -> sin(x)
    b = sin(x)  # Value: sin(x)
    db = cos(x)  # Derivative: cos(x)

    # Step 3: Combine using product rule
    result = a * b  # Value: x¬≤ * sin(x)
    dresult = a * db + b * da  # Derivative: x¬≤*cos(x) + sin(x)*2x

    return result, dresult
```
:::

**Implementation:** "Dual numbers" - ka≈ºda liczba augmented derivative value.

::: {#lst-forward_mode_dual lst-cap="**Forward Mode**: Oblicza derivatives obok values u≈ºywajƒÖc dual numbers."}
```{.python}
x = 2.0  # Initial value
dx = 1.0  # Tracking derivative wrt x

# Step 1: x¬≤
a = 4.0  # (2.0)¬≤
da = 4.0  # 2 * 2.0

# Step 2: sin(x)
b = 0.909  # sin(2.0)
db = -0.416  # cos(2.0)

# Final result
result = 3.637  # 4.0 * 0.909
dresult = 2.805  # 4.0 * (-0.416) + 0.909 * 4.0
```
:::

**Performance characteristics:**
- **Compute:** ~2x original computation dla jednej input variable
- **Scaling:** Linearly z liczbƒÖ input variables (problem dla neural networks z thousands of weights)
- **Memory:** Constant, predictable (dobry dla embedded systems)

**Use cases:**
- Sensitivity analysis (jak zmiana single pixel wp≈Çywa na prediction)
- Feature importance (saliency maps)
- Few inputs, many outputs

Forward mode NIE jest primary choice dla training full neural networks (zbyt du≈ºo input variables), ale przydatny dla analytical tasks.

##### Reverse Mode {#sec-ai-frameworks-reverse-mode-086f}

**Dlaczego reverse mode dominuje:** Training neural networks = jeden scalar output (loss) + derivatives wzglƒôdem milion√≥w parameters (weights). Reverse mode exceptionally efficient dla tego pattern.

::: {#lst-reverse_simple_nn lst-cap="**Reverse Mode**: Sieci neuronowe obliczajƒÖ gradienty przez backward passes na layered computations."}
```{.python}
def simple_network(x, w1, w2):
    # Forward pass
    hidden = x * w1  # First layer multiplication
    activated = max(0, hidden)  # ReLU activation
    output = activated * w2  # Second layer multiplication
    return output
```
:::

**Forward pass:** Framework nie tylko oblicza values - buduje graf operacji ≈õledzƒÖc intermediate results.

::: {#lst-reverse_nn_forward lst-cap="**Forward Pass**: Oblicza intermediate states u≈ºywajƒÖc linear i non-linear transformations."}
```{.python}
x = 1.0
w1 = 2.0
w2 = 3.0

hidden = 2.0  # x * w1 = 1.0 * 2.0
activated = 2.0  # max(0, 2.0) = 2.0
output = 6.0  # activated * w2 = 2.0 * 3.0
```
:::

**Backward pass:** Gradienty obliczane w reverse topological order:

::: {#lst-reverse_nn_backward lst-cap="**Backward Pass**: Kod oblicza gradienty dla weights, pokazujƒÖc jak zmiany propagujƒÖ backward przez layers."}
```{.python}
d_output = 1.0  # Start with derivative of output

d_w2 = activated  # d_output * d(output)/d_w2 = 1.0 * 2.0 = 2.0
d_activated = w2  # d_output * d(output)/d_activated = 1.0 * 3.0 = 3.0

# ReLU gradient: 1 if input was > 0, 0 otherwise
d_hidden = d_activated * (1 if hidden > 0 else 0)  # 3.0 * 1 = 3.0

d_w1 = x * d_hidden  # 1.0 * 3.0 = 3.0
d_x = w1 * d_hidden  # 2.0 * 3.0 = 6.0
```
:::

**Key implementation considerations:**
1. Framework must track dependencies miƒôdzy operations
2. Intermediate values muszƒÖ byƒá stored dla backward pass
3. Gradient computations follow reverse topological order
4. Ka≈ºda operacja needs forward + backward implementation

###### Memory Management Strategies {#sec-ai-frameworks-memory-management-strategies-dca8}

**Problem:** Forward mode mo≈ºe discard intermediate values. Reverse mode MUSI store results z forward pass dla backward pass gradient computation.

::: {#lst-reverse_memory lst-cap="**Reverse Mode Memory Management**: Stores intermediate values dla gradient computation podczas backpropagation."}
```{.python}
def deep_network(x, w1, w2, w3):
    # Forward pass - must store intermediates
    hidden1 = x * w1
    activated1 = max(0, hidden1)  # Store for backward
    hidden2 = activated1 * w2
    activated2 = max(0, hidden2)  # Store for backward
    output = activated2 * w3
    return output
```
:::

**Memory requirement:** Liniowy wzrost z g≈Çƒôboko≈õciƒÖ sieci. Dla deep neural network processingu batch of images = gigabajty stored activations.

**Strategies:**
1. **Gradient checkpointing:** Selectively store tylko niekt√≥re activations, reszta recompute podczas backward pass. Trade computation za memory.
2. **Memory reuse:** Automatically detect gdy intermediate values mogƒÖ byƒá freed
3. **Mixed precision:** FP16 dla activations, FP32 dla gradients

[^fn-gradient-checkpointing]: **Gradient Checkpointing**: Memory optimization kt√≥ra trades computation time za memory przez selective storing tylko certain intermediate activations podczas forward pass, potem recomputing discarded values podczas gradient computation. Mo≈ºe zredukowaƒá memory usage o 50-90% dla deep networks zwiƒôkszajƒÖc training time tylko o 20-33%.

::: {#lst-memory_strategies lst-cap="**Memory Management Strategies**: Checkpointing pozwala intermediate values byƒá freed podczas treningu, redukujƒÖc memory usage."}
```{.python}
def training_step(model, input_batch):
    # Strategy 1: Checkpointing
    with checkpoint_scope():
        hidden1 = activation(layer1(input_batch))
        # Framework might free some memory here
        hidden2 = activation(layer2(hidden1))
        output = layer3(hidden2)

    # Strategy 2: Gradient accumulation
    loss = compute_loss(output)
    loss.backward()
```
:::

###### Optimization Techniques {#sec-ai-frameworks-optimization-techniques-8564}

**Operation fusion:** Zamiast osobne operacje dla ka≈ºdej math operation, combine operations razem. Matrix multiplication + bias addition + ReLU ‚Üí single fused kernel. Reduces memory transfers, improves hardware utilization.

[^fn-operation-fusion]: **Operation Fusion**: Compiler optimization kt√≥ra combines multiple sequential operations w single kernel aby reduce memory bandwidth i latency. Np. fusing matrix multiplication, bias addition i ReLU activation mo≈ºe eliminate intermediate memory allocations i achieve 2-3x speedup na modern GPUs.

**Compute graph optimization:** Framework mo≈ºe reorder operations, eliminate redundant computations, optimize memory layouts - ale tylko je≈õli widzi ca≈Çy graf (advantage static graphs).

**Hardware-specific kernels:** R√≥≈ºne backward implementations dla r√≥≈ºnego hardware (CPU vs GPU vs TPU). Framework automatycznie wybiera optimal implementation.

#### Framework Implementation of Automatic Differentiation {#sec-ai-frameworks-framework-implementation-automatic-differentiation-289a}

Frameworki expose AD przez high-level APIs maskujƒÖc sophisticated machinery.

**Typical API (PyTorch-style):**
```python
# Framework tracks operations transparently
output = model(input)  # Forward pass
loss = loss_function(output, target)
loss.backward()  # Automatic backward pass - framework handles all AD
optimizer.step()  # Parameter updates using computed gradients
```

Za kulisami framework musi:
1. Track wszystkie operations podczas forward pass
2. Build i maintain computational graph
3. Manage memory dla intermediate values
4. Schedule gradient computations efficiently
5. Interface z hardware accelerators (GPU/TPU)

**Higher-order gradients:** Computing derivatives of derivatives. Kluczowe dla niekt√≥rych optimization techniques (second-order methods). Framework must support nested AD.

**Mixed-precision training:** FP16 dla speed, FP32 dla numerical stability. AD system musi handle r√≥≈ºne precisions correctly.

#### Framework-Specific Differentiation Strategies {#sec-ai-frameworks-frameworkspecific-differentiation-strategies-c906}

Ka≈ºdy framework implementuje AD inaczej. To wp≈Çywa na development workflows i performance.

##### PyTorch's Dynamic Autograd System {#sec-ai-frameworks-pytorchs-dynamic-autograd-system-b679}

**PyTorch:** Dynamic tape-based system. Graf budowany podczas execution.

```python
import torch

# PyTorch builds computational graph during execution
x = torch.tensor(2.0, requires_grad=True)
y = torch.tensor(3.0, requires_grad=True)

z = x * y  # Creates MulBackward node
w = z + x  # Creates AddBackward node
loss = w**2  # Creates PowBackward node

# Graph exists only after forward pass completes
loss.backward()  # Traverses dynamically built graph
print(f"dx/dloss = {x.grad}")  # Immediate access to gradients
```

**Zalety:** Operations tracked automatically, natural Python control flow, gradients available immediately, variable-length computations handled naturally.

**Wady:** Must maintain entire graph w memory until backward pass, nie mo≈ºna benefit z global graph optimizations.

##### TensorFlow's Static Graph Optimization {#sec-ai-frameworks-tensorflows-static-graph-optimization-3f21}

**TensorFlow 1.x:** Static graph approach. Separate graph construction od execution. (TensorFlow 2.x u≈ºywa eager execution by default, ale `tf.function` compilation available).

```python
import tensorflow.compat.v1 as tf
tf.disable_v2_behavior()

# Graph definition phase - no computation
x = tf.placeholder(tf.float32, shape=())
y = tf.placeholder(tf.float32, shape=())

z = x * y
w = z + x
loss = w**2

# Symbolic gradient computation during graph construction
gradients = tf.gradients(loss, [x, y])

# Execution phase - actual computation
with tf.Session() as sess:
    for step in range(1000):
        grad_vals, loss_val = sess.run(
            [gradients, loss], feed_dict={x: 2.0, y: 3.0}
        )
```

**Zalety:** Complete graph analysis enables aggressive optimizations (operation fusion, memory layout optimization, parallel execution scheduling). Once compiled, efficient repeated execution. 2-3x performance improvements possible.

**Wady:** More complex debugging workflows, limited flexibility dla dynamic computation patterns.

##### JAX's Functional Differentiation {#sec-ai-frameworks-jaxs-functional-differentiation-4a45}

**JAX:** Functional programming approach. Program transformation instead of operation tracking.

```python
import jax
import jax.numpy as jnp

# Pure function definition
def compute_loss(params, x, y):
    z = x * params["w1"] + y * params["w2"]
    return z**2

# JAX transforms functions rather than tracking operations
grad_fn = jax.grad(compute_loss)  # Returns gradient function
value_and_grad_fn = jax.value_and_grad(compute_loss)

# Multiple gradient modes available
forward_grad_fn = jax.jacfwd(compute_loss)  # Forward mode
reverse_grad_fn = jax.jacrev(compute_loss)  # Reverse mode

# Function transformations compose naturally
batched_grad_fn = jax.vmap(grad_fn)  # Vectorized gradients
jit_grad_fn = jax.jit(grad_fn)  # Compiled gradients

# Execution with immutable parameters
params = {"w1": 2.0, "w2": 3.0}
gradients = grad_fn(params, 1.0, 2.0)
```

**Zalety:** Same function transformed dla different differentiation modes. Forward i reverse mode equally accessible. Powerful composition patterns. Functional purity enables mathematical reasoning.

**Wady:** Requires functional programming discipline (immutable data, pure functions). Steeper learning curve.

### Data Structures {#sec-ai-frameworks-data-structures-fe2d}

Machine learning frameworks organizujƒÖ numerical data przez specialized data structures. Najwa≈ºniejsza: **Tensors**.

#### Tensors {#sec-ai-frameworks-tensors-3577}

:::{.callout-definition title="Tensor"}
***Tensors*** = multidimensional arrays. Fundamental data structure w ML systems. Unified representation dla scalars, vectors, matrices i higher-dimensional data z hardware-optimized operations.
:::

**Hierarchy:**
- 0D tensor (scalar): single value
- 1D tensor (vector): sequence of values
- 2D tensor (matrix): rows √ó columns
- 3D+ tensor: nested structures (np. batch of images: batch √ó height √ó width √ó channels)

**Framework tensor properties:**
1. **Shape:** Dimensional structure (np. `[32, 224, 224, 3]` dla batch 32 images, 224√ó224, 3 color channels)
2. **Dtype:** Data type (FP32, FP16, INT8, etc.)
3. **Device:** Gdzie stored (CPU, GPU, TPU)
4. **Strides:** Memory layout pattern (row-major vs column-major)
5. **Gradient tracking:** Czy requires_grad dla automatic differentiation

**Memory layout:** Physical memory jest linear. Tensors = multidimensional abstraction. **Strides** mapujƒÖ multidimensional indices ‚Üí linear memory addresses.

**Operations:** Matrix multiplication, convolution, element-wise ops (add, multiply, ReLU), reduction ops (sum, mean, max), reshaping (view, transpose, permute).

**Performance considerations:**
- Contiguous memory access faster ni≈º strided access
- Hardware (GPU) preferuje okre≈õlone layouts
- Broadcast operations avoid explicit copying
- In-place operations save memory ale break gradient tracking

## Framework Architecture {#sec-ai-frameworks-framework-architecture-0982}

Modern frameworks majƒÖ layered architecture:
1. **User-facing API:** High-level (Keras, PyTorch Lightning, Hugging Face)
2. **Core framework:** Graph construction, AD, optimization (TensorFlow, PyTorch, JAX)
3. **Computation kernels:** Optimized implementations (cuDNN, cuBLAS, MKL)
4. **Hardware layer:** CPU, GPU, TPU, custom accelerators

**Modularity:** Mo≈ºliwo≈õƒá swap components (np. r√≥≈ºne backends dla Keras).

## Framework Ecosystem {#sec-ai-frameworks-framework-ecosystem-4f2e}

**PyTorch ecosystem:**
- PyTorch Lightning: High-level training abstractions
- torchvision, torchaudio, torchtext: Domain-specific utilities
- ONNX export: Interoperability
- torch.distributed: Multi-GPU/multi-node training

**TensorFlow ecosystem:**
- Keras: High-level API
- TensorFlow Serving: Production serving
- TensorFlow Lite: Mobile/embedded
- TensorFlow.js: Browser deployment
- TFX: End-to-end ML pipelines

**JAX ecosystem:**
- Flax: Neural network library
- Optax: Optimization library
- Haiku: Neural network modules

## System Integration {#sec-ai-frameworks-system-integration-624f}

Frameworks muszƒÖ integrate z:
- **Data loading:** Efficient data pipelines (tf.data, torch.utils.data.DataLoader)
- **Distributed training:** Horovod, PyTorch DDP, TensorFlow MirroredStrategy
- **Monitoring:** TensorBoard, Weights & Biases, MLflow
- **Deployment:** Docker, Kubernetes, cloud platforms (SageMaker, Vertex AI)
- **Hardware:** CUDA, ROCm, Metal Performance Shaders

## Major Framework Platform Analysis {#sec-ai-frameworks-major-framework-platform-analysis-6177}

**TensorFlow:**
- **Strengths:** Production deployment, mobile (TFLite), serving infrastructure, comprehensive ecosystem
- **Weaknesses:** Historically complex API, steeper learning curve
- **Use cases:** Production systems, mobile apps, distributed training at scale

**PyTorch:**
- **Strengths:** Research-friendly, Pythonic API, excellent debugging, dynamic graphs
- **Weaknesses:** Historically worse production story (improving z TorchServe, torch.jit)
- **Use cases:** Research, prototyping, academia

**JAX:**
- **Strengths:** Functional programming, composable transformations, automatic vectorization, TPU-first
- **Weaknesses:** Smaller ecosystem, functional paradigm unfamiliar dla wielu
- **Use cases:** Research requiring mathematical elegance, TPU deployment, high-performance computing

**ONNX (Open Neural Network Exchange):**
- Interoperability format miƒôdzy frameworks
- Train w PyTorch, deploy w TensorFlow Serving (przez ONNX)
- Runtime optimizations (ONNX Runtime)

## Deployment Environment-Specific Frameworks {#sec-ai-frameworks-deployment-environmentspecific-frameworks-f333}

### Distributed Computing {#sec-ai-frameworks-distributed-computing-platform-optimization-5423}

**Ray:** Distributed computing framework. Distribute training across clusters. Automatic scaling, fault tolerance.

**Horovod:** Distributed training library. Works z TensorFlow, PyTorch, JAX. Efficient multi-GPU/multi-node communication (NCCL, Gloo).

### Production Serving {#sec-ai-frameworks-local-processing-lowlatency-optimization-6c65}

**TensorFlow Serving:** Production-grade serving system. REST i gRPC APIs. Model versioning, batching, monitoring.

**TorchServe:** PyTorch production serving. Similar capabilities dla PyTorch models.

**NVIDIA Triton:** Multi-framework inference server. Supports TensorFlow, PyTorch, ONNX. GPU optimization, dynamic batching.

### Mobile & Edge {#sec-ai-frameworks-resourceconstrained-device-optimization-2966}

**TensorFlow Lite:** Mobile i embedded devices. Model optimization (quantization, pruning). Hardware acceleration delegates (GPU, NPU).

**PyTorch Mobile:** PyTorch dla mobile. Supports iOS i Android.

**Core ML (Apple):** Native iOS deployment. Optimized dla Apple Neural Engine.

### Microcontrollers (TinyML) {#sec-ai-frameworks-microcontroller-embedded-system-implementation-5555}

**TensorFlow Lite Micro:** Ultra-constrained devices (microcontrollers). KB memory footprint.

**Edge Impulse:** End-to-end platform dla edge ML. Data collection ‚Üí training ‚Üí deployment.

## Systematic Framework Selection Methodology {#sec-ai-frameworks-systematic-framework-selection-methodology-530e}

**Factors:**

1. **Model requirements:**
   - Architecture complexity (standard vs custom)
   - Dynamic vs static computation
   - Model size i performance needs

2. **Deployment target:**
   - Cloud, edge, mobile, embedded?
   - Latency requirements
   - Hardware availability (GPU, TPU, NPU)

3. **Development priorities:**
   - Research velocity vs production performance
   - Team expertise
   - Debugging requirements

4. **Ecosystem needs:**
   - Pre-trained models availability
   - Community support
   - Integration requirements

5. **Long-term viability:**
   - Community size i activity
   - Corporate backing
   - License considerations

**Decision matrix:**
- **Research/Academia:** PyTorch (ease of use, debugging, flexibility)
- **Production/Enterprise:** TensorFlow (deployment ecosystem, mobile support)
- **High-performance computing:** JAX (TPU optimization, functional programming)
- **Mobile:** TensorFlow Lite, Core ML
- **Embedded:** TensorFlow Lite Micro

## Systematic Framework Performance Assessment {#sec-ai-frameworks-systematic-framework-performance-assessment-30d3}

**Benchmarking dimensions:**
1. **Training speed:** Time per epoch, throughput (samples/sec)
2. **Inference latency:** Single sample prediction time
3. **Memory usage:** Peak memory during training/inference
4. **Scalability:** Multi-GPU efficiency, distributed training overhead
5. **Model accuracy:** Numerical precision differences

**Standardized benchmarks:**
- MLPerf Training i Inference
- DAWNBench
- Framework-specific benchmarks (TorchBench, TensorFlow benchmarks)

**Real-world considerations:**
- End-to-end pipeline performance (data loading, preprocessing, training, deployment)
- Development velocity (time to prototype, debug, optimize)
- Total cost of ownership (compute costs, development time, maintenance)

## Common Framework Selection Misconceptions {#sec-ai-frameworks-common-framework-selection-misconceptions-afb3}

**‚ùå "PyTorch tylko dla research, TensorFlow tylko dla production"**
Reality: PyTorch ma TorchServe, torch.jit dla production. TensorFlow 2.x ma eager execution dla research.

**‚ùå "Framework X jest zawsze szybszy ni≈º Y"**
Reality: Performance zale≈ºy od model architecture, hardware, optimization. Benchmarki pokazujƒÖ mixed results.

**‚ùå "Muszƒô u≈ºyƒá najnowszego framework"**
Reality: Stable, well-supported framework czƒôsto lepszy ni≈º bleeding-edge. Ecosystem maturity matters.

**‚ùå "Static graphs zawsze szybsze ni≈º dynamic"**
Reality: Modern frameworks (TF 2.x, PyTorch JIT) blur distinction. Difference often minimal z proper optimization.

**‚ùå "Mogƒô ≈Çatwo switch framework p√≥≈∫niej"**
Reality: Migration jest costly. Architectural assumptions, custom ops, trained models - wszystko tied do framework. Choose carefully upfront.

## Summary {#sec-ai-frameworks-summary-c1f4}

**Key takeaways:**

1. **Frameworki abstrahujƒÖ complexity:** Automatic differentiation, hardware acceleration, distributed training - wszystko handled automatically.

2. **Static vs Dynamic graphs = trade-off:** Optimization potential (static) vs flexibility (dynamic). Modern frameworks offer hybrid approaches.

3. **Automatic differentiation = core capability:** Forward mode (few inputs, many outputs), Reverse mode (many inputs, one output - ideal dla neural networks).

4. **Framework wyb√≥r = architectural decision:** PyTorch (research/flexibility), TensorFlow (production/mobile), JAX (HPC/mathematical elegance).

5. **Deployment target matters:** Different frameworks optimize dla different targets (cloud, edge, mobile, embedded).

6. **Ecosystem > raw performance:** Pre-trained models, community support, documentation czƒôsto wa≈ºniejsze ni≈º micro-benchmarks.

7. **Migration jest trudny:** Architectural lock-in. Choose carefully based on long-term needs.

**Praktyczne porady:**
- Start z najpopularniejszym framework w swojej domain (PyTorch dla research, TensorFlow dla production mobile)
- Don't cargo-cult - understand WHY framework suits your needs
- Benchmark na YOUR models i hardware, nie synthetic benchmarks
- Consider total cost: development time + compute costs + maintenance
- Interoperability (ONNX) mo≈ºe help ale isn't free lunch

**Final thought:** Frameworki evolve szybko. Principles (computational graphs, AD, hardware abstraction) remain constant. Understand fundamentals ‚Üí adapt as tools change.
