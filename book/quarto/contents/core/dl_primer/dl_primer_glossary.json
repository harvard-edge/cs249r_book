{
  "metadata": {
    "chapter": "dl_primer",
    "version": "1.0.0",
    "generated": "2025-09-15T14:08:03.493898",
    "total_terms": 41,
    "standardized": true,
    "last_updated": "2025-09-15T15:01:37.285078"
  },
  "terms": [
    {
      "term": "activation function",
      "definition": "A mathematical function applied to the weighted sum of inputs in a neural network neuron to introduce nonlinearity, enabling the network to learn complex patterns beyond simple linear combinations.",
      "chapter_source": "dl_primer",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "artificial intelligence",
      "definition": "The field of computer science focused on creating systems that can perform tasks typically requiring human intelligence, such as perception, reasoning, learning, and decision-making.",
      "chapter_source": "dl_primer",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "artificial neural network",
      "definition": "A computational model inspired by biological neural networks, consisting of interconnected nodes (neurons) organized in layers that can learn patterns from data through adjustable weights and biases.",
      "chapter_source": "dl_primer",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "backpropagation",
      "definition": "An algorithm that computes gradients of the loss function with respect to network weights by propagating error signals backward through the network layers, enabling systematic weight updates during training.",
      "chapter_source": "dl_primer",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "batch size",
      "definition": "The number of training examples processed simultaneously during one iteration of neural network training, affecting both computational efficiency and gradient estimation quality.",
      "chapter_source": "dl_primer",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "bias",
      "definition": "A learnable parameter added to the weighted sum in each neuron that allows the activation function to shift, providing additional flexibility for the network to fit complex patterns.",
      "chapter_source": "dl_primer",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "biological neuron",
      "definition": "A cell in the nervous system that receives, processes, and transmits information through electrical and chemical signals, serving as inspiration for artificial neural networks.",
      "chapter_source": "dl_primer",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "convolutional neural network",
      "definition": "A specialized neural network architecture designed for processing grid-like data such as images, using convolutional layers that apply filters to detect local features.",
      "chapter_source": "dl_primer",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "cross-entropy loss",
      "definition": "A loss function commonly used in classification tasks that measures the difference between predicted probability distributions and true class labels, providing strong gradients for effective learning.",
      "chapter_source": "dl_primer",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "deep learning",
      "definition": "A subfield of machine learning that uses artificial neural networks with multiple layers to automatically learn hierarchical representations from data without explicit feature engineering.",
      "chapter_source": "dl_primer",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "dense layer",
      "definition": "A fully-connected neural network layer where each neuron receives input from all neurons in the previous layer, enabling comprehensive information integration across features.",
      "chapter_source": "dl_primer",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "epoch",
      "definition": "One complete pass through the entire training dataset during neural network training, consisting of multiple batch iterations depending on dataset size and batch size.",
      "chapter_source": "dl_primer",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "feature engineering",
      "definition": "The process of manually designing and extracting relevant features from raw data to improve machine learning model performance, largely automated in deep learning systems.",
      "chapter_source": "dl_primer",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "feedforward network",
      "definition": "A neural network architecture where information flows in one direction from input to output layers without cycles, forming the foundation for many deep learning models.",
      "chapter_source": "dl_primer",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "flops",
      "definition": "Floating Point Operations Per Second, a measure of computational throughput that quantifies the number of mathematical operations involving decimal numbers a system can perform.",
      "chapter_source": "dl_primer",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "forward propagation",
      "definition": "The process of computing neural network predictions by passing input data through successive layers, applying weights, biases, and activation functions at each stage.",
      "chapter_source": "dl_primer",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "gradient descent",
      "definition": "An optimization algorithm that iteratively adjusts neural network parameters in the direction that minimizes the loss function, using gradients to determine update directions and magnitudes.",
      "chapter_source": "dl_primer",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "graphics processing unit",
      "definition": "A specialized processor originally designed for rendering graphics that provides parallel processing capabilities essential for efficient neural network computation and training.",
      "chapter_source": "dl_primer",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "hidden layer",
      "definition": "An intermediate layer in a neural network between input and output layers that learns abstract representations by transforming data through learned weights and activation functions.",
      "chapter_source": "dl_primer",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "hyperparameter",
      "definition": "A configuration setting that controls the learning process but is not learned from data, such as learning rate, batch size, or network architecture choices.",
      "chapter_source": "dl_primer",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "learning rate",
      "definition": "A hyperparameter that determines the step size for weight updates during gradient descent optimization, critically affecting training stability and convergence speed.",
      "chapter_source": "dl_primer",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "loss function",
      "definition": "A mathematical function that quantifies the difference between neural network predictions and true labels, providing the optimization objective for training algorithms.",
      "chapter_source": "dl_primer",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "machine learning",
      "definition": "A subset of artificial intelligence that enables systems to automatically improve performance on tasks through experience and data rather than explicit programming.",
      "chapter_source": "dl_primer",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "mini-batch gradient descent",
      "definition": "A training approach that computes gradients and updates weights using a small subset of training examples simultaneously, balancing computational efficiency with gradient estimation quality.",
      "chapter_source": "dl_primer",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "multilayer perceptron",
      "definition": "A feedforward neural network with one or more hidden layers between input and output layers, capable of learning nonlinear relationships in data.",
      "chapter_source": "dl_primer",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "neural network",
      "definition": "A computational model consisting of interconnected nodes organized in layers that can learn to map inputs to outputs through adjustable connection weights.",
      "chapter_source": "dl_primer",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "overfitting",
      "definition": "A phenomenon where a model learns specific details of training data so well that it fails to generalize to new, unseen examples, typically indicated by high training accuracy but poor validation performance.",
      "chapter_source": "dl_primer",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "parameter",
      "definition": "A learnable component of a neural network, including weights and biases, that gets adjusted during training to minimize the loss function.",
      "chapter_source": "dl_primer",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "perceptron",
      "definition": "The fundamental building block of neural networks, consisting of weighted inputs, a bias term, and an activation function that produces a single output.",
      "chapter_source": "dl_primer",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "relu",
      "definition": "Rectified Linear Unit activation function defined as f(x) = max(0,x) that introduces nonlinearity while maintaining computational efficiency and avoiding vanishing gradient problems.",
      "chapter_source": "dl_primer",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "sigmoid",
      "definition": "An activation function that maps input values to a range between 0 and 1, historically popular but prone to vanishing gradient problems in deep networks.",
      "chapter_source": "dl_primer",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "supervised learning",
      "definition": "A machine learning approach where models learn from labeled training examples to make predictions on new, unlabeled data.",
      "chapter_source": "dl_primer",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "tensor",
      "definition": "A multi-dimensional array used to represent data in neural networks, generalizing scalars (0D), vectors (1D), and matrices (2D) to higher dimensions.",
      "chapter_source": "dl_primer",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "training",
      "definition": "The process of adjusting neural network parameters using labeled data and optimization algorithms to minimize prediction errors and improve performance.",
      "chapter_source": "dl_primer",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "vanishing gradient",
      "definition": "A problem in deep neural networks where gradients become exponentially smaller as they propagate backward through layers, making it difficult for early layers to learn effectively.",
      "chapter_source": "dl_primer",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "weight",
      "definition": "A learnable parameter that determines the strength of connection between neurons in different layers, adjusted during training to minimize the loss function.",
      "chapter_source": "dl_primer",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "weight matrix",
      "definition": "An organized collection of weights connecting one layer to another in a neural network, enabling efficient computation through matrix operations.",
      "chapter_source": "dl_primer",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "ml systems spectrum",
      "definition": "The range of machine learning system deployments from cloud-based systems with abundant resources to tiny embedded devices with severe constraints, each requiring different optimization strategies and trade-offs.",
      "chapter_source": "dl_primer",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "virtuous cycle",
      "definition": "The self-reinforcing process in deep learning where improvements in data availability, algorithms, and computing power each enable further advances in the other areas, accelerating overall progress.",
      "chapter_source": "dl_primer",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "bias terms",
      "definition": "Learnable parameters in neural networks that shift the activation function, allowing neurons to activate even when all inputs are zero, providing additional flexibility for fitting complex patterns.",
      "chapter_source": "dl_primer",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "threshold for activation",
      "definition": "The input level at which a neuron begins to produce significant output, determined by the combination of weights, biases, and the chosen activation function, controlling when the neuron contributes to the network's computation.",
      "chapter_source": "dl_primer",
      "aliases": [],
      "see_also": []
    }
  ]
}
