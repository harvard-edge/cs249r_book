{
  "metadata": {
    "chapter": "data_engineering",
    "version": "1.0.0",
    "generated": "2025-09-15T14:08:03.493315",
    "total_terms": 52,
    "standardized": true,
    "last_updated": "2025-09-15T15:01:37.284610"
  },
  "terms": [
    {
      "term": "active learning",
      "definition": "An approach that intelligently selects the most informative examples for human annotation based on model uncertainty, reducing the amount of labeled data needed for effective training.",
      "chapter_source": "data_engineering",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "apache kafka",
      "definition": "A distributed streaming platform that handles real-time data feeds using a publish-subscribe messaging system, commonly used for building ML data pipelines with high throughput and fault tolerance.",
      "chapter_source": "data_engineering",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "apache spark",
      "definition": "An open-source distributed computing framework that enables large-scale data processing across clusters of computers, revolutionizing ETL operations with in-memory computing capabilities.",
      "chapter_source": "data_engineering",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "batch ingestion",
      "definition": "A data processing pattern that collects and processes data in groups or batches at scheduled intervals, suitable for scenarios where real-time processing is not critical.",
      "chapter_source": "data_engineering",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "batch processing",
      "definition": "A computational approach that processes large volumes of data in discrete chunks during scheduled time windows, enabling efficient handling of massive datasets for ML training.",
      "chapter_source": "data_engineering",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "bounding box",
      "definition": "A rectangular annotation that identifies object locations in images by drawing a box around each object of interest, commonly used in computer vision training datasets.",
      "chapter_source": "data_engineering",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "caching",
      "definition": "A technique for storing frequently accessed data in high-speed storage systems to reduce retrieval latency and improve system performance in ML pipelines.",
      "chapter_source": "data_engineering",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "classification labels",
      "definition": "Simple categorical annotations that assign specific tags or categories to data examples, representing the most basic form of supervised learning annotation.",
      "chapter_source": "data_engineering",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "consensus labeling",
      "definition": "A quality control approach that collects multiple annotations for the same data point to identify controversial cases and improve label reliability through inter-annotator agreement.",
      "chapter_source": "data_engineering",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "crowdsourcing",
      "definition": "A collaborative data collection approach that leverages distributed individuals via the internet to perform annotation tasks, enabling scalable dataset creation through platforms like Amazon Mechanical Turk.",
      "chapter_source": "data_engineering",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "data augmentation",
      "definition": "The process of artificially expanding training datasets by creating modified versions of existing data through transformations like rotation, scaling, or noise injection.",
      "chapter_source": "data_engineering",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "data cascades",
      "definition": "Systemic failures where data quality issues compound over time, creating downstream negative consequences such as model failures, costly rebuilding, or project termination.",
      "chapter_source": "data_engineering",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "data governance",
      "definition": "The framework of policies, procedures, and technologies that ensure data security, privacy, compliance, and ethical use throughout the machine learning pipeline.",
      "chapter_source": "data_engineering",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "data ingestion",
      "definition": "The process of collecting and importing data from various sources into a storage system for further processing, involving both batch and stream processing patterns.",
      "chapter_source": "data_engineering",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "data lake",
      "definition": "A storage repository that holds structured, semi-structured, and unstructured data in its native format, using schema-on-read approaches for flexible data analysis.",
      "chapter_source": "data_engineering",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "data lineage",
      "definition": "The documentation and tracking of data flow through various transformations and processes, providing visibility into data origins and modifications for compliance and debugging.",
      "chapter_source": "data_engineering",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "data pipeline",
      "definition": "The infrastructure and workflows that automate the movement and transformation of data from sources through processing stages to final storage or consumption.",
      "chapter_source": "data_engineering",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "data quality",
      "definition": "The degree to which data meets requirements for accuracy, completeness, consistency, and timeliness, directly impacting machine learning model performance.",
      "chapter_source": "data_engineering",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "data validation",
      "definition": "The process of ensuring incoming data meets quality standards and conforms to expected schemas, preventing downstream issues in ML pipelines.",
      "chapter_source": "data_engineering",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "data warehouse",
      "definition": "A centralized repository optimized for analytical queries (OLAP) that stores integrated, structured data from multiple sources in a standardized schema.",
      "chapter_source": "data_engineering",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "dead letter queue",
      "definition": "A separate storage mechanism for data that fails processing, allowing for later analysis and potential reprocessing of problematic data without blocking the main pipeline.",
      "chapter_source": "data_engineering",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "differential privacy",
      "definition": "A mathematical framework that adds calibrated noise to datasets or query results to protect individual privacy while preserving overall data utility for analysis.",
      "chapter_source": "data_engineering",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "distributed computing",
      "definition": "An approach that processes data across multiple machines or processors simultaneously, enabling scalable handling of large datasets through frameworks like Apache Spark.",
      "chapter_source": "data_engineering",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "elt (extract, load, transform)",
      "definition": "A data processing paradigm that first loads raw data into the target system before applying transformations, providing flexibility for evolving analytical needs.",
      "chapter_source": "data_engineering",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "etl (extract, transform, load)",
      "definition": "A traditional data processing paradigm that transforms data before loading it into a data warehouse, resulting in ready-to-query formatted data.",
      "chapter_source": "data_engineering",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "feature engineering",
      "definition": "The process of using domain knowledge to create new features from existing data that make machine learning algorithms work more effectively.",
      "chapter_source": "data_engineering",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "feature store",
      "definition": "A specialized data storage system that provides standardized, reusable features for machine learning, enabling feature sharing across multiple models and teams.",
      "chapter_source": "data_engineering",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "generalization",
      "definition": "The ability of a machine learning model to perform well on unseen data that differs from the training set, often improved through diverse and high-quality training data.",
      "chapter_source": "data_engineering",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "graceful degradation",
      "definition": "A system design principle where services continue functioning with reduced capabilities when faced with partial failures or data unavailability.",
      "chapter_source": "data_engineering",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "hdfs (hadoop distributed file system)",
      "definition": "A distributed file system designed to store large datasets across clusters of commodity hardware, providing scalability and fault tolerance for big data applications.",
      "chapter_source": "data_engineering",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "k-anonymity",
      "definition": "A privacy technique that ensures each record in a dataset is indistinguishable from at least k-1 other records by generalizing quasi-identifiers.",
      "chapter_source": "data_engineering",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "keyword spotting (kws)",
      "definition": "A technology that detects specific wake words or phrases in audio streams, typically used in voice-activated devices with constraints on power consumption and latency.",
      "chapter_source": "data_engineering",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "masking",
      "definition": "An anonymization technique that alters or obfuscates sensitive values so they cannot be directly traced back to the original data subject.",
      "chapter_source": "data_engineering",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "metadata",
      "definition": "Descriptive information about datasets that includes details about data collection, quality metrics, validation status, and other contextual information essential for data management.",
      "chapter_source": "data_engineering",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "nosql",
      "definition": "A category of database systems designed to handle large volumes of unstructured or semi-structured data with flexible schemas, often used in big data applications.",
      "chapter_source": "data_engineering",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "olap (online analytical processing)",
      "definition": "A database approach optimized for complex analytical queries across large datasets, typically used in data warehouses for business intelligence.",
      "chapter_source": "data_engineering",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "oltp (online transaction processing)",
      "definition": "A database approach optimized for frequent, short transactions and real-time processing, commonly used in operational applications.",
      "chapter_source": "data_engineering",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "outlier detection",
      "definition": "The process of identifying data points that significantly deviate from normal patterns, which may represent errors, anomalies, or valuable rare events.",
      "chapter_source": "data_engineering",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "partitioning",
      "definition": "A database technique that divides large datasets into smaller, manageable segments based on specific criteria to improve query performance and system scalability.",
      "chapter_source": "data_engineering",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "pseudonymization",
      "definition": "A privacy technique that replaces direct identifiers with artificial identifiers while maintaining the ability to trace records for analysis purposes.",
      "chapter_source": "data_engineering",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "schema",
      "definition": "The structure and format definition of data that specifies data types, field names, and relationships, essential for data validation and processing consistency.",
      "chapter_source": "data_engineering",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "schema-on-read",
      "definition": "An approach used in data lakes where data structure is defined and enforced at the time of reading rather than when storing, providing flexibility for diverse data types.",
      "chapter_source": "data_engineering",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "segmentation maps",
      "definition": "Detailed annotations that classify objects at the pixel level, providing the most granular labeling information but requiring significantly more storage and processing resources.",
      "chapter_source": "data_engineering",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "semi-supervised learning",
      "definition": "A machine learning approach that uses both labeled and unlabeled data for training, leveraging structural assumptions to improve model performance with limited labels.",
      "chapter_source": "data_engineering",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "stream ingestion",
      "definition": "A data processing pattern that handles data in real-time as it arrives, essential for applications requiring immediate processing and low-latency responses.",
      "chapter_source": "data_engineering",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "synthetic data",
      "definition": "Artificially generated data created using algorithms, simulations, or generative models to supplement real-world datasets, addressing limitations in data availability or privacy concerns.",
      "chapter_source": "data_engineering",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "transfer learning",
      "definition": "A machine learning technique that leverages models already trained on one task to improve performance on a different but related task, reducing data requirements.",
      "chapter_source": "data_engineering",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "versioning",
      "definition": "The practice of tracking changes to datasets, models, and pipelines over time, enabling reproducibility, rollback capabilities, and audit trails in ML systems.",
      "chapter_source": "data_engineering",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "weak supervision",
      "definition": "An approach that uses lower-quality labels obtained more efficiently through heuristics, distant supervision, or programmatic methods rather than manual expert annotation.",
      "chapter_source": "data_engineering",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "web scraping",
      "definition": "An automated technique for extracting data from websites to build custom datasets, requiring careful consideration of legal, ethical, and technical constraints.",
      "chapter_source": "data_engineering",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "schema evolution",
      "definition": "The process of modifying data schemas over time while maintaining backward compatibility and ensuring continued functionality of dependent systems and applications.",
      "chapter_source": "data_engineering",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "stream processing",
      "definition": "Real-time data processing approach that handles continuous flows of data as it arrives, enabling immediate responses to events and pattern detection.",
      "chapter_source": "data_engineering",
      "aliases": [],
      "see_also": []
    }
  ]
}
