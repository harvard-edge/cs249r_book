---
bibliography: optimizations.bib
quiz: optimizations_quizzes.json
concepts: optimizations_concepts.yml
glossary: optimizations_glossary.json
crossrefs: optimizations_xrefs.json
---

# Model Optimizations {#sec-model-optimizations}

::: {layout-narrow}
::: {.column-margin}
*DALL·E 3 Prompt: Illustration of a neural network model represented as a busy construction site, with a diverse group of construction workers, both male and female, of various ethnicities, labeled as 'pruning', 'quantization', and 'sparsity'. They are working together to make the neural network more efficient and smaller, while maintaining high accuracy. The 'pruning' worker, a Hispanic female, is cutting unnecessary connections from the middle of the network. The 'quantization' worker, a Caucasian male, is adjusting or tweaking the weights all over the place. The 'sparsity' worker, an African female, is removing unnecessary nodes to shrink the model. Construction trucks and cranes are in the background, assisting the workers in their tasks. The neural network is visually transforming from a complex and large structure to a more streamlined and smaller one.*
:::

\noindent
![](images/png/cover_model_optimizations.png)

:::

## Purpose {.unnumbered}

**Problem:** Research-optimized models (accuracy first) vs production deployment constraints (memory, latency, energy, cost).

**Gap przykłady:**
- State-of-the-art LLM: setki GB RAM needed
- Mobile device: kilka GB available
- Cloud inference: latency <100ms target, cost optimization
- Edge devices: energy constraints, limited compute

**Model optimization = bridge:** Transform research models → deployable systems. Maintain performance while meeting constraints.

::: {.callout-tip title="Learning Objectives"}

- Porównać optimization techniques: pruning, quantization, knowledge distillation, NAS
- Evaluate trade-offs między numerical precision a accuracy/energy/hardware compatibility
- Apply tripartite optimization framework (model representation, numerical precision, architectural efficiency)
- Analyze hardware-aware design principles
- Implement sparsity exploitation i dynamic computation
- Design integrated optimization pipelines kombinując multiple techniques
- Assess automated optimization approaches (AutoML)

:::

## Model Optimization Fundamentals {#sec-model-optimizations-model-optimization-fundamentals-064e}

**Central tension:** Model sophistication vs computational feasibility.

**Resource gap:** Contemporary research models często exceed deployment constraints przez orders of magnitude.
- Memory: Setki GB (research) vs kilka GB (mobile)
- Latency: Seconds (acceptable w research) vs milliseconds (production)
- Energy: Unlimited power (datacenter) vs battery constrained (mobile)
- Cost: Expensive inference OK (research) vs cost-effective at scale (production)

**Optimization goal:** Reduce resource requirements zachowując accuracy.

## Optimization Framework {#sec-model-optimizations-optimization-framework-1c8e}

**3 orthogonal dimensions:**

1. **Model Representation** (Structure/Sparsity):
   - Pruning (remove parameters/connections)
   - Knowledge distillation (compress knowledge do smaller model)
   - Neural architecture search (find efficient architectures)
   - Low-rank factorization (approximate weight matrices)

2. **Numerical Precision** (Compute/Memory):
   - FP32 → FP16 → INT8 → binary
   - Quantization-aware training
   - Mixed precision (różne precision dla różnych layers)

3. **Architectural Efficiency** (Design):
   - Hardware-aware design (optimize dla konkretnego hardware)
   - Adaptive computation (dynamic depth/width)
   - Sparse operations (exploit sparsity w computation)

**Framework navigation:** Different deployment contexts wymagają different optimization strategies.

## Deployment Context {#sec-model-optimizations-deployment-context-c1b0}

### Practical Deployment {#sec-model-optimizations-practical-deployment-6148}

**Cloud deployment:**
- Constraints: Cost per inference, throughput (queries/sec)
- Priorities: Batch size optimization, GPU utilization
- Techniques: Quantization (INT8), model parallelism, batching

**Mobile deployment:**
- Constraints: Memory (2-4GB), energy (battery life), latency (<50ms)
- Priorities: Model size, energy efficiency
- Techniques: Aggressive quantization (INT8/INT4), pruning, knowledge distillation, hardware-specific (CoreML, TFLite)

**Edge/IoT deployment:**
- Constraints: Extreme memory (KB-MB), ultra-low power, real-time inference
- Priorities: Tiny models, minimal energy
- Techniques: Extreme quantization (binary), pruning 90%+, specialized architectures (MobileNet, EfficientNet)

### Balancing Trade-offs {#sec-model-optimizations-balancing-tradeoffs-27bb}

**Accuracy vs Efficiency:** Inevitable trade-off. Optimization reduces accuracy (usually 1-5%). Goal: minimize accuracy loss przy achieving efficiency gains.

**Memory vs Compute:** Quantization saves memory i compute. Pruning saves memory i może save compute (jeśli sparse operations supported przez hardware).

**Latency vs Throughput:** Batch processing improves throughput ale increases latency. Real-time apps require low batch sizes.

## Framework Application and Navigation {#sec-model-optimizations-framework-application-navigation-03d4}

### Mapping Constraints {#sec-model-optimizations-mapping-constraints-021d}

**Constraint mapping przykład:**
- Cloud (cost optimization): INT8 quantization + batching → 4x throughput improvement, <1% accuracy loss
- Mobile (memory + latency): Pruning 50% + INT8 + distillation → 75% size reduction, 3x faster, ~2% accuracy loss
- Edge (extreme constraints): 90% pruning + binary quantization + NAS → 95% size reduction, 10x faster, ~5% accuracy loss (acceptable dla some applications)

### Navigation Strategies {#sec-model-optimizations-navigation-strategies-1c74}

**Systematic approach:**
1. **Profile model:** Identify bottlenecks (memory, compute, latency)
2. **Set targets:** Define acceptable accuracy loss, required speedup
3. **Select techniques:** Choose based na deployment context
4. **Iterate:** Apply techniques, measure, refine
5. **Validate:** Test na target hardware z real workloads

## Optimization Dimensions {#sec-model-optimizations-optimization-dimensions-e571}

### Model Representation {#sec-model-optimizations-model-representation-051a}

**Goal:** Reduce model parameters zachowując representational capacity.

**Techniques:**
- **Pruning:** Remove individual weights (unstructured) lub entire filters/channels (structured)
- **Knowledge distillation:** Train small "student" model do mimic large "teacher" model
- **Low-rank factorization:** Decompose weight matrices W = U × V gdzie U, V są smaller
- **Weight sharing:** Multiple weights share same value (reduces unique parameters)

### Numerical Precision {#sec-model-optimizations-numerical-precision-a93d}

**Goal:** Use lower precision representations reduce memory i compute requirements.

**Precision hierarchy:**
- **FP32 (32-bit floating point):** Standard training precision. 4 bytes per parameter.
- **FP16 (16-bit floating point):** Half precision. 2 bytes. ~2x speedup na modern GPUs. Minimal accuracy loss dla inference, some challenges dla training (gradient underflow).
- **INT8 (8-bit integer):** 1 byte. 4x memory reduction, significant speedup. Requires careful quantization. Typical accuracy loss <1-2%.
- **INT4/Binary:** Extreme quantization. 8-16x compression. Larger accuracy loss (3-10%) ale acceptable dla some applications.

**Quantization benefit:** Memory reduction + faster compute (INT operations faster than FP operations) + energy efficiency (INT ops consume less power).

### Architectural Efficiency {#sec-model-optimizations-architectural-efficiency-d507}

**Goal:** Design architectures optimized dla target hardware.

**Principles:**
- **Depthwise separable convolutions:** Factorize standard convolution → drastically reduce parameters i FLOPs (MobileNet)
- **Inverted residuals:** Expand → depthwise → project pattern (MobileNetV2)
- **Squeeze-and-excitation:** Adaptive channel weighting (improves accuracy przy minimal compute cost)
- **Neural architecture search:** Automatically discover efficient architectures

### Three-Dimensional Optimization Framework {#sec-model-optimizations-threedimensional-optimization-framework-a60e}

**Combined optimization:**

Przykład (mobile deployment):
1. **Architecture:** Start z efficient architecture (MobileNetV3)
2. **Quantization:** Apply INT8 quantization → 4x memory reduction
3. **Pruning:** Prune 30% weights → dodatkowa memory i speedup
4. **Result:** 75-80% size reduction, 3-4x faster inference, <2% accuracy loss

**Synergies:** Techniques często compose well. Quantization + pruning particularly effective together.

## Structural Model Optimization Methods {#sec-model-optimizations-structural-model-optimization-methods-ca9e}

### Pruning {#sec-model-optimizations-pruning-3f36}

**Concept:** Remove unnecessary parameters lub connections z neural network.

**Motivation:** Neural networks often over-parameterized. Wiele weights contribute minimal do final prediction. Pruning removes low-impact weights.

**Types:**

1. **Unstructured pruning:** Remove individual weights based na magnitude. Creates sparse weight matrices.
   - **Pros:** Maximum flexibility, high compression rates (90%+ sparsity możliwe)
   - **Cons:** Sparse matrices require specialized hardware dla speedup. Standard GPUs nie benefit much.

2. **Structured pruning:** Remove entire structures (filters, channels, layers).
   - **Pros:** Immediate speedup na standard hardware (reduced matrix dimensions)
   - **Cons:** Less flexible, lower compression rates (typically 30-50%)

3. **Dynamic pruning:** Adaptive pruning during inference (decide which paths activate based na input).
   - **Pros:** Input-dependent efficiency
   - **Cons:** Overhead z dynamic decisions

**Pruning workflow:**
1. Train full model do convergence
2. Identify weights do pruning (magnitude-based, gradient-based, etc.)
3. Remove selected weights
4. Fine-tune (retrain) pruned model do recover accuracy
5. (Optional) Iterative pruning: repeat steps 2-4

**Lottery Ticket Hypothesis:** Randomly initialized networks contain sparse subnetworks ("winning tickets") that when trained in isolation can match performance full network. Implies pruning może discover these efficient subnetworks [@frankle2019lottery].

### Knowledge Distillation {#sec-model-optimizations-knowledge-distillation-72e7}

**Concept:** Transfer knowledge z large "teacher" model do small "student" model.

**Mechanism:**
- Train large teacher model (high accuracy)
- Use teacher's softmax outputs ("soft targets") do train student
- Soft targets contain more information niż hard labels (class probabilities encode similarity relationships)
- Student learns do mimic teacher's behavior, nie tylko correct labels

**Loss function:** Combination of:
- **Distillation loss:** KL divergence między student i teacher outputs
- **Student loss:** Standard cross-entropy z true labels
- L = α × L_distillation + (1 - α) × L_student

**Temperature scaling:** Soften probability distributions (higher temperature = more uniform distribution, exposes more knowledge).

**Effectiveness:** Student models często achieve 90-95% teacher accuracy przy 10-50x fewer parameters.

**Variations:**
- **Self-distillation:** Model distills knowledge do itself (iterative refinement)
- **Multi-teacher distillation:** Ensemble of teachers → single student
- **Cross-modal distillation:** Transfer knowledge across different modalities

### Structured Approximations {#sec-model-optimizations-structured-approximations-83c1}

**Low-rank factorization:** Decompose weight matrices using SVD lub tensor decomposition.

**Example:** Matrix W (m × n) → U (m × k) × V (k × n) where k << min(m, n).
- **Parameters:** mn → m×k + k×n = k(m+n)
- **Savings:** Significant jeśli k much smaller than m, n

**Tensor decomposition:** Extend do higher-order tensors (convolutional filters). Tucker decomposition, CP decomposition.

**Trade-offs:** Accuracy vs compression ratio. Lower rank = more compression ale potential accuracy loss.

### Neural Architecture Search {#sec-model-optimizations-neural-architecture-search-3915}

**Concept:** Automatically discover efficient neural network architectures.

**Search space:** Defines possible architectures (layer types, connections, hyperparameters).

**Search strategy:**
- **Reinforcement learning:** Train controller do generate architectures, reward based na accuracy/efficiency
- **Evolutionary algorithms:** Mutate i crossover architectures, select fittest
- **Gradient-based:** DARTS - relax discrete architecture search do continuous optimization

**Objective:** Multi-objective optimization - maximize accuracy while minimizing FLOPs/latency/energy.

**Hardware-aware NAS:** Include hardware constraints w search objective. Measure actual latency na target device.

**Challenges:**
- **Computational cost:** Search może require thousands of GPU-hours
- **Techniques to reduce cost:** Weight sharing (share weights across candidate architectures), early stopping, proxy tasks

**Notable results:** EfficientNet (discovered przez NAS) achieves better accuracy-efficiency trade-off niż manually designed architectures.

## Quantization and Precision Optimization {#sec-model-optimizations-quantization-precision-optimization-e90a}

### Precision and Energy {#sec-model-optimizations-precision-energy-2b5a}

**Why quantization matters:**
- **Memory:** FP32 uses 4 bytes, INT8 uses 1 byte → 4x reduction
- **Compute:** INT operations faster niż FP operations (2-4x speedup na modern hardware)
- **Energy:** INT operations consume significantly less power (~5-10x more energy efficient)

**Energy consumption hierarchy:** Binary < INT4 < INT8 < INT16 < FP16 < FP32

### Numeric Encoding and Storage {#sec-model-optimizations-numeric-encoding-storage-d9b4}

**Floating point (FP32):** Sign (1 bit) + Exponent (8 bits) + Mantissa (23 bits). Represents wide range of values z varying precision.

**Float16 (FP16):** Sign (1 bit) + Exponent (5 bits) + Mantissa (10 bits). Reduced range i precision ale sufficient dla many ML tasks.

**Integer (INT8):** Uniform quantization. Represents integers -128 to 127. Requires scale i zero-point dla mapping do original FP range.

**Quantization formula:**
- Forward: q = round(x / scale) + zero_point
- Inverse: x ≈ (q - zero_point) × scale

### Numerical Format Comparison {#sec-model-optimizations-numerical-format-comparison-e4ad}

**Format characteristics:**

| Format  | Bytes | Range            | Typical Use                          |
|---------|-------|------------------|--------------------------------------|
| FP32    | 4     | ±3.4×10³⁸       | Training, high-precision inference   |
| FP16    | 2     | ±6.5×10⁴        | Mixed precision training, inference  |
| BF16    | 2     | ±3.4×10³⁸       | Training (same range as FP32)        |
| INT8    | 1     | -128 to 127     | Inference optimization               |
| INT4    | 0.5   | -8 to 7         | Extreme compression                  |
| Binary  | 0.125 | -1 or +1        | Ultra-low power devices              |

**BFloat16 (BF16):** Google's brain floating point. Same exponent range as FP32 (8 bits) ale reduced mantissa (7 bits). Good dla training (maintains numeric range).

### Precision Reduction Trade-offs {#sec-model-optimizations-precision-reduction-tradeoffs-dcd9}

**Accuracy impact:**
- FP32 → FP16: Minimal (<0.1% typically)
- FP32 → INT8: Small (0.5-2% typically)
- FP32 → INT4: Moderate (2-5%)
- FP32 → Binary: Significant (5-15%) ale acceptable dla some applications

**Speedup:**
- FP32 → INT8: 2-4x inference speedup na modern hardware
- Additional speedup z specialized hardware (INT8 engines na GPUs, NPUs)

**Hardware support:**
- Most modern GPUs: FP16, INT8
- Specialized accelerators (Google TPU, Apple Neural Engine): INT8 i lower
- CPUs: Variable support, generally better dla INT8 than FP16

### Precision Reduction Strategies {#sec-model-optimizations-precision-reduction-strategies-09f1}

**Post-training quantization (PTQ):**
- Quantize already-trained model without retraining
- Calibrate scale/zero-point using small calibration dataset
- **Pros:** Fast, no retraining needed
- **Cons:** Może lose more accuracy (1-3%)

**Quantization-aware training (QAT):**
- Simulate quantization during training (insert fake quantization ops)
- Model learns do compensate dla quantization errors
- **Pros:** Better accuracy preservation (<1% loss typically)
- **Cons:** Requires retraining (more time i compute)

**Mixed precision:**
- Use different precision dla different layers
- Keep sensitive layers (first/last, batch norm) w higher precision
- Quantize remaining layers aggressively
- **Result:** Better accuracy than uniform quantization przy similar efficiency gains

### Extreme Quantization {#sec-model-optimizations-extreme-quantization-a22e}

**Binary neural networks:** Weights i activations constrained do {-1, +1}.
- **Compute:** XNOR operations + popcount (extremely fast)
- **Memory:** 32x compression vs FP32
- **Accuracy:** Significant degradation (10-20%) ale improving z better training techniques
- **Use cases:** Ultra-low power devices, specialized hardware (XNOR gates)

**Ternary quantization:** Weights ∈ {-1, 0, +1}.
- **Sparsity + extreme quantization:** Zero values introduce sparsity
- **Slightly better accuracy** niż binary przy similar compression

### Multi-Technique Optimization Strategies {#sec-model-optimizations-multitechnique-optimization-strategies-8263}

**Combining techniques:**

**Pruning + Quantization:** Powerful combination.
1. Prune model (remove 50-70% weights)
2. Quantize do INT8
3. **Result:** 10-20x compression, 5-10x speedup, <3% accuracy loss

**Distillation + Quantization:**
1. Distill large FP32 teacher → small FP32 student
2. Quantize student do INT8
3. **Result:** Small, fast, accurate model

**NAS + Quantization:** Discover architecture optimized dla quantization.
- Include quantization-aware metrics w NAS objective
- Find architectures that naturally tolerate quantization better

## Architectural Efficiency Techniques {#sec-model-optimizations-architectural-efficiency-techniques-ba84}

### Hardware-Aware Design {#sec-model-optimizations-hardwareaware-design-c30a}

**Principle:** Design architectures specifically dla target hardware characteristics.

**GPU optimization:**
- Prefer large matrix multiplications (high arithmetic intensity)
- Minimize memory bandwidth bottlenecks
- Batch normalization i large batch sizes

**Mobile NPU optimization:**
- Depthwise separable convolutions (supported efficiently)
- Channel counts jako multiples of 8 lub 16 (hardware SIMD units)
- Minimize dynamic shapes (enable static compilation)

**Edge/MCU optimization:**
- Extremely small models (<100KB)
- Integer-only operations
- Minimal control flow (predictable execution)

**Examples:**
- **MobileNet:** Depthwise separable convolutions optimized dla mobile
- **EfficientNet:** NAS-discovered architecture balancing depth, width, resolution
- **SqueezeNet:** Fire modules (squeeze + expand) dla tiny models

### Adaptive Computation Methods {#sec-model-optimizations-adaptive-computation-methods-4513}

**Dynamic depth:** Adjust network depth based na input difficulty.
- **Early exit:** Easy inputs exit after shallow layers, hard inputs use full depth
- **Implementation:** Add auxiliary classifiers at intermediate layers
- **Savings:** Average inference time reduces (easy inputs dominate many datasets)

**Dynamic width:** Adjust number channels based na input.
- **Slimmable networks:** Train single network that works at multiple width multipliers
- **Runtime selection:** Choose width based na latency budget

**Input-dependent routing:** Route different inputs through different network paths.
- **Mixture of Experts:** Sparse gating selects subset of experts dla each input
- **Conditional computation:** Only activate portions of network based na input

### Sparsity Exploitation {#sec-model-optimizations-sparsity-exploitation-d3d7}

**Sources of sparsity:**
1. **Weight sparsity:** From pruning (many weights set do zero)
2. **Activation sparsity:** ReLU creates zeros w activations (~50% for typical networks)

**Sparse matrix operations:**
- **Dense matrix multiply:** O(n³) operations, uses all matrix elements
- **Sparse matrix multiply:** Only compute non-zero elements → significant savings

**Hardware considerations:**
- **CPUs:** Moderate benefit z sparse ops (library support: Intel MKL Sparse)
- **GPUs:** Limited benefit na standard GPUs (irregular memory access). Specialized sparse engines (NVIDIA A100 Sparse Tensor Cores) show 2x speedup.
- **Specialized accelerators:** Hardware designed dla sparse operations (Google TPU sparsity support, Cerebras)

**Structured sparsity:** Impose patterns (block sparsity, channel pruning) easier dla hardware do exploit.

## Implementation Strategy and Evaluation {#sec-model-optimizations-implementation-strategy-evaluation-a052}

### Profiling and Opportunity Analysis {#sec-model-optimizations-profiling-opportunity-analysis-206b}

**Step 1: Profile baseline:**
- Measure latency (layer-by-layer breakdown)
- Memory usage (weights, activations, peak)
- Energy consumption (jeśli possible)
- Identify bottlenecks

**Tools:**
- TensorFlow Profiler, PyTorch Profiler
- NVIDIA Nsight (GPU profiling)
- Mobile: Xcode Instruments (iOS), Android Profiler

**Step 2: Analyze opportunities:**
- Which layers consume most compute/memory?
- Which operations have low arithmetic intensity (memory-bound)?
- Which parts are redundant/over-parameterized?

### Measuring Optimization Effectiveness {#sec-model-optimizations-measuring-optimization-effectiveness-63fb}

**Metrics:**
1. **Accuracy:** Measure degradation (validation set)
2. **Model size:** Parameters count, disk size
3. **Inference latency:** Measure na target hardware
4. **Throughput:** Queries per second (particularly dla server deployment)
5. **Energy consumption:** Battery drain (mobile), power usage (datacenter)
6. **Memory footprint:** Peak RAM usage during inference

**Pareto frontier:** Plot accuracy vs efficiency. Goal: push frontier (better accuracy dla given efficiency lub better efficiency dla given accuracy).

### Multi-Technique Integration Strategies {#sec-model-optimizations-multitechnique-integration-strategies-70dc}

**Systematic pipeline:**

1. **Start z efficient architecture:** MobileNet, EfficientNet, etc. (lub NAS)
2. **Train z best practices:** Proper regularization, data augmentation
3. **Apply pruning:** Iterative magnitude pruning (30-50%)
4. **Knowledge distillation** (optional): jeśli large teacher model available
5. **Quantization:** QAT preferred (better accuracy), PTQ acceptable jeśli time-constrained
6. **Profile i iterate:** Measure na target hardware, refine

**Order matters:** Generally: Architecture selection → Training → Pruning → Distillation → Quantization. But experimentation recommended.

## AutoML and Automated Optimization Strategies {#sec-model-optimizations-automl-automated-optimization-strategies-329f}

### AutoML Optimizations {#sec-model-optimizations-automl-optimizations-dbbf}

**Automated search dla optimal optimization strategy:**
- **What do optimize:** Architecture, pruning ratio, quantization scheme, etc.
- **Search algorithm:** Bayesian optimization, evolutionary algorithms, RL
- **Objective:** Multi-objective (accuracy, latency, memory, energy)

**Examples:**
- **AutoML for Model Compression (AMC):** RL-based automated pruning
- **Once-for-All (OFA):** Single network supports many deployment scenarios (progressive shrinking)
- **ProxylessNAS:** NAS that directly optimizes dla target hardware latency

### Optimization Strategies {#sec-model-optimizations-optimization-strategies-c725}

**Multi-objective optimization:**
- Pareto frontier discovery: Find best accuracy-efficiency trade-offs
- Constraint satisfaction: Given max latency, find best accuracy
- Cost minimization: Minimize inference cost subject do accuracy threshold

**Strategies:**
- **Evolutionary algorithms:** Population of candidate optimizations, mutate/crossover/select
- **Reinforcement learning:** Agent learns do apply optimization techniques
- **Gradient-based:** Where possible (architecture search, differentiable pruning)

### AutoML Optimization Challenges {#sec-model-optimizations-automl-optimization-challenges-be63}

**Computational cost:** AutoML search extremely expensive (thousands GPU-hours).

**Solutions:**
- **Early stopping:** Terminate bad candidates early
- **Weight sharing:** Share weights across candidate architectures
- **Proxy tasks:** Search na smaller dataset/model, transfer do full scale
- **Transfer learning:** Reuse search results across similar tasks

**Generalization:** Search results may not transfer perfectly do different hardware lub datasets. Validation na target platform kluczowy.

## Implementation Tools and Software Frameworks {#sec-model-optimizations-implementation-tools-software-frameworks-5681}

**TensorFlow:**
- TensorFlow Lite: Mobile/embedded optimization (quantization, pruning)
- TensorFlow Model Optimization Toolkit: Pruning, quantization APIs

**PyTorch:**
- PyTorch Quantization: PTQ i QAT support
- torch.nn.utils.prune: Pruning utilities
- TorchServe: Production serving z optimization

**ONNX:**
- ONNX Runtime: Cross-framework inference z optimizations
- Quantization tools

**Specialized tools:**
- **Neural Network Distiller (Intel):** Pruning, quantization, knowledge distillation
- **NVIDIA TensorRT:** High-performance inference optimization dla NVIDIA GPUs
- **Apache TVM:** Compiler framework dla optimizing models dla diverse hardware

**Hardware-specific:**
- **Core ML (Apple):** iOS optimization
- **SNPE (Qualcomm):** Snapdragon NPU optimization
- **OpenVINO (Intel):** Intel hardware optimization

## Summary {#sec-model-optimizations-summary-c1f4}

**Key takeaways:**

1. **Optimization = necessity, not luxury:** Research models rarely deployable as-is. Optimization makes ML practical.

2. **Three orthogonal dimensions:**
   - Model representation (pruning, distillation, NAS)
   - Numerical precision (quantization)
   - Architectural efficiency (hardware-aware design)

3. **Trade-offs inevitable:** Efficiency gains → small accuracy loss (typically 1-5%). Goal: minimize loss while maximizing gains.

4. **Techniques compose:** Pruning + quantization particularly powerful. Typical: 10-20x compression, 5-10x speedup, <3% accuracy loss.

5. **Hardware matters:** Different hardware prefers different optimizations. Profile na target device.

6. **Context-specific strategies:**
   - Cloud: INT8 quantization, batching
   - Mobile: Pruning + INT8 + distillation
   - Edge/IoT: Extreme quantization + tiny architectures (NAS)

7. **AutoML emerging:** Automated optimization reduces manual tuning. Still expensive ale improving.

8. **Systematic workflow:**
   - Profile baseline
   - Select techniques based na constraints
   - Apply iteratively
   - Measure na target hardware
   - Refine

**Practical recommendations:**
- Start z efficient architecture (don't optimize inefficient designs)
- Quantization (INT8) = best bang for buck (4x compression, 2-4x speedup, <2% loss)
- Combine techniques dla maximum effect
- Always validate na target hardware (simulations misleading)
- Consider total cost (development time + compute + deployment)

**Future directions:** Better AutoML (cheaper search), hardware co-design (optimize models + hardware together), novel quantization schemes (learned quantization), sparsity exploitation (better hardware support).
