{
  "metadata": {
    "chapter": "training",
    "version": "1.0.0",
    "generated": "2025-09-15T14:08:03.503564",
    "total_terms": 43,
    "standardized": true,
    "last_updated": "2025-09-15T15:01:37.291387"
  },
  "terms": [
    {
      "term": "activation checkpointing",
      "definition": "A memory optimization technique that reduces memory usage during backpropagation by selectively discarding and recomputing activations instead of storing all intermediate results.",
      "chapter_source": "training",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "activation function",
      "definition": "A mathematical function applied element-wise to neural network outputs to introduce non-linearity, enabling networks to learn complex patterns and preventing them from collapsing into linear models.",
      "chapter_source": "training",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "adam optimization",
      "definition": "An adaptive learning rate optimization algorithm that combines momentum and RMSprop by maintaining exponentially decaying averages of both gradients and squared gradients for each parameter.",
      "chapter_source": "training",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "all-reduce",
      "definition": "A collective communication operation in distributed computing where each process contributes data and all processes receive the combined result, commonly used for gradient aggregation in distributed training.",
      "chapter_source": "training",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "application-specific integrated circuit",
      "definition": "A specialized chip designed for specific tasks that offers maximum efficiency by abandoning general-purpose flexibility, exemplified by Cerebras Wafer-Scale Engine for machine learning training.",
      "chapter_source": "training",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "automatic mixed precision",
      "definition": "A training technique that automatically manages the use of different numerical precisions (FP16, FP32) to optimize memory usage and computational speed while maintaining model accuracy.",
      "chapter_source": "training",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "backpropagation",
      "definition": "An algorithm that computes gradients by systematically applying the chain rule backward through a neural network's computational graph, enabling parameter updates during training.",
      "chapter_source": "training",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "batch normalization",
      "definition": "A technique that normalizes inputs to each layer to have zero mean and unit variance, which stabilizes training and often allows for higher learning rates and faster convergence.",
      "chapter_source": "training",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "batched operations",
      "definition": "Matrix computations that process multiple inputs simultaneously, converting matrix-vector operations into more efficient matrix-matrix operations to improve hardware utilization.",
      "chapter_source": "training",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "bfloat16",
      "definition": "A 16-bit floating-point format developed by Google Brain that maintains the same dynamic range as FP32 but with reduced precision, making it particularly suitable for deep learning training.",
      "chapter_source": "training",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "cerebras wafer-scale engine",
      "definition": "A revolutionary single-wafer processor containing 2.6 trillion transistors and 850,000 cores, designed to eliminate inter-device communication bottlenecks in large-scale machine learning training.",
      "chapter_source": "training",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "data parallelism",
      "definition": "A distributed training strategy that splits the dataset across multiple devices while each device maintains a complete copy of the model, enabling parallel computation of gradients.",
      "chapter_source": "training",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "dense matrix-matrix multiplication",
      "definition": "The fundamental computational operation in neural networks that dominates training time, accounting for 60-90% of computation in typical models.",
      "chapter_source": "training",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "distributed training",
      "definition": "A method of training machine learning models across multiple machines or devices to handle larger datasets and models that exceed single-device computational or memory capacity.",
      "chapter_source": "training",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "dying relu problem",
      "definition": "A phenomenon where ReLU neurons become permanently inactive and output zero for all inputs, preventing them from contributing to learning when weighted inputs consistently produce negative values.",
      "chapter_source": "training",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "field-programmable gate array",
      "definition": "Reconfigurable hardware that can be programmed for specific tasks, offering flexibility between general-purpose processors and application-specific integrated circuits, useful for custom ML accelerations.",
      "chapter_source": "training",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "forward pass",
      "definition": "The computation phase where input data flows through a neural network's layers to produce outputs, involving matrix multiplications and activation function applications.",
      "chapter_source": "training",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "fp16 computation",
      "definition": "The use of 16-bit floating-point arithmetic for neural network operations to reduce memory usage and increase computational speed on modern hardware accelerators.",
      "chapter_source": "training",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "gradient accumulation",
      "definition": "A technique that simulates larger batch sizes by accumulating gradients from multiple smaller batches before updating model parameters, enabling training with limited memory.",
      "chapter_source": "training",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "gradient clipping",
      "definition": "A regularization technique that prevents gradient explosion by limiting the magnitude of gradients during backpropagation, typically by scaling gradients when their norm exceeds a threshold.",
      "chapter_source": "training",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "gradient descent",
      "definition": "The fundamental optimization algorithm that iteratively adjusts model parameters in the direction opposite to the gradient of the loss function to minimize training error.",
      "chapter_source": "training",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "gradient synchronization",
      "definition": "The process in distributed training where locally computed gradients are aggregated across devices to ensure all devices update their parameters consistently.",
      "chapter_source": "training",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "graphics processing unit",
      "definition": "Specialized parallel processors originally designed for graphics rendering but widely adopted for machine learning due to their ability to efficiently execute matrix operations and handle thousands of parallel computations.",
      "chapter_source": "training",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "hybrid parallelism",
      "definition": "A distributed training approach that combines data parallelism and model parallelism to leverage the benefits of both strategies for training very large models.",
      "chapter_source": "training",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "loss scaling",
      "definition": "A technique used in mixed-precision training that multiplies the loss by a large factor before backpropagation to prevent gradient underflow in reduced precision formats.",
      "chapter_source": "training",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "mini-batch processing",
      "definition": "An optimization approach that computes gradients over small batches of examples, balancing the computational efficiency of batch processing with the memory constraints of stochastic methods.",
      "chapter_source": "training",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "mixed-precision training",
      "definition": "A training methodology that combines different numerical precisions (typically FP16 and FP32) to optimize memory usage and computational speed while maintaining training stability.",
      "chapter_source": "training",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "model parallelism",
      "definition": "A distributed training strategy that splits a neural network model across multiple devices, with each device responsible for computing a portion of the network.",
      "chapter_source": "training",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "momentum",
      "definition": "An optimization technique that accumulates a velocity vector across iterations to help gradient descent navigate through local minima and accelerate convergence in consistent gradient directions.",
      "chapter_source": "training",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "pipeline parallelism",
      "definition": "A form of model parallelism where different layers of a model are placed on different devices and data flows through them in a pipeline fashion, allowing multiple batches to be processed simultaneously.",
      "chapter_source": "training",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "prefetching",
      "definition": "A system optimization technique that loads data into memory before it is needed, overlapping data loading with computation to reduce idle time and improve training throughput.",
      "chapter_source": "training",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "rectified linear unit",
      "definition": "An activation function that outputs the input if positive and zero otherwise, widely used in modern neural networks for its computational simplicity and ability to avoid vanishing gradients.",
      "chapter_source": "training",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "rmsprop",
      "definition": "An adaptive learning rate optimization algorithm that maintains a moving average of squared gradients to automatically adjust learning rates for each parameter during training.",
      "chapter_source": "training",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "sigmoid",
      "definition": "An activation function that maps inputs to the range (0,1) through an S-shaped curve, commonly used in output layers for binary classification but prone to vanishing gradient problems.",
      "chapter_source": "training",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "softmax",
      "definition": "An activation function that converts raw scores into a probability distribution where outputs sum to 1, essential for multi-class classification tasks.",
      "chapter_source": "training",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "stochastic gradient descent",
      "definition": "A variant of gradient descent that estimates gradients using individual training examples or small batches rather than the entire dataset, reducing memory requirements and enabling online learning.",
      "chapter_source": "training",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "systolic array",
      "definition": "A specialized hardware architecture that efficiently performs matrix operations by streaming data through a grid of processing elements, minimized data movement and energy consumption.",
      "chapter_source": "training",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "tanh",
      "definition": "An activation function that maps inputs to the range (-1,1) with zero-centered output, helping to stabilize gradient-based optimization compared to sigmoid functions.",
      "chapter_source": "training",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "tensor processing unit",
      "definition": "Google's custom application-specific integrated circuit designed specifically for machine learning workloads, optimized for matrix operations and featuring systolic array architecture.",
      "chapter_source": "training",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "vanishing gradient problem",
      "definition": "A challenge in training deep neural networks where gradients become exponentially smaller as they propagate backward through layers, making it difficult to train early layers effectively.",
      "chapter_source": "training",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "learning rate scheduling",
      "definition": "The systematic adjustment of learning rates during training, using strategies like step decay, exponential decay, or cosine annealing to improve convergence and final model performance.",
      "chapter_source": "training",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "batch normalization",
      "definition": "A technique that normalizes the inputs to each layer during training, reducing internal covariate shift and allowing higher learning rates while improving training stability and speed.",
      "chapter_source": "training",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "distributed training",
      "definition": "Training neural networks across multiple devices or machines, using strategies like data parallelism, model parallelism, or pipeline parallelism to handle larger models and datasets efficiently.",
      "chapter_source": "training",
      "aliases": [],
      "see_also": []
    }
  ]
}
