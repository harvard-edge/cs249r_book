{
  "metadata": {
    "chapter": "ops",
    "version": "1.0.0",
    "generated": "2025-09-15T14:08:03.500340",
    "total_terms": 39,
    "standardized": true,
    "last_updated": "2025-09-15T15:01:37.289052"
  },
  "terms": [
    {
      "term": "alerting",
      "definition": "Automated notification systems that inform teams when metrics exceed predefined thresholds or anomalies are detected in production ML systems.",
      "chapter_source": "ops",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "autoscaling",
      "definition": "Dynamic adjustment of compute resources based on workload demand, automatically scaling up during peak usage and scaling down during low usage to optimize costs and performance.",
      "chapter_source": "ops",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "batch inference",
      "definition": "Processing large volumes of data predictions asynchronously in scheduled batches rather than real-time, suitable for non-interactive applications like reporting or analytics.",
      "chapter_source": "ops",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "canary deployment",
      "definition": "Gradual rollout strategy where a new model version serves a small percentage of traffic while monitoring performance before full deployment, allowing safe validation in production.",
      "chapter_source": "ops",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "ci/cd pipelines",
      "definition": "Continuous Integration and Continuous Delivery automated workflows that streamline model development by integrating testing, validation, and deployment processes.",
      "chapter_source": "ops",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "concept drift",
      "definition": "Performance degradation that occurs when the underlying relationship between input features and target outcomes changes over time, requiring model retraining.",
      "chapter_source": "ops",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "containerization",
      "definition": "Packaging applications and their dependencies into portable, isolated containers using tools like Docker to ensure consistent execution across different environments.",
      "chapter_source": "ops",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "data drift",
      "definition": "Changes in the statistical distribution of input data over time that can degrade model performance, requiring monitoring and potential model updates.",
      "chapter_source": "ops",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "data lineage",
      "definition": "Complete traceable record of data flow from source to consumption, including transformations and dependencies, essential for reproducibility and debugging.",
      "chapter_source": "ops",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "data versioning",
      "definition": "Systematic tracking and management of dataset versions over time, enabling reproducibility and rollback capabilities for machine learning experiments.",
      "chapter_source": "ops",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "devops",
      "definition": "Software development practice that combines development and operations teams to shorten development cycles and deliver high-quality software through automation and collaboration.",
      "chapter_source": "ops",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "experiment tracking",
      "definition": "Systematic logging and management of machine learning experiments, including hyperparameters, metrics, and artifacts, to enable reproducibility and comparison.",
      "chapter_source": "ops",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "feature store",
      "definition": "Centralized repository for storing and serving engineered features consistently across training and inference workflows, preventing training-serving skew.",
      "chapter_source": "ops",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "horizontal scaling",
      "definition": "Increasing system capacity by adding more machines or instances rather than upgrading existing hardware, providing better fault tolerance and load distribution.",
      "chapter_source": "ops",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "infrastructure as code",
      "definition": "Practice of managing and provisioning computing infrastructure through machine-readable configuration files rather than manual processes, enabling version control and automation.",
      "chapter_source": "ops",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "load balancing",
      "definition": "Distribution of incoming requests across multiple server instances to prevent bottlenecks, improve response times, and ensure high availability.",
      "chapter_source": "ops",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "mlops",
      "definition": "Engineering discipline that manages the end-to-end lifecycle of machine learning systems, combining ML development with operational practices for reliable production deployment.",
      "chapter_source": "ops",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "model compression",
      "definition": "Techniques such as quantization, pruning, and distillation used to reduce model size and computational requirements while maintaining acceptable performance.",
      "chapter_source": "ops",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "model deployment",
      "definition": "Process of integrating trained machine learning models into production environments where they can serve predictions to end users or applications.",
      "chapter_source": "ops",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "model drift",
      "definition": "Gradual decline in model performance over time due to changes in data patterns, requiring continuous monitoring and periodic retraining.",
      "chapter_source": "ops",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "model registry",
      "definition": "Centralized repository for storing, versioning, and managing trained machine learning models with associated metadata, facilitating model governance and deployment.",
      "chapter_source": "ops",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "model serving",
      "definition": "Infrastructure and systems that expose deployed machine learning models through APIs to handle prediction requests at scale with appropriate latency and throughput.",
      "chapter_source": "ops",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "model validation",
      "definition": "Systematic evaluation process to ensure models meet performance, robustness, and reliability criteria before production deployment.",
      "chapter_source": "ops",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "model versioning",
      "definition": "Practice of systematically tracking different iterations of machine learning models with associated metadata to enable rollbacks and comparisons.",
      "chapter_source": "ops",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "observability",
      "definition": "Comprehensive monitoring approach that provides insight into system behavior through metrics, logs, and traces, enabling understanding of internal states from external outputs.",
      "chapter_source": "ops",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "online inference",
      "definition": "Real-time prediction serving that processes individual requests with low latency, suitable for interactive applications requiring immediate responses.",
      "chapter_source": "ops",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "orchestration",
      "definition": "Coordination and management of complex workflows and distributed computing tasks, often using platforms like Kubernetes for container management.",
      "chapter_source": "ops",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "rollback",
      "definition": "Process of reverting to a previous stable version of a model or system when issues are detected in production, ensuring service continuity.",
      "chapter_source": "ops",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "serverless",
      "definition": "Cloud computing model where infrastructure is automatically managed by the provider, allowing code execution without server management concerns.",
      "chapter_source": "ops",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "service level agreement (sla)",
      "definition": "Formal contract specifying minimum performance standards and uptime guarantees for production services, with penalties for non-compliance.",
      "chapter_source": "ops",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "service level objective (slo)",
      "definition": "Internal targets for service reliability and performance metrics such as latency, error rates, and availability that guide operational decisions.",
      "chapter_source": "ops",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "shadow deployment",
      "definition": "Testing strategy where new model versions process live traffic in parallel with production models without affecting user-facing results, enabling safe validation.",
      "chapter_source": "ops",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "technical debt",
      "definition": "Long-term maintenance cost accumulated from expedient design decisions during development, particularly problematic in ML systems due to data dependencies and model complexity.",
      "chapter_source": "ops",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "telemetry",
      "definition": "Automated collection and transmission of performance data and metrics from distributed systems, enabling remote monitoring and analysis.",
      "chapter_source": "ops",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "training-serving skew",
      "definition": "Inconsistency between feature preprocessing logic used during model training versus serving, leading to degraded production performance.",
      "chapter_source": "ops",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "a/b testing",
      "definition": "Controlled experimentation method for comparing model performance by randomly assigning users to different model versions and measuring statistical differences in outcomes.",
      "chapter_source": "ops",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "pipeline jungle",
      "definition": "Anti-pattern where complex, interdependent data processing pipelines become difficult to maintain, debug, and modify, leading to technical debt and operational complexity.",
      "chapter_source": "ops",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "programmatic logic controllers",
      "definition": "Industrial control systems used in manufacturing and IoT environments that can be integrated with ML models for automated decision-making in operational technology contexts.",
      "chapter_source": "ops",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "containerized microservices",
      "definition": "Architectural pattern using lightweight containers to package individual services, enabling scalable, maintainable deployment of ML systems across distributed environments.",
      "chapter_source": "ops",
      "aliases": [],
      "see_also": []
    }
  ]
}
