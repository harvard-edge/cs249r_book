{
  "metadata": {
    "chapter": "privacy_security",
    "version": "1.0.0",
    "generated": "2025-09-15T14:08:03.501518",
    "total_terms": 37,
    "standardized": true,
    "last_updated": "2025-09-15T15:01:37.289819"
  },
  "terms": [
    {
      "term": "adversarial attack",
      "definition": "A type of attack where carefully crafted inputs are designed to cause machine learning models to make incorrect predictions while remaining nearly indistinguishable from legitimate data to humans.",
      "chapter_source": "privacy_security",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "adversarial example",
      "definition": "A maliciously modified input that is designed to fool a machine learning model into making an incorrect prediction, often created by adding small, imperceptible perturbations to legitimate data.",
      "chapter_source": "privacy_security",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "adversarial training",
      "definition": "A defense technique that involves training machine learning models on adversarial examples to improve their robustness against adversarial attacks.",
      "chapter_source": "privacy_security",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "anonymization",
      "definition": "The process of removing or modifying personally identifiable information from datasets to protect individual privacy, though often insufficient against sophisticated re-identification attacks.",
      "chapter_source": "privacy_security",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "availability attack",
      "definition": "A type of data poisoning attack that aims to degrade the overall performance of a machine learning model by introducing noise or corrupting training data across multiple classes.",
      "chapter_source": "privacy_security",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "backdoor attack",
      "definition": "A type of attack where hidden triggers are embedded in a model during training, causing malicious behavior only when specific trigger patterns are present in the input.",
      "chapter_source": "privacy_security",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "black-box attack",
      "definition": "An adversarial attack where the attacker has no knowledge of the model's internal architecture, parameters, or training data, and must rely solely on querying the model and observing outputs.",
      "chapter_source": "privacy_security",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "cache timing attack",
      "definition": "A type of side-channel attack that exploits variations in memory cache access patterns to infer sensitive information about program execution or data.",
      "chapter_source": "privacy_security",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "data poisoning",
      "definition": "An attack method where adversaries inject carefully crafted malicious data points into the training dataset to manipulate model behavior in targeted or systematic ways.",
      "chapter_source": "privacy_security",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "differential privacy",
      "definition": "A mathematical framework that provides formal privacy guarantees by adding calibrated noise to computations, ensuring that the inclusion or exclusion of any individual's data has a provably limited effect on the output.",
      "chapter_source": "privacy_security",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "exact model theft",
      "definition": "An attack that aims to extract the precise internal structure, parameters, and architecture of a machine learning model, allowing complete reproduction of the original model.",
      "chapter_source": "privacy_security",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "fault injection attack",
      "definition": "A physical attack that deliberately disrupts hardware operations through techniques like voltage manipulation or electromagnetic interference to induce computational errors and compromise system integrity.",
      "chapter_source": "privacy_security",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "federated learning",
      "definition": "A distributed machine learning approach where models are trained across multiple devices or organizations without centralizing the raw data, helping preserve privacy by keeping data local.",
      "chapter_source": "privacy_security",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "grey-box attack",
      "definition": "An adversarial attack where the attacker has partial knowledge about the model, such as knowing the architecture but not the specific parameters or training data.",
      "chapter_source": "privacy_security",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "hardware trojan",
      "definition": "A malicious modification embedded in hardware components during manufacturing that can remain dormant under normal conditions but trigger harmful behavior when specific conditions are met.",
      "chapter_source": "privacy_security",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "homomorphic encryption",
      "definition": "A cryptographic technique that allows computations to be performed directly on encrypted data without decrypting it first, enabling privacy-preserving machine learning inference.",
      "chapter_source": "privacy_security",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "machine learning security",
      "definition": "The protection of data, models, and infrastructure from unauthorized access, manipulation, or disruption throughout the entire machine learning lifecycle.",
      "chapter_source": "privacy_security",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "membership inference attack",
      "definition": "An attack that attempts to determine whether a specific data point was included in a model's training dataset by analyzing the model's behavior and outputs.",
      "chapter_source": "privacy_security",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "model extraction",
      "definition": "The process of stealing or recreating a machine learning model by observing its input-output behavior, often through systematic querying of model APIs.",
      "chapter_source": "privacy_security",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "model inversion attack",
      "definition": "An attack that attempts to reconstruct training data or infer sensitive information about the dataset by analyzing a model's outputs and confidence scores.",
      "chapter_source": "privacy_security",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "model watermarking",
      "definition": "A technique for embedding verifiable ownership signatures into machine learning models that can be used to detect unauthorized use or prove intellectual property theft.",
      "chapter_source": "privacy_security",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "physical attack",
      "definition": "Direct manipulation or tampering with computing hardware to compromise the security and integrity of machine learning systems, bypassing traditional software defenses.",
      "chapter_source": "privacy_security",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "privacy budget",
      "definition": "A concept in differential privacy that represents the total amount of privacy loss allowed across all queries or computations, with each operation consuming part of this finite budget.",
      "chapter_source": "privacy_security",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "privacy-preserving machine learning",
      "definition": "Techniques and approaches that enable machine learning while protecting the privacy of individuals whose data is used for training or inference.",
      "chapter_source": "privacy_security",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "secure aggregation",
      "definition": "A cryptographic protocol used in federated learning to combine model updates from multiple parties without revealing individual contributions to the central server.",
      "chapter_source": "privacy_security",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "secure multi-party computation",
      "definition": "A cryptographic method that allows multiple parties to jointly compute a function over their private inputs without revealing those inputs to each other.",
      "chapter_source": "privacy_security",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "side-channel attack",
      "definition": "An attack that exploits information leaked through the physical implementation of computing systems, such as power consumption, electromagnetic emissions, or timing variations.",
      "chapter_source": "privacy_security",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "speculative execution",
      "definition": "A performance optimization in processors that executes instructions before confirming they are needed, which can inadvertently expose sensitive data through microarchitectural side channels.",
      "chapter_source": "privacy_security",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "supply chain attack",
      "definition": "An attack that compromises hardware or software components during the manufacturing, distribution, or integration process, potentially affecting multiple downstream systems.",
      "chapter_source": "privacy_security",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "synthetic data generation",
      "definition": "The creation of artificial datasets that approximate the statistical properties of real data while reducing privacy risks and avoiding direct exposure of sensitive information.",
      "chapter_source": "privacy_security",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "targeted attack",
      "definition": "A type of data poisoning attack that aims to cause misclassification of specific inputs or classes while leaving the model's general performance largely intact.",
      "chapter_source": "privacy_security",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "trusted execution environment",
      "definition": "A secure area within a processor that provides hardware-based protection for code and data, ensuring confidentiality and integrity even from privileged system software.",
      "chapter_source": "privacy_security",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "white-box attack",
      "definition": "An adversarial attack where the attacker has complete knowledge of the model's architecture, parameters, training data, and internal workings, enabling highly effective attack strategies.",
      "chapter_source": "privacy_security",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "zero-day vulnerability",
      "definition": "A previously unknown security flaw in software or hardware that can be exploited by attackers before developers have had a chance to create and distribute a patch.",
      "chapter_source": "privacy_security",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "privacy-utility tradeoff",
      "definition": "The fundamental tension between preserving individual privacy and maintaining the utility of data for machine learning, requiring careful balance through techniques like differential privacy.",
      "chapter_source": "privacy_security",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "secure computation",
      "definition": "Cryptographic protocols that enable multiple parties to jointly compute functions over private inputs without revealing those inputs to each other.",
      "chapter_source": "privacy_security",
      "aliases": [],
      "see_also": []
    }
  ]
}
