---
bibliography: data_efficiency.bib
quiz: data_efficiency_quizzes.json
concepts: data_efficiency_concepts.yml
glossary: data_efficiency_glossary.json
crossrefs: data_efficiency_xrefs.json
---

# Data Efficiency {#sec-data-efficiency}

::: {layout-narrow}
::: {.column-margin}
_DALL·E 3 Prompt: A futuristic digital illustration depicting the concept of data efficiency in machine learning. On one side of the image, there is a sleek, powerful computing unit, symbolizing AI processing. On the other side, streams of binary code (1s and 0s) flow into the computer, but the data is represented with glowing golden elements, signifying valuable, high-quality information. The background has a high-tech, digital ambiance, emphasizing the role of refined, efficient data in machine learning. No text, only a strong visual representation of the relationship between computation and valuable data._
:::

\noindent
![](images/png/cover_data_efficiency.png)

:::

## Purpose {.unnumbered}

_How does data influence the efficiency of machine learning systems?_

Machine learning advances have traditionally emphasized algorithmic improvements and system optimizations. Researchers and practitioners focused on developing faster training methods, reducing model complexity, and optimizing hardware utilization. While these approaches yielded significant gains, they addressed only two of the three fundamental pillars of machine learning: algorithms and systems. Data efficiency, the third pillar, remained underexplored despite its profound impact on system performance and scalability. The volume and velocity of data moving through machine learning pipelines affect computation requirements, memory usage, and energy consumption. Understanding data efficiency uncovers optimization opportunities beyond model architecture and hardware utilization. Each data-centric decision influences training time, resource allocation, and inference speed, demonstrating how data shapes overall system efficiency. Examining this overlooked dimension enables machine learning systems that optimize across all three pillars, leading to implementations that achieve better performance with fewer resources.

::: {.callout-tip title="Learning Objectives"}

- Layout the concept of data efficiency within the broader context of data-centric AI

- Define and outline the key components of data efficiency

- Explore techniques and strategies for improving data efficiency

- Establish a standard set of metrics for measuring data efficiency

- Analyze the role of benchmarks in evaluating data efficiency

- Investigate tools and methods to enhance data efficiency

- Develop a roadmap for developing data efficiency benchmarks

:::

## The Information-Compute Ratio {#sec-data-efficiency-info-compute}

As defined in the **Pareto Frontier** (@sec-optimize-principles), optimization is the search for the best trade-off between Accuracy and Efficiency. While model compression and hardware acceleration focus on the *execution* of the math, **Data Efficiency** is our first and most powerful move: it reduces the *amount* of math required by optimizing the training dataset itself.

Data engineering (@sec-data-engineering) ensures that data is clean, accessible, and correctly formatted. Data efficiency asks a different question: *how much information does each sample contribute to the model's learning per unit of computation?*

In the optimization triad (@fig-optimization-triad), data efficiency plays the role of **Input Optimization**. While model compression minimizes the math per parameter and hardware acceleration maximizes the math per second, data efficiency minimizes the total math required to reach convergence.

::: {#fig-optimization-triad fig-cap="**The Optimization Triad**: Machine learning performance relies on three pillars: Algorithms (models), Systems (hardware/software), and Data Efficiency. While algorithms and systems have traditionally received the most attention, optimizing data efficiency (Input Optimization) offers a third, powerful lever for scaling performance." fig-alt="A triangular diagram with three nodes: Algorithms, Systems, and Data Efficiency. Arrows connect all three, forming a cycle. Data Efficiency is highlighted."}
```{.tikz}
\begin{tikzpicture}[font=\small\usefont{T1}{phv}{m}{n}, scale=0.9, transform shape]
  % Nodes
  \node[draw, circle, minimum size=2.8cm, fill=blue!10, align=center, text width=2.5cm] (Alg) at (90:2.5) {Algorithms\\(Model)};
  \node[draw, circle, minimum size=2.8cm, fill=green!10, align=center, text width=2.5cm] (Sys) at (210:2.5) {Systems\\(Hardware)};
  \node[draw, circle, minimum size=2.8cm, fill=orange!10, align=center, text width=2.5cm, line width=1.5pt] (Data) at (330:2.5) {\textbf{Data}\\\textbf{Efficiency}};

  % Connections
  \draw[<->, thick] (Alg) -- node[left, font=\footnotesize, text width=1.5cm, align=right] {Compute\\Bound} (Sys);
  \draw[<->, thick] (Sys) -- node[below, font=\footnotesize] {I/O Bound} (Data);
  \draw[<->, thick] (Data) -- node[right, font=\footnotesize, text width=1.5cm, align=left] {Sample\\Efficiency} (Alg);

  % Center Label
  \node[align=center, font=\bfseries] at (0,0) {ML\\Scale};
\end{tikzpicture}
```
:::

We can formalize this as the **Information-Compute Ratio (ICR)**:

$$ 	ext{ICR} = rac{\Delta 	ext{Model Performance}}{\Delta 	ext{FLOPs}} $$

A random batch of raw data often has low ICR: it contains redundant examples, noisy samples, or "easy" examples the model has already mastered. Training on such a batch wastes GPU cycles on zero-information updates. High-efficiency data pipelines (@fig-data-efficiency-pipeline) filter, order, and synthesize data to maximize ICR, ensuring that every FLOP contributes to learning.

::: {#fig-data-efficiency-pipeline fig-cap="**The Data Efficiency Pipeline**: A structured approach to increasing data value. Raw data is first pruned to remove redundancy (Static Pruning), then dynamically selected during training (Active Learning), and finally augmented to increase diversity (Synthesis). Each stage increases the Information-Compute Ratio (ICR)." fig-alt="A flow diagram showing the progression of data: Raw Data -> Static Pruning -> Dynamic Selection -> Synthetic Generation -> High Value Model. Arrows indicate the flow."}
```{.tikz}
\begin{tikzpicture}[font=\small\usefont{T1}{phv}{m}{n}, node distance=1.5cm, >={Stealth[length=3mm]}]
  % Nodes
  \node[draw, rectangle, fill=gray!10, minimum height=1cm, minimum width=2cm] (Raw) {Raw Data};

  \node[draw, rectangle, fill=blue!10, minimum height=1cm, minimum width=2.5cm, right=1cm of Raw, align=center] (Prune) {1. Static\\Pruning};

  \node[draw, rectangle, fill=green!10, minimum height=1cm, minimum width=2.5cm, right=1cm of Prune, align=center] (Select) {2. Dynamic\\Selection};

  \node[draw, rectangle, fill=orange!10, minimum height=1cm, minimum width=2.5cm, right=1cm of Select, align=center] (Synth) {3. Synthetic\\Gen};

  \node[draw, circle, fill=red!10, minimum size=1.2cm, right=1cm of Synth] (Model) {Model};

  % Edges
  \draw[->, thick] (Raw) -- (Prune);
  \draw[->, thick] (Prune) -- (Select);
  \draw[->, thick] (Select) -- (Synth);
  \draw[->, thick] (Synth) -- node[above, font=\footnotesize] {High ICR} (Model);

  % Annotations
  \node[below=0.2cm of Prune, font=\footnotesize, color=gray] {Pre-training};
  \node[below=0.2cm of Select, font=\footnotesize, color=gray] {During Training};
  \node[below=0.2cm of Synth, font=\footnotesize, color=gray] {On-Demand};
\end{tikzpicture}
```
:::

This chapter explores three strategies to maximize this ratio:
1.  **Static Data Pruning**: Removing low-value samples before training begins (Coresets, Deduplication).
2.  **Dynamic Selection**: Selecting high-value samples during training (Curriculum Learning, Active Learning).
3.  **Synthetic Generation**: Creating high-value samples on demand (Augmentation, Distillation).
## Static Data Pruning: Pre-Training Filtration

Machine learning models are often trained on large datasets under the assumption that more data leads to better performance. While increasing data volume can be beneficial, it is not always necessary. In many cases, a significant portion of a dataset is redundant or contributes little to learning, meaning that models can achieve the same—or even better—performance by training on a smaller, carefully selected subset of data. By identifying and retaining only the most valuable samples, dataset size can be reduced without compromising accuracy, leading to more efficient training, faster inference, and lower storage and labeling costs.

Not all data points provide equal value for training a model. Some samples are highly informative, capturing the underlying patterns of a dataset, while others are repetitive or noisy. Training on redundant data can introduce unnecessary computational overhead, while noisy or mislabeled samples can degrade model performance. Instead of treating all data equally, machine learning systems can be designed to identify and prioritize the most important training examples, so that models learn from high-quality, representative samples while ignoring less useful data.

One method for reducing dataset size is core-set selection, which focuses on identifying a small subset of data that preserves the statistical properties of the entire dataset. The goal is to find a compact set of examples that allows a model to generalize as well as it would if trained on the full dataset. Core-set selection is particularly valuable in resource-constrained environments, such as edge computing or mobile AI, where memory and processing power are limited. By selecting only the most informative data points, models can be trained more efficiently without sacrificing accuracy.

Another approach is data pruning, which removes samples that do not contribute meaningfully to the learning process. Data pruning can be performed at different stages of training. Before training begins, redundant or low-variance samples can be removed based on statistical analysis. During training, misclassified or high-loss examples can be identified and selectively discarded to improve convergence. After training, unimportant samples can be filtered out to reduce storage costs for future retraining. By eliminating unnecessary data, pruning not only reduces dataset size but also improves model robustness by reducing exposure to noisy or ambiguous samples.

A related technique, curriculum learning, improves training efficiency by structuring the order in which data is presented to the model. Instead of training on all samples at once, curriculum learning starts with simpler examples and gradually introduces more complex ones. This approach mirrors how humans learn by mastering fundamental concepts before tackling advanced topics. By prioritizing easier samples in the early stages of training, models can learn more efficiently, making better use of limited data and improving generalization.

These techniques—core-set selection, data pruning, and curriculum learning—demonstrate that bigger datasets are not always better. By carefully selecting and structuring training data, machine learning models can achieve the same or better performance with fewer examples, reducing computational demands and making AI development more efficient. This approach is particularly beneficial in scenarios where collecting large-scale datasets is costly, impractical, or unnecessary.

## Dynamic Data Selection: Active Learning

Supervised learning has been a cornerstone of modern AI, enabling models to achieve high performance across a wide range of tasks. However, it comes with a significant limitation: it requires vast amounts of labeled data, which is often expensive and time-consuming to obtain. In specialized fields such as medical diagnosis, autonomous driving, and scientific research, annotation requires domain expertise, further increasing costs. The reliance on large-scale labeled datasets raises a critical challenge—how can machine learning models learn effectively while minimizing the need for manual labeling?

Not all data points contribute equally to a model’s learning. Some samples provide high-value information, while others are redundant or even misleading. Instead of treating all data as equally useful, machine learning systems can be designed to prioritize the most informative data points, leverage unlabeled data effectively, and generalize from minimal supervision. These approaches reduce the burden of annotation while maintaining model accuracy and generalization.

One way to achieve this is through active learning, a technique where the model itself identifies which examples are most valuable for training and selectively queries human annotators for labels. Unlike traditional supervised learning, where all data points are labeled in advance, active learning (@fig-active-learning-loop) prioritizes uncertain or high-impact samples, allowing models to improve with fewer labeled examples.

::: {#fig-active-learning-loop fig-cap="**Active Learning Loop**: Instead of labeling all data, the model selects the most 'confusing' or informative samples from an unlabeled pool. These samples are sent to an Oracle (human annotator) and added to the training set. The model is retrained, and the cycle repeats, creating a feedback loop that maximizes data efficiency." fig-alt="A cycle diagram: Unlabeled Pool -> Selection Strategy -> Oracle -> Labeled Set -> Model Training -> back to Selection Strategy."}
```{.tikz}
\begin{tikzpicture}[font=\small\usefont{T1}{phv}{m}{n}, >={Stealth[length=3mm]}]
  % Nodes arranged in a circle
  \node[draw, cylinder, shape border rotate=90, aspect=0.25, fill=gray!10, minimum height=1.5cm, minimum width=1.2cm, align=center] (Pool) at (0, 2) {Unlabeled\\Pool};

  \node[draw, rectangle, rounded corners, fill=blue!10, minimum height=1cm, align=center] (Select) at (4, 2) {Selection\\Strategy};

  \node[draw, circle, fill=orange!10, minimum size=1.5cm, align=center] (Oracle) at (4, -1) {Oracle\\(Human)};

  \node[draw, rectangle, fill=green!10, minimum height=1cm, align=center] (Train) at (0, -1) {Training\\Set};

  \node[draw, rectangle, fill=red!10, minimum height=1cm, align=center] (Model) at (-2, 0.5) {Model};

  % Edges representing the flow
  \draw[->, thick] (Pool) -- node[above, font=\footnotesize] {Query} (Select);
  \draw[->, thick] (Select) -- node[right, font=\footnotesize] {Uncertainty} (Oracle);
  \draw[->, thick] (Oracle) -- node[below, font=\footnotesize] {Labels} (Train);
  \draw[->, thick] (Train) -- node[left, font=\footnotesize] {Train} (Model);
  \draw[->, thick, dashed] (Model) |- node[near start, left, font=\footnotesize] {Update} (Select);
\end{tikzpicture}
```
::: For instance, in medical imaging, an AI system diagnosing diseases from X-rays may already be confident in classifying common conditions but uncertain about rarer cases. Instead of labeling the entire dataset, active learning focuses human annotation efforts on these ambiguous cases, optimizing the use of labeling resources. By iteratively refining the dataset with the most informative examples, active learning reduces annotation costs while accelerating model performance gains.

::: {.callout-perspective title="Back-of-the-Napkin: The Active Learning ROI"}
**Problem**: You are building a medical diagnostic AI. You have a pool of **1 Million unlabeled scans**. A specialist doctor charges **$5.00** to label one scan. You have a budget of **$500,000** and a deadline of **1 month**.

**Scenario A: Naive Labeling**
1.  **Cost**: Labeling all 1M scans would cost **$5,000,000** (10x over budget).
2.  **Time**: You can only afford to label 100,000 random scans.
3.  **Result**: Your model misses rare pathologies because they weren't in the random 10%.

**Scenario B: Active Learning**
1.  **Strategy**: Use an uncertainty-based selection to pick the **50,000** "hardest" scans for the doctor to label.
2.  **Cost**: $50,000 \times 5.00 = \mathbf{\$250,000}$. (50% under budget).
3.  **Training Speed**: With 20x less data, each training epoch is **20x faster**.
4.  **Result**: Research shows that these 50k "high-information" samples often achieve higher accuracy than 500k random samples.

**The Systems Conclusion**: Data Efficiency is not just a "data trick"—it is a **20x compute accelerator** and a **$4.75 Million** cost-saving measure.
:::

Another approach to minimizing labeled data requirements...

For cases where some labeled data is available but insufficient for fully supervised learning, semi-supervised learning offers a middle ground. This technique leverages a small set of labeled examples to guide the learning process for a much larger set of unlabeled data. The model first learns from the labeled subset and then generalizes its knowledge to make predictions on the remaining dataset. In speech recognition, for example, only a fraction of audio recordings may be transcribed, but semi-supervised learning enables models to infer patterns in the unannotated data, improving recognition accuracy without requiring full transcription of every sample. By making efficient use of limited labeled data, semi-supervised learning reduces annotation costs while maintaining strong model performance.

Each of these techniques contributes to reducing labeling needs in different ways. Active learning optimizes the annotation process by focusing on the most valuable data points, self-supervised learning eliminates the need for explicit labels by deriving supervision from raw data, and semi-supervised learning extends the utility of small labeled datasets to improve model generalization. Together, these methods address one of the fundamental challenges in machine learning—how to develop powerful models while minimizing the dependence on large-scale labeled datasets. By reducing labeling requirements, they make AI development more accessible, cost-effective, and scalable, particularly in domains where data annotation is costly or limited.

## Synthetic Data Generation and Augmentation

Machine learning models often rely on large datasets to achieve strong performance, but collecting and labeling data is costly and time-consuming. However, rather than continuously expanding datasets, machine learning systems can be designed to extract more value from the data that is already available. This approach improves data efficiency by ensuring that each sample contributes as much useful information as possible, reducing the need for additional data collection while still enhancing model performance.

One way to achieve this is through data augmentation, which artificially expands a dataset by applying transformations to existing data. In image classification, for instance, an image can be rotated, flipped, blurred, or color-adjusted to create multiple variations, exposing the model to a wider range of input conditions without requiring additional real-world samples. This technique is particularly useful for improving generalization, as models trained on augmented datasets are more robust to variations in lighting, viewpoint, and background clutter. In natural language processing, augmentation can involve paraphrasing, back-translation, or synonym replacement, allowing models to learn more diverse sentence structures and vocabulary. The key benefit of data augmentation is that it increases dataset diversity without requiring new labeled examples, making it a cost-effective way to enhance model learning.

Synthetic data generation extends this idea further by creating entirely new data points rather than modifying existing ones. Unlike data augmentation, which applies transformations to real-world samples, synthetic data generation relies on computational models to produce realistic training examples. This is particularly useful in cases where collecting real data is impractical or ethically challenging. For instance, in autonomous driving research, simulated environments can generate diverse road conditions, traffic scenarios, and edge cases that would be difficult to capture in real-world driving data. Similarly, in medical AI applications, synthetic patient data can be generated to train models without exposing sensitive health records. Advances in generative modeling, such as GANs (Generative Adversarial Networks) and diffusion models, have made synthetic data increasingly realistic, allowing it to serve as a substitute or complement to real-world data.

Another powerful approach to improving data efficiency is transfer learning, which allows knowledge gained from one task to be repurposed for another. Instead of training a model from scratch, transfer learning uses a model pre-trained on a large dataset and fine-tunes it on a smaller, domain-specific dataset. This significantly reduces the amount of labeled data needed while still achieving high performance. For example, a neural network trained on millions of general-purpose images can be adapted to recognize rare medical conditions with only a few hundred annotated examples. Transfer learning is especially useful in low-resource settings where labeled data is scarce, as it leverages previously learned features and representations to accelerate learning on new tasks.

While these techniques differ in implementation, they all contribute to data efficiency by maximizing the utility of available data. Data augmentation enhances learning by introducing variability, synthetic data generation expands dataset diversity in controlled ways, and transfer learning reduces data requirements by leveraging existing knowledge. Together, these approaches ensure that models achieve strong performance without relying on massive labeled datasets, making AI development more scalable, accessible, and adaptable to real-world constraints.

## Advanced Utilization Strategies

Even when high-quality data is available, the way it is structured, retrieved, and used can significantly impact the efficiency and effectiveness of a machine learning system. Simply having large amounts of data is not enough; models must be designed to make the most of the data they have. Optimizing data utilization involves ensuring that the right data is used at the right time and in the right way, minimizing waste while maximizing learning efficiency.

One key challenge in machine learning is that not all data points contribute equally to model improvement at every stage of training. Some samples provide critical learning signals early on, while others become more useful as the model develops its understanding. Effective data utilization requires strategies that allow models to dynamically adapt to the most relevant data throughout the training process.

One approach to optimizing data usage is reinforcement learning-based data selection, where models learn to prioritize training examples based on their potential impact on learning. Instead of feeding all data to the model indiscriminately, reinforcement learning techniques can guide data selection by rewarding choices that lead to faster convergence and better generalization. This method is particularly useful in domains where training on the entire dataset is computationally expensive, as it ensures that only the most valuable samples are used at any given time.

Another strategy is meta-learning, often referred to as "learning to learn," which enables models to adapt quickly to new tasks with minimal data. Meta-learning techniques optimize how models use data by extracting transferable knowledge across tasks, allowing them to generalize from limited examples. In few-shot learning scenarios, for example, a model trained using meta-learning can efficiently classify new categories with just a handful of labeled samples by leveraging prior experience. This ability to make the most of small amounts of data reduces dependence on large-scale datasets while maintaining strong performance.

Beyond training efficiency, optimizing data utilization also involves improving how data is stored, accessed, and managed. Large-scale machine learning systems require well-structured data pipelines that ensure seamless data retrieval and minimize redundancy. Efficient data indexing and retrieval mechanisms help models access relevant data points without unnecessary computation, particularly in applications involving real-time decision-making, such as search engines or recommendation systems. By structuring data intelligently, machine learning workflows can significantly reduce storage and processing overhead while improving responsiveness.

Together, these strategies—reinforcement learning-based data selection, meta-learning, and efficient data management—demonstrate that improving how data is used is just as important as improving the data itself. By ensuring that models are trained on the most relevant examples, adapt quickly to new tasks, and retrieve information efficiently, AI systems can achieve higher performance while reducing computational and data requirements. These approaches are particularly valuable in large-scale, real-time, or resource-constrained settings, where optimizing data utilization can make the difference between a scalable, efficient system and an impractical one.

### Technique Overview

Efficient use of data is essential for developing scalable and cost-effective machine learning systems. While conventional approaches rely on collecting ever-larger datasets, a data-centric perspective emphasizes optimizing how data is collected, structured, and used to achieve the same or better performance with fewer resources. The strategies discussed in this section represent five fundamental approaches for improving data efficiency.

By integrating these strategies, machine learning systems can achieve higher performance with fewer resources, making AI more accessible and environmentally sustainable. Table @tbl-data-efficiency summarizes these optimization goals, their approaches, key techniques, and benefits.

| Optimization Goal                | Approach                                      | Key Techniques | Benefit |
|--------------------------------------|------------------------------------------------|--------------------|------------|
| Improving Data Quality | Enhance dataset accuracy, consistency, and bias mitigation to ensure high-quality inputs. | Data Cleaning, Label Error Detection, Bias Correction, Data Validation | Reduces model errors caused by poor data quality and improves fairness and reliability. |
| Reducing Labeling Needs          | Minimize reliance on labeled data by prioritizing essential examples or leveraging unlabeled data. | Active Learning, Self-Supervised Learning, Semi-Supervised Learning | Reduces annotation costs and effort while maintaining model performance. |
| Extracting More Information from Data | Increase the value of existing data through transformation, generation, or transfer of knowledge. | Data Augmentation, Synthetic Data Generation, Transfer Learning | Enhances model generalization without requiring additional real-world data collection. |
| Reducing Dataset Size Without Performance Loss | Identify and retain only the most important samples to reduce redundancy and improve efficiency. | Core-Set Selection, Data Pruning, Curriculum Learning | Decreases storage and computational costs while preserving model accuracy. |
| Optimizing Data Utilization      | Improve how data is structured, retrieved, and used during training and inference. | Reinforcement Learning-Based Data Selection, Meta-Learning, Efficient Data Management | Ensures models prioritize relevant data and adapt quickly, reducing unnecessary computation. |

: Summary of key data efficiency strategies and techniques. {#tbl-data-efficiency .striped .hover}

## Engineering Data Efficiency Systems

The strategies discussed so far—pruning, active learning, and synthesis—are algorithmic interventions. However, implementing them at scale requires robust systems engineering. A naive active learning loop that scans the entire dataset every epoch to select the "best" samples will turn a compute-bound training job into an I/O-bound bottleneck. This section examines the architectural patterns required to implement data efficiency in production.

### The Selection Bottleneck

Dynamic data selection introduces a new bottleneck: the selection latency. In standard training, the data loader simply reads the next batch. In active learning or curriculum learning, the system must evaluate a selection function $f(x)$ over a large candidate pool to determine the next batch.

If $f(x)$ requires a forward pass of the model (e.g., measuring uncertainty), the cost of selection can exceed the cost of training. To mitigate this, high-efficiency systems employ **Proxy Selection**:
1.  **Small Proxy Models:** Use a distilled, lightweight model to score the data pool, selecting candidates for the large training model.
2.  **Embedding Indices:** Pre-compute embeddings for the entire dataset and store them in a vector search index (e.g., FAISS, ScaNN). Selection becomes a nearest-neighbor search (finding samples diverse from the current training set) rather than a linear scan.

### Optimizing Data Loaders

Data efficiency often leads to non-sequential access patterns. While standard training reads files sequentially (optimizing disk readahead), strategies like dynamic subset selection require random access to specific high-value samples. Standard filesystems and object stores (S3) suffer significant latency penalties under random access loads.

To maintain GPU utilization, data loaders must be architected for **sharded random access**:
*   **Sharded Formats:** Formats like WebDataset or FFCV group thousands of samples into `tar` or `record` shards.
*   **Shuffle Buffers:** Instead of true global random access, the loader reads large sequential shards into a memory buffer and samples randomly from within the buffer. This approximates true random selection while preserving sequential I/O throughput.

### Augmented Pipeline Parallelism

Synthetic data generation moves the data bottleneck from I/O (disk speed) to CPU (augmentation compute). Heavy augmentations (e.g., 3D rotations, MixUp) or generative synthesis can starve the GPU if the CPU cannot keep up.

**Data Echoing** is a system optimization that reuses a batch of data multiple times in the GPU, applying different cheap augmentations on the GPU itself, or simply replaying the batch to amortize the cost of fetching and decoding. This reduces the pressure on the upstream data pipeline, ensuring that the GPU remains the bottleneck.
## Measuring Data Efficiency

Data efficiency plays a critical role in determining the scalability, sustainability, and accessibility of machine learning systems. However, its abstract nature makes it challenging to evaluate and compare systematically. Measuring data efficiency requires quantifying how effectively a model utilizes data to achieve performance goals with minimal resources, such as computational cost, time, or labeling effort.

Without standardized metrics, it can be difficult to assess trade-offs between dataset size, quality, and computational efficiency. For instance, how do we evaluate whether a smaller, curated dataset performs better than a larger, noisier one? How do we ensure that efforts to improve data diversity or reduce labeling requirements do not inadvertently degrade performance? Developing reliable methods to measure data efficiency helps address these questions by offering insights into which data-centric strategies provide the greatest value.

This section explores the principles and techniques for quantifying data efficiency. We begin by identifying key metrics that capture different dimensions of data efficiency, followed by practical methods to evaluate these metrics. Understanding these measurement strategies is essential for making informed decisions about data-centric optimizations and for advancing the broader goal of building efficient, resource-conscious AI systems.

### Performance-Per-Data Unit (PPD)

Machine learning models depend on data to improve their predictions, but more data does not always mean better performance. While increasing dataset size can enhance accuracy, the benefits are not always proportional—at some point, additional data contributes less meaningful information, making the learning process inefficient. This diminishing return creates an important question: how much data is actually needed to achieve a target level of model performance?

#### Measuring PPD

Performance-Per-Data Unit (PPD) provides a way to measure how efficiently a model learns from additional data. It quantifies the improvement in model performance per unit of data added, helping determine whether increasing dataset size is justified given the computational cost. A high PPD indicates that each additional data point meaningfully improves performance, making data collection and processing more efficient. Conversely, a low PPD suggests that adding more data increases system costs (e.g., training time, energy usage, storage) without significant performance gains.

Mathematically, PPD is expressed as:

$$
PPD = \frac{\Delta \mathcal{M}}{\Delta N}
$$

where:

- $\Delta \mathcal{M}$ represents the change in model performance (e.g., accuracy, F1-score, or loss reduction).
- $\Delta N$ represents the number of new training samples added.

For example, suppose a model trained on 10,000 images achieves 85% accuracy on a test set. If adding 1,000 more images increases accuracy to 86%, the PPD is:

$$
PPD = \frac{86 - 85}{1000} = 0.001 \text{% per sample}
$$

This suggests that each additional sample contributed, on average, a 0.001% improvement in accuracy. However, if adding another 10,000 images only improves accuracy to 86.5%, then:

$$
PPD = \frac{86.5 - 86}{10000} = 0.00005 \text{% per sample}
$$

The sharp decline in PPD (@fig-ppd-curve) indicates diminishing returns—even though more data is added, it contributes progressively less to overall accuracy. At this stage, continuing to expand the dataset increases computational costs without yielding meaningful accuracy improvements.

::: {#fig-ppd-curve fig-cap="**Diminishing Returns of Data**: A comparison of random sampling (gray dashed line) versus data-efficient selection (blue solid line). The efficient strategy achieves higher performance with less data (High PPD region), reaching the convergence plateau much earlier. Adding data beyond this point (Low PPD) yields minimal gains." fig-alt="A plot with X-axis 'Dataset Size' and Y-axis 'Performance'. Two curves start at 0. The 'Random' curve rises slowly. The 'Efficient' curve rises steeply and plateaus early."}
```{.tikz}
\begin{tikzpicture}[font=\small\usefont{T1}{phv}{m}{n}]
\begin{axis}[
    width=8cm, height=6cm,
    xlabel={Dataset Size},
    ylabel={Performance},
    xmin=0, xmax=100,
    ymin=0, ymax=100,
    xtick={0,100}, xticklabels={0, Max},
    ytick={0,100}, yticklabels={0, Max},
    axis lines=left,
    legend pos=south east,
    legend style={draw=none, fill=none},
    clip=false
]
    % Random Selection - Log-like slow growth
    \addplot[color=gray, thick, dashed, domain=0:100, samples=50] {100 * (1 - exp(-0.03 * x))};
    \addlegendentry{Random Selection}

    % Efficient Selection - Fast growth then plateau
    \addplot[color=blue, very thick, domain=0:100, samples=50] {100 * (1 - exp(-0.1 * x))};
    \addlegendentry{Efficient Selection}

    % Annotation
    \node[anchor=west, color=blue] at (axis cs: 10, 80) {High PPD};
    \node[anchor=west, color=gray] at (axis cs: 60, 50) {Low PPD};

    % Vertical line showing efficiency gap
    \draw[<->, red, thick] (axis cs: 20, 45) -- (axis cs: 20, 86);
    \node[right, color=red, font=\footnotesize, align=left] at (axis cs: 20, 65) {Efficiency\\Gap};

\end{axis}
\end{tikzpicture}
```
:::

#### PPD and System Efficiency

From a systems perspective, tracking PPD is essential because excessive data usage can lead to inefficiencies in computation, storage, and energy consumption. Machine learning pipelines must balance dataset size with the computational resources required for training and inference. If PPD is low, increasing the dataset size primarily increases costs without substantial performance improvements.

The total computational cost $C_T$ of training a model depends on both the dataset size $N$ and the computational cost per sample $C_s$:

$$
C_T = N \cdot C_s
$$

If PPD is high, each new data sample significantly enhances learning, leading to efficient compute usage. However, if PPD is low, increasing $N$ leads to a higher training cost $C_T$ without proportional accuracy improvements, making training more expensive and time-consuming.

A similar relationship exists with energy efficiency. The total energy consumption $E_T$ required for training is:

$$
E_T = C_T \cdot P
$$

where $P$ is the power consumption of the computing infrastructure. When PPD is low, increasing dataset size wastes energy by requiring more computational cycles for only marginal accuracy gains. This inefficiency is particularly problematic for large-scale AI systems, where massive datasets can consume significant financial and environmental resources.

For example, training a large language model such as GPT-4 requires thousands of GPU hours. If much of the dataset has a low PPD, then the training process wastes both compute power and financial resources, making it less scalable and sustainable.

#### Improving PPD

Given the direct impact of PPD on system efficiency, it is essential to optimize data efficiency rather than blindly increasing dataset size. Several strategies can help maintain a high PPD, so that each data sample contributes maximum learning value:

- Active Learning: Instead of labeling all available data, selectively annotate only the most uncertain or informative samples.
- Dataset Pruning: Identify and remove redundant or uninformative samples, reducing dataset size without compromising accuracy.
- Data Augmentation: Generate variations of existing data rather than collecting more raw data, increasing diversity without increasing labeling costs.

By focusing on data quality rather than sheer volume, machine learning practitioners can reduce computational overhead, decrease energy consumption, and accelerate training—all while maintaining or even improving model performance.

Performance-Per-Data Unit (PPD) provides a structured way to evaluate the impact of dataset expansion. While adding more data can improve model performance, monitoring PPD ensures that these improvements justify the associated computational and energy costs. By integrating PPD into system design decisions, machine learning practitioners can develop models that are not only accurate but also efficient, scalable, and sustainable.

### Data Usage Efficiency (DUE)

Machine learning models require vast amounts of data to generalize well, but not all data contributes equally to learning. Some samples provide essential information that improves model accuracy, while others are redundant, mislabeled, or even misleading. When a dataset contains a high proportion of unnecessary samples, the system incurs extra computational, storage, and energy costs without significant performance gains. The concept of Data Usage Efficiency (DUE) addresses this challenge by measuring how effectively a dataset is utilized in achieving a target model performance. Instead of focusing purely on dataset size, DUE evaluates whether the available data is being used optimally to minimize resource consumption while maximizing learning.

A high DUE indicates that the dataset is well-optimized, meaning that most of the data points contribute to the model’s performance. In contrast, a low DUE suggests that a significant portion of the dataset is redundant or irrelevant, creating inefficiencies that extend beyond model training into storage, deployment, and inference. Understanding and optimizing DUE is critical for building scalable, cost-effective, and environmentally sustainable machine learning systems.

#### Measuring DUE

To formally quantify DUE, we define it as the ratio of the minimum dataset size required to reach a target performance level to the total available dataset size:

$$
DUE = \frac{N_{\text{eff}}}{N_{\text{total}}}
$$

where:

- $N_{\text{eff}}$ is the smallest number of training samples needed to achieve a given accuracy or performance threshold.
- $N_{\text{total}}$ is the total dataset size.

Consider a classification model trained on one million samples. If it reaches the desired accuracy after using only 100,000 of those samples, then the DUE is:

$$
DUE = \frac{100,000}{1,000,000} = 0.1 \quad (10\%)
$$

This result implies that 90% of the dataset is not contributing significantly to model improvement. In such cases, reducing dataset size through pruning, core-set selection, or active learning can significantly enhance efficiency without compromising performance.

#### DUE and System Efficiency

When DUE is low, machine learning systems become computationally inefficient, requiring more resources than necessary to achieve the same performance. The impact of a low DUE extends beyond just the training phase—it affects computation, memory, energy consumption, and deployment efficiency. Addressing these inefficiencies is essential for reducing operational costs and improving overall system performance.

One of the most direct consequences of a low DUE is increased computational cost during training. Since every additional sample in the dataset incurs a computational overhead, the total training cost $C_T$ is given by:

$$
C_T = N_{\text{total}} \cdot C_s
$$

where $C_s$ represents the compute cost per sample. When $N_{\text{total}}$ is significantly larger than $N_{\text{eff}}$, resources are wasted on processing unnecessary data. This inefficiency slows down training, increases GPU/TPU usage, and leads to unnecessary cloud computing expenses.

Beyond computational cost, low DUE impacts storage and memory requirements. Large datasets require more disk space for storage and greater memory bandwidth for loading and processing, creating bottlenecks in data pipelines. In cloud-based environments, where storage costs scale with usage, a dataset with low DUE unnecessarily inflates infrastructure costs. Moreover, when models are deployed in real-time systems or edge devices, an inefficient dataset may lead to higher latency and slower inference times, limiting scalability.

Another major consequence of low DUE is its effect on energy consumption and sustainability. Since training a machine learning model is energy-intensive, excessive dataset size leads to increased power usage. The total energy consumption $E_T$ for training is proportional to computational cost:

$$
E_T = C_T \cdot P
$$

where $P$ is the power consumption of the computing infrastructure. When a dataset contains a high proportion of redundant samples, $C_T$ increases without substantial performance improvements, causing unnecessary energy expenditure. This inefficiency is particularly concerning for large-scale AI models, which require extensive computing clusters. Training a low-DUE dataset on thousands of GPUs results in a significant environmental footprint, making AI less sustainable.

Low DUE also affects model inference efficiency. A model trained on an unnecessarily large dataset might have higher inference latency and memory requirements due to bloated architectures or inefficient feature representations. This can be problematic in real-time applications, such as autonomous systems, healthcare, or financial decision-making, where response time is critical. By optimizing DUE, machine learning models can achieve comparable accuracy while being leaner, faster, and more energy-efficient.

#### Improving DUE

To increase DUE, it is essential to refine how data is selected, structured, and utilized within the training pipeline. Several techniques can help maximize efficiency:

- Core-Set Selection: Instead of using the entire dataset, identify and train on a small subset of highly informative samples that provide the same learning signal as the full dataset.
- Active Learning: Prioritize labeling only the most uncertain or impactful samples, reducing annotation costs and dataset size.
- Data Pruning: Remove redundant, mislabeled, or unimportant samples that do not contribute meaningfully to model learning.
- Self-Supervised Learning: Reduce reliance on labeled data by leveraging unlabeled samples more effectively, allowing models to learn meaningful representations with fewer labeled examples.

By applying these methods, machine learning systems can achieve the same level of performance with significantly less data, reducing training time, energy costs, and storage requirements.

Data Usage Efficiency (DUE) provides a structured way to evaluate whether a dataset is being used optimally in machine learning training. A high DUE means that a dataset is well-curated, with most samples contributing valuable information to model learning. In contrast, a low DUE indicates inefficiency, leading to higher compute costs, longer training times, increased storage needs, and greater energy consumption.

By optimizing DUE, machine learning practitioners can develop more scalable, cost-effective, and environmentally sustainable AI systems. This ensures that every data point contributes meaningfully to learning, leading to leaner, more efficient models that perform well without excessive resource consumption.

### Computational Efficiency of Data (CED)

The efficiency of a machine learning system is not only determined by the size and quality of its dataset but also by how effectively data interacts with computational resources. Some data points require significantly more processing to extract useful features, while others are easier to learn from. This difference affects both the time it takes for a model to converge and the overall computational burden of training and inference.

Computational Efficiency of Data (CED) provides a way to measure how efficiently a dataset translates into model improvements given the compute resources available. Unlike Performance-Per-Data Unit (PPD), which focuses solely on learning gains from data, and Data Usage Efficiency (DUE), which evaluates whether a dataset is being used effectively, CED quantifies the computational cost of processing data relative to its learning contribution. A high CED means that a dataset is well-structured, providing maximum learning benefits for minimal computational effort. A low CED, on the other hand, indicates that excessive compute resources are being spent on processing data that does not yield meaningful model improvements.

#### Measuring CED

Formally, we define CED as the ratio of the model performance improvement per unit of compute cost:

$$
CED = \frac{\Delta \mathcal{M}}{\Delta C_T}
$$

where:

- $\Delta \mathcal{M}$ represents the change in model performance (e.g., accuracy, loss reduction).
- $\Delta C_T$ represents the additional computational cost incurred, measured in FLOPs (floating-point operations), GPU-hours, or energy consumption.

To illustrate this concept, suppose that two datasets—Dataset A and Dataset B—are used to train a model. If Dataset A leads to a 2% increase in accuracy but requires 100 GPU-hours, while Dataset B provides the same 2% improvement but requires only 50 GPU-hours, then the CED of Dataset B is twice as high as that of Dataset A. This means that Dataset B is computationally more efficient, allowing the model to achieve the same performance while using fewer resources.

The efficiency of data processing is important in deep learning, where training large-scale models involves billions to trillions of FLOPs. If a dataset has low CED, the model may require excessive computation to extract useful patterns, leading to longer training cycles, increased cloud costs, and higher energy consumption.

#### CED and System Efficiency

The computational burden of data processing directly affects the overall efficiency of machine learning pipelines. When a dataset is computationally inefficient, training and inference become unnecessarily expensive, consuming excessive hardware resources without yielding proportional improvements. Understanding CED helps diagnose these inefficiencies and improve the balance between model performance and compute usage.

One major consequence of a low CED is the increased cost of training. The total training cost $C_T$ depends on both dataset size and the compute cost per sample $C_s$:

$$
C_T = N \cdot C_s
$$

where:

- $N$ represents the number of training samples, and
- $C_s$ represents the compute cost per sample, which varies depending on data complexity and model architecture.

A dataset with low CED has a high $C_s$, meaning that each sample requires more processing than necessary to achieve learning improvements. As a result, training time is extended, compute resources are over-utilized, and overall system efficiency decreases.

Beyond training costs, low CED affects inference efficiency as well. Many real-world applications, such as edge AI and real-time decision-making systems, require models to process data quickly and efficiently. If a model is trained on data that requires excessive computation per sample, inference latency increases, limiting scalability. For example, an autonomous vehicle must process sensor data in milliseconds—if the model’s learned representations are computationally inefficient, the system may fail to react in real-time, creating potential safety risks.

Low CED also impacts energy efficiency and sustainability. Since computational cost is directly linked to power consumption, the total energy required for training is given by:

$$
E_T = C_T \cdot P
$$

where $P$ is the power draw of the computing infrastructure. When datasets require high $C_T$ to extract meaningful features, the result is higher power consumption, increased operational costs, and a larger carbon footprint. Large-scale AI models already consume significant energy—optimizing CED ensures that training resources are used responsibly.

#### Improving CED

Since CED is influenced by both data complexity and computational cost per sample, optimizing it requires improving how data is structured, processed, and utilized within machine learning pipelines. Several techniques can enhance CED while maintaining strong model performance:

- Data Representation Optimization: Transform raw data into more structured, efficient representations before training. Techniques such as dimensionality reduction, feature extraction, and data compression reduce computational overhead while retaining essential information.
- Efficient Data Augmentation: Apply computationally inexpensive transformations instead of using brute-force dataset expansion. For example, generating synthetic data using simple transformations rather than high-cost generative models can improve CED.
- Adaptive Sampling: Dynamically adjust how much computation is spent on each sample. Some machine learning frameworks prioritize high-value samples that improve learning while reducing effort spent on redundant or less informative data.
- Self-Supervised Pretraining: Learning robust representations from raw data before fine-tuning on a target dataset reduces the need for large-scale labeled data and lowers computational costs during training.

By integrating these approaches, machine learning practitioners can reduce compute costs, speed up training, and improve model scalability, all while maintaining accuracy.

Computational Efficiency of Data (CED) captures the relationship between data complexity and compute cost, highlighting how efficiently a dataset contributes to model improvements given the resources available. A high CED dataset provides strong performance gains while minimizing computational burden, whereas a low CED dataset requires excessive computation for limited improvements, leading to longer training times, higher energy consumption, and reduced system scalability.

By optimizing CED, machine learning practitioners can build faster, more efficient, and more sustainable AI systems, so that computational resources are allocated effectively. This optimization is particularly useful for large-scale AI models, real-time applications, and edge computing, where efficiency directly impacts performance and deployment feasibility.

### Redundancy Ratio (RR)

In machine learning, not all data is equally useful. Some samples contribute significantly to model learning, while others are highly repetitive, redundant, or even noisy, offering little additional value. As datasets grow larger, redundant data increases storage costs, prolongs training times, and unnecessarily inflates computational expenses. A dataset with excessive redundancy not only wastes resources but also slows down model convergence, making the training process less efficient.

The Redundancy Ratio (RR) provides a structured way to quantify this inefficiency. It measures the proportion of a dataset that could be removed without negatively impacting model performance. A high RR suggests that a significant fraction of the dataset is unnecessary for learning, while a low RR indicates that most of the dataset is contributing meaningfully. Understanding RR is important for optimizing data storage, training efficiency, and system scalability—particularly in large-scale machine learning applications where dataset size directly impacts infrastructure costs.

#### Measuring Redundancy Ratio

Redundancy in a dataset can be estimated by identifying the minimum number of samples needed to achieve the desired model accuracy and comparing it to the total dataset size. The Redundancy Ratio (RR) is defined as:

$$
RR = 1 - \frac{N_{\text{eff}}}{N_{\text{total}}}
$$

where:

- $N_{\text{eff}}$ is the effective dataset size, meaning the smallest subset of data required to reach the target model performance.
- $N_{\text{total}}$ is the total dataset size.

For example, suppose a dataset contains 500,000 training samples, but an experiment finds that the model achieves the same accuracy when trained on only 200,000 of them. The RR in this case is:

$$
RR = 1 - \frac{200,000}{500,000} = 0.6 \quad (60\%)
$$

This means that 60% of the dataset is redundant, meaning that removing these samples would not degrade performance. A high RR like this suggests that dataset pruning, deduplication, or better sampling strategies could significantly improve efficiency.

#### RR and System Efficiency

A high redundancy ratio leads to inefficiencies across multiple stages of the machine learning pipeline, affecting training, storage, and inference:

Increased Training Cost and Time

Training on highly redundant data means the model is processing similar or identical examples multiple times, which prolongs convergence and increases compute requirements. The total computational cost of training is given by:

$$
C_T = N_{\text{total}} \cdot C_s
$$

where $C_s$ is the per-sample compute cost. If a large portion of $N_{\text{total}}$ is redundant, training costs will increase unnecessarily, consuming additional GPU/TPU resources without proportional improvements in accuracy.

Storage and Memory Overhead

A dataset with high RR occupies unnecessary disk space and requires extra memory bandwidth for data retrieval, increasing infrastructure costs. In cloud-based machine learning environments, where data storage is often a significant expense, storing redundant data adds avoidable costs.

Moreover, when datasets are loaded into memory for processing, redundancy results in higher memory demands, which can slow down data pipelines—especially in large-scale distributed training scenarios.

Increased Energy Consumption

Since computational cost is directly tied to power consumption, redundant data leads to higher energy usage. The total training energy required, $E_T$, is given by:

$$
E_T = C_T \cdot P
$$

where $P$ is the power draw of the computing hardware. A high RR dataset inflates $C_T$, leading to higher energy consumption and increased carbon footprint, making AI training less environmentally sustainable.

Inefficient Inference and Deployment

Redundancy is not just a training-time issue—it also impacts model deployment. If a model is trained on highly redundant data, it may learn unnecessary representations, leading to larger model sizes, increased inference latency, and higher memory requirements. This is particularly problematic for mobile and edge computing applications, where efficiency is critical.

#### Improving RR

To improve system efficiency, redundancy should be minimized while preserving model performance. Several strategies can reduce RR without compromising learning:

- Data Deduplication: Identify and remove duplicate or near-duplicate samples before training.
- Core-Set Selection: Identify the most informative subset of the data that achieves the same performance with fewer examples.
- Contrastive Learning: Encourage the model to differentiate between similar examples, reducing the impact of redundancy.
- Active Learning: Prioritize annotation for diverse, high-value samples, preventing unnecessary repetition in dataset construction.

By removing redundant data, machine learning pipelines can accelerate training, lower compute costs, and improve scalability without affecting accuracy.

The Redundancy Ratio (RR) provides a structured way to measure how much of a dataset is unnecessary for achieving optimal model performance. A high RR indicates excessive duplication, which slows down training, inflates computational costs, and increases storage demands. Conversely, a low RR suggests that the dataset is well-optimized, so that every sample contributes meaningfully to learning.

By actively monitoring and reducing redundancy, machine learning practitioners can develop more efficient, scalable, and sustainable AI systems. Removing redundant data not only reduces training costs and energy consumption but also improves deployment efficiency, making AI systems more adaptable to real-world constraints.

### Dataset Growth Efficiency (DGE)

As machine learning models continue to scale, the ability to efficiently incorporate new data becomes increasingly important. While larger datasets can improve performance, the benefits often diminish beyond a certain point, meaning that each additional data point contributes less to learning. Collecting and processing data without considering its actual impact on model performance leads to wasted resources, longer training times, and unnecessary computational overhead.

Dataset Growth Efficiency (DGE) provides a way to measure how much additional data is required to achieve a fixed improvement in model performance. Unlike Performance-Per-Data Unit (PPD), which evaluates the impact of each new data point, DGE captures the scalability of data collection efforts—helping determine whether adding more data is cost-effective or if the dataset has reached a point of diminishing returns.

A high DGE means that additional data contributes significantly to model improvements, making continued data collection valuable. Conversely, a low DGE suggests that expanding the dataset further is inefficient, and efforts should instead focus on data quality, selection, or augmentation strategies.

#### Measuring DGE

DGE is defined as the ratio of the amount of additional data required to achieve a given improvement in model performance:

$$
DGE = \frac{\Delta N}{\Delta \mathcal{M}}
$$

where:

- $\Delta N$ represents the number of additional data points added.
- $\Delta \mathcal{M}$ represents the corresponding improvement in model performance (e.g., accuracy, F1-score, loss reduction).

For example, suppose a model trained on 10,000 samples achieves 80% accuracy, and adding 5,000 more samples increases accuracy to 82%. The DGE in this case is:

$$
DGE = \frac{5,000}{82 - 80} = 2,500 \text{ samples per 1% accuracy improvement}
$$

This means that 2,500 additional samples were required to achieve a 1% accuracy gain. If the model’s accuracy improves by only 0.2% after adding another 10,000 samples, then:

$$
DGE = \frac{10,000}{0.2} = 50,000 \text{ samples per 1% accuracy improvement}
$$

This steep increase in DGE indicates that the dataset is reaching diminishing returns, meaning that further data collection is becoming less effective. At this stage, alternative approaches such as data curation, augmentation, or active learning may be more efficient than simply adding more raw data.

#### DGE and System Efficiency

A low Dataset Growth Efficiency can lead to substantial inefficiencies across the entire machine learning pipeline, impacting training cost, storage, and overall system performance:

Increasing Data Collection Costs

As datasets grow, the cost of acquiring and labeling new samples rises, particularly for domains where annotation is expensive (e.g., medical imaging, autonomous driving). If DGE is low, additional data provides little performance gain, meaning that data collection is no longer cost-effective.

#### 2. Higher Computational and Energy Costs

When additional data yields minimal improvements, the computational cost of processing it becomes disproportionate to the benefit. The total training cost $C_T$ scales with dataset size:

$$
C_T = N \cdot C_s
$$

where $C_s$ is the compute cost per sample. If DGE is low, $N$ grows without yielding substantial improvements, making training increasingly expensive. This also increases energy consumption, as total power usage is given by:

$$
E_T = C_T \cdot P
$$

where $P$ represents the power draw of the computing infrastructure. In large-scale AI training, low DGE datasets contribute significantly to carbon emissions and overall environmental costs.

Storage and Memory Inefficiencies

Datasets with low DGE require disproportionately large storage resources relative to their learning impact. Cloud-based machine learning pipelines, where storage is often a major expense, become less cost-effective when newly added data provides minimal gains.

Slower Training and Inference Pipelines

As datasets grow beyond an optimal point, training time increases significantly, even when performance gains are marginal. This can delay model development cycles, making AI systems less agile in responding to new tasks. Furthermore, when models are trained on unnecessary additional data, they may inherit inefficiencies that affect inference speed and model size, impacting deployment performance.

#### Improving DGE

Rather than blindly increasing dataset size, improving DGE requires targeted data collection and curation strategies that prioritize high-value samples. The following techniques can help optimize dataset growth while maintaining system efficiency:

- Active Learning: Instead of adding data randomly, select the most informative or uncertain samples for annotation, reducing the total number of new samples required.
- Data Augmentation: When DGE is low, augmenting existing data may provide better performance gains than collecting new raw data.
- Self-Supervised Learning: Leverage unlabeled data efficiently to reduce the need for manually labeled samples.
- Dataset Pruning and Curation: Before expanding a dataset, remove redundant, low-impact, or noisy samples to ensure new data contributes meaningfully.

These methods ensure that each new data point provides maximum learning value, preventing unnecessary dataset expansion and reducing training costs.

Dataset Growth Efficiency (DGE) measures how effectively new data improves model performance, helping determine when further data collection becomes inefficient. A high DGE indicates that additional data contributes significantly to learning, making continued dataset expansion valuable. Conversely, a low DGE suggests that adding more data provides minimal gains, leading to higher costs, longer training times, and wasted computational resources.

By monitoring and optimizing DGE, machine learning practitioners can ensure that dataset expansion remains cost-effective, preventing unnecessary data collection and focusing instead on data quality, curation, and augmentation strategies. This approach leads to more scalable, efficient, and sustainable AI systems, making the best use of computational resources while minimizing storage and energy costs.

### Metrics Reference

The table below translates data efficiency metrics into actionable steps that directly impact system efficiency.

| Metric                  | What It Measures                                         | Why It Matters for Data Efficiency                           | How It Affects System Efficiency                            | What Actions to Take                                        |
|-----------------------------|-------------------------------------------------------------|------------------------------------------------------------------|----------------------------------------------------------------|----------------------------------------------------------------|
| Performance-Per-Data Unit (PPD)  | The learning improvement per additional data point  | Helps decide whether more data is necessary or wasteful  | Low PPD inflates dataset size unnecessarily, increasing storage and compute costs | Stop adding data if PPD is low—focus on improving data quality instead |
| Data Usage Efficiency (DUE)  | The proportion of data actually needed to reach a target performance | A low DUE suggests too much data is being used for the same result | Low DUE increases training time, memory usage, and energy consumption | Reduce dataset size by removing unnecessary data points |
| Computational Efficiency of Data (CED)  | The computational cost per unit of model improvement | Shows whether data is expensive to process relative to its value | Low CED increases GPU/TPU costs, slows training, and raises power consumption | Use more efficient preprocessing or feature extraction to reduce computational burden |
| Redundancy Ratio (RR)  | The fraction of the dataset that is redundant or unnecessary  | High RR means data is being repeated without adding new insights | Inflates storage needs, wastes compute, and slows down training | Prune duplicate or near-duplicate samples to reduce redundancy |
| Dataset Growth Efficiency (DGE)  | How much additional data is required for a fixed model improvement | Helps determine when data collection becomes inefficient | Low DGE means that further data expansion yields minimal returns, increasing labeling and storage costs | Stop data collection if DGE is high—consider synthetic data or augmentation instead |

<!-- ✅ If PPD is low → More data is NOT the solution → Focus on data quality instead of dataset expansion

✅ If DUE is low → Too much data is being used inefficiently → Reduce dataset size while maintaining performance

✅ If CED is low → Data is computationally expensive to process → Use better data representations or feature extraction

✅ If RR is high → Dataset contains excessive duplication → Remove redundant samples to accelerate training

✅ If DGE is low → Adding more data isn’t improving accuracy enough → Stop data collection, focus on curation instead   -->

## Recipes for Optimization

The ability to measure data efficiency is essential for understanding how effectively a machine learning system utilizes data. However, measurement alone does not lead to improvement. Once inefficiencies are identified, the next step is to apply structured optimizations to ensure that datasets are not only sufficient for learning but also computationally efficient.

Data inefficiencies can arise from redundancy, excessive volume, high computational cost, or diminishing returns from data expansion. Without a systematic approach to optimization, interventions may be applied inconsistently, leading to suboptimal model performance, increased training time, and unnecessary resource consumption.

This section provides a concise roadmap for improving data efficiency using the previously introduced metrics:

- Performance-Per-Data Unit (PPD): Determines whether adding more data is beneficial.
- Data Usage Efficiency (DUE): Measures the proportion of data actually contributing to learning.
- Computational Efficiency of Data (CED): Evaluates how efficiently a dataset is processed.
- Redundancy Ratio (RR): Quantifies unnecessary duplication in data.
- Dataset Growth Efficiency (DGE): Assesses whether dataset expansion is justified.

By analyzing these metrics, practitioners can identify bottlenecks and apply targeted interventions to optimize dataset composition, reduce computational overhead, and improve overall system efficiency. The following sections outline a structured approach to diagnosing inefficiencies, selecting appropriate optimization strategies, and evaluating the impact of interventions.

### Optimization Strategies and Their Impact

Once inefficiencies in data usage have been identified, the next step is to apply targeted optimization strategies to improve both data efficiency and system performance. However, optimization should not be applied in an ad hoc manner. Instead, it should follow a structured, stepwise process that prioritizes eliminating redundancies first, then improving data selection and computational efficiency, before making adjustments to data augmentation or dataset growth strategies.

A well-structured optimization sequence ensures that each intervention builds on prior improvements, preventing unnecessary computation and ensuring that data is being used effectively. The process follows the five-stage optimization framework described below.

### Step 1. Pruning Redundant and Duplicate Data

#### Objective

Remove unnecessary data points that add little to model learning.

#### Metric to Optimize

Redundancy Ratio (RR)

#### Rationale

The first step in improving data efficiency is to eliminate redundant and duplicate samples from the dataset. Many datasets contain near-identical examples or repetitive patterns that do not contribute to model learning but increase storage costs and prolong training times. By identifying and removing these redundant samples, dataset size can be reduced without sacrificing performance, leading to a more compact and computationally efficient training process.

#### System Impact

✅ Reduces dataset size, leading to lower memory and storage costs.
✅ Speeds up training, as fewer data points need to be processed.
✅ Ensures that only unique and diverse samples remain, improving model generalization.

### Step 2. Selecting the Most Informative Data (Active Learning & Data Selection)

#### Objective

Retain only the most valuable training samples to maximize learning efficiency.

#### Metric to Optimize

Data Usage Efficiency (DUE)

#### Rationale

After removing redundancy, the next step is to ensure that every remaining data point contributes meaningfully to the learning process. Many datasets contain low-value or irrelevant samples that do not improve model generalization. By using data selection techniques, such as active learning, practitioners can identify the most informative examples and eliminate those that do not meaningfully impact the model’s learning curve.

#### System Impact

✅ Prevents unnecessary training on low-value data, reducing computational overhead.
✅ Improves model convergence, as training focuses only on relevant information.
✅ Optimizes data collection efforts, reducing labeling costs.

### Step 3. Optimizing Computational Efficiency (Feature Selection & Representation Learning)

#### Objective

Reduce the processing cost per data point by improving how data is represented.

#### Metric to Optimize

Computational Efficiency of Data (CED)

#### Rationale

Even when data is well-selected and free of redundancy, it may still be computationally expensive to process. Some datasets require excessive preprocessing, while others contain high-dimensional features that increase memory and compute requirements without necessarily improving performance. By applying feature selection and representation learning techniques, data can be structured more efficiently, enabling faster and more effective training.

#### System Impact

✅ Reduces the computational cost of data processing during training and inference.
✅ Lowers memory and storage requirements, improving system scalability.
✅ Enhances feature extraction, potentially improving model accuracy with fewer data points.

### Step 4. Increasing Dataset Diversity (Data Augmentation & Synthetic Data Generation)

#### Objective

Improve model generalization without excessive data collection.

#### Metric to Optimize

Performance-Per-Data Unit (PPD)

#### Rationale

Once a dataset is pruned, optimized, and computationally efficient, the next consideration is whether additional diversity is needed. Some models benefit from increased variation in training samples, but collecting more real-world data is often expensive and impractical. Instead, data augmentation and synthetic data generation techniques can be used to create additional training examples without requiring manual annotation. These methods expand the effective dataset size, improving generalization while keeping storage and compute costs low.

#### System Impact

✅ Improves model robustness to variations, reducing the risk of overfitting.
✅ Minimizes reliance on real-world data collection, lowering annotation costs.
✅ Balances dataset composition, ensuring better performance across different data distributions.

### Step 5. Assessing the Need for Further Data Collection

#### Objective

Determine whether additional data acquisition is necessary.

#### Metric to Optimize

Dataset Growth Efficiency (DGE)

#### Rationale

The final step in the optimization process is determining whether additional data collection is justified. While increasing dataset size can sometimes improve model performance, it is not always the most efficient approach. If the Performance-Per-Data Unit (PPD) is low and the Dataset Growth Efficiency (DGE) is poor, then further data collection may not be necessary. Instead, efforts should be directed toward improving data selection, augmentation, or feature representation.

#### System Impact

✅ Prevents unnecessary data accumulation, reducing long-term storage and compute costs.
✅ Ensures that data collection efforts are cost-effective and targeted toward actual performance gains.
✅ Encourages smarter data acquisition strategies, focusing on underrepresented or high-value samples.

### Structured Optimization for Maximum Efficiency

By following a stepwise approach to data efficiency optimization, machine learning practitioners can ensure that each improvement builds upon the previous one, leading to a streamlined and computationally efficient dataset.

The table below provides an overview of this structured process.

| Step | Optimization Strategy | Metric to Optimize | Expected System Impact |
|---------|-------------------------|---------------------|------------------------|
| 1 | Prune redundant and duplicate data | Redundancy Ratio (RR) | Eliminates excess data, reducing storage and training costs |
| 2 | Select the most informative samples (Active Learning) | Data Usage Efficiency (DUE) | Ensures that only useful data is retained, optimizing training efficiency |
| 3 | Improve computational efficiency (Feature Learning, Representation Optimization) | Computational Efficiency of Data (CED) | Reduces the computational burden of processing data |
| 4 | Increase dataset diversity (Augmentation, Synthetic Data) | Performance-Per-Data Unit (PPD) | Enhances generalization without requiring additional labeled data |
| 5 | Determine whether additional data is needed | Dataset Growth Efficiency (DGE) | Ensures further data collection is only pursued when justified |

By applying each optimization in sequence, machine learning pipelines can achieve higher performance while minimizing resource consumption. This ensures that data remains a valuable asset rather than an unnecessary burden in machine learning system design.

### Evaluating Data Efficiency Optimizations

Optimizing data efficiency is valuable only if it leads to measurable improvements in both data quality and system efficiency. Removing redundant samples, selecting more informative data points, or improving computational efficiency should not come at the cost of model accuracy or robustness. Therefore, evaluating the impact of optimizations is essential to ensure that efficiency gains translate into meaningful improvements without unintended consequences.

At a high level, the success of data efficiency optimizations can be assessed across three dimensions:

1. Dataset Quality: Has the dataset been improved without losing critical information?
2. Computational Efficiency: Has training or inference become faster without sacrificing performance?
3. Model Performance: Has the model maintained or improved accuracy, generalization, and robustness?

These three factors provide a structured way to analyze whether an optimization process has led to real system improvements. If dataset pruning, for instance, reduces redundancy but inadvertently removes essential edge cases, then it may not be a true improvement. Similarly, an optimization that improves inference speed at the cost of increased model bias is not a meaningful gain.

However, assessing optimization success is not always straightforward. Many efficiency improvements involve complex trade-offs, and the metrics used to measure these trade-offs are not always standardized. Without a systematic evaluation framework, different teams and researchers may apply different criteria for success, making it difficult to compare results.

This challenge highlights the need for benchmarking in data efficiency. Just as machine learning models are evaluated using standardized benchmarks for accuracy, data efficiency optimizations require structured benchmarks that measure improvements in a consistent and reproducible manner. Benchmarks help ensure that data-centric optimizations are evaluated rigorously, enabling fair comparisons across different models, datasets, and optimization strategies.

## Benchmarking Data Efficiency

### Motivation for Data Efficiency Benchmarks

Machine learning benchmarking has played a vital role in advancing AI systems, providing researchers and practitioners with standardized ways to evaluate models and compare different techniques. Benchmarks like MLPerf have helped optimize training and inference efficiency, allowing for fair comparisons between hardware accelerators, model architectures, and optimization strategies. However, while these benchmarks have greatly improved model and system efficiency, they largely overlook an equally important factor: data efficiency.

As machine learning continues to scale, the ability to use data effectively has become just as important as optimizing models and hardware. A well-designed neural network trained on redundant, noisy, or excessive amounts of data can be far less efficient than a more compact model trained on well-curated, high-value examples. Yet, despite the growing emphasis on data-centric AI, there is currently no widely accepted way to measure data efficiency in a standardized manner.

Imagine two machine learning models trained for the same image classification task. Both models achieve identical accuracy on a test set. However, one model required twice as much labeled data and took 40% longer to train than the other. From a data efficiency perspective, these models are not equal—one has learned to generalize effectively with fewer data points, while the other has relied on sheer volume to achieve the same result.

Currently, most benchmarks would treat both models as equally successful, since they focus primarily on accuracy and computational cost, without considering how efficiently the data was used. This gap in evaluation highlights the need for a structured benchmark for data efficiency—one that measures not just model performance, but how well a system utilizes data to achieve that performance.

A dedicated data efficiency benchmark would provide a structured, standardized way to evaluate dataset optimizations and training efficiency. Several challenges in modern AI development make this especially important:

1. Current Benchmarks Focus on Model and System Efficiency, Not Data Efficiency
   - Existing benchmarks prioritize accuracy, throughput, and computational efficiency, but do not assess whether a model is learning efficiently from its dataset.
   - Two models may reach the same accuracy, but if one requires significantly more training data, it is less efficient in utilizing data as a resource.

2. No Standardized Way to Evaluate Dataset Optimizations
   - Techniques like data pruning, augmentation, and active learning are widely used, but their impact is measured inconsistently across studies.
   - Without a benchmark, it is difficult to determine whether one data efficiency method truly outperforms another.

3. Data Efficiency Directly Affects System Performance and Sustainability
   - Using excessively large datasets increases training costs, energy consumption, and storage requirements without necessarily improving results.
   - A data-efficient model achieves similar performance while using fewer resources, leading to more sustainable and accessible AI systems.

4. Fair Comparisons Require Standardized Evaluation
   - Without a structured benchmark, different ML teams rely on ad hoc methods to measure data efficiency, making it difficult to compare approaches.
   - A well-defined benchmark would ensure fair, reproducible, and interpretable evaluations across models and datasets.

In the following sections, we will explore the key  of a data efficiency benchmark, discuss how to design fair and reproducible evaluation protocols, and introduce a proposal for DataPerf 2.0, a next-generation benchmark that will enable the ML community to measure, compare, and optimize how efficiently models use data.

### Key Components of a Data Efficiency Benchmark

A well-designed benchmark for data efficiency must be built on clear and quantifiable metrics that measure how effectively a machine learning system utilizes data. The metrics introduced earlier---Performance-Per-Data Unit (PPD), Data Usage Efficiency (DUE), Computational Efficiency of Data (CED), Redundancy Ratio (RR), and Dataset Growth Efficiency (DGE)---serve as the core foundation for evaluating data efficiency in a structured manner.

Each of these metrics captures a different aspect of data efficiency. PPD and DUE assess how well a dataset contributes to model learning, while CED links data efficiency to computational resource usage. RR helps determine whether dataset reductions remove meaningful diversity, and DGE measures how dataset expansion affects model performance over time. A comprehensive benchmark must incorporate all of these dimensions to ensure that efficiency improvements are genuinely beneficial across multiple axes, rather than simply reducing dataset size at the cost of generalization or fairness.

#### Dataset Efficiency Evaluation

The first essential component of a data efficiency benchmark is the dataset efficiency evaluation, which assesses how well a model performs given a constrained amount of data. A benchmark should quantify whether a model can achieve similar accuracy while using fewer data points, or whether dataset optimizations—such as pruning, augmentation, or active learning—lead to meaningful efficiency gains.

The Performance-Per-Data Unit (PPD) metric plays an important role here, as it helps establish whether a given model is learning efficiently or simply benefiting from an abundance of training samples. Similarly, Data Usage Efficiency (DUE) provides insights into how effectively each data point contributes to the learning process, allowing practitioners to compare different dataset selection strategies in a structured manner. These metrics help ensure that benchmarks do not just evaluate raw model performance but also how efficiently models leverage the data available.

#### System Efficiency Evaluation

Beyond dataset efficiency, a comprehensive benchmark must also incorporate system efficiency metrics to ensure that improvements in data utilization lead to tangible reductions in computational cost. An optimized dataset should not just improve model accuracy; it should also reduce training time, memory consumption, and inference latency to prevent trade-offs that might outweigh the benefits.

The Computational Efficiency of Data (CED) metric is important in this context. CED evaluates whether data optimizations reduce the time and resources required for training and inference. By tracking how much compute power is required per unit of training progress, the benchmark can identify approaches that genuinely improve efficiency rather than simply shifting the computational burden elsewhere.

#### Trade-offs Between Data Efficiency and Model Robustness

Another fundamental aspect of a data efficiency benchmark is its ability to capture trade-offs between data efficiency and model robustness. Many optimization techniques—such as dataset pruning or aggressive feature selection—may improve efficiency in controlled settings but degrade model generalization in real-world scenarios.

To account for this, a well-designed benchmark must evaluate whether efficiency gains come at the expense of performance consistency. The Redundancy Ratio (RR) helps ensure that optimizations do not remove valuable variations in the data, leading to models that are overfitted to a limited dataset. Likewise, Dataset Growth Efficiency (DGE) determines whether increasing dataset size provides meaningful performance improvements or whether a model has already reached a saturation point where additional data is no longer beneficial.

Together, these metrics offer a holistic view of how data-centric optimizations impact model behavior, so that benchmarking efforts capture both efficiency and generalization performance.

#### Task-Specific Evaluation Protocols

To be effective across different domains, a data efficiency benchmark must define task-specific evaluation protocols that align with the unique challenges of each machine learning application. The optimization of data efficiency in computer vision differs from that in natural language processing (NLP) or structured data analysis, requiring specialized benchmarks that account for these variations.

For instance, in computer vision tasks, a benchmark may evaluate how dataset optimizations impact fine-grained classification accuracy, particularly in scenarios where labeling costs are high. In NLP, on the other hand, the focus might be on how well language models retain performance when token-level supervision is reduced. Similarly, in structured data tasks, efficiency evaluations may consider the impact of missing or imbalanced data on model learning.

By ensuring that evaluation protocols are adaptable across different tasks, a benchmark allows practitioners to compare data efficiency improvements in a meaningful and fair manner, rather than imposing one-size-fits-all evaluation criteria.

#### Balancing Accuracy, Dataset Size, and Computational Efficiency

Ultimately, a benchmark for data efficiency must balance accuracy, dataset size, and computational efficiency, providing clear guidance on how to optimize machine learning workflows. By incorporating metrics that measure dataset effectiveness, system efficiency, and model robustness, the benchmark ensures that data-centric improvements are evaluated in a structured, reproducible manner.

### Standard Protocols and Best Practices

Evaluating data efficiency requires more than just defining relevant metrics; it demands a structured approach that ensures fair and reproducible comparisons across different machine learning tasks, datasets, and models. Without standardized protocols, results may vary depending on experimental conditions, making it difficult to determine whether an optimization method genuinely improves data efficiency or if its benefits are specific to a particular setup. A well-designed evaluation framework should provide clear guidelines on how to measure and report data efficiency improvements in a way that is both reproducible and applicable across a wide range of machine learning applications.

A key requirement for designing effective benchmarking protocols is ensuring alignment with the core data efficiency metrics introduced earlier. The Performance-Per-Data Unit (PPD), Data Usage Efficiency (DUE), Computational Efficiency of Data (CED), Redundancy Ratio (RR), and Dataset Growth Efficiency (DGE) provide a quantifiable foundation for comparing different optimization strategies. Standardized benchmarking protocols must define how and when these metrics are measured to ensure consistency across evaluations.

#### Reproducibility and Fair Comparisons

The first consideration in designing a benchmarking protocol is reproducibility. A reliable benchmark must produce consistent results when the same experiment is repeated under similar conditions. This means controlling for variables such as model architecture, hyperparameter settings, and data preprocessing methods. Without reproducibility, improvements in data efficiency may be attributed to factors unrelated to the actual optimization techniques being tested, leading to misleading conclusions. Establishing clearly defined baselines and ensuring that evaluations adhere to a standardized methodology allows researchers to draw meaningful comparisons between different approaches.

Fairness is another essential principle in designing a data efficiency benchmark. Because machine learning tasks vary widely in terms of data availability, complexity, and computational requirements, an evaluation framework must account for these differences to ensure fair assessments. Benchmarks should define controlled scenarios that allow different data efficiency techniques to be tested under comparable conditions. For instance, if one approach is evaluated on a limited dataset, while another benefits from access to significantly more training data, any observed improvements may be due to data volume differences rather than the inherent efficiency of the method. Standardizing the evaluation process ensures that techniques are judged on their ability to make the most of available data, rather than simply leveraging more resources.

The Redundancy Ratio (RR) plays a crucial role in ensuring fair comparisons, as it helps determine whether dataset optimizations preserve meaningful diversity rather than merely reducing dataset size. Similarly, Dataset Growth Efficiency (DGE) ensures that models benefiting from larger datasets are evaluated fairly by assessing whether data scaling leads to meaningful improvements or if diminishing returns are at play.

#### Scalability and Generalization Across Domains

Scalability is crucial in designing an effective benchmark. As machine learning models continue to grow in size and complexity, data efficiency evaluations must be designed to accommodate both small-scale and large-scale applications. A benchmark that works well for a limited dataset may not be practical for massive datasets used in industrial-scale machine learning. To address this, evaluation protocols should be adaptable to different dataset sizes and computational budgets while maintaining their ability to provide meaningful insights.

To account for these differences, benchmarking protocols must integrate system-aware efficiency metrics, such as Computational Efficiency of Data (CED). A data efficiency technique should not only improve model accuracy or reduce dataset size but also enhance overall system performance. Benchmarks should explicitly track memory usage, training time, and compute cost to ensure that optimizations are viable at scale.

Additionally, different domains—such as computer vision, NLP, and structured data analysis—have unique data efficiency challenges. A vision-based benchmark may focus on how efficiently models leverage labeled images, whereas an NLP benchmark might assess token-level supervision and pretraining efficiency. Standardized evaluation protocols should be flexible enough to accommodate these differences, so that fair comparisons are possible across diverse AI applications.

#### Benchmarking Methodologies and Their Connection to Metrics

To ensure comprehensive evaluation, a data efficiency benchmark can adopt multiple methodologies, depending on the nature of the task and the optimization approach being tested. These methodologies must be explicitly linked to relevant metrics, so that efficiency is measured systematically.

1. Fixed-Dataset Benchmarks
   - Models are trained and evaluated using a constrained dataset size.
   - Measures how well an optimization technique improves performance under data limitations.
   - Key Metrics: PPD (to measure model performance per data unit), DUE (to assess how effectively data points contribute to learning).

2. Adaptive Benchmarks
   - Models dynamically select and refine their training data over time.
   - Evaluates active learning, self-supervised learning, and data pruning.
   - Key Metrics: DUE (to measure selective learning efficiency), RR (to ensure that dataset pruning does not degrade model generalization).

3. Real-World Task-Based Benchmarks
   - Simulates practical constraints encountered in production, such as limited compute, storage, and real-time data availability.
   - Evaluates how well a model can maintain efficiency in unpredictable conditions.
   - Key Metrics: CED (to measure trade-offs between data and compute cost), DGE (to determine whether dataset expansion leads to meaningful improvements).

By structuring benchmarking methodologies around specific, measurable criteria, we ensure that DataPerf 2.0 provides transparent and meaningful insights, rather than relying on subjective efficiency claims.

#### Standardized Reporting and Open Benchmarking Efforts

Beyond defining the evaluation methodologies, the benchmarking process must establish standardized procedures for measuring impact and reporting results. Each optimization method should be tested against a clearly defined baseline, with improvements measured using a consistent set of metrics. Results should be reported in a structured format, highlighting not only performance gains but also any trade-offs introduced by the optimization.

A structured reporting format should include:

- Dataset size and composition before and after optimization.
- Model performance (accuracy, robustness, fairness) relative to dataset size.
- Computational cost metrics (training time, inference latency, memory usage).
- Trade-offs between data efficiency, system efficiency, and real-world constraints.

Adopting open benchmarking efforts, such as community-driven leaderboards and reproducibility challenges, ensures that findings are transparent, interpretable, and comparable across studies. Integration with MLPerf, Hugging Face, and other machine learning evaluation platforms could further encourage industry adoption and establish data efficiency as a standard benchmarking practice.

#### Challenges and Future Directions

Designing effective protocols for evaluating data efficiency remains a complex task, with several open challenges:

- Task-Specific Considerations: Some efficiency improvements may be highly domain-dependent, requiring custom evaluation methods.
- Long-Term Learning Impact: Standardized benchmarks must account for how optimizations affect training efficiency over time.
- Interoperability Across Workflows: Ensuring that benchmarks can be seamlessly integrated into existing ML pipelines is essential for adoption.

Addressing these challenges requires ongoing collaboration between researchers, industry practitioners, and benchmarking organizations. By refining evaluation methodologies and establishing best practices, the ML community can ensure that data efficiency remains a core consideration in future AI development.

A well-structured data efficiency benchmark provides the foundation for systematic evaluation, enabling the machine learning community to identify best practices and drive progress toward more efficient and sustainable AI systems. The next section will introduce DataPerf 2.0, a benchmark designed to fill this gap by establishing standardized evaluation protocols for data efficiency across diverse machine learning applications.

### Proposal: Defining DataPerf 2.0

The development of benchmarks for machine learning has traditionally focused on model performance and computational efficiency, but recent initiatives have recognized the need to evaluate the efficiency of data itself. One such initiative is DataPerf 1.0, a benchmark suite introduced to assess the quality and impact of data-centric AI techniques. DataPerf 1.0 was a foundational effort in defining structured evaluation methodologies for data quality, dataset selection, and active learning. However, as machine learning systems continue to evolve, it has become evident that a more comprehensive and scalable benchmarking framework is needed—one that extends beyond data selection and into the broader landscape of data efficiency.

#### DataPerf 1.0: The First Step Toward Data Benchmarking

DataPerf 1.0 was developed to address a critical gap in AI evaluation: while model benchmarks like MLPerf measure accuracy, latency, and throughput, there was no equivalent framework for evaluating how data optimization impacts learning efficiency. The first iteration of DataPerf introduced evaluation tracks for data selection, active learning, and labeling strategies, helping researchers compare methods for improving dataset quality. These benchmarks provided initial insights into how better-curated data could lead to improved model performance, even when using fewer labeled examples.

Despite its contributions, DataPerf 1.0 had several limitations. It primarily focused on specific data curation techniques, such as dataset selection and active learning, but did not comprehensively evaluate the broader spectrum of data efficiency optimizations. Additionally, it lacked systematic assessments of trade-offs between dataset size, compute cost, and model generalization. As machine learning applications scale, data efficiency must be measured in a way that accounts for its impact not just on accuracy, but on overall system performance.

#### The Need for DataPerf 2.0

Building on the foundation of DataPerf 1.0, DataPerf 2.0 is designed to provide a more holistic and structured approach to benchmarking data efficiency. It expands the scope of evaluation beyond data selection and labeling to include data pruning, augmentation, compression, and dataset scalability. Unlike its predecessor, which focused primarily on isolated data quality improvements, DataPerf 2.0 introduces comprehensive metrics that capture the interplay between dataset optimization and system efficiency.

A key advancement in DataPerf 2.0 is the introduction of a unified data efficiency score that integrates multiple dimensions of evaluation. While DataPerf 1.0 provided individual benchmarks for different data curation strategies, the new framework quantifies the trade-offs between dataset size reduction, model accuracy retention, and computational resource consumption. By incorporating metrics such as Performance-Per-Data Unit (PPD), Data Usage Efficiency (DUE), and Computational Efficiency of Data (CED), DataPerf 2.0 ensures that improvements in data efficiency are evaluated from both a data-centric and a system-centric perspective.

#### Key Innovations in DataPerf 2.0

To overcome the limitations of its predecessor, DataPerf 2.0 introduces several key innovations:

- A broader evaluation framework that measures not just data selection efficiency, but also dataset pruning, augmentation strategies, self-supervised learning, and synthetic data generation.
- Task-specific benchmarks covering computer vision, natural language processing, and structured data, so that data efficiency is measured across diverse AI applications.
- Real-world deployment simulations that assess how data efficiency optimizations perform in production settings with limited compute, storage, and real-time constraints.
- Standardized baselines and reporting protocols that ensure reproducibility and fair comparisons across different methods.

Unlike DataPerf 1.0, which largely evaluated offline dataset curation, the new version also accounts for adaptive learning paradigms, where models refine their training data selection dynamically. By introducing benchmarks that assess the efficiency of data-centric AI techniques over time, DataPerf 2.0 enables a more realistic evaluation of how data optimizations impact long-term model training and deployment.

With DataPerf 2.0, the machine learning community gains a standardized and scalable way to assess the efficiency of data-driven optimizations. This benchmark not only refines the methodologies introduced in DataPerf 1.0, but also aligns data efficiency evaluation with broader system performance considerations. By integrating dataset quality, computational resource usage, and model generalization into a single benchmark, DataPerf 2.0 ensures that data efficiency improvements are measured holistically rather than in isolation.

To illustrate the progress from DataPerf 1.0 to 2.0, the following table highlights the key differences and improvements in the new benchmark:

| Feature             | DataPerf 1.0 | DataPerf 2.0 |
|-------------------------|-----------------|-----------------|
| Focus              | Dataset selection and labeling strategies | Comprehensive data efficiency evaluation, including pruning, augmentation, and self-supervised learning |
| Evaluation Scope   | Static dataset selection | Dynamic learning processes and adaptive data refinement |
| Metrics Used       | Basic dataset filtering metrics | Holistic data efficiency metrics (PPD, DUE, CED, RR, DGE) |
| System Considerations | Limited system-level evaluation | Measures trade-offs between data efficiency, computational cost, and model generalization |
| Task Coverage      | Focused on dataset curation for vision and NLP | Broader scope covering vision, NLP, structured data, and real-world deployment constraints |
| Benchmark Structure | Independent evaluations of data selection methods | Unified framework integrating multiple data efficiency optimizations |

This comparison highlights how DataPerf 2.0 expands the scope of evaluation and establishes a more structured approach to data efficiency benchmarking. By moving beyond static dataset selection and incorporating system-aware evaluations, the new benchmark ensures that data efficiency improvements align with scalability and real-world applicability.

## Fallacies and Pitfalls {#sec-data-efficiency-fallacies}

**Fallacy: "Data is the new oil, so more is always better."**
The "Data is Oil" metaphor fails to capture the diminishing returns of information. As shown by the Dataset Growth Efficiency (DGE) metric, there is a saturation point where adding terabytes of data yields negligible accuracy gains while exploding compute costs.
**Reality:** Data is like fuel—it has a specific energy density. High-quality, curated data (high octane) powers models more efficiently than vast quantities of raw data (crude oil).

**Fallacy: "Synthetic data can completely replace real data."**
While synthetic data addresses scarcity, it is bounded by the generator's knowledge. A model trained purely on synthetic data from another model risks "Model Collapse"—a degenerative feedback loop where errors are amplified.
**Reality:** Synthetic data augments, but rarely replaces, the grounding provided by real-world distributions. It is best used to fill gaps in the data manifold, not to define it.

**Fallacy: "Data efficiency is just data cleaning."**
Cleaning (removing errors) is necessary but insufficient. True data efficiency involves *selection* (finding the decision boundary) and *synthesis* (creating hard negatives).
**Reality:** You can have a perfectly clean dataset that is highly inefficient because it is filled with redundant, easy examples. Efficiency requires optimizing the information content, not just the hygiene.

## Summary {#sec-data-efficiency-summary}

As machine learning systems continue to grow in complexity, data efficiency will play an increasingly central role in AI scalability, accessibility, and sustainability. This chapter has reframed data not as a static asset to be collected, but as a dynamic resource to be optimized. By applying the Information-Compute Ratio (ICR), we can rigorously evaluate whether our data pipelines are contributing to learning or merely consuming GPU cycles.

We explored the three-stage optimization pipeline: **Static Pruning** to remove redundancy before training, **Dynamic Selection** to focus compute on hard examples during training, and **Synthetic Generation** to create information where none exists. Together, these strategies allow us to break the "Data Wall" and scale performance without linearly scaling costs.

::: {.callout-important title="Key Takeaways"}

* **Information > Volume**: The goal of data efficiency is to maximize the Information-Compute Ratio (ICR). A smaller, higher-density dataset often yields better convergence than a massive, redundant one.
* **The Three Stages of Efficiency**: Optimization happens at three stages: *Pre-training* (Coresets/Deduplication), *During Training* (Active/Curriculum Learning), and *On-Demand* (Augmentation/Synthesis).
* **Metrics Matter**: You cannot optimize what you cannot measure. Metrics like Performance-Per-Data (PPD) and Data Usage Efficiency (DUE) are as critical as accuracy or latency.
* **Sustainability**: Data efficiency is the most powerful lever for Green AI. Reducing dataset size by 50% cuts training energy by 50%—a gain that hardware optimization alone rarely achieves.

:::

With a high-efficiency data pipeline in place, we have minimized the "math required to learn." The next step in the optimization triad is to minimize the "math required to represent."

The next chapter, @sec-model-compression, examines how we can shrink the model itself through pruning, quantization, and distillation, ensuring that our efficient data feeds into an efficient engine.

<!-- This is here to make sure that quizzes are inserted properly before a part begins. -->
::: { .quiz-end }
:::
