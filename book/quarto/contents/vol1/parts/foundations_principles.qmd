# Part I: Foundations {.unnumbered}

Part I establishes the invariant constraints that govern all machine learning systems. These are not best practices that evolve with frameworks or opinions that differ between teams—they are the physics of ML engineering. Every architectural decision in this volume is a response to these laws.

Before diving into architectures, frameworks, and optimizations, we must first understand *why* certain designs succeed while others fail. The answer lies not in clever algorithms but in constraints imposed by hardware, mathematics, and information theory. Just as civil engineers cannot ignore gravity, ML engineers cannot ignore the laws that govern data, computation, and complexity.

## The Invariants {#sec-foundations-invariants}

Four principles define the physics of ML systems. We begin with the meta-principle that governs all system design:

::: {.callout-note title="Conservation of Complexity" icon="false"}
**The Invariant**: You cannot destroy complexity; you can only displace it. In ML systems, complexity flows between the **Information** (Data), the **Logic** (Algorithm), and the **Physics** (Machine).
$$ \Delta C_{total} = \Delta C_{data} + \Delta C_{algorithm} + \Delta C_{machine} \approx 0 $$

**The Implication**: The job of the AI Engineer is not to remove complexity, but to choose where it lives. "Software 2.0" moves complexity from code (Machine) to optimization (Algorithm) and curation (Information). If you simplify the logic, you *must* increase the complexity of your data pipeline or your physical infrastructure to maintain performance.
:::

If complexity must live somewhere, where does it reside in ML systems? The answer reveals the fundamental difference between traditional software and machine learning:

::: {.callout-note title="The Data as Code Invariant" icon="false"}
**The Invariant**: Data *is* the source code of the ML system. A change in the training dataset ($\Delta D$) is functionally equivalent to a change in the executable logic ($\Delta P$).
$$ \text{System Behavior} \approx f(\text{Data}) $$

**The Implication**: Data engineering requires the same rigor as software engineering. Datasets must be **versioned** (like git), **unit-tested** (data quality checks), and **debugged**. Deleting a row of training data is the engineering equivalent of deleting a line of code; retraining a model is simply recompiling the binary.
:::

But data is not just logically important—it has physical properties that constrain system architecture. Unlike code, which can be copied and distributed freely, data resists movement:

::: {.callout-note title="The Data Gravity Invariant" icon="false"}
**The Invariant**: Data possesses mass. As dataset scale ($D$) increases, the cost (latency, bandwidth, energy) of moving data exceeds the cost of moving compute.
$$ C_{move}(D) \gg C_{move}(Compute) $$

**The Implication**: Data is not just cargo; it is the gravitational center of the architecture. Systems must shift from "Data-to-Compute" (downloading datasets) to "Compute-to-Data" (shipping queries or code to the storage layer). This invariant drives the design of **data lakes**, **warehouse-scale computers**, and **federated learning**.
:::

Data gravity explains *where* data lives; the final principle explains how data *flows*. Even with perfectly positioned data, systems face a fundamental throughput constraint:

::: {.callout-note title="The Pipeline Stall Law" icon="false"}
**The Invariant**: The throughput of a training system is strictly bounded by the slowest component in the data pipeline, not the fastest accelerator.
$$ T_{system} = \min(T_{IO}, T_{CPU}, T_{GPU}) $$

**The Implication**: GPU starvation is the default state of deep learning. To saturate accelerators, preprocessing (CPU) and loading (I/O) must be perfectly prefetched to hide latency. If $T_{IO} > T_{GPU}$, no amount of GPU optimization will improve performance.
:::

## Part I Roadmap {#sec-foundations-roadmap}

The chapters in Part I build the conceptual foundation for everything that follows:

1. **Introduction (@sec-introduction)**: Why ML systems engineering exists as a discipline. We establish the core metric—*Samples per Dollar*—and frame the optimization problem that the rest of the book solves.

2. **ML System Architecture (@sec-ml-system-architecture)**: How physical constraints (speed of light, power wall, memory wall) create the deployment spectrum from cloud to TinyML. This chapter applies the Pipeline Stall Law to explain why different environments demand fundamentally different architectures.

3. **The AI Development Workflow (@sec-ai-development-workflow)**: The lifecycle that manages the Conservation of Complexity. From data collection through deployment, we trace how complexity moves between stages and how teams coordinate that flow.

4. **Data Engineering for ML (@sec-data-engineering-ml)**: The practical application of the Data-as-Code and Data Gravity invariants. This chapter covers the tools and techniques for treating data with the same engineering rigor as source code.

By the end of Part I, you will have internalized the invariants that constrain every ML system and learned to reason about where complexity must reside in your designs. With these foundations in place, Part II turns from understanding constraints to constructing systems that operate within them, beginning with the mathematical machinery of deep learning and progressing through architectures, frameworks, and training.
