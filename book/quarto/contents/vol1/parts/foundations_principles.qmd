# Part I: Foundations {#sec-part-foundations .unnumbered}

Machine learning systems obey a deceptively simple conservation law: you cannot destroy complexity, you can only move it. Complexity in an ML system flows between three domains: the *information* (data), the *logic* (algorithm), and the *physics* (machine). Simplify one and you necessarily burden the others. A hand-crafted feature pipeline reduces algorithmic complexity but demands more data engineering effort. A larger model absorbs messy data but shifts complexity onto the hardware that must train and serve it. The entire discipline of ML systems engineering reduces to a single strategic question: *where should complexity reside?*

This insight, which we call the Conservation of Complexity, is the meta-principle that motivates everything in this book. The twelve quantitative principles introduced across Parts I through IV are its specific, measurable instantiations: each one quantifies a constraint that emerges from where complexity currently resides. Traditional software engineering encodes logic explicitly in source code; machine learning inverts this process, deriving logic implicitly from data through optimization. The movement Richard Socher and Andrej Karpathy termed "Software 2.0" is, at its core, a deliberate displacement of complexity from hand-written code to curated datasets and learned parameters. Understanding this displacement is what separates principled system design from trial and error.

Before diving into architectures, frameworks, and optimizations, we must first understand *why* certain designs succeed while others fail. The answer lies not in clever algorithms but in constraints imposed by hardware, mathematics, and information theory. Just as civil engineers cannot ignore gravity, ML engineers cannot ignore the physical laws that govern data, computation, and system throughput. Part I establishes these invariant constraints. They are not best practices that evolve with frameworks or opinions that differ between teams. They are the physics of ML engineering, and every architectural decision in this book is a response to them.

## The Invariants {#sec-foundations-invariants}

Two invariants define the physics of data in ML systems. The first reveals the fundamental difference between traditional software and machine learning:

::: {.callout-principle title="The Data as Code Invariant" icon="false"}
**The Invariant**: Data *is* the source code of the ML system. A change in the training dataset ($\Delta D$) is functionally equivalent to a change in the executable logic ($\Delta P$).
$$ \text{System Behavior} \approx f(\text{Data}) $$

**The Implication**: Data engineering requires the same rigor as software engineering. Datasets must be **versioned** (like git), **unit-tested** (data quality checks), and **debugged**. Deleting a row of training data is the engineering equivalent of deleting a line of code; retraining a model is simply recompiling the binary.
:::

If data is the source code, then it is not merely a logical artifact. Data also has physical properties that constrain system architecture. Unlike code, which can be copied and distributed freely, data resists movement:

::: {.callout-principle title="The Data Gravity Invariant" icon="false"}
**The Invariant**: Data possesses mass. As dataset scale ($D$) increases, the cost (latency, bandwidth, energy) of moving data exceeds the cost of moving compute.
$$ C_{move}(D) \gg C_{move}(Compute) $$

**The Implication**: Data is not just cargo; it is the gravitational center of the architecture. Systems must shift from "Data-to-Compute" (downloading datasets) to "Compute-to-Data" (shipping queries or code to the storage layer).
:::

Together, these two invariants establish that data is both the logical program and the physical anchor of every ML system. But understanding where data lives and what it means is only half the picture. Engineers must also reason about how data *flows* through the system. Even with perfectly positioned data, a training pipeline is only as fast as its slowest stage: if I/O cannot feed the accelerator, the most powerful GPU in the world sits idle. This throughput bottleneck, where system performance is bounded by $T_{system} = \min(T_{IO}, T_{CPU}, T_{GPU})$, means that GPU starvation is the default state of ML training. Preprocessing and data loading must be carefully prefetched to hide latency and keep accelerators saturated. This is a specific instance of a general principle that students will encounter formally as **Amdahl's Law** in Part III, where we develop the full quantitative framework for analyzing bottlenecks across computation, memory, and communication.

## Part I Roadmap {#sec-foundations-roadmap}

The chapters in Part I build the conceptual foundation for everything that follows:

1. **Introduction (@sec-introduction)**: *Why* ML systems engineering exists as a discipline. We establish the core metric, *Samples per Dollar*, and frame the optimization problem that the rest of the book solves.

2. **ML Systems (@sec-ml-system-architecture)**: *How* physical constraints (speed of light, power wall, memory wall) create the deployment spectrum from cloud to TinyML. This chapter examines *why* different environments demand fundamentally different architectures and *how* pipeline throughput constrains every design.

3. **ML Workflow (@sec-ai-development-workflow)**: The lifecycle that manages complexity across stages. From data collection through deployment, we trace how engineering effort shifts between data, algorithms, and infrastructure, and how teams coordinate that flow.

4. **Data Engineering (@sec-data-engineering-ml)**: The practical application of the Data as Code and Data Gravity invariants. This chapter covers the tools and techniques for treating data with the same engineering rigor as source code.

By the end of Part I, you will have internalized the invariants that constrain every ML system and learned to reason about where complexity must reside in your designs. With these foundations in place, Part II turns from understanding constraints to constructing systems that operate within them, beginning with the mathematical machinery of deep learning and progressing through architectures, frameworks, and training.
