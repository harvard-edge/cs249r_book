# Part III: Optimize { .unnumbered}

A working model is rarely an efficient one. Part II established how to construct ML systems that respect physical constraints; Part III addresses how to meet real-world demands on time, memory, and energy. Every optimization involves navigating a frontier: improving one metric (accuracy, latency, energy) while managing the cost to others. The principles here define the physics of efficiency — the laws that determine *why* some models are fast and affordable while others are slow and prohibitively expensive.

::: {.callout-principle title="The Pareto Frontier" icon="false"}
**The Invariant**: Optimization is not a single-objective problem. It is a multi-dimensional search for the **Pareto Frontier** — the boundary where no metric can be improved without degrading at least one other.

- **Quantization** trades numerical precision for reduced memory footprint.
- **Pruning** trades model capacity for computational speed.
- **Distillation** trades training compute for inference efficiency.

**The Implication**: Your job as a systems engineer is to navigate this frontier to find the "sweet spot" for your specific deployment environment. There is no universal optimum.
:::

Navigating the Pareto Frontier requires knowing *which* resource to optimize. Before selecting a technique, you must diagnose the bottleneck:

::: {.callout-principle title="Arithmetic Intensity Law" icon="false"}
**The Invariant**: Attainable throughput ($R$) is bounded by the minimum of peak compute ($R_{peak}$) and memory bandwidth ($BW$) relative to the workload's arithmetic intensity ($I$) [@williams2009roofline]:
$$ R = \min(R_{peak}, I \times BW) $$

**The Implication**: Adding compute power to a memory-bound model yields **zero** performance gain. You must identify whether your bottleneck is Math (Compute-Bound) or Memory (Bandwidth-Bound) before selecting an optimization technique.
:::

@tbl-bottleneck-diagnostic maps each bottleneck type to the optimization that addresses it — and, equally important, the optimization that would be wasted.

| **If You're...**  | **Dominant Term**         | **Optimization That Works**              | **Optimization That's Wasted** |
|:------------------|:--------------------------|:-----------------------------------------|:-------------------------------|
| **Memory-Bound**  | $D_{vol}/BW$              | Quantization, pruning, batching          | Faster GPU (more FLOP/s)       |
| **Compute-Bound** | $O/(R_{peak} \cdot \eta)$ | Better kernels, Tensor Cores, faster GPU | More memory bandwidth          |
| **Latency-Bound** | $L_{lat}$                 | Batching, kernel fusion, async dispatch  | More FLOP/s or bandwidth alone |

: **The Bottleneck Diagnostic.** Before optimizing, identify which Iron Law term dominates. Optimizing the wrong term yields zero improvement. {#tbl-bottleneck-diagnostic}

The majority of ML workloads fall on the memory-bound side of the ridge point. This is not coincidence but a consequence of the **Memory Wall**: processor speed has historically outpaced memory bandwidth, and the cumulative gap has widened over three decades. Neural networks, with their massive weight tensors and poor temporal locality, are especially vulnerable. The Arithmetic Intensity Law diagnoses *where* you sit relative to this wall; the next principle explains *why* that wall is so punishing:

::: {.callout-principle title="The Energy-Movement Invariant" icon="false"}
**The Invariant**: Moving 1 bit of data from DRAM costs 100--1,000$\times$ more energy than performing an arithmetic operation on it.
$$ E_{move} \gg E_{compute} $$

**The Implication**: **Data Locality** is the primary driver of efficiency. Optimization strategies must prioritize **kernel fusion** (keeping data in registers) and **quantization** (reducing data size) over reducing raw operation counts.
:::

Even with perfect data locality and optimal bottleneck targeting, a final constraint limits how much speedup is achievable:

::: {.callout-principle title="Amdahl's Law" icon="false"}
**The Invariant**: The maximum speedup of a system is limited by the fraction of the workload that cannot be accelerated.
$$ \text{Speedup} = \frac{1}{(1-p) + \frac{p}{s}} $$
where $p$ is the parallelizable fraction and $s$ is the speedup of that fraction.

**The Implication**: If 95% of your model runs 100$\times$ faster on a GPU, your total system speedup is capped at ~16.8$\times$. This explains *why* **data loading** and **preprocessing** often become the ultimate bottlenecks in highly optimized systems.
:::

Part III applies these principles systematically through the D·A·M taxonomy — Data, Algorithm, Machine — asking first whether the work is necessary, then whether it can be simplified, and finally how to do it faster (see @sec-dam-taxonomy for the full diagnostic framework).
