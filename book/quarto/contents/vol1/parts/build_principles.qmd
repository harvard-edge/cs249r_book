# Part II: Build {.unnumbered}

Part II moves from understanding constraints to constructing systems that work within them. Building a machine learning system is not merely about stacking layers; it is about managing the flow of information and energy through silicon.

If Part I established *what* limits us, Part II addresses *how* we design around those limits. The principles here define the "Physics of Build"—the laws that govern why certain architectures succeed while others fail at scale.

## The Invariants {#sec-build-invariants}

Four principles govern the construction of ML systems:

::: {.callout-note title="The Iron Law of ML Systems" icon="false"}
**The Invariant**: The latency ($L$) of any machine learning operation is determined by the sum of three components, divided by the system's efficiency ($\eta$):
$$ L = \frac{\text{Data Movement} + \text{Compute} + \text{System Overhead}}{\eta} $$

**The Implication**: Optimization is a zero-sum game. If you reduce Compute (e.g., via pruning), you often increase Data Movement (random memory access). A "faster" algorithm on paper is only faster in reality if it reduces the **dominant** term in this equation for your specific hardware.
:::

::: {#sec-silicon-contract .callout-note title="The Silicon Contract" icon="false"}
**The Invariant**: Every model architecture makes an implicit agreement with the hardware.
- **ResNet-50** assumes high-density floating-point compute (Compute-Bound).
- **Llama-3-8B** assumes high-bandwidth memory access (Bandwidth-Bound).
- **DLRM** assumes massive memory capacity for lookup tables (Capacity-Bound).

**The Implication**: Designing a model without knowing which hardware resource it will saturate is like designing a bridge without knowing the strength of the steel. You must design for the **Bottleneck**.
:::

::: {.callout-note title="The Depth-Efficiency Law" icon="false"}
**The Invariant**: In hierarchical feature extractors, computational efficiency is maximized when early layers process high-resolution, low-semantic data (edges, textures) and later layers process low-resolution, high-semantic data (objects, concepts).

**The Implication**: Architectures should be tapered (e.g., Pyramidal CNNs). Allocating equal compute to every layer is inefficient; capacity should be concentrated where semantic density is highest.
:::

::: {.callout-note title="The Residual Gradient Invariant" icon="false"}
**The Invariant**: Deep network trainability is preserved only if there exists a direct path for gradient flow that bypasses non-linear transformations.
$$ \frac{\partial L}{\partial x} \approx 1 $$

**The Implication**: As networks grow deeper, **Skip Connections** (ResNets, Transformers) become mandatory. Without them, the "Shattered Gradients" problem prevents error signals from propagating to early layers, halting learning.
:::

## Part II Roadmap {#sec-build-roadmap}

The chapters translate these principles into the components of the ML stack. Each chapter assembles one leg of the AI Triad: we define the **Algorithm** (architectures), implement it in **Machine** (frameworks and training infrastructure), and prepare the **Data** pipelines that feed both.

1. **Deep Learning Foundations (@sec-deep-learning-systems-foundations)**: The mathematical foundations of gradient flow—backpropagation, loss landscapes, and the calculus that makes learning possible.

2. **DNN Architectures (@sec-dnn-architectures)**: How we structure the Silicon Contract for Vision, Language, and Recommendations. Each architecture family represents a different bet on which hardware resource to saturate.

3. **AI Frameworks (@sec-ai-frameworks)**: The software that automates the Iron Law—automatic differentiation, graph optimization, and the abstractions that let engineers focus on architecture rather than gradients.

4. **AI Training (@sec-ai-training)**: The physical execution of the build process at scale—distributed training, mixed precision, and the infrastructure that turns algorithms into trained models.
