# Part II: Build {#sec-part-build .unnumbered}

Part I established that data is both the program and the physical anchor of every ML system. Part II now turns to the compute side of the equation: constructing systems that work within those constraints. Building a machine learning system is not merely about stacking layers; it is about managing the flow of information and energy through silicon.

Where Part I focused on *what* limits us, Part II addresses *how* we design around those limits. The principles here are the physics of building ML systems — the laws that govern *why* certain architectures succeed while others fail at scale.

## The Invariants {#sec-build-invariants}

Two principles govern the construction of ML systems. We begin with the fundamental equation that determines all ML performance:

::: {.callout-principle title="The Iron Law of ML Systems" icon="false"}
**The Invariant**: The total time ($T$) of any machine learning operation is governed by three components — data movement, compute, and fixed system overhead:
$$ T = \frac{D_{vol}}{BW} + \frac{O}{R_{peak} \cdot \eta} + L_{lat} $$
where $D_{vol}$ is data volume (bytes moved), $BW$ is memory bandwidth, $O$ is total floating-point operations, $R_{peak}$ is peak compute rate, $\eta$ is hardware utilization efficiency, and $L_{lat}$ is fixed latency overhead such as kernel launch or network round-trip time. (For the full notation rationale, see the Notation and Conventions section.)

When these stages overlap on modern hardware, wall-clock time is dominated by whichever term is largest. This is why the equation's practical lesson is about **dominance**, not summation: the term that takes longest sets the floor.

**The Implication**: Optimization is rarely free of trade-offs. Reducing one term often shifts the bottleneck to another. For example, unstructured pruning reduces compute ($O$) but introduces irregular memory access patterns that can increase data movement ($D_{vol}/BW$). A "faster" algorithm on paper is only faster in reality if it reduces the **dominant** term for your specific hardware.
:::

The **Iron Law** tells us *what* to optimize, but not *how*. The answer depends on which hardware resource your architecture will saturate, a choice that defines an implicit contract:

::: {.callout-principle title="The Silicon Contract" icon="false"}
**The Invariant**: Every model architecture makes an implicit commitment to the hardware — a wager on which resource it will saturate first.

- **ResNet-50** assumes high-density floating-point compute. It is **Compute-Bound**: performance is limited by $O / (R_{peak} \cdot \eta)$.
- **Llama-3-8B** assumes high-bandwidth memory access. It is **Bandwidth-Bound**: performance is limited by $D_{vol} / BW$.
- **DLRM** assumes massive memory capacity for embedding lookup tables. It is **Capacity-Bound**: performance is limited by whether the working set fits in fast memory at all.

**The Implication**: Designing a model without knowing which hardware resource it will saturate is like designing a bridge without knowing the strength of the steel. You must design for the **Bottleneck**.
:::

Together, the **Iron Law** and the **Silicon Contract** frame every design decision in Part II. But these system-level constraints do not operate in a vacuum. They interact with architectural choices that determine how computation is distributed across a network's layers.

Consider how convolutional networks allocate work. Early layers process high-resolution spatial data (edges, textures), while later layers process lower-resolution but semantically richer data (objects, concepts). This tapering is not arbitrary; it reflects an efficiency insight. Allocating equal compute to every layer wastes resources on stages where semantic density is low. By concentrating capacity where it matters most, these architectures make an implicit commitment under the **Silicon Contract** — one whose compute-to-memory implications are examined in detail in @sec-dnn-architectures.

Depth itself introduces a further consideration. As networks grow deeper, expressiveness increases, but so does the difficulty of training: learning signals can weaken as they travel backward through many layers, a problem known as *vanishing gradients*. Skip connections — the architectural innovation behind ResNets and Transformers — solve this by providing a direct path for these signals, ensuring that the optimization process the **Iron Law** governs can actually converge. The mathematical foundations of gradient flow are developed in @sec-deep-learning-systems-foundations, and the architectural patterns that maintain it are explored in @sec-dnn-architectures.

## Part II Roadmap {#sec-build-roadmap}

The chapters translate these principles into the components of the ML stack. We focus on the constructive elements of the **D·A·M taxonomy**: defining the **Logic** (Algorithm) through architectures, and provisioning the **Physics** (Machine) through frameworks and training systems.

1. **Neural Computation (@sec-deep-learning-systems-foundations)**: The mathematical foundations of gradient flow, including backpropagation, loss landscapes, and the calculus that makes learning possible.

2. **Network Architectures (@sec-dnn-architectures)**: Defining the **Logic**. How we structure the Silicon Contract for Vision, Language, and Recommendations. Each architecture family represents a different commitment to which hardware resource it will saturate.

3. **ML Frameworks (@sec-ai-frameworks)**: Provisioning the **Physics**. The software that automates the Iron Law, including automatic differentiation, graph optimization, and the abstractions that let engineers focus on architecture rather than gradients.

4. **Model Training (@sec-ai-training)**: Executing the **Physics** at scale. The physical execution of the build process, including distributed training, mixed precision, and the infrastructure that turns algorithms into trained models.

By the end of Part II, you will be able to design, implement, and train ML systems that respect the **Silicon Contract** and the **Iron Law**. However, a working model is rarely an efficient one. Part III introduces the optimization principles and techniques that transform a functional system into one that meets real-world constraints on latency, memory, and energy.
