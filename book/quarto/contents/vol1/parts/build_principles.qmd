# Part II: Build {#sec-part-build .unnumbered}

Part II moves from understanding constraints to constructing systems that work within them. Building a machine learning system is not merely about stacking layers; it is about managing the flow of information and energy through silicon.

If Part I established *what* limits us, Part II addresses *how* we design around those limits. The principles here define the "Physics of Build," the laws that govern *why* certain architectures succeed while others fail at scale.

## The Invariants {#sec-build-invariants}

Two principles govern the construction of ML systems. We begin with the fundamental equation that determines all ML performance:

::: {.callout-principle title="The Iron Law of ML Systems" icon="false"}
**The Invariant**: The total time ($T$) of any machine learning operation is determined by the sum of three components: data movement, compute (scaled by efficiency $\eta$), and system overhead:
$$ T = \frac{D_{vol}}{BW} + \frac{O}{R_{peak} \cdot \eta} + L_{lat} $$

**The Implication**: Optimization is a zero-sum game. If you reduce Compute (e.g., via pruning), you often increase Data Movement (random memory access). A "faster" algorithm on paper is only faster in reality if it reduces the **dominant** term in this equation for your specific hardware.
:::

The Iron Law tells us *what* to optimize, but not *how*. The answer depends on which hardware resource your architecture will saturate, a choice that defines an implicit contract:

::: {.callout-principle title="The Silicon Contract" icon="false"}
**The Invariant**: Every model architecture makes an implicit agreement with the hardware.

- **ResNet-50** assumes high-density floating-point compute (Compute-Bound).
- **Llama-3-8B** assumes high-bandwidth memory access (Bandwidth-Bound).
- **DLRM** assumes massive memory capacity for lookup tables (Capacity-Bound).

**The Implication**: Designing a model without knowing which hardware resource it will saturate is like designing a bridge without knowing the strength of the steel. You must design for the **Bottleneck**.
:::

Together, the Iron Law and the Silicon Contract frame every design decision in Part II. But these system-level constraints do not operate in a vacuum. They interact with architectural choices that determine how computation is distributed across a network's layers.

Consider how hierarchical feature extractors, such as convolutional networks, allocate work. Early layers process high-resolution, low-semantic data (edges, textures), while later layers process low-resolution, high-semantic data (objects, concepts). This tapering of computation is not arbitrary; it reflects an efficiency insight. Allocating equal compute to every layer wastes resources on stages where semantic density is low. Pyramidal architectures concentrate capacity where it matters most, and in doing so, they make a specific bet under the Silicon Contract: that the compute-to-memory ratio will shift as spatial resolution decreases. Students will see this pattern in detail when @sec-dnn-architectures examines convolutional and vision architectures.

Depth itself introduces a further consideration. As networks grow deeper, expressiveness increases, but so does the difficulty of training. Without a direct path for gradient flow that bypasses nonlinear transformations, error signals attenuate exponentially and learning stalls. This is *why* skip connections, the architectural innovation behind ResNets and Transformers, have become standard practice in deep network design. They preserve gradient magnitude across many layers, ensuring that the optimization process the Iron Law governs can actually converge. The mathematical foundations of gradient flow are developed in @sec-deep-learning-systems-foundations, and the architectural patterns that maintain it are explored in @sec-dnn-architectures.

## Part II Roadmap {#sec-build-roadmap}

The chapters translate these principles into the components of the ML stack. We focus on the constructive elements of the **DAM Taxonomy**: defining the **Logic** (Algorithm) through architectures, and provisioning the **Physics** (Machine) through frameworks and training systems.

1. **Neural Computation (@sec-deep-learning-systems-foundations)**: The mathematical foundations of gradient flow, including backpropagation, loss landscapes, and the calculus that makes learning possible.

2. **Network Architectures (@sec-dnn-architectures)**: Defining the **Logic**. How we structure the Silicon Contract for Vision, Language, and Recommendations. Each architecture family represents a different bet on which hardware resource to saturate.

3. **ML Frameworks (@sec-ai-frameworks)**: Provisioning the **Physics**. The software that automates the Iron Law, including automatic differentiation, graph optimization, and the abstractions that let engineers focus on architecture rather than gradients.

4. **Model Training (@sec-ai-training)**: Executing the **Physics** at scale. The physical execution of the build process, including distributed training, mixed precision, and the infrastructure that turns algorithms into trained models.

By the end of Part II, you will be able to design, implement, and train ML systems that respect the Silicon Contract and the Iron Law. However, a working model is rarely an efficient one. Part III introduces the optimization principles and techniques that transform a functional system into one that meets real-world constraints on latency, memory, and energy.
