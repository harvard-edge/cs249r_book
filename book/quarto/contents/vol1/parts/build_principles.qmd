# Part II: Build { .unnumbered}

Building a machine learning system is not merely about stacking layers; it is about managing the flow of information and energy through silicon. Part I established that data is both the program and the physical anchor of every ML system. Part II turns to the compute side of the equation: *how* we design around those limits. The principles here are the physics of construction — the laws that govern why certain architectures succeed while others fail at scale.

::: {.callout-principle title="The Iron Law of ML Systems" icon="false"}
**The Invariant**: The total time ($T$) of any machine learning operation is governed by three components — data movement, compute, and fixed system overhead:
$$ T = \frac{D_{vol}}{BW} + \frac{O}{R_{peak} \cdot \eta} + L_{lat} $$
where $D_{vol}$ is data volume (bytes moved), $BW$ is memory bandwidth, $O$ is total floating-point operations, $R_{peak}$ is peak compute rate, $\eta$ is hardware utilization efficiency, and $L_{lat}$ is fixed latency overhead such as kernel launch or network round-trip time. (For the full notation rationale, see the Notation and Conventions section.)

When these stages overlap on modern hardware, wall-clock time is dominated by whichever term is largest. This is why the equation's practical lesson is about **dominance**, not summation: the term that takes longest sets the floor.

**The Implication**: Optimization is rarely free of trade-offs. Reducing one term often shifts the bottleneck to another. For example, unstructured pruning reduces compute ($O$) but introduces irregular memory access patterns that can increase data movement ($D_{vol}/BW$). A "faster" algorithm on paper is only faster in reality if it reduces the **dominant** term for your specific hardware.
:::

The Iron Law tells us *what* to optimize, but not *how*. The answer depends on which hardware resource your architecture will saturate — a choice that defines an implicit contract:

::: {.callout-principle title="The Silicon Contract" icon="false"}
**The Invariant**: Every model architecture makes an implicit commitment to the hardware — a wager on which resource it will saturate first.

- **ResNet-50** assumes high-density floating-point compute. It is **Compute-Bound**: performance is limited by $O / (R_{peak} \cdot \eta)$.
- **Llama-3-8B** assumes high-bandwidth memory access. It is **Bandwidth-Bound**: performance is limited by $D_{vol} / BW$.
- **DLRM** assumes massive memory capacity for embedding lookup tables. It is **Capacity-Bound**: performance is limited by whether the working set fits in fast memory at all.

**The Implication**: Designing a model without knowing which hardware resource it will saturate is like designing a bridge without knowing the strength of the steel. You must design for the **Bottleneck**.
:::

Together, the Iron Law and the Silicon Contract frame every design decision in Part II. The chapters that follow translate these principles into the components of the ML stack: the mathematical foundations of gradient flow, the architectural patterns that commit to specific hardware resources, the frameworks that automate the Iron Law, and the training systems that execute the physics at scale.
