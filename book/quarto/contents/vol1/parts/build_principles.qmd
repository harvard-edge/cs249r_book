# Part II: Build {.unnumbered}

Part II moves from understanding constraints to constructing systems that work within them. Building a machine learning system is not merely about stacking layers; it is about managing the flow of information and energy through silicon.

If Part I established *what* limits us, Part II addresses *how* we design around those limits. The principles here define the "Physics of Build"—the laws that govern why certain architectures succeed while others fail at scale.

## The Invariants {#sec-build-invariants}

Four principles govern the construction of ML systems. We begin with the fundamental equation that determines all ML performance:

::: {#nte-iron-law .callout-note title="The Iron Law of ML Systems" icon="false"}
**The Invariant**: The latency ($L$) of any machine learning operation is determined by the sum of three components, divided by the system's efficiency ($\eta$):
$$ L = \frac{\text{Data Movement} + \text{Compute} + \text{System Overhead}}{\eta} $$

**The Implication**: Optimization is a zero-sum game. If you reduce Compute (e.g., via pruning), you often increase Data Movement (random memory access). A "faster" algorithm on paper is only faster in reality if it reduces the **dominant** term in this equation for your specific hardware.
:::

The Iron Law tells us *what* to optimize, but not *how*. The answer depends on which hardware resource your architecture will saturate—a choice that defines an implicit contract:

::: {#nte-silicon-contract .callout-note title="The Silicon Contract" icon="false"}
**The Invariant**: Every model architecture makes an implicit agreement with the hardware.

- **ResNet-50** assumes high-density floating-point compute (Compute-Bound).
- **Llama-3-8B** assumes high-bandwidth memory access (Bandwidth-Bound).
- **DLRM** assumes massive memory capacity for lookup tables (Capacity-Bound).

**The Implication**: Designing a model without knowing which hardware resource it will saturate is like designing a bridge without knowing the strength of the steel. You must design for the **Bottleneck**.
:::

Once you've committed to a hardware target, efficiency emerges from how you distribute computation across layers. Not all layers are created equal:

::: {#nte-depth-efficiency .callout-note title="The Depth-Efficiency Law" icon="false"}
**The Invariant**: In hierarchical feature extractors, computational efficiency is maximized when early layers process high-resolution, low-semantic data (edges, textures) and later layers process low-resolution, high-semantic data (objects, concepts).

**The Implication**: Architectures should be tapered (e.g., Pyramidal CNNs). Allocating equal compute to every layer is inefficient; capacity should be concentrated where semantic density is highest.
:::

Depth enables expressiveness, but it introduces its own constraint. As networks grow deeper, a mathematical invariant determines whether learning is even possible:

::: {#nte-residual-gradient .callout-note title="The Residual Gradient Invariant" icon="false"}
**The Invariant**: Deep network trainability is preserved only if there exists a direct path for gradient flow that bypasses non-linear transformations.
$$ \frac{\partial L}{\partial x} \approx 1 $$

**The Implication**: As networks grow deeper, **Skip Connections** (ResNets, Transformers) become mandatory. Without them, the "Shattered Gradients" problem prevents error signals from propagating to early layers, halting learning.
:::

## Part II Roadmap {#sec-build-roadmap}

The chapters translate these principles into the components of the ML stack. Each chapter assembles one leg of the **DAM Taxonomy**: we define the **Logic** (Algorithm), provision the **Physics** (Machine), and refine the **Information** (Data).

1. **Deep Learning Foundations (@sec-deep-learning-systems-foundations)**: The mathematical foundations of gradient flow—backpropagation, loss landscapes, and the calculus that makes learning possible.

2. **DNN Architectures (@sec-dnn-architectures)**: Defining the **Logic**. How we structure the Silicon Contract for Vision, Language, and Recommendations. Each architecture family represents a different bet on which hardware resource to saturate.

3. **AI Frameworks (@sec-ai-frameworks)**: Provisioning the **Physics**. The software that automates the Iron Law—automatic differentiation, graph optimization, and the abstractions that let engineers focus on architecture rather than gradients.

4. **AI Training (@sec-ai-training)**: Executing the **Physics** at scale. The physical execution of the build process—distributed training, mixed precision, and the infrastructure that turns algorithms into trained models.

By the end of Part II, you will be able to design, implement, and train ML systems that respect the Silicon Contract and the Iron Law. However, a working model is rarely an efficient one. Part III introduces the optimization principles and techniques that transform a functional system into one that meets real-world constraints on latency, memory, and energy.
