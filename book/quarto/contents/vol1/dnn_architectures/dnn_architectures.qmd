---
bibliography: dnn_architectures.bib
quiz: dnn_architectures_quizzes.json
concepts: dnn_architectures_concepts.yml
glossary: dnn_architectures_glossary.json
crossrefs: dnn_architectures_xrefs.json
---

# DNN Architectures {#sec-dnn-architectures}

::: {layout-narrow}
::: {.column-margin}
_DALL·E 3 Prompt: A visually striking rectangular image illustrating the interplay between deep learning algorithms like CNNs, RNNs, and Attention Networks, interconnected with machine learning systems. The composition features neural network diagrams blending seamlessly with representations of computational systems such as processors, graphs, and data streams. Bright neon tones contrast against a dark futuristic background, symbolizing cutting-edge technology and intricate system complexity._
:::

\noindent
![](images/png/cover_dl_arch.png)

:::

## Purpose {.unnumbered}

_Why do architectural choices in neural networks affect system design decisions that determine computational feasibility, hardware requirements, and deployment constraints?_

Neural network architectures represent engineering decisions that directly determine system performance and deployment viability. Each architectural choice creates cascading effects throughout the system stack: memory bandwidth demands, computational complexity patterns, parallelization opportunities, and hardware acceleration compatibility. Understanding these architectural implications enables engineers to make informed trade-offs between model capability and system constraints, predict computational bottlenecks before they occur, and select appropriate hardware platforms. Architectural decisions determine whether machine learning systems meet performance requirements within available computational resources. This understanding proves essential for building scalable AI systems that can be deployed effectively across diverse environments.

::: {.callout-tip title="Learning Objectives"}

- Distinguish the computational characteristics and inductive biases of the four main neural network architectural families (MLPs, CNNs, RNNs, Transformers)

- Analyze how architectural design choices determine computational complexity, memory requirements, and parallelization opportunities

- Evaluate the system-level implications of architectural patterns on hardware utilization, memory bandwidth, and deployment constraints

- Apply the architecture selection framework to match data characteristics with appropriate neural network designs for specific applications

- Assess computational and memory trade-offs between different architectural approaches using complexity analysis

- Examine how fundamental computational primitives (matrix multiplication, convolution, attention) map to hardware acceleration opportunities

- Critique common architectural selection fallacies and their impact on system performance and deployment success

- Synthesize the unified inductive bias framework explaining architecture-data compatibility patterns

:::

## Architectural Principles and Engineering Trade-offs {#sec-dnn-architectures-architectural-principles-engineering-tradeoffs-89de}

The systematic organization of neural computations into effective architectures represents one of the most consequential developments in contemporary machine learning systems. Building on the mathematical foundations of neural computation established in @sec-dl-primer, this chapter investigates the architectural principles that govern how operations (matrix multiplications, nonlinear activations, and gradient-based optimization) are structured to address complex computational problems. This architectural perspective bridges the gap between mathematical theory and practical systems implementation, examining how design choices at the network level determine system-wide performance characteristics.

This chapter centers on an engineering trade-off that permeates machine learning systems design. While mathematical theory, particularly universal approximation results, establishes that neural networks possess remarkable representational flexibility, practical deployment necessitates computational efficiency achievable only through judicious architectural specialization. This tension manifests across multiple dimensions: theoretical universality versus computational tractability, representational completeness versus memory efficiency, and mathematical generality versus domain-specific optimization. The resolution of these tensions through architectural innovation constitutes a primary driver of progress in machine learning systems.

Contemporary neural architectures emerge from systematic responses to specific computational challenges encountered when deploying general mathematical frameworks on structured data. Each architectural paradigm embodies distinct inductive biases (implicit assumptions about data structure and relationships) that enable efficient learning while constraining the hypothesis space in domain-appropriate ways. These architectural innovations represent engineering solutions to the challenge of organizing computational primitives into patterns that achieve optimal balance between representational capacity and computational efficiency.

This chapter examines four architectural families that collectively define the conceptual landscape of modern neural computation. Multi-Layer Perceptrons serve as the canonical implementation of universal approximation theory, demonstrating how dense connectivity enables general pattern recognition while illustrating the computational costs of architectural generality. Convolutional Neural Networks introduce the paradigm of spatial architectural specialization, exploiting translational invariance and local connectivity to achieve significant efficiency gains while preserving representational power for spatial data. Recurrent Neural Networks extend architectural specialization to temporal domains, incorporating explicit memory mechanisms that enable sequential processing capabilities absent from feedforward architectures. Attention mechanisms and Transformer architectures represent the current evolutionary frontier, replacing fixed structural assumptions with dynamic, content-dependent computation that achieves remarkable capability while maintaining computational efficiency through parallelizable operations.

The systems engineering significance of these architectural patterns extends beyond mere algorithmic considerations. Each architectural choice creates distinct computational signatures that propagate through every level of the implementation stack, determining memory access patterns, parallelization strategies, hardware utilization characteristics, and ultimately system feasibility within resource constraints. Understanding these architectural implications proves essential for engineers responsible for system design, resource allocation, and performance optimization in production environments.

This chapter adopts a systems-oriented analytical framework that illuminates the relationships between architectural abstractions and concrete implementation requirements. For each architectural family, we systematically examine the computational primitives that determine hardware resource demands, the organizational principles that enable efficient algorithmic implementation, the memory hierarchy implications that affect system scalability, and the trade-offs between architectural sophistication and computational overhead.

The analytical approach builds systematically upon the neural network foundations established in @sec-dl-primer, extending core concepts of forward propagation, backpropagation, and gradient-based optimization by examining how architectural specialization organizes these operations to exploit problem-specific structure. Understanding the evolutionary relationships connecting these architectural paradigms and their distinct computational characteristics, practitioners develop the conceptual tools necessary for principled decision-making regarding architectural selection, resource planning, and system optimization in complex deployment scenarios.

## Multi-Layer Perceptrons: Dense Pattern Processing {#sec-dnn-architectures-multilayer-perceptrons-dense-pattern-processing-259f}

Multi-Layer Perceptrons (MLPs) represent the fully-connected architectures introduced in @sec-dl-primer, now examined through the lens of architectural choice and systems trade-offs. MLPs embody an inductive bias: **they assume no prior structure in the data, allowing any input to relate to any output**. This architectural choice enables maximum flexibility by treating all input relationships as equally plausible, making MLPs versatile but computationally intensive compared to specialized alternatives. Their computational power was established theoretically by the Universal Approximation Theorem (UAT)[^fn-uat] [@cybenko1989approximation; @hornik1989multilayer], which we encountered as a footnote in @sec-dl-primer. This theorem states that a sufficiently large MLP with non-linear activation functions can approximate any continuous function on a compact domain, given suitable weights and biases.

[^fn-uat]: **Universal Approximation Theorem**: Proven independently by Cybenko (1989) and Hornik (1989), this result showed that neural networks could theoretically learn any function, a discovery that reinvigorated interest in neural networks after the "AI Winter" of the 1980s and established mathematical foundations for modern deep learning.

::: {.callout-definition title="Multi-Layer Perceptrons"}

**Multi-Layer Perceptrons (MLPs)** refer to _fully-connected neural networks_ where every neuron connects to all neurons in adjacent layers, providing _maximum flexibility_ through _universal approximation_ at the cost of _high parameter counts_ and _computational intensity_.

:::

In practice, the UAT explains why MLPs succeed across diverse tasks while revealing the gap between theoretical capability and practical implementation. The theorem guarantees that *some* MLP can approximate any function, yet provides no guidance on requisite network size or weight determination. This gap becomes critical in real-world applications: while MLPs can theoretically solve any pattern recognition problem, achieving this capability may require impractically large networks or extensive computation. This theoretical power drives the selection of MLPs for tabular data, recommendation systems, and problems where input relationships are unknown, while these practical limitations motivated the development of specialized architectures that exploit data structure for computational efficiency, as detailed in @sec-dnn-architectures-architectural-principles-engineering-tradeoffs-89de.

#### The Learnability Gap: Why Universal Approximation Is Not Enough {#sec-dnn-architectures-learnability-gap-c8e9}

The Universal Approximation Theorem establishes that a single hidden layer MLP with sufficient width can approximate any continuous function to arbitrary accuracy. This sounds definitive: if MLPs are universal, why do we need specialized architectures like CNNs, RNNs, or Transformers? The answer lies in a critical distinction between what a network *can represent* and what it *can learn*.

**Representation Capacity** refers to the set of functions an architecture can express given unlimited width and perfect weight settings. The UAT guarantees that MLPs have universal representation capacity: with enough neurons, optimal weights exist that can approximate any continuous function. This is a statement about theoretical expressiveness, not about practical achievability.

**Learnability** refers to whether gradient descent can find good weights given finite training samples and practical computational resources. A function may be representable yet practically unlearnable if it requires exponentially many samples, prohibitively large networks, or optimization landscapes where gradient descent fails.

This distinction explains a fundamental paradox in neural architecture design: if MLPs are universal approximators, why has architectural innovation (ResNets, Transformers) driven most progress in deep learning? The answer is that specialized architectures improve *learnability* by embedding inductive biases that match data structure, even though they may restrict *representational capacity*.

**Sample Complexity Gap**: The UAT provides no bounds on how many training examples are needed to learn a target function. For functions with structure (images with spatial locality, sequences with temporal dependencies), MLPs exhibit poor sample efficiency. Consider learning to classify 28×28 images: an MLP treats each of the 784 pixels independently, requiring enough training examples to learn all possible pixel correlations from data. A CNN, by embedding locality bias, learns that nearby pixels are related, drastically reducing the sample complexity for spatially structured data.

Mathematically, sample complexity often scales with the VC dimension or Rademacher complexity of the hypothesis class. For an MLP approximating a function over a $d$-dimensional input space, sample complexity can grow exponentially with $d$ (the curse of dimensionality). Specialized architectures constrain the hypothesis space through structural assumptions, reducing effective dimensionality and improving sample efficiency when those assumptions match the data.

**Computational Cost of Representation**: The UAT guarantees that *some* width suffices for approximation, but provides no constructive bounds. For functions with $d$-dimensional inputs, the required width can be exponential in $d$. A concrete example: approximating simple functions like $\sin(x_1) + \sin(x_2) + \cdots + \sin(x_d)$ with an MLP may require width exponential in $d$, whereas an architecture that processes each dimension independently needs only $O(d)$ parameters.

For image classification on 28×28 MNIST digits, a fully connected MLP might require hidden layers with thousands of neurons to achieve 98% accuracy. In contrast, a CNN with convolutional layers exploiting spatial structure achieves 99%+ accuracy with far fewer parameters. The UAT tells us the MLP *can* represent the classification function, but practice reveals it requires impractically large width relative to specialized architectures.

**Optimization Difficulty**: Even when optimal weights exist and sample complexity is manageable, gradient descent may fail to find them. High-dimensional loss surfaces for MLPs exhibit complex topology: numerous local minima, saddle points, and plateaus. Without structural constraints, the optimization landscape lacks the regularizing effect of architectural inductive biases.

Specialized architectures constrain the hypothesis space, effectively reducing the search space dimensionality. This constraint improves optimization in two ways. First, fewer parameters mean fewer degrees of freedom, reducing the complexity of the loss landscape. Second, architectural structure (e.g., convolution's weight sharing) introduces symmetries that gradient descent can exploit. The result: specialized architectures often train faster and reach better solutions than MLPs of equivalent representational capacity.

::: {.callout-example title="MNIST: Representation vs Learnability in Practice"}

Consider classifying 28×28 MNIST digits (784 input pixels, 10 output classes).

**MLP Approach**:

- Architecture: 784 → 4096 → 4096 → 10
- Parameters: $(784 \times 4096) + (4096 \times 4096) + (4096 \times 10) \approx 20\text{M parameters}$
- Training: 60,000 examples (standard MNIST training set)
- Test Accuracy: ~97-98%
- Rationale: Treats every pixel independently. Must learn all spatial correlations from data alone. No prior knowledge about spatial structure.

**CNN Approach**:

- Architecture: Conv(32, 3×3) → Pool → Conv(64, 3×3) → Pool → FC(128) → 10
- Parameters: $(3 \times 3 \times 32) + (3 \times 3 \times 32 \times 64) + (64 \times 7 \times 7 \times 128) + (128 \times 10) \approx 100\text{K parameters}$
- Training: 60,000 examples (same data)
- Test Accuracy: ~99%+
- Rationale: Embeds locality bias (nearby pixels are related) and translation invariance (digit patterns are meaningful regardless of position). These structural assumptions reduce parameter count and improve generalization.

**Comparison**:

- Parameter Efficiency: CNN uses 200× fewer parameters
- Sample Efficiency: CNN achieves better accuracy with the same training data
- Systems Implications: CNN requires 20× less memory, trains faster, and runs faster at inference

Both architectures can *represent* the digit classification function (UAT guarantees this for MLPs; CNNs have similar or greater representational capacity). The difference is *learnability*: the CNN's inductive bias matches the spatial structure of images, enabling efficient learning with limited data and compute.
:::

**Why Specialized Architectures Exist**: The learnability gap motivates the core design principle of modern neural architectures: embed inductive biases that match data structure. CNNs embed locality and translation invariance for spatial data. RNNs embed sequential dependencies for temporal data. Transformers embed permutation invariance with learned attention for relational data. Each architecture sacrifices some theoretical generality (constraining representational capacity) to gain practical learnability (better sample efficiency, faster optimization, smaller networks).

This trade-off explains why architectural innovation continues despite universal approximation. New architectures succeed not because existing ones lack representational power, but because they improve learnability for specific data structures. The Transformer revolution in NLP occurred not because RNNs could not represent language (they could), but because Transformers learn more efficiently at scale through parallelizable attention mechanisms.

**No Free Lunch**: The inductive bias that improves learnability for one data distribution may hurt performance on others. CNN's translation invariance helps image classification but hurts tasks where absolute position matters (e.g., detecting whether an object is in the top-left vs. bottom-right corner). Architecture selection is fundamentally about matching inductive bias to data structure: when the bias aligns with true data properties, learning becomes efficient; when it conflicts, performance degrades.

**Systems Engineering Implications**: Understanding the representation vs. learnability distinction informs practical systems decisions:

- **Memory Budgets**: Inductive biases reduce parameter counts, enabling deployment on resource-constrained devices. A 20M-parameter MLP may be infeasible for edge deployment, while a 100K-parameter CNN fits comfortably.

- **Training Costs**: Sample and computational efficiency determine training feasibility. Architectures with appropriate biases converge faster, reducing GPU-hours and energy consumption.

- **Generalization**: Learnability improvements translate to better generalization, producing more robust production systems that perform reliably on unseen data.

- **Hardware Alignment**: Structured computation patterns (convolution, attention) map efficiently to specialized hardware (GPUs, TPUs) in ways that fully connected layers do not, as detailed in @sec-ai-acceleration.

The remainder of this chapter explores how different architectural families embed specific inductive biases through their computational structure, and how these choices determine both learning efficiency and systems requirements. Understanding that architectural choice is fundamentally about learnability, not representational capacity, provides the conceptual foundation for principled architecture selection in ML systems engineering.

When applied to the MNIST handwritten digit recognition challenge[^fn-mnist-dataset], an MLP demonstrates its computational approach by transforming a $28\times 28$ pixel image into digit classification.

[^fn-mnist-dataset]: **MNIST Dataset**: Created by Yann LeCun, Corinna Cortes, and Chris Burges in 1998 from NIST's database of handwritten digits, MNIST's 60,000 training images became the "fruit fly" of machine learning research. Despite human-level accuracy of 99.77% being achieved by various models, MNIST remains valuable for education because its simplicity allows students to focus on architectural concepts without data complexity distractions.

### Pattern Processing Needs {#sec-dnn-architectures-pattern-processing-needs-c45a}

Deep learning models frequently encounter problems where any input feature may influence any output, absent inherent constraints on these relationships. Financial market analysis exemplifies this challenge: any economic indicator may affect any market outcome. Similarly, in natural language processing, the meaning of a word may depend on any other word in the sentence. These scenarios demand an architectural pattern capable of learning arbitrary relationships across all input features.

Dense pattern processing addresses these challenges through several key capabilities. First, it enables unrestricted feature interactions where each output can depend on any combination of inputs. Second, it supports learned feature importance, enabling the system to determine which connections matter rather than relying on prescribed relationships. Finally, it provides adaptive representation, enabling the network to reshape its internal representations based on the data.

The MNIST digit recognition task illustrates this uncertainty: while humans might focus on specific parts of digits (loops in '6' or crossings in '8'), the pixel combinations critical for classification remain indeterminate. A '7' written with a serif may share pixel patterns with a '2', while variations in handwriting mean discriminative features may appear anywhere in the image. This uncertainty about feature relationships necessitates a dense processing approach where every pixel can potentially influence the classification decision.

This requirement for unrestricted connectivity leads directly to the mathematical foundation of MLPs.

### Algorithmic Structure {#sec-dnn-architectures-algorithmic-structure-c012}

MLPs enable unrestricted feature interactions through a direct algorithmic solution: complete connectivity between all nodes. This connectivity requirement manifests through a series of fully-connected layers, where each neuron connects to every neuron in adjacent layers, the "dense" connectivity pattern introduced in @sec-dl-primer.

This architectural principle translates the dense connectivity pattern into matrix multiplication operations[^fn-gemm], establishing the mathematical foundation that makes MLPs computationally tractable. As illustrated in @fig-mlp, each layer transforms its input through the fundamental operation introduced in @sec-dl-primer:

[^fn-gemm]: **GEMM (General Matrix Multiply)**: A fundamental operation underlying many neural network layers. GEMM performs \(C = \alpha AB + \beta C\) and has been optimized for decades. In many dense models, a large fraction of runtime is spent in GEMM-like kernels, and well-tuned libraries can approach peak throughput on suitable workloads. This is why matrix-kernel efficiency often dominates end-to-end performance in practice.

$$
\mathbf{h}^{(l)} = f\big(\mathbf{h}^{(l-1)}\mathbf{W}^{(l)} + \mathbf{b}^{(l)}\big)
$$

Recall that $\mathbf{h}^{(l)}$ represents the layer $l$ output (activation vector), $\mathbf{h}^{(l-1)}$ represents the input from the previous layer, $\mathbf{W}^{(l)}$ denotes the weight matrix for layer $l$, $\mathbf{b}^{(l)}$ denotes the bias vector, and $f(\cdot)$ denotes the activation function (such as ReLU, as detailed in @sec-dl-primer). This layer-wise transformation, while conceptually simple, creates computational patterns whose efficiency depends critically on how we organize these operations for different problem structures.

::: {#fig-mlp fig-env="figure" fig-pos="htb"}
```{.tikz}
 \begin{tikzpicture}[line join=round,font=\usefont{T1}{phv}{m}{n}]
\tikzset{%
Line/.style={line width=0.35pt,black!60}
}

\tikzset{
  box/.pic={
    \pgfkeys{/box/.cd, #1}
\foreach \x in {1,...,\columns}{
    \foreach \y in {1,...,\rows}{
        %
        \node[draw=black, fill=\ffill, minimum width=\cellsize,
                    minimum height=\cellheight, line width=\linewidth] (cell-\x-\y\br) at (\x*\cellsize,-\y*\cellheight) {};
    }
}
 } }

\pgfkeys{
  /box/.cd,
  cellsize/.store in=\cellsize,
  linewidth/.store in=\linewidth,
  cellheight/.store in=\cellheight,
  columns/.store in=\columns,
  br/.store in=\br,
  ffill/.store in=\ffill,
  rows/.store in=\rows,
  columns=1,
  rows=3,
  br=A,
  ffill=GreenL!22,
  cellsize=8mm,
  cellheight=8mm,
  linewidth=0.75pt
}
\def\radius{4mm}
\pic at (0,0) {box={columns=1,rows=4,br=A}};
\pic at (2,0) {box={columns=5,rows=4,br=B,}};
\pic at (7,4mm) {box={columns=1,rows=5,br=C}};
\pic at (9,4mm) {box={columns=2,rows=5,br=D}};
\pic at (12,-8mm) {box={columns=1,rows=2,br=E}};
%
\foreach \x in {1,...,4}{
\node[fill=green!80!black!80,minimum size=\cellsize,draw, line width=0.75pt]at(cell-2-\x B){};
}
\node[fill=green!80!black!80,minimum size=\cellsize,draw, line width=0.75pt]at(cell-1-2C){};

\begin{scope}[scale=1, every node/.append style={transform shape},
local bounding box=D1,shift={($(cell-1-3A)+(-4.5,-2.6)$)}]
\foreach \x in {1,...,5}{
\coordinate (2ball-\x) at (0,\x);
\shade[shading=ball,ball color=red!50!yellow] (0,\x) circle (\radius);
}
\shade[shading=ball,ball color=green!50!green] (0,4) circle (\radius);

\foreach \x/\i in {2.5/1,3.5/2}{
\coordinate (3ball-\i) at (2,\x);
\shade[shading=ball,ball color=red!50!yellow] (2,\x) circle (\radius)coordinate(3C\i);
}

\foreach \x/\i in  {1.5/1,2.5/2,3.5/3,4.5/4}{
\coordinate (1ball-\i) at (-2,\x);
\shade[shading=ball,ball color=red!50!yellow] (-2,\x) circle (\radius)coordinate(1C\i);
}
% Connect 1. and 2. column
\foreach \x in {1,2,3,4}{
  \foreach \y in {1,2,3,5}{
    \edef\from{1ball-\x}
    \edef\to{2ball-\y}
\path let
    \p1 = (\from),
    \p2 = (\to),
    \n1 = {atan2(\y2-\y1,\x2-\x1)}
  in
    coordinate (from) at ($ (\from) + (\n1:\radius) $)
    coordinate (to) at ($ (\to) + (\n1+180:\radius) $);
  \draw[Line] (from) -- (to);
  }
}
%red line
\foreach \x in {1,2,3,4}{
  \foreach \y in {4}{
    \edef\from{1ball-\x}
    \edef\to{2ball-\y}
\path let
    \p1 = (\from),
    \p2 = (\to),
    \n1 = {atan2(\y2-\y1,\x2-\x1)}
  in
    coordinate (from) at ($ (\from) + (\n1:\radius) $)
    coordinate (to) at ($ (\to) + (\n1+180:\radius) $);
  \draw[Line,red] (from) -- (to);
  }
}
% Connect 2. and 3. column
\foreach \x in {1,2,3,4,5}{
  \foreach \y in {1,2}{
    \edef\from{2ball-\x}
    \edef\to{3ball-\y}
\path let
    \p1 = (\from),
    \p2 = (\to),
    \n1 = {atan2(\y2-\y1,\x2-\x1)}
  in
    coordinate (from) at ($ (\from) + (\n1:\radius) $)
    coordinate (to) at ($ (\to) + (\n1+180:\radius) $);
  \draw[Line] (from) -- node[inner sep=0pt](L\x){}(to);
  }
}
%%
\draw[latex-](L5)--++(30:1)node[above]{Weighted Edge};
\draw[latex-](2ball-4)--++(180:2.5)node[left](NE){Neuron};
\draw[thick,latex-](cell-2-2B.center)--++(90:1.52)node[above]{Weighted Edge};
\draw[thick,latex-](cell-1-2C.center)--++(90:1.52)node[above]{Neuron};
%
\node[font=\huge]at($(cell-1-2A.south east)!0.5!(cell-1-2B.south west)$){$\times$};
\node[font=\huge]at($(cell-1-3C.east)!0.5!(cell-1-3D.west)$){$\times$};
%
\node[single arrow, draw=red, fill=red,
      minimum width = 10pt, single arrow head extend=3pt,
      minimum height=7mm]at($(cell-5-2B.south east)!0.5!(cell-1-3C.west)$){};
\node[single arrow, draw=red, fill=red,
      minimum width = 10pt, single arrow head extend=3pt,
      minimum height=7mm]at($(cell-2-3D.east)!0.5!(cell-1-1E.south west)$){};
\end{scope}

\path(NE.west)--++(270:4.0)coordinate(IL1)-|coordinate(HL1)($(1ball-1)!0.4!(2ball-1)$);
\path(NE)--++(270:4.0)-|coordinate(OL1)($(2ball-1)!0.55!(3ball-1)$);
\path(NE)--++(270:4.0)-|coordinate(OL2)($(3ball-1)!0.5!(cell-1-2A.south east)$);
\path(NE)--++(270:4.0)-|coordinate(IL2)($(3ball-1)!0.6!(cell-1-2A.south east)$);

\path[blue, line width=2pt](IL2)-|coordinate(HL2)($(cell-1-2A.south east)!0.7!(cell-1-2B.south west)$);
\path[blue, line width=2pt](IL2)-|coordinate(OL3)($(cell-1-3C.east)!0.5!(cell-1-3D.west)$);
\path[blue, line width=2pt](IL2)-|coordinate(OL4)(cell-1-2E.south east);

\draw[red,line width=2pt](IL1)--node[below,text=black]{Input Layer}(HL1);
\draw[cyan,line width=2pt](HL1)--node[below,text=black]{Hidden Layer}(OL1);
\draw[brown,line width=2pt](OL1)--node[below,text=black]{Output Layer}(OL2);
%
\draw[red,line width=2pt](IL2)--node[below,text=black]{Input Layer}(HL2);
\draw[cyan,line width=2pt](HL2)--node[below,text=black]{Hidden Layer}(OL3);
\draw[brown,line width=2pt](OL3)--node[below,text=black]{Output Layer}(OL4);
\end{tikzpicture}
```
**Layered Transformations**: Multi-Layer Perceptrons (MLPs) implement dense connectivity through sequential matrix multiplications and non-linear activations, supporting complex feature interactions and hierarchical representations of input data. Each layer transforms the input vector from the previous layer, producing a new vector that serves as input to the subsequent layer, as defined by the equation in the text. Source: [@reagen2017deep].
:::

The dimensions of these operations reveal the computational scale of dense pattern processing:

* Input vector: $\mathbf{h}^{(0)} \in \mathbb{R}^{d_{\text{in}}}$ (treated as a row vector in this formulation) represents all potential input features
* Weight matrices: $\mathbf{W}^{(l)} \in \mathbb{R}^{d_{\text{in}} \times d_{\text{out}}}$ capture all possible input-output relationships
* Output vector: $\mathbf{h}^{(l)} \in \mathbb{R}^{d_{\text{out}}}$ produces transformed representations

::: {.callout-example title="Concrete Computation Example"}
Consider a simplified 4-pixel image processed by a 3-neuron hidden layer:

**Input**: $\mathbf{h}^{(0)} = [0.8, 0.2, 0.9, 0.1]$ (4 pixel intensities)

**Weight matrix**: $\mathbf{W}^{(1)} = \begin{bmatrix} 0.5 & 0.1 & -0.2 \\ -0.3 & 0.8 & 0.4 \\ 0.2 & -0.4 & 0.6 \\ 0.7 & 0.3 & -0.1 \end{bmatrix}$ (4×3 matrix)

**Computation**:
\begin{gather*}
\mathbf{z}^{(1)} = \mathbf{h}^{(0)}\mathbf{W}^{(1)} = \begin{bmatrix} 0.5×0.8 + (-0.3)×0.2 + 0.2×0.9 + 0.7×0.1 \\ 0.1×0.8 + 0.8×0.2 + (-0.4)×0.9 + 0.3×0.1 \\ (-0.2)×0.8 + 0.4×0.2 + 0.6×0.9 + (-0.1)×0.1 \end{bmatrix}
\\
= \begin{bmatrix} 0.65 \\ -0.17 \\ 0.47 \end{bmatrix}
\end{gather*}
**After ReLU**: $\mathbf{h}^{(1)} = [0.65, 0, 0.47]$ (negative values zeroed)

Each hidden neuron combines ALL input pixels with different weights, demonstrating unrestricted feature interaction.
:::

The MNIST example demonstrates the practical scale of these operations:

* Each 784-dimensional input ($28\times 28$ pixels) connects to every neuron in the first hidden layer
* A hidden layer with 100 neurons requires a $784\times 100$ weight matrix
* Each weight in this matrix represents a learnable relationship between an input pixel and a hidden feature

This algorithmic structure addresses the need for arbitrary feature relationships while creating specific computational patterns that computer systems must accommodate.

#### Architectural Characteristics {#sec-dnn-architectures-architectural-characteristics-47b4}

This dense connectivity approach creates both advantages and trade-offs. Dense connectivity provides the universal approximation capability established earlier but introduces computational redundancy. While this theoretical power enables MLPs to model any continuous function given sufficient width, this flexibility necessitates numerous parameters to learn relatively simple patterns. The dense connections ensure that every input feature influences every output, yielding maximum expressiveness at the cost of maximum computational expense.

These trade-offs motivate sophisticated optimization techniques that reduce computational demands while preserving model capability. Structured pruning can eliminate 80-90% of connections with minimal accuracy loss, while quantization reduces precision requirements from 32-bit to 8-bit or lower. While @sec-model-optimizations details these compression strategies, the architectural foundations established here determine which optimization approaches prove most effective for dense connectivity patterns, with @sec-ai-acceleration exploring hardware-specific implementations that exploit regular matrix operation structure.

### Computational Mapping {#sec-dnn-architectures-computational-mapping-fe7e}

The mathematical representation of dense matrix multiplication maps to specific computational patterns that systems must handle. This mapping progresses from mathematical abstraction to computational reality, as demonstrated in the first implementation shown in @lst-mlp_layer_matrix.

The function mlp_layer_matrix directly mirrors the mathematical equation, employing high-level matrix operations (`matmul`) to express the computation in a single line while abstracting the underlying complexity. This implementation style characterizes deep learning frameworks, where optimized libraries manage the actual computation.

::: {#lst-mlp_layer_matrix}
```{.python}
def mlp_layer_matrix(X, W, b):
    # X: input matrix (batch_size × num_inputs)
    # W: weight matrix (num_inputs × num_outputs)
    # b: bias vector (num_outputs)
    H = activation(matmul(X, W) + b)
    # One clean line of math
    return H
```
This implementation shows neural networks performing weighted sum and activation functions across layers using matrix operations. The code emphasizes the core computational pattern in multi-layer perceptrons.
:::

To understand the system implications of this architecture, we must look "under the hood" of the high-level framework call. The elegant one-line matrix multiplication `output = matmul(X, W)` is, from the hardware's perspective, a series of nested loops that expose the true computational demands on the system. This translation from logical model to physical execution reveals critical patterns that determine memory access, parallelization strategies, and hardware utilization.

The second implementation, `mlp_layer_compute` (shown in @lst-mlp_layer_compute), exposes the actual computational pattern through nested loops. This version reveals what really happens when we compute a layer's output: we process each sample in the batch, computing each output neuron by accumulating weighted contributions from all inputs.

::: {#lst-mlp_layer_compute}
```{.python}
def mlp_layer_compute(X, W, b):
    # Process each sample in the batch
    for batch in range(batch_size):
        # Compute each output neuron
        for out in range(num_outputs):
            # Initialize with bias
            Z[batch, out] = b[out]
            # Accumulate weighted inputs
            for in_ in range(num_inputs):
                Z[batch, out] += X[batch, in_] * W[in_, out]

    H = activation(Z)
    return H
```
This implementation computes each output neuron by accumulating weighted contributions from all inputs across the batch. The detailed step-by-step process exposes how a single layer in a neural network processes data, emphasizing the role of biases and weighted sums in producing outputs.
:::

This translation from mathematical abstraction to concrete computation exposes how dense matrix multiplication decomposes into nested loops of simpler operations. The outer loop processes each sample in the batch, while the middle loop computes values for each output neuron. Within the innermost loop, the system performs repeated multiply-accumulate operations[^fn-mac], combining each input with its corresponding weight.

In the MNIST example, each output neuron requires 784 multiply-accumulate operations and at least 1,568 memory accesses (784 for inputs, 784 for weights). While actual implementations use optimizations through libraries like BLAS[^fn-dnn-blas] or cuBLAS, these patterns drive key system design decisions. The hardware architectures that accelerate these matrix operations, including GPU tensor cores[^fn-tensor-cores] and specialized AI accelerators, are covered in @sec-ai-acceleration.

[^fn-dnn-blas]: **Basic Linear Algebra Subprograms (BLAS)**: Developed in the 1970s as a standard for basic vector and matrix operations, BLAS became the foundation for virtually all scientific computing. Modern implementations like Intel MKL and OpenBLAS can achieve 80-95% of theoretical peak performance on well-optimized workloads, making them necessary for neural network efficiency.

[^fn-mac]: **Multiply-Accumulate (MAC)**: The atomic operation in neural networks: multiply two values and add to a running sum (\(\text{result} \mathrel{+}= a \times b\)). Modern accelerators measure performance in MACs per second: datacenter-class accelerators can sustain on the order of \(10^{14}\) to \(10^{15}\) MAC/s on dense kernels, while mobile-class chips are often on the order of \(10^{12}\) to \(10^{13}\) MAC/s. At the hardware level, the energy cost of the arithmetic itself is typically in the picojoule range per MAC, while moving data to and from off-chip memory can cost orders of magnitude more, which is why data movement is often the dominant systems concern.

[^fn-tensor-cores]: **Tensor Cores**: Specialized matrix units in NVIDIA GPUs (Volta+) accelerating mixed-precision GEMM operations. A100 Tensor Cores deliver 312 TFLOPS for FP16 vs. 19.5 TFLOPS for FP32 on CUDA cores—16× faster. Require specific matrix dimensions (multiples of 8/16) and data layouts; frameworks like cuBLAS automatically tile operations to maximize Tensor Core utilization.

### System Implications {#sec-dnn-architectures-system-implications-7a8f}

Neural network architectures exhibit distinct system-level characteristics that exhibit three core dimensions for systematic analysis: memory requirements, computation needs, and data movement. This framework enables consistent analysis of how algorithmic patterns influence system design decisions, revealing both commonalities and architecture-specific optimizations. We apply this framework throughout our analysis of each architecture family. These system-level considerations build directly on the foundational concepts of neural network computation patterns, memory systems, and system scaling discussed in @sec-dl-primer.

#### Memory Requirements {#sec-dnn-architectures-memory-requirements-4900}

For dense pattern processing, the memory requirements stem from storing and accessing weights, inputs, and intermediate results. In our MNIST example, connecting our 784-dimensional input layer to a hidden layer of 100 neurons requires 78,400 weight parameters. Each forward pass must access all these weights, along with input data and intermediate results. The all-to-all connectivity pattern means there's no inherent locality in these accesses; every output needs every input and its corresponding weights.

These memory access patterns enable optimization through careful data organization and reuse. Modern processors handle these dense access patterns through specialized approaches: CPUs leverage their cache hierarchy for data reuse, while GPUs employ memory architectures designed for high-bandwidth access to large parameter matrices. Frameworks abstract these optimizations through high-performance matrix operations (as detailed in our earlier analysis).

#### Computation Needs {#sec-dnn-architectures-computation-needs-9cb4}

The core computation revolves around multiply-accumulate operations arranged in nested loops. Each output value requires as many multiply-accumulates as there are inputs. For MNIST, this requires 784 multiply-accumulates per output neuron. With 100 neurons in the hidden layer, 78,400 multiply-accumulates are performed for a single input image. While these operations are simple, their volume and arrangement create specific demands on processing resources.

This computational structure enables specific optimization strategies in modern hardware. The dense matrix multiplication pattern parallelizes across multiple processing units, with each handling different subsets of neurons. Modern hardware accelerators take advantage of this through specialized matrix multiplication units, while software frameworks automatically convert these operations into optimized BLAS (Basic Linear Algebra Subprograms) calls. CPUs and GPUs can both exploit cache locality by carefully tiling the computation to maximize data reuse, though their specific approaches differ based on their architectural strengths.

#### Data Movement {#sec-dnn-architectures-data-movement-fc16}

The all-to-all connectivity pattern in MLPs creates significant data movement requirements. Each multiply-accumulate operation needs three pieces of data: an input value, a weight value, and the running sum. For our MNIST example layer, computing a single output value requires moving 784 inputs and 784 weights to wherever the computation occurs. This movement pattern repeats for each of the 100 output neurons, creating large data transfer demands between memory and compute units.

The predictable data movement patterns enable strategic data staging and transfer optimizations. Different architectures address this challenge through various mechanisms; CPUs use prefetching and multi-level caches, while GPUs employ high-bandwidth memory systems and latency hiding through massive threading. Software frameworks orchestrate these data movements through memory management systems that reduce redundant transfers and increase data reuse.

This analysis of MLP computational demands reveals a crucial insight: while dense connectivity provides universal approximation capabilities, it creates significant inefficiencies when data exhibits inherent structure. This mismatch between architectural assumptions and data characteristics motivated the development of specialized approaches that could exploit structural patterns for computational gain.

## CNNs: Spatial Pattern Processing {#sec-dnn-architectures-cnns-spatial-pattern-processing-f8ff}

The computational intensity and parameter requirements of MLPs reveal a mismatch when applied to structured data. Building on the computational complexity considerations outlined in @sec-dnn-architectures-architectural-principles-engineering-tradeoffs-89de, this inefficiency motivated the development of architectural patterns that exploit inherent data structure.

Convolutional Neural Networks emerged as the solution to this challenge [@lecun1998gradient; @krizhevsky2012imagenet], embodying a specific inductive bias: they assume spatial locality and translation invariance, where nearby pixels are related and patterns can appear anywhere. This architectural assumption enables two key innovations that enhance efficiency for spatially structured data. Parameter sharing allows the same feature detector to be applied across different spatial positions, reducing parameters from millions to thousands while improving generalization. Local connectivity restricts connections to spatially adjacent regions, reflecting the insight that spatial proximity correlates with feature relevance.

::: {.callout-definition title="Convolutional Neural Networks"}

**Convolutional Neural Networks (CNNs)** refer to neural architectures that exploit _spatial structure_ through _local connectivity_ and _parameter sharing_, using _learnable filters_ to build _hierarchical representations_ with substantially fewer parameters than fully-connected networks.

:::

These architectural innovations represent a trade-off in deep learning design: sacrificing the theoretical generality of MLPs for practical efficiency gains when data exhibits known structure. While MLPs treat each input element independently, CNNs exploit spatial relationships to achieve computational savings and improved performance on vision tasks.

### Pattern Processing Needs {#sec-dnn-architectures-pattern-processing-needs-a4ce}

Spatial pattern processing addresses scenarios where the relationship between data points depends on their relative positions or proximity. Consider processing a natural image: a pixel's relationship with its neighbors is important for detecting edges, textures, and shapes. These local patterns then combine hierarchically to form more complex features: edges form shapes, shapes form objects, and objects form scenes.

This hierarchical spatial pattern processing appears across many domains. In computer vision, local pixel patterns form edges and textures that combine into recognizable objects. Speech processing relies on patterns across nearby time segments to identify phonemes and words. Sensor networks analyze correlations between physically proximate sensors to understand environmental patterns. Medical imaging depends on recognizing tissue patterns that indicate biological structures.

Focusing on image processing to illustrate these principles, if we want to detect a cat in an image, certain spatial patterns must be recognized: the triangular shape of ears, the round contours of the face, the texture of fur. Importantly, these patterns maintain their meaning regardless of where they appear in the image. A cat is still a cat whether it appears in the top-left or bottom-right corner. This indicates two key requirements for spatial pattern processing: the ability to detect local patterns and the ability to recognize these patterns regardless of their position[^fn-dnn-imagenet]. @fig-cnn-spatial-processing shows convolutional neural networks achieving this through hierarchical feature extraction, where simple patterns compose into increasingly complex representations at successive layers.

[^fn-dnn-imagenet]: **ImageNet Revolution**: AlexNet's dramatic victory in the 2012 ImageNet challenge [@krizhevsky2012imagenet] (reducing top-5 error from 25.8% to 15.3%) sparked the deep learning renaissance. ImageNet's 14 million labeled images across 20,000 categories provided the scale needed to train deep CNNs, proving that "big data + big compute + big models" could achieve superhuman performance.

::: {#fig-cnn-spatial-processing fig-env="figure" fig-pos="htb"}
```{.tikz}
\begin{tikzpicture}[line join=round,font=\usefont{T1}{phv}{m}{n}]
\tikzset{
 Line/.style={line width=0.5pt,black!50,text=black},
 LineD/.style={line width=0.5pt,black!50,text=black,dashed},
}

\tikzset{
channel/.pic={
\pgfkeys{/channel/.cd, #1}
\begin{scope}[yscale=\scalefac,xscale=\scalefac,every node/.append style={scale=\scalefac}]
\node[rectangle,draw=\channelcolor,line width=1pt,fill=\channelcolor!10,
minimum width=46,minimum height=56](\picname){};
\end{scope}
        }
}

\pgfkeys{
  /channel/.cd,
  channelcolor/.store in=\channelcolor,
  scalefac/.store in=\scalefac,
  picname/.store in=\picname,
  channelcolor=BrownLine,
  scalefac=1,
  picname=C
}
%circles sty
\tikzset{
circles/.pic={
\pgfkeys{/channel/.cd, #1}
\node[circle,draw=\channelcolor,line width=1pt,fill=\channelcolor!10,
minimum size=6mm](\picname){};
        }
}
%Zebra sty
\input{includes/zebra_pic.tex}


\pgfkeys{
  /zebra/.cd,
  globalscale/.store in=\globalscale,
  zebracolor/.store in=\zebracolor,
  zebracolor=BrownLine,
  globalscale=1,
}

\node[rectangle,draw=BrownLine,line width=1pt,fill=BrownLine!10,
minimum width=35mm,minimum height=30mm](ZEB){};
%
\begin{scope}[local bounding box=ZEBRA,shift={($(ZEB)+(-1.3,-1.45)$)}]
\pic {zebra={globalscale=0.09}};
\node[rectangle,draw=red,minimum width=8mm,minimum height=6mm,line width=1.5pt](REC1)
at($(ZEB.80)!0.47!(ZEB.280)$){};
\draw[Line,-latex](REC1.south)--++(270:2)node[below](KE){Kernel};
\end{scope}
%
\begin{scope}[local bounding box=CHANEL1,shift={($(ZEB)+(3.3,0.2)$)}]
\foreach \i in {1,2,3} {
\pic at ({\i*0.1}, {-0.1*\i}) {channel={scalefac=1.5,picname=\i-CH1}};
}
\end{scope}
%
\begin{scope}[local bounding box=CHANEL2,shift={($(CHANEL1)+(2.6,0.2)$)}]
\foreach \i in {1,2,3,4,5} {
\pic at ({\i*0.1}, {-0.1*\i}) {channel={scalefac=1.15,picname=\i-CH2}};
}
\end{scope}
%
\begin{scope}[local bounding box=CHANEL3,shift={($(CHANEL2)+(2.2,0.4)$)}]
\foreach \i in {1,...,8} {
\pic at ({\i*0.1}, {-0.1*\i}) {channel={scalefac=1.0,picname=\i-CH3}};
}
\end{scope}
\begin{scope}[local bounding box=CHANEL4,shift={($(CHANEL3)+(1.6,2.0)$)}]
\foreach \i in {1,...,10} {
\pic at ({\i*0.3}, {-0.3*\i}) {channel={scalefac=0.75,picname=\i-CH4}};
}
\end{scope}
\begin{scope}[local bounding box=CHANEL5,shift={($(CHANEL4)+(2.8,3.5)$)}]
\foreach \i in {1,...,11} {
\pic at ({\i*0}, {-0.6*\i}) {channel={scalefac=0.3,picname=\i-CH5}};
}
\end{scope}
%%%%
%11 neurons
\begin{scope}[local bounding box=CIRCLES,shift={($(CHANEL4)+(4.3,0)$)}]
\foreach \i in {1,...,11} {
  \pgfmathsetmacro{\y}{(6-\i)*0.8}
  \pic at (0,\y) {circles={channelcolor=VioletLine,picname=1CI\i,}};
}
%2row -7 neurons
\foreach \j in {1,...,7} {
  \pgfmathsetmacro{\y}{(4-\j)*0.8 + 0}
  \pic at (2.2,\y) {circles={channelcolor=VioletLine,picname=2CI\j}};
}
%3row -7 neurons
\foreach \j in {1,...,5} {
  \pgfmathsetmacro{\y}{(3-\j)*0.8 + 0}
  \pic at (4.0,\y) {circles={channelcolor=VioletLine,picname=3CI\j}};
}
%4row -7 neurons
\foreach \j in {1,...,3} {
  \pgfmathsetmacro{\y}{(2-\j)*0.8 + 0}
  \pic at (5.5,\y) {circles={channelcolor=VioletLine,picname=4CI\j}};
}
%
\foreach \i in {1,...,11}{
\draw[Line](\i-CH5)--(1CI\i);
}
\foreach \i in {1,...,11}{
  \foreach \j in {1,...,7}{
\draw[Line](1CI\i)--(2CI\j);
}}
\foreach \i in {1,...,7}{
  \foreach \j in {1,...,5}{
\draw[Line](2CI\i)--(3CI\j);
}}

\foreach \i in {1,...,5}{
  \foreach \j in {1,...,3}{
\draw[Line](3CI\i)--(4CI\j);
}}
\end{scope}
\node[rectangle,draw=GreenLine,line width=1pt,fill=GreenL,
minimum width=46,minimum height=80](OU)at($(CIRCLES.east)+(1.1,0)$){};
\draw[LineD](4CI1)--node[above]{0.2}($(OU.north east)!0.25!(OU.south east)$)coordinate(HO);
\draw[LineD](4CI2)--node[above]{0.7}($(OU.north east)!0.5!(OU.south east)$)coordinate(ZE);
\draw[LineD](4CI3)--node[above]{0.1}($(OU.north east)!0.75!(OU.south east)$)coordinate(DO);
\draw[thick](HO)--++(0:0.2)node[right](HORSE){Horse};
\draw[thick](ZE)--++(0:0.2)node[right]{Zebra};
\draw[thick](DO)--++(0:0.2)node[right]{Dog};
\node[above=6pt of OU]{Output};
\node[below=6pt of OU,align=center]{SoftMax Activation\\ Function};
%\draw[](ZEB.80)--(ZEB.280);
%%%
\node[rectangle,draw=red,minimum width=5mm,minimum height=6mm,line width=1.5pt](REC2)
at($(3-CH1.80)!0.27!(3-CH1.280)$){};
\node[rectangle,draw=red,minimum width=5mm,minimum height=6mm,line width=1.5pt](REC3)
at($(5-CH2.70)!0.7!(5-CH2.290)$){};
\node[rectangle,draw=red,minimum width=5mm,minimum height=6mm,line width=1.5pt](REC4)
at($(8-CH3.70)!0.3!(8-CH3.290)$){};
\draw[LineD](REC1.north east)--($(3-CH1.100)!0.7!(3-CH1.260)$);
\draw[LineD](REC1.south east)--($(3-CH1.100)!0.7!(3-CH1.260)$);
\draw[LineD](REC2.north east)--($(5-CH2.100)!0.3!(5-CH2.260)$);
\draw[LineD](REC2.south east)--($(5-CH2.100)!0.3!(5-CH2.260)$);
\draw[LineD](REC3.north east)--($(8-CH3.100)!0.3!(8-CH3.260)$);
\draw[LineD](REC3.south east)--($(8-CH3.100)!0.3!(8-CH3.260)$);
\draw[LineD](REC4.north east)--($(10-CH4.100)!0.3!(10-CH4.260)$);
\draw[LineD](REC4.south east)--($(10-CH4.100)!0.3!(10-CH4.260)$);
\draw[LineD](1-CH4.north west)--(1-CH5.north west);
\draw[LineD](1-CH4.north east)--(1-CH5.north west);
\draw[LineD](10-CH4.south west)--(11-CH5.south west);
\draw[LineD](10-CH4.south east)--(11-CH5.south west);
%Text
\node[below=6pt of 3-CH1,align=center]{Convolution\\ + \\ ReLU};
\node[below=10pt of 5-CH2,align=center]{Convolution\\ + \\ ReLU};
\node[below=10pt of 8-CH3,align=center]{Convolution\\ + \\ ReLU};
\node[below=4pt of 11-CH5,align=center]{Flatten\\ Layer};
\path[red](3-CH1.south west)--++(270:2.3)coordinate(FM1)-|coordinate(FM2)(10-CH4.south east);
\path[red](ZEB.south west)--++(270:3.8)coordinate(FE1)-|coordinate(FE2)(11-CH5.south west);
\path[red](ZEB.south west)--++(270:3.8)-|coordinate(CL1)(11-CH5.south east);
\path[red](ZEB.south west)--++(270:3.8)-|coordinate(CL2)(4CI1.east);
\path[red](ZEB.south west)--++(270:3.8)-|coordinate(PD1)(OU.south west);
\path[red](ZEB.south west)--++(270:3.8)-|coordinate(PD2)(HORSE.east);
\path[red](1CI11.south)--++(270:0.1)coordinate(FCL1)-|coordinate(FCL2)(4CI3.south);
%
\draw[latex-latex,line width=0.75pt](FM1)--node[above]{Feature Maps}(FM2);
\draw[BlueLine,decoration={brace,amplitude=9pt,mirror},decorate,line width=0.75pt]([yshift=0mm]FE1)--([yshift=0mm]FE2)
 node [midway,below=4mm,black] {Feature Extraction};
\draw[BlueLine,decoration={brace,amplitude=9pt,mirror},decorate,line width=0.75pt]([yshift=0mm]CL1)--([yshift=0mm]CL2)
 node [midway,below=4mm,black] {Classification};
\draw[BlueLine,decoration={brace,amplitude=9pt,mirror},decorate,line width=0.75pt]([yshift=0mm]PD1)--([yshift=0mm]PD2)
 node [midway,below=4mm,black] {Probabilistic Distribution};
 \draw[VioletLine,decoration={brace,amplitude=9pt,mirror},decorate,line width=0.75pt]([yshift=0mm]FCL1)--([yshift=0mm]FCL2)
 node [midway,below=3mm,black] {Fully Connected Layer};
 %text above
\path[red](ZEB.north)--++(90:0.7)coordinate(IN)-|coordinate(PO1)(1-CH1.north east);
\path[red](ZEB.north)--++(90:0.7)-|coordinate(PO2)(1-CH2.north east);
\path[red](ZEB.north)--++(90:0.7)-|coordinate(PO3)(1-CH3.north east);
\node at(IN){Input};
\node at(PO1){Pooling};
\node at(PO2){Pooling};
\node at(PO3){Pooling};
\end{tikzpicture}
```
**Spatial Feature Extraction**: Convolutional neural networks identify patterns independent of their location in an image by applying learnable filters across the input, enabling robust object recognition. These filters detect local features, and their repeated application across the image creates translation invariance, the ability to recognize a pattern regardless of its position.
:::

This leads us to the convolutional neural network architecture (CNN), pioneered by Yann LeCun[^fn-lecun-cnn] and @lecun1989backpropagation. CNNs achieve this through several key innovations: parameter sharing[^fn-parameter-sharing], local connectivity, and translation invariance[^fn-translation-invariance].

[^fn-lecun-cnn]: **Yann LeCun and CNNs**: LeCun's 1989 LeNet architecture was inspired by Hubel and Wiesel's discovery of simple and complex cells in cat visual cortex [@hubel1962receptive]. LeNet-5 achieved 0.8% error rate on MNIST in 1998 (though this was measured on the original NIST database, which differs slightly from the modern MNIST benchmark) and was deployed by banks to read millions of checks daily, among the first large-scale commercial applications of neural networks.

[^fn-parameter-sharing]: **Parameter Sharing**: CNNs reuse the same filter weights across spatial positions, reducing parameters substantially. A CNN processing 224×224 images might use 3×3 filters with only 9 parameters per channel, versus an equivalent MLP requiring 50,176 parameters per neuron, a ~5,575x reduction per neuron enabling practical computer vision.

[^fn-translation-invariance]: **Translation Invariance**: CNNs detect features regardless of spatial position. A cat's ear is recognized whether in the top-left or bottom-right corner. This property emerges from convolution's sliding window design and is important for computer vision, where objects appear at arbitrary locations in images.

### Algorithmic Structure {#sec-dnn-architectures-algorithmic-structure-a10c}

The core operation in a CNN can be expressed mathematically as:

$$
\mathbf{H}^{(l)}_{i,j,k} = f\left(\sum_{di}\sum_{dj}\sum_{c} \mathbf{W}^{(l)}_{di,dj,c,k}\mathbf{H}^{(l-1)}_{i+di,j+dj,c} + \mathbf{b}^{(l)}_k\right)
$$

This equation describes how CNNs process spatial data. $\mathbf{H}^{(l)}_{i,j,k}$ is the output at spatial position $(i,j)$ in channel $k$ of layer $l$. The triple sum iterates over the filter dimensions: $(di,dj)$ scans the spatial filter size, and $c$ covers input channels. $\mathbf{W}^{(l)}_{di,dj,c,k}$ represents the filter weights, capturing local spatial patterns. Unlike MLPs that connect all inputs to outputs, CNNs only connect local spatial neighborhoods.

Breaking down the notation further, $(i,j)$ corresponds to spatial positions, $k$ indexes output channels, $c$ indexes input channels, and $(di,dj)$ spans the local receptive field[^fn-receptive-field]. Unlike the dense matrix multiplication of MLPs, this operation:

[^fn-receptive-field]: **Receptive Field**: The region of the input that influences a particular output neuron. In CNNs, receptive fields grow with depth. A neuron in layer 3 might "see" a 7×7 region even with 3×3 filters, due to stacking. Understanding receptive field size is important for ensuring networks can capture features at the right scale for the task.

* Processes local neighborhoods (typically $3\times 3$ or $5\times 5$)
* Reuses the same weights at each spatial position
* Maintains spatial structure in its output

To illustrate this process concretely, consider the MNIST digit classification task with $28\times 28$ grayscale images. Each convolutional layer applies a set of filters (e.g., $3\times 3$) that slide across the image, computing local weighted sums. If we use 32 filters with padding to preserve dimensions, the layer produces a $28\times 28\times 32$ output, where each spatial position contains 32 different feature measurements of its local neighborhood. This contrasts sharply with the Multi-Layer Perceptron (MLP) approach, where the entire image is flattened into a 784-dimensional vector before processing.

This algorithmic structure directly implements the requirements for spatial pattern processing, creating distinct computational patterns that influence system design. Unlike MLPs, convolutional networks preserve spatial locality, leveraging the hierarchical feature extraction principles established above. These properties drive architectural optimizations in AI accelerators, where operations such as data reuse, tiling, and parallel filter computation are important for performance.

::: {.callout-note title="Mathematical Background"}
Group theory provides the mathematical language for symmetries and transformations in data. Convolution implements **translation equivariance**: shifting the input shifts the output feature map. Formally, for translation operator \(T_v\), a convolutional layer \(f\) satisfies \(f(T_v x) = T_v f(x)\). This is distinct from **translation invariance** (\(f(T_v x) = f(x)\)), which is often introduced via pooling or other aggregation.
:::

#### Translation Equivariance: Preserving Spatial Structure {#sec-dnn-architectures-translation-equivariance-8f3a}

The mathematical property of translation equivariance is central to understanding why CNNs work effectively for spatial data. While the brief note above introduced the concept, its implications for learning efficiency and systems design warrant deeper examination.

**Equivariance vs Invariance**: These related but distinct concepts determine how architectures handle transformations:

**Equivariance** means transforming the input produces the same transformation in the output:

$$
f(T(\mathbf{x})) = T(f(\mathbf{x}))
$$

For CNNs with translation $T_v$ (shift by vector $v$), if the input shifts by 5 pixels right, the feature maps also shift by 5 pixels right. Position information is preserved through the transformation.

**Invariance** means transforming the input does not change the output:

$$
f(T(\mathbf{x})) = f(\mathbf{x})
$$

Global average pooling over an entire feature map exhibits translation invariance: shifting the input does not change the averaged output. Position information is discarded.

**Why Equivariance Matters for Learning**: Equivariance preserves information needed for structured learning:

**Spatial Relationships**: A feature detector responding to an eye at position $(x, y)$ will respond to the same eye at position $(x+5, y)$, but the response moves to reflect the new position. The network can learn spatial relationships: "eye above nose" matters for face detection. Full invariance would lose this: "eye and nose both present somewhere" is insufficient.

**Object Detection**: Localization tasks require position. Object detection outputs bounding boxes like "car at $(100, 200)$ with size $50 \times 80$". Equivariant layers track position through the network, while invariant final layers determine class. This architectural choice matches task structure: equivariance for localization, invariance for classification.

**Hierarchical Composition**: Equivariance enables compositional learning. Early layers detect edges equivariantly at all positions. Middle layers combine edges into shapes, also equivariantly. Final layers may use partial invariance (via pooling) for classification. This hierarchy works because intermediate features maintain spatial structure for composition.

**Mathematical Formulation**: For a convolutional layer with filter $\mathbf{w}$ and input $\mathbf{x}$:

$$
(f * \mathbf{w})[i, j] = \sum_{m,n} \mathbf{w}[m, n] \cdot \mathbf{x}[i + m, j + n]
$$

Applying translation $T_v$ (shift by $v = (v_1, v_2)$) to the input:

$$
(T_v \mathbf{x})[i, j] = \mathbf{x}[i - v_1, j - v_2]
$$

The convolution of the translated input becomes:

$$
(f * \mathbf{w})[T_v \mathbf{x}][i, j] = \sum_{m,n} \mathbf{w}[m, n] \cdot \mathbf{x}[(i - v_1) + m, (j - v_2) + n]
$$

$$
= \sum_{m,n} \mathbf{w}[m, n] \cdot \mathbf{x}[(i + m) - v_1, (j + n) - v_2]
$$

$$
= (f * \mathbf{w})[\mathbf{x}][i - v_1, j - v_2] = T_v((f * \mathbf{w})[\mathbf{x}])[i, j]
$$

This proves translation equivariance: $f(T_v \mathbf{x}) = T_v(f(\mathbf{x}))$.

**Concrete Example**: Consider detecting whisker patterns in a cat image:

**Input Image**: Cat face at position $(50, 50)$

**Equivariant Convolutional Layer**:

- Operation: $3 \times 3$ filter detecting whisker textures
- Output: Whisker features detected at position $(50, 50)$ in feature map
- Shifted Input: Cat face at position $(55, 55)$
- Shifted Output: Whisker features at position $(55, 55)$ in feature map
- Property: Feature position tracks input position

**Invariant Global Pooling Layer**:

- Operation: Average pooling over entire spatial dimensions
- Output: Average whisker strength $= 0.8$ (scalar, no position)
- Shifted Input: Cat face at position $(55, 55)$
- Shifted Output: Average whisker strength $= 0.8$ (same value)
- Property: Output ignores spatial position

The equivariant layers preserve where features occur, enabling the network to learn that "whiskers near mouth" and "ears above eyes" matter for cat classification. Invariant final layers discard absolute position for classification.

::: {.callout-example title="Equivariance in Action: Feature Detection"}

Consider a $7 \times 7$ image with a vertical edge at column 3:

$$
\mathbf{x} = \begin{bmatrix}
0 & 0 & 1 & 0 & 0 & 0 & 0 \\
0 & 0 & 1 & 0 & 0 & 0 & 0 \\
0 & 0 & 1 & 0 & 0 & 0 & 0 \\
0 & 0 & 1 & 0 & 0 & 0 & 0 \\
0 & 0 & 1 & 0 & 0 & 0 & 0 \\
0 & 0 & 1 & 0 & 0 & 0 & 0 \\
0 & 0 & 1 & 0 & 0 & 0 & 0
\end{bmatrix}
$$

Vertical edge detector filter:

$$
\mathbf{w} = \begin{bmatrix}
-1 & 0 & 1 \\
-1 & 0 & 1 \\
-1 & 0 & 1
\end{bmatrix}
$$

**Convolving original image**:

Output feature map has strong activation at column 3 (where edge is):

$$
f(\mathbf{x}) = \begin{bmatrix}
0 & 3 & 0 & 0 & 0 \\
0 & 3 & 0 & 0 & 0 \\
0 & 3 & 0 & 0 & 0 \\
0 & 3 & 0 & 0 & 0 \\
0 & 3 & 0 & 0 & 0
\end{bmatrix}
$$

**Shifted input** (edge moved to column 5):

$$
T_2 \mathbf{x} = \begin{bmatrix}
0 & 0 & 0 & 0 & 1 & 0 & 0 \\
0 & 0 & 0 & 0 & 1 & 0 & 0 \\
0 & 0 & 0 & 0 & 1 & 0 & 0 \\
0 & 0 & 0 & 0 & 1 & 0 & 0 \\
0 & 0 & 0 & 0 & 1 & 0 & 0 \\
0 & 0 & 0 & 0 & 1 & 0 & 0 \\
0 & 0 & 0 & 0 & 1 & 0 & 0
\end{bmatrix}
$$

**Convolving shifted image**:

$$
f(T_2 \mathbf{x}) = \begin{bmatrix}
0 & 0 & 0 & 3 & 0 \\
0 & 0 & 0 & 3 & 0 \\
0 & 0 & 0 & 3 & 0 \\
0 & 0 & 0 & 3 & 0 \\
0 & 0 & 0 & 3 & 0
\end{bmatrix} = T_2(f(\mathbf{x}))
$$

The feature activation shifts by the same amount as the input, demonstrating equivariance. The network knows the edge is at column 5 in the shifted image, not just that an edge exists somewhere.

:::

**Systems Implications of Equivariance**:

**Parameter Efficiency**: Equivariance through parameter sharing enables dramatic parameter reduction:

- MLP for $224 \times 224$ RGB image: Each hidden neuron connects to all $224 \times 224 \times 3 = 150,528$ inputs
- CNN with $3 \times 3$ filter: Only $3 \times 3 \times 3 = 27$ parameters per filter, reused across all $224 \times 224$ positions
- Parameter reduction: $150,528 / 27 \approx 5,575 \times$ fewer parameters per feature detector
- Memory savings enable larger models and bigger batches on fixed hardware

**Computational Structure**: Equivariance creates regular, parallelizable computation:

- **Sliding Window**: Same operation at every spatial position
- **Data Reuse**: Input pixels used by multiple filter positions (im2col optimizations exploit this)
- **SIMD Friendly**: Modern GPUs execute same instruction across spatial positions simultaneously
- **Hardware Acceleration**: TPUs and AI accelerators have specialized units for convolution specifically because this structure maps efficiently to silicon

**Generalization**: Equivariance improves sample efficiency:

- Network learning edge detector at one position automatically has it at all positions
- No need for training examples with edges at every possible location
- This "built-in" data augmentation means fewer training samples needed
- Systems benefit: less data storage, faster training, lower bandwidth requirements

**Group Theory Perspective**: Convolution's equivariance to translations is one instance of a general principle. The translation group $(\mathbb{R}^2, +)$ consists of all 2D translations, closed under composition (translating by $v$ then $u$ equals translating by $v + u$). Convolution is equivariant to this group.

Recent research extends this framework to other symmetry groups. Cohen & Welling [@cohen2016group] developed Group-Equivariant CNNs that handle rotations and reflections by constructing filters equivariant to rotation groups. This enables learning rotation-invariant features for tasks like satellite imagery or medical imaging where orientation does not determine meaning.

The mathematical framework: for group $G$ acting on input space $X$ and output space $Y$, a function $f: X \to Y$ is $G$-equivariant if:

$$
f(g \cdot \mathbf{x}) = g \cdot f(\mathbf{x}) \quad \forall g \in G, \mathbf{x} \in X
$$

Standard CNNs are translation-equivariant. Rotation-equivariant networks extend this to rotation groups. The architectural principle generalizes: embed symmetries of your data as equivariances in your architecture.

For systems engineering, this means:

- Identifying data symmetries informs architecture choice
- More constrained architectures (stronger symmetries) often mean smaller models
- Specialized equivariances may require custom operations (rotation convolutions) that need hardware support or efficient software implementations

**Practical Considerations**:

**Breaking Equivariance**: Some operations break translation equivariance:

- Padding: Asymmetric padding at image boundaries breaks perfect equivariance
- Stride $> 1$: Downsampling introduces quantization (shift by 1 pixel produces non-integer shift in output)
- Batch normalization: Statistics computed per position in some implementations break equivariance

Modern networks accept these deviations as necessary trade-offs for computational efficiency or training stability.

**Equivariance vs Task Requirements**:

- Image classification: Only final class label needs invariance; intermediate layers stay equivariant
- Object detection: Bounding box coordinates require equivariance throughout
- Semantic segmentation: Per-pixel labels need full equivariance to output layer
- Image generation: Output image must maintain spatial structure (equivariance essential)

The choice of where to introduce invariance (via pooling, global averaging) vs. maintaining equivariance reflects task requirements and directly impacts architectural decisions.

The choice of convolution reflects deeper principles about inductive bias[^fn-inductive-bias] in neural architecture design. By restricting connectivity to local neighborhoods and sharing parameters across spatial positions, CNNs encode prior knowledge about the structure of visual data: that important features are local and translation-invariant. This architectural constraint reduces the hypothesis space[^fn-hypothesis-space] that the network must search, enabling more efficient learning from limited data compared to fully connected networks.

[^fn-inductive-bias]: **Inductive Bias**: Prior assumptions built into model architecture about the structure of data. CNNs assume spatial locality and translation equivariance, which can yield translation-invariant predictions when combined with pooling or other aggregation. This constraint reduces the space of functions they can learn compared to MLPs and often enables better generalization with fewer parameters.

[^fn-hypothesis-space]: **Hypothesis Space**: The set of all possible functions a model can represent given its architecture and parameters. MLPs have a larger hypothesis space than CNNs for images, but CNNs' constrained space contains better solutions for visual tasks, demonstrating that architectural constraints often improve rather than limit performance. Recent work has extended these principles to other symmetry groups, developing Group-Equivariant CNNs that handle rotations and reflections [@cohen2016group].

CNNs naturally implement hierarchical representation learning through their layered structure. Early layers detect low-level features like edges and textures with small receptive fields, while deeper layers combine these into increasingly complex patterns with larger receptive fields. This hierarchical organization mirrors the structure of the visual cortex and enables CNNs to build compositional representations: complex objects are represented as compositions of simpler parts. The mathematical foundation for this emerges from the fact that stacking convolutional layers creates a tree-like dependency structure, where each deep neuron depends on an exponentially large set of input pixels, enabling efficient representation of hierarchical patterns.

#### Architectural Characteristics {#sec-dnn-architectures-architectural-characteristics-e309}

Parameter sharing dramatically reduces complexity compared to MLPs by reusing the same filters across spatial locations. This sharing embodies the assumption that useful features (such as edges or textures) can appear anywhere in an image, making the same feature detector valuable across all spatial positions.

The architectural efficiency of CNNs enables further optimization through specialized techniques. Depthwise separable convolutions decompose standard convolutions into depthwise and pointwise operations, reducing computation by 8-9× for typical mobile deployments. Channel pruning eliminates entire feature maps based on importance metrics, achieving 40-50% FLOPs reduction with <1% accuracy loss. These optimization strategies build on spatial locality principles, with @sec-model-optimizations exploring hardware-specific implementations and @sec-ai-acceleration detailing how modern processors exploit convolution's inherent data reuse patterns.

As illustrated in @fig-cnn, convolution operations involve sliding a small filter over the input image to generate a feature map[^fn-feature-map]. This process captures local structures while maintaining translation invariance. For an interactive visual exploration of convolutional networks, the [CNN Explainer](https://poloclub.github.io/cnn-explainer/) project provides an insightful demonstration of how these networks are constructed.

[^fn-feature-map]: **Feature Map**: The output of applying a convolutional filter to an input, representing detected features at different spatial locations. A 64-filter layer produces 64 feature maps, each highlighting different patterns like edges, textures, or shapes. Feature maps become more abstract (detecting objects, faces) in deeper layers compared to early layers (detecting edges, colors).

::: {#fig-cnn fig-env="figure" fig-pos="htb"}
```{.tikz}
\scalebox{0.7}{%
\begin{tikzpicture}[x={(-1,-0.4)}, y={(0.8,0.8)}, line join=round,font=\usefont{T1}{phv}{m}{n}]
\makeatletter
\newif\ifbox@dashed
\box@dashedfalse % default: not dashed

\tikzset{
  box/.pic={
    \pgfkeys{/box/.cd,#1}
    \coordinate (origin) at (0,0);
    % intersection points
    \foreach \x in {0,...,\columns}{
      \foreach \y in {0,...,\rows}{
        \coordinate (pt-\x-\y\br) at ($ (origin) + \x*\cellsize*(1,0) + \y*\cellheight*(0,1) $);
      }
    }
    % Drawing cells
    \foreach \x in {0,...,\numexpr\columns-1}{
      \foreach \y in {0,...,\numexpr\rows-1}{
        \draw[fill=\ffill, line width=\linewidth, \ifbox@dashed dashed\fi]
          (pt-\x-\y\br) --
          (pt-\the\numexpr\x+1\relax-\y\br) --
          (pt-\the\numexpr\x+1\relax-\the\numexpr\y+1\relax\br) --
          (pt-\x-\the\numexpr\y+1\relax\br) -- cycle;
      }
    }
  }
}

\pgfkeys{
  /box/.cd,
  cellsize/.store in=\cellsize,
  linewidth/.store in=\linewidth,
  cellheight/.store in=\cellheight,
  columns/.store in=\columns,
  rows/.store in=\rows,
  br/.store in=\br,
  ffill/.store in=\ffill,
  dashed/.code={\box@dashedtrue},
  columns=1,
  rows=3,
  br=A,
  ffill=red,
  cellsize=0.5pt,
  cellheight=0.5pt,
  linewidth=0.5pt
}
\makeatother

\pic at (0,0.66) {box={columns=3,rows=3,br=A,ffill=violet!20,linewidth=0.5pt}};
\pic at (0.5,1.16) {box={columns=1,rows=1,br=B,ffill=violet!50,linewidth=0.5pt}};

\pic at (1,-4) {box={columns=7,rows=7,br=C,ffill=none,linewidth=0.5pt,dashed}};
\pic at (1.5,-3.5) {box={columns=5,rows=5,br=D,ffill=OrangeLine!40,linewidth=0.35pt}};
\pic at (2,-3) {box={columns=3,rows=3,br=E,ffill=orange,linewidth=0.35pt}};
 \draw[blue](pt-0-0B)--(pt-0-0E);
\draw[blue](pt-0-1B)--(pt-0-3E);
\draw[blue](pt-1-1B)--(pt-3-3E);
\draw[blue](pt-1-0B)--(pt-3-0E);
\pic at (0,0.66) {box={columns=3,rows=3,br=A,ffill=violet!20,linewidth=0.35pt}};
\pic at (0.5,1.16) {box={columns=1,rows=1,br=B,ffill=violet!70,linewidth=0.35pt}};
\end{tikzpicture}}
```
The convolution operation processes input data through localized feature extraction using filters that slide across the image to identify patterns regardless of their position.
:::

### Computational Mapping {#sec-dnn-architectures-computational-mapping-fea5}

Convolution operations create computational patterns different from MLP dense matrix multiplication. This translation from mathematical operations to implementation details reveals distinct computational characteristics.

The first implementation, `conv_layer_spatial` (shown in @lst-conv_layer_spatial), uses high-level convolution operations to express the computation concisely. This is typical in deep learning frameworks, where optimized libraries handle the underlying complexity.

::: {#lst-conv_layer_spatial}
```{.python}
def conv_layer_spatial(input, kernel, bias):
    output = convolution(input, kernel) + bias
    return activation(output)
```
This hierarchical approach processes input data through feature extraction using a convolution operation that combines a kernel and bias before applying an activation function.
:::

The bridge between the logical model and physical execution becomes critical for understanding CNN system requirements. While the high-level convolution operation appears as a simple sliding window computation, the hardware must orchestrate complex data movement patterns and exploit spatial locality for efficiency.

The second implementation, conv_layer_compute (see @lst-conv_layer_compute), reveals the actual computational pattern: nested loops that process each spatial position, applying the same filter weights to local regions of the input. These seven nested loops expose the true nature of convolution's computational structure and the optimization opportunities it creates.

::: {#lst-conv_layer_compute}
```python
def conv_layer_compute(input, kernel, bias):
    # Loop 1: Process each image in batch
    for image in range(batch_size):

        # Loop 2&3: Move across image spatially
        for y in range(height):
            for x in range(width):

                # Loop 4: Compute each output feature
                for out_channel in range(num_output_channels):
                    result = bias[out_channel]

                    # Loop 5&6: Move across kernel window
                    for ky in range(kernel_height):
                        for kx in range(kernel_width):

                            # Loop 7: Process each input feature
                            for in_channel in range(
                                num_input_channels
                            ):
                                # Get input value from correct window position
                                in_y = y + ky
                                in_x = x + kx
                                # Perform multiply-accumulate operation
                                result += (
                                    input[
                                        image, in_y, in_x, in_channel
                                    ]
                                    * kernel[
                                        ky,
                                        kx,
                                        in_channel,
                                        out_channel,
                                    ]
                                )

                    # Store result for this output position
                    output[image, y, x, out_channel] = result
```
**Nested Loops**: Convolutional layers process input through multiple nested loops that handle batched images, spatial dimensions, output channels, kernel windows, and input features, revealing the detailed computational structure of convolution operations.
:::

The seven nested loops reveal different aspects of the computation:

* Outer loops (1-3) manage position: which image and where in the image
* Middle loop (4) handles output features: computing different learned patterns
* Inner loops (5-7) perform the actual convolution: sliding the kernel window

Examining this process in detail, the outer two loops (`for y` and `for x`) traverse each spatial position in the output feature map (for the MNIST example, this traverses all $28\times 28$ positions). At each position, values are computed for each output channel (`for k` loop), representing different learned features or patterns—the 32 different feature detectors.

The inner three loops implement the actual convolution operation at each position. For each output value, we process a local $3\times 3$ region of the input (the `dy` and `dx` loops) across all input channels (`for c` loop). This creates a sliding window effect, where the same $3\times 3$ filter moves across the image, performing multiply-accumulates between the filter weights and the local input values. Unlike the MLP's global connectivity, this local processing pattern means each output value depends only on a small neighborhood of the input.

For our MNIST example with $3\times 3$ filters and 32 output channels, each output position requires only 9 multiply-accumulate operations per input channel, compared to the 784 operations needed in our MLP layer. This operation must be repeated for every spatial position
$(28\times 28)$ and every output channel (32).

While using fewer operations per output, the spatial structure creates different patterns of memory access and computation that systems must handle. These patterns influence system design, creating both challenges and opportunities for optimization.

### System Implications {#sec-dnn-architectures-system-implications-f25d}

CNNs exhibit distinctive system-level patterns that differ significantly from MLP dense connectivity across all three analysis dimensions.

#### Memory Requirements {#sec-dnn-architectures-memory-requirements-7e2b}

For convolutional layers, memory requirements center around two key components: filter weights and feature maps. Unlike MLPs that require storing full connection matrices, CNNs use small, reusable filters. For a typical CNN processing 224×224 ImageNet images, a convolutional layer with 64 filters of size $3\times 3$ requires storing only 576 weight parameters $(3\times 3\times 64)$, dramatically less than the millions of weights needed for equivalent fully-connected processing. The system must store feature maps for all spatial positions, creating a different memory demand. A 224×224 input with 64 output channels requires storing 3.2 million activation values (224×224×64).

These memory access patterns suggest opportunities for optimization through weight reuse and careful feature map management. Processors optimize these spatial patterns by caching filter weights for reuse across positions while streaming feature map data. Frameworks implement spatial optimizations through specialized memory layouts that enable filter reuse and spatial locality in feature map access. CPUs and GPUs approach this differently. CPUs use their cache hierarchy to keep frequently used filters resident, while GPUs employ specialized memory architectures designed for the spatial access patterns of image processing. The detailed architecture design principles for these specialized processors are covered in @sec-ai-acceleration.

#### Computation Needs {#sec-dnn-architectures-computation-needs-22a5}

The core computation in CNNs involves repeatedly applying small filters across spatial positions. Each output value requires a local multiply-accumulate operation over the filter region. For ImageNet processing with $3\times 3$ filters and 64 output channels, computing one spatial position involves 576 multiply-accumulates $(3\times 3\times 64)$, and this must be repeated for all 50,176 spatial positions (224×224). While each individual computation involves fewer operations than an MLP layer, the total computational load remains large due to spatial repetition.

This computational pattern presents different optimization opportunities than MLPs. The regular, repeated nature of convolution operations enables efficient hardware utilization through structured parallelism. Modern processors exploit this pattern in various ways. CPUs leverage SIMD instructions[^fn-simd] to process multiple filter positions simultaneously, while GPUs parallelize computation across spatial positions and channels. The model optimization techniques that further reduce these computational demands, including specialized convolution optimizations and sparsity patterns, are detailed in @sec-model-optimizations.

[^fn-simd]: **SIMD (Single Instruction, Multiple Data)**: CPU instructions that perform the same operation on multiple data elements simultaneously. Modern x86 processors support AVX-512, enabling 16 single-precision operations per instruction, a 16x speedup over scalar code. SIMD is important for efficient neural network inference on CPUs, especially for edge deployment. Deep learning frameworks further optimize this through specialized convolution algorithms that transform the computation to better match hardware capabilities.

#### Data Movement {#sec-dnn-architectures-data-movement-9a0e}

The sliding window pattern of convolutions creates a distinctive data movement profile. Unlike MLPs where each weight is used once per forward pass, CNN filter weights are reused many times as the filter slides across spatial positions. For ImageNet processing, each $3\times 3$ filter weight is reused 50,176 times (once for each position in the 224×224 feature map). This creates a different challenge: the system must stream input features through the computation unit while keeping filter weights stable.

The predictable spatial access pattern enables strategic data movement optimizations. Different architectures handle this movement pattern through specialized mechanisms. CPUs maintain frequently used filter weights in cache while streaming through input features. GPUs employ memory architectures optimized for spatial locality and provide hardware support for efficient sliding window operations. Deep learning frameworks orchestrate these movements by organizing computations to maximize filter weight reuse and minimize redundant feature map accesses.

## RNNs: Sequential Pattern Processing {#sec-dnn-architectures-rnns-sequential-pattern-processing-ea14}

Convolutional Neural Networks achieved efficiency gains by exploiting spatial locality, yet their architectural assumptions fail when patterns depend on temporal order rather than spatial proximity. While CNNs excel at recognizing \"what\" is present in data through shared feature detectors, they cannot capture \"when\" events occur or how they relate across time. This limitation manifests in domains such as natural language processing, where word meaning depends on sentential context, and time-series analysis, where future values depend on historical patterns.

Sequential data presents a challenge distinct from spatial processing: patterns can span arbitrary temporal distances, rendering fixed-size kernels ineffective. While spatial convolution leverages the principle that nearby pixels are typically related, temporal relationships operate differently. Important connections may span hundreds or thousands of time steps with no correlation to proximity. Traditional feedforward architectures, including CNNs, process each input independently and cannot maintain the temporal context necessary for these long-range dependencies.

Recurrent Neural Networks address this architectural limitation [@elman1990finding; @hochreiter1997long] by embodying a temporal inductive bias: they assume sequential dependence, where the order of information matters and the past influences the present. This architectural assumption guides the introduction of memory as a component of the computational model. Rather than processing inputs in isolation, RNNs maintain an internal state that propagates information from previous time steps, enabling the network to condition its current output on historical context. This architecture embodies another trade-off: while CNNs sacrifice theoretical generality for spatial efficiency, RNNs introduce computational dependencies that challenge parallel execution in exchange for temporal processing capabilities.

::: {.callout-definition title="Recurrent Neural Networks"}

**Recurrent Neural Networks (RNNs)** refer to sequential neural architectures that maintain _internal memory state_ across time steps through _recurrent connections_, enabling _variable-length sequence processing_ at the cost of _sequential computation_ that prevents parallelization.

:::

::: {.callout-note title="Coverage Note"}
This section covers of RNNs, emphasizing their core contributions to sequential processing and the architectural principles that influenced modern attention mechanisms. While RNNs introduced critical concepts—memory states, temporal dependencies, and sequential computation—contemporary practice increasingly favors attention-based architectures for sequence modeling. We focus on foundational principles rather than extensive implementation variants, dedicating significant depth to the attention mechanisms and Transformers (@sec-dnn-architectures-attention-mechanisms-dynamic-pattern-processing-566d) that have largely superseded RNNs in production systems while building directly on the insights gained from recurrent architectures.
:::

### Pattern Processing Needs {#sec-dnn-architectures-pattern-processing-needs-c18e}

Sequential pattern processing addresses scenarios where current input interpretation depends on preceding information. In natural language processing, word meaning often depends heavily on previous words in the sentence. Context determines interpretation, as evidenced by the varying meanings of words based on surrounding terms. Similarly, in speech recognition, phoneme interpretation depends on surrounding sounds, while financial forecasting requires understanding historical data patterns.

The challenge in sequential processing lies in maintaining and updating relevant context over time. Human text comprehension does not restart with each word; rather, a running understanding evolves as new information is processed. Similarly, time-series data processing encounters patterns spanning different timescales, from immediate dependencies to long-term trends. This necessitates an architecture capable of both maintaining state over time and updating it based on new inputs.

These requirements translate into specific architectural demands: the system must maintain internal state to capture temporal context, update this state based on new inputs, and learn which historical information is relevant for current predictions. Unlike MLPs and CNNs, which process fixed-size inputs, sequential processing must accommodate variable-length sequences while maintaining computational efficiency. These requirements culminate in the recurrent neural network (RNN) architecture.

### Algorithmic Structure {#sec-dnn-architectures-algorithmic-structure-279b}

RNNs address sequential processing through recurrent connections, distinguishing them from MLPs and CNNs. Rather than merely mapping inputs to outputs, RNNs maintain an internal state updated at each time step, creating a memory mechanism that propagates information forward in time. This temporal dependency modeling capability was first explored by @elman1990finding, who demonstrated RNN capacity to identify structure in time-dependent data. Basic RNNs suffer from the vanishing gradient problem[^fn-vanishing-gradient], constraining their ability to learn long-term dependencies.

[^fn-vanishing-gradient]: **Vanishing Gradient Problem**: During backpropagation through time, gradients shrink exponentially as they propagate backward through RNN layers. When recurrent weights have magnitude < 1, gradients multiply by values < 1 at each time step, vanishing after 5-10 steps and preventing learning of long-term dependencies—a key limitation solved by LSTMs and attention mechanisms.

The core operation in a basic RNN can be expressed mathematically as:
$$
\mathbf{h}_t = f(\mathbf{W}_{hh}\mathbf{h}_{t-1} + \mathbf{W}_{xh}\mathbf{x}_t + \mathbf{b}_h)
$$
where $\mathbf{h}_t$ denotes the hidden state at time $t$, $\mathbf{x}_t$ denotes the input at time $t$, $\mathbf{W}_{hh}$ contains the recurrent weights, and $\mathbf{W}_{xh}$ contains the input weights, as illustrated in the unfolded network structure in @fig-rnn.

In word sequence processing, each word may be represented as a 100-dimensional vector ($\mathbf{x}_t$), with a hidden state of 128 dimensions ($\mathbf{h}_t$). At each time step, the network combines the current input with its previous state to update its sequential understanding, establishing a memory mechanism capable of capturing patterns across time steps.

This recurrent structure fulfills sequential processing requirements through connections that maintain internal state and propagate information forward in time. Rather than processing all inputs independently, RNNs process sequential data by iteratively updating a hidden state based on the current input and the previous hidden state, as depicted in @fig-rnn. This architecture suits tasks including language modeling, speech recognition, and time-series forecasting.

RNNs implement a recursive algorithm where each time step's function call depends on the result of the previous call. Analogous to recursive functions that maintain state through the call stack, RNNs maintain state through their hidden vectors. The mathematical formula $\mathbf{h}_t = f(\mathbf{h}_{t-1}, \mathbf{x}_t)$ directly parallels recursive function definitions where `f(n) = g(f(n-1), input(n))`. This correspondence explains RNN capacity to handle variable-length sequences: just as recursive algorithms process lists of arbitrary length by applying the same function recursively, RNNs process sequences of any length by applying the same recurrent computation.

#### Efficiency and Optimization {#sec-dnn-architectures-efficiency-optimization-ce92}

Sequential processing creates computational bottlenecks but enables unique efficiency characteristics for memory usage. RNNs achieve constant memory overhead for hidden state storage regardless of sequence length, making them extremely memory-efficient for long sequences. While Transformers require O(n²) memory for sequence length n, RNNs maintain fixed memory usage, enabling processing of sequences thousands of steps long on modest hardware.

Structured pruning of hidden-to-hidden connections can achieve 10x speedup while maintaining sequence modeling capability. The recurrent weight matrix $W_{hh}$ typically dominates parameter count for large hidden states, but magnitude-based pruning reveals that 70-80% of these connections contribute minimally to temporal dependencies. Block-structured pruning maintains computational efficiency while enabling significant model compression.

Sequential operations accumulate quantization errors, requiring careful quantization point placement and gradient scaling for stable low-precision training. Unlike feedforward networks where quantization errors remain localized, RNN errors propagate through time, making INT8 quantization more challenging. Per-timestep quantization schemes and careful handling of hidden state precision are required for maintaining accuracy in quantized RNN deployments.

::: {#fig-rnn fig-env="figure" fig-pos="htb"}
```{.tikz}
 \begin{tikzpicture}[line join=round,font=\large\usefont{T1}{phv}{m}{n}]
\definecolor{myorange}{RGB}{255,127,14}
\definecolor{mygreen}{RGB}{44,160,44}
\definecolor{mypurple}{RGB}{148,103,189}

\tikzset{%
Line/.style={line width=0.75pt,black!60,text=black}
}
\def\radius{7mm}
\foreach \x/\i/\f in {0/1/h,5/2/h\textsubscript{t - 1},9/3/h\textsubscript{t},13/4/h\textsubscript{t + 1}}{
\coordinate (2ball-\i) at  (\x,0);
\fill[fill=mygreen!50] (\x,0) circle (\radius)node[]{\f};
}

\def\radiuss{6mm}
\foreach \x/\i/\f in {0/1/y,5/2/y\textsubscript{t - 1},9/3/y\textsubscript{t},13/4/y\textsubscript{t + 1}}{
\coordinate (1ball-\i) at  (\x,3);
\fill[fill=myorange!50] (\x,3) circle (\radiuss)node[]{\f};
}

\foreach \x/\i/\f in {0/1/x,5/2/x\textsubscript{t - 1},9/3/x\textsubscript{t},13/4/x\textsubscript{t + 1}}{
\coordinate (3ball-\i) at  (\x,-2.5);
\fill[fill=mypurple!50] (\x,-2.5) circle (\radiuss)node[]{\f};
}

\foreach \x/\f in {1/W\textsubscript{hx},2/W\textsubscript{hx},3/W\textsubscript{hx},4/W\textsubscript{hx}}{
    \edef\from{3ball-\x}
    \edef\to{2ball-\x}
\path let
    \p1 = (\from),
    \p2 = (\to),
    \n1 = {atan2(\y2-\y1,\x2-\x1)}
  in
    coordinate (from) at ($ (\from) + (\n1:\radiuss) $)
    coordinate (to) at ($ (\to) + (\n1+180:\radius) $);
  \draw[Line,-latex] (from) --node[fill=white]{\f} (to);
 }

\foreach \x/\f in {1/W\textsubscript{yh},2/W\textsubscript{yh},3/W\textsubscript{yh},4/W\textsubscript{yh}}{
    \edef\from{2ball-\x}
    \edef\to{1ball-\x}
\path let
    \p1 = (\from),
    \p2 = (\to),
    \n1 = {atan2(\y2-\y1,\x2-\x1)}
  in
    coordinate (from) at ($ (\from) + (\n1:\radius) $)
    coordinate (to) at ($ (\to) + (\n1+180:\radiuss) $);
  \draw[Line,-latex] (from) --node[fill=white,pos=0.55]{\f} (to);
 }

\foreach \x/\f in {2/W\textsubscript{hh},3/W\textsubscript{hh}}{
\pgfmathtruncatemacro{\newX}{\x + 1} %
    \edef\from{2ball-\x}
    \edef\to{2ball-\newX}
\path let
    \p1 = (\from),
    \p2 = (\to),
    \n1 = {atan2(\y2-\y1,\x2-\x1)}
  in
    coordinate (from) at ($ (\from) + (\n1:\radius) $)
    coordinate (to) at ($ (\to) + (\n1+180:\radius) $);
  \draw[Line,-latex] (from) --node[fill=white]{\f} (to);
 }

\draw[Line,-latex,shorten <=6.96mm] (2ball-4)--node[fill=white,pos=0.6]{W\textsubscript{hh}}++(0:2.25);
\draw[Line,-latex,shorten <=6.96mm,shorten >=6.96mm] (2ball-1)--++(0:1.2)coordinate(A)--++(90:1)--
node[fill=white,pos=0.50]{W\textsubscript{hh}}++(180:2.5)|-(2ball-1);

\node[single arrow, draw=none, fill=cyan!50, anchor=west,
      minimum width = 20pt, single arrow head extend=3pt,
      minimum height=7mm](AR)at($(A)+(0.1,0)$){\small unfold};
\draw[Line,-latex,shorten >=6.96mm] (AR)--node[fill=white,pos=0.3]{W\textsubscript{hh}}(2ball-2);
\end{tikzpicture}
```
**Recurrent Neural Network Unfolding**: Rnns process sequential data by maintaining a hidden state that incorporates information from previous time steps through this diagram. the unfolded structure explicitly represents the temporal dependencies modeled by the recurrent weights, enabling the network to learn patterns across variable-length sequences.
:::

### Computational Mapping {#sec-dnn-architectures-computational-mapping-0096}

RNN sequential processing creates computational patterns different from both MLPs and CNNs, extending the architectural diversity discussed in @sec-dnn-architectures-architectural-principles-engineering-tradeoffs-89de. This implementation approach shows temporal dependencies translating into specific computational requirements.

As shown in @lst-rnn_layer_step, the `rnn_layer_step` function shows the operation using high-level matrix operations found in deep learning frameworks. It handles a single time step, taking the current input `x_t` and previous hidden state `h_prev`, along with two weight matrices: `W_hh` for hidden-to-hidden connections and `W_xh` for input-to-hidden connections. Through matrix multiplication operations (`matmul`), it merges the previous state and current input to generate the next hidden state.

::: {#lst-rnn_layer_step}
```python
def rnn_layer_step(x_t, h_prev, W_hh, W_xh, b):
    # x_t: input at time t (batch_size × input_dim)
    # h_prev: previous hidden state (batch_size × hidden_dim)
    # W_hh: recurrent weights (hidden_dim × hidden_dim)
    # W_xh: input weights (input_dim × hidden_dim)
    h_t = activation(matmul(h_prev, W_hh) + matmul(x_t, W_xh) + b)
    return h_t
```
**RNN Layer Step**: Neural networks process sequential data through transformations that integrate current inputs and past states.
:::

Understanding RNN system implications requires examining how the elegant mathematical abstraction translates into hardware execution patterns. The simple recurrence relation `h_t = tanh(W_hh h_{t-1} + W_xh x_t + b)` conceals a computational structure that creates unique challenges: sequential dependencies that prevent parallelization, memory access patterns that differ from feedforward networks, and state management requirements that affect system design.

The detailed implementation (@lst-rnn_layer_compute) reveals the computational reality beneath the mathematical abstraction. The nested loop structure exposes how sequential processing creates both limitations and opportunities in system optimization.

::: {#lst-rnn_layer_compute}
```python
def rnn_layer_compute(x_t, h_prev, W_hh, W_xh, b):
    # Initialize next hidden state
    h_t = np.zeros_like(h_prev)

    # Loop 1: Process each sequence in the batch
    for batch in range(batch_size):
        # Loop 2: Compute recurrent contribution
        # (h_prev × W_hh)
        for i in range(hidden_dim):
            for j in range(hidden_dim):
                h_t[batch, i] += h_prev[batch, j] * W_hh[j, i]

        # Loop 3: Compute input contribution (x_t × W_xh)
        for i in range(hidden_dim):
            for j in range(input_dim):
                h_t[batch, i] += x_t[batch, j] * W_xh[j, i]

        # Loop 4: Add bias and apply activation
        for i in range(hidden_dim):
            h_t[batch, i] = activation(h_t[batch, i] + b[i])

    return h_t
```
**Recurrent Layer Computation**: Computes the hidden state at each time step through sequential transformations involving previous states and current inputs.
:::

The nested loops in `rnn_layer_compute` expose the core computational pattern of RNNs (see @lst-rnn_layer_compute). Loop 1 processes each sequence in the batch independently, allowing for batch-level parallelism. Within each batch item, Loop 2 computes how the previous hidden state influences the next state through the recurrent weights `W_hh`. Loop 3 then incorporates new information from the current input through the input weights `W_xh`. Finally, Loop 4 adds biases and applies the activation function to produce the new hidden state.

For a sequence processing task with input dimension 100 and hidden state dimension 128, each time step requires two matrix multiplications: one $128\times 128$ for the recurrent connection and one $100\times 128$ for the input projection. While individual time steps can process in parallel across batch elements, the time steps themselves must process sequentially. This creates a unique computational pattern that systems must handle.

### System Implications {#sec-dnn-architectures-system-implications-ecf5}

Following the analytical framework established for MLPs, RNNs exhibit distinctive patterns in memory requirements, computation needs, and data movement that differ significantly from both dense and spatial processing architectures.

#### Memory Requirements {#sec-dnn-architectures-memory-requirements-eb37}

RNNs require storing two sets of weights (input-to-hidden and hidden-to-hidden) along with the hidden state. For the example with input dimension 100 and hidden state dimension 128, this requires storing 12,800 weights for input projection $(100\times 128)$ and 16,384 weights for recurrent connections $(128\times 128)$. Unlike CNNs where weights are reused across spatial positions, RNN weights are reused across time steps. The system must maintain the hidden state, which constitutes a key factor in memory usage and access patterns.

These memory access patterns create a different profile from MLPs and CNNs. Processors optimize sequential patterns by maintaining weight matrices in cache while streaming through temporal elements. Frameworks optimize temporal processing by batching sequences and managing hidden state storage between time steps. CPUs and GPUs approach this through different strategies; CPUs leverage their cache hierarchy for weight reuse; meanwhile, GPUs use specialized memory architectures designed for maintaining state across sequential operations. The specialized hardware optimizations for sequential processing, including memory banking and pipeline architectures, are detailed in @sec-ai-acceleration.

#### Computation Needs {#sec-dnn-architectures-computation-needs-29be}

The core computation in RNNs involves repeatedly applying weight matrices across time steps. For each time step, we perform two matrix multiplications: one with the input weights and one with the recurrent weights. In our example, processing a single time step requires 12,800 multiply-accumulates for the input projection $(100\times 128)$ and 16,384 multiply-accumulates for the recurrent connection $(128\times 128)$.

This computational pattern differs from both MLPs and CNNs in a key way: while we can parallelize across batch elements, we cannot parallelize across time steps due to the sequential dependency. Each time step must wait for the previous step's hidden state before it can begin computation. This creates a tension between the inherent sequential nature of the algorithm and the desire for parallel execution in modern hardware.

Processors address sequential constraints through specialized approaches. CPUs pipeline operations within time steps while maintaining temporal ordering. GPUs batch multiple sequences together to maintain high throughput despite sequential dependencies. Software frameworks optimize this further by techniques like sequence packing and unrolling computations across multiple time steps when possible, enabling more efficient utilization of parallel processing resources while respecting the sequential constraints inherent in recurrent architectures.

#### Data Movement {#sec-dnn-architectures-data-movement-8591}

The sequential processing in RNNs creates a distinctive data movement pattern that differs from both MLPs and CNNs. While MLPs need each weight only once per forward pass and CNNs reuse weights across spatial positions, RNNs reuse their weights across time steps while requiring careful management of the hidden state data flow.

For our example with a 128-dimensional hidden state, each time step must: load the previous hidden state (128 values), access both weight matrices (29,184 total weights from both input and recurrent connections), and store the new hidden state (128 values). This pattern repeats for every element in the sequence. Unlike CNNs where we can predict and prefetch data based on spatial patterns, RNN data movement is driven by temporal dependencies.

Different architectures handle this sequential data movement through specialized mechanisms. CPUs maintain weight matrices in cache while streaming through sequence elements and managing hidden state updates. GPUs employ memory architectures optimized for maintaining state information across sequential operations while processing multiple sequences in parallel. Deep learning frameworks orchestrate these movements by managing data transfers between time steps and optimizing batch operations.

While RNNs established concepts for sequential processing, their architectural constraints create bottlenecks: sequential dependencies prevent parallelization across time steps, fixed-capacity hidden states create information bottlenecks for long sequences, and temporal proximity assumptions break down when important relationships span distant positions. These limitations motivated the development of attention mechanisms, which eliminate sequential processing constraints through dynamic, content-dependent connectivity. The following section examines how attention mechanisms address each of these RNN limitations while introducing new computational challenges. This extensive treatment reflects attention mechanisms' dominance in modern ML systems and their fundamental reimagining of sequential pattern processing.

## Attention Mechanisms: Dynamic Pattern Processing {#sec-dnn-architectures-attention-mechanisms-dynamic-pattern-processing-566d}

Recurrent Neural Networks successfully introduced memory to handle sequential dependencies, but their fixed sequential processing creates limitations. RNNs process information in temporal order, making it difficult to capture relationships between distant elements and difficult to parallelize computation across sequence positions in standard implementations. More critically, RNNs assume that temporal proximity correlates with importance—that nearby words or time steps are more relevant than distant ones. This assumption breaks down in many real-world scenarios.

Consider the sentence \"The cat, which was sitting by the window overlooking the garden, was sleeping.\" Here, \"cat\" and \"sleeping\" are separated by multiple intervening words, yet they form the core subject-predicate relationship. RNN architectures would process all the intervening elements sequentially, potentially losing this crucial connection in their fixed-capacity hidden state. This limitation revealed the need for architectures that could identify and weight relationships based on content rather than position.

Attention mechanisms emerged as the solution to this architectural constraint [@bahdanau2014neural] by introducing dynamic connectivity patterns that adapt based on input content. Rather than processing elements in predetermined order with fixed relationships, attention mechanisms compute the relevance between all pairs of elements and weight their interactions accordingly. This represents a shift from structural constraints to learned, data-dependent processing patterns.

::: {.callout-definition title="Attention Mechanisms"}

**Attention Mechanisms** refer to neural components that compute _content-dependent relationships_ between sequence elements through _query-key-value operations_, enabling _selective focus_ on relevant information and _long-range dependencies_ without positional constraints.

:::

While attention mechanisms were initially used as components within recurrent architectures, the Transformer architecture [@vaswani2017attention] demonstrated that attention alone could entirely replace sequential processing, creating a new architectural paradigm.

::: {.callout-definition title="Transformers"}

**Transformers** refer to neural architectures based entirely on _attention mechanisms_, using _multi-head self-attention_ and _position encodings_ to process sequences in _parallel_ rather than sequentially, enabling efficient training and inference at scale.

:::

### Pattern Processing Needs {#sec-dnn-architectures-pattern-processing-needs-b5e0}

Dynamic pattern processing addresses scenarios where relationships between elements are not fixed by architecture but instead emerge from content. Language translation exemplifies this challenge: when translating "the bank by the river," understanding "bank" requires attending to "river," but in "the bank approved the loan," the important relationship is with "approved" and "loan." Unlike RNNs that process information sequentially or CNNs that use fixed spatial patterns, an architecture is required that can dynamically determine which relationships matter.

Expanding beyond language, this requirement for dynamic processing appears across many domains. In protein structure prediction, interactions between amino acids depend on their chemical properties and spatial arrangements. In graph analysis, node relationships vary based on graph structure and node features. In document analysis, connections between different sections depend on semantic content rather than just proximity.

Synthesizing these requirements, dynamic processing demands specific capabilities from our processing architecture. The system must compute relationships between all pairs of elements, weigh these relationships based on content, and use these weights to selectively combine information. Unlike previous architectures with fixed connectivity patterns, dynamic processing requires the flexibility to modify its computation graph based on the input itself. These capabilities naturally lead us to the attention mechanism, which serves as the foundation for the Transformer architecture examined in detail in the following sections. @fig-transformer-attention-visualized shows attention enabling this dynamic information flow.

::: {#fig-transformer-attention-visualized fig-env="figure" fig-pos="htb"}
```{.tikz}
\scalebox{0.85}{%
\begin{tikzpicture}[line join=round,font=\small\usefont{T1}{phv}{m}{n}]
\tikzset{
  token/.style={rectangle, text width=24mm,minimum width=25mm, minimum height=6mm, draw=none, fill=cyan!50},
  highlight/.style={fill=violet!30},
  attention/.style={draw=cyan, line width=1.5pt, -{Latex[length=6pt]}},
}
% Left
\foreach \i/\clr/\txt in {
  1/cyan!50/The\_, 2/cyan!50/student\_, 3/cyan!30/didn\_, 4/cyan!30/'\_, 5/cyan!35/t\_,
  6/cyan!50/finish\_, 7/cyan!20/the\_, 8/cyan!40/homework\_, 9/cyan!10/because\_,
  10/cyan!30/they\_, 11/cyan!30/were\_, 12/cyan!20/tired\_
} {
  \node[token,fill=\clr,align=right] (T\i l) at (0,-\i*0.62) {\txt};
}
% Right
\foreach \i/\txt in {
  1/The\_, 2/student\_, 3/didn\_, 4/'\_, 5/t\_,
  6/finish\_, 7/the\_, 8/homework\_, 9/because\_,
  10/they\_, 11/were\_, 12/tired\_
} {
  \node[token,fill=white,align=left] (T\i r) at ($(T\i l) + (6.5cm, 0)$) {\txt};
}

% Highlight "they" on the right side
\path (T10l) ++(65mm, 0) node[token, highlight] (T10r) {they\_};

% Attention with "they"
\draw[attention] (T10r.west) -- (T2l.east); % student
\draw[attention] (T10r.west) -- (T1l.east); % The
\draw[attention] (T10r.west) -- (T6l.east); % finish
\draw[attention, opacity=0.75] (T10r.west) -- (T8l.east); % homework
\foreach \i in {3,4,5,7,9,10,11,12}{
\draw[attention,line width=0.25] (T10r.west) -- (T\i l.east);
}
% Title
\node[above=14pt of current bounding box.north,align=center] (TS){The student didn’t finish the homework because they were tired.};
\node[below=-3pt of TS,anchor=north] {\footnotesize Layer: 4 \quad Head: 2};
\end{tikzpicture}}
```
**Attention Weights**: Transformer attention mechanisms dynamically assess relationships between subwords, assigning higher weights to more relevant connections within a sequence and enabling the model to focus on key information. These learned weights, visualized as connection strengths, reveal how the model attends to different parts of the input when processing language.
:::

### Basic Attention Mechanism {#sec-dnn-architectures-basic-attention-mechanism-9500}

Attention mechanisms represent a shift from fixed architectural connections to dynamic, content-based interactions between sequence elements. This section explores the mathematical foundations of attention, examining how query-key-value operations enable flexible pattern processing. We analyze the computational requirements, memory access patterns, and system implications that make attention both powerful and computationally demanding.

#### Algorithmic Structure {#sec-dnn-architectures-algorithmic-structure-1af4}

Attention mechanisms form the foundation of dynamic pattern processing by computing weighted connections between elements based on their content [@bahdanau2014neural]. This approach processes relationships that are not fixed by architecture but instead emerge from the data itself. At the core of an attention mechanism lies an operation that can be expressed mathematically as:

$$
\text{Attention}(\mathbf{Q}, \mathbf{K}, \mathbf{V}) = \text{softmax}
\left(\frac{\mathbf{Q}\mathbf{K}^T}{\sqrt{d_k}}\right)\mathbf{V}
$$

This equation shows scaled dot-product attention. $\mathbf{Q}$ (queries) and $\mathbf{K}$ (keys) are matrix-multiplied to compute similarity scores, divided by $\sqrt{d_k}$ (key dimension) for numerical stability, then normalized with softmax[^fn-softmax] to get attention weights. These weights are applied to $\mathbf{V}$ (values) to produce the output. The result is a weighted combination where each position receives information from all relevant positions based on content similarity.

[^fn-softmax]: **Softmax Function**: Converts a vector of real numbers into a probability distribution where all values sum to 1. Defined as $\text{softmax}(x_i) = \frac{e^{x_i}}{\sum_j e^{x_j}}$, softmax amplifies differences between inputs (larger values get disproportionately higher probabilities) while ensuring valid attention weights for combining information sources.

In this equation, $\mathbf{Q}$ (queries), $\mathbf{K}$ (keys), and $\mathbf{V}$ (values)[^fn-attention-qkv] represent learned projections of the input. For a sequence of length $N$ with dimension $d$, this operation creates an $N\times N$ attention matrix, determining how each position should attend to all others.

[^fn-attention-qkv]: **Query-Key-Value Attention**: Inspired by information retrieval systems where queries search through keys to retrieve values. In neural attention, queries and keys compute similarity scores (like a search engine matching queries to documents), while values contain the actual information to retrieve—a design that enables flexible, content-based information access.

The attention operation involves several key steps. First, it computes query, key, and value projections for each position in the sequence. Next, it generates an $N\times N$ attention matrix through query-key interactions. These steps are illustrated in @fig-attention. Finally, it uses these attention weights to combine value vectors, producing the output.

::: {#fig-attention fig-env="figure" fig-pos="htb"}
```{.tikz}
\begin{tikzpicture}[line join=round,font=\usefont{T1}{phv}{m}{n}\small]
\tikzset{%
   Line/.style={line width=1.1pt,BrownLine,text=black},
   LineB/.style={line width=1.1pt,cyan!99!red,text=black},
   LineR/.style={line width=1.1pt,red!99!black,text=black,rounded corners=7pt},
   LineBD/.style={line width=2pt,cyan!99!red,text=black,shorten <=126},
   LineBDE/.style={line width=2pt,BrownLine,text=black},
   LineBDD/.style={line width=2pt,GreenD,text=black,shorten <=126},
   LineBDG/.style={line width=2pt,red!99!black,text=black,shorten <=126}
}
  %
  \def\rows{6}
  \def\cols{6}
  \def\r{0.2}
  \def\xgap{0.07}
  \def\ygap{0.07}

\begin{scope}[local bounding box=CEN,shift={($(0,0)+(0,0)$)}]
  %Coordinates of the upper right corner (gradient of the gradient)
  \pgfmathsetmacro\dx{(\cols - 1)*(2*\r + \xgap)}
  \pgfmathsetmacro\dy{(\rows - 1)*(2*\r + \ygap)}
  \pgfmathsetmacro\dmax{max(sqrt(pow(\dx,2) + pow(\dy,2)), 0.0001)}

  \foreach \i [count=\c] in {0,...,\numexpr\rows-1} {
    \foreach \j  in {0,...,\numexpr\cols-1} {
\pgfmathtruncatemacro{\newX}{\j + 1} %
      % Pozicija kruga
      \pgfmathsetmacro\x{\j*(2*\r + \xgap)}
      \pgfmathsetmacro\y{-\i*(2*\r + \ygap)}

      % Udaljenost do gornjeg desnog ugla
      \pgfmathsetmacro\ux{\dx - \x}
      \pgfmathsetmacro\uy{-\dy - \y}
      \pgfmathsetmacro\d{sqrt(pow(\ux,2) + pow(\uy,2))}
      \pgfmathsetmacro\norm{0.9 - min(\d/\dmax,1)} % intenzitet preliva

      % Interpolacija od plave (0,0,1) do svetlo crvene (1,0.6,0.6)
      \pgfmathsetmacro\R{(1 - \norm)*0.2 + \norm*1.0}
      \pgfmathsetmacro\G{(1 - \norm)*0.42 + \norm*0.2}
      \pgfmathsetmacro\B{(1 - \norm)*1.0 + \norm*0.2}

      \definecolor{cellcol}{rgb}{\R,\G,\B}
      \fill[cellcol] (\x,-\y) circle(\r)coordinate(C\c\newX);
    }
  }
\end{scope}

\begin{scope}[on background layer]
  \foreach \i in{1,2,3,4,5,6}{
\draw[Line](C1\i)--(C6\i);
}

 \foreach \i in{1,3,4,5,6}{
\draw[Line](C\i 1)--(C\i 6);
}
\end{scope}
\begin{scope}[local bounding box=QUE,on background layer,shift={(0,0)}]
\node[below=3mm of CEN]{Attention};
  \foreach \i/\tt in{1/to,2/users,3/powers,4/em,5/visualization,6/Data}{
 \ifnum\i=2
    \path (C\i 1) ++(-5,0) node[left]{\tt};
    \draw[LineBD] (C\i 1)--++(-5,0);
  \else
    \draw[LineB]  (C\i 1)--++(-5,0) node[left]{\tt};
    \draw[LineBD] (C\i 1)coordinate(QW\i)--++(-5,0);
  \fi
}
\end{scope}
\node[above left=1mm and 23mm of QW6,cyan!90!black]{Query};

\coordinate(1D)at($(QW1)+(0,-1.25)$);
\coordinate(2D)at($(1D)+(0,-0.45)$);
\coordinate(3D)at($(1D)+(0,-0.9)$);
\coordinate(4D)at($(1D)+(0,-1.35)$);
\coordinate(5D)at($(1D)+(0,-1.8)$);
\coordinate(6D)at($(1D)+(0,-2.25)$);

 \foreach \i/\tt in{6/to,5/users,4/powers,3/em,2/visualization,1/Data}{
    \path (\i D) ++(-5,0) node[left]{\tt};
    \draw[LineBDD]  (\i D)coordinate(QD\i)--++(-5,0);
}
\node[above left=1mm and 23mm of QD1,GreenD]{Value};

\coordinate(1G)at($(QW6)+(0,1.25)$);
\coordinate(2G)at($(1G)+(0,0.45)$);
\coordinate(3G)at($(1G)+(0,0.9)$);
\coordinate(4G)at($(1G)+(0,1.35)$);
\coordinate(5G)at($(1G)+(0,1.8)$);
\coordinate(6G)at($(1G)+(0,2.25)$);
 \foreach \i/\tt in{1/to,2/users,3/powers,4/em,5/visualization,6/Data}{
    \path (\i G) ++(-5,0) node[left]{\tt};
    \draw[LineBDG]  (\i G)coordinate(QG\i)--++(-5,0)coordinate(GO\i);
}
\node[above left=1mm and 23mm of QG6,red!99!black]{Key};

\begin{scope}[on background layer,fill opacity=0.5]
 \foreach \i in{1,2,3,4,5,6}{
\draw[LineR](GO\i)-|(C6\i);
}
\end{scope}

\begin{scope}[fill opacity=0.4]
\fill[fill=blue!10](C66)++(0.25,0)to[out=350,in=0]++(355:4.5)coordinate(O1)--++
(270:1.65)coordinate(O2)to[out=185,in=10]($(C16)+(0.25,0)$)--cycle;
\fill[fill=green!20](O1)to[out=245,in=0](QD1)--
++(180:4.3)--++(270:2.3)to[out=0,in=240](O2)--cycle;
 \end{scope}

\foreach \i[count=\x] in{0.01,0.2,0.4,0.6,0.8,0.99}{
\draw[LineBDE]($(O1)!\i!(O2)$)--++(0.5,0)coordinate(X\x);
}
\node[above=1mm of X1]{Out};
\end{tikzpicture}
```
**Query-Key-Value Interaction**: Transformer attention mechanisms dynamically weigh input sequence elements by computing relationships between queries, keys, and values, enabling the model to focus on relevant information. these projections facilitate the creation of an attention matrix that determines the contribution of each value vector to the final output, effectively capturing contextual dependencies within the sequence. Source: [transformer explainer](HTTPS://poloclub.GitHub.io/transformer-explainer/).
:::

The key is that, unlike the fixed weight matrices found in previous architectures, as shown in @fig-attention-weightcalc, these attention weights are computed dynamically for each input. This allows the model to adapt its processing based on the dynamic content at hand.

::: {#fig-attention-weightcalc fig-env="figure" fig-pos="htb"}
```{.tikz}
 \begin{tikzpicture}[line join=round,font=\small\usefont{T1}{phv}{m}{n}]

\tikzset{
  pics/key/.style = {
    code = {
      % Trougao (strelica)
      \fill[#1] (0,0) -- (1,0.75) -- (0,1.5) -- cycle;

      % Vertikalna linija desno
      \draw[line width=3pt, #1] (1,0.1) -- (1,1.4);
    }
  },
  pics/key/.default = black % podrazumevana boja
}

 \begin{scope}[local bounding box=QKV]
 \def\rows{15}
  \def\cols{25}
  \def\lastrows{7}  % number of rows in last column
  \def\size{0.1}

  \foreach \j in {0,...,\cols} {
    % Last column - number of rows
    \pgfmathsetmacro\maxrows{
      (\j == \cols) ? \lastrows : \rows
    }

    \foreach \i in {0,...,\rows} {
      \ifnum\i>\maxrows
        \relax %
      \else
        % Random blend: 30-60
        \pgfmathsetmacro\blend{rnd*60 + 30}
        %
        \ifnum\j<16
          \fill[blue!\blend!white] (\j*\size, -\i*\size) rectangle ++(\size, -\size);
        \else
          \fill[red!\blend!white] (\j*\size, -\i*\size) rectangle ++(\size, -\size);
        \fi
      \fi
    }
  }
 \coordinate (1topLeft) at (0, 0);
\pgfmathsetmacro\ycoord{-\rows*\size - \size}
\pgfmathsetmacro\xcoord{\cols*\size + \size}
\coordinate (1bottomLeft) at (0,{\ycoord});
\coordinate (1topRight) at ({\xcoord},0);
\coordinate (1bottomRight) at (\xcoord,\ycoord);
\end{scope}

\begin{scope}[local bounding box=LEFT,shift={($(0,0)+(-2.5,0)$)}]
  \def\numrects{5}
  \def\w{0.1}
  \def\h{0.4}
  \def\bluefrac{0.6}
  \foreach \i in {0,...,\numexpr\numrects-1} {
    \pgfmathsetmacro\blend{rnd*80 + 10}
    \pgfmathsetmacro\cutoff{\bluefrac * \numrects}
      \fill[black!\blend!white] (\i*\w, 0) rectangle ++(\w, \h);
  }
\coordinate (0bottomLeft) at (0, 0);
\coordinate (0topLeft) at (0,\h);
\coordinate (0topRight) at ({\numrects*\w},{\h});
\coordinate (0bottomRight) at ({\numrects*\w},0);
%%
\def\vi{7pt}
\node[align=right,anchor=east,left= 1pt of $(0bottomLeft)!0.5!(0topLeft)$](0DA){Data};
\node[align=right,below=\vi of 0DA.south east,anchor=east](0VI){visualization};
\node[align=right,below=\vi of 0VI.south east,anchor=east](0EM){em};
\node[align=right,below=\vi of 0EM.south east,anchor=east](0PO){powers};
\node[align=right,below=\vi of 0PO.south east,anchor=east](0US){users};
\node[align=right,below=\vi of 0US.south east,anchor=east](0TO){to};
\end{scope}

\begin{scope}[local bounding box=BIAS,shift={($(QKV)+(2.75,2.5)$)}]
  \def\numrects{27}
  \def\w{0.3}
  \def\h{0.1}

  \def\bluefrac{0.6}
    \foreach \i in {0,...,\numexpr\numrects-1} {
     % colors (4% do 60%)
    \pgfmathsetmacro\blend{rnd*60 + 4}
\fill[black!\blend!white] (0, -\i*\h) rectangle ++(\w, -\h);
  }
\coordinate (2topLeft) at (0, 0);
\coordinate (2bottomRight) at ({\w}, {-\numrects*\h});
\coordinate (2bottomLeft) at ({0}, {-\numrects*\h});
\coordinate (2topRight) at ({\w}, 0);
\end{scope}

\begin{scope}[local bounding box=RIGHT,shift={($(0,0)+(7.5,0)$)}]
  \def\numrects{17}
  \def\w{0.1}
  \def\h{0.4}
  \def\bluefrac{0.6}
  \foreach \i in {0,...,\numexpr\numrects-1} {
    \pgfmathsetmacro\blend{rnd*90 + 10}
    \pgfmathsetmacro\cutoff{\bluefrac * \numrects}
    \ifnum\i<\cutoff
      \fill[blue!\blend!white] (\i*\w, 0) rectangle ++(\w, \h);
    \else
      \fill[red!\blend!white] (\i*\w, 0) rectangle ++(\w, \h);
    \fi
  }
\coordinate (3bottomLeft) at (0, 0);
\coordinate (3topLeft) at (0,\h);
\coordinate (3topRight) at ({\numrects*\w},{\h});
\coordinate (3bottomRight) at ({\numrects*\w},0);
%%
\def\vi{7pt}
\node[align=right,anchor=east,left= 1pt of $(3bottomLeft)!0.5!(3topLeft)$](DA){Data};
\node[align=right,below=\vi of DA.south east,anchor=east](VI){visualization};
\node[align=right,below=\vi of VI.south east,anchor=east](EM){em};
\node[align=right,below=\vi of EM.south east,anchor=east](PO){powers};
\node[align=right,below=\vi of PO.south east,anchor=east](US){users};
\node[align=right,below=\vi of US.south east,anchor=east](TO){to};
\end{scope}
%%%%%
\node[align=center,above= 4pt of $(2topLeft)!0.5!(2topRight)$](2BI){Bias};
\node[above=5mm of 3topRight](Q1){Q $\cdot$ K $\cdot$ V};
\path[red](Q1)-|coordinate(T1)($(1topLeft)!0.5!(1topRight)$);;
\node[]at(T1){Q $\cdot$ K $\cdot$ V Weights};
\path[red](Q1)-|coordinate(T0)(0topRight);
\node[]at(T0){Embedding};
%%
\node[below=25mm of 3bottomRight](Q2){matrix(6,2304)};
\path[red](Q2)-|coordinate(TT1)($(1bottomLeft)!0.5!(1bottomRight)$);
\node[]at(TT1){matrix(768,2304)};
\path[red](Q2)-|coordinate(TT2)(0bottomRight);
\node[]at(TT2){matrix(6,768)};
\path[red](Q2)-|coordinate(TT3)(2bottomRight);
\node[]at(TT3){vector(2304)};

\node[left= 20pt of $(1bottomLeft)!0.5!(1topLeft)$](PLUS){\LARGE  $\times$};
\node[right= 20pt of $(1bottomRight)!0.5!(1topRight)$](PLUS){\LARGE +};
\node[right= 20pt of PLUS](JED){\LARGE=};
%%
\scoped[on background layer]
\node[outer sep=0pt,draw=BackLine,inner xsep=3mm,inner ysep=27,yshift=-8mm,
           fill=BackColor!20,fit=(0VI)(2BI)(Q2),line width=1pt](BB1){};
\node[above=7pt of  BB1.south,anchor=south]{$\displaystyle\sum\limits_{d=1}^{768}E_{id}\cdot W_{dj}+b_j=GKV_{ij}$};
%
\draw[outer sep=0pt,line width=1pt,draw=BackLine,fill=BackColor!45](BB1.north west)rectangle($(BB1.north east)+(0,1)$);
\coordinate (B) at ($(BB1.north west)+(0,1)$);
\coordinate (C) at ($(BB1.north east)+(0,1)$);
\node[right=5pt of $(BB1.north west)!0.5!(B)$]{\textbf{QKV Calculation}};

 \pic[shift={(-1,0)},scale=0.4,rotate=0] at ($(BB1.north east)!0.2!(C)$) {key=OliveLine!90!black};
\end{tikzpicture}
```
**Dynamic Attention Weights**: Transformer models calculate attention weights dynamically based on the relationships between query, key, and value vectors, allowing the model to focus on relevant parts of the input sequence for each processing step. this contrasts with fixed-weight architectures and enables adaptive pattern processing for handling variable-length inputs and complex dependencies. Source: [transformer explainer](HTTPS://poloclub.GitHub.io/transformer-explainer/).
:::

#### Computational Mapping {#sec-dnn-architectures-computational-mapping-6c7e}

Attention mechanisms create computational patterns that differ significantly from previous architectures. The implementation approach shown in @lst-attention_layer_compute shows dynamic connectivity translating into specific computational requirements.

::: {#lst-attention_layer_compute}
```{.python}
def attention_layer_matrix(Q, K, V):
    # Q, K, V: (batch_size × seq_len × d_model)
    scores = matmul(Q, K.transpose(-2, -1)) / sqrt(
        d_k
    )  # Compute attention scores
    weights = softmax(scores)  # Normalize scores
    output = matmul(weights, V)  # Combine values
    return output


# Core computational pattern
def attention_layer_compute(Q, K, V):
    # Initialize outputs
    scores = np.zeros((batch_size, seq_len, seq_len))
    outputs = np.zeros_like(V)

    # Loop 1: Process each sequence in batch
    for b in range(batch_size):
        # Loop 2: Compute attention for each query position
        for i in range(seq_len):
            # Loop 3: Compare with each key position
            for j in range(seq_len):
                # Compute attention score
                for d in range(d_model):
                    scores[b, i, j] += Q[b, i, d] * K[b, j, d]
                scores[b, i, j] /= sqrt(d_k)

        # Apply softmax to scores
        for i in range(seq_len):
            scores[b, i] = softmax(scores[b, i])

        # Loop 4: Combine values using attention weights
        for i in range(seq_len):
            for j in range(seq_len):
                for d in range(d_model):
                    outputs[b, i, d] += scores[b, i, j] * V[b, j, d]

    return outputs
```
**Attention Mechanism**: Transformer models compute attention through query-key-value interactions, enabling dynamic focus across input sequences for improved language understanding.
:::

The translation from attention's mathematical elegance to hardware execution reveals the computational price of dynamic connectivity. While the attention equation `Attention(Q,K,V) = softmax(QK^T/√d_k)V` appears as a straightforward matrix operation, the physical implementation requires orchestrating quadratic numbers of pairwise computations that create different system demands than previous architectures.

The nested loops in `attention_layer_compute` expose attention's true computational signature (see @lst-attention_layer_compute). The first loop processes each sequence in the batch independently. The second and third loops compute attention scores between all pairs of positions, creating the quadratic computation pattern that makes attention both powerful and computationally demanding. The fourth loop uses these attention weights to combine values from all positions, completing the dynamic connectivity pattern that defines attention mechanisms.

#### System Implications {#sec-dnn-architectures-system-implications-b5aa}

Attention mechanisms exhibit distinctive system-level patterns that differ from previous architectures through their dynamic connectivity requirements.

##### Memory Requirements {#sec-dnn-architectures-memory-requirements-3dc1}

In terms of memory requirements, attention mechanisms necessitate storage for attention weights, key-query-value projections, and intermediate feature representations. For a sequence length $N$ and dimension d, each attention layer must store an $N\times N$ attention weight matrix for each sequence in the batch, three sets of projection matrices for queries, keys, and values (each sized $d\times d$), and input and output feature maps of size $N\times d$. The dynamic generation of attention weights for every input creates a memory access pattern where intermediate attention weights become a significant factor in memory usage.

##### Computation Needs {#sec-dnn-architectures-computation-needs-a41e}

Computation needs in attention mechanisms center around two main phases: generating attention weights and applying them to values. For each attention layer, the system performs many multiply-accumulate operations across multiple computational stages. The query-key interactions alone require $N\times N\times d$ multiply-accumulates, with an equal number needed for applying attention weights to values. Additional computations are required for the projection matrices and softmax operations. This computational pattern differs from previous architectures due to its quadratic scaling with sequence length and the need to perform fresh computations for each input.

##### Data Movement {#sec-dnn-architectures-data-movement-12b1}

Data movement in attention mechanisms presents unique challenges. Each attention operation involves projecting and moving query, key, and value vectors for each position, storing and accessing the full attention weight matrix, and coordinating the movement of value vectors during the weighted combination phase. This creates a data movement pattern where intermediate attention weights become a major factor in system bandwidth requirements. Unlike the more predictable access patterns of CNNs or the sequential access of RNNs, attention operations require frequent movement of dynamically computed weights across the memory hierarchy.

These distinctive characteristics of attention mechanisms in terms of memory, computation, and data movement have significant implications for system design and optimization, setting the stage for the development of more advanced architectures like Transformers.

### Transformers: Attention-Only Architecture {#sec-dnn-architectures-transformers-attentiononly-architecture-c4f0}

While attention mechanisms introduced the concept of dynamic pattern processing, they were initially applied as additions to existing architectures, particularly RNNs for sequence-to-sequence tasks. This hybrid approach still suffered from the fundamental limitations of recurrent architectures: sequential processing constraints that prevented efficient parallelization and difficulties with very long sequences. The breakthrough insight was recognizing that attention mechanisms alone could replace both convolutional and recurrent processing entirely.

Transformers, introduced in the landmark \"Attention is All You Need\" paper[^fn-attention-is-all-you-need] by @vaswani2017attention, embody a revolutionary inductive bias: **they assume no prior structure but allow the model to learn all pairwise relationships dynamically based on content**. This architectural assumption represents the culmination of the architectural evolution detailed in @sec-dnn-architectures-architectural-principles-engineering-tradeoffs-89de by eliminating all structural constraints in favor of pure content-dependent processing. Rather than adding attention to RNNs, Transformers built the entire architecture around attention mechanisms, introducing self-attention as the primary computational pattern. This architectural decision traded the parameter efficiency of CNNs and the sequential coherence of RNNs for maximum flexibility and parallelizability.

[^fn-attention-is-all-you-need]: **"Attention is All You Need"**: This 2017 paper by Google researchers eliminated recurrence entirely, showing that attention mechanisms alone could achieve state-of-the-art results. The title itself became a rallying cry, and within 5 years, transformer-based models achieved breakthrough performance in language (GPT, BERT), vision (ViT), and beyond [@radford2018improving; @devlin2018bert; @dosovitskiy2021image]. This paper marked a historical turning point in deep learning, demonstrating that the sequential processing that defined RNNs and LSTMs was no longer necessary; attention mechanisms could capture both short and long-range dependencies through parallel computation. While the basic attention mechanism allows for content-based weighting of information from a source sequence, Transformers extend this idea by applying attention within a single sequence, enabling each element to attend to all other elements including itself.

This represents the final step in our architectural journey: from MLPs that connected everything to everything, to CNNs that connected locally, to RNNs that connected sequentially, to Transformers that connect dynamically based on learned content relationships. Each evolution sacrificed constraints for capabilities, with Transformers achieving maximum expressivity at the computational cost established in @sec-dnn-architectures-architectural-principles-engineering-tradeoffs-89de.

#### Algorithmic Structure {#sec-dnn-architectures-algorithmic-structure-9d4b}

The key innovation in Transformers lies in their use of self-attention layers. In the self-attention mechanism used by Transformers, the Query, Key, and Value vectors are all derived from the same input sequence. This is the key distinction from earlier attention mechanisms where the query might come from a decoder while the keys and values came from an encoder. By making all components self-referential, self-attention allows the model to weigh the importance of different positions within the same sequence when encoding each position. For instance, in processing the sentence "The animal didn't cross the street because it was too wide," self-attention allows the model to link "it" with "street," capturing long-range dependencies that are challenging for traditional sequential models.

The self-attention mechanism can be expressed mathematically in a form similar to the basic attention mechanism:
$$
\text{SelfAttention}(\mathbf{X}) = \text{softmax}
\left(\frac{\mathbf{XW_Q}(\mathbf{XW_K})^T}{\sqrt{d_k}}\right)\mathbf{XW_V}
$$

Here, $\mathbf{X}$ is the input sequence, and $\mathbf{W_Q}$, $\mathbf{W_K}$, and $\mathbf{W_V}$ are learned weight matrices for queries, keys, and values respectively. This formulation highlights how self-attention derives all its components from the same input, creating a dynamic, content-dependent processing pattern.

Building on this foundation, Transformers employ multi-head attention, which extends the self-attention mechanism by running multiple attention functions in parallel. Each "head" involves a separate set of query/key/value projections that can focus on different aspects of the input, allowing the model to jointly attend to information from different representation subspaces. This multi-head structure provides the model with a richer representational capability, enabling it to capture various types of relationships within the data simultaneously.

The mathematical formulation for multi-head attention is:
$$
\text{MultiHead}(\mathbf{Q}, \mathbf{K}, \mathbf{V}) = \text{Concat}(\text{head}_1, \ldots, \text{head}_h)\mathbf{W}^O
$$
where each attention head is computed as:
$$
\text{head}_i = \text{Attention}(\mathbf{Q}\mathbf{W}_i^Q, \mathbf{K}\mathbf{W}_i^K, \mathbf{V}\mathbf{W}_i^V)
$$

A critical component in both self-attention and multi-head attention is the scaling factor $\sqrt{d_k}$, which serves an important mathematical purpose. This factor prevents the dot products from growing too large, which would push the softmax function into regions with extremely small gradients. For queries and keys of dimension $d_k$, their dot product has variance $d_k$, so dividing by $\sqrt{d_k}$ normalizes the variance to 1, maintaining stable gradients and enabling effective learning.[^fn-attention-scaling]

[^fn-attention-scaling]: **Attention Scaling**: Division by √d_k prevents softmax saturation in attention computation. Without scaling, dot products grow with dimension (E[q·k] = 0, Var[q·k] = d_k), pushing softmax toward one-hot extremes with vanishing gradients. This mathematical insight from Vaswani et al. (2017) enables stable training of transformers from BERT-base (d_k=64) to GPT-4 (d_k=128+).

Beyond the mathematical mechanics, attention mechanisms can be understood conceptually as implementing a form of content-addressable memory system. Like hash tables that retrieve values based on key matching, attention computes similarity between a query and all available keys, then retrieves a weighted combination of corresponding values. The dot product similarity `Q·K` functions like a hash function that measures how well each key matches the query. The softmax normalization ensures the weights sum to 1, implementing a probabilistic retrieval mechanism. This connection explains why attention proves effective for tasks requiring flexible information retrieval—it provides a differentiable approximation to database lookup operations.

From an information-theoretic perspective, attention mechanisms implement optimal information aggregation under uncertainty. The attention weights represent uncertainty about which parts of the input contain relevant information for the current processing step. The softmax operation implements a maximum entropy principle: among all possible ways to distribute attention across input positions, softmax selects the distribution with maximum entropy subject to the constraint that similarity scores determine relative importance [@cover2006elements].

#### Efficiency and Optimization {#sec-dnn-architectures-efficiency-optimization-94d7}

Attention mechanisms are highly redundant, with many heads learning similar patterns. Head pruning and low-rank attention factorization can reduce computation by 50-80% with careful implementation. Analysis of large Transformer models reveals that most attention heads fall into a few common patterns (positional, syntactic, semantic), suggesting that explicit architectural specialization could replace learned redundancy.

Attention operations are particularly sensitive to quantization due to the softmax operation and the quadratic number of attention scores. Separate quantization schemes for Q, K, V projections and careful handling of softmax operations are required for stable quantization. Post-training INT8 quantization typically achieves 2-3% accuracy loss, while INT4 quantization requires more sophisticated quantization-aware training approaches.

The quadratic scaling with sequence length creates efficiency limitations. Sparse attention patterns (such as local windows, strided patterns, or learned sparsity) can reduce complexity from O(n²) to O(n log n) or O(n) while maintaining most modeling capability. Linear attention approximations trade some expressive power for linear scaling, enabling processing of much longer sequences on limited hardware.

This information-theoretic interpretation reveals why attention is so effective for selective processing. The mechanism automatically balances two competing objectives: focusing on the most relevant information (minimizing entropy) while maintaining sufficient breadth to avoid missing important details (maximizing entropy). The attention pattern emerges as the optimal trade-off between these objectives, explaining why transformers can effectively handle long sequences and complex dependencies.

Self-attention learns dynamic activation patterns across the input sequence. Unlike CNNs which apply fixed filters or RNNs which use fixed recurrence patterns, attention learns which elements should activate together based on their content. This creates a form of adaptive connectivity where the effective network topology changes for each input. Recent research has shown that attention heads in trained models often specialize in detecting specific linguistic or semantic patterns [@clark2019what], suggesting that the mechanism naturally discovers interpretable structural regularities in data.

The Transformer architecture leverages this self-attention mechanism within a broader structure that typically includes feed-forward layers, layer normalization, and residual connections (see @fig-transformer). This combination allows Transformers to process input sequences in parallel, capturing complex dependencies without the need for sequential computation. As a result, Transformers have demonstrated significant effectiveness across a wide range of tasks, from natural language processing to computer vision, transforming deep learning architectures across domains.

::: {#fig-transformer fig-env="figure" fig-pos="htb"}
```{.tikz}
\scalebox{0.7}{
\begin{tikzpicture}[font=\small\usefont{T1}{phv}{m}{n}]
\tikzset{%
    Line/.style={line width=1.0pt,black!50,text=black}
}
\draw[fill=BackColor!50, line width=0.5pt,draw=BackLine] (-0.975, 6.455) -- (2.725, 6.455) -- (2.725, 1.305) -- (-0.975, 1.305) -- cycle;
\draw[fill=BackColor!50, line width=0.5pt,draw=BackLine] (3.775, 9.405) -- (7.475, 9.405) -- (7.475, 1.305) -- (3.775, 1.305) -- cycle;
\draw[line width=0.5pt, fill=RedL] (0, 0) -- (2.5, 0) -- (2.5, -0.9) -- (0, -0.9) -- cycle;
\node[text width=2.5cm, align=center] at (1.25,-0.45) {Input \vspace{-0.05cm} \linebreak Embedding};
\draw[line width=0.5pt, fill=RedL] (4, 0) -- (6.5, 0) -- (6.5, -0.9) -- (4, -0.9) -- cycle;
\node[text width=2.5cm, align=center] at (5.25,-0.45) {Output \vspace{-0.05cm} \linebreak Embedding};
\draw[line width=0.5pt, fill=BrownL] (0.0, 3.68) -- (2.5, 3.68) -- (2.5, 3.18) -- (0, 3.18) -- cycle;
\node[text width=2.5cm, align=center] at (1.25,3.43) {Add \& Norm};
\draw[line width=0.5pt, fill=GreenL] (0.0, 3.03) -- (2.5, 3.03) -- (2.50, 2.13) -- (0, 2.13) -- cycle;
\node[text width=2.5cm, align=center] at (1.25,2.58) {Multi-Head \vspace{-0.05cm} \linebreak Attention};
\draw[line width=0.5pt] (1.25, 3.03) -- (1.25, 3.18);
\draw[line width=0.5pt, fill=BrownL] (4, 6.63) -- (6.5, 6.63) -- (6.50, 6.13) -- (4, 6.13) -- cycle;
\node[text width=2.5cm, align=center] at (5.25,6.38) {Add \& Norm};
\draw[line width=0.5pt, fill=GreenL] (4, 5.98) -- (6.5, 5.98) -- (6.5, 5.08) -- (4, 5.08) -- cycle;
\node[text width=2.5cm, align=center] at (5.25,5.53) {Multi-Head \vspace{-0.05cm} \linebreak Attention};
\draw[line width=0.5pt] (5.25, 5.98) -- (5.25, 6.13);
\draw[line width=0.5pt, fill=BrownL] (4, 4.080) -- (6.5, 4.08) -- (6.5, 3.58) -- (4, 3.58) -- cycle;
\node[text width=2.5cm, align=center] at (5.25,3.83) {Add \& Norm};
\draw[line width=0.5pt, fill=GreenL] (4, 3.43) -- (6.5, 3.43) -- (6.5, 2.13) -- (4, 2.13) -- cycle;
\node[text width=2.5cm, align=center] at (5.25,2.78) {Masked \vspace{-0.05cm} \linebreak Multi-Head \vspace{-0.05cm} \linebreak Attention};
\draw[line width=0.5pt] (5.25, 3.43) -- (5.25, 3.58);
\draw[line width=0.5pt, fill=BrownL] (0, 6.23) -- (2.5, 6.23) -- (2.5, 5.73) -- (0, 5.73) -- cycle;
\node[text width=2.5cm, align=center] at (1.25,5.98) {Add \& Norm};
\draw[line width=0.5pt, fill=BlueL] (0, 5.58) -- (2.5, 5.58) -- (2.5, 4.68) -- (0, 4.68) -- cycle;
\node[text width=2.5cm, align=center] at (1.25,5.13) {Feed \vspace{-0.05cm} \linebreak Forward};
\draw[line width=0.5pt] (1.25, 5.58) -- (1.25, 5.73);
\draw[line width=0.5pt, fill=BrownL] (4, 9.18) -- (6.5, 9.18) -- (6.5, 8.68) -- (4, 8.68) -- cycle;
\node[text width=2.5cm, align=center] at (5.25,8.93) {Add \& Norm};
\draw[line width=0.5pt, fill=BlueL] (4, 8.53) -- (6.5, 8.53) -- (6.5, 7.63) -- (4, 7.63) -- cycle;
\node[text width=2.5cm, align=center] at (5.250,8.080) {Feed \vspace{-0.05cm} \linebreak Forward};
\draw[line width=0.5pt] (5.25, 8.53) -- (5.25, 8.68);
\draw[line width=0.5pt, fill=cyan!50] (4, 10.28) -- (6.5, 10.28) -- (6.5, 9.78) -- (4, 9.78) -- cycle;
\node[text width=2.5cm, align=center] at (5.250,10.030) {Linear};
\draw[line width=0.5pt, fill=OrangeL] (4, 11.38) -- (6.5, 11.38) -- (6.5, 10.88) -- (4, 10.88) -- cycle;
\node[text width=2.5cm, align=center] at (5.25,11.13) {Softmax};
\draw[line width=0.75pt,fill=green!40] (1.25, 0.6) circle (0.2);
\draw[line width=1.25pt] (1.41, 0.6) -- (1.09, 0.6);
\draw[line width=1.25pt] (1.25, 0.76) -- (1.25, 0.44);
\draw[line width=0.75pt,fill=green!40] (5.25, 0.6) circle (0.2);
\draw[line width=1.25pt] (5.41, 0.6) -- (5.09, 0.6);
\draw[line width=1.25pt] (5.25, 0.76) -- (5.25, 0.44);
\draw[line width=0.75pt,fill=green!20] (0.35, 0.6) circle (0.40);
\draw[line width=1.25pt] (-0.03, 0.6) -- (-0.014490, 0.629156) -- (0.00102, 0.657833) -- (0.016531, 0.685561) -- (0.032041, 0.711884) -- (0.047551, 0.736369) -- (0.063061, 0.758616) -- (0.078571, 0.778258) -- (0.094082, 0.794973) -- (0.109592, 0.808486) -- (0.125102, 0.818576) -- (0.140612, 0.825077) -- (0.156122, 0.827883) -- (0.171633, 0.826946) -- (0.187143, 0.822284) -- (0.202653, 0.813971) -- (0.218163, 0.802145) -- (0.233673, 0.786999) -- (0.249184, 0.768783) -- (0.264694, 0.747796) -- (0.280204, 0.724382) -- (0.295714, 0.698925) -- (0.311224, 0.671845) -- (0.326735, 0.643584) -- (0.342245, 0.614608) -- (0.357755, 0.585392) -- (0.373265, 0.556416) -- (0.388776, 0.528155) -- (0.404286, 0.501075) -- (0.419796, 0.475618) -- (0.435306, 0.452204) -- (0.450816, 0.431217) -- (0.466327, 0.413001) -- (0.481837, 0.397855) -- (0.497347, 0.386029) -- (0.512857, 0.377716) -- (0.528367, 0.373054) -- (0.543878, 0.372117) -- (0.559388, 0.374923) -- (0.574898, 0.381424) -- (0.590408, 0.391514) -- (0.605918, 0.405027) -- (0.621429, 0.421742) -- (0.636939, 0.441384) -- (0.652449, 0.463631) -- (0.667959, 0.488116) -- (0.683469, 0.514439) -- (0.698980, 0.542167) -- (0.714490, 0.570844) -- (0.730, 0.60);
\draw[line width=0.75pt,fill=green!20] (6.15, 0.6) circle (0.4);
\draw[line width=1.25pt] (5.77, 0.6) -- (5.78551, 0.629156) -- (5.801020, 0.657833) -- (5.816531, 0.685561) -- (5.832041, 0.711884) -- (5.847551, 0.736369) -- (5.863061, 0.758616) -- (5.878571, 0.778258) -- (5.894082, 0.794973) -- (5.909592, 0.808486) -- (5.925102, 0.818576) -- (5.940612, 0.825077) -- (5.956122, 0.827883) -- (5.971633, 0.826946) -- (5.987143, 0.822284) -- (6.002653, 0.813971) -- (6.018163, 0.802145) -- (6.033673, 0.786999) -- (6.049184, 0.768783) -- (6.064694, 0.747796) -- (6.080204, 0.724382) -- (6.095714, 0.698925) -- (6.111224, 0.671845) -- (6.126735, 0.643584) -- (6.142245, 0.614608) -- (6.157755, 0.585392) -- (6.173265, 0.556416) -- (6.188776, 0.528155) -- (6.204286, 0.501075) -- (6.219796, 0.475618) -- (6.235306, 0.452204) -- (6.250816, 0.431217) -- (6.266327, 0.413001) -- (6.281837, 0.397855) -- (6.297347, 0.386029) -- (6.312857, 0.377716) -- (6.328367, 0.373054) -- (6.343878, 0.372117) -- (6.359388, 0.374923) -- (6.374898, 0.381424) -- (6.390408, 0.391514) -- (6.405918, 0.405027) -- (6.421429, 0.421742) -- (6.436939, 0.441384) -- (6.452449, 0.463631) -- (6.467959, 0.488116) -- (6.483469, 0.514439) -- (6.498980, 0.542167) -- (6.514490, 0.570844) -- (6.530, 0.60);
\draw[Line, -latex] (1.25, 3.68) -- (1.25, 4.68);
\draw[Line, -latex] (5.25, 6.63) -- (5.25, 7.63);
\draw[Line, -latex] (5.25, 9.18) -- (5.25, 9.78);
\draw[Line, -latex] (5.25, 10.28) -- (5.25, 10.88);
\draw[Line, -latex] (1.25, 0) -- (1.25, 0.4);
\draw[Line, -latex] (1.25, 0.8) -- (1.25, 2.13);
\draw[Line, -latex] (5.25, 0.8) -- (5.25, 2.13);
\draw[Line, -latex] (5.25, 0) -- (5.25, 0.4);
\draw[Line] (0.75, 0.6) -- (1.05, 0.6);
\draw[Line] (5.45, 0.6) -- (5.75, 0.6);
\draw[-latex, Line] (1.25, 4.08) -- (-0.75, 4.08) -- (-0.75, 5.98) -- (0, 5.98);
\draw[-latex, Line] (1.25, 1.53) -- (-0.75, 1.53) -- (-0.75, 3.43) -- (0, 3.43);
\draw[-latex, Line] (5.25, 1.53) -- (7.25, 1.53) -- (7.25, 3.83) -- (6.5, 3.83);
\draw[-latex, Line] (5.25, 4.48) -- (7.25, 4.48) -- (7.25, 6.38) -- (6.5, 6.38);
\draw[-latex, Line] (5.25, 7.03) -- (7.25, 7.03) -- (7.25, 8.93) -- (6.5, 8.93);
\draw[-latex, Line] (1.25, 1.73) -- (0.3125, 1.73) -- (0.3125, 2.13);
\draw[-latex,Line] (1.25, 1.73) -- (2.1875, 1.73) -- (2.1875, 2.13);
\draw[-latex, Line] (5.25, 1.73) -- (4.3125, 1.73) -- (4.3125, 2.13);
\draw[-latex,Line] (5.25, 1.73) -- (6.1875, 1.73) -- (6.1875, 2.13);
\draw[-latex,Line] (1.25, 6.23) -- (1.25, 7.23) -- (3.25, 7.23) -- (3.25, 4.68) -- (4.3125, 4.68) -- (4.3125, 5.08);
\draw[-latex, Line] (1.25, 6.23) -- (1.25, 7.23) -- (3.25, 7.23) -- (3.25, 4.68) -- (5.25, 4.68) -- (5.25, 5.08);
\draw[-latex,Line] (5.25, 4.08) -- (5.25, 4.68) -- (6.1875, 4.68) -- (6.1875, 5.08);
\draw[Line, -latex] (1.25, -1.5) -- (1.25, -0.9);
\draw[Line, -latex] (5.25, -1.5) -- (5.25, -0.9);
\draw[Line, -latex] (5.25, 11.38) -- (5.25, 11.80);
\node[anchor=north, align=center] at (1.25,-1.5) {Inputs};
\node[anchor=north, align=center] at (5.25,-1.5) {Outputs (shifted right)};
\node[anchor=south, align=center] at (5.25,11.70) {Output Probabilities};
\node[anchor=east] at (-1.175,3.88) {N$\times$};
\node[anchor=west] at (7.675,5.355) {N$\times$};
\node[align=center] at (-0.95,0.6) {Positional \\ Encoding};
\node[text width=2cm, anchor=west] at (6.75,0.6) {Positional \vspace{-0.05cm} \linebreak Encoding};
\end{tikzpicture}}
```
**Attention Head**: Neural networks compute attention through query-key-value interactions, enabling dynamic focus across subwords for improved sentence understanding. Source: Attention Is All You Need.
:::

#### Computational Mapping {#sec-dnn-architectures-computational-mapping-7fe9}

While Transformer self-attention builds upon the basic attention mechanism, it introduces distinct computational patterns that set it apart. To understand these patterns, we must examine the typical implementation of self-attention in Transformers (see @lst-self_attention_layer):

::: {#lst-self_attention_layer}
```{.python}
def self_attention_layer(X, W_Q, W_K, W_V, d_k):
    # X: input tensor (batch_size × seq_len × d_model)
    # W_Q, W_K, W_V: weight matrices (d_model × d_k)

    Q = matmul(X, W_Q)
    K = matmul(X, W_K)
    V = matmul(X, W_V)

    scores = matmul(Q, K.transpose(-2, -1)) / sqrt(d_k)
    attention_weights = softmax(scores, dim=-1)
    output = matmul(attention_weights, V)

    return output


def multi_head_attention(X, W_Q, W_K, W_V, W_O, num_heads, d_k):
    outputs = []
    for i in range(num_heads):
        head_output = self_attention_layer(
            X, W_Q[i], W_K[i], W_V[i], d_k
        )
        outputs.append(head_output)

    concat_output = torch.cat(outputs, dim=-1)
    final_output = matmul(concat_output, W_O)

    return final_output
```
**Self-Attention Mechanism**: Transformer models compute attention through query-key-value interactions, enabling dynamic focus across input sequences for improved language understanding.
:::

#### System Implications {#sec-dnn-architectures-system-implications-76dd}

This implementation reveals key computational characteristics that apply to basic attention mechanisms, with Transformer self-attention representing a specific case. First, self-attention enables parallel processing across all positions in the sequence. This is evident in the matrix multiplications that compute `Q`, `K`, and `V` simultaneously for all positions. Unlike recurrent architectures that process inputs sequentially, this parallel nature allows for more efficient computation, especially on modern hardware designed for parallel operations.

Second, the attention score computation results in a matrix of size `(seq_len × seq_len)`, leading to quadratic complexity with respect to sequence length. This quadratic relationship becomes a significant computational bottleneck when processing long sequences, a challenge that has spurred research into more efficient attention mechanisms.

Third, the multi-head attention mechanism effectively runs multiple self-attention operations in parallel, each with its own set of learned projections. While this increases the computational load linearly with the number of heads, it allows the model to capture different types of relationships within the same input, enhancing the model's representational power.

Fourth, the core computations in self-attention are dominated by large matrix multiplications. For a sequence of length $N$ and embedding dimension $d$, the main operations involve matrices of sizes $(N\times d)$, $(d\times d)$, and $(N\times N)$. These intensive matrix operations are well-suited for acceleration on specialized hardware like GPUs, but they also contribute significantly to the overall computational cost of the model.

Finally, self-attention generates memory-intensive intermediate results. The attention weights matrix $(N\times N)$ and the intermediate results for each attention head create large memory requirements, especially for long sequences. This can pose challenges for deployment on memory-constrained devices and necessitates careful memory management in implementations.

These computational patterns create a unique profile for Transformer self-attention, distinct from previous architectures. The parallel nature of the computations makes Transformers well-suited for modern parallel processing hardware, but the quadratic complexity with sequence length poses challenges for processing long sequences. As a result, much research has focused on developing optimization techniques, such as sparse attention patterns or low-rank approximations, to address these challenges. Each of these optimizations presents its own trade-offs between computational efficiency and model expressiveness, a balance that must be carefully considered in practical applications.

This examination of four distinct architectural families reveals both their individual characteristics and their collective evolution. Rather than viewing these architectures in isolation, a deeper understanding emerges when we consider how they relate to each other and build upon shared foundations.

## Architectural Building Blocks {#sec-dnn-architectures-architectural-building-blocks-a575}

Having examined four major architectural families—MLPs, CNNs, RNNs, and Transformers—each with distinct computational characteristics and system implications, a unifying perspective emerges. Deep learning architectures, while presented as distinct approaches in previous sections, are better understood as compositions of building blocks that evolved over time. Like complex LEGO structures built from basic bricks, modern neural networks combine and iterate on core computational patterns that emerged through decades of research [@lecun2015deep]. Each architectural innovation introduced new building blocks while discovering novel applications of existing ones.

These building blocks and their evolution illuminate modern architectural design. The simple perceptron [@rosenblatt1958perceptron] evolved into multi-layer networks [@rumelhart1986learning], which subsequently spawned specialized patterns for spatial and sequential processing. Each advancement preserved useful elements from predecessors while introducing new computational primitives. Contemporary architectures, such as Transformers, represent carefully engineered combinations of these building blocks.

This progression reveals both the evolution of neural networks and the discovery and refinement of core computational patterns that remain relevant. Building on the architectural progression outlined in @sec-dnn-architectures-architectural-principles-engineering-tradeoffs-89de, each new architecture introduces distinct computational demands and system-level challenges.

@tbl-dl-evolution summarizes this evolution, highlighting the key primitives and system focus for each era of deep learning development. This table captures the major shifts in deep learning architecture design and corresponding changes in system-level considerations. The progression spans from early dense matrix operations optimized for CPUs, through convolutions leveraging GPU acceleration and sequential operations necessitating sophisticated memory hierarchies, to the current era of attention mechanisms requiring flexible accelerators and high-bandwidth memory.

+-----------------------+---------------------------+--------------------+------------------------+
| **Era**               | **Dominant Architecture** | **Key Primitives** | **System Focus**       |
+:======================+:==========================+:===================+:=======================+
| **Early NN**          | MLP                       | Dense Matrix Ops   | CPU optimization       |
+-----------------------+---------------------------+--------------------+------------------------+
| **CNN Revolution**    | CNN                       | Convolutions       | GPU acceleration       |
+-----------------------+---------------------------+--------------------+------------------------+
| **Sequence Modeling** | RNN                       | Sequential Ops     | Memory hierarchies     |
+-----------------------+---------------------------+--------------------+------------------------+
| **Attention Era**     | Transformer               | Attention, Dynamic | Flexible accelerators, |
|                       |                           | Compute            | High-bandwidth memory  |
+-----------------------+---------------------------+--------------------+------------------------+

: **Deep Learning Evolution**: Neural network architectures have progressed from simple, fully connected layers to complex models leveraging specialized hardware and addressing sequential data dependencies. This table maps architectural eras to key computational primitives and corresponding system-level optimizations, revealing a historical trend toward increased parallelism and memory bandwidth requirements. {#tbl-dl-evolution}

Examination of these building blocks shows primitives evolving and combining to create increasingly powerful neural network architectures.

### Evolution from Perceptron to Multi-Layer Networks {#sec-dnn-architectures-evolution-perceptron-multilayer-networks-0f3f}

While we examined MLPs in @sec-dnn-architectures-multilayer-perceptrons-dense-pattern-processing-259f as a mechanism for dense pattern processing, here we focus on how they established building blocks that appear throughout deep learning. The evolution from perceptron to MLP introduced several key concepts: the power of layer stacking, the importance of non-linear transformations, and the basic feedforward computation pattern.

The introduction of hidden layers between input and output created a template for feature transformation that appears in virtually every modern architecture. Even in sophisticated networks like Transformers, we find MLP-style feedforward layers performing feature processing. The concept of transforming data through successive non-linear layers has become a paradigm that transcends specific architecture types.

Most significantly, the development of MLPs established the backpropagation algorithm[^fn-dnn-backpropagation], which to this day remains the cornerstone of neural network optimization. This key contribution has enabled the development of deep architectures and influenced how later architectures would be designed to maintain gradient flow.

[^fn-dnn-backpropagation]: **Backpropagation Algorithm**: While the chain rule was known since the 1600s, Rumelhart, Hinton, and Williams (1986) showed how to efficiently apply it to train multi-layer networks. This "learning by error propagation" algorithm made deep networks practical and remains virtually unchanged in modern systems—a testament to its importance.

These building blocks, layered feature transformation, non-linear activation, and gradient-based learning, set the foundation for more specialized architectures. Subsequent innovations often focused on structuring these basic components in new ways rather than replacing them entirely.

### Evolution from Dense to Spatial Processing {#sec-dnn-architectures-evolution-dense-spatial-processing-1d3b}

The development of CNNs marked an architectural innovation, specifically the realization that we could specialize the dense connectivity of MLPs for spatial patterns. While retaining the core concept of layer-wise processing, CNNs introduced several building blocks that would influence all future architectures.

The first key innovation was the concept of parameter sharing. Unlike MLPs where each connection had its own weight, CNNs showed how the same parameters could be reused across different parts of the input. This not only made the networks more efficient but introduced the powerful idea that architectural structure could encode useful priors about the data [@lecun1998gradient].

#### Gradient Flow and the Depth Problem {#sec-dnn-architectures-gradient-flow-depth-problem}

Before examining the architectural innovations that enabled training very deep networks, we must understand the fundamental challenge that depth creates: the gradient flow problem. This mathematical constraint has profound implications for architecture design and explains why certain architectural patterns became essential rather than optional.

**The Mathematical Challenge of Depth**

Backpropagation through $L$ layers applies the chain rule repeatedly. For a deep network with layers $f_1, f_2, \ldots, f_L$, the gradient of the loss $\mathcal{L}$ with respect to the weights in layer 1 is:

$$
\frac{\partial \mathcal{L}}{\partial W_1} = \frac{\partial \mathcal{L}}{\partial a_L} \cdot \frac{\partial a_L}{\partial z_L} \cdot \frac{\partial z_L}{\partial a_{L-1}} \cdot \ldots \cdot \frac{\partial z_2}{\partial a_1} \cdot \frac{\partial a_1}{\partial z_1} \cdot \frac{\partial z_1}{\partial W_1}
$$

where $z_l$ represents the pre-activation and $a_l = \sigma(z_l)$ the post-activation output of layer $l$. The gradient becomes a product of $L$ terms, each depending on the activation function derivative $\sigma'(z_l)$.

**Vanishing Gradients: The Silent Training Failure**

For sigmoid activation functions, the derivative is $\sigma'(z) = \sigma(z)(1 - \sigma(z))$, with maximum value $\sigma'(0) = 0.25$. Through $L$ layers, the gradient magnitude is multiplied by approximately $(0.25)^L$:

- **10 layers**: gradient multiplied by $(0.25)^{10} \approx 10^{-6}$
- **20 layers**: gradient multiplied by $(0.25)^{20} \approx 10^{-12}$
- **30 layers**: gradient multiplied by $(0.25)^{30} \approx 10^{-18}$

With such extreme attenuation, early layers receive infinitesimal gradient signals. Weight updates become negligible, effectively preventing these layers from training. The network may appear to train (loss decreases), but only the final layers learn meaningful representations while early layers remain stuck near initialization.

**Exploding Gradients: The Catastrophic Counterpart**

If activation function derivatives exceed 1, gradients grow exponentially through the layers. Consider a network where each layer's Jacobian has eigenvalues around 1.5:

- **10 layers**: gradient multiplied by $(1.5)^{10} \approx 58$
- **20 layers**: gradient multiplied by $(1.5)^{20} \approx 3,325$
- **30 layers**: gradient multiplied by $(1.5)^{30} \approx 191,751$

This exponential growth causes numerical overflow (NaN values), extreme parameter updates, and training divergence. Unlike vanishing gradients which silently prevent learning, exploding gradients cause immediate training failure.

**Quantitative Analysis: Plain Deep Networks**

Consider training a 50-layer convolutional network on CIFAR-10 without architectural interventions. Even with ReLU activations (which have derivative 1 for positive inputs), gradient magnitudes vary dramatically across depth:

- **Layer 50** (near output): $\|\nabla_{W_{50}} \mathcal{L}\| \approx 0.1$
- **Layer 25** (middle): $\|\nabla_{W_{25}} \mathcal{L}\| \approx 0.001$
- **Layer 1** (early): $\|\nabla_{W_1} \mathcal{L}\| \approx 10^{-8}$

The training behavior reflects this gradient distribution. After 50 epochs:

- **Training loss**: Starts at 2.3 (random chance), improves to 1.8
- **Test accuracy**: Reaches only 45% (compared to 60%+ for shallow networks)
- **Observation**: Network barely learns, significantly underperforming shallow counterparts

This "degradation problem" is not overfitting. Deeper networks train worse than shallow ones, contradicting the intuition that more layers should provide more representational capacity.

**Why ReLU Helps But Is Not Sufficient**

ReLU activation ($\text{ReLU}(z) = \max(0, z)$) has derivative:

$$
\text{ReLU}'(z) = \begin{cases}
1 & \text{if } z > 0 \\
0 & \text{if } z \leq 0
\end{cases}
$$

Through active paths ($z > 0$), the derivative equals 1, avoiding gradient decay from the activation function. This represents significant improvement over sigmoid, enabling training of networks with 10-20 layers.

However, ReLU introduces a different problem: dead neurons. When $z \leq 0$, the gradient is exactly zero, permanently blocking gradient flow through that path. A poorly initialized neuron or large gradient update can push a ReLU unit into the negative regime across all training examples, causing it to "die" and never recover. Moreover, ReLU does not solve gradient flow issues arising from weight matrices themselves. If weight matrices have eigenvalues far from 1, gradients still vanish or explode regardless of activation function.

**Residual Connections: The Architectural Solution**

ResNet blocks introduce skip connections that fundamentally change gradient flow. A residual block computes:

$$
y = \mathcal{F}(x) + x
$$

where $\mathcal{F}(x)$ represents the residual function (typically two convolutional layers with batch normalization and ReLU) and $x$ is the identity skip connection.

During backpropagation, the gradient flows through this addition:

$$
\frac{\partial \mathcal{L}}{\partial x} = \frac{\partial \mathcal{L}}{\partial y} \cdot \frac{\partial y}{\partial x} = \frac{\partial \mathcal{L}}{\partial y} \cdot \frac{\partial (\mathcal{F}(x) + x)}{\partial x}
$$

Applying the chain rule:

$$
\frac{\partial \mathcal{L}}{\partial x} = \frac{\partial \mathcal{L}}{\partial y} \cdot \left(\frac{\partial \mathcal{F}(x)}{\partial x} + 1\right) = \frac{\partial \mathcal{L}}{\partial y} \cdot \mathcal{F}'(x) + \frac{\partial \mathcal{L}}{\partial y}
$$

This equation reveals the critical insight: the gradient has two paths:

1. **Residual path**: $\frac{\partial \mathcal{L}}{\partial y} \cdot \mathcal{F}'(x)$ (can vanish if $\mathcal{F}'(x) \to 0$)
2. **Identity path**: $\frac{\partial \mathcal{L}}{\partial y}$ (always flows unimpeded)

The identity term ensures that even if the residual function produces vanishing gradients, the gradient signal $\frac{\partial \mathcal{L}}{\partial y}$ flows directly to earlier layers.

**Gradient Flow Through Multiple Residual Blocks**

Through $L$ residual blocks, the gradient becomes:

$$
\frac{\partial \mathcal{L}}{\partial x_0} = \frac{\partial \mathcal{L}}{\partial x_L} \cdot \prod_{l=1}^{L} \left(\mathcal{F}'_l(x_l) + 1\right)
$$

Each factor $(\mathcal{F}'_l + 1)$ has expectation at least 1 (assuming $\mathcal{F}'_l$ is non-negative on average). Unlike plain networks where gradients multiply factors potentially less than 1, ResNets multiply factors that maintain or increase gradient magnitude. This mathematical property enables training of networks with 100+ layers.

**Jacobian Perspective: Eigenvalue Analysis**

Consider each layer as a function $f_l: \mathbb{R}^d \to \mathbb{R}^d$ with Jacobian $J_l = \frac{\partial f_l}{\partial x_l}$. Through $L$ layers:

$$
\frac{\partial \mathcal{L}}{\partial x_0} = \frac{\partial \mathcal{L}}{\partial x_L} \cdot J_L \cdot J_{L-1} \cdot \ldots \cdot J_1
$$

For plain networks, the product of Jacobians causes problems:

- **Eigenvalues $< 1$**: Gradients vanish exponentially
- **Eigenvalues $> 1$**: Gradients explode exponentially
- **Perfect eigenvalues $= 1$**: Extremely difficult to achieve in practice

For ResNets with $f_l(x) = \mathcal{F}_l(x) + x$, the Jacobian becomes:

$$
J_l = \mathcal{F}'_l + I
$$

where $I$ is the identity matrix. The identity ensures that the Jacobian has eigenvalues at least 1, providing a "gradient highway" that prevents vanishing gradients regardless of what happens in the residual function $\mathcal{F}_l$.

**Empirical Validation: 50-Layer Comparison**

Training identical 50-layer networks on CIFAR-10 with and without residual connections demonstrates the practical impact:

**Plain 50-layer network**:

- Architecture: 50 convolutional layers with ReLU, no skip connections
- Training loss (epoch 1): 2.3 (random initialization)
- Training loss (epoch 50): 1.8 (minimal improvement)
- Test accuracy: 45%
- Gradient norm (layer 1): $10^{-8}$
- Gradient norm (layer 50): $0.1$
- Observation: Training stagnates, early layers barely update

**ResNet-50**:

- Architecture: 50 layers organized in residual blocks
- Training loss (epoch 1): 2.3
- Training loss (epoch 50): 0.05 (near perfect training fit)
- Test accuracy: 93%
- Gradient norm (layer 1): $0.01$ (four orders of magnitude larger than plain network)
- Gradient norm (layer 50): $0.1$
- Observation: All layers train effectively, achieving high accuracy

The difference is stark: the plain network fails to train despite having identical representational capacity, while ResNet trains successfully. The architectural difference is skip connections; the mathematical difference is gradient flow.

**System Implications of Skip Connections**

While skip connections solve gradient flow, they introduce system-level costs:

**Memory overhead**: Skip connections require storing the input to each residual block for the addition operation during the forward pass and for backpropagation. For a ResNet-50 with batch size 32 processing $224 \times 224$ RGB images, this adds approximately 20% memory overhead compared to a plain network.

**Computational cost**: The addition operation ($y = \mathcal{F}(x) + x$) is computationally trivial (element-wise addition), adding negligible compute time. The primary cost is the residual function $\mathcal{F}(x)$ itself.

**Training time**: Better gradient flow accelerates convergence. ResNet-50 typically converges in 90 epochs on ImageNet, while plain 50-layer networks may not converge at all. The per-epoch cost increases by approximately 10% due to memory overhead, but total training time decreases dramatically because the network actually learns.

**Architectural Design Principle**: These empirical results establish a systems constraint: depth requires architectural support for gradient flow. The relationship is quantitative:

- **$< 20$ layers**: Can train without skip connections (e.g., VGG-16)
- **20-100 layers**: Require skip connections (e.g., ResNet-50, ResNet-101)
- **$> 100$ layers**: Require skip connections plus careful normalization (e.g., ResNet-v2 with pre-activation)

This constraint shapes architecture selection: if the task benefits from depth (and empirically, most vision and language tasks do), the architecture must incorporate mechanisms to maintain gradient flow. Skip connections became not just an optimization but a necessity.

Perhaps even more influential was the introduction of skip connections through ResNets[^fn-dnn-resnet] [@he2016deep]. Originally they were designed to help train very deep CNNs, skip connections have become a building block that appears in virtually every modern architecture. They showed how direct paths through the network could help gradient flow and information propagation, a concept now central to Transformer designs.

[^fn-dnn-resnet]: **ResNet Revolution**: ResNet (2016) solved the "degradation problem" where deeper networks performed worse than shallow ones. The key insight: adding identity shortcuts ($\mathcal{F}(\mathbf{x}) + \mathbf{x}$) let networks learn residual mappings instead of full transformations, enabling training of 1000+ layer networks and winning ImageNet 2015.

#### Normalization Layers: Mathematical Foundations {#sec-normalization-layers}

While skip connections provide direct gradient pathways, normalization layers address a complementary problem: controlling the scale and distribution of activations throughout the network. CNNs introduced batch normalization [@ioffe2015batch], which has since evolved into variants essential for modern architectures. Understanding the mathematics of normalization reveals why these layers are not merely optimization tricks but fundamental components enabling deep network training.

**Batch Normalization: Definition and Formulation**

Batch normalization normalizes activations across the batch dimension during training. For a mini-batch $\mathcal{B} = \{x_1, \ldots, x_m\}$ of activations at a particular layer, the transformation proceeds in two stages.

First, compute the batch statistics:

$$
\mu_{\mathcal{B}} = \frac{1}{m}\sum_{i=1}^{m} x_i \qquad \sigma_{\mathcal{B}}^2 = \frac{1}{m}\sum_{i=1}^{m} (x_i - \mu_{\mathcal{B}})^2
$$

Then normalize and apply learnable scale and shift:

$$
\hat{x}_i = \frac{x_i - \mu_{\mathcal{B}}}{\sqrt{\sigma_{\mathcal{B}}^2 + \epsilon}}
$$ {#eq-batchnorm-normalize}

$$
y_i = \gamma \hat{x}_i + \beta
$$ {#eq-batchnorm-transform}

The parameters $\gamma$ (scale) and $\beta$ (shift) are learned during training, while $\epsilon$ (typically $10^{-5}$) prevents division by zero. This formulation ensures the network can represent the identity transformation if optimal ($\gamma = \sigma_{\mathcal{B}}$, $\beta = \mu_{\mathcal{B}}$), preserving representational capacity.

**Jacobian Conditioning: Why Normalization Helps Gradient Flow**

The mathematical insight into why normalization aids training lies in how it conditions the Jacobian matrix of the layer. Consider the gradient of the normalized output with respect to the input:

$$
\frac{\partial \hat{x}_i}{\partial x_j} = \frac{1}{\sqrt{\sigma_{\mathcal{B}}^2 + \epsilon}} \left( \delta_{ij} - \frac{1}{m} - \frac{(x_i - \mu_{\mathcal{B}})(x_j - \mu_{\mathcal{B}})}{m\sigma_{\mathcal{B}}^2} \right)
$$

where $\delta_{ij}$ is the Kronecker delta. The critical observation is that this Jacobian has bounded eigenvalues. Without normalization, the Jacobian of a linear layer $W$ can have eigenvalues spanning orders of magnitude (empirically, 0.01 to 100 in deep networks). With batch normalization, the effective Jacobian eigenvalues are constrained to a much narrower range, typically within $[0.5, 2.0]$.

This constraint prevents both vanishing gradients (eigenvalues $\ll 1$) and exploding gradients (eigenvalues $\gg 1$) through the normalization layer itself. The quantitative impact on training stability is substantial:

- **Without normalization**: Gradient norms can vary by factors of $10^4$ across layers
- **With batch normalization**: Gradient norms typically vary by factors of 2 to 4 across layers

This stability enables significantly higher learning rates. Networks with batch normalization commonly train with learning rates 10 to 30 times larger than unnormalized networks, directly accelerating convergence.

**Layer Normalization: Architecture Independence**

While batch normalization proved transformative for CNNs, it introduced a problematic dependency on batch statistics. This creates issues for small batch sizes (noisy statistics), varying sequence lengths (incompatible batch dimensions), and inference (requires running mean/variance estimation). Layer normalization addresses these limitations by normalizing across features rather than across the batch [@ba2016layer].

For an input vector $\mathbf{x} \in \mathbb{R}^H$ with $H$ features:

$$
\mu_L = \frac{1}{H}\sum_{i=1}^{H} x_i \qquad \sigma_L^2 = \frac{1}{H}\sum_{i=1}^{H} (x_i - \mu_L)^2
$$

$$
\text{LayerNorm}(\mathbf{x}) = \frac{\mathbf{x} - \mu_L}{\sqrt{\sigma_L^2 + \epsilon}} \odot \boldsymbol{\gamma} + \boldsymbol{\beta}
$$ {#eq-layernorm}

where $\odot$ denotes element-wise multiplication. Each sample is normalized independently, making layer normalization invariant to batch size and suitable for autoregressive models where future tokens must not influence current computations.

This architectural difference explains why Transformers universally adopt layer normalization: the self-attention mechanism processes sequences of varying length, and autoregressive generation requires each position to be normalized independently of batch composition.

**Comparative Analysis: When to Use Each Variant**

The choice between normalization variants depends on the computational context:

+---------------------------+---------------------------+---------------------------+---------------------------+
| **Characteristic**        | **BatchNorm**             | **LayerNorm**             | **RMSNorm**               |
+:==========================+:==========================+:==========================+:==========================+
| **Normalization Axis**    | Batch dimension           | Feature dimension         | Feature dimension         |
+---------------------------+---------------------------+---------------------------+---------------------------+
| **Batch Size Dependency** | High (noisy for small     | None                      | None                      |
|                           | batches)                  |                           |                           |
+---------------------------+---------------------------+---------------------------+---------------------------+
| **Typical Use Case**      | CNNs, vision models       | Transformers, RNNs        | LLaMA, efficient          |
|                           |                           |                           | Transformers              |
+---------------------------+---------------------------+---------------------------+---------------------------+
| **Computation Cost**      | Higher (mean + variance)  | Higher (mean + variance)  | Lower (variance only)     |
+---------------------------+---------------------------+---------------------------+---------------------------+
| **Training/Inference**    | Different (running stats) | Identical                 | Identical                 |
| **Behavior**              |                           |                           |                           |
+---------------------------+---------------------------+---------------------------+---------------------------+

: **Normalization Variant Comparison**: Different normalization techniques trade off between computational efficiency, batch size sensitivity, and architectural compatibility. RMSNorm, used in LLaMA and other efficient architectures, omits mean centering: $\text{RMSNorm}(\mathbf{x}) = \mathbf{x} / \sqrt{\frac{1}{H}\sum_i x_i^2 + \epsilon} \cdot \boldsymbol{\gamma}$. {#tbl-normalization-comparison}

**Systems Implications**

Normalization layers introduce specific systems costs that architects must consider:

**Memory overhead**: Batch normalization maintains running statistics (mean and variance) for each normalized feature, requiring $2 \times H$ additional parameters per layer during training, where $H$ is the feature dimension. For inference, these become fixed constants. Layer normalization computes statistics on-the-fly, adding no persistent memory but requiring temporary buffers.

**Batch size constraints**: Batch normalization requires sufficiently large batches for stable statistics. Empirically, batch sizes below 16 degrade performance noticeably, and sizes below 8 can cause training instability. This constraint impacts memory-limited scenarios such as high-resolution images or very large models.

In distributed training scenarios, batch normalization statistics may need synchronization across workers, introducing coordination complexity and communication overhead. These multi-node considerations are covered in @sec-distributed-training (Volume II).

**Computational cost**: Computing mean and variance adds $O(m \times H)$ operations per batch normalization layer for batch size $m$ and feature dimension $H$. For layer normalization, the cost is $O(H)$ per sample. RMSNorm reduces this further by eliminating the mean computation.

**Training versus inference**: Batch normalization exhibits different behavior between training (batch statistics) and inference (running statistics), requiring explicit mode switching. Incorrect mode handling is a common source of training-serving skew. Layer normalization behaves identically in both modes, simplifying deployment.

These innovations, including parameter sharing, skip connections, and normalization, transcended their origins in spatial processing to become essential building blocks in the deep learning toolkit.

### Evolution of Sequence Processing {#sec-dnn-architectures-evolution-sequence-processing-8a78}

While CNNs specialized MLPs for spatial patterns, sequence models adapted neural networks for temporal dependencies. RNNs introduced the concept of maintaining and updating state, a building block that influenced how networks could process sequential information, [@elman1990finding].

The development of LSTMs[^fn-lstm-invention] and GRUs[^fn-gru] brought sophisticated gating mechanisms to neural networks [@hochreiter1997long; @cho2014properties]. These gates, themselves small MLPs, showed how simple feedforward computations could be composed to control information flow. This concept of using neural networks to modulate other neural networks became a recurring pattern in architecture design.

[^fn-lstm-invention]: **LSTM Origins**: Sepp Hochreiter and Jürgen Schmidhuber invented LSTMs in 1997 to solve the "vanishing gradient problem" that plagued RNNs. Their gating mechanism was inspired by biological neurons' ability to selectively retain information—a breakthrough that enabled sequence modeling and facilitated modern language models.

[^fn-gru]: **Gated Recurrent Unit (GRU)**: Simplified version of LSTM introduced by Cho et al. (2014) with only 2 gates instead of 3, reducing parameters by ~25% while maintaining similar performance. GRUs became popular for their computational efficiency and easier training, proving that architectural simplification can sometimes improve rather than hurt performance.

Perhaps most significantly, sequence models demonstrated the power of adaptive computation paths. Unlike the fixed patterns of MLPs and CNNs, RNNs showed how networks could process variable-length inputs by reusing weights over time. This insight, that architectural patterns could adapt to input structure, laid groundwork for more flexible architectures.

Sequence models also popularized the concept of attention through encoder-decoder architectures [@bahdanau2014neural]. Initially introduced as an improvement to machine translation, attention mechanisms showed how networks could learn to dynamically focus on relevant information. This building block would later become the foundation of Transformer architectures.

### Modern Architectures: Synthesis and Unification {#sec-dnn-architectures-modern-architectures-synthesis-unification-b5ef}

Modern architectures, particularly Transformers, represent a sophisticated synthesis of these fundamental building blocks. Rather than introducing entirely new patterns, they innovate through strategic combination and refinement of existing components. The Transformer architecture exemplifies this approach: at its core, MLP-style feedforward networks process features between attention layers. The attention mechanism itself builds on sequence model concepts while eliminating recurrent connections, instead employing position embeddings inspired by CNN intuitions. The architecture extensively utilizes skip connections (see @fig-example-skip-connection), inherited from ResNets, while layer normalization, evolved from CNN batch normalization, stabilizes optimization [@ba2016layer].

::: {#fig-example-skip-connection fig-env="figure" fig-pos="htb"}
```{.tikz}
\begin{tikzpicture}[line join=round,font=\small\usefont{T1}{phv}{m}{n}]
\tikzset{%
  Line/.style={line width=1.0pt,black!50,text=black},
  Box/.style={align=center,
    inner xsep=2pt,
    node distance=2,
    draw=BlueLine,
    line width=0.75pt,
    fill=BlueL,
    text width=24mm,
    minimum width=24mm, minimum height=10mm
  },
  op/.style={circle, draw=GreenLine, minimum size=9mm,fill=GreenL,line width=0.75pt},
  >={Latex[length=2mm]},line/.style={-latex, thick},
do path picture/.style={%
    path picture={%
      \pgfpointdiff{\pgfpointanchor{path picture bounding box}{south west}}%
        {\pgfpointanchor{path picture bounding box}{north east}}%
      \pgfgetlastxy\x\y%
      \tikzset{x=\x/2,y=\y/2}%
      #1
    }
  },
plus/.style={do path picture={
    \draw [black,line cap=round, line width=1pt] (-3/5,0) -- (3/5,0) (0,-3/5) -- (0,3/5);
  }}
}
%
\node[Box] (in) {Weight Layer};
\node[Box, right=4 of in] (hidden) {Weight Layer};
\node[op] (relu1) at ($(in)!0.5!(hidden)$) {ReLU};
\node[circle,draw,minimum size=7mm,node distance=1.5,right=of hidden,plus,
draw=BrownLine,fill=BrownL,line width=0.75pt] (add) {};
\node[op, below=0.7 of add] (relu2) {ReLU};
%
\draw[Line,-latex] (in) -- (relu1);
\draw[Line,-latex] (relu1) -- (hidden);
\draw[Line,-latex] (hidden) -- (add);
\draw[Line,-latex] (add) -- (relu2);
\draw[Line,-latex] (relu2) -- ++(1.2,0);
%  Feedback (identity)
\draw[Line,latex-] (in.west) --coordinate(SR) ++(-1.5,0) node[left] {$\mathbf{x}$};
\draw[Line,-latex] (SR) --++(0,0.9)  -| node[pos=0.25,above]{$\mathbf{x}$ identity}(add);
%
\node[below=2pt of relu2] {$\mathcal{F}(\mathbf{x}) + \mathbf{x}$};
\node[below=2pt of relu1] {$\mathcal{F}(\mathbf{x})$};
\end{tikzpicture}
```
**Residual Connection**: Skip connections add the input of a layer to its output, enabling gradients to flow directly through the network and mitigating the vanishing gradient problem in deep architectures. This allows training of significantly deeper networks, as seen in resnets and adopted in modern transformer architectures to improve optimization and performance.
:::

This composition of building blocks creates emergent capabilities exceeding the sum of individual components. The self-attention mechanism, while building on previous attention concepts, enables novel forms of dynamic pattern processing. The arrangement of these components—attention followed by feedforward layers, with skip connections and normalization—has proven sufficiently effective to become a template for new architectures.

Recent innovations in vision and language models follow this pattern of recombining building blocks. Vision Transformers[^fn-vision-transformers] adapt the Transformer architecture to images while maintaining its essential components [@dosovitskiy2021image]. Large language models scale up these patterns while introducing refinements like grouped-query attention or sliding window attention, yet still rely on the core building blocks established through this architectural evolution [@brown2020language]. These modern architectural innovations demonstrate the principles of efficient scaling covered in @sec-efficient-ai, while their practical implementation challenges and optimizations are explored in @sec-model-optimizations.

[^fn-vision-transformers]: **Vision Transformers (ViTs)**: Google's 2021 breakthrough showed that pure transformers could match CNN performance on ImageNet by treating image patches as "words." ViTs split a $224\times 224$ image into $16\times 16$ patches (196 "tokens"), proving that attention mechanisms could replace convolutional inductive biases with sufficient data.

The following comparison of primitive utilization across different neural network architectures shows modern architectures synthesizing and innovating upon previous approaches:

+--------------------+----------------+----------------+---------------------+--------------------+
| **Primitive Type** | **MLP**        | **CNN**        | **RNN**             | **Transformer**    |
+:===================+:===============+:===============+:====================+:===================+
| **Computational**  | Matrix         | Convolution    | Matrix Mult. +      | Matrix Mult. +     |
|                    | Multiplication | (Matrix Mult.) | State Update        | Attention          |
+--------------------+----------------+----------------+---------------------+--------------------+
| **Memory Access**  | Sequential     | Strided        | Sequential + Random | Random (Attention) |
+--------------------+----------------+----------------+---------------------+--------------------+
| **Data Movement**  | Broadcast      | Sliding Window | Sequential          | Broadcast + Gather |
+--------------------+----------------+----------------+---------------------+--------------------+

: **Primitive Utilization**: Neural network architectures differ in their core computational and memory access patterns, impacting hardware requirements and efficiency. Transformers uniquely combine matrix multiplication with attention mechanisms, resulting in random memory access and data movement patterns distinct from sequential rnns or strided cnns. {#tbl-primitive-comparison}

As shown in @tbl-primitive-comparison, Transformers combine elements from previous architectures while introducing new patterns. They retain the core matrix multiplication operations common to all architectures but introduce a more complex memory access pattern with their attention mechanism. Their data movement patterns blend the broadcast operations of MLPs with the gather operations reminiscent of more dynamic architectures.

This synthesis of primitives in Transformers shows modern architectures innovating by recombining and refining existing building blocks from the architectural progression established in @sec-dnn-architectures-architectural-principles-engineering-tradeoffs-89de, rather than inventing entirely new computational paradigms. This evolutionary process guides the development of future architectures and helps design of efficient systems to support them.

## System-Level Building Blocks {#sec-dnn-architectures-systemlevel-building-blocks-72f6}

Examination of different deep learning architectures enables distillation of their system requirements into primitives that underpin both hardware and software implementations. These primitives represent operations that cannot be decomposed further while maintaining their essential characteristics. Just as complex molecules are built from basic atoms, sophisticated neural networks are constructed from these operations.

### Core Computational Primitives {#sec-dnn-architectures-core-computational-primitives-bd67}

Three operations serve as the building blocks for all deep learning computations: matrix multiplication, sliding window operations, and dynamic computation. These operations are primitive because they cannot be further decomposed without losing their essential computational properties and efficiency characteristics.

Matrix multiplication represents the basic form of transforming sets of features. When we multiply a matrix of inputs by a matrix of weights, we're computing weighted combinations, which is the core operation of neural networks. For example, in our MNIST network, each 784-dimensional input vector multiplies with a $784\times 100$ weight matrix. This pattern appears everywhere: MLPs use it directly for layer computations, CNNs reshape convolutions into matrix multiplications (turning a $3\times 3$ convolution into a matrix operation, as illustrated in @fig-im2col-diagram), and Transformers use it extensively in their attention mechanisms.

#### Computational Building Blocks {#sec-dnn-architectures-computational-building-blocks-c3c0}

Modern neural networks operate through three computational patterns that appear across all architectures. These patterns explain how different architectures achieve their computational goals and why certain hardware optimizations are effective.

The detailed analysis of sparse computation patterns, including structured and unstructured sparsity, hardware-aware optimization strategies, and algorithm-hardware co-design principles, is addressed in @sec-model-optimizations and @sec-ai-acceleration.

::: {#fig-im2col-diagram fig-env="figure" fig-pos="htb"}
```{.tikz}
\begin{tikzpicture}[line join=round,font=\usefont{T1}{phv}{m}{n}\small]
\tikzset{
Line/.style={line width=1.0pt,VioletLine,text=black},
  mymatrix/.style={
    matrix of nodes,
    nodes={
      draw,
      fill=orange!40,
      minimum size=8mm,
      text centered,
      execute at begin node=\strut, % za vertikalno poravnanje
      text depth=0.25ex,
      text height=1.25ex,
    },
    column sep=-\pgflinewidth,
    row sep=-\pgflinewidth,
    nodes in empty cells
  }
}
%MA1
\begin{scope}[local bounding box=MA1,shift={(0,0)}]
    \matrix[mymatrix,
  column 1/.style={nodes={fill=cyan!30}},
  column 2/.style={nodes={fill=cyan!30}},
  column 3/.style={nodes={fill=cyan!30}},
  column 4/.style={nodes={fill=cyan!30}},](M1){%
1&  2 &4&5&10&11&13&14 \\
2& 3  &5&6&11&12&14&15 \\
4& 5  &7&8&13&14&16&17 \\
5& 6  &8&9&14&15&17&18 \\
      };
\node[draw=red,inner sep=-2pt,rounded corners=8pt,
yshift=0mm,fill=none,fit=(M1-1-1)(M1-1-4),line width=1.5pt](F1){};
\node[draw=red,inner sep=-2pt,rounded corners=8pt,
yshift=0mm,fill=none,fit=(M1-2-5)(M1-2-8),line width=1.5pt](F3){};
\node[above=3pt of M1]{Transformed GEMM};
\end{scope}
%MA2
\begin{scope}[local bounding box=MA2,shift={($(MA1.north west)+(-2.5,-1)$)}]
    \matrix[mymatrix,
 nodes={fill=green!40}](M2){%
1& 2 \\
3& 4 \\
      };
\node[draw=red,inner sep=-2pt,rounded corners=8pt,
yshift=0mm,fill=none,fit=(M2-1-1)(M2-2-2),line width=1.5pt](F5){};
\end{scope}
%MA3
\begin{scope}[local bounding box=MA3,shift={($(MA1.south west)+(-2.5,0.5)$)}]
    \matrix[mymatrix,
 nodes={
      fill=yellow!30}](M3){%
5& 6 \\
7& 8 \\
      };
\node[below=3pt of M3]{Filter Kernels};
\end{scope}
%MA4
\begin{scope}[local bounding box=MA4,shift={($(M2-2-1.north west)+(-2,0.5)$)}]
    \matrix[mymatrix,
 nodes={
      fill=cyan!30}](M4){%
1& 2&3 \\
4& 5&6 \\
7& 8&9 \\
      };
\node[draw=red,inner sep=-2pt,rounded corners=8pt,
yshift=0mm,fill=none,fit=(M4-1-1)(M4-2-2),line width=1.5pt](F2){};
\node[below=3pt of M4]{Input feature maps};
\end{scope}
%MA5
\begin{scope}[local bounding box=MA4,shift={($(M3-2-1.north west)+(-2,-0.5)$)}]
    \matrix[mymatrix,
 nodes={ fill=orange!40}](M5){%
10& 11&12 \\
13& 14&15 \\
16& 17&18 \\
      };
\node[draw=red,inner sep=-2pt,rounded corners=8pt,
yshift=0mm,fill=none,fit=(M5-1-2)(M5-2-3),line width=1.5pt](F4){};
\end{scope}
%MA6
\begin{scope}[local bounding box=MA6,shift={($(MA1.east)+(1.8,0)$)}]
    \matrix[mymatrix,
 nodes={fill=green!40},
row 5/.style={nodes={fill=yellow!30}},
row 6/.style={nodes={fill=yellow!30}},
row 7/.style={nodes={fill=yellow!30}},
row 8/.style={nodes={fill=yellow!30}}](M6){%
1\\ 2 \\ 3\\ 4 \\ 5\\ 6\\7\\8\\
      };
\node[draw=red,inner sep=-2pt,rounded corners=8pt,
yshift=0mm,fill=none,fit=(M6-1-1)(M6-4-1),line width=1.5pt](F6){};
\end{scope}
%%
\draw[Line,-latex](F5)--++(0,2.5)-|(F6);
\draw[Line,-latex](F2)--++(0,1.2)-|(F1);
\draw[Line,-latex](F4)--++(0,-2.0)-|(F3);
\node[font=\huge] at($(M1.east)!0.5!(M6.west)$){$\times$};
\coordinate(SR)at($(M2-2-2.south east)!0.65!(M3-1-2.north east)$);
\node[single arrow, draw=black,thick, fill=VioletL,
      minimum width = 20pt, single arrow head extend=3pt,
      minimum height=11mm]at($(SR)!0.5!(M1.west)$) {};
\end{tikzpicture}
```
**Convolution as Matrix Multiplication**: Reshaping convolutional layers into matrix multiplications using the `im2col` technique, enables efficient computation using optimized BLAS libraries and allows for parallel processing on standard hardware. This transformation is crucial for accelerating cnns and forms the basis for implementing convolutions on diverse platforms.
:::

The im2col[^fn-im2col] (image to column) technique, developed by Intel in the 1990s, accomplishes matrix reshaping by unfolding overlapping image patches into columns of a matrix, as illustrated in @fig-im2col-diagram. Each sliding window position in the convolution becomes a column in the transformed matrix, while the filter kernels are arranged as rows. This allows the convolution operation to be expressed as a standard GEMM (General Matrix Multiply) operation. The transformation trades memory consumption—duplicating data where windows overlap—for computational efficiency, enabling CNNs to leverage decades of BLAS optimizations and achieving 5-10x speedups on CPUs. In modern systems, these matrix multiplications map to specific hardware and software implementations. Datacenter accelerators can deliver on the order of hundreds of TFLOPS on mixed-precision matrix operations, and software frameworks like PyTorch and TensorFlow automatically map these high-level operations to optimized matrix libraries (for example, NVIDIA cuBLAS and Intel oneMKL) that exploit available hardware capabilities.

[^fn-im2col]: **im2col (Image to Column)**: A data layout transformation developed by Intel in the 1990s that converts convolution operations into matrix multiplications by unfolding image patches into columns. This approach trades memory consumption (through data duplication) for computational efficiency, enabling CNNs to leverage decades of GEMM optimizations and achieving 5-10x speedups on CPUs.

Sliding window operations compute local relationships by applying the same operation to chunks of data. In CNNs processing MNIST images, a $3\times 3$ convolution filter slides across the $28\times 28$ input, requiring $26\times 26$ windows of computation, assuming a stride size of 1. Modern hardware accelerators implement this through specialized memory access patterns and data buffering schemes that optimize data reuse. For example, TPUs use systolic arrays[^fn-systolic-array] where data flows systematically through processing elements, allowing each input value to be reused across multiple computations without repeatedly accessing off-chip memory.

[^fn-systolic-array]: **Systolic Array**: A network of processing elements that rhythmically compute and pass data through neighbors, like a "heartbeat" of computation. Invented by H.T. Kung and Charles Leiserson in 1978, systolic arrays achieve high throughput by overlapping computation with data movement. A \(128\times128\) systolic array contains 16,384 processing elements and can perform tens of thousands of multiply-accumulate operations per cycle, depending on the dataflow and precision.

Dynamic computation, where the operation itself depends on the input data, emerged prominently with attention mechanisms but represents a capability needed for adaptive processing. In Transformer attention, each query dynamically determines its interaction weights with all keys; for a sequence of length 512, 512 different weight patterns must be computed on the fly. Unlike fixed patterns where the computation graph is known in advance, dynamic computation requires runtime decisions. This creates specific implementation challenges: hardware must provide flexible data routing (modern GPUs employ dynamic scheduling) and support variable computation patterns, while software frameworks require efficient mechanisms for handling data-dependent execution paths (PyTorch's dynamic computation graphs, TensorFlow's dynamic control flow).

These primitives combine in sophisticated ways in modern architectures. A Transformer layer processing a sequence of 512 tokens demonstrates this clearly: it uses matrix multiplications for feature projections ($512\times 512$ operations implemented through tensor cores), may employ sliding windows for efficient attention over long sequences (using specialized memory access patterns for local regions), and requires dynamic computation for attention weights (computing $512\times 512$ attention patterns at runtime). The way these primitives interact creates specific demands on system design, ranging from memory hierarchy organization to computation scheduling.

The building blocks we've discussed help explain why certain hardware features exist (like tensor cores for matrix multiplication) and why software frameworks organize computations in particular ways (like batching similar operations together). As we move from computational primitives to consider memory access and data movement patterns, recognizing how these operations shape the demands placed on memory systems becomes essential and data transfer mechanisms. The way computational primitives are implemented and combined has direct implications for how data needs to be stored, accessed, and moved within the system.

### Memory Access Primitives {#sec-dnn-architectures-memory-access-primitives-4e2e}

The efficiency of deep learning models depends heavily on memory access and management. Memory access often constitutes the primary bottleneck in modern ML systems; even though a matrix multiplication unit may be capable of performing thousands of operations per cycle, it will remain idle if data is not available at the requisite time. For example, accessing data from DRAM typically requires hundreds of cycles, while on-chip computation requires only a few cycles.

Three memory access patterns dominate in deep learning architectures: sequential access, strided access, and random access. Each pattern creates different demands on the memory system and offers different opportunities for optimization.

Sequential access is the simplest and most efficient pattern. Consider an MLP performing matrix multiplication with a batch of MNIST images: it needs to access both the $784\times 100$ weight matrix and the input vectors sequentially. This pattern maps well to modern memory systems; DRAM can operate in burst mode for sequential reads (reaching on the order of hundreds of GB/s in modern GPUs), and hardware prefetchers can effectively predict and fetch upcoming data. Software frameworks optimize for this by ensuring data is laid out contiguously in memory and aligning data to cache line boundaries.

Strided access appears prominently in CNNs, where each output position needs to access a window of input values at regular intervals. For a CNN processing MNIST images with $3\times 3$ filters, each output position requires accessing 9 input values with a stride matching the input width. While less efficient than sequential access, hardware supports this through pattern-aware caching strategies and specialized memory controllers. Software frameworks often transform these strided patterns into sequential access through data layout reorganization, where the im2col transformation in deep learning frameworks converts convolution's strided access into efficient matrix multiplications.

Random access poses the greatest challenge for system efficiency. In a Transformer processing a sequence of 512 tokens, each attention operation potentially needs to access any position in the sequence, creating unpredictable memory access patterns. Random access can severely impact performance through cache misses (potentially causing 100+ cycle stalls per access) and unpredictable memory latencies. Systems address this through large cache hierarchies (modern GPUs have several MB of L2 cache) and sophisticated prefetching strategies, while software frameworks employ techniques like attention pattern pruning to reduce random access requirements.

These different memory access patterns contribute to the overall memory requirements of each architecture. To illustrate this, @tbl-arch-complexity compares the memory complexity of MLPs, CNNs, RNNs, and Transformers.

+------------------+----------------------+-----------------------+----------------------------+----------------------+
| **Architecture** | **Input Dependency** | **Parameter Storage** | **Activation Storage**     | **Scaling Behavior** |
+:=================+:=====================+:======================+:===========================+:=====================+
| **MLP**          | Linear               | $O(N \times W)$       | $O(B \times W)$            | Predictable          |
+------------------+----------------------+-----------------------+----------------------------+----------------------+
| **CNN**          | Constant             | $O(K \times C)$       | $O(B\times H_{\text{img}}$ | Efficient            |
|                  |                      |                       | $\times W_{\text{img}})$   |                      |
+------------------+----------------------+-----------------------+----------------------------+----------------------+
| **RNN**          | Linear               | $O(h^2)$              | $O(B \times T \times h)$   | Challenging          |
+------------------+----------------------+-----------------------+----------------------------+----------------------+
| **Transformer**  | Quadratic            | $O(N \times d)$       | $O(B \times N^2)$          | Problematic          |
+------------------+----------------------+-----------------------+----------------------------+----------------------+

: **Memory Access Complexity**: Different neural network architectures exhibit varying memory access patterns and storage requirements, impacting system performance and scalability. Parameter storage scales with input dependency and model size, while activation storage represents a significant runtime cost, particularly for sequence-based models where rnns offer a parameter efficiency advantage when sequence length exceeds hidden state size ($n > h$). {#tbl-arch-complexity}

Where:

- $N$: Input or sequence size
- $W$: Layer width
- $B$: Batch size
- $K$: Kernel size
- $C$: Number of channels
- $H_{\text{img}}$: Height of input feature map (CNN)
- $W_{\text{img}}$: Width of input feature map (CNN)
- $h$: Hidden state size (RNN)
- $T$: Sequence length
- $d$: Model dimensionality

@tbl-arch-complexity reveals how memory requirements scale with different architectural choices. The quadratic scaling of activation storage in Transformers, for instance, highlights the need for large memory capacities and efficient memory management in systems designed for Transformer-based workloads. In contrast, CNNs exhibit more favorable memory scaling due to their parameter sharing and localized processing. These memory access patterns complement the computational scaling behaviors examined later in @tbl-computational-complexity, completing the picture of each architecture's resource requirements. These memory complexity considerations inform system-level design decisions, such as choosing memory hierarchy configurations and developing memory optimization strategies.

The impact of these patterns becomes clear when we consider data reuse opportunities. In CNNs, each input pixel participates in multiple convolution windows (typically 9 times for a $3\times 3$ filter), making effective data reuse necessary for performance. Modern GPUs provide multi-level cache hierarchies (L1, L2, shared memory) to capture this reuse, while software techniques like loop tiling ensure data remains in cache once loaded.

Working set size, the amount of data needed simultaneously for computation, varies dramatically across architectures. An MLP layer processing MNIST images might need only a few hundred KB (weights plus activations), while a Transformer processing long sequences can require several MB just for storing attention patterns. These differences directly influence hardware design choices, like the balance between compute units and on-chip memory, and software optimizations like activation checkpointing or attention approximation techniques.

Understanding these memory access patterns is essential as architectures evolve. The shift from CNNs to Transformers, for instance, has driven the development of hardware with larger on-chip memories and more sophisticated caching strategies to handle increased working sets and more dynamic access patterns. Future architectures will likely continue to be shaped by their memory access characteristics as much as their computational requirements.

### Data Movement Primitives {#sec-dnn-architectures-data-movement-primitives-101a}

While computational and memory access patterns define what operations occur where, data movement primitives characterize how information flows through the system. These patterns are key because data movement often consumes more time and energy than computation itself, as moving data from off-chip memory typically requires 100-1000$\times$ more energy than performing a floating-point operation.

Four data movement patterns are prevalent in deep learning architectures: broadcast, scatter, gather, and reduction. @fig-collective-comm illustrates these patterns and their relationships. Broadcast operations send the same data to multiple destinations simultaneously. In matrix multiplication with batch size 32, each weight must be broadcast to process different inputs in parallel. Modern hardware supports this through specialized interconnects and hardware multicast capabilities, with bandwidth on the order of hundreds of GB/s in high-end accelerator interconnects, while some accelerators also use dedicated on-chip broadcast fabrics. Software frameworks optimize broadcasts by restructuring computations (like matrix tiling) to maximize data reuse.

::: {#fig-collective-comm fig-env="figure" fig-pos="htb"}
```{.tikz}
 \begin{tikzpicture}[line join=round,font=\usefont{T1}{phv}{m}{n},scale=0.8, every node/.append style={transform shape}]
\definecolor{Ballcol}{RGB}{172,245,164}
\colorlet{Ballcol}{brown!60!black!20}
\tikzset{%
LineA/.style={line width=1.0pt,black!50,-latex},
ALine/.style={line width=1.0pt,black!50,latex-}
}

\tikzset{
  box/.pic={
    \pgfkeys{/box/.cd, #1}
\foreach \x in {1,...,\columns}{
    \foreach \y in {1,...,\rows}{
        %
        \node[draw=none, fill=\ffill, minimum width=\cellsize, inner sep=0pt,
                    minimum height=\cellheight, line width=\llinewidth] (cell-\x-\y\br) at (\x*\cellsize,-\y*\cellheight) {};
      }
     }
   }
 }

\pgfkeys{
  /box/.cd,
  cellsize/.store in=\cellsize,
  llinewidth/.store in=\llinewidth,
  cellheight/.store in=\cellheight,
  columns/.store in=\columns,
  br/.store in=\br,
  ffill/.store in=\ffill,
  rows/.store in=\rows,
  columns=1,
  rows=3,
  br=A,
  ffill=red,
  cellsize=2mm,
  cellheight=5mm,
  llinewidth=0pt
}
\def\radius{6mm}

\begin{scope}[scale=1, every node/.append style={transform shape},
local bounding box=B1,shift={($(0,0)+(0,0)$)}]
\foreach \x in {1,2,3,4}{
\pgfmathsetmacro{\xcord}{1.6*\x}
\coordinate (1B-\x) at (\xcord,0);
\fill[fill=Ballcol] (\xcord,0) circle (\radius);
\pic[shift={(-0.20,0.5)}] at (1B-\x) {box={columns=1,rows=1,br=A,ffill=red}};
}
\pgfmathsetmacro{\xcenter}{(1.6*1 + 1.6*4)/2} %
\coordinate (1CB) at (\xcenter,-2.2);
\fill[fill=Ballcol]  (1CB) circle (\radius);
\pic[shift={({-0.20,0.5})}] at (1CB) {box={columns=1,rows=1,br=A,ffill=red}};

\foreach \x in {1}{
  \foreach \y in {1,2,3,4}{
    \edef\from{\x CB}
    \edef\to{1B-\y}
\path let
    \p1 = (\from),
    \p2 = (\to),
    \n1 = {atan2(\y2-\y1,\x2-\x1)}
  in
    coordinate (from) at ($ (\from) + (\n1:\radius) $)
    coordinate (to) at ($ (\to) + (\n1+180:\radius) $);
  \draw[LineA] (from) -- node[inner sep=0pt](L\x){}(to);
  }
}
\node[below=7mm of 1CB]{Broadcast};
\end{scope}

\begin{scope}[scale=1, every node/.append style={transform shape},
local bounding box=B2,shift={($(0,0)+(10,0)$)}]

\foreach \x/\col in {1/red,2/yellow,3/green,4/blue}{
\pgfmathsetmacro{\xcord}{1.6*\x}
\coordinate (2B-\x) at (\xcord,0);
\fill[fill=Ballcol]  (\xcord,0) circle (\radius);
\pic[shift={(-0.20,0.5)}] at (2B-\x) {box={columns=1,rows=1,br=A,ffill=\col}};
}
\pgfmathsetmacro{\xcenter}{(1.6*1 + 1.6*4)/2} % Sredina između prve i četvrte lopte
\coordinate (2CB) at (\xcenter,-2.2);
\fill[fill=Ballcol]  (2CB) circle (\radius);

\foreach \x/\col in {1/red,2/yellow,3/green,4/blue}{
\pgfmathsetmacro{\xcord}{0.2*\x}
\pic[shift={({-0.70,0.5})}] at ($(2CB)+(\xcord,0)$) {box={columns=1,rows=1,br=A,ffill=\col}};
}

\foreach \x in {2}{
  \foreach \y in {1,2,3,4}{
    \edef\from{\x CB}
    \edef\to{2B-\y}
\path let
    \p1 = (\from),
    \p2 = (\to),
    \n1 = {atan2(\y2-\y1,\x2-\x1)}
  in
    coordinate (from) at ($ (\from) + (\n1:\radius) $)
    coordinate (to) at ($ (\to) + (\n1+180:\radius) $);
  \draw[LineA] (from) -- node[inner sep=0pt](L\x){}(to);
  }
}
\node[below=7mm of 2CB]{Scatter};
\end{scope}
%%%%%
\begin{scope}[scale=1, every node/.append style={transform shape},
local bounding box=B3,shift={($(0,0)+(0,-5)$)}]

\foreach \x/\col in {1/red,2/yellow,3/green,4/blue}{
\pgfmathsetmacro{\xcord}{1.6*\x}
\coordinate (3B-\x) at (\xcord,0);
\fill[fill=Ballcol]  (\xcord,0) circle (\radius);
\pic[shift={(-0.20,0.5)}] at (3B-\x) {box={columns=1,rows=1,br=A,ffill=\col}};
}
\pgfmathsetmacro{\xcenter}{(1.6*1 + 1.6*4)/2} % Sredina između prve i četvrte lopte
\coordinate (3CB) at (\xcenter,-2.2);
\fill[fill=Ballcol]  (3CB) circle (\radius);

\foreach \x/\col in {1/red,2/yellow,3/green,4/blue}{
\pgfmathsetmacro{\xcord}{0.2*\x}
\pic[shift={({-0.70,0.5})}] at ($(3CB)+(\xcord,0)$) {box={columns=1,rows=1,br=A,ffill=\col}};
}

\foreach \x in {3}{
  \foreach \y in {1,2,3,4}{
    \edef\from{\x CB}
    \edef\to{3B-\y}
\path let
    \p1 = (\from),
    \p2 = (\to),
    \n1 = {atan2(\y2-\y1,\x2-\x1)}
  in
    coordinate (from) at ($ (\from) + (\n1:\radius) $)
    coordinate (to) at ($ (\to) + (\n1+180:\radius) $);
  \draw[ALine] (from) -- node[inner sep=0pt](L\x){}(to);
  }
}
\node[below=7mm of 3CB]{Gather};
\end{scope}
%%%%%%%
\begin{scope}[scale=1, every node/.append style={transform shape},
local bounding box=B4,shift={($(0,0)+(10,-5)$)}]

\foreach \x/\col in {1/1,2/3,3/5,4/7}{
\pgfmathsetmacro{\xcord}{1.6*\x}
\coordinate (4B-\x) at (\xcord,0);
\fill[fill=Ballcol]  (\xcord,0) circle (\radius);
%\pic[shift={(-0.20,0.5)}] at (4B-\x) {box={columns=1,rows=1,br=A,ffill=\col}};
\node[]at(4B-\x){\LARGE\bfseries\col};
}
\pgfmathsetmacro{\xcenter}{(1.6*1 + 1.6*4)/2}
\coordinate (4CB) at (\xcenter,-2.2);
\fill[fill=Ballcol]  (4CB) circle (\radius);
\node[]at(4CB){\LARGE\bfseries 16};

\foreach \x in {4}{
  \foreach \y in {1,2,3,4}{
    \edef\from{\x CB}
    \edef\to{4B-\y}
\path let
    \p1 = (\from),
    \p2 = (\to),
    \n1 = {atan2(\y2-\y1,\x2-\x1)}
  in
    coordinate (from) at ($ (\from) + (\n1:\radius) $)
    coordinate (to) at ($ (\to) + (\n1+180:\radius) $);
  \draw[ALine] (from) -- node[inner sep=0pt](L\x){}(to);
  }
}
\node[below=7mm of 4CB]{Reduction};
\end{scope}
\end{tikzpicture}
```
**Collective Communication Patterns**: Deep learning training and inference frequently require data exchange between processing units; this figure outlines four core patterns (broadcast, scatter, gather, and reduction) that define how data moves within a distributed system and impact overall performance. Understanding these patterns enables optimization of data movement, critical because communication costs often dominate computation in modern machine learning workloads.
:::

Scatter operations distribute different elements to different destinations. When parallelizing a $512\times 512$ matrix multiplication across accelerator cores, each core receives a subset of the computation. This parallelization is important for performance but challenging, as memory conflicts and load imbalance can reduce efficiency substantially. Hardware provides flexible high-bandwidth interconnects (often in the hundreds of GB/s class within a node), while software frameworks employ sophisticated work distribution algorithms to maintain high utilization.

Gather operations collect data from multiple sources. In Transformer attention with sequence length 512, each query must gather information from 512 different key-value pairs. These irregular access patterns are challenging, random gathering can be $10\times$ slower than sequential access. Hardware supports this through high-bandwidth interconnects and large caches, while software frameworks employ techniques like attention pattern pruning to reduce gathering overhead.

Reduction operations combine multiple values into a single result through operations like summation. When computing attention scores in Transformers or layer outputs in MLPs, efficient reduction is essential. Hardware implements tree-structured reduction networks (reducing latency from $O(n)$ to $O(\log n)$), while software frameworks use optimized parallel reduction algorithms that can achieve near-theoretical peak performance.

These patterns combine in sophisticated ways. A Transformer attention operation with sequence length 512 and batch size 32 involves:

* Broadcasting query vectors ($512\times 64$ elements)
* Gathering relevant keys and values ($512\times 512\times 64$ elements)
* Reducing attention scores ($512\times 512$ elements per sequence)

The evolution from CNNs to Transformers has increased reliance on gather and reduction operations, driving hardware innovations like more flexible interconnects and larger on-chip memories. As models grow (some now exceeding 100 billion parameters[^fn-parameter-scaling]), efficient data movement becomes increasingly critical, leading to innovations like near-memory processing and sophisticated data flow optimizations.

[^fn-parameter-scaling]: **Parameter Scaling**: Model sizes have grown from tens of millions of parameters (early deep CNNs) to hundreds of billions and, in some cases, trillion-scale parameter counts. Training at this scale requires specialized distributed computing infrastructure and can consume megawatt-scale power during training.

### System Design Impact {#sec-dnn-architectures-system-design-impact-cd41}

The computational, memory access, and data movement primitives we've explored form the foundational requirements that shape the design of systems for deep learning. The way these primitives influence hardware design, create common bottlenecks, and drive trade-offs is important for developing efficient and effective ML systems.

One of the most significant impacts of these primitives on system design is the push towards specialized hardware. The prevalence of matrix multiplications and convolutions in deep learning has led to the development of tensor processing units (TPUs)[^fn-dnn-tpu] and tensor cores in GPUs, which are specifically designed to perform these operations efficiently.

[^fn-dnn-tpu]: **Tensor Processing Units**: Google's TPUs emerged from the need to run neural networks at large scale. They exemplify domain-specific hardware that uses systolic arrays and a tailored memory hierarchy to achieve large gains in throughput per watt on targeted inference and training kernels. These specialized units can perform many multiply-accumulate operations in parallel, dramatically accelerating the core computations of neural networks.

Memory systems have also been profoundly influenced by the demands of deep learning primitives. The need to support both sequential and random access patterns efficiently has driven the development of sophisticated memory hierarchies. High-bandwidth memory (HBM)[^fn-hbm] has become common in AI accelerators to support the massive data movement requirements, especially for operations like attention mechanisms in Transformers. On-chip memory hierarchies have grown in complexity, with multiple levels of caching and scratchpad memories[^fn-scratchpad] to support the diverse working set sizes of different neural network layers.

[^fn-hbm]: **High Bandwidth Memory (HBM)**: Stacked DRAM technology that can provide bandwidth on the order of TB/s, higher than many traditional graphics memory and CPU memory subsystems (often on the order of hundreds of GB/s). HBM enables the massive data movement required by modern AI workloads; for very large models, parameter streaming through memory can be TB-scale per forward pass, making bandwidth a central limiter.

[^fn-scratchpad]: **Scratchpad Memory**: Programmer-controlled on-chip memory providing predictable, fast access without cache management overhead. Unlike caches, scratchpads require explicit data movement but enable precise control over memory allocation—critical for neural network accelerators where memory access patterns are known and performance must be deterministic.

The data movement primitives have particularly influenced the design of interconnects and on-chip networks. The need to support efficient broadcasts, gathers, and reductions has led to the development of more flexible and higher-bandwidth interconnects. Some AI chips now feature specialized networks-on-chip designed to accelerate common data movement patterns in neural networks.

@tbl-sys-design-implications summarizes the system implications of these primitives:

+---------------------------+---------------------------+---------------------------+----------------------------+
| **Primitive**             | **Hardware Impact**       | **Software Optimization** | **Key Challenges**         |
+:==========================+:==========================+:==========================+:===========================+
| **Matrix Multiplication** | Tensor Cores              | Batching, GEMM libraries  | Parallelization, precision |
+---------------------------+---------------------------+---------------------------+----------------------------+
| **Sliding Window**        | Specialized datapaths     | Data layout optimization  | Stride handling            |
+---------------------------+---------------------------+---------------------------+----------------------------+
| **Dynamic Computation**   | Flexible routing          | Dynamic graph execution   | Load balancing             |
+---------------------------+---------------------------+---------------------------+----------------------------+
| **Sequential Access**     | Burst mode DRAM           | Contiguous allocation     | Access latency             |
+---------------------------+---------------------------+---------------------------+----------------------------+
| **Random Access**         | Large caches              | Memory-aware scheduling   | Cache misses               |
+---------------------------+---------------------------+---------------------------+----------------------------+
| **Broadcast**             | Specialized interconnects | Operation fusion          | Bandwidth                  |
+---------------------------+---------------------------+---------------------------+----------------------------+
| **Gather/Scatter**        | High-bandwidth memory     | Work distribution         | Load balancing             |
+---------------------------+---------------------------+---------------------------+----------------------------+

: **Primitive-Hardware Co-Design**: Efficient machine learning systems require tight integration between algorithmic primitives and underlying hardware; this table maps common primitives to specific hardware accelerations and software optimizations, highlighting key challenges in their implementation. Specialized hardware, such as tensor cores and datapaths, address the computational demands of primitives like matrix multiplication and sliding windows, while software techniques like batching and dynamic graph execution further enhance performance. {#tbl-sys-design-implications}

Despite these advancements, several bottlenecks persist in deep learning models. Memory bandwidth often remains a key limitation, particularly for models with large working sets or those that require frequent random access. The energy cost of data movement, especially between off-chip memory and processing units, continues to be a significant concern. For large-scale models, the communication overhead in distributed training can become a bottleneck, limiting scaling efficiency.

#### Energy Consumption Analysis Across Architectures {#sec-dnn-architectures-energy-consumption-analysis-across-architectures-b681}

Energy consumption patterns vary dramatically across neural network architectures, with implications for both datacenter deployment and edge computing scenarios. Each architectural pattern exhibits distinct energy characteristics that inform deployment decisions and optimization strategies.

Dense matrix operations in MLPs achieve excellent arithmetic intensity[^fn-arithmetic-intensity] (computation per data movement) but consume significant absolute energy. Each multiply-accumulate operation consumes approximately 4.6pJ, while data movement from DRAM costs 640pJ per 32-bit value. For typical MLP inference, 70-80% of energy goes to data movement rather than computation, making memory bandwidth optimization critical for energy efficiency.

[^fn-arithmetic-intensity]: **Arithmetic Intensity**: The ratio of floating-point operations to memory accesses, measured in FLOPS per byte. High arithmetic intensity (>10 FLOPS/byte) enables efficient hardware utilization, while low intensity (<1 FLOPS/byte) makes workloads memory-bound. Attention mechanisms typically have low arithmetic intensity, explaining their energy inefficiency.

Convolutional operations reduce energy consumption through data reuse but exhibit variable efficiency depending on implementation. Im2col-based convolution implementations trade memory for simplicity, often doubling memory requirements and energy consumption. Direct convolution implementations achieve 3-5x better energy efficiency by eliminating redundant data movement, particularly for larger kernel sizes.

Sequential processing in RNNs creates energy efficiency opportunities through temporal data reuse. The constant memory footprint of RNN hidden states enables aggressive caching strategies, reducing DRAM access energy by 80-90% for long sequences. The sequential dependencies limit parallelization opportunities, often resulting in suboptimal hardware utilization and higher energy per operation.

Attention mechanisms in Transformers exhibit the highest energy consumption per operation due to quadratic scaling and complex data movement patterns. Self-attention operations consume 2-3x more energy per FLOP than standard matrix multiplication due to irregular memory access patterns and the need to store attention matrices. This energy cost scales quadratically with sequence length, making long-sequence processing energy-prohibitive without architectural modifications.

System designers must navigate trade-offs in supporting different primitives, each with unique characteristics that influence system design and performance. For example, optimizing for the dense matrix operations common in MLPs and CNNs might come at the cost of flexibility needed for the more dynamic computations in attention mechanisms. Supporting large working sets for Transformers might require sacrificing energy efficiency.

Balancing these trade-offs requires consideration of the target workloads and deployment scenarios. Understanding the nature of each primitive guides the development of both hardware and software optimizations in ML systems, allowing designers to make informed decisions about system architecture and resource allocation.

The analysis of architectural patterns, computational primitives, and system implications establishes the foundation for addressing a practical challenge: how do engineers systematically choose the right architecture for their specific problem? The diversity of neural network architectures, each optimized for different data patterns and computational constraints, requires a structured approach to architecture selection. This selection process must consider not only algorithmic performance but also deployment constraints covered in @sec-ml-systems and operational efficiency requirements detailed in @sec-ml-operations.

## Architecture Selection Framework {#sec-dnn-architectures-architecture-selection-framework-7a37}

The exploration of neural network architectures, from dense MLPs to dynamic Transformers, demonstrates how each design embodies specific assumptions about data structure and computational patterns. MLPs assume arbitrary feature relationships, CNNs exploit spatial locality, RNNs capture temporal dependencies, and Transformers model complex relational patterns. For practitioners facing real-world problems, a question emerges: how to systematically select the appropriate architecture for a specific use case?

The diversity of available architectures overwhelms practitioners, when each claims superiority for different scenarios. Successful architecture selection requires understanding principles rather than following trends: matching data characteristics to architectural strengths, evaluating computational constraints against system capabilities, and balancing accuracy requirements with deployment realities.

This systematic approach to architecture selection draws upon the computational patterns and system implications explored in the preceding analysis. By understanding how different architectures process information and their corresponding resource requirements, engineers can make informed decisions that align with both problem requirements and practical constraints. The framework integrates principles from efficient AI design @sec-efficient-ai with practical deployment considerations as discussed in ML operations @sec-ml-operations.

### Data-to-Architecture Mapping {#sec-dnn-architectures-datatoarchitecture-mapping-0b9c}

The first step in systematic architecture selection involves understanding how different data types align with architectural strengths. Each neural network architecture evolved to address specific patterns in data: MLPs handle arbitrary relationships in tabular data, CNNs exploit spatial locality in images, RNNs capture temporal dependencies in sequences, and Transformers model complex relational patterns where any element might influence any other.

This alignment is not coincidental. It reflects computational trade-offs. Architectures that match data characteristics can leverage natural structure for efficiency, while mismatched architectures must work against their design assumptions, leading to poor performance or excessive resource consumption.

@tbl-architecture-selection provides a systematic framework for matching data characteristics to appropriate architectures:

+------------------+---------------------+--------------------------------------+----------------------------------+
| **Architecture** | **Data Type**       | **Key Characteristics**              | **Example Applications**         |
+:=================+:====================+:=====================================+:=================================+
| **MLPs**         | Tabular/Structured  | • No spatial/temporal                | • Financial modeling             |
|                  |                     | •&nbsp;Arbitrary&nbsp;relationships  | •&nbsp;Medical&nbsp;measurements |
|                  |                     | •&nbsp;Dense&nbsp;connectivity       | •&nbsp;Recommendation systems    |
+------------------+---------------------+--------------------------------------+----------------------------------+
| **CNNs**         | Spatial/Grid-like   | • Local patterns                     | • Image recognition              |
|                  |                     | •&nbsp;Translation&nbsp;equivariance | •&nbsp;2D&nbsp;sensor&nbsp;data  |
|                  |                     | •&nbsp;Parameter&nbsp;sharing        | •&nbsp;Signal&nbsp;processing    |
+------------------+---------------------+--------------------------------------+----------------------------------+
| **RNNs**         | Sequential/Temporal | • Temporal dependencies              | • Time series forecasting        |
|                  |                     | •&nbsp;Variable&nbsp;length          | •&nbsp;Simple language tasks     |
|                  |                     | •&nbsp;Memory&nbsp;across time       | •&nbsp;Speech recognition        |
+------------------+---------------------+--------------------------------------+----------------------------------+
| **Transformers** | Complex Relational  | • Long-range dependencies            | • Language understanding         |
|                  |                     | •&nbsp;Attention&nbsp;mechanisms     | •&nbsp;Machine translation       |
|                  |                     | •&nbsp;Dynamic relationships         | •&nbsp;Complex reasoning tasks   |
+------------------+---------------------+--------------------------------------+----------------------------------+

: **Architecture Selection Framework**: Systematic matching of data characteristics to neural network architectures based on computational requirements and pattern types. {#tbl-architecture-selection}

While data characteristics guide initial architecture selection, computational constraints often determine final feasibility. Understanding the scaling behavior of each architecture enables realistic resource planning and deployment decisions.

### Computational Complexity Considerations {#sec-dnn-architectures-computational-complexity-considerations-93fb}

Architecture selection must account for computational and memory trade-offs that determine deployment feasibility. Each architecture exhibits distinct scaling behaviors that create different bottlenecks as problem size increases. Understanding these patterns enables realistic resource planning and prevents costly architectural mismatches during deployment.

The computational profile of each architecture reflects its underlying design philosophy. Dense architectures like MLPs prioritize representational capacity through full connectivity, while structured architectures like CNNs achieve efficiency through parameter sharing and locality assumptions. Sequential architectures like RNNs trade parallelization for memory efficiency, while attention-based architectures like Transformers exchange memory for computational flexibility. For completeness, we examine these same architectures from both computational scaling and memory access perspectives (see @tbl-arch-complexity), as each viewpoint reveals different optimization opportunities and system design considerations.

@tbl-computational-complexity summarizes the key computational characteristics of each architecture:

+------------------+-------------------------+-------------------------+------------------------+-----------------------+
| **Architecture** | **Parameters**          | **Forward Pass**        | **Memory**             | **Parallelization**   |
+:=================+:========================+:========================+:=======================+:======================+
| **MLPs**         | $O(d_{\text{in}}\times$ | $O(d_{\text{in}}\times$ | $O(d^2)$ weights       | Excellent             |
|                  | $d_{\text{out}})$       | $d_{\text{out}})$       | $O(d\times b)$         | Matrix ops parallel   |
|                  | per&nbsp;layer          | per&nbsp;layer          | activations            |                       |
+------------------+-------------------------+-------------------------+------------------------+-----------------------+
| **CNNs**         | $O(k^2\times$           | $O(H\times W\times$     | $O(H\times W\times c)$ | Good                  |
|                  | $c_{\text{in}}\times$   | $k^2\times$             | features               | Spatial independence  |
|                  | $c_{\text{out}})$       | $c_{\text{in}}\times$   | $O(k^2\times c^2)$     |                       |
|                  | per layer               | $c_{\text{out}})$       | weights                |                       |
+------------------+-------------------------+-------------------------+------------------------+-----------------------+
| **RNNs**         | $O(h^2+h\times d)$      | $O(T\times h^2)$        | $O(h)$ hidden state    | Poor                  |
|                  | total                   | for $T$ time&nbsp;steps | (constant)             | Sequential deps       |
+------------------+-------------------------+-------------------------+------------------------+-----------------------+
| **Transformers** | $O(d^2)$ projections    | $O(n^2\times d+n$       | $O(n^2)$ attention     | Excellent (positions) |
|                  | $O(d^2\times h)$        | $\times d²)$ per layer  | $O(n\times d)$         | Limited by memory     |
|                  | multi-head              |                         | sequences              |                       |
+------------------+-------------------------+-------------------------+------------------------+-----------------------+

: **Computational Complexity Comparison**: Scaling behaviors and resource requirements for major neural network architectures. Variables: $d$ = dimension, $h$ = hidden size, $k$ = kernel size, $c$&nbsp;=&nbsp;channels, $H,W$ = spatial dimensions, $T$ = time steps, $n$ = sequence length, $b$ = batch size. {#tbl-computational-complexity}

#### Scalability and Production Considerations {#sec-dnn-architectures-scalability-production-considerations-dcb0}

Production deployment introduces constraints beyond algorithmic performance, including latency requirements, memory limitations, energy budgets, and fault tolerance needs. Each architecture exhibits distinct production characteristics that determine real-world feasibility.

MLPs and CNNs scale well across multiple devices through data parallelism, achieving near-linear speedups with proper batch size scaling. RNNs face parallelization challenges due to sequential dependencies, requiring pipeline parallelism or other specialized techniques. Transformers achieve excellent parallelization across sequence positions but suffer from quadratic memory scaling that limits batch sizes and effective utilization.

MLPs provide predictable latency proportional to layer size, making them suitable for real-time applications with strict SLA requirements. CNNs exhibit variable latency depending on implementation strategy and hardware capabilities, with optimized implementations achieving sub-millisecond inference. RNNs create latency dependencies on sequence length, making them challenging for interactive applications. Transformers provide excellent throughput for batch processing but struggle with single-inference latency due to attention overhead.

Memory requirements vary significantly across architectures in production environments. MLPs require fixed memory proportional to model size, enabling straightforward capacity planning. CNNs need variable memory for feature maps that scales with input resolution, requiring dynamic memory management for variable-size inputs. RNNs maintain constant memory for hidden states but may require unbounded memory for very long sequences. Transformers face quadratic memory growth that creates hard limits on sequence length in production.

Fault tolerance and recovery characteristics differ significantly between architectures. MLPs and CNNs exhibit stateless computation that enables straightforward checkpointing and recovery. RNNs maintain temporal state that complicates distributed training and failure recovery procedures. Transformers combine stateless computation with massive memory requirements, making checkpoint sizes a practical concern for large models.

Hardware mapping efficiency varies considerably across architectural patterns. Modern MLPs achieve 80-90% of peak hardware performance on specialized tensor units. CNNs reach 60-75% efficiency depending on layer configuration and memory hierarchy design. RNNs typically achieve 30-50% of peak performance due to sequential constraints and irregular memory access patterns. Transformers achieve 70-85% efficiency for large batch sizes but drop significantly for small batches due to attention overhead.

#### Hardware Mapping and Optimization Strategies {#sec-dnn-architectures-hardware-mapping-optimization-strategies-5a66}

Different architectural patterns require distinct optimization strategies for efficient hardware mapping. Understanding these patterns enables systematic performance tuning and hardware selection decisions.

Dense matrix operations in MLPs map naturally to tensor processing units and GPU tensor cores. These operations benefit from several key optimizations: matrix tiling to fit cache hierarchies, mixed-precision computation to double throughput, and operation fusion to reduce memory traffic. Optimal tile sizes depend on cache hierarchy, typically 64x64 for L1 cache and 256x256 for L2, while tensor cores achieve peak efficiency with specific dimension multiples such as 16x16 blocks for Volta architecture.

CNNs benefit from specialized convolution algorithms and data layout optimizations that differ significantly from dense matrix operations. Im2col transformations convert convolutions to matrix multiplication but double memory usage. Winograd algorithms reduce arithmetic complexity by 2.25x for 3x3 convolutions at the cost of numerical stability. Direct convolution with custom kernels achieves optimal memory efficiency but requires architecture-specific tuning.

RNNs require different optimization approaches due to their temporal dependencies. Loop unrolling reduces control overhead but increases memory usage. State vectorization enables SIMD operations across multiple sequences. Wavefront parallelization exploits independence across timesteps for bidirectional processing.

Transformers demand specialized attention optimizations due to their quadratic complexity. FlashAttention algorithms reduce memory usage from O(n²) to O(n) through online softmax computation and gradient recomputation. Sparse attention patterns including local, strided, and random approaches maintain modeling capability while reducing complexity. Multi-query attention shares key and value projections across heads, reducing memory bandwidth by 30-50%.

Multi-Layer Perceptrons represent the most straightforward computational pattern, with costs dominated by matrix multiplications. The dense connectivity that enables MLPs to model arbitrary relationships comes at the price of quadratic parameter growth with layer width. Each neuron connects to every neuron in the previous layer, creating large parameter counts that grow quadratically with network width. The computation is dominated by matrix-vector products, which are highly optimized on modern hardware. Matrix operations are inherently parallel and map efficiently to GPU architectures, with each output neuron computed independently. The optimization techniques for reducing these parameter counts, including pruning and low-rank approximations specifically targeting dense layers, are covered in @sec-model-optimizations.

Convolutional Neural Networks achieve computational efficiency through parameter sharing and spatial locality, but their costs scale with both spatial dimensions and channel depth. The convolution operation's computational intensity depends heavily on kernel size and feature map resolution. Parameter sharing across spatial locations dramatically reduces memory compared to equivalent MLPs, while computational cost grows linearly with image resolution and quadratically with kernel size. Feature map memory dominates usage and becomes prohibitive for high-resolution inputs. Spatial independence enables parallel processing across different spatial locations and channels, though memory bandwidth often becomes the limiting factor.

Recurrent Neural Networks optimize for memory efficiency at the cost of parallelization. Their sequential nature creates computational bottlenecks but enables processing of variable-length sequences with constant memory overhead. The hidden-to-hidden connections ($h^2$ term) dominate parameter count for large hidden states. Sequential dependencies prevent parallel processing across time, making RNNs inherently slower than feedforward alternatives. Their constant memory usage for hidden state storage makes RNNs memory-efficient for long sequences, with this efficiency coming at the cost of computational speed.

Transformers achieve maximum flexibility through attention mechanisms but pay a steep price in memory usage. Their quadratic scaling with sequence length creates limits on the sequences they can process. Parameter count scales with model dimension but remains independent of sequence length. The $n^2$ term from attention computation dominates for long sequences, while the $n \times d^2$ term from feed-forward layers dominates for short sequences. Attention matrices create the primary memory bottleneck, as each attention head must store pairwise similarities between all sequence positions, leading to prohibitive memory usage for long sequences. While parallelization is excellent across sequence positions and attention heads, the quadratic memory requirement often forces smaller batch sizes, limiting effective parallelization.

These complexity patterns define optimal domains for each architecture. MLPs excel when parameter efficiency is not critical, CNNs dominate for moderate-resolution spatial data, RNNs remain viable for very long sequences where memory is constrained, and Transformers excel for complex relational tasks where their computational cost is justified through superior performance.

### Architectural Comparison Summary {#sec-dnn-architectures-architectural-comparison-summary-f918}

The systematic analysis of each architectural family reveals distinct computational signatures that determine their suitability for different deployment scenarios. @tbl-architecture-comparison provides a quantitative comparison across key systems metrics, enabling engineers to make informed trade-offs between model capability and computational constraints.

+--------------------+-----------+-------------+------------------+-----------------+
| **Metric**         | **MLP**   | **CNN**     | **RNN**          | **Transformer** |
+:===================+:==========+:============+:=================+================:+
| **Parameters**     | O(N×M)    | O(K²×C×D)   | O(H²)            | O(N×d²)         |
+--------------------+-----------+-------------+------------------+-----------------+
| **FLOPs/Sample**   | O(N×M)    | O(K²×H×W×C) | O(T×H²)          | O(N²×d)         |
+--------------------+-----------+-------------+------------------+-----------------+
| **Memory**         | O(B×M)    | O(B×H×W×C)  | O(B×T×H)         | O(B×N²)         |
+--------------------+-----------+-------------+------------------+-----------------+
| **(Activations)**  |           |             |                  |                 |
+--------------------+-----------+-------------+------------------+-----------------+
| **Parallelism**    | High      | High        | Low (Sequential) | High            |
+--------------------+-----------+-------------+------------------+-----------------+
| **Key Bottleneck** | Memory BW | Memory BW   | Sequential Dep.  | Memory (N²)     |
+--------------------+-----------+-------------+------------------+-----------------+

: **Quantitative Architecture Comparison**: Computational complexity analysis across four major neural network architectures. Parameters scale with network dimensions (N=neurons, M=inputs, K=kernel size, C=channels, D=depth, H=hidden size, T=time steps, d=model dimension). Memory requirements reflect peak activation storage during training. Parallelism indicates amenability to parallel computation. Key bottlenecks represent primary performance limiting factors in typical deployments. {#tbl-architecture-comparison}

This quantitative framework enables systematic architecture selection by explicitly revealing the scaling behaviors that determine computational feasibility. MLPs and CNNs achieve high parallelism but face memory bandwidth constraints as model size grows. RNNs maintain constant memory usage but sacrifice parallelism for sequential processing. Transformers achieve maximum expressivity but face quadratic memory scaling that limits sequence length. Understanding these trade-offs proves essential for matching architectural choices to deployment constraints.

### Decision Framework {#sec-dnn-architectures-decision-framework-dbe8}

Effective architecture selection requires balancing multiple competing factors: data characteristics, computational resources, performance requirements, and deployment constraints. While data patterns provide initial guidance and complexity analysis establishes feasibility bounds, final architectural choices often involve nuanced trade-offs demanding systematic evaluation.

::: {#fig-dnn-fm-framework fig-env="figure" fig-pos="htb"}
```{.tikz}
\begin{tikzpicture}[font=\small\usefont{T1}{phv}{m}{n}]
\tikzset{%
Line/.style={line width=1.0pt,black!50,text=black},
LineA/.style={BrownLine!80,, line width=1.0pt,{-{Triangle[width=0.7*6pt,length=1.40*6pt]}},text=black},
  Box/.style={inner xsep=2pt,
    node distance=7mm,
    draw=GreenLine, line width=0.75pt,
    fill=GreenL,
    text width=32mm,align=flush center,
    minimum width=33mm, minimum height=10mm
  },
    Box1/.style={Box, draw=RedLine, fill=RedL, node distance=30mm},
    Box2/.style={Box, draw=BlueLine, fill=BlueL!50, node distance=9mm,
    },
    RedT/.style={blue!55!black,font=\footnotesize\usefont{T1}{phv}{m}{n}},
 decision/.style = {align=flush center,text width=42mm,minimum width=40mm,diamond, aspect=2.2, node distance=6mm,
                             inner xsep=-3pt,  inner ysep=-2.95ex,fill=VioletL2, draw=VioletLine},
}
\node[Box2](B1){Transformers: Attention mechanisms};
\node[Box2,right=of B1](B2){RNNs: Temporal dependencies};
\node[Box2,right=of B2](B3){CNNs: Local feature detection};
\node[Box2,right=of B3](B4){MLPs: Dense connectivity};
%start
\node[decision,node distance=15mm,above=of $(B2.north east)!0.5!(B3.north west)$](D1){What type\\ of data?};
\node[Box,above=of D1](B0){Start:\\ Define Problem};
%below
\node[Box,node distance=10mm,below=of $(B2.south east)!0.5!(B3.south west)$](D2){Check\\ Constraints};
\node[decision,below=of D2](D3){Memory\\ budget?};
\node[decision,below=of D3](D4){Computational\\ budget?};
\node[decision,below=of D4](D5){Inference\\ speed?};
\node[decision,below=of D5](D6){Accuracy\\ target?};
\node[decision,below=of D6](D7){Deployment \\ready?};
%End
\node[Box,below=of D7](AS){Architecture\\ Selected};
\node[Box1,right=of D7](SD){Scale down model or change architecture};
\node[Box1,left=2.5 of D6](IM){Increase model capacity or change architecture};
%arrows
\draw[LineA](B0)--(D1);
\draw[LineA](D1.south)--++(0,-3mm)-|node[right,align=left,pos=0.77]{Text, complex\\ relations}(B1);
\draw[LineA](D1.south)--++(0,-3mm)-|node[right,align=left,pos=0.75]{Time series,\\ sequences}(B2);
\draw[LineA](D1.south)--++(0,-3mm)-|node[right,align=left,pos=0.75]{Images, spatial\\ patterns}(B3);
\draw[LineA](D1.south)--++(0,-3mm)-|node[right,,align=left,pos=0.75]{Tabular, features\\ unrelated}(B4);
\foreach \a in{1,2,3,4}{
\draw[LineA](B\a.south)--++(0,-4mm)-|(D2);
}
\draw[LineA](D2)--(D3);
\draw[LineA](D3)--node[right]{Yes}node[left,RedT]{Sufficient}(D4);
\draw[LineA](D4)--node[right]{Yes}node[left,RedT]{Acceptable}(D5);
\draw[LineA](D5)--node[right]{Yes}node[left,RedT]{Fast enough}(D6);
\draw[LineA](D6)--node[right]{Yes}node[left,RedT]{Met}(D7);
\draw[LineA](D7)--node[right]{Yes}node[left,RedT]{Hardware suitable}(AS);
%
\draw[LineA](D7)--node[above,pos=0,anchor=south west]{No}node[below,RedT,pos=0,anchor=north west]{Hardware issues}(SD);
\draw[LineA](D3)-|node[above,pos=0,anchor=south west]{No}node[below,RedT,pos=0,anchor=north west]{Insufficient}(SD);
\draw[LineA](D4)-|node[above,pos=0,anchor=south west]{No}node[below,RedT,pos=0,anchor=north west]{Too slow}(SD);
\draw[LineA](D5)-|node[above,pos=0,anchor=south west]{No}node[below,RedT,pos=0,anchor=north west]{Too slow}(SD);
%
\draw[LineA](D6)--node[above,pos=0,anchor=south east]{No}node[below,RedT,pos=0,anchor=north east]{Not met}(IM);
%
\draw[LineA](SD.east)--++(10mm,0)|-node[above,RedT,pos=0.8,anchor=south east]{Insufficient}(D1);
\draw[LineA](IM.west)--++(-6mm,0)|-(D1);
\end{tikzpicture}
```
**Architecture Selection Decision Framework**: A systematic flowchart for choosing neural network architectures based on data characteristics and deployment constraints. The process begins with data type identification (text/sequences/images/tabular) to select initial architecture candidates (Transformers/RNNs/CNNs/MLPs), then iteratively evaluates memory budget, computational cost, inference speed, accuracy targets, and hardware compatibility.
:::

@fig-dnn-fm-framework provides a structured approach to architecture selection decisions, ensuring consideration of all relevant factors while avoiding common pitfalls such as selection based on novelty or perceived sophistication. The decision flowchart guides systematic architecture selection by first matching data characteristics to architectural strengths, then validating against practical constraints. The process is inherently iterative—resource limitations or performance gaps often necessitate reconsidering earlier choices.

This framework applies through four key steps. First, data analysis: pattern types in data provide the strongest initial signal. Spatial data naturally aligns with CNNs, sequential data with RNNs. Second, progressive constraint validation: each constraint check (memory, computational budget, inference speed) acts as a filter. Failing any constraint necessitates either scaling down the current architecture or considering a fundamentally different approach.

Third, iterative trade-off handling when accuracy targets remain unmet. Additional model capacity may be required, necessitating a return to constraint checking. If deployment hardware cannot support the chosen architecture, reconsidering the entire architectural approach may be necessary. Fourth, anticipate multiple iterations, as real projects typically cycle through this framework several times before achieving optimal balance between data fit, computational feasibility, and deployment requirements.

This systematic approach prevents architecture selection based solely on novelty or perceived sophistication, ensuring alignment of choices with both problem requirements and system capabilities.

## Unified Framework: Inductive Biases {#sec-dnn-architectures-unified-framework-inductive-biases-099d}

The architectural diversity explored—from MLPs to Transformers—share a unified theoretical framework: each architecture embodies specific inductive biases that constrain the hypothesis space and guide learning toward solutions appropriate for different data types and problem structures.

Different architectures form a hierarchy of decreasing inductive bias. CNNs exhibit the strongest inductive biases through local connectivity, parameter sharing, and translation equivariance. These constraints dramatically reduce the parameter space while limiting flexibility to spatial data with local structure. RNNs demonstrate moderate inductive bias through sequential processing and shared temporal weights. The hidden state mechanism assumes that past information influences current processing, rendering them appropriate for temporal sequences.

MLPs maintain minimal architectural bias beyond layer-wise processing. Dense connectivity allows modeling arbitrary relationships but requires more data to learn structure that other architectures encode explicitly. Transformers represent adaptive inductive bias through learned attention patterns. The architecture can dynamically adjust its inductive bias based on the data, combining flexibility with the ability to discover relevant structural regularities.

All successful architectures implement forms of hierarchical representation learning, but through different mechanisms. CNNs build spatial hierarchies through progressive receptive field expansion, applying the spatial pattern processing framework detailed in @sec-dnn-architectures-cnns-spatial-pattern-processing-f8ff. RNNs build temporal hierarchies through hidden state evolution, extending the sequential processing approach from @sec-dnn-architectures-rnns-sequential-pattern-processing-ea14. Transformers build content-dependent hierarchies through multi-head attention, applying the dynamic pattern processing mechanisms described in @sec-dnn-architectures-attention-mechanisms-dynamic-pattern-processing-566d.

This hierarchical organization reflects a principle: complex patterns can be efficiently represented through composition of simpler components. The success of deep learning stems from the discovery that gradient-based optimization can effectively learn these compositional structures when provided with appropriate architectural inductive biases.

The theoretical insights about representation learning have direct implications for systems engineering. Hierarchical representations require computational patterns that can efficiently compose lower-level features into higher-level abstractions. This drives system design decisions:

- Memory hierarchies must align with representational hierarchies to minimize data movement costs
- Parallelization strategies must respect the dependency structure of hierarchical computation
- Hardware accelerators must efficiently support the matrix operations that implement feature composition
- Software frameworks must provide abstractions that enable efficient hierarchical computation across diverse architectures

Understanding architectures as embodying different inductive biases helps explain both their strengths and their systems requirements, providing a principled foundation for architecture selection and system optimization decisions.

## Fallacies and Pitfalls {#sec-dnn-architectures-fallacies-pitfalls-3e82}

Neural network architectures represent specialized computational structures designed for different data types and problem domains, which creates common misconceptions about their selection and deployment. The rich variety of architectural patterns—from dense networks to transformers—often leads engineers to make choices based on novelty or perceived sophistication rather than task-specific requirements and computational constraints.

**Fallacy:** _More complex architectures always perform better than simpler ones._

This misconception prompts teams to immediately adopt transformer-based models or elaborate architectures without understanding their requirements. While sophisticated architectures such as transformers excel at complex tasks requiring long-range dependencies, they require significantly more computational resources and memory. For numerous problems, particularly those with limited data or clear structural patterns, simpler architectures such as MLPs or CNNs achieve comparable accuracy with significantly less computational overhead. Architecture selection should correspond to problem complexity rather than defaulting to the most advanced option.

**Pitfall:** _Ignoring the computational implications of architectural choices during model selection._

Many practitioners select architectures based solely on accuracy metrics from academic papers without considering computational requirements. A CNN's spatial locality assumptions might deliver excellent accuracy for image tasks but require specialized memory access patterns. Similarly, RNNs' sequential dependencies create serialization bottlenecks that limit parallelization opportunities. This oversight leads to deployment failures when models cannot meet latency requirements or exceed memory constraints in production environments.

**Fallacy:** _Architecture performance is independent of hardware characteristics._

This belief assumes that all architectures perform equally well across different hardware platforms. In reality, different architectures exploit different hardware features: CNNs benefit from specialized tensor cores, MLPs leverage high-bandwidth memory, and RNNs require efficient sequential processing capabilities. A model that achieves optimal performance on GPUs might perform poorly on mobile devices or embedded processors. Understanding hardware-architecture alignment is crucial for effective deployment strategies.

**Pitfall:** _Mixing architectural patterns without understanding their interaction effects._

Combining different architectural components (such as adding attention layers to CNNs or using skip connections in RNNs) can create unexpected computational bottlenecks. Each architectural pattern exhibits distinct memory access patterns and computational characteristics. Naive combinations may eliminate the performance benefits of individual components or create memory bandwidth conflicts. Successful hybrid architectures require careful analysis of how different patterns interact at the system level.

**Pitfall:** _Designing architectures without considering the full hardware-software co-design implications across the deployment pipeline._

Many architecture decisions optimize for high-end GPU performance without considering the complete system lifecycle from development through deployment. An architecture designed for large-scale compute clusters may be poorly suited for edge deployment due to memory constraints, lack of specialized compute units, or limited parallelization capabilities. Similarly, architectures optimized for inference latency might sacrifice development efficiency, leading to longer development cycles and higher computational costs. Effective architecture selection requires analyzing the entire system stack including compute infrastructure, model compilation and optimization tools, target deployment hardware, and operational constraints. The choice between CNN depth and width, transformer head configurations, or activation functions has cascading effects on memory bandwidth utilization, cache efficiency, and numerical precision requirements that must be considered holistically rather than in isolation.

## Summary {#sec-dnn-architectures-summary-c495}

Neural network architectures form specialized computational structures tailored to process different types of data and solve distinct classes of problems. Multi-Layer Perceptrons handle tabular data through dense connections, convolutional networks exploit spatial locality in images, and recurrent networks process sequential information. Each architecture embodies specific assumptions about data structure and computational patterns. Modern transformer architectures unify many of these concepts through attention mechanisms that dynamically route information based on relevance rather than fixed connectivity patterns.

Despite their apparent diversity, these architectures share fundamental computational primitives that recur across different designs. Matrix multiplication operations form the computational core, whether in dense layers, convolutions, or attention mechanisms. Memory access patterns vary significantly between architectures, with some requiring sliding window operations for local processing while others demand global information aggregation. Dynamic computation patterns in attention mechanisms create data-dependent execution flows that challenge traditional optimization approaches.

::: {.callout-important title="Key Takeaways"}
* Different architectures embody specific assumptions about data structure: MLPs for tabular data, CNNs for spatial relationships, RNNs for sequences, Transformers for flexible attention
* Shared computational primitives including matrix operations, sliding windows, and dynamic routing form the foundation across diverse architectures
* Memory access patterns and data movement requirements vary significantly between architectures, directly impacting system performance and optimization strategies
* Understanding the mapping between algorithmic intent and system implementation enables effective performance optimization and hardware selection
:::

The architectural foundations established in this chapter—computational patterns, memory access characteristics, and data movement primitives—directly inform the design of specialized hardware and optimization strategies explored in subsequent chapters. Understanding that CNNs exhibit spatial locality enables the development of systolic arrays optimized for convolution operations (@sec-ai-acceleration). Recognizing that Transformers demand quadratic memory scaling motivates attention-specific optimizations such as FlashAttention and sparse attention patterns (@sec-model-optimizations). The progression from architectural understanding to hardware design to algorithmic optimization represents a systematic approach to ML systems engineering.

As architectures become more dynamic and sophisticated, the relationship between algorithmic innovation and systems optimization becomes increasingly critical for achieving practical performance gains in real-world deployments. Yet these architectures do not exist in isolation. Implementing and experimenting with CNNs, RNNs, Transformers, and their variants requires software frameworks that abstract the complexity of automatic differentiation, memory management, and hardware acceleration. The next chapter (@sec-ai-frameworks) examines these ML frameworks, revealing how software infrastructure transforms architectural concepts into practical implementations and how framework design choices shape what architectures practitioners can efficiently develop and deploy.

<!-- This is here to make sure that quizzes are inserted properly before a part begins. -->
::: { .quiz-end }
:::
