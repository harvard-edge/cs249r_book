{
  "metadata": {
    "chapter": "dnn_architectures",
    "version": "1.0.0",
    "generated": "2025-09-15T14:08:03.494438",
    "total_terms": 40,
    "standardized": true,
    "last_updated": "2025-09-15T15:01:37.285470"
  },
  "terms": [
    {
      "term": "activation function",
      "definition": "A mathematical function applied to the output of neural network layers to introduce non-linearity, enabling the network to learn complex patterns beyond linear relationships.",
      "chapter_source": "dnn_architectures",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "alexnet",
      "definition": "A groundbreaking convolutional neural network architecture that won the 2012 ImageNet challenge, reducing error rates from 26% to 16% and sparking the deep learning renaissance.",
      "chapter_source": "dnn_architectures",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "attention mechanism",
      "definition": "A neural network component that computes weighted connections between elements based on their content, allowing dynamic focus on relevant parts of the input rather than fixed architectural connections.",
      "chapter_source": "dnn_architectures",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "backpropagation",
      "definition": "The algorithm used to train neural networks by computing gradients of the loss function with respect to network parameters and propagating error signals backward through the network layers.",
      "chapter_source": "dnn_architectures",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "batch normalization",
      "definition": "A technique that normalizes the inputs of each layer to have zero mean and unit variance, stabilizing training and allowing higher learning rates in deep networks.",
      "chapter_source": "dnn_architectures",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "convolution operation",
      "definition": "A mathematical operation that slides a filter (kernel) across input data to detect local features, forming the foundation of convolutional neural networks for spatial pattern recognition.",
      "chapter_source": "dnn_architectures",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "convolutional neural network",
      "definition": "A type of neural network architecture designed for processing spatial data like images, using convolutional layers with local receptive fields and weight sharing to detect spatial patterns.",
      "chapter_source": "dnn_architectures",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "deep learning",
      "definition": "A subset of machine learning that uses neural networks with multiple hidden layers to automatically learn hierarchical representations of data.",
      "chapter_source": "dnn_architectures",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "dense layer",
      "definition": "A fully connected neural network layer where every neuron connects to every neuron in the adjacent layers, enabling unrestricted feature interactions across all inputs.",
      "chapter_source": "dnn_architectures",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "dropout",
      "definition": "A regularization technique that randomly sets a fraction of input units to zero during training to prevent overfitting and improve generalization.",
      "chapter_source": "dnn_architectures",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "encoder-decoder",
      "definition": "An architectural pattern where an encoder processes input into a compressed representation and a decoder generates output from this representation, commonly used in sequence-to-sequence tasks.",
      "chapter_source": "dnn_architectures",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "feature map",
      "definition": "The output of a convolutional layer representing the response of learned filters to different spatial locations in the input, capturing detected features at various positions.",
      "chapter_source": "dnn_architectures",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "feedforward network",
      "definition": "A neural network where information flows in one direction from input to output without cycles, contrasting with recurrent networks that have feedback connections.",
      "chapter_source": "dnn_architectures",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "gradient descent",
      "definition": "An optimization algorithm that iteratively adjusts neural network parameters in the direction that minimizes the loss function by following the negative gradient.",
      "chapter_source": "dnn_architectures",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "gru",
      "definition": "Gated Recurrent Unit, a simplified variant of LSTM that uses fewer gates while maintaining the ability to capture long-term dependencies in sequential data.",
      "chapter_source": "dnn_architectures",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "hidden state",
      "definition": "The internal memory of recurrent neural networks that carries information from previous time steps, enabling the network to maintain context across sequential inputs.",
      "chapter_source": "dnn_architectures",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "kernel",
      "definition": "A small matrix of learnable weights used in convolutional layers to detect specific features through the convolution operation, also called a filter.",
      "chapter_source": "dnn_architectures",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "layer normalization",
      "definition": "A normalization technique that normalizes inputs across the features dimension for each sample, commonly used in transformer architectures to stabilize training.",
      "chapter_source": "dnn_architectures",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "lstm",
      "definition": "Long Short-Term Memory, a type of recurrent neural network architecture designed to handle long-term dependencies through gating mechanisms that control information flow.",
      "chapter_source": "dnn_architectures",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "multi-head attention",
      "definition": "An attention mechanism that uses multiple parallel attention heads, each focusing on different aspects of the input to capture diverse types of relationships simultaneously.",
      "chapter_source": "dnn_architectures",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "multi-layer perceptron",
      "definition": "A feedforward neural network with one or more hidden layers between input and output, capable of learning non-linear mappings through dense connections and activation functions.",
      "chapter_source": "dnn_architectures",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "neural network",
      "definition": "A computational model inspired by biological neural networks, consisting of interconnected nodes (neurons) that process information through weighted connections and activation functions.",
      "chapter_source": "dnn_architectures",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "padding",
      "definition": "A technique in convolutional networks that adds zeros or other values around the input borders to control the spatial dimensions of the output feature maps.",
      "chapter_source": "dnn_architectures",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "pooling",
      "definition": "A downsampling operation in convolutional networks that reduces spatial dimensions while retaining important features, commonly using max or average operations over local regions.",
      "chapter_source": "dnn_architectures",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "positional encoding",
      "definition": "A method used in transformer architectures to inject information about the position of tokens in a sequence, since transformers lack inherent sequential processing.",
      "chapter_source": "dnn_architectures",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "query key value",
      "definition": "The three components of attention mechanisms where queries determine what to look for, keys represent what is available, and values contain the actual information to be weighted and combined.",
      "chapter_source": "dnn_architectures",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "receptive field",
      "definition": "The region of the input that influences a particular neuron's output, determining the spatial extent of patterns that can be detected by that neuron.",
      "chapter_source": "dnn_architectures",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "recurrent neural network",
      "definition": "A type of neural network designed for sequential data processing, featuring connections that create loops allowing information to persist across time steps.",
      "chapter_source": "dnn_architectures",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "regularization",
      "definition": "Techniques used to prevent overfitting in neural networks by adding constraints or penalties, including methods like dropout, weight decay, and data augmentation.",
      "chapter_source": "dnn_architectures",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "residual connection",
      "definition": "A skip connection that adds the input of a layer to its output, enabling the training of very deep networks by mitigating the vanishing gradient problem.",
      "chapter_source": "dnn_architectures",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "resnet",
      "definition": "Residual Network, a deep convolutional architecture that introduced skip connections, enabling the training of networks with hundreds of layers and achieving breakthrough performance.",
      "chapter_source": "dnn_architectures",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "self-attention",
      "definition": "An attention mechanism where queries, keys, and values all come from the same sequence, allowing each position to attend to all positions including itself.",
      "chapter_source": "dnn_architectures",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "skip connection",
      "definition": "A direct connection that bypasses one or more layers, allowing gradients to flow more easily through deep networks and enabling better training of very deep architectures.",
      "chapter_source": "dnn_architectures",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "softmax",
      "definition": "An activation function that converts a vector of real numbers into a probability distribution, commonly used in the output layer for multi-class classification tasks.",
      "chapter_source": "dnn_architectures",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "stride",
      "definition": "The step size by which a convolutional filter moves across the input, controlling the spatial dimensions of the output and the degree of overlap between filter applications.",
      "chapter_source": "dnn_architectures",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "transformer",
      "definition": "A neural network architecture based entirely on attention mechanisms, eliminating recurrence and convolution while achieving state-of-the-art performance across many domains.",
      "chapter_source": "dnn_architectures",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "translation invariance",
      "definition": "The property of convolutional networks to recognize patterns regardless of their position in the input, achieved through weight sharing and pooling operations.",
      "chapter_source": "dnn_architectures",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "universal approximation theorem",
      "definition": "A theoretical result proving that neural networks with sufficient width and non-linear activation functions can approximate any continuous function on a compact domain.",
      "chapter_source": "dnn_architectures",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "vanishing gradient",
      "definition": "A problem in deep neural networks where gradients become exponentially smaller as they propagate backward, making it difficult to train early layers effectively.",
      "chapter_source": "dnn_architectures",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "weight sharing",
      "definition": "The practice of using the same parameters across different spatial locations, as in convolutional networks, reducing the number of parameters while maintaining pattern detection capabilities.",
      "chapter_source": "dnn_architectures",
      "aliases": [],
      "see_also": []
    }
  ]
}
