{
  "metadata": {
    "chapter": "ml_systems",
    "version": "1.0.0",
    "generated": "2025-09-15T14:08:03.499150",
    "total_terms": 27,
    "standardized": true,
    "last_updated": "2025-09-15T15:01:37.288351"
  },
  "terms": [
    {
      "term": "artificial intelligence",
      "definition": "The simulation of human intelligence processes by machines, particularly computer systems, encompassing learning, reasoning, and self-correction capabilities.",
      "chapter_source": "ml_systems",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "machine learning",
      "definition": "The methodological approach to implementing intelligent systems through computational techniques that automatically discover patterns in data, rather than through predetermined rules or explicit programming.",
      "chapter_source": "ml_systems",
      "aliases": ["ML"],
      "see_also": ["artificial intelligence"]
    },
    {
      "term": "speed of light (latency)",
      "definition": "The physical speed limit of information transmission (approx. 200,000 km/s in fiber), creating an irreducible lower bound on network latency that necessitates edge computing for applications requiring sub-10ms response times over long distances.",
      "chapter_source": "ml_systems",
      "aliases": [],
      "see_also": ["latency"]
    },
    {
      "term": "power wall",
      "definition": "The technological barrier reached around 2005 where increasing processor frequency no longer yielded performance gains without unsustainable increases in power density and heat generation, forcing a shift to parallel and specialized architectures.",
      "chapter_source": "ml_systems",
      "aliases": [],
      "see_also": ["dennard scaling"]
    },
    {
      "term": "memory wall",
      "definition": "The widening performance gap between processor speed and memory bandwidth, where computational capacity outpaces the rate at which data can be delivered to the processor, becoming a primary bottleneck for large ML models.",
      "chapter_source": "ml_systems",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "dennard scaling",
      "definition": "The observation that as transistors became smaller, their power density remained constant, enabling higher performance at the same power envelope. Its breakdown around 2005 ended the era of free frequency scaling.",
      "chapter_source": "ml_systems",
      "aliases": [],
      "see_also": ["power wall"]
    },
    {
      "term": "deployment paradigm",
      "definition": "A distinct approach to hosting and executing ML models characterized by specific resource constraints and operational properties, such as Cloud ML, Edge ML, Mobile ML, or TinyML.",
      "chapter_source": "ml_systems",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "train-serve split",
      "definition": "A hybrid architecture pattern where computationally intensive model training occurs on powerful cloud infrastructure, while the trained model is optimized and deployed for inference on resource-constrained edge or mobile devices.",
      "chapter_source": "ml_systems",
      "aliases": [],
      "see_also": ["hybrid machine learning"]
    },
    {
      "term": "collaborative filtering",
      "definition": "A technique used in recommendation systems that predicts user preferences by identifying patterns in interactions from many users, leveraging the collective behavior of the crowd rather than just item properties.",
      "chapter_source": "ml_systems",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "thermal throttling",
      "definition": "A protective mechanism in mobile and embedded devices that reduces processor clock speed and performance to prevent overheating, often limiting the sustained performance of on-device ML inference.",
      "chapter_source": "ml_systems",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "data center",
      "definition": "A facility that houses computer systems and associated components such as telecommunications and storage systems, typically containing thousands of servers for cloud computing operations.",
      "chapter_source": "ml_systems",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "distributed intelligence",
      "definition": "The placement of computational capabilities across multiple devices and locations rather than relying on a single centralized system, enabling local processing and decision-making.",
      "chapter_source": "ml_systems",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "edge computing",
      "definition": "A distributed computing paradigm that brings computation and data storage closer to the sources of data, reducing latency and bandwidth usage.",
      "chapter_source": "ml_systems",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "embedded systems",
      "definition": "Computer systems with dedicated functions within larger mechanical or electrical systems, typically designed for specific tasks with real-time computing constraints.",
      "chapter_source": "ml_systems",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "federated learning",
      "definition": "A machine learning approach that trains algorithms across decentralized edge devices or servers holding local data samples, without exchanging the raw data.",
      "chapter_source": "ml_systems",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "foundation model",
      "definition": "Large-scale machine learning models trained on broad data that can be adapted to a wide range of downstream tasks, serving as a base for specialized applications.",
      "chapter_source": "ml_systems",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "gpu",
      "definition": "Graphics Processing Unit, a specialized electronic circuit designed to rapidly manipulate and alter memory to accelerate the creation of images and parallel processing tasks.",
      "chapter_source": "ml_systems",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "hierarchical processing",
      "definition": "A multi-tier system architecture where data and intelligence flow between different levels of the computing stack, from sensors to edge devices to cloud systems.",
      "chapter_source": "ml_systems",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "hybrid machine learning",
      "definition": "The integration of multiple ML paradigms such as cloud, edge, mobile, and tiny ML to form unified distributed systems that leverage complementary strengths.",
      "chapter_source": "ml_systems",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "hyperscale data center",
      "definition": "Large-scale data center facilities containing thousands of servers and covering extensive floor space, designed to efficiently support massive computing workloads.",
      "chapter_source": "ml_systems",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "batch inference",
      "definition": "The process of using a trained machine learning model to make predictions or decisions on new, previously unseen data.",
      "chapter_source": "ml_systems",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "internet of things",
      "definition": "A network of physical objects embedded with sensors, software, and other technologies that connect and exchange data with other devices and systems over the internet.",
      "chapter_source": "ml_systems",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "latency",
      "definition": "The time delay between a request for data and the delivery of that data, critical in real-time applications where immediate responses are required.",
      "chapter_source": "ml_systems",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "microcontroller",
      "definition": "A small computer on a single integrated circuit containing a processor core, memory, and programmable input/output peripherals, commonly used in embedded systems.",
      "chapter_source": "ml_systems",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "mobile machine learning",
      "definition": "The execution of machine learning models directly on portable, battery-powered devices like smartphones and tablets, enabling personalized and responsive applications.",
      "chapter_source": "ml_systems",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "model compression",
      "definition": "Techniques used to reduce the size and computational requirements of machine learning models while preserving accuracy, enabling deployment on resource-constrained devices.",
      "chapter_source": "ml_systems",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "model quantization",
      "definition": "The process of reducing the precision of numerical representations in machine learning models, typically from 32-bit to 8-bit integers, to decrease model size and increase inference speed.",
      "chapter_source": "ml_systems",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "neural processing unit (npu)",
      "definition": "Specialized processors designed specifically for accelerating neural network operations and machine learning computations, optimized for parallel processing of AI workloads.",
      "chapter_source": "ml_systems",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "real-time processing",
      "definition": "The processing of data as it becomes available, with guaranteed response times that meet strict timing constraints for immediate decision-making.",
      "chapter_source": "ml_systems",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "system on chip",
      "definition": "An integrated circuit that incorporates most or all components of a computer or electronic system, including CPU, GPU, memory, and specialized processors on a single chip.",
      "chapter_source": "ml_systems",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "tensor processing unit (tpu)",
      "definition": "Google's custom application-specific integrated circuit designed specifically for neural network machine learning, optimized for TensorFlow operations.",
      "chapter_source": "ml_systems",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "tiny machine learning",
      "definition": "The execution of machine learning models on ultra-constrained devices such as microcontrollers and sensors, operating in the milliwatt to sub-watt power range.",
      "chapter_source": "ml_systems",
      "aliases": [],
      "see_also": []
    },
    {
      "term": "training",
      "definition": "The process of teaching a machine learning algorithm to make accurate predictions by feeding it large amounts of labeled data and allowing it to learn patterns.",
      "chapter_source": "ml_systems",
      "aliases": [],
      "see_also": []
    }
  ]
}
