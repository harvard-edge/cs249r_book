---
quiz: footnote_context_quizzes.json
concepts: ml_systems_concepts.yml
glossary: ml_systems_glossary.json
engine: jupyter
---

# ML Systems {#sec-ml-system-architecture}

```{python}
#| echo: false
#| label: chapter-start
from physx.registry import start_chapter

start_chapter("vol1:ml_systems")
```

::: {layout-narrow}
::: {.column-margin}
\chapterminitoc
:::

\noindent
![](images/png/cover_ml_systems.png){fig-alt="Split-brain illustration with the left hemisphere showing circuit board patterns and processors on a white background, and the right hemisphere displaying a colorful neural network with various AI application icons and data connections on a blue background."}

:::

## Purpose {.unnumbered}

\begin{marginfigure}
\mlsysstack{35}{20}{30}{30}{30}{30}{25}{25}
\end{marginfigure}

_Why does deploying the same model to a phone versus a datacenter demand fundamentally different engineering?_

\index{ML systems!deployment paradigms} \index{deployment paradigms|see{ML systems}}The defining insight of ML systems engineering is that constraints drive architecture. The speed of light sets an absolute floor on how quickly distant servers can respond. Thermodynamics limits how much computation can occur in a given volume before heat becomes unmanageable. Memory physics makes moving data often more expensive than processing it. These are not engineering limitations awaiting better technology; they are permanent physical boundaries that partition the world into fundamentally distinct operating regimes. A datacenter can train billion-parameter models but cannot guarantee low-latency responses to users thousands of miles away. A smartphone can respond instantly but has a fraction of the memory budget. A microcontroller can run on a coin-cell battery for years but has barely enough compute for a simple keyword detector. The same model—the same algorithm applied to the same data—demands radically different engineering in each regime, not because of design preferences but because different physics governs each environment. Teams that treat deployment as an afterthought—training a model in the cloud and then asking "how do we ship this?"—discover too late that the physics of their target environment invalidates months of architectural decisions. Understanding these regimes transforms deployment from an operational detail into a first-order engineering decision: the question is never simply "how do I make this model work?" but rather "which physical constraints govern my problem, and how do they shape what is even possible?"

::: {.content-visible when-format="pdf"}
\newpage
:::

::: {.callout-tip title="Learning Objectives"}

- Explain how physical constraints (speed of light, power wall, memory wall) necessitate the deployment spectrum from cloud to TinyML.
- Apply the Iron Law and Bottleneck Principle to determine whether a workload is compute-bound, memory-bound, or I/O-bound.
- Map workload archetypes to deployment paradigms using Lighthouse Model examples.
- Distinguish the four deployment paradigms (Cloud, Edge, Mobile, TinyML) by their operational characteristics and quantitative trade-offs.
- Apply the decision framework to select deployment paradigms based on privacy, latency, computational, and cost requirements.
- Analyze hybrid integration patterns to determine which combinations address specific system constraints.
- Evaluate deployment decisions by identifying common fallacies (including Amdahl's Law limits on system speedup) and assessing alignment between architecture and requirements.
- Identify the universal principles (data pipelines, resource management, system architecture) that apply across deployment paradigms and explain why optimization techniques transfer between scales.

:::

```{python}
#| label: ml-systems-setup
#| echo: false
# ┌─────────────────────────────────────────────────────────────────────────────
# │ CHAPTER-WIDE DEPLOYMENT SPECTRUM CONSTANTS
# ├─────────────────────────────────────────────────────────────────────────────
# │ Context: Used across entire chapter — deployment tables, paradigm sections,
# │          physical constraints narrative, and Lighthouse Model summaries.
# │
# │ Why: Foundational chapter setup providing deployment paradigm parameters
# │      (latency ranges, power budgets, memory specs) that appear in multiple
# │      sections. Centralized here to ensure consistency across the chapter.
# │
# │ Imports: physx.constants (MOBILE_TDP_W, MOBILE_NPU_TOPS_INT8, PHONE_BATTERY_WH,
# │          KWS_DSCNN_PARAMS, DLRM_MODEL_SIZE_FP32), physx.formatting (fmt)
# │ Exports: *_range_str (latency/RAM/storage), *_str (paradigm specs), gpt3_*_str
# └─────────────────────────────────────────────────────────────────────────────
from physx.constants import (
    MOBILE_TDP_W,
    MOBILE_NPU_TOPS_INT8,
    PHONE_BATTERY_WH,
    KWS_DSCNN_PARAMS,
    DLRM_MODEL_SIZE_FP32,
    TFLOPs,
    Kparam,
    second,
    watt,
    hour,
    GB,
)
from physx.formatting import fmt

# --- Inputs (typical ranges from industry data) ---
mobile_ram_range_str = "8-16"                            # GB, mid-to-flagship phones
mobile_storage_range_str = "128 GB-1 TB"                 # typical smartphone range
mobile_bw_range_str = "25-50"                            # GB/s LPDDR5 bandwidth
cloud_latency_range_str = "100-500"                      # ms round-trip
edge_latency_range_str = "10-100"                        # ms local processing
mobile_latency_range_str = "5-50"                        # ms on-device inference
tiny_latency_range_str = "1-10"                          # ms microcontroller response
mobilenet_flops_reduction_str = "8-9"                    # × vs ResNet-50
compute_doubling_months_value = 18                       # months (Moore's Law approx)
mem_bw_growth_pct_value = 20                             # % annual improvement
gpt3_petaflop_days_value = 3_640                         # petaflop-days training
gpt3_v100_count_value = 10_000                           # GPUs used
gpt3_days_value = 15                                     # approximate training time
gpt3_cost_m_value = 4.6                                  # $M estimated cost (2020)

# --- Process (unit conversions) ---
mobile_tdp_w_value = MOBILE_TDP_W.magnitude
mobile_npu_tops_value = MOBILE_NPU_TOPS_INT8.to(TFLOPs / second).magnitude
phone_battery_wh_value = PHONE_BATTERY_WH.to(watt * hour).magnitude
kws_params_value = int(KWS_DSCNN_PARAMS.to(Kparam).magnitude)
dlrm_embedding_gb_value = int(DLRM_MODEL_SIZE_FP32.to(GB).magnitude)

# --- Outputs (formatted strings for prose) ---
mobile_tdp_str = fmt(mobile_tdp_w_value, precision=0, commas=False)   # e.g. "5" W
mobile_tdp_range_str = "3-5"                                          # W typical range
mobile_npu_tops_str = fmt(mobile_npu_tops_value, precision=0, commas=False)  # e.g. "10" TOPS
mobile_npu_range_str = "1-10"                                         # TOPS typical range
compute_doubling_months_str = fmt(compute_doubling_months_value, precision=0, commas=False)
mem_bw_growth_pct_str = fmt(mem_bw_growth_pct_value, precision=0, commas=False)
gpt3_petaflop_days_str = fmt(gpt3_petaflop_days_value, precision=0, commas=True)   # "3,640"
gpt3_v100_count_str = fmt(gpt3_v100_count_value, precision=0, commas=True)         # "10,000"
gpt3_days_str = fmt(gpt3_days_value, precision=0, commas=False)                    # "15"
gpt3_cost_m_str = fmt(gpt3_cost_m_value, precision=1, commas=False)                # "4.6"
phone_battery_str = fmt(phone_battery_wh_value, precision=0, commas=False)         # e.g. "15" Wh
kws_params_str = "70,000"                                # typical small KWS model
kws_size_kb_str = "100"                                  # KB model size
dlrm_embedding_str = f"{dlrm_embedding_gb_value}"        # e.g. "100" GB
```

## Deployment Paradigm Framework {#sec-ml-system-architecture-deployment-paradigm-framework-0d25}

\index{ML systems!deployment spectrum} \index{physical constraints!deployment implications}Where an ML model runs shapes what is possible in ways no algorithmic choice can override. Yet deployment is far harder than it appears, and the reason is not the model itself. In production ML systems, the model accounts for roughly 5% of the codebase [@sculley2015hidden]. The remaining 95% consists of data collection, feature processing, serving infrastructure, monitoring, and resource management. All of this surrounding infrastructure changes dramatically depending on where the model executes.

Consider two extremes: a wake-word detector on a smartwatch and a recommendation engine in a data center. The wake-word detector represents a **TinyML** workload operating under milliwatt power budgets and kilobyte memory limits; the recommendation engine exemplifies a **Cloud ML** workload requiring terabytes of embedding tables and megawatt-scale infrastructure. These systems solve different problems under opposite physical constraints, and the infrastructure that supports them shares almost nothing in common. This reality transforms deployment from an operational afterthought into a first-order engineering decision, one that the AI Triad from @sec-introduction helps us reason about by foregrounding infrastructure alongside data and algorithms.

What makes these systems so different? The physical constraints that govern each environment—latency, power, and memory—force ML deployment into four distinct paradigms, each with its own engineering trade-offs and system design patterns. **Cloud ML**\index{Cloud ML!characteristics} aggregates computational resources in data centers, offering virtually unlimited compute and storage at the cost of network latency. **Edge ML**\index{Edge ML!latency benefits} moves computation closer to where data originates—factory floors, retail stores, hospitals—achieving lower latency and keeping sensitive data on-premises. **Mobile ML**\index{Mobile ML!energy constraints} brings intelligence directly to smartphones and tablets, balancing computational capability against battery life and thermal constraints. **TinyML**\index{TinyML!always-on sensing} pushes intelligence to the smallest devices—microcontrollers costing dollars and consuming milliwatts—enabling always-on sensing that runs for months on a coin-cell battery. These four paradigms span nine orders of magnitude in power consumption (megawatts to milliwatts) and memory capacity (terabytes to kilobytes), a range so vast that the engineering principles governing one end of the spectrum barely apply at the other.

*Why do these paradigms exist?* The answer lies not in engineering choices but in physical laws that no amount of optimization can overcome. Three fundamental constraints—the speed of light (establishing latency floors), thermodynamic limits on power dissipation (capping computation per watt), and the energy cost of memory signaling (creating the Memory Wall)—carve the deployment landscape into distinct operating regimes. These are not design preferences but physical boundaries: you cannot serve a self-driving car from a data center 100 ms away, and you cannot train a 175-billion-parameter model on a microcontroller. Understanding *why* these boundaries exist, not just *where* they fall, is what separates systems engineering from ad hoc deployment.

These physical constraints interact with the **Iron Law of ML Systems** (@sec-silicon-contract), which decomposes end-to-end latency into data movement, computation, and overhead. Different deployment environments stress different terms of this equation: cloud systems are typically compute-bound, mobile systems hit power walls, and TinyML devices are memory-capacity-limited. By pairing the physical constraints with the Iron Law, we develop a quantitative vocabulary for reasoning about *which* paradigm fits a given workload and *why*. To anchor this analysis concretely, the chapter introduces five **Lighthouse Models**—ResNet-50, GPT-2, DLRM, MobileNet, and a Keyword Spotter—that span the deployment spectrum and isolate distinct system bottlenecks. These reference workloads recur throughout the book, providing a consistent basis for comparing optimization techniques across chapters.

This chapter proceeds in three stages. First, we examine the physics that creates the paradigm boundaries—the Light Barrier, Power Wall, and Memory Wall—and develop the analytical tools (Iron Law, Bottleneck Principle, Workload Archetypes) for mapping workloads to deployment targets. Second, we trace each paradigm in depth, from Cloud through Edge, Mobile, and TinyML, analyzing the infrastructure, trade-offs, and representative applications that define each operating regime. Third, we develop a comparative decision framework and explore the hybrid architectures that combine paradigms when no single deployment target can satisfy all requirements. The chapter concludes with the fallacies and pitfalls that commonly mislead practitioners making deployment decisions.

\index{deployment paradigms!four paradigms} \index{Cloud ML|seealso{Edge ML}} \index{Edge ML|seealso{Mobile ML}} \index{Mobile ML|seealso{TinyML}}Think of these four paradigms as distinct "operating envelopes"—each defined by how much power, memory, and network connectivity is available. Every ML application must fit within at least one of these envelopes, and that fit determines which algorithms, hardware, and engineering trade-offs apply. The four paradigms span a continuous spectrum from centralized cloud infrastructure to distributed ultra-low-power devices—trace this spectrum visually in @fig-cloud-edge-TinyML-comparison and compare the quantitative trade-offs in @tbl-deployment-paradigms-overview. The figure maps where each paradigm sits along the centralization axis; the table pins down the numbers behind each one.

::: {#fig-cloud-edge-TinyML-comparison fig-env="figure" fig-pos="t" fig-cap="**Distributed Intelligence Spectrum**: Machine learning deployment spans from centralized cloud infrastructure to resource-constrained TinyML devices, each balancing processing location, device capability, and network dependence. Source: [@abiresearch2024tinyml]." fig-alt="Horizontal spectrum showing 5 deployment tiers from left to right: ultra-low-power devices and sensors, intelligent device, gateway, on-premise servers, and cloud. Arrows indicate TinyML, Edge AI, and Cloud AI spans across the spectrum."}
```{.tikz}
\begin{tikzpicture}[line cap=round,line join=round,font=\usefont{T1}{phv}{m}{n}\small]
  % Parameters
  \def\angle{10}        % angle
  \def\length{18}       % Lengths (cm)
  \def\npoints{5}       % number of points
  \def\startfrac{0.13}  % start (e.g.. 0.2 = 20%)
  \def\endfrac{0.87}    % end (e.g.. 0.8 = 80%)

 \draw[line width=1pt, black!70] (0,0) -- ({\length*cos(\angle)}, {\length*sin(\angle)})coordinate(end);
 %
  \foreach \i in {0,1,...,\numexpr\npoints-1} {
    \pgfmathsetmacro{\t}{\startfrac + (\endfrac - \startfrac)*\i/(\npoints-1)}
\coordinate(T\i)at({\t*\length*cos(\angle)}, {\t*\length*sin(\angle)});
  }

\tikzset {
pics/gatewey/.style = {
        code = {
\colorlet{red}{white}
\begin{scope}[local bounding box=GAT,scale=0.9, every node/.append style={transform shape}]
\def\rI{4mm}
\def\rII{2.8mm}
\def\rIII{1.6mm}
\draw[red,line width=1.25pt](0,0)--(0,0.38)--(1.2,0.38)--(1.2,0)--cycle;
\draw[red,line width=1.5pt](0.6,0.4)--(0.6,0.9);

\draw[red, line width=1.5pt] (0.6,0.9)+(60:\rI) arc[start angle=60, end angle=-60, radius=\rI];
\draw[red, line width=1.5pt] (0.6,0.9)+(50:\rII) arc[start angle=50, end angle=-50, radius=\rII];
\draw[red, line width=1.5pt] (0.6,0.9)+(30:\rIII) arc[start angle=30, end angle=-30, radius=\rIII];
%
 \draw[red, line width=1.5pt] (0.6,0.9)+(120:\rI) arc[start angle=120, end angle=240, radius=\rI];
\draw[red, line width=1.5pt] (0.6,0.9)+(130:\rII) arc[start angle=130, end angle=230, radius=\rII];
\draw[red, line width=1.5pt] (0.6,0.9)+(150:\rIII) arc[start angle=150, end angle=210, radius=\rIII];
\fill[red](0.6,0.9)circle (1.5pt);

\foreach\i in{0.15,0.3,0.45,0.6}{
\fill[red](\i,0.19)circle (1.5pt);
}

\fill[red](1,0.19)circle (2pt);
\end{scope}
}}}

\tikzset {
pics/cloud/.style = {
        code = {
\colorlet{red}{white}
\begin{scope}[local bounding box=CLO,scale=0.6, every node/.append style={transform shape}]
\draw[red,line width=1.5pt](0,0)to[out=170,in=180,distance=11](0.1,0.61)
to[out=90,in=105,distance=17](1.07,0.71)
to[out=20,in=75,distance=7](1.48,0.36)
to[out=350,in=0,distance=7](1.48,0)--(0,0);
\draw[red,line width=1.5pt](0.27,0.71)to[bend left=25](0.49,0.96);
\draw[red,line width=1.5pt](0.67,1.21)to[out=55,in=90,distance=13](1.5,0.96)
to[out=360,in=30,distance=9](1.68,0.42);
\end{scope}
}}}

\tikzset {
  pics/server/.style = {
    code = {
      \colorlet{red}{white}
      \begin{scope}[anchor=center, transform shape,scale=0.8, every node/.append style={transform shape}]
        \draw[red,line width=1.25pt,fill=white](-0.55,-0.5) rectangle (0.55,0.5);
\foreach \i in {-0.25,0,0.25} {
                \draw[BlueLine,line width=1.25pt]( -0.55,\i) -- (0.55, \i);
}
        \foreach \i in {-0.375, -0.125, 0.125, 0.375} {
          \draw[BlueLine,line width=1.25pt](-0.45,\i)--(0,\i);
          \fill[BlueLine](0.35,\i) circle (1.5pt);
        }

\draw[red,line width=1.75pt](0,-0.53) |- (-0.55,-0.7);
        \draw[red,line width=1.75pt](0,-0.53) |- (0.55,-0.7);
      \end{scope}
    }
  }
}

\tikzset {
pics/cpu/.style = {
        code = {
\definecolor{CPU}{RGB}{0,120,176}
\colorlet{CPU}{white}
\begin{scope}[local bounding box = CPU,scale=0.33, every node/.append style={transform shape}]
\node[fill=CPU,minimum width=66, minimum height=66,
            rounded corners=2,outer sep=2pt] (C1) {};
\node[fill=violet,minimum width=54, minimum height=54] (C2) {};
%\node[fill=CPU!40,minimum width=44, minimum height=44] (C3) {CPU};

\foreach \x/\y in {0.11/1,0.26/2,0.41/3,0.56/4,0.71/5,0.85/6}{
\node[fill=CPU,minimum width=4, minimum height=15,
           inner sep=0pt,anchor=south](GO\y)at($(C1.north west)!\x!(C1.north east)$){};
}
\foreach \x/\y in {0.11/1,0.26/2,0.41/3,0.56/4,0.71/5,0.85/6}{
\node[fill=CPU,minimum width=4, minimum height=15,
           inner sep=0pt,anchor=north](DO\y)at($(C1.south west)!\x!(C1.south east)$){};
}
\foreach \x/\y in {0.11/1,0.26/2,0.41/3,0.56/4,0.71/5,0.85/6}{
\node[fill=CPU,minimum width=15, minimum height=4,
           inner sep=0pt,anchor=east](LE\y)at($(C1.north west)!\x!(C1.south west)$){};
}
\foreach \x/\y in {0.11/1,0.26/2,0.41/3,0.56/4,0.71/5,0.85/6}{
\node[fill=CPU,minimum width=15, minimum height=4,
           inner sep=0pt,anchor=west](DE\y)at($(C1.north east)!\x!(C1.south east)$){};
}
\end{scope}
    }  }}

\tikzset {
pics/mobile/.style = {
        code = {
\colorlet{red}{white}
\begin{scope}[local bounding box=MOB,scale=0.4, every node/.append style={transform shape}]
\node[rectangle,draw=red,minimum height=94,minimum width=47,
            rounded corners=6,thick,fill=white](R1){};
\node[rectangle,draw=red,minimum height=67,minimum width=38,thick,fill=GreenFill](R2){};
\node[circle,minimum size=8,below= 2pt of R2,inner sep=0pt,thick,fill=GreenFill]{};
\node[rectangle,fill=GreenFill,minimum height=2,minimum width=20,above= 4pt of R2,inner sep=0pt,thick]{};
%
 \end{scope}
     }  }}

\node[draw=none,fill=RedFill,circle,minimum size=20mm](GA)at(T2){};
\pic[shift={(-0.55,-0.5)}] at (T2) {gatewey};
\node[above=0 of GA]{Gateway};
\node[draw=none,fill=VioletL,circle,minimum size=20mm](CP)at(T0){};
\pic[shift={(0,-0)}] at (T0) {cpu};
\node[above=0 of CP,align=center]{Ultra Low Powered\\Devices and Sensors};
\node[draw=none,fill=GreenFill,,circle,minimum size=20mm](MO)at(T1){};
 \pic[shift={(0,0)}] at (T1) {mobile};
 \node[above=0 of MO,align=center]{Intelligent\\Device};
\node[draw=none,fill=BlueFill,circle,minimum size=20mm](SE)at(T3){};
\pic[shift={(-0.03,0.1)}] at (T3) {server};
 \node[above=0 of SE,align=center]{On Premise\\Servers};
\node[draw=none,fill=BrownL,circle,minimum size=20mm](CL)at(T4){};
\pic[shift={(-0.48,-0.35)}] at (T4) {cloud};
 \node[above=0 of CL,align=center]{Cloud};
%
\path (T0) -- (T1) coordinate[pos=0.5] (M1);
\path (0,0) -- (T0) coordinate[pos=0.25] (M0);
\path (T3) -- (T4) coordinate[pos=0.5] (M2);
\path (T4) -- (end) coordinate[pos=0.75] (M3);

\foreach \x in {0,1,2,3}{
\fill[OliveLine](M\x)circle (2.5pt);
}

\path[red](M0)--++(270:1.6)coordinate(LL1)-|coordinate(LL2)(M2);
\path[red](M0)--++(270:1.1)coordinate(L1)-|coordinate(L2)(M1);
\path[red](M0)--++(270:1.1)-|coordinate(L3)(M2);
\path[red](M0)--++(270:1.1)-|coordinate(L4)(M3);
%
\draw[black!70,thick](M0)--(LL1);
\draw[black!70,thick](M1)--(L2);
\draw[black!70,thick](M3)--(L4);
\draw[black!70,thick](M2)--(LL2);
\draw[latex-latex,line width=1pt,draw=black!60](L1)--node[red,fill=white]{TinyML}(L2);
\draw[latex-latex,line width=1pt,draw=black!60](L3)--node[fill=white]{Cloud AI}(L4);
\draw[latex-latex,line width=1pt,draw=black!60]([yshift=4pt]LL1)--node[fill=white,text=black]{Edge AI}([yshift=4pt]LL2);
\foreach \x in {0,1,2,3}{
\fill[OliveLine](M\x)circle (2.5pt);
}
%
\path[](M0)--++(90:4.2)-|node[pos=0.25]{\textbf{The Distributed Intelligence Spectrum}}(M3);
\end{tikzpicture}

```
:::

@tbl-deployment-paradigms-overview compares the quantitative trade-offs across these four paradigms:

| **Paradigm**  | **Where**        |                            **Latency** | **Power**                         | **Memory** | **Best For**                 |
|:------------|:---------------|-------------------------------------:|:--------------------------------|:---------|:---------------------------|
| **Cloud ML**  | Data centers     |  `{python} cloud_latency_range_str` ms | MW                                | TB         | Training, complex inference  |
| **Edge ML**   | Local servers    |   `{python} edge_latency_range_str` ms | 100s W                            | GB         | Real-time inference, privacy |
| **Mobile ML** | Smartphones      | `{python} mobile_latency_range_str` ms | `{python} mobile_tdp_range_str` W | GB         | Personal AI, offline         |
| **TinyML**    | Microcontrollers |   `{python} tiny_latency_range_str` ms | mW                                | KB         | Always-on sensing            |

: **The Deployment Spectrum (Conceptual)**\index{deployment spectrum!power range}\index{deployment spectrum!memory range}: Four paradigms span nine orders of magnitude in power (MW to mW) and memory (TB to KB). This conceptual overview defines each paradigm by its operating regime; @tbl-representative-systems later grounds these categories in specific hardware platforms and quantitative decision thresholds. The hardware specifications and physical constants underpinning these numbers are catalogued in @sec-system-assumptions. {#tbl-deployment-paradigms-overview}

The nine-order-of-magnitude span in @tbl-deployment-paradigms-overview is not an accident of engineering history—it is a consequence of physics. No amount of optimization can make a datacenter respond faster than light can travel, or make a microcontroller dissipate more heat than its surface area allows. The question "why do these four paradigms exist, rather than a single universal solution?" has a precise answer rooted in three physical laws.

## Physical Constraints: Why Paradigms Exist {#sec-ml-system-architecture-deployment-spectrum-71be}

\index{Silicon Contract!physical constraints} \index{physical constraints!speed of light} \index{physical constraints!thermodynamics} \index{physical constraints!memory signaling}The physical laws of **speed of light**, **power thermodynamics**, and **memory signaling** dictate that no single "ideal" computer exists. Where a system runs reshapes the contract between model and hardware. These three constraints—which we call the *Light Barrier*, *Power Wall*, and *Memory Wall*—govern the engineering trade-offs ahead.[^fn-paradigm]

### The Light Barrier {.unnumbered}

\index{Light Barrier!latency floor}The Light Barrier establishes the absolute latency[^fn-latency-etymology] floor. The minimum round-trip time is governed by @eq-latency-physics:

$$\text{Latency}_{\min} = \frac{2 \times \text{Distance}}{c_{\text{fiber}}} \approx \frac{2 \times \text{Distance}}{200{,}000 \text{ km/s}}$$ {#eq-latency-physics}

```{python}
#| label: light-barrier-calc
#| echo: false
# ┌─────────────────────────────────────────────────────────────────────────────
# │ LIGHT BARRIER LATENCY CALCULATION
# ├─────────────────────────────────────────────────────────────────────────────
# │ Context: "The Light Barrier" narrative (Physical Constraints section)
# │
# │ Why: Demonstrates that physics, not engineering, sets the latency floor.
# │      California-to-Virginia round-trip shows ~36ms minimum BEFORE any
# │      computation begins — motivating why Edge/TinyML exist.
# │
# │ Imports: physx.constants (SPEED_OF_LIGHT_FIBER_KM_S), physx.formulas
# │ Exports: min_latency_str, distance_str
# └─────────────────────────────────────────────────────────────────────────────
from physx.constants import SPEED_OF_LIGHT_FIBER_KM_S
from physx.formulas import calc_network_latency_ms
from physx.formatting import fmt

# --- Inputs (cross-country distance) ---
distance_km_value = 3600                                 # California to Virginia (straight-line)

# --- Process (light-speed round-trip) ---
min_latency_ms_value = calc_network_latency_ms(distance_km_value)

# --- Outputs (formatted strings for prose) ---
min_latency_str = fmt(min_latency_ms_value, precision=0, commas=False)  # e.g. "36" ms
distance_str = f"{distance_km_value:,}"                                  # e.g. "3,600" km
```

California to Virginia (~`{python} distance_str` km straight-line) requires **~`{python} min_latency_str` ms minimum** before any computation begins. Actual cloud services typically add 60–150 ms of software overhead. Applications requiring sub-10 ms response *cannot* use distant cloud infrastructure—physics forbids it. This constraint creates the need for **Edge ML** and **TinyML**: when latency budgets are tight, computation must move closer to the data source.

### The Power Wall {.unnumbered}

\index{Power Wall!thermal limits}\index{Power Wall!frequency scaling}\index{thermal design power (TDP)|see{Power Wall}}The Power Wall emerged because thermodynamics limits how much computation can occur in a given volume. Under classical Dennard scaling (which held until approximately 2006), the relationship between power and frequency was cubic. Here $C$ is effective capacitance, $V$ is voltage, and $f$ is clock frequency. As voltage tracks frequency ($V \propto f$), power rises as $f^3$, as @eq-power-scaling shows:

$$\text{Power} \propto C \times V^2 \times f \quad \text{where } V \propto f \implies \text{Power} \propto f^3$$ {#eq-power-scaling}

Doubling clock frequency required approximately 8× more power. The breakdown of this scaling relationship ended the era of "free" speedups via frequency scaling and forced the industry toward the parallelism (multi-core) and specialization (GPUs, TPUs) that defines modern ML. Mobile devices hit hard thermal limits at `{python} mobile_tdp_range_str` W; exceeding this causes "throttling," where the device reduces performance to prevent overheating. This physical limit gives rise to **Mobile ML**: battery-powered devices cannot simply run cloud-scale models locally.

```{python}
#| label: memory-wall-calc
#| echo: false
# ┌─────────────────────────────────────────────────────────────────────────────
# │ MEMORY WALL CALCULATION
# ├─────────────────────────────────────────────────────────────────────────────
# │ Context: "The Memory Wall" narrative (Physical Constraints section)
# │
# │ Why: Shows the widening gap between compute growth (~1.6×/yr) and memory
# │      bandwidth growth (~1.2×/yr). This ~1.33× annual divergence explains
# │      why data movement dominates modern ML system costs.
# │
# │ Imports: physx.formatting (fmt)
# │ Exports: compute_growth_str, mem_bw_growth_str, mem_wall_ratio_str
# └─────────────────────────────────────────────────────────────────────────────
from physx.formatting import fmt

# --- Inputs (historical growth rates) ---
compute_growth_per_yr_value = 1.6                        # × per year (compute scaling)
mem_bw_growth_per_yr_value = 1.2                         # × per year (memory bandwidth)

# --- Process (compute divergence ratio) ---
mem_wall_ratio_value = compute_growth_per_yr_value / mem_bw_growth_per_yr_value

# --- Outputs (formatted strings for prose) ---
compute_growth_str = fmt(compute_growth_per_yr_value, precision=1, commas=False)  # "1.6"
mem_bw_growth_str = fmt(mem_bw_growth_per_yr_value, precision=1, commas=False)    # "1.2"
mem_wall_ratio_str = fmt(mem_wall_ratio_value, precision=2, commas=False)         # "1.33"
```

### The Memory Wall {.unnumbered}

\index{Memory Wall!bandwidth divergence}\index{Memory Wall!compute-memory gap}\index{memory bandwidth|see{Memory Wall}}The Memory Wall [@wulf1995memory] reflects the widening bandwidth[^fn-bandwidth] gap:

$$\frac{\text{Compute Growth}}{\text{Memory BW Growth}} \approx \frac{1.6\times\text{/year}}{1.2\times\text{/year}} \approx 1.33\times\text{/year}$$ {#eq-memory-wall}

@eq-memory-wall quantifies this divergence: processors have doubled in compute capacity roughly every `{python} compute_doubling_months_str` months, but memory bandwidth has improved only ~`{python} mem_bw_growth_pct_str`% annually. This widening gap makes data movement the dominant bottleneck and energy cost for most ML workloads. This constraint affects all paradigms but is especially acute for **TinyML**, where devices have only kilobytes of memory to work with.

::: {.callout-checkpoint title="Physical Constraints and Deployment" collapse="false"}
Deployment choices are governed by physics, not just preference. Check your understanding:

- [ ] **Light Barrier**: Can you explain why the speed of light makes Cloud ML impossible for <10 ms safety tasks?
- [ ] **Power Wall**: Do you understand why thermodynamics (heat dissipation) prevents datacenter models from running on mobile devices?
- [ ] **Memory Wall**: Can you explain why data movement is often more expensive (in time and energy) than computation?
:::

These physical laws explain *why* the four paradigms exist. Physics creates the boundaries; other forces—privacy regulation, economic incentives, data sovereignty requirements—reinforce and sharpen them. We examine these additional drivers within each paradigm section below, but the key insight is that the paradigms would exist even without those concerns. No amount of regulation can make the speed of light faster, and no economic model can repeal thermodynamics.

Knowing *that* these barriers exist is necessary but not sufficient. Given a specific ML workload—say, a recommendation engine or a wake-word detector—how do we determine which paradigm fits? Which barrier will the workload hit first? The answer requires analytical tools that connect workload characteristics to these physical constraints: the Iron Law to decompose latency, the Bottleneck Principle to identify the dominant constraint, and a set of workload archetypes to classify where each model falls on the spectrum.

## Analyzing Workloads {#sec-ml-system-architecture-analyzing-workloads}

\index{Iron Law of ML Systems!components} \index{Silicon Contract!Iron Law}The central analytical tool for this chapter is the **Iron Law of ML Systems**, established in @sec-introduction (@sec-silicon-contract) and restated here as @eq-iron-law:

$$T = \frac{D_{vol}}{BW} + \frac{O}{R_{peak} \cdot \eta} + L_{lat}$$ {#eq-iron-law}

This equation decomposes total latency into three terms: data movement ($D_{vol}/BW$), compute ($O / (R_{peak} \cdot \eta)$), and fixed overhead ($L_{lat}$). For a single inference, these costs simply add up—you pay each one sequentially. But in production systems, tasks are processed continuously as a stream, and the question shifts from "how long does one task take?" to "which of these three terms actually limits the system?" The answer depends entirely on the deployment environment: a model that is compute-bound during training may become memory-bound during inference; a system that runs efficiently in the cloud may hit power limits on mobile devices. To determine which term dominates, we need a companion principle.

### The Bottleneck Principle {#sec-ml-systems-bottleneck-principle}

\index{bottleneck principle!pipelined execution} \index{system bottlenecks!identifying} \index{compute-bound vs memory-bound!definition}The Iron Law tells us the cost of each term. The **Bottleneck Principle** tells us which term *matters*. Unlike traditional software where optimizing the average case works, ML systems are dominated by their slowest component—optimizing fast operations yields zero benefit while the slowest stage remains unchanged. Modern accelerators use **pipelined execution** to overlap data movement with computation: while the GPU computes on batch $n$, the memory system prefetches batch $n+1$. With this overlap, the system's throughput is determined by whichever operation is slower—the faster one "hides" behind it. The Iron Law's sum becomes a maximum, as @eq-bottleneck formalizes:

$$ T_{bottleneck} = \max\left(\frac{D_{vol}}{BW}, \frac{O}{R_{peak} \cdot \eta}, T_{network}\right) + L_{lat} $$ {#eq-bottleneck}

*   **$\frac{D_{vol}}{BW}$ (Memory)**: Time to move data between memory and processor.
*   **$\frac{O}{R_{peak} \cdot \eta}$ (Compute)**: Time to execute calculations.
*   **$T_{network}$**: Time for network communication (if offloading).
*   **$L_{lat}$ (Overhead)**: Fixed latency (kernel launch, runtime overhead).

This principle dictates that if your system is **Memory Bound**\index{memory-bound workloads!optimization strategy}\index{compute-bound vs memory-bound!memory-bound} ($D_{vol}/BW > O/(R_{peak} \cdot \eta)$), buying faster processors ($R_{peak}$) yields exactly **0% speedup**—just as widening a six-lane highway yields no benefit when all traffic must funnel through a two-lane bridge. You must identify the dominant term before optimizing. This tradeoff is governed by *the energy of transmission*.

```{python}
#| label: energy-transmission-calc
#| echo: false
# ┌─────────────────────────────────────────────────────────────────────────────
# │ ENERGY OF TRANSMISSION CALCULATION
# ├─────────────────────────────────────────────────────────────────────────────
# │ Context: Callout "The Energy of Transmission" (Bottleneck Principle section)
# │
# │ Why: Compares energy cost of cloud offload vs local compute. Transmitting
# │      1MB costs ~100mJ (Wi-Fi/LTE), while NPU inference costs ~0.1mJ — a
# │      1000× difference that makes cloud offloading physically prohibitive
# │      for always-on battery devices.
# │
# │ Imports: physx.formatting (fmt)
# │ Exports: et_*_str variables for callout
# └─────────────────────────────────────────────────────────────────────────────
from physx.formatting import fmt

# --- Inputs (energy costs per operation) ---
et_data_mb_value = 1                                     # MB (1 second of audio)
et_tx_energy_mj_value = 100                              # mJ/MB (Wi-Fi/LTE transmission)
et_compute_energy_mj_value = 0.1                         # mJ/inference (MobileNet on NPU)

# --- Process (energy comparison) ---
et_cloud_energy_mj_value = et_data_mb_value * et_tx_energy_mj_value
et_local_energy_mj_value = et_compute_energy_mj_value
et_energy_ratio_value = int(et_cloud_energy_mj_value / et_local_energy_mj_value)

# --- Outputs (formatted strings for prose) ---
et_data_mb_str = fmt(et_data_mb_value, precision=0, commas=False)        # "1" MB
et_tx_energy_str = fmt(et_tx_energy_mj_value, precision=0, commas=False) # "100" mJ/MB
et_compute_energy_str = fmt(et_compute_energy_mj_value, precision=1, commas=False)  # "0.1" mJ
et_cloud_energy_str = str(int(et_cloud_energy_mj_value))                 # "100" mJ total
et_local_energy_str = fmt(et_local_energy_mj_value, precision=1, commas=False)      # "0.1" mJ
et_energy_ratio_str = fmt(et_energy_ratio_value, precision=0, commas=True)  # "1,000" × ratio
```

::: {.callout-notebook title="The Energy of Transmission"}

\index{energy of transmission!local vs cloud} \index{Energy Wall!battery constraints}**Problem**: Should a battery-powered sensor process data locally (TinyML) or send it to the cloud?

**The Variables**:

*   **Data ($D_{vol}$)**: `{python} et_data_mb_str` MB (e.g., 1 second of audio).
*   **Transmission Energy ($E_{tx}$)**: `{python} et_tx_energy_str` mJ/MB (Wi-Fi/LTE).
*   **Compute Energy ($E_{op}$)**: `{python} et_compute_energy_str` mJ/inference (MobileNet on NPU).

**The Calculation**:

1.  **Cloud Approach**: $E_{cloud} \approx D_{vol} \times E_{tx}$ = `{python} et_data_mb_str` MB × `{python} et_tx_energy_str` mJ/MB = **`{python} et_cloud_energy_str` mJ**.
2.  **Local Approach**: $E_{local} \approx$ Inference = **`{python} et_local_energy_str` mJ**.

**The Systems Conclusion**: Transmitting raw data is **`{python} et_energy_ratio_str`$\times$ more expensive** than processing it locally. Even if the cloud had infinite speed ($Time \approx 0$), the **Energy Wall** makes cloud offloading physically impossible for always-on battery devices. The "Machine" constraint (Battery) dictates the "Algorithm" choice (TinyML).
:::

The **Iron Law's** variables interact differently across deployment scenarios. Before examining specific workload archetypes, verify your understanding of these core performance determinants.

::: {.callout-checkpoint title="The Iron Law" collapse="false"}
The **Iron Law** ($T = \frac{D_{vol}}{BW} + \frac{O}{R_{peak} \cdot \eta} + L_{lat}$) governs all ML performance. For a formal dimensional analysis and physical derivation of this equation, see @sec-machine-foundations in the appendices.

**The Variables**

- [ ] **Bandwidth ($BW$)**: Is your problem memory-bound? (Does increasing compute power $R_{peak}$ fail to speed it up?).
- [ ] **Overhead ($L_{lat}$)**: Why does batching improve throughput? (It amortizes the fixed overhead $L_{lat}$ across many samples).

**The Trade-off**

- [ ] **Latency vs. Throughput**: Why does training optimize for throughput (large batches, hiding $L$) while serving optimizes for latency (small batches, minimizing $L$)?
:::

To summarize: the Iron Law tells you the *cost of each ingredient*; the Bottleneck Principle tells you the *speed of the assembly line*. As a rule of thumb, use the **additive form** (@eq-iron-law) when analyzing the **latency** of a single task, and the **max form** (@eq-bottleneck) when analyzing the **throughput** of a continuous stream of tasks.

### Workload Archetypes

Now that we understand bottlenecks, we can classify workloads by which constraint dominates. Recall the **D·A·M taxonomy** from @sec-introduction: every ML system comprises **Data**, **Algorithm**, and **Machine**. Different deployment environments create different bottlenecks along these axes—a cloud server with terabytes of memory faces Algorithm constraints, while a microcontroller with kilobytes faces Machine constraints.

To navigate these constraints systematically, we categorize ML workloads into four **Archetypes**\index{Workload Archetypes}\index{Workload Archetypes!physical bottlenecks}.[^fn-archetype] These represent the primary physical bottlenecks, not just specific model architectures. We introduce each archetype briefly here; the Lighthouse Models that follow will ground each category in concrete, recurring examples.

**Archetype I: The Compute Beast**\index{Workload Archetypes!Compute Beast}\index{arithmetic intensity!high intensity workloads}. These workloads perform many calculations per byte of data loaded. The binding constraint is raw computational throughput. Training large neural networks falls into this category.

**Archetype II: The Bandwidth Hog**\index{Workload Archetypes!Bandwidth Hog}\index{autoregressive generation!memory-bound}. These workloads spend more time loading data than computing. Memory bandwidth becomes the binding constraint. Autoregressive text generation (like ChatGPT producing one token at a time) falls into this category.

**Archetype III: The Sparse Scatter**\index{Workload Archetypes!Sparse Scatter}\index{embedding tables!memory capacity bound}. Irregular memory access patterns with poor cache locality define this archetype. Memory capacity and access latency constrain performance. Recommendation systems with massive embedding tables are canonical examples.

**Archetype IV: The Tiny Constraint**\index{Workload Archetypes!Tiny Constraint}\index{energy per inference!binding constraint}\index{always-on sensing!power constraints}. Extreme power envelopes ($< 1$ mW) and memory limits ($< 256$ KB) characterize these workloads. The binding constraint is energy per inference—efficiency, not raw speed. Always-on sensing operates in this regime.

These archetypes map naturally to deployment paradigms: **Compute Beasts** and **Sparse Scatter** workloads gravitate toward **Cloud ML** where resources are abundant. **Bandwidth Hogs** span Cloud and Edge depending on latency requirements. **Tiny Constraint** workloads are exclusively **TinyML** territory. To make these abstractions concrete, we anchor each archetype to a specific model that recurs throughout this book as one of *five reference workloads*.

[^fn-archetype]: **Archetype**: From Greek *arkhetypon*, combining *arkhe* (beginning, origin) and *typos* (pattern, model). Plato used the term for the original forms from which all instances derive. In ML systems, archetypes represent the primary workload patterns from which all specific models derive their computational characteristics.

[^fn-paradigm]: **Paradigm**: See @sec-introduction for the Kuhnian definition of paradigm shift. In ML systems, deployment paradigms (Cloud, Edge, Mobile, TinyML) represent distinct operational frameworks, each with its own constraints, trade-offs, and engineering practices.

[^fn-latency-etymology]: **Latency**: See the etymology in @sec-introduction. For ML systems, latency budgets often determine deployment paradigm: sub-10 ms requirements mandate edge deployment, while 100 ms+ tolerances permit cloud inference.

[^fn-bandwidth]: **Bandwidth**: Originally a radio engineering term from the 1930s describing the width of frequency bands. In computing, it measures data transfer rate (bytes/second). The "Memory Wall" (a constraint quantified during hardware analysis in @sec-ai-acceleration) describes the growing gap between processor speed and memory bandwidth—historically, compute capability has doubled roughly every `{python} compute_doubling_months_str` months while memory bandwidth has improved only ~`{python} mem_bw_growth_pct_str`% annually, making data movement the dominant bottleneck in modern ML systems.

[^fn-critical-path]: **Critical Path**: From project management and systems engineering, the longest sequence of dependent operations that determines total completion time. In ML systems, the critical path is the chain of computations and data movements that sets end-to-end latency—no operation outside this path can reduce total time, while any delay *on* this path directly increases it. Optimizing the critical path is therefore the only way to improve latency.

```{python}
#| label: lighthouse-setup
#| echo: false
# ┌─────────────────────────────────────────────────────────────────────────────
# │ LIGHTHOUSE MODEL SPECIFICATIONS
# ├─────────────────────────────────────────────────────────────────────────────
# │ Context: Lighthouse Models callout (Five Reference Workloads)
# │
# │ Why: The five Lighthouse Models (ResNet-50, GPT-2/Llama, DLRM, MobileNet,
# │      KWS) anchor abstract concepts to concrete systems. This cell provides
# │      the FLOPs, parameters, and memory sizes that appear in the summary.
# │
# │ Imports: physx.constants (RESNET50_*, GPT2_PARAMS), physx.formatting (fmt)
# │ Exports: resnet_*_str, gpt2_*_str, llama_range_str
# └─────────────────────────────────────────────────────────────────────────────
from physx.constants import RESNET50_FLOPs, RESNET50_PARAMS, GPT2_PARAMS, GFLOPs, Mparam, Bparam, byte, MB
from physx.formatting import fmt

# --- Inputs (model specs from constants) ---
llama_min_b_value = 7                                    # Llama-7B (smallest)
llama_max_b_value = 70                                   # Llama-70B (largest common)

# --- Process (unit conversions) ---
resnet_flops_g_value = RESNET50_FLOPs.to(GFLOPs).magnitude
resnet_params_m_value = RESNET50_PARAMS.to(Mparam).magnitude
resnet_fp32_mb_value = (RESNET50_PARAMS.magnitude * 4 * byte).to(MB).magnitude
gpt2_params_b_value = GPT2_PARAMS.to(Bparam).magnitude

# --- Outputs (formatted strings for prose) ---
resnet_gflops_str = fmt(resnet_flops_g_value, precision=1, commas=False)   # e.g. "4.1" GFLOPs
resnet_params_m_str = fmt(resnet_params_m_value, precision=1, commas=False) # e.g. "25.6" M params
resnet_fp32_mb_str = fmt(resnet_fp32_mb_value, precision=0, commas=False)   # e.g. "102" MB
gpt2_params_b_str = fmt(gpt2_params_b_value, precision=1, commas=False)     # e.g. "1.5" B params
llama_range_str = f"{llama_min_b_value} to {llama_max_b_value}"             # "7 to 70" B
```

::: {.callout-lighthouse title="Five Reference Workloads"}

\index{Lighthouse Models!deployment mapping} \index{reference workloads!paradigm selection}Throughout this book, we use five **Lighthouse Models** introduced in @sec-introduction—concrete workloads that span the deployment spectrum and isolate distinct system bottlenecks. @sec-dnn-architectures provides full architectural details and model biographies.

| **Lighthouse**             | **Archetype**             | **Deployment Paradigm**        |
|:-------------------------|:------------------------|:-----------------------------|
| **ResNet-50**              | Compute Beast             | Cloud training, edge inference |
| **GPT-2 / Llama**          | Bandwidth Hog             | Cloud inference                |
| **DLRM**                   | Sparse Scatter            | Cloud only (distributed)       |
| **MobileNet**              | Compute Beast (efficient) | Mobile, edge                   |
| **Keyword Spotting (KWS)** | Tiny Constraint           | TinyML, always-on              |

To ground the abstract interdependencies of the Iron Law in concrete practice, we analyze the **Lighthouse Models** introduced in @sec-introduction. The following summaries recap each workload from a systems perspective, connecting them to the specific Iron Law bottlenecks they exemplify.

**ResNet-50**\index{ResNet-50!systems characteristics} classifies images into 1,000 categories, processing each image through approximately `{python} resnet_gflops_str` billion floating-point operations using `{python} resnet_params_m_str` million parameters (`{python} resnet_fp32_mb_str` MB at FP32). Used in medical imaging diagnostics, autonomous vehicle perception pipelines, and as the backbone for content moderation systems, its regular, compute-dense structure makes it the canonical benchmark for hardware accelerator performance.

**GPT-2 / Llama**\index{GPT-2!autoregressive bottleneck}\index{Llama!memory-bound inference} power chatbots, code assistants, and content generation tools. These models generate text one token at a time, requiring the model to read its full parameter set (`{python} gpt2_params_b_str` billion for GPT-2, `{python} llama_range_str` billion for Llama) from memory for each output token. This sequential memory access pattern creates the autoregressive bottleneck that dominates serving costs.

**DLRM**\index{DLRM!memory capacity bound}\index{recommendation systems!DLRM} (Deep Learning Recommendation Model) powers the "You might also like" recommendations on platforms like Meta and Netflix. It maps users and items to embedding vectors stored in tables that can exceed `{python} dlrm_embedding_str` GB, making memory capacity rather than computation the binding constraint.

**MobileNet**\index{MobileNet!depthwise separable convolutions}\index{MobileNet!efficiency gains} runs in smartphone camera apps for real-time photo categorization and on-device visual search. It performs the same image classification task as ResNet but uses depthwise separable convolutions to reduce computation by `{python} mobilenet_flops_reduction_str`×, enabling real-time inference on smartphones at `{python} mobile_tdp_range_str` watts.

**Keyword Spotting (KWS)**\index{Keyword Spotting (KWS)!TinyML archetype}\index{KWS|see{Keyword Spotting}} represents the always-on TinyML archetype. Used in applications like Smart Doorbells, it detects wake words ("Ding Dong", "Hello") using a depthwise separable CNN with approximately `{python} kws_params_str` parameters (small variants; the DS-CNN benchmark in MLPerf Tiny uses ~200K) fitting in under `{python} kws_size_kb_str` KB, running continuously at under 1 milliwatt.

**Key insight**: The huge range in compute requirements (20 MFLOPs → 4 GFLOPs) and memory (800 KB → 100 GB) explains why no single deployment paradigm fits all workloads. A keyword spotter runs comfortably on a \$2 microcontroller; a recommendation system requires a warehouse-scale computer.
:::

These five Lighthouse Models will serve as concrete anchors throughout the book, each isolating a distinct system bottleneck that we will revisit in every chapter.

## System Balance and Hardware {#sec-ml-system-architecture-framework-to-practice}

\index{latency!decision thresholds} \index{latency vs throughput!trade-offs}How do physical constraints translate into engineering decisions? The answer starts with numbers. @tbl-latency-numbers provides order-of-magnitude latencies that should inform every deployment decision—spanning eight orders of magnitude from nanosecond compute operations to hundreds of milliseconds for cross-region network calls. Detailed hardware latencies and bandwidth constraints are covered in @sec-ai-acceleration. The key decision rule: if your latency budget is $X$ ms, you cannot use any operation with latency $> X$ in your critical path[^fn-critical-path].

```{python}
#| label: latency-constants
#| echo: false
# ┌─────────────────────────────────────────────────────────────────────────────
# │ LATENCY NUMBERS FOR ML SYSTEM DESIGN
# ├─────────────────────────────────────────────────────────────────────────────
# │ Context: Table "Latency Numbers for ML System Design"
# │
# │ Why: Provides order-of-magnitude latencies that every ML engineer should
# │      internalize. Spans 8 orders of magnitude (ns to 100s of ms) to show
# │      why deployment paradigm selection is constrained by physics.
# │
# │ Imports: (none — display constants only)
# │ Exports: lat_*_str variables for table
# └─────────────────────────────────────────────────────────────────────────────

# --- Outputs: Compute latencies ---
lat_compute_str = "~1 ns"                                # GPU matrix multiply (per op)
lat_npu_str = "5–20 ms"                                  # NPU inference (MobileNet)
lat_llm_str = "20–100 ms"                                # LLM token generation

# --- Outputs: Memory latencies ---
lat_l1_str = "~1 ns"                                     # L1 cache hit
lat_hbm_str = "20–50 ns"                                 # HBM read (GPU)
lat_dram_str = "50–100 ns"                               # DRAM read (mobile)

# --- Outputs: Network latencies ---
lat_net_dc_str = "0.5 ms"                                # same datacenter
lat_net_region_str = "1–5 ms"                            # same region
lat_net_cross_str = "50–150 ms"                          # cross-region

# --- Outputs: ML operation latencies ---
lat_kws_str = "100 μs"                                   # wake-word detection (TinyML)
lat_face_str = "10–30 ms"                                # face detection (mobile)
lat_gpt4_str = "200–500 ms"                              # GPT-4 first token
lat_train_str = "200–400 ms"                             # ResNet-50 training step
```

These latencies, organized by category in @tbl-latency-numbers, span eight orders of magnitude:

| **Operation**                    | **Latency**                   | **Deployment Implication**       |
|:-------------------------------|:----------------------------|:-------------------------------|
| **Compute**                      |                               |                                  |
| **GPU matrix multiply (per op)** | `{python} lat_compute_str`    | Compute is rarely the bottleneck |
| **NPU inference (MobileNet)**    | `{python} lat_npu_str`        | Mobile can do real-time vision   |
| **LLM token generation**         | `{python} lat_llm_str`        | Perceived as "typing speed"      |
| **Memory**                       |                               |                                  |
| **L1 cache hit**                 | `{python} lat_l1_str`         | Keep hot data in registers       |
| **HBM read (GPU)**               | `{python} lat_hbm_str`        | 100× slower than compute         |
| **DRAM read (mobile)**           | `{python} lat_dram_str`       | Memory-bound on most devices     |
| **Network**                      |                               |                                  |
| **Same datacenter**              | `{python} lat_net_dc_str`     | Microservices feasible           |
| **Same region**                  | `{python} lat_net_region_str` | Edge servers viable              |
| **Cross-region**                 | `{python} lat_net_cross_str`  | Batch processing only            |
| **ML Operations**                |                               |                                  |
| **Wake-word detection (TinyML)** | `{python} lat_kws_str`        | Always-on feasible at <1 mW      |
| **Face detection (mobile)**      | `{python} lat_face_str`       | Real-time at 30 FPS              |
| **GPT-4 first token**            | `{python} lat_gpt4_str`       | User notices delay               |
| **ResNet-50 training step**      | `{python} lat_train_str`      | Throughput-optimized             |

: **Latency Numbers for ML System Design**\index{latency numbers!deployment constraints}\index{memory hierarchy!latency comparison}: Order-of-magnitude latencies across compute, memory, network, and ML operations that determine deployment feasibility. Spanning eight orders of magnitude, from nanosecond compute operations to hundreds of milliseconds for cross-region network calls, these physical constraints shape architectural decisions. For a comprehensive quick-reference including energy ratios and scaling rules, see @sec-numbers-to-know. {#tbl-latency-numbers}

We can now ground the four deployment paradigms in concrete hardware. While @tbl-deployment-paradigms-overview defined the paradigms conceptually, @tbl-representative-systems (which appears later in this section, after the System Balance discussion) provides specific devices, processors, and quantitative thresholds that practitioners use to select deployment targets.[^fn-cost-spectrum][^fn-pue] The 6-order-of-magnitude range in compute (MW cloud vs. mW TinyML) and cost ($millions vs. $10) determines which paradigm can serve a given workload economically.

These hardware differences translate directly into performance bottlenecks. To understand which constraint dominates in each paradigm, we apply the **Bottleneck Principle** (@sec-ml-systems-bottleneck-principle) using the pipelined form of the **Iron Law** from the previous chapter.

::: {.callout-perspective title="System Balance Across Paradigms"}

\index{Iron Law of ML Systems!paradigm comparison} \index{system bottlenecks!dominant constraints}The pipelined form of the **Iron Law of ML Systems**\index{Iron Law of ML Systems!execution time bound} from @sec-silicon-contract states that execution time is bounded by the slowest resource, as @eq-iron-law-extended formalizes:

$$T = \max\left( \frac{O}{R_{peak} \cdot \eta}, \frac{D_{vol}}{BW}, \frac{D_{vol}}{BW_{IO}} \right) + L_{lat}$$ {#eq-iron-law-extended}

Here, $O$ represents total operations, $R_{peak}$ is peak compute rate, $\eta$ is hardware utilization efficiency, $D_{vol}$ is data volume, $BW$ is memory bandwidth, $BW_{IO}$ is I/O bandwidth (storage or network), and $L_{lat}$ is fixed overhead. The equation identifies which resource—compute, memory, or I/O—limits performance. For a systematic diagnostic guide to identifying these bottlenecks, consult the D·A·M taxonomy\index{D·A·M taxonomy!bottleneck diagnosis} (@sec-appendix-dam).

The **dominant term varies by paradigm and workload**, changing the optimization strategy entirely:

| **Paradigm**            | **Dominant Constraint**   | **Why**                                                    | **Optimization Focus**               |
|:----------------------|:------------------------|:---------------------------------------------------------|:-----------------------------------|
| **Cloud Training**      | $O/R_{peak}$ (Compute)    | Abundant memory/network; FLOPS limit throughput            | Maximize GPU utilization, batch size |
| **Cloud LLM Inference** | $D_{vol}/BW$ (Memory BW)  | Autoregressive: ~1 FLOP/byte, memory-bound                 | KV-caching, quantization, batching   |
| **Edge Inference**      | $D_{vol}/BW$ (Memory BW)  | Limited HBM; models often memory-bound                     | Model compression, operator fusion   |
| **Mobile**              | Energy (implicit)         | Battery = $\int \text{Power} \cdot dt$; thermal throttling | Reduced precision, duty cycling      |
| **TinyML**              | $D_{vol}/\text{Capacity}$ | 256 KB total; model must fit on-chip                       | Extreme compression, binary networks |

The same ResNet-50 model is **compute-bound**\index{compute-bound vs memory-bound!training vs inference}\index{roofline model!bottleneck analysis} during cloud training (high batch size, high arithmetic intensity) but **memory-bound** during single-image inference (batch=1, low arithmetic intensity) [@williams2009roofline]. Deployment paradigm selection must account for this shift.
:::

This shift between training and inference is critical to understand. Recall the AI Triad from @sec-introduction: every ML system comprises Data, Algorithm, and Machine. The D·A·M taxonomy (@tbl-dam-phase) shows how each component behaves differently depending on whether the system is training (learning patterns) or serving (applying them).

| **Component**                                           | **Training (Mutable)**                                      | **Inference (Immutable)**                               |
|:------------------------------------------------------|:----------------------------------------------------------|:------------------------------------------------------|
| **Data**\index{training!data throughput}                | Massive throughput: large batches, shuffling, augmentation  | Low latency: single samples, freshness, speed           |
| **Algorithm**\index{training!bidirectional computation} | Bidirectional: forward + backward pass, optimizer state     | Unidirectional: forward pass only, weights frozen       |
| **Machine**\index{inference!latency optimization}       | Throughput-optimized: high-bandwidth clusters, large memory | Latency-optimized: edge devices, inference accelerators |

: **D·A·M × Phase**\index{D·A·M taxonomy!training vs inference}: The same model imposes starkly different demands on Data, Algorithm, and Machine depending on whether the system is training or serving. When bottlenecks shift unexpectedly, check which phase you're optimizing for. {#tbl-dam-phase}

The following worked example demonstrates how to apply this analysis quantitatively by comparing *ResNet-50 on cloud vs mobile* deployment targets.

```{python}
#| echo: false
#| label: resnet-setup
# ┌─────────────────────────────────────────────────────────────────────────────
# │ RESNET-50 MODEL SIZE SETUP
# ├─────────────────────────────────────────────────────────────────────────────
# │ Context: Callout "ResNet-50 on Cloud vs Mobile"
# │
# │ Why: Sets up ResNet-50 model sizes at FP32/FP16/INT8 precision for the
# │      cloud vs mobile bottleneck comparison. Shows how quantization (fewer
# │      bits per weight) directly reduces memory traffic.
# │
# │ Imports: physx.constants (RESNET50_FLOPs, RESNET50_PARAMS), physx.formatting
# │ Exports: resnet_*_str (GFLOPs, params, MB at each precision)
# └─────────────────────────────────────────────────────────────────────────────
from physx.constants import RESNET50_FLOPs, RESNET50_PARAMS, GFLOPs, Mparam, byte, MB
from physx.formatting import fmt

# --- Process (model sizes at different precisions) ---
resnet_fp32_bytes_value = RESNET50_PARAMS.magnitude * 4 * byte  # 4 bytes per FP32 param
resnet_fp16_bytes_value = RESNET50_PARAMS.magnitude * 2 * byte  # 2 bytes per FP16 param
resnet_int8_bytes_value = RESNET50_PARAMS.magnitude * 1 * byte  # 1 byte per INT8 param
resnet_gflops_value = RESNET50_FLOPs.to(GFLOPs).magnitude
resnet_params_m_value = RESNET50_PARAMS.to(Mparam).magnitude

# --- Outputs (formatted strings for prose) ---
resnet_gflops_str = fmt(resnet_gflops_value, precision=1, commas=False)                 # e.g. "4.1" GFLOPs
resnet_params_m_str = fmt(resnet_params_m_value, precision=1, commas=False)             # e.g. "25.6" M
resnet_fp32_mb_str = fmt(resnet_fp32_bytes_value.to(MB).magnitude, precision=0, commas=False)  # e.g. "102" MB
resnet_fp16_mb_str = fmt(resnet_fp16_bytes_value.to(MB).magnitude, precision=0, commas=False)  # e.g. "51" MB
resnet_int8_mb_str = fmt(resnet_int8_bytes_value.to(MB).magnitude, precision=0, commas=False)  # e.g. "26" MB
```

```{python}
#| echo: false
#| label: resnet-cloud
# ┌─────────────────────────────────────────────────────────────────────────────
# │ RESNET-50 CLOUD (A100) BOTTLENECK ANALYSIS
# ├─────────────────────────────────────────────────────────────────────────────
# │ Context: Callout "ResNet-50 on Cloud vs Mobile" — part (a) Cloud analysis
# │
# │ Why: Demonstrates that even a 312 TFLOPS A100 is MEMORY-BOUND for single-
# │      image inference (batch=1). The key insight: memory bandwidth, not raw
# │      compute, limits inference throughput when arithmetic intensity is low.
# │
# │ Imports: physx.constants (A100_*, RESNET50_*), physx.formulas (calc_bottleneck)
# │ Exports: a100_*_str, cloud_*_str, cloud_*_frac (Markdown fractions)
# └─────────────────────────────────────────────────────────────────────────────
from physx.constants import (
    RESNET50_FLOPs, A100_FLOPS_FP16_TENSOR, A100_MEM_BW,
    TFLOPs, second, TB, byte, flop,
)
from physx.formulas import calc_bottleneck
from physx.formatting import sci, fmt, sci_latex, md_frac

# --- Process (bottleneck analysis) ---
cloud_stats = calc_bottleneck(
    ops=RESNET50_FLOPs,
    model_bytes=resnet_fp16_bytes_value,                 # from resnet-setup cell
    device_flops=A100_FLOPS_FP16_TENSOR,
    device_bw=A100_MEM_BW,
)
a100_tflops_value = A100_FLOPS_FP16_TENSOR.to(TFLOPs / second).magnitude
a100_bw_tbs_value = A100_MEM_BW.to(TB / second).magnitude
cloud_compute_ms_value = cloud_stats["compute_ms"]
cloud_memory_ms_value = cloud_stats["memory_ms"]
cloud_ratio_x_value = cloud_stats["ratio"]
cloud_ai_value = cloud_stats["intensity"]
cloud_bottleneck_value = cloud_stats["bottleneck"]

# --- LaTeX fraction components (for nice rendering) ---
resnet_flops_latex = sci_latex(RESNET50_FLOPs.to(flop))
a100_flops_latex = sci_latex(A100_FLOPS_FP16_TENSOR.to(flop / second))
resnet_fp16_bytes_latex = sci_latex(resnet_fp16_bytes_value.to(byte))
a100_bw_latex = sci_latex(A100_MEM_BW.to(byte / second))
cloud_compute_frac = md_frac(resnet_flops_latex, a100_flops_latex, f"{cloud_compute_ms_value:.3f}", "ms")
cloud_memory_frac = md_frac(resnet_fp16_bytes_latex, a100_bw_latex, f"{cloud_memory_ms_value:.3f}", "ms")
cloud_ai_frac = md_frac(resnet_flops_latex, resnet_fp16_bytes_latex, f"{cloud_ai_value:.0f}", "FLOPs/byte")

# --- Outputs (formatted strings for prose) ---
a100_tflops_str = fmt(a100_tflops_value, precision=0, commas=False)        # e.g. "312" TFLOPS
a100_bw_tbs_str = fmt(a100_bw_tbs_value, precision=0, commas=False)        # e.g. "2" TB/s
cloud_compute_ms_str = fmt(cloud_compute_ms_value, precision=3, commas=False)
cloud_memory_ms_str = fmt(cloud_memory_ms_value, precision=3, commas=False)
cloud_ratio_x_str = fmt(cloud_ratio_x_value, precision=0, commas=False)    # memory/compute ratio
cloud_bottleneck_str = cloud_bottleneck_value                              # "Memory" or "Compute"
```

```{python}
#| echo: false
#| label: resnet-mobile
# ┌─────────────────────────────────────────────────────────────────────────────
# │ RESNET-50 MOBILE (NPU) BOTTLENECK ANALYSIS
# ├─────────────────────────────────────────────────────────────────────────────
# │ Context: Callout "ResNet-50 on Cloud vs Mobile" — part (b) Mobile analysis
# │
# │ Why: Shows that mobile NPUs are ALSO memory-bound at batch=1, just like
# │      cloud GPUs. The ~40× bandwidth gap (2 TB/s vs 50 GB/s) translates to
# │      ~40× slower inference — NOT the 10,000× compute gap. This explains
# │      why quantization (reducing bytes) beats faster hardware (more FLOPS).
# │
# │ Imports: physx.constants (MOBILE_NPU_*, A100_MEM_BW), physx.formulas
# │ Exports: mobile_*_str, bw_advantage_x_str, inference_speed_x_str
# └─────────────────────────────────────────────────────────────────────────────
from physx.constants import (
    RESNET50_FLOPs, MOBILE_NPU_TOPS_INT8, MOBILE_NPU_MEM_BW, A100_MEM_BW,
    TFLOPs, second, GB, byte, flop,
)
from physx.formulas import calc_bottleneck
from physx.formatting import sci_latex, md_frac, fmt

# --- Process (bottleneck analysis) ---
mobile_stats = calc_bottleneck(
    ops=RESNET50_FLOPs,
    model_bytes=resnet_int8_bytes_value,                 # from resnet-setup cell
    device_flops=MOBILE_NPU_TOPS_INT8,
    device_bw=MOBILE_NPU_MEM_BW,
)
mobile_tops_value = MOBILE_NPU_TOPS_INT8.to(TFLOPs / second).magnitude
mobile_bw_gbs_value = MOBILE_NPU_MEM_BW.to(GB / second).magnitude
mobile_compute_ms_value = mobile_stats["compute_ms"]
mobile_memory_ms_value = mobile_stats["memory_ms"]
mobile_ratio_x_value = mobile_stats["ratio"]
mobile_bottleneck_value = mobile_stats["bottleneck"]

# --- Cross-platform comparison ---
bw_advantage_x_value = A100_MEM_BW / MOBILE_NPU_MEM_BW
inference_speed_x_value = mobile_memory_ms_value / cloud_stats["memory_ms"]  # uses cloud_stats

# --- LaTeX fraction components (for nice rendering) ---
mobile_npu_flops_latex = sci_latex(MOBILE_NPU_TOPS_INT8.to(flop / second))
resnet_int8_bytes_latex = sci_latex(resnet_int8_bytes_value.to(byte))
mobile_npu_bw_latex = sci_latex(MOBILE_NPU_MEM_BW.to(byte / second))
mobile_compute_frac = md_frac(resnet_flops_latex, mobile_npu_flops_latex, f"{mobile_compute_ms_value:.2f}", "ms")
mobile_memory_frac = md_frac(resnet_int8_bytes_latex, mobile_npu_bw_latex, f"{mobile_memory_ms_value:.2f}", "ms")

# --- Outputs (formatted strings for prose) ---
mobile_tops_str = fmt(mobile_tops_value, precision=0, commas=False)        # e.g. "10" TOPS
mobile_bw_gbs_str = fmt(mobile_bw_gbs_value, precision=0, commas=False)    # e.g. "50" GB/s
mobile_ratio_x_str = fmt(mobile_ratio_x_value, precision=0, commas=False)  # memory/compute ratio
mobile_bottleneck_str = mobile_bottleneck_value                            # "Memory" or "Compute"
bw_advantage_x_str = fmt(bw_advantage_x_value, precision=0, commas=False)  # A100 vs NPU bandwidth
inference_speed_x_str = fmt(inference_speed_x_value, precision=0, commas=False)  # latency ratio
```

::: {.callout-notebook title="ResNet-50 on Cloud vs Mobile"}

\index{ResNet-50!cloud vs mobile}\index{arithmetic intensity!bottleneck analysis}**Problem**: Determine whether ResNet-50 inference is compute-bound or memory-bound on (a) a high-end datacenter GPU (NVIDIA A100 class) and (b) a flagship mobile NPU (Apple/Qualcomm class).

**Given** (from Lighthouse Models):

- ResNet-50: `{python} resnet_gflops_str` GFLOPs per inference, `{python} resnet_params_m_str` M parameters (`{python} resnet_fp32_mb_str` MB at FP32, `{python} resnet_fp16_mb_str` MB at FP16)

**Analysis**:

**(a) Cloud: NVIDIA A100 (batch=1, FP16)**

- Peak compute: `{python} a100_tflops_str` TFLOPS (FP16)
- Memory bandwidth: `{python} a100_bw_tbs_str` TB/s (HBM2e)
- Compute time: $T_{\text{comp}}$ = `{python} cloud_compute_frac`
- Memory time: $T_{\text{mem}}$ = `{python} cloud_memory_frac`
- **Bottleneck**: `{python} cloud_bottleneck_str` (`{python} cloud_ratio_x_str`× slower than compute)
- **Arithmetic Intensity**: `{python} cloud_ai_frac` — this ratio of compute operations to bytes loaded measures how efficiently a workload uses the hardware. When arithmetic intensity exceeds the hardware's *compute-to-bandwidth ratio* ($R_{peak}/BW$), the workload is compute-bound; below it, the workload is memory-bound. For single-image inference, the low batch size yields low arithmetic intensity, explaining why even powerful GPUs are memory-bound at batch=1.

**(b) Mobile: Flagship NPU (batch=1, INT8)**

- Peak compute: ~`{python} mobile_tops_str` TOPS (INT8) — representative of modern mobile NPUs
- Memory bandwidth: ~`{python} mobile_bw_gbs_str` GB/s (LPDDR5)
- Model size: `{python} resnet_int8_mb_str` MB (INT8 quantized)
- Compute time: $T_{\text{comp}}$ = `{python} mobile_compute_frac`
- Memory time: $T_{\text{mem}}$ = `{python} mobile_memory_frac`
- **Bottleneck**: `{python} mobile_bottleneck_str` (`{python} mobile_ratio_x_str`× slower than compute)

**Key Insight**\index{quantization!deployment benefits}: Both platforms are memory-bound for single-image inference! The A100's faster memory bandwidth (`{python} a100_bw_tbs_str` TB/s vs `{python} mobile_bw_gbs_str` GB/s = `{python} bw_advantage_x_str`×) translates to roughly `{python} inference_speed_x_str`× faster inference, not the 10,000× compute advantage. This explains why quantization (reducing bytes) often beats faster hardware (increasing FLOPS) for deployment.

**When does ResNet-50 become compute-bound?** Increase batch size until $\frac{\text{Ops}}{\text{Compute}} > \frac{\text{Bytes}}{\text{Memory BW}}$. On A100, this occurs around batch=64, where activations dominate memory traffic and high arithmetic intensity is sustained.
:::

As systems transition from Cloud to Edge to TinyML, available resources decrease dramatically. @tbl-representative-systems quantifies this progression with concrete hardware examples: memory drops from 131 TB (cloud) to 520 KB (TinyML), a 250 million-fold reduction, while power budgets span nine orders of magnitude from megawatts to milliwatts. This resource disparity is most acute on microcontrollers, the primary hardware platform for TinyML, where memory and storage capacities are insufficient for conventional ML models.

[^fn-cost-spectrum]: **ML Hardware Cost Spectrum**: The cost range spans 6 orders of magnitude, from \$10 ESP32-CAM modules to multi-million dollar TPU Pod systems. This 100,000$\times$+ cost difference reflects proportional differences in computational capability, enabling deployment across vastly different economic contexts and use cases, from hobbyist projects to hyperscale cloud infrastructure.

[^fn-pue]: **Power Usage Effectiveness (PUE)**: Data center efficiency metric measuring total facility power divided by IT equipment power. A PUE of 1.0 represents the theoretical lower bound (not achievable in practice), while 1.1–1.3 indicates highly efficient facilities using advanced cooling and power management. Google's data centers achieve fleet-wide PUE of approximately 1.10 (with some facilities as low as 1.06) compared to industry average of approximately 1.58.

```{python}
#| label: hardware-spectrum-setup
#| echo: false
# ┌─────────────────────────────────────────────────────────────────────────────
# │ HARDWARE SPECTRUM: REPRESENTATIVE SYSTEMS TABLE
# ├─────────────────────────────────────────────────────────────────────────────
# │ Context: Tables "Hardware Spectrum" and "Deployment Decision Thresholds"
# │
# │ Why: Grounds abstract paradigms in concrete hardware. TPU v4 Pod (cloud),
# │      DGX Spark (edge), and ESP32-CAM (TinyML) span 6 orders of magnitude
# │      in cost ($10 to $millions) and 9 orders in power (mW to MW).
# │
# │ Imports: physx.constants (TPU_POD_*, DGX_*, ESP32_*), physx.formatting (fmt)
# │ Exports: tpu_*_str, edge_*_str, tiny_*_str, *_thresh_str
# └─────────────────────────────────────────────────────────────────────────────
from physx.constants import (
    TPU_POD_MEM, TPU_POD_POWER, TPU_POD_CHIPS,
    DGX_RAM, DGX_STORAGE, DGX_POWER, DGX_PRICE_MIN, DGX_PRICE_MAX,
    ESP32_RAM, ESP32_FLASH, ESP32_POWER_MIN, ESP32_POWER_MAX, ESP32_PRICE,
    TB, GB, KiB, MB, watt, USD
)
from physx.formatting import fmt

# --- Outputs: Cloud (TPU v4 Pod) ---
tpu_chips_str = f"{TPU_POD_CHIPS:,}"                     # e.g. "4,096" chips
cloud_mem_tb_str = fmt(TPU_POD_MEM.to(TB).magnitude, precision=0, commas=False)  # e.g. "131" TB
cloud_pwr_mw_str = fmt(TPU_POD_POWER.to("megawatt").magnitude, precision=0, commas=False)  # e.g. "4" MW

# --- Outputs: Edge (DGX Spark) ---
edge_mem_gb_str = fmt(DGX_RAM.to(GB).magnitude, precision=0, commas=False)       # e.g. "128" GB
edge_stor_tb_str = fmt(DGX_STORAGE.to(TB).magnitude, precision=0, commas=False)  # e.g. "4" TB
edge_pwr_w_str = fmt(DGX_POWER.to(watt).magnitude, precision=0, commas=False)    # e.g. "500" W
edge_price_min_str = f"{DGX_PRICE_MIN.magnitude:,.0f}"                           # e.g. "3,000"
edge_price_max_str = f"{DGX_PRICE_MAX.magnitude:,.0f}"                           # e.g. "5,000"

# --- Outputs: TinyML (ESP32-CAM) ---
tiny_ram_kb_str = fmt(ESP32_RAM.to(KiB).magnitude, precision=0, commas=False)    # e.g. "520" KB
tiny_flash_mb_str = fmt(ESP32_FLASH.to(MB).magnitude, precision=0, commas=False) # e.g. "4" MB
tiny_pwr_min_str = f"{ESP32_POWER_MIN.magnitude}"                                # e.g. "0.1" W
tiny_pwr_max_str = f"{ESP32_POWER_MAX.magnitude}"                                # e.g. "0.5" W
tiny_price_str = f"{ESP32_PRICE.magnitude}"                                      # e.g. "10" USD

# --- Outputs: Decision thresholds ---
cloud_thresh_tflops_str = "1000"                         # TFLOPS threshold for cloud
cloud_thresh_bw_str = "100"                              # GB/s memory bandwidth
edge_thresh_pflops_str = "1"                             # PFLOPS AI compute threshold
edge_thresh_bw_str = "270"                               # GB/s memory bandwidth
tiny_thresh_tops_str = "1"                               # TOPS compute threshold
tiny_thresh_mw_str = "1"                                 # mW power threshold
```

@tbl-representative-systems grounds these paradigms in concrete hardware platforms and price points:

\begingroup\small

| **Category**  | **Example Device**  |                                   **Processor** |                             **Memory** | **Storage**                           |                                                 **Power** | **Price Range**                                                |
|:------------|:------------------|----------------------------------------------:|-------------------------------------:|:------------------------------------|--------------------------------------------------------:|:-------------------------------------------------------------|
| **Cloud ML**  | Google TPU v4 Pod   | `{python} tpu_chips_str` TPU v4 chips, >1 EFLOP |    `{python} cloud_mem_tb_str` TB HBM2 | Cloud-scale (PB)                      |                           ~`{python} cloud_pwr_mw_str` MW | Cloud service (rental)                                         |
| **Edge ML**   | NVIDIA DGX Spark    |               GB10 Grace Blackwell, 1 PFLOPS AI |  `{python} edge_mem_gb_str` GB LPDDR5x | `{python} edge_stor_tb_str` TB NVMe   |                              ~`{python} edge_pwr_w_str` W | ~\$`{python} edge_price_min_str`–`{python} edge_price_max_str` |
| **Mobile ML** | Flagship Smartphone |                    Mobile SoC (CPU + GPU + NPU) | `{python} mobile_ram_range_str` GB RAM | `{python} mobile_storage_range_str`   |                         `{python} mobile_tdp_range_str` W | USD 999+                                                       |
| **TinyML**    | ESP32-CAM           |                             Dual-core @ 240 MHz |      `{python} tiny_ram_kb_str` KB RAM | `{python} tiny_flash_mb_str` MB Flash | `{python} tiny_pwr_min_str`–`{python} tiny_pwr_max_str` W | \$`{python} tiny_price_str`                                    |

: **Hardware Spectrum (Concrete Platforms)**\index{hardware spectrum!deployment platforms}\index{TPU!datacenter infrastructure}\index{NVIDIA DGX!edge deployment}\index{ESP32!TinyML platform}: Representative devices that instantiate each deployment paradigm from @tbl-deployment-paradigms-overview. Where the conceptual table defines operating regimes, this table provides the specific processors, memory capacities, power envelopes, and price points that practitioners use to match workloads to hardware. The DGX Spark sits at the high end of the edge spectrum; most edge deployments use far smaller devices (e.g., Jetson Orin Nano). We include it to illustrate the *ceiling* of non-cloud deployment. {#tbl-representative-systems}

\endgroup

| **Category**  | **Quantitative Thresholds**                                                                                                                                            |
|:------------|:---------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| **Cloud ML**  | >`{python} cloud_thresh_tflops_str` TFLOPS compute, real-time video processing, >`{python} cloud_thresh_bw_str` GB/s memory bandwidth, PUE 1.1–1.3, 100–500 ms latency |
| **Edge ML**   | ~`{python} edge_thresh_pflops_str` PFLOPS AI compute, >`{python} edge_thresh_bw_str` GB/s memory bandwidth, desktop deployment, local processing                       |
| **Mobile ML** | `{python} mobile_npu_range_str` TOPS compute, <2 W sustained power, <`{python} mobile_latency_range_str` ms UI response                                                |
| **TinyML**    | <`{python} tiny_thresh_tops_str` TOPS compute, <`{python} tiny_thresh_mw_str` mW power, microsecond response times                                                     |

: **Deployment Decision Thresholds**\index{deployment paradigms!quantitative thresholds}: Quantitative thresholds corresponding to each paradigm in @tbl-representative-systems that practitioners use to determine deployment feasibility. {#tbl-deployment-thresholds}

These deployment paradigms emerged from decades of hardware evolution, from floating-point coprocessors in the 1980s through graphics processors in the 2000s to today's domain-specific AI accelerators. @sec-ai-acceleration traces this historical progression and the architectural principles that drove it. Here, we focus on the *consequences* of this evolution: the deployment spectrum that results from having qualitatively different hardware available at different points in the infrastructure.

Each paradigm occupies a distinct region of the deployment spectrum, governed by the physical constraints (Light Barrier, Power Wall, Memory Wall) and quantified by the analytical tools (**Iron Law**, Bottleneck Principle) introduced above. The quantitative thresholds in @tbl-deployment-thresholds help practitioners determine which paradigm suits their workload. The following four sections progress from cloud to TinyML, tracing the gradient from maximum computational resources to maximum efficiency constraints.

Each section follows a consistent structure: definition, key characteristics, benefits and trade-offs, and representative applications. This parallel treatment reveals both what distinguishes each paradigm and what principles they share, setting the stage for the hybrid architectures that combine them. We begin at the resource-rich end of the spectrum and progressively tighten the constraints.

## Cloud ML: Computational Power {#sec-ml-system-architecture-cloud-ml-maximizing-computational-power-a338}

\index{Cloud ML!datacenter scale} \index{data centers!ML infrastructure}Consider what it took to train GPT-3: `{python} gpt3_petaflop_days_str` petaflop-days of computation, `{python} gpt3_v100_count_str` GPUs running for approximately `{python} gpt3_days_str` days, consuming megawatts of power—at an estimated cost of ~$`{python} gpt3_cost_m_str`M[^fn-nlp-compute]. No smartphone, no edge server, no single machine on Earth could have performed this computation. Only a datacenter, with its virtually unlimited compute, memory, and storage, could aggregate enough resources to make this possible. This is the defining proposition of Cloud ML: if you can tolerate latency, you can access computational scale that no other paradigm can match.

Cloud Machine Learning aggregates computational resources in data centers[^fn-cloud-evolution] to handle computationally intensive tasks: large-scale data processing, collaborative model development, and advanced analytics. This infrastructure serves as the natural home for three of the four Workload Archetypes: **Compute Beasts** like ResNet training that demand sustained TFLOPS across thousands of accelerators, **Bandwidth Hogs** like large language model inference that benefit from TB/s HBM bandwidth, and **Sparse Scatter** workloads like recommendation systems that require terabytes of embedding tables and high-bandwidth interconnects for all-to-all communication patterns.

Cloud deployments range from single-machine instances (workstations, multi-GPU servers, DGX systems) to large-scale distributed systems spanning multiple data centers. This book focuses on single-machine cloud systems, where students learn to build and optimize ML systems on individual powerful machines. Future studies can address distributed cloud infrastructure, where systems coordinate computation across multiple networked machines. This progression follows the pedagogical principle of establishing foundations before adding complexity.

[^fn-cloud-evolution]: **Cloud Infrastructure Evolution**: Cloud computing for ML emerged from Amazon's decision in 2002 to treat their internal infrastructure as a service. AWS launched in 2006, followed by Google Cloud Platform (2011) and Google Compute Engine (2012), and Azure (2010). By 2024, worldwide public cloud spending was projected to reach approximately $679 billion [@gartner2024cloud].

[^fn-nlp-compute]: **NLP Computational Demands**: GPT-3 training cost is estimated at ~\$`{python} gpt3_cost_m_str`M at 2020 V100 cloud rates [@brown2020language]; GPT-4 is estimated at 10–100× more. This exponential scaling drove hyperscaler investment in specialized infrastructure (TPUs, custom ASICs) and raised concerns about AI's environmental impact [@strubell2019energy] and access inequality.

What unifies these diverse cloud workloads is a single defining trade-off:

::: {.callout-definition title="Cloud ML"}

***Cloud Machine Learning***\index{Cloud ML!resource elasticity}\index{Cloud ML!network latency constraint}\index{resource elasticity|see{Cloud ML}} is the deployment paradigm that optimizes for **Resource Elasticity**. It decouples computational capacity from physical location, allowing systems to dynamically scale resources proportional to workload variance (bursting to petaflops), constrained principally by **Network Latency** (Speed of Light) and operational dependence.
:::

@fig-cloud-ml breaks down Cloud ML across several key dimensions that define its computational paradigm. Notice how the **Characteristics** branch emphasizes centralization and dynamic scalability, which directly enables the **Benefits** of scalable data processing and global accessibility. However, this centralization creates the **Challenges** of latency and internet dependence, shaping the kinds of **Examples** that thrive in the cloud—from virtual assistants to recommendation systems and fraud detection.

::: {#fig-cloud-ml fig-env="figure" fig-pos="t" fig-cap="**Cloud ML Decomposition.** Characteristics, benefits, challenges, and representative applications of cloud machine learning, where centralized infrastructure and specialized hardware address scale, complexity, and resource management for large datasets and complex computations." fig-alt="Tree diagram with Cloud ML branching to four categories: Characteristics, Benefits, Challenges, and Examples. Each lists items like computational power, scalability, vendor lock-in, and virtual assistants."}
```{.tikz}
\begin{tikzpicture}[font=\small\usefont{T1}{phv}{m}{n}]
\tikzset{
  Box/.style={inner xsep=2pt,
  draw=GreenLine,
  fill=GreenL!50,
  node distance=0.4,
    line width=0.75pt,
    anchor=west,
    text width=30mm,align=flush center,
    minimum width=30mm, minimum height=9.5mm
  },
  Box2/.style={Box,draw=BlueLine,fill=BlueL!50, text width=27mm, minimum width=27mm
  },
  Box3/.style={Box,draw=OrangeLine,fill=OrangeL!40, text width=38mm, minimum width=38mm
  },
 Box4/.style={Box,draw=VioletLine,fill=VioletL2!40, text width=32mm, minimum width=32mm
  },
 Line/.style={line width=1.0pt,black!50,text=black,-{Triangle[width=0.8*6pt,length=0.98*6pt]}},
}
\node[Box4, fill=VioletL2!90!violet!50,](B1){Characteristics};
\node[Box2,right=2 of B1,fill=BlueL](B2){Benefits};
\node[Box,right=2 of B2,fill=GreenL](B3){Challenges};
\node[Box3,right=2 of B3,fill=OrangeL](B4){Examples};
\node[Box,draw=OliveLine,fill=OliveL!30, minimum height=11.5mm,
above=1of $(B2.north east)!0.5!(B3.north west)$](B0){Cloud ML};
%
\node[Box4,below=0.7 of B1](B11){Immense Computational Power};
\node[Box4,below=of B11](B12){Collaborative Environment};
\node[Box4,below=of B12](B13){Access to Advanced Tools};
\node[Box4,below=of B13](B14){Dynamic Scalability};
\node[Box4,below=of B14](B15){Centralized Infrastructure};
%
\node[Box2,below=0.7 of B2](B21){Scalable Data Processing and Model Training};
\node[Box2,below=of B21](B22){Collaboration and Resource Sharing};
\node[Box2,below=of B22](B23){Flexible Deployment and Accessibility};
\node[Box2,below=of B23](B24){Cost-Effectiveness and Scalability};
\node[Box2,below=of B24](B25){Global Accessibility};
%
\node[Box,below=0.7 of B3](B31){Vendor Lock-In};
\node[Box,below=of B31](B32){Latency Issues};
\node[Box,below=of B32](B33){Data Privacy and Security};
\node[Box,below=of B33](B34){Dependency on Internet};
\node[Box,below=of B34](B35){Cost Considerations};
%
\node[Box3,below=0.7 of B4](B41){Virtual Assistants};
\node[Box3,below=of B41](B42){Security and Anomaly Detection};
\node[Box3,below=of B42](B43){Recommendation Systems};
\node[Box3,below=of B43](B44){Fraud Detection};
\node[Box3,below=of B44](B45){Personalized User Experience};
%
\foreach \i in{1,2,3,4,5}{
  \foreach \x in{1,2,3,4}{
\draw[Line](B\x.west)--++(180:0.5)|-(B\x\i);
}
}
\foreach \x in{1,2,3,4}{
\draw[Line](B0)-|(B\x);
}
\end{tikzpicture}

```
:::

```{python}
#| echo: false
#| label: distance-penalty
# ┌─────────────────────────────────────────────────────────────────────────────
# │ DISTANCE PENALTY CALCULATION
# ├─────────────────────────────────────────────────────────────────────────────
# │ Context: Callout "The Distance Penalty" (Cloud ML section)
# │
# │ Why: Shows that for safety-critical systems (10ms budget), cloud is
# │      physically impossible. At 1,500km, round-trip light-speed latency
# │      (~15ms) already exceeds the entire safety budget — leaving NEGATIVE
# │      time for computation.
# │
# │ Imports: physx.constants (SPEED_OF_LIGHT_FIBER_KM_S), physx.formulas
# │ Exports: sol_kms_str, rtt_formatted_str, deficit_str, distance_km_str
# └─────────────────────────────────────────────────────────────────────────────
from physx.constants import SPEED_OF_LIGHT_FIBER_KM_S
from physx.formulas import calc_network_latency_ms
from physx.formatting import fmt

# --- Inputs (safety-critical scenario) ---
distance_km_value = 1500                                 # km to cloud datacenter
safety_budget_ms_value = 10                              # ms safety requirement

# --- Process (light-speed round-trip) ---
round_trip_ms_value = calc_network_latency_ms(distance_km_value)
deficit_ms_value = safety_budget_ms_value - round_trip_ms_value

# --- Outputs (formatted strings for prose) ---
sol_kms_str = f"{SPEED_OF_LIGHT_FIBER_KM_S:,}"           # e.g. "200,000" km/s
rtt_formatted_str = fmt(round_trip_ms_value, precision=0, commas=False)  # e.g. "15" ms
deficit_str = fmt(deficit_ms_value, precision=0, commas=False)           # e.g. "-5" ms
distance_km_str = f"{distance_km_value:,}"                               # e.g. "1,500" km
safety_budget_str = f"{safety_budget_ms_value}"                          # "10" ms
```

::: {.callout-notebook title="The Distance Penalty"}

\index{distance penalty!cloud latency} \index{Light Barrier!safety-critical systems}**Problem**: You are deploying a real-time safety monitor for a robotic arm. The safety logic requires a **`{python} safety_budget_str` ms** end-to-end response time to prevent injury. Your model runs in a high-performance cloud data center `{python} distance_km_str` km away.

**The Physics**:

1.  **Light in Fiber**: ~`{python} sol_kms_str` km/s.
2.  **Round-trip Propagation**: (`{python} distance_km_str` km × 2) / `{python} sol_kms_str` km/s = **`{python} rtt_formatted_str` ms**.
3.  **The Result**: Your safety budget is already **negative** (`{python} deficit_str` ms) before the model even starts its first calculation.

**The Engineering Conclusion**: Physics has made Cloud ML **impossible** for this application. You must move to the Edge.
:::

### Cloud Infrastructure and Scale {#sec-ml-system-architecture-cloud-infrastructure-scale-f0b1}

\index{Cloud ML!TPU infrastructure} \index{exaflop-scale training|see{Cloud ML}}Cloud ML aggregates computational resources in data centers at unprecedented scale. To grasp the physical scale behind this abstraction, look at @fig-cloudml-example—Google's Cloud TPU[^fn-mlsys-tpu] data center, where row upon row of specialized accelerators deliver petaflop-scale training throughput. @tbl-representative-systems quantifies how cloud systems provide orders-of-magnitude more compute and memory bandwidth than mobile devices, at correspondingly higher power and operational cost. Modern cloud accelerator systems operate at *petaflops to exaflops* of peak reduced-precision throughput and require *megawatt-scale* facility power when deployed in large clusters. These data center facilities enable computational workloads that are impractical on resource-constrained devices. The remote location of cloud resources introduces critical trade-offs: network round-trip latency of 100--500 ms eliminates real-time applications, while operational costs scale linearly with usage.

[^fn-mlsys-tpu]: **Tensor Processing Unit (TPU)**: Google's custom ASIC designed specifically for tensor operations, first used internally in 2015 for neural network inference [@jouppi2017datacenter], building on earlier distributed training work like DistBelief [@dean2012large]. The name derives from "tensor," coined by mathematician William Rowan Hamilton in 1846 from Latin *tendere* (to stretch), describing mathematical objects that transform under coordinate changes. Neural networks are, at their core, tensor computations: weights are matrices (rank-2 tensors), batched inputs form higher-rank tensors. A single TPU v4 Pod contains 4,096 chips and delivers over 1 exaflop of peak performance [@jouppi2023tpu].

![**Cloud Data Center Scale**: Rows of server racks illuminated by blue LEDs extend across a Google Cloud TPU data center floor, housing thousands of specialized AI accelerator chips that collectively deliver petaflop-scale training throughput. Source: [@google2024gemini].](images/jpg/cloud_ml_tpu.jpeg){#fig-cloudml-example fig-pos='t' fig-alt="Aerial view of Google Cloud TPU data center with long rows of server racks illuminated by blue LEDs extending toward the horizon across a large facility floor."}

Cloud ML excels in processing massive data volumes through parallelized architectures. Training techniques covered in @sec-ai-training enable processing datasets that would be impractical on single devices, with @sec-ai-acceleration covering the underlying hardware analysis. This enables training on datasets requiring hundreds of terabytes of storage and petaflops of computation, resources that are impractical on constrained devices.

Cloud infrastructure creates deployment flexibility through cloud APIs[^fn-ml-apis], making trained models accessible worldwide across mobile, web, and IoT platforms. Shared infrastructure enables multiple teams to access projects simultaneously with integrated version control. Pay-as-you-go pricing models[^fn-paas-pricing] eliminate upfront capital expenditure while resources scale elastically with demand.

A common misconception holds that Cloud ML's vast computational resources make it universally superior. Cloud infrastructure offers exceptional computational power and storage, yet this advantage does not automatically translate to optimal solutions for all applications. The **Data Gravity Invariant**\index{Data Gravity Invariant!cloud limitations} (Part I) explains why: as data scales, the cost of moving it to compute ($C_{move}(D) \gg C_{move}(Compute)$) eventually dominates. The trade-offs listed in the definition above become concrete when we consider where edge and embedded deployments excel: real-time response with sub-10 ms decision making in autonomous control loops, strict data privacy for medical devices processing patient data, predictable costs through one-time hardware investment versus recurring cloud fees, or operation in disconnected environments such as industrial equipment in remote locations. The optimal deployment paradigm depends on specific application requirements rather than raw computational capability.

[^fn-ml-apis]: **ML APIs**: Application Programming Interfaces that democratized AI by providing pre-trained models as web services. Google's Vision API launched in 2016, processing over 1 billion images monthly within two years, enabling developers to add AI capabilities without ML expertise.

[^fn-paas-pricing]: **Pay-as-You-Go Pricing**: Cloud pricing model charging for actual compute consumption (GPU-hours, inference requests) rather than hardware ownership. A100 GPUs cost $2–4/hour on-demand vs. $15,000+ to purchase. Training GPT-3 was estimated at ~$4.6M at 2020 on-demand V100 rates; amortized cluster ownership becomes economical only above 80% utilization over 3+ years.

### Cloud ML Trade-offs and Constraints {#sec-ml-system-architecture-cloud-ml-tradeoffs-constraints-96ed}

\index{Cloud ML!latency limitations} \index{Cloud ML!privacy concerns} \index{GDPR compliance!cloud deployment} \index{HIPAA compliance!cloud deployment}Cloud ML's substantial advantages carry inherent trade-offs that shape deployment decisions. Latency represents the most consequential physical constraint. Network round-trip delays typically range from 100--500 ms, making cloud processing unsuitable for real-time applications requiring sub-10 ms responses, such as autonomous vehicles and industrial control systems. Beyond basic timing constraints, unpredictable response times complicate performance monitoring and debugging across geographically distributed infrastructure.

Privacy and security present serious challenges when adopting cloud deployment. Transmitting sensitive data to remote data centers creates potential vulnerabilities and complicates regulatory compliance. Organizations handling data subject to regulations like GDPR[^fn-gdpr] or HIPAA[^fn-hipaa] must implement comprehensive security measures including encryption, strict access controls, and continuous monitoring to meet stringent data handling requirements. Privacy-preserving ML techniques, including federated learning and differential privacy, address these challenges at the systems level.

[^fn-gdpr]: **GDPR (General Data Protection Regulation)**: European privacy law (2018) imposing fines up to €20M or 4% of global revenue. Mandates "right to be forgotten," data processing transparency, and explicit consent. ML systems must implement model unlearning, audit trails, and explainability. Total fines exceeded €4.5B by 2024, including €746M against Amazon.

[^fn-hipaa]: **HIPAA (Health Insurance Portability and Accountability Act)**: US healthcare privacy law (1996) mandating encryption, access controls, and audit trails for Protected Health Information. ML systems handling medical data require HIPAA-compliant infrastructure, adding 30–50% to development costs. Violations incur $100–50,000 per incident, with annual maximums up to $1.5M per category.

Cost management introduces operational complexity requiring total cost of ownership (TCO)[^fn-tco]\index{Total Cost of Ownership (TCO)!cloud vs. edge}\index{TCO analysis!deployment decisions} analysis rather than naive unit comparisons. A worked *cloud vs. edge TCO* comparison illustrates the gap between sticker price and true system cost.

[^fn-tco]: **Total Cost of Ownership (TCO)**: The sum of all direct and indirect costs over a system's lifetime, not just the purchase price. TCO includes hardware acquisition, power consumption, cooling infrastructure, network connectivity, software licensing, DevOps labor, maintenance, and opportunity costs from delayed deployment. A \$500 edge device may cost \$5,000 over three years when these factors are included; a \$2/hour cloud instance may cost \$50,000/year when accounting for redundancy, egress fees, and monitoring.

```{python}
#| label: tco-calc
#| echo: false
# ┌─────────────────────────────────────────────────────────────────────────────
# │ CLOUD VS. EDGE TOTAL COST OF OWNERSHIP (TCO)
# ├─────────────────────────────────────────────────────────────────────────────
# │ Context: Callout "Cloud vs. Edge TCO" (Cloud ML Trade-offs section)
# │
# │ Why: Demonstrates that naive unit-cost comparisons miss the full picture.
# │      Cloud wins at low volume (<500K/day) due to no fixed costs; edge wins
# │      at high steady volume (>1M/day) by ~45%. Critical insight: edge TCO is
# │      dominated by LABOR (~60%), not hardware.
# │
# │ Imports: physx.constants (DAYS_PER_YEAR, HOURS_PER_YEAR, CLOUD_EGRESS_PER_GB,
# │          SERVER_POWER_W, CLOUD_ELECTRICITY_PER_KWH), physx.formatting (fmt)
# │ Exports: c_*_str (cloud costs), e_*_str (edge costs), edge_savings_str
# └─────────────────────────────────────────────────────────────────────────────
from physx.constants import (
    DAYS_PER_YEAR, HOURS_PER_YEAR, CLOUD_EGRESS_PER_GB,
    SERVER_POWER_W, CLOUD_ELECTRICITY_PER_KWH, USD, GB, watt, ureg,
)
from physx.formatting import fmt

# --- Inputs: Scenario ---
requests_per_day_value = 1_000_000                       # 1M daily inferences
inference_ms_value = 10                                  # ms latency target
response_kb_value = 100                                  # KB response size
days_per_year_value = DAYS_PER_YEAR
hours_per_year_value = HOURS_PER_YEAR

# --- Inputs: Cloud pricing (AWS 2024) ---
gpu_price_per_hr_value = 0.75                            # A10G (g5.xlarge)
gpu_instances_value = 4                                  # HA + peak handling
egress_per_gb_value = CLOUD_EGRESS_PER_GB.to(USD / GB).magnitude
lb_base_per_hr_value = 0.025                             # ALB hourly
lb_lcu_per_hr_value = 0.008                              # LCU-hour
avg_lcu_value = 50                                       # typical for 1M req/day

# --- Inputs: Edge pricing ---
server_cost_value = 15000                                # $ hardware
server_life_years_value = 3                              # depreciation period
power_watts_value = SERVER_POWER_W.to(watt).magnitude
electricity_per_kwh_value = CLOUD_ELECTRICITY_PER_KWH.to(USD / ureg.kilowatt_hour).magnitude
cooling_overhead_value = 0.30                            # 30% cooling overhead
fiber_annual_value = 1200                                # $ network connectivity
devops_fte_value = 0.1                                   # FTE for maintenance
devops_salary_value = 150000                             # $ annual salary

# --- Process: Cloud costs ---
c_gpu_value = gpu_instances_value * hours_per_year_value * gpu_price_per_hr_value
egress_gb_per_day_value = (requests_per_day_value * response_kb_value) / (1024 * 1024)
c_egress_value = egress_gb_per_day_value * days_per_year_value * egress_per_gb_value
c_lb_value = lb_base_per_hr_value * hours_per_year_value + lb_lcu_per_hr_value * avg_lcu_value * hours_per_year_value
c_logs_value = 2000
c_total_value = c_gpu_value + c_egress_value + c_lb_value + c_logs_value

# --- Process: Edge costs ---
e_capex_value = server_cost_value / server_life_years_value
e_power_value = (power_watts_value * hours_per_year_value * electricity_per_kwh_value) / 1000
e_cool_value = e_power_value * cooling_overhead_value
e_net_value = fiber_annual_value
e_labor_value = devops_fte_value * devops_salary_value
e_total_value = e_capex_value + e_power_value + e_cool_value + e_net_value + e_labor_value
edge_savings_pct_value = ((c_total_value - e_total_value) / c_total_value) * 100
labor_pct_value = (e_labor_value / e_total_value) * 100

# --- Outputs: Scenario strings ---
requests_str = f"{requests_per_day_value/1e6:.0f}M"
inference_str = f"{inference_ms_value}ms"
response_str = f"{response_kb_value}KB"
gpu_instances_str = f"{gpu_instances_value}"
gpu_price_str = f"${gpu_price_per_hr_value:.2f}"
egress_gb_str = fmt(egress_gb_per_day_value, precision=0, commas=False)

# --- Outputs: Cloud cost strings ---
c_gpu_str = f"~${c_gpu_value:,.0f}"
c_egress_str = f"~${c_egress_value:,.0f}"
c_lb_str = f"~${c_lb_value:,.0f}"
c_logs_str = f"~${c_logs_value:,.0f}"
c_total_str = f"~${c_total_value:,.0f}/year"

# --- Outputs: Edge cost strings ---
server_cost_str = f"${server_cost_value:,}"
server_life_str = f"{server_life_years_value}"
power_str = f"{power_watts_value}W"
electricity_str = f"${electricity_per_kwh_value:.2f}"
devops_fte_str = f"{devops_fte_value}"
devops_salary_str = f"${devops_salary_value:,}"
e_capex_str = f"${e_capex_value:,.0f}"
e_power_str = f"${e_power_value:,.0f}"
e_cool_str = f"~${e_cool_value:,.0f}"
e_net_str = f"${e_net_value:,}"
e_labor_str = f"${e_labor_value:,}"
e_total_str = f"~${e_total_value:,.0f}/year"
edge_savings_str = f"{edge_savings_pct_value:.0f}%"
labor_pct_str = f"{labor_pct_value:.0f}%"
```

::: {.callout-notebook title="Cloud vs. Edge TCO"}
**Scenario**: A vision system serving `{python} requests_str` daily inferences (ResNet-50 scale, `{python} inference_str` latency, `{python} response_str` response).

**Cloud Implementation** (AWS/GCP pricing, 2024)

| **Cost Component**       |                                                                  **Calculation** |            **Annual Cost** |
|:-----------------------|-------------------------------------------------------------------------------:|-------------------------:|
| **GPU inference (A10G)** | `{python} gpu_instances_str` instances × 8,760 hrs × `{python} gpu_price_str`/hr |       `{python} c_gpu_str` |
| **Network egress**       |                                 `{python} egress_gb_str` GB/day × 365 × USD 0.09/GB |    `{python} c_egress_str` |
| **Load balancer**        |                                                          USD 0.025/hr + LCU charges |        `{python} c_lb_str` |
| **CloudWatch/logging**   |                                                               Monitoring, alerts |      `{python} c_logs_str` |
| **Total Cloud**          |                                                                                  | **`{python} c_total_str`** |

**Edge Implementation** (On-premise NVIDIA T4 server)

| **Cost Component**   |                                                          **Calculation** |            **Annual Cost** |
|:-------------------|-----------------------------------------------------------------------:|-------------------------:|
| **Hardware CAPEX**   | `{python} server_cost_str` server ÷ `{python} server_life_str`-year life |     `{python} e_capex_str` |
| **Power (24/7)**     |        `{python} power_str` × 8,760 hrs × `{python} electricity_str`/kWh |     `{python} e_power_str` |
| **Cooling overhead** |                                                            ~30% of power |      `{python} e_cool_str` |
| **Network (fiber)**  |                                         Fixed line for remote management |       `{python} e_net_str` |
| **DevOps labor**     |      `{python} devops_fte_str` FTE × `{python} devops_salary_str` salary |     `{python} e_labor_str` |
| **Total Edge**       |                                                                          | **`{python} e_total_str`** |

**Break-even Analysis**: @eq-edge-breakeven determines when edge deployment becomes cost-effective. **Edge Fixed Costs** include hardware amortization and maintenance, **Cloud Variable Cost per Unit** is the per-inference cloud pricing, and **Capacity** is the maximum inference rate of the edge system:

$$\text{Break-even utilization} = \frac{\text{Edge Fixed Costs}}{\text{Cloud Variable Cost per Unit} \times \text{Capacity}}$$ {#eq-edge-breakeven}

At low volume (<500K inferences/day), cloud wins due to no fixed costs. At high, steady volume (>1M/day), edge wins by ~`{python} edge_savings_str`. The crossover occurs around **60% sustained utilization**.

**Key insight**: Edge TCO is dominated by **labor** (`{python} labor_pct_str`), not hardware. Organizations without existing DevOps capacity should factor in the full cost of maintaining on-premise infrastructure.
:::

Unpredictable usage spikes complicate budgeting, requiring sophisticated monitoring and cost governance frameworks.

Network dependency creates another constraint. Any connectivity disruption directly impacts system availability, particularly where network access is limited or unreliable. Vendor lock-in compounds the problem: dependencies on specific tools and APIs create portability and interoperability challenges when transitioning between providers. Organizations must balance these constraints against cloud benefits based on application requirements and risk tolerance.

Despite these trade-offs, Cloud ML's computational advantages make it indispensable for consumer applications at global scale.

### Large-Scale Training and Inference {#sec-ml-system-architecture-largescale-training-inference-e16d}

\index{Cloud ML!training at scale} \index{hybrid architectures!wake-word detection}Cloud ML's computational advantages manifest most visibly in consumer-facing applications requiring massive scale. Virtual assistants like Siri and Alexa demonstrate the hybrid architectures that characterize modern ML systems. Wake word detection runs on dedicated low-power hardware (often sub-milliwatt) directly on the device, enabling always-on listening without draining batteries. Initial speech recognition increasingly runs on-device for privacy and responsiveness, while complex natural language understanding and generation leverage cloud infrastructure for access to larger models and broader knowledge.

The economics drive this architecture as much as latency.

```{python}
#| label: voice-assistant-wall-calc
#| echo: false
# ┌─────────────────────────────────────────────────────────────────────────────
# │ VOICE ASSISTANT WALL: ECONOMICS + INFRASTRUCTURE
# ├─────────────────────────────────────────────────────────────────────────────
# │ Context: Callout "The Voice Assistant Wall" (Cloud ML large-scale section)
# │
# │ Why: Two-part argument that cloud-only voice processing fails at global
# │      scale. Part 1: $0.50/device × 1B devices = $500M/year (economic wall).
# │      Part 2: 1B devices streaming audio = 32 PB/s, exceeding global internet
# │      backbone; even query-only needs 20+ data centers at peak (infra wall).
# │
# │ Imports: physx.formatting (fmt)
# │ Exports: ww_*_str (economics), vi_*_str (infrastructure)
# └─────────────────────────────────────────────────────────────────────────────
from physx.formatting import fmt

# === Part 1: Economics ===
ww_devices_b_value = 1                                   # billion devices worldwide
ww_cloud_cost_per_device_value = 0.50                    # $ per device/year (conservative)
ww_edge_power_min_mw_value = 0.1                         # mW power (sleep)
ww_edge_power_max_mw_value = 1                           # mW power (active)
ww_edge_cost_per_year_value = 0.01                       # $ battery cost/year

ww_total_cloud_cost_value = ww_devices_b_value * 1e9 * ww_cloud_cost_per_device_value

ww_devices_b_str = fmt(ww_devices_b_value, precision=0, commas=False)   # "1" billion
ww_cloud_cost_str = fmt(ww_cloud_cost_per_device_value, precision=2, commas=False)  # "0.50"
ww_total_cost_str = fmt(ww_total_cloud_cost_value, precision=0, commas=True)        # "500,000,000"
ww_edge_power_range_str = f"{ww_edge_power_min_mw_value}--{ww_edge_power_max_mw_value}"  # "0.1--1" mW
ww_edge_cost_str = fmt(ww_edge_cost_per_year_value, precision=2, commas=False)          # "0.01"

# === Part 2: Infrastructure ===
vi_devices_b_value = 1                                   # billion devices
vi_queries_per_day_value = 20                            # average queries per device per day
vi_gpu_ms_per_query_value = 200                          # ms of GPU time per voice query
vi_gpus_per_datacenter_value = 10_000                    # GPUs in a large data center
vi_audio_sample_rate_value = 16_000                      # Hz
vi_audio_bits_value = 16                                 # bits per sample
vi_waking_hours_value = 16                               # hours users are awake
vi_peak_multiplier_value = 3                             # peak-to-average ratio

vi_total_queries_day_value = vi_devices_b_value * 1e9 * vi_queries_per_day_value
vi_gpu_seconds_day_value = vi_total_queries_day_value * vi_gpu_ms_per_query_value / 1000
vi_gpu_hours_day_value = vi_gpu_seconds_day_value / 3600
vi_datacenters_avg_value = vi_gpu_hours_day_value / (vi_gpus_per_datacenter_value * 24)
vi_datacenters_peak_value = vi_datacenters_avg_value * vi_peak_multiplier_value * (24 / vi_waking_hours_value)

vi_audio_bytes_per_sec_value = vi_audio_sample_rate_value * (vi_audio_bits_value / 8)
vi_audio_kb_per_sec_value = vi_audio_bytes_per_sec_value / 1024
vi_total_audio_pb_per_sec_value = (vi_audio_bytes_per_sec_value * vi_devices_b_value * 1e9) / 1e15

vi_peak_ratio_value = vi_peak_multiplier_value * (24 / vi_waking_hours_value)

vi_devices_str = fmt(vi_devices_b_value, precision=0, commas=False)
vi_queries_str = fmt(vi_queries_per_day_value, precision=0, commas=False)
vi_total_queries_str = fmt(vi_total_queries_day_value / 1e9, precision=0, commas=False)
vi_gpu_ms_str = fmt(vi_gpu_ms_per_query_value, precision=0, commas=False)
vi_gpu_hours_str = fmt(vi_gpu_hours_day_value, precision=0, commas=True)
vi_gpus_dc_str = fmt(vi_gpus_per_datacenter_value, precision=0, commas=True)
vi_dc_avg_str = fmt(vi_datacenters_avg_value, precision=0, commas=False)
vi_dc_peak_str = fmt(vi_datacenters_peak_value, precision=0, commas=False)
vi_peak_ratio_str = fmt(vi_peak_ratio_value, precision=1, commas=False)
vi_audio_kb_str = fmt(vi_audio_kb_per_sec_value, precision=0, commas=False)
vi_audio_pb_str = fmt(vi_total_audio_pb_per_sec_value, precision=0, commas=False)
```

::: {.callout-notebook title="The Voice Assistant Wall"}
\index{infrastructure scaling!voice assistants}\index{Cloud ML!scaling limits}**Scenario**: `{python} ww_devices_b_str` billion voice assistant devices (smartphones, smart speakers, earbuds). Can cloud data centers handle this?

**Part 1 — The Economic Wall**

- **Cloud Cost**: ~USD `{python} ww_cloud_cost_str` per device/year → `{python} ww_devices_b_str` B devices = **USD `{python} ww_total_cost_str`/year**. Economically prohibitive for a free feature.
- **TinyML Alternative**: `{python} ww_edge_power_range_str` mW local wake-word detection, <USD `{python} ww_edge_cost_str`/year per device. Viable at any scale.

**Part 2 — The Infrastructure Wall**

The economic argument is compelling, but the *physics* argument is decisive:

1.  **Query volume**: `{python} vi_devices_str` B devices × `{python} vi_queries_str` queries/day = **`{python} vi_total_queries_str` billion queries/day**.
2.  **GPU demand**: Each query requires ~`{python} vi_gpu_ms_str` ms of GPU time. Total: **`{python} vi_gpu_hours_str` GPU-hours/day**.
3.  **Data center capacity**: A large data center (~`{python} vi_gpus_dc_str` GPUs) provides 240,000 GPU-hours/day.
4.  **Average requirement**: ~**`{python} vi_dc_avg_str` dedicated data centers** just for voice inference.
5.  **Peak reality**: Queries cluster in waking hours (~`{python} vi_peak_ratio_str`$\times$ peak-to-average), requiring **~`{python} vi_dc_peak_str` data centers** at peak.

**The Bandwidth Wall**: Wake-word detection requires *continuous* audio monitoring. If devices streamed audio to the cloud (16 kHz, 16-bit), each transmits ~`{python} vi_audio_kb_str` KB/s. Across `{python} vi_devices_str` billion devices: **`{python} vi_audio_pb_str` PB/s**—orders of magnitude beyond total global internet backbone capacity.

**The Engineering Conclusion**: Cloud-only voice processing is not merely expensive; it is **physically impossible** at global scale. Local wake-word detection is an infrastructure necessity, not an optimization.
:::

This demonstrates a core systems principle: deployment decisions are constrained by performance requirements, economic realities, and infrastructure physics. The hybrid approach reduces end-to-end latency relative to pure cloud processing while maintaining the computational power needed for sophisticated language understanding, all within sustainable cost boundaries.

Recommendation engines deployed by Netflix and Amazon demonstrate another compelling application of cloud resources. These systems process massive datasets using collaborative filtering and deep learning architectures like the **Deep Learning Recommendation Model (DLRM)**[^fn-dlrm] to uncover patterns in user preferences. DLRM exemplifies a memory-capacity-bound workload: its massive embedding tables, representing millions of users and items, can exceed terabytes in size, requiring distributed memory across many servers just to store the model parameters. Cloud computational resources enable continuous updates and refinements as user data grows, with Netflix processing over 100 billion data points daily to deliver personalized content suggestions that directly enhance user engagement.

Financial institutions have revolutionized fraud detection through cloud ML capabilities. By analyzing vast amounts of transactional data in real-time, ML algorithms trained on historical fraud patterns can detect anomalies and suspicious behavior across millions of accounts, enabling proactive fraud prevention that minimizes financial losses.

Beyond these flagship applications, cloud ML permeates everyday online experiences through personalized advertisements on social media, predictive text in email services, product recommendations in e-commerce, enhanced search results, and security anomaly detection systems that continuously monitor for cyber threats at scale.

Yet for all its computational power, Cloud ML accepts an inescapable trade-off: every request must travel to a datacenter and back. When that round-trip delay exceeds the application's tolerance—as it inevitably does for safety-critical systems, real-time control, and privacy-sensitive workloads—the computation must move closer to the data.

[^fn-dlrm]: **Deep Learning Recommendation Model (DLRM)**: Meta's open-source architecture (2019) that became the industry benchmark for personalized recommendations [@naumov2019deep]. DLRM exemplifies the "Sparse Scatter" archetype: embedding tables for millions of users and items can exceed 100 TB, requiring distributed memory across hundreds of servers. The model's arithmetic intensity is extremely low (< 1 FLOP/byte), making it memory-capacity-bound rather than compute-bound, which shapes infrastructure design for recommendation systems in ways distinct from compute-bound workloads.

## Edge ML: Latency and Privacy {#sec-ml-system-architecture-edge-ml-reducing-latency-privacy-risk-2625}

\index{Edge ML!distance penalty} \index{Edge ML!data sovereignty}When latency budgets drop below 100 ms, cloud infrastructure hits a hard physical wall. The Distance Penalty means the speed of light alone imposes minimum latencies of 40--150 ms for cross-region requests—before any computation begins. When an autonomous vehicle needs to decide whether to brake, or an industrial robot needs to stop before hitting an obstacle, 100 ms is an eternity. The logical engineering response is to move the computation closer to the data source.

Edge ML emerged from this constraint, trading unlimited computational resources for sub-100 ms latency and local data retention. In Archetype terms, edge deployment transforms the optimization target: a **Bandwidth Hog** workload like LLM inference that is memory-bound in the cloud becomes *latency-bound* at the edge, where the 50–100 ms network penalty dominates the 10–20 ms compute time. Edge hardware with sufficient local memory can eliminate this penalty entirely, shifting the bottleneck back to the underlying memory bandwidth constraint. Recall the Iron Law from @eq-iron-law-extended: by processing locally, edge deployment eliminates the $D_{vol}/BW_{IO}$ (network I/O) term entirely, collapsing the latency to $\max(D_{vol}/BW, O/(R_{peak} \cdot \eta)) + L_{lat}$—the same memory-vs-compute trade-off, but without the network penalty that dominates cloud inference.

This paradigm shift is essential for applications where cloud's 100--500 ms round-trip delays are unacceptable. Autonomous systems requiring split-second decisions and industrial IoT[^fn-industrial-iot] applications demanding real-time response cannot tolerate network delays. Similarly, applications subject to strict data privacy regulations must process information locally rather than transmitting it to remote data centers. Edge devices (gateways and IoT hubs[^fn-iot-hubs]) occupy a middle ground in the deployment spectrum, maintaining acceptable performance while operating under intermediate resource constraints.

[^fn-industrial-iot]: **Industrial IoT (IIoT)**: Manufacturing generates massive data volumes annually but analyzes only a small fraction due to connectivity and latency constraints. Edge ML enables real-time quality control, predictive maintenance, and process optimization. Industry analyses project IIoT will contribute trillions of dollars annually to manufacturing by 2030 [@mckinsey2021iot].

[^fn-iot-hubs]: **IoT Hubs**: Edge gateways aggregating data from hundreds of sensors before cloud transmission, performing local preprocessing, filtering, and anomaly detection. AWS IoT Greengrass and Azure IoT Edge enable ML inference at the hub level. Reduces cloud traffic by 90%+ while enabling <10 ms local decisions for time-critical applications.

We define this paradigm formally as *Edge ML*.

::: {.callout-definition title="Edge ML"}

***Edge Machine Learning***\index{Edge ML!latency determinism}\index{Edge ML!local compute capacity}\index{latency determinism|see{Edge ML}}\index{data locality|see{Edge ML}} is the deployment paradigm optimized for **Latency Determinism** and **Data Locality**. By locating computation physically adjacent to data sources (gateways, on-premise servers, workstation-class accelerators), it circumvents the **Distance Penalty** of the cloud, trading elastic scale for the hard constraint of **Local Compute Capacity**. Edge ML spans a wide range from IoT gateways to workstation-class hardware; the unifying characteristic is physical proximity to data sources, not a specific resource constraint level.
:::

Trace these trade-offs in @fig-edge-ml, which organizes edge deployment into four operational dimensions. The **Characteristics** branch highlights decentralized processing, which drives the key **Benefit** of reduced latency. This trade-off, however, introduces **Challenges** in maintenance and security, as the physical hardware is distributed and harder to secure than a centralized datacenter.

::: {#fig-edge-ml fig-env="figure" fig-pos="t" fig-cap="**Edge ML Decomposition.** Characteristics, benefits, challenges, and representative applications of edge machine learning, where decentralized processing on nearby hardware reduces latency and network dependence at the cost of constrained compute and memory." fig-alt="Tree diagram with Edge ML branching to four categories: Characteristics, Benefits, Challenges, and Examples, listing items like decentralized processing, reduced latency, security concerns, and industrial IoT."}
```{.tikz}
\begin{tikzpicture}[font=\small\usefont{T1}{phv}{m}{n}]
\tikzset{
  Box/.style={inner xsep=2pt,
  draw=GreenLine,
  fill=GreenL!50,
  node distance=0.4,
    line width=0.75pt,
    anchor=west,
    text width=37mm,align=flush center,
    minimum width=37mm, minimum height=9.5mm
  },
  Box2/.style={Box,draw=BlueLine,fill=BlueL!50, text width=27mm, minimum width=27mm
  },
  Box3/.style={Box,draw=OrangeLine,fill=OrangeL!40, text width=28mm, minimum width=28mm
  },
 Box4/.style={Box,draw=VioletLine,fill=VioletL2!40, text width=30mm, minimum width=30mm
  },
 Line/.style={line width=1.0pt,black!50,text=black,-{Triangle[width=0.8*6pt,length=0.98*6pt]}},
}
\node[Box4, fill=VioletL2!90!violet!50,](B1){Characteristics};
\node[Box2,right=2 of B1,fill=BlueL](B2){Benefits};
\node[Box,right=2 of B2,fill=GreenL](B3){Challenges};
\node[Box3,right=2 of B3,fill=OrangeL](B4){Examples};
\node[Box,draw=OliveLine,fill=OliveL!30, minimum height=11.5mm,
above=1of $(B2.north east)!0.5!(B3.north west)$](B0){Edge ML};
%
\node[Box4,below=0.7 of B1](B11){Decentralized Data Processing};
\node[Box4,below=of B11](B12){Local Data Storage and Computation};
\node[Box4,below=of B12](B13){Proximity to Data Sources};
%
\node[Box2,below=0.7 of B2](B21){Reduced Latency};
\node[Box2,below=of B21](B22){Enhanced Data Privacy};
\node[Box2,below=of B22](B23){Lower Bandwidth Usage};
%
\node[Box,below=0.7 of B3](B31){Security Concerns at the Edge Nodes};
\node[Box,below=of B31](B32){Complexity in Managing Edge Nodes};
\node[Box,below=of B32](B33){Limited Computational Resources};
%
\node[Box3,below=0.7 of B4](B41){Industrial IoT};
\node[Box3,below=of B41](B42){Smart Homes and Cities};
\node[Box3,below=of B42](B43){Autonomous Vehicles};
%
\foreach \i in{1,2,3}{
\draw[Line](B1.west)--++(180:0.5)|-(B1\i);
}
\foreach \i in{1,2,3}{
\draw[Line](B2.west)--++(180:0.5)|-(B2\i);
}
\foreach \i in{1,2,3}{
\draw[Line](B3.west)--++(180:0.5)|-(B3\i);
}
\foreach \i in{1,2,3}{
\draw[Line](B4.west)--++(180:0.5)|-(B4\i);
}
\foreach \x in{1,2,3,4}{
\draw[Line](B0)-|(B\x);
}
\end{tikzpicture}
```
:::

The benefits of lower bandwidth usage and reduced latency become stark when we examine real-world data rates. The defining characteristic of edge deployment is not just *where* processing occurs, but *how much data* that location must handle. Consider what happens when the data rate exceeds available network capacity.

```{python}
#| echo: false
#| label: bandwidth-bottleneck
# ┌─────────────────────────────────────────────────────────────────────────────
# │ BANDWIDTH BOTTLENECK CALCULATION
# ├─────────────────────────────────────────────────────────────────────────────
# │ Context: Callout "The Bandwidth Bottleneck" (Edge ML section)
# │
# │ Why: 100 cameras at 1080p/30fps generate ~6 GB/s raw data — exceeding a
# │      10 Gbps line by 5×. Cloud streaming is PHYSICALLY IMPOSSIBLE. Edge
# │      processing reduces transmission to ~1 KB/detection, a 1,000,000× cut.
# │
# │ Imports: physx.constants (VIDEO_*, CLOUD_EGRESS_PER_GB, NETWORK_10G_BW),
# │          physx.formulas (calc_monthly_egress_cost), physx.formatting (fmt)
# │ Exports: cam_rate_mbs_str, total_rate_gbs_str, monthly_cost_m_str, etc.
# └─────────────────────────────────────────────────────────────────────────────
from physx.formulas import calc_monthly_egress_cost
from physx.formatting import fmt
from physx.constants import (
    VIDEO_1080P_WIDTH, VIDEO_1080P_HEIGHT, VIDEO_BYTES_PER_PIXEL_RGB,
    VIDEO_FPS_STANDARD, CLOUD_EGRESS_PER_GB, NETWORK_10G_BW, MB, GB,
)

# --- Inputs (factory scenario) ---
num_cameras_value = 100                                  # cameras on factory floor

# --- Process (bandwidth calculations) ---
raw_bytes_per_frame_value = VIDEO_1080P_WIDTH * VIDEO_1080P_HEIGHT * VIDEO_BYTES_PER_PIXEL_RGB
raw_bytes_per_sec_value = (raw_bytes_per_frame_value * VIDEO_FPS_STANDARD).to("byte/second")
total_bytes_per_sec_value = num_cameras_value * raw_bytes_per_sec_value
monthly_cost_value = calc_monthly_egress_cost(total_bytes_per_sec_value, CLOUD_EGRESS_PER_GB)
bw_shortage_value = (total_bytes_per_sec_value / NETWORK_10G_BW.to("byte/second")).magnitude

# --- Outputs (formatted strings for prose) ---
cam_rate_mbs_str = fmt(raw_bytes_per_sec_value.to(MB/second).magnitude, precision=0, commas=False)
total_rate_gbs_str = fmt(total_bytes_per_sec_value.to(GB/second).magnitude, precision=1, commas=False)
monthly_cost_m_str = fmt(monthly_cost_value / 1e6, precision=1, commas=False)   # $M/month
net_cap_gbs_str = fmt(NETWORK_10G_BW.to(GB/second).magnitude, precision=2, commas=False)
bw_short_x_str = fmt(bw_shortage_value, precision=0, commas=False)              # × shortfall
num_cameras_str = f"{num_cameras_value}"
fps_str = f"{int(VIDEO_FPS_STANDARD.magnitude)}"
egress_cost_str = f"{CLOUD_EGRESS_PER_GB.magnitude}"
video_width_str = fmt(VIDEO_1080P_WIDTH, precision=0, commas=False)
video_height_str = fmt(VIDEO_1080P_HEIGHT, precision=0, commas=False)
bytes_per_pixel_str = fmt(VIDEO_BYTES_PER_PIXEL_RGB, precision=0, commas=False)
```

::: {.callout-notebook title="The Bandwidth Bottleneck"}

\index{bandwidth bottleneck!video streaming} \index{Edge ML!bandwidth reduction}**Problem**: You are designing a quality control system for a factory floor with **`{python} num_cameras_str` cameras** running at **`{python} fps_str` FPS** with **1080p resolution**. Should you stream to the cloud or process at the edge?

**The Physics**:

1.  **Raw data rate per camera**: `{python} video_width_str` × `{python} video_height_str` × `{python} bytes_per_pixel_str` bytes × `{python} fps_str` FPS ≈ **`{python} cam_rate_mbs_str` MB/s**.
2.  **Total data rate**: `{python} num_cameras_str` cameras × `{python} cam_rate_mbs_str` MB/s = **`{python} total_rate_gbs_str` GB/s**.
3.  **Cloud upload cost**: At USD `{python} egress_cost_str`/GB egress, streaming 24/7 costs **USD `{python} monthly_cost_m_str` M/month**.
4.  **Network reality**: Even a dedicated 10 Gbps line (`{python} net_cap_gbs_str` GB/s) cannot carry the load—you need **`{python} bw_short_x_str`× more bandwidth** than exists.

**The Engineering Conclusion**: Physics has made cloud streaming **impossible** for this application. Edge processing is not optional—it is mandatory. An edge server running local inference transmits only defect metadata (~1 KB per detection), reducing bandwidth requirements by **1,000,000×**.
:::

The bandwidth calculation above reveals why edge processing is mandatory for high-volume sensor deployments. For battery-powered edge devices (wireless cameras, drones, wearables), the constraint is even more severe: as "The Energy of Transmission" (@sec-ml-systems-bottleneck-principle) established, radio transmission costs `{python} et_energy_ratio_str`$\times$ more energy than local inference, making cloud offloading physically impossible for battery-powered devices regardless of available bandwidth.

### Edge ML Benefits and Deployment Challenges {#sec-ml-system-architecture-edge-ml-benefits-deployment-challenges-b2d0}

\index{Edge ML!distributed processing} \index{Edge ML!deployment challenges}Edge ML spans wearables, industrial sensors, and smart home appliances that process data locally[^fn-iot-growth] without depending on central servers. Look at the range of hardware in @fig-edgeml-example—these edge devices occupy the middle ground between cloud systems and mobile devices in computational resources, power consumption, and cost. Memory bandwidth at 25--100 GB/s enables models requiring 100 MB--1 GB parameters. The optimization techniques covered in @sec-model-compression achieve 2--4$\times$ speedup compared to cloud models. Local processing generates substantial bandwidth savings: processing 1000 camera feeds locally avoids 1 Gbps uplink costs and reduces cloud expenses by $10,000--100,000 annually.

[^fn-iot-growth]: **IoT Device Growth**: Explosive growth from 8.4B connected devices (2017) to projected 25.4B by 2030 [@mckinsey2021iot]. Daily data generation approaches 2.5 quintillion bytes, with 90% requiring real-time processing. Network bandwidth and cloud costs make edge processing economically essential; uploading raw sensor data would cost $10–100 per device monthly.

![**Edge Device Deployment**: Diverse IoT devices, from wearables to home appliances, enable decentralized machine learning by performing inference locally, reducing reliance on cloud connectivity and improving response times. Source: Edge Impulse.](images/svg/edge_ml_iot.svg){#fig-edgeml-example fig-pos="t" fig-alt="Collection of IoT devices arranged on a surface: smart home sensors, fitness wearables, environmental monitors, and connected appliances in various sizes and form factors."}

[^fn-latency-critical]: **Latency-Critical Applications**: Systems where response time directly impacts safety or user experience. Autonomous vehicles require <10 ms for emergency braking; high-frequency trading operates at <100 µs; VR/AR needs <20 ms to prevent motion sickness. Cloud latency (100–500 ms) makes edge processing mandatory for these real-time applications.

Edge ML provides quantifiable benefits that address key cloud limitations. Latency reduction from 100--500 ms in cloud deployments to 1--50 ms at the edge enables safety-critical applications[^fn-latency-critical] requiring real-time response. A retail store with 50 cameras streaming video can reduce bandwidth requirements from 100 Mbps (costing $1,000--2,000 monthly) to less than 1 Mbps by processing locally and transmitting only metadata, a 99% reduction. Privacy improves through local processing, eliminating transmission risks and simplifying regulatory compliance. Operational resilience ensures systems continue functioning during network outages, critical for manufacturing, healthcare, and building management applications.

These benefits carry corresponding limitations that compound as deployments scale. Limited computational resources[^fn-endpoint-constraints] sharply constrain model complexity: edge servers often provide an order of magnitude or more less processing throughput than cloud infrastructure, limiting deployable models to millions rather than billions of parameters. Managing distributed networks introduces complexity that scales nonlinearly with deployment size—coordinating version control and updates across thousands of devices requires sophisticated orchestration systems[^fn-edge-coordination], and hardware heterogeneity across diverse platforms demands different optimization strategies for each target. Security challenges intensify because edge devices are physically accessible: equipment deployed in retail stores or public infrastructure faces tampering risks that centralized datacenters do not, requiring hardware-based protection mechanisms such as secure boot, encrypted storage, and tamper-evident enclosures. Initial deployment costs of $500–2,000 per edge server compound across locations: instrumenting 1,000 sites requires $500,000–2,000,000 upfront, though these capital costs are offset by lower long-term operational expenses compared to equivalent cloud spending.

[^fn-endpoint-constraints]: **Edge Server Constraints**: Edge hardware operates with 10–100× less memory (1–8 GB vs. 128–1024 GB), storage (2–32 GB vs. petabytes), and compute compared to cloud servers. Power budgets of 5–50 W vs. 500 W+ per server limit accelerator options. These constraints drive specialized model compression, quantization, and architecture search for edge-deployable models.

[^fn-edge-coordination]: **Edge Network Coordination**: Managing distributed edge devices requires sophisticated orchestration to handle the communication complexity of many interconnected nodes. Hierarchical architectures reduce coordination overhead, and specialized frameworks manage models, data, and updates across heterogeneous devices. We examine these operational patterns, including distributed orchestration and model registries, in @sec-machine-learning-operations-mlops.

To make these trade-offs concrete, the following worked example applies *edge inference sizing* to a realistic retail deployment scenario.

```{python}
#| echo: false
#| label: edge-sizing
# ┌─────────────────────────────────────────────────────────────────────────────
# │ EDGE INFERENCE SIZING: RETAIL DEPLOYMENT
# ├─────────────────────────────────────────────────────────────────────────────
# │ Context: Callout "Edge Inference Sizing" — hardware selection for retail chain
# │
# │ Why: Walks through a realistic edge deployment decision: given N stores with
# │      M cameras at F fps, what hardware meets throughput requirements at
# │      minimum TCO? Demonstrates that low-power edge devices (Coral) can beat
# │      higher-performance options (Jetson) when throughput needs are modest.
# │
# │ Imports: physx.constants (YOLOV8_NANO_FLOPs, GFLOPs, CLOUD_ELECTRICITY_PER_KWH,
# │                           HOURS_PER_YEAR), physx.formulas (calc_fleet_tco)
# │ Exports: stores_str, cameras_per_store_str, fps_str, inf_per_sec_str,
# │          yolo_gflops_str, sustained_gf_str, req_tflops_str, coral_*_str,
# │          jetson_*_str, nuc_*_str, coral_tco_k_str, years_str, etc.
# └─────────────────────────────────────────────────────────────────────────────
from physx.constants import YOLOV8_NANO_FLOPs, GFLOPs, CLOUD_ELECTRICITY_PER_KWH, HOURS_PER_YEAR
from physx.formulas import calc_fleet_tco
from physx.formatting import fmt

# --- Deployment scenario ---
stores_value = 500                                                               # retail locations
cameras_per_store_value = 20                                                     # cameras per store
fps_value = 15                                                                   # frames per second
headroom_value = 2                                                               # 2× engineering margin

# --- Hardware options (specs + costs) ---
coral_cost_value = 150                                                           # USD, Google Coral Dev
coral_power_w_value = 2                                                          # W
coral_tops_value = 4                                                             # INT8 TOPS
jetson_cost_value = 600                                                          # USD, NVIDIA Jetson Orin NX
jetson_power_w_value = 25                                                        # W (midpoint of 10-40W)
jetson_power_range_str = "10-40"                                                 # W range
jetson_tops_value = 100                                                          # INT8 TOPS
nuc_cost_value = 400                                                             # USD, Intel NUC + Movidius
nuc_power_w_value = 15                                                           # W
nuc_tops_value = 4                                                               # INT8 TOPS
years_value = 3                                                                  # TCO horizon
hours_per_year_value = HOURS_PER_YEAR                                            # 8760 hours
cloud_inference_cost_k_value = 500                                               # ~$500K cloud baseline
int8_throughput_mult_value = 4                                                   # INT8 vs FP32 speedup

# --- Throughput requirements ---
inferences_per_sec_value = cameras_per_store_value * fps_value                   # 300 inf/s per store
sustained_gflops_value = (
    inferences_per_sec_value * YOLOV8_NANO_FLOPs.to(GFLOPs).magnitude            # YOLOv8-nano compute
)
required_tflops_value = (sustained_gflops_value * headroom_value * GFLOPs).to(TFLOPs).magnitude

# --- TCO calculations ---
coral_tco_value = calc_fleet_tco(
    coral_cost_value, coral_power_w_value, stores_value, years_value, CLOUD_ELECTRICITY_PER_KWH
)
jetson_tco_value = calc_fleet_tco(
    jetson_cost_value, jetson_power_w_value, stores_value, years_value, CLOUD_ELECTRICITY_PER_KWH
)
nuc_tco_value = calc_fleet_tco(
    nuc_cost_value, nuc_power_w_value, stores_value, years_value, CLOUD_ELECTRICITY_PER_KWH
)
coral_fleet_value = coral_cost_value * stores_value                              # hardware capex
jetson_fleet_value = jetson_cost_value * stores_value
coral_power_cost_value = coral_tco_value - coral_fleet_value                     # electricity opex

# --- Output strings ---
stores_str = f"{stores_value}"                                                   # e.g. "500"
cameras_per_store_str = f"{cameras_per_store_value}"                             # e.g. "20"
fps_str = f"{fps_value}"                                                         # e.g. "15"
headroom_str = f"{headroom_value}"                                               # e.g. "2"
inf_per_sec_str = f"{inferences_per_sec_value}"                                  # e.g. "300"
yolo_gflops_str = fmt(YOLOV8_NANO_FLOPs.to(GFLOPs).magnitude, precision=1, commas=False)  # e.g. "4.1"
sustained_gf_str = fmt(sustained_gflops_value, precision=0, commas=False)        # e.g. "1230"
req_tflops_str = fmt(required_tflops_value, precision=0, commas=False)           # e.g. "2"
coral_cost_str = f"{coral_cost_value}"                                           # e.g. "150"
coral_power_w_str = f"{coral_power_w_value}"                                     # e.g. "2"
coral_tops_str = f"{coral_tops_value}"                                           # e.g. "4"
jetson_cost_str = f"{jetson_cost_value}"                                         # e.g. "600"
jetson_tops_str = f"{jetson_tops_value}"                                         # e.g. "100"
nuc_cost_str = f"{nuc_cost_value}"                                               # e.g. "400"
nuc_power_w_str = f"{nuc_power_w_value}"                                         # e.g. "15"
nuc_tops_str = f"{nuc_tops_value}"                                               # e.g. "4"
coral_fleet_k_str = fmt(coral_fleet_value / 1000, precision=0, commas=False)     # e.g. "75" ($K)
jetson_fleet_k_str = fmt(jetson_fleet_value / 1000, precision=0, commas=False)   # e.g. "300" ($K)
nuc_fleet_k_str = fmt(nuc_cost_value * stores_value / 1000, precision=0, commas=False)  # e.g. "200" ($K)
coral_pwr_k_str = fmt(coral_power_cost_value / 1000, precision=0, commas=False)  # e.g. "3" ($K)
coral_tco_k_str = fmt(coral_tco_value / 1000, precision=0, commas=False)         # e.g. "78" ($K)
years_str = f"{years_value}"                                                     # e.g. "3"
hours_per_year_str = f"{hours_per_year_value}"                                   # e.g. "8760"
power_ratio_str = fmt(jetson_power_w_value / coral_power_w_value, precision=0, commas=False)  # e.g. "12"
elec_cost_str = f"{CLOUD_ELECTRICITY_PER_KWH.magnitude}"                         # e.g. "0.12"
cloud_cost_k_str = f"{cloud_inference_cost_k_value}"                             # e.g. "500" ($K)
int8_mult_str = f"{int8_throughput_mult_value}"                                  # e.g. "4"
cost_ratio_str = f"{jetson_cost_value // coral_cost_value}"                      # e.g. "4"
```

::: {.callout-notebook title="Edge Inference Sizing"}
**Scenario**: A smart retail chain deploying person detection across `{python} stores_str` stores, each with `{python} cameras_per_store_str` cameras at `{python} fps_str` FPS.

**Requirements Analysis**

| **Metric**               | **Calculation**                                                         | **Result**                                |
|:-----------------------|:----------------------------------------------------------------------|:----------------------------------------|
| **Inferences per store** | `{python} cameras_per_store_str` cameras × `{python} fps_str` FPS       | `{python} inf_per_sec_str` inferences/sec |
| **Model compute**        | YOLOv8-nano: `{python} yolo_gflops_str` GFLOPs/inference                | `{python} sustained_gf_str` GFLOPs/sec    |
| **Required throughput**  | `{python} sustained_gf_str` GFLOPs × `{python} headroom_str` (headroom) | ~`{python} req_tflops_str` TFLOPS         |

**Hardware Selection**

| **Edge Device**           |                   **INT8 TOPS** | **Power**                           | **Unit Cost**                  |                        **Fleet Cost** |
|:------------------------|------------------------------:|:----------------------------------|:-----------------------------|------------------------------------:|
| **NVIDIA Jetson Orin NX** | `{python} jetson_tops_str` TOPS | `{python} jetson_power_range_str` W | USD `{python} jetson_cost_str` | USD `{python} jetson_fleet_k_str`,000 |
| **Intel NUC + Movidius**  |    `{python} nuc_tops_str` TOPS | `{python} nuc_power_w_str` W        | USD `{python} nuc_cost_str`    |    USD `{python} nuc_fleet_k_str`,000 |
| **Google Coral Dev**      |  `{python} coral_tops_str` TOPS | `{python} coral_power_w_str` W      | USD `{python} coral_cost_str`  |  USD `{python} coral_fleet_k_str`,000 |

**Decision**: At `{python} req_tflops_str` TFLOPS required and INT8 quantization providing ~`{python} int8_mult_str`× effective throughput, the Coral Dev Board (`{python} coral_tops_str` TOPS) meets requirements at 1/`{python} cost_ratio_str` the cost of Jetson, with `{python} power_ratio_str`× lower power consumption. Note: peak TOPS should be derated by ~50% for realistic sustained throughput (due to operator support, data loading, and memory constraints); the `{python} headroom_str`× engineering headroom partially accounts for this gap.

**TCO over `{python} years_str` years** (Coral): Hardware USD `{python} coral_fleet_k_str` K + Power (USD `{python} coral_power_w_str`×`{python} stores_str`×`{python} hours_per_year_str` h×`{python} years_str` yr×USD `{python} elec_cost_str`/kWh) = USD `{python} coral_fleet_k_str` K + USD `{python} coral_pwr_k_str` K = **USD `{python} coral_tco_k_str`,000 total** vs. cloud inference at ~USD `{python} cloud_cost_k_str` K.
:::

### Real-Time Industrial and IoT Systems {#sec-ml-system-architecture-realtime-industrial-iot-systems-373a}

\index{Edge ML!autonomous vehicles} \index{Edge ML!industrial IoT} \index{Edge ML!smart retail} \index{predictive maintenance!edge deployment}Industries deploy Edge ML widely where low latency, data privacy, and operational resilience justify the additional complexity. Autonomous vehicles represent the most demanding application, where safety-critical decisions must occur within milliseconds based on sensor data that cannot be transmitted to remote servers. Systems like Tesla's Full Self-Driving process inputs from multiple cameras at high frame rates through custom edge hardware, making driving decisions with end-to-end latency on the order of milliseconds. This response time is infeasible with cloud processing due to network delays.

Smart retail environments demonstrate edge ML's practical advantages for privacy-sensitive, bandwidth-intensive applications. Amazon Go[^fn-amazon-go] stores process video from hundreds of cameras through local edge servers, tracking customer movements and item selections to enable checkout-free shopping. This edge-based approach addresses both technical and privacy concerns. Transmitting high-resolution video from hundreds of cameras would require substantial sustained bandwidth, while local processing keeps raw video on premises, reducing exposure and simplifying compliance.

The Industrial IoT[^fn-industry-40] leverages edge ML for applications where millisecond-level responsiveness directly impacts production efficiency and worker safety. Manufacturing facilities deploy edge ML systems for real-time quality control. Vision systems inspect welds at speeds exceeding 60 parts per minute. Predictive maintenance[^fn-predictive-maintenance] applications monitor over 10,000 industrial assets per facility. This approach has demonstrated 25–35% reductions in unplanned downtime across various manufacturing sectors.

Smart buildings utilize edge ML to optimize energy consumption while maintaining operational continuity during network outages. Commercial buildings equipped with edge-based building management systems process data from thousands of sensors monitoring temperature, occupancy, air quality, and energy usage. This reduces cloud transmission requirements by an order of magnitude or more while enabling sub-second response times. Healthcare applications similarly leverage edge ML for patient monitoring and surgical assistance, maintaining HIPAA compliance through local processing while supporting low-latency workflows for real-time guidance.

These applications share a common assumption: the edge device is stationary and plugged into wall power. But billions of users carry their computing devices with them, and those devices run on batteries. When we shift from stationary edge infrastructure to the smartphone in a user's pocket, the dominant constraint changes from latency to *energy*—and with it, the entire engineering calculus.

[^fn-amazon-go]: **Amazon Go**: Launched in 2018, this checkout-free retail concept demonstrates edge ML at scale. Each store deploys hundreds of cameras and shelf sensors processed by local GPU clusters running multi-object tracking, pose estimation, and activity recognition. The system must process ~1 TB/hour of video data locally because cloud transmission would require impractical bandwidth (100+ Mbps sustained) and add unacceptable latency for real-time tracking.

[^fn-industry-40]: **Industry 4.0**: Fourth industrial revolution (term coined 2011 at Hannover Fair) integrating AI, IoT, and cyber-physical systems into manufacturing. Digital twins simulate production lines; ML optimizes scheduling and quality control. Industry analyses project significant productivity gains and cost reductions across global manufacturing through smart factory adoption [@mckinsey2021iot].

[^fn-predictive-maintenance]: **Predictive Maintenance**: ML-driven maintenance scheduling analyzing vibration, temperature, and acoustic signatures to predict equipment failures. Industry deployments report significant reductions in unplanned downtime, maintenance costs, and extended equipment life. Large-scale industrial IoT platforms monitor millions of assets, demonstrating substantial savings through failure avoidance [@mckinsey2021iot].

## Mobile ML: Offline Intelligence {#sec-ml-system-architecture-mobile-ml-personal-offline-intelligence-0983}

\index{Mobile ML!battery constraints} \index{Mobile ML!thermal envelope}Edge ML solves the distance problem that limits cloud deployments, achieving sub-100 ms latency through local processing. However, edge devices remain tethered to stationary infrastructure—gateways, factory servers, retail edge systems. Users do not stay in one place, so neither can their AI. To bring ML capabilities to users in motion, we must solve a different constraint: the **Battery**. Unlike plugged-in edge servers that can consume hundreds of watts continuously, mobile devices must operate for hours or days on fixed energy budgets.

Mobile ML addresses this challenge by integrating machine learning directly into portable devices like smartphones and tablets, providing users with real-time, personalized capabilities. This paradigm excels when user privacy, offline operation, and immediate responsiveness matter more than computational sophistication, supporting applications such as voice recognition[^fn-voice-recognition], computational photography[^fn-computational-photography], and health monitoring while maintaining data privacy through on-device computation. These battery-powered devices must balance performance with power efficiency and thermal management, making them suited to frequent, short-duration AI tasks.

The mobile environment introduces a critical constraint absent from stationary deployments: *energy per inference* becomes a first-order design parameter. In the Iron Law (@eq-iron-law-extended), cloud and edge systems optimize for minimizing $T$—total latency. Mobile systems face an additional constraint: $\text{Energy} = \text{Power} \times T$, and the Power Wall (@eq-power-scaling) caps sustained power at `{python} mobile_tdp_range_str` W. In Archetype terms, a **Compute Beast** workload like image classification must be transformed through architectural efficiency (e.g., depthwise separable convolutions[^fn-depthwise-separable] in MobileNet) to become a **Compute Beast (efficient)**—reducing FLOPs by `{python} mobilenet_flops_reduction_str`× while preserving accuracy. This is not merely optimization; it represents a qualitative shift in the arithmetic intensity trade-off, accepting lower peak throughput in exchange for sustainable operation within a `{python} mobile_tdp_range_str` W thermal envelope.

[^fn-voice-recognition]: **Voice Recognition Evolution**: Apple Siri (2011) required cloud processing with 200–500 ms latency and privacy concerns. By 2017, on-device models reduced latency to <50 ms while keeping audio local. Modern NPUs process 16 kHz audio in 20–30 ms using transformer-based models; Google's on-device transcription achieves 95%+ accuracy entirely locally.

[^fn-computational-photography]: **Computational Photography**: Combines multiple exposures and ML algorithms to enhance image quality. Google's Night Sight captures 15 frames in 6 seconds, using ML to align and merge them. Portrait mode uses depth estimation ML models to create professional-looking bokeh effects in real-time.

[^fn-depthwise-separable]: **Depthwise Separable Convolutions**: Architectural innovation introduced by MobileNet (2017) that factorizes standard convolutions into depthwise and pointwise operations. For a $D_K \times D_K$ kernel on $M$ input channels producing $N$ outputs, standard convolution costs $D_K^2 \times M \times N$ multiplications, while depthwise separable costs $D_K^2 \times M + M \times N$, yielding 8–9× reduction for typical parameters. This efficiency enables running vision models within mobile power budgets.

We define this paradigm formally as *Mobile ML*.

::: {.callout-definition title="Mobile ML"}

***Mobile Machine Learning***\index{Mobile ML!thermal design power}\index{thermal throttling!mobile devices}\index{TDP|see{Mobile ML}} is the deployment paradigm bounded by **Thermal Design Power (TDP)**. Unlike cloud systems constrained by cost, Mobile ML is limited by the **Heat Dissipation** capacity of passive cooling (typically 2–3W) and total battery energy, requiring architectures that trade peak throughput for **Sustained Energy Efficiency**.
:::

See how these constraints play out in @fig-mobile-ml, which lays out the unique characteristics of mobile deployment. The **Characteristics** branch emphasizes sensor integration and on-device processing, which enables key **Benefits** like real-time processing and enhanced privacy. However, the **Challenges** branch reveals battery life constraints and limited computational resources that force engineers to optimize for sustained efficiency over raw performance.

::: {#fig-mobile-ml fig-env="figure" fig-pos="t" fig-cap="**Mobile ML Decomposition.** Characteristics, benefits, challenges, and representative applications of mobile machine learning, where on-device processing and hardware acceleration balance computational efficiency, battery life, and model performance on smartphones and tablets." fig-alt="Tree diagram with Mobile ML branching to four categories: Characteristics, Benefits, Challenges, and Examples. Each lists items like on-device processing, real-time response, battery constraints, and voice recognition."}
```{.tikz}
\begin{tikzpicture}[font=\small\usefont{T1}{phv}{m}{n}]
\tikzset{
  Box/.style={inner xsep=2pt,
  draw=GreenLine,
  fill=GreenL!50,
  node distance=0.4,
    line width=0.75pt,
    anchor=west,
    text width=32mm,align=flush center,
    minimum width=32mm, minimum height=9.5mm
  },
  Box2/.style={Box,draw=BlueLine,fill=BlueL!50, text width=30mm, minimum width=30mm
  },
  Box3/.style={Box,draw=OrangeLine,fill=OrangeL!40, text width=35mm, minimum width=35mm
  },
 Box4/.style={Box,draw=VioletLine,fill=VioletL2!40, text width=32mm, minimum width=32mm
  },
 Line/.style={line width=1.0pt,black!50,text=black,-{Triangle[width=0.8*6pt,length=0.98*6pt]}},
}
\node[Box4, fill=VioletL2!90!violet!50,](B1){Characteristics};
\node[Box2,right=2 of B1,fill=BlueL](B2){Benefits};
\node[Box,right=2 of B2,fill=GreenL](B3){Challenges};
\node[Box3,right=2 of B3,fill=OrangeL](B4){Examples};
\node[Box,draw=OliveLine,fill=OliveL!30, minimum height=11.5mm,
above=1of $(B2.north east)!0.5!(B3.north west)$](B0){Mobile ML};
%
\node[Box4,below=0.7 of B1](B11){On-Device Processing};
\node[Box4,below=of B11](B12){Battery-Powered Operation};
\node[Box4,below=of B12](B13){Sensor Integration};
\node[Box4,below=of B13](B14){Optimized Frameworks};
%
\node[Box2,below=0.7 of B2](B21){Real-Time Processing};
\node[Box2,below=of B21](B22){Enhanced Privacy};
\node[Box2,below=of B22](B23){Offline Functionality};
\node[Box2,below=of B23](B24){Personalized Experience};
%
\node[Box,below=0.7 of B3](B31){Limited Computational Resources};
\node[Box,below=of B31](B32){Battery Life Constraints};
\node[Box,below=of B32](B33){Storage Limitations};
\node[Box,below=of B33](B34){Model Optimization Requirements};
%
\node[Box3,below=0.7 of B4](B41){Voice Recognition};
\node[Box3,below=of B41](B42){Computational Photography};
\node[Box3,below=of B42](B43){Health Monitoring};
\node[Box3,below=of B43](B44){Real-Time Translation};
%
\foreach \i in{1,2,3,4}{
\draw[Line](B1.west)--++(180:0.5)|-(B1\i);
}
\foreach \i in{1,2,3,4}{
\draw[Line](B2.west)--++(180:0.5)|-(B2\i);
}
\foreach \i in{1,2,3,4}{
\draw[Line](B3.west)--++(180:0.5)|-(B3\i);
}
\foreach \i in{1,2,3,4}{
\draw[Line](B4.west)--++(180:0.5)|-(B4\i);
}
\foreach \x in{1,2,3,4}{
\draw[Line](B0)-|(B\x);
}
\end{tikzpicture}

```
:::

The battery life and resource constraints listed above translate directly into engineering requirements. Consider the implications for always-on ML features.

```{python}
#| echo: false
#| label: battery-tax
# ┌─────────────────────────────────────────────────────────────────────────────
# │ BATTERY TAX: ALWAYS-ON MOBILE ML POWER BUDGET
# ├─────────────────────────────────────────────────────────────────────────────
# │ Context: Callout "The Battery Tax" — shows why continuous mobile ML drains batteries
# │
# │ Why: Quantifies the energy cost of always-on inference. A 2W object detector
# │      depletes a 15 Wh phone battery in ~7.5 hours, consuming 100% of daily
# │      energy budget. This explains why mobile ML requires aggressive power
# │      management (duty cycling, wake words) rather than continuous inference.
# │
# │ Imports: physx.constants (PHONE_BATTERY_WH, OBJECT_DETECTOR_POWER_W)
# │ Exports: pwr_w_str, batt_wh_str, runtime_str, budget_pct_str, runtime_frac
# └─────────────────────────────────────────────────────────────────────────────
from physx.constants import PHONE_BATTERY_WH, OBJECT_DETECTOR_POWER_W, ureg
from physx.formatting import md_frac, fmt

runtime_q_value = PHONE_BATTERY_WH / OBJECT_DETECTOR_POWER_W                     # battery / power = hours
daily_budget_pct_value = (OBJECT_DETECTOR_POWER_W * runtime_q_value) / PHONE_BATTERY_WH * 100  # 100%

runtime_str = fmt(runtime_q_value.to(ureg.hour).magnitude, precision=1, commas=False)          # e.g. "7.5" hours
pwr_w_str = fmt(OBJECT_DETECTOR_POWER_W.to(ureg.watt).magnitude, precision=0, commas=False)    # e.g. "2" W
batt_wh_str = fmt(PHONE_BATTERY_WH.to(ureg.watt * ureg.hour).magnitude, precision=0, commas=False)  # e.g. "15" Wh
budget_pct_str = fmt(daily_budget_pct_value.magnitude, precision=0, commas=False)              # e.g. "100" %
runtime_frac = md_frac(f"{batt_wh_str} Wh", f"{pwr_w_str} W", f"**{runtime_str} hours**")      # LaTeX fraction
```

::: {.callout-notebook title="The Battery Tax"}

\index{battery life!ML impact} \index{Mobile ML!energy budget}**Problem**: You want to deploy a "real-time" background object detector on a smartphone. The model consumes **`{python} pwr_w_str` Watts** of continuous power when active. The phone has a standard **`{python} batt_wh_str` Watt-hour (Wh)** battery.

**The Physics**:

1.  **Ideal Runtime**: `{python} runtime_frac`
2.  **The Reality**: A user expects their phone to last 24 hours. Your single feature has just consumed **`{python} budget_pct_str`%** of the entire daily energy budget in a few hours.

**The Engineering Conclusion**: You cannot simply "deploy" the model. You must use the techniques in @sec-model-compression (quantization, duty-cycling) to reduce the power to **<100 mW** if you want it to stay on all day.
:::

The battery constraint limits total energy consumption over time. However, even if we could ignore battery life—perhaps for a plugged-in tablet or a short demo—a second physical law intervenes: thermodynamics. Every watt of computation becomes a watt of heat that must be dissipated. In a data center, massive cooling systems remove this heat. In a thin, sealed mobile device with no fan, the only heat path is through the glass and metal casing to the surrounding air. This thermal reality creates a hard ceiling on sustained power consumption that exists independently of battery capacity.

```{python}
#| label: thermal-quant-calc
#| echo: false
# ┌─────────────────────────────────────────────────────────────────────────────
# │ THERMAL WALL: QUANTIZATION POWER REDUCTION
# ├─────────────────────────────────────────────────────────────────────────────
# │ Context: Callout "The Thermal Wall" — shows limits of quantization for thermal
# │
# │ Why: Even 4× power reduction from FP32→INT8 quantization cannot save a 12W
# │      model from thermal throttling on mobile (3W TDP limit). The resulting
# │      3W is still at the absolute limit, demonstrating that some workloads
# │      are physically impossible on mobile regardless of optimization.
# │
# │ Imports: physx.formatting (fmt)
# │ Exports: baseline_str, quant_power_str, quant_red_str
# └─────────────────────────────────────────────────────────────────────────────
from physx.formatting import fmt

baseline_power_w_value = 12                                                      # W, unoptimized LLM power
quant_reduction_value = 4                                                        # FP32→INT8 power reduction

quant_power_w_value = baseline_power_w_value / quant_reduction_value             # 12W / 4 = 3W

baseline_str = fmt(baseline_power_w_value, precision=0, commas=False)            # e.g. "12" W
quant_power_str = fmt(quant_power_w_value, precision=0, commas=False)            # e.g. "3" W
quant_red_str = fmt(quant_reduction_value, precision=0, commas=False)            # e.g. "4" ×
```

::: {.callout-notebook title="The Thermal Wall"}

\index{thermal wall!mobile constraints} \index{Power Wall!mobile implications}**Problem**: Your unoptimized LLM requires **10 W** peak compute. Can you deploy it on a mobile device?

**The Physics**:

1. **Thermal Design Power (TDP)**: A mobile SoC allows $\approx \mathbf{3 \text{ W}}$ for passive cooling.
2. **Temperature Rise**: At 10 W, the device temperature rises at $\approx 1^\circ\text{C}$ per second.
3. **Thermal Trip**: Within 60 seconds, the hardware reaches the **Thermal Trip Point** ($80^\circ\text{C}$), triggering OS throttling.
4. **The Result**: Your 100 FPS model suddenly drops to **30 FPS** to avoid melting the hardware.

**The Engineering Conclusion**: Quantization from FP32 to INT8 reduces power by approximately `{python} quant_red_str`×, but if the baseline power is `{python} baseline_str` W, you are still at `{python} quant_power_str` W—the absolute limit of the hardware. Physics sets a hard ceiling that no optimization can exceed.
:::

### Mobile ML Benefits and Resource Constraints {#sec-ml-system-architecture-mobile-ml-benefits-resource-constraints-c568}

\index{Mobile ML!NPU inference} \index{Mobile ML!memory bandwidth limits} \index{Neural Processing Unit (NPU)!mobile devices}Mobile devices exemplify intermediate constraints: `{python} mobile_ram_range_str` GB RAM (varying from mid-range to flagship), `{python} mobile_storage_range_str` storage, `{python} mobile_npu_range_str` TOPS AI compute through Neural Processing Units[^fn-npu] consuming `{python} mobile_tdp_range_str` W power. System-on-Chip architectures[^fn-mobile-soc] integrate computation and memory to minimize energy costs. Memory bandwidth of `{python} mobile_bw_range_str` GB/s limits models to 10–100 MB parameters, requiring the aggressive optimization techniques that @sec-model-compression details. Battery constraints (`{python} phone_battery_str`–22 Wh capacity) make energy optimization critical: 1 W continuous ML processing reduces device lifetime from 24 to 18 hours. Specialized frameworks (TensorFlow Lite[^fn-tflite], Core ML[^fn-coreml]) provide hardware-optimized inference enabling <`{python} mobile_latency_range_str` ms UI response times.

[^fn-mobile-soc]: **Mobile System-on-Chip (SoC)**: Heterogeneous processors integrating CPU, GPU, NPU, ISP, and memory controller on a single die. Apple's A17 Pro (3nm, 19B transistors) delivers 35 TOPS via its 16-core Neural Engine; Qualcomm's Snapdragon 8 Gen 3 delivers approximately 34 TOPS through its Hexagon NPU. SoC integration reduces data movement energy 10–100× compared to discrete components.

[^fn-npu]: **Neural Processing Unit (NPU)**: Specialized processors optimized for efficient neural network inference on mobile devices. NPUs achieve high inference performance within tight power budgets, enabling on-device AI. We examine NPU architectures and their performance characteristics in @sec-ai-acceleration.

[^fn-tflite]: **TensorFlow Lite**: Google's mobile/embedded ML framework (2017) optimizing models through quantization, pruning, and operator fusion. Supports Android, iOS, Linux, and microcontrollers. Deploys on 4B+ devices running applications from Google Translate (35 MB multilingual model) to on-device speech recognition with <100 ms latency.

[^fn-coreml]: **Core ML**: Apple's on-device ML framework (iOS 11, 2017) with automatic optimization for Apple Silicon. Seamlessly schedules across CPU, GPU, and Neural Engine based on model characteristics. Supports vision, NLP, and audio models from 1 KB--1 GB with compiler optimizations achieving 2--10$\times$ speedups over naive deployment.

Mobile ML excels at delivering responsive, privacy-preserving user experiences. Real-time processing can reach sub-10 ms latency for some tasks, enabling imperceptible response in interactive applications. Stronger privacy properties emerge when sensitive inputs are processed locally, reducing data transmission and central storage, though privacy still depends on system design and threat model. On-device enclaves and similar hardware isolation mechanisms can help protect sensitive computations (for example, biometric processing)[^fn-face-detection]. Offline functionality reduces network dependency: navigation, translation[^fn-real-time-translation], and media processing can run locally within mobile resource budgets. Personalization improves because models can leverage on-device signals and user context while keeping raw data local.

[^fn-real-time-translation]: **Real-Time Translation**: On-device neural machine translation processing 40+ language pairs without internet connectivity. Google Translate's offline models (35–45 MB per language) achieve 90% of cloud quality (2 GB+ models) through knowledge distillation and quantization. Enables privacy-preserving translation with <500 ms latency on mid-range smartphones.

[^fn-face-detection]: **Mobile Face Detection**: Apple's Face ID projects 30,000 IR dots for 3D face mapping, processed entirely in the Secure Enclave (isolated cryptographic coprocessor). Biometric templates never leave the device; even Apple cannot access them. Achieves 1:1,000,000 false acceptance rate vs. Touch ID's 1:50,000, demonstrating privacy-preserving edge AI.

These benefits require accepting tight resource constraints. Compared to cloud deployments, mobile applications often operate under much tighter memory, storage, and latency budgets, which constrains model size and batch behavior. Battery life[^fn-mobile-constraints] presents visible user impact, and thermal throttling can materially limit sustained performance: peak NPU throughput is often substantially higher than what is sustainable under prolonged workloads. Development complexity multiplies across platforms, demanding separate implementations and careful performance tuning, while device heterogeneity requires multiple model variants. Deployment friction adds further challenges: app store review processes can take days, slowing iteration compared to cloud workflows.

[^fn-mobile-constraints]: **Mobile Device Constraints**: Flagship phones (12–24 GB RAM, 15–25 W peak power) operate with 10–100× less resources than cloud servers (256–2048 GB RAM, 200–400 W). Thermal throttling limits sustained performance; battery life requires <500 mW average inference power. These constraints drove innovations in efficient architectures (MobileNet, EfficientNet) and on-device optimization.

### Personal Assistant and Media Processing {#sec-ml-system-architecture-personal-assistant-media-processing-98d7}

\index{Mobile ML!computational photography} \index{Mobile ML!voice recognition} \index{Mobile ML!health monitoring}Mobile ML has achieved success across diverse applications for billions of users worldwide. Computational photography transformed smartphone cameras into sophisticated imaging systems. Modern flagships process every photo through multiple ML pipelines operating in real-time: portrait mode[^fn-portrait-mode] uses depth estimation and segmentation networks to achieve DSLR-quality bokeh effects, night mode captures and aligns 9–15 frames with ML-based denoising that reduces noise by 10–20 dB, and systems like Google Pixel process 10–15 distinct ML models per photo for HDR merging, super-resolution, and scene optimization.

Voice-driven interactions demonstrate mobile ML's transformation of human-device communication. These systems combine ultra-low-power wake-word detection consuming less than 1 mW with on-device speech recognition achieving under 10 ms latency for simple commands. Keyboard prediction has evolved to context-aware neural models achieving 60–70% phrase prediction accuracy, reducing typing effort by 30–40%. Real-time camera translation processes over 100 languages at 15–30 FPS entirely on-device, enabling instant visual translation without internet connectivity.

Health monitoring through wearables like Apple Watch extracts sophisticated insights from sensor data while maintaining complete privacy. These systems achieve over 95% accuracy in activity detection and include FDA-cleared atrial fibrillation detection with 98%+ sensitivity, processing extraordinarily sensitive health data entirely on-device to maintain HIPAA compliance. Accessibility features demonstrate transformative social impact through continuous local processing: Live Text detects and recognizes text from camera feeds, Sound Recognition alerts deaf users to environmental cues through haptic feedback, and VoiceOver generates natural language descriptions of visual content.

Augmented reality frameworks leverage mobile ML for real-time environment understanding at 60 FPS. ARCore and ARKit track device position with centimeter-level accuracy while simultaneously mapping 3D surroundings, enabling hand tracking that extracts 21-joint 3D poses and face analysis of 50+ landmark meshes for real-time effects. These applications demand consistent sub-16 ms frame times, making only on-device processing viable for delivering the seamless experiences users expect.

[^fn-portrait-mode]: **Portrait Mode Photography**: Computational photography using ML segmentation to separate subjects from backgrounds, applying synthetic depth-of-field effects mimicking DSLR bokeh. Dual cameras or LiDAR provide depth estimation; neural networks refine edges around hair and translucent objects. Processing occurs in real-time (<100 ms) on NPUs, enabling live preview before capture.

Despite mobile ML's demonstrated capabilities, a common pitfall involves attempting to deploy desktop-trained models directly to mobile or edge devices without architecture modifications. Models developed on powerful workstations often fail when deployed to resource-constrained devices. A ResNet-50 model requiring 4 GB memory for inference (including activations and batch processing) and `{python} resnet_gflops_str` billion FLOPs per inference cannot run on a device with 512 MB of RAM and a 1 GFLOP/s processor. Beyond simple resource violations, desktop-optimized models may use operations unsupported by mobile hardware (specialized mathematical operations), assume floating-point precision unavailable on embedded systems, or require batch processing incompatible with single-sample inference. Successful deployment demands architecture-aware design from the beginning, including specialized architectural techniques for mobile devices such as MobileNet's depthwise separable convolutions [@howard2017mobilenets] (detailed in @sec-dnn-architectures), integer-only operations for microcontrollers, and optimization strategies that maintain accuracy while reducing computation.

Mobile ML demonstrates that useful intelligence can operate within a `{python} mobile_tdp_range_str` W thermal envelope on battery power—but smartphones still cost hundreds of dollars and require gigabytes of memory. What happens when we tighten the constraints by three more orders of magnitude: kilobytes of memory, milliwatts of power, and dollars per device?

## TinyML: Ubiquitous Sensing {#sec-ml-system-architecture-tinyml-ubiquitous-sensing-scale-a67b}

\index{TinyML!ubiquitous sensing} \index{TinyML!cost efficiency}Mobile ML brings intelligence to users on the move, but smartphones cost hundreds to thousands of dollars and are far too large to embed in everyday objects. To put "eyes and ears" on every physical object—from individual boxes in a warehouse to every leaf in a field—we must push beyond mobile constraints entirely, solving for both **cost** (dollars, not hundreds of dollars) and **size** (millimeters, not centimeters).

TinyML [@reddi2022widening] completes the deployment spectrum by pushing intelligence to its physical limits. Devices costing less than $10 and consuming less than 1 milliwatt of power make ubiquitous[^fn-ubiquitous] sensing economically practical at massive scale. This is the exclusive domain of the **Tiny Constraint** Archetype, where the optimization objective shifts from maximizing throughput to minimizing energy per inference. A keyword spotting model consuming 10 µJ per inference can operate for years on a coin-cell battery, achieving million-fold improvements in energy efficiency by trading model capacity for operational longevity.

Where mobile ML requires sophisticated hardware with gigabytes of memory and multi-core processors, Tiny Machine Learning operates on microcontrollers with kilobytes of RAM and single-digit dollar price points. This radical constraint forces a shift in machine learning deployment, prioritizing ultra-low power consumption and minimal cost over computational sophistication.

TinyML brings intelligence to the smallest devices, from microcontrollers[^fn-microcontrollers-specs] to embedded sensors, enabling real-time computation in severely resource-constrained environments [@banbury2021mlperftiny; @lin2020mcunet]. This paradigm excels in applications requiring ubiquitous sensing, autonomous operation, and maximal energy efficiency. TinyML systems power applications such as predictive maintenance, environmental monitoring, and simple gesture recognition. The energy gap between TinyML and cloud inference spans six orders of magnitude[^fn-energy-efficiency]—a 1,000,000× difference that drives entirely different system architectures and deployment models. This extraordinary efficiency enables operation for months or years on limited power sources such as coin-cell batteries[^fn-coin-cell], as exemplified by the device kits in @fig-TinyML-example. These systems deliver actionable insights in remote or disconnected environments where power, connectivity, and maintenance access are impractical.

[^fn-microcontrollers-specs]: **Microcontrollers**: Single-chip computers with integrated CPU, memory, and peripherals, typically operating at 1–100 MHz with 32 KiB–2 MiB RAM. Arduino Uno uses an ATmega328P with 32 KiB flash and 2 KiB RAM, while ESP32 provides WiFi capability with 520 KiB RAM, still thousands of times less than a smartphone.

[^fn-energy-efficiency]: **Energy Efficiency in TinyML**: Ultra-low power enables decade-long deployment in remote locations. ARM Cortex-M0+ consumes <1 µW in sleep, 100–300 µW/MHz active. Specialized accelerators (Syntiant NDP, MAX78000) achieve <1 µJ per inference.

[^fn-coin-cell]: **Coin-Cell Batteries**: Compact power sources (CR2032: 225 mAh at 3 V) enabling "deploy-and-forget" IoT devices. TinyML models consuming 10–50 µW average power can operate 1–10 years on a single cell. Constrains models to <100 KB (fitting in on-chip SRAM), driving innovation in efficient neural network architectures and intermittent computing paradigms.

![**TinyML System Scale**: Small development boards, including Arduino Nano BLE Sense and similar microcontroller kits approximately 2 to 5 cm in length, with visible processor chips and pin connectors that enable sensor integration for always-on ML inference at milliwatt power budgets. Source: [@warden2018speech]](images/png/tiny_ml.png){#fig-TinyML-example fig-alt="Small development boards including Arduino Nano BLE Sense and similar microcontroller kits arranged on a surface, each approximately 2–5 cm in length with visible chips and connectors."}

[^fn-ubiquitous]: **Ubiquitous**: From Latin *ubique* (everywhere), combining *ubi* (where) and the generalizing suffix *-que*. Mark Weiser at Xerox PARC coined "ubiquitous computing" in 1988 to describe technology so embedded in the environment that it becomes invisible. TinyML realizes this vision: when sensors cost dollars and run for years on a battery, intelligence can literally be *everywhere*, disappearing into the physical world.

We define this paradigm formally as *TinyML*.

::: {.callout-definition title="TinyML"}

***TinyML***\index{TinyML!kilobyte-scale memory}\index{TinyML!milliwatt-scale power}\index{on-chip SRAM!TinyML requirement}\index{energy harvesting!TinyML deployment} is the domain of **Always-On Sensing** constrained by **Kilobyte-Scale Memory** and **Milliwatt-Scale Power**. It necessitates models small enough to reside entirely in on-chip SRAM (avoiding the energy cost of DRAM access) to enable continuous inference on energy-harvested or coin-cell power sources.
:::

TinyML's milliwatt-scale power consumption represents a six-order-of-magnitude reduction from cloud inference, a gap with profound implications for system design. In terms of the Iron Law (@eq-iron-law-extended), TinyML operates in a regime where the dominant constraint is neither $O/(R_{peak} \cdot \eta)$ nor $D_{vol}/BW$, but a term the equation does not explicitly capture: $D_{vol}/\text{Capacity}$. When total memory is measured in kilobytes, the model must fit entirely on-chip, and every byte of data movement costs energy measured in picojoules. The optimization objective shifts from minimizing latency to minimizing energy per inference—efficiency, not speed.

```{python}
#| label: energy-inference-calc
#| echo: false
# ┌─────────────────────────────────────────────────────────────────────────────
# │ ENERGY PER INFERENCE: PARADIGM COMPARISON TABLE
# ├─────────────────────────────────────────────────────────────────────────────
# │ Context: Table "Energy Per Inference" — 8 orders of magnitude across paradigms
# │
# │ Why: Demonstrates the massive energy gap between deployment tiers. A TinyML
# │      wake-word detector (10 µJ) is 100,000,000× more efficient than a cloud
# │      LLM query (1 kJ). This explains why always-on sensing requires TinyML,
# │      not cloud or even mobile inference. Battery life numbers make it visceral.
# │
# │ Imports: physx.constants (BATTERY_*, ENERGY_MOBILENET_INF_MJ)
# │ Exports: e_*_str (energy per inference), q_*_str (queries per battery),
# │          batt_cap_mah_str, batt_volt_str
# └─────────────────────────────────────────────────────────────────────────────
from physx.constants import (
    BATTERY_CAPACITY_MAH, BATTERY_VOLTAGE_V, BATTERY_ENERGY_J,
    ENERGY_MOBILENET_INF_MJ, ureg
)
from physx.formatting import fmt

# --- Energy per inference (full-system, not just accelerator) ---
e_gpt4_j = 1000 * ureg.joule                                                     # ~1 kJ cloud LLM query
e_resnet_cloud_j = 10 * ureg.joule                                               # ~10 J cloud ResNet-50
e_resnet_edge_j = 0.5 * ureg.joule                                               # ~500 mJ edge ResNet-50
e_mobilenet_j = 0.05 * ureg.joule                                                # ~50 mJ mobile MobileNet
e_kws_j = 0.00001 * ureg.joule                                                   # ~10 µJ TinyML keyword spotting

batt_energy_j = BATTERY_ENERGY_J                                                 # Phone battery in joules

# --- Queries per full battery charge ---
q_gpt4 = batt_energy_j / e_gpt4_j                                                # ~54 queries
q_resnet_cloud = batt_energy_j / e_resnet_cloud_j                                # ~5,400 queries
q_resnet_edge = batt_energy_j / e_resnet_edge_j                                  # ~108,000 queries
q_mobilenet = batt_energy_j / e_mobilenet_j                                      # ~1,080,000 queries
q_kws = batt_energy_j / e_kws_j                                                  # ~5.4 billion queries

# --- Output strings (formatted for table) ---
e_gpt4_str = "~1 kJ"                                                             # display string
e_resnet_cloud_str = "~10 J"
e_resnet_edge_str = "~500 mJ"
e_mobilenet_str = "~50 mJ"
e_kws_str = "~10 µJ"

q_gpt4_str = fmt(q_gpt4.magnitude, precision=0, commas=True)                     # e.g. "54"
q_resnet_cloud_str = fmt(q_resnet_cloud.magnitude, precision=0, commas=True)     # e.g. "5,400"
q_resnet_edge_str = fmt(q_resnet_edge.magnitude, precision=0, commas=True)       # e.g. "108,000"
q_mobilenet_str = fmt(q_mobilenet.magnitude, precision=0, commas=True)           # e.g. "1,080,000"
q_kws_str = fmt(q_kws.magnitude / 1e9, precision=0, commas=False) + " billion"   # e.g. "5 billion"

batt_cap_mah_str = f"{BATTERY_CAPACITY_MAH.magnitude:.0f}"                       # e.g. "4000" mAh
batt_volt_str = f"{BATTERY_VOLTAGE_V.magnitude}"                                 # e.g. "3.7" V
```

::: {.callout-notebook title="Energy Per Inference"}

\index{energy per inference!paradigm comparison} \index{TinyML!energy efficiency}Energy consumption spans eight orders of magnitude across deployment paradigms:

| **Paradigm** | **Example Workload** |          **Energy/Inference** | **Battery Life (`{python} batt_volt_str`V, `{python} batt_cap_mah_str`mAh)** |
|:-----------|:-------------------|----------------------------:|---------------------------------------------------------------------------:|
| **Cloud**    | GPT-4 query          |         `{python} e_gpt4_str` |                                               ~`{python} q_gpt4_str` queries |
| **Cloud**    | ResNet-50 (A100)     | `{python} e_resnet_cloud_str` |                                       ~`{python} q_resnet_cloud_str` queries |
| **Edge**     | ResNet-50 (Jetson)   |  `{python} e_resnet_edge_str` |                                        ~`{python} q_resnet_edge_str` queries |
| **Mobile**   | MobileNet (NPU)      |    `{python} e_mobilenet_str` |                                          ~`{python} q_mobilenet_str` queries |
| **TinyML**   | Keyword spotting     |          `{python} e_kws_str` |                                                ~`{python} q_kws_str` queries |

Energy values represent *full-system energy* (including server CPUs, memory, networking, and cooling overhead), not isolated accelerator compute energy. For example, the A100 GPU alone executes ResNet-50 inference in under 1 ms (~0.3 J), but the full server draws ~1 kW when amortized across queuing, preprocessing, and idle power.

**Key insight**: A TinyML wake-word detector at 10 µJ/inference is **100,000,000×** more energy-efficient than a cloud LLM query. This gap explains why always-on sensing is only practical at the TinyML tier—a smartphone running continuous cloud queries would drain in minutes.
:::

To see where TinyML sits relative to the other paradigms, turn to @fig-tiny-ml. The **Characteristics** branch reveals the extreme constraints: milliwatt power and kilobyte memory. These limits enable the **Benefit** of "always-on" sensing that no other paradigm can sustain, but force engineers to solve the **Challenge** of extreme model compression.

::: {#fig-tiny-ml fig-env="figure" fig-pos="t" fig-cap="**TinyML Decomposition.** Characteristics, benefits, challenges, and representative applications of TinyML, where milliwatt power budgets and kilobyte memory limits enable always-on sensing and localized intelligence in embedded applications." fig-alt="Tree diagram with TinyML branching to four categories: Characteristics, Benefits, Challenges, and Examples, listing items like low-power operation, always-on capability, resource limitations, and predictive maintenance."}
```{.tikz}
\begin{tikzpicture}[font=\small\usefont{T1}{phv}{m}{n}]
\tikzset{
  Box/.style={inner xsep=2pt,
  draw=GreenLine,
  fill=GreenL!50,
  node distance=0.4,
    line width=0.75pt,
    anchor=west,
    text width=32mm,align=flush center,
    minimum width=32mm, minimum height=9.5mm
  },
  Box2/.style={Box,draw=BlueLine,fill=BlueL!50, text width=27mm, minimum width=27mm
  },
  Box3/.style={Box,draw=OrangeLine,fill=OrangeL!40, text width=28mm, minimum width=28mm
  },
Box4/.style={Box,draw=VioletLine,fill=VioletL2!40, text width=39mm, minimum width=39mm
  },
 Line/.style={line width=1.0pt,black!50,text=black,-{Triangle[width=0.8*6pt,length=0.98*6pt]}},
}
\node[Box4, fill=VioletL2!90!violet!50,](B1){Characteristics};
\node[Box2,right=2 of B1,fill=BlueL](B2){Benefits};
\node[Box,right=2 of B2,fill=GreenL](B3){Challenges};
\node[Box3,right=2 of B3,fill=OrangeL](B4){Examples};
\node[Box,draw=OliveLine,fill=OliveL!30, minimum height=11.5mm,
above=1of $(B2.north east)!0.5!(B3.north west)$](B0){TinyML};
%
\node[Box4,below=0.7 of B1](B11){Low Power and Resource Constrained Environments};
\node[Box4,below=of B11](B12){On-Device Machine Learning};
\node[Box4,below=of B12](B13){Ultra-Small Form Factor};
%
\node[Box2,below=0.7 of B2](B21){Extremely Low Latency};
\node[Box2,below=of B21](B22){High Data Security};
\node[Box2,below=of B22](B23){Energy Efficiency};
\node[Box2,below=of B23](B24){Always-On Operation};
%
\node[Box,below=0.7 of B3](B31){Complex Development Cycle};
\node[Box,below=of B31](B32){Model Optimization and Compression};
\node[Box,below=of B32](B33){Resource Limitations};
%
\node[Box3,below=0.7 of B4](B41){Anomaly Detection};
\node[Box3,below=of B41](B42){Environmental Monitoring};
\node[Box3,below=of B42](B43){Predictive Maintenance};
\node[Box3,below=of B43](B44){Wearable Devices};
%
\foreach \i in{1,2,3}{
\draw[Line](B1.west)--++(180:0.5)|-(B1\i);
}
\foreach \i in{1,2,3,4}{
\draw[Line](B2.west)--++(180:0.5)|-(B2\i);
}
\foreach \i in{1,2,3}{
\draw[Line](B3.west)--++(180:0.5)|-(B3\i);
}
\foreach \i in{1,2,3,4}{
\draw[Line](B4.west)--++(180:0.5)|-(B4\i);
}
\foreach \x in{1,2,3,4}{
\draw[Line](B0)-|(B\x);
}
\end{tikzpicture}
```
:::

### TinyML Advantages and Operational Trade-offs {#sec-ml-system-architecture-tinyml-advantages-operational-tradeoffs-2d40}

\index{TinyML!resource constraints} \index{TinyML!model compression} \index{microcontrollers!ML deployment}TinyML operates at hardware extremes. Compared to cloud systems, TinyML deployments often have on the order of \(10^4\) to \(10^5\) times less memory and power budgets in the milliwatt range. Such strict limitations enable months or years of autonomous operation[^fn-on-device-training] but demand specialized algorithms and careful systems co-design. Devices range from palm-sized developer kits to millimeter-scale chips[^fn-device-size], enabling ubiquitous sensing in contexts where networking, power, or maintenance are costly. Representative developer kits include the Arduino Nano 33 BLE Sense (256 KiB RAM, 1 MiB flash, 20–40 mW) and ESP32-CAM (520 KiB RAM, 4 MiB flash, 50–250 mW).

[^fn-on-device-training]: **On-Device Training Constraints**: Microcontrollers (256KiB-2MiB RAM) cannot support full backpropagation through large networks. Alternatives include on-device fine-tuning of final layers, federated learning with local gradient computation, and TinyTL (memory-efficient training using <50KiB). Apple's on-device personalization adapts keyboard predictions without uploading typing data.

[^fn-device-size]: **TinyML Device Scale**: ML-capable chips range from 5×5 mm (Syntiant NDP: 140 µW, 1 MiB SRAM) to full single-board computers (Coral Dev Board Mini: 40×48 mm, 4 TOPS). This 100× size range reflects diverse deployment needs from implantable medical devices to industrial edge gateways processing multiple sensor streams simultaneously.

TinyML's extreme resource constraints enable unique advantages that are difficult to achieve at other scales. Avoiding network transmission reduces end-to-end latency and eliminates communication overhead, enabling rapid local responses for sensing and control loops. The economics can be compelling for massive-scale deployments because per-node costs can be low enough to instrument large physical environments. Energy efficiency enables multi-year operation on small batteries, and energy harvesting can further extend lifetimes in some settings. Privacy improves because raw data can remain local, reducing transmission and central storage, though this does not automatically provide formal privacy guarantees without additional privacy and security mechanisms.

These capabilities require substantial trade-offs. Computational constraints impose severe limits: microcontrollers commonly provide on the order of \(10^5\) to \(10^6\) bytes of RAM, which can force models and intermediate activations into the tens of kilobytes to low megabytes range, depending on the workload. Development complexity requires expertise spanning neural network optimization, hardware-level memory management, embedded toolchains, and specialized debugging across diverse microcontroller architectures.

Beyond these technical constraints, operational challenges compound the difficulty. Model quality can suffer from aggressive compression and reduced precision, limiting suitability for applications requiring high accuracy or robustness. Deployment can also be inflexible: devices may run a small set of fixed models, and updates may require firmware workflows that are slower and riskier than cloud rollouts. Ecosystem fragmentation[^fn-tinyml-optimization] across microcontroller vendors and ML frameworks creates additional overhead and portability challenges.

[^fn-tinyml-optimization]: **TinyML Model Optimization**: Compression techniques enable running ML on microcontrollers by dramatically reducing model size while preserving accuracy. Quantization, pruning, knowledge distillation, and architecture search work together to achieve these reductions. Detailed compression methods, including quantization and pruning, are covered in @sec-model-compression.

### Environmental and Health Monitoring {#sec-ml-system-architecture-environmental-health-monitoring-14ad}

\index{TinyML!wake-word detection} \index{TinyML!precision agriculture} \index{TinyML!medical wearables}TinyML succeeds across domains where its advantages of ultra-low power, low per-node cost, and local processing enable applications impractical with other paradigms.

Wake-word detection represents a visible consumer application of TinyML, where always-listening capabilities can be implemented at sub-milliwatt continuous power consumption. These systems typically process audio streams locally and only activate higher-power components when a wake phrase is detected, reducing average device power draw[^fn-fitness-trackers].

Precision agriculture leverages TinyML's economic advantages where traditional solutions prove cost-prohibitive. Deployments can instrument thousands of monitoring points with multi-year battery operation, transmitting summaries instead of raw sensor streams to reduce connectivity costs.

Wildlife conservation uses TinyML for remote environmental monitoring. Researchers deploy solar-powered audio sensors consuming 100–500 mW that process continuous audio streams for species identification. By performing local analysis, these systems reduce satellite transmission requirements from 4.3 GB per day to 400 KB of detection summaries, a 10,000× reduction that makes large-scale deployments of 100–1,000 sensors economically feasible. Medical wearables achieve FDA-cleared cardiac monitoring with 95–98% sensitivity while processing 250–500 ECG samples per second at under 5 mW power consumption. This efficiency enables week-long continuous monitoring versus hours for smartphone-based alternatives, while reducing diagnostic costs from $2,000–5,000 for traditional in-lab studies to under $100 for at-home testing.

[^fn-fitness-trackers]: **TinyML in Fitness Trackers**: Wearables run continuous ML inference on accelerometer, gyroscope, and heart rate data. Apple Watch's fall detection analyzes motion patterns at 50 Hz, distinguishing falls from sitting down with high accuracy. Operating at <1 mW enables week-long battery life while monitoring health metrics 24/7, a defining example of always-on TinyML.

## Paradigm Selection {#sec-ml-system-architecture-comparative-analysis-paradigm-selection-bf66}

\index{deployment paradigms!comparative analysis} \index{paradigm selection!decision framework}Each paradigm emerged as a response to specific physical constraints: Cloud ML accepts latency for unlimited compute, Edge ML trades compute for latency, Mobile ML trades compute for portability, and TinyML trades compute for ubiquity. How do these paradigms compare quantitatively across all dimensions? And given a specific application, how should an engineer select among them? This section synthesizes the individual paradigm analyses into a unified comparison framework and a structured decision process.

### Quantitative Trade-off Analysis {#sec-ml-system-architecture-quantitative-tradeoff-analysis-56a8}

\index{deployment paradigms!quantitative comparison} \index{latency vs throughput!paradigm trade-offs}The preceding four sections traced each paradigm individually, revealing its strengths, constraints, and representative applications. But deployment decisions require seeing all four paradigms *side by side* across the dimensions that matter. A system architect choosing between edge and mobile deployment needs to compare not just latency, but also power, cost, privacy, and development complexity simultaneously.

@tbl-big_vs_tiny provides this comparison across fourteen dimensions, from compute power and latency to cost and deployment speed.

```{python}
#| label: paradigms-table
#| echo: false
# ┌─────────────────────────────────────────────────────────────────────────────
# │ PARADIGMS TABLE: CLOUD VS EDGE VS MOBILE VS TINYML
# ├─────────────────────────────────────────────────────────────────────────────
# │ Context: Table @tbl-big_vs_tiny — 14-dimension paradigm comparison
# │
# │ Why: Synthesizes the four deployment paradigms into a single reference table.
# │      Students can use this for quick lookups when making deployment decisions.
# │      Each dimension (latency, compute, storage, energy, cost) shows the
# │      orders-of-magnitude differences that drive paradigm selection.
# │
# │ Imports: (none — pure display constants)
# │ Exports: cloud_*_str, edge_*_str, mobile_*_str, tiny_*_str
# └─────────────────────────────────────────────────────────────────────────────

# --- Latency (network + inference time) ---
cloud_lat_str = "100 ms-1000 ms+"                                                # cloud round-trip
edge_lat_str = "10-100 ms"                                                       # local network + inference
mobile_lat_str = "5-50 ms"                                                       # on-device inference
tiny_lat_str = "1-10 ms"                                                         # MCU response time

# --- Compute capability ---
cloud_comp_str = "Very High (Multiple GPUs/TPUs)"                                # kW-class accelerators
edge_comp_str = "High (Edge GPUs)"                                               # 10s-100s W accelerators
mobile_comp_str = "Moderate (Mobile NPUs/GPUs)"                                  # 1-10 W NPUs
tiny_comp_str = "Very Low (MCU/tiny processors)"                                 # mW-class MCUs

# --- Storage capacity ---
cloud_stor_str = "Unlimited (petabytes+)"                                        # elastic cloud storage
edge_stor_str = "Large (terabytes)"                                              # local SSDs
mobile_stor_str = "Moderate (gigabytes)"                                         # phone flash
tiny_stor_str = "Very Limited (kilobytes-megabytes)"                             # SRAM/flash

# --- Energy consumption ---
cloud_pwr_str = "Very High (kW-MW range)"                                        # data center scale
edge_pwr_str = "High (100 s W)"                                                  # edge server scale
mobile_pwr_str = "Moderate (1-10 W)"                                             # phone TDP
tiny_pwr_str = "Very Low (mW range)"                                             # energy harvesting

# --- Cost structure ---
cloud_cost_str = "High ($1000s+/month)"                                          # usage-based cloud
edge_cost_str = "Moderate ($100s-1000s)"                                         # hardware capex
mobile_cost_str = "Low ($0-10s)"                                                 # app distribution
tiny_cost_str = "Very Low ($1-10s)"                                              # MCU unit cost
```

The resulting fourteen-dimension comparison appears in @tbl-big_vs_tiny:

| **Aspect**                 | **Cloud ML**                             | **Edge ML**                            | **Mobile ML**                 | **TinyML**                                            |
|:-------------------------|:---------------------------------------|:-------------------------------------|:----------------------------|:----------------------------------------------------|
| **Processing Location**    | Centralized cloud servers (Data Centers) | Local edge devices (gateways, servers) | Smartphones and tablets       | Ultra-low-power microcontrollers and embedded systems |
| **Latency**                | `{python} cloud_lat_str`                     | `{python} edge_lat_str`                    | `{python} mobile_lat_str`         | `{python} tiny_lat_str`                                   |
| **Compute Power**          | `{python} cloud_comp_str`                    | `{python} edge_comp_str`                   | `{python} mobile_comp_str`        | `{python} tiny_comp_str`                                  |
| **Storage Capacity**       | `{python} cloud_stor_str`                    | `{python} edge_stor_str`                   | `{python} mobile_stor_str`        | `{python} tiny_stor_str`                                  |
| **Energy Consumption**     | `{python} cloud_pwr_str`                     | `{python} edge_pwr_str`                    | `{python} mobile_pwr_str`         | `{python} tiny_pwr_str`                                   |
| **Scalability**            | Excellent (virtually unlimited)          | Good (limited by edge hardware)        | Moderate (per-device scaling) | Limited (fixed hardware)                              |
| **Data Privacy**           | Basic-Moderate (Data leaves device)      | High (Data stays in local network)     | High (Data stays on phone)    | Very High (Raw data can remain local)                 |
| **Connectivity Required**  | Constant high-bandwidth                  | Intermittent                           | Optional                      | None                                                  |
| **Offline Capability**     | None                                     | Good                                   | Excellent                     | Complete                                              |
| **Real-time Processing**   | Dependent on network                     | Good                                   | Very Good                     | Excellent                                             |
| **Cost**                   | `{python} cloud_cost_str`                    | `{python} edge_cost_str`                   | `{python} mobile_cost_str`        | `{python} tiny_cost_str`                                  |
| **Hardware Requirements**  | Cloud infrastructure                     | Edge servers/gateways                  | Modern smartphones            | MCUs/embedded systems                                 |
| **Development Complexity** | High (cloud expertise needed)            | Moderate-High (edge+networking)        | Moderate (mobile SDKs)        | High (embedded expertise)                             |
| **Deployment Speed**       | Fast                                     | Moderate                               | Fast                          | Slow                                                  |

: **Deployment Locations**\index{deployment paradigms!fourteen-dimension comparison}\index{scalability!paradigm comparison}\index{offline capability!paradigm comparison}: Machine learning systems vary in where computation occurs, from centralized cloud servers to local edge devices and ultra-low-power TinyML chips, each impacting latency, bandwidth, and energy consumption. Deployments are categorized by processing location and associated characteristics, enabling informed decisions about system architecture and resource allocation. {#tbl-big_vs_tiny}

Note that privacy and compute show inverse relationships across @tbl-big_vs_tiny: TinyML provides the strongest privacy guarantees but the weakest compute, while cloud provides the strongest compute but weaker privacy guarantees. This inverse relationship is not coincidental; it reflects the inherent trade-off between data locality and computational scale.

The archetype-paradigm mapping established in @sec-ml-system-architecture-analyzing-workloads helps connect these characteristics to specific workload requirements, with each archetype gravitating toward paradigms that address its binding constraint.

Compare these trade-offs side by side in @fig-op_char, which plots performance and operational characteristics as radar charts—each paradigm forms a polygon, and larger areas indicate stronger performance on that axis. Plot a) contrasts compute power and scalability where Cloud ML excels against latency and energy efficiency where TinyML dominates, with Edge and Mobile ML occupying intermediate positions.

::: {#fig-op_char fig-env="figure" fig-pos="t" fig-cap="**Paradigm Comparison Radar Plots.** Two radar plots quantify performance and operational characteristics across cloud, edge, mobile, and TinyML paradigms. The left plot contrasts compute power, latency, scalability, and energy efficiency; the right plot contrasts connectivity independence, privacy, real-time capability, and offline operation. In both plots, higher scores indicate better performance on that dimension." fig-alt="Two radar plots with four overlapping polygons each. Left plot axes: compute power, latency, scalability, energy efficiency. Right plot axes: connectivity independence, privacy, real-time, offline capability."}
```{.tikz}
\begin{tikzpicture}[font=\usefont{T1}{phv}{m}{n}]
\pgfplotsset{myaxis/.style={
   y axis line style={draw=none},
   x axis line style={draw=black,line width=1 pt},
    width=8cm,
    height=8cm,
    grid=both,
    grid style={black!30,dashed},
    tick align=inside,
    tick style={draw=none},
    ymin=0, ymax=10,
    ytick={1,3,5,7,9},
    yticklabels={},
    xtick={0,90,180,270},
    xticklabel style={align=left,font=\fontsize{8pt}{9}\selectfont\usefont{T1}{phv}{m}{n}},
 % yticklabel style={font=\fontsize{7pt}{7}\selectfont\usefont{T1}{phv}{m}{n}},
     yticklabel style={
     rotate around={50:(axis cs:0,0)},
     anchor=center
    },
   xlabel style={font=\fontsize{7pt}{7}\selectfont\usefont{T1}{phv}{m}{n},rotate=30},
   label distance=5pt,
   legend style={at={(1.25,1)}, anchor=north},
   legend cell align=left,
   legend style={fill=BrownL!30,draw=BrownLine,row sep=2.1pt,
   font=\fontsize{7pt}{7}\selectfont\usefont{T1}{phv}{m}{n}},
      cycle list={
     {myblue,line width=1.5pt,fill=myblue!70,fill opacity=0.9},
     {mygreen,line width=1.5pt,fill=mygreen!70,fill opacity=0.4},
     {myorange,line width=1.5pt,fill=myorange!20,fill opacity=0.4},
     {myred,line width=1.5pt,fill=myred!70,fill opacity=0.4},
  },
    after end axis/.code={
      % manua y-tick labele on 50°
      \foreach \R in {1,3,5,7,9}{
      \pgfmathtruncatemacro{\newR}{\R + 0.5} %
        \node[
          font=\footnotesize\usefont{T1}{phv}{m}{n},
          anchor=base
        ]
        at (axis cs:50,\newR) {\R};
      }
    },
    legend image code/.code={
      % rectangle in Legend
      \draw[fill=#1,draw=none,fill opacity=1]
        (0pt,-2pt) rectangle (4mm,3pt);
    }
    }}
 %left graph
\begin{scope}[local bounding box=GR1,shift={(0,0)}]
\begin{polaraxis}[myaxis,
    xticklabels={Compute\\ Power, Latency, Scalability,Energy\\ Efficiency},
]
% Cloud ML
\addplot+[]  coordinates {(0,10) (90,2) (180,10) (270,3) (360,10)};
% Edge ML
\addplot+[] coordinates {(0,8) (90,7) (180,8) (270,5) (360,8)};
% Mobile ML
\addplot+[] coordinates {(0,6) (90,8) (180,7) (270,7) (360,6)};
% TinyML
\addplot+[]  coordinates {(0,3) (90,9) (180,5) (270,10) (360,3)};
\legend{Cloud ML, Edge ML, Mobile ML, TinyML}
\addplot[draw=myblue,line width=1.5pt]   coordinates {(0,10) (90,2) (180,10) (270,3) (360,10)};
\addplot[draw=mygreen,line width=1.5pt]  coordinates {(0,8) (90,7) (180,8) (270,5) (360,8)};

\end{polaraxis}
\end{scope}
\node[below=2mm of GR1,xshift=-5mm]{\large a)};
 %right graph
\begin{scope}[local bounding box=GR2,shift={(10,0)}]
\begin{polaraxis}[myaxis,
xticklabels={Connectivity\\ Independence, Data Privacy, Real-time\\ Processing,Offline Capability},
]
% Cloud ML
\addplot+[]  coordinates {(0,2) (90,3) (180,2) (270,2) (360,2)};
% Edge ML
\addplot+[] coordinates {(0,7) (90,7) (180,8) (270,6) (360,7)};
% Mobile ML
\addplot+[] coordinates {(0,8) (90,9) (180,7) (270,8) (360,8)};
% TinyML
\addplot+[]  coordinates {(0,10) (90,10) (180,10) (270,10) (360,10)};
%\legend{Cloud ML, Edge ML, Mobile ML, TinyML}
\addplot[draw=myblue,line width=1.5pt]  coordinates {(0,2) (90,3) (180,2) (270,2) (360,2)};
\addplot[draw=mygreen,line width=1.5pt] coordinates {(0,7) (90,7) (180,8) (270,6) (360,7)};
\end{polaraxis}
\end{scope}
\node[below=2mm of GR2]{\large b)};
\end{tikzpicture}
```
:::

The right plot (b) emphasizes operational dimensions where TinyML excels (privacy, connectivity independence, offline capability) versus Cloud ML's reliance on centralized infrastructure and constant connectivity.

Development complexity varies inversely with hardware capability: Cloud and TinyML require deep expertise (cloud infrastructure and embedded systems respectively), while Mobile and Edge leverage more accessible SDKs and tooling. Cost structures show similar inversion: Cloud incurs ongoing operational expenses ($1000s+/month), Edge requires moderate upfront investment ($100s-1000s), Mobile leverages existing devices ($0-10s), and TinyML minimizes hardware costs ($1-10s) while demanding higher development investment.

A critical pitfall in deployment selection involves choosing paradigms based solely on model accuracy metrics without considering system-level constraints. Teams often select deployment strategies by comparing model accuracy in isolation, overlooking critical system requirements that determine real-world viability. A cloud-deployed model achieving 99% accuracy becomes useless for autonomous emergency braking if network latency exceeds reaction time requirements. Similarly, a sophisticated edge model that drains a mobile device's battery in minutes fails despite superior accuracy. Successful deployment requires evaluating multiple dimensions simultaneously: latency requirements, power budgets, network reliability, data privacy regulations, and total cost of ownership. Establish these constraints before model development to avoid expensive architectural pivots late in the project.

### Decision Framework {#sec-ml-system-architecture-decision-framework-241f}

\index{decision framework!paradigm selection} \index{deployment paradigms!selection criteria}Selecting the appropriate deployment paradigm requires systematic evaluation of application constraints rather than organizational biases or technology trends. Follow the decision tree in @fig-mlsys-playbook-flowchart, which filters options through a hierarchy of critical requirements: privacy, latency, computational demands, and cost constraints.

::: {#fig-mlsys-playbook-flowchart fig-env="figure" fig-pos="t" fig-cap="**Deployment Decision Logic**: This flowchart guides selection of an appropriate machine learning deployment paradigm by systematically evaluating privacy requirements and processing constraints, ultimately balancing performance, cost, and data security. Navigating the decision tree helps practitioners determine whether cloud, edge, mobile, or tiny machine learning best suits a given application." fig-alt="Decision flowchart with four layers: Privacy, Performance, Compute Needs, and Cost. Each layer filters toward deployment options: Cloud ML, Edge ML, Mobile ML, or TinyML based on constraints."}
```{.tikz}
\resizebox{.7\textwidth}{!}{%
\begin{tikzpicture}[font=\small\usefont{T1}{phv}{m}{n},line width=0.75pt]
\tikzset{
  Line/.style={line width=1.0pt,black!50,text=black},
  Box/.style={inner xsep=2pt,
    draw=GreenLine, line width=0.65pt,
    fill=GreenL,
    text width=25mm,align=flush center,
    minimum width=25mm, minimum height=9mm
  },
  Box1/.style={inner xsep=2pt,
    node distance=0.5,
    draw=BlueLine, line width=0.65pt,
    fill=BlueL,
    text width=33mm,align=flush center,
    minimum width=33mm, minimum height=9mm
  },
  Text/.style={inner xsep=2pt,
    draw=none, line width=0.75pt,
    fill=TextColor,
    font=\footnotesize\usefont{T1}{phv}{m}{n},
    align=flush center,
    minimum width=7mm, minimum height=5mm
  },
}
%
\begin{scope}
\node[Box, rounded corners=12pt,fill=magenta!20](B1){Start};
\node[Box1,below=of B1](B2){Is privacy critical?};
\node[Box,below left=0.1 and 1 of B2](B3){Cloud Processing Allowed};
\node[Box,below right=0.1 and 1 of B2](B4){Local Processing Preferred};
\draw[Line,-latex](B1)--(B2);
\draw[Line,-latex](B2)-|node[Text,pos=0.2]{No}(B3);
\draw[Line,-latex](B2)-|node[Text,pos=0.2]{Yes}(B4);
\scoped[on background layer]
\node[draw=BackLine,inner xsep=12mm,inner ysep=3mm,yshift=-1mm,
       fill=BackColor,fit=(B1)(B3)(B4),line width=0.75pt](BB){};
\node[below=11pt of BB.north east,anchor=east]{Layer: Privacy};
\end{scope}
%
\begin{scope}[shift={(0,-4.6)}]
\node[Box1](2B1){Is low latency required ($<$10 ms)?};
\node[Box,below left=0.1 and 1 of 2B1](2B2){Latency Tolerant};
\node[Box,below right=0.1 and 1 of 2B1](2B3){Tiny or Edge ML};
\draw[Line,-latex](2B1)-|node[Text,pos=0.2]{No}(2B2);
\draw[Line,-latex](2B1)-|node[Text,pos=0.2]{Yes}(2B3);
\scoped[on background layer]
\node[draw=BackLine,inner xsep=12mm,inner ysep=4mm,yshift=0mm,
       fill=BackColor,fit=(2B1)(2B2)(2B3),line width=0.75pt](BB1){};
\node[below=11pt of BB1.north east,anchor=east]{Layer: Performance};
\end{scope}
\draw[Line,-latex](B3)--++(270:1.1)-|(2B1.110);
\draw[Line,-latex](B4)--++(270:1.1)-|(2B1.70);
%
\begin{scope}[shift={(0,-8.0)}]
\node[Box1](3B1){Does the model require significant compute?};
\node[Box,below left=0.1 and 1 of 3B1](3B2){Heavy Compute};
\node[Box,below right=0.1 and 1 of 3B1](3B3){Lightweight Processing};
\draw[Line,-latex](3B1)-|node[Text,pos=0.2]{Yes}(3B2);
\draw[Line,-latex](3B1)-|node[Text,pos=0.2]{No}(3B3);
\scoped[on background layer]
\node[draw=BackLine,inner xsep=12mm,inner ysep=5mm,yshift=1mm,
       fill=BackColor,fit=(3B1)(3B2)(3B3),line width=0.75pt](BB2){};
\node[below=11pt of BB2.north east,anchor=east]{Layer: Compute Needs};
\end{scope}
\draw[Line,-latex](2B2)--++(270:1.1)-|(3B1.110);
\draw[Line,-latex](2B3)--++(270:1.1)-|(3B1.70);
%4
\begin{scope}[shift={(0,-11.4)}]
\node[Box1](4B1){Are there strict cost constraints?};
\node[Box,below left=0.1 and 1 of 4B1](4B2){Flexible Budget};
\node[Box,below right=0.1 and 1 of 4B1](4B3){Low-Cost Options};
\draw[Line,-latex](4B1)-|node[Text,pos=0.2]{No}(4B2);
\draw[Line,-latex](4B1)-|node[Text,pos=0.2]{Yes}(4B3);
\scoped[on background layer]
\node[draw=BackLine,inner xsep=12mm,inner ysep=4mm,yshift=2mm,
       fill=BackColor,fit=(4B1)(4B2)(4B3),line width=0.75pt](BB3){};
\node[below=11pt of  BB3.north east,anchor=east]{Layer: Cost};
\end{scope}
\draw[Line,-latex](3B2)--++(270:1.1)-|(4B1.110);
\draw[Line,-latex](3B3)--++(270:1.1)-|(4B1.70);
%5
\begin{scope}[shift={(-0.45,-14.0)},anchor=north east]
\node[Box,fill=magenta!20,rounded corners=12pt,text width=18mm,
       minimum width=17mm](5B1){Cloud ML};
\node[Box,node distance=1.0,fill=magenta!20,rounded corners=12pt,left=of 5B1,text width=18mm,
       minimum width=17mm](5B2){Edge ML};
\node[Box,node distance=1.0,fill=magenta!20, rounded corners=12pt,right=of 5B1,text width=18mm,
       minimum width=17mm](5B3){Mobile ML};
\node[Box,node distance=1.0,fill=magenta!20, rounded corners=12pt,right=of 5B3,text width=18mm,
       minimum width=17mm](5B4){TinyML};
%
\scoped[on background layer]
\node[draw=BackLine,inner xsep=12mm,inner ysep=5mm,yshift=-1mm,
       fill=BackColor,fit=(5B1)(5B2)(5B4),line width=0.75pt](BB4){};
\node[above=8pt of BB4.south east,anchor=east]{Layer: Deployment Options};
\end{scope}
\draw[Line,-latex](4B3)-|(5B3);
\draw[Line,-latex](4B3)--++(270:0.92)-|(5B4);
\draw[Line,-latex](4B2)--++(270:0.92)-|(5B1);
\draw[Line,-latex](3B2.west)--++(180:0.5)|-(5B2);
\end{tikzpicture}}
```
:::

The framework evaluates four critical decision layers sequentially. Privacy constraints form the first filter, determining whether data can be transmitted externally. Applications handling sensitive data under GDPR, HIPAA, or proprietary restrictions mandate local processing, immediately eliminating cloud-only deployments. Latency requirements establish the second constraint through response time budgets: applications requiring sub-10 ms response times cannot use cloud processing, as physics-imposed network delays alone exceed this threshold. Computational demands form the third evaluation layer, assessing whether applications require high-performance infrastructure that only cloud or edge systems provide, or whether they can operate within the resource constraints of mobile or tiny devices. Cost considerations complete the framework by balancing capital expenditure, operational expenses, and energy efficiency across expected deployment lifetimes.

The following worked example applies this framework step by step to a safety-critical application: *autonomous vehicle emergency braking*.

::: {.callout-notebook title="Autonomous Vehicle Emergency Braking"}
**Application**: Vision-based pedestrian detection for emergency braking.

**Walking through the decision framework**:

1. **Privacy**: Vehicle camera data is not transmitted to third parties → No strong privacy constraint. *Could use cloud.*
2. **Latency**: Emergency braking requires <100 ms total response. At 60 mph, a car travels 2.7 meters in 100 ms.
   - Network latency to cloud: 50-150 ms (variable) → **Fails requirement**
   - Edge processing: 10-30 ms → **Passes**
   - *Decision: Cloud eliminated by physics.*
3. **Compute**: Pedestrian detection requires ~10 GFLOPs at 30 FPS = 300 GFLOPs/s sustained.
   - TinyML (<1 GFLOP/s): **Fails**
   - Mobile NPU (~35 TOPS): Possible but thermal constraints limit sustained operation
   - Edge GPU (~10+ TFLOPS): **Passes with margin**
   - *Decision: Edge or high-end Mobile.*
4. **Cost**: Safety-critical, high-volume production (millions of vehicles).
   - Edge GPU: $500-1000 per vehicle, amortized over 10+ year vehicle life = $50-100/year
   - *Decision: Edge GPU justified for safety-critical application.*

**Result**: Edge ML with local GPU (NVIDIA Drive Orin class). Cloud used only for training, model updates, and fleet-wide analytics—not real-time inference.

**Key insight**: Latency constraints eliminated 75% of options before we considered compute or cost.
:::

The decision framework above identifies technically feasible options, but feasibility does not guarantee success. Production deployment also depends on organizational capabilities that determine whether a technically sound choice can be implemented and maintained effectively.

Successful deployment requires considering factors beyond pure engineering constraints. Organizational factors shape success by determining whether teams possess the capabilities to implement and maintain chosen paradigms. Team expertise must align with paradigm requirements: Cloud ML demands distributed systems knowledge, Edge ML requires device management capabilities, Mobile ML needs platform-specific optimization skills, and TinyML requires embedded systems expertise. Organizations lacking appropriate skills face extended development timelines and ongoing maintenance challenges that undermine technical advantages. Monitoring and maintenance capabilities similarly determine viability at scale: edge deployments require distributed device orchestration, while TinyML demands specialized firmware management that many organizations lack. Cost structures further complicate decisions through their temporal patterns: Cloud incurs recurring operational expenses favorable for unpredictable workloads, Edge requires substantial upfront investment offset by lower ongoing costs, Mobile leverages user-provided devices to minimize infrastructure expenses, and TinyML minimizes hardware and connectivity costs while demanding significant development investment.

These organizational realities raise a broader question: given the infrastructure, expertise, and maintenance burden that ML systems require, is a machine learning approach always the right choice? Every ML deployment carries a *complexity tax* that must be weighed against simpler alternatives.

::: {.callout-perspective title="The Complexity Tax"}

\index{complexity tax!ML vs heuristics} \index{deployment paradigms!complexity considerations}Before committing to any ML deployment, weigh the **Complexity Tax** against simpler alternatives.

Consider a classification problem solvable by either a **Heuristic** (if-then rules) or a **Deep Learning Pipeline**:

1.  **The Heuristic**: 50 lines of code. Near-zero compute cost. Maintenance: ~1 hour/month to update rules. No drift.
2.  **The ML System**: 50 lines of model code + 2,000 lines of infrastructure (data pipelines, monitoring, GPU drivers). Maintenance: ~40 hours/month debugging drift and managing infrastructure.

If the ML system provides 95% accuracy and the heuristic provides 90%, is that 5% gain worth a **40× increase** in complexity? ML systems engineering is the art of minimizing this tax through robust architecture. If you cannot afford the operational cost to maintain model quality over time, the simpler heuristic may be the superior systems choice.
:::

This complexity tax applies to every deployment decision. Before proceeding to hybrid architectures, reflect on how you would make this trade-off in your own systems.

::: {.callout-checkpoint title="System Design" collapse="false"}
The central trade-off is often **Accuracy vs. Complexity**.

**Decision Gates**

- [ ] **The Baseline**: Have you measured the accuracy of a simple heuristic (regex, logistic regression) before training a Deep Network?
- [ ] **The Infrastructure Cost**: Is the 2% accuracy gain from a Transformer worth the 10× inference cost and maintenance burden compared to a smaller model?
:::

Successful deployment emerges from balancing technical optimization against organizational capability. Paradigm selection represents systems engineering challenges that extend well beyond pure technical requirements, encompassing team skills, operational capacity, and economic constraints. These decisions remain constrained by physical scaling laws, with operational aspects detailed in @sec-machine-learning-operations-mlops and benchmarking approaches covered in @sec-benchmarking-ai.

## Hybrid Architectures {#sec-ml-system-architecture-hybrid-architectures-combining-paradigms-7cdd}

\index{hybrid architectures!combining paradigms} \index{Hybrid ML!integration strategies}The decision framework (@fig-mlsys-playbook-flowchart) helps select the best single paradigm for a given application. In practice, however, production systems rarely use just one paradigm. Voice assistants combine TinyML wake-word detection with mobile speech recognition and cloud natural language understanding. Autonomous vehicles pair edge inference for real-time perception with cloud training for model updates. These hybrid architectures leverage the strengths of multiple paradigms while mitigating their individual weaknesses. This section formalizes the integration strategies that make such combinations effective.

::: {.callout-definition title="Hybrid ML"}

***Hybrid Machine Learning***\index{Hybrid ML!hierarchical distribution} is the architectural strategy of **Hierarchical Distribution**. It partitions the ML workload across the **Latency-Compute Pareto Frontier**, placing latency-critical tasks on Edge/Tiny devices and throughput-intensive tasks in the Cloud, linked by a unified data fabric.
:::

### Integration Patterns {#sec-ml-system-architecture-integration-patterns-5935}

\index{Hybrid ML!train-serve split} \index{Hybrid ML!hierarchical processing} \index{Hybrid ML!progressive deployment}Three essential patterns address common integration challenges:

**Train-Serve Split**\index{train-serve split!economics}: Training occurs in the cloud while inference happens on edge, mobile, or tiny devices. This pattern leverages cloud scale for training while benefiting from local inference latency and privacy. Training costs may reach millions of dollars for large models, while inference costs mere cents per query when deployed efficiently.[^fn-train-serve-split]

**Hierarchical Processing**\index{hierarchical processing!data flow}: Data and intelligence flow between computational tiers. TinyML sensors perform basic anomaly detection, edge devices aggregate and analyze data from multiple sensors, and cloud systems handle complex analytics and model updates. Each tier handles tasks appropriate to its capabilities.

**Progressive Deployment**\index{progressive deployment!model compression}: Models are systematically compressed for deployment across tiers. A large cloud model becomes progressively optimized versions for edge servers, mobile devices, and tiny sensors. Amazon Alexa exemplifies this: wake-word detection uses <1 KB models consuming <1 mW, while complex natural language understanding requires GB+ models in cloud infrastructure.

[^fn-train-serve-split]: **Train-Serve Split Economics**: Training large models can cost $1-10M (GPT-3: estimated ~$4.6M at 2020 V100 cloud rates) but inference costs <$0.01 per query when deployed efficiently [@brown2020language]. This 1,000,000× cost difference drives the pattern of expensive cloud training with cost-effective edge inference.

With three integration patterns available, selecting the right one for a given application requires matching the pattern's trade-off profile to the system's dominant constraints. The following *pattern selection guide* summarizes when each pattern applies.

::: {.callout-perspective title="Pattern Selection Guide"}

**Train-Serve Split** — *Trade-off: Training cost vs. inference latency*

- *Choose when*: Training requires scale that inference does not; privacy matters for inference but not training
- *Avoid when*: Model needs continuous learning from deployed data

**Hierarchical Processing** — *Trade-off: Local autonomy vs. global optimization*

- *Choose when*: Data volume exceeds transmission capacity; decisions needed at multiple timescales
- *Avoid when*: All processing can occur at one tier; network is reliable and fast

**Progressive Deployment** — *Trade-off: Model quality vs. deployment reach*

- *Choose when*: Same model needed at multiple capability levels; graceful degradation required
- *Avoid when*: Model cannot be meaningfully compressed; single deployment target

**Common combinations**: Voice assistants use Train-Serve Split + Progressive Deployment + Hierarchical Processing. Autonomous vehicles combine Hierarchical Processing with Progressive Deployment to run optimized models at each tier.

Additional patterns including federated and collaborative learning enable privacy-preserving distributed training across devices.
:::

### Production System Integration {#sec-ml-system-architecture-production-system-integration-3bb3}

\index{Hybrid ML!production systems} \index{data pipelines!hybrid architectures}Real-world implementations integrate multiple design patterns into cohesive solutions. @fig-hybrid makes these interactions concrete through specific connection types. Notice the bidirectional flow: "Deploy" paths show how models flow *downward* from cloud training to various devices, while "Data" and "Results" flow *upward* from sensors through processing stages to cloud analytics. "Sync" connections demonstrate device coordination across tiers. This bidirectional architecture—models flowing down, data flowing up—is the defining characteristic of production hybrid systems.

::: {#fig-hybrid fig-env="figure" fig-pos="t" fig-cap="**Hybrid System Interactions**: Data flows upward from sensors through processing layers to cloud analytics, while trained models deploy downward to edge, mobile, and TinyML inference points. Five connection types (deploy, data, results, assist, and sync) establish a distributed architecture where each paradigm contributes unique capabilities." fig-alt="System diagram with four ML paradigms: TinyML sensors, Edge inference, Mobile processing, and Cloud training. Arrows show deploy, data, results, sync, and assist flows between tiers."}
```{.tikz}
\begin{tikzpicture}[font=\small\usefont{T1}{phv}{m}{n}]
\tikzset{
Line/.style={line width=1.0pt,black!50,text=black},
  Box/.style={inner xsep=2pt,
    node distance=0.6,
    draw=GreenLine, line width=0.75pt,
    fill=GreenL,
    text width=20mm,align=flush center,
    minimum width=20mm, minimum height=9mm
  },
   Text/.style={inner xsep=2pt,
    draw=none, line width=0.75pt,
    fill=TextColor,
    font=\footnotesize\usefont{T1}{phv}{m}{n},
    align=flush center,
    minimum width=7mm, minimum height=5mm
  },
  }

\node[Box,fill=RedL,draw=RedLine](G2){Training};
\node[Box,fill=none,draw=none,below =1.2 of G2](A){};
\node[Box,node distance=2.25, left=of A](B2){Inference};
\node[Box,node distance=2.25,left=of B2,fill=BlueFill,draw=BlueLine](B1){Inference};
\node[Box,node distance=2.25, right=of A,fill=OrangeFill,draw=OrangeLine](B3){Inference};
%
\node[Box,node distance=1.15, below=of B1,fill=BlueFill,draw=BlueLine](1DB1){Processing};
\node[Box,node distance=1.15, below=of B3,fill=OrangeFill,draw=OrangeLine](1DB3){Processing};
\path[](1DB3)-|coordinate(S)(G2);
\node[Box,node distance=1.5,fill=RedL,draw=RedLine]at(S)(1DB2){Analytics};
\path[](G2)-|coordinate(SS)(B2);
\node[Box](G1)at(SS){Sensors};
%
\scoped[on background layer]
\node[draw=BackLine,inner xsep=4mm,inner ysep=6mm,anchor= west,
       yshift=1mm,fill=BackColor,fit=(G1)(B2),line width=0.75pt](BB2){};
\node[below=3pt of  BB2.north,anchor=north]{TinyML};
%
\scoped[on background layer]
\node[draw=BackLine,inner xsep=4mm,inner ysep=7mm,anchor= west,
       yshift=0mm,fill=BackColor,fit=(G2)(1DB2),line width=0.75pt](BB2){};
\node[below=3pt of  BB2.north,anchor=north]{Cloud ML};
%
\draw[Line,-latex](G1.west)--++(180:0.9)|-node[Text,pos=0.1]{Data}(B2);
\draw[Line,-latex](G2)--++(270:1.20)-|(B2);
\draw[Line,-latex](G2)--++(270:1.20)-|(B3);
\draw[Line,-latex](G2)--node[Text,pos=0.46]{Deploy}++(270:1.20)-|(B1);
%
\draw[Line,-latex](B1)--node[Text,pos=0.5]{Results}(1DB1);
\draw[Line,-latex](B2)|-node[Text,pos=0.75]{Results}(1DB1.10);
%
\draw[Line,-latex](B1.330)--++(270:0.9)-|node[Text,pos=0.2]{Assist}(B3.220);
\draw[Line,-latex](B2.east)--node[Text,pos=0.5]{Sync}++(0:5.4)|-(1DB3.170);
%
\draw[Line,-latex](1DB1.350)--node[Text,pos=0.75]{Results}(1DB2.190);
\draw[Line,-latex](1DB3.190)--node[Text,pos=0.50]{Data}(1DB2.350);
\draw[Line,-latex](B3.290)--node[Text,pos=0.5]{Results}(1DB3.70);
%
\scoped[on background layer]
\node[draw=BackLine,inner xsep=4mm,inner ysep=5mm,anchor= west,
      yshift=-2mm,fill=BackColor,fit=(B1)(1DB1),line width=0.75pt](BB2){};
\node[above=3pt of  BB2.south,anchor=south]{Edge ML};
%
\scoped[on background layer]
\node[draw=BackLine,inner xsep=4mm,inner ysep=5mm,anchor= west,
      yshift=-2mm,fill=BackColor,fit=(B3)(1DB3),line width=0.75pt](BB2){};
\node[above=3pt of  BB2.south,anchor=south]{Mobile ML};
\end{tikzpicture}
```
:::

Production systems demonstrate these integration patterns across diverse applications:

- **Industrial defect detection** exemplifies Train-Serve Split: cloud infrastructure trains vision models on datasets from multiple facilities, then distributes optimized versions to edge servers managing factory floors, tablets for quality inspectors, and embedded cameras on production lines.
- **Agricultural monitoring** illustrates Hierarchical Processing: soil sensors perform local anomaly detection at the TinyML tier, edge processors aggregate data from dozens of sensors and identify field-level patterns, while cloud infrastructure handles farm-wide analytics and seasonal planning.
- **Fitness tracking** exemplifies Progressive Deployment with gateway patterns: wearables continuously monitor activity using microcontroller-optimized algorithms consuming <1 mW, sync processed summaries to smartphones that combine metrics from multiple sources, then transmit periodic updates to cloud infrastructure for longitudinal health analysis.

### Why Hybrid Approaches Work {#sec-ml-system-architecture-hybrid-approaches-work-4bb8}

\index{Hybrid ML!convergence principles} \index{ML systems!shared foundations}The success of hybrid architectures stems from a deeper truth: despite their diversity, all ML deployment paradigms share core principles. Look at @fig-ml-systems-convergence to see this convergence in action: implementations spanning cloud to tiny devices meet at the same core system challenges—managing data pipelines, balancing resource constraints, and implementing reliable architectures.

::: {#fig-ml-systems-convergence fig-env="figure" fig-pos="t" fig-cap="**Convergence of ML Systems**: Three-layer structure showing how diverse deployments converge. The top layer lists four paradigms (Cloud, Edge, Mobile, TinyML); the middle layer identifies shared foundations (data pipelines, resource management, architecture principles); and the bottom layer presents cross-cutting concerns (optimization, operations, trustworthy AI) that apply across all paradigms." fig-alt="Three-layer diagram. Top: Cloud, Edge, Mobile, TinyML implementations. Middle: data pipeline, resource management, architecture principles. Bottom: optimization, operations, trustworthy AI. Arrows connect layers."}
```{.tikz}
\begin{tikzpicture}[font=\small\usefont{T1}{phv}{m}{n}]
\tikzset{
Line/.style={line width=1.0pt,black!50,text=black},
  Box/.style={inner xsep=2pt,
    node distance=0.6,
    draw=GreenLine, line width=0.75pt,
    fill=GreenL,
    text width=30mm,align=flush center,
    minimum width=30mm, minimum height=13mm
  },
  Box1/.style={inner xsep=2pt,
    node distance=0.8,
    draw=BlueLine, line width=0.75pt,
    fill=BlueL,
    text width=36mm,align=flush center,
    minimum width=40mm, minimum height=13mm
  },
}

\begin{scope}[anchor=west]
\node[Box](B1){Cloud ML Data Centers Training at Scale};
\node[Box,right=of B1](B2){Edge ML Local Processing Inference Focus};
\node[Box,right=of B2](B3){Mobile ML Personal DevicesUser Applications};
\node[Box, right=of B3](B4){TinyML Embedded Systems Resource Constrained};
%
\scoped[on background layer]
\node[draw=BackLine,inner xsep=5mm,inner ysep=5mm,minimum width=170mm,
      anchor=west,yshift=2mm,fill=BackColor,
      fit=(B1)(B2)(B3)(B4),line width=0.75pt](BB){};
\node[below=11pt of  BB.north east,anchor=east]{ML System Implementations};
\end{scope}
%
\begin{scope}[shift={(0.4,-2.8)}, anchor=west]
\node[Box1](2B1){Data Pipeline Collection -- Processing -- Deployment};
\node[Box1,right=of 2B1](2B2){Resource Management Compute -- Memory -- Energy -- Network};
\node[Box1,right=of 2B2](2B3){System Architecture Models -- Hardware -- Software};
%
\scoped[on background layer]
\node[draw=BackLine,inner xsep=5mm,inner ysep=5mm,minimum width=170mm,
      anchor= west,yshift=-1mm,fill=BackColor,fit=(2B1)(2B2)(2B3),line width=0.75pt](BB2){};
\node[above=8pt of  BB2.south east,anchor=east]{Core System Principles};
\end{scope}
%
\begin{scope}[shift={(0.4,-6.0)}, anchor=west]
\node[Box1, fill=VioletL,draw=VioletLine](3B1){Optimization \& Efficiency Model -- Hardware -- Energy};
\node[Box1,right=of 3B1, fill=VioletL,draw=VioletLine](3B2){Operational Aspects Deployment -- Monitoring -- Updates};
\node[Box1,right=of 3B2, fill=VioletL,draw=VioletLine](3B3){Trustworthy AI Security -- Privacy -- Reliability};
%
\scoped[on background layer]
\node[draw=BackLine,inner xsep=5mm,inner ysep=5mm,minimum width=170mm,
       anchor= west,yshift=-1mm,fill=BackColor,fit=(3B1)(3B2)(3B3),line width=0.75pt](BB3){};
\node[above=8pt of  BB3.south east,anchor=east]{System Considerations};
\end{scope}
%
\draw[-latex,Line](B1.south)--++(270:0.75)-|(2B1);
\draw[-latex,Line](B2.south)--++(270:0.75)-|(2B1);
\draw[-latex,Line](B3.south)--++(270:0.75)-|(2B1);
\draw[-latex,Line](B4.south)--++(270:0.75)-|(2B1);
\draw[-latex,Line](B2.south)--++(270:0.75)-|(2B2);
\draw[-latex,Line](B3.south)--++(270:0.75)-|(2B3);
%
\draw[-latex,Line](2B1.south)--++(270:0.95)-|(3B1);
\draw[-latex,Line](2B2.south)--++(270:0.95)-|(3B1);
\draw[-latex,Line](2B3.south)--++(270:0.95)-|(3B1);
\draw[-latex,Line](2B2.south)--++(270:0.95)-|(3B2);
\draw[-latex,Line](2B3.south)--++(270:0.95)-|(3B3);
\end{tikzpicture}
```
:::

This convergence explains why techniques transfer effectively between scales. Three examples illustrate:

- **Cloud-trained models deploy to edge** because both training and inference minimize the same loss function—only the compute budget differs. Quantization techniques developed for edge deployment reduce cloud serving costs; distributed training strategies inform edge model parallelism.
- **Mobile optimization insights inform cloud efficiency** because memory bandwidth constraints appear at every scale. Techniques like operator fusion and activation checkpointing, developed for mobile's tight memory budgets, reduce cloud inference costs by 2-3× when applied to batch serving.
- **TinyML innovations drive cross-paradigm advances** because extreme constraints force genuinely novel algorithmic breakthroughs. Binary neural networks, developed for microcontrollers, now accelerate cloud recommendation systems. Sparse attention mechanisms, essential for fitting transformers in kilobytes, reduce cloud training costs.

The remaining chapters explore each layer: @sec-data-engineering-ml examines data pipelines, @sec-model-compression addresses optimization, and @sec-machine-learning-operations-mlops covers operational aspects. All of these apply whether you deploy to a TPU Pod or an ESP32. But shared principles also mean shared vulnerabilities: the same operational challenges—data drift, model decay, monitoring—appear at every tier and demand attention before we consider the chapter's remaining lessons.

:::: {.callout-checkpoint title="Hybrid ML Patterns" collapse="false"}
Hybrid architectures work when you partition *work* across tiers—not when you copy the same pipeline everywhere.

**Integration Patterns**

- [ ] **Train-Serve Split**: Can you explain why training in the cloud and serving on edge/mobile is often economically optimal, even when the model runs locally?
- [ ] **Hierarchical Processing**: Can you describe what each tier does in a sensor → edge → cloud pipeline, and why pushing *some* decisions down reduces both latency and bandwidth?
- [ ] **Progressive Deployment**: Can you explain how one model family becomes multiple deployed artifacts (cloud, edge, mobile, tiny) through systematic compression?

**Design Sanity Checks**

- [ ] **Boundary choice**: Given a concrete application, can you justify *where* the tier boundary should fall (latency, privacy, bandwidth, power), not just *what* model to use?
- [ ] **Data fabric**: Can you name the minimal data flows that must go *up* (telemetry, labels, drift signals) to keep the deployed system from decaying?
::::

[]{#sec-ml-system-architecture-from-deployment-to-operations}The shared foundations in @fig-ml-systems-convergence also share a vulnerability. Deployment is not the end of the engineering challenge—it is the beginning of a new one. Traditional software, once deployed correctly, remains correct indefinitely: a sorting algorithm that works today will work tomorrow, next year, and a decade from now. ML systems face a fundamentally different reality: **System Entropy (statistical decay)**\index{system entropy!model decay}\index{model drift|see{system entropy}}.

Unlike a sorting algorithm that remains correct as long as the code is unchanged, an ML model's accuracy degrades as the world drifts away from its training distribution. The **Degradation Equation** from @sec-introduction captures this formally: system quality decays as the distance between the training distribution and the live data distribution grows, at a rate proportional to the model's sensitivity to distributional shift. Every deployed model is in a state of unobserved decay from the moment it ships. Reliability in ML systems is therefore not a property of the code but a property of the monitoring and retraining infrastructure built to detect and correct this drift. The operational aspects covered in @sec-machine-learning-operations-mlops address precisely this challenge.

Beyond statistical decay, engineers also fall prey to common misconceptions about ML deployment. The physical constraints we have examined throughout this chapter create counterintuitive behaviors that challenge intuitions from traditional software engineering. The following fallacies and pitfalls distill these hard-won lessons into actionable guidance.

## Fallacies and Pitfalls {#sec-ml-system-architecture-fallacies-pitfalls-3dfe}

\index{ML systems!common fallacies} \index{deployment paradigms!pitfalls}The following fallacies and pitfalls capture architectural mistakes that waste development resources, miss performance targets, or deploy systems critically mismatched to their operating constraints. Each represents a pattern we have seen repeatedly in production ML systems.

**Fallacy:** *One deployment paradigm solves all ML problems.*

Physical constraints create hard boundaries that no single paradigm can span. As @sec-ml-system-architecture-framework-to-practice establishes, memory bandwidth scales as the square root of chip area (constrained by die perimeter and pin count) while compute scales linearly with die area, producing qualitatively different bottlenecks across paradigms. @tbl-big_vs_tiny quantifies this: cloud ML achieves 100--1000 ms latency while TinyML delivers 1--10 ms, a 100× difference rooted in speed-of-light limits, not implementation quality. A real-time robotics system requiring sub-10 ms response cannot use cloud inference regardless of optimization, and a billion-parameter language model cannot fit on a microcontroller with 256 KB RAM regardless of quantization. The optimal architecture typically combines paradigms, such as cloud training with edge inference or mobile preprocessing with cloud analysis.

A related misconception holds that moving computation closer to the user always reduces latency, ignoring the processing overhead introduced by less powerful edge hardware—a trade-off explored in **Inference Benchmarks** (@sec-benchmarking-ai-inference-benchmarks-2c1f).

```{python}
#| label: mobile-power-fallacy-calc
#| echo: false
# ┌─────────────────────────────────────────────────────────────────────────────
# │ MOBILE POWER FALLACY: BATTERY DEPLETION CALCULATIONS
# ├─────────────────────────────────────────────────────────────────────────────
# │ Context: Fallacy "Model optimization overcomes mobile device power limits"
# │
# │ Why: Demonstrates that battery physics sets hard limits. Shows that a 5W
# │      workload depletes a 15Wh battery in 3 hours regardless of optimization.
# │
# │ Imports: physx.formatting (fmt, md_frac)
# │ Exports: low_power_hours_str, high_power_hours_str,
# │          low_power_frac, high_power_frac
# └─────────────────────────────────────────────────────────────────────────────
from physx.formatting import fmt, md_frac

battery_wh_value = 15                                                           # Wh, typical smartphone
low_power_w_value = 1                                                           # W, light inference
high_power_w_value = 5                                                          # W, heavy on-device model

low_power_hours_value = battery_wh_value / low_power_w_value                    # 15 / 1 = 15 hours
high_power_hours_value = battery_wh_value / high_power_w_value                  # 15 / 5 = 3 hours

low_power_hours_str = fmt(low_power_hours_value, precision=0, commas=False)     # "15"
high_power_hours_str = fmt(high_power_hours_value, precision=0, commas=False)   # "3"

# --- Inline fractions showing the physics ---
low_power_frac = md_frac(f"{battery_wh_value} Wh", f"{low_power_w_value} W", f"**{low_power_hours_str} hours**")
high_power_frac = md_frac(f"{battery_wh_value} Wh", f"{high_power_w_value} W", f"**{high_power_hours_str} hours**")
```

**Fallacy:** *Model optimization overcomes mobile device power and thermal limits.*

Compression techniques do not scale indefinitely against physics. Consider a smartphone with a `{python} phone_battery_str` Wh battery:

- **Light workload** (1 W inference): `{python} low_power_frac`
- **Heavy workload** (5 W, common for large on-device models): `{python} high_power_frac`

The 5 W workload also triggers thermal throttling that reduces performance by 40–60 percent. As @sec-ml-system-architecture-mobile-ml-benefits-resource-constraints-c568 establishes, sustained mobile inference cannot exceed 2–3 W without active cooling. Reducing numerical precision (using fewer bits to represent each weight; see @sec-model-compression) cuts power by approximately 4×, but aggressive precision reduction often causes 5–10 percent accuracy loss. Applications requiring continuous inference beyond mobile thermal envelopes remain physically impossible regardless of algorithmic improvements.

**Fallacy:** *TinyML represents scaled-down mobile ML.*

The difference is qualitative, not just quantitative. As @sec-ml-system-architecture-tinyml-advantages-operational-tradeoffs-2d40 establishes, TinyML microcontrollers provide 256 KB to 1 MB of memory versus mobile devices with 4–12 GB, a 10,000× difference requiring entirely different algorithms. Mobile ML uses reduced-precision arithmetic with minimal accuracy loss; TinyML requires extreme precision reduction that sacrifices 10–15 percent accuracy for 32× memory reduction. Mobile devices run models with millions of parameters; TinyML models contain 10,000–100,000 parameters, demanding distinct architectural choices such as specialized lightweight operations designed to minimize multiply-accumulate counts. Power budgets show similar discontinuities: mobile inference consumes 1–5 W, while TinyML targets 1–10 mW for battery-free energy harvesting. These thousand-fold gaps make TinyML a distinct problem class, not a smaller version of mobile ML. Teams that apply mobile optimization techniques directly to TinyML projects discover that quantization from FP32 to INT8 (reducing each weight from 32 bits to 8 bits; see @sec-model-compression) is insufficient when models must fit in 64 KB, forcing complete architectural redesign.

```{python}
#| label: tco-pitfall-calc
#| echo: false
# ┌─────────────────────────────────────────────────────────────────────────────
# │ TCO PITFALL: EDGE VS CLOUD TOTAL COST OF OWNERSHIP
# ├─────────────────────────────────────────────────────────────────────────────
# │ Context: Pitfall "Minimizing computational resources minimizes total cost"
# │
# │ Why: Shows that edge deployment with lower compute cost can have 3x higher
# │      total cost when operational overhead is included. Teaches TCO thinking.
# │
# │ Imports: physx.formatting (fmt)
# │ Exports: cloud_compute_str, edge_hw_str, edge_network_str, edge_maint_str,
# │          edge_reliability_str, edge_total_str, tco_ratio_str
# └─────────────────────────────────────────────────────────────────────────────
from physx.formatting import fmt

# Cloud costs (monthly)
cloud_compute_value = 2000                                                      # $, inference compute

# Edge costs (monthly)
edge_hardware_value = 500                                                       # $, amortized hardware
edge_network_value = 3000                                                       # $, network engineering
edge_maintenance_value = 500                                                    # $, hardware maintenance
edge_reliability_value = 2000                                                   # $, reliability engineering

edge_total_value = (edge_hardware_value + edge_network_value +
                    edge_maintenance_value + edge_reliability_value)            # $6,000

tco_ratio_value = edge_total_value / cloud_compute_value                        # 3x

cloud_compute_str = fmt(cloud_compute_value, precision=0, commas=True)          # "2,000"
edge_hw_str = fmt(edge_hardware_value, precision=0, commas=False)               # "500"
edge_network_str = fmt(edge_network_value, precision=0, commas=True)            # "3,000"
edge_maint_str = fmt(edge_maintenance_value, precision=0, commas=False)         # "500"
edge_reliability_str = fmt(edge_reliability_value, precision=0, commas=True)    # "2,000"
edge_total_str = fmt(edge_total_value, precision=0, commas=True)                # "6,000"
tco_ratio_str = fmt(tco_ratio_value, precision=0, commas=False)                 # "3"
```

**Pitfall:** *Minimizing computational resources minimizes total cost.*

Teams optimize per-unit resource consumption while ignoring operational overhead and development velocity. As the decision framework in @sec-ml-system-architecture-decision-framework-241f emphasizes, paradigm selection requires evaluating total cost of ownership, not just compute costs. A cloud inference service costing $`{python} cloud_compute_str` monthly in compute appears expensive versus $`{python} edge_hw_str` monthly edge hardware amortization, but edge deployments add network engineering ($`{python} edge_network_str` monthly), hardware maintenance ($`{python} edge_maint_str` monthly), and reliability engineering ($`{python} edge_reliability_str` monthly), totaling $`{python} edge_total_str`---a `{python} tco_ratio_str`x difference. Development velocity compounds the gap: cloud deployments reaching production in 2 months versus 6 months for custom edge infrastructure represent 4 months of delayed revenue. The optimal cost solution requires total cost of ownership analysis including development time, operational complexity, and opportunity costs, not merely minimizing compute expenses.

```{python}
#| label: amdahl-camera-calc
#| echo: false
# ┌─────────────────────────────────────────────────────────────────────────────
# │ AMDAHL'S LAW: CAMERA PIPELINE EXAMPLE
# ├─────────────────────────────────────────────────────────────────────────────
# │ Context: Fallacy "Model optimization translates linearly to system speedup"
# │
# │ Why: Uses a concrete smartphone camera pipeline to show that a 10x faster
# │      ML classifier yields only 1.37x end-to-end speedup when ML is 30% of
# │      the total pipeline. Makes Amdahl's Law tangible for students.
# │
# │ Imports: physx.formatting (fmt)
# │ Exports: cam_*_str variables for prose
# └─────────────────────────────────────────────────────────────────────────────
from physx.formatting import fmt

# --- Inputs (smartphone camera pipeline stages) ---
cam_isp_ms_value = 100                                                         # ms, ISP + auto-exposure
cam_ml_ms_value = 60                                                           # ms, ML scene classification
cam_post_ms_value = 40                                                         # ms, tone mapping + HDR merge
cam_ml_speedup_value = 10                                                      # 10x faster ML model

# --- Process (Amdahl's Law) ---
cam_total_ms_value = cam_isp_ms_value + cam_ml_ms_value + cam_post_ms_value    # 200 ms
cam_ml_frac_value = cam_ml_ms_value / cam_total_ms_value                       # 0.30
cam_non_ml_frac_value = 1 - cam_ml_frac_value                                 # 0.70

cam_speedup_10x_value = 1 / (cam_non_ml_frac_value + cam_ml_frac_value / cam_ml_speedup_value)
cam_speedup_inf_value = 1 / cam_non_ml_frac_value                             # theoretical max
cam_ml_optimized_ms_value = cam_ml_ms_value / cam_ml_speedup_value             # 6 ms
cam_total_optimized_ms_value = (cam_isp_ms_value +
                                cam_ml_optimized_ms_value +
                                cam_post_ms_value)                             # 146 ms

# --- Outputs (formatted strings for prose) ---
cam_isp_str = fmt(cam_isp_ms_value, precision=0, commas=False)                 # "100"
cam_ml_str = fmt(cam_ml_ms_value, precision=0, commas=False)                   # "60"
cam_post_str = fmt(cam_post_ms_value, precision=0, commas=False)               # "40"
cam_total_str = fmt(cam_total_ms_value, precision=0, commas=False)             # "200"
cam_ml_pct_str = fmt(cam_ml_frac_value * 100, precision=0, commas=False)       # "30"
cam_non_ml_pct_str = fmt(cam_non_ml_frac_value * 100, precision=0, commas=False)  # "70"
cam_speedup_10x_str = fmt(cam_speedup_10x_value, precision=2, commas=False)    # "1.37"
cam_speedup_inf_str = fmt(cam_speedup_inf_value, precision=2, commas=False)    # "1.43"
cam_ml_opt_str = fmt(cam_ml_optimized_ms_value, precision=0, commas=False)     # "6"
cam_total_opt_str = fmt(cam_total_optimized_ms_value, precision=0, commas=False)  # "146"
```

**Fallacy:** *Model optimization translates linearly to system speedup.*

**Amdahl's Law**\index{Amdahl's Law!speedup limits}\index{optimization!Amdahl's Law}[^fn-amdahls-law] establishes hard limits that the Bottleneck Principle (@sec-ml-systems-bottleneck-principle) formalizes: $Speedup_{overall} = \frac{1}{(1-p) + \frac{p}{s}}$ where $p$ is the fraction of work that can be improved and $s$ is the speedup of that fraction. Imagine you tap the shutter on a smartphone camera. The image passes through `{python} cam_isp_str` ms of signal processing (auto-exposure, white balance), `{python} cam_ml_str` ms of ML scene classification, and `{python} cam_post_str` ms of post-processing (tone mapping, HDR merge)---`{python} cam_total_str` ms total. You optimize the ML classifier to run 10$\times$ faster (`{python} cam_ml_opt_str` ms instead of `{python} cam_ml_str` ms), but total time drops from `{python} cam_total_str` ms to `{python} cam_total_opt_str` ms---only `{python} cam_speedup_10x_str`$\times$ overall, not 10$\times$. Even eliminating ML entirely ($s = \infty$) achieves only `{python} cam_speedup_inf_str`$\times$ speedup, because the remaining `{python} cam_non_ml_pct_str` percent of the pipeline is untouched. Effective optimization requires profiling the entire pipeline and addressing bottlenecks systematically, because system performance depends on the slowest unoptimized stage.

[^fn-amdahls-law]: **Amdahl's Law**: Formulated by Gene Amdahl in 1967 [@amdahl1967validity], this law quantifies theoretical speedup when only part of a system can be improved. The formula $S = 1/((1-p) + p/s)$ shows that even infinite speedup ($s \to \infty$) of the parallelizable fraction $p$ cannot exceed $1/(1-p)$. For ML systems, this explains why end-to-end optimization matters: a 10× faster GPU yields minimal gains if data loading or preprocessing dominates total latency.

**Pitfall:** *Assuming more training data always improves deployed model performance.*

\index{scaling laws!data limitations}Three constraints limit data scaling benefits, as the workload archetypes in @sec-ml-system-architecture-analyzing-workloads illustrate. First, model size limits what can be learned: a keyword spotting model with 250K parameters achieves 95% accuracy on 50K samples but only 96.5% on 1M samples, a 0.3% gain for 5× more data, storage, and labeling cost. The model simply cannot represent more complex patterns. Second, data quality dominates quantity: 1M curated samples often outperform 100M noisy web-scraped samples, because mislabeled examples and misleading patterns degrade performance even as dataset size grows. Third, deployment distribution matters more than training scale: a model trained on 1B web images may perform worse on medical imaging than one trained on 100K domain-specific samples. Teams that maximize dataset scale without analyzing model capacity waste months of labeling effort for negligible accuracy gains.

## Summary {#sec-ml-system-architecture-summary-d75c}

\index{ML systems!key takeaways}This chapter answered a deceptively simple question: *why does the same model demand fundamentally different engineering on a phone versus a datacenter?* The answer is physics. Three immutable constraints—the speed of light, the power wall, and the memory wall—carve the deployment landscape into four distinct paradigms spanning nine orders of magnitude in power and memory. No single paradigm suffices for production systems; hybrid architectures that partition work across Cloud, Edge, Mobile, and TinyML tiers define the state of the art.

::: {.callout-takeaways title="Key Takeaways"}

* **Physical constraints are permanent**\index{physical constraints!permanent boundaries}: Speed of light (5 ms cross-country), power wall, and memory wall create hard boundaries that engineering cannot overcome—only navigate.
* **Identify bottlenecks before optimizing**\index{bottleneck principle!optimization strategy}: The same model is compute-bound in training but memory-bound in inference. The Iron Law and Bottleneck Principle pinpoint which constraint dominates; optimizing the wrong term yields zero speedup.
* **Workload archetypes determine deployment feasibility**\index{Workload Archetypes!deployment mapping}: A Compute Beast (ResNet-50 training) requires cloud scale; a Tiny Constraint (keyword spotting) requires microcontroller efficiency. The same optimization strategy cannot serve both—match the archetype to the paradigm.
* **The deployment spectrum spans 1,000,000× in energy**: Cloud (1 kW) to TinyML (1 mW). This gap enables entirely different application classes rather than representing a limitation.
* **Hybrid architectures are prevalent in production systems**\index{hybrid architectures!voice assistant example}: Voice assistants span TinyML (wake-word), Mobile (speech-to-text), and Cloud (language understanding). Rarely does one paradigm suffice; integration patterns (Train-Serve Split, Hierarchical Processing, Progressive Deployment) formalize how paradigms combine.
* **Latency budgets reveal feasibility**\index{latency budgets!feasibility analysis}: 100 ms round-trip to cloud eliminates real-time applications; 10 ms edge inference enables them. Apply the decision framework (@fig-mlsys-playbook-flowchart) to filter paradigms by privacy, latency, compute, and cost.
* **System-level speedup obeys Amdahl's Law, not model-level gains**\index{Amdahl's Law!system optimization}: A 10× faster model yields only 1.37× system speedup when ML accounts for 30% of the pipeline. Profile the full system before optimizing any component.
* **Universal system principles transfer across paradigms**\index{ML systems!universal principles}: Data pipelines, resource management, and system architecture recur at every scale, which is why optimization ideas can migrate from cloud to edge and back again.

:::

The analytical tools developed here—the Iron Law, Bottleneck Principle, Workload Archetypes, and Lighthouse Models—recur throughout the remainder of this book. Every subsequent chapter, from data engineering to model compression to serving, operates within the deployment constraints established here. The decision framework (@fig-mlsys-playbook-flowchart) and the quantitative comparison (@tbl-big_vs_tiny) provide the reference points for those discussions. But knowing *where* to deploy is only the beginning. Every deployed model faces **System Entropy**—accuracy degradation as the world drifts from its training distribution—making the operational infrastructure for monitoring and retraining as important as the deployment decision itself.

::: {.callout-chapter-connection title="From Theory to Process"}

Understanding *where* ML systems run provides the foundation for understanding *how* to build them. The next chapter, @sec-ai-development-workflow, establishes the systematic development process that guides ML systems from conception through deployment, translating the physical constraints examined here into reliable, production-ready systems.

:::

::: {.quiz-end}
:::
