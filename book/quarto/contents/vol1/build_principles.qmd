# Principles of Model Building {.unnumbered}

These principles govern the architecture of neural networks. They define why deep learning works and how to structure models that are both expressive and trainable.

::: {.callout-note icon=false title="The Depth-Efficiency Law"}
**The Law**: In hierarchical feature extractors, computational efficiency is maximized when early layers process high-resolution, low-semantic data (edges, textures) and later layers process low-resolution, high-semantic data (objects, concepts).

**The Engineering Implication**:
Architectures should be tapered (e.g., Pyramidal CNNs) rather than uniform. Allocating equal compute to every layer is inefficient; capacity should be concentrated where semantic density is highest.
:::

::: {.callout-note icon=false title="The Residual Gradient Invariant"}
**The Law**: Deep network trainability is preserved only if there exists a direct path for gradient flow that bypasses non-linear transformations.
$$ \frac{\partial L}{\partial x} \approx 1 $$

**The Engineering Implication**:
As networks grow deeper, **Skip Connections** (ResNets, Transformers) become mandatory, not optional. Without them, the "Shattered Gradients" problem prevents error signals from propagating to early layers, halting learning.
:::
