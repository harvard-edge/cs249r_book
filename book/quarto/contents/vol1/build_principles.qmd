# Principles of Model Building {#sec-build-principles .unnumbered}

Building a machine learning system is not merely about stacking layers; it is about managing the flow of information and energy through silicon. These principles define the "Physics of Build" that govern why certain architectures succeed while others fail at scale.

::: {.callout-perspective title="The Iron Law of ML Systems"}
**The Law**: The latency ($L$) of any machine learning operation is determined by the sum of three components, divided by the system's efficiency ($\eta$):

$$ L = \frac{\text{Data Movement} + \text{Compute} + \text{System Overhead}}{\eta} $$

**The Engineering Implication**:
Optimization is a zero-sum game. If you reduce Compute (e.g., via pruning), you often increase Data Movement (random memory access). A "faster" algorithm on paper is only faster in reality if it reduces the **dominant** term in this equation for your specific hardware.
:::

::: {.callout-note icon=false title="The Silicon Contract"}
**The Concept**: Every model architecture makes an implicit agreement with the hardware.
- **ResNet-50** assumes high-density floating-point compute (Compute-Bound).
- **Llama-3-8B** assumes high-bandwidth memory access (Bandwidth-Bound).
- **DLRM** assumes massive memory capacity for lookup tables (Capacity-Bound).

**The Engineering Implication**:
Designing a model without knowing which hardware resource it will saturate is like designing a bridge without knowing the strength of the steel. You must design for the **Bottleneck**.
:::

::: {.callout-note icon=false title="The Depth-Efficiency Law"}
**The Law**: In hierarchical feature extractors, computational efficiency is maximized when early layers process high-resolution, low-semantic data (edges, textures) and later layers process low-resolution, high-semantic data (objects, concepts).

**The Engineering Implication**:
Architectures should be tapered (e.g., Pyramidal CNNs). Allocating equal compute to every layer is inefficient; capacity should be concentrated where semantic density is highest.
:::

::: {.callout-note icon=false title="The Residual Gradient Invariant"}
**The Law**: Deep network trainability is preserved only if there exists a direct path for gradient flow that bypasses non-linear transformations.
$$ \frac{\partial L}{\partial x} \approx 1 $$

**The Engineering Implication**:
As networks grow deeper, **Skip Connections** (ResNets, Transformers) become mandatory. Without them, the "Shattered Gradients" problem prevents error signals from propagating to early layers, halting learning.
:::

## Part II Roadmap: From Math to Infrastructure

This section translates these principles into the components of the ML stack:

1.  **The Primer (@sec-dl-primer)**: The mathematical foundations of the gradient flow.
2.  **The Architectures (@sec-dnn-architectures)**: How we structure the "Silicon Contract" for Vision, Language, and Recommendations.
3.  **The Frameworks (@sec-ai-frameworks)**: The software that automates the Iron Law.
4.  **The Training (@sec-ai-training)**: The physical execution of the build process at scale.
