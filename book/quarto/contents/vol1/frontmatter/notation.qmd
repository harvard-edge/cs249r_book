---
title: "Notation and Conventions"
---

This book spans two distinct disciplines: **Machine Learning** (computer science/statistics) and **Systems** (computer architecture/hardware). Each field developed its notation independently, and many symbols mean different things depending on which community you're reading. This collision creates real confusion when the disciplines merge—which is exactly what ML Systems requires.

Consider a simple statement: *"Increasing $B$ improves throughput."* To an ML researcher, $B$ means batch size. To a hardware engineer, $B$ means bandwidth. Both interpretations are correct in their respective fields, but in ML Systems, we need both concepts in the same equation. This section establishes our notation to eliminate such ambiguity.

## The Iron Law of ML Systems

The fundamental performance equation of this book (@sec-silicon-contract) is:

$$T = \frac{D_{vol}}{BW} + \frac{O}{R_{peak} \cdot \eta} + L_{lat}$$

Each variable was chosen deliberately to avoid collision with standard ML terminology.

| **Symbol**     | **Definition**  | **Unit** | **Why This Symbol?**                                                                                                                                                                         |
|:-------------|:--------------|:-------|:-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| **$T$**        | **Time**        | seconds  | Unambiguous. Wall-clock time for an operation.                                                                                                                                               |
| **$D_{vol}$**  | **Data Volume** | bytes    | **Avoids collision with $D$ (Dataset Size)**. In scaling laws, $D$ means training tokens. Here we need bytes moved through memory. The subscript disambiguates.                              |
| **$BW$**       | **Bandwidth**   | bytes/s  | **Avoids collision with $B$ (Batch Size)**. Physics uses $B$ for bandwidth, but every ML paper uses $B$ for batch size. We preserve the ML convention.                                       |
| **$O$**        | **Operations**  | FLOPs    | Total floating-point operations. Clean in equations (vs. "$Ops$").                                                                                                                           |
| **$R_{peak}$** | **Peak Rate**   | FLOP/s   | **Avoids collision with $P$ (Parameters)**. Roofline models use $P$ for peak performance, but ML universally uses $P$ for parameter count. We preserve the ML convention.                    |
| **$\eta$**     | **Efficiency**  | —        | Hardware utilization ($0 \le \eta \le 1$). Overloaded with learning rate, but context always disambiguates (you never optimize learning rate and hardware utilization in the same equation). |
| **$L_{lat}$**  | **Latency**     | seconds  | **Avoids collision with $\mathcal{L}$ (Loss)**. Fixed overhead time (kernel launch, network RTT). The subscript distinguishes from the loss function.                                        |

### Why These Choices Matter

Without careful notation, sentences become ambiguous:

> *"Reducing $D$ improves performance."*

Does this mean:

- Reducing **dataset size** (fewer training samples)? → Faster training, but potentially worse accuracy.
- Reducing **data volume moved** (smaller model, quantization)? → Faster inference, accuracy preserved.

With our notation, we can write precisely:

> *"Reducing $D_{vol}$ through INT8 quantization cuts memory traffic to one quarter while $D$ (training data) remains unchanged."*

Similarly, *"$B$ limits throughput"* could mean batch size is too small (ML interpretation) or bandwidth is saturated (systems interpretation). Our notation makes this explicit: *"$BW$ limits throughput"* vs. *"small $B$ underutilizes the GPU."*

## Deep Learning Notation

We follow standard deep learning conventions (@goodfellow2016deep) with explicit disambiguation for systems variables.

| **Symbol**        | **Definition**       | **Dimensions / Type**                                                                        |
|:----------------|:-------------------|:-------------------------------------------------------------------------------------------|
| **$B$**           | **Batch Size**       | Integer. The number of samples processed in parallel. *(Never bandwidth.)*                   |
| **$P$**           | **Parameters**       | Integer. The total count of trainable weights in a model. *(Never peak FLOP/s.)*             |
| **$D$**           | **Dataset Size**     | Integer. Number of training samples or tokens. *(Never data volume in bytes—use $D_{vol}$.)* |
| **$S$**           | **Sequence Length**  | Integer. Number of tokens or time steps.                                                     |
| **$d$**           | **Hidden Dimension** | Integer. Size of the hidden state vector.                                                    |
| **$\mathcal{L}$** | **Loss Function**    | Scalar. The objective function minimized during training.                                    |
| **$\eta$**        | **Learning Rate**    | Scalar. Step size for the optimizer. *(Also efficiency—context distinguishes.)*              |
| **$\theta$**      | **Model Weights**    | Vector/Matrix. The set of all learnable parameters.                                          |

## Units and Precision

*   **Storage**: We use industry-standard decimal prefixes (KB, MB, GB, TB) throughout the book for consistency with common ML literature. While memory is technically addressed in binary units (1024-based), we follow industry convention where "80 GB GPU" means 80 × $1024^3$ bytes.
    *   1 KB = 1000 bytes (industry convention, also used for ~1024 bytes in memory contexts)
    *   1 GB = $10^9$ bytes (storage/network) or $\approx 2^{30}$ bytes (memory, per industry convention)
*   **Compute**: We use decimal prefixes for operations.
    *   1 TFLOP = $10^{12}$ FLOPs
*   **Precision**:
    *   **FP32**: Single precision (4 bytes)
    *   **FP16**: Half precision (2 bytes, standard range)
    *   **BF16**: Brain float (2 bytes, wide dynamic range)
    *   **INT8**: 8-bit integer (1 byte)

## Quick Reference: Resolving Collisions

When reading ML Systems literature (including this book), watch for these common collision points:

| **Symbol** | **ML Meaning** | **Systems Meaning** | **Our Convention**                                   |
|:---------|:-------------|:------------------|:---------------------------------------------------|
| $B$        | Batch Size     | Bandwidth           | **Batch Size**. Use $BW$ for bandwidth.              |
| $P$        | Parameters     | Peak FLOP/s         | **Parameters**. Use $R_{peak}$ for peak rate.        |
| $D$        | Dataset Size   | Data Volume         | **Dataset Size**. Use $D_{vol}$ for bytes moved.     |
| $L$        | Loss           | Latency             | **Loss** ($\mathcal{L}$). Use $L_{lat}$ for latency. |
| $\eta$     | Learning Rate  | Efficiency          | **Both**. Context disambiguates.                     |

The general principle: **ML conventions take precedence for single letters**; systems concepts get subscripts or multi-letter symbols. This reflects the primary audience (ML practitioners learning systems) and preserves compatibility with the vast ML literature.
