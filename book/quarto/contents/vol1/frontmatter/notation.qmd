---
title: "Notation and Conventions"
---

This book spans two distinct disciplines: **Machine Learning** (computer science/statistics) and **Systems** (computer architecture/hardware). Each field often reuses variables for different concepts. To avoid ambiguity, this text adheres to the following standard notation.

## The Iron Law of ML Systems

The fundamental performance equation of this book (@sec-silicon-contract) uses the following specific variables:

| **Symbol**     | **Definition**  | **Unit** | **Systems Context**                                                                                                                           |
|:-------------|:--------------|:-------|:--------------------------------------------------------------------------------------------------------------------------------------------|
| **$T$**        | **Time**        | seconds  | The total wall-clock time for an operation.                                                                                                   |
| **$D_{vol}$**  | **Data Volume** | bytes    | The total amount of data moved (e.g., model weights + activations). Distinguished from dataset size.                                          |
| **$BW$**       | **Bandwidth**   | bytes/s  | The sustained transfer rate of the memory or network link. <br>*(Note: Often denoted as $B$ in physics, but we reserve $B$ for Batch Size)*.  |
| **$O$**        | **Operations**  | FLOPs    | Total floating-point operations required.                                                                                                     |
| **$R_{peak}$** | **Peak Rate**   | FLOP/s   | The theoretical maximum throughput of the hardware. <br>*(Note: Often denoted as $P$ in roofline models, but we reserve $P$ for Parameters)*. |
| **$\eta$**     | **Efficiency**  | %        | Hardware utilization factor ($0 \le \eta \le 1$).                                                                                             |
| **$L_{lat}$**  | **Latency**     | seconds  | Fixed overhead time (kernel launch, network ping).                                                                                            |

## Deep Learning Notation

We follow standard deep learning conventions (Goodfellow et al.) with explicit disambiguation for systems variables.

| **Symbol**        | **Definition**       | **Dimensions / Type**                                                                    |
|:----------------|:-------------------|:---------------------------------------------------------------------------------------|
| **$B$**           | **Batch Size**       | Integer. The number of samples processed in parallel.                                    |
| **$P$**           | **Parameters**       | Integer. The total count of trainable weights in a model.                                |
| **$S$**           | **Sequence Length**  | Integer. Number of tokens or time steps.                                                 |
| **$d$**           | **Hidden Dimension** | Integer. Size of the hidden state vector.                                                |
| **$\mathcal{L}$** | **Loss Function**    | Scalar. The objective function minimized during training.                                |
| **$\eta$**        | **Learning Rate**    | Scalar. Step size for the optimizer (overloaded with efficiency, context distinguishes). |
| **$\theta$**      | **Model Weights**    | Vector/Matrix. The set of all learnable parameters.                                      |

## Units and Precision

*   **Storage**: We use industry-standard decimal prefixes (KB, MB, GB, TB) throughout the book for consistency with common ML literature. While memory is technically addressed in binary units (1024-based), we follow industry convention where "80 GB GPU" means 80 Ã— $1024^3$ bytes.
    *   1 KB = 1000 bytes (industry convention, also used for ~1024 bytes in memory contexts)
    *   1 GB = $10^9$ bytes (storage/network) or $\approx 2^{30}$ bytes (memory, per industry convention)
*   **Compute**: We use decimal prefixes for operations.
    *   1 TFLOP = $10^{12}$ FLOPs
*   **Precision**:
    *   **FP32**: Single precision (4 bytes)
    *   **FP16**: Half precision (2 bytes, standard range)
    *   **BF16**: Brain float (2 bytes, wide dynamic range)
    *   **INT8**: 8-bit integer (1 byte)

## Common Collisions to Avoid

Be careful with these overloaded terms that appear in both ML and Systems contexts:

*   **$B$**: Usually **Batch Size**, never Bandwidth (use $BW$).
*   **$P$**: Usually **Parameters**, never Peak FLOPs (use $R_{peak}$).
*   **$D$**: Usually **Dataset Size** (number of samples/tokens), not Data Volume in bytes (use $D_{vol}$).
*   **$M$**: Often used for Model Size (bytes) or simply "Millions". Context is key.
