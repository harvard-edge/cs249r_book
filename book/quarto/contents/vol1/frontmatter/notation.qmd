---
number-sections: false
---

# Notation and Conventions {.unnumbered}

This book spans two distinct disciplines: **Machine Learning** (computer science/statistics) and **Systems** (computer architecture/hardware). Each field developed its notation independently, and many symbols mean different things depending on which community you're reading. This collision creates real confusion when the disciplines merge—which is exactly what ML Systems requires.

Consider a simple statement: *"Increasing $B$ improves throughput."* To an ML researcher, $B$ means batch size. To a hardware engineer, $B$ means bandwidth. Both interpretations are correct in their respective fields, but in ML Systems, we need both concepts in the same equation. This section establishes our notation to eliminate such ambiguity.

## The Iron Law of ML Systems {#sec-notation-conventions-iron-law-ml-systems-ce9d}

The fundamental performance equation of this book (@sec-introduction-iron-law-ml-systems-c32a) is:

$$T = \frac{D_{\text{vol}}}{\text{BW}} + \frac{O}{R_{\text{peak}} \cdot \eta} + L_{\text{lat}}$$

Each variable was chosen deliberately to avoid collision with standard ML terminology.

| **Symbol**            | **Definition**  | **Unit** | **Why This Symbol?**                                                                                                                                                                         |
|:----------------------|:----------------|:---------|:---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| **$T$**               | **Time**        | seconds  | Unambiguous. Wall-clock time for an operation.                                                                                                                                               |
| **$D_{\text{vol}}$**  | **Data Volume** | bytes    | **Avoids collision with $D$ (Dataset Size)**. In scaling laws, $D$ means training tokens. Here we need bytes moved through memory. The subscript disambiguates.                              |
| **$\text{BW}$**       | **Bandwidth**   | bytes/s  | **Avoids collision with $B$ (Batch Size)**. Physics uses $B$ for bandwidth, but every ML paper uses $B$ for batch size. We preserve the ML convention.                                       |
| **$O$**               | **Operations**  | FLOPs    | Total floating-point operations. Clean in equations (vs. "$Ops$").                                                                                                                           |
| **$R_{\text{peak}}$** | **Peak Rate**   | FLOP/s   | **Avoids collision with $P$ (Parameters)**. Roofline models use $P$ for peak performance, but ML universally uses $P$ for parameter count. We preserve the ML convention.                    |
| **$\eta$**            | **Efficiency**  | —        | Hardware utilization ($0 \le \eta \le 1$). Overloaded with learning rate, but context always disambiguates (you never optimize learning rate and hardware utilization in the same equation). |
| **$L_{\text{lat}}$**  | **Latency**     | seconds  | **Avoids collision with $\mathcal{L}$ (Loss)**. Fixed overhead time (kernel launch, network RTT). The subscript distinguishes from the loss function.                                        |

### Why These Choices Matter {#sec-notation-conventions-choices-matter-0c16}

Without careful notation, sentences become ambiguous:

> *"Reducing $D$ improves performance."*

Does this mean:

- Reducing **dataset size** (fewer training samples)? → Faster training, but potentially worse accuracy.
- Reducing **data volume moved** (smaller model, quantization)? → Faster inference, accuracy preserved.

With our notation, we can write precisely:

> *"Reducing $D_{vol}$ through INT8 quantization cuts memory traffic to one quarter while $D$ (training data) remains unchanged."*

Our notation makes such ambiguity explicit: *"$BW$ limits throughput"* is unambiguous.

## The Degradation Equation {#sec-notation-conventions-degradation-equation-ad12}

The silent failure mode of ML systems is captured by the Degradation Equation (@eq-degradation):

$$\text{Accuracy}(t) \approx \text{Accuracy}_0 - \lambda \cdot D(P_t \| P_0)$$

| **Symbol**          | **Definition**             | **Unit / Type** | **Notes**                                                                                                   |
|:--------------------|:---------------------------|:----------------|:------------------------------------------------------------------------------------------------------------|
| $\text{Accuracy}_0$ | **Initial Accuracy**       | Scalar          | Model accuracy at deployment time.                                                                          |
| $\lambda$           | **Sensitivity**            | Scalar          | Model sensitivity to distribution shift. Architecture-dependent. *(Not wavelength.)*                        |
| $P_t$               | **Current Distribution**   | Distribution    | The data distribution at time $t$. *(Not parameters — use $P$ for parameter count.)*                        |
| $P_0$               | **Training Distribution**  | Distribution    | The data distribution at training time.                                                                     |
| $D(P_t \lVert P_0)$ | **Statistical Divergence** | Scalar $\ge 0$  | Measures how far $P_t$ has drifted from $P_0$. Common choices: KL divergence, total variation, Wasserstein. |
| $\tau$              | **Drift Threshold**        | Scalar $> 0$    | Retraining is triggered when $D(P_t \lVert P_0) > \tau$.                                                    |

## The Energy Corollary {#sec-notation-conventions-energy-corollary-a1da}

The energy cost of ML workloads (@sec-introduction-iron-law-ml-systems-c32a) decomposes as:

$$E_{\text{total}} \approx D_{vol} \times E_{\text{move}} + O \times E_{\text{compute}}$$

| **Symbol**               | **Definition**            | **Unit**    | **Notes**                                                                                        |
|:-------------------------|:--------------------------|:------------|:-------------------------------------------------------------------------------------------------|
| **$E_{\text{move}}$**    | **Energy per Byte Moved** | joules/byte | Energy cost of data movement. Dominates total energy ($E_{\text{move}} \gg E_{\text{compute}}$). |
| **$E_{\text{compute}}$** | **Energy per Operation**  | joules/FLOP | Energy cost of a single arithmetic operation.                                                    |

## Deep Learning Notation {#sec-notation-conventions-deep-learning-notation-b40d}

We follow standard deep learning conventions (@goodfellow2016deep) with explicit disambiguation for systems variables.

| **Symbol**        | **Definition**       | **Dimensions / Type**                                                                        |
|:------------------|:---------------------|:---------------------------------------------------------------------------------------------|
| **$B$**           | **Batch Size**       | Integer. The number of samples processed in parallel. *(Never bandwidth.)*                   |
| **$P$**           | **Parameters**       | Integer. The total count of trainable weights in a model. *(Never peak FLOP/s.)*             |
| **$D$**           | **Dataset Size**     | Integer. Number of training samples or tokens. *(Never data volume in bytes—use $D_{vol}$.)* |
| **$S$**           | **Sequence Length**  | Integer. Number of tokens or time steps.                                                     |
| **$d$**           | **Hidden Dimension** | Integer. Size of the hidden state vector.                                                    |
| **$\mathcal{L}$** | **Loss Function**    | Scalar. The objective function minimized during training.                                    |
| **$\eta$**        | **Learning Rate**    | Scalar. Step size for the optimizer. *(Also efficiency—context distinguishes.)*              |
| **$\theta$**      | **Model Weights**    | Vector/Matrix. The set of all learnable parameters.                                          |

## Units and Precision {#sec-notation-conventions-units-precision-fdaf}

*   **Storage**: We use decimal prefixes (1 KB = 1000 bytes, 1 GB = $10^9$ bytes) throughout for consistency with ML literature. In memory contexts, industry convention uses these same symbols for binary values ($2^{10}$, $2^{30}$); the difference is negligible for our purposes.
*   **Compute**: We use decimal prefixes for operations.
    *   1 TFLOP = $10^{12}$ FLOPs
*   **Precision**:
    *   **FP32**: Single precision (4 bytes)
    *   **FP16**: Half precision (2 bytes, standard range)
    *   **BF16**: Brain float (2 bytes, wide dynamic range)
    *   **INT8**: 8-bit integer (1 byte)

## Quick Reference: Resolving Collisions {#sec-notation-conventions-quick-reference-resolving-collisions-a9cf}

When reading ML Systems literature (including this book), watch for these common collision points:

| **Symbol** | **ML Meaning** | **Systems Meaning** | **Our Convention**                                   |
|:-----------|:---------------|:--------------------|:-----------------------------------------------------|
| $B$        | Batch Size     | Bandwidth           | **Batch Size**. Use $BW$ for bandwidth.              |
| $P$        | Parameters     | Peak FLOP/s         | **Parameters**. Use $R_{peak}$ for peak rate.        |
| $D$        | Dataset Size   | Data Volume         | **Dataset Size**. Use $D_{vol}$ for bytes moved.     |
| $L$        | Loss           | Latency             | **Loss** ($\mathcal{L}$). Use $L_{lat}$ for latency. |
| $\eta$     | Learning Rate  | Efficiency          | **Both**. Context disambiguates.                     |

The general principle: **ML conventions take precedence for single letters**; systems concepts get subscripts or multi-letter symbols. This reflects the primary audience (ML practitioners learning systems) and preserves compatibility with the vast ML literature.

```{=latex}
\part{key:vol1_foundations}
```
