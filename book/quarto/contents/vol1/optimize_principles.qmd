# Principles of Optimization {.unnumbered}

If **Part II: Build** was about establishing the logical structure of a machine learning system, **Part III: Optimize** is about managing its physical limits. These principles define the "Physics of Efficiency"—the laws that determine why some models are fast and affordable while others are slow and prohibitively expensive.

::: {.callout-perspective title="The Pareto Frontier of AI Engineering"}
**The Concept**: Optimization is not a single-objective goal. It is a multi-dimensional search for the **Pareto Frontier**—the boundary where you cannot improve one metric (e.g., Accuracy) without degrading another (e.g., Latency or Energy).

**The Engineering Implication**:
Every optimization we explore in this section involves a trade-off.
*   **Quantization** trades numerical precision for memory bandwidth.
*   **Pruning** trades model capacity for computational speed.
*   **Distillation** trades training compute for inference efficiency.
Your job as a systems engineer is to navigate this frontier to find the "sweet spot" for your specific deployment environment.
:::

::: {.callout-note icon=false title="The Arithmetic Intensity Law (The Roofline)"}
**The Law**: System throughput ($P$) is bounded by the minimum of peak compute ($P_{peak}$) and memory bandwidth ($B_{mem}$) relative to the workload's arithmetic intensity ($I$):
$$ P = \min(P_{peak}, I \times B_{mem}) $$

**The Engineering Implication**:
Adding compute power to a memory-bound model yields **zero** performance gain. You must identify whether your bottleneck is Math (Compute-Bound) or Memory (Bandwidth-Bound) before selecting an optimization technique.
:::

::: {.callout-note icon=false title="The Energy-Movement Invariant"}
**The Law**: Moving 1 bit of data from DRAM costs 100–500$	imes$ more energy than performing an arithmetic operation on it.
$$ E_{move} \gg E_{compute} $$

**The Engineering Implication**:
**Data Locality** is the primary driver of efficiency. Optimization strategies must prioritize **Kernel Fusion** (keeping data in registers) and **Quantization** (reducing data size) over reducing raw operation counts.
:::

::: {.callout-note icon=false title="The Systems Law (Amdahl's Law)"}
**The Law**: The maximum speedup of a system is limited by the fraction of the workload that cannot be accelerated.
$$ \text{Speedup} = \frac{1}{(1-p) + \frac{p}{s}} $$
where $p$ is the parallelizable fraction and $s$ is the speedup of that fraction.

**The Engineering Implication**:
If 95% of your model ($p=0.95$) runs 100$	imes$ faster on a GPU ($s=100$), your total system speedup is capped at $\approx 20\times$. This explains why **Data Loading** and **Preprocessing** (the remaining 5%) often become the ultimate bottlenecks in highly optimized systems.
:::

## Part III Roadmap: The Three Pillars of Efficiency

This section explores how we apply these laws to the three primary layers of the stack:

1.  **Data Efficiency (@sec-data-efficiency)**: Optimizing the "Source Code." How to achieve high accuracy with fewer, higher-quality samples.
2.  **Model Compression (@sec-model-compression)**: Optimizing the "Binary." Techniques like Quantization and Pruning to shrink the model footprint.
3.  **Hardware Acceleration (@sec-hw-acceleration)**: Optimizing the "Execution." How to design silicon (GPUs, TPUs, NPUs) to match the math of modern AI.