# Principles of Optimization {.unnumbered}

These principles define the physical limits of computation. They explain why some models are fast while others are slow, regardless of the hardware brand.

::: {.callout-note icon=false title="The Arithmetic Intensity Law (The Roofline)"}
**The Law**: System throughput is bounded by the minimum of peak compute (FLOPS) and memory bandwidth (Bytes/sec) relative to the workload's ridge point.
$$ P = \min(P_{peak}, I \times B_{mem}) $$

**The Engineering Implication**:
Adding compute power to a memory-bound model yields **zero** performance gain. You must identify whether your bottleneck is Math (Matrix Multiply) or Memory (Attention/Element-wise) before optimizing.
:::

::: {.callout-note icon=false title="The Energy-Movement Invariant"}
**The Law**: Moving 1 bit of data from DRAM costs 100â€“500$\times$ more energy than performing an arithmetic operation on it.
$$ E_{move} \gg E_{compute} $$

**The Engineering Implication**:
**Data Locality** is the primary driver of efficiency. Optimization strategies must prioritize **Kernel Fusion** (keeping data in registers) and **Quantization** (reducing data size) over reducing operation counts.
:::

::: {.callout-note icon=false title="The Kernel Fusion Imperative"}
**The Law**: To minimize the "Memory Wall," adjacent element-wise operations must be fused into single kernels to ensure $O(1)$ reuse per memory fetch.

**The Engineering Implication**:
Framework overhead (launching many small kernels) dominates execution time for modern networks. Compilers (TorchCompile, XLA) that fuse operations are essential for utilizing modern accelerators.
:::
