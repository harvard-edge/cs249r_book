---
quiz: data_selection_quizzes.json
concepts: data_selection_concepts.yml
glossary: data_selection_glossary.json
crossrefs: data_selection_xrefs.json
engine: jupyter
---

# Data Selection {#sec-data-selection}

::: {layout-narrow}
::: {.column-margin}
_DALL·E 3 Prompt: A futuristic digital illustration depicting the concept of data selection in machine learning. On one side of the image, there is a sleek, powerful computing unit, symbolizing AI processing. On the other side, streams of binary code (1s and 0s) flow into the computer, but the data is represented with glowing golden elements, signifying valuable, high-quality information. The background has a high-tech, digital ambiance, emphasizing the role of refined, efficient data in machine learning. No text, only a strong visual representation of the relationship between computation and valuable data._
:::

\noindent
![A futuristic digital illustration of data selection in machine learning, showing a sleek computing unit on one side with streams of binary code flowing in, where valuable data elements glow golden against a high-tech digital background.](images/png/cover_data_efficiency.png){fig-alt="A futuristic digital illustration of data selection in machine learning, showing a sleek computing unit on one side with streams of binary code flowing in, where valuable data elements glow golden against a high-tech digital background."}

:::

## Purpose {.unnumbered}

*Why can a carefully selected 10% of your data match the accuracy of 100%?*

The most leveraged optimization in machine learning operates upstream, before a single gradient is computed: on the data itself. Naive scaling assumes data is homogeneous—that every sample contributes equally to learning. Reality differs dramatically: in large-scale datasets, a tiny fraction of examples provides the majority of the gradient signal while the vast majority are redundant, noisy, or misaligned with the target distribution. This heterogeneity is not a statistical artifact but a systems optimization opportunity. Compressing models and accelerating hardware speed up the execution of work; data selection reduces the *fundamental workload itself*, shifting the paradigm from accumulating data as a massive liability to optimizing it as a precise resource, where the savings compound through every subsequent stage of the pipeline.

::: {.callout-tip title="Learning Objectives"}

- Explain data selection as the third pillar of ML optimization alongside algorithms and systems
- Apply the Information-Compute Ratio (ICR) framework to evaluate dataset value
- Compare coreset and deduplication techniques for pre-training data reduction
- Implement the three-stage optimization pipeline: static pruning, dynamic selection, and synthetic generation
- Design curriculum learning and active learning strategies for training-time optimization
- Analyze cost-benefit trade-offs and systems engineering challenges in data selection pipelines

:::

## Data Selection Fundamentals {#sec-data-selection-fundamentals}

Data selection asks a simple question with profound engineering consequences: given a clean, well-engineered dataset, which examples contribute the most learning per unit of compute cost? The preceding chapter on data engineering (@sec-data-engineering-ml) established the infrastructure for collecting, cleaning, and preparing data, producing pipelines that ingest raw signals and yield well-governed, versioned datasets ready for training. That chapter ensured data *quality* through correct labels, consistent schemas, and clean records. Data selection optimizes data *value* by extracting maximum learning from minimum samples, directly shrinking the **Total Operations ($O$)** term in the **Iron Law** (@sec-silicon-contract). The distinction is fundamental: quality asks *whether* data is correct, while value asks *whether* correct data is worth the compute spent processing it.

For decades, the dominant strategy was straightforward: more data, better models. Scaling laws [@kaplan2020scaling; @hoffmann2022training] confirmed that model performance improves predictably with dataset size, and teams responded rationally by scraping more web pages, labeling more images, and generating more synthetic examples. A fundamental asymmetry has since emerged. Hardware acceleration (@sec-ai-acceleration) has outpaced the growth of high-quality data. GPU compute capacity has increased faster than traditional Moore's Law projections—architectural innovations like Tensor Cores and reduced-precision arithmetic stack on top of process-node gains, yielding effective throughput improvements faster than every two years—while the supply of novel, high-quality human-generated text and images grows at roughly 2$\times$ every five years (@tbl-scaling-asymmetry). The internet has already been scraped. Domain experts cannot label faster. This asymmetry, which researchers call the **Data Wall**\index{Data Wall!definition}[^fn-data-wall] [@villalobos2022will], has inverted the optimization priority from "get more data" to "get more from existing data."

[^fn-data-wall]: **Data Wall**: A term popularized by Epoch AI researchers in 2022. Their analysis projected that high-quality language data (books, academic papers, filtered web text) could be exhausted within one to two decades at then-current scaling rates. The "wall" metaphor emphasizes that unlike compute (which can be purchased) or algorithms (which can be improved), the stock of human-generated training data grows slowly and may represent a fundamental constraint on scaling.

Trace the trend line in @fig-running-out-of-human-data: foundation models are consuming the stock of human-generated text at an accelerating rate, with projections suggesting exhaustion of high-quality public data on a timeline measured in years, not decades. This is not a distant concern. It shapes training strategies today.

![**Dataset Growth Approaching Limits**: Foundation models are increasingly trained on vast datasets, approaching the total stock of human-generated text. Current projections suggest that high-quality public text data faces exhaustion on a near-term horizon, forcing a shift toward data selection, synthetic generation, and multimodal learning. Source: @epochai2022trends.](images/png/running_out_of_data.png){#fig-running-out-of-human-data fig-alt="Line chart showing dataset size in tokens on y-axis from 10^10 to 10^14 versus year on x-axis from 2010 to 2030. Blue line shows training data growth with markers for models like GPT-2, GPT-3, and Chinchilla. Orange shaded region shows projected high-quality text exhaustion in the near term."}

@tbl-scaling-asymmetry quantifies the growth rates underlying @fig-running-out-of-human-data: compute budgets grow approximately 10$\times$ every three years, while high-quality text grows roughly 2$\times$ every five years. The gap between what compute can process and what quality data exists is widening, necessitating the **Continuous Training** loops of **MLOps** (@sec-machine-learning-operations-mlops) to maintain relevance and making intelligent data selection increasingly critical.

```{python}
#| label: scaling-asymmetry-calc
#| echo: false
from physx.formatting import fmt

# Growth rates
gpu_growth_str = "10\\times"
gpu_period_str = "3 years"
web_data_growth_str = "2\\times"
web_data_period_str = "5 years"
label_data_growth_str = "1.5\\times"
label_data_period_str = "5 years"
```

| **Resource**            |                                                      **Growth Rate** | **Implication**                                       |
|:----------------------|-------------------------------------------------------------------:|:----------------------------------------------------|
| **GPU Compute**         |               ~`{python} gpu_growth_str` / `{python} gpu_period_str` | Hardware vendors deliver reliable exponential gains   |
| **Training Data (Web)** |     ~`{python} web_data_growth_str` / `{python} web_data_period_str` | High-quality web text is finite; much already scraped |
| **Labeled Data**        | ~`{python} label_data_growth_str` / `{python} label_data_period_str` | Human annotation throughput is fundamentally bounded  |
| **Synthetic Data**      |                                                            Unbounded | Bounded by generator quality (risk of model collapse) |

: **Scaling Asymmetry in ML Resources.** Compute grows exponentially while high-quality data grows linearly or sub-linearly, creating an increasing compute-to-data imbalance that makes data selection essential. {#tbl-scaling-asymmetry .striped .hover}

::: {.callout-notebook title="The Scaling Asymmetry"}
**The Problem**: Compute scales exponentially. Data does not (@tbl-scaling-asymmetry).

**The Consequence**: Compute budgets now support training runs that far exceed what available high-quality data can fill. The field has become *compute-rich and data-poor*.
:::

This asymmetry inverts the optimization priority. When data was abundant and compute was scarce, the right strategy was algorithmic efficiency: squeeze more accuracy from limited GPU cycles. Now that compute is abundant and *quality data* is scarce, the winning strategy is **data selection**: squeeze more learning from each sample. Data selection operates upstream of all other optimizations. By pruning redundancy and selecting high-value samples, we reduce the workload before it ever enters the model or hits the hardware, directly shrinking the Total Operations ($O$) term in the Iron Law (see the callout "Data Selection and the Iron Law" below for a detailed analysis). Companies training frontier models are no longer bottlenecked by GPU access but by the quality and diversity of their training corpora.

This chapter provides the engineering toolkit for intelligent data selection, organized around Part III's **D·A·M taxonomy**, which establishes a deliberate optimization ordering: Data first, then Algorithm, then Machine. Data selection puts the "highest leverage first" principle into practice by addressing whether work is necessary before asking how to simplify or accelerate it. The chapter follows a three-stage optimization pipeline that structures the practical response to the Data Wall:

1. **Static Data Pruning**: Removing low-value samples before training begins (coresets, deduplication).
2. **Dynamic Selection**: Selecting high-value samples during training (curriculum learning, active learning).
3. **Synthetic Generation**: Creating high-value samples on demand (augmentation, distillation).

Each stage increases the *information density* of the data that reaches the model, and together they form a complementary toolkit: pruning reduces *what* you have, selection focuses *how* you use it, and synthesis expands *what* you can access. The next two sections formalize *what* "data selection" means and *why* it is fundamentally a systems problem.

## Defining Data Selection {#sec-data-selection-defining}

::: {.callout-definition title="Data Selection"}
***Data Selection***\index{Data Selection!definition} is the process of maximizing the **Information-Compute Ratio**\index{Data Selection!Information-Compute Ratio (ICR)}. It operates upstream of training to identify the **Minimum Viable Subset** of data required to define the decision boundary, reducing the **Total Operations** ($O$) term of the Iron Law by eliminating redundant, noisy, or non-informative samples before they consume GPU cycles.

$$
\text{Selection Efficiency} = \frac{\Delta \text{Model Capability}}{\Delta \text{Data Cost}}
$$

where Data Cost encompasses:

- **Acquisition cost**: Time and money to collect or generate samples
- **Labeling cost**: Human expert annotation effort
- **Storage cost**: Bytes required to persist the dataset
- **Compute cost**: FLOPs to process samples during training

A perfectly efficient dataset would contain only samples that contribute unique information to the model's decision boundary: no redundancy, no noise, no "easy" examples already mastered.
:::

To make this concrete, consider training a model in the **GPT-2/Llama Lighthouse** family (@sec-dnn-architectures)---this lighthouse spans the autoregressive LLM family, from GPT-2's 1.5B parameters to Llama's 7B--70B range---here using a 70B parameter language model:

```{python}
#| label: gpt-llama-calc
#| echo: false
from physx.formatting import fmt

llama_params_value = 70e9
h100_count_value = 10000
training_months_value = 3
tokens_capacity_value = 10e12
tokens_available_value = 5e12
compute_gap_value = tokens_capacity_value / tokens_available_value

llama_params_str = fmt(llama_params_value, precision=0, unit="B")
h100_count_str = fmt(h100_count_value, precision=0, commas=True)
tokens_capacity_str = fmt(tokens_capacity_value, precision=0, unit="T")
tokens_available_str = fmt(tokens_available_value, precision=0, unit="T")
compute_gap_str = fmt(compute_gap_value, precision=0)
```

- **Compute available**: `{python} h100_count_str` H100 GPUs for `{python} training_months_value` months represents tens of millions of dollars in compute budget, capable of processing over `{python} tokens_capacity_str` tokens
- **High-quality data available**: ~`{python} tokens_available_str` tokens of deduplicated, filtered web text
- **The gap**: `{python} compute_gap_str`× more compute than data can utilize

The team faces a choice:

1. Train on the same data for multiple epochs (diminishing returns after epochs 2–3)
2. Lower quality thresholds to include more data (degrades model quality)
3. Invest in data selection through better filtering, curriculum design, and synthetic augmentation to extract more learning from each token

Option 3 is increasingly the dominant approach.

This data selection imperative applies across model architectures, though the bottlenecks differ. Unlike our compute-bound ResNet-50 Lighthouse, GPT-2/Llama models are **memory bandwidth-bound** during inference (though often compute-bound during training as well) and still benefit enormously from data selection during training. Each token processed requires the same forward/backward pass cost regardless of model bottleneck, so fewer tokens means fewer FLOPs.

## Data Selection as a Systems Problem {#sec-data-selection-data-selection-systems-problem-d857}

The Data Wall establishes *why* data selection matters; the systems perspective reveals *how* to approach it effectively. Data selection is typically framed as a machine learning problem: *how do I achieve the same accuracy with fewer samples?* This framing focuses on statistical sample complexity and generalization theory. While valid, it misses the larger picture.

In this textbook, we adopt a **systems framing**: *how do I reduce the total cost of achieving target performance across the entire ML lifecycle?* This shifts attention from accuracy curves to resource consumption, as @tbl-ml-vs-systems-framing illustrates.

| **ML Framing**                        | **Systems Framing**                         |
|:------------------------------------|:------------------------------------------|
| **"Fewer samples for same accuracy"** | "Fewer FLOPs for same accuracy"             |
| **"Better generalization"**           | "Lower training cost (time, money, energy)" |
| **"Sample complexity bounds"**        | "End-to-end resource efficiency"            |
| **"Learning theory"**                 | "Cost engineering"                          |

: **ML vs. Systems Perspectives on Data Selection.** The ML framing optimizes sample complexity; the systems framing optimizes total resource cost across the pipeline. {#tbl-ml-vs-systems-framing .striped .hover}

The systems framing reveals optimization opportunities invisible to the ML framing. To see *why*, consider *how* data selection interacts with the **Iron Law** introduced in @sec-ml-system-architecture.

```{python}
#| label: data-selection-savings-calc
#| echo: false
from physx.formatting import fmt

# =============================================================================
# PURPOSE
# =============================================================================
# Purpose: Quantify cost savings from dataset reduction
# Used in: Data selection cost savings example

# =============================================================================
# INPUT
# =============================================================================
training_cost_m_value = 100  # $M
dataset_reduction_pct_value = 50  # percent
data_selection_factor_value = 2
model_compression_factor_value = 2
hardware_accel_factor_value = 2

# =============================================================================
# PROCESS
# =============================================================================
compute_savings_m_value = training_cost_m_value * dataset_reduction_pct_value / 100
combined_factor_value = (
    data_selection_factor_value
    * model_compression_factor_value
    * hardware_accel_factor_value
)

# =============================================================================
# OUTPUT
# =============================================================================
training_cost_m_str = fmt(training_cost_m_value, precision=0, commas=False)
dataset_reduction_pct_str = fmt(dataset_reduction_pct_value, precision=0, commas=False)
compute_savings_m_str = fmt(compute_savings_m_value, precision=0, commas=False)
combined_factor_str = fmt(combined_factor_value, precision=0, commas=False)
```

::: {.callout-perspective title="Data Selection and the Iron Law"}
In the **Iron Law of ML Systems** ($T = \frac{D_{vol}}{BW} + \frac{O}{R_{peak} \cdot \eta} + L_{lat}$), data selection is the only technique that reduces the *Total Operations* term at its source. Model compression reduces operations per sample; hardware acceleration increases throughput per operation. Data selection, by contrast, reduces the number of samples processed entirely.

- **Model compression**: Reduces $O$ per forward/backward pass
- **Hardware acceleration**: Increases $R_{peak}$ (peak throughput) and $\eta$ (utilization)
- **Data selection**: Reduces the number of passes through the entire equation

This makes data selection multiplicatively valuable: when all three optimizations act on the same bottleneck, a 2× reduction in dataset size with 2× model compression and 2× hardware acceleration yields `{python} combined_factor_str`× total cost reduction, not 6×.
:::

Consider training cost reduction: a `{python} dataset_reduction_pct_str`% reduction in dataset size does not merely improve sample efficiency; it directly halves the number of forward passes, backward passes, and gradient updates. For a USD `{python} training_cost_m_str` M training run, this translates to USD `{python} compute_savings_m_str` M in compute savings. The relationship is linear and immediate.

These compute savings cascade into storage and I/O costs. Large datasets consume petabytes of storage and saturate network bandwidth during distributed training. Data selection techniques like deduplication reduce storage costs and eliminate I/O bottlenecks that can idle expensive GPU clusters.

Data selection also transforms labeling economics. Expert labeling costs (\$5–100+ per sample in domains like medical imaging) often exceed compute costs. Active learning and semi-supervised methods are not merely algorithmic techniques but cost engineering tools that can reduce labeling budgets by 10–100$\times$.

The environmental implications are substantial. Training a large language model can emit hundreds of tons of CO₂. Data selection is the most direct lever for Green AI: halving the dataset halves training energy, with no accuracy trade-off if done correctly.

Smaller, curated datasets also enable faster iteration velocity. A team that can iterate in hours rather than days has a compounding advantage in model development.

These cascading benefits illustrate a broader point about how systems engineers think differently about data than ML researchers.

::: {.callout-perspective title="The Systems Engineer's View of Data"}
**The ML Researcher asks:** "What is the sample complexity of this learning problem?"

**The Systems Engineer asks:** "What is the cost-per-accuracy-point across the entire pipeline, from data acquisition through deployment?"

This chapter equips you with the systems engineer's toolkit: techniques to minimize total cost, metrics to quantify efficiency gains, and architectural patterns to implement data selection at scale.
:::

## The Information-Compute Ratio {#sec-data-selection-informationcompute-ratio-8e9b}

The systems framing established above calls for a quantitative metric. The Optimize Principles (Part III) introduced the **Pareto Frontier** as the boundary where improving one metric necessarily degrades another, and identified three pillars of efficiency following the D·A·M taxonomy: Data, Algorithm (model compression, @sec-model-compression), and Machine (hardware acceleration, @sec-ai-acceleration). As the first pillar in the D·A·M ordering, data selection addresses the most fundamental question: can we reduce the work before it begins? We formalize this with a central metric: the Information-Compute Ratio.

While model compression and hardware acceleration focus on the *execution* of the math, *Data Selection* reduces the *amount* of math required by optimizing what enters the training pipeline.

Data engineering (@sec-data-engineering-ml) ensures that data is clean, accessible, and correctly formatted. Data selection asks a different question: *how much information does each sample contribute to the model's learning per unit of computation?*

In the optimization triad (@fig-optimization-triad), data selection plays the role of *Input Optimization*, reducing total workload before it enters the model or hardware. While model compression minimizes the math per parameter and hardware acceleration maximizes the math per second, data selection minimizes the total math required to reach convergence. The three edges of the triad capture the dominant bottlenecks: *Compute Bound* describes systems limited by arithmetic throughput, *I/O Bound* describes systems limited by data movement, and *Sample Efficiency* describes systems limited by the information content of training data.

::: {#fig-optimization-triad fig-cap="**The Optimization Triad**: Machine learning performance relies on three pillars: Algorithms (models), Systems (hardware/software), and Data Selection. While algorithms and systems have traditionally received the most attention, optimizing data selection (Input Optimization) offers a third, powerful lever for scaling performance." fig-alt="A triangular diagram with three nodes: Algorithms (Model), Systems (Hardware), and Data Selection. Bidirectional arrows connect all three with edge labels: Compute Bound between Algorithms and Systems, I/O Bound between Systems and Data Selection, and Sample Efficiency between Data Selection and Algorithms. Data Selection is highlighted with a bold border. ML Scale appears at the center."}
```{.tikz}
\begin{tikzpicture}[font=\small\usefont{T1}{phv}{m}{n}, scale=0.9, transform shape]
  % Nodes
  \node[draw, circle, minimum size=2.8cm, fill=blue!10, align=center, text width=2.5cm] (Alg) at (90:2.5) {Algorithms\\(Model)};
  \node[draw, circle, minimum size=2.8cm, fill=green!10, align=center, text width=2.5cm] (Sys) at (210:2.5) {Systems\\(Hardware)};
  \node[draw, circle, minimum size=2.8cm, fill=orange!10, align=center, text width=2.5cm, line width=1.5pt] (Data) at (330:2.5) {\textbf{Data}\\\textbf{Selection}};

  % Connections
  \draw[<->, thick] (Alg) -- node[left, font=\footnotesize\usefont{T1}{phv}{m}{n}, text width=1.5cm, align=right] {Compute\\Bound} (Sys);
  \draw[<->, thick] (Sys) -- node[below, font=\footnotesize\usefont{T1}{phv}{m}{n}] {I/O Bound} (Data);
  \draw[<->, thick] (Data) -- node[right, font=\footnotesize\usefont{T1}{phv}{m}{n}, text width=1.5cm, align=left] {Sample\\Efficiency} (Alg);

  % Center Label
  \node[align=center, font=\bfseries\usefont{T1}{phv}{m}{n}] at (0,0) {ML\\Scale};
\end{tikzpicture}
```
:::

We can formalize this as the **Information-Compute Ratio (ICR)**:

$$
\text{ICR} = \frac{\Delta \text{Model Performance}}{\Delta \text{FLOPs}}
$$

**The Iron Law Connection**: As detailed in the "Data Selection and the Iron Law" callout above, data selection turns the Total Operations ($O$) term from a fixed constant into a variable. By maximizing ICR, we reduce the total FLOPs required to reach a target performance level. A 2x improvement in ICR is mathematically equivalent to a 2x improvement in hardware Peak Throughput ($R_{peak}$), but often much cheaper to achieve. Note that ICR focuses specifically on the compute component of the broader Selection Efficiency metric defined earlier, which also accounts for acquisition, labeling, and storage costs.

A random batch of raw data often has low ICR because it contains redundant examples, noisy samples, or "easy" examples the model has already mastered. Training on such a batch wastes GPU cycles on zero-information updates. High-efficiency data pipelines (@fig-data-selection-pipeline) filter, order, and synthesize data to maximize ICR, ensuring that every FLOP contributes to learning. To illustrate, consider *computing ICR* on a concrete coreset selection task. Later in this chapter, @sec-data-selection-measuring-data-selection-7957 provides the complete measurement framework for evaluating these efficiency gains, including the Data Roofline model that diagnoses whether a system is data-bound or compute-bound.

::: {#fig-data-selection-pipeline fig-cap="**The Data Selection Pipeline**: A structured approach to increasing data value. Raw data is first pruned to remove redundancy (Static Pruning), then dynamically selected during training (Active Learning), and finally augmented to increase diversity (Synthesis). Each stage increases the Information-Compute Ratio (ICR)." fig-alt="A flow diagram showing the progression of data: Raw Data -> Static Pruning -> Dynamic Selection -> Synthetic Generation -> High Value Model. Arrows indicate the flow."}
```{.tikz}
\begin{tikzpicture}[font=\small\usefont{T1}{phv}{m}{n}, node distance=1.5cm, >={Stealth[length=3mm]}]
  % Nodes
  \node[draw, rectangle, fill=gray!10, minimum height=1cm, minimum width=2cm] (Raw) {Raw Data};

  \node[draw, rectangle, fill=blue!10, minimum height=1cm, minimum width=2.5cm, right=1cm of Raw, align=center] (Prune) {1. Static\\Pruning};

  \node[draw, rectangle, fill=green!10, minimum height=1cm, minimum width=2.5cm, right=1cm of Prune, align=center] (Select) {2. Dynamic\\Selection};

  \node[draw, rectangle, fill=orange!10, minimum height=1cm, minimum width=2.5cm, right=1cm of Select, align=center] (Synth) {3. Synthetic\\Gen};

  \node[draw, circle, fill=red!10, minimum size=1.2cm, right=1cm of Synth] (Model) {Model};

  % Edges
  \draw[->, thick] (Raw) -- (Prune);
  \draw[->, thick] (Prune) -- (Select);
  \draw[->, thick] (Select) -- (Synth);
  \draw[->, thick] (Synth) -- node[above, font=\footnotesize\usefont{T1}{phv}{m}{n}] {High ICR} (Model);

  % Annotations
  \node[below=0.2cm of Prune, font=\footnotesize\usefont{T1}{phv}{m}{n}, color=gray] {Pre-training};
  \node[below=0.2cm of Select, font=\footnotesize\usefont{T1}{phv}{m}{n}, color=gray] {During Training};
  \node[below=0.2cm of Synth, font=\footnotesize\usefont{T1}{phv}{m}{n}, color=gray] {On-Demand};
\end{tikzpicture}
```
:::

Before diving into calculation examples, ensure you have a solid grasp of the core ICR concept.

::: {.callout-checkpoint title="Data Selection Efficiency" collapse="false"}
The goal of data selection is to maximize the **Information-Compute Ratio (ICR)**.

**Metrics**

- [ ] **ICR Application**: Given two training runs with identical accuracy gains but different compute budgets, can you determine which had higher ICR?
- [ ] **Data Efficiency**: Do you understand why a 50% smaller dataset with 2x higher ICR yields the same model for half the training cost?

**The Pipeline**

- [ ] **The Three Stages**: Can you map Static Pruning, Dynamic Selection, and Synthetic Generation to the training lifecycle?
:::

To make the Information-Compute Ratio concrete, consider how coreset selection improves training efficiency on a real workload.

```{python}
#| label: data-selection-setup
#| echo: false

from physx.constants import RESNET50_FLOPs, GFLOPs, IMAGENET_IMAGES
from physx.formatting import fmt

# =============================================================================
# PURPOSE
# =============================================================================
# Purpose: Compute ICR for random vs coreset selection
# Used in: Callout "Computing ICR: Coresets"

# =============================================================================
# INPUT
# =============================================================================
imagenet_size_value = IMAGENET_IMAGES.magnitude
acc_gain_random_value = 5.0
acc_gain_coreset_value = 4.5
coreset_fraction_value = 0.5

# =============================================================================
# PROCESS
# =============================================================================
resnet50_fwd_gflops_value = RESNET50_FLOPs.to(GFLOPs).magnitude
resnet50_fwdbwd_gflops_value = (RESNET50_FLOPs * 2).to(GFLOPs).magnitude
full_epoch_flops_value = imagenet_size_value * resnet50_fwdbwd_gflops_value * 1e9

icr_random_value = acc_gain_random_value / full_epoch_flops_value

coreset_size_value = int(imagenet_size_value * coreset_fraction_value)
coreset_flops_value = coreset_size_value * resnet50_fwdbwd_gflops_value * 1e9
icr_coreset_value = acc_gain_coreset_value / coreset_flops_value
icr_ratio_value = icr_coreset_value / icr_random_value
acc_diff_value = acc_gain_random_value - acc_gain_coreset_value

# =============================================================================
# OUTPUT
# =============================================================================
resnet50_fwd_gflops_str = fmt(RESNET50_FLOPs.to(GFLOPs), precision=1)
resnet50_fwdbwd_gflops_str = fmt((RESNET50_FLOPs * 2).to(GFLOPs), precision=1)
full_epoch_flops_str = f"{full_epoch_flops_value:.2e}"
icr_random_str = f"{icr_random_value:.1e}"
imagenet_size_str = fmt(imagenet_size_value / 1e6, precision=2, unit="M")
coreset_size_str = f"{coreset_size_value / 1000:.0f}K"
coreset_flops_str = f"{coreset_flops_value:.1e}"
icr_coreset_str = f"{icr_coreset_value:.1e}"
icr_ratio_str = fmt(icr_ratio_value, precision=1, commas=False)
acc_gain_random_str = fmt(acc_gain_random_value, precision=1, commas=False)
acc_gain_coreset_str = fmt(acc_gain_coreset_value, precision=1, commas=False)
acc_diff_str = fmt(acc_diff_value, precision=1, commas=False)
coreset_pct_str = fmt(coreset_fraction_value * 100, precision=0, commas=False)
```

::: {.callout-example title="Computing ICR: Coresets"}
**Scenario**: Training our **ResNet-50 Lighthouse model** (@sec-dnn-architectures) on ImageNet for one epoch. We compare random batch selection versus EL2N-based coreset selection (EL2N, or Error L2-Norm, scores each sample by how uncertain the model's prediction is; it is defined formally in @sec-data-selection-coreset-selection-algorithms-a520). ResNet-50's compute-bound nature (high **arithmetic intensity**) makes it an ideal candidate for data selection optimization: reducing dataset size directly reduces training FLOPs with minimal I/O impact.

**Setup**:

- Dataset: ImageNet (`{python} imagenet_size_str` images)
- Model: ResNet-50 Lighthouse (~`{python} resnet50_fwd_gflops_str` GFLOPs per forward pass, ~`{python} resnet50_fwdbwd_gflops_str` GFLOPs forward + backward)
- One epoch: `{python} imagenet_size_str` × `{python} resnet50_fwdbwd_gflops_str` GFLOPs = **`{python} full_epoch_flops_str` FLOPs**
- Accuracy improvement per epoch (early training): ~`{python} acc_gain_random_str`% points

**Random Selection (baseline)**:

- Process all `{python} imagenet_size_str` samples uniformly
- Accuracy gain: `{python} acc_gain_random_str` percentage points
- ICR_random = `{python} acc_gain_random_str` / (`{python} full_epoch_flops_str`) = **`{python} icr_random_str` per FLOP**

**EL2N Coreset (`{python} coreset_pct_str`% of data)**:

- Process `{python} coreset_size_str` high-uncertainty samples selected by EL2N scoring
- Coreset focuses on decision boundary samples
- Accuracy gain: `{python} acc_gain_coreset_str` percentage points (90% of full data performance)
- Compute: `{python} coreset_size_str` × `{python} resnet50_fwdbwd_gflops_str` GFLOPs = **`{python} coreset_flops_str` FLOPs**
- ICR_coreset = `{python} acc_gain_coreset_str` / (`{python} coreset_flops_str`) = **`{python} icr_coreset_str` per FLOP**

**Result**: The coreset achieves **`{python} icr_ratio_str`× higher ICR**, nearly twice the learning per FLOP, by eliminating low-information "easy" samples that contribute little to the decision boundary. The `{python} acc_diff_str` percentage point accuracy difference is often acceptable given the `{python} coreset_pct_str`% compute savings.
:::

The remainder of this chapter explores each stage of the three-stage optimization pipeline introduced above—static pruning, dynamic selection, and synthetic generation—in depth. We begin with static pruning, the techniques that can reduce a dataset by 30 to 50 percent before training even begins.

## Static Data Pruning: Pre-Training Filtration {#sec-data-selection-static-data-pruning-pretraining-filtration-d6e6}

Before a single gradient is computed, significant efficiency gains are available by removing low-value samples from the dataset. This pre-training filtration reduces total computation without affecting, and sometimes improving, final model accuracy. The techniques in this section operate on the dataset itself, requiring no changes to the training loop or model architecture.

### The Case for Smaller Datasets {#sec-data-selection-case-smaller-datasets-0336}

The most counterintuitive finding in data selection is that training on *less* data often produces models just as accurate as training on the full dataset. Practitioners have long assumed that more data yields better performance, and while this holds in many scenarios, it obscures a critical reality: typical large-scale datasets contain massive redundancy. Empirical studies on coreset selection and data pruning have consistently demonstrated this redundancy across standard benchmarks:

- **CIFAR-10**: Studies using gradient-based selection (EL2N, GraNd) [@paul2021deep] have shown that training on 50% of CIFAR-10 with carefully selected samples matches the accuracy of the full dataset. Aggressive pruning can reach 10–30% of samples while matching 90%+ of original performance.
- **ImageNet-1K**: Pruning is harder on less redundant datasets. However, researchers have demonstrated that 20–30% of ImageNet can be pruned with negligible loss, and up to 50% reduction is possible with a small accuracy trade-off (~1% point), yielding 2$\times$ fewer training FLOPs [@paul2021deep; @sorscher2022beyond].
- **Large Language Model Corpora**: Web-scraped datasets like The Pile and C4 contain substantial exact and near-duplicate content. Deduplication studies [@lee2022deduplicating] report 10–30% redundancy ratios, with deduplicated training yielding *better* downstream performance (less memorization, more generalization).

These numbers are benchmark-specific. Gains from pruning depend on the dataset's intrinsic redundancy, the selection algorithm, and the model architecture; always validate on your specific task before deploying aggressive pruning in production. The key insight remains: not all data points provide equal value for training.

Why does this heterogeneity exist? The answer lies in how neural networks learn decision boundaries. Most samples fall far from any class boundary: a picture of a dog in good lighting is obviously a dog. These "easy" examples provide diminishing returns after the first few epochs because the model has already mastered them. The informative samples cluster near boundaries where classes become ambiguous. Beyond sample redundancy, label quality also dramatically affects data requirements. The following analysis quantifies this effect.

::: {.callout-notebook title="The Data Quality Multiplier"}
**The Physics of Noise**: Why is one clean sample worth 100 noisy ones?

**The Math**: Classical learning theory (for convex optimization with SGD) tells us that convergence rates depend on label noise. While deep learning operates in a non-convex regime, the qualitative relationship holds broadly.
1.  **Clean Data**: Convergence rate is typically $O(1/N)$. To halve the error, you need **2x** data.
2.  **Noisy Data**: Convergence rate drops to $O(1/\sqrt{N})$. To halve the error, you need **4x** data.

```{python}
#| label: data-quality-multiplier-calc
#| echo: false
from physx.formatting import fmt

epsilon_value = 0.01
n_clean_value = 100
n_noisy_value = 10000
ratio_value = n_noisy_value / n_clean_value

epsilon_str = fmt(epsilon_value, precision=2, commas=False)
epsilon_pct_str = fmt(epsilon_value * 100, precision=0, commas=False)
n_clean_str = fmt(n_clean_value, precision=0, commas=False)
n_noisy_str = fmt(n_noisy_value, precision=0, commas=True)
ratio_str = fmt(ratio_value, precision=0, commas=False)
```

**The Multiplier**:
To reach a target error $\epsilon$:

*   $N_{clean} \propto 1/\epsilon$
*   $N_{noisy} \propto 1/\epsilon^2$

**Example**: For target error $\epsilon$ = `{python} epsilon_str` (`{python} epsilon_pct_str`%):

*   $N_{clean}$ ≈ `{python} n_clean_str`
*   $N_{noisy}$ ≈ `{python} n_noisy_str`
*   **Ratio**: `{python} ratio_str`× more data required if noisy.

**The Systems Conclusion**: Cleaning your data (removing label noise) is a **`{python} ratio_str`x compute accelerator**.
:::

The practical question then becomes: *how* do we identify which samples to keep?

### Coreset Selection Algorithms {#sec-data-selection-coreset-selection-algorithms-a520}

**Coreset selection**\index{Static Pruning!coreset selection}[^fn-coreset] answers this question by identifying a small subset of data that preserves the statistical properties of the entire dataset.

[^fn-coreset]: **Coreset**: The term "coreset" combines "core" and "set," reflecting its purpose as a core representative subset. The concept emerged from computational geometry in the early 2000s, where researchers sought provably small subsets that approximate solutions to geometric optimization problems. For ML applications, coresets provide theoretical guarantees: a well-constructed coreset of size independent of the original dataset can approximate the full dataset's loss function within a factor of $(1 + \delta)$.

The goal is to find a compact set of examples that allows a model to generalize as well as it would if trained on the full dataset. Several algorithmic families have proven effective, each with distinct computational trade-offs.

Geometry-based methods select samples that cover the data distribution without requiring any model training. The k-Center algorithm[^fn-k-center] (also known as Facility Location) selects samples that minimize the maximum distance from any point to its nearest selected center, ensuring coverage of the entire data manifold.

[^fn-k-center]: **k-Center Algorithm**: Dorit Hochbaum and David Shmoys established the modern approach to this problem in 1985 [@hochbaum1985best], proving that their 2-approximation algorithm is "best possible": no polynomial-time algorithm can achieve a better approximation factor unless P=NP. The algorithm's origin in facility location (placing warehouses to minimize maximum customer distance) explains why it transfers well to coreset selection: both seek coverage of a space with minimal representatives.

Herding takes a different approach, iteratively selecting samples whose features best approximate the mean of the full dataset, thereby maintaining distributional fidelity. These methods are computationally attractive because they operate purely on feature representations, but they ignore label information entirely.

Gradient-based methods offer higher selection quality by using training dynamics to identify important samples, though they require training a proxy model first. GraNd (Gradient Normed) and EL2N (Error L2-Norm)[^fn-el2n-grand] score samples by gradient magnitude or prediction error early in training; high-scoring samples lie near the decision boundary and are most informative for learning. Crucially, these scores transfer across architectures: scores computed on a smaller model like ResNet-18 predict importance for larger models like ResNet-50, enabling inexpensive proxy-based selection. Forgetting Events[^fn-forgetting] tracks how often a sample is "forgotten" (correctly classified, then later misclassified) during training, identifying harder and more valuable examples.

[^fn-el2n-grand]: **EL2N and GraNd**: Introduced by Mansheej Paul and colleagues at NeurIPS 2021 in their paper "Deep Learning on a Data Diet." These scores identify important examples using only information from the first few training epochs, unlike forgetting-based methods that require full training. The key insight: samples the model finds uncertain early in training remain important throughout, and these scores transfer across architectures. Scores computed on ResNet-18 predict importance for ResNet-50.

[^fn-forgetting]: **Forgetting Events**: Coined by Mariya Toneva and colleagues at ICLR 2019. A "forgetting event" occurs when a sample transitions from correctly to incorrectly classified during training (the opposite of a learning event). The surprising finding: a large fraction of samples are never forgotten once learned, and these "unforgettable" examples can be safely pruned with minimal accuracy impact.

These gradient-based approaches generally outperform geometry-based methods in selection quality but incur the overhead of proxy model training. This quality advantage justifies the proxy training overhead for most production workloads, as @tbl-coreset-comparison quantifies:

| **Method**     | **Compute Cost** | **Requires Training** | **Best For**          | **Limitation**            |
|:-------------|:---------------|:--------------------|:--------------------|:------------------------|
| **k-Center**   | O(N²) or O(NK)   | No                    | Coverage, exploration | Ignores label information |
| **Herding**    | O(NK)            | No                    | Distribution matching | Assumes Gaussian-like     |
| **GraNd**      | O(epochs × N)    | Yes (few epochs)      | Decision boundaries   | Requires proxy training   |
| **Forgetting** | O(full training) | Yes (full)            | Hard examples         | Expensive to compute      |
| **EL2N**       | O(epochs × N)    | Yes (few epochs)      | Uncertainty sampling  | Best with proxy model     |

: **Coreset Selection Algorithm Comparison.** N = dataset size, K = coreset size. Gradient-based methods generally outperform geometry-based methods but require proxy model training. {#tbl-coreset-comparison .striped .hover}

@fig-coreset-selection makes the core insight behind coreset methods concrete—compare the two panels: random sampling (left) selects points uniformly across the feature space, capturing many samples deep within class regions where the model is already confident. Coreset selection (right) concentrates the selection budget on samples near the decision boundary (the yellow uncertainty band) where the model's predictions are most uncertain. These boundary samples are precisely where additional training provides the most learning signal.

::: {#fig-coreset-selection fig-env="figure" fig-pos="htb" fig-cap="**Coreset Selection Strategy**: Random sampling (left) selects uniformly, wasting budget on easy samples far from the decision boundary. Coreset selection (right) prioritizes samples near the boundary where the model is uncertain, capturing more information per sample." fig-alt="Two scatter plots with a diagonal decision boundary. Left plot shows random dots selected. Right plot highlights dots near the boundary as selected."}
```{.tikz}
\begin{tikzpicture}[font=\small\usefont{T1}{phv}{m}{n}]
% Left plot: Random Sampling
\begin{scope}
    \node[font=\bfseries\usefont{T1}{phv}{m}{n}] at (2.5, 4.2) {Random Sampling};

    % Decision boundary
    \draw[thick, dashed, gray] (0, 0) -- (5, 5);

    % Class A points (below line) - circles
    \foreach \x/\y in {0.5/0.2, 1.0/0.5, 0.8/1.2, 1.5/0.8, 2.0/1.0,
                       2.5/1.5, 1.2/0.3, 0.3/0.8, 1.8/1.5, 2.2/0.5,
                       3.0/2.0, 3.5/2.5, 2.8/1.8, 3.2/1.2, 4.0/2.8} {
        \fill[blue!60] (\x, \y) circle (2pt);
    }

    % Class B points (above line) - triangles
    \foreach \x/\y in {0.5/1.5, 1.0/2.0, 0.3/2.5, 1.5/2.5, 2.0/3.0,
                       2.5/3.5, 1.2/3.2, 0.8/3.8, 1.8/3.5, 2.2/4.0,
                       3.0/4.0, 3.5/4.5, 2.8/3.8, 3.2/4.2, 4.0/4.5} {
        \fill[red!60] (\x, \y) circle (2pt);
    }

    % Randomly selected (circled) - some easy, some hard
    \foreach \x/\y in {0.5/0.2, 1.5/2.5, 3.0/2.0, 0.8/3.8, 2.2/0.5} {
        \draw[thick, orange] (\x, \y) circle (5pt);
    }

    % Axis
    \draw[->] (0, 0) -- (5.2, 0) node[right, font=\tiny\usefont{T1}{phv}{m}{n}] {$x_1$};
    \draw[->] (0, 0) -- (0, 5.2) node[above, font=\tiny\usefont{T1}{phv}{m}{n}] {$x_2$};

    % Label
    \node[font=\footnotesize\usefont{T1}{phv}{m}{n}, orange] at (2.5, -0.5) {Selected (random)};
\end{scope}

% Right plot: Coreset Selection
\begin{scope}[xshift=7cm]
    \node[font=\bfseries\usefont{T1}{phv}{m}{n}] at (2.5, 4.2) {Coreset Selection};

    % Decision boundary
    \draw[thick, dashed, gray] (0, 0) -- (5, 5);

    % Uncertainty band near boundary
    \fill[yellow!20] (0, 0) -- (0, 1) -- (4, 5) -- (5, 5) -- (5, 4) -- (1, 0) -- cycle;
    \node[font=\tiny\usefont{T1}{phv}{m}{n}, fill=white, inner sep=1pt] at (3.5, 3.0) {High uncertainty};

    % Class A points (below line) - circles
    \foreach \x/\y in {0.5/0.2, 1.0/0.5, 0.8/1.2, 1.5/0.8, 2.0/1.0,
                       2.5/1.5, 1.2/0.3, 0.3/0.8, 1.8/1.5, 2.2/0.5,
                       3.0/2.0, 3.5/2.5, 2.8/1.8, 3.2/1.2, 4.0/2.8} {
        \fill[blue!60] (\x, \y) circle (2pt);
    }

    % Class B points (above line) - triangles
    \foreach \x/\y in {0.5/1.5, 1.0/2.0, 0.3/2.5, 1.5/2.5, 2.0/3.0,
                       2.5/3.5, 1.2/3.2, 0.8/3.8, 1.8/3.5, 2.2/4.0,
                       3.0/4.0, 3.5/4.5, 2.8/3.8, 3.2/4.2, 4.0/4.5} {
        \fill[red!60] (\x, \y) circle (2pt);
    }

    % Coreset selected (near boundary) - circled
    \foreach \x/\y in {0.8/1.2, 2.5/1.5, 3.0/2.0, 1.0/2.0, 2.0/3.0} {
        \draw[thick, green!60!black] (\x, \y) circle (5pt);
    }

    % Axis
    \draw[->] (0, 0) -- (5.2, 0) node[right, font=\tiny\usefont{T1}{phv}{m}{n}] {$x_1$};
    \draw[->] (0, 0) -- (0, 5.2) node[above, font=\tiny\usefont{T1}{phv}{m}{n}] {$x_2$};

    % Label
    \node[font=\footnotesize\usefont{T1}{phv}{m}{n}, green!60!black] at (2.5, -0.5) {Selected (boundary)};
\end{scope}
\end{tikzpicture}
```
:::

Given these trade-offs, most practitioners find that EL2N with a small proxy model offers the best balance of selection quality and computational cost. The approach is straightforward: train a lightweight model (for example, ResNet-18 instead of ResNet-50) for 5 to 10 epochs, compute EL2N scores for all samples, then select the highest-scoring subset. The proxy does not need to be accurate; it only needs to identify which samples are hard. This upfront investment in proxy training typically yields substantial returns when the coreset reduces subsequent training by 50% or more. The following example illustrates this workflow in a concrete scenario.

```{python}
#| label: coreset-practice-calc
#| echo: false
from physx.formatting import fmt

n_train_images_value = 1_000_000
coreset_fraction_value = 0.1
n_coreset_value = int(n_train_images_value * coreset_fraction_value)
n_epochs_proxy_value = 5

n_train_images_str = fmt(n_train_images_value / 1e6, precision=0, unit=" million")
coreset_fraction_pct_str = fmt(coreset_fraction_value * 100, precision=0, commas=False)
n_coreset_str = fmt(n_coreset_value, precision=0, commas=True)
n_epochs_proxy_str = fmt(n_epochs_proxy_value, precision=0, commas=False)
```

::: {.callout-example title="Coreset Selection in Practice"}
**Scenario**: You have `{python} n_train_images_str` training images and want to reduce to `{python} n_coreset_str` (`{python} coreset_fraction_pct_str`%) for faster experimentation.

**Naive Approach**: Random sampling loses rare classes and edge cases.

**Coreset Approach**:

1. Train a small proxy model for `{python} n_epochs_proxy_str` epochs
2. Compute EL2N scores for all samples
3. Select the `{python} n_coreset_str` samples with highest uncertainty
4. Train your full model on this coreset

**Result**: The coreset often achieves **higher accuracy** than random sampling because it focuses on the decision boundary rather than redundant "easy" examples.
:::

@lst-el2n-coreset demonstrates how to compute EL2N scores and select a coreset using a lightweight proxy model. The code shows two functions: `compute_el2n_scores` trains a proxy model briefly and measures prediction confidence via L2 distance from one-hot labels, while `select_coreset` retains only the highest-uncertainty samples.

::: {#lst-el2n-coreset lst-cap="**EL2N-Based Coreset Selection**: Computing uncertainty scores with a proxy model enables 10x data reduction while preserving accuracy. The `compute_el2n_scores` function trains a small model for a few epochs, then measures prediction confidence via L2 distance from one-hot labels. High scores indicate uncertain samples near decision boundaries. The `select_coreset` function retains only these informative samples, discarding redundant easy examples."}
```{.python}
def compute_el2n_scores(model, dataloader, num_epochs=5):
    """Compute EL2N scores.

    Returns L2 norm of (prediction - one_hot_label).
    """
    # Train proxy model for a few epochs to get meaningful predictions
    train_proxy(model, dataloader, num_epochs)

    scores = []
    model.eval()
    for x, y in dataloader:
        logits = model(x)
        probs = softmax(logits, dim=1)
        # One-hot encode labels
        one_hot = zeros_like(probs).scatter_(1, y.unsqueeze(1), 1)
        # EL2N score = L2 distance from confident prediction
        el2n = (probs - one_hot).norm(dim=1)  # High = uncertain
        scores.extend(el2n.tolist())
    return scores


def select_coreset(scores, dataset, fraction=0.1):
    """Select top-k highest-scoring (most uncertain) samples."""
    k = int(len(dataset) * fraction)
    # Sort by score descending (highest uncertainty first)
    indices = argsort(scores, descending=True)[:k]
    return Subset(dataset, indices)


# Usage: 10x data reduction with minimal accuracy loss
scores = compute_el2n_scores(proxy_model, full_loader)
coreset = select_coreset(scores, full_dataset, fraction=0.1)
train_full_model(model, coreset)  # 10x faster training
```
:::

### Data Deduplication {#sec-data-selection-data-deduplication-9035}

While coreset selection identifies which samples to keep based on their informativeness, a complementary approach targets what to remove: exact and near-duplicates. Deduplication provides immediate efficiency gains with no accuracy penalty and requires no model training. This makes it the most accessible optimization in data selection, offering guaranteed compute savings with zero risk of degrading model quality.

The simplest form of deduplication (introduced as a data engineering pipeline stage in @sec-data-engineering-ml-systematic-data-processing-aebc, and here elevated to an optimization lever) uses hash-based methods for exact matches. By computing a cryptographic hash (MD5 or SHA-256) for each sample and removing those with identical hashes, practitioners can eliminate byte-for-byte duplicates that inevitably accumulate in large web-scraped corpora. This process is computationally cheap, scaling linearly with dataset size, and can be parallelized trivially.

Near-duplicate detection addresses the more subtle problem of semantically redundant content that differs at the byte level. For text, MinHash[^fn-minhash] with **Locality-Sensitive Hashing** (LSH) approximates Jaccard similarity efficiently, detecting paraphrased or lightly edited content. The core idea is to create compact "fingerprints" of each document such that similar documents produce similar fingerprints with high probability, enabling fast approximate similarity detection without comparing every document pair.

[^fn-minhash]: **MinHash**: Invented by Andrei Broder in 1997 [@broder1997resemblance], originally to detect duplicate web pages for the AltaVista search engine. The algorithm uses random hash functions to create compact "signatures" that preserve set similarity: two documents with similar content produce similar signatures with high probability. Broder received the 2012 ACM Kanellakis Award for this work, recognizing its foundational impact on web-scale similarity detection.

For images, perceptual hashing produces signatures robust to minor transformations like resizing and compression, identifying visually identical images stored in different formats. Embedding-based similarity offers the most powerful detection by computing dense representations (CLIP for images, sentence transformers for text) and clustering similar items, though this approach incurs higher computational overhead.

For foundation model pre-training, deduplication has become essential rather than optional. Studies on GPT-3 and LLaMA training demonstrate that deduplicated data improves both training efficiency and downstream performance by preventing memorization of repeated content. The benefit is twofold: fewer wasted FLOPs on redundant samples, and better generalization because the model sees more diverse examples per training token.

Deduplication benefits extend beyond text corpora. The DLRM lighthouse presents a unique variant of this challenge centered on *embedding deduplication*.

::: {.callout-lighthouse title="DLRM and Embedding Deduplication"}
Our **DLRM Lighthouse model** (@sec-dnn-architectures) presents a unique deduplication challenge. Recommendation systems are memory capacity-bound, with embedding tables consuming terabytes of storage for billions of user/item IDs. Much of this capacity is wasted on *cold embeddings*, IDs that appear rarely in training data.

Data selection for DLRM focuses on **interaction deduplication** (removing redundant user-item pairs) and **embedding pruning** (removing or sharing cold embeddings). A 20% reduction in unique interactions can reduce embedding table size by 30–40%, directly addressing DLRM's primary bottleneck: memory capacity rather than compute.
:::

### Data Pruning by Quality {#sec-data-selection-data-pruning-quality-72ea}

Deduplication removes redundant samples, but a third category of problematic data remains: samples that actively harm learning. Quality-based pruning eliminates samples that either contribute no meaningful signal or introduce contradictory information that confuses the optimization process.

Label error detection represents the most impactful form of quality pruning. Tools like Cleanlab identify samples where the assigned label is likely incorrect based on model confidence patterns across training. A sample that the model consistently predicts as class A but is labeled class B either represents a hard case near the decision boundary or, more commonly, an annotation mistake. Removing or correcting these mislabeled samples prevents the model from learning contradictory signals that degrade its decision boundary.

Outlier removal addresses a different pathology: samples far from any cluster center in feature space. While outliers might represent valuable edge cases, they more often indicate noise, annotation errors, or data corruption. The key is distinguishing between informative outliers (rare but valid examples of a class) and noise (samples that do not belong to any class). Conservative thresholds help avoid discarding genuinely rare examples.

Low-information filtering applies domain-specific heuristics to remove samples that lack sufficient signal for learning. For text corpora, this means removing documents below a perplexity threshold or with low semantic coherence, often indicative of machine-generated spam or garbled content. For image datasets, filtering targets blurry, corrupted, or near-uniform samples that provide little visual information.

Together, these three static pruning techniques (coreset selection, deduplication, and quality filtering) demonstrate that careful curation before training yields significant efficiency gains. The compute savings are multiplicative across the entire training process: a 50% dataset reduction means 50% fewer forward passes, backward passes, and gradient updates across all training epochs. For a model trained for 100 epochs, this translates to 50 epochs worth of saved compute, yielding substantial reductions in both training time and energy consumption.

## Dynamic Data Selection: Training-Time Optimization {#sec-data-selection-dynamic-data-selection-trainingtime-optimization-cd62}

Static pruning answers a question about *what* to keep: which samples deserve inclusion in the training set? But this framing assumes the answer is fixed. Static pruning commits to a dataset before training begins, but what if the optimal training samples change as the model learns? Early in training, the model benefits from diverse coverage to build broad feature representations; later, it benefits from focusing on hard examples near the decision boundary to refine its predictions. Dynamic selection exploits this insight by optimizing which samples to use *during* training, adapting the data diet based on the model's evolving state.

### Curriculum Learning: Easy to Hard {#sec-data-selection-curriculum-learning-easy-hard-3428}

The first dynamic selection technique, **curriculum learning**\index{Dynamic Selection!curriculum learning}[^fn-curriculum] [@bengio2009curriculum; @soviany2022curriculum], structures the order in which data is presented to the model. Instead of random shuffling, it starts with simpler examples and gradually introduces more complex ones, mirroring how humans learn by mastering fundamentals before advancing to harder material.

[^fn-curriculum]: **Curriculum Learning**: Formalized by Yoshua Bengio and colleagues at ICML 2009, drawing explicit inspiration from human education where students master basics before advanced topics. The paper's key insight was that curriculum learning acts as a "continuation method" for non-convex optimization: starting with easy examples smooths the loss landscape, helping the optimizer find better local minima. The paper has accumulated thousands of citations, reflecting its influence on training methodology.

The effectiveness of curriculum learning stems from how neural networks respond to gradient signals at different training stages. Easy examples provide clear, consistent gradients that establish strong feature representations early in training, when the loss landscape is highly irregular. Hard examples introduced too early produce noisy gradient signals that slow convergence or cause the model to memorize outliers rather than learn general patterns. By sequencing examples from easy to hard, curriculum learning smooths the optimization trajectory.

Implementing a curriculum requires two components: a difficulty scorer that ranks samples, and a pacing function that controls how quickly hard samples are introduced. A common choice is linear pacing:

$$
\text{samples}_t = \texttt{sort\_by\_difficulty}[:N \cdot \min(1, t/T_{\text{warmup}})]
$$

where $t$ is the current epoch and $T_{warmup}$ is the epoch at which the full dataset becomes available. Early epochs train on the easiest $N \cdot (t/T_{warmup})$ fraction; after warmup, training proceeds on the full dataset.

The difficulty scorer can be designed in several ways, each with different computational requirements and applicability (@tbl-difficulty-scoring).

| **Strategy**          | **Difficulty Score**                      | **Best For**                                |
|:--------------------|:----------------------------------------|:------------------------------------------|
| **Loss-Based**        | Loss from probe model (low = easy)        | General-purpose; requires probe training    |
| **Confidence-Based**  | Teacher model confidence (high = easy)    | When teacher available; distillation setups |
| **Domain Heuristics** | Sentence length, image complexity         | No extra compute; domain knowledge required |
| **Self-Paced**        | Current model's loss (updated each epoch) | Adaptive; no probe needed                   |

: **Difficulty Scoring Strategies for Curriculum Learning.** Loss-based and confidence-based methods require additional model inference; domain heuristics are free but require expertise; self-paced methods adapt dynamically during training. {#tbl-difficulty-scoring .striped .hover}

From a systems perspective, curriculum learning improves convergence by reducing wasted gradient updates on samples the model cannot yet learn from. The Information-Compute Ratio is higher in early training because easy samples provide strong learning signal relative to their compute cost. The efficiency gains manifest as faster convergence to target accuracy, not higher final accuracy.

@tbl-curriculum-benchmarks summarizes measured speedups from curriculum learning across standard benchmarks:

```{python}
#| label: curriculum-benchmarks-calc
#| echo: false
from physx.formatting import fmt

cifar10_baseline_epochs = 150
cifar10_curriculum_epochs = 115
cifar10_speedup_pct = (cifar10_baseline_epochs - cifar10_curriculum_epochs) / cifar10_baseline_epochs * 100

cifar100_baseline_epochs = 220
cifar100_curriculum_epochs = 180
cifar100_speedup_pct = (cifar100_baseline_epochs - cifar100_curriculum_epochs) / cifar100_baseline_epochs * 100

imagenet_baseline_epochs = 90
imagenet_curriculum_epochs = 80
imagenet_speedup_pct = (imagenet_baseline_epochs - imagenet_curriculum_epochs) / imagenet_baseline_epochs * 100

mentornet_baseline_epochs = 90
mentornet_curriculum_epochs = 70
mentornet_speedup_pct = (mentornet_baseline_epochs - mentornet_curriculum_epochs) / mentornet_baseline_epochs * 100

cifar10_speedup_str = fmt(cifar10_speedup_pct, precision=0, commas=False)
cifar100_speedup_str = fmt(cifar100_speedup_pct, precision=0, commas=False)
imagenet_speedup_str = fmt(imagenet_speedup_pct, precision=0, commas=False)
mentornet_speedup_str = fmt(mentornet_speedup_pct, precision=0, commas=False)
```

| **Dataset**   | **Model** | **Pacing Strategy** |                                                                **Epochs to Target Acc.** |                                  **Speedup** |
|:------------|--------:|:------------------|---------------------------------------------------------------------------------------:|-------------------------------------------:|
| **CIFAR-10**  | ResNet-18 | Linear warmup       |     `{python} cifar10_curriculum_epochs` vs. `{python} cifar10_baseline_epochs` baseline |   **`{python} cifar10_speedup_str`%** faster |
| **CIFAR-100** | ResNet-32 | Self-paced          |   `{python} cifar100_curriculum_epochs` vs. `{python} cifar100_baseline_epochs` baseline |  **`{python} cifar100_speedup_str`%** faster |
| **ImageNet**  | ResNet-50 | Loss-based          |   `{python} imagenet_curriculum_epochs` vs. `{python} imagenet_baseline_epochs` baseline |  **`{python} imagenet_speedup_str`%** faster |
| **ImageNet**  | ResNet-50 | MentorNet (noisy)   | `{python} mentornet_curriculum_epochs` vs. `{python} mentornet_baseline_epochs` baseline | **`{python} mentornet_speedup_str`%** faster |

: **Curriculum Learning Convergence Speedups.** Target accuracy is 95% of final baseline performance. Gains are larger on redundant datasets (CIFAR-10) and noisy datasets (MentorNet removes approximately 40% noise). ImageNet shows smaller gains because the dataset is less redundant. {#tbl-curriculum-benchmarks .striped .hover}

The table reveals an important pattern: curriculum learning gains are **inversely proportional to dataset quality**. On highly curated datasets like ImageNet, the `{python} imagenet_speedup_str`% speedup is modest. On noisy or redundant data, gains can exceed 20%. The optimal ordering is also task-dependent: **anti-curriculum** (hard examples first) can work when the decision boundary is complex and easy examples contribute little to defining it, while **self-paced learning** lets the model dynamically adjust difficulty based on its current loss, eliminating the need to pre-define a curriculum. Empirically, self-paced methods often match or exceed hand-designed curricula.

### Active Learning: Human-in-the-Loop {#sec-data-selection-active-learning-humanintheloop-a9fa}

Curriculum learning optimizes the order in which samples are presented but assumes all samples are already labeled. This assumption breaks down in specialized fields such as medical diagnosis, autonomous driving, and scientific research, where labeling requires domain expertise and can cost \$5–\$100 or more per sample. Rather than labeling everything upfront, **active learning**\index{Dynamic Selection!active learning}[^fn-active-learning-theory] [@settles2009active; @ren2021survey] shifts the optimization target: instead of choosing which labeled samples to train on, it chooses which unlabeled samples are worth labeling at all.

[^fn-active-learning-theory]: **Active Learning**: The concept traces to statistical experimental design, but Dana Angluin's work on learning from queries [@angluin1988queries] established theoretical foundations for machine learning. The term "active" contrasts with "passive" learning from pre-labeled data: the learner actively queries an oracle (the human annotator or labeling source) rather than passively receiving examples. Early work in the 1990s demonstrated that active selection could achieve the same accuracy as passive learning with exponentially fewer labels in favorable cases.

Notice the fundamental shift from static pruning: rather than discarding samples permanently, active learning maintains an unlabeled pool and queries it strategically over time. Follow the cycle in @fig-active-learning-loop: the model's current uncertainty determines what gets labeled next, creating a feedback loop where each labeling round improves the model's ability to identify what it still needs to learn.

::: {#fig-active-learning-loop fig-cap="**Active Learning Loop**: Instead of labeling all data, the model selects the most 'confusing' or informative samples from an unlabeled pool. These samples are sent to an Oracle (human annotator) and added to the training set. The model is retrained, and the cycle repeats, creating a feedback loop that maximizes information gain per label." fig-alt="A cycle diagram: Unlabeled Pool -> Selection Strategy -> Oracle -> Labeled Set -> Model Training -> back to Selection Strategy."}
```{.tikz}
\begin{tikzpicture}[font=\small\usefont{T1}{phv}{m}{n}, >={Stealth[length=3mm]}]
  % Nodes arranged in a circle
  \node[draw=black!70, cylinder, shape border rotate=90, aspect=0.25, fill=gray!10, minimum height=1.5cm, minimum width=1.2cm, align=center, line width=0.75pt] (Pool) at (0, 2) {Unlabeled\\Pool};

  \node[draw=BlueLine, rectangle, rounded corners, fill=BlueL, minimum height=1cm, align=center, line width=0.75pt] (Select) at (4, 2) {Selection\\Strategy};

  \node[draw=OrangeLine, circle, fill=OrangeL, minimum size=1.5cm, align=center, line width=0.75pt] (Oracle) at (4, -1) {Oracle\\(Human)};

  \node[draw=GreenLine, rectangle, fill=GreenL, minimum height=1cm, align=center, line width=0.75pt] (Train) at (0, -1) {Training\\Set};

  \node[draw=RedLine, rectangle, fill=RedL, minimum height=1cm, align=center, line width=0.75pt] (Model) at (-2, 0.5) {Model};

  % Edges representing the flow
  \draw[->, thick, black!70] (Pool) -- node[above, font=\footnotesize\usefont{T1}{phv}{m}{n}] {Query} (Select);
  \draw[->, thick, black!70] (Select) -- node[right, font=\footnotesize\usefont{T1}{phv}{m}{n}] {Uncertainty} (Oracle);
  \draw[->, thick, black!70] (Oracle) -- node[below, font=\footnotesize\usefont{T1}{phv}{m}{n}] {Labels} (Train);
  \draw[->, thick, black!70] (Train) -- node[left, font=\footnotesize\usefont{T1}{phv}{m}{n}] {Train} (Model);
  \draw[->, thick, dashed, black!70] (Model) |- node[near start, left, font=\footnotesize\usefont{T1}{phv}{m}{n}] {Update} (Select);
\end{tikzpicture}
```
:::

The effectiveness of active learning depends critically on the query strategy used to select samples for annotation. The simplest approach, uncertainty sampling, selects samples where the model is least confident, such as predictions near 0.5 probability for binary classification. This strategy is computationally cheap and effective in practice. Query-by-committee extends this idea by training multiple models and selecting samples where they disagree most, capturing epistemic uncertainty that a single model might miss.

For practitioners willing to invest more compute, expected model change selects samples that would cause the largest gradient update if labeled. This approach provides a theoretically grounded but expensive alternative. Diversity sampling complements uncertainty-based methods by selecting samples dissimilar from currently labeled data, ensuring the labeled set covers the full input space rather than clustering around ambiguous regions.

Active learning is particularly valuable in domains where labeling requires expertise. In medical imaging, for instance, an AI system diagnosing diseases from X-rays may be confident on common conditions but uncertain about rarer cases. By focusing human annotation on these ambiguous cases, active learning optimizes the use of expensive expert time while accelerating model improvement.

The economic implications are substantial. In production settings, labeling costs often dwarf compute costs because a specialist's time is far more expensive than GPU hours. These query strategies drive each iteration of the active learning loop in @fig-active-learning-loop, and the *active learning ROI* can exceed 10$\times$, as the following example demonstrates.

::: {.callout-notebook title="The Active Learning ROI"}
```{python}
#| label: active-learning-roi-calc
#| echo: false
from physx.formatting import fmt

n_unlabeled_value = 1_000_000
cost_per_label_value = 5.00
budget_value = 500_000
deadline_months_value = 1

cost_all_value = n_unlabeled_value * cost_per_label_value
n_random_value = int(budget_value / cost_per_label_value)
n_random_pct_value = n_random_value / n_unlabeled_value * 100

n_active_value = 50_000
cost_active_value = n_active_value * cost_per_label_value
cost_active_pct_value = (budget_value - cost_active_value) / budget_value * 100
speedup_value = n_unlabeled_value / n_active_value

cost_saving_value = cost_all_value - cost_active_value

n_unlabeled_str = fmt(n_unlabeled_value / 1e6, precision=0, unit=" Million")
cost_per_label_str = fmt(cost_per_label_value, precision=2, commas=False)
budget_str = fmt(budget_value, precision=0, commas=True)
cost_all_str = fmt(cost_all_value, precision=0, commas=True)
n_random_str = fmt(n_random_value, precision=0, commas=True)
n_random_pct_str = fmt(n_random_pct_value, precision=0, commas=False)

n_active_str = fmt(n_active_value, precision=0, commas=True)
cost_active_str = fmt(cost_active_value, precision=0, commas=True)
cost_active_pct_str = fmt(cost_active_pct_value, precision=0, commas=False)
speedup_str = fmt(speedup_value, precision=0, commas=False)
cost_saving_str = fmt(cost_saving_value / 1e6, precision=2, unit=" Million")
```

**Problem**: You are building a medical diagnostic AI. You have a pool of **`{python} n_unlabeled_str` unlabeled scans**. A specialist doctor charges **\$`{python} cost_per_label_str`** to label one scan. You have a budget of **\$`{python} budget_str`** and a deadline of **`{python} deadline_months_value` month**.

**Scenario A: Naive Labeling**

1.  **Cost**: Labeling all 1M scans would cost **\$`{python} cost_all_str`** (10× over budget).
2.  **Time**: You can only afford to label `{python} n_random_str` random scans.
3.  **Result**: Your model misses rare pathologies because they weren't in the random `{python} n_random_pct_str`%.

**Scenario B: Active Learning**

1.  **Strategy**: Use an uncertainty-based selection to pick the **`{python} n_active_str`** "hardest" scans for the doctor to label.
2.  **Cost**: `{python} n_active_str` × `{python} cost_per_label_str` = **\$`{python} cost_active_str`**. (`{python} cost_active_pct_str`% under budget).
3.  **Training Speed**: With `{python} speedup_str`× less data, each training epoch is **`{python} speedup_str`× faster**.
4.  **Result**: Empirical studies suggest that these `{python} n_active_str` "high-information" samples often achieve higher accuracy than `{python} n_random_str` random samples.

**The Systems Conclusion**: Data Selection is not just a "data trick"; it is a **`{python} speedup_str`× compute accelerator** and a **\$`{python} cost_saving_str`** cost-saving measure.
:::

Compare the two curves in @fig-active-learning-multiplier: Active Learning shifts the learning curve to the left, achieving target accuracy with far fewer samples than random selection. The curves are illustrative to highlight the qualitative gap.

```{python}
#| label: fig-active-learning-multiplier
#| echo: false
#| fig-cap: "**The Active Learning Multiplier**: Model Accuracy vs. Number of Labeled Samples (Log Scale). Random sampling (gray dashed) yields linear improvements, often requiring massive datasets to capture rare edge cases. Active Learning (green solid) targets informative samples, reaching the same accuracy with fewer labels. Curves are illustrative to show the qualitative advantage."
#| fig-alt: "Line chart of Accuracy vs Labeled Samples (log scale). Green line (Active Learning) rises much faster than gray line (Random Sampling). Shaded area between them shows cost savings."

import numpy as np
from physx import viz

fig, ax, COLORS, plt = viz.setup_plot()

# =============================================================================
# PLOT: The Active Learning Multiplier
# =============================================================================
samples = np.logspace(2, 4, 100)
acc_random = 50 + 40 * np.log10(samples/100 + 1) / np.log10(101)
acc_active = 50 + 45 * (1 - np.exp(-samples/1000))
acc_active = np.minimum(acc_active, 95)
acc_random = np.minimum(acc_random, 95)

ax.plot(samples, acc_random, '--', color=COLORS['grid'], label='Random Sampling', linewidth=2)
ax.plot(samples, acc_active, '-', color=COLORS['GreenLine'], label='Active Learning', linewidth=2.5)
ax.fill_between(samples, acc_random, acc_active, color=COLORS['GreenL'], alpha=0.3)

ax.set_xscale('log')
ax.set_xlabel('Labeled Samples')
ax.set_ylabel('Accuracy (%)')
ax.annotate("", xy=(2500, 90), xytext=(9000, 90), arrowprops=dict(arrowstyle="->", color='black'))
ax.text(5000, 91, "4x Data Efficiency", ha='center', fontsize=9)
ax.legend(loc='lower right', fontsize=8)
plt.show()
```

The benefits of active learning extend beyond cost savings: they enable models to learn from precisely the examples that matter most. The Smart Doorbell Lighthouse illustrates this principle in the context of hard negative mining.

::: {.callout-lighthouse title="Mining for Hard Negatives"}
**The "Hard Negative" Problem**: Our **Smart Doorbell** faces a classic data selection challenge. The vast majority of its video feed is empty (easy negatives) or clearly people (easy positives). The model fails on the 0.01% of "Hard Negatives": statues, posters of people, or laundry piles that cast human-like shadows.

Random sampling will miss these rare failures. Instead, the Wake Vision team uses **Active Learning** to specifically query the Oracle (human reviewers) on low-confidence predictions. If the model sees a "statue" and predicts "Person (51%)", that sample is flagged for labeling. This turns the feedback loop from a random walk into a guided search for the decision boundary, reducing the data required to solve the "statue problem" by orders of magnitude compared to random collection.
:::

### Semi-Supervised Learning: Using Unlabeled Data {#sec-data-selection-semisupervised-learning-leveraging-unlabeled-data-53b7}

Active learning optimizes which samples to label but still requires human annotation for every selected example. A more aggressive approach asks: *can we* extract learning signal from unlabeled data directly? **Semi-supervised learning** addresses this question. It uses a small set of labeled examples to guide learning on a much larger unlabeled pool, typically achieving 80–95% of fully supervised accuracy with only 10–20% of the labels.

The core insight behind semi-supervised learning is that unlabeled data, while it cannot directly teach the mapping from inputs to outputs, contains structural information about the input distribution $P(X)$ that constrains the hypothesis space. A decision boundary that cuts through dense regions of $P(X)$ is unlikely to generalize well because it would assign different labels to similar inputs. Semi-supervised methods use unlabeled data to push decision boundaries toward low-density regions, where class transitions are more likely to occur naturally.

Three main techniques implement this insight. **Pseudo-labeling** takes the most direct approach: train on labeled data, use the model to generate "pseudo-labels" for high-confidence unlabeled predictions, then retrain on both. The confidence threshold is critical: setting it too low introduces label noise that degrades learning, while setting it too high wastes potentially useful data.

**Consistency regularization** takes a different angle by enforcing that the model produces similar predictions for augmented versions of the same input. A robust classifier should be invariant to realistic perturbations like cropping, rotation, or color shifts. Methods like FixMatch combine both approaches, assigning pseudo-labels only to samples where the unaugmented prediction is confident but training the model to predict these labels on strongly augmented versions of the same images.

Label propagation offers a third paradigm through graph-based reasoning: construct a similarity graph over all samples and propagate labels from labeled nodes to their neighbors. This approach works particularly well when the feature space exhibits clear cluster structure.

The systems trade-off in semi-supervised learning is straightforward: it typically achieves the same accuracy as fully supervised training with 5–10$\times$ fewer labels but requires more compute because training processes both labeled and unlabeled samples. Since labeling costs often dominate compute costs in production settings, this trade-off is usually favorable. The results of *FixMatch on CIFAR-10* illustrate this label efficiency concretely.

```{python}
#| label: fixmatch-calc
#| echo: false
from physx.formatting import fmt

cifar10_full_labels = 50000
cifar10_full_acc = 96.1

cifar10_fixmatch_4k_labels = 4000
cifar10_fixmatch_4k_acc = 95.7
cifar10_fixmatch_4k_eff = cifar10_full_labels / cifar10_fixmatch_4k_labels

cifar10_fixmatch_250_labels = 250
cifar10_fixmatch_250_acc = 94.9
cifar10_fixmatch_250_eff = cifar10_full_labels / cifar10_fixmatch_250_labels

cifar10_fixmatch_40_labels = 40
cifar10_fixmatch_40_acc = 88.6
cifar10_fixmatch_40_eff = cifar10_full_labels / cifar10_fixmatch_40_labels

cost_label = 1
cost_gpu_hr = 0.50

supervised_labels = 4000
supervised_label_cost = supervised_labels * cost_label
supervised_compute_cost = 50
supervised_total = supervised_label_cost + supervised_compute_cost

fixmatch_labels = 250
fixmatch_label_cost = fixmatch_labels * cost_label
fixmatch_compute_cost = 250
fixmatch_total = fixmatch_label_cost + fixmatch_compute_cost

fixmatch_compute_multiplier_value = fixmatch_compute_cost / supervised_compute_cost

cost_reduction = supervised_total / fixmatch_total
acc_loss = cifar10_full_acc - cifar10_fixmatch_250_acc

supervised_label_cost_str = fmt(supervised_label_cost, precision=0, commas=True)
supervised_total_str = fmt(supervised_total, precision=0, commas=True)
fixmatch_label_cost_str = fmt(fixmatch_label_cost, precision=0, commas=True)
fixmatch_total_str = fmt(fixmatch_total, precision=0, commas=True)
cost_reduction_str = fmt(cost_reduction, precision=0, commas=False)
acc_loss_str = fmt(acc_loss, precision=1, commas=False)
fixmatch_compute_multiplier_str = fmt(
    fixmatch_compute_multiplier_value, precision=0, commas=False
)

cifar10_full_labels_str = fmt(cifar10_full_labels, precision=0, commas=True)
cifar10_fixmatch_4k_labels_str = fmt(cifar10_fixmatch_4k_labels, precision=0, commas=True)
cifar10_fixmatch_250_labels_str = fmt(cifar10_fixmatch_250_labels, precision=0, commas=True)
cifar10_fixmatch_40_labels_str = fmt(cifar10_fixmatch_40_labels, precision=0, commas=True)
```

::: {.callout-example title="FixMatch on CIFAR-10"}
**FixMatch** [@sohn2020fixmatch] combines pseudo-labeling with consistency regularization to achieve high label efficiency (@tbl-fixmatch-cifar10).

| **Label Budget**                                      | **Method**       |                         **Accuracy** |                                    **Label Efficiency** |
|:----------------------------------------------------|:---------------|-----------------------------------:|------------------------------------------------------:|
| **`{python} cifar10_full_labels_str` (100%)**         | Fully Supervised |         `{python} cifar10_full_acc`% |                                                Baseline |
| **`{python} cifar10_fixmatch_4k_labels_str` (8%)**    | FixMatch         |  `{python} cifar10_fixmatch_4k_acc`% |  **`{python} cifar10_fixmatch_4k_eff`× more efficient** |
| **`{python} cifar10_fixmatch_250_labels_str` (0.5%)** | FixMatch         | `{python} cifar10_fixmatch_250_acc`% | **`{python} cifar10_fixmatch_250_eff`× more efficient** |
| **`{python} cifar10_fixmatch_40_labels_str` (0.08%)** | FixMatch         |  `{python} cifar10_fixmatch_40_acc`% |      `{python} cifar10_fixmatch_40_eff`× more efficient |

: **FixMatch Label Efficiency on CIFAR-10.** With `{python} cifar10_fixmatch_250_labels_str` labels (0.5% of the dataset), FixMatch achieves within `{python} acc_loss_str` points of full supervision, demonstrating `{python} cifar10_fixmatch_250_eff`× label efficiency. {#tbl-fixmatch-cifar10 .striped .hover}

With only `{python} cifar10_fixmatch_250_labels_str` labeled samples (25 per class), FixMatch achieves `{python} cifar10_fixmatch_250_acc`% accuracy, within `{python} acc_loss_str` points of full supervision using `{python} cifar10_fixmatch_250_eff`× fewer labels. The technique works by generating pseudo-labels on weakly augmented unlabeled images (only when model confidence exceeds 0.95), then training to predict these labels on strongly augmented versions of the same images.

**The Systems Insight**: Semi-supervised learning trades labeled data for unlabeled data and compute. On CIFAR-10, training FixMatch requires ~`{python} fixmatch_compute_multiplier_str`× more compute than supervised training (processing 50K unlabeled samples per epoch). When labels cost \$1 each and GPU hours cost \$0.50, the math favors semi-supervised:

- Supervised (`{python} cifar10_fixmatch_4k_labels_str` labels): \$`{python} supervised_label_cost_str` labeling + \$`{python} supervised_compute_cost` compute = **\$`{python} supervised_total_str`**
- FixMatch (`{python} cifar10_fixmatch_250_labels_str` labels): \$`{python} fixmatch_label_cost_str` labeling + \$`{python} fixmatch_compute_cost` compute = **\$`{python} fixmatch_total_str`**

An `{python} cost_reduction_str`× cost reduction for ~`{python} acc_loss_str` points of accuracy loss.
:::

These gains are substantial, but semi-supervised learning is not universally applicable. The technique assumes that unlabeled data comes from the same distribution as labeled data, and it struggles when unlabeled data contains out-of-distribution samples (the model confidently mislabels them), when class imbalance is severe (pseudo-labels amplify majority class bias), or when the labeled set does not cover all classes (preventing label propagation for unseen classes). Always validate on a held-out set with true labels to catch distribution mismatch.

Despite these limitations, semi-supervised learning reduces label requirements by 5–10$\times$ while maintaining accuracy. Notice what we have not yet questioned: the assumption that we need *any* task-specific labels at all. What if the structure of data itself (the fact that cat images resemble other cat images, that coherent sentences follow grammatical patterns) could provide the supervision signal?

We have progressively reduced labeling demands: active learning queries only informative samples, semi-supervised learning exploits unlabeled data to stretch labeled examples further. The logical endpoint of this trajectory is eliminating task-specific labels entirely. Self-supervised learning achieves this by reframing the data selection problem: rather than selecting which examples to label, it defines learning objectives that derive supervision from the data itself. This approach transforms the pipeline's annotation stage from a bottleneck into an opportunity, and represents the most dramatic shift in how we think about data efficiency.

## Self-Supervised Learning: Eliminating the Label Bottleneck {#sec-data-selection-selfsupervised-learning-eliminating-label-bottleneck-1005}

Active learning reduces labeling cost by 10$\times$. Semi-supervised learning reduces it by another 5–10$\times$. The most dramatic gain, however, comes from **self-supervised learning**[^fn-self-supervised], which removes the human annotation bottleneck entirely by learning from data structure. The key insight is that the structure itself provides supervision: predicting masked words or matching augmented images forces models to learn useful representations without any human labels. Self-supervised learning does not map neatly onto the three-stage pipeline (static pruning, dynamic selection, synthetic generation) introduced earlier. Rather than optimizing which labeled samples to use, SSL eliminates the label bottleneck by redefining what counts as supervision. It is best understood as a paradigm shift that makes the entire pipeline more effective: pre-trained representations improve the quality of coreset selection, active learning, and downstream fine-tuning alike.

[^fn-self-supervised]: **Self-Supervised Learning**: While self-supervision ideas existed earlier, 2018 marked the paradigm's breakthrough year. BERT (Google, October 2018) demonstrated that masked language modeling could produce representations achieving state-of-the-art results on 11 NLP tasks. GPT (OpenAI, June 2018) showed that next-token prediction at scale yielded surprisingly general language understanding. Together, they established pre-training on unlabeled data as the dominant paradigm for NLP, later extended to vision and multimodal domains.

### The Paradigm Shift: Labels from Structure {#sec-data-selection-paradigm-shift-labels-structure-e9cc}

Labels represent just one form of supervision. The structure of data itself provides rich learning signals that require no human annotation, as @tbl-self-supervised-tasks summarizes.

| **Modality**    | **Self-Supervised Task** | **Supervision Signal**                      |
|:--------------|:-----------------------|:------------------------------------------|
| **Text**        | Masked language modeling | Predict [MASK] from context                 |
| **Text**        | Next-token prediction    | Predict next word in sequence               |
| **Images**      | Contrastive learning     | Same image (augmented) vs. different images |
| **Images**      | Masked autoencoding      | Reconstruct masked patches                  |
| **Multi-modal** | CLIP-style alignment     | Match image-text pairs                      |

: **Self-Supervised Pretext Tasks by Modality.** Each task extracts supervision from data structure rather than human labels, enabling pre-training on unlimited unlabeled corpora. {#tbl-self-supervised-tasks .striped .hover}

These *pretext tasks* generate supervision signals automatically from the data itself. A model that can predict masked words has necessarily learned grammar, semantics, and world knowledge to make accurate predictions. Similarly, a model that distinguishes augmented views of the same image from different images has learned robust visual features invariant to transformations.

The systems implication is direct: self-supervised pre-training moves the data cost off the critical path. Instead of waiting for labels before training begins, pre-training can start immediately on unlabeled data, often web-scale corpora of billions of samples. This separation of pre-training from task-specific labeling restructures the economics of machine learning.

### The Economics of Amortization {#sec-data-selection-economics-amortization-79e6}

Understanding *why* self-supervised learning dominates modern ML practice requires examining its economic structure. The shift translates into concrete cost savings through *cost amortization*, where expensive pre-training is performed once and reused across many applications (@tbl-cost-amortization).

| **Approach**                   | **Labels per Task** |  **Compute per Task** | **Data Acquisition**      |
|:-----------------------------|------------------:|--------------------:|:------------------------|
| **Train from scratch**         |     100K–1M labeled |    100% full training | Task-specific collection  |
| **Fine-tune foundation model** |      100–1K labeled | 1–5% of full training | Reuse pre-training corpus |

: **Cost Amortization in Foundation Model Fine-Tuning.** Pre-training costs are paid once; fine-tuning costs scale with task count but remain small per task. {#tbl-cost-amortization .striped .hover}

To illustrate this economic transformation, consider a company building ten specialized classifiers for tasks such as fraud detection, content moderation, and medical diagnosis.

```{python}
#| label: foundation-cost-calc
#| echo: false
from physx.formatting import fmt

# =============================================================================
# PURPOSE
# =============================================================================
# Purpose: Compare GPU-hour costs for scratch vs foundation models
# Used in: Figure "Cost Amortization in Foundation Models"

# =============================================================================
# INPUT
# =============================================================================
cost_scratch_per_task_value = 1000
n_tasks_value = 10
cost_pretrain_value = 10000
cost_finetune_value = 50

# Labeling costs
labels_per_task_scratch = 100_000
cost_per_label = 1
labels_per_task_finetune = 1_000

# =============================================================================
# PROCESS
# =============================================================================
cost_scratch_total_value = cost_scratch_per_task_value * n_tasks_value
cost_foundation_total_value = cost_pretrain_value + (cost_finetune_value * n_tasks_value)

label_cost_scratch_total = labels_per_task_scratch * cost_per_label * n_tasks_value
label_cost_finetune_total = labels_per_task_finetune * cost_per_label * n_tasks_value

label_cost_reduction = label_cost_scratch_total / label_cost_finetune_total
# Marginal cost reduction: 1000 -> 50 = 20x
marginal_compute_reduction = cost_scratch_per_task_value / cost_finetune_value
crossover_tasks_value = cost_pretrain_value / (
    cost_scratch_per_task_value - cost_finetune_value
)

# =============================================================================
# OUTPUT
# =============================================================================
total_a_hrs_str = f"{cost_scratch_total_value:,}"
total_b_hrs_str = f"{cost_foundation_total_value:,}"

labels_per_task_scratch_str = fmt(labels_per_task_scratch, precision=0, commas=True)
label_cost_scratch_total_str = f"${label_cost_scratch_total / 1_000_000:.0f}M"  # e.g., "$1M"
cost_scratch_per_task_str = fmt(cost_scratch_per_task_value, precision=0, commas=True)
cost_scratch_total_str = fmt(cost_scratch_total_value, precision=0, commas=True)

labels_per_task_finetune_str = fmt(labels_per_task_finetune, precision=0, commas=True)
label_cost_finetune_total_str = f"${label_cost_finetune_total / 1_000:.0f}K"  # e.g., "$10K"
cost_finetune_value_str = fmt(cost_finetune_value, precision=0, commas=True)
cost_pretrain_value_str = fmt(cost_pretrain_value, precision=0, commas=True)
marginal_compute_reduction_str = fmt(marginal_compute_reduction, precision=0, commas=False)
crossover_tasks_str = fmt(crossover_tasks_value, precision=0, commas=False)

label_cost_drop_str = fmt(label_cost_reduction, precision=0, commas=False)
marginal_compute_reduction_str = fmt(marginal_compute_reduction, precision=0, commas=False)
```

Training each classifier from scratch would require substantial investment in both labeling and compute. With ten tasks each needing `{python} labels_per_task_scratch_str` labels at \$1 per label, the total labeling cost reaches **`{python} label_cost_scratch_total_str`**. The compute burden amounts to `{python} cost_scratch_total_str` GPU-hours across all tasks, with each requiring its own data collection effort. From start to finish, each task takes 6–12 months to complete.

The fine-tuning approach restructures these costs. Pre-training requires a one-time investment of `{python} cost_pretrain_value_str` GPU-hours on unlabeled data, but this cost is paid only once. Fine-tuning each task then requires just `{python} labels_per_task_finetune_str` labels (`{python} label_cost_finetune_total_str` total across all ten tasks) and only `{python} cost_finetune_value_str` GPU-hours of compute. Each task reaches deployment in 1–2 weeks after pre-training completes.

The return on investment is substantial across every dimension: labeling costs drop by **`{python} label_cost_drop_str`×** (from `{python} label_cost_scratch_total_str` to `{python} label_cost_finetune_total_str`), per-task marginal compute decreases by **`{python} marginal_compute_reduction_str`×**, and time to deployment accelerates by **20–50×** per task.

This explains *why* the fine-tuning paradigm dominates production ML. The pre-training cost is high but amortized across many downstream applications, while fine-tuning cost remains low on a per-task basis.

```{python}
#| label: fig-foundation-cost-data
#| echo: false

# =============================================================================
# PURPOSE
# =============================================================================
# Purpose: Compare GPU-hour costs for scratch vs foundation models
# Used in: Figure "Cost Amortization in Foundation Models"

# =============================================================================
# INPUT
# =============================================================================
cost_scratch_per_task_value = 1000
n_tasks_value = 10
cost_pretrain_value = 10000
cost_finetune_value = 50

# =============================================================================
# PROCESS
# =============================================================================
cost_scratch_total_value = cost_scratch_per_task_value * n_tasks_value
cost_foundation_total_value = cost_pretrain_value + (cost_finetune_value * n_tasks_value)

# =============================================================================
# OUTPUT
# =============================================================================
total_a_hrs_str = f"{cost_scratch_total_value:,}"
total_b_hrs_str = f"{cost_foundation_total_value:,}"
```

Contrast the two bar charts in @fig-amortization-comparison to see this cost structure in action. Training from scratch (left) incurs the full cost for each task independently. The foundation model approach (right) pays a large upfront pre-training cost but then fine-tunes each task at a fraction of the per-task cost.

::: {#fig-amortization-comparison fig-env="figure" fig-pos="htb" fig-cap="**Cost Amortization in Foundation Models**: Training from scratch (left) requires 1,000 GPU-hours per task (10,000 total for 10 tasks). The foundation model approach (right) pays 10,000 GPU-hours upfront for pre-training but reduces each subsequent task to just 50 GPU-hours. At 10 tasks the totals are comparable (10,000 vs 10,500), but the per-task marginal cost drops by 20x, and the crossover favoring the foundation model occurs around 11 tasks." fig-alt="Two bar charts side by side. Left (Train from Scratch) shows 10 equal bars of 1,000 GPU-hours each, totaling 10,000 hours. Right (Foundation Model) shows one tall pre-training bar of 10,000 GPU-hours followed by 10 short fine-tuning bars of 50 GPU-hours each, totaling 10,500 hours. The per-task marginal cost drops dramatically from 1,000 to 50 GPU-hours."}
```{.tikz}
\begin{tikzpicture}[font=\small\usefont{T1}{phv}{m}{n}]
\begin{axis}[
    name=scratch,
    ybar,
    bar width=8pt,
    width=6cm, height=5cm,
    ylabel={Cost (1000 GPU-hours)},
    xlabel={Task Number},
    title={Train from Scratch},
    ymin=0, ymax=12,
    xtick={1,2,3,4,5,6,7,8,9,10},
    xticklabel style={font=\tiny\usefont{T1}{phv}{m}{n}},
    ytick={0,2,4,6,8,10},
    yticklabel style={font=\tiny\usefont{T1}{phv}{m}{n}},
    axis lines=left,
    enlarge x limits=0.08,
]
    \addplot[fill=RedL, draw=RedLine] coordinates {
        (1,1) (2,1) (3,1) (4,1) (5,1)
        (6,1) (7,1) (8,1) (9,1) (10,1)
    };
\end{axis}

\begin{axis}[
    at={(scratch.east)},
    anchor=west,
    xshift=1.5cm,
    ybar stacked,
    bar width=8pt,
    width=6cm, height=5cm,
    xlabel={Task Number},
    title={Foundation Model},
    ymin=0, ymax=12,
    xtick={0,1,2,3,4,5,6,7,8,9,10},
    xticklabels={Pre,1,2,3,4,5,6,7,8,9,10},
    xticklabel style={font=\tiny\usefont{T1}{phv}{m}{n}},
    ytick={0,2,4,6,8,10},
    yticklabel style={font=\tiny\usefont{T1}{phv}{m}{n}},
    axis lines=left,
    enlarge x limits=0.08,
    legend style={at={(0.98,0.98)}, anchor=north east, font=\tiny\usefont{T1}{phv}{m}{n}, draw=none},
]
    % Pre-training cost (only for task 0)
    \addplot[fill=BlueL, draw=BlueLine] coordinates {
        (0,10) (1,0) (2,0) (3,0) (4,0) (5,0)
        (6,0) (7,0) (8,0) (9,0) (10,0)
    };
    % Fine-tuning cost (for tasks 1-10)
    \addplot[fill=GreenL, draw=GreenLine] coordinates {
        (0,0) (1,0.05) (2,0.05) (3,0.05) (4,0.05) (5,0.05)
        (6,0.05) (7,0.05) (8,0.05) (9,0.05) (10,0.05)
    };
    \legend{Pre-training, Fine-tuning}
\end{axis}

% Totals annotation
\node[font=\footnotesize\bfseries\usefont{T1}{phv}{m}{n}, RedLine] at (2.2, -1.0) {Total: 10,000 hrs};
\node[font=\footnotesize\bfseries\usefont{T1}{phv}{m}{n}, BlueLine] at (8.7, -1.0) {Total: 10,500 hrs};
\end{tikzpicture}
```
:::

### Trade-Offs Across Self-Supervised Approaches {#sec-data-selection-tradeoffs-across-selfsupervised-approaches-b473}

The economics of amortization favor self-supervised learning broadly, but not all self-supervised methods are equivalent. Different approaches occupy different points on the efficiency frontier, trading off pre-training cost, batch size requirements, and downstream data efficiency (@tbl-ssl-tradeoffs).

| **Method**          | **Batch Size Requirement** | **Data Efficiency**       | **Best Use Case**            |
|:------------------|-------------------------:|:------------------------|:---------------------------|
| **Contrastive**     |         Very large (4096+) | High (per labeled sample) | Vision, small datasets       |
| **(SimCLR, MoCo)**  |                            | Low (pre-training cost)   |                              |
| **Masked Modeling** |        Moderate (256–1024) | Moderate                  | NLP, balanced efficiency     |
| **(BERT, MAE)**     |                            |                           |                              |
| **Generative**      |           Large (512–2048) | Highest at scale          | Foundation models, unlimited |
| **(GPT)**           |                            |                           | unlabeled data               |

: **Self-Supervised Learning Method Trade-Offs.** Contrastive methods excel at downstream data efficiency but require massive batches; masked modeling balances cost and efficiency; generative methods scale best with unlimited data. {#tbl-ssl-tradeoffs .striped .hover}

Contrastive learning methods such as SimCLR [@chen2020simclr] and MoCo [@he2020momentum][^fn-simclr-moco] require many negative examples per batch to distinguish similar samples, making them compute-intensive during pre-training.

[^fn-simclr-moco]: **SimCLR and MoCo**: Both published in early 2020, these papers marked a turning point for self-supervised vision. SimCLR (Ting Chen et al., Google) achieved 76.5% ImageNet accuracy with a linear classifier, matching supervised ResNet-50, using only self-supervised pre-training. MoCo (Kaiming He et al., Facebook AI) introduced the momentum encoder trick that enabled contrastive learning without requiring enormous batch sizes. Their combined impact closed the gap between supervised and self-supervised visual representations.

The batch size requirement is substantial: these methods need batches of 4,096 or more samples to work effectively. This upfront investment yields excellent downstream performance with minimal labeled data, making contrastive learning particularly effective for vision applications with small datasets.[^fn-batch-size-sensitivity]

[^fn-batch-size-sensitivity]: **Batch Size Sensitivity**: The batch size sensitivity is substantial: SimCLR achieves 66.6% ImageNet top-1 accuracy with batch size 8192 but drops to 61.9% with batch size 256, a 4.7 percentage point degradation [@chen2020mocov2]. This occurs because contrastive learning treats all other samples in a batch as negatives; with fewer negatives, the pretext task becomes easier and the learned representations are weaker.

Masked modeling approaches such as BERT and MAE occupy a middle ground in this efficiency spectrum. These methods work with smaller batches (256–1024 samples) but require more training iterations to converge. The result is a balanced trade-off between pre-training cost and downstream data efficiency that has made masked modeling the dominant paradigm in natural language processing.

Generative pre-training, exemplified by the GPT family of models, scales well with data volume. Performance improves log-linearly with dataset size up to trillions of tokens, exhibiting no saturation within current data availability. This scaling behavior makes generative pre-training the preferred method for foundation models, where the substantial pre-training cost can be amortized across thousands of downstream tasks.

### From 1000× Multiplier to Foundation Model Paradigm {#sec-data-selection-1000-multiplier-foundation-model-paradigm-7866}

These trade-offs converge on a clear conclusion: from a data selection perspective, self-supervised pre-training represents a **1000$\times$ or greater multiplier** on the value of labeled data. Instead of labeling millions of task-specific examples, practitioners fine-tune on hundreds or thousands of labeled samples while inheriting knowledge distilled from billions of unlabeled tokens.

This multiplicative advantage extends the amortization economics examined in the previous section, creating the *foundation model paradigm*\index{Self-supervised learning!foundation model paradigm}[^fn-foundation-model] [@bommasani2021opportunities] that defines modern ML systems.

[^fn-foundation-model]: **Foundation Model**: Term coined by Stanford's Center for Research on Foundation Models in 2021 to describe models like BERT, GPT-3, and DALL-E. The name emphasizes a critical property: these models serve as a "foundation" for many downstream tasks, but this creates dangerous homogenization. Defects in the foundation model propagate to all applications built upon it, making them single points of failure that can "radiate harms" across an ecosystem.

The architectural and training details for these methods appear in @sec-ai-training. From a data selection perspective, self-supervised learning represents the current ceiling of what the field has achieved, transforming data from the primary bottleneck into an abundant resource. At scale, self-supervised pre-training requires distributed infrastructure: gradient accumulation across mini-batches, mixed precision to reduce memory footprint, and pipeline parallelism to split models across devices are essential for pre-training billion-parameter models. The data selection principles discussed here (coreset selection, curriculum learning) apply regardless of scale, but their implementation must account for distributed coordination overhead.

Self-supervised learning addresses the label bottleneck by learning from data structure rather than human annotation. What happens, though, when the data itself is scarce? When rare classes have too few examples, when edge cases never appear in the wild, or when privacy constraints prevent collecting real samples? The third stage of our data selection pipeline addresses this gap: rather than selecting or curating existing data, we create new data on demand.

## Synthetic Data Generation and Augmentation {#sec-data-selection-synthetic-data-generation-augmentation-f3c5}

Static pruning removed redundancy before training began. Dynamic selection focused compute on the most informative samples during training. The third and final stage of the data selection pipeline takes the opposite approach: rather than subtracting or selecting from existing data, it creates new high-value samples when real data is scarce, expensive, or lacks diversity. The strategy shifts from curation to **creation**.

### Data Augmentation: Transformation-Based Synthesis {#sec-data-selection-data-augmentation-transformationbased-synthesis-20a5}

Data augmentation expands a dataset by applying transformations to existing samples. Because many transformations preserve label semantics while creating novel inputs, augmentation effectively multiplies the diversity of a training set without requiring additional data collection.

For image data, augmentation techniques span a range of complexity. Geometric transformations such as rotation, flipping, cropping, and scaling introduce spatial variation that makes models robust to viewpoint changes. Photometric transformations adjust brightness, contrast, saturation, and hue to simulate different lighting conditions and camera characteristics. More advanced techniques like Cutout (which applies random rectangular masks), MixUp[^fn-mixup] [@zhang2018mixup] (which blends two images and their labels), and CutMix (which pastes patches between images) push augmentation further by creating entirely synthetic training examples that regularize learning.

[^fn-mixup]: **MixUp**: Introduced by Hongyi Zhang and colleagues at ICLR 2018. The elegantly simple idea (train on linear interpolations of image pairs with correspondingly interpolated labels) produces surprisingly strong regularization. The paper showed MixUp reduces memorization of corrupt labels, improves adversarial robustness, and stabilizes GAN training, all from a technique requiring just two lines of code to implement.

Text augmentation presents different challenges because language is discrete rather than continuous. Back-translation offers one solution: translating text to another language and back generates paraphrases that preserve meaning while varying surface form. Simpler approaches include synonym replacement, which swaps words while preserving semantics, and random insertion or deletion, which adds noise that makes models robust to typos and informal input. Rather than hand-designing these augmentation policies, **AutoAugment** uses reinforcement learning to discover optimal augmentation strategies for specific datasets, while RandAugment simplifies this by randomly sampling from a fixed set of transformations, achieving similar performance with less computation.

These learned augmentation policies are particularly effective for resource-constrained models, where overfitting risk is highest. The MobileNet lighthouse illustrates this principle: when model capacity is deliberately reduced for edge deployment, augmentation becomes the primary defense against overfitting.

::: {.callout-lighthouse title="MobileNet and Aggressive Augmentation"}
Our **MobileNet Lighthouse model** (@sec-dnn-architectures) exemplifies how data augmentation compensates for model capacity constraints. MobileNet's depthwise separable convolutions reduce parameters by 8–9$\times$ compared to standard convolutions, but this efficiency comes at a cost: smaller models are more prone to overfitting on limited data.

The solution is **aggressive augmentation**. MobileNet training typically uses stronger augmentation than ResNet-50 training, including RandAugment with higher magnitude, more aggressive cropping, and longer training schedules. The augmentation effectively increases dataset diversity without increasing model capacity, allowing MobileNet to achieve near-ResNet accuracy at a fraction of the parameter count. For edge deployment where both data collection and model size are constrained, augmentation is essential rather than optional.
:::

### Generative Synthesis: Creating New Samples {#sec-data-selection-generative-synthesis}

Augmentation transforms existing samples; synthetic data generation goes further by creating entirely new examples using generative models. This capability becomes essential in three common scenarios: when real data is privacy-sensitive (as with medical records or financial transactions), when edge cases are rare (such as autonomous driving failure scenarios that must be covered but seldom occur), or when data collection is prohibitively expensive (as in robotics or scientific experiments where each sample requires physical resources).

Three classes of generative approaches address these needs, each with distinct cost and fidelity trade-offs. Generative Adversarial Networks (GANs) train a generator against a discriminator in an adversarial setup, producing realistic images through competition; StyleGAN, for instance, generates photorealistic faces that have augmented facial recognition datasets. Diffusion models use iterative denoising to produce high-quality images; systems like Stable Diffusion enable text-to-image synthesis, allowing you to generate targeted training examples from natural language descriptions. Finally, simulation engines such as CARLA for autonomous driving or Unity and Unreal for robotics offer physics-based rendering that generates unlimited labeled data with perfect ground-truth annotations, making them particularly valuable for safety-critical applications where edge case coverage is essential.

### Bridging the Domain Gap {#sec-data-selection-bridging-domain-gap-15f5}

Synthetic data's greatest limitation is the **domain gap**: the statistical difference between generated and real-world data, as illustrated in @fig-domain-gap. A model trained only on synthetic data learns a decision boundary optimized for the wrong distribution, potentially performing well on synthetic test data while failing on real deployment data.

::: {#fig-domain-gap fig-env="figure" fig-pos="htb" fig-cap="**The Domain Gap Problem**: Synthetic data (blue) and real data (orange) have different distributions. A model trained on synthetic data alone learns a boundary that fails on real data. Domain adaptation techniques aim to align these distributions or learn domain-invariant features." fig-alt="Two overlapping bell curves representing synthetic and real data distributions, with a decision boundary that works for synthetic but misses real data."}
```{.tikz}
\begin{tikzpicture}[font=\small\usefont{T1}{phv}{m}{n}]
\begin{axis}[
    width=10cm, height=5cm,
    xlabel={Feature Space},
    ylabel={Density},
    xmin=-3, xmax=7,
    ymin=0, ymax=0.5,
    axis lines=left,
    xtick=\empty,
    ytick=\empty,
    legend style={at={(0.98,0.98)}, anchor=north east, font=\footnotesize\usefont{T1}{phv}{m}{n}, draw=none},
    clip=false
]
    % Synthetic data distribution (centered at 0)
    \addplot[thick, blue, domain=-3:4, samples=100, fill=blue!20, fill opacity=0.5]
        {0.4*exp(-0.5*(x)^2)};
    \addlegendentry{Synthetic}

    % Real data distribution (centered at 3, slightly different shape)
    \addplot[thick, orange, domain=0:7, samples=100, fill=orange!20, fill opacity=0.5]
        {0.35*exp(-0.4*(x-3)^2)};
    \addlegendentry{Real}

    % Decision boundary learned from synthetic
    \draw[thick, dashed, red] (axis cs: 1.5, 0) -- (axis cs: 1.5, 0.45);
    \node[red, font=\footnotesize\usefont{T1}{phv}{m}{n}, align=center] at (axis cs: 1.5, 0.48) {Synthetic\\boundary};

    % Ideal boundary for real data
    \draw[thick, dotted, green!60!black] (axis cs: 3, 0) -- (axis cs: 3, 0.45);

    % Domain gap annotation
    \draw[<->, thick, purple] (axis cs: 0, 0.42) -- (axis cs: 3, 0.42);
    \node[purple, font=\footnotesize\usefont{T1}{phv}{m}{n}, fill=white, inner sep=1pt] at (axis cs: 1.5, 0.42) {Domain Gap};

\end{axis}
\end{tikzpicture}
```
:::

Two complementary strategies address this distribution mismatch. Domain randomization takes an aggressive approach: rather than trying to match the real world precisely, it trains on wildly varied synthetic data by randomizing lighting, textures, backgrounds, and camera parameters during generation. If the model encounters sufficient variation during training, the real world becomes "just another variation" within its learned distribution. This strategy produces strong results for robotics and autonomous driving, where simulation technology is mature enough to generate physically plausible variations across a wide range.

Domain adaptation takes the opposite approach by explicitly aligning synthetic and real distributions. Feature alignment methods train on synthetic data while simultaneously minimizing the distance between synthetic and real feature distributions, often using adversarial training to learn domain-invariant representations. Fine-tuning offers a simpler path: pre-train on abundant synthetic data to learn general features, then fine-tune on a small real dataset to adapt to deployment conditions. Self-training combines these ideas by using a synthetic-trained model to pseudo-label real unlabeled data, then retraining on the combined labeled set.

In practice, the best results often come from mixing synthetic and real data rather than relying on either source alone. @tbl-synthetic-mix summarizes typical outcomes across different mixing ratios.

| **Synthetic Fraction**       | **Typical Outcome**                        |
|:---------------------------|:-----------------------------------------|
| **100% synthetic**           | Poor real-world generalization             |
| **80% synthetic + 20% real** | Good performance, significant cost savings |
| **50% synthetic + 50% real** | Best performance in many domains           |
| **100% real**                | Baseline (expensive)                       |

: **Synthetic-to-Real Data Mixing Ratios.** Pure synthetic data suffers from distribution shift; pure real data is expensive. The optimal ratio varies by domain but typically falls in the 50–80% synthetic range when simulation fidelity is high. {#tbl-synthetic-mix .striped .hover}

The optimal mix depends on simulation fidelity, domain complexity, and the cost differential between synthetic and real data. One important distinction applies when synthetic data comes from ML models rather than simulators: there is a risk of *model collapse*, where training on model-generated data amplifies errors and reduces diversity over generations. This concern is particularly acute for foundation models, where synthetic data from earlier model generations may contaminate future training corpora. With appropriate safeguards, synthetic data generation remains a powerful tool. The following example illustrates how to combine multiple data selection techniques (augmentation, noise injection, and simulation) into a coherent strategy for a real deployment scenario.

::: {.callout-example title="KWS Data Selection"}
**Scenario**: Our **Keyword Spotting Lighthouse model** (@sec-dnn-architectures), a DS-CNN with **200 K** parameters, represents the extreme end of data selection challenges. You are building a wake-word detector ("Hey Device") for a microcontroller with 256 KB SRAM (see @sec-ml-system-architecture-tinyml-ubiquitous-sensing-scale-a67b for hardware constraints). The model must be tiny (~50 KB quantized), but you need 10,000+ labeled audio samples to train it, samples that do not yet exist.

**The Data Collection Problem**:

- Recording 10,000 real utterances requires 500+ speakers for diversity
- Professional recording costs \$2–5 per sample (\$20–50K total)
- Target deployment environment (noisy kitchen, car interior) differs from recording studio

**Data Selection Solution Stack**:

1. **Seed Data (500 samples)**: Record 50 speakers × 10 utterances in controlled conditions
2. **Augmentation (5,000 samples)**: Apply pitch shift, time stretch, speed variation to 10$\times$ the seed data
3. **Noise Injection (10,000 samples)**: Mix clean audio with environmental noise (kitchen appliances, HVAC, traffic) sampled from AudioSet
4. **Negative Mining**: Use acoustic similarity to find hard negatives ("Hey Siri", "Hey Google") from public datasets
5. **Simulation (optional)**: Text-to-speech synthesis with diverse voice models

**Result**: 500 real recordings → 10,000+ training samples at 5% of the cost. The noise injection serves as domain randomization, improving deployment robustness.

**Key Insight for TinyML**: When the target model is tiny, the data selection challenge shifts from "reduce terabytes to gigabytes" to "create a useful dataset from almost nothing." Augmentation and simulation become essential rather than optional.
:::

### Knowledge Distillation: Compressing Information {#sec-data-selection-knowledge-distillation-compressing-information-40a5}

The techniques above create new input samples, but there is another form of synthesis that creates enhanced labels. Knowledge distillation[^fn-distillation] [@hinton2015distilling], examined in depth as a compression technique in @sec-model-compression, also serves as a data selection technique where a smaller "student" model learns from a larger "teacher" model's outputs rather than raw labels. This section treats distillation as a *data selection* technique, where the teacher's outputs serve as enriched training data that carries more information per sample than hard labels. @sec-model-compression examines the complementary perspective: distillation as a model compression technique for producing smaller, faster student models suitable for resource-constrained deployment.

[^fn-distillation]: **Knowledge Distillation**: Introduced by Geoffrey Hinton, Oriol Vinyals, and Jeff Dean in 2015. Hinton coined the evocative term "dark knowledge" for the information in soft probability distributions: the teacher reveals not just which class is correct but which incorrect classes are most plausible. The temperature parameter in the softmax function controls how much dark knowledge is exposed: higher temperatures produce softer distributions that transfer more nuanced inter-class relationships.

The key insight is that the teacher's soft predictions contain more information than hard labels: a teacher predicting [0.7, 0.2, 0.1] for three classes reveals inter-class relationships (classes 1 and 2 are more similar) that a hard label [1, 0, 0] obscures entirely.

This richer supervision signal enables student models to learn more efficiently from the same data. From a systems perspective, distillation is particularly powerful for creating synthetic labels at scale: run a large model (such as GPT-4) on unlabeled data to generate high-quality annotations, then train a smaller model on these synthetic labels. The smaller model inherits much of the teacher's capability at a fraction of the inference cost, amortizing the expensive teacher computation across many student deployments.

Together, augmentation, generative synthesis, and distillation complete the third stage of our data selection pipeline. Where static pruning removes redundancy and dynamic selection focuses compute on high-value samples, synthetic generation fills gaps by creating samples that never existed. These three stages form a complementary toolkit: pruning reduces what you have, selection focuses how you use it, and synthesis expands what you can access.

Having examined selection techniques across all three pipeline stages, we now consolidate their characteristics to guide practical application.

## Technique Summary {#sec-data-selection-technique-summary-0ee8}

@tbl-data-selection summarizes the three-stage optimization pipeline introduced at the beginning of this chapter.

| **Stage**                   | **When Applied** | **Techniques**                                        |                     **Typical Gains** |
|:--------------------------|:---------------|:----------------------------------------------------|------------------------------------:|
| **1. Static Pruning**       | Before training  | Coreset Selection, Deduplication, Quality Filtering   |              30–50% dataset reduction |
| **2. Dynamic Selection**    | During training  | Curriculum Learning, Active Learning, Semi-Supervised |             10–30% faster convergence |
| **3. Synthetic Generation** | On-demand        | Augmentation, Generative Models, Distillation         | 2–10$\times$ effective data expansion |

: **Three-Stage Data Selection Pipeline.** Each stage increases ICR by different mechanisms: pruning removes low-value samples, dynamic selection focuses compute on high-value samples, and synthesis creates new high-value samples. {#tbl-data-selection .striped .hover}

@tbl-technique-selection provides a decision guide for selecting techniques based on your specific constraints.

| **Constraint**                 | **Best Technique**      | **Why**                                              |
|:-----------------------------|:----------------------|:---------------------------------------------------|
| **Limited labeling budget**    | Active Learning         | Maximizes label ROI by selecting informative samples |
| **High redundancy in data**    | Deduplication + Coreset | Removes waste before training begins                 |
| **Rare classes or edge cases** | Synthetic Generation    | Creates samples that do not exist in raw data        |
| **Slow convergence**           | Curriculum Learning     | Improves gradient quality in early training          |
| **Privacy requirements**       | Synthetic Data          | Train on generated data, not real user data          |
| **Large model, small dataset** | Knowledge Distillation  | Use teacher model's knowledge as "data"              |

: **Technique Selection Guide by Primary Constraint.** Recommended data selection techniques mapped to the dominant resource constraint in the ML pipeline. {#tbl-technique-selection .striped .hover}

### Decision Framework: Choosing the Right Technique {#sec-data-selection-decision-framework-choosing-right-technique-c36a}

With many techniques available, practitioners need a systematic approach to selection. Start at the top of @fig-technique-decision-tree and follow the branches that match your constraints:

::: {#fig-technique-decision-tree fig-env="figure" fig-pos="htb" fig-cap="**Data Selection Technique Selection Tree**: Start at the top by identifying your primary bottleneck, then follow the branches to find the most appropriate technique. Leaf nodes show recommended methods. Multiple paths may apply; combine techniques as needed." fig-alt="A decision tree flowchart with diamond decision nodes and rectangular technique recommendations. Starts with bottleneck identification and branches to specific techniques."}
```{.tikz}
\begin{tikzpicture}[
    font=\small\usefont{T1}{phv}{m}{n},
    decision/.style={diamond, draw=BlueLine, fill=BlueL, line width=0.75pt, text width=2cm, align=center, inner sep=1pt, aspect=2},
    technique/.style={rectangle, draw=GreenLine, fill=GreenL, line width=0.75pt, rounded corners, text width=2cm, align=center, minimum height=0.8cm},
    Line/.style={draw=black!50, -latex, line width=1.0pt},
    label/.style={font=\tiny\usefont{T1}{phv}{m}{n}, fill=white, inner sep=1pt},
    scale=0.85, transform shape
]
% Standard color definitions
\definecolor{BlueLine}{HTML}{006395}
\definecolor{BlueL}{HTML}{D1E6F3}
\definecolor{GreenLine}{HTML}{008F45}
\definecolor{GreenL}{HTML}{D4EFDF}

% Row 0: Root
\node[decision] (root) at (0, 0) {What is your primary bottleneck?};

% Row 1: Three main branches
\node[decision] (labeling) at (-5, -2.5) {Labeling cost};
\node[decision] (compute) at (0, -2.5) {Compute cost};
\node[decision] (scarcity) at (5, -2.5) {Data scarcity};

% Row 2: Sub-decisions
\node[decision] (oracle) at (-6.5, -5) {Oracle available?};
\node[technique] (selfsup) at (-3.5, -5) {Self-\\Supervised};
\node[decision] (redundant) at (0, -5) {Data redundant?};
\node[decision] (simulator) at (3.5, -5) {Simulator?};
\node[technique] (distill) at (6.5, -5) {Knowledge\\Distillation};

% Row 3: Leaf techniques
\node[technique] (active) at (-7.5, -7.5) {Active\\Learning};
\node[technique] (semisup) at (-5.5, -7.5) {Semi-\\Supervised};
\node[technique] (dedup) at (-1, -7.5) {Dedup +\\Coreset};
\node[technique] (curriculum) at (1, -7.5) {Curriculum\\Learning};
\node[technique] (synthetic) at (2.5, -7.5) {Synthetic\\Generation};
\node[technique] (augment) at (4.5, -7.5) {Data\\Augmentation};

% Arrows from root to level 1
\draw[Line] (root.south) -- ++(0,-0.3) -| node[label, pos=0.75, above] {Labeling \$\$\$} (labeling.north);
\draw[Line] (root.south) -- node[label, right] {Compute \$\$\$} (compute.north);
\draw[Line] (root.south) -- ++(0,-0.3) -| node[label, pos=0.75, above] {Not enough data} (scarcity.north);

% Arrows from labeling
\draw[Line] (labeling.south) -- ++(0,-0.3) -| node[label, pos=0.25, left] {Yes} (oracle.north);
\draw[Line] (labeling.south) -- ++(0,-0.3) -| node[label, pos=0.25, right] {Large pool} (selfsup.north);

% Arrows from compute
\draw[Line] (compute.south) -- (redundant.north);

% Arrows from scarcity
\draw[Line] (scarcity.south) -- ++(0,-0.3) -| node[label, pos=0.25, left] {Domain} (simulator.north);
\draw[Line] (scarcity.south) -- ++(0,-0.3) -| node[label, pos=0.25, right] {Teacher} (distill.north);

% Arrows from oracle
\draw[Line] (oracle.south) -- ++(0,-0.3) -| node[label, pos=0.25, left] {Yes} (active.north);
\draw[Line] (oracle.south) -- ++(0,-0.3) -| node[label, pos=0.25, right] {No} (semisup.north);

% Arrows from redundant
\draw[Line] (redundant.south) -- ++(0,-0.3) -| node[label, pos=0.25, left] {High} (dedup.north);
\draw[Line] (redundant.south) -- ++(0,-0.3) -| node[label, pos=0.25, right] {Low} (curriculum.north);

% Arrows from simulator
\draw[Line] (simulator.south) -- ++(0,-0.3) -| node[label, pos=0.25, left] {Yes} (synthetic.north);
\draw[Line] (simulator.south) -- ++(0,-0.3) -| node[label, pos=0.25, right] {No} (augment.north);

\end{tikzpicture}
```
:::

The following text-based decision tree elaborates on each path, guiding practitioners from initial bottleneck identification through implementation.

**Step 1: Assess Your Bottleneck.** Identify which resource constraint most severely limits your training pipeline. If labeling cost dominates your budget, consider label efficiency techniques such as Active Learning, Semi-Supervised, or Self-Supervised learning. These methods maximize the value extracted from each human annotation. When compute cost is the primary concern, prioritize dataset reduction through Coreset selection, Deduplication, and Curriculum Learning, all of which reduce the number of training iterations required. If data scarcity is the fundamental problem, pursue data creation through Augmentation, Synthesis, and Distillation to expand your effective training set beyond what raw collection provides.

**Step 2: Check Prerequisites.** With the bottleneck identified, verify that the corresponding techniques are feasible given your infrastructure and data. Each approach carries specific requirements that must be met before implementation can begin (@tbl-technique-prerequisites).

| **Technique**            | **Prerequisites**                                           |
|:-----------------------|:----------------------------------------------------------|
| **Active Learning**      | Access to oracle, unlabeled pool, retraining infrastructure |
| **Coreset Selection**    | Proxy model or embedding extractor, full dataset accessible |
| **Curriculum Learning**  | Difficulty scoring method, pacing schedule                  |
| **Semi-Supervised**      | Some labeled data, unlabeled data from same distribution    |
| **Self-Supervised**      | Large unlabeled corpus, pre-training compute budget         |
| **Augmentation**         | Domain knowledge of invariances, augmentation library       |
| **Synthetic Generation** | Generative model or simulator, domain gap mitigation        |

: **Technique Prerequisites.** Required resources and capabilities for each data selection technique. Verify these requirements before committing to implementation. {#tbl-technique-prerequisites .striped .hover}

**Step 3: Estimate ROI.** Meeting the prerequisites is necessary but not sufficient. Before committing engineering resources, estimate the return on investment for each candidate technique:

$$
\text{ROI} = \frac{\text{(Baseline Cost)} - \text{(Technique Cost + Implementation Cost)}}{\text{Technique Cost + Implementation Cost}}
$$

A technique with high theoretical gains but high implementation cost may deliver lower ROI than a simpler approach. Deduplication, for example, often achieves the highest ROI because implementation cost is minimal and gains are immediate. Active Learning, by contrast, requires oracle access, retraining infrastructure, and selection algorithm development, so its ROI depends heavily on how many labeling cycles you expect to amortize that investment across.

**Step 4: Combine Techniques Strategically.** The techniques in this chapter are not mutually exclusive; in practice, the most effective pipelines combine multiple approaches. A typical production workflow begins by deduplicating the raw corpus for immediate gains at minimal cost. This cleaned dataset then undergoes coreset selection to identify the most informative samples. During training, curriculum learning orders these samples to optimize gradient quality, while data augmentation increases effective diversity at runtime. Finally, starting from a self-supervised foundation model rather than random initialization allows the pipeline to leverage knowledge learned from massive unlabeled corpora.

Each stage compounds the efficiency gains of previous stages, turning individual percentage improvements into multiplicative savings.

The preceding sections answer the *what* of data selection: which samples to prune, when to select dynamically, and how to synthesize new data. Understanding these algorithmic choices is essential, but algorithms alone do not translate into faster training. A perfectly designed coreset algorithm that takes 10 hours to select samples for a 2-hour training run yields no practical benefit. Similarly, a curriculum learning strategy that requires scanning the entire dataset to determine difficulty rankings may idle GPUs while CPUs compute scores. The *how* of implementation matters as much as the *what* of algorithm choice.

This gap between algorithmic elegance and practical value raises several systems questions. How do you avoid selection overhead negating your theoretical gains? How do you handle non-sequential I/O patterns that confuse prefetching logic? How do you coordinate selection decisions across distributed workers without introducing synchronization bottlenecks? The following sections address these engineering challenges, bridging the gap between data selection theory and production reality.

## Engineering Data Selection Systems {#sec-data-selection-engineering-data-selection-systems-7aef}

The strategies discussed so far (pruning, active learning, and synthesis) are algorithmic interventions. Implementing them at scale requires robust systems engineering. A naive active learning loop that scans the entire dataset every epoch to select the "best" samples will turn a compute-bound training job into an I/O-bound bottleneck. This section examines the architectural patterns required to implement data selection in production.

### The Selection Bottleneck {#sec-data-selection-selection-bottleneck-f1d4}

Dynamic data selection introduces a new bottleneck: **selection latency**. In standard training, the data loader reads the next batch sequentially. In active learning or curriculum learning, the system must evaluate a selection function $f(x)$ over a large candidate pool to determine the next batch.

For a selection strategy to be systems-efficient, it must satisfy the **Selection Inequality**:

$$ T_{selection} + T_{train}(N_{subset}) < T_{train}(N_{total}) $$

Here $T_{selection}$ is the time spent scoring the pool and $T_{train}$ is the compute time. If $f(x)$ requires a forward pass of a large model, the cost of selection can exceed the cost of training, producing negative ROI. A concrete scenario illustrates this trade-off.

```{python}
#| label: selection-inequality-calc
#| echo: false
from physx.formatting import fmt

# =============================================================================
# PURPOSE
# =============================================================================
# Purpose: Quantify selection inequality trade-offs for coreset selection
# Used in: Callout "Selection Inequality in Practice"

# =============================================================================
# INPUT
# =============================================================================
n_images_value = 1_000_000
n_coreset_value = 100_000
n_epochs_value = 100
resnet50_time_per_image_value = 0.01  # seconds
resnet18_time_per_image_value = 0.002
trap_sel_hrs_value = 50

# =============================================================================
# PROCESS
# =============================================================================
score_a_sec_value = n_images_value * resnet50_time_per_image_value
train_a_sec_value = n_coreset_value * n_epochs_value * resnet50_time_per_image_value
total_a_sec_value = score_a_sec_value + train_a_sec_value
total_a_hrs_value = total_a_sec_value / 3600

score_b_sec_value = n_images_value * resnet18_time_per_image_value
train_b_sec_value = train_a_sec_value  # same training
total_b_sec_value = score_b_sec_value + train_b_sec_value
total_b_hrs_value = total_b_sec_value / 3600

baseline_sec_value = n_images_value * n_epochs_value * resnet50_time_per_image_value
baseline_hrs_value = baseline_sec_value / 3600

savings_a_hrs_value = baseline_hrs_value - total_a_hrs_value
savings_a_pct_value = savings_a_hrs_value / baseline_hrs_value * 100
savings_b_hrs_value = baseline_hrs_value - total_b_hrs_value
savings_b_pct_value = savings_b_hrs_value / baseline_hrs_value * 100
b_beats_a_hrs_value = total_a_hrs_value - total_b_hrs_value

trap_total_hrs_value = trap_sel_hrs_value + train_a_sec_value / 3600
trap_overhead_pct_value = trap_sel_hrs_value / (baseline_hrs_value - trap_total_hrs_value) * 100

# =============================================================================
# OUTPUT
# =============================================================================
score_a_str = fmt(score_a_sec_value, precision=0, commas=True)
score_a_hrs_str = fmt(score_a_sec_value/3600, precision=1, commas=False)
train_a_str = fmt(train_a_sec_value, precision=0, commas=True)
train_a_hrs_str = fmt(train_a_sec_value/3600, precision=1, commas=False)
total_a_hrs_str = fmt(total_a_hrs_value, precision=1, commas=False)
score_b_str = fmt(score_b_sec_value, precision=0, commas=True)
score_b_hrs_str = fmt(score_b_sec_value/3600, precision=1, commas=False)
total_b_hrs_str = fmt(total_b_hrs_value, precision=1, commas=False)
baseline_str = fmt(baseline_sec_value, precision=0, commas=True)
baseline_hrs_str = fmt(baseline_hrs_value, precision=0, commas=False)
savings_a_str = fmt(savings_a_hrs_value, precision=0, commas=False)
savings_a_pct_str = fmt(savings_a_pct_value, precision=0, commas=False)
savings_b_str = fmt(savings_b_hrs_value, precision=0, commas=False)
savings_b_pct_str = fmt(savings_b_pct_value, precision=0, commas=False)
b_beats_a_str = fmt(b_beats_a_hrs_value, precision=1, commas=False)
trap_total_str = fmt(trap_total_hrs_value, precision=1, commas=False)
trap_pct_str = fmt(trap_overhead_pct_value, precision=0, commas=False)
```

::: {.callout-example title="Selection Inequality in Practice"}
**Scenario**: You have 1 million training images and want to select a 100k coreset (10%) using EL2N scoring.

**Option A: Full Model Selection**

- Score all 1M images with your target ResNet-50: 1M × 0.01 s = **`{python} score_a_str` seconds** (`{python} score_a_hrs_str` hours)
- Train on 100k coreset for 100 epochs: 100k × 100 × 0.01 s = **`{python} train_a_str` seconds** (`{python} train_a_hrs_str` hours)
- **Total: `{python} total_a_hrs_str` hours**

**Option B: Proxy Model Selection**

- Score all 1M images with a small proxy (ResNet-18): 1M × 0.002 s = **`{python} score_b_str` seconds** (`{python} score_b_hrs_str` hours)
- Train on 100k coreset for 100 epochs: **`{python} train_a_str` seconds** (`{python} train_a_hrs_str` hours)
- **Total: `{python} total_b_hrs_str` hours**

**Baseline: No Selection**

- Train on full 1M dataset for 100 epochs: 1M × 100 × 0.01 s = **`{python} baseline_str` seconds** (`{python} baseline_hrs_str` hours)

**Analysis**:

- Option A saves `{python} savings_a_str` hours vs. baseline (`{python} savings_a_pct_str`% reduction) ✓
- Option B saves `{python} savings_b_str` hours vs. baseline (`{python} savings_b_pct_str`% reduction) ✓
- Option B beats Option A by `{python} b_beats_a_str` hours. Proxy selection yields better ROI.

**The Trap**: If your selection required 50 hours (e.g., running a 7B parameter model), you would spend `{python} trap_total_str` hours total, still better than baseline, but the selection overhead consumes `{python} trap_pct_str`% of your savings.

**Rule of thumb**: Selection time should be <10% of subset training time for good ROI.
:::

The following analysis formalizes this heuristic, deriving what we call *the selection inequality*.

::: {.callout-notebook title="The Selection Inequality"}

```{python}
#| label: selection-inequality-math-calc
#| echo: false
from physx.formatting import fmt

n_epochs_full = 100
subset_fraction = 0.1
n_epochs_subset = n_epochs_full * subset_fraction
cost_selection_full = 1 # equivalent to 1 epoch
cost_total_efficient = cost_selection_full + n_epochs_subset
speedup_efficient = n_epochs_full / cost_total_efficient

cost_selection_iterative = n_epochs_full * 1
cost_total_iterative = cost_selection_iterative + n_epochs_subset

proxy_factor = 0.1
cost_selection_proxy = cost_selection_full * proxy_factor
cost_total_proxy = cost_selection_proxy + n_epochs_subset

n_epochs_full_str = fmt(n_epochs_full, precision=0, commas=False)
subset_fraction_pct_str = fmt(subset_fraction * 100, precision=0, commas=False)
cost_selection_full_str = fmt(cost_selection_full, precision=0, commas=False)
n_epochs_subset_str = fmt(n_epochs_subset, precision=0, commas=False)
cost_total_efficient_str = fmt(cost_total_efficient, precision=0, commas=False)
speedup_efficient_str = fmt(speedup_efficient, precision=0, commas=False)

cost_selection_iterative_str = fmt(cost_selection_iterative, precision=0, commas=False)
cost_total_iterative_str = fmt(cost_total_iterative, precision=0, commas=False)

proxy_factor_inv_str = fmt(1/proxy_factor, precision=0, commas=False)
cost_selection_proxy_str = fmt(cost_selection_proxy, precision=1, commas=False)
cost_total_proxy_str = fmt(cost_total_proxy, precision=1, commas=False)
```

**Problem**: You are using active learning to select the best `{python} subset_fraction_pct_str`% of samples for training. Your selection algorithm requires running the full model on the unlabeled pool. Is this efficient?

**The Math**:

1.  **Full Training**: `{python} n_epochs_full_str` epochs. Total cost = `{python} n_epochs_full_str` × C_epoch.
2.  **Selection (Full Model)**: Scoring the full dataset is equivalent to **`{python} cost_selection_full_str` epoch** of training. T_selection = `{python} cost_selection_full_str` × C_epoch.
3.  **Subset Training**: `{python} n_epochs_full_str` epochs on `{python} subset_fraction_pct_str`% data = `{python} n_epochs_full_str` × `{python} subset_fraction` × C_epoch = `{python} n_epochs_subset_str` × C_epoch.
4.  **Total Time**: `{python} cost_selection_full_str` + `{python} n_epochs_subset_str` = **`{python} cost_total_efficient_str`** × C_epoch.
5.  **Speedup**: `{python} n_epochs_full_str` / `{python} cost_total_efficient_str` ≈ **`{python} speedup_efficient_str`×**.

**The Trap**: If your selection algorithm is iterative (e.g., repeating selection every epoch), T_selection becomes `{python} n_epochs_full_str` × 1 = `{python} cost_selection_iterative_str` × C_epoch. Total time = `{python} cost_selection_iterative_str` + `{python} n_epochs_subset_str` = `{python} cost_total_iterative_str` × C_epoch. You are now **slower** than the baseline.

**The Failure Condition**: If the cost of selecting data exceeds the cost of training on the discarded data, you have failed. The goal is to spend compute to save *more* compute. As discussed in the coreset selection section, proxy models solve this problem by reducing T_selection by an order of magnitude.

:::

Look at the stacked bars in @fig-selection-inequality to see this trade-off in action: efficient selection (center) saves `{python} savings_b_pct_str`% of total compute, while expensive selection (right) consumes all the savings in overhead.

::: {#fig-selection-inequality fig-cap="**The Selection Inequality**\index{Data Selection Systems!selection inequality}: Data selection only improves end-to-end efficiency if the overhead of selection plus training on the subset is less than training on the full dataset. A lightweight selection function (proxy model, cached embeddings) keeps selection overhead low; an expensive selection function (full model forward pass) can negate the savings." fig-alt="Stacked bar chart comparing three approaches: Baseline shows a single tall bar (100) for full training; Efficient Selection shows two short stacked bars (5 selection overhead plus 40 subset training) totaling 45 with a 55 percent savings annotation; Expensive Selection shows two stacked bars (60 selection overhead plus 40 subset training) totaling 100 with a No savings annotation."}
```{python}
#| label: fig-selection-inequality
#| echo: false

import numpy as np
from physx import viz

fig, ax, COLORS, plt = viz.setup_plot(figsize=(8, 6))

# =============================================================================
# PLOT: The Selection Inequality
# =============================================================================
categories = ['Baseline', 'Efficient Selection', 'Expensive Selection']
full_train_cost = np.array([100, 0, 0])
selection_overhead = np.array([0, 5, 60])
subset_train_cost = np.array([0, 40, 40])

x = np.arange(len(categories))
width = 0.6

p1 = ax.bar(x, full_train_cost, width, label='Full Training', color=COLORS['BlueFill'], edgecolor=COLORS['BlueLine'])
p2 = ax.bar(x, selection_overhead, width, bottom=full_train_cost, label='Selection Overhead', color=COLORS['OrangeL'], edgecolor=COLORS['OrangeLine'])
p3 = ax.bar(x, subset_train_cost, width, bottom=full_train_cost + selection_overhead, label='Subset Training', color=COLORS['GreenFill'], edgecolor=COLORS['GreenLine'])

ax.set_ylabel('Total Time (Normalized)')
ax.set_xticks(x)
ax.set_xticklabels(categories)
ax.set_ylim(0, 130)
ax.legend(loc='upper right', ncol=3, fontsize=9)

ax.annotate("", xy=(1, 45), xytext=(1, 100), arrowprops=dict(arrowstyle="<->", color=COLORS['GreenLine'], lw=2))
ax.text(1.1, 72, "55% Savings", color=COLORS['GreenLine'], fontweight='bold', va='center')
ax.annotate("", xy=(2, 100), xytext=(0, 100), arrowprops=dict(arrowstyle="-", linestyle="--", color=COLORS['grid']))
ax.text(2, 105, "No Savings!", color=COLORS['RedLine'], fontweight='bold', ha='center')
plt.show()
```
:::

The lesson from @fig-selection-inequality is unambiguous: selection overhead can negate the benefits of training on a smaller subset. Before examining hardware-aware optimizations, verify your understanding of this trade-off.

::: {.callout-checkpoint title="The Selection Inequality" collapse="false"}
Data selection is not free. It introduces a new term to the **Iron Law**.

**The Equation**

- [ ] **Selection Cost**: Do you understand why $T_{selection} + T_{train}(subset)$ must be less than $T_{train}(full)$ for the technique to be valid?
- [ ] **Overhead Management**: How do proxy models and cached embeddings keep $T_{selection}$ low?

**Systems Implications**

- [ ] **I/O patterns**: Why does random access (required for dynamic selection) kill data loader throughput compared to sequential reads?
:::

### Hardware Empathy: The Random Access Penalty {#sec-data-selection-hardware-empathy-random-access-penalty-f9c1}

The selection inequality addresses compute overhead, but data selection introduces a second, often overlooked cost: I/O pattern degradation. Data selection strategies like coresets or dynamic sampling often require **random access** to samples across the dataset, jumping to sample 47,231, then 892,104, then 3,417 based on selection scores. Standard training uses sequential reads that benefit from hardware readahead and OS page caching; random access patterns devastate throughput, especially on distributed filesystems or traditional hard drives. @tbl-io-performance quantifies this penalty across storage tiers.

::: {#tbl-io-performance fig-cap="**The Cost of Randomness.** Comparative I/O throughput for sequential vs. random 4KB reads across different storage tiers. Standard data loaders optimize for sequential throughput, while data selection strategies often incur the random access penalty."}

| **Storage Tier** | **Sequential Throughput** | **Random I/O (IOPS)** | **Random Throughput (approx)** | **Random Penalty** |
|:---------------|------------------------:|--------------------:|-----------------------------:|-----------------:|
| **HDD (7.2k)**   |                 ~150 MB/s |                   ~80 |                      ~0.3 MB/s |           **500x** |
| **SATA SSD**     |                 ~550 MB/s |                  ~10k |                       ~40 MB/s |            **14x** |
| **NVMe SSD**     |               ~3,500 MB/s |                 ~500k |                    ~2,000 MB/s |          **1.75x** |
| **Cloud (S3)**   |      ~100 MB/s (per conn) |        ~10-50ms (lat) |            Very Low (per conn) |        **Extreme** |

:::

High-efficiency systems mitigate this penalty through several techniques. Small proxy models (a 10M parameter "student" scoring on behalf of a 7B "teacher") reduce selection cost by an order of magnitude while preserving ranking quality. Embedding indices (e.g., FAISS) transform selection from $O(N)$ linear scans into $O(\log N)$ lookups. Both approaches share a common principle: decoupling selection from training enables independent optimization.

Data loaders also require architectural adaptation. Modern formats like WebDataset or FFCV group thousands of samples into shards, enabling efficient bulk reads even when target samples are scattered. Shuffle buffers provide a practical approximation: the loader reads large sequential shards into memory and samples randomly within the buffer, preserving sequential I/O throughput while achieving the statistical benefits of random sampling.

### Data Echoing: Amortizing I/O Costs {#sec-data-selection-data-echoing-amortizing-io-costs-24e3}

The optimizations discussed so far address I/O bandwidth, but modern data selection pipelines introduce another bottleneck: CPU computation. Synthetic data generation and heavy augmentation shift the constraint from disk speed to augmentation throughput. Heavy augmentations like 3D rotations and MixUp, or on-the-fly generative synthesis, can leave the GPU idle if the CPU cannot keep pace with sample production. When the data pipeline produces samples slower than the GPU can consume them, GPU utilization drops and training time extends, negating the efficiency gains from smarter data selection.

Data echoing [@choi2020dataechoing] offers an elegant solution to this CPU-GPU imbalance. The technique reuses batches of data multiple times before fetching new samples, effectively trading sample diversity for GPU utilization. When the data pipeline (reading, decoding, augmenting) is slower than GPU processing, the GPU idles waiting for data. Data echoing fills this gap by "echoing" (repeating) each batch $e$ times, applying different augmentations to each repetition so that the model still sees varied inputs.

The optimal echo factor depends on the ratio $R$ of upstream processing time to downstream training time:

$$
R = \frac{T_{\text{data pipeline}}}{T_{\text{GPU training}}}
$$

If $R > 1$ (data pipeline is the bottleneck), set echo factor $e \leq R$ to fully utilize GPU capacity. If $R < 1$ (GPU is the bottleneck), data echoing provides no benefit. The following worked example calculates these trade-offs for a realistic scenario.

```{python}
#| label: data-echoing-calc
#| echo: false
from physx.formatting import fmt

# =============================================================================
# PURPOSE
# =============================================================================
# Purpose: Estimate training time impact from data echoing
# Used in: Worked example "Data Echoing ROI"

# =============================================================================
# INPUT
# =============================================================================
pipeline_throughput_value = 300  # images/sec
gpu_throughput_value = 800       # images/sec
n_epochs_echo_value = 90
imagenet_size_value = 1_280_000  # ~1.28M images
echo_factor_value = 2

# =============================================================================
# PROCESS
# =============================================================================
ratio_r_value = gpu_throughput_value / pipeline_throughput_value
gpu_idle_pct_value = (1 - pipeline_throughput_value / gpu_throughput_value) * 100

no_echo_throughput_value = pipeline_throughput_value
no_echo_sec_value = n_epochs_echo_value * imagenet_size_value / no_echo_throughput_value
no_echo_hrs_value = no_echo_sec_value / 3600
gpu_util_no_echo_value = pipeline_throughput_value / gpu_throughput_value * 100

echo_throughput_value = pipeline_throughput_value * echo_factor_value
echo_sec_value = n_epochs_echo_value * imagenet_size_value / echo_throughput_value
echo_hrs_value = echo_sec_value / 3600

# =============================================================================
# OUTPUT
# =============================================================================
pipeline_throughput_str = fmt(pipeline_throughput_value, precision=0, commas=False)
gpu_throughput_str = fmt(gpu_throughput_value, precision=0, commas=False)
ratio_str = fmt(ratio_r_value, precision=2, commas=False)
idle_pct_str = fmt(gpu_idle_pct_value, precision=0, commas=False)
no_echo_sec_str = fmt(no_echo_sec_value, precision=0, commas=True)
no_echo_hrs_str = fmt(no_echo_hrs_value, precision=0, commas=False)
gpu_util_str = fmt(gpu_util_no_echo_value, precision=0, commas=False)
echo_sec_str = fmt(echo_sec_value, precision=0, commas=True)
echo_hrs_str = fmt(echo_hrs_value, precision=0, commas=False)
echo_factor_str = fmt(echo_factor_value, precision=0, commas=False)
effective_throughput_str = fmt(echo_throughput_value, precision=0, commas=False)
```

::: {.callout-example title="Worked Example: Data Echoing ROI"}
**Scenario**: Training ResNet-50 on ImageNet with heavy augmentation (RandAugment + MixUp).

**Measurements**:

- Data pipeline throughput: `{python} pipeline_throughput_str` images/second (reading, decoding, augmenting on CPU)
- GPU training throughput: `{python} gpu_throughput_str` images/second (forward + backward pass)
- Ratio $R = T_{\text{pipeline}} / T_{\text{GPU}}$ = (1/`{python} pipeline_throughput_str`) / (1/`{python} gpu_throughput_str`) = `{python} gpu_throughput_str`/`{python} pipeline_throughput_str` ≈ `{python} ratio_str` (GPU waiting `{python} idle_pct_str`% of time)

**Without Echoing**:

- Effective throughput: `{python} pipeline_throughput_str` images/second (limited by data pipeline)
- Training time for 90 epochs: 90 × 1.28M / `{python} pipeline_throughput_str` = **`{python} no_echo_sec_str` seconds (`{python} no_echo_hrs_str` hours)**
- GPU utilization: ~`{python} gpu_util_str`%

**With Echo Factor $e$ = `{python} echo_factor_str`**:

- Each batch is processed twice with different augmentations
- Effective throughput: `{python} effective_throughput_str` images/second (still below GPU capacity)
- Unique images per second: `{python} pipeline_throughput_str` (unchanged)
- Training time: 90 × 1.28M / `{python} effective_throughput_str` = **`{python} echo_sec_str` seconds (`{python} echo_hrs_str` hours)** if echoed data is equally valuable

**Echoed data has diminishing returns**: Research shows echoed samples provide approximately 70–90% of the value of fresh samples, depending on augmentation diversity. Empirically, Choi et al. measured a **3.25$\times$ speedup** on ResNet-50 ImageNet training when reading data over a network, with minimal accuracy degradation.

**The Trade-Off**: Data echoing trades sample diversity for GPU utilization. It works best when:

1. Augmentation is diverse (each echo sees different transforms)
2. The dataset is already somewhat redundant
3. The echo factor $e$ stays below the critical threshold (~$4\times$ for ImageNet)

Above this threshold, the model starts memorizing and accuracy degrades.
:::

Data echoing also interacts with batch normalization. When the same image appears multiple times in a batch (or across nearby batches), batch normalization statistics become less representative of the true data distribution. This correlation violates the independence assumption underlying batch normalization's effectiveness. Practitioners address this by excluding consecutive echoes from the same batch or by maintaining separate batch normalization statistics for echoed samples.

These engineering patterns provide production-ready implementations of data selection principles. Proxy selection reduces the computational cost of identifying valuable samples. Sharded formats and shuffle buffers reconcile random access algorithms with sequential storage hardware. Data echoing maximizes GPU utilization when the data pipeline becomes the bottleneck. Together, they transform data selection from an algorithmic idea into a deployable system. The question then becomes: which techniques merit investment for a given workload?

## Cost Modeling and Economics {#sec-data-selection-cost-modeling-economics-b702}

The systems framing of data selection demands quantitative answers: *Should I label 10,000 more samples or buy more GPU hours? When does active learning pay for itself? What is the ROI of investing in deduplication infrastructure?*

### Quantifying Data Costs and ROI {#sec-data-selection-total-cost-training-data-92b9}

Answering these questions requires understanding what training data actually costs. Total expense encompasses the full lifecycle of data acquisition, preparation, and utilization, extending well beyond storage fees:

$$
C_{\text{total}} = C_{\text{acquire}} + C_{\text{label}} + C_{\text{store}} + C_{\text{process}}
$$

where:

| **Component**            | **Formula**                                           |                             **Typical Range** |
|:-----------------------|:----------------------------------------------------|--------------------------------------------:|
| **$C_{\text{acquire}}$** | $N \times c_{\text{sample}}$                          | \$0.001–\$10/sample (web scrape vs. licensed) |
| **$C_{\text{label}}$**   | $N_{\text{labeled}} \times c_{\text{label}}$          |        \$0.10–\$100/sample (crowd vs. expert) |
| **$C_{\text{store}}$**   | $S_{\text{bytes}} \times c_{\text{storage}} \times T$ |                        \$0.02–\$0.10/GB/month |
| **$C_{\text{process}}$** | $N \times E \times c_{\text{FLOP}}$                   |                Proportional to training FLOPs |

For a concrete example, consider training a vision model:

```{python}
#| label: cost-breakdown-calc
#| echo: false

# =============================================================================
# PURPOSE
# =============================================================================
# Purpose: Compute cost breakdown for supervised training example
# Used in: Callout "Cost Breakdown: ImageNet-Scale Training"

# =============================================================================
# INPUT
# =============================================================================
c_raw_value = 50000
n_labels_value = 1_200_000
cost_per_label_value = 0.05
c_store_value = 200
c_train_value = 25000

# =============================================================================
# PROCESS
# =============================================================================
c_label_value = n_labels_value * cost_per_label_value
c_total_value = c_raw_value + c_label_value + c_store_value + c_train_value

p_data_value = (c_raw_value + c_label_value + c_store_value) / c_total_value * 100
p_compute_value = c_train_value / c_total_value * 100

# =============================================================================
# OUTPUT
# =============================================================================
c_raw_str = f"${c_raw_value:,}"
c_label_str = f"${c_label_value:,.0f}"
c_store_str = f"${c_store_value}"
c_train_str = f"${c_train_value:,}"
c_total_str = f"${c_total_value:,.0f}"
p_data_str = f"{p_data_value:.0f}%"
p_compute_str = f"{p_compute_value:.0f}%"

n_labels_str = fmt(n_labels_value / 1e6, precision=1, unit="M")
cost_per_label_str = fmt(cost_per_label_value, precision=2, commas=False)
storage_gb = 150
storage_months = 12
storage_str = f"{storage_gb} GB × {storage_months} months"
train_epochs = 100
train_gpus = 8
train_hours = 24
train_desc_str = f"{train_epochs} epochs × {train_gpus} A100s × {train_hours} h"
```

::: {.callout-example title="Cost Breakdown: ImageNet-Scale Training"}
| **Cost Component**                                                       | **Calculation**  | **Amount**                                                       |
|:-----------------------------------------------------------------------|:---------------|:---------------------------------------------------------------|
| **Raw data (`{python} n_labels_str` images)**                            | Licensed dataset | `{python} c_raw_str`                                             |
| **Labels (`{python} n_labels_str` × USD `{python} cost_per_label_str`)** | Crowd annotation | `{python} c_label_str`                                           |
| **Storage (`{python} storage_str`)**                                     | Cloud storage    | `{python} c_store_str`                                           |
| **Training (`{python} train_desc_str`)**                                 | GPU compute      | `{python} c_train_str`                                           |
| **Total**                                                                |                  | **`{python} c_total_str`**                                       |
| **Data vs. Compute ratio**                                               |                  | **`{python} p_data_str` data, `{python} p_compute_str` compute** |

This ratio, where data costs dominate, is typical for supervised learning. The ratio inverts for self-supervised learning on web-scraped data, where compute dominates.
:::

**ROI Framework for Data Selection Techniques.** Understanding total costs enables rational decisions about which efficiency techniques merit investment. Every technique carries both a cost (implementation effort, compute overhead) and a benefit (reduced data requirements, faster training). Comparing these trade-offs requires a common framework: **Return on Investment (ROI)**.

$$
\text{ROI} = \frac{\text{Savings} - \text{Investment}}{\text{Investment}} \times 100\%
$$

The challenge lies in quantifying both sides accurately. Different techniques offer distinct cost-benefit profiles:

| **Technique**         | **Investment (Cost)**                                   | **Savings (Benefit)**                                        |
|:--------------------|:------------------------------------------------------|:-----------------------------------------------------------|
| **Deduplication**     | One-time compute for hashing + infrastructure           | Reduced storage, fewer epochs for same accuracy              |
| **Coreset Selection** | Proxy model training + selection compute                | Train on 10–50% of data with minimal accuracy loss           |
| **Active Learning**   | Inference on unlabeled pool + human-in-the-loop latency | 2–10$\times$ reduction in labeling budget for same acc.      |
| **Data Augmentation** | CPU/GPU cycles for transforms                           | Effective dataset size increase without new data acquisition |

### Break-Even Analysis {#sec-data-selection-breakeven-analysis-ec3a}

ROI calculations assume that techniques deliver their promised benefits, but actual outcomes vary. For any technique, there exists a **break-even point** where investment equals savings. Below this threshold, the technique costs more than it saves; above it, the technique generates value. Identifying this threshold determines whether a technique makes sense for a given project.

**Example: Active Learning Break-Even**

```{python}
#| label: breakeven-calc
#| echo: false
from physx.formatting import fmt

cost_label = 10
n_initial = 1000
n_queries_per_round = 100
cost_inference = 50
n_random = 5000
n_active = 2000
n_rounds = 10

cost_random_total = n_random * cost_label
cost_active_label = n_active * cost_label
cost_active_inference = n_rounds * cost_inference
cost_active_total = cost_active_label + cost_active_inference

roi_pct = (cost_random_total - cost_active_total) / cost_active_total * 100

cost_label_str = fmt(cost_label, precision=0, commas=False)
n_initial_str = fmt(n_initial, precision=0, commas=True)
cost_initial_str = fmt(n_initial * cost_label, precision=0, commas=True)
n_queries_str = fmt(n_queries_per_round, precision=0, commas=False)
cost_inference_str = fmt(cost_inference, precision=0, commas=False)
n_random_str = fmt(n_random, precision=0, commas=True)
n_active_str = fmt(n_active, precision=0, commas=True)
n_rounds_str = fmt(n_rounds, precision=0, commas=False)

cost_random_total_str = fmt(cost_random_total, precision=0, commas=True)
cost_active_label_str = fmt(cost_active_label, precision=0, commas=True)
cost_active_inference_str = fmt(cost_active_inference, precision=0, commas=True)
cost_active_total_str = fmt(cost_active_total, precision=0, commas=True)
roi_pct_str = fmt(roi_pct, precision=0, commas=False)
```

Suppose labeling costs \$`{python} cost_label_str`/sample and active learning requires:

- Initial labeled set: `{python} n_initial_str` samples (\$`{python} cost_initial_str`)
- Oracle queries per round: `{python} n_queries_str` samples
- Inference cost per round: \$`{python} cost_inference_str` (scoring unlabeled pool)
- Target accuracy achievable with `{python} n_random_str` random samples

If active learning reaches target accuracy with only `{python} n_active_str` labeled samples:

**Random labeling cost** = `{python} n_random_str` × \$`{python} cost_label_str` = **\$`{python} cost_random_total_str`**

**Active learning cost** = `{python} n_active_str` × \$`{python} cost_label_str` + `{python} n_rounds_str` rounds × \$`{python} cost_inference_str` = **\$`{python} cost_active_total_str`**

**ROI** = (\$`{python} cost_random_total_str` − \$`{python} cost_active_total_str`) / \$`{python} cost_active_total_str` × 100% = **`{python} roi_pct_str`%**

The break-even occurs when the labeling reduction equals the selection overhead. If active learning only reduces labeling by 20%, and selection overhead is high, ROI may be negative.

### Amortization: The Time Value of Data Selection {#sec-data-selection-amortization-time-value-data-selection-26c5}

Break-even analysis captures a snapshot in time, but many data selection investments span multiple projects. Techniques with high upfront costs yield significant returns when their benefits compound across repeated training runs. **Amortized ROI** accounts for this temporal dimension:

$$
\text{Amortized ROI} = \frac{N_{runs} \times \text{Per-Run Savings} - \text{One-Time Investment}}{\text{One-Time Investment}}
$$

**Example: Deduplication Infrastructure**

```{python}
#| label: amortization-calc
#| echo: false
from physx.formatting import fmt

cost_build = 50000
cost_compute_once = 5000
cost_investment = cost_build + cost_compute_once
savings_per_run = 10000

runs = [1, 5, 10, 50]
rois = [(r * savings_per_run - cost_investment) / cost_investment * 100 for r in runs]

cost_build_str = fmt(cost_build, precision=0, commas=True)
cost_compute_once_str = fmt(cost_compute_once, precision=0, commas=True)
savings_per_run_str = fmt(savings_per_run, precision=0, commas=True)

roi_1_str = fmt(rois[0], precision=0, commas=False)
roi_5_str = fmt(rois[1], precision=0, commas=False)
roi_10_str = fmt(rois[2], precision=0, commas=False)
roi_50_str = fmt(rois[3], precision=0, commas=False)
```

| **Component**                             |                                       **Cost** |
|:----------------------------------------|---------------------------------------------:|
| **Build deduplication pipeline**          | \$`{python} cost_build_str` (engineering time) |
| **Compute MinHash signatures (one-time)** |             \$`{python} cost_compute_once_str` |
| **Per-run savings (20% less data)**       |           \$`{python} savings_per_run_str`/run |

| **Number of Runs** |                           **Amortized ROI** |
|:-----------------|------------------------------------------:|
| 1 run              |            `{python} roi_1_str`% (net loss) |
| 5 runs             |     `{python} roi_5_str`% (near break-even) |
| 10 runs            |          +`{python} roi_10_str`% (positive) |
| 50 runs            | +`{python} roi_50_str`% (highly profitable) |

This pattern reveals which circumstances favor infrastructure investment. Data selection investments deliver the highest returns under three conditions: training runs repeat frequently (hyperparameter search, model iterations, or scheduled retraining); datasets are shared across multiple teams or model architectures; and the technique generalizes broadly. Deduplication exemplifies a high-transfer investment because it benefits all models trained on the cleaned dataset. Task-specific coresets, by contrast, may not transfer across architectures, limiting their amortization potential. For one-off training runs, simple techniques like random sampling or basic augmentation often yield better ROI than sophisticated methods requiring substantial infrastructure investment. The following guidelines summarize these considerations.

::: {.callout-perspective title="When to Invest in Data Selection"}
**High ROI scenarios:**

- Labeling is expensive (medical, legal, scientific domains)
- Dataset is large and redundant (web-scraped corpora)
- Training runs are repeated frequently (hyperparameter search, retraining)
- Iteration speed matters more than final accuracy

**Low ROI scenarios:**

- Labeling is cheap or already done
- Dataset is small and curated
- Single training run (one-time cost)
- Accuracy matters more than efficiency
:::

## Distributed Data Selection {#sec-data-selection-distributed-data-selection-25a8}

The preceding sections assumed centralized access to the full dataset: a single-machine view where one process can see the entire dataset, compute global statistics, and make coordinated selection decisions. This assumption simplifies algorithm design: coreset selection can rank all samples globally, curriculum learning can establish a universal difficulty ordering, and active learning can query the single most uncertain example. Production ML training breaks this assumption. When data is sharded across hundreds of workers, each seeing only a local slice, fundamental questions arise: How do you compute a global coreset when no single node sees all samples? How do you maintain consistent curriculum difficulty rankings when the model updates asynchronously across workers?

The distributed training infrastructure that underlies these challenges---including collective communication, fault tolerance, and elastic scheduling---constitutes an advanced topic beyond this chapter's scope. This section focuses specifically on how data selection techniques adapt to distributed settings, and where they fail to do so.

### Strategies for Distributed Selection {#sec-data-selection-strategies-distributed-selection-fcd6}

In standard distributed training, data parallelism is straightforward: shard the dataset across workers, each processes its shard independently. Data selection techniques, however, introduce **selection dependencies**:

| **Technique**           | **Single-Node Assumption**      | **Distributed Challenge**                    |
|:----------------------|:------------------------------|:-------------------------------------------|
| **Coreset Selection**   | Global view of dataset          | Each worker sees only its shard              |
| **Active Learning**     | Centralized uncertainty scoring | Scoring requires model synchronization       |
| **Curriculum Learning** | Global difficulty ordering      | Workers may have different "hardest" samples |
| **Deduplication**       | Hash table fits in memory       | Distributed hash tables add latency          |

These selection dependencies admit several architectural solutions, each navigating a different point in the consistency-scalability trade-off space.

The most straightforward approach centralizes selection while distributing training. A coordinator node performs selection on the full dataset, then distributes selected indices to workers. This preserves selection quality but introduces a single bottleneck:

```
Coordinator: score_all_samples() → selected_indices
Broadcast: selected_indices → all workers
Workers: train on subset(local_shard, selected_indices)
```

The semantics remain clean, but the coordinator becomes a single point of failure and a bandwidth bottleneck for large selections. For modest cluster sizes, this overhead is acceptable; for thousand-node deployments, it becomes prohibitive.

Hierarchical selection addresses this scalability limitation by distributing the selection computation itself. Each worker performs local selection on its shard, then a coordinator merges results:

```
Workers: local_selected = select_top_k(local_shard)
Coordinator: global_selected = merge_and_rerank(all local_selected)
Broadcast: final_indices → all workers
```

This approach reduces coordinator load substantially, but introduces a quality trade-off: the system may miss globally important samples that appear unimportant within their local shard. A sample that is only moderately difficult on one worker might be the hardest example in the entire dataset when considered globally.

When even hierarchical approaches prove too expensive, approximate global selection offers a fallback. These methods trade exactness for scalability through distributed approximate algorithms. Distributed MinHash enables deduplication by having each worker compute MinHash signatures independently; signatures are then aggregated to find near-duplicates across shards without requiring any single node to see all the data. Similarly, federated uncertainty sampling allows workers to compute local uncertainty scores, with a global threshold determined by score distribution statistics rather than exact ranking.

### Consistency Challenges in Active Learning {#sec-data-selection-consistency-challenges-active-learning-c071}

The approximate selection strategies above assume static selection criteria, but active learning introduces an additional complication: the model changes during selection. Consider what happens when Worker A scores samples using the model at step $t$ while Worker B simultaneously updates the model to step $t+1$. Worker A's scores are now stale and may select samples that the updated model would rank differently.

Several strategies mitigate this staleness problem, each with distinct overhead characteristics. Synchronous scoring forces all workers to pause training and score simultaneously, guaranteeing consistency but at substantial cost in GPU utilization. Periodic score refresh offers a middle ground by re-scoring every $k$ epochs rather than every batch, trading freshness for reduced overhead. The most robust approach selects samples that exhibit high uncertainty under multiple model checkpoints, ensuring that selection decisions remain valid even as the model evolves. The following example demonstrates how these distributed selection strategies combine in practice.

::: {.callout-example title="Distributed Coreset Selection"}
**Scenario**: Select a 10% coreset from ImageNet (1.2M images) using 8 workers with 4 GPUs each.

**Architecture**:

```{.tikz}
\begin{tikzpicture}[font=\small\usefont{T1}{phv}{m}{n}]
% Coordinator node
\node[draw, rectangle, rounded corners, fill=blue!10, minimum width=10cm, minimum height=2cm, align=left] (coord) at (0, 2.5) {
    \textbf{Coordinator Node}\\[2pt]
    \textbullet\ Maintains global embedding index (FAISS)\\
    \textbullet\ Merges local selections\\
    \textbullet\ Broadcasts final coreset indices
};

% Worker nodes
\node[draw, rectangle, rounded corners, fill=green!10, minimum width=2.5cm, minimum height=1.8cm, align=center] (w0) at (-4, -1) {
    \textbf{Worker 0}\\
    150K images\\
    Local EL2N
};

\node[draw, rectangle, rounded corners, fill=green!10, minimum width=2.5cm, minimum height=1.8cm, align=center] (w1) at (0, -1) {
    \textbf{Worker 1}\\
    150K images\\
    Local EL2N
};

\node[draw, rectangle, rounded corners, fill=green!10, minimum width=2.5cm, minimum height=1.8cm, align=center] (wn) at (4, -1) {
    \textbf{Worker N}\\
    150K images\\
    Local EL2N
};

% Arrows
\draw[->, thick] (w0.north) -- node[left, font=\footnotesize\usefont{T1}{phv}{m}{n}] {local\_scores} (w0.north |- coord.south);
\draw[->, thick] (w1.north) -- node[right, font=\footnotesize\usefont{T1}{phv}{m}{n}] {local\_scores} (w1.north |- coord.south);
\draw[->, thick] (wn.north) -- node[right, font=\footnotesize\usefont{T1}{phv}{m}{n}] {local\_scores} (wn.north |- coord.south);

% Ellipsis between workers
\node at (2, -1) {\Large$\cdots$};
\node at (-2, -1) {\Large$\cdots$};
\end{tikzpicture}
```

**Pipeline**:

1. **Embedding phase** (parallel): Each worker computes ResNet-18 embeddings for its shard → store in shared filesystem
2. **Deduplication phase** (distributed): Coordinator builds FAISS index, workers query for near-duplicates → remove 15% duplicates
3. **Scoring phase** (parallel): Each worker computes EL2N scores on its deduplicated shard using proxy model
4. **Selection phase** (centralized): Coordinator collects top-20% scores from each worker, re-ranks globally, selects final 10%
5. **Broadcast**: Selected indices distributed to all workers for training

```{python}
#| label: distributed-overhead-calc
#| echo: false

# =============================================================================
# PURPOSE
# =============================================================================
# Purpose: Estimate distributed coreset selection overhead
# Used in: Distributed selection performance summary

# =============================================================================
# INPUT
# =============================================================================
t_embed_value = 20
t_dedup_value = 15
t_score_value = 30
t_select_value = 2

# =============================================================================
# PROCESS
# =============================================================================
t_total_overhead_value = t_embed_value + t_dedup_value + t_score_value + t_select_value

# =============================================================================
# OUTPUT
# =============================================================================
t_embed_str = f"{t_embed_value} minutes"
t_dedup_str = f"{t_dedup_value} minutes"
t_score_str = f"{t_score_value} minutes"
t_select_str = f"{t_select_value} minutes"
t_total_overhead_str = f"{t_total_overhead_value} minutes"
```

**Performance** (measured on $8\times$ A100 cluster):

- Embedding: `{python} t_embed_str` (parallel)
- Deduplication: `{python} t_dedup_str` (distributed hash join)
- Scoring: `{python} t_score_str` (parallel, 5 epochs proxy training)
- Selection: `{python} t_select_str` (centralized)
- **Total overhead: `{python} t_total_overhead_str`** for $10\times$ training speedup

**Key insight**: The `{python} t_total_overhead_str` selection overhead pays for itself if full training takes >12 hours. For ImageNet with modern architectures, full training is ~24 hours, so coreset selection has clear positive ROI.
:::

This positive ROI can erode quickly when workers must coordinate frequently during training. Distributed data selection always incurs a *coordination tax*: the overhead of maintaining consistent selection across workers. This tax must be smaller than the efficiency gains, or distributed selection yields negative ROI. As a rule of thumb, if selection overhead exceeds 10% of training time, simplify the selection strategy or increase the selection interval.

The preceding sections treated data selection as an isolated optimization: techniques to reduce dataset size, select better samples, or generate synthetic data. Real ML systems, however, combine multiple optimizations simultaneously. How do these optimizations interact?

## Interactions with Other Optimizations {#sec-data-selection-interactions-optimizations-02bc}

Data selection does not exist in isolation. A coreset-trained model will eventually be quantized for deployment. A curriculum-learning pipeline will run on specialized accelerators. An actively-learned dataset will feed into distributed training. These downstream optimizations interact with data selection in ways that can amplify gains or introduce unexpected conflicts. Understanding these interactions helps practitioners design end-to-end efficient systems rather than optimizing components independently.

**Data Selection and Model Compression.** Model compression (@sec-model-compression) reduces the size of the trained model through pruning, quantization, and distillation. The training dataset directly affects how compressible the resulting model becomes. Perhaps counterintuitively, models trained on smaller, higher-quality datasets may be *more* compressible than those trained on larger, noisier ones.

The mechanism behind this effect relates to how models encode information. A model trained on repetitive data learns redundant features that pruning later removes. The training compute required to learn those features was wasted, only to be discarded during compression. By contrast, a model trained on diverse, informative samples learns compact, non-redundant representations from the start, making subsequent compression more effective.

Empirical evidence supports this relationship. In experiments on ImageNet, models trained on 50% coresets selected by EL2N compress to 4-bit precision with 2% less accuracy loss than models trained on the full dataset. The curated training led to cleaner weight distributions that quantize more gracefully.

Data selection and model compression are therefore *complementary*. The techniques in this chapter can reduce both training cost *and* post-training compression effort. When planning an efficiency pipeline, apply data selection first; the resulting model will be easier to compress.

**Data Selection and Hardware Acceleration.** While model compression affects what happens after training, hardware acceleration determines how efficiently training itself proceeds. Hardware acceleration (@sec-ai-acceleration) increases throughput through specialized accelerators, kernel optimization, and parallelization. Data selection affects which hardware bottlenecks dominate, and this relationship is more nuanced than simple speedup calculations suggest.

| **Scenario**                  | **Likely Bottleneck**               | **Hardware Optimization**                 |
|:----------------------------|:----------------------------------|:----------------------------------------|
| **Large, sequential dataset** | Memory bandwidth                    | Larger batch sizes, gradient accumulation |
| **Small, curated dataset**    | Compute (GPU idle waiting for data) | Faster data loaders, data echoing         |
| **Dynamic selection**         | Selection compute                   | Proxy models, cached embeddings           |

Data selection can therefore shift the system from one bottleneck regime to another. A technique that reduces dataset size by 80% may move the bottleneck from I/O to GPU compute, requiring different hardware optimizations. Before applying aggressive data reduction, profile your system to understand which bottleneck you're targeting.

**Data Selection and Distributed Training.** The hardware bottleneck analysis above assumes single-machine training. The interactions become more complex when scaling to multiple machines, because data selection affects different parallelism strategies in distinct ways.

Under strong scaling, where a fixed dataset is distributed across more workers, data selection reduces communication overhead by reducing gradient updates per epoch. Fewer samples means fewer synchronization points, and communication costs often dominate at large worker counts. Under weak scaling, where each worker processes more data as the cluster grows, data selection techniques can maintain accuracy while adding workers without proportionally increasing total data. This capability proves essential when data collection rather than compute is the bottleneck. Even within straightforward data parallelism, smaller curated datasets reduce per-worker shard sizes, potentially improving cache utilization and reducing I/O stalls on each node.

These benefits must be weighed against the distributed selection challenges discussed in @sec-data-selection-distributed-data-selection-25a8. A technique that works well on a single GPU may incur prohibitive coordination overhead across 1,000 workers, negating its efficiency gains.

### The Optimization Stack {#sec-data-selection-optimization-stack-05ca}

The preceding sections examined pairwise interactions, but production systems apply all these optimizations together. Trace the full optimization stack in @fig-optimization-stack, from data to deployment: each stage in this pipeline amplifies or attenuates the effects of others.

::: {#fig-optimization-stack fig-cap="**The Optimization Stack**: The complete pipeline from raw data to deployed system, showing how optimizations at each stage propagate downstream. Data artifacts (rounded boxes) flow through processing stages (rectangular boxes). Optimizations early in the pipeline---particularly data selection---have multiplicative effects because they reduce the workload for all subsequent stages." fig-alt="Pipeline diagram with two rows. Top row shows Raw Data flowing through Data Selection to Curated Data, then through Training to produce a Model. Bottom row shows the Model flowing through Compression to a Compact Model, then through Hardware optimization to a Deployed System. Arrows indicate the flow direction between stages."}
```{.tikz}
\begin{tikzpicture}[
    font=\small\usefont{T1}{phv}{m}{n},
    data/.style={rectangle, rounded corners, draw=BlueLine, fill=BlueL, line width=0.75pt, minimum height=0.8cm, align=center},
    process/.style={rectangle, draw=OrangeLine, fill=OrangeL, line width=0.75pt, minimum height=0.8cm, align=center},
    arrow/.style={->, thick, >=stealth, draw=black!60}
]
% Standard color definitions
\definecolor{BlueLine}{HTML}{006395}
\definecolor{BlueL}{HTML}{D1E6F3}
\definecolor{OrangeLine}{HTML}{CC5500}
\definecolor{OrangeL}{HTML}{FFE5CC}

% Row 1
\node[data] (raw) at (0, 0) {Raw Data};
\node[process] (de) at (2.8, 0) {Data Selection};
\node[data] (curated) at (5.8, 0) {Curated Data};
\node[process] (train) at (8.4, 0) {Training};
\node[data] (model) at (10.8, 0) {Model};

% Row 2
\node[process] (comp) at (2.2, -1.2) {Compression};
\node[data] (compact) at (5.2, -1.2) {Compact Model};
\node[process] (hw) at (8, -1.2) {Hardware};
\node[data] (deployed) at (11.2, -1.2) {Deployed System};

% Arrows row 1
\draw[arrow] (raw) -- (de);
\draw[arrow] (de) -- (curated);
\draw[arrow] (curated) -- (train);
\draw[arrow] (train) -- (model);

% Connector from row 1 to row 2
\draw[arrow] (model.south) -- ++(0, -0.3) -| (comp.north);

% Arrows row 2
\draw[arrow] (comp) -- (compact);
\draw[arrow] (compact) -- (hw);
\draw[arrow] (hw) -- (deployed);
\end{tikzpicture}
```
:::

The pipeline in @fig-optimization-stack reveals why data selection occupies a strategic position: it sits at the head of the optimization stack. Reducing the dataset by 50% through intelligent selection doesn't just halve data processing time---it halves the training compute, which in turn produces a model that may require less aggressive compression, which then demands less from hardware acceleration. Each downstream stage inherits the efficiency gains (or quality losses) from upstream decisions.

This **multiplicative effect** means that every FLOP saved in data processing is a FLOP that never needs to be executed, compressed, or accelerated. Conversely, poor data selection that degrades model quality forces downstream stages to compensate---either through longer training, less aggressive compression, or over-provisioned hardware.

How do we quantify this multiplicative effect? How do we know whether a 50% dataset reduction actually delivers 50% compute savings, or whether it has inadvertently degraded model quality in ways that surface only in production?

## Measuring Data Selection {#sec-data-selection-measuring-data-selection-7957}

The techniques in this chapter (coreset selection, active learning, augmentation) all claim to improve efficiency, and rigorous measurement separates effective techniques from intuition.

### Core Metrics {#sec-data-selection-core-metrics-c0b5}

**Performance-Per-Data (PPD)**: The most direct metric measures accuracy gain per sample:

$$
\text{PPD}(n) = \frac{\text{Accuracy}(n) - \text{Accuracy}(0)}{n}
$$

where $n$ is the number of training samples. A higher PPD indicates more efficient use of data. The key insight is that PPD exhibits **diminishing returns**: the first 10,000 samples contribute far more to model performance than the next 10,000.

**Area Under the Learning Curve (AULC)**: Rather than comparing at a single point, AULC integrates performance across all dataset sizes:

$$
\text{AULC} = \int_0^N \text{Accuracy}(n) \, dn
$$

A data-efficient strategy has higher AULC because it achieves good accuracy faster. This metric is particularly useful for comparing coreset selection algorithms.

**Data Compression Ratio (DCR)**: For coreset methods, measure how much data reduction is achieved at a target accuracy:

$$
\text{DCR} = \frac{N_{\text{full}}}{N_{\text{coreset}}} \text{ at } \text{Accuracy}_{\text{target}}
$$

A DCR of 5$\times$ means the coreset achieves target accuracy with 20% of the data.

### The Compute-Optimal Frontier {#sec-data-selection-data-roofline-model-b400}

The metrics above measure individual techniques. How, then, do you diagnose whether your *overall* training strategy is data-limited or compute-limited? The diagnostic question is: given my current performance, am I limited by data quality or by training compute? Scaling laws provide the answer.

Research on neural scaling laws [@kaplan2020scaling; @hoffmann2022training] established that model performance follows predictable power laws with respect to compute, data, and model size. These laws provide more than theoretical interest: they offer a diagnostic framework for understanding whether your training is limited by data quality or compute budget. For the relationship between scaling laws and the **Information Roofline**—the theoretical ceiling on what can be learned from any dataset—see @sec-appendix-dam.

The Chinchilla study [@hoffmann2022training] revealed a key insight: for any fixed compute budget, there exists an **optimal balance** between model size and training data. Train on too little data relative to model size, and you waste compute on an undertrained model. Train on too much data with too small a model, and you waste data on a model that cannot absorb it.

This optimal balance defines a **compute-optimal frontier**\index{Compute-optimal frontier!Chinchilla scaling laws}: the best achievable performance at each compute budget when data and model size are properly balanced (@fig-compute-optimal-frontier).

::: {#fig-compute-optimal-frontier fig-env="figure" fig-pos="htb" fig-cap="**The Compute-Optimal Frontier**: For any training compute budget, there is a best achievable performance when data and model size are optimally balanced (green curve). Operating points below the frontier indicate inefficiency. **Data-starved** systems (orange) have compute capacity but insufficient quality data; the techniques in this chapter move them toward the frontier. **Compute-starved** systems (red) have quality data but insufficient training budget; hardware acceleration or distributed training helps here. The goal is to operate *on* the frontier, extracting maximum performance from available resources." fig-alt="A log-log plot with Training Compute on x-axis and Model Performance on y-axis. A green curve shows the optimal frontier. Orange point below curve labeled Data-starved. Red point below curve labeled Compute-starved. Purple point on curve labeled Optimal."}
```{.tikz}
\begin{tikzpicture}[font=\small\usefont{T1}{phv}{m}{n}]
% Standard color definitions
\definecolor{BlueLine}{HTML}{006395}
\definecolor{BlueL}{HTML}{D1E6F3}
\definecolor{GreenLine}{HTML}{008F45}
\definecolor{GreenL}{HTML}{D4EFDF}
\definecolor{RedLine}{HTML}{CB202D}
\definecolor{RedL}{HTML}{F5D2D5}
\definecolor{OrangeLine}{HTML}{CC5500}
\definecolor{OrangeL}{HTML}{FFE5CC}
\definecolor{PurpleLine}{HTML}{7B2D8E}

\begin{axis}[
    width=10cm, height=7cm,
    xlabel={Training Compute (FLOPs)},
    ylabel={Model Performance (Accuracy)},
    xmin=1, xmax=1000,
    ymin=50, ymax=100,
    xmode=log,
    axis lines=left,
    xtick={1, 10, 100, 1000},
    xticklabels={$10^{18}$, $10^{19}$, $10^{20}$, $10^{21}$},
    ytick={50, 60, 70, 80, 90, 100},
    clip=false,
    legend style={at={(0.02,0.98)}, anchor=north west, font=\footnotesize\usefont{T1}{phv}{m}{n}, draw=none, fill=white, fill opacity=0.8}
]
    % Fill region below frontier (suboptimal) - must come before the line
    \addplot[fill=gray!10, opacity=0.5, draw=none, domain=1:1000, samples=100] {95 - 45*exp(-0.8*ln(x))} \closedcycle;

    % Compute-optimal frontier (log curve approaching asymptote)
    \addplot[very thick, GreenLine, domain=1:1000, samples=100] {95 - 45*exp(-0.8*ln(x))};
    \addlegendentry{Compute-optimal frontier}

    % Data-starved operating point (high compute, poor performance)
    \node[circle, fill=OrangeLine, inner sep=3pt] (data_starved) at (axis cs: 100, 72) {};
    \node[font=\scriptsize\usefont{T1}{phv}{b}{n}, anchor=south, OrangeLine!80!black, align=center] at (axis cs: 100, 74) {Data-starved};

    % Arrow showing data selection improvement (toward frontier)
    \draw[->, thick, OrangeLine, dashed] (axis cs: 100, 72) -- (axis cs: 100, 84);
    \node[OrangeLine, font=\tiny\usefont{T1}{phv}{m}{n}, anchor=west, align=left] at (axis cs: 105, 78) {Better data\\selection};

    % Compute-starved operating point (low compute, poor performance for data quality)
    \node[circle, fill=RedLine, inner sep=3pt] (compute_starved) at (axis cs: 10, 62) {};
    \node[font=\scriptsize\usefont{T1}{phv}{b}{n}, anchor=north, RedLine, align=center] at (axis cs: 10, 60) {Compute-starved};

    % Arrow showing compute improvement
    \draw[->, thick, RedLine, dashed] (axis cs: 10, 62) -- (axis cs: 30, 75);
    \node[RedLine, font=\tiny\usefont{T1}{phv}{m}{n}, anchor=south west, align=left] at (axis cs: 15, 67) {More\\training};

    % Optimal operating point (on the frontier)
    \node[circle, fill=PurpleLine, inner sep=3pt] (optimal) at (axis cs: 300, 88.5) {};
    \node[font=\scriptsize\usefont{T1}{phv}{b}{n}, anchor=south west, PurpleLine] at (axis cs: 320, 89) {Optimal};

    % Suboptimal region label
    \node[gray!60!black, font=\footnotesize\usefont{T1}{phv}{m}{n}] at (axis cs: 30, 55) {Suboptimal region};

\end{axis}
\end{tikzpicture}
```
:::

**Diagnosing your position**: The frontier provides a practical diagnostic framework:

- **Data-starved** (orange): You have training compute available, but performance falls short of what the frontier predicts. The bottleneck is data quality or quantity. *Solution*: Apply the techniques from this chapter (deduplication, coreset selection, curriculum learning, or synthetic augmentation) to extract more learning per sample.
- **Compute-starved** (red): You have high-quality data, but insufficient compute to fully exploit it. Adding more data will not help. *Solution*: Invest in hardware acceleration (@sec-ai-acceleration), longer training runs, or distributed training.
- **On the frontier** (purple): Data and compute are balanced. You are extracting maximum value from both resources. Further improvement requires increasing *both* data quality and compute proportionally.

**The Chinchilla rule of thumb**: For compute-optimal training, the number of training tokens should scale roughly as $D_{opt} \propto C^{0.5}$. Doubling your compute budget means you should increase data by about 40%, not 100%. This explains why the Data Wall is so constraining: as compute grows exponentially, the demand for quality data grows with its square root, but even that slower growth outpaces the supply of high-quality human-generated content.

**Applying the diagnostic**: If your training run underperforms expectations, ask: *Am I data-starved or compute-starved?* A simple test: train for 2$\times$ longer. If performance improves substantially, you were compute-starved. If it plateaus quickly, you are data-starved and need better data, not more training. The techniques in this chapter address the data-starved regime; hardware acceleration and distributed training address the compute-starved regime.

Watch how the two curves diverge in @fig-ppd-curve: a data-efficient selection strategy (blue) reaches the performance plateau much faster than random sampling (gray). The gap between the curves at any dataset size represents the efficiency opportunity: compute that could be saved by smarter data curation.

::: {#fig-ppd-curve fig-cap="**Diminishing Returns of Data**: Random sampling (gray) versus data-efficient selection (blue). The efficient strategy achieves higher performance with less data, reaching the convergence plateau much earlier. The red arrow shows the efficiency gap at a fixed dataset size." fig-alt="A plot with X-axis 'Dataset Size' and Y-axis 'Performance'. Two curves start at 0. The 'Random' curve rises slowly. The 'Efficient' curve rises steeply and plateaus early."}
```{python}
#| label: fig-ppd-curve
#| echo: false

import numpy as np
from physx import viz

fig, ax, COLORS, plt = viz.setup_plot(figsize=(8, 6))

# =============================================================================
# PLOT: Diminishing Returns of Data
# =============================================================================
x = np.linspace(0, 100, 200)
y_random = 95 * (1 - np.exp(-0.04 * x))
y_efficient = 95 * (1 - np.exp(-0.15 * x))

ax.plot(x, y_random, '--', color=COLORS['grid'], label='Random Sampling', linewidth=2)
ax.plot(x, y_efficient, '-', color=COLORS['BlueLine'], label='Efficient Selection', linewidth=2.5)
ax.fill_between(x, y_random, y_efficient, color=COLORS['BlueL'], alpha=0.1)

ax.set_xlabel('Dataset Size (% of Total)')
ax.set_ylabel('Model Accuracy (%)')
ax.set_xlim(0, 100)
ax.set_ylim(0, 100)

idx = 40
x_val, y_eff, y_rnd = x[idx], y_efficient[idx], y_random[idx]
ax.annotate("", xy=(x_val, y_eff), xytext=(x_val, y_rnd),
            arrowprops=dict(arrowstyle="<->", color=COLORS['RedLine'], lw=1.5))
ax.text(x_val+2, (y_eff+y_rnd)/2, "Efficiency Gap\n(Saved Compute)", color=COLORS['RedLine'], fontsize=9, va='center', fontweight='bold')

ax.legend(loc='lower right', fontsize=9)
plt.show()
```
:::

**The practical question** for practitioners: at what point should you stop collecting data and start curating it? When does adding more samples waste compute rather than improve accuracy? These questions require rigorous metrics: ways to quantify diminishing returns, compare selection strategies, and evaluate the cost-effectiveness of different data sources. Data selection techniques (coreset selection, active learning, deduplication) all make implicit claims about the value of different samples. Validating that a curated dataset actually preserves model quality requires systematic benchmarking: coverage metrics validate that coreset selection preserved representation across classes and demographic groups; distribution alignment metrics (such as KL divergence and PSI; see @sec-system-foundations-measuring-drift-divergence-0525) detect whether the curated training set drifted from the deployment distribution; and label quality metrics (inter-annotator agreement, confident learning) validate that active learning did not introduce systematic labeling errors. A 50% dataset reduction is only valuable if benchmarking confirms the model maintains target accuracy, calibration, and robustness.

For a comprehensive treatment of data selection metrics and benchmarking methodologies, including how initiatives like DataPerf are standardizing evaluation protocols, see @sec-benchmarking-ai. That chapter provides the measurement framework needed to quantify the ROI of the techniques introduced here. Before concluding, let us revisit how data selection applies to our Lighthouse Models.

::: {.callout-lighthouse title="Lighthouse Data Selection"}
This chapter has applied data selection principles to all five Lighthouse Models, demonstrating that the techniques are universal but the priorities differ by bottleneck:

| **Lighthouse**       | **Primary Bottleneck** | **Data Selection Priority**                                                      |
|:-------------------|:---------------------|:-------------------------------------------------------------------------------|
| **ResNet-50**        | Compute                | Coreset selection directly reduces training FLOPs                                |
| **GPT-2/Llama**      | Memory bandwidth       | Deduplication reduces corpus size; curriculum learning improves token efficiency |
| **MobileNet**        | Latency/Power          | Aggressive augmentation compensates for reduced model capacity                   |
| **DLRM**             | Memory capacity        | Interaction deduplication and embedding pruning reduce table size                |
| **Keyword Spotting** | Extreme constraints    | Augmentation and synthesis create datasets from minimal seeds                    |

The common thread: **data selection is not a single technique but a systems optimization** tailored to whichever resource is most constrained.
:::

## Fallacies and Pitfalls {#sec-data-selection-fallacies-pitfalls-6285}

With metrics in hand, practitioners often rush to implement techniques without recognizing the conceptual traps and implementation errors that undermine their efforts. The following fallacies represent persistent misconceptions about data selection, while the pitfalls capture practical mistakes that sabotage otherwise sound strategies. Understanding both is essential for translating theory into production gains.

##### Fallacy: *Data is the new oil, so more is always better.* {.unnumbered}

The "Data is Oil" metaphor fails to capture diminishing returns. A saturation point exists where adding terabytes of data yields negligible accuracy gains while exploding compute costs. Data is better understood as fuel with specific energy density: high-quality, curated data (high octane) powers models more efficiently than vast quantities of raw data (crude oil).

##### Fallacy: *Synthetic data can completely replace real data.* {.unnumbered}

While synthetic data addresses scarcity, it is bounded by the generator's knowledge. A model trained purely on synthetic data from another model risks "Model Collapse," a degenerative feedback loop where errors are amplified. Synthetic data augments, but rarely replaces, the grounding provided by real-world distributions. It is best used to fill gaps in the data manifold, not to define it.

##### Fallacy: *Data selection is just data cleaning.* {.unnumbered}

Cleaning (removing errors) is necessary but insufficient. True data selection involves *selection* (finding the decision boundary) and *synthesis* (creating hard negatives). You can have a perfectly clean dataset that is highly inefficient because it is filled with redundant, easy examples. Efficiency requires optimizing the information content, not just the hygiene.

##### Fallacy: *Data selection is only for resource-constrained settings.* {.unnumbered}

Many practitioners view data selection as a corner-case optimization for TinyML or budget-limited startups, irrelevant when training foundation models with massive budgets. In reality, data selection is *most* valuable at scale. A 10% efficiency gain on a \$100M training run saves \$10M. The Data Wall affects frontier labs more acutely than anyone; they have the compute but lack the data. The techniques in this chapter are increasingly adopted by exactly those organizations with "unlimited" resources.

These conceptual misunderstandings often lead to flawed strategies. Equally damaging are the implementation pitfalls that arise when correct strategies meet messy engineering realities.

##### Pitfall: *Optimizing selection without measuring selection overhead.* {.unnumbered}

A sophisticated coreset algorithm that takes 10 hours to select samples for a 2-hour training run has negative ROI. Always measure the Selection Inequality: $T_{selection} + T_{train}(subset) < T_{train}(full)$. Use lightweight proxy models or cached embeddings for selection, and profile selection time alongside training time.

##### Pitfall: *Pruning rare classes into oblivion.* {.unnumbered}

Aggressive coreset selection often removes rare classes entirely because they contribute little to average loss. The model then fails catastrophically on these classes in production. Stratify selection by class and set minimum samples per class before applying any pruning algorithm.

##### Pitfall: *Training on deduplicated data while evaluating on duplicated test sets.* {.unnumbered}

If your test set contains duplicates of training samples (common in web-scraped data), deduplication appears to hurt performance when it actually improves generalization. Deduplicate train and test sets jointly, or use truly held-out evaluation data.

##### Pitfall: *Active learning without considering annotation latency.* {.unnumbered}

Active learning theory assumes instant oracle responses. In practice, getting expert labels takes days or weeks. By the time labels arrive, the model has moved on, and the selected samples may no longer be optimal. Select larger batches to amortize latency and use diversity sampling to hedge against model drift.

A subtler class of errors emerges when practitioners assume that benchmark results transfer directly to their specific domains and deployment contexts.

##### Fallacy: *If a technique works on ImageNet, it will work on my dataset.* {.unnumbered}

Data selection results are highly dataset-dependent. CIFAR-10 is highly redundant, so 50% coresets work well. ImageNet has moderate redundancy. Domain-specific datasets (medical imaging, satellite imagery, scientific data) may have near-zero redundancy, where every sample captures unique information. A coreset that preserves 95% accuracy on ImageNet may fail catastrophically on a well-curated radiology dataset. Always pilot data selection techniques on your specific distribution. The "free lunch" ratios reported in benchmark papers (50% pruning, 10$\times$ label reduction) rarely transfer directly. Start with conservative pruning (20–30%) and validate on held-out data before aggressive reduction.

##### Pitfall: *Optimizing data selection metrics instead of deployment metrics.* {.unnumbered}

A team achieves excellent PPD (Performance-Per-Data) and DCR (Data Compression Ratio) during development, having created a beautifully efficient 10% coreset. At deployment, the model fails on edge cases: rare classes, unusual lighting conditions, demographic subgroups underrepresented in the coreset. The efficiency metrics looked great; the production metrics are catastrophic. Include deployment-relevant evaluation in data selection optimization. If the task requires 99.9% reliability on edge cases, ensure the coreset *oversamples* those cases, even if it reduces average PPD. Stratify evaluation by subgroup, not just overall accuracy. The goal is deployment success, not benchmark efficiency.

## Summary {#sec-data-selection-summary-8b2a}

This chapter opened with a question: why do smaller, curated datasets sometimes outperform massive ones? The answer lies in recognizing data selection as a *systems* problem rather than a purely statistical one. Where traditional machine learning asks "how few samples achieve target accuracy?", the systems perspective asks "how do we minimize total cost across the entire pipeline?"

This reframing transforms how practitioners approach the ML development lifecycle. Rather than treating data as a static input to be collected and labeled, data selection treats it as a dynamic resource to be engineered. The goal is minimizing total cost across compute, storage, labeling, energy, and time, not merely maximizing accuracy.

We explored the three-stage optimization pipeline: **Static Pruning** removes redundancy before training through coreset selection and deduplication, **Dynamic Selection** prioritizes informative examples during training through curriculum and active learning, and **Synthetic Generation** creates data where none exists through augmentation, simulation, and distillation. Together, these strategies address the "Data Wall," the fundamental asymmetry between exponentially growing compute and slowly growing high-quality data.

The self-supervised learning paradigm represents a ceiling of data selection: by eliminating task-specific labels entirely, foundation models achieve 1,000$\times$ multipliers on downstream tasks through cost amortization. This paradigm shift from "train from scratch" to "pre-train once, fine-tune many" has become the dominant approach in production ML precisely because of its superior data economics.

::: {.callout-takeaways title="Key Takeaways"}

* **Data selection is a systems problem**: The goal is reduced cost across the entire pipeline (compute, storage, labeling, energy), not just "fewer samples for same accuracy."
* **Start with deduplication**: Deduplication typically offers the highest return on investment: low cost, immediate gains, and no accuracy penalty. Deduplication should precede sophisticated selection methods.
* **The Selection Inequality must hold**: $T_{selection} + T_{train}(subset) < T_{train}(full)$. Selection overhead should be kept below 10% of training time using proxy models or cached embeddings.
* **Amortization determines ROI**: Data selection techniques are most effective when training repeats (hyperparameter search) or datasets are reused across multiple teams. For one-off training, simpler methods often outperform sophisticated approaches.
* **Fine-tuning typically outperforms training from scratch**: Self-supervised pre-training amortizes cost across applications. Fine-tuning typically requires 100× fewer labels and 10× less per-task compute (when amortized) than from-scratch training.
* **Avoid exclusive use of synthetic data**: Mixing 50–80% synthetic with 20–50% real data typically yields better results. Domain gap without mitigation can significantly degrade real-world performance.

:::

The techniques explored throughout this chapter (deduplication, coreset selection, curriculum learning, active learning, and synthetic generation) provide practitioners with a systematic toolkit for breaking through the Data Wall. Organizations that master these techniques gain compound advantages: reduced labeling budgets, faster iteration cycles, lower storage costs, and models that generalize better because they learn from higher-quality examples rather than redundant noise.

::: {.callout-chapter-connection title="From Data to Algorithms"}

With high-quality data in hand, we have optimized the source of the system. Even the best data, however, cannot make an inefficient model run fast on constrained hardware. In @sec-model-compression, we move from optimizing *what* the system learns to optimizing *how* it represents that knowledge, applying pruning, quantization, and knowledge distillation to reduce the computational cost of the model artifact itself.

:::

<!-- This is here to make sure that quizzes are inserted properly before a part begins. -->
::: { .quiz-end }
:::
