# Principles of Machine Learning Systems {.unnumbered}

These principles are the "Physics" of data engineering. They are not best practices that change with software versions; they are invariant constraints imposed by hardware, mathematics, and information theory. Every engineering decision in this Part is a response to these laws.

::: {.callout-note icon=false title="The Law of Conservation of Complexity (The Zeroth Law)"}
**The Law**: You cannot destroy complexity; you can only displace it. In ML systems, complexity flows between the **Data** (Preprocessing/Curating), the **Model** (Architecture/Parameters), and the **System** (Infrastructure/Scale).
$$ \Delta C_{total} = \Delta C_{data} + \Delta C_{model} + \Delta C_{system} \approx 0 $$

**The Engineering Implication**:
The job of the AI Engineer is not to remove complexity, but to choose where it lives. "Software 2.0" moves complexity from code (System) to optimization (Model) and curation (Data). If you simplify the model, you *must* increase the complexity of your data pipeline or your infrastructure to maintain performance.
:::

::: {.callout-note icon=false title="The Data as Code Invariant"}
**The Law**: Data *is* the source code of the ML system. A change in the training dataset ($\Delta D$) is functionally equivalent to a change in the executable logic ($\Delta P$).
$$ \text{System Behavior} \approx f(\text{Data}) $$

**The Engineering Implication**:
Data engineering requires the same rigor as software engineering. Datasets must be **versioned** (like git), **unit-tested** (data quality checks), and **debugged**. Deleting a row of training data is the engineering equivalent of deleting a line of code; retraining a model is simply recompiling the binary.
:::

::: {.callout-note icon=false title="The Data Gravity Invariant"}
**The Law**: Data possesses mass. As dataset scale ($D$) increases, the cost (latency, bandwidth, energy) of moving data exceeds the cost of moving compute.
$$ C_{move}(D) \gg C_{move}(Compute) $$

**The Engineering Implication**:
Data is not just cargo; it is the gravitational center of the architecture. Systems must shift from "Data-to-Compute" (downloading datasets) to "Compute-to-Data" (shipping queries or code to the storage layer). This invariant drives the design of **data lakes**, **warehouse-scale computers**, and **federated learning**.
:::

::: {.callout-note icon=false title="The Pipeline Stall Law"}
**The Law**: The throughput of a training system is strictly bounded by the slowest component in the data pipeline, not the fastest accelerator.
$$ T_{system} = \min(T_{IO}, T_{CPU}, T_{GPU}) $$

**The Engineering Implication**:
GPU starvation is the default state of deep learning. To saturate accelerators, preprocessing (CPU) and loading (I/O) must be perfectly prefetched to hide latency. If $T_{IO} > T_{GPU}$, no amount of GPU optimization will improve performance.
:::
