---
engine: jupyter
---

# The D·A·M Taxonomy {#sec-appendix-dam}

## Purpose {.unnumbered}

_When an ML system fails, where should you look first: the data path, the algorithm, or the machine?_

In production, “it’s slow” and “it’s wrong” are rarely informative symptoms. A serving stack can miss its latency Service Level Objective (SLO) because the GPU is idle (data starvation), because the model is doing unnecessary work (algorithmic overhead), or because the accelerator is genuinely saturated (machine-bound). Without a taxonomy, teams often optimize the wrong thing—buying faster GPUs to fix a slow input pipeline, or rewriting kernels when the model is simply too large for the latency budget.

This appendix provides a compact diagnostic framework—**Data · Algorithm · Machine (D·A·M)**—and shows how to map symptoms and measurements to the term of the Iron Law that dominates. Use it as your “first response” checklist before you commit to deeper optimization.

## How to Use This Appendix {.unnumbered}

This appendix is designed as a reference. Start with the scorecard-style metrics, form a hypothesis about which axis dominates, and then pick the tool that can confirm (or falsify) that hypothesis. Conventions used here follow the book-wide notation in @sec-notation-conventions (for example, we reserve $B$ for batch size and use $\text{BW}$ for bandwidth).

When training is slow, check GPU utilization, data wait time, and MFU, then map each to its Data, Algorithm, or Machine axis. When serving misses a Service Level Agreement (SLA), identify whether you are latency-bound (overhead), memory-bound (weight/KV movement), or compute-bound. When cost is exploding, use the D·A·M rubric to ensure you are improving the dominant term, not polishing a non-bottleneck.

```{python}
#| echo: false
#| label: appendix-dam-setup
from physx.constants import *
from physx.formatting import fmt, md, md_frac, md_math

# =============================================================================
# EXERCISE 1: Component Identification
# =============================================================================
ex1_gpu_util_pct = 25
ex1_disk_sat_pct = 100

ex1_gpu_util_str = f"{ex1_gpu_util_pct}"
ex1_disk_sat_str = f"{ex1_disk_sat_pct}"

# =============================================================================
# EXERCISE 2: Iron Law Analysis
# =============================================================================
# Inputs
ex2_params = 7e9
ex2_latency_s = 0.050

# Constants / Derived
ex2_bytes_per_param = 2 # FP16
ex2_flops_per_param = 2 # Fwd pass
h100_fp16_tflops_val = int(H100_FLOPS_FP16_TENSOR.to(TFLOPs / second).magnitude)

# Calculations
ex2_flops_per_pass_val = (ex2_params * ex2_flops_per_param * flop).to(TFLOPs).magnitude
ex2_achieved_tflops_val = ex2_flops_per_pass_val / ex2_latency_s
ex2_util_val = (ex2_achieved_tflops_val / h100_fp16_tflops_val) * 100
ex2_model_size_gb_val = (ex2_params * ex2_bytes_per_param * byte).to(GB).magnitude

# Formatting
ex2_params_str = "7B"
ex2_flops_per_pass_str = f"{ex2_flops_per_pass_val:.3f}"
ex2_latency_ms_str = f"{int(ex2_latency_s * 1000)}"
ex2_achieved_str = fmt(ex2_achieved_tflops_val, precision=2, commas=False)
ex2_util_str = fmt(ex2_util_val, precision=2, commas=False)
h100_fp16_tflops_str = f"{h100_fp16_tflops_val}"
ex2_model_size_gb_str = f"{ex2_model_size_gb_val:.0f}"

# LaTeX
ex2_achieved_eq = md(
    f"$$\\text{{Achieved FLOP/s}} = \\frac{{{ex2_flops_per_pass_str} \\text{{ TFLOPs}}}}"
    f"{{0.050 \\text{{ s}}}} = {ex2_achieved_str} \\text{{ TFLOP/s}}$$"
)
ex2_util_eq = md(
    f"$$\\eta = \\frac{{{ex2_achieved_str}}}{{{h100_fp16_tflops_val}}} \\approx {ex2_util_str}\\%$$"
)

# =============================================================================
# EXERCISE 3: Scaling Law vs. Information Roofline
# =============================================================================
ex3_params_start_str = "125M"
ex3_params_end_str = "1B"
ex3_scale_factor = 8
ex3_loss_start = 0.45
ex3_loss_end = 0.42
ex3_imp_pct = (ex3_loss_start - ex3_loss_end) / ex3_loss_start * 100
ex3_chin_pred_pct = 15

ex3_imp_str = f"{ex3_imp_pct:.1f}"
ex3_chin_pred_str = f"{ex3_chin_pred_pct}"

# =============================================================================
# EXERCISE 4: Anti-Pattern Detection
# =============================================================================
ex4_gpu_old_n = 4
ex4_gpu_old_type = "A100"
ex4_gpu_new_n = 8
ex4_gpu_new_type = "H100"
ex4_cost_k = 200

ex4_gpu_old_str = f"{ex4_gpu_old_n}× {ex4_gpu_old_type}"
ex4_gpu_new_str = f"{ex4_gpu_new_n}× {ex4_gpu_new_type}"
ex4_cost_str = f"${ex4_cost_k}K"
```

::: {.callout-tip title="Learning Objectives"}
By the end of this appendix, you will be able to:

- **Classify** any ML system bottleneck into one of three MECE categories: Data, Algorithm, or Machine.
- **Map** optimization techniques to their D·A·M intersection zone to understand which axes they span.
- **Apply** the **Iron Law** equation to quantitatively diagnose performance problems.
- **Distinguish** between memory-bound and compute-bound workloads using **Arithmetic Intensity**.
- **Select** appropriate profiling tools and optimization strategies for each D·A·M axis.
- **Evaluate** system health using the D·A·M Scorecard metrics (I/O Overhead, Active Params, MFU).
:::

The **Data · Algorithm · Machine (D·A·M) taxonomy** is the primary diagnostic framework for ML systems engineering. It formalizes the interdependence between information flow, mathematical logic, and physical execution. When performance stalls or behavior degrades, ask: *where is the flow blocked?* This taxonomy enables practitioners to isolate the bottleneck to one of three mutually exclusive and collectively exhaustive (MECE[^fn-mece]) axes.

[^fn-mece]: **MECE (Mutually Exclusive, Collectively Exhaustive)**: A classification principle from management consulting (popularized by McKinsey) requiring that categories do not overlap and together cover every possibility. Applied to systems engineering, MECE ensures that every bottleneck maps to exactly one D·A·M axis, preventing both diagnostic gaps and double-counting.

## Diagnostic Summary

The taxonomy maps directly to the **Iron Law of ML Systems**, as established in @sec-introduction. @tbl-dam-components-ref summarizes the role, primary physical constraint, and core optimization pathway for each axis.

| **Axis**          | **Role**                   | **Physical Constraint** | **High-Leverage Optimization**               |
|:----------------|:-------------------------|:----------------------|:-------------------------------------------|
| **Data (D)**      | **Information** (The Fuel) | Bandwidth ($BW$)        | Data Selection (@sec-data-selection)         |
| **Algorithm (A)** | **Logic** (The Blueprint)  | Operations ($O$)        | Model Compression (@sec-model-compression)   |
| **Machine (M)**   | **Physics** (The Engine)   | Throughput ($R_{peak}$) | Hardware Acceleration (@sec-ai-acceleration) |

: **D·A·M Axis Reference.** Each axis maps to a distinct physical constraint and a high-leverage optimization strategy. Start diagnosis here: identify which constraint is binding, then follow the optimization pointer to the relevant chapter. {#tbl-dam-components-ref}

This clean separation is useful as a first diagnostic step, but production systems rarely suffer from a single pure-axis bottleneck. More often, the problem sits at the *boundary* between two axes—a data format choice that determines whether the GPU can be saturated, or a pruning strategy that changes the memory access pattern. To handle these cases, we need to map the intersections.

## Intersection Landscape {#sec-dam-venn}

Real systems engineering lives at the *boundaries* between axes. @fig-dam-venn maps the intersection landscape: what concepts and techniques emerge when two or three axes overlap.

![**The D·A·M Intersection Landscape.** Each circle represents a pure domain: Data (information), Algorithm (logic), and Machine (physics). The pairwise intersections capture techniques that require reasoning about two domains simultaneously. The center---where all three converge---is ML Systems Engineering itself: the discipline of balancing data flow, algorithmic complexity, and hardware constraints within a single system.](images/dam_venn.svg){#fig-dam-venn fig-alt="Venn diagram with three overlapping circles labeled Data, Algorithm, and Machine. Pairwise intersections are labeled: D intersection A shows data selection and curriculum learning; D intersection M shows I/O bandwidth and data formats; A intersection M shows quantization and kernel fusion. The center intersection is labeled ML Systems Engineering with Iron Law and Roofline."}

@tbl-dam-intersections provides a scannable reference for each zone.

| **Zone**                | **Name**                   | **Key Techniques**                                           | **Book Coverage**                                                  |
|:----------------------|:-------------------------|:-----------------------------------------------------------|:-----------------------------------------------------------------|
| **D** (pure)            | Information                | Storage formats, data quality, distributions                 | @sec-data-engineering-ml                                           |
| **A** (pure)            | Logic                      | Loss functions, architectures, gradients                     | @sec-deep-learning-systems-foundations, @sec-dnn-architectures     |
| **D $\cap$ A**          | What to Learn From         | Data selection, curriculum learning, compute-optimal scaling | @sec-data-selection, @sec-ai-training                              |
| **D $\cap$ M**          | How to Move Information    | I/O bandwidth, prefetching, data formats                     | @sec-data-engineering-ml, @sec-ai-acceleration                     |
| **A $\cap$ M**          | How to Execute Efficiently | Quantization, pruning, kernel fusion, mixed precision        | @sec-ai-frameworks, @sec-model-compression                         |
| **M** (pure)            | Physics                    | Silicon, memory hierarchy, peak FLOPS                        | @sec-ai-acceleration                                               |
| **D $\cap$ A $\cap$ M** | ML Systems Engineering     | Iron Law, Roofline, training loops, serving                  | @sec-ai-training, @sec-model-serving-systems, @sec-benchmarking-ai |

: **D·A·M Intersection Reference.** Each zone maps specific techniques to the axes they span and the chapters that cover them. The pairwise intersections require reasoning about two domains simultaneously; the center requires all three. {#tbl-dam-intersections}

The pure zones contain concepts that belong entirely to one axis: storage formats and distribution properties are purely Data concerns, loss functions and gradient computations are purely Algorithm, and silicon physics and peak FLOPS are purely Machine. These are the topics where single-domain expertise suffices.

The pairwise intersections are where systems thinking begins. **D $\cap$ A** (*What to Learn From*) encompasses data selection, curriculum learning, active learning, and scaling laws like Chinchilla ($D \approx 20N$)---all requiring joint reasoning about information content and algorithmic capacity. Adding data without considering whether the model can learn from it wastes compute; choosing architectures without considering data availability wastes engineering time. **D $\cap$ M** (*How to Move Information*) covers I/O bandwidth, prefetching strategies, data formats, and the Energy-Movement Invariant. This intersection is where Data Gravity manifests: the physical cost of moving bytes through the memory hierarchy determines whether the machine can be fed fast enough. **A $\cap$ M** (*How to Execute Efficiently*) spans quantization, pruning, kernel fusion, mixed precision, and computational graph optimization. A pruning strategy that reduces FLOPs but destroys memory access patterns can *slow down* execution on real hardware.

The center---$D \cap A \cap M$---is where all three axes converge. The Iron Law, the Roofline Model, end-to-end training loops, serving pipelines, and holistic benchmarking all require simultaneous reasoning about data flow, algorithmic complexity, and hardware utilization. This center is not a single technique; it is the discipline itself.

Understanding the landscape tells you *where* a technique lives. The next step is quantifying *which axis dominates* for a given workload---and for that, we need the Iron Law.

## Iron Law Mapping

The performance of any ML task is governed by the distribution of work across the D·A·M axes. The Iron Law Mapping reveals which component's variables dominate the execution time:

$$ T = \underbrace{ \frac{D_{vol}}{BW} }_{\text{Data (D)}} + \underbrace{ \frac{O}{R_{peak} \cdot \eta} }_{\text{Algorithm (A) / Machine (M)}} + \underbrace{ L_{lat} }_{\text{Overhead}} $$

Note that Algorithm and Machine share the compute term; they are separated by which variable you control. Reducing the total operations ($O$) is an **Algorithm** lever, while improving the hardware's peak throughput ($R_{peak}$) or utilization ($\eta$) is a **Machine** lever.

This equation transforms performance debugging from a qualitative guessing game into a quantitative engineering problem. Every bottleneck hides in one of these terms. If your system is slow, it is because you are moving too much data ($D_{vol}$), lacking bandwidth ($BW$), executing too many operations ($O$), or failing to utilize your hardware's peak capability ($\eta$). The levers below map specific optimizations to the variable they improve.

### Component Levers

*   **Data Lever**: Reducing the volume of data ($D_{vol}$) through deduplication or curriculum learning, or increasing I/O bandwidth ($BW$).
*   **Algorithm Lever**: Reducing total arithmetic operations ($O$) through pruning, quantization, or architectural refinement.
*   **Machine Lever**: Increasing the denominator of the compute term by improving peak throughput ($R_{peak}$) or increasing the utilization factor ($\eta$) via kernel fusion.

### D·A·M Coordination: From Sum to Max

The additive Iron Law represents **sequential execution**—the worst case where Data, Algorithm, and Machine take turns. But skilled systems engineering transforms the sum into a max:

$$ T_{sequential} = \frac{D_{vol}}{BW} + \frac{O}{R_{peak} \cdot \eta} + L_{lat} \quad \xrightarrow{\text{overlap}} \quad T_{pipelined} = \max\left(\frac{D_{vol}}{BW}, \frac{O}{R_{peak} \cdot \eta}\right) + L_{lat} $$

The systems engineer's job is to make these components run in parallel, not in series. @tbl-dam-overlap summarizes key D·A·M Coordination techniques:

| **Technique**           | **D·A·M Axes Overlapped**    | **Implementation**                                   |
|:----------------------|:---------------------------|:---------------------------------------------------|
| **Prefetching**         | D overlaps M                 | DataLoader with `prefetch_factor`, `pin_memory=True` |
| **CUDA Streams**        | D overlaps M                 | Separate streams for H2D transfer and compute        |
| **Async Gradient Sync** | M (communication) overlaps A | Overlap AllReduce with next forward pass             |
| **Double Buffering**    | D overlaps M                 | Fill buffer N+1 while computing on buffer N          |

: **D·A·M Overlap Techniques.** Each technique allows one D·A·M axis to execute while another is in flight, converting the Iron Law's additive terms into overlapped terms. The payoff is transforming $T = a + b$ into $T = \max(a, b)$, which can cut latency nearly in half when the terms are balanced. {#tbl-dam-overlap}

Overlap only helps when the D·A·M axes are reasonably balanced. If one term dominates (e.g., severely memory-bound), overlapping the smaller term with the larger yields negligible gain—the max is still dominated by the same bottleneck. Overlap provides the greatest benefit when $D_{vol}/BW \approx O/(R_{peak} \cdot \eta)$.

::: {.callout-warning title="The Overhead That Cannot Hide"}
The latency term $L_{lat}$ (kernel launch, synchronization barriers, Python dispatch) typically cannot be overlapped—it represents serialization points where all components must wait. This is why **kernel fusion** is so powerful: it eliminates $L_{lat}$ by combining operations, not just by speeding up any single component.
:::

The Iron Law tells you *how much* time each axis consumes. But it leaves one critical question unanswered: when the bottleneck sits at the boundary between Data and Machine, how do you tell which side you're on? The answer lies in a single ratio.

## Arithmetic Intensity Boundary

The boundary between **Data** (Memory-Bound) and **Machine** (Compute-Bound) is not arbitrary; it is defined mathematically by the **Arithmetic Intensity**[^fn-arith-intensity] ($I$) of the workload.

[^fn-arith-intensity]: **Arithmetic Intensity**: The ratio of floating-point operations to bytes transferred (FLOPs/byte), introduced by Williams, Waterman, and Patterson (2009) as the key parameter in the Roofline Model. It determines whether a workload is memory-bound or compute-bound by comparison against the hardware's *ridge point* ($R_{peak}/BW$). @sec-system-foundations-roofline-model-5f7c provides a complete derivation.

@sec-system-foundations-roofline-model-5f7c provides rigorous definitions of Arithmetic Intensity and the Roofline Model. Use that model to quantitatively distinguish between Data and Machine bottlenecks before applying the optimizations below.

The Roofline Model provides exact answers when you have time to profile. But in the middle of a production incident, you often need a faster heuristic—a set of quick thresholds that point you toward the right axis within seconds.

## Rules of Thumb

In the heat of a production outage, you rarely have time to solve the full Iron Law equation. Instead, veteran systems engineers rely on these quantitative heuristics to quickly narrow down the search space. Use these thresholds as your first line of defense.

*   **If GPU Utilization $<$ 80%**: You are likely **Data Bound** (or CPU bound). The accelerator is starving.
*   **If GPU Utilization $>$ 95%**: You are likely **Machine Bound**. The accelerator is fully saturated.
*   **If Batch Size is 1**: You are likely **Latency Bound** (Algorithm overhead dominates).
*   **If Arithmetic Intensity $<$ 100 FLOPs/byte**: You are likely **Memory Bound** (Data/Machine boundary). This threshold is approximate for current-generation accelerators; compute your hardware's specific ridge point ($R_{peak}/BW$) for a precise boundary.
*   **If System works in Dev but fails in Prod**: Suspect **Data Drift** (Data component).

Note that common industry labels map to DAM components as follows: **Memory Bound** typically indicates a **Data** bottleneck (information cannot reach the accelerator fast enough), **Compute Bound** indicates a **Machine** bottleneck (the accelerator is fully saturated), and **Latency Bound** indicates an **Algorithm** bottleneck (serial operation depth or overhead dominates).

### Bottleneck Diagnostic

Once you identify the bottleneck, @tbl-bottleneck-actions tells you what to do—and what NOT to do:

| **If You're...**  | **Dominant Term**         | **Optimization That Works**                               | **Optimization That's Wasted**                     |
|:----------------|:------------------------|:--------------------------------------------------------|:-------------------------------------------------|
| **Memory-Bound**  | $D_{vol}/BW$              | Quantization, pruning, batching, kernel fusion            | Faster GPU (more FLOP/s won't help)                |
| **Compute-Bound** | $O/(R_{peak} \cdot \eta)$ | Better kernels, Tensor Cores, faster GPU, lower precision | More memory bandwidth (already saturated)          |
| **Latency-Bound** | $L_{lat}$                 | Batching requests, kernel fusion, async dispatch          | Neither compute nor bandwidth (overhead dominates) |

: **What Works vs. What's Wasted.** Optimizing the wrong term yields exactly zero improvement. A memory-bound LLM will not speed up from a faster GPU; the GPU will simply idle faster while waiting for memory. {#tbl-bottleneck-actions}

Knowing what works also means recognizing what doesn't. In practice, teams under deadline pressure repeatedly fall into the same traps—optimizing the wrong axis with confidence. These failure modes are common enough to deserve their own names.

## Anti-Patterns

Diagnosing systems is often a process of elimination. Before committing to complex kernel optimizations, ensure you are not falling into one of these common traps that waste engineering cycles.

*   **The Hardware Crutch**: Buying faster accelerators (**Machine**) to fix a slow Python data loader (**Data**). The new hardware will just idle faster.
*   **The Model Twiddle**: Changing neural architectures (**Algorithm**) when the bottleneck is actually network bandwidth or disk I/O.
*   **The Premature Optimizer**: Writing custom CUDA kernels (**Machine**) before verifying if the Algorithm is simply doing too many unnecessary operations.

Each anti-pattern follows the same root cause: acting before diagnosing. The case studies below show what proper diagnosis looks like—starting from a confusing symptom and systematically narrowing to the dominant D·A·M axis.

## D·A·M Case Studies

Theoretical constraints often manifest as confusing symptoms in production. These real-world scenarios illustrate how to apply the taxonomy.

### Case 1: The Starving GPU (Data)

#### Symptom {.unnumbered}

You provision a massive A100 GPU instance to speed up training, but your training time hardly improves. `nvidia-smi` shows GPU utilization fluctuating between 10% and 40%.

#### Diagnosis {.unnumbered}

The **Data** component cannot supply the **Machine** fast enough. You are I/O bound.

#### The Fix {.unnumbered}

This is not a model or hardware problem. You must optimize the Extract-Transform-Load (ETL) pipeline: move from raw JPEGs (CPU decoding heavy) to TFRecords or WebDataset (sequential reads), increase the number of data loader workers, and prefetch batches to GPU memory.

### Case 2: The Latency Cliff (Algorithm)

#### Symptom {.unnumbered}

Your real-time recommendation system fails to meet the 20 ms latency SLA. The GPU utilization is low, and the batch size is 1.

#### Diagnosis {.unnumbered}

The **Algorithm** is too computationally deep for the sequential deadline. You are latency-bound by serial operations.

#### The Fix {.unnumbered}

Throwing more hardware (Machine) won't help because latency is limited by the serial execution of layers. You must change the Algorithm:

*   **Quantization**: Switch to INT8 to reduce memory fetch time.
*   **Pruning**: Remove redundant heads or channels.
*   **Knowledge Distillation**: Train a smaller student model.

### Case 3: The Compute Wall (Machine)

#### Symptom {.unnumbered}

GPU utilization is pinned at 99%. Memory bandwidth is unsaturated. Training is stable but takes 3 weeks.

#### Diagnosis {.unnumbered}

You have successfully fed the beast. The system is **Compute Bound**.

#### The Fix {.unnumbered}

You have hit the physical limits of the single chip.

*   **Scale Up**: Move to a newer generation GPU (e.g., A100 to H100).
*   **Scale Out**: Distribute training across multiple GPUs (Data Parallelism).
*   **Lower Precision**: Switch from FP32 to BF16 (doubling theoretical TFLOPS).

::: {.callout-checkpoint title="D·A·M Diagnosis Check"}

1. A training job shows 95% GPU utilization but loss has plateaued for 2 epochs. Which D·A·M axis should you investigate, and why?
2. Your colleague suggests adding more data loader workers to a job where `nvidia-smi` shows 98% GPU utilization. Using the Iron Law, explain why this will not help.
3. An inference server meets its latency SLO at batch size 1 but fails at batch size 16. Which term in the Iron Law changed, and what does this tell you about the bottleneck regime?

:::

These three cases illustrate clean, single-axis bottlenecks. Production incidents are rarely so tidy—symptoms often overlap, and the dominant axis can shift during debugging. The next section provides a systematic troubleshooting matrix for the messier scenarios you will encounter in practice.

## Production Troubleshooting

Identifying the root cause of performance bottlenecks requires systematic elimination. @tbl-dam-troubleshooting provides a diagnostic matrix for common failure modes observed in production deployments.

| **Symptom**               | **Likely D·A·M Culprit** | **Diagnostic Question**                                      | **Recommended Action**                                 |
|:------------------------|:-----------------------|:-----------------------------------------------------------|:-----------------------------------------------------|
| **Low GPU Utilization**   | **Data**                 | Is the data loader keeping up with the accelerator?          | Implement prefetching and use binary formats.          |
| **High Latency (P99)**    | **Algorithm**            | Is the model depth or width exceeding the latency budget?    | Apply quantization (INT8) or structured pruning.       |
| **High Training Cost**    | **Machine**              | Is the hardware utilization ($\eta$) below 30%?              | Optimize CUDA kernels or use spot instances.           |
| **Silent Accuracy Drift** | **Data**                 | Has the statistical distribution ($P_t$) shifted from $P_0$? | Trigger retraining and update active learning filters. |
| **Out-of-Memory (OOM)**   | **Algorithm/Machine**    | Does the model state fit in available VRAM?                  | Use gradient checkpointing or reduce batch size.       |

: **D·A·M Diagnostic Matrix.** Root cause identification and remediation strategies for common ML systems failures. Each row connects a user-visible symptom to the D·A·M axis most likely responsible, reducing the search space before you reach for a profiler. {#tbl-dam-troubleshooting}

The diagnostic matrix tells you *what* to suspect. The next question is *how* to confirm that suspicion with evidence—which requires the right profiling tools.

## Tooling Map

Once you have a hypothesis (e.g., "I suspect I am Machine Bound"), you need evidence to prove it. Abstract concepts must be measured with concrete utilities. @tbl-dam-tooling connects the theoretical components to the specific Linux and Python profiling tools you should reach for.

| **Axis** | **Key Metric**                | **Primary Tool**        | **Secondary Tool**             |
|:------------|:----------------------------|:----------------------|:-----------------------------|
| **Data**      | Batch Load Time               | `tqdm` (iterations/sec) | `iotop`, `dstat` (Disk I/O)    |
| **Algorithm** | FLOPs, Model Depth            | PyTorch Profiler        | DeepSpeed Flops Profiler       |
| **Machine**   | GPU Utilization, SM Occupancy | `nvidia-smi`            | Nsight Compute, Nsight Systems |

: **D·A·M Tooling Map.** Profiling utilities for diagnosing bottlenecks along each D·A·M axis. Start with the primary tool for quick triage; use secondary tools for deep-dive analysis when the primary tool's output is inconclusive. {#tbl-dam-tooling}

Profiling tools generate raw numbers—utilization percentages, FLOP counts, bandwidth measurements. But raw numbers only become actionable when compared against a standard. The D·A·M Scorecard provides that standard: a set of efficiency thresholds that distinguish healthy systems from those that need intervention.

## D·A·M Scorecard

To surpass qualitative guessing, use the efficiency ratios in @tbl-dam-scorecard to grade your system's performance against its theoretical limit. This "Report Card" standardizes what "good" looks like, anchored by **MFU**[^fn-mfu]—the single most important metric for large-scale training.

[^fn-mfu]: **MFU (Model FLOPs Utilization)**: The ratio of achieved model FLOPs to the hardware's theoretical peak FLOPs, introduced by Chowdhery et al. (2022) in the PaLM paper. Unlike raw GPU utilization (which counts any work the GPU performs), MFU measures only *useful* model computation, excluding overhead like gradient synchronization and memory management. @sec-benchmarking-ai covers MFU in depth.

| **Axis** | **Metric**        | **Definition**                                         | **Failing Grade** | **Passing Grade** |
|:------------|:----------------|:-----------------------------------------------------|----------------:|----------------:|
| **Data**      | **I/O Overhead**  | $\frac{\text{Data Wait Time}}{\text{Total Step Time}}$ |           $>$ 10% |            $<$ 1% |
| **Algorithm** | **Active Params** | $\frac{\text{Non-Zero Params}}{\text{Total Params}}$   |      100% (Dense) |  $<$ 50% (Sparse) |
| **Machine**   | **MFU**           | $\frac{\text{Achieved FLOPs}}{\text{Peak FLOPs}}$      |           $<$ 30% |           $>$ 50% |

: **The D·A·M Efficiency Rubric.** Use these three numbers to characterize any ML system's maturity. A system that passes all three thresholds has exhausted its easy optimizations; further gains require architectural changes or hardware upgrades. {#tbl-dam-scorecard}

The Scorecard and the Roofline Model both answer efficiency questions, but at different scales. The Scorecard grades your current system against known thresholds. Scaling Laws and the Information Roofline address a more strategic question: what happens as you scale *beyond* the current system?

## Scaling Laws vs. Roofline

Systems engineering requires distinguishing between *growth trajectories* and *fundamental limits*.

### Scaling Laws (The Journey)

Scaling Laws[^fn-scaling-laws] are empirical power laws that predict *how fast* model performance improves as we increase resources. The two landmark results are Kaplan Scaling [@kaplan2020scaling], which showed that performance improves predictably with parameters ($N$), data ($D$), and compute ($C$), and Chinchilla Scaling [@hoffmann2022training], which refined this insight by defining the *optimal ratio* of these resources (e.g., $D \approx 20N$ tokens per parameter).

[^fn-scaling-laws]: **Scaling Laws**: Empirical relationships, typically power laws of the form $L(x) \propto x^{-\alpha}$, that predict model loss as a function of dataset size, parameter count, or compute budget. First systematically studied by Kaplan et al. (2020) at OpenAI, then refined by Hoffmann et al. (2022) with the Chinchilla result. @sec-ai-training discusses scaling laws in detail.

These laws are economic guides. They tell you: "If I double my compute budget, my error rate should drop by $X$%." They assume the information is there to be learned.

### Information Roofline (The Destination)

The Information Roofline is the theoretical limit of what *can* be learned from the data, regardless of scale. Three quantities define it: the *ceiling* is the Bayes Error Rate[^fn-bayes-error] (the irreducible error inherent in the data); the *slope* is the information density, or signal-to-noise ratio, of the training distribution; and the *bottleneck* appears when data has low information density—as with noisy financial tickers—causing you to hit the "Data Quality Wall" long before you hit the Compute Wall.

[^fn-bayes-error]: **Bayes Error Rate**: The lowest achievable error rate for any classifier on a given data distribution, determined by the overlap between class-conditional distributions. Named after Thomas Bayes (1701–1761). No amount of data, parameters, or compute can reduce error below this theoretical floor.

The diagnostic lesson is this: Scaling Laws predict the *slope* of improvement, while the Information Roofline predicts the *ceiling*. If your loss curve flattens *before* the Scaling Law prediction, you have hit the Information Roofline. Adding more accelerators (Machine) or parameters (Algorithm) at this point is futile; you must improve Data quality.

This distinction closes the loop on the D·A·M taxonomy. Whether you are debugging a single training step (Iron Law), evaluating hardware utilization (Roofline), or planning a multi-million-dollar scaling campaign (Scaling Laws), the diagnostic question is always the same: *which axis dominates, and what lever moves it?*

## Summary

The D·A·M taxonomy provides a systematic framework for diagnosing ML systems bottlenecks. Each axis maps to a distinct physical constraint: Data is bounded by bandwidth, Algorithm by total operations, and Machine by peak throughput. The Iron Law quantifies these constraints, enabling systematic diagnosis. Use Arithmetic Intensity to determine the Data/Machine boundary, and the D·A·M Scorecard to evaluate system maturity.

::: {.callout-takeaways title="Key Takeaways"}

* **Every bottleneck lives in one of three places**: Data, Algorithm, or Machine. Identify the dominant axis before optimizing.
* **Profile Arithmetic Intensity before optimizing** to determine if you are Data-bound or Machine-bound.
* **Diagnose the D·A·M axis before proposing solutions**. Optimizing the wrong term yields zero improvement.
* **Grade your system with the D·A·M Scorecard** (I/O Overhead < 1%, Active Params < 50%, MFU > 50%) before investing in optimizations.

:::

## Exercises

##### Exercise 1: *Component Identification* {.unnumbered}

A production image classification service runs on an A100 GPU. The `nvidia-smi` output shows `{python} ex1_gpu_util_str`% GPU utilization, while `iotop` reveals the disk is saturated at `{python} ex1_disk_sat_str`%. Which D·A·M axis is the bottleneck? What are two specific optimizations you would recommend?

*Answer*: The bottleneck is **Data**. The disk at `{python} ex1_disk_sat_str`% saturation while GPU sits at `{python} ex1_gpu_util_str`% utilization is the classic "starving GPU" pattern. The Machine (GPU) has capacity to spare, but the Data pipeline cannot feed it fast enough. Two optimizations: (1) Convert raw images (JPEG/PNG) to a sequential binary format like TFRecords or WebDataset to reduce CPU decoding overhead and enable sequential disk reads. (2) Increase DataLoader workers and implement prefetching to overlap I/O with computation, ensuring the next batch is ready before the GPU finishes the current one.

##### Exercise 2: *Iron Law Analysis* {.unnumbered}

Consider a transformer model with `{python} ex2_params_str` parameters performing inference with batch size 1. The model requires `{python} ex2_flops_per_pass_str` TFLOPs per forward pass. On an H100 GPU (`{python} h100_fp16_tflops_str` TFLOPS peak FP16 Tensor Core), the measured latency is `{python} ex2_latency_ms_str` ms. Calculate the achieved utilization ($\eta$). Is this system Data-bound, Algorithm-bound, or Machine-bound? Justify your answer.

*Answer*: First, calculate achieved throughput:

`{python} ex2_achieved_eq`

Then calculate utilization:

`{python} ex2_util_eq`

Computed values: Achieved = `{python} ex2_achieved_str` TFLOP/s, Utilization = `{python} ex2_util_str`%.

This system is **Memory-bound** (a **Data** bottleneck). At batch size 1, each layer performs a matrix-vector multiply (GEMV) rather than a matrix-matrix multiply (GEMM). The model's `{python} ex2_params_str` parameters (~`{python} ex2_model_size_gb_str` GB in FP16) must be loaded from HBM for every forward pass, but each loaded weight is used for only a single input vector—yielding very low arithmetic intensity. The GPU's compute units sit idle waiting for memory transfers, which is why utilization is only `{python} ex2_util_str`%.

The fix targets the **Data/Algorithm boundary**: increasing the batch size transforms GEMV into GEMM, dramatically raising arithmetic intensity and pushing the workload toward compute-bound. Other effective strategies include quantization (INT8 halves the bytes moved per parameter, directly reducing the $D_{vol}/BW$ term) or speculative decoding to amortize weight loads across multiple tokens.

##### Exercise 3: *Scaling Law vs. Information Roofline* {.unnumbered}

Your team has been training a sentiment analysis model. After scaling from `{python} ex3_params_start_str` to `{python} ex3_params_end_str` parameters (`{python} ex3_scale_factor`× increase), validation loss improved from `{python} ex3_loss_start` to `{python} ex3_loss_end` (`{python} ex3_imp_str`% improvement). Chinchilla scaling would predict a ~`{python} ex3_chin_pred_str`% improvement for this compute increase. What does this discrepancy suggest? Which D·A·M axis should you investigate first, and why?

*Answer*: The discrepancy suggests you have hit the **Information Roofline**—the ceiling imposed by data quality rather than model capacity. Scaling laws predict the *slope* of improvement assuming sufficient information in the data. When actual improvement (`{python} ex3_imp_str`%) falls well short of predicted improvement (`{python} ex3_chin_pred_str`%), the model has likely extracted most learnable signal from the training distribution.

Investigate the **Data** component first. Specifically: (1) Measure the noise level in your labels—sentiment is subjective and inter-annotator agreement may be low. (2) Check for class imbalance or distribution gaps. (3) Evaluate whether your dataset covers the linguistic patterns in your target domain. Adding more parameters (Algorithm) or faster hardware (Machine) will not help—you are hitting the Bayes error rate for this data. The path forward is better data: cleaner labels, more diverse examples, or domain-specific fine-tuning data.

##### Exercise 4: *Anti-Pattern Detection* {.unnumbered}

A colleague proposes upgrading from `{python} ex4_gpu_old_str` GPUs to `{python} ex4_gpu_new_str` GPUs because training is "too slow." Before approving the `{python} ex4_cost_str` hardware purchase, what three diagnostic questions would you ask? Map each question to the D·A·M axis it investigates.

*Answer*: Before spending `{python} ex4_cost_str`, ask:

1. **"What is the current GPU utilization during training?"** → *Machine*. If utilization is below 80%, faster GPUs will just idle faster. The bottleneck is elsewhere.

2. **"What percentage of each training step is spent waiting for data?"** → *Data*. Run `torch.profiler` or check if `DataLoader` time exceeds 10% of step time. If data loading dominates, the fix is pipeline optimization (binary formats, more workers, prefetching), not new GPUs.

3. **"Can the batch size scale with 2× more GPUs without degrading convergence?"** → *Algorithm*. Doubling GPUs typically requires doubling batch size to maintain efficiency. If the model already uses the maximum stable batch size, or if larger batches hurt convergence, additional GPUs provide diminishing returns. Check the scaling efficiency: if 4 GPUs achieve only 3× speedup over 1 GPU, the communication overhead suggests 8 GPUs might achieve only 4–5× speedup.

Only if all three answers are favorable—high utilization, minimal data wait, and batch size headroom—does the hardware upgrade make sense.
