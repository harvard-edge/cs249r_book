---
engine: jupyter
---

# The D·A·M Taxonomy {#sec-appendix-dam}

## Purpose {.unnumbered}

_When an ML system fails, where should you look first: the data path, the algorithm, or the machine?_

In production, “it’s slow” and “it’s wrong” are rarely informative symptoms. A serving stack can miss its latency SLO because the GPU is idle (data starvation), because the model is doing unnecessary work (algorithmic overhead), or because the accelerator is genuinely saturated (machine-bound). Without a taxonomy, teams often optimize the wrong thing—buying faster GPUs to fix a slow input pipeline, or rewriting kernels when the model is simply too large for the latency budget.

This appendix provides a compact diagnostic framework—**Data · Algorithm · Machine (D·A·M)**—and shows how to map symptoms and measurements to the term of the Iron Law that dominates. Use it as your “first response” checklist before you commit to deeper optimization.

## How to Use This Appendix {.unnumbered}

This appendix is designed as a **reference**. Start with the scorecard-style metrics, form a hypothesis about which axis dominates, and then pick the tool that can confirm (or falsify) that hypothesis.

Conventions used here follow the book-wide notation in @sec-notation-conventions (for example, we reserve \(B\) for batch size and use \(\text{BW}\) for bandwidth).

- **When training is slow**: check GPU utilization, data wait time, and MFU; then map to Data/Algorithm/Machine.
- **When serving misses an SLA**: identify whether you are latency-bound (overhead), memory-bound (weight/KV movement), or compute-bound.
- **When cost is exploding**: use the D·A·M rubric to ensure you are improving the dominant term, not polishing a non-bottleneck.

```{python}
#| echo: false
#| label: appendix-dam-setup
from physx.constants import *
from physx.formatting import fmt, md, md_frac, md_math

# =============================================================================
# EXERCISE 1: Component Identification
# =============================================================================
ex1_gpu_util_pct = 25
ex1_disk_sat_pct = 100

ex1_gpu_util_str = f"{ex1_gpu_util_pct}"
ex1_disk_sat_str = f"{ex1_disk_sat_pct}"

# =============================================================================
# EXERCISE 2: Iron Law Analysis
# =============================================================================
# Inputs
ex2_params = 7e9
ex2_latency_s = 0.050

# Constants / Derived
ex2_bytes_per_param = 2 # FP16
ex2_flops_per_param = 2 # Fwd pass
h100_fp16_tflops_val = int(H100_FLOPS_FP16_TENSOR.to(TFLOPs / second).magnitude)

# Calculations
ex2_flops_per_pass_val = (ex2_params * ex2_flops_per_param * flop).to(TFLOPs).magnitude
ex2_achieved_tflops_val = ex2_flops_per_pass_val / ex2_latency_s
ex2_util_val = (ex2_achieved_tflops_val / h100_fp16_tflops_val) * 100
ex2_model_size_gb_val = (ex2_params * ex2_bytes_per_param * byte).to(GB).magnitude

# Formatting
ex2_params_str = "7B"
ex2_flops_per_pass_str = f"{ex2_flops_per_pass_val:.3f}"
ex2_latency_ms_str = f"{int(ex2_latency_s * 1000)}"
ex2_achieved_str = fmt(ex2_achieved_tflops_val, precision=2, commas=False)
ex2_util_str = fmt(ex2_util_val, precision=2, commas=False)
h100_fp16_tflops_str = f"{h100_fp16_tflops_val}"
ex2_model_size_gb_str = f"{ex2_model_size_gb_val:.0f}"

# LaTeX
ex2_achieved_eq = md(
    f"$$\\text{{Achieved FLOP/s}} = \\frac{{{ex2_flops_per_pass_str} \\text{{ TFLOPs}}}}"
    f"{{0.050 \\text{{ s}}}} = {ex2_achieved_str} \\text{{ TFLOP/s}}$$"
)
ex2_util_eq = md(
    f"$$\\eta = \\frac{{{ex2_achieved_str}}}{{{h100_fp16_tflops_val}}} \\approx {ex2_util_str}\\%$$"
)

# =============================================================================
# EXERCISE 3: Scaling Law vs. Information Roofline
# =============================================================================
ex3_params_start_str = "125M"
ex3_params_end_str = "1B"
ex3_scale_factor = 8
ex3_loss_start = 0.45
ex3_loss_end = 0.42
ex3_imp_pct = (ex3_loss_start - ex3_loss_end) / ex3_loss_start * 100
ex3_chin_pred_pct = 15

ex3_imp_str = f"{ex3_imp_pct:.1f}"
ex3_chin_pred_str = f"{ex3_chin_pred_pct}"

# =============================================================================
# EXERCISE 4: Anti-Pattern Detection
# =============================================================================
ex4_gpu_old_n = 4
ex4_gpu_old_type = "A100"
ex4_gpu_new_n = 8
ex4_gpu_new_type = "H100"
ex4_cost_k = 200

ex4_gpu_old_str = f"{ex4_gpu_old_n}× {ex4_gpu_old_type}"
ex4_gpu_new_str = f"{ex4_gpu_new_n}× {ex4_gpu_new_type}"
ex4_cost_str = f"${ex4_cost_k}K"
```

::: {.callout-tip title="Learning Objectives"}
By the end of this appendix, you will be able to:

- **Classify** any ML system bottleneck into one of three MECE categories: Data, Algorithm, or Machine.
- **Map** optimization techniques to their D·A·M intersection zone to understand which axes they span.
- **Apply** the **Iron Law** equation to quantitatively diagnose performance problems.
- **Distinguish** between memory-bound and compute-bound workloads using **Arithmetic Intensity**.
- **Select** appropriate profiling tools and optimization strategies for each D·A·M axis.
- **Evaluate** system health using the D·A·M Scorecard metrics (I/O Overhead, Active Params, MFU).
:::

The **Data · Algorithm · Machine (D·A·M) taxonomy** is the primary diagnostic framework for ML systems engineering. It formalizes the interdependence between information flow, mathematical logic, and physical execution. When performance stalls or behavior degrades, ask: *where is the flow blocked?* This taxonomy enables practitioners to isolate the bottleneck to one of three mutually exclusive and collectively exhaustive (MECE) axes.

## Diagnostic Summary

The taxonomy maps directly to the **Iron Law of ML Systems**, as established in @sec-introduction. @tbl-dam-components-ref summarizes the role, primary physical constraint, and core optimization pathway for each axis.

| **Axis**          | **Role**                   | **Physical Constraint** | **High-Leverage Optimization**               |
|:----------------|:-------------------------|:----------------------|:-------------------------------------------|
| **Data (D)**      | **Information** (The Fuel) | Bandwidth ($BW$)        | Data Selection (@sec-data-selection)         |
| **Algorithm (A)** | **Logic** (The Blueprint)  | Operations ($O$)        | Model Compression (@sec-model-compression)   |
| **Machine (M)**   | **Physics** (The Engine)   | Throughput ($R_{peak}$) | Hardware Acceleration (@sec-ai-acceleration) |

: **D·A·M Axis Reference.** Role and constraints of Data, Algorithm, and Machine axes within the ML systems stack. {#tbl-dam-components-ref}

## Intersection Landscape {#sec-dam-venn}

The three-row table above presents the axes as independent categories, which is useful for diagnosis. But real systems engineering lives at the *boundaries*. @fig-dam-venn maps the intersection landscape: what concepts and techniques emerge when two or three axes overlap.

![**The D·A·M Intersection Landscape.** Each circle represents a pure domain: Data (information), Algorithm (logic), and Machine (physics). The pairwise intersections capture techniques that require reasoning about two domains simultaneously. The center---where all three converge---is ML Systems Engineering itself: the discipline of balancing data flow, algorithmic complexity, and hardware constraints within a single system.](images/dam_venn.svg){#fig-dam-venn fig-alt="Venn diagram with three overlapping circles labeled Data, Algorithm, and Machine. Pairwise intersections are labeled: D intersection A shows data selection and curriculum learning; D intersection M shows I/O bandwidth and data formats; A intersection M shows quantization and kernel fusion. The center intersection is labeled ML Systems Engineering with Iron Law and Roofline."}

@tbl-dam-intersections provides a scannable reference for each zone.

| **Zone** | **Name** | **Key Techniques** | **Book Coverage** |
|:---------|:---------|:-------------------|:------------------|
| **D** (pure) | Information | Storage formats, data quality, distributions | @sec-data-engineering |
| **A** (pure) | Logic | Loss functions, architectures, gradients | @sec-deep-learning-primer, @sec-neural-network-architectures |
| **D $\cap$ A** | What to Learn From | Data selection, curriculum learning, compute-optimal scaling | @sec-data-selection, @sec-ai-training |
| **D $\cap$ M** | How to Move Information | I/O bandwidth, prefetching, data formats | @sec-data-engineering, @sec-ai-acceleration |
| **A $\cap$ M** | How to Execute Efficiently | Quantization, pruning, kernel fusion, mixed precision | @sec-ml-frameworks, @sec-model-compression |
| **M** (pure) | Physics | Silicon, memory hierarchy, peak FLOPS | @sec-ai-acceleration |
| **D $\cap$ A $\cap$ M** | ML Systems Engineering | Iron Law, Roofline, training loops, serving | @sec-ai-training, @sec-ai-serving, @sec-benchmarking-ai |

: **D·A·M Intersection Reference.** Each zone maps specific techniques to the axes they span and the chapters that cover them. The pairwise intersections require reasoning about two domains simultaneously; the center requires all three. {#tbl-dam-intersections}

The pure zones contain concepts that belong entirely to one axis: storage formats and distribution properties are purely Data concerns, loss functions and gradient computations are purely Algorithm, and silicon physics and peak FLOPS are purely Machine. These are the topics where single-domain expertise suffices.

The pairwise intersections are where systems thinking begins. **D $\cap$ A** (*What to Learn From*) encompasses data selection, curriculum learning, active learning, and scaling laws like Chinchilla ($D \approx 20N$)---all requiring joint reasoning about information content and algorithmic capacity. Adding data without considering whether the model can learn from it wastes compute; choosing architectures without considering data availability wastes engineering time. **D $\cap$ M** (*How to Move Information*) covers I/O bandwidth, prefetching strategies, data formats, and the Energy-Movement Invariant. This intersection is where Data Gravity manifests: the physical cost of moving bytes through the memory hierarchy determines whether the machine can be fed fast enough. **A $\cap$ M** (*How to Execute Efficiently*) spans quantization, pruning, kernel fusion, mixed precision, and computational graph optimization. A pruning strategy that reduces FLOPs but destroys memory access patterns can *slow down* execution on real hardware.

The center---$D \cap A \cap M$---is where all three axes converge. The Iron Law, the Roofline Model, end-to-end training loops, serving pipelines, and holistic benchmarking all require simultaneous reasoning about data flow, algorithmic complexity, and hardware utilization. This center is not a single technique; it is the discipline itself.

Understanding the landscape tells you *where* a technique lives. The next step is quantifying *which axis dominates* for a given workload---and for that, we need the Iron Law.

## Iron Law Mapping

The performance of any ML task is governed by the distribution of work across the D·A·M axes. The **Iron Law Mapping** reveals which component's variables dominate the execution time:

$$ T = \underbrace{ \frac{D_{vol}}{BW} }_{\text{Data (D)}} + \underbrace{ \frac{O}{R_{peak} \cdot \eta} }_{\text{Algorithm (A) / Machine (M)}} + \underbrace{ L_{lat} }_{\text{Overhead}} $$

Note that Algorithm and Machine share the compute term; they are separated by which variable you control. Reducing the total operations ($O$) is an **Algorithm** lever, while improving the hardware's peak throughput ($R_{peak}$) or utilization ($\eta$) is a **Machine** lever.

This equation transforms performance debugging from a qualitative guessing game into a quantitative engineering problem. Every bottleneck hides in one of these terms. If your system is slow, it is because you are moving too much data ($D_{vol}$), lacking bandwidth ($BW$), executing too many operations ($O$), or failing to utilize your hardware's peak capability ($\eta$). The levers below map specific optimizations to the variable they improve.

### Component Levers

*   **Data Lever**: Reducing the volume of data ($D_{vol}$) through deduplication or curriculum learning, or increasing I/O bandwidth ($BW$).
*   **Algorithm Lever**: Reducing total arithmetic operations ($O$) through pruning, quantization, or architectural refinement.
*   **Machine Lever**: Increasing the denominator of the compute term by improving peak throughput ($R_{peak}$) or increasing the utilization factor ($\eta$) via kernel fusion.

### D·A·M Coordination: From Sum to Max

The additive **Iron Law** represents **sequential execution**—the worst case where Data, Algorithm, and Machine take turns. But skilled systems engineering transforms the sum into a **max**:

$$ T_{sequential} = \frac{D_{vol}}{BW} + \frac{O}{R_{peak} \cdot \eta} + L_{lat} \quad \xrightarrow{\text{overlap}} \quad T_{pipelined} = \max\left(\frac{D_{vol}}{BW}, \frac{O}{R_{peak} \cdot \eta}\right) + L_{lat} $$

**The systems engineer's job is to make the components run in parallel, not in series.** @tbl-dam-overlap summarizes key D·A·M Coordination techniques:

| **Technique**           | **D·A·M Axes Overlapped**    | **Implementation**                                   |
|:----------------------|:---------------------------|:---------------------------------------------------|
| **Prefetching**         | D overlaps M                 | DataLoader with `prefetch_factor`, `pin_memory=True` |
| **CUDA Streams**        | D overlaps M                 | Separate streams for H2D transfer and compute        |
| **Async Gradient Sync** | M (communication) overlaps A | Overlap AllReduce with next forward pass             |
| **Double Buffering**    | D overlaps M                 | Fill buffer N+1 while computing on buffer N          |

: **D·A·M Overlap Techniques.** Each technique allows one D·A·M axis to execute while another is in flight, converting additive latency into overlapped latency. {#tbl-dam-overlap}

**When does overlap help?** Only when the D·A·M axes are **balanced**. If one term dominates (e.g., severely memory-bound), overlapping the smaller term with the larger yields negligible gain—the max is still dominated by the same bottleneck. Overlap provides the greatest benefit when $D_{vol}/BW \approx O/(R_{peak} \cdot \eta)$.

::: {.callout-warning title="The Overhead That Cannot Hide"}
The latency term $L_{lat}$ (kernel launch, synchronization barriers, Python dispatch) typically cannot be overlapped—it represents serialization points where all components must wait. This is why **kernel fusion** is so powerful: it eliminates $L_{lat}$ by combining operations, not just by speeding up any single component.
:::

## Arithmetic Intensity Boundary

The boundary between **Data** (Memory-Bound) and **Machine** (Compute-Bound) is not arbitrary; it is defined mathematically by the **Arithmetic Intensity** ($I$) of the workload.

For a rigorous definition of **Arithmetic Intensity** and the **Roofline Model**, see @sec-system-foundations-roofline-model-5f7c. Use that model to quantitatively distinguish between Data and Machine bottlenecks before applying the optimizations below.

## Rules of Thumb

In the heat of a production outage, you rarely have time to solve the full **Iron Law** equation. Instead, veteran systems engineers rely on these quantitative heuristics to quickly narrow down the search space. Use these thresholds as your first line of defense.

*   **If GPU Utilization $<$ 80%**: You are likely **Data Bound** (or CPU bound). The accelerator is starving.
*   **If GPU Utilization $>$ 95%**: You are likely **Machine Bound**. The accelerator is fully saturated.
*   **If Batch Size is 1**: You are likely **Latency Bound** (Algorithm overhead dominates).
*   **If **Arithmetic Intensity** $<$ 100 FLOPs/byte**: You are likely **Memory Bound** (Data/Machine boundary). This threshold is approximate for current-generation accelerators; compute your hardware's specific ridge point ($R_{peak}/BW$) for a precise boundary.
*   **If System works in Dev but fails in Prod**: Suspect **Data Drift** (Data component).

Note that common industry labels map to DAM components as follows: **Memory Bound** typically indicates a **Data** bottleneck (information cannot reach the accelerator fast enough), **Compute Bound** indicates a **Machine** bottleneck (the accelerator is fully saturated), and **Latency Bound** indicates an **Algorithm** bottleneck (serial operation depth or overhead dominates).

### The Bottleneck Diagnostic

Once you identify the bottleneck, @tbl-bottleneck-actions tells you what to do—and what NOT to do:

| **If You're...**  | **Dominant Term**         | **Optimization That Works**                               | **Optimization That's Wasted**                     |
|:----------------|:------------------------|:--------------------------------------------------------|:-------------------------------------------------|
| **Memory-Bound**  | $D_{vol}/BW$              | Quantization, pruning, batching, kernel fusion            | Faster GPU (more FLOP/s won't help)                |
| **Compute-Bound** | $O/(R_{peak} \cdot \eta)$ | Better kernels, Tensor Cores, faster GPU, lower precision | More memory bandwidth (already saturated)          |
| **Latency-Bound** | $L_{lat}$                 | Batching requests, kernel fusion, async dispatch          | Neither compute nor bandwidth (overhead dominates) |

: **What Works vs. What's Wasted.** Optimizing the wrong term yields exactly zero improvement. A memory-bound LLM will not speed up from a faster GPU; the GPU will simply idle faster while waiting for memory. {#tbl-bottleneck-actions}

## Anti-Patterns

Diagnosing systems is often a process of elimination. Before diving into complex kernel optimizations, ensure you aren't falling into one of these common traps that waste engineering cycles.

*   **The Hardware Crutch**: Buying faster accelerators (**Machine**) to fix a slow Python data loader (**Data**). The new hardware will just idle faster.
*   **The Model Twiddle**: Changing neural architectures (**Algorithm**) when the bottleneck is actually network bandwidth or disk I/O.
*   **The Premature Optimizer**: Writing custom CUDA kernels (**Machine**) before verifying if the Algorithm is simply doing too many unnecessary operations.

## D·A·M Case Studies

Theoretical constraints often manifest as confusing symptoms in production. These real-world scenarios illustrate how to apply the taxonomy.

### Case 1: The "Data" Bottleneck (The Starving GPU)

**Symptom**: You provision a massive A100 GPU instance to speed up training, but your training time hardly improves. `nvidia-smi` shows GPU utilization fluctuating between 10% and 40%.

**Diagnosis**: The **Data** component cannot supply the **Machine** fast enough. You are I/O bound.

**The Fix**: This is not a model or hardware problem. You must optimize the ETL pipeline:

*   Move from raw JPEGs (CPU decoding heavy) to TFRecords or WebDataset (sequential reads).
*   Increase the number of data loader workers.
*   Prefetch batches to GPU memory.

### Case 2: The "Algorithm" Bottleneck (The Latency Cliff)

**Symptom**: Your real-time recommendation system fails to meet the 20ms latency SLA (Service Level Agreement). The GPU utilization is low, and the batch size is 1.

**Diagnosis**: The **Algorithm** is too computationally deep for the sequential deadline. You are latency-bound by serial operations.

**The Fix**: Throwing more hardware (Machine) won't help because latency is limited by the serial execution of layers. You must change the Algorithm:

*   **Quantization**: Switch to INT8 to reduce memory fetch time.
*   **Pruning**: Remove redundant heads or channels.
*   **Knowledge Distillation**: Train a smaller student model.

### Case 3: The "Machine" Bottleneck (The Compute Wall)

**Symptom**: GPU utilization is pinned at 99%. Memory bandwidth is unsaturated. Training is stable but takes 3 weeks.

**Diagnosis**: You have successfully fed the beast. The system is **Compute Bound**.

**The Fix**: You have hit the physical limits of the single chip.

*   **Scale Up**: Move to a newer generation GPU (e.g., A100 to H100).
*   **Scale Out**: Distribute training across multiple GPUs (Data Parallelism).
*   **Lower Precision**: Switch from FP32 to BF16 (doubling theoretical TFLOPS).

## Production Troubleshooting

Identifying the root cause of performance bottlenecks requires systematic elimination. @tbl-dam-troubleshooting provides a diagnostic matrix for common failure modes observed in production deployments.

| **Symptom**               | **Likely D·A·M Culprit** | **Diagnostic Question**                                      | **Recommended Action**                                 |
|:------------------------|:-----------------------|:-----------------------------------------------------------|:-----------------------------------------------------|
| **Low GPU Utilization**   | **Data**                 | Is the data loader keeping up with the accelerator?          | Implement prefetching and use binary formats.          |
| **High Latency (P99)**    | **Algorithm**            | Is the model depth or width exceeding the latency budget?    | Apply quantization (INT8) or structured pruning.       |
| **High Training Cost**    | **Machine**              | Is the hardware utilization ($\eta$) below 30%?              | Optimize CUDA kernels or use spot instances.           |
| **Silent Accuracy Drift** | **Data**                 | Has the statistical distribution ($P_t$) shifted from $P_0$? | Trigger retraining and update active learning filters. |
| **Out-of-Memory (OOM)**   | **Algorithm/Machine**    | Does the model state fit in available VRAM?                  | Use gradient checkpointing or reduce batch size.       |

: **D·A·M Diagnostic Matrix.** Root cause identification and remediation strategies for common ML systems failures. {#tbl-dam-troubleshooting}

## Tooling Map

Once you have a hypothesis (e.g., "I suspect I am Machine Bound"), you need evidence to prove it. Abstract concepts must be measured with concrete utilities. @tbl-dam-tooling connects the theoretical components to the specific Linux and Python profiling tools you should reach for.

| **Axis** | **Key Metric**                | **Primary Tool**        | **Secondary Tool**             |
|:------------|:----------------------------|:----------------------|:-----------------------------|
| **Data**      | Batch Load Time               | `tqdm` (iterations/sec) | `iotop`, `dstat` (Disk I/O)    |
| **Algorithm** | FLOPs, Model Depth            | PyTorch Profiler        | DeepSpeed Flops Profiler       |
| **Machine**   | GPU Utilization, SM Occupancy | `nvidia-smi`            | Nsight Compute, Nsight Systems |

: **D·A·M Tooling Map.** Profiling utilities for diagnosing bottlenecks along each D·A·M axis. Start with the primary tool for quick triage; use secondary tools for deep-dive analysis. {#tbl-dam-tooling}

## D·A·M Scorecard

To surpass qualitative guessing, use the efficiency ratios in @tbl-dam-scorecard to grade your system's performance against its theoretical limit. This "Report Card" standardizes what "good" looks like.

| **Axis** | **Metric**        | **Definition**                                         | **Failing Grade** | **Passing Grade** |
|:------------|:----------------|:-----------------------------------------------------|----------------:|----------------:|
| **Data**      | **I/O Overhead**  | $\frac{\text{Data Wait Time}}{\text{Total Step Time}}$ |           $>$ 10% |            $<$ 1% |
| **Algorithm** | **Active Params** | $\frac{\text{Non-Zero Params}}{\text{Total Params}}$   |      100% (Dense) |  $<$ 50% (Sparse) |
| **Machine**   | **MFU**           | $\frac{\text{Achieved FLOPs}}{\text{Peak FLOPs}}$      |           $<$ 30% |           $>$ 50% |

: **The D·A·M Efficiency Rubric.** Use these three numbers to characterize any ML system's maturity. **MFU (Model FLOPs Utilization)** is the single most important metric for large-scale training. {#tbl-dam-scorecard}

## Scaling Laws vs. Roofline

Systems engineering requires distinguishing between *growth trajectories* and *fundamental limits*.

### Scaling Laws (The Journey)
**Scaling Laws** are empirical power laws that predict *how fast* model performance improves as we increase resources.

*   **Kaplan Scaling** [@kaplan2020scaling]: Performance improves predictably with Parameters ($N$), Data ($D$), and Compute ($C$).
*   **Chinchilla Scaling** [@hoffmann2022training]: Defines the *optimal ratio* of these resources (e.g., $D \approx 20N$ tokens per parameter).

These laws are **economic guides**. They tell you: "If I double my compute budget, my error rate should drop by $X$%." They assume the information is there to be learned.

### The Information Roofline (The Destination)
The **Information Roofline** is the theoretical limit of what *can* be learned from the data, regardless of scale.

*   **The Ceiling**: The **Bayes Error Rate** (the irreducible error inherent in the data).
*   **The Slope**: **Information Density** (Signal-to-Noise Ratio).
*   **The Bottleneck**: If your data has low information density (e.g., noisy financial tickers), you hit the "Data Quality Wall" long before you hit the Compute Wall.

**The Diagnostic Lesson**: Scaling Laws predict the *slope* of improvement. The Information Roofline predicts the *ceiling*. If your loss curve flattens *before* the Scaling Law prediction, you have hit the Information Roofline. Adding more accelerators (Machine) or Parameters (Algorithm) at this point is futile; you must improve Data Quality.

## Summary

The D·A·M taxonomy provides a systematic framework for diagnosing ML systems bottlenecks. Each axis maps to a distinct physical constraint: Data is bounded by bandwidth, Algorithm by total operations, and Machine by peak throughput. The **Iron Law** quantifies these constraints, enabling systematic diagnosis. Use **Arithmetic Intensity** to determine the Data/Machine boundary, and the D·A·M Scorecard to evaluate system maturity.

::: {.callout-takeaways title="Key Takeaways"}

* **Every bottleneck lives in one of three places**: Data, Algorithm, or Machine. Identify the dominant axis before optimizing.
* **Profile Arithmetic Intensity before optimizing** to determine if you are Data-bound or Machine-bound.
* **Diagnose the D·A·M axis before proposing solutions**. Optimizing the wrong term yields zero improvement.
* **Grade your system with the D·A·M Scorecard** (I/O Overhead < 1%, Active Params < 50%, MFU > 50%) before investing in optimizations.

:::

## Exercises

##### Exercise 1: *Component Identification* {.unnumbered}

A production image classification service runs on an A100 GPU. The `nvidia-smi` output shows `{python} ex1_gpu_util_str`% GPU utilization, while `iotop` reveals the disk is saturated at `{python} ex1_disk_sat_str`%. Which D·A·M axis is the bottleneck? What are two specific optimizations you would recommend?

*Answer*: The bottleneck is **Data**. The disk at `{python} ex1_disk_sat_str`% saturation while GPU sits at `{python} ex1_gpu_util_str`% utilization is the classic "starving GPU" pattern. The Machine (GPU) has capacity to spare, but the Data pipeline cannot feed it fast enough. Two optimizations: (1) Convert raw images (JPEG/PNG) to a sequential binary format like TFRecords or WebDataset to reduce CPU decoding overhead and enable sequential disk reads. (2) Increase DataLoader workers and implement prefetching to overlap I/O with computation, ensuring the next batch is ready before the GPU finishes the current one.

##### Exercise 2: *Iron Law Analysis* {.unnumbered}

Consider a transformer model with `{python} ex2_params_str` parameters performing inference with batch size 1. The model requires `{python} ex2_flops_per_pass_str` TFLOPs per forward pass. On an H100 GPU (`{python} h100_fp16_tflops_str` TFLOPS peak FP16 Tensor Core), the measured latency is `{python} ex2_latency_ms_str`ms. Calculate the achieved utilization ($\eta$). Is this system Data-bound, Algorithm-bound, or Machine-bound? Justify your answer.

*Answer*: First, calculate achieved throughput:

`{python} ex2_achieved_eq`

Then calculate utilization:

`{python} ex2_util_eq`

Computed values: Achieved = `{python} ex2_achieved_str` TFLOP/s, Utilization = `{python} ex2_util_str`%.

This system is **Memory-bound** (a **Data** bottleneck). At batch size 1, each layer performs a matrix-vector multiply (GEMV) rather than a matrix-matrix multiply (GEMM). The model's `{python} ex2_params_str` parameters (~`{python} ex2_model_size_gb_str` GB in FP16) must be loaded from HBM for every forward pass, but each loaded weight is used for only a single input vector—yielding very low arithmetic intensity. The GPU's compute units sit idle waiting for memory transfers, which is why utilization is only `{python} ex2_util_str`%.

The fix targets the **Data/Algorithm boundary**: increasing the batch size transforms GEMV into GEMM, dramatically raising arithmetic intensity and pushing the workload toward compute-bound. Other effective strategies include quantization (INT8 halves the bytes moved per parameter, directly reducing the $D_{vol}/BW$ term) or speculative decoding to amortize weight loads across multiple tokens.

##### Exercise 3: *Scaling Law vs. Information Roofline* {.unnumbered}

Your team has been training a sentiment analysis model. After scaling from `{python} ex3_params_start_str` to `{python} ex3_params_end_str` parameters (`{python} ex3_scale_factor`× increase), validation loss improved from `{python} ex3_loss_start` to `{python} ex3_loss_end` (`{python} ex3_imp_str`% improvement). Chinchilla scaling would predict a ~`{python} ex3_chin_pred_str`% improvement for this compute increase. What does this discrepancy suggest? Which D·A·M axis should you investigate first, and why?

*Answer*: The discrepancy suggests you have hit the **Information Roofline**—the ceiling imposed by data quality rather than model capacity. Scaling laws predict the *slope* of improvement assuming sufficient information in the data. When actual improvement (`{python} ex3_imp_str`%) falls well short of predicted improvement (`{python} ex3_chin_pred_str`%), the model has likely extracted most learnable signal from the training distribution.

Investigate the **Data** component first. Specifically: (1) Measure the noise level in your labels—sentiment is subjective and inter-annotator agreement may be low. (2) Check for class imbalance or distribution gaps. (3) Evaluate whether your dataset covers the linguistic patterns in your target domain. Adding more parameters (Algorithm) or faster hardware (Machine) will not help—you are hitting the Bayes error rate for this data. The path forward is better data: cleaner labels, more diverse examples, or domain-specific fine-tuning data.

##### Exercise 4: *Anti-Pattern Detection* {.unnumbered}

A colleague proposes upgrading from `{python} ex4_gpu_old_str` GPUs to `{python} ex4_gpu_new_str` GPUs because training is "too slow." Before approving the `{python} ex4_cost_str` hardware purchase, what three diagnostic questions would you ask? Map each question to the D·A·M axis it investigates.

*Answer*: Before spending `{python} ex4_cost_str`, ask:

1. **"What is the current GPU utilization during training?"** → *Machine*. If utilization is below 80%, faster GPUs will just idle faster. The bottleneck is elsewhere.

2. **"What percentage of each training step is spent waiting for data?"** → *Data*. Run `torch.profiler` or check if `DataLoader` time exceeds 10% of step time. If data loading dominates, the fix is pipeline optimization (binary formats, more workers, prefetching), not new GPUs.

3. **"Can the batch size scale with 2× more GPUs without degrading convergence?"** → *Algorithm*. Doubling GPUs typically requires doubling batch size to maintain efficiency. If the model already uses the maximum stable batch size, or if larger batches hurt convergence, additional GPUs provide diminishing returns. Check the scaling efficiency: if 4 GPUs achieve only 3× speedup over 1 GPU, the communication overhead suggests 8 GPUs might achieve only 4-5× speedup.

Only if all three answers are favorable—high utilization, minimal data wait, and batch size headroom—does the hardware upgrade make sense.
