---
engine: jupyter
---

# Machine Foundations {#sec-machine-foundations}

This appendix covers the hardware and physical constraints that govern ML system performance. From performance modeling to memory hierarchies to numerical precision, these are the "speed limits" that every ML engineer must understand. The concepts here underpin the hardware acceleration strategies in @sec-ai-acceleration, the training optimizations in @sec-ai-training, and the serving architectures in @sec-model-serving-systems.

## Numbers Every ML Systems Engineer Should Know {#sec-numbers-to-know}

```{python}
#| label: numbers-to-know-setup
#| echo: false

from physx.constants import *
from physx.formatting import fmt

# =============================================================================
# PURPOSE
# =============================================================================
# Purpose: Compute all values for the "Numbers Every ML Systems Engineer
#          Should Know" quick reference section. Inspired by Jeff Dean's
#          famous "Latency Numbers Every Programmer Should Know."
#
# Philosophy: RATIOS and RULES are eternal; absolute numbers are snapshots.
#             We organize this section to emphasize what will still be true
#             in 10 years, while providing current hardware specs as reference.

# =============================================================================
# PHYSICS CONSTANTS (Eternal)
# =============================================================================
speed_of_light_km_ms = int(SPEED_OF_LIGHT_FIBER_KM_S.to(ureg.kilometer / second).magnitude / 1000)

# =============================================================================
# ENERGY RATIOS (Stable across process nodes)
# =============================================================================
dram_vs_compute = int(ENERGY_DRAM_ACCESS_PJ.magnitude / ENERGY_FLOP_FP16_PJ.magnitude)
fp32_vs_int8 = int(ENERGY_FLOP_FP32_PJ.magnitude / ENERGY_FLOP_INT8_PJ.magnitude)
fp32_vs_fp16 = round(ENERGY_FLOP_FP32_PJ.magnitude / ENERGY_FLOP_FP16_PJ.magnitude, 1)
l1_vs_reg = int(ENERGY_SRAM_L1_PJ.magnitude / ENERGY_REG_PJ.magnitude)

# =============================================================================
# MEMORY HIERARCHY RATIOS (Stable)
# =============================================================================
hbm_vs_l1 = int(LATENCY_HBM3.to(NS).magnitude / LATENCY_L1_REGISTER.to(NS).magnitude)
ssd_vs_l1 = int(LATENCY_NVME_SSD.to(NS).magnitude / LATENCY_L1_REGISTER.to(NS).magnitude)
network_vs_local = int(LATENCY_INFINIBAND.to(NS).magnitude / LATENCY_HBM3.to(NS).magnitude)

# GPU bandwidth vs CPU-GPU link (architectural ratio)
gpu_bw_vs_pcie = int(H100_MEM_BW.to(GB / second).magnitude / PCIE_GEN5_BW.to(GB / second).magnitude)

# =============================================================================
# CURRENT HARDWARE REFERENCE (circa 2024 — will need updates)
# =============================================================================
# Latencies
lat_l1_ns = int(LATENCY_L1_REGISTER.to(NS).magnitude)
lat_l2_ns = int(LATENCY_L2_CACHE.to(NS).magnitude)
lat_hbm_ns = int(LATENCY_HBM3.to(NS).magnitude)
lat_pcie_ns = int(LATENCY_PCIE_GEN5.to(NS).magnitude)
lat_ib_ns = int(LATENCY_INFINIBAND.to(NS).magnitude)
lat_ssd_ns = int(LATENCY_NVME_SSD.to(NS).magnitude)

# Bandwidths
bw_hbm_h100 = f"{H100_MEM_BW.to(TB / second).magnitude:.1f}"
bw_pcie5 = int(PCIE_GEN5_BW.to(GB / second).magnitude)
bw_dram = int(SYSTEM_MEMORY_BW.to(GB / second).magnitude)
bw_nvme = f"{NVME_SEQUENTIAL_BW.to(GB / second).magnitude:.1f}"

# Compute
flops_h100_fp16 = int(H100_FLOPS_FP16_TENSOR.to(TFLOPs / second).magnitude)
flops_h100_fp8 = int(H100_FLOPS_FP8_TENSOR.to(TFLOPs / second).magnitude)
flops_a100_fp16 = int(A100_FLOPS_FP16_TENSOR.to(TFLOPs / second).magnitude)
flops_mobile_int8 = int(MOBILE_NPU_TOPS_INT8.to(TFLOPs / second).magnitude)
dc_mobile_ratio = int(flops_h100_fp16 / flops_mobile_int8)

# Ridge points
ridge_a100 = int(A100_FLOPS_FP16_TENSOR.to(TFLOPs / second).magnitude / A100_MEM_BW.to(TB / second).magnitude)
ridge_h100 = int(H100_FLOPS_FP16_TENSOR.to(TFLOPs / second).magnitude / H100_MEM_BW.to(TB / second).magnitude)
```

Just as Jeff Dean's "Latency Numbers Every Programmer Should Know"[^fn-jeff-dean] shaped a generation of systems engineers, these reference numbers provide the order-of-magnitude intuition essential for ML systems design. While absolute values evolve with hardware generations, the *ratios* between categories remain remarkably stable. **Memorize the relationships; use the specific numbers as sanity checks.**

::: {.callout-note title="Three Numbers That Matter Most"}
If you memorize nothing else from this section, memorize these:

1. **~600× energy ratio**: DRAM access costs ~`{python} dram_vs_compute`× more energy than an FP16 multiply-add. This is why arithmetic intensity is everything.

2. **16 bytes/parameter for training**: Model weights (2B FP16) + master weights (4B FP32) + optimizer states (8B Adam). A 7B model needs 112 GB just to start training.

3. **~200 km/ms speed of light in fiber**: Cross-country latency is ~40 ms. No optimization can reduce this—it's physics.
:::

[^fn-jeff-dean]: **Jeff Dean** is a Google Senior Fellow and one of the architects of Google's distributed systems infrastructure, including MapReduce, BigTable, and TensorFlow. His latency numbers, originally presented with Peter Norvig around 2010, became a canonical reference for systems engineers. The numbers have been updated over the years as hardware evolved, but the *hierarchy* of latencies remains remarkably stable. See Colin Scott's interactive visualization at <https://colin-scott.github.io/personal_website/research/interactive_latency.html>.

### The Invariants: Numbers That Won't Change {.unnumbered}

These relationships are governed by physics or arithmetic—they will still be true in 2035.

**The Speed of Light Tax**

| Distance           | Round-Trip Latency | Implication                   |
|:-----------------|-----------------:|:----------------------------|
| Same datacenter    |              ~1 ms | Distributed training feasible |
| Cross-country (US) |             ~40 ms | Edge needed for <100 ms apps  |
| Cross-Atlantic     |             ~60 ms | CDN required for global users |
| Cross-Pacific      |            ~100 ms | Data locality is critical     |

: Light in fiber travels ~200 km/ms. These latencies are physics—no optimization can reduce them. {#tbl-speed-of-light-ref}

**The Energy Hierarchy**

| Relationship                 |                        Ratio | Why It's Stable                       |
|:---------------------------|---------------------------:|:------------------------------------|
| DRAM access vs. FP16 compute | ~`{python} dram_vs_compute`× | Wire capacitance scales with distance |
| FP32 vs. INT8 energy         |    ~`{python} fp32_vs_int8`× | Bit width determines switching energy |
| FP32 vs. FP16 energy         |    ~`{python} fp32_vs_fp16`× | Halving bits roughly halves energy    |
| L1 SRAM vs. register         |       ~`{python} l1_vs_reg`× | Distance to ALU                       |

: **The Energy Wall.** Moving data costs ~580× more energy than computing on it. This ratio is physics, not engineering.[^fn-horowitz-energy] {#tbl-energy-ratios-ref}

[^fn-horowitz-energy]: Energy numbers from Horowitz's classic "Computing's Energy Problem" (ISSCC 2014, 45nm process). While absolute values scale with process node, the *ratios* between memory access and compute remain remarkably stable because wire capacitance (distance) dominates.

**The Memory Hierarchy**

| Relationship                   |                                Ratio | Why It Persists                   |
|:-----------------------------|-----------------------------------:|:--------------------------------|
| GPU memory (HBM) vs. L1 cache  |        ~`{python} hbm_vs_l1`× slower | On-chip vs. off-chip              |
| SSD vs. L1 cache               |        ~`{python} ssd_vs_l1`× slower | Electrical vs. mechanical/flash   |
| Network vs. local memory       | ~`{python} network_vs_local`× slower | Speed of light + switching        |
| GPU memory BW vs. CPU↔GPU link |   ~`{python} gpu_bw_vs_pcie`× faster | Architectural investment priority |

: **The Latency Hierarchy.** Each level costs roughly 10–100× more than the one above it. {#tbl-memory-ratios-ref}

**The Scaling Laws**

| Rule                          | Formula                      | Example                         |
|:----------------------------|:---------------------------|:------------------------------|
| Inference memory (FP16)       | 2 bytes × parameters         | 7B params → 14 GB               |
| Inference memory (INT8)       | 1 byte × parameters          | 7B params → 7 GB                |
| Training memory (Adam)        | 16 bytes × parameters        | 7B params → 112 GB              |
| Inference FLOPs (transformer) | ~2 × parameters per token    | 7B model → ~14 GFLOPs/token     |
| Training FLOPs                | ~6 × parameters × tokens     | 7B on 1T tokens → 4×10²² FLOPs  |
| Datacenter vs. edge compute   | ~`{python} dc_mobile_ratio`× | Compute per watt × power budget |

: **Scaling Rules.** These are arithmetic, not hardware-specific. Training memory includes FP16 weights (2B), FP32 master weights (4B), and Adam optimizer states (8B for momentum + variance).[^fn-training-memory] {#tbl-scaling-rules-ref}

[^fn-training-memory]: The 16 bytes/parameter rule assumes mixed-precision training with Adam. ZeRO optimization (@sec-distributed-training) can reduce this by sharding optimizer states across GPUs, but the total memory across all GPUs remains ~16× parameters.

### Latency Budgets: The Non-Negotiables {#sec-latency-budgets-ref}

These budgets are set by physics (safety) or psychology (human perception)—not by engineering choice. Unlike hardware specs that improve each generation, these are *constraints* your system must meet.

| Application        |     Budget | Constraint                           |
|:-----------------|---------:|:-----------------------------------|
| Autonomous braking |     <10 ms | At 100 km/h, 10 ms = 28 cm of travel |
| Voice assistant    |    <100 ms | Human perception of "instant"        |
| Web search         |    <200 ms | User patience threshold              |
| Video streaming    |       <1 s | Buffer tolerance                     |
| Batch training     | hours–days | Throughput dominates latency         |

: **Latency Targets.** Miss these and the application fails, regardless of accuracy. {#tbl-latency-targets-ref}

### Current Hardware Reference (c. 2024) {.unnumbered}

These numbers reflect the current generation. Use them for back-of-envelope calculations, but expect them to improve ~2× every 2–3 years.

**Memory Latency & Bandwidth**

| Level                |                    Latency |                   Bandwidth |
|:-------------------|-------------------------:|--------------------------:|
| Register             |                    ~0.3 ns |                           — |
| L1 Cache             |   ~`{python} lat_l1_ns` ns |                           — |
| L2 Cache             |   ~`{python} lat_l2_ns` ns |                           — |
| GPU HBM3             |  ~`{python} lat_hbm_ns` ns | `{python} bw_hbm_h100` TB/s |
| PCIe Gen5 (CPU↔GPU)  | ~`{python} lat_pcie_ns` ns |    `{python} bw_pcie5` GB/s |
| CPU DRAM             |                    ~100 ns |     `{python} bw_dram` GB/s |
| InfiniBand (network) |   ~`{python} lat_ib_ns` ns |                     50 GB/s |
| NVMe SSD             |  ~`{python} lat_ssd_ns` ns |     `{python} bw_nvme` GB/s |

: **Memory Hierarchy (c. 2024).** Specific values for current hardware. {#tbl-memory-current-ref}

**Compute Throughput**

| Platform              |                         FP16/BF16 |                              INT8 | Power |
|:--------------------|--------------------------------:|--------------------------------:|----:|
| Datacenter GPU (H100) | `{python} flops_h100_fp16` TFLOPS |    `{python} flops_h100_fp8` TOPS |  700W |
| Datacenter GPU (A100) | `{python} flops_a100_fp16` TFLOPS |                          624 TOPS |  400W |
| Mobile NPU            |                                 — | `{python} flops_mobile_int8` TOPS |  3–5W |

: **Compute Reference (c. 2024).** Datacenter is ~28× more powerful than mobile—this ratio persists across generations. {#tbl-compute-current-ref}

**Roofline Ridge Points**

| Accelerator |                    Ridge Point | Implication                  |
|:----------|-----------------------------:|:---------------------------|
| A100 (FP16) | `{python} ridge_a100` ops/byte | Below → memory-bound         |
| H100 (FP16) | `{python} ridge_h100` ops/byte | Higher bar for compute-bound |

: **Arithmetic Intensity Thresholds (c. 2024).** Most inference workloads are <10 ops/byte—firmly memory-bound. {#tbl-ridge-current-ref}

---

::: {.callout-note title="A Note on Terminology: GPUs and Accelerators"}
Throughout this book, we often use "GPU" when discussing hardware acceleration. However, the principles—roofline analysis, memory hierarchies, numerical precision, and performance modeling—apply equally to **TPUs**, **NPUs**, **custom ASICs**, and other specialized AI accelerators. We use "GPU" as convenient shorthand given its prevalence in the ML ecosystem, but readers should understand these concepts as **accelerator-general** unless we explicitly discuss vendor-specific features (e.g., CUDA, NVLink).
:::

## The Physics of Computing {#sec-system-foundations-physics-computing-b6a4}

::: {.callout-perspective title="Why This Matters"}
You have trained a model that achieves good accuracy, but inference takes 200ms when your SLA requires 50ms. Where do you start? Performance analysis models give you a systematic way to diagnose whether you are limited by computation, memory bandwidth, or something else entirely. Without these tools, optimization is guesswork.
:::

The models in this section provide the quantitative tools for diagnosing performance bottlenecks.

### The Roofline Model {#sec-system-foundations-roofline-model-5f7c}

The Roofline Model [@williams2009roofline] answers a deceptively simple question: *how fast can this workload possibly run on this hardware?* The answer depends on whether you run out of compute or memory bandwidth first.

Every operation has an **arithmetic intensity**: the ratio of computations performed to bytes moved from memory. Matrix multiplication has high arithmetic intensity because you can reuse each loaded element many times. Element-wise operations like ReLU have low intensity because you load a number, do one operation, and write it back. Look at @fig-roofline to see this tradeoff in action: each workload is bounded by either memory bandwidth or compute throughput, and its arithmetic intensity determines which ceiling it hits first.

::: {#fig-roofline fig-env="figure" fig-pos="htb" fig-cap="**The Roofline Model**: Performance ceiling for a hypothetical accelerator. The sloped line represents memory bandwidth limits; the horizontal line represents peak compute. Every workload can be plotted on this diagram to determine its optimization strategy." fig-alt="A plot with arithmetic intensity on the x-axis and performance on the y-axis. Two lines form a roofline shape: a diagonal line rising from the origin labeled Memory Bound, and a horizontal line labeled Compute Bound. They meet at the Ridge Point."}
```{.tikz}
\begin{tikzpicture}[font=\small\usefont{T1}{phv}{m}{n}, scale=1.0]
  \tikzset{
    Axis/.style={line width=1.0pt, draw=GrayLine, ->, >=stealth},
    Guide/.style={dashed, draw=GrayLine!60, line width=0.6pt},
    Label/.style={text=TextBlack, align=center, font=\footnotesize\usefont{T1}{phv}{m}{n}},
    Dot/.style={circle, fill=#1, draw=white, line width=0.5pt, minimum size=5pt, inner sep=0pt}
  }
  \draw[step=0.5, gray!10, very thin] (0,0) grid (6,4);
  \draw[Axis] (0,0) -- (6,0) node[right, text=TextBlack] {Arithmetic Intensity (Ops/Byte)};
  \draw[Axis] (0,0) -- (0,4.2) node[above, text=TextBlack] {Performance (FLOP/s)};
  \draw[BlueLine, line width=2pt] (0,0) -- (3,3);
  \draw[RedLine, line width=2pt] (3,3) -- (5.8,3);
  \node[Label, text=BlueLine, rotate=45, anchor=south, yshift=2pt] at (1.5, 1.5) {\textbf{Memory Bound}};
  \node[Label, text=RedLine, anchor=south, yshift=2pt] at (4.4, 3) {\textbf{Compute Bound}};
  \node[Dot=TextBlack] at (3,3) {};
  \draw[Guide] (3,0) -- (3,3);
  \node[below, font=\scriptsize\usefont{T1}{phv}{m}{n}, text=TextBlack] at (3,0) {Ridge Point};
\end{tikzpicture}
```
:::

The **ridge point** determines the hardware's balance. If your workload's intensity is below this point, you are **memory-bound** (sloped region). If it is above, you are **compute-bound** (flat region).

$$ \text{Arithmetic Intensity} = \frac{\text{FLOPs}}{\text{Bytes Accessed}} $$

$$ \text{Ridge Point} = \frac{\text{Peak FLOP/s}}{\text{Memory Bandwidth}} $$

::: {.callout-tip title="Batch Size Controls Arithmetic Intensity"}
For matrix multiplications, arithmetic intensity scales with the batch dimension. When you compute $Y = XW$ where $X$ is $(B \times D_{in})$ and $W$ is $(D_{in} \times D_{out})$:

- **FLOPs**: $2 \times B \times D_{in} \times D_{out}$ (multiply-adds)
- **Bytes**: Weights are loaded once: $D_{in} \times D_{out} \times \text{bytes}_{\text{precision}}$

Doubling the batch size $B$ doubles FLOPs while keeping weight loads constant—directly increasing arithmetic intensity. This is why inference serving batches requests: batch size 1 is almost always memory-bound, while batch size 64+ can approach the compute ceiling.
:::

```{python}
#| label: appendix-machine-setup
#| echo: false

from physx.constants import *
from physx.formatting import fmt

# =============================================================================
# PURPOSE
# =============================================================================
# Purpose: Compute roofline and hardware cheat sheet values
# Used in: Appendix machine foundations

# =============================================================================
# INPUT
# =============================================================================
n_gemm_value = 4096
relu_intensity_value = 0.25

# =============================================================================
# PROCESS
# =============================================================================
a100_fp16_raw_value = A100_FLOPS_FP16_TENSOR.to(TFLOPs / second).magnitude
a100_bw_raw_value = A100_MEM_BW.to(TB / second).magnitude
ridge_point_value = int(a100_fp16_raw_value / a100_bw_raw_value)

gemm_intensity_value = int(n_gemm_value / 3)
relu_achieved_tflops_value = relu_intensity_value * a100_bw_raw_value
relu_utilization_value = relu_achieved_tflops_value / a100_fp16_raw_value * 100

dram_pj_value = int(ENERGY_DRAM_ACCESS_PJ.magnitude)
flop_pj_value = ENERGY_FLOP_FP16_PJ.magnitude
energy_ratio_value = int(dram_pj_value / flop_pj_value)

l1_ns_value = int(LATENCY_L1_REGISTER.to(NS).magnitude)
l2_ns_value = int(LATENCY_L2_CACHE.to(NS).magnitude)
hbm_ns_value = int(LATENCY_HBM3.to(NS).magnitude)
nvlink_ns_value = int(LATENCY_NVLINK.to(NS).magnitude)
pcie_ns_value = int(LATENCY_PCIE_GEN5.to(NS).magnitude)
ib_ns_value = int(LATENCY_INFINIBAND.to(NS).magnitude)
ssd_ns_value = int(LATENCY_NVME_SSD.to(NS).magnitude)

h100_flops_value = int(H100_FLOPS_FP16_TENSOR.to(TFLOPs / second).magnitude)
h100_bw_value = H100_MEM_BW.to(TB / second).magnitude
h100_cap_value = int(H100_MEM_CAPACITY.to(GiB).magnitude)
h100_nvlink_value = int(NVLINK_H100_BW.to(GB/second).magnitude)
h100_l2_mb_value = 50

tpuv5_flops_value = int(TPUV5P_FLOPS_BF16.to(TFLOPs / second).magnitude)
tpuv5_bw_value = TPUV5P_MEM_BW.to(TB / second).magnitude
tpuv5_cap_value = int(TPUV5P_MEM_CAPACITY.to(GiB).magnitude)
tpuv5_ici_value = int(TPUV5P_ICI_BW.to(GB/second).magnitude)
tpuv5_l2_mb_value = 100

# =============================================================================
# OUTPUT
# =============================================================================
a100_fp16 = fmt(A100_FLOPS_FP16_TENSOR, "TFLOP/s", precision=0)
a100_bw_tb = fmt(A100_MEM_BW, "TB/s", precision=1)

n_gemm = n_gemm_value
ridge_point = ridge_point_value
gemm_intensity = gemm_intensity_value
relu_intensity = relu_intensity_value
relu_util_str = fmt(relu_utilization_value, precision=2, commas=False)

dram_pj = dram_pj_value
flop_pj = f"{flop_pj_value:.0f}"
energy_ratio_str = f"{energy_ratio_value}"

l1_ns = l1_ns_value
l2_ns = l2_ns_value
hbm_ns = hbm_ns_value
nvlink_ns = nvlink_ns_value
pcie_ns = pcie_ns_value
ib_ns = ib_ns_value
ssd_ns = ssd_ns_value

h100_flops = h100_flops_value
h100_bw = f"{h100_bw_value:.2f}"
h100_cap = h100_cap_value
h100_nvlink = h100_nvlink_value
h100_l2_mb = h100_l2_mb_value

tpuv5_flops = tpuv5_flops_value
tpuv5_bw = f"{tpuv5_bw_value:.2f}"
tpuv5_cap = tpuv5_cap_value
tpuv5_ici = tpuv5_ici_value
tpuv5_l2_mb = tpuv5_l2_mb_value
```

#### A Concrete Example: The A100 Analysis {#sec-system-foundations-concrete-example-a100-analysis-0bb9}

Consider an NVIDIA A100 GPU with FP16 Tensor Core performance of `{python} a100_fp16` TFLOP/s and HBM2e bandwidth of `{python} a100_bw_tb` TB/s. The ridge point is `{python} a100_fp16` / `{python} a100_bw_tb` = `{python} ridge_point` FLOP/byte (the Tera prefixes cancel, yielding FLOP/byte).

Now compare two common operations:

**GEMM (Matrix Multiplication)**: For two `{python} n_gemm`×`{python} n_gemm` matrices, arithmetic intensity is approximately `{python} gemm_intensity` FLOP/byte. Since `{python} gemm_intensity` > `{python} ridge_point`, this operation is compute-bound. You are using the hardware efficiently.

**ReLU (Element-wise)**: For a `{python} n_gemm`×`{python} n_gemm` tensor, intensity is approximately `{python} relu_intensity` op/byte. Since `{python} relu_intensity` ≪ `{python} ridge_point`, this operation is severely memory-bound, achieving only about `{python} relu_util_str`% of peak TFLOP/s. The hardware is mostly waiting for data.

This explains why modern frameworks fuse operations: combining ReLU with the preceding MatMul avoids writing intermediate results to memory, effectively increasing arithmetic intensity.

### Dimensional Analysis {#sec-system-foundations-dimensional-analysis}

The Roofline Model helps diagnose *where* a bottleneck lies. But before applying any performance equation, we should verify that it is physically meaningful. Dimensional analysis provides this sanity check: any valid equation must be **dimensionally homogeneous**—every term must resolve to the same units. If they do not, the equation contains an error.

Consider the **Iron Law of ML Systems** introduced in @sec-silicon-contract:

$$ T = \frac{D_{vol}}{BW} + \frac{O}{R_{peak} \cdot \eta} + L_{lat} $$

We verify correctness by confirming that every term resolves to **Time (seconds)**:

$$ T [s] = \underbrace{ \frac{D_{vol} [\text{Bytes}]}{BW [\text{Bytes/s}]} }_{\text{Seconds}} + \underbrace{ \frac{O [\text{FLOPs}]}{R_{peak} [\text{FLOPs/s}] \cdot \eta [1]} }_{\text{Seconds}} + \underbrace{ L_{lat} [s] }_{\text{Seconds}} $$

*   **Data Term**: $\frac{\text{Bytes}}{\text{Bytes/s}} = \text{Bytes} \times \frac{\text{s}}{\text{Bytes}} = \mathbf{s}$
*   **Compute Term**: $\frac{\text{FLOPs}}{\text{FLOPs/s}} = \text{FLOPs} \times \frac{\text{s}}{\text{FLOPs}} = \mathbf{s}$
*   **Overhead Term**: Already in seconds.

The equation is physically consistent. Apply this technique to any systems equation you encounter: if the dimensions do not match, the formula is wrong. Note also that you cannot directly trade "FLOPs" for "Bandwidth"—they have different units. Any such trade-off must convert through Time, which is precisely what the **Iron Law** quantifies.

With tools for diagnosing single-device bottlenecks in hand, we now turn to the fundamental limits of scaling across multiple devices.

### Amdahl's Law and Gustafson's Law {#sec-system-foundations-amdahls-law-gustafsons-law-2f5c}

Parallelization is the primary tool for scaling ML, but its limits depend on *how* you scale. These two laws frame the fundamental tension in parallel computing. **Amdahl's Law** is the pessimist's view, governing how much faster a *fixed* task can run (optimizing latency). **Gustafson's Law** is the optimist's view, governing how much *more* work we can do in the same time (optimizing throughput).

#### Strong Scaling (Amdahl's Law)

**Strong scaling** answers the question: *If I add more processors to a fixed-size problem, how much faster will it run?*

**Amdahl's Law** [@amdahl1967validity] states that the speedup is limited by the serial portion of the task.[^fn-amdahl] If a fraction $s$ of your task is serial (cannot be parallelized) and $p = 1-s$ is parallelizable, the maximum speedup with $n$ processors is:

[^fn-amdahl]: **Gene Amdahl** (1922–2015) was a legendary computer architect at IBM, where he was the chief architect of the System/360. He later founded Amdahl Corporation to compete with IBM in the mainframe market.

$$ \text{Speedup}(n) = \frac{1}{s + \frac{1-s}{n}} $$

As $n \to \infty$, the term $\frac{1-s}{n} \to 0$, and the speedup converges to $1/s$.

```{python}
#| label: amdahl-setup
#| echo: false

# =============================================================================
# PURPOSE
# =============================================================================
# Purpose: Define constants for Amdahl's Law prose references.
# Used in: Amdahl's Law example lead-in text.

# =============================================================================
# INPUT
# =============================================================================
s_value = 0.05   # Serial fraction
n_8_value = 8    # Number of processors for example

# =============================================================================
# PROCESS
# =============================================================================
p_value = 1 - s_value  # Parallel fraction

# =============================================================================
# OUTPUT
# =============================================================================
s_pct_str = "5"
p_pct_str = "95"
n_8_str = "8"
s_str = "0.05"
p_str = "0.95"
```

**Example**: If `{python} s_pct_str`% of your training step is serial overhead (e.g., Python GIL, kernel launch latency) and `{python} p_pct_str`% is parallelizable matrix math:

```{python}
#| label: amdahl-example
#| echo: false
from physx.formatting import fmt

# =============================================================================
# PURPOSE
# =============================================================================
# Purpose: Compute Amdahl's Law speedup examples
# Used in: Amdahl's Law example

# =============================================================================
# INPUT
# =============================================================================
s_value = 0.05
n_8_value = 8

# =============================================================================
# PROCESS
# =============================================================================
p_value = 1 - s_value
amdahl_8_value = 1 / (s_value + p_value / n_8_value)
amdahl_inf_value = 1 / s_value

# =============================================================================
# OUTPUT
# =============================================================================
amdahl_8_str = fmt(amdahl_8_value, precision=1, commas=False)
amdahl_inf_str = fmt(amdahl_inf_value, precision=0, commas=False)
```

*   With $n=1$, speedup is 1.
*   With n=`{python} n_8_str`, speedup is 1/(`{python} s_str` + `{python} p_str`/`{python} n_8_str`) ≈ `{python} amdahl_8_str`×.
*   With n=infinity, speedup is capped at 1/`{python} s_str` = `{python} amdahl_inf_str`×.

No matter how many accelerators you buy, you cannot make this fixed workload run faster than `{python} amdahl_inf_str`×.

#### Weak Scaling (Gustafson's Law)

**Weak scaling** answers the question: *If I add more processors, how much larger of a problem can I solve in the same amount of time?*

This is the reality of Large Language Models. We don't use 1,000 GPUs to train GPT-4 on a laptop-sized dataset in milliseconds; we use them to train on a dataset 1,000x larger in reasonable time.

**Gustafson's Law** [@gustafson1988reevaluating] models this "scaled speedup":[^fn-gustafson]

[^fn-gustafson]: **John Gustafson** is a computer scientist known for his work in parallel computing and for introducing the Unum (universal number) format. His law was a direct response to the perceived "limits" of **Amdahl's Law** when applied to massive scale.

$$ \text{Scaled Speedup}(n) = n - s(n - 1) $$

Here, the parallel part of the workload grows linearly with $n$, while the serial part $s$ remains fixed.

```{python}
#| label: gustafson-setup
#| echo: false

# =============================================================================
# PURPOSE
# =============================================================================
# Purpose: Define constants for Gustafson's Law prose references.
# Used in: Gustafson's Law example lead-in text.

# =============================================================================
# INPUT
# =============================================================================
s_g_value = 0.05  # Serial fraction (same as Amdahl example)

# =============================================================================
# OUTPUT
# =============================================================================
s_g_pct_str = "5"
s_g_str = "0.05"
```

**Example**: Using the same `{python} s_g_pct_str`% serial overhead ($s$ = `{python} s_g_str`):

```{python}
#| label: gustafson-example
#| echo: false
from physx.formatting import fmt

# =============================================================================
# PURPOSE
# =============================================================================
# Purpose: Compute Gustafson's Law speedup examples
# Used in: Gustafson's Law example

# =============================================================================
# INPUT
# =============================================================================
s_g_value = 0.05
n_8_g_value = 8
n_1000_value = 1000

# =============================================================================
# PROCESS
# =============================================================================
gustafson_8_value = n_8_g_value - s_g_value * (n_8_g_value - 1)
gustafson_8_serial_value = s_g_value * (n_8_g_value - 1)
gustafson_1000_value = n_1000_value - s_g_value * (n_1000_value - 1)

# =============================================================================
# OUTPUT
# =============================================================================
gustafson_8_str = fmt(gustafson_8_value, precision=2, commas=False)
gustafson_8_serial = f"{gustafson_8_serial_value:.2f}"
gustafson_1000_str = fmt(gustafson_1000_value, precision=0, commas=False)
n_8_g_str = str(n_8_g_value)
n_8_g_minus_1_str = str(n_8_g_value - 1)
n_1000_str = str(n_1000_value)
n_1000_minus_1_str = str(n_1000_value - 1)
```

*   With $n=1$, speedup is 1.
*   With n=`{python} n_8_g_str`, Scaled Speedup is `{python} n_8_g_str` - `{python} s_g_str` × (`{python} n_8_g_minus_1_str`) = `{python} n_8_g_str` - `{python} gustafson_8_serial` = `{python} gustafson_8_str`×.
*   With n=`{python} n_1000_str`, Scaled Speedup is `{python} n_1000_str` - `{python} s_g_str` × (`{python} n_1000_minus_1_str`) ≈ `{python} gustafson_1000_str`×.

In weak scaling, efficiency remains high because the useful work (training the model) scales up to dwarf the fixed overheads.

```{python}
#| label: training-time-example
#| echo: false
from physx.formatting import fmt, md_math, sci_latex

# =============================================================================
# PURPOSE
# =============================================================================
# Purpose: Estimate training time from model scale and utilization
# Used in: Training time equation example

# =============================================================================
# INPUT
# =============================================================================
p_params_value = 1e9           # 1B parameters
d_tokens_value = 20e9          # 20B tokens
n_gpus_value = 1
x_flops_value = 312e12         # A100 FP16 peak FLOP/s
u_mfu_value = 0.40             # 40% MFU

# =============================================================================
# PROCESS
# =============================================================================
total_flops_value = 6 * p_params_value * d_tokens_value
throughput_value = n_gpus_value * x_flops_value * u_mfu_value
t_seconds_value = total_flops_value / throughput_value
t_minutes_value = t_seconds_value / 60

# =============================================================================
# OUTPUT
# =============================================================================
t_days_value = t_seconds_value / 86400
T_seconds_str = fmt(t_seconds_value, precision=0, commas=False)
T_minutes_str = fmt(t_minutes_value, precision=0, commas=False)
T_days_str = fmt(t_days_value, precision=0, commas=False)
p_params_str = f"{p_params_value/1e9:.0f}B"
d_tokens_str = f"{d_tokens_value/1e9:.0f}B"
n_gpus_str = str(int(n_gpus_value))
u_mfu_pct_str = str(int(u_mfu_value * 100))

eq_total_flops = md_math(f"\\text{{Total FLOPs}} = 6 \\times {sci_latex(p_params_value, 0)} \\times {sci_latex(d_tokens_value, 0)} = {sci_latex(total_flops_value, 1)} \\text{{ FLOPs}}")
eq_throughput = md_math(f"\\text{{Throughput}} = {int(n_gpus_value)} \\times ({sci_latex(x_flops_value, 0)}) \\times {u_mfu_value:.2f} \\approx {sci_latex(throughput_value, 2)} \\text{{ FLOP/s}}")
eq_time = md_math(f"T = \\frac{{{sci_latex(total_flops_value, 1)}}}{{{sci_latex(throughput_value, 2)}}} \\approx {fmt(t_seconds_value, precision=0, commas=True)} \\text{{ seconds}} \\approx {fmt(t_minutes_value, precision=0, commas=True)} \\text{{ minutes}}")
```

::: {.callout-notebook title="The Training Time Equation"}

Just as classical architecture has an "Iron Law" of performance, Large Language Model training has a fundamental governing equation. To estimate training time $T$:

$$ T \approx \frac{6 \cdot P \cdot D}{N \cdot X \cdot U} $$

Where:

*   **$6$**: The factor deriving from the forward pass ($2PD$) and backward pass ($4PD$) FLOPs per token.
*   **$P$**: Number of model parameters.
*   **$D$**: Number of training tokens.
*   **$N$**: Number of accelerators (GPUs).
*   **$X$**: Peak FLOP/s of one accelerator.
*   **$U$**: Model FLOPs Utilization (MFU), typically 30%–50%.

**Example**: Training a **`{python} p_params_str` parameter** model on **`{python} d_tokens_str` tokens** using **`{python} n_gpus_str` A100** (`{python} a100_fp16` TFLOPS) at **`{python} u_mfu_pct_str`% utilization**.
`{python} eq_total_flops`
`{python} eq_throughput`
`{python} eq_time`

The computed result: **`{python} T_seconds_str` seconds** (≈ `{python} T_minutes_str` minutes, or about `{python} T_days_str` days).

:::

### Little's Law {#sec-system-foundations-littles-law-9c4c}

For capacity planning in inference systems, **Little's Law** [@little1961proof] relates concurrency ($L$), arrival rate ($\lambda$), and latency ($W$):[^fn-little]

[^fn-little]: **John Little** is an Institute Professor at MIT and a pioneer in the field of operations research. His law, proved in 1961, is fundamental to queuing theory and is used across fields from manufacturing to computer network analysis.

$$ L = \lambda \times W $$

```{python}
#| label: littles-law-example
#| echo: false
from physx.formatting import fmt, md_math

# =============================================================================
# PURPOSE
# =============================================================================
# Purpose: Apply Little's Law for concurrency sizing
# Used in: Little's Law example

# =============================================================================
# INPUT
# =============================================================================
lambda_qps_value = 1000        # queries per second
w_latency_s_value = 0.050      # 50 ms in seconds
mem_per_req_gb_value = 1
gpu_mem_gb_value = 24

# =============================================================================
# PROCESS
# =============================================================================
l_concurrent_value = lambda_qps_value * w_latency_s_value
max_concurrent_value = int(gpu_mem_gb_value / mem_per_req_gb_value)
max_throughput_value = max_concurrent_value / w_latency_s_value

# =============================================================================
# OUTPUT
# =============================================================================
L_concurrent_str = fmt(l_concurrent_value, precision=0, commas=False)
lambda_qps_str = fmt(lambda_qps_value, precision=0)
lambda_qps_raw_str = fmt(lambda_qps_value, precision=0, commas=False)
w_latency_ms_str = str(int(w_latency_s_value * 1000))
w_latency_s_str = f"{w_latency_s_value:.2f}"

mem_per_req_gb_str = str(mem_per_req_gb_value)
gpu_mem_gb_str = str(gpu_mem_gb_value)
max_concurrent_str = str(max_concurrent_value)

eq_max_throughput = md_math(f"L/W = {max_concurrent_value} / {w_latency_s_value} = {int(max_throughput_value)} \\text{{ QPS}}")
```

**Example**: To sustain `{python} lambda_qps_str` queries per second (QPS) with `{python} w_latency_ms_str`ms average latency, your system must support `{python} lambda_qps_raw_str` × `{python} w_latency_s_str` = `{python} L_concurrent_str` concurrent requests.

**Implication for Memory**: This tells you exactly how to size your inference worker pools. If serving one request requires `{python} mem_per_req_gb_str` GB of temporary memory (KV cache, activations), handling `{python} L_concurrent_str` concurrent requests requires `{python} L_concurrent_str` GB of memory. If your accelerator only has `{python} gpu_mem_gb_str` GB, you are physically limited to `{python} max_concurrent_str` concurrent requests. Your maximum throughput is capped at `{python} eq_max_throughput`, regardless of how many requests arrive.

## Computer Architecture Essentials {#sec-system-foundations-computer-architecture-essentials-bb18}

While physics sets theoretical performance bounds, computer architecture defines the machinery to approach them. The gap between peak performance (Roofline) and actual throughput lies in how effectively we utilize the memory hierarchy. This section covers the latency and bandwidth trade-offs that shape system design.

### Latencies Every Programmer Should Know

The first step in systems intuition is understanding the cost of distance. @tbl-latency-hierarchy quantifies how long the processor waits for data from different levels of the memory hierarchy. If accessing a register is like picking up a pencil from your desk, fetching from HBM is walking across the office, and fetching from disk is flying to the moon.

| **Component**            |         **Latency (ns)** | **Cycles (Approx)** | **Relative "Distance"** |
|:-----------------------|-----------------------:|------------------:|:---------------------:|
| **Register**             |               ~0.3 ns |            1 cycle |        10 seconds       |
| **L1 Cache**             |     ~`{python} l1_ns` ns |          3-4 cycles |         1 minute        |
| **L2 Cache**             |     ~`{python} l2_ns` ns |           12 cycles |        4 minutes        |
| **HBM3 (GPU Memory)**    |    ~`{python} hbm_ns` ns |        1,000 cycles |         5 hours         |
| **NVLink (GPU-GPU)**     | ~`{python} nvlink_ns` ns |        1,500 cycles |         8 hours         |
| **PCIe (CPU-GPU)**       |   ~`{python} pcie_ns` ns |        3,000 cycles |          1 day          |
| **InfiniBand (Network)** |     ~`{python} ib_ns` ns |       15,000 cycles |          1 week         |
| **SSD (NVMe)**           |    ~`{python} ssd_ns` ns |      300,000 cycles |         3 months        |

: **The Latency Hierarchy.** Access times for modern AI hardware. Note the massive jump from SRAM (Cache) to HBM. Any kernel that misses cache pays a heavy penalty. {#tbl-latency-hierarchy}

### The AI Hardware Cheat Sheet (Modern Reference)

While latency tells us how long we wait for the *first* byte, bandwidth tells us how many bytes follow. @tbl-hardware-cheatsheet provides the constants for back-of-the-envelope "Roofline" calculations. These represent the "standard units of compute" for the current era of machine learning.

| **Spec**             |                **NVIDIA H100 (SXM)** |              **Google TPU v5p** | **System Impact**                       |
|:-------------------|-----------------------------------:|------------------------------:|:--------------------------------------|
| **FP16/BF16 Peak**   |         `{python} h100_flops` TFLOPS |   `{python} tpuv5_flops` TFLOPS | The "Speed Limit" ($R_{peak}$)          |
| **Memory Bandwidth** |              `{python} h100_bw` TB/s |        `{python} tpuv5_bw` TB/s | The "Width of the Pipe" ($BW$)          |
| **HBM Capacity**     |               `{python} h100_cap` GB |         `{python} tpuv5_cap` GB | Max Model Size ($P$) / Batch Size ($B$) |
| **L2/SRAM Cache**    |                                `{python} h100_l2_mb` MB |                         ~`{python} tpuv5_l2_mb` MB | Critical for Operator Fusion            |
| **Interconnect**     | `{python} h100_nvlink` GB/s (NVLink) | `{python} tpuv5_ici` GB/s (ICI) | Determines Model Parallelism Scaling    |

: **Reference Specs.** Key constants for quantitative analysis. Always check specific datasheets, but these serve as standard units of compute. {#tbl-hardware-cheatsheet}

### The Memory Hierarchy {#sec-system-foundations-memory-hierarchy-674b}

Computer systems use a hierarchy because no single technology provides both high capacity and low latency. Examine the pyramid in @fig-memory-hierarchy to see how each level balances this tradeoff: every technique that keeps data higher in the pyramid (registers/cache) directly improves performance.

::: {#fig-memory-hierarchy fig-env="figure" fig-pos="htb" fig-cap="**The Memory Hierarchy**: Performance depends on data proximity. Accessing HBM is ~100x slower than registers; accessing SSD is ~100,000x slower." fig-alt="Pyramid showing Registers at top, followed by Cache, HBM/DRAM, and Storage at bottom."}
```{.tikz}
\begin{tikzpicture}[line cap=round, line join=round, font=\usefont{T1}{phv}{m}{n}\small, scale=1.0]
  \tikzset{
    LabelAxis/.style={text=TextBlack!60, font=\footnotesize\usefont{T1}{phv}{m}{n}, align=center}
  }
  \filldraw[fill=RedFill, draw=RedLine, line width=1pt] (0, 5.2) -- (-1.2, 4.2) -- (1.2, 4.2) -- cycle;
  \node[text=RedLine] at (0, 4.55) {\textbf{Registers}};
  \filldraw[fill=YellowFill, draw=YellowLine, line width=1pt] (-1.2, 4.2) -- (-2.4, 2.8) -- (2.4, 2.8) -- (1.2, 4.2) -- cycle;
  \node[text=YellowLine!80!black] at (0, 3.5) {\textbf{L1 / L2 / L3 Cache}};
  \filldraw[fill=BlueFill, draw=BlueLine, line width=1pt] (-2.4, 2.8) -- (-3.6, 1.4) -- (3.6, 1.4) -- (2.4, 2.8) -- cycle;
  \node[text=BlueLine] at (0, 2.1) {\textbf{HBM / DRAM}};
  \filldraw[fill=GreenFill, draw=GreenLine, line width=1pt] (-3.6, 1.4) -- (-4.8, 0) -- (4.8, 0) -- (3.6, 1.4) -- cycle;
  \node[text=GreenLine] at (0, 0.7) {\textbf{Storage (SSD / Disk)}};
  \draw[->, ultra thick, gray!40] (5.5, 0) -- (5.5, 5.2) node[midway, right, LabelAxis, text=gray] {Faster Speed\\ Lower Latency};
  \draw[->, ultra thick, gray!40] (-5.5, 5.2) -- (-5.5, 0) node[midway, left, LabelAxis, text=gray] {Larger Capacity\\ Lower Cost};
\end{tikzpicture}
```
:::

The hierarchy's energy costs reveal why data movement dominates modern system design.

::: {.callout-notebook title="The High Cost of Data Movement"}
Fetching a 32-bit value from DRAM costs roughly **`{python} energy_ratio_str`× more energy** than performing a floating-point operation on it (e.g., ~`{python} dram_pj` pJ vs ~`{python} flop_pj` pJ). This "Energy Wall" means that maximizing **arithmetic intensity** (doing many ops per loaded byte) is the only way to be energy efficient.
:::

### Bandwidth vs. Latency {#sec-system-foundations-bandwidth-vs-latency-e155}

Bandwidth (throughput) and latency (delay) are distinct constraints. Total transfer time follows:

$$ T = \text{Latency} + \frac{\text{Data Size}}{\text{Bandwidth}} $$

For small transfers (e.g., single-token inference), latency dominates. For large transfers (e.g., loading weights), bandwidth dominates.

```{python}
#| label: bandwidth-latency-setup
#| echo: false

# =============================================================================
# PURPOSE
# =============================================================================
# Purpose: Define constants for bandwidth-latency prose references.
# Used in: Bandwidth vs Latency example lead-in text.

# =============================================================================
# INPUT
# =============================================================================
bw_gbps_value = 10    # Link bandwidth in Gbps
ping_ms_value = 10    # Network latency in ms
data_kb_value = 1     # Packet size in KB

# =============================================================================
# OUTPUT
# =============================================================================
bw_gbps_str = "10"
ping_ms_str = "10"
data_kb_str = "1"
```

**Example**: Sending data over a `{python} bw_gbps_str` Gbps link with `{python} ping_ms_str`ms ping (latency).

```{python}
#| label: bandwidth-latency-example
#| echo: false
from physx.formatting import fmt, md_math

# =============================================================================
# PURPOSE
# =============================================================================
# Purpose: Estimate transmission time for small packets
# Used in: Bandwidth vs. latency example

# =============================================================================
# INPUT
# =============================================================================
data_kb_value = 1e3             # 1 KB in bytes
bw_gbps_value = 10e9            # 10 Gbps in bits/s
large_data_gb_value = 1
ping_ms_value = 10

# =============================================================================
# PROCESS
# =============================================================================
tx_time_s_value = (data_kb_value * 8) / bw_gbps_value
tx_time_us_value = tx_time_s_value * 1e6

large_data_bits_value = large_data_gb_value * 8e9
large_tx_time_s_value = large_data_bits_value / bw_gbps_value
total_large_time_ms_value = ping_ms_value + large_tx_time_s_value * 1000

# =============================================================================
# OUTPUT
# =============================================================================
tx_time_us_str = fmt(tx_time_us_value, precision=1, commas=False)
large_data_gb_str = str(large_data_gb_value)

eq_large_tx = md_math(f"{large_data_gb_value}\\text{{GB}} / {int(bw_gbps_value/1e9)}\\text{{Gbps}} \\approx {int(large_tx_time_s_value*1000)}\\text{{ms}}")
eq_large_total = md_math(f"\\approx {int(ping_ms_value)}\\text{{ms}} + {int(large_tx_time_s_value*1000)}\\text{{ms}} = {int(total_large_time_ms_value)}\\text{{ms}}")
```

*   **Latency-Bound (`{python} data_kb_str`KB Packet)**:
    *   Transmission: `{python} data_kb_str`KB / `{python} bw_gbps_str`Gbps ≈ `{python} tx_time_us_str` μs.
    *   Total Time ≈ `{python} ping_ms_str`ms + `{python} tx_time_us_str` μs ≈ `{python} ping_ms_str`ms.
    *   *Result*: The bandwidth is irrelevant; the speed of light (ping) is the bottleneck.

*   **Bandwidth-Bound (`{python} large_data_gb_str`GB Checkpoint)**:
    *   Transmission: `{python} eq_large_tx`.
    *   Total Time `{python} eq_large_total`.
    *   *Result*: The ping is negligible; the pipe size is the bottleneck.

## Numerical Representations {#sec-system-foundations-numerical-representations-7b2f}

While statistics helps us understand data distributions, **Numerical Representations** determine how we store the values themselves. In ML systems, the choice of precision (FP32 vs. BF16 vs. INT8) is a direct trade-off between statistical fidelity and hardware throughput.

::: {.callout-perspective title="Why This Matters"}
Your production model runs at 50 QPS in FP32 but your target is 200 QPS. Switching to INT8 could get you there, but will accuracy suffer? Understanding numerical formats lets you make this trade-off quantitatively rather than hoping for the best.
:::

### Floating-Point Format Comparison {#sec-system-foundations-floatingpoint-format-comparison-c861}

The IEEE 754 standard and its AI-specific derivatives define different trade-offs between dynamic range (the span of representable values) and precision (how finely you can represent values within that range). @tbl-numerical-formats summarizes the key formats and their use cases, while @fig-float-formats visualizes the bit allocations.

| **Format** | **Bits** | **Exponent** | **Mantissa** |                     **Dynamic Range** | **Typical Use Case**                           |
|:---------|-------:|-----------:|-----------:|------------------------------------:|:---------------------------------------------|
| **FP32**   |       32 |            8 |           23 |          $\sim 10^{-38}$ to $10^{38}$ | Training (full precision), reference inference |
| **FP16**   |       16 |            5 |           10 | $\sim 10^{-5}$ to $6.5 \times 10^{4}$ | Training with loss scaling, inference          |
| **BF16**   |       16 |            8 |            7 |                          Same as FP32 | Training (preferred), avoids loss scaling      |
| **FP8**    |        8 |       4 or 5 |       3 or 2 |                                Varies | Inference on newest hardware (H100+)           |
| **INT8**   |        8 |          N/A |          N/A |                           -128 to 127 | Inference after quantization                   |

: **Numerical Format Comparison**: Each format trades off precision, dynamic range, memory footprint, and compute throughput. BF16 has emerged as the preferred training format because it matches FP32's range while using half the memory. {#tbl-numerical-formats}

::: {#fig-float-formats fig-env="figure" fig-pos="htb" fig-cap="**Numerical Format Bit Layouts**: A visual comparison of bit allocations. Note how **BF16** (Brain Float 16) preserves the 8-bit exponent of **FP32**, ensuring the same dynamic range for training stability. **FP16** trades range for precision, often requiring loss scaling to prevent underflow." fig-alt="Stacked horizontal bars showing bit breakdown. FP32: 1 Sign, 8 Exp, 23 Mantissa. BF16: 1 Sign, 8 Exp, 7 Mantissa. FP16: 1 Sign, 5 Exp, 10 Mantissa. INT8: 8 Integer bits."}
```{.tikz}
\begin{tikzpicture}[font=\small\usefont{T1}{phv}{m}{n}, scale=1.0]
  \tikzset{
    BitBox/.style={draw=white, line width=0.8pt, minimum height=0.6cm, align=center, font=\scriptsize\bfseries\usefont{T1}{phv}{m}{n}, text=white},
    Label/.style={text=TextBlack, font=\small\bfseries\usefont{T1}{phv}{m}{n}, anchor=east}
  }

  % Colors
  \definecolor{SignColor}{HTML}{D9534F}      % Red
  \definecolor{ExpColor}{HTML}{5BC0DE}       % Blue
  \definecolor{MantColor}{HTML}{F0AD4E}      % Orange
  \definecolor{IntColor}{HTML}{5CB85C}       % Green

  % FP32
  \node[Label] at (-0.2, 3) {FP32 (32-bit)};
  \node[BitBox, fill=SignColor, minimum width=0.3cm] (fp32_s) at (0.15, 3) {S};
  \node[BitBox, fill=ExpColor, minimum width=2.4cm, right=0pt of fp32_s] (fp32_e) {Exponent (8)};
  \node[BitBox, fill=MantColor, minimum width=6.9cm, right=0pt of fp32_e] (fp32_m) {Mantissa (23)};

  % BF16
  \node[Label] at (-0.2, 2) {BF16 (16-bit)};
  \node[BitBox, fill=SignColor, minimum width=0.3cm] (bf16_s) at (0.15, 2) {S};
  \node[BitBox, fill=ExpColor, minimum width=2.4cm, right=0pt of bf16_s] (bf16_e) {Exponent (8)};
  \node[BitBox, fill=MantColor, minimum width=2.1cm, right=0pt of bf16_e] (bf16_m) {Mant (7)};
  \node[right=0.2cm of bf16_m, font=\scriptsize\usefont{T1}{phv}{m}{n}, text=gray] {Matches FP32 Range};

  % FP16
  \node[Label] at (-0.2, 1) {FP16 (16-bit)};
  \node[BitBox, fill=SignColor, minimum width=0.3cm] (fp16_s) at (0.15, 1) {S};
  \node[BitBox, fill=ExpColor, minimum width=1.5cm, right=0pt of fp16_s] (fp16_e) {Exp (5)};
  \node[BitBox, fill=MantColor, minimum width=3.0cm, right=0pt of fp16_e] (fp16_m) {Mantissa (10)};

  % INT8
  \node[Label] at (-0.2, 0) {INT8 (8-bit)};
  \node[BitBox, fill=IntColor, minimum width=2.4cm] (int8) at (1.2, 0) {Integer (8)};

  % Grid / Scale markers (approximate)
  \draw[gray!30, dashed] (0, -0.5) -- (0, 3.5);
  \draw[gray!30, dashed] (9.6, -0.5) -- (9.6, 3.5);
  \node[below, font=\scriptsize\usefont{T1}{phv}{m}{n}, text=gray] at (0, -0.5) {Bit 31/15/7};
  \node[below, font=\scriptsize\usefont{T1}{phv}{m}{n}, text=gray] at (9.6, -0.5) {Bit 0};

\end{tikzpicture}
```
:::

Beyond bit width, the allocation of bits between exponent and mantissa determines what range of values each format can represent.

::: {.callout-perspective title="The Dynamic Range Wall"}

The choice of numerical format is a direct application of the **Iron Law of ML Systems**. Reducing precision from FP32 to BF16 or FP16 halves the **Data Movement** term in the denominator, potentially doubling throughput on memory-bound workloads. However, the *type* of 16-bit format determines the engineering complexity:

*   **Dynamic Range (The Exponent)**: BF16 preserves the 8-bit exponent of FP32. This means it can represent the same range of extremely large and extremely small values (gradients).
*   **Precision (The Mantissa)**: FP16 has a larger 10-bit mantissa than BF16 (7 bits), offering higher precision for values within its range. But its 5-bit exponent is a major constraint; gradients often "vanish" to zero (underflow) because the exponent cannot represent them. To solve this, FP16 training requires **Loss Scaling**, an operational overhead where gradients are multiplied by a large constant to push them into the representable range.
*   **Energy Efficiency**: INT8 operations are significantly more energy-efficient than floating-point equivalents because they utilize simpler integer ALUs and require less silicon area. Moving to INT8 for inference is the primary lever for deploying LLMs on battery-constrained edge devices.

:::

**Brain Float 16 (BF16)** deserves special attention [@google_bfloat16]. It matches FP32's 8-bit exponent (preserving dynamic range) while truncating the mantissa to 7 bits. This avoids the gradient underflow problems that plague FP16 training, eliminating the need for complex loss scaling. Most modern training uses BF16 for this reason.[^fn-bf16]

[^fn-bf16]: **BF16** was originally introduced with the Google TPUv2 and has since been adopted by Intel, Arm, and NVIDIA (starting with Ampere architectures).

### Integer Quantization {#sec-system-foundations-integer-quantization-9d02}

Quantization maps continuous floating-point values to discrete integers, typically INT8. The key challenge is choosing how to map the floating-point range to integers.

**Symmetric quantization** centers the mapping at zero:
$$ x_{int} = \text{round}\left(\frac{x}{\alpha} \times 127\right) $$

where $\alpha$ is the scale factor (typically the maximum absolute value). This works well for weight distributions centered around zero.

**Asymmetric quantization** handles distributions that are not centered (common after ReLU, which produces only non-negative values) by shifting the range before scaling. If $x_{min}$ is the minimum of the range and $\alpha$ is the range width ($x_{max} - x_{min}$):
$$ x_{int} = \text{round}\left(\frac{x - x_{min}}{\alpha} \times 255\right) $$

The choice between symmetric and asymmetric quantization depends on your tensor's distribution and has measurable accuracy implications.

## Fallacies and Pitfalls {#sec-machine-foundations-fallacies-pitfalls}

::: {.callout-warning title="Fallacy: Doubling GPUs halves training time."}
**The Reality**: This assumes perfect **strong scaling** (**Amdahl's Law**). In practice, communication overhead (all-reduce) grows with $N$, and batch size constraints may limit parallelism. At large scale, you often hit diminishing returns unless you also scale the problem size (Weak Scaling).
:::

A related misconception concerns numerical precision.

::: {.callout-warning title="Fallacy: Higher precision (FP32) is always better."}
**The Reality**: For deep learning, FP32 often hurts performance without improving convergence. It consumes 2x memory bandwidth and energy compared to BF16. Since neural networks are resilient to noise, the extra mantissa bits in FP32 are often modeling random variance rather than signal.
:::
