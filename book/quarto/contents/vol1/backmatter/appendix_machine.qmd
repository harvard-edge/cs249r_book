---
engine: jupyter
---

# Machine Foundations {#sec-machine-foundations}

This appendix covers the hardware and physical constraints that govern ML system performance. From performance modeling to memory hierarchies to numerical precision, these are the "speed limits" that every ML engineer must understand. The concepts here underpin the hardware acceleration strategies in @sec-ai-acceleration, the training optimizations in @sec-ai-training, and the serving architectures in @sec-model-serving-systems.

::: {.callout-note title="A Note on Terminology: GPUs and Accelerators"}
Throughout this book, we often use "GPU" when discussing hardware acceleration. However, the principles—roofline analysis, memory hierarchies, numerical precision, and performance modeling—apply equally to **TPUs**, **NPUs**, **custom ASICs**, and other specialized AI accelerators. We use "GPU" as convenient shorthand given its prevalence in the ML ecosystem, but readers should understand these concepts as **accelerator-general** unless we explicitly discuss vendor-specific features (e.g., CUDA, NVLink).
:::

## The Physics of Computing {#sec-system-foundations-physics-computing-b6a4}

::: {.callout-perspective title="Why This Matters"}
You have trained a model that achieves good accuracy, but inference takes 200ms when your SLA requires 50ms. Where do you start? Performance analysis models give you a systematic way to diagnose whether you are limited by computation, memory bandwidth, or something else entirely. Without these tools, optimization is guesswork.
:::

The models in this section form the foundation of quantitative systems thinking. They define the "speed limits" set by physics and hardware design.

### The Roofline Model {#sec-system-foundations-roofline-model-5f7c}

The Roofline Model [@williams2009roofline] answers a deceptively simple question: *how fast can this workload possibly run on this hardware?* The answer depends on whether you run out of compute or memory bandwidth first.

Every operation has an **arithmetic intensity**: the ratio of computations performed to bytes moved from memory. Matrix multiplication has high arithmetic intensity because you can reuse each loaded element many times. Element-wise operations like ReLU have low intensity because you load a number, do one operation, and write it back. @fig-roofline illustrates how workloads are bounded by either memory bandwidth or compute throughput.

::: {#fig-roofline fig-env="figure" fig-pos="htb" fig-cap="**The Roofline Model**: Performance ceiling for a hypothetical accelerator. The sloped line represents memory bandwidth limits; the horizontal line represents peak compute. Every workload can be plotted on this diagram to determine its optimization strategy." fig-alt="A plot with arithmetic intensity on the x-axis and performance on the y-axis. Two lines form a roofline shape: a diagonal line rising from the origin labeled Memory Bound, and a horizontal line labeled Compute Bound. They meet at the Ridge Point."}
```{.tikz}
\begin{tikzpicture}[font=\small\usefont{T1}{phv}{m}{n}, scale=1.0]
  \tikzset{
    Axis/.style={line width=1.0pt, draw=GrayLine, ->, >=stealth},
    Guide/.style={dashed, draw=GrayLine!60, line width=0.6pt},
    Label/.style={text=TextBlack, align=center, font=\footnotesize\usefont{T1}{phv}{m}{n}},
    Dot/.style={circle, fill=#1, draw=white, line width=0.5pt, minimum size=5pt, inner sep=0pt}
  }
  \draw[step=0.5, gray!10, very thin] (0,0) grid (6,4);
  \draw[Axis] (0,0) -- (6,0) node[right, text=TextBlack] {Arithmetic Intensity (Ops/Byte)};
  \draw[Axis] (0,0) -- (0,4.2) node[above, text=TextBlack] {Performance (FLOP/s)};
  \draw[BlueLine, line width=2pt] (0,0) -- (3,3);
  \draw[RedLine, line width=2pt] (3,3) -- (5.8,3);
  \node[Label, text=BlueLine, rotate=45, anchor=south, yshift=2pt] at (1.5, 1.5) {\textbf{Memory Bound}};
  \node[Label, text=RedLine, anchor=south, yshift=2pt] at (4.4, 3) {\textbf{Compute Bound}};
  \node[Dot=TextBlack] at (3,3) {};
  \draw[Guide] (3,0) -- (3,3);
  \node[below, font=\scriptsize\usefont{T1}{phv}{m}{n}, text=TextBlack] at (3,0) {Ridge Point};
\end{tikzpicture}
```
:::

The **ridge point** determines the hardware's balance. If your workload's intensity is below this point, you are **memory-bound** (sloped region). If it is above, you are **compute-bound** (flat region).

$$ \text{Arithmetic Intensity} = \frac{\text{FLOPs}}{\text{Bytes Accessed}} $$

$$ \text{Ridge Point} = \frac{\text{Peak FLOP/s}}{\text{Memory Bandwidth}} $$

```{python}
#| label: appendix-machine-setup
#| echo: false

from calc.constants import *
from calc.formulas import fmt

# A100 specs for roofline example
a100_fp16 = fmt(A100_FLOPS_FP16_TENSOR, "TFLOP/s", precision=0)       # "312"
a100_bw_tb = fmt(A100_MEM_BW, "TB/s", precision=1)                     # "2.0"

# Ridge point and arithmetic intensity calculations
a100_fp16_raw = A100_FLOPS_FP16_TENSOR.to(TFLOPs / second).magnitude  # 312
a100_bw_raw = A100_MEM_BW.to(TB / second).magnitude                    # 2.0
ridge_point = int(a100_fp16_raw / a100_bw_raw)                         # 156 FLOP/byte

# GEMM: For n×n matrices (FP16), Intensity = 2n³ FLOPs / (3n² × 2 bytes) = n/3
# ReLU: For n×n tensor (FP16), Intensity = n² ops / (2n² × 2 bytes) = 0.25
n_gemm = 4096
gemm_intensity = int(n_gemm / 3)                                       # 1365 FLOP/byte
relu_intensity = 0.25                                                  # FLOP/byte

# ReLU achieved performance (memory-bound: perf = intensity × bandwidth)
relu_achieved_tflops = relu_intensity * a100_bw_raw                    # 0.5 TFLOP/s
relu_utilization = relu_achieved_tflops / a100_fp16_raw * 100          # 0.16%
relu_util_str = f"{relu_utilization:.2f}"

# Energy constants (Horowitz 2014)
dram_pj = int(ENERGY_DRAM_ACCESS_PJ.magnitude)                         # 640
flop_pj = f"{ENERGY_FLOP_FP16_PJ.magnitude:.0f}"                       # "1" (1.1 rounds)

# Latency Hierarchy (ns) - for pipe tables
l1_ns = int(LATENCY_L1_REGISTER.to(NS).magnitude)
l2_ns = int(LATENCY_L2_CACHE.to(NS).magnitude)
hbm_ns = int(LATENCY_HBM3.to(NS).magnitude)
nvlink_ns = int(LATENCY_NVLINK.to(NS).magnitude)
pcie_ns = int(LATENCY_PCIE_GEN5.to(NS).magnitude)
ib_ns = int(LATENCY_INFINIBAND.to(NS).magnitude)
ssd_ns = int(LATENCY_NVME_SSD.to(NS).magnitude)

# Hardware Cheat Sheet - for pipe tables
h100_flops = int(H100_FLOPS_FP16_TENSOR.to(TFLOPs / second).magnitude)
h100_bw = f"{H100_MEM_BW.to(TB / second).magnitude:.2f}"
h100_cap = int(H100_MEM_CAPACITY.to(GiB).magnitude)
h100_nvlink = int(NVLINK_H100_BW.to(GB/second).magnitude)

tpuv5_flops = int(TPUV5P_FLOPS_BF16.to(TFLOPs / second).magnitude)
tpuv5_bw = f"{TPUV5P_MEM_BW.to(TB / second).magnitude:.2f}"
tpuv5_cap = int(TPUV5P_MEM_CAPACITY.to(GiB).magnitude)
tpuv5_ici = int(TPUV5P_ICI_BW.to(GB/second).magnitude)
```

#### A Concrete Example: The A100 Analysis {#sec-system-foundations-concrete-example-a100-analysis-0bb9}

Consider an NVIDIA A100 GPU with FP16 Tensor Core performance of `{python} a100_fp16` TFLOP/s and HBM2e bandwidth of `{python} a100_bw_tb` TB/s. The ridge point is `{python} a100_fp16` / `{python} a100_bw_tb` = `{python} ridge_point` FLOP/byte.

Now compare two common operations:

**GEMM (Matrix Multiplication)**: For two `{python} n_gemm`×`{python} n_gemm` matrices, arithmetic intensity is approximately `{python} gemm_intensity` FLOP/byte. Since `{python} gemm_intensity` > `{python} ridge_point`, this operation is compute-bound. You are using the hardware efficiently.

**ReLU (Element-wise)**: For a `{python} n_gemm`×`{python} n_gemm` tensor, intensity is approximately `{python} relu_intensity` op/byte. Since `{python} relu_intensity` ≪ `{python} ridge_point`, this operation is severely memory-bound, achieving only about `{python} relu_util_str`% of peak TFLOP/s. The hardware is mostly waiting for data.

This explains why modern frameworks fuse operations: combining ReLU with the preceding MatMul avoids writing intermediate results to memory, effectively increasing arithmetic intensity.

### Dimensional Analysis {#sec-system-foundations-dimensional-analysis}

The Roofline Model helps diagnose *where* a bottleneck lies. But before applying any performance equation, we should verify that it is physically meaningful. Dimensional analysis provides this sanity check: any valid equation must be **dimensionally homogeneous**—every term must resolve to the same units. If they do not, the equation contains an error.

Consider the **Iron Law of ML Systems** introduced in @sec-silicon-contract:

$$ T = \frac{D_{vol}}{BW} + \frac{O}{R_{peak} \cdot \eta} + L_{lat} $$

We verify correctness by confirming that every term resolves to **Time (seconds)**:

$$ T [s] = \underbrace{ \frac{D_{vol} [\text{Bytes}]}{BW [\text{Bytes/s}]} }_{\text{Seconds}} + \underbrace{ \frac{O [\text{FLOPs}]}{R_{peak} [\text{FLOPs/s}] \cdot \eta [1]} }_{\text{Seconds}} + \underbrace{ L_{lat} [s] }_{\text{Seconds}} $$

*   **Data Term**: $\frac{\text{Bytes}}{\text{Bytes/s}} = \text{Bytes} \times \frac{\text{s}}{\text{Bytes}} = \mathbf{s}$
*   **Compute Term**: $\frac{\text{FLOPs}}{\text{FLOPs/s}} = \text{FLOPs} \times \frac{\text{s}}{\text{FLOPs}} = \mathbf{s}$
*   **Overhead Term**: Already in seconds.

The equation is physically consistent. Apply this technique to any systems equation you encounter: if the dimensions do not match, the formula is wrong. Note also that you cannot directly trade "FLOPs" for "Bandwidth"—they have different units. Any such trade-off must convert through Time, which is precisely what the Iron Law quantifies.

With tools for diagnosing single-device bottlenecks in hand, we now turn to the fundamental limits of scaling across multiple devices.

### Amdahl's Law and Gustafson's Law {#sec-system-foundations-amdahls-law-gustafsons-law-2f5c}

Parallelization is the primary tool for scaling ML, but its limits depend on *how* you scale. These two laws frame the fundamental tension in parallel computing. **Amdahl's Law** is the pessimist's view, governing how much faster a *fixed* task can run (optimizing latency). **Gustafson's Law** is the optimist's view, governing how much *more* work we can do in the same time (optimizing throughput).

#### Strong Scaling (Amdahl's Law)

**Strong scaling** answers the question: *If I add more processors to a fixed-size problem, how much faster will it run?*

**Amdahl's Law** [@amdahl1967validity] states that the speedup is limited by the serial portion of the task.[^fn-amdahl] If a fraction $s$ of your task is serial (cannot be parallelized) and $p = 1-s$ is parallelizable, the maximum speedup with $n$ processors is:

[^fn-amdahl]: **Gene Amdahl** (1922–2015) was a legendary computer architect at IBM, where he was the chief architect of the System/360. He later founded Amdahl Corporation to compete with IBM in the mainframe market.

$$ \text{Speedup}(n) = \frac{1}{s + \frac{1-s}{n}} $$

As $n \to \infty$, the term $\frac{1-s}{n} \to 0$, and the speedup converges to $1/s$.

**Example**: If 5% of your training step is serial overhead (e.g., Python GIL, kernel launch latency) and 95% is parallelizable matrix math:

```{python}
#| label: amdahl-example
#| echo: false

# Amdahl's Law: Speedup(n) = 1 / (s + (1-s)/n)
s = 0.05
p = 1 - s  # parallel fraction
n_8 = 8
amdahl_8 = 1 / (s + p / n_8)
amdahl_8_str = f"{amdahl_8:.1f}"
amdahl_inf = 1 / s
amdahl_inf_str = f"{amdahl_inf:.0f}"
```

*   With $n=1$, speedup is 1.
*   With $n=8$, speedup is $\frac{1}{0.05 + 0.95/8}$ ≈ `{python} amdahl_8_str`×.
*   With $n=\infty$, speedup is capped at $1/0.05$ = `{python} amdahl_inf_str`×.

No matter how many accelerators you buy, you cannot make this fixed workload run faster than `{python} amdahl_inf_str`×.

#### Weak Scaling (Gustafson's Law)

**Weak scaling** answers the question: *If I add more processors, how much larger of a problem can I solve in the same amount of time?*

This is the reality of Large Language Models. We don't use 1,000 GPUs to train GPT-4 on a laptop-sized dataset in milliseconds; we use them to train on a dataset 1,000x larger in reasonable time.

**Gustafson's Law** [@gustafson1988reevaluating] models this "scaled speedup":[^fn-gustafson]

[^fn-gustafson]: **John Gustafson** is a computer scientist known for his work in parallel computing and for introducing the Unum (universal number) format. His law was a direct response to the perceived "limits" of Amdahl's Law when applied to massive scale.

$$ \text{Scaled Speedup}(n) = n - s(n - 1) $$

Here, the parallel part of the workload grows linearly with $n$, while the serial part $s$ remains fixed.

**Example**: Using the same 5% serial overhead ($s=0.05$):

```{python}
#| label: gustafson-example
#| echo: false

# Gustafson's Law: Scaled Speedup(n) = n - s*(n-1)
s_g = 0.05
gustafson_8 = 8 - s_g * (8 - 1)
gustafson_8_str = f"{gustafson_8:.2f}"             # "7.65"
gustafson_8_serial = f"{s_g * 7:.2f}"              # "0.35"
gustafson_1000 = 1000 - s_g * (1000 - 1)
gustafson_1000_str = f"{gustafson_1000:.0f}"       # "950"
```

*   With $n=1$, speedup is 1.
*   With $n=8$, Scaled Speedup is $8 - 0.05 \times (7) = 8 - 0.35$ = `{python} gustafson_8_str`×.
*   With $n=1000$, Scaled Speedup is $1000 - 0.05 \times (999)$ ≈ `{python} gustafson_1000_str`×.

In weak scaling, efficiency remains high because the useful work (training the model) scales up to dwarf the fixed overheads.

:::: {.callout-notebook title="The Training Time Equation"}

Just as classical architecture has an "Iron Law" of performance, Large Language Model training has a fundamental governing equation. To estimate training time $T$:

$$ T \approx \frac{6 \cdot P \cdot D}{N \cdot X \cdot U} $$

Where:

*   **$6$**: The factor deriving from the forward pass ($2PD$) and backward pass ($4PD$) FLOPs per token.
*   **$P$**: Number of model parameters.
*   **$D$**: Number of training tokens.
*   **$N$**: Number of accelerators (GPUs).
*   **$X$**: Peak FLOP/s of one accelerator.
*   **$U$**: Model FLOPS Utilization (MFU), typically 30%–50%.

```{python}
#| label: training-time-example
#| echo: false

# Training time equation: T = 6*P*D / (N*X*U)
P_params = 1e9           # 1B parameters
D_tokens = 20e9          # 20B tokens
N_gpus = 1
X_flops = 312e12         # A100 FP16 peak FLOP/s
U_mfu = 0.40             # 40% MFU

total_flops = 6 * P_params * D_tokens
throughput = N_gpus * X_flops * U_mfu
T_seconds = total_flops / throughput
T_minutes = T_seconds / 60

T_seconds_str = f"{T_seconds:.0f}"                                            # "960"
T_minutes_str = f"{T_minutes:.0f}"                                            # "16"
```

**Example**: Training a **1B parameter** model on **20B tokens** using **1 A100** (`{python} a100_fp16` TFLOPS) at **40% utilization**.
$$ \text{Total FLOPs} = 6 \times 10^9 \times 20 \times 10^9 = 1.2 \times 10^{20} \text{ FLOPs} $$
$$ \text{Throughput} = 1 \times (312 \times 10^{12}) \times 0.40 \approx 1.25 \times 10^{14} \text{ FLOP/s} $$
$$ T = \frac{1.2 \times 10^{20}}{1.25 \times 10^{14}} \approx 960{,}000 \text{ seconds} \approx 16{,}000 \text{ minutes} $$

The computed result: **`{python} T_seconds_str` seconds** (≈ `{python} T_minutes_str` minutes, or about 11 days).

::::

### Little's Law {#sec-system-foundations-littles-law-9c4c}

For capacity planning in inference systems, **Little's Law** [@little1961proof] relates concurrency ($L$), arrival rate ($\lambda$), and latency ($W$):[^fn-little]

[^fn-little]: **John Little** is an Institute Professor at MIT and a pioneer in the field of operations research. His law, proved in 1961, is fundamental to queuing theory and is used across fields from manufacturing to computer network analysis.

$$ L = \lambda \times W $$

```{python}
#| label: littles-law-example
#| echo: false

# Little's Law: L = lambda * W
lambda_qps = 1000        # queries per second
W_latency_s = 0.050      # 50 ms in seconds
L_concurrent = lambda_qps * W_latency_s
L_concurrent_str = f"{L_concurrent:.0f}"   # "50"
```

**Example**: To sustain 1,000 queries per second (QPS) with 50ms average latency, your system must support 1000 × 0.05 = `{python} L_concurrent_str` concurrent requests.

**Implication for Memory**: This tells you exactly how to size your inference worker pools. If serving one request requires 1 GB of temporary memory (KV cache, activations), handling 50 concurrent requests requires 50 GB of memory. If your accelerator only has 24 GB, you are physically limited to 24 concurrent requests. Your maximum throughput is capped at $L/W = 24 / 0.05 = 480$ QPS, regardless of how many requests arrive.

## Computer Architecture Essentials {#sec-system-foundations-computer-architecture-essentials-bb18}

While physics sets the theoretical speed limits, **Computer Architecture** defines the actual machinery we use to approach them. The gap between theoretical peaks (Roofline) and realized performance often lies in how well we utilize the memory hierarchy.

Understanding computer architecture is fundamental to optimizing ML systems. The physical constraints of hardware—memory capacity, bandwidth, and latency—determine what is achievable in practice. This section covers the memory hierarchy that governs data access patterns and the distinction between bandwidth and latency that shapes system design decisions.

To design these systems, we must move beyond abstract concepts and look at the actual "speed limits" of modern silicon.

### Latencies Every Programmer Should Know (2025 Edition)

The first step in systems intuition is understanding the cost of distance. To write efficient kernels or distributed algorithms, you must have an intuitive sense of how long the processor waits for data. If accessing a register is like picking up a pencil from your desk, fetching from HBM is walking across the office, and fetching from disk is flying to the moon.

| **Component**            |         **Latency (ns)** | **Cycles (Approx)** | **Relative "Distance"** |
|:-----------------------|-----------------------:|------------------:|:---------------------:|
| **L1 Cache / Register**  |     ~`{python} l1_ns` ns |          3-4 cycles |         1 minute        |
| **L2 Cache**             |     ~`{python} l2_ns` ns |           12 cycles |        4 minutes        |
| **HBM3 (GPU Memory)**    |    ~`{python} hbm_ns` ns |        1,000 cycles |         5 hours         |
| **NVLink (GPU-GPU)**     | ~`{python} nvlink_ns` ns |        1,500 cycles |         8 hours         |
| **PCIe (CPU-GPU)**       |   ~`{python} pcie_ns` ns |        3,000 cycles |          1 day          |
| **InfiniBand (Network)** |     ~`{python} ib_ns` ns |       15,000 cycles |          1 week         |
| **SSD (NVMe)**           |    ~`{python} ssd_ns` ns |      300,000 cycles |         3 months        |

: **The Latency Hierarchy.** Access times for modern AI hardware. Note the massive jump from SRAM (Cache) to HBM. Any kernel that misses cache pays a heavy penalty. {#tbl-latency-hierarchy}

### The AI Hardware Cheat Sheet (Modern Reference)

While latency tells us how long we wait for the *first* byte, bandwidth tells us how many bytes follow. Use these constants for back-of-the-envelope "Roofline" calculations. These represent the "standard units of compute" for the 2025 era of machine learning.

| **Spec**             |                **NVIDIA H100 (SXM)** |              **Google TPU v5p** | **System Impact**                       |
|:-------------------|-----------------------------------:|------------------------------:|:--------------------------------------|
| **FP16/BF16 Peak**   |         `{python} h100_flops` TFLOPS |   `{python} tpuv5_flops` TFLOPS | The "Speed Limit" ($R_{peak}$)          |
| **Memory Bandwidth** |              `{python} h100_bw` TB/s |        `{python} tpuv5_bw` TB/s | The "Width of the Pipe" ($BW$)          |
| **HBM Capacity**     |               `{python} h100_cap` GB |         `{python} tpuv5_cap` GB | Max Model Size ($P$) / Batch Size ($B$) |
| **L2/SRAM Cache**    |                                50 MB |                         ~100 MB | Critical for Operator Fusion            |
| **Interconnect**     | `{python} h100_nvlink` GB/s (NVLink) | `{python} tpuv5_ici` GB/s (ICI) | Determines Model Parallelism Scaling    |

: **Reference Specs (2025).** Key constants for quantitative analysis. Always check specific datasheets, but these serve as standard units of compute. {#tbl-hardware-cheatsheet}

### The Memory Hierarchy {#sec-system-foundations-memory-hierarchy-674b}

Computer systems use a hierarchy because no single technology provides both high capacity and low latency, as shown in @fig-memory-hierarchy. Every technique that keeps data higher in the pyramid (registers/cache) directly improves performance.

::: {#fig-memory-hierarchy fig-env="figure" fig-pos="htb" fig-cap="**The Memory Hierarchy**: Performance depends on data proximity. Accessing HBM is ~100x slower than registers; accessing SSD is ~100,000x slower." fig-alt="Pyramid showing Registers at top, followed by Cache, HBM/DRAM, and Storage at bottom."}
```{.tikz}
\begin{tikzpicture}[line cap=round, line join=round, font=\usefont{T1}{phv}{m}{n}\small, scale=1.0]
  \tikzset{
    LabelAxis/.style={text=TextBlack!60, font=\footnotesize\usefont{T1}{phv}{m}{n}, align=center}
  }
  \filldraw[fill=RedFill, draw=RedLine, line width=1pt] (0, 5.2) -- (-1.2, 4.2) -- (1.2, 4.2) -- cycle;
  \node[text=RedLine] at (0, 4.55) {\textbf{Registers}};
  \filldraw[fill=YellowFill, draw=YellowLine, line width=1pt] (-1.2, 4.2) -- (-2.4, 2.8) -- (2.4, 2.8) -- (1.2, 4.2) -- cycle;
  \node[text=YellowLine!80!black] at (0, 3.5) {\textbf{L1 / L2 / L3 Cache}};
  \filldraw[fill=BlueFill, draw=BlueLine, line width=1pt] (-2.4, 2.8) -- (-3.6, 1.4) -- (3.6, 1.4) -- (2.4, 2.8) -- cycle;
  \node[text=BlueLine] at (0, 2.1) {\textbf{HBM / DRAM}};
  \filldraw[fill=GreenFill, draw=GreenLine, line width=1pt] (-3.6, 1.4) -- (-4.8, 0) -- (4.8, 0) -- (3.6, 1.4) -- cycle;
  \node[text=GreenLine] at (0, 0.7) {\textbf{Storage (SSD / Disk)}};
  \draw[->, ultra thick, gray!40] (5.5, 0) -- (5.5, 5.2) node[midway, right, LabelAxis, text=gray] {Faster Speed\\ Lower Latency};
  \draw[->, ultra thick, gray!40] (-5.5, 5.2) -- (-5.5, 0) node[midway, left, LabelAxis, text=gray] {Larger Capacity\\ Lower Cost};
\end{tikzpicture}
```
:::

The hierarchy's energy costs reveal why data movement dominates modern system design.

:::: {.callout-notebook title="The High Cost of Data Movement"}
Fetching a 32-bit value from DRAM costs roughly **1000× more energy** than performing a floating-point operation on it (e.g., ~`{python} dram_pj` pJ vs ~`{python} flop_pj` pJ). This "Energy Wall" means that maximizing **arithmetic intensity** (doing many ops per loaded byte) is the only way to be energy efficient.
::::

### Bandwidth vs. Latency {#sec-system-foundations-bandwidth-vs-latency-e155}

Bandwidth (throughput) and latency (delay) are distinct constraints. Total transfer time follows:

$$ T = \text{Latency} + \frac{\text{Data Size}}{\text{Bandwidth}} $$

For small transfers (e.g., single-token inference), latency dominates. For large transfers (e.g., loading weights), bandwidth dominates.

**Example**: Sending data over a 10 Gbps link with 10ms ping (latency).

```{python}
#| label: bandwidth-latency-example
#| echo: false

# Transmission time = Data Size / BW
data_kb = 1e3             # 1 KB in bytes
bw_gbps = 10e9            # 10 Gbps in bits/s
tx_time_s = (data_kb * 8) / bw_gbps   # convert bytes to bits
tx_time_us = tx_time_s * 1e6
tx_time_us_str = f"{tx_time_us:.1f}"   # "0.8"
```

*   **Latency-Bound (1KB Packet)**:
    *   Transmission: 1KB / 10Gbps ≈ `{python} tx_time_us_str` μs.
    *   Total Time ≈ 10ms + `{python} tx_time_us_str`μs ≈ 10ms.
    *   *Result*: The bandwidth is irrelevant; the speed of light (ping) is the bottleneck.

*   **Bandwidth-Bound (1GB Checkpoint)**:
    *   Transmission: $1\text{GB} / 10\text{Gbps} \approx 800\text{ms}$.
    *   Total Time $\approx 10\text{ms} + 800\text{ms} = 810\text{ms}$.
    *   *Result*: The ping is negligible; the pipe size is the bottleneck.

## Numerical Representations {#sec-system-foundations-numerical-representations-7b2f}

While statistics helps us understand data distributions, **Numerical Representations** determine how we store the values themselves. In ML systems, the choice of precision (FP32 vs. BF16 vs. INT8) is a direct trade-off between statistical fidelity and hardware throughput.

::: {.callout-perspective title="Why This Matters"}
Your production model runs at 50 QPS in FP32 but your target is 200 QPS. Switching to INT8 could get you there, but will accuracy suffer? Understanding numerical formats lets you make this trade-off quantitatively rather than hoping for the best.
:::

Neural networks are remarkably tolerant of reduced numerical precision. This section explains the formats you will encounter and their trade-offs.

### Floating-Point Format Comparison {#sec-system-foundations-floatingpoint-format-comparison-c861}

The IEEE 754 standard and its AI-specific derivatives define different trade-offs between dynamic range (the span of representable values) and precision (how finely you can represent values within that range). @tbl-numerical-formats summarizes the key formats and their use cases.

| **Format** | **Bits** | **Exponent** | **Mantissa** |                     **Dynamic Range** | **Typical Use Case**                           |
|:---------|-------:|-----------:|-----------:|------------------------------------:|:---------------------------------------------|
| **FP32**   |       32 |            8 |           23 |          $\sim 10^{-38}$ to $10^{38}$ | Training (full precision), reference inference |
| **FP16**   |       16 |            5 |           10 | $\sim 10^{-5}$ to $6.5 \times 10^{4}$ | Training with loss scaling, inference          |
| **BF16**   |       16 |            8 |            7 |                          Same as FP32 | Training (preferred), avoids loss scaling      |
| **FP8**    |        8 |       4 or 5 |       3 or 2 |                                Varies | Inference on newest hardware (H100+)           |
| **INT8**   |        8 |          N/A |          N/A |                           -128 to 127 | Inference after quantization                   |

: **Numerical Format Comparison**: Each format trades off precision, dynamic range, memory footprint, and compute throughput. BF16 has emerged as the preferred training format because it matches FP32's range while using half the memory. {#tbl-numerical-formats}

::: {#fig-float-formats fig-env="figure" fig-pos="htb" fig-cap="**Numerical Format Bit Layouts**: A visual comparison of bit allocations. Note how **BF16** (Brain Float 16) preserves the 8-bit exponent of **FP32**, ensuring the same dynamic range for training stability. **FP16** trades range for precision, often requiring loss scaling to prevent underflow." fig-alt="Stacked horizontal bars showing bit breakdown. FP32: 1 Sign, 8 Exp, 23 Mantissa. BF16: 1 Sign, 8 Exp, 7 Mantissa. FP16: 1 Sign, 5 Exp, 10 Mantissa. INT8: 8 Integer bits."}
```{.tikz}
\begin{tikzpicture}[font=\small\usefont{T1}{phv}{m}{n}, scale=1.0]
  \tikzset{
    BitBox/.style={draw=white, line width=0.8pt, minimum height=0.6cm, align=center, font=\scriptsize\bfseries\usefont{T1}{phv}{m}{n}, text=white},
    Label/.style={text=TextBlack, font=\small\bfseries\usefont{T1}{phv}{m}{n}, anchor=east}
  }

  % Colors
  \definecolor{SignColor}{HTML}{D9534F}      % Red
  \definecolor{ExpColor}{HTML}{5BC0DE}       % Blue
  \definecolor{MantColor}{HTML}{F0AD4E}      % Orange
  \definecolor{IntColor}{HTML}{5CB85C}       % Green

  % FP32
  \node[Label] at (-0.2, 3) {FP32 (32-bit)};
  \node[BitBox, fill=SignColor, minimum width=0.3cm] (fp32_s) at (0.15, 3) {S};
  \node[BitBox, fill=ExpColor, minimum width=2.4cm, right=0pt of fp32_s] (fp32_e) {Exponent (8)};
  \node[BitBox, fill=MantColor, minimum width=6.9cm, right=0pt of fp32_e] (fp32_m) {Mantissa (23)};

  % BF16
  \node[Label] at (-0.2, 2) {BF16 (16-bit)};
  \node[BitBox, fill=SignColor, minimum width=0.3cm] (bf16_s) at (0.15, 2) {S};
  \node[BitBox, fill=ExpColor, minimum width=2.4cm, right=0pt of bf16_s] (bf16_e) {Exponent (8)};
  \node[BitBox, fill=MantColor, minimum width=2.1cm, right=0pt of bf16_e] (bf16_m) {Mant (7)};
  \node[right=0.2cm of bf16_m, font=\scriptsize\usefont{T1}{phv}{m}{n}, text=gray] {Matches FP32 Range};

  % FP16
  \node[Label] at (-0.2, 1) {FP16 (16-bit)};
  \node[BitBox, fill=SignColor, minimum width=0.3cm] (fp16_s) at (0.15, 1) {S};
  \node[BitBox, fill=ExpColor, minimum width=1.5cm, right=0pt of fp16_s] (fp16_e) {Exp (5)};
  \node[BitBox, fill=MantColor, minimum width=3.0cm, right=0pt of fp16_e] (fp16_m) {Mantissa (10)};

  % INT8
  \node[Label] at (-0.2, 0) {INT8 (8-bit)};
  \node[BitBox, fill=IntColor, minimum width=2.4cm] (int8) at (1.2, 0) {Integer (8)};

  % Grid / Scale markers (approximate)
  \draw[gray!30, dashed] (0, -0.5) -- (0, 3.5);
  \draw[gray!30, dashed] (9.6, -0.5) -- (9.6, 3.5);
  \node[below, font=\scriptsize\usefont{T1}{phv}{m}{n}, text=gray] at (0, -0.5) {Bit 31/15/7};
  \node[below, font=\scriptsize\usefont{T1}{phv}{m}{n}, text=gray] at (9.6, -0.5) {Bit 0};

\end{tikzpicture}
```
:::

Beyond bit width, the allocation of bits between exponent and mantissa determines what range of values each format can represent.

::: {.callout-perspective title="The Dynamic Range Wall"}

The choice of numerical format is a direct application of the **Iron Law of ML Systems**. Reducing precision from FP32 to BF16 or FP16 halves the **Data Movement** term in the denominator, potentially doubling throughput on memory-bound workloads. However, the *type* of 16-bit format determines the engineering complexity:

*   **Dynamic Range (The Exponent)**: BF16 preserves the 8-bit exponent of FP32. This means it can represent the same range of extremely large and extremely small values (gradients). This is why BF16 is the "gold standard" for stable training—it rarely suffers from numerical overflow or underflow.
*   **Precision (The Mantissa)**: FP16 has a larger 10-bit mantissa than BF16 (7 bits), offering higher precision for values within its range. But its 5-bit exponent is a major constraint; gradients often "vanish" to zero (underflow) because the exponent cannot represent them. To solve this, FP16 training requires **Loss Scaling**, an operational overhead where gradients are multiplied by a large constant to push them into the representable range.
*   **Energy Efficiency**: INT8 operations are significantly more energy-efficient than floating-point equivalents because they utilize simpler integer ALUs and require less silicon area. Moving to INT8 for inference is the primary lever for deploying LLMs on battery-constrained edge devices.

:::

**Brain Float 16 (BF16)** deserves special attention [@wang_bfloat16_2019]. It matches FP32's 8-bit exponent (preserving dynamic range) while truncating the mantissa to 7 bits. This avoids the gradient underflow problems that plague FP16 training, eliminating the need for complex loss scaling. Most modern training uses BF16 for this reason.[^fn-bf16]

[^fn-bf16]: **BF16** was originally introduced with the Google TPUv2 and has since been adopted by Intel, Arm, and NVIDIA (starting with Ampere architectures).

### Integer Quantization {#sec-system-foundations-integer-quantization-9d02}

Quantization maps continuous floating-point values to discrete integers, typically INT8. The key challenge is choosing how to map the floating-point range to integers.

**Symmetric quantization** centers the mapping at zero:
$$ x_{int} = \text{round}\left(\frac{x}{\alpha} \times 127\right) $$

where $\alpha$ is the scale factor (typically the maximum absolute value). This works well for weight distributions centered around zero.

**Asymmetric quantization** handles distributions that are not centered (common after ReLU, which produces only non-negative values) by adding a zero-point offset:
$$ x_{int} = \text{round}\left(\frac{x - z}{\alpha} \times 255\right) $$

The choice between symmetric and asymmetric quantization depends on your tensor's distribution and has measurable accuracy implications.

## Fallacies and Pitfalls {#sec-machine-foundations-fallacies-pitfalls}

::: {.callout-warning title="Fallacy: Doubling GPUs halves training time."}
**The Reality**: This assumes perfect **strong scaling** (Amdahl's Law). In practice, communication overhead (all-reduce) grows with $N$, and batch size constraints may limit parallelism. At large scale, you often hit diminishing returns unless you also scale the problem size (Weak Scaling).
:::

A related misconception concerns numerical precision.

::: {.callout-warning title="Fallacy: Higher precision (FP32) is always better."}
**The Reality**: For deep learning, FP32 often hurts performance without improving convergence. It consumes 2x memory bandwidth and energy compared to BF16. Since neural networks are resilient to noise, the extra mantissa bits in FP32 are often modeling random variance rather than signal.
:::
