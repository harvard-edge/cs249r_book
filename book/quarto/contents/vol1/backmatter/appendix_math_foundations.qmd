# Mathematical Foundations {#sec-appendix-math-foundations}

This appendix provides the mathematical tools you will need to reason about ML system performance. Rather than an exhaustive reference, think of this as a practitioner's toolkit: the concepts that come up again and again when debugging slow training jobs, optimizing inference latency, or choosing between hardware configurations.

## Quick Reference {.unnumbered}

Before diving in, here is a map of what this appendix covers and where each concept appears in the main text.

+-------------------------------+--------------------------------------------+-------------------------------------------+
| Topic                         | What It Helps You Do                       | Where It Appears                          |
+:==============================+:===========================================+:==========================================+
| Roofline Model                | Determine if a workload is memory-bound    | @sec-ai-acceleration, @sec-ai-training,   |
|                               | or compute-bound                           | @sec-serving                              |
+-------------------------------+--------------------------------------------+-------------------------------------------+
| Amdahl's Law                  | Calculate maximum speedup from             | @sec-ai-acceleration, @sec-ai-training    |
|                               | parallelization                            |                                           |
+-------------------------------+--------------------------------------------+-------------------------------------------+
| Gustafson's Law               | Understand weak scaling for large models   | @sec-ai-training                          |
+-------------------------------+--------------------------------------------+-------------------------------------------+
| Little's Law                  | Size serving infrastructure for target QPS | @sec-serving                              |
+-------------------------------+--------------------------------------------+-------------------------------------------+
| Memory Hierarchy              | Optimize data movement and cache usage     | @sec-ai-acceleration, @sec-efficient-ai   |
+-------------------------------+--------------------------------------------+-------------------------------------------+
| Numerical Formats             | Choose precision for training vs inference | @sec-model-optimizations, @sec-serving    |
+-------------------------------+--------------------------------------------+-------------------------------------------+
| GEMM Operations               | Understand the core computation in         | @sec-dl-primer, @sec-ai-acceleration      |
|                               | neural networks                            |                                           |
+-------------------------------+--------------------------------------------+-------------------------------------------+
| Backpropagation               | Debug gradient issues and memory usage     | @sec-dl-primer, @sec-ai-training          |
+-------------------------------+--------------------------------------------+-------------------------------------------+
| Sparse Formats (CSR)          | Work with recommendation systems and       | @sec-model-optimizations                  |
|                               | pruned models                              |                                           |
+-------------------------------+--------------------------------------------+-------------------------------------------+
| Computational Graphs          | Understand compiler optimizations          | @sec-ai-frameworks, @sec-ai-acceleration  |
+-------------------------------+--------------------------------------------+-------------------------------------------+

: **Appendix Quick Reference**: Each mathematical concept in this appendix, what engineering decisions it informs, and where it appears in the main text. {#tbl-appendix-overview}

---

## Performance Analysis Models {#sec-appendix-performance-models}

::: {.callout-tip title="Why This Matters"}
You have trained a model that achieves good accuracy, but inference takes 200ms when your SLA requires 50ms. Where do you start? Performance analysis models give you a systematic way to diagnose whether you are limited by computation, memory bandwidth, or something else entirely. Without these tools, optimization becomes guesswork.
:::

The models in this section form the foundation of quantitative systems thinking. Master them, and you will be able to look at almost any ML workload and predict where the bottlenecks lie.

### The Roofline Model {#sec-appendix-roofline}

The Roofline Model [@williams2009roofline] answers a deceptively simple question: *how fast can this workload possibly run on this hardware?* The answer depends on whether you run out of compute or memory bandwidth first.

Every operation has an **arithmetic intensity**: the ratio of computations performed to bytes moved from memory. Matrix multiplication has high arithmetic intensity because you can reuse each loaded element many times. Element-wise operations like ReLU have low intensity because you load a number, do one operation, and write it back.

As illustrated in @fig-roofline, the model plots achievable performance against arithmetic intensity. The resulting "roofline" shape reveals two distinct operating regions.

::: {#fig-roofline fig-env="figure" fig-pos="htb" fig-cap="**The Roofline Model**: The plot visualizes the memory-bound (sloped) and compute-bound (horizontal) regions of a hardware platform. Workload A, situated on the slope, is constrained by memory bandwidth, while Workload B, on the plateau, is limited by peak compute capability."
```{.tikz}
\begin{tikzpicture}[font=\small\usefont{T1}{phv}{m}{n}, scale=1.0]
  % Define Project Colors
  \definecolor{BlueLine}{RGB}{66, 133, 244}
  \definecolor{BlueFill}{RGB}{232, 240, 254}
  \definecolor{RedLine}{RGB}{234, 67, 53}
  \definecolor{RedFill}{RGB}{252, 232, 230}
  \definecolor{GreenLine}{RGB}{52, 168, 83}
  \definecolor{GreenFill}{RGB}{230, 244, 234}
  \definecolor{YellowLine}{RGB}{251, 188, 4}
  \definecolor{GrayLine}{RGB}{95, 99, 104}
  \definecolor{TextBlack}{RGB}{32, 33, 36}

  % Styles
  \tikzset{
    Axis/.style={line width=1.0pt, draw=GrayLine, ->, >=stealth},
    Guide/.style={dashed, draw=GrayLine!60, line width=0.6pt},
    Label/.style={text=TextBlack, align=center, font=\footnotesize\usefont{T1}{phv}{m}{n}},
    Dot/.style={circle, fill=#1, draw=white, line width=0.5pt, minimum size=5pt, inner sep=0pt}
  }

  % Grid
  \draw[step=0.5, gray!10, very thin] (0,0) grid (6,4);
  
  % Axes
  \draw[Axis] (0,0) -- (6,0) node[right, text=TextBlack] {Arithmetic Intensity (Ops/Byte)};
  \draw[Axis] (0,0) -- (0,4.2) node[above, text=TextBlack] {Performance (FLOP/s)};

  % Roofline
  % Memory Bound Line
  \draw[BlueLine, line width=2pt] (0,0) -- (3,3);
  % Compute Bound Line
  \draw[RedLine, line width=2pt] (3,3) -- (5.8,3);

  % Region Labels
  \node[Label, text=BlueLine, rotate=45, anchor=south, yshift=2pt] at (1.5, 1.5) {\textbf{Memory Bound}};
  \node[Label, text=RedLine, anchor=south, yshift=2pt] at (4.4, 3) {\textbf{Compute Bound}};

  % Ridge Point
  \node[Dot=TextBlack] at (3,3) {};
  \draw[Guide] (3,0) -- (3,3);
  \node[below, font=\scriptsize\usefont{T1}{phv}{m}{n}, text=TextBlack] at (3,0) {Ridge Point};

  % Workloads
  % Workload A (Memory Bound)
  \node[Dot=YellowLine] (WA) at (1.5, 1.5) {};
  \node[Label, right, xshift=4pt, align=left] at (1.5, 1.3) {Workload A\\(Mem Bound)};
  \draw[->, gray, thin, shorten >=2pt] (1.6, 1.3) -- (WA);

  % Workload B (Compute Bound)
  \node[Dot=GreenLine] (WB) at (4.5, 3) {};
  \node[Label, below, yshift=-4pt] at (4.5, 3) {Workload B (Compute Bound)};

\end{tikzpicture}
```
:::

The key equations are straightforward. Arithmetic intensity is:

$$ \text{Arithmetic Intensity} = \frac{\text{FLOPs}}{\text{Bytes Accessed}} $$ {#eq-arithmetic-intensity}

In the **memory-bound region** (the sloped portion), performance scales with bandwidth:

$$ \text{Attainable Performance} = \text{Bandwidth} \times \text{Arithmetic Intensity} $$ {#eq-memory-bound}

In the **compute-bound region** (the flat portion), you hit the hardware ceiling:

$$ \text{Attainable Performance} = \text{Peak FLOP/s} $$ {#eq-compute-bound}

The transition point, called the **ridge point**, characterizes the hardware's balance between compute and memory:

$$ \text{Ridge Point} = \frac{\text{Peak FLOP/s}}{\text{Memory Bandwidth}} $$ {#eq-ridge-point}

#### A Concrete Example

Consider an NVIDIA A100 GPU with FP16 Tensor Core performance of 312 TFLOP/s and HBM2e bandwidth of 2.0 TB/s. The ridge point is $312 / 2.0 = 156$ FLOP/byte.

Now compare two common operations:

**GEMM (Matrix Multiplication)**: For two $4096 \times 4096$ matrices, arithmetic intensity is approximately $1365$ FLOP/byte. Since $1365 > 156$, this operation is compute-bound. You are using the hardware efficiently.

**ReLU (Element-wise)**: For a $4096 \times 4096$ tensor, intensity is approximately $0.25$ op/byte. Since $0.25 \ll 156$, this operation is severely memory-bound, achieving only about $0.16\%$ of peak TFLOP/s. The hardware is mostly waiting for data.

This explains why modern frameworks fuse operations: combining ReLU with the preceding MatMul avoids writing intermediate results to memory, effectively increasing arithmetic intensity.

### Amdahl's Law and Gustafson's Law {#sec-appendix-scaling-laws}

You have access to a cluster with 64 GPUs. How much faster will training be compared to a single GPU? The answer is almost never "64 times faster," and these two laws explain why.

**Amdahl's Law** [@amdahl1967validity] captures the fundamental limit of parallelization. If a fraction $p$ of your workload can be parallelized, the maximum speedup with $n$ processors is:

$$ \text{Speedup}(n) = \frac{1}{(1-p) + \frac{p}{n}} $$ {#eq-amdahl}

As $n \to \infty$, speedup approaches $\frac{1}{1-p}$. If 10% of your training iteration is serial (data loading on CPU, gradient aggregation), the maximum possible speedup is 10x. Adding more GPUs beyond that point yields diminishing returns.

This sounds pessimistic, but **Gustafson's Law** [@gustafson1988reevaluating] offers a more optimistic view for ML workloads. In practice, when you get more hardware, you often train larger models or use larger batches rather than just trying to finish faster. Under this "weak scaling" model, speedup becomes:

$$ \text{Speedup}(n) = (1-p) + p \times n $$ {#eq-gustafson}

This linear scaling better describes large-scale ML training, where increased resources enable training models that would otherwise be impossible, not just faster training of the same model.

### Little's Law {#sec-appendix-little}

Shifting from training to serving, **Little's Law** [@little1961proof] is the essential tool for capacity planning. It relates the average number of items in a system ($L$), the arrival rate ($\lambda$), and the average wait time ($W$):

$$ L = \lambda \times W $$ {#eq-little}

For ML serving: to sustain 1,000 queries per second (QPS) with 20ms average latency, your system must handle $1000 \times 0.02 = 20$ concurrent requests. This tells you how many inference workers you need and helps you right-size your infrastructure.

---

## Computer Architecture Essentials {#sec-appendix-architecture}

::: {.callout-tip title="Why This Matters"}
Your model runs 100x slower on CPU than GPU, but why? Understanding the memory hierarchy explains not just why GPUs are faster, but also why some optimizations (like batching) help dramatically while others (like buying more RAM) barely matter. The concepts here underpin every hardware decision you will make.
:::

The previous section gave you tools to analyze workloads. This section explains the hardware constraints those analyses reveal.

### The Memory Hierarchy {#sec-appendix-memory-hierarchy}

Computer systems use a hierarchy of memory technologies because no single technology provides both high capacity and low latency. As shown in @fig-memory-hierarchy, memories closer to the compute units are faster but dramatically smaller.

::: {#fig-memory-hierarchy fig-env="figure" fig-pos="htb" fig-cap="**The Memory Hierarchy**: Computational efficiency in ML depends on keeping the 'working set' (active weights and activations) as high up the pyramid as possible. Transfers between the HBM/DRAM layer and the Compute Units represent the primary performance bottleneck in modern accelerators."
```{.tikz}
\begin{tikzpicture}[font=\small\usefont{T1}{phv}{m}{n}, scale=1.0]
  % Define Project Colors
  \definecolor{BlueLine}{RGB}{66, 133, 244}
  \definecolor{BlueFill}{RGB}{232, 240, 254}
  \definecolor{RedLine}{RGB}{234, 67, 53}
  \definecolor{RedFill}{RGB}{252, 232, 230}
  \definecolor{GreenLine}{RGB}{52, 168, 83}
  \definecolor{GreenFill}{RGB}{230, 244, 234}
  \definecolor{YellowLine}{RGB}{251, 188, 4}
  \definecolor{YellowFill}{RGB}{254, 247, 224}
  \definecolor{TextBlack}{RGB}{32, 33, 36}

  % Styles
  \tikzset{
    Level/.style={draw=black!80, line width=0.8pt, align=center, text=TextBlack},
    LabelAxis/.style={text=TextBlack!60, font=\footnotesize\usefont{T1}{phv}{m}{n}, align=center}
  }

  % Pyramid Levels
  % Registers
  \filldraw[fill=RedFill, draw=RedLine, line width=1pt] (0, 5.2) -- (-1.2, 4.2) -- (1.2, 4.2) -- cycle;
  \node[text=RedLine] at (0, 4.55) {\textbf{Registers}};
  
  % Cache
  \filldraw[fill=YellowFill, draw=YellowLine, line width=1pt] (-1.2, 4.2) -- (-2.4, 2.8) -- (2.4, 2.8) -- (1.2, 4.2) -- cycle;
  \node[text=YellowLine!80!black] at (0, 3.5) {\textbf{L1 / L2 / L3 Cache}};
  
  % HBM
  \filldraw[fill=BlueFill, draw=BlueLine, line width=1pt] (-2.4, 2.8) -- (-3.6, 1.4) -- (3.6, 1.4) -- (2.4, 2.8) -- cycle;
  \node[text=BlueLine] at (0, 2.1) {\textbf{HBM / DRAM}};
  
  % Storage
  \filldraw[fill=GreenFill, draw=GreenLine, line width=1pt] (-3.6, 1.4) -- (-4.8, 0) -- (4.8, 0) -- (3.6, 1.4) -- cycle;
  \node[text=GreenLine] at (0, 0.7) {\textbf{Storage (SSD / Disk)}};
  
  % Side Arrows
  \draw[->, ultra thick, gray!40] (5.5, 0) -- (5.5, 5.2) node[midway, right, LabelAxis, text=gray] {Faster Speed\\Lower Latency};
  \draw[->, ultra thick, gray!40] (-5.5, 5.2) -- (-5.5, 0) node[midway, left, LabelAxis, text=gray] {Larger Capacity\\Lower Cost};
\end{tikzpicture}
```
:::

The performance gap between compute and memory is called the "Memory Wall." Modern ML accelerators attack this problem with High Bandwidth Memory (HBM), which provides several terabytes per second of bandwidth compared to hundreds of gigabytes per second in standard DDR5 DRAM. Even so, the gap remains: an NVIDIA H100 can perform computations far faster than it can feed data to those computations.

This is why **data reuse** is the primary goal of ML systems optimization. Every technique that keeps data in registers or cache longer (tiling, fusion, batching) directly addresses the memory wall.

### Bandwidth vs. Latency {#sec-appendix-bandwidth-latency}

Bandwidth (throughput) and latency (delay) are distinct constraints that matter in different situations. Total transfer time follows:

$$ T = L + \frac{B}{BW} $$

where $L$ is latency, $B$ is the data size, and $BW$ is bandwidth.

For small transfers (e.g., KV cache lookups in autoregressive generation), latency dominates. For large transfers (e.g., loading model checkpoints), bandwidth dominates. Understanding which regime you are in determines whether you should optimize for lower latency or higher throughput.

---

## Numerical Representations {#sec-appendix-numerical}

::: {.callout-tip title="Why This Matters"}
Your production model runs at 50 QPS in FP32 but your target is 200 QPS. Switching to INT8 could get you there, but will accuracy suffer? Understanding numerical formats lets you make this trade-off quantitatively rather than hoping for the best.
:::

Neural networks are remarkably tolerant of reduced numerical precision. This section explains the formats you will encounter and their trade-offs.

### Floating-Point Format Comparison {#sec-appendix-floating-point}

The IEEE 754 standard and its AI-specific derivatives define different trade-offs between dynamic range (the span of representable values) and precision (how finely you can represent values within that range).

+----------+-------+----------+----------+-----------------+--------------------------------------------+
| Format   | Bits  | Exponent | Mantissa | Dynamic Range   | Typical Use Case                           |
+:=========+:======+:=========+:=========+:================+:===========================================+
| FP32     | 32    | 8        | 23       | ~$10^{-38}$ to  | Training (full precision), reference       |
|          |       |          |          | $10^{38}$       | inference                                  |
+----------+-------+----------+----------+-----------------+--------------------------------------------+
| FP16     | 16    | 5        | 10       | ~$10^{-5}$ to   | Training with loss scaling, inference      |
|          |       |          |          | $6.5 \times     |                                            |
|          |       |          |          | 10^{4}$         |                                            |
+----------+-------+----------+----------+-----------------+--------------------------------------------+
| BF16     | 16    | 8        | 7        | Same as FP32    | Training (preferred), avoids loss scaling  |
+----------+-------+----------+----------+-----------------+--------------------------------------------+
| FP8      | 8     | 4 or 5   | 3 or 2   | Varies          | Inference on newest hardware (H100+)       |
+----------+-------+----------+----------+-----------------+--------------------------------------------+
| INT8     | 8     | N/A      | N/A      | -128 to 127     | Inference after quantization               |
+----------+-------+----------+----------+-----------------+--------------------------------------------+

: **Numerical Format Comparison**: Each format trades off precision, dynamic range, memory footprint, and compute throughput. BF16 has emerged as the preferred training format because it matches FP32's range while using half the memory. {#tbl-numerical-formats}

**Brain Float 16 (BF16)** deserves special attention. It matches FP32's 8-bit exponent (preserving dynamic range) while truncating the mantissa to 7 bits. This avoids the gradient underflow problems that plague FP16 training, eliminating the need for complex loss scaling. Most modern training uses BF16 for this reason.

### Integer Quantization {#sec-appendix-quantization}

Quantization maps continuous floating-point values to discrete integers, typically INT8. The key challenge is choosing how to map the floating-point range to integers.

**Symmetric quantization** centers the mapping at zero:
$$ x_{int} = \text{round}\left(\frac{x}{\alpha} \times 127\right) $$

where $\alpha$ is the scale factor (typically the maximum absolute value). This works well for weight distributions centered around zero.

**Asymmetric quantization** handles distributions that are not centered (common after ReLU, which produces only non-negative values) by adding a zero-point offset:
$$ x_{int} = \text{round}\left(\frac{x - z}{\alpha} \times 255\right) $$

The choice between symmetric and asymmetric quantization depends on your tensor's distribution and has measurable accuracy implications.

---

## Linear Algebra for Neural Networks {#sec-appendix-linear-algebra}

::: {.callout-tip title="Why This Matters"}
Every neural network, regardless of architecture, spends most of its time doing matrix multiplication. Understanding GEMM performance characteristics explains why batch size affects throughput, why certain layer dimensions are "better" than others, and how to interpret profiler output.
:::

### Tensor Operations and Notation {#sec-appendix-tensors}

We use **Einstein summation** notation throughout this book because it makes complex operations explicit. Matrix multiplication $C = AB$ becomes:

$$ C_{ij} = \sum_k A_{ik} B_{kj} $$

Or in einsum notation: `ik,kj->ij`. This notation extends naturally to the multi-dimensional operations in attention mechanisms. For example, batched multi-head attention is `bhid,bhjd->bhij` (batch, head, sequence indices).

### General Matrix Multiply (GEMM) {#sec-appendix-gemm}

GEMM is the computational workhorse of deep learning. For matrices of size $M \times K$ and $K \times N$, GEMM performs $2MNK$ floating-point operations (multiply-accumulate counts as two operations).

The arithmetic intensity of GEMM scales linearly with matrix dimension. For square $n \times n$ matrices, intensity is approximately $n/3$ FLOP/byte. This explains several important phenomena:

- **Larger batches improve efficiency**: Batching increases the effective matrix dimensions, pushing workloads toward the compute-bound region of the roofline.
- **Power-of-two dimensions help**: Hardware tensor cores are optimized for specific tile sizes (typically 16x16 or 32x32). Dimensions that align with these sizes avoid padding overhead.
- **Small matrices are inefficient**: A 64x64 GEMM may achieve only 10% of peak throughput because it cannot fully utilize the hardware.

### Memory Layouts and Performance {#sec-appendix-memory-layouts}

Data layout in memory (row-major vs. column-major) directly affects cache efficiency. When iterating over a matrix, accessing contiguous memory locations is dramatically faster than strided access. The difference can be 10x to 100x in effective bandwidth.

A common optimization pattern: transpose tensors once before repeated operations to ensure contiguous access in the hot loop. The one-time transpose cost is amortized across many subsequent operations.

---

## Calculus for Backpropagation {#sec-appendix-calculus}

::: {.callout-tip title="Why This Matters"}
When training fails (loss goes to NaN, gradients explode, memory runs out), understanding what backpropagation actually does helps you diagnose the problem. This section gives you the mental model to reason about gradient flow and memory usage during training.
:::

### The Chain Rule in Deep Networks {#sec-appendix-chain-rule}

For a composed function $y = f(g(x))$, the derivative is:

$$ \frac{dy}{dx} = \frac{dy}{dg} \cdot \frac{dg}{dx} $$

In deep networks, this chain extends across many layers. Each layer contributes a local Jacobian that multiplies during the backward pass. When these multiplied terms consistently exceed 1.0, gradients explode; when they consistently fall below 1.0, gradients vanish.

### The Backpropagation Algorithm {#sec-appendix-backprop}

Backpropagation implements the chain rule efficiently through two passes. As shown in @fig-backprop-graph, the forward pass computes outputs while caching intermediate activations. The backward pass then reuses these cached values to compute gradients.

::: {#fig-backprop-graph fig-env="figure" fig-pos="htb" fig-cap="**Backpropagation Computational Graph**: The forward pass (black) computes activations $h$ and loss $L$. The backward pass (red) propagates the error signal $\\frac{\\partial L}{\\partial y}$ through local derivatives to update parameters $W_1$ and $W_2$."
```{.tikz}
\begin{tikzpicture}[font=\small\usefont{T1}{phv}{m}{n}, node distance=3cm, auto, >=stealth, thick]
  % Define Project Colors
  \definecolor{BlueLine}{RGB}{66, 133, 244}
  \definecolor{BlueFill}{RGB}{232, 240, 254}
  \definecolor{RedLine}{RGB}{234, 67, 53}
  \definecolor{GrayLine}{RGB}{95, 99, 104}
  \definecolor{TextBlack}{RGB}{32, 33, 36}

  % Styles
  \tikzset{
    Node/.style={circle, draw=BlueLine, fill=BlueFill, line width=0.8pt, minimum size=0.9cm, text=TextBlack},
    Edge/.style={->, draw=GrayLine, line width=0.8pt},
    BackEdge/.style={->, dashed, draw=RedLine, line width=0.8pt, bend right=50},
    Label/.style={font=\footnotesize\usefont{T1}{phv}{m}{n}, text=TextBlack}
  }

  % Nodes
  \node[Node] (x) {x};
  \node[Node, right of=x] (h) {h};
  \node[Node, right of=h] (y) {y};
  \node[Node, right of=y, fill=RedLine!10, draw=RedLine] (L) {L};

  % Forward Pass
  \draw[Edge] (x) -- node[below, Label] {$W_1$} (h);
  \draw[Edge] (h) -- node[below, Label] {$W_2$} (y);
  \draw[Edge] (y) -- node[below, Label] {Loss} (L);

  % Backward Pass
  \draw[BackEdge] (L) to node[above, Label, text=RedLine] {$\frac{\partial L}{\partial y}$} (y);
  \draw[BackEdge] (y) to node[above, Label, text=RedLine] {$\frac{\partial L}{\partial W_2}$} (h);
  \draw[BackEdge] (h) to node[above, Label, text=RedLine] {$\frac{\partial L}{\partial W_1}$} (x);
\end{tikzpicture}
```
:::

The memory implication is significant: you must store all intermediate activations during the forward pass to use them in the backward pass. For a transformer with billions of parameters and long sequences, this activation memory often exceeds the memory needed for the model weights themselves. Gradient checkpointing trades computation for memory by recomputing activations instead of storing them.

### Automatic Differentiation {#sec-appendix-autodiff}

Modern frameworks use **reverse-mode automatic differentiation**, which computes gradients for all $N$ parameters in a single backward pass. This is why training (which needs gradients) has similar compute cost to inference (which does not): one forward pass plus one backward pass, where the backward pass has roughly the same cost as the forward pass.

Forward-mode autodiff, by contrast, would require $N$ passes to compute all gradients. This would make training prohibitively expensive.

---

## Sparse Matrix Formats {#sec-appendix-sparse}

::: {.callout-tip title="Why This Matters"}
Recommendation systems and pruned models contain mostly zeros. Storing and computing with dense matrices wastes both memory and compute. Understanding sparse formats lets you work efficiently with these common workloads.
:::

When most elements in a matrix are zero, specialized storage formats avoid wasting memory on zeros and enable computations that skip them entirely.

The **Compressed Sparse Row (CSR)** format uses three arrays to represent a sparse matrix efficiently, as shown in @fig-csr-format:

- `Values`: The non-zero elements, in row order
- `Col_Idx`: The column index of each non-zero element
- `Row_Ptr`: The starting position in `Values` for each row

::: {#fig-csr-format fig-env="figure" fig-pos="htb" fig-cap="**Compressed Sparse Row (CSR)**: The `Row_Ptr` array [0, 2, 3, 3, 4] indicates that Row 0 has 2 non-zero elements, Row 1 has 1, Row 2 is empty, and Row 3 has 1. This representation avoids storing the 12 zero values in this $4 \\times 4$ matrix."
```{.tikz}
\begin{tikzpicture}[font=\small\usefont{T1}{phv}{m}{n}, scale=1.0]
  % Define Project Colors
  \definecolor{BlueLine}{RGB}{66, 133, 244}
  \definecolor{BlueFill}{RGB}{232, 240, 254}
  \definecolor{RedLine}{RGB}{234, 67, 53}
  \definecolor{GreenLine}{RGB}{52, 168, 83}
  \definecolor{GrayLine}{RGB}{95, 99, 104}
  \definecolor{TextBlack}{RGB}{32, 33, 36}

  % Styles
  \tikzset{
    GridLine/.style={draw=GrayLine!30, line width=0.5pt},
    Cell/.style={font=\normalsize\bfseries, text=TextBlack},
    Label/.style={font=\footnotesize\usefont{T1}{phv}{m}{n}, text=TextBlack, anchor=west}
  }

  % Matrix Grid
  \begin{scope}
    \draw[GridLine, step=0.6cm] (0,0) grid (2.4, 2.4);
    % Values
    \node[Cell, text=BlueLine] at (0.3, 2.1) {5}; \node[Cell, text=BlueLine] at (1.5, 2.1) {8}; 
    \node[Cell, text=BlueLine] at (0.9, 1.5) {3};                          
    \node[Cell, text=BlueLine] at (2.1, 0.3) {6};                          
    \node[anchor=south, font=\small\bfseries, text=TextBlack] at (1.2, 2.5) {Sparse Matrix};
  \end{scope}

  % Arrow
  \draw[->, thick, GrayLine, bend left=20] (2.6, 2.0) to (3.3, 2.0);

  % CSR Arrays
  \begin{scope}[xshift=3.6cm, yshift=0.3cm]
      \node[Label] at (0, 1.8) {\texttt{Values}: \textcolor{BlueLine}{[5, 8, 3, 6]}};
      \node[Label] at (0, 1.2) {\texttt{Col\_Idx}: \textcolor{RedLine}{[0, 2, 1, 3]}};
      \node[Label] at (0, 0.6) {\texttt{Row\_Ptr}: \textcolor{GreenLine}{[0, 2, 3, 3, 4]}};
  \end{scope}
\end{tikzpicture}
```
:::

CSR enables fast row-wise access, which is the dominant access pattern in sparse matrix-vector multiplication. For a matrix with $N$ total elements and $K$ non-zeros, CSR uses $O(K)$ storage instead of $O(N)$, with corresponding speedups for sparse-aware computations.

---

## Computational Graphs and Optimization {#sec-appendix-graphs}

::: {.callout-tip title="Why This Matters"}
When you call `torch.compile()` or use TensorRT, the framework transforms your model into an optimized computational graph. Understanding what these transformations do helps you write code that compilers can optimize effectively and debug cases where optimization fails.
:::

ML compilers represent models as directed acyclic graphs (DAGs) where nodes are operations and edges are data dependencies. This representation enables hardware-independent optimizations.

The most impactful optimization is **operator fusion**. Consider a sequence like Conv followed by BatchNorm followed by ReLU. Without fusion, each operation reads its input from memory, computes, and writes its output back to memory. With fusion, all three operations execute as a single kernel: read input once, compute all three operations in registers, write final output once.

This fusion eliminates two round trips to HBM memory. Given the memory wall discussed earlier, the speedup from fusion often exceeds 2x for memory-bound operation sequences.

**Static Single Assignment (SSA)** form simplifies the dataflow analysis needed to identify fusion opportunities. In SSA, each variable is assigned exactly once, making data dependencies explicit and enabling safe code transformations.

---

## Summary {.unnumbered}

The concepts in this appendix share a common theme: **understanding the relationship between computation and data movement**. The Roofline Model quantifies this relationship. The memory hierarchy explains why it matters. Numerical formats and sparse representations reduce data movement. GEMM and computational graphs are the contexts where these trade-offs play out.

When you encounter a slow ML system, reach for these tools. Is the workload memory-bound or compute-bound? Where does the data live, and how often must it move? What precision does the application actually require? Answering these questions systematically, rather than guessing, is what separates effective ML systems engineering from trial and error.
