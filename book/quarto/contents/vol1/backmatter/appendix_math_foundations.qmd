# Mathematical Foundations {#sec-appendix-math-foundations}

This appendix provides the mathematical tools you will need to reason about ML system performance. Rather than an exhaustive reference, think of this as a practitioner's toolkit: the concepts that come up again and again when debugging slow training jobs, optimizing inference latency, or choosing between hardware configurations.

## Quick Reference {.unnumbered}

Before diving in, here is a map of what this appendix covers and where each concept appears in the main text.

+-------------------------------+--------------------------------------------+-------------------------------------------+
| **Topic**                     | **What It Helps You Do**                   | **Where It Appears**                      |
+:==============================+:===========================================+:==========================================+
| **Roofline Model**            | Determine if a workload is memory-bound    | @sec-ai-acceleration, @sec-ai-training,   |
|                               | or compute-bound                           | @sec-serving                              |
+-------------------------------+--------------------------------------------+-------------------------------------------+
| **Amdahl's Law**              | Calculate maximum speedup from             | @sec-ai-acceleration, @sec-ai-training    |
|                               | parallelization                            |                                           |
+-------------------------------+--------------------------------------------+-------------------------------------------+
| **Gustafson's Law**           | Understand weak scaling for large models   | @sec-ai-training                          |
+-------------------------------+--------------------------------------------+-------------------------------------------+
| **Little's Law**              | Size serving infrastructure for target QPS | @sec-serving                              |
+-------------------------------+--------------------------------------------+-------------------------------------------+
| **Memory Hierarchy**          | Optimize data movement and cache usage     | @sec-ai-acceleration, @sec-model-compression   |
+-------------------------------+--------------------------------------------+-------------------------------------------+
| **Numerical Formats**         | Choose precision for training vs inference | @sec-model-compression, @sec-serving    |
+-------------------------------+--------------------------------------------+-------------------------------------------+
| **GEMM Operations**           | Understand the core computation in         | @sec-dl-primer, @sec-ai-acceleration      |
|                               | neural networks                            |                                           |
+-------------------------------+--------------------------------------------+-------------------------------------------+
| **Backpropagation**           | Debug gradient issues and memory usage     | @sec-dl-primer, @sec-ai-training          |
+-------------------------------+--------------------------------------------+-------------------------------------------+
| **Sparse Formats (CSR)**      | Work with recommendation systems and       | @sec-model-compression                  |
|                               | pruned models                              |                                           |
+-------------------------------+--------------------------------------------+-------------------------------------------+
| **Computational Graphs**      | Understand compiler optimizations          | @sec-ai-frameworks, @sec-ai-acceleration  |
+-------------------------------+--------------------------------------------+-------------------------------------------+

: **Appendix Quick Reference**: Each mathematical concept in this appendix, what engineering decisions it informs, and where it appears in the main text. {#tbl-appendix-overview}

---

## Performance Analysis Models {#sec-appendix-performance-models}

::: {.callout-tip title="Why This Matters"}
You have trained a model that achieves good accuracy, but inference takes 200ms when your SLA requires 50ms. Where do you start? Performance analysis models give you a systematic way to diagnose whether you are limited by computation, memory bandwidth, or something else entirely. Without these tools, optimization becomes guesswork.
:::

The models in this section form the foundation of quantitative systems thinking. Master them, and you will be able to look at almost any ML workload and predict where the bottlenecks lie.

### The Roofline Model {#sec-appendix-roofline}

The Roofline Model [@williams2009roofline] answers a deceptively simple question: *how fast can this workload possibly run on this hardware?* The answer depends on whether you run out of compute or memory bandwidth first.

Every operation has an **arithmetic intensity**: the ratio of computations performed to bytes moved from memory. Matrix multiplication has high arithmetic intensity because you can reuse each loaded element many times. Element-wise operations like ReLU have low intensity because you load a number, do one operation, and write it back.

::: {#fig-roofline fig-env="figure" fig-pos="htb" fig-cap="**The Roofline Model**: Performance ceiling for a hypothetical accelerator. The sloped line represents memory bandwidth limits; the horizontal line represents peak compute. Every workload can be plotted on this diagram to determine its optimization strategy." fig-alt="A plot with arithmetic intensity on the x-axis and performance on the y-axis. Two lines form a roofline shape: a diagonal line rising from the origin labeled Memory Bound, and a horizontal line labeled Compute Bound. They meet at the Ridge Point. Two workloads are plotted: Workload A on the diagonal slope, Workload B on the horizontal plateau."
```{.tikz}
\begin{tikzpicture}[font=\small\usefont{T1}{phv}{m}{n}, scale=1.0]


  % Styles
  \tikzset{
    Axis/.style={line width=1.0pt, draw=GrayLine, ->, >=stealth},
    Guide/.style={dashed, draw=GrayLine!60, line width=0.6pt},
    Label/.style={text=TextBlack, align=center, font=\footnotesize\usefont{T1}{phv}{m}{n}},
    Dot/.style={circle, fill=#1, draw=white, line width=0.5pt, minimum size=5pt, inner sep=0pt}
  }

  % Grid
  \draw[step=0.5, gray!10, very thin] (0,0) grid (6,4);

  % Axes
  \draw[Axis] (0,0) -- (6,0) node[right, text=TextBlack] {Arithmetic Intensity (Ops/Byte)};
  \draw[Axis] (0,0) -- (0,4.2) node[above, text=TextBlack] {Performance (FLOP/s)};

  % Roofline
  % Memory Bound Line
  \draw[BlueLine, line width=2pt] (0,0) -- (3,3);
  % Compute Bound Line
  \draw[RedLine, line width=2pt] (3,3) -- (5.8,3);

  % Region Labels
  \node[Label, text=BlueLine, rotate=45, anchor=south, yshift=2pt] at (1.5, 1.5) {\textbf{Memory Bound}};
  \node[Label, text=RedLine, anchor=south, yshift=2pt] at (4.4, 3) {\textbf{Compute Bound}};

  % Ridge Point
  \node[Dot=TextBlack] at (3,3) {};
  \draw[Guide] (3,0) -- (3,3);
  \node[below, font=\scriptsize\usefont{T1}{phv}{m}{n}, text=TextBlack] at (3,0) {Ridge Point};

  % Workloads
  % Workload A (Memory Bound)
  \node[Dot=YellowLine] (WA) at (1.5, 1.5) {};
  \node[Label, right, xshift=4pt, align=left] at (1.5, 1.3) {Workload A\\(Mem Bound)};
  \draw[->, gray, thin, shorten >=2pt] (1.6, 1.3) -- (WA);

  % Workload B (Compute Bound)
  \node[Dot=GreenLine] (WB) at (4.5, 3) {};
  \node[Label, below, yshift=-4pt] at (4.5, 3) {Workload B (Compute Bound)};

\end{tikzpicture}
```
:::

**How to read a roofline plot.** @fig-roofline shows the roofline for a hypothetical accelerator. The key to reading this diagram is the **ridge point**, the corner where the two lines meet. Start there and work outward:

1. **Find the ridge point.** This tells you the hardware's balance between compute and memory. For this accelerator, the ridge point occurs at a specific arithmetic intensity (marked on the x-axis). Calculate yours using @eq-ridge-point below.

2. **Plot your workload's arithmetic intensity.** Compute @eq-arithmetic-intensity for your operation. Draw a vertical line up from that x-value until it hits the roofline.

3. **Read off the regime.** If your vertical line hits the sloped portion, you are **memory-bound**. If it hits the flat portion, you are **compute-bound**. This distinction determines your entire optimization strategy.

Now examine where Workload A sits on the plot. It falls on the sloped portion, meaning it operates in the memory-bound regime. What does this mean practically? Every byte of additional memory bandwidth would directly translate to more FLOP/s. Buying a faster GPU would not help much because the compute units are already waiting for data. Instead, you should focus on data reuse: can you tile the computation to keep data in cache? Can you fuse this operation with adjacent operations to avoid writing intermediate results to HBM?

Contrast this with Workload B on the flat portion. This workload is already hitting peak compute. The memory system is feeding data fast enough; the bottleneck is raw arithmetic throughput. Adding more memory bandwidth would not help. The only paths to better performance are: (1) a faster processor, (2) algorithmic improvements that reduce total FLOPs, or (3) mixed-precision arithmetic that doubles throughput.

The power of this diagram is that once you learn to read it, you can interpret ANY roofline plot for any hardware. The specific numbers change (an A100 has a different ridge point than an H100), but the interpretation process is identical.

The key equations formalize these observations. Arithmetic intensity is:

$$ \text{Arithmetic Intensity} = \frac{\text{FLOPs}}{\text{Bytes Accessed}} $$ {#eq-arithmetic-intensity}

In the **memory-bound region** (the sloped portion), performance scales with bandwidth:

$$ \text{Attainable Performance} = \text{Bandwidth} \times \text{Arithmetic Intensity} $$ {#eq-memory-bound}

In the **compute-bound region** (the flat portion), you hit the hardware ceiling:

$$ \text{Attainable Performance} = \text{Peak FLOP/s} $$ {#eq-compute-bound}

The transition point, called the **ridge point**, characterizes the hardware's balance between compute and memory:

$$ \text{Ridge Point} = \frac{\text{Peak FLOP/s}}{\text{Memory Bandwidth}} $$ {#eq-ridge-point}

#### A Concrete Example

Consider an NVIDIA A100 GPU with FP16 Tensor Core performance of 312 TFLOP/s and HBM2e bandwidth of 2.0 TB/s. The ridge point is $312 / 2.0 = 156$ FLOP/byte.

Now compare two common operations:

**GEMM (Matrix Multiplication)**: For two $4096 \times 4096$ matrices, arithmetic intensity is approximately $1365$ FLOP/byte. Since $1365 > 156$, this operation is compute-bound. You are using the hardware efficiently.

**ReLU (Element-wise)**: For a $4096 \times 4096$ tensor, intensity is approximately $0.25$ op/byte. Since $0.25 \ll 156$, this operation is severely memory-bound, achieving only about $0.16\%$ of peak TFLOP/s. The hardware is mostly waiting for data.

This explains why modern frameworks fuse operations: combining ReLU with the preceding MatMul avoids writing intermediate results to memory, effectively increasing arithmetic intensity.

### Amdahl's Law and Gustafson's Law {#sec-appendix-scaling-laws}

You have access to a cluster with 64 GPUs. How much faster will training be compared to a single GPU? The answer is almost never "64 times faster," and these two laws explain why.

**Amdahl's Law** [@amdahl1967validity] captures the fundamental limit of parallelization. If a fraction $p$ of your workload can be parallelized, the maximum speedup with $n$ processors is:

$$ \text{Speedup}(n) = \frac{1}{(1-p) + \frac{p}{n}} $$ {#eq-amdahl}

As $n \to \infty$, speedup approaches $\frac{1}{1-p}$. If 10% of your training iteration is serial (data loading on CPU, gradient aggregation), the maximum possible speedup is 10x. Adding more GPUs beyond that point yields diminishing returns.

This sounds pessimistic, but **Gustafson's Law** [@gustafson1988reevaluating] offers a more optimistic view for ML workloads. In practice, when you get more hardware, you often train larger models or use larger batches rather than just trying to finish faster. Under this "weak scaling" model, speedup becomes:

$$ \text{Speedup}(n) = (1-p) + p \times n $$ {#eq-gustafson}

This linear scaling better describes large-scale ML training, where increased resources enable training models that would otherwise be impossible, not just faster training of the same model.

### Little's Law {#sec-appendix-little}

Shifting from training to serving, **Little's Law** [@little1961proof] is the essential tool for capacity planning. It relates the average number of items in a system ($L$), the arrival rate ($\lambda$), and the average wait time ($W$):

$$ L = \lambda \times W $$ {#eq-little}

For ML serving: to sustain 1,000 queries per second (QPS) with 20ms average latency, your system must handle $1000 \times 0.02 = 20$ concurrent requests. This tells you how many inference workers you need and helps you right-size your infrastructure.

---

## Computer Architecture Essentials {#sec-appendix-architecture}

::: {.callout-tip title="Why This Matters"}
Your model runs 100x slower on CPU than GPU, but why? Understanding the memory hierarchy explains not just why GPUs are faster, but also why some optimizations (like batching) help dramatically while others (like buying more RAM) barely matter. The concepts here underpin every hardware decision you will make.
:::

The previous section gave you tools to analyze workloads. This section explains the hardware constraints those analyses reveal.

### The Memory Hierarchy {#sec-appendix-memory-hierarchy}

Computer systems use a hierarchy of memory technologies because no single technology provides both high capacity and low latency. Understanding this hierarchy explains why your model's memory access patterns matter as much as its computational complexity.

::: {#fig-memory-hierarchy fig-env="figure" fig-pos="htb" fig-cap="**The Memory Hierarchy**: Each level trades off speed against capacity. The performance of your ML code depends on which level your working data resides in during the critical inner loops." fig-alt="A pyramid diagram with four levels from top to bottom: Registers (smallest, fastest), L1/L2/L3 Cache, HBM/DRAM, and Storage/SSD/Disk (largest, slowest). Arrows on the left indicate increasing capacity downward; arrows on the right indicate increasing speed upward."
```{.tikz}
\begin{tikzpicture}[font=\small\usefont{T1}{phv}{m}{n}, scale=1.0]


  % Styles
  \tikzset{
    Level/.style={draw=black!80, line width=0.8pt, align=center, text=TextBlack},
    LabelAxis/.style={text=TextBlack!60, font=\footnotesize\usefont{T1}{phv}{m}{n}, align=center}
  }

  % Pyramid Levels
  % Registers
  \filldraw[fill=RedFill, draw=RedLine, line width=1pt] (0, 5.2) -- (-1.2, 4.2) -- (1.2, 4.2) -- cycle;
  \node[text=RedLine] at (0, 4.55) {\textbf{Registers}};

  % Cache
  \filldraw[fill=YellowFill, draw=YellowLine, line width=1pt] (-1.2, 4.2) -- (-2.4, 2.8) -- (2.4, 2.8) -- (1.2, 4.2) -- cycle;
  \node[text=YellowLine!80!black] at (0, 3.5) {\textbf{L1 / L2 / L3 Cache}};

  % HBM
  \filldraw[fill=BlueFill, draw=BlueLine, line width=1pt] (-2.4, 2.8) -- (-3.6, 1.4) -- (3.6, 1.4) -- (2.4, 2.8) -- cycle;
  \node[text=BlueLine] at (0, 2.1) {\textbf{HBM / DRAM}};

  % Storage
  \filldraw[fill=GreenFill, draw=GreenLine, line width=1pt] (-3.6, 1.4) -- (-4.8, 0) -- (4.8, 0) -- (3.6, 1.4) -- cycle;
  \node[text=GreenLine] at (0, 0.7) {\textbf{Storage (SSD / Disk)}};

  % Side Arrows
  \draw[->, ultra thick, gray!40] (5.5, 0) -- (5.5, 5.2) node[midway, right, LabelAxis, text=gray] {Faster Speed\\Lower Latency};
  \draw[->, ultra thick, gray!40] (-5.5, 5.2) -- (-5.5, 0) node[midway, left, LabelAxis, text=gray] {Larger Capacity\\Lower Cost};
\end{tikzpicture}
```
:::

**How to use this mental model.** @fig-memory-hierarchy is not just a diagram to memorize; it is a tool for reasoning about code performance. When analyzing any ML workload, ask: *where does my data live during the inner loop?*

Consider a concrete scenario. You are writing a custom attention kernel, and your inner loop computes dot products between query and key vectors. If those vectors fit in registers, each dot product takes about 1 nanosecond. If they must be fetched from HBM on every iteration, each access takes roughly 100 nanoseconds. That is a 100x difference in memory access time alone.

The latency numbers to internalize:

| Level | Latency | Relative to Registers | Example Capacity |
|-------|---------|----------------------|------------------|
| Registers | ~1 ns | 1x | ~20 MB (A100 total) |
| L1 Cache | ~4 ns | 4x | 128 KB per SM |
| L2 Cache | ~20 ns | 20x | 40 MB (A100) |
| HBM | ~100 ns | 100x | 80 GB (A100) |
| SSD | ~100 us | 100,000x | TB scale |

Now apply this to real code patterns:

**Data loading**: If your training loop reads each batch from SSD, you pay 100 microseconds per access. At 1,000 batches per epoch, that is 100 milliseconds of I/O per epoch, which may dominate training time for small models. This is why data loaders prefetch batches into DRAM while the GPU computes.

**Activation memory**: During backpropagation, you need intermediate activations stored during the forward pass. If these exceed HBM capacity, they spill to CPU memory or SSD. A single access to CPU memory over PCIe takes roughly 1 microsecond. If your backward pass touches millions of activations, the slowdown is catastrophic. This is why gradient checkpointing exists: it trades compute (recomputing activations) for memory (not storing them).

**Weight updates**: During training, you read weights from HBM, compute gradients, and write updated weights back. If you can keep the optimizer state in HBM rather than swapping to CPU, you avoid the 10x latency penalty of crossing the PCIe bus.

The design principle this diagram teaches: **every technique that keeps data higher in the pyramid directly improves performance**. Tiling keeps working sets in cache. Operator fusion avoids writing intermediates to HBM. Batching amortizes the cost of loading weights from HBM across multiple inputs. Once you internalize this hierarchy, optimization strategies become intuitive rather than mysterious.

### Bandwidth vs. Latency {#sec-appendix-bandwidth-latency}

Bandwidth (throughput) and latency (delay) are distinct constraints that matter in different situations. Total transfer time follows:

$$ T = L + \frac{B}{BW} $$

where $L$ is latency, $B$ is the data size, and $BW$ is bandwidth.

For small transfers (e.g., KV cache lookups in autoregressive generation), latency dominates. For large transfers (e.g., loading model checkpoints), bandwidth dominates. Understanding which regime you are in determines whether you should optimize for lower latency or higher throughput.

---

## Numerical Representations {#sec-appendix-numerical}

::: {.callout-tip title="Why This Matters"}
Your production model runs at 50 QPS in FP32 but your target is 200 QPS. Switching to INT8 could get you there, but will accuracy suffer? Understanding numerical formats lets you make this trade-off quantitatively rather than hoping for the best.
:::

Neural networks are remarkably tolerant of reduced numerical precision. This section explains the formats you will encounter and their trade-offs.

### Floating-Point Format Comparison {#sec-appendix-floating-point}

The IEEE 754 standard and its AI-specific derivatives define different trade-offs between dynamic range (the span of representable values) and precision (how finely you can represent values within that range).

+----------+-------+----------+----------+-----------------+--------------------------------------------+
| **Format** | **Bits** | **Exponent** | **Mantissa** | **Dynamic Range** | **Typical Use Case**                     |
+:=========+:======+:=========+:=========+:================+:===========================================+
| **FP32** | 32    | 8        | 23       | ~$10^{-38}$ to  | Training (full precision), reference       |
|          |       |          |          | $10^{38}$       | inference                                  |
+----------+-------+----------+----------+-----------------+--------------------------------------------+
| **FP16** | 16    | 5        | 10       | ~$10^{-5}$ to   | Training with loss scaling, inference      |
|          |       |          |          | $6.5 \times     |                                            |
|          |       |          |          | 10^{4}$         |                                            |
+----------+-------+----------+----------+-----------------+--------------------------------------------+
| **BF16** | 16    | 8        | 7        | Same as FP32    | Training (preferred), avoids loss scaling  |
+----------+-------+----------+----------+-----------------+--------------------------------------------+
| **FP8**  | 8     | 4 or 5   | 3 or 2   | Varies          | Inference on newest hardware (H100+)       |
+----------+-------+----------+----------+-----------------+--------------------------------------------+
| **INT8** | 8     | N/A      | N/A      | -128 to 127     | Inference after quantization               |
+----------+-------+----------+----------+-----------------+--------------------------------------------+

: **Numerical Format Comparison**: Each format trades off precision, dynamic range, memory footprint, and compute throughput. BF16 has emerged as the preferred training format because it matches FP32's range while using half the memory. {#tbl-numerical-formats}

**Brain Float 16 (BF16)** deserves special attention. It matches FP32's 8-bit exponent (preserving dynamic range) while truncating the mantissa to 7 bits. This avoids the gradient underflow problems that plague FP16 training, eliminating the need for complex loss scaling. Most modern training uses BF16 for this reason.

### Integer Quantization {#sec-appendix-quantization}

Quantization maps continuous floating-point values to discrete integers, typically INT8. The key challenge is choosing how to map the floating-point range to integers.

**Symmetric quantization** centers the mapping at zero:
$$ x_{int} = \text{round}\left(\frac{x}{\alpha} \times 127\right) $$

where $\alpha$ is the scale factor (typically the maximum absolute value). This works well for weight distributions centered around zero.

**Asymmetric quantization** handles distributions that are not centered (common after ReLU, which produces only non-negative values) by adding a zero-point offset:
$$ x_{int} = \text{round}\left(\frac{x - z}{\alpha} \times 255\right) $$

The choice between symmetric and asymmetric quantization depends on your tensor's distribution and has measurable accuracy implications.

---

## Linear Algebra for Neural Networks {#sec-appendix-linear-algebra}

::: {.callout-tip title="Why This Matters"}
Every neural network, regardless of architecture, spends most of its time doing matrix multiplication. Understanding GEMM performance characteristics explains why batch size affects throughput, why certain layer dimensions are "better" than others, and how to interpret profiler output.
:::

### Tensor Operations and Notation {#sec-appendix-tensors}

We use **Einstein summation** notation throughout this book because it makes complex operations explicit. Matrix multiplication $C = AB$ becomes:

$$ C_{ij} = \sum_k A_{ik} B_{kj} $$

Or in einsum notation: `ik,kj->ij`. This notation extends naturally to the multi-dimensional operations in attention mechanisms. For example, batched multi-head attention is `bhid,bhjd->bhij` (batch, head, sequence indices).

### General Matrix Multiply (GEMM) {#sec-appendix-gemm}

GEMM is the computational workhorse of deep learning. For matrices of size $M \times K$ and $K \times N$, GEMM performs $2MNK$ floating-point operations (multiply-accumulate counts as two operations).

The arithmetic intensity of GEMM scales linearly with matrix dimension. For square $n \times n$ matrices, intensity is approximately $n/3$ FLOP/byte. This explains several important phenomena:

- **Larger batches improve efficiency**: Batching increases the effective matrix dimensions, pushing workloads toward the compute-bound region of the roofline.
- **Power-of-two dimensions help**: Hardware tensor cores are optimized for specific tile sizes (typically 16x16 or 32x32). Dimensions that align with these sizes avoid padding overhead.
- **Small matrices are inefficient**: A 64x64 GEMM may achieve only 10% of peak throughput because it cannot fully utilize the hardware.

### Memory Layouts and Performance {#sec-appendix-memory-layouts}

Data layout in memory (row-major vs. column-major) directly affects cache efficiency. When iterating over a matrix, accessing contiguous memory locations is dramatically faster than strided access. The difference can be 10x to 100x in effective bandwidth.

A common optimization pattern: transpose tensors once before repeated operations to ensure contiguous access in the hot loop. The one-time transpose cost is amortized across many subsequent operations.

---

## Calculus for Backpropagation {#sec-appendix-calculus}

::: {.callout-tip title="Why This Matters"}
When training fails (loss goes to NaN, gradients explode, memory runs out), understanding what backpropagation actually does helps you diagnose the problem. This section gives you the mental model to reason about gradient flow and memory usage during training.
:::

### The Chain Rule in Deep Networks {#sec-appendix-chain-rule}

For a composed function $y = f(g(x))$, the derivative is:

$$ \frac{dy}{dx} = \frac{dy}{dg} \cdot \frac{dg}{dx} $$

In deep networks, this chain extends across many layers. Each layer contributes a local Jacobian that multiplies during the backward pass. When these multiplied terms consistently exceed 1.0, gradients explode; when they consistently fall below 1.0, gradients vanish.

### The Backpropagation Algorithm {#sec-appendix-backprop}

Backpropagation implements the chain rule efficiently through two passes: forward to compute outputs, backward to compute gradients.

::: {#fig-backprop-graph fig-env="figure" fig-pos="htb" fig-cap="**Backpropagation Computational Graph**: A two-layer network showing the forward pass (black arrows) and backward pass (red dashed arrows). Each node caches values during the forward pass that are reused during the backward pass." fig-alt="A computational graph with four nodes labeled x, h, y, and L connected left to right. Solid black arrows show the forward pass with weights W1 and W2. Dashed red arrows curve backward showing gradient flow with partial derivative notation."
```{.tikz}
\begin{tikzpicture}[font=\small\usefont{T1}{phv}{m}{n}, node distance=3cm, auto, >=stealth, thick]


  % Styles
  \tikzset{
    Node/.style={circle, draw=BlueLine, fill=BlueFill, line width=0.8pt, minimum size=0.9cm, text=TextBlack},
    Edge/.style={->, draw=GrayLine, line width=0.8pt},
    BackEdge/.style={->, dashed, draw=RedLine, line width=0.8pt, bend right=50},
    Label/.style={font=\footnotesize\usefont{T1}{phv}{m}{n}, text=TextBlack}
  }

  % Nodes
  \node[Node] (x) {x};
  \node[Node, right of=x] (h) {h};
  \node[Node, right of=h] (y) {y};
  \node[Node, right of=y, fill=RedLine!10, draw=RedLine] (L) {L};

  % Forward Pass
  \draw[Edge] (x) -- node[below, Label] {$W_1$} (h);
  \draw[Edge] (h) -- node[below, Label] {$W_2$} (y);
  \draw[Edge] (y) -- node[below, Label] {Loss} (L);

  % Backward Pass
  \draw[BackEdge] (L) to node[above, Label, text=RedLine] {$\frac{\partial L}{\partial y}$} (y);
  \draw[BackEdge] (y) to node[above, Label, text=RedLine] {$\frac{\partial L}{\partial W_2}$} (h);
  \draw[BackEdge] (h) to node[above, Label, text=RedLine] {$\frac{\partial L}{\partial W_1}$} (x);
\end{tikzpicture}
```
:::

**How to trace the computation.** @fig-backprop-graph shows a simple two-layer network. Practice tracing both passes to understand what happens during training:

**Forward pass (black arrows, left to right)**: Start at $x$, your input. Multiply by $W_1$ to get hidden activation $h$. Cache $h$ because you will need it later. Multiply $h$ by $W_2$ to get output $y$. Cache $y$. Compare $y$ to the target label to compute loss $L$.

At this point, you have computed the loss and your memory contains: the input $x$, the cached activation $h$, the cached output $y$, and the loss $L$. For a large model, these cached activations dominate memory usage.

**Backward pass (red arrows, right to left)**: Now trace backward from $L$. The loss function tells you $\frac{\partial L}{\partial y}$, the gradient of loss with respect to your prediction. This is where the error signal enters the network.

To compute $\frac{\partial L}{\partial W_2}$, you need to know how $W_2$ affected $y$. That requires the cached value of $h$. The chain rule gives you: $\frac{\partial L}{\partial W_2} = \frac{\partial L}{\partial y} \cdot h^T$.

To continue backward to $W_1$, you need $\frac{\partial L}{\partial h}$, then multiply by the cached input $x$. Each step backward requires the activations cached during the forward pass.

**Why this matters for memory.** The cached activations at each layer cannot be freed until the backward pass reaches that layer. For a 100-layer network, you store 100 layers of activations simultaneously. For a transformer processing a 4096-token sequence with hidden dimension 4096, a single layer's activations consume $4096 \times 4096 \times 4$ bytes $\approx 67$ MB in FP32. Multiply by 100 layers: 6.7 GB just for activations, often exceeding the memory for weights themselves.

**Gradient checkpointing** addresses this by not caching all activations. Instead, you cache every $k$th layer's activations and recompute the intermediate ones during the backward pass. This trades compute (recomputing activations) for memory (not storing them). For a 100-layer network with checkpointing every 10 layers, you store only 10 activations instead of 100, reducing activation memory by 10x at the cost of roughly 33% more forward-pass compute.

### Automatic Differentiation {#sec-appendix-autodiff}

Modern frameworks use **reverse-mode automatic differentiation**, which computes gradients for all $N$ parameters in a single backward pass. This is why training (which needs gradients) has similar compute cost to inference (which does not): one forward pass plus one backward pass, where the backward pass has roughly the same cost as the forward pass.

Forward-mode autodiff, by contrast, would require $N$ passes to compute all gradients. This would make training prohibitively expensive.

---

## Sparse Matrix Formats {#sec-appendix-sparse}

::: {.callout-tip title="Why This Matters"}
Recommendation systems and pruned models contain mostly zeros. Storing and computing with dense matrices wastes both memory and compute. Understanding sparse formats lets you work efficiently with these common workloads.
:::

When most elements in a matrix are zero, specialized storage formats avoid wasting memory on zeros and enable computations that skip them entirely.

The **Compressed Sparse Row (CSR)** format uses three arrays:

- `Values`: The non-zero elements, stored in row order
- `Col_Idx`: The column index of each non-zero element
- `Row_Ptr`: The starting position in `Values` for each row (length = num_rows + 1)

::: {#fig-csr-format fig-env="figure" fig-pos="htb" fig-cap="**Compressed Sparse Row (CSR)**: A $4 \\times 4$ matrix with only 4 non-zero values. CSR stores these 4 values plus indexing overhead, rather than all 16 elements." fig-alt="A 4x4 grid showing a sparse matrix with values 5, 8, 3, and 6 in specific cells, most cells empty. An arrow points to three arrays: Values containing the four numbers, Col_Idx containing their column positions, and Row_Ptr containing row boundary indices."
```{.tikz}
\begin{tikzpicture}[font=\small\usefont{T1}{phv}{m}{n}, scale=1.0]


  % Styles
  \tikzset{
    GridLine/.style={draw=GrayLine!30, line width=0.5pt},
    Cell/.style={font=\normalsize\bfseries, text=TextBlack},
    Label/.style={font=\footnotesize\usefont{T1}{phv}{m}{n}, text=TextBlack, anchor=west}
  }

  % Matrix Grid
  \begin{scope}
    \draw[GridLine, step=0.6cm] (0,0) grid (2.4, 2.4);
    % Values
    \node[Cell, text=BlueLine] at (0.3, 2.1) {5}; \node[Cell, text=BlueLine] at (1.5, 2.1) {8};
    \node[Cell, text=BlueLine] at (0.9, 1.5) {3};
    \node[Cell, text=BlueLine] at (2.1, 0.3) {6};
    \node[anchor=south, font=\small\bfseries, text=TextBlack] at (1.2, 2.5) {Sparse Matrix};
  \end{scope}

  % Arrow
  \draw[->, thick, GrayLine, bend left=20] (2.6, 2.0) to (3.3, 2.0);

  % CSR Arrays
  \begin{scope}[xshift=3.6cm, yshift=0.3cm]
      \node[Label] at (0, 1.8) {\texttt{Values}: \textcolor{BlueLine}{[5, 8, 3, 6]}};
      \node[Label] at (0, 1.2) {\texttt{Col\_Idx}: \textcolor{RedLine}{[0, 2, 1, 3]}};
      \node[Label] at (0, 0.6) {\texttt{Row\_Ptr}: \textcolor{GreenLine}{[0, 2, 3, 3, 4]}};
  \end{scope}
\end{tikzpicture}
```
:::

**How to read a CSR representation.** @fig-csr-format shows a $4 \times 4$ sparse matrix and its CSR encoding. The skill you need is reconstructing the matrix from the three arrays. Work through this step by step:

**Finding all values in a specific row.** Suppose you want row 0. Look at `Row_Ptr[0] = 0` and `Row_Ptr[1] = 2`. This tells you row 0's non-zeros are at indices 0 through 1 (inclusive) in the `Values` array. Check `Values[0:2] = [5, 8]` and `Col_Idx[0:2] = [0, 2]`. So row 0 has value 5 in column 0 and value 8 in column 2.

**Finding a specific element.** To find the element at row 1, column 1: First, `Row_Ptr[1] = 2` and `Row_Ptr[2] = 3`, so row 1 has one non-zero at index 2 in `Values`. Check `Col_Idx[2] = 1`. Since we are looking for column 1 and this matches, the value is `Values[2] = 3`. If the column did not appear in `Col_Idx` for this row, the element would be zero.

**Detecting empty rows.** Look at row 2: `Row_Ptr[2] = 3` and `Row_Ptr[3] = 3`. Since these are equal, row 2 has no non-zeros. The entire row is zeros.

**Why this matters for ML.** Recommendation systems use embedding tables where each user or item maps to a sparse high-dimensional vector. A typical embedding table might have millions of users but each user only interacts with hundreds of items. Storing this as a dense matrix wastes 99.99% of memory on zeros. CSR stores only the non-zero interactions.

Pruned neural networks exhibit similar sparsity. After pruning 90% of weights, you have 10x fewer values to store and 10x fewer multiplications to perform, but only if your storage format and compute kernels exploit sparsity. Dense GEMM on a 90%-sparse matrix does all the work for zeros that contribute nothing to the result.

For a matrix with $N$ total elements and $K$ non-zeros, CSR uses $O(K)$ storage instead of $O(N)$. The crossover where CSR becomes more efficient than dense storage depends on the sparsity pattern, but typically occurs around 90% sparsity.

---

## Computational Graphs and Optimization {#sec-appendix-graphs}

::: {.callout-tip title="Why This Matters"}
When you call `torch.compile()` or use TensorRT, the framework transforms your model into an optimized computational graph. Understanding what these transformations do helps you write code that compilers can optimize effectively and debug cases where optimization fails.
:::

ML compilers represent models as directed acyclic graphs (DAGs) where nodes are operations and edges are data dependencies. This representation enables hardware-independent optimizations.

The most impactful optimization is **operator fusion**. Consider a sequence like Conv followed by BatchNorm followed by ReLU. Without fusion, each operation reads its input from memory, computes, and writes its output back to memory. With fusion, all three operations execute as a single kernel: read input once, compute all three operations in registers, write final output once.

This fusion eliminates two round trips to HBM memory. Given the memory wall discussed earlier, the speedup from fusion often exceeds 2x for memory-bound operation sequences.

**Static Single Assignment (SSA)** form simplifies the dataflow analysis needed to identify fusion opportunities. In SSA, each variable is assigned exactly once, making data dependencies explicit and enabling safe code transformations.

---

## Summary {.unnumbered}

The concepts in this appendix share a common theme: **understanding the relationship between computation and data movement**. The Roofline Model quantifies this relationship. The memory hierarchy explains why it matters. Numerical formats and sparse representations reduce data movement. GEMM and computational graphs are the contexts where these trade-offs play out.

When you encounter a slow ML system, reach for these tools. Is the workload memory-bound or compute-bound? Where does the data live, and how often must it move? What precision does the application actually require? Answering these questions systematically, rather than guessing, is what separates effective ML systems engineering from trial and error.
