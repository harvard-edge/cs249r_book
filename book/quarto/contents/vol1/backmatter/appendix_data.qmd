---
title: "Data Foundations"
id: sec-data-foundations
---

This appendix covers the principles of data engineering and statistical monitoring that keep ML systems healthy. From storage formats that determine I/O throughput to drift metrics that detect silent failures, these foundations connect directly to the data pipelines in @sec-data-engineering-ml and the operational monitoring in @sec-machine-learning-operations-mlops.

## Data Engineering Foundations {#sec-system-foundations-data-engineering-foundations-9ba2}

Understanding hardware constraints is only half the battle; we must also shape our data to fit them. **Data Engineering** applies the principles of the memory hierarchy to storage formats and pipeline design, ensuring that the GPU never starves.

::: {.callout-perspective title="Why This Matters"}
Your training job is stalling because the GPU is waiting for data. Is the bottleneck your disk speed, your file format, or your row-based parsing logic? Data engineering is the practice of shaping data so it flows efficiently through the physical constraints of your system.
:::

### Row vs. Columnar Formats {#sec-system-foundations-row-vs-columnar-formats-1f97}

The choice of file format determines the "physics" of how your data is read.

**Row-Oriented (CSV, JSON)**: Data is stored record-by-record. To read just the `age` column, you must scan every byte of every row. This is efficient for *writing* (appending a log) but inefficient for *analytics* (training on specific features).

**Column-Oriented (Parquet, Arrow)**: Data is stored column-by-column. To read the `age` column, the disk head seeks to that column's block and reads it sequentially. This enables **projection pushdown** (reading only the bytes you need) and **vectorized processing** (SIMD operations on columns). @fig-row-vs-col contrasts these two storage layouts.

::: {#fig-row-vs-col fig-env="figure" fig-pos="htb" fig-cap="**Storage Layouts**: Row-oriented formats pack data together by record (good for transactions). Column-oriented formats pack data by feature (good for analytics)." fig-alt="Diagram contrasting Row Store vs Column Store. Row store shows Record 1 [ID, Name, Age] followed by Record 2. Column store shows Column 1 [ID1, ID2...] followed by Column 2 [Name1, Name2...]."}
```{.tikz}
\begin{tikzpicture}[font=\small\usefont{T1}{phv}{m}{n}, scale=1.0]
  \tikzset{
    DataBlock/.style={draw=GrayLine, line width=0.8pt, minimum height=0.6cm, minimum width=1.2cm, align=center, font=\scriptsize\usefont{T1}{phv}{m}{n}},
    Label/.style={text=TextBlack, font=\small\bfseries\usefont{T1}{phv}{m}{n}}
  }
...

  % Row Store
  \node[Label] at (0, 3.5) {Row-Oriented (CSV)};
  \node[DataBlock, fill=RedFill] (r1a) at (0, 2.8) {ID: 1};
  \node[DataBlock, fill=BlueFill, right=0pt of r1a] (r1b) {Age: 25};
  \node[DataBlock, fill=GreenFill, right=0pt of r1b] (r1c) {Loc: US};

  \node[DataBlock, fill=RedFill, right=0.2cm of r1c] (r2a) {ID: 2};
  \node[DataBlock, fill=BlueFill, right=0pt of r2a] (r2b) {Age: 30};
  \node[DataBlock, fill=GreenFill, right=0pt of r2b] (r2c) {Loc: UK};

  % Column Store
  \node[Label] at (0, 1.5) {Column-Oriented (Parquet)};
  \node[DataBlock, fill=RedFill] (c1a) at (0, 0.8) {ID: 1};
  \node[DataBlock, fill=RedFill, right=0pt of c1a] (c1b) {ID: 2};

  \node[DataBlock, fill=BlueFill, right=0.2cm of c1b] (c2a) {Age: 25};
  \node[DataBlock, fill=BlueFill, right=0pt of c2a] (c2b) {Age: 30};

  \node[DataBlock, fill=GreenFill, right=0.2cm of c2b] (c3a) {Loc: US};
  \node[DataBlock, fill=GreenFill, right=0pt of c3a] (c3b) {Loc: UK};

  % Scan Path
  \draw[->, thick, dashed, gray] (-0.8, 2.5) -- (7.5, 2.5) node[right, font=\scriptsize\usefont{T1}{phv}{m}{n}] {Scan reads all fields};
  \draw[->, thick, dashed, BlueLine] (2.6, 0.2) -- (5.2, 0.2) node[right, font=\scriptsize\usefont{T1}{phv}{m}{n}] {Scan reads ONLY Age};

\end{tikzpicture}
```
:::

::: {.callout-note title="Systems Insight: The GPU Starvation Problem"}

The choice of file format determines whether your system is **I/O Bound** or **Compute Bound**. In row-oriented formats (CSV/JSON), the CPU must parse text strings into floating-point numbers—a task that is often 10–100x slower than the GPU's ability to process those numbers. Using columnar formats like Parquet or Arrow allows for **zero-copy reads** and **projection pushdown**, ensuring that the "Data Movement" term in the Iron Law does not become a bottleneck that leaves your expensive GPUs idling.

:::

### The Algebra of Data {#sec-system-foundations-algebra-data-7b3d}

Feature engineering is built on three SQL primitives. Understanding their computational cost prevents pipeline bottlenecks.

1.  **Selection ($\sigma$)**: Filtering rows (e.g., `WHERE age > 30`).
    *   *Cost*: Cheap ($O(\log N)$) if indexed; expensive ($O(N)$) if full scan.

2.  **Projection ($\pi$)**: Selecting columns (e.g., `SELECT age`).
    *   *Cost*: **Free** in columnar formats (only read relevant blocks). In row formats, you must read the entire 1KB row just to extract the 4-byte integer, wasting 99% of I/O bandwidth.

3.  **Join ($\bowtie$)**: Combining tables. The most expensive operation.
    *   **Shuffle Join**: Both tables are partitioned by key and exchanged over the network.
        *   *Cost*: Massive network traffic. Joining two 1 TB tables requires moving ~2 TB over the network.
    *   **Broadcast Join**: One small table is sent to all workers.
        *   *Cost*: Minimal network, but the small table *must* fit in RAM.

## Probability & Statistics for Systems {#sec-system-foundations-probability-statistics-systems-a2cb}

Once data is flowing through our pipelines, we need mathematical tools to ensure its quality and consistency. **Probability & Statistics** provide the language for monitoring system health, detecting the silent failures of data drift, and managing uncertainty.

::: {.callout-perspective title="Why This Matters"}
Your monitoring dashboard says average latency is fine, but users are complaining. Why? Because systems live in the "long tail." Statistics gives us the tools to measure uncertainty, detect drift, and handle numerical stability.
:::

### Distributions and the Long Tail {#sec-system-foundations-distributions-long-tail-37cc}

In systems, the **mean** is often misleading. Latency distributions are almost always **long-tailed** (log-normal or power law). A "P99" (99th percentile) latency of 500ms means 1% of your users experience extreme slowness, which at scale (1M users) is 10,000 unhappy people.

:::: {.callout-notebook title="Napkin Math: The Median Experience"}
```{python}
#| label: tail-latency-probability
#| echo: false

# P(at least one slow) = 1 - (1 - p)^N
p_fast = 0.99
N_requests = 100
p_all_fast = p_fast ** N_requests
p_slow = 1 - p_all_fast
p_all_fast_str = f"{p_all_fast:.3f}"    # "0.366"
p_slow_pct_str = f"{p_slow * 100:.1f}"  # "63.4"
```

If a user session involves $N=100$ requests (common for a web page load or chat session), the probability of experiencing *at least one* P99 latency spike is:
$$ P(\text{Slow}) = 1 - (0.99)^{100} \approx 1 - `{python} p_all_fast_str` = `{python} p_slow_pct_str`\% $$
For a heavy user ($N$ is large), the "tail" latency becomes their *median* experience. This is why Google and Amazon optimize for P99.9 or P99.99.
::::

### Measuring Drift (Divergence) {#sec-system-foundations-measuring-drift-divergence-0525}

How do we know if serving data ($Q$) has drifted from training data ($P$)? We measure the "distance" between their distributions.

**KL Divergence** measures how much information is lost if we approximate $P$ with $Q$ [@kullback1951information]:
$$ D_{KL}(P || Q) = \sum P(x) \log \frac{P(x)}{Q(x)} $$

**Population Stability Index (PSI)** is a symmetric, industry-standard metric derived from KL divergence. A PSI > 0.2 typically triggers an alert for retraining.

### Logits and Numerical Stability {#sec-system-foundations-logits-numerical-stability-9f55}

Neural networks output **logits** (unnormalized scores), not probabilities. We convert them using Softmax:
$$ \text{Softmax}(z_i) = \frac{e^{z_i}}{\sum e^{z_j}} $$

**The Problem**: If $z_i$ is large (e.g., 100), $e^{100}$ overflows floating-point range.
**The Solution**: We compute in **log-space**. The "Log-Sum-Exp" trick allows us to compute $\log(\sum e^{z_j})$ without ever calculating the massive exponentials directly, preserving numerical precision.^[This is implemented as `torch.logsumexp` in PyTorch and `scipy.special.logsumexp` in SciPy. It relies on the identity $\log(\sum e^{x_i}) = a + \log(\sum e^{x_i - a})$, where $a = \max(x_i)$. Shifted values $x_i - a$ are $\le 0$, ensuring exponentials never overflow.]

## Fallacies and Pitfalls {#sec-data-foundations-fallacies-pitfalls}

::: {.callout-warning title="Pitfall: Ignoring data serialization cost."}
**The Reality**: Engineers often optimize their GPU kernels but leave data loading as JSON or uncompressed CSV. Parsing text formats is extremely CPU-intensive. Your GPU will idle while your CPU struggles to parse strings into floats.
:::
