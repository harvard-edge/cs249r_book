---
title: "Data Foundations"
id: sec-data-foundations
engine: jupyter
---

# Data Foundations {#sec-data-foundations}

```{python}
#| label: appendix-data-setup
#| echo: false

from physx.constants import TB, PB, Gbps, byte
from physx.formatting import fmt

# =============================================================================
# PURPOSE
# =============================================================================
# Purpose: Compute data transfer times for data gravity examples
# Used in: Table "The Cost of Inertia"

# =============================================================================
# INPUT
# =============================================================================
# Convert 1 TB and 1 PB to bytes (create Quantity then extract magnitude)
tb_value = (1 * TB).to(byte).magnitude  # 1e12 bytes
pb_value = (1 * PB).to(byte).magnitude  # 1e15 bytes

bw_1g_value = 1e9      # 1 Gbps
bw_10g_value = 10e9    # 10 Gbps
bw_100g_value = 100e9  # 100 Gbps

# Serialization Constants
csv_speed_mb = 100
parquet_speed_mb = 1000
csv_cycles = 100
parquet_cycles = 10
proto_speed_mb = 300
proto_cycles = 200

# Algebra of Data Constants
row_size_kb = 1
int_size_bytes = 4
waste_pct = (1 - (int_size_bytes / (row_size_kb * 1024))) * 100
join_table_tb = 1
join_network_tb = 2 * join_table_tb

# Long Tail Constants
n_users = 1_000_000
p99_ratio = 0.01
n_unhappy = n_users * p99_ratio

# =============================================================================
# PROCESS
# =============================================================================
def transfer_time(volume_bytes, bandwidth_bps):
    """Calculate transfer time in seconds."""
    return volume_bytes / (bandwidth_bps / 8)  # bps to bytes/s

def format_time(seconds):
    """Format seconds into human-readable string."""
    if seconds < 60:
        return f"{seconds:.0f} Seconds"
    elif seconds < 3600:
        return f"{seconds / 60:.0f} Minutes"
    elif seconds < 86400:
        return f"{seconds / 3600:.1f} Hours"
    elif seconds < 86400 * 30:
        return f"{seconds / 86400:.0f} Days"
    else:
        return f"{seconds / (86400 * 30):.0f} Months"

t_1tb_1g_value = transfer_time(tb_value, bw_1g_value)
t_1tb_10g_value = transfer_time(tb_value, bw_10g_value)
t_1tb_100g_value = transfer_time(tb_value, bw_100g_value)
t_100tb_1g_value = transfer_time(100 * tb_value, bw_1g_value)
t_100tb_10g_value = transfer_time(100 * tb_value, bw_10g_value)
t_100tb_100g_value = transfer_time(100 * tb_value, bw_100g_value)
t_1pb_1g_value = transfer_time(pb_value, bw_1g_value)
t_1pb_100g_value = transfer_time(pb_value, bw_100g_value)
t_1pb_10g_value = transfer_time(pb_value, bw_10g_value)

# =============================================================================
# OUTPUT
# =============================================================================
t_1tb_1g = format_time(t_1tb_1g_value)
t_1tb_10g = format_time(t_1tb_10g_value)
t_1tb_100g = format_time(t_1tb_100g_value)
t_100tb_1g = format_time(t_100tb_1g_value)
t_100tb_10g = format_time(t_100tb_10g_value)
t_100tb_100g = format_time(t_100tb_100g_value)
t_1pb_1g = format_time(t_1pb_1g_value)
t_1pb_10g = format_time(t_1pb_10g_value)
t_1pb_100g = format_time(t_1pb_100g_value)

# Formatted strings for text
csv_speed_str = f"~{csv_speed_mb}"
parquet_speed_str = f"> {parquet_speed_mb:,}"
csv_cycles_str = f"~{csv_cycles}"
parquet_cycles_str = f"~{parquet_cycles}"
proto_speed_str = f"~{proto_speed_mb}"
proto_cycles_str = f"~{proto_cycles}"

waste_pct_str = f"{waste_pct:.1f}"
join_table_tb_str = f"{join_table_tb}"
join_network_tb_str = f"~{join_network_tb}"

n_users_str = "1M"
n_unhappy_str = f"{n_unhappy:,.0f}"

psi_threshold = 0.2
logit_val = 100
```

This appendix covers the principles of data engineering and statistical monitoring that keep ML systems healthy. From storage formats that determine I/O throughput to drift metrics that detect silent failures, these foundations connect directly to the data pipelines in @sec-data-engineering-ml and the operational monitoring in @sec-machine-learning-operations-mlops.

## Data Engineering Foundations {#sec-system-foundations-data-engineering-foundations-9ba2}

Understanding hardware constraints is only half the battle; we must also shape our data to fit them. **Data Engineering** applies the principles of the memory hierarchy to storage formats and pipeline design, ensuring that the accelerator never starves.

This process begins with recognizing that data is physical—it has volume, it takes time to move, and it requires energy to parse.

### Napkin Math: The Physics of Data Gravity

**Data Gravity** is not a metaphor; it is a calculation of transfer time. Unlike compute, which gets faster every year, the speed of light is fixed and network bandwidth is a finite resource. When we speak of "Data Gravity," we are describing the phenomenon where datasets become so large that they effectively become stationary.

**Transfer Time Equation**: $T = \text{Data Volume} / \text{Bandwidth}$ (for the large volumes here, latency is negligible; see @sec-system-foundations-bandwidth-vs-latency-e155 for the full equation). @tbl-data-gravity illustrates the sobering reality:

| **Data Volume** | **1 Gbps (Standard WAN)** | **10 Gbps (High-End WAN)** | **100 Gbps (Direct Connect)** | **Snowmobile (Truck)** |
|:--------------|------------------------:|-------------------------:|----------------------------:|:---------------------|
| **1 TB**        |       `{python} t_1tb_1g` |       `{python} t_1tb_10g` |         `{python} t_1tb_100g` | N/A                    |
| **100 TB**      |     `{python} t_100tb_1g` |     `{python} t_100tb_10g` |       `{python} t_100tb_100g` | N/A                    |
| **1 PB**        |   **`{python} t_1pb_1g`** |   **`{python} t_1pb_10g`** |         `{python} t_1pb_100g` | **2 Days**             |

: **The Cost of Inertia.** Why we "ship compute to data" rather than "ship data to compute." If you have 1 PB, the network is often not a viable option. {#tbl-data-gravity}

### The Cost of Serialization

Even after data arrives at the machine, we face one final hurdle: **The Serialization Tax**. As @tbl-serialization-cost shows, many engineers meticulously optimize their accelerator kernels while ignoring the CPU overhead of decoding data. Parsing text-based formats like JSON or CSV is extremely CPU-intensive, often leaving the accelerator idling while the CPU struggles to convert strings into floating-point numbers.

| **Format**          |             **Decoding Speed (MB/s)** |                  **CPU Cycles per Byte** | **Suitability**         |
|:------------------|------------------------------------:|---------------------------------------:|:----------------------|
| **CSV / JSON**      |         `{python} csv_speed_str` MB/s |         `{python} csv_cycles_str` cycles | Debugging only          |
| **Protobuf**        |       `{python} proto_speed_str` MB/s |       `{python} proto_cycles_str` cycles | RPC / Messages          |
| **Parquet / Arrow** | **`{python} parquet_speed_str` MB/s** | **`{python} parquet_cycles_str` cycles** | **High-Scale Training** |

: **Serialization Overhead.** Zero-copy formats like Arrow are 10-100x faster than row-based text formats because they align directly with internal memory structures. {#tbl-serialization-cost}

### Row vs. Columnar Formats {#sec-system-foundations-row-vs-columnar-formats-1f97}

The choice of file format determines the "physics" of how your data is read.

**Row-Oriented (CSV, JSON)**: Data is stored record-by-record. To read just the `age` column, you must scan every byte of every row. This is efficient for *writing* (appending a log) but inefficient for *analytics* (training on specific features).

**Column-Oriented (Parquet, Arrow)**: Data is stored column-by-column. To read the `age` column, the disk head seeks to that column's block and reads it sequentially. This enables **projection pushdown** (reading only the bytes you need) and **vectorized processing** (SIMD operations on columns). Compare the two arrangements side by side in @fig-row-vs-col to see why columnar access avoids scanning unnecessary bytes.

::: {#fig-row-vs-col fig-env="figure" fig-pos="htb" fig-cap="**Storage Layouts**: Row-oriented formats pack data together by record (good for transactions). Column-oriented formats pack data by feature (good for analytics)." fig-alt="Diagram contrasting Row Store vs Column Store. Row store shows Record 1 [ID, Name, Age] followed by Record 2. Column store shows Column 1 [ID1, ID2...] followed by Column 2 [Name1, Name2...]."}
```{.tikz}
\begin{tikzpicture}[font=\small\usefont{T1}{phv}{m}{n}, scale=1.0]
  \tikzset{
    DataBlock/.style={draw=GrayLine, line width=0.8pt, minimum height=0.6cm, minimum width=1.2cm, align=center, font=\scriptsize\usefont{T1}{phv}{m}{n}},
    Label/.style={text=TextBlack, font=\small\bfseries\usefont{T1}{phv}{m}{n}}
  }

  % Row Store
  \node[Label] at (0, 3.5) {Row-Oriented (CSV)};
  \node[DataBlock, fill=RedFill] (r1a) at (0, 2.8) {ID: 1};
  \node[DataBlock, fill=BlueFill, right=0pt of r1a] (r1b) {Age: 25};
  \node[DataBlock, fill=GreenFill, right=0pt of r1b] (r1c) {Loc: US};

  \node[DataBlock, fill=RedFill, right=0.2cm of r1c] (r2a) {ID: 2};
  \node[DataBlock, fill=BlueFill, right=0pt of r2a] (r2b) {Age: 30};
  \node[DataBlock, fill=GreenFill, right=0pt of r2b] (r2c) {Loc: UK};

  % Column Store
  \node[Label] at (0, 1.5) {Column-Oriented (Parquet)};
  \node[DataBlock, fill=RedFill] (c1a) at (0, 0.8) {ID: 1};
  \node[DataBlock, fill=RedFill, right=0pt of c1a] (c1b) {ID: 2};

  \node[DataBlock, fill=BlueFill, right=0.2cm of c1b] (c2a) {Age: 25};
  \node[DataBlock, fill=BlueFill, right=0pt of c2a] (c2b) {Age: 30};

  \node[DataBlock, fill=GreenFill, right=0.2cm of c2b] (c3a) {Loc: US};
  \node[DataBlock, fill=GreenFill, right=0pt of c3a] (c3b) {Loc: UK};

  % Scan Path
  \draw[->, thick, dashed, gray] (-0.8, 2.5) -- (7.5, 2.5) node[right, font=\scriptsize\usefont{T1}{phv}{m}{n}] {Scan reads all fields};
  \draw[->, thick, dashed, BlueLine] (2.6, 0.2) -- (5.2, 0.2) node[right, font=\scriptsize\usefont{T1}{phv}{m}{n}] {Scan reads ONLY Age};

\end{tikzpicture}
```
:::

This layout difference has direct consequences for ML pipelines where accelerator utilization depends on data loading speed.

::: {.callout-perspective title="The Accelerator Starvation Problem"}

The choice of file format determines whether your system is **I/O Bound** or **Compute Bound**. As @tbl-serialization-cost shows, the serialization tax compounds with storage layout: row-oriented formats force full-row scans while columnar formats enable **projection pushdown**, reading only the bytes your model needs. The result is that the "Data Movement" term in the **Iron Law** can silently become the bottleneck that leaves your expensive accelerators idling.

:::

### The Algebra of Data {#sec-system-foundations-algebra-data-7b3d}

Feature engineering is built on three SQL primitives. Understanding their computational cost prevents pipeline bottlenecks.

1.  **Selection ($\sigma$)**: Filtering rows (e.g., `WHERE age > 30`).
    *   *Cost*: Cheap ($O(\log N)$) if indexed; expensive ($O(N)$) if full scan.

2.  **Projection ($\pi$)**: Selecting columns (e.g., `SELECT age`).
    *   *Cost*: **Free** in columnar formats (only read relevant blocks). In row formats, you must read the entire `{python} row_size_kb` KB row just to extract the `{python} int_size_bytes`-byte integer, wasting `{python} waste_pct_str`% of I/O bandwidth.

3.  **Join ($\bowtie$)**: Combining tables. The most expensive operation.
    *   **Shuffle Join**: Both tables are partitioned by key and exchanged over the network.
        *   *Cost*: Massive network traffic. Joining two `{python} join_table_tb_str` TB tables requires moving `{python} join_network_tb_str` TB over the network.
    *   **Broadcast Join**: One small table is sent to all workers.
        *   *Cost*: Minimal network, but the small table *must* fit in RAM.

## Probability & Statistics for Systems {#sec-system-foundations-probability-statistics-systems-a2cb}

Once data is flowing through our pipelines, we need mathematical tools to ensure its quality and consistency. **Probability & Statistics** provide the language for monitoring system health, detecting the silent failures of data drift, and managing uncertainty.

::: {.callout-perspective title="Why This Matters"}
Your monitoring dashboard says average latency is fine, but users are complaining. Why? Because systems live in the "long tail." Statistics gives us the tools to measure uncertainty, detect drift, and handle numerical stability.
:::

### Distributions and the Long Tail {#sec-system-foundations-distributions-long-tail-37cc}

In systems, the **mean** is often misleading. Latency distributions are almost always **long-tailed** (log-normal or power law). A "P99" (99th percentile) latency of 500ms means 1% of your users experience extreme slowness, which at scale (`{python} n_users_str` users) is `{python} n_unhappy_str` unhappy people.

::: {.callout-notebook title="The Median Experience"}
```{python}
#| label: tail-latency-probability
#| echo: false
from physx.formatting import fmt, md_math

# =============================================================================
# PURPOSE
# =============================================================================
# Purpose: Compute probability of at least one slow request
# Used in: Callout "The Median Experience"

# =============================================================================
# INPUT
# =============================================================================
p_fast_value = 0.99
n_requests_value = 100

# =============================================================================
# PROCESS
# =============================================================================
p_all_fast_value = p_fast_value ** n_requests_value
p_slow_value = 1 - p_all_fast_value

# =============================================================================
# OUTPUT
# =============================================================================
p_all_fast_str = fmt(p_all_fast_value, precision=3, commas=False)
p_slow_pct_str = fmt(p_slow_value * 100, precision=1, commas=False)
p_slow_eq = md_math(f"P(\\text{{Slow}}) = 1 - ({p_fast_value})^{{{n_requests_value}}} \\approx 1 - {p_all_fast_value:.3f} = {p_slow_pct_str}\\%")
```

If a user session involves $N=100$ requests (common for a web page load or chat session), the probability of experiencing *at least one* P99 latency spike is:
`{python} p_slow_eq`

Computed: P(Slow) = `{python} p_slow_pct_str`%.
For a heavy user ($N$ is large), the "tail" latency becomes their *median* experience. This is why Google and Amazon optimize for P99.9 or P99.99.
:::

### Measuring Drift (Divergence) {#sec-system-foundations-measuring-drift-divergence-0525}

Because distributions have long tails where the most dangerous failures hide, simple metrics like "mean shift" are insufficient. We need tools that compare the *entire shape* of the distribution.

How do we know if serving data ($Q$) has drifted from training data ($P$)? We measure the "distance" between their distributions.

**KL Divergence** measures how much information is lost if we approximate $P$ with $Q$ [@kullback1951information]:
$$ D_{KL}(P || Q) = \sum P(x) \log \frac{P(x)}{Q(x)} $$

**Population Stability Index (PSI)** is a symmetric, industry-standard metric derived from KL divergence. A PSI > `{python} psi_threshold` typically triggers an alert for retraining.

### Information Theory for Systems {#sec-system-foundations-information-theory-a8f2}

In the "Information Roofline" model (@sec-appendix-dam), we treat data quality as a physical constraint similar to bandwidth. Information Theory provides the units for this constraint.

*   **Entropy ($H$)**: The average information content (uncertainty) in a distribution, defined as $H(X) = -\sum p(x) \log p(x)$. A uniform distribution has maximum entropy (maximum uncertainty). Note that this connects directly to KL Divergence above: $D_{KL}$ measures excess bits needed when using the wrong distribution.
*   **Information Density**: The amount of useful signal per unit of storage. High-quality data has high information density; noisy data has low density.
*   **Signal-to-Noise Ratio (SNR)**: The ratio of useful information to irrelevant variance. In ML, training on low-SNR data is like trying to learn a pattern from static—you hit the "Information Roofline" where adding more compute yields no improvement.

### Logits and Numerical Stability {#sec-system-foundations-logits-numerical-stability-9f55}

Neural networks output **logits** (unnormalized scores), not probabilities. We convert them using Softmax:
$$ \text{Softmax}(z_i) = \frac{e^{z_i}}{\sum e^{z_j}} $$

**The Problem**: If $z_i$ is large (e.g., `{python} logit_val`), the exponential $e^{z_i}$ overflows floating-point range.
**The Solution**: We compute in **log-space**. The "Log-Sum-Exp" trick allows us to compute $\log(\sum e^{z_j})$ without ever calculating the massive exponentials directly, preserving numerical precision.[^fn-logsumexp]

[^fn-logsumexp]: **Log-Sum-Exp**: Implemented as `torch.logsumexp` in PyTorch and `scipy.special.logsumexp` in SciPy. It relies on the identity $\log(\sum e^{x_i}) = a + \log(\sum e^{x_i - a})$, where $a = \max(x_i)$. Shifted values $x_i - a$ are $\le 0$, ensuring exponentials never overflow.

## Fallacies and Pitfalls {#sec-data-foundations-fallacies-pitfalls}

::: {.callout-warning title="Pitfall: Ignoring data serialization cost."}
**The Reality**: Engineers meticulously optimize accelerator kernels while leaving data loading as JSON or CSV. See @sec-system-foundations-data-engineering-foundations-9ba2 for why this is often the hidden bottleneck.
:::
