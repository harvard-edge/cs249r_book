@inproceedings{agrawal2024sarathi,
  title = {Taming Throughput-Latency Tradeoff in LLM Inference with Sarathi-Serve},
  author = {
    Agrawal, Amey and Kedia, Nitin and Panwar, Ashish and Mohan, Jayashree and Kwatra, Nipun and
    Gulavani, Bhargav and Tumanov, Alexey and Ramjee, Ramachandran
  },
  year = {2024},
  booktitle = {18th USENIX Symposium on Operating Systems Design and Implementation (OSDI 24)},
  organization = {USENIX Association},
}

@article{barroso2017attack,
  title = {Attack of the Killer Microseconds},
  author = {Barroso, Luiz Andr{\'e} and Marty, Mike and Patterson, David and Ranganathan, Parthasarathy},
  year = {2017},
  journal = {Communications of the ACM},
  publisher = {ACM},
  volume = {60},
  number = {4},
  pages = {48--54},
  doi = {10.1145/3015146},
}

@article{breck2019data,
  title = {Data Validation for Machine Learning},
  author = {
    Breck, Eric and Polyzotis, Neoklis and Roy, Sudip and Whang, Steven Euijong and Zinkevich,
    Martin
  },
  year = {2019},
  journal = {Proceedings of Machine Learning and Systems},
  volume = {1},
  pages = {334--347},
}

% Inference Optimization and Runtimes
@article{chen2018tvm,
  title = {TVM: An Automated End-to-End Optimizing Compiler for Deep Learning},
  author = {
    Chen, Tianqi and Moreau, Thierry and Jiang, Ziheng and Zheng, Lianmin and Yan, Eddie and Shen,
    Haichen and Cowan, Meghan and Wang, Leyuan and Hu, Yuwei and Ceze, Luis and Guestrin, Carlos
    and Krishnamurthy, Arvind
  },
  year = {2018},
  journal = {13th USENIX Symposium on Operating Systems Design and Implementation (OSDI 18)},
  pages = {578--594},
}

% Batching and Serving Systems
@inproceedings{crankshaw2017clipper,
  title = {Clipper: A Low-Latency Online Prediction Serving System},
  author = {
    Crankshaw, Daniel and Wang, Xin and Zhou, Guilio and Franklin, Michael J. and Gonzalez, Joseph
    E. and Stoica, Ion
  },
  year = {2017},
  booktitle = {14th USENIX Symposium on Networked Systems Design and Implementation (NSDI 17)},
  pages = {613--627},
  organization = {USENIX Association},
}

@misc{dean2012rapid,
  title = {Achieving Rapid Response Times in Large Online Services},
  author = {Dean, Jeffrey},
  year = {2012},
  url = {https://research.google/pubs/pub44875/},
  note = {Keynote presentation on tail-tolerant distributed systems},
  howpublished = {Berkeley AMPLab Cloud Seminar},
}

% Foundational Latency Papers
@article{dean2013tail,
  title = {The Tail at Scale},
  author = {Dean, Jeffrey and Barroso, Luiz Andr{\'e}},
  year = {2013},
  journal = {Communications of the ACM},
  publisher = {ACM},
  volume = {56},
  number = {2},
  pages = {74--80},
  doi = {10.1145/2408776.2408794},
}

% Bibliography for Serving chapter

% Static vs Dynamic Inference
@misc{google2024staticdynamic,
  title = {Static vs. Dynamic Inference},
  author = {{Google}},
  year = {2024},
  url = {
    https://developers.google.com/machine-learning/crash-course/production-ml-systems/static-vs-dynamic-inference
  },
  note = {Accessed: 2024},
  howpublished = {Google Machine Learning Crash Course},
}

@inproceedings{gujarati2020serving,
  title = {Serving DNNs like Clockwork: Performance Predictability from the Bottom Up},
  author = {
    Gujarati, Arpan and Karber, Reza and Musaev, Safraz and Liu, Weiyang and Narayanan, Anurag and
    Zhong, Shi Quan and Kandemir, Mahmut and Sekar, Vyas and Zadorozhny, Alexander
  },
  year = {2020},
  booktitle = {14th USENIX Symposium on Operating Systems Design and Implementation (OSDI 20)},
  pages = {443--462},
  organization = {USENIX Association},
}

% Queuing Theory for ML Serving
@book{harchol2013performance,
  title = {Performance Modeling and Design of Computer Systems: Queueing Theory in Action},
  author = {Harchol-Balter, Mor},
  year = {2013},
  publisher = {Cambridge University Press},
}

% Google Play Training-Serving Skew Example
@inproceedings{hazelwood2018applied,
  title = {Applied Machine Learning at Facebook: A Datacenter Infrastructure Perspective},
  author = {
    Hazelwood, Kim and Bird, Sarah and Brooks, David and Chintala, Soumith and Diril, Utku and
    Dzhulgakov, Dmytro and Fawzy, Mohamed and Jia, Bill and Jia, Yangqing and Kalro, Aditya and
    Law, James and Lee, Kevin and Lu, Jason and Noordhuis, Pieter and Smelyanskiy, Misha and Xiong,
    Liang and Wang, Xiaodong
  },
  year = {2018},
  booktitle = {2018 IEEE International Symposium on High Performance Computer Architecture (HPCA)},
  pages = {620--629},
  organization = {IEEE},
}

% Decoding Strategies
@article{holtzman2020curious,
  title = {The Curious Case of Neural Text Degeneration},
  author = {Holtzman, Ari and Buys, Jan and Du, Li and Forbes, Maxwell and Choi, Yejin},
  year = {2020},
  journal = {International Conference on Learning Representations},
}

@inproceedings{kwon2023vllm,
  title = {Efficient Memory Management for Large Language Model Serving with PagedAttention},
  author = {
    Kwon, Woosuk and Li, Zhuohan and Zhuang, Siyuan and Sheng, Ying and Zheng, Lianmin and Yu, Cody
    Hao and Gonzalez, Joseph E. and Zhang, Hao and Stoica, Ion
  },
  year = {2023},
  booktitle = {Proceedings of the 29th Symposium on Operating Systems Principles},
  pages = {611--626},
  organization = {ACM},
}

@article{meister2020beam,
  title = {If Beam Search is the Answer, What was the Question?},
  author = {Meister, Clara and Vieira, Tim and Cotterell, Ryan},
  year = {2020},
  journal = {Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing},
  pages = {2173--2185},
}

% Preprocessing and Data Pipelines
@inproceedings{murray2021tf,
  title = {tf.data: A Machine Learning Data Processing Framework},
  author = {Murray, Derek G. and Simsa, Jiri and Klimovic, Ana and Indyk, Irina},
  year = {2021},
  booktitle = {Proceedings of the VLDB Endowment},
  volume = {14},
  number = {12},
  pages = {2945--2958},
}

@misc{nvidia2024tensorrt,
  title = {TensorRT: High-Performance Deep Learning Inference},
  author = {{NVIDIA}},
  year = {2024},
  url = {https://developer.nvidia.com/tensorrt},
  note = {Accessed: 2024},
  howpublished = {NVIDIA Developer},
}

% Triton Inference Server
@misc{nvidia2024triton,
  title = {Triton Inference Server},
  author = {{NVIDIA}},
  year = {2024},
  url = {https://developer.nvidia.com/triton-inference-server},
  note = {Accessed: 2024},
  howpublished = {NVIDIA Developer},
}

@misc{nvidia2024tritontutorial,
  title = {Serving ML Model Pipelines on NVIDIA Triton Inference Server with Ensemble Models},
  author = {{NVIDIA}},
  year = {2024},
  url = {
    https://developer.nvidia.com/blog/serving-ml-model-pipelines-on-nvidia-triton-inference-server-with-ensemble-models/
  },
  note = {Accessed: 2024},
  howpublished = {NVIDIA Technical Blog},
}

@inproceedings{olston2017tensorflow,
  title = {TensorFlow-Serving: Flexible, High-Performance ML Serving},
  author = {
    Olston, Christopher and Fiedel, Noah and Gorovoy, Kiril and Harmsen, Jeremiah and Lao, Li and
    Li, Fangwei and Rajashekhar, Vinu and Ramesh, Sukriti and Soyke, Jordan
  },
  year = {2017},
  booktitle = {Workshop on ML Systems at NIPS},
}

@misc{onnxruntime2024,
  title = {ONNX Runtime: Cross-Platform Inference and Training Machine-Learning Accelerator},
  author = {{Microsoft}},
  year = {2024},
  url = {https://github.com/microsoft/onnxruntime},
  note = {Accessed: 2024},
  howpublished = {GitHub},
}

% Feature Stores
@inproceedings{orr2021managing,
  title = {Managing ML Pipelines: Feature Stores and the Coming Wave of Embedding Ecosystems},
  author = {
    Orr, Laurel and Sanyal, Atindriyo and Ling, Xiao and Qiao, Mengdi and Subramonian, Arjun and
    Lesnikova, Kira and Stoica, Ion and Sen, Koushik
  },
  year = {2021},
  booktitle = {Proceedings of the VLDB Endowment},
  volume = {14},
  number = {12},
  pages = {3178--3181},
}

@inproceedings{polyzotis2017data,
  title = {Data Management Challenges in Production Machine Learning},
  author = {Polyzotis, Neoklis and Roy, Sudip and Whang, Steven Euijong and Zinkevich, Martin},
  year = {2017},
  booktitle = {Proceedings of the 2017 ACM International Conference on Management of Data},
  pages = {1723--1726},
  organization = {ACM},
}

% Cold Start and Model Loading
@inproceedings{romero2021infaas,
  title = {INFaaS: Automated Model-less Inference Serving},
  author = {Romero, Francisco and Li, Qian and Yadwadkar, Neeraja J. and Kozyrakis, Christos},
  year = {2021},
  booktitle = {2021 USENIX Annual Technical Conference (USENIX ATC 21)},
  pages = {397--411},
  organization = {USENIX Association},
}

% Training-Serving Skew
@article{sculley2015hidden,
  title = {Hidden Technical Debt in Machine Learning Systems},
  author = {
    Sculley, D. and Holt, Gary and Golovin, Daniel and Davydov, Eugene and Phillips, Todd and
    Ebner, Dietmar and Chaudhary, Vinay and Young, Michael and Crespo, Jean-Fran{\c{c}}ois and
    Dennison, Dan
  },
  year = {2015},
  journal = {Advances in Neural Information Processing Systems},
  publisher = {Curran Associates, Inc.},
  volume = {28},
}

% Latency Analysis
@inproceedings{shen2019nexus,
  title = {Nexus: A GPU Cluster Engine for Accelerating DNN-Based Video Analysis},
  author = {
    Shen, Haichen and Chen, Lequn and Jin, Yuchen and Zhao, Liangyu and Kong, Bingyu and Philipose,
    Matthai and Krishnamurthy, Arvind and Sundaram, Ravi
  },
  year = {2019},
  booktitle = {Proceedings of the 27th ACM Symposium on Operating Systems Principles},
  pages = {322--337},
  organization = {ACM},
}

% Model Serving Economics
@article{wu2019machine,
  title = {Machine Learning at Facebook: Understanding Inference at the Edge},
  author = {
    Wu, Carole-Jean and Brooks, David and Chen, Kevin and Chen, Doug and Choudhury, Sy and Dukhan,
    Marat and Hazelwood, Kim and Isaac, Eldad and Jia, Yangqing and Jia, Bill and Leber, Tommer and
    Lee, Changkyu and Lu, Hao and Lu, Yang and others
  },
  year = {2019},
  journal = {2019 IEEE International Symposium on High Performance Computer Architecture (HPCA)},
  publisher = {IEEE},
  pages = {331--344},
}

% Continuous Batching and LLM Serving
@inproceedings{yu2022orca,
  title = {Orca: A Distributed Serving System for Transformer-Based Generative Models},
  author = {Yu, Gyeong-In and Jeong, Joo Seong and Kim, Geon-Woo and Kim, Soojeong and Chun, Byung-Gon},
  year = {2022},
  booktitle = {16th USENIX Symposium on Operating Systems Design and Implementation (OSDI 22)},
  pages = {521--538},
  organization = {USENIX Association},
}

@inproceedings{zhang2019mark,
  title = {
    Mark: Exploiting Cloud Services for Cost-Effective, SLO-Aware Machine Learning Inference
    Serving
  },
  author = {Zhang, Chengliang and Yu, Minchen and Wang, Wei and Yan, Feng},
  year = {2019},
  booktitle = {2019 USENIX Annual Technical Conference (USENIX ATC 19)},
  pages = {1049--1062},
  organization = {USENIX Association},
}
