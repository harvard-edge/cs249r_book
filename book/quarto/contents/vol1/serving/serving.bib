@article{agrawal2024sarathi,
  title = {Efficient LLM Inference via Chunked Prefills},
  author = {
    Agrawal, Arney and Kedia, Nitin and Panwar, Ashish and Mohan, Jayashree and Kwatra, Nipun and
    Gulavani, Bhargav S. and Tumanov, Alexey and Ramjee, Ramachandran
  },
  year = {2025},
  month = aug,
  journal = {ACM SIGOPS Operating Systems Review},
  booktitle = {18th USENIX Symposium on Operating Systems Design and Implementation (OSDI 24)},
  publisher = {Association for Computing Machinery (ACM)},
  volume = {59},
  number = {1},
  pages = {9--16},
  doi = {10.1145/3759441.3759444},
  issn = {0163-5980},
  url = {https://doi.org/10.1145/3759441.3759444},
  source = {Crossref},
  organization = {USENIX Association},
}

@article{barroso2017attack,
  title = {Attack of the killer microseconds},
  author = {Barroso, Luiz and Marty, Mike and Patterson, David and Ranganathan, Parthasarathy},
  year = {2017},
  month = mar,
  journal = {Communications of the ACM},
  publisher = {Association for Computing Machinery (ACM)},
  volume = {60},
  number = {4},
  pages = {48--54},
  doi = {10.1145/3015146},
  issn = {0001-0782,1557-7317},
  url = {https://doi.org/10.1145/3015146},
  source = {Crossref},
}

@article{breck2019data,
  title = {Data Validation for Machine Learning},
  author = {
    Breck, Eric and Polyzotis, Neoklis and Roy, Sudip and Whang, Steven Euijong and Zinkevich,
    Martin
  },
  year = {2019},
  journal = {Proceedings of Machine Learning and Systems},
  volume = {1},
  pages = {334--347},
}

% Inference Optimization and Runtimes
@article{chen2018tvm,
  title = {TVM: An Automated End-to-End Optimizing Compiler for Deep Learning},
  author = {
    Chen, Tianqi and Moreau, Thierry and Jiang, Ziheng and Zheng, Lianmin and Yan, Eddie and Shen,
    Haichen and Cowan, Meghan and Wang, Leyuan and Hu, Yuwei and Ceze, Luis and Guestrin, Carlos
    and Krishnamurthy, Arvind
  },
  year = {2018},
  journal = {13th USENIX Symposium on Operating Systems Design and Implementation (OSDI 18)},
  pages = {578--594},
}

% Batching and Serving Systems
@inproceedings{crankshaw2017clipper,
  title = {Clipper: A Low-Latency Online Prediction Serving System},
  author = {
    Crankshaw, Daniel and Wang, Xin and Zhou, Guilio and Franklin, Michael J. and Gonzalez, Joseph
    E. and Stoica, Ion
  },
  year = {2017},
  booktitle = {14th USENIX Symposium on Networked Systems Design and Implementation (NSDI 17)},
  pages = {613--627},
  organization = {USENIX Association},
}

@misc{dean2012rapid,
  title = {Achieving Rapid Response Times in Large Online Services},
  author = {Dean, Jeffrey},
  year = {2012},
  url = {https://research.google/pubs/pub44875/},
  note = {Keynote presentation on tail-tolerant distributed systems},
  howpublished = {Berkeley AMPLab Cloud Seminar},
}

% Foundational Latency Papers
@article{dean2013tail,
  title = {The tail at scale},
  author = {Dean, Jeffrey and Barroso, Luiz Andr\'e},
  year = {2013},
  month = feb,
  journal = {Communications of the ACM},
  publisher = {Association for Computing Machinery (ACM)},
  volume = {56},
  number = {2},
  pages = {74--80},
  doi = {10.1145/2408776.2408794},
  issn = {0001-0782,1557-7317},
  url = {https://doi.org/10.1145/2408776.2408794},
  source = {Crossref},
}

% Bibliography for Serving chapter

% Static vs Dynamic Inference
@misc{google2024staticdynamic,
  title = {Static vs. Dynamic Inference},
  author = {Google},
  year = {2024},
  url = {
    https://developers.google.com/machine-learning/crash-course/production-ml-systems/static-vs-dynamic-inference
  },
  note = {Accessed: 2024},
  howpublished = {Google Machine Learning Crash Course},
}

@inproceedings{gujarati2020serving,
  title = {Serving DNNs like Clockwork: Performance Predictability from the Bottom Up},
  author = {
    Gujarati, Arpan and Karber, Reza and Musaev, Safraz and Liu, Weiyang and Narayanan, Anurag and
    Zhong, Shi Quan and Kandemir, Mahmut and Sekar, Vyas and Zadorozhny, Alexander
  },
  year = {2020},
  booktitle = {14th USENIX Symposium on Operating Systems Design and Implementation (OSDI 20)},
  pages = {443--462},
  organization = {USENIX Association},
}

% Queuing Theory for ML Serving
@book{harchol2013performance,
  title = {Performance Modeling and Design of Computer Systems},
  author = {Harchol-Balter, Mor},
  year = {2013},
  month = feb,
  publisher = {Cambridge University Press},
  doi = {10.1017/cbo9781139226424},
  isbn = {9781107027503,9781139226424},
  url = {https://doi.org/10.1017/cbo9781139226424},
  source = {Crossref},
  subtitle = {Queueing Theory in Action},
}

% Google Play Training-Serving Skew Example
@inproceedings{hazelwood2018applied,
  title = {Applied Machine Learning at Facebook: A Datacenter Infrastructure Perspective},
  author = {
    Hazelwood, Kim and Bird, Sarah and Brooks, David and Chintala, Soumith and Diril, Utku and
    Dzhulgakov, Dmytro and Fawzy, Mohamed and Jia, Bill and Jia, Yangqing and Kalro, Aditya and
    Law, James and Lee, Kevin and Lu, Jason and Noordhuis, Pieter and Smelyanskiy, Misha and Xiong,
    Liang and Wang, Xiaodong
  },
  year = {2018},
  month = feb,
  booktitle = {2018 IEEE International Symposium on High Performance Computer Architecture (HPCA)},
  publisher = {IEEE},
  pages = {620--629},
  doi = {10.1109/hpca.2018.00059},
  url = {https://doi.org/10.1109/hpca.2018.00059},
  source = {Crossref},
  organization = {IEEE},
}

% Decoding Strategies
@article{holtzman2020curious,
  title = {The Curious Case of Neural Text Degeneration},
  author = {Holtzman, Ari and Buys, Jan and Du, Li and Forbes, Maxwell and Choi, Yejin},
  year = {2020},
  journal = {International Conference on Learning Representations},
}

@inproceedings{kwon2023vllm,
  title = {Efficient Memory Management for Large Language Model Serving with PagedAttention},
  author = {
    Kwon, Woosuk and Li, Zhuohan and Zhuang, Siyuan and Sheng, Ying and Zheng, Lianmin and Yu, Cody
    Hao and Gonzalez, Joseph and Zhang, Hao and Stoica, Ion
  },
  year = {2023},
  month = oct,
  booktitle = {Proceedings of the 29th Symposium on Operating Systems Principles},
  publisher = {ACM},
  pages = {611--626},
  doi = {10.1145/3600006.3613165},
  url = {https://doi.org/10.1145/3600006.3613165},
  source = {Crossref},
  organization = {ACM},
}

@inproceedings{meister2020beam,
  title = {If beam search is the answer, what was the question?},
  author = {Meister, Clara and Cotterell, Ryan and Vieira, Tim},
  year = {2020},
  journal = {Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing},
  booktitle = {
    Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)
  },
  publisher = {Association for Computational Linguistics},
  pages = {2173--2185},
  doi = {10.18653/v1/2020.emnlp-main.170},
  url = {https://doi.org/10.18653/v1/2020.emnlp-main.170},
  source = {Crossref},
}

% Preprocessing and Data Pipelines
@article{murray2021tf,
  title = {tf.data},
  author = {Murray, Derek G. and \v{S}im\v{s}a, Ji\v{r}\'i and Klimovic, Ana and Indyk, Ihor},
  year = {2021},
  month = jul,
  journal = {Proceedings of the VLDB Endowment},
  booktitle = {Proceedings of the VLDB Endowment},
  publisher = {Association for Computing Machinery (ACM)},
  volume = {14},
  number = {12},
  pages = {2945--2958},
  doi = {10.14778/3476311.3476374},
  issn = {2150-8097},
  url = {https://doi.org/10.14778/3476311.3476374},
  source = {Crossref},
  subtitle = {a machine learning data processing framework},
}

@misc{nvidia2024tensorrt,
  title = {TensorRT: High-Performance Deep Learning Inference},
  author = {NVIDIA},
  year = {2024},
  url = {https://developer.nvidia.com/tensorrt},
  note = {Accessed: 2024},
  howpublished = {NVIDIA Developer},
}

% Triton Inference Server
@misc{nvidia2024triton,
  title = {Triton Inference Server},
  author = {NVIDIA},
  year = {2024},
  url = {https://developer.nvidia.com/triton-inference-server},
  note = {Accessed: 2024},
  howpublished = {NVIDIA Developer},
}

@article{nvidia2024tritontutorial,
  title = {Productionizing GPU Inference on EKS with KServe and NVIDIA Triton},
  author = {NVIDIA},
  year = {2025},
  journal = {American International Journal of Computer Science and Technology},
  publisher = {ScienceTech Xplore},
  volume = {7},
  number = {6},
  doi = {10.63282/3117-5481/aijcst-v7i6p104},
  issn = {3117-5481},
  url = {https://doi.org/10.63282/3117-5481/aijcst-v7i6p104},
  note = {Accessed: 2024},
  source = {Crossref},
  howpublished = {NVIDIA Technical Blog},
}

@inproceedings{olston2017tensorflow,
  title = {TensorFlow-Serving: Flexible, High-Performance ML Serving},
  author = {
    Olston, Christopher and Fiedel, Noah and Gorovoy, Kiril and Harmsen, Jeremiah and Lao, Li and
    Li, Fangwei and Rajashekhar, Vinu and Ramesh, Sukriti and Soyke, Jordan
  },
  year = {2017},
  booktitle = {Workshop on ML Systems at NIPS},
}

@misc{onnxruntime2024,
  title = {ONNX Runtime: Cross-Platform Inference and Training Machine-Learning Accelerator},
  author = {Microsoft},
  year = {2024},
  url = {https://github.com/microsoft/onnxruntime},
  note = {Accessed: 2024},
  howpublished = {GitHub},
}

% Feature Stores
@article{orr2021managing,
  title = {Managing ML pipelines},
  author = {Orr, Laurel and Sanyal, Atindriyo and Ling, Xiao and Goel, Karan and Leszczynski, Megan},
  year = {2021},
  month = jul,
  journal = {Proceedings of the VLDB Endowment},
  booktitle = {Proceedings of the VLDB Endowment},
  publisher = {Association for Computing Machinery (ACM)},
  volume = {14},
  number = {12},
  pages = {3178--3181},
  doi = {10.14778/3476311.3476402},
  issn = {2150-8097},
  url = {https://doi.org/10.14778/3476311.3476402},
  source = {Crossref},
  subtitle = {feature stores and the coming wave of embedding ecosystems},
}

@inproceedings{polyzotis2017data,
  title = {Data Management Challenges in Production Machine Learning},
  author = {Polyzotis, Neoklis and Roy, Sudip and Whang, Steven Euijong and Zinkevich, Martin},
  year = {2017},
  month = may,
  booktitle = {Proceedings of the 2017 ACM International Conference on Management of Data},
  publisher = {ACM},
  pages = {1723--1726},
  doi = {10.1145/3035918.3054782},
  url = {https://doi.org/10.1145/3035918.3054782},
  source = {Crossref},
  organization = {ACM},
}

% Cold Start and Model Loading
@inproceedings{romero2021infaas,
  title = {INFaaS: Automated Model-less Inference Serving},
  author = {Romero, Francisco and Li, Qian and Yadwadkar, Neeraja J. and Kozyrakis, Christos},
  year = {2021},
  booktitle = {2021 USENIX Annual Technical Conference (USENIX ATC 21)},
  pages = {397--411},
  organization = {USENIX Association},
}

% Training-Serving Skew
@incollection{sculley2015hidden,
  title = {Technical Debt in Machine Learning Systems},
  author = {
    Sculley, D. and Holt, Gary and Golovin, Daniel and Davydov, Eugene and Phillips, Todd and
    Ebner, Dietmar and Chaudhary, Vinay and Young, Michael and Crespo, Jean-Fran\c{c}ois and
    Dennison, Dan
  },
  year = {2021},
  month = aug,
  journal = {Advances in Neural Information Processing Systems},
  booktitle = {Technical Debt in Practice},
  publisher = {The MIT Press},
  volume = {28},
  pages = {177--192},
  doi = {10.7551/mitpress/12440.003.0011},
  isbn = {9780262366304},
  url = {https://doi.org/10.7551/mitpress/12440.003.0011},
  source = {Crossref},
}

% Latency Analysis
@inproceedings{shen2019nexus,
  title = {Nexus},
  author = {
    Shen, Haichen and Chen, Lequn and Jin, Yuchen and Zhao, Liangyu and Kong, Bingyu and Philipose,
    Matthai and Krishnamurthy, Arvind and Sundaram, Ravi
  },
  year = {2019},
  month = oct,
  booktitle = {Proceedings of the 27th ACM Symposium on Operating Systems Principles},
  publisher = {ACM},
  pages = {322--337},
  doi = {10.1145/3341301.3359658},
  url = {https://doi.org/10.1145/3341301.3359658},
  source = {Crossref},
  subtitle = {a GPU cluster engine for accelerating DNN-based video analysis},
  organization = {ACM},
}

% Model Serving Economics
@inproceedings{wu2019machine,
  title = {Machine Learning at Facebook: Understanding Inference at the Edge},
  author = {
    Wu, Carole-Jean and Brooks, David and Chen, Kevin and Chen, Douglas and Choudhury, Sy and
    Dukhan, Marat and Hazelwood, Kim and Isaac, Eldad and Jia, Yangqing and Jia, Bill and Leyvand,
    Tommer and Lu, Hao and Lu, Yang and Qiao, Lin and Reagen, Brandon and Spisak, Joe and Sun, Fei
    and Tulloch, Andrew and Vajda, Peter and Wang, Xiaodong and Wang, Yanghan and Wasti, Bram and
    Wu, Yiming and Xian, Ran and Yoo, Sungjoo and Zhang, Peizhao
  },
  year = {2019},
  month = feb,
  journal = {2019 IEEE International Symposium on High Performance Computer Architecture (HPCA)},
  booktitle = {2019 IEEE International Symposium on High Performance Computer Architecture (HPCA)},
  publisher = {IEEE},
  pages = {331--344},
  doi = {10.1109/hpca.2019.00048},
  url = {https://doi.org/10.1109/hpca.2019.00048},
  source = {Crossref},
}

% Continuous Batching and LLM Serving
@inproceedings{yu2022orca,
  title = {Orca: A Distributed Serving System for Transformer-Based Generative Models.},
  author = {Yu, Gyeong-In and Jeong, Joo Seong and Kim, Geon-Woo and Kim, Soojeong and Chun, Byung-Gon},
  year = {2022},
  journal = {OSDI},
  booktitle = {16th USENIX Symposium on Operating Systems Design and Implementation (OSDI 22)},
  pages = {521--538},
  url = {https://www.usenix.org/conference/osdi22/presentation/yu},
  source = {DBLP},
  organization = {USENIX Association},
}

@inproceedings{zhang2019mark,
  title = {
    Mark: Exploiting Cloud Services for Cost-Effective, SLO-Aware Machine Learning Inference
    Serving
  },
  author = {Zhang, Chengliang and Yu, Minchen and Wang, Wei and Yan, Feng},
  year = {2019},
  booktitle = {2019 USENIX Annual Technical Conference (USENIX ATC 19)},
  pages = {1049--1062},
  organization = {USENIX Association},
}
