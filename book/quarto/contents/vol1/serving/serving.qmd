---
bibliography: serving.bib
quiz: serving_quizzes.json
concepts: serving_concepts.yml
glossary: serving_glossary.json
---

# Serving {#sec-serving}

::: {.callout-tip title="Learning Objectives"}

- Distinguish between training and inference optimization objectives, explaining why latency constraints fundamentally change system design
- Analyze end-to-end serving latency by decomposing requests into preprocessing, inference, and postprocessing stages
- Identify preprocessing bottlenecks and implement strategies to maintain parity between training and serving pipelines
- Apply batching strategies that balance throughput and latency to meet service level objectives
- Evaluate model loading strategies and memory management techniques for production serving
- Implement appropriate postprocessing pipelines including decoding strategies and output formatting
- Design request handling systems that manage concurrency, backpressure, and graceful degradation
- Profile serving systems to identify bottlenecks and optimize hardware utilization

:::

## Purpose {.unnumbered}

This chapter addresses a fundamental question in machine learning systems: how do we transform a trained model into a responsive service that delivers predictions reliably and efficiently? While training optimizes for throughput over hours or days, serving demands consistent, low-latency responses for individual requests. This shift from batch processing to real-time response creates entirely different system requirements that many practitioners underestimate.

Understanding serving fundamentals is essential because even the most sophisticated model provides no value until it serves predictions to users. The techniques covered here, from preprocessing optimization to error handling, form the foundation for all deployment scenarios. A common misconception is that faster GPUs automatically mean faster serving. In practice, preprocessing, data movement, and postprocessing often dominate latency. Mastering single-machine serving prepares you for the distributed scaling challenges addressed in Volume II while providing immediately applicable skills for production deployment.

## The Serving Mindset {#sec-serving-mindset}

TODO: Mental model shift from training to inference optimization
- Latency vs throughput trade-offs
- Anatomy of end-to-end latency
- Why p99 latency matters more than mean latency
- Latency budgets and allocation across pipeline stages

## Preprocessing: The Hidden Bottleneck {#sec-serving-preprocessing}

TODO: Input transformation before model inference
- Input validation and sanitization
- Normalization and feature scaling (must match training exactly)
- Tokenization for text models
- Image preprocessing (resizing, color normalization, format conversion)
- Feature extraction for tabular data
- Critical importance of preprocessing parity between training and serving
- CPU vs GPU preprocessing trade-offs

## Model Loading and Memory Management {#sec-serving-model-loading}

TODO: How models occupy memory and loading strategies
- Model serialization formats (SavedModel, ONNX, TorchScript)
- Cold-start latency: disk to CPU to GPU
- Memory mapping vs full loading
- Multi-model serving on single machine
- GPU memory allocation and fragmentation
- Model warmup (why first inference is slower)

## Inference Optimization {#sec-serving-inference-optimization}

TODO: Making model computation fast
- Batching strategies: static, dynamic, continuous batching for LLMs
- Batching trade-offs (throughput vs individual request latency)
- Quantization for inference (INT8, FP16)
- Graph optimization: operator fusion, constant folding
- Runtime selection (TensorRT, ONNX Runtime, TorchScript)
- KV-cache management for autoregressive models

## Postprocessing: From Tensors to Responses {#sec-serving-postprocessing}

TODO: Converting raw outputs to useful responses
- Output decoding: softmax, beam search, sampling strategies
- Structured output generation (JSON mode, constrained decoding)
- Confidence thresholds and fallback logic
- Multi-head output handling
- Response formatting and serialization
- Streaming responses for generative models

## Request Handling and Concurrency {#sec-serving-concurrency}

TODO: Managing multiple concurrent requests
- Synchronous vs asynchronous request handling
- Thread pools, async/await, process-based concurrency
- The GIL problem in Python and workarounds
- Request queuing and backpressure
- Timeout handling and graceful degradation
- Connection management

## Hardware Utilization and Profiling {#sec-serving-profiling}

TODO: Measuring and optimizing performance
- GPU utilization metrics and what they mean
- Profiling tools (NVIDIA Nsight, PyTorch Profiler)
- Identifying bottlenecks: CPU-bound, GPU-bound, memory-bound, I/O-bound
- Power and thermal considerations
- Multi-GPU inference on single machine

## Serving Frameworks and APIs {#sec-serving-frameworks}

TODO: Evaluating and using serving infrastructure
- Framework comparison (TorchServe, Triton, TensorFlow Serving, vLLM)
- REST vs gRPC trade-offs
- Protocol buffers and serialization
- Health checks and readiness probes
- Graceful shutdown and connection draining

## Error Handling and Reliability {#sec-serving-reliability}

TODO: Handling failures gracefully
- Input validation and error responses
- OOM handling and recovery
- Timeout implementation
- Retry logic with exponential backoff
- Circuit breakers
- Logging and debugging in production

## Security Considerations {#sec-serving-security}

TODO: Security implications of ML serving
- Input validation as security (prompt injection, adversarial inputs)
- Model access control
- Rate limiting
- Output filtering (PII detection, content safety)
- Audit logging

## Cost and Resource Planning {#sec-serving-cost}

TODO: Operational economics
- Estimating compute requirements from latency SLOs
- Cost per inference calculation
- GPU vs CPU inference trade-offs
- Right-sizing and model selection
- Capacity planning for traffic patterns

## Summary {#sec-serving-summary}

TODO: Chapter synthesis and bridge to MLOps

## Exercises {#sec-serving-exercises}

TODO: Hands-on exercises for serving systems
